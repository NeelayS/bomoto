Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=102, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5712-5767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899473
Iteration 2/25 | Loss: 0.00151515
Iteration 3/25 | Loss: 0.00115999
Iteration 4/25 | Loss: 0.00108715
Iteration 5/25 | Loss: 0.00106973
Iteration 6/25 | Loss: 0.00107239
Iteration 7/25 | Loss: 0.00106081
Iteration 8/25 | Loss: 0.00105395
Iteration 9/25 | Loss: 0.00105471
Iteration 10/25 | Loss: 0.00105417
Iteration 11/25 | Loss: 0.00104042
Iteration 12/25 | Loss: 0.00103447
Iteration 13/25 | Loss: 0.00103345
Iteration 14/25 | Loss: 0.00103359
Iteration 15/25 | Loss: 0.00103354
Iteration 16/25 | Loss: 0.00103110
Iteration 17/25 | Loss: 0.00104312
Iteration 18/25 | Loss: 0.00104505
Iteration 19/25 | Loss: 0.00103508
Iteration 20/25 | Loss: 0.00102413
Iteration 21/25 | Loss: 0.00101904
Iteration 22/25 | Loss: 0.00101883
Iteration 23/25 | Loss: 0.00101667
Iteration 24/25 | Loss: 0.00101323
Iteration 25/25 | Loss: 0.00101453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33528495
Iteration 2/25 | Loss: 0.00273539
Iteration 3/25 | Loss: 0.00273539
Iteration 4/25 | Loss: 0.00273539
Iteration 5/25 | Loss: 0.00273539
Iteration 6/25 | Loss: 0.00273539
Iteration 7/25 | Loss: 0.00273539
Iteration 8/25 | Loss: 0.00273539
Iteration 9/25 | Loss: 0.00273539
Iteration 10/25 | Loss: 0.00273539
Iteration 11/25 | Loss: 0.00273539
Iteration 12/25 | Loss: 0.00273539
Iteration 13/25 | Loss: 0.00273539
Iteration 14/25 | Loss: 0.00273539
Iteration 15/25 | Loss: 0.00273539
Iteration 16/25 | Loss: 0.00273539
Iteration 17/25 | Loss: 0.00273539
Iteration 18/25 | Loss: 0.00273539
Iteration 19/25 | Loss: 0.00273539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002735388232395053, 0.002735388232395053, 0.002735388232395053, 0.002735388232395053, 0.002735388232395053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002735388232395053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273539
Iteration 2/1000 | Loss: 0.00070663
Iteration 3/1000 | Loss: 0.00051731
Iteration 4/1000 | Loss: 0.00067914
Iteration 5/1000 | Loss: 0.00032526
Iteration 6/1000 | Loss: 0.00065115
Iteration 7/1000 | Loss: 0.00038948
Iteration 8/1000 | Loss: 0.00074310
Iteration 9/1000 | Loss: 0.00050158
Iteration 10/1000 | Loss: 0.00078283
Iteration 11/1000 | Loss: 0.00029092
Iteration 12/1000 | Loss: 0.00069610
Iteration 13/1000 | Loss: 0.00045655
Iteration 14/1000 | Loss: 0.00082527
Iteration 15/1000 | Loss: 0.00075152
Iteration 16/1000 | Loss: 0.00097561
Iteration 17/1000 | Loss: 0.00040765
Iteration 18/1000 | Loss: 0.00161606
Iteration 19/1000 | Loss: 0.00063172
Iteration 20/1000 | Loss: 0.00048603
Iteration 21/1000 | Loss: 0.00067439
Iteration 22/1000 | Loss: 0.00024728
Iteration 23/1000 | Loss: 0.00043044
Iteration 24/1000 | Loss: 0.00071810
Iteration 25/1000 | Loss: 0.00017721
Iteration 26/1000 | Loss: 0.00016888
Iteration 27/1000 | Loss: 0.00065028
Iteration 28/1000 | Loss: 0.00021607
Iteration 29/1000 | Loss: 0.00016153
Iteration 30/1000 | Loss: 0.00014870
Iteration 31/1000 | Loss: 0.00014372
Iteration 32/1000 | Loss: 0.00013828
Iteration 33/1000 | Loss: 0.00013399
Iteration 34/1000 | Loss: 0.00012989
Iteration 35/1000 | Loss: 0.00129168
Iteration 36/1000 | Loss: 0.00012938
Iteration 37/1000 | Loss: 0.00012604
Iteration 38/1000 | Loss: 0.00012194
Iteration 39/1000 | Loss: 0.00011844
Iteration 40/1000 | Loss: 0.00129944
Iteration 41/1000 | Loss: 0.01508050
Iteration 42/1000 | Loss: 0.00731235
Iteration 43/1000 | Loss: 0.00460436
Iteration 44/1000 | Loss: 0.00099422
Iteration 45/1000 | Loss: 0.00149141
Iteration 46/1000 | Loss: 0.00127145
Iteration 47/1000 | Loss: 0.00126026
Iteration 48/1000 | Loss: 0.00072126
Iteration 49/1000 | Loss: 0.00015129
Iteration 50/1000 | Loss: 0.00017753
Iteration 51/1000 | Loss: 0.00114396
Iteration 52/1000 | Loss: 0.00089301
Iteration 53/1000 | Loss: 0.00092099
Iteration 54/1000 | Loss: 0.00129491
Iteration 55/1000 | Loss: 0.00069277
Iteration 56/1000 | Loss: 0.00094654
Iteration 57/1000 | Loss: 0.00104409
Iteration 58/1000 | Loss: 0.00090028
Iteration 59/1000 | Loss: 0.00089491
Iteration 60/1000 | Loss: 0.00074686
Iteration 61/1000 | Loss: 0.00009176
Iteration 62/1000 | Loss: 0.00224883
Iteration 63/1000 | Loss: 0.00053388
Iteration 64/1000 | Loss: 0.00012510
Iteration 65/1000 | Loss: 0.00010575
Iteration 66/1000 | Loss: 0.00007704
Iteration 67/1000 | Loss: 0.00007278
Iteration 68/1000 | Loss: 0.00006186
Iteration 69/1000 | Loss: 0.00021871
Iteration 70/1000 | Loss: 0.00113822
Iteration 71/1000 | Loss: 0.00061045
Iteration 72/1000 | Loss: 0.00067611
Iteration 73/1000 | Loss: 0.00006801
Iteration 74/1000 | Loss: 0.00005704
Iteration 75/1000 | Loss: 0.00005189
Iteration 76/1000 | Loss: 0.00004863
Iteration 77/1000 | Loss: 0.00098486
Iteration 78/1000 | Loss: 0.00025212
Iteration 79/1000 | Loss: 0.00093100
Iteration 80/1000 | Loss: 0.00047255
Iteration 81/1000 | Loss: 0.00080939
Iteration 82/1000 | Loss: 0.00054815
Iteration 83/1000 | Loss: 0.00056253
Iteration 84/1000 | Loss: 0.00004582
Iteration 85/1000 | Loss: 0.00004053
Iteration 86/1000 | Loss: 0.00003890
Iteration 87/1000 | Loss: 0.00108492
Iteration 88/1000 | Loss: 0.00004912
Iteration 89/1000 | Loss: 0.00004006
Iteration 90/1000 | Loss: 0.00003676
Iteration 91/1000 | Loss: 0.00003449
Iteration 92/1000 | Loss: 0.00003196
Iteration 93/1000 | Loss: 0.00003056
Iteration 94/1000 | Loss: 0.00002987
Iteration 95/1000 | Loss: 0.00002921
Iteration 96/1000 | Loss: 0.00002883
Iteration 97/1000 | Loss: 0.00002829
Iteration 98/1000 | Loss: 0.00002801
Iteration 99/1000 | Loss: 0.00002795
Iteration 100/1000 | Loss: 0.00002789
Iteration 101/1000 | Loss: 0.00002765
Iteration 102/1000 | Loss: 0.00089204
Iteration 103/1000 | Loss: 0.00003291
Iteration 104/1000 | Loss: 0.00002765
Iteration 105/1000 | Loss: 0.00002592
Iteration 106/1000 | Loss: 0.00002484
Iteration 107/1000 | Loss: 0.00002416
Iteration 108/1000 | Loss: 0.00002377
Iteration 109/1000 | Loss: 0.00002353
Iteration 110/1000 | Loss: 0.00002343
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002335
Iteration 113/1000 | Loss: 0.00002328
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00002325
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002324
Iteration 120/1000 | Loss: 0.00002323
Iteration 121/1000 | Loss: 0.00002316
Iteration 122/1000 | Loss: 0.00002309
Iteration 123/1000 | Loss: 0.00002306
Iteration 124/1000 | Loss: 0.00002306
Iteration 125/1000 | Loss: 0.00002306
Iteration 126/1000 | Loss: 0.00002306
Iteration 127/1000 | Loss: 0.00002306
Iteration 128/1000 | Loss: 0.00002306
Iteration 129/1000 | Loss: 0.00002306
Iteration 130/1000 | Loss: 0.00002306
Iteration 131/1000 | Loss: 0.00002306
Iteration 132/1000 | Loss: 0.00002306
Iteration 133/1000 | Loss: 0.00002306
Iteration 134/1000 | Loss: 0.00002305
Iteration 135/1000 | Loss: 0.00002305
Iteration 136/1000 | Loss: 0.00002305
Iteration 137/1000 | Loss: 0.00002305
Iteration 138/1000 | Loss: 0.00002305
Iteration 139/1000 | Loss: 0.00002305
Iteration 140/1000 | Loss: 0.00002305
Iteration 141/1000 | Loss: 0.00002304
Iteration 142/1000 | Loss: 0.00002304
Iteration 143/1000 | Loss: 0.00002304
Iteration 144/1000 | Loss: 0.00002304
Iteration 145/1000 | Loss: 0.00002304
Iteration 146/1000 | Loss: 0.00002304
Iteration 147/1000 | Loss: 0.00002304
Iteration 148/1000 | Loss: 0.00002304
Iteration 149/1000 | Loss: 0.00002304
Iteration 150/1000 | Loss: 0.00002303
Iteration 151/1000 | Loss: 0.00002303
Iteration 152/1000 | Loss: 0.00002303
Iteration 153/1000 | Loss: 0.00002303
Iteration 154/1000 | Loss: 0.00002302
Iteration 155/1000 | Loss: 0.00002302
Iteration 156/1000 | Loss: 0.00002302
Iteration 157/1000 | Loss: 0.00002302
Iteration 158/1000 | Loss: 0.00002302
Iteration 159/1000 | Loss: 0.00002302
Iteration 160/1000 | Loss: 0.00002301
Iteration 161/1000 | Loss: 0.00002301
Iteration 162/1000 | Loss: 0.00002301
Iteration 163/1000 | Loss: 0.00002300
Iteration 164/1000 | Loss: 0.00002300
Iteration 165/1000 | Loss: 0.00002300
Iteration 166/1000 | Loss: 0.00002300
Iteration 167/1000 | Loss: 0.00002299
Iteration 168/1000 | Loss: 0.00002299
Iteration 169/1000 | Loss: 0.00002299
Iteration 170/1000 | Loss: 0.00002299
Iteration 171/1000 | Loss: 0.00002299
Iteration 172/1000 | Loss: 0.00002298
Iteration 173/1000 | Loss: 0.00002298
Iteration 174/1000 | Loss: 0.00002298
Iteration 175/1000 | Loss: 0.00002298
Iteration 176/1000 | Loss: 0.00002298
Iteration 177/1000 | Loss: 0.00002298
Iteration 178/1000 | Loss: 0.00002298
Iteration 179/1000 | Loss: 0.00002298
Iteration 180/1000 | Loss: 0.00002298
Iteration 181/1000 | Loss: 0.00002298
Iteration 182/1000 | Loss: 0.00002298
Iteration 183/1000 | Loss: 0.00002297
Iteration 184/1000 | Loss: 0.00002297
Iteration 185/1000 | Loss: 0.00002297
Iteration 186/1000 | Loss: 0.00002297
Iteration 187/1000 | Loss: 0.00002297
Iteration 188/1000 | Loss: 0.00002296
Iteration 189/1000 | Loss: 0.00002296
Iteration 190/1000 | Loss: 0.00002296
Iteration 191/1000 | Loss: 0.00002296
Iteration 192/1000 | Loss: 0.00002296
Iteration 193/1000 | Loss: 0.00002296
Iteration 194/1000 | Loss: 0.00002296
Iteration 195/1000 | Loss: 0.00002296
Iteration 196/1000 | Loss: 0.00002296
Iteration 197/1000 | Loss: 0.00002296
Iteration 198/1000 | Loss: 0.00002296
Iteration 199/1000 | Loss: 0.00002296
Iteration 200/1000 | Loss: 0.00002296
Iteration 201/1000 | Loss: 0.00002296
Iteration 202/1000 | Loss: 0.00002296
Iteration 203/1000 | Loss: 0.00002296
Iteration 204/1000 | Loss: 0.00002295
Iteration 205/1000 | Loss: 0.00002295
Iteration 206/1000 | Loss: 0.00002295
Iteration 207/1000 | Loss: 0.00002295
Iteration 208/1000 | Loss: 0.00002295
Iteration 209/1000 | Loss: 0.00002295
Iteration 210/1000 | Loss: 0.00002295
Iteration 211/1000 | Loss: 0.00002295
Iteration 212/1000 | Loss: 0.00002294
Iteration 213/1000 | Loss: 0.00002294
Iteration 214/1000 | Loss: 0.00002294
Iteration 215/1000 | Loss: 0.00002294
Iteration 216/1000 | Loss: 0.00002294
Iteration 217/1000 | Loss: 0.00002293
Iteration 218/1000 | Loss: 0.00002293
Iteration 219/1000 | Loss: 0.00002293
Iteration 220/1000 | Loss: 0.00002293
Iteration 221/1000 | Loss: 0.00002293
Iteration 222/1000 | Loss: 0.00002293
Iteration 223/1000 | Loss: 0.00002293
Iteration 224/1000 | Loss: 0.00002293
Iteration 225/1000 | Loss: 0.00002293
Iteration 226/1000 | Loss: 0.00002293
Iteration 227/1000 | Loss: 0.00002293
Iteration 228/1000 | Loss: 0.00002293
Iteration 229/1000 | Loss: 0.00002293
Iteration 230/1000 | Loss: 0.00002293
Iteration 231/1000 | Loss: 0.00002293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.2931553758098744e-05, 2.2931553758098744e-05, 2.2931553758098744e-05, 2.2931553758098744e-05, 2.2931553758098744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2931553758098744e-05

Optimization complete. Final v2v error: 3.9528353214263916 mm

Highest mean error: 4.790337085723877 mm for frame 109

Lowest mean error: 3.690133571624756 mm for frame 13

Saving results

Total time: 208.22503757476807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021969
Iteration 2/25 | Loss: 0.00292442
Iteration 3/25 | Loss: 0.00186750
Iteration 4/25 | Loss: 0.00169776
Iteration 5/25 | Loss: 0.00153328
Iteration 6/25 | Loss: 0.00154784
Iteration 7/25 | Loss: 0.00152546
Iteration 8/25 | Loss: 0.00145730
Iteration 9/25 | Loss: 0.00139937
Iteration 10/25 | Loss: 0.00131311
Iteration 11/25 | Loss: 0.00126597
Iteration 12/25 | Loss: 0.00121680
Iteration 13/25 | Loss: 0.00120443
Iteration 14/25 | Loss: 0.00118167
Iteration 15/25 | Loss: 0.00116197
Iteration 16/25 | Loss: 0.00116681
Iteration 17/25 | Loss: 0.00114732
Iteration 18/25 | Loss: 0.00114289
Iteration 19/25 | Loss: 0.00113357
Iteration 20/25 | Loss: 0.00112720
Iteration 21/25 | Loss: 0.00112940
Iteration 22/25 | Loss: 0.00112374
Iteration 23/25 | Loss: 0.00112378
Iteration 24/25 | Loss: 0.00112623
Iteration 25/25 | Loss: 0.00112018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51647723
Iteration 2/25 | Loss: 0.00463494
Iteration 3/25 | Loss: 0.00270085
Iteration 4/25 | Loss: 0.00270084
Iteration 5/25 | Loss: 0.00270084
Iteration 6/25 | Loss: 0.00270084
Iteration 7/25 | Loss: 0.00270084
Iteration 8/25 | Loss: 0.00270084
Iteration 9/25 | Loss: 0.00270084
Iteration 10/25 | Loss: 0.00270084
Iteration 11/25 | Loss: 0.00270084
Iteration 12/25 | Loss: 0.00270084
Iteration 13/25 | Loss: 0.00270084
Iteration 14/25 | Loss: 0.00270084
Iteration 15/25 | Loss: 0.00270084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027008424513041973, 0.0027008424513041973, 0.0027008424513041973, 0.0027008424513041973, 0.0027008424513041973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027008424513041973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270084
Iteration 2/1000 | Loss: 0.00294118
Iteration 3/1000 | Loss: 0.00274350
Iteration 4/1000 | Loss: 0.00234898
Iteration 5/1000 | Loss: 0.00089094
Iteration 6/1000 | Loss: 0.00278460
Iteration 7/1000 | Loss: 0.00129022
Iteration 8/1000 | Loss: 0.00208685
Iteration 9/1000 | Loss: 0.00103715
Iteration 10/1000 | Loss: 0.00135565
Iteration 11/1000 | Loss: 0.00110613
Iteration 12/1000 | Loss: 0.00103095
Iteration 13/1000 | Loss: 0.00085659
Iteration 14/1000 | Loss: 0.00093345
Iteration 15/1000 | Loss: 0.00120512
Iteration 16/1000 | Loss: 0.00042431
Iteration 17/1000 | Loss: 0.00040159
Iteration 18/1000 | Loss: 0.00055720
Iteration 19/1000 | Loss: 0.00035485
Iteration 20/1000 | Loss: 0.00023455
Iteration 21/1000 | Loss: 0.00038249
Iteration 22/1000 | Loss: 0.00023920
Iteration 23/1000 | Loss: 0.00020147
Iteration 24/1000 | Loss: 0.00047735
Iteration 25/1000 | Loss: 0.00073973
Iteration 26/1000 | Loss: 0.00024653
Iteration 27/1000 | Loss: 0.00044055
Iteration 28/1000 | Loss: 0.00097018
Iteration 29/1000 | Loss: 0.00075809
Iteration 30/1000 | Loss: 0.00047654
Iteration 31/1000 | Loss: 0.00042779
Iteration 32/1000 | Loss: 0.00034968
Iteration 33/1000 | Loss: 0.00059086
Iteration 34/1000 | Loss: 0.00023201
Iteration 35/1000 | Loss: 0.00017553
Iteration 36/1000 | Loss: 0.00042950
Iteration 37/1000 | Loss: 0.00017459
Iteration 38/1000 | Loss: 0.00024915
Iteration 39/1000 | Loss: 0.00022143
Iteration 40/1000 | Loss: 0.00021485
Iteration 41/1000 | Loss: 0.00017385
Iteration 42/1000 | Loss: 0.00022930
Iteration 43/1000 | Loss: 0.00015743
Iteration 44/1000 | Loss: 0.00062149
Iteration 45/1000 | Loss: 0.00054982
Iteration 46/1000 | Loss: 0.00086788
Iteration 47/1000 | Loss: 0.00059090
Iteration 48/1000 | Loss: 0.00098846
Iteration 49/1000 | Loss: 0.00091035
Iteration 50/1000 | Loss: 0.00080890
Iteration 51/1000 | Loss: 0.00111262
Iteration 52/1000 | Loss: 0.00183599
Iteration 53/1000 | Loss: 0.00093352
Iteration 54/1000 | Loss: 0.00018618
Iteration 55/1000 | Loss: 0.00016487
Iteration 56/1000 | Loss: 0.00017535
Iteration 57/1000 | Loss: 0.00016691
Iteration 58/1000 | Loss: 0.00043653
Iteration 59/1000 | Loss: 0.00016145
Iteration 60/1000 | Loss: 0.00015261
Iteration 61/1000 | Loss: 0.00015584
Iteration 62/1000 | Loss: 0.00030625
Iteration 63/1000 | Loss: 0.00040505
Iteration 64/1000 | Loss: 0.00084836
Iteration 65/1000 | Loss: 0.00051409
Iteration 66/1000 | Loss: 0.00044185
Iteration 67/1000 | Loss: 0.00019584
Iteration 68/1000 | Loss: 0.00027520
Iteration 69/1000 | Loss: 0.00025374
Iteration 70/1000 | Loss: 0.00016176
Iteration 71/1000 | Loss: 0.00014855
Iteration 72/1000 | Loss: 0.00015241
Iteration 73/1000 | Loss: 0.00015583
Iteration 74/1000 | Loss: 0.00015998
Iteration 75/1000 | Loss: 0.00045335
Iteration 76/1000 | Loss: 0.00025557
Iteration 77/1000 | Loss: 0.00043679
Iteration 78/1000 | Loss: 0.00015626
Iteration 79/1000 | Loss: 0.00027391
Iteration 80/1000 | Loss: 0.00017370
Iteration 81/1000 | Loss: 0.00016149
Iteration 82/1000 | Loss: 0.00021412
Iteration 83/1000 | Loss: 0.00015539
Iteration 84/1000 | Loss: 0.00025584
Iteration 85/1000 | Loss: 0.00052683
Iteration 86/1000 | Loss: 0.00036538
Iteration 87/1000 | Loss: 0.00021153
Iteration 88/1000 | Loss: 0.00015436
Iteration 89/1000 | Loss: 0.00015595
Iteration 90/1000 | Loss: 0.00014941
Iteration 91/1000 | Loss: 0.00015076
Iteration 92/1000 | Loss: 0.00015658
Iteration 93/1000 | Loss: 0.00029903
Iteration 94/1000 | Loss: 0.00023637
Iteration 95/1000 | Loss: 0.00023111
Iteration 96/1000 | Loss: 0.00057451
Iteration 97/1000 | Loss: 0.00020220
Iteration 98/1000 | Loss: 0.00028255
Iteration 99/1000 | Loss: 0.00065389
Iteration 100/1000 | Loss: 0.00019316
Iteration 101/1000 | Loss: 0.00020691
Iteration 102/1000 | Loss: 0.00018934
Iteration 103/1000 | Loss: 0.00033130
Iteration 104/1000 | Loss: 0.00027556
Iteration 105/1000 | Loss: 0.00020744
Iteration 106/1000 | Loss: 0.00019854
Iteration 107/1000 | Loss: 0.00021272
Iteration 108/1000 | Loss: 0.00022280
Iteration 109/1000 | Loss: 0.00021458
Iteration 110/1000 | Loss: 0.00021205
Iteration 111/1000 | Loss: 0.00019323
Iteration 112/1000 | Loss: 0.00027908
Iteration 113/1000 | Loss: 0.00025241
Iteration 114/1000 | Loss: 0.00043461
Iteration 115/1000 | Loss: 0.00057114
Iteration 116/1000 | Loss: 0.00021399
Iteration 117/1000 | Loss: 0.00058115
Iteration 118/1000 | Loss: 0.00024613
Iteration 119/1000 | Loss: 0.00016831
Iteration 120/1000 | Loss: 0.00014570
Iteration 121/1000 | Loss: 0.00014435
Iteration 122/1000 | Loss: 0.00032679
Iteration 123/1000 | Loss: 0.00133050
Iteration 124/1000 | Loss: 0.00099725
Iteration 125/1000 | Loss: 0.00104848
Iteration 126/1000 | Loss: 0.00044850
Iteration 127/1000 | Loss: 0.00015941
Iteration 128/1000 | Loss: 0.00014471
Iteration 129/1000 | Loss: 0.00027720
Iteration 130/1000 | Loss: 0.00014291
Iteration 131/1000 | Loss: 0.00017287
Iteration 132/1000 | Loss: 0.00014170
Iteration 133/1000 | Loss: 0.00018380
Iteration 134/1000 | Loss: 0.00055123
Iteration 135/1000 | Loss: 0.00050562
Iteration 136/1000 | Loss: 0.00048794
Iteration 137/1000 | Loss: 0.00021144
Iteration 138/1000 | Loss: 0.00022413
Iteration 139/1000 | Loss: 0.00042097
Iteration 140/1000 | Loss: 0.00014372
Iteration 141/1000 | Loss: 0.00014148
Iteration 142/1000 | Loss: 0.00014084
Iteration 143/1000 | Loss: 0.00034834
Iteration 144/1000 | Loss: 0.00024132
Iteration 145/1000 | Loss: 0.00016989
Iteration 146/1000 | Loss: 0.00038530
Iteration 147/1000 | Loss: 0.00069424
Iteration 148/1000 | Loss: 0.00120947
Iteration 149/1000 | Loss: 0.00031952
Iteration 150/1000 | Loss: 0.00021570
Iteration 151/1000 | Loss: 0.00022616
Iteration 152/1000 | Loss: 0.00021324
Iteration 153/1000 | Loss: 0.00021742
Iteration 154/1000 | Loss: 0.00027672
Iteration 155/1000 | Loss: 0.00021575
Iteration 156/1000 | Loss: 0.00033672
Iteration 157/1000 | Loss: 0.00023738
Iteration 158/1000 | Loss: 0.00028702
Iteration 159/1000 | Loss: 0.00015722
Iteration 160/1000 | Loss: 0.00021860
Iteration 161/1000 | Loss: 0.00021430
Iteration 162/1000 | Loss: 0.00017766
Iteration 163/1000 | Loss: 0.00023119
Iteration 164/1000 | Loss: 0.00020967
Iteration 165/1000 | Loss: 0.00013974
Iteration 166/1000 | Loss: 0.00034935
Iteration 167/1000 | Loss: 0.00013902
Iteration 168/1000 | Loss: 0.00022831
Iteration 169/1000 | Loss: 0.00038211
Iteration 170/1000 | Loss: 0.00022885
Iteration 171/1000 | Loss: 0.00015540
Iteration 172/1000 | Loss: 0.00014524
Iteration 173/1000 | Loss: 0.00013795
Iteration 174/1000 | Loss: 0.00013764
Iteration 175/1000 | Loss: 0.00027651
Iteration 176/1000 | Loss: 0.00057212
Iteration 177/1000 | Loss: 0.00016287
Iteration 178/1000 | Loss: 0.00016690
Iteration 179/1000 | Loss: 0.00013986
Iteration 180/1000 | Loss: 0.00013881
Iteration 181/1000 | Loss: 0.00030730
Iteration 182/1000 | Loss: 0.00032462
Iteration 183/1000 | Loss: 0.00034695
Iteration 184/1000 | Loss: 0.00046192
Iteration 185/1000 | Loss: 0.00029858
Iteration 186/1000 | Loss: 0.00046181
Iteration 187/1000 | Loss: 0.00034582
Iteration 188/1000 | Loss: 0.00016054
Iteration 189/1000 | Loss: 0.00015609
Iteration 190/1000 | Loss: 0.00013839
Iteration 191/1000 | Loss: 0.00013740
Iteration 192/1000 | Loss: 0.00013675
Iteration 193/1000 | Loss: 0.00013644
Iteration 194/1000 | Loss: 0.00025572
Iteration 195/1000 | Loss: 0.00031920
Iteration 196/1000 | Loss: 0.00020109
Iteration 197/1000 | Loss: 0.00020945
Iteration 198/1000 | Loss: 0.00035475
Iteration 199/1000 | Loss: 0.00014681
Iteration 200/1000 | Loss: 0.00021986
Iteration 201/1000 | Loss: 0.00014398
Iteration 202/1000 | Loss: 0.00014083
Iteration 203/1000 | Loss: 0.00016138
Iteration 204/1000 | Loss: 0.00018058
Iteration 205/1000 | Loss: 0.00013920
Iteration 206/1000 | Loss: 0.00044475
Iteration 207/1000 | Loss: 0.00014140
Iteration 208/1000 | Loss: 0.00017950
Iteration 209/1000 | Loss: 0.00023744
Iteration 210/1000 | Loss: 0.00019535
Iteration 211/1000 | Loss: 0.00014001
Iteration 212/1000 | Loss: 0.00022136
Iteration 213/1000 | Loss: 0.00021595
Iteration 214/1000 | Loss: 0.00019752
Iteration 215/1000 | Loss: 0.00020083
Iteration 216/1000 | Loss: 0.00019516
Iteration 217/1000 | Loss: 0.00020811
Iteration 218/1000 | Loss: 0.00075275
Iteration 219/1000 | Loss: 0.00032449
Iteration 220/1000 | Loss: 0.00027903
Iteration 221/1000 | Loss: 0.00015298
Iteration 222/1000 | Loss: 0.00018277
Iteration 223/1000 | Loss: 0.00047034
Iteration 224/1000 | Loss: 0.00021831
Iteration 225/1000 | Loss: 0.00017183
Iteration 226/1000 | Loss: 0.00019584
Iteration 227/1000 | Loss: 0.00023243
Iteration 228/1000 | Loss: 0.00020445
Iteration 229/1000 | Loss: 0.00024217
Iteration 230/1000 | Loss: 0.00022240
Iteration 231/1000 | Loss: 0.00023925
Iteration 232/1000 | Loss: 0.00021331
Iteration 233/1000 | Loss: 0.00066577
Iteration 234/1000 | Loss: 0.00027400
Iteration 235/1000 | Loss: 0.00026182
Iteration 236/1000 | Loss: 0.00020183
Iteration 237/1000 | Loss: 0.00070274
Iteration 238/1000 | Loss: 0.00025771
Iteration 239/1000 | Loss: 0.00034148
Iteration 240/1000 | Loss: 0.00024188
Iteration 241/1000 | Loss: 0.00034081
Iteration 242/1000 | Loss: 0.00020891
Iteration 243/1000 | Loss: 0.00021391
Iteration 244/1000 | Loss: 0.00019876
Iteration 245/1000 | Loss: 0.00054813
Iteration 246/1000 | Loss: 0.00045044
Iteration 247/1000 | Loss: 0.00056671
Iteration 248/1000 | Loss: 0.00022287
Iteration 249/1000 | Loss: 0.00014447
Iteration 250/1000 | Loss: 0.00014573
Iteration 251/1000 | Loss: 0.00013749
Iteration 252/1000 | Loss: 0.00021845
Iteration 253/1000 | Loss: 0.00013716
Iteration 254/1000 | Loss: 0.00013606
Iteration 255/1000 | Loss: 0.00013562
Iteration 256/1000 | Loss: 0.00013551
Iteration 257/1000 | Loss: 0.00013536
Iteration 258/1000 | Loss: 0.00013526
Iteration 259/1000 | Loss: 0.00013515
Iteration 260/1000 | Loss: 0.00027235
Iteration 261/1000 | Loss: 0.00020296
Iteration 262/1000 | Loss: 0.00013543
Iteration 263/1000 | Loss: 0.00027961
Iteration 264/1000 | Loss: 0.00018614
Iteration 265/1000 | Loss: 0.00016022
Iteration 266/1000 | Loss: 0.00014283
Iteration 267/1000 | Loss: 0.00014507
Iteration 268/1000 | Loss: 0.00026643
Iteration 269/1000 | Loss: 0.00015584
Iteration 270/1000 | Loss: 0.00013708
Iteration 271/1000 | Loss: 0.00021525
Iteration 272/1000 | Loss: 0.00014587
Iteration 273/1000 | Loss: 0.00014051
Iteration 274/1000 | Loss: 0.00040551
Iteration 275/1000 | Loss: 0.00042130
Iteration 276/1000 | Loss: 0.00038678
Iteration 277/1000 | Loss: 0.00154736
Iteration 278/1000 | Loss: 0.00016984
Iteration 279/1000 | Loss: 0.00013690
Iteration 280/1000 | Loss: 0.00013564
Iteration 281/1000 | Loss: 0.00022590
Iteration 282/1000 | Loss: 0.00018443
Iteration 283/1000 | Loss: 0.00013684
Iteration 284/1000 | Loss: 0.00038503
Iteration 285/1000 | Loss: 0.00014758
Iteration 286/1000 | Loss: 0.00022132
Iteration 287/1000 | Loss: 0.00015478
Iteration 288/1000 | Loss: 0.00027972
Iteration 289/1000 | Loss: 0.00014543
Iteration 290/1000 | Loss: 0.00022910
Iteration 291/1000 | Loss: 0.00014389
Iteration 292/1000 | Loss: 0.00023418
Iteration 293/1000 | Loss: 0.00019697
Iteration 294/1000 | Loss: 0.00013955
Iteration 295/1000 | Loss: 0.00013720
Iteration 296/1000 | Loss: 0.00013571
Iteration 297/1000 | Loss: 0.00013532
Iteration 298/1000 | Loss: 0.00059772
Iteration 299/1000 | Loss: 0.00014871
Iteration 300/1000 | Loss: 0.00013666
Iteration 301/1000 | Loss: 0.00022908
Iteration 302/1000 | Loss: 0.00014436
Iteration 303/1000 | Loss: 0.00025591
Iteration 304/1000 | Loss: 0.00015037
Iteration 305/1000 | Loss: 0.00013601
Iteration 306/1000 | Loss: 0.00013548
Iteration 307/1000 | Loss: 0.00041910
Iteration 308/1000 | Loss: 0.00014762
Iteration 309/1000 | Loss: 0.00013796
Iteration 310/1000 | Loss: 0.00024433
Iteration 311/1000 | Loss: 0.00015116
Iteration 312/1000 | Loss: 0.00015786
Iteration 313/1000 | Loss: 0.00015987
Iteration 314/1000 | Loss: 0.00013578
Iteration 315/1000 | Loss: 0.00041725
Iteration 316/1000 | Loss: 0.00014318
Iteration 317/1000 | Loss: 0.00014119
Iteration 318/1000 | Loss: 0.00037094
Iteration 319/1000 | Loss: 0.00014315
Iteration 320/1000 | Loss: 0.00013618
Iteration 321/1000 | Loss: 0.00026974
Iteration 322/1000 | Loss: 0.00014120
Iteration 323/1000 | Loss: 0.00013625
Iteration 324/1000 | Loss: 0.00023739
Iteration 325/1000 | Loss: 0.00017369
Iteration 326/1000 | Loss: 0.00013571
Iteration 327/1000 | Loss: 0.00019500
Iteration 328/1000 | Loss: 0.00013567
Iteration 329/1000 | Loss: 0.00013500
Iteration 330/1000 | Loss: 0.00026402
Iteration 331/1000 | Loss: 0.00013634
Iteration 332/1000 | Loss: 0.00014972
Iteration 333/1000 | Loss: 0.00013486
Iteration 334/1000 | Loss: 0.00013474
Iteration 335/1000 | Loss: 0.00013471
Iteration 336/1000 | Loss: 0.00013471
Iteration 337/1000 | Loss: 0.00013470
Iteration 338/1000 | Loss: 0.00013469
Iteration 339/1000 | Loss: 0.00013469
Iteration 340/1000 | Loss: 0.00013468
Iteration 341/1000 | Loss: 0.00013468
Iteration 342/1000 | Loss: 0.00013468
Iteration 343/1000 | Loss: 0.00013468
Iteration 344/1000 | Loss: 0.00013463
Iteration 345/1000 | Loss: 0.00013459
Iteration 346/1000 | Loss: 0.00013459
Iteration 347/1000 | Loss: 0.00013459
Iteration 348/1000 | Loss: 0.00013459
Iteration 349/1000 | Loss: 0.00013459
Iteration 350/1000 | Loss: 0.00013459
Iteration 351/1000 | Loss: 0.00013459
Iteration 352/1000 | Loss: 0.00013459
Iteration 353/1000 | Loss: 0.00013459
Iteration 354/1000 | Loss: 0.00013459
Iteration 355/1000 | Loss: 0.00013459
Iteration 356/1000 | Loss: 0.00013459
Iteration 357/1000 | Loss: 0.00013459
Iteration 358/1000 | Loss: 0.00013458
Iteration 359/1000 | Loss: 0.00013458
Iteration 360/1000 | Loss: 0.00013458
Iteration 361/1000 | Loss: 0.00013457
Iteration 362/1000 | Loss: 0.00013457
Iteration 363/1000 | Loss: 0.00013457
Iteration 364/1000 | Loss: 0.00013457
Iteration 365/1000 | Loss: 0.00013457
Iteration 366/1000 | Loss: 0.00013456
Iteration 367/1000 | Loss: 0.00013456
Iteration 368/1000 | Loss: 0.00013456
Iteration 369/1000 | Loss: 0.00013456
Iteration 370/1000 | Loss: 0.00013456
Iteration 371/1000 | Loss: 0.00013456
Iteration 372/1000 | Loss: 0.00013456
Iteration 373/1000 | Loss: 0.00013456
Iteration 374/1000 | Loss: 0.00013456
Iteration 375/1000 | Loss: 0.00013456
Iteration 376/1000 | Loss: 0.00013455
Iteration 377/1000 | Loss: 0.00013455
Iteration 378/1000 | Loss: 0.00013455
Iteration 379/1000 | Loss: 0.00013455
Iteration 380/1000 | Loss: 0.00013455
Iteration 381/1000 | Loss: 0.00013455
Iteration 382/1000 | Loss: 0.00013454
Iteration 383/1000 | Loss: 0.00013454
Iteration 384/1000 | Loss: 0.00013454
Iteration 385/1000 | Loss: 0.00013454
Iteration 386/1000 | Loss: 0.00013453
Iteration 387/1000 | Loss: 0.00013453
Iteration 388/1000 | Loss: 0.00013453
Iteration 389/1000 | Loss: 0.00013453
Iteration 390/1000 | Loss: 0.00013453
Iteration 391/1000 | Loss: 0.00013453
Iteration 392/1000 | Loss: 0.00013453
Iteration 393/1000 | Loss: 0.00013453
Iteration 394/1000 | Loss: 0.00013453
Iteration 395/1000 | Loss: 0.00013453
Iteration 396/1000 | Loss: 0.00013453
Iteration 397/1000 | Loss: 0.00013453
Iteration 398/1000 | Loss: 0.00013453
Iteration 399/1000 | Loss: 0.00013453
Iteration 400/1000 | Loss: 0.00013453
Iteration 401/1000 | Loss: 0.00013453
Iteration 402/1000 | Loss: 0.00013453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 402. Stopping optimization.
Last 5 losses: [0.00013452785788103938, 0.00013452785788103938, 0.00013452785788103938, 0.00013452785788103938, 0.00013452785788103938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013452785788103938

Optimization complete. Final v2v error: 6.396754264831543 mm

Highest mean error: 12.468220710754395 mm for frame 134

Lowest mean error: 3.749223470687866 mm for frame 104

Saving results

Total time: 588.3099648952484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381221
Iteration 2/25 | Loss: 0.00090196
Iteration 3/25 | Loss: 0.00072434
Iteration 4/25 | Loss: 0.00068154
Iteration 5/25 | Loss: 0.00066936
Iteration 6/25 | Loss: 0.00066622
Iteration 7/25 | Loss: 0.00066544
Iteration 8/25 | Loss: 0.00066528
Iteration 9/25 | Loss: 0.00066528
Iteration 10/25 | Loss: 0.00066528
Iteration 11/25 | Loss: 0.00066528
Iteration 12/25 | Loss: 0.00066528
Iteration 13/25 | Loss: 0.00066528
Iteration 14/25 | Loss: 0.00066528
Iteration 15/25 | Loss: 0.00066528
Iteration 16/25 | Loss: 0.00066528
Iteration 17/25 | Loss: 0.00066528
Iteration 18/25 | Loss: 0.00066528
Iteration 19/25 | Loss: 0.00066528
Iteration 20/25 | Loss: 0.00066528
Iteration 21/25 | Loss: 0.00066528
Iteration 22/25 | Loss: 0.00066528
Iteration 23/25 | Loss: 0.00066528
Iteration 24/25 | Loss: 0.00066528
Iteration 25/25 | Loss: 0.00066528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42711949
Iteration 2/25 | Loss: 0.00030419
Iteration 3/25 | Loss: 0.00030419
Iteration 4/25 | Loss: 0.00030419
Iteration 5/25 | Loss: 0.00030419
Iteration 6/25 | Loss: 0.00030419
Iteration 7/25 | Loss: 0.00030419
Iteration 8/25 | Loss: 0.00030419
Iteration 9/25 | Loss: 0.00030419
Iteration 10/25 | Loss: 0.00030419
Iteration 11/25 | Loss: 0.00030419
Iteration 12/25 | Loss: 0.00030419
Iteration 13/25 | Loss: 0.00030419
Iteration 14/25 | Loss: 0.00030419
Iteration 15/25 | Loss: 0.00030419
Iteration 16/25 | Loss: 0.00030419
Iteration 17/25 | Loss: 0.00030419
Iteration 18/25 | Loss: 0.00030419
Iteration 19/25 | Loss: 0.00030419
Iteration 20/25 | Loss: 0.00030419
Iteration 21/25 | Loss: 0.00030419
Iteration 22/25 | Loss: 0.00030419
Iteration 23/25 | Loss: 0.00030419
Iteration 24/25 | Loss: 0.00030419
Iteration 25/25 | Loss: 0.00030419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030419
Iteration 2/1000 | Loss: 0.00004750
Iteration 3/1000 | Loss: 0.00003656
Iteration 4/1000 | Loss: 0.00002953
Iteration 5/1000 | Loss: 0.00002729
Iteration 6/1000 | Loss: 0.00002595
Iteration 7/1000 | Loss: 0.00002484
Iteration 8/1000 | Loss: 0.00002416
Iteration 9/1000 | Loss: 0.00002364
Iteration 10/1000 | Loss: 0.00002313
Iteration 11/1000 | Loss: 0.00002286
Iteration 12/1000 | Loss: 0.00002274
Iteration 13/1000 | Loss: 0.00002269
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002252
Iteration 16/1000 | Loss: 0.00002247
Iteration 17/1000 | Loss: 0.00002240
Iteration 18/1000 | Loss: 0.00002239
Iteration 19/1000 | Loss: 0.00002239
Iteration 20/1000 | Loss: 0.00002239
Iteration 21/1000 | Loss: 0.00002238
Iteration 22/1000 | Loss: 0.00002238
Iteration 23/1000 | Loss: 0.00002238
Iteration 24/1000 | Loss: 0.00002236
Iteration 25/1000 | Loss: 0.00002232
Iteration 26/1000 | Loss: 0.00002231
Iteration 27/1000 | Loss: 0.00002230
Iteration 28/1000 | Loss: 0.00002230
Iteration 29/1000 | Loss: 0.00002228
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002226
Iteration 32/1000 | Loss: 0.00002224
Iteration 33/1000 | Loss: 0.00002224
Iteration 34/1000 | Loss: 0.00002223
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002220
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002219
Iteration 41/1000 | Loss: 0.00002218
Iteration 42/1000 | Loss: 0.00002217
Iteration 43/1000 | Loss: 0.00002217
Iteration 44/1000 | Loss: 0.00002217
Iteration 45/1000 | Loss: 0.00002216
Iteration 46/1000 | Loss: 0.00002216
Iteration 47/1000 | Loss: 0.00002216
Iteration 48/1000 | Loss: 0.00002215
Iteration 49/1000 | Loss: 0.00002215
Iteration 50/1000 | Loss: 0.00002214
Iteration 51/1000 | Loss: 0.00002214
Iteration 52/1000 | Loss: 0.00002213
Iteration 53/1000 | Loss: 0.00002213
Iteration 54/1000 | Loss: 0.00002213
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002212
Iteration 58/1000 | Loss: 0.00002212
Iteration 59/1000 | Loss: 0.00002212
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002210
Iteration 63/1000 | Loss: 0.00002210
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002210
Iteration 66/1000 | Loss: 0.00002210
Iteration 67/1000 | Loss: 0.00002209
Iteration 68/1000 | Loss: 0.00002209
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002208
Iteration 73/1000 | Loss: 0.00002208
Iteration 74/1000 | Loss: 0.00002208
Iteration 75/1000 | Loss: 0.00002208
Iteration 76/1000 | Loss: 0.00002208
Iteration 77/1000 | Loss: 0.00002208
Iteration 78/1000 | Loss: 0.00002208
Iteration 79/1000 | Loss: 0.00002208
Iteration 80/1000 | Loss: 0.00002208
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002207
Iteration 83/1000 | Loss: 0.00002207
Iteration 84/1000 | Loss: 0.00002207
Iteration 85/1000 | Loss: 0.00002206
Iteration 86/1000 | Loss: 0.00002206
Iteration 87/1000 | Loss: 0.00002206
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002205
Iteration 90/1000 | Loss: 0.00002205
Iteration 91/1000 | Loss: 0.00002205
Iteration 92/1000 | Loss: 0.00002205
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002205
Iteration 96/1000 | Loss: 0.00002205
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002205
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002204
Iteration 103/1000 | Loss: 0.00002204
Iteration 104/1000 | Loss: 0.00002204
Iteration 105/1000 | Loss: 0.00002204
Iteration 106/1000 | Loss: 0.00002204
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002204
Iteration 110/1000 | Loss: 0.00002204
Iteration 111/1000 | Loss: 0.00002203
Iteration 112/1000 | Loss: 0.00002203
Iteration 113/1000 | Loss: 0.00002203
Iteration 114/1000 | Loss: 0.00002203
Iteration 115/1000 | Loss: 0.00002203
Iteration 116/1000 | Loss: 0.00002203
Iteration 117/1000 | Loss: 0.00002203
Iteration 118/1000 | Loss: 0.00002203
Iteration 119/1000 | Loss: 0.00002203
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002202
Iteration 122/1000 | Loss: 0.00002202
Iteration 123/1000 | Loss: 0.00002202
Iteration 124/1000 | Loss: 0.00002202
Iteration 125/1000 | Loss: 0.00002202
Iteration 126/1000 | Loss: 0.00002202
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002201
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002201
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002200
Iteration 144/1000 | Loss: 0.00002200
Iteration 145/1000 | Loss: 0.00002200
Iteration 146/1000 | Loss: 0.00002200
Iteration 147/1000 | Loss: 0.00002200
Iteration 148/1000 | Loss: 0.00002200
Iteration 149/1000 | Loss: 0.00002200
Iteration 150/1000 | Loss: 0.00002200
Iteration 151/1000 | Loss: 0.00002199
Iteration 152/1000 | Loss: 0.00002199
Iteration 153/1000 | Loss: 0.00002199
Iteration 154/1000 | Loss: 0.00002199
Iteration 155/1000 | Loss: 0.00002199
Iteration 156/1000 | Loss: 0.00002199
Iteration 157/1000 | Loss: 0.00002199
Iteration 158/1000 | Loss: 0.00002199
Iteration 159/1000 | Loss: 0.00002199
Iteration 160/1000 | Loss: 0.00002199
Iteration 161/1000 | Loss: 0.00002199
Iteration 162/1000 | Loss: 0.00002199
Iteration 163/1000 | Loss: 0.00002199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.1988413209328428e-05, 2.1988413209328428e-05, 2.1988413209328428e-05, 2.1988413209328428e-05, 2.1988413209328428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1988413209328428e-05

Optimization complete. Final v2v error: 3.8790454864501953 mm

Highest mean error: 4.333846092224121 mm for frame 80

Lowest mean error: 3.112355947494507 mm for frame 70

Saving results

Total time: 41.916510581970215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046894
Iteration 2/25 | Loss: 0.00200274
Iteration 3/25 | Loss: 0.00121292
Iteration 4/25 | Loss: 0.00116566
Iteration 5/25 | Loss: 0.00095063
Iteration 6/25 | Loss: 0.00073915
Iteration 7/25 | Loss: 0.00080890
Iteration 8/25 | Loss: 0.00072401
Iteration 9/25 | Loss: 0.00072699
Iteration 10/25 | Loss: 0.00072036
Iteration 11/25 | Loss: 0.00072331
Iteration 12/25 | Loss: 0.00071929
Iteration 13/25 | Loss: 0.00072078
Iteration 14/25 | Loss: 0.00071098
Iteration 15/25 | Loss: 0.00071034
Iteration 16/25 | Loss: 0.00070764
Iteration 17/25 | Loss: 0.00070525
Iteration 18/25 | Loss: 0.00070003
Iteration 19/25 | Loss: 0.00070339
Iteration 20/25 | Loss: 0.00068999
Iteration 21/25 | Loss: 0.00068424
Iteration 22/25 | Loss: 0.00067836
Iteration 23/25 | Loss: 0.00067867
Iteration 24/25 | Loss: 0.00067462
Iteration 25/25 | Loss: 0.00067200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45601547
Iteration 2/25 | Loss: 0.00027950
Iteration 3/25 | Loss: 0.00027950
Iteration 4/25 | Loss: 0.00027950
Iteration 5/25 | Loss: 0.00027950
Iteration 6/25 | Loss: 0.00027950
Iteration 7/25 | Loss: 0.00027950
Iteration 8/25 | Loss: 0.00027950
Iteration 9/25 | Loss: 0.00027950
Iteration 10/25 | Loss: 0.00027950
Iteration 11/25 | Loss: 0.00027950
Iteration 12/25 | Loss: 0.00027950
Iteration 13/25 | Loss: 0.00027950
Iteration 14/25 | Loss: 0.00027950
Iteration 15/25 | Loss: 0.00027950
Iteration 16/25 | Loss: 0.00027950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00027949915966019034, 0.00027949915966019034, 0.00027949915966019034, 0.00027949915966019034, 0.00027949915966019034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027949915966019034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027950
Iteration 2/1000 | Loss: 0.00003514
Iteration 3/1000 | Loss: 0.00002626
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00003033
Iteration 6/1000 | Loss: 0.00002014
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00003080
Iteration 9/1000 | Loss: 0.00001922
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00004279
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001723
Iteration 15/1000 | Loss: 0.00001715
Iteration 16/1000 | Loss: 0.00001710
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001701
Iteration 19/1000 | Loss: 0.00001699
Iteration 20/1000 | Loss: 0.00001698
Iteration 21/1000 | Loss: 0.00001698
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001696
Iteration 25/1000 | Loss: 0.00001696
Iteration 26/1000 | Loss: 0.00001696
Iteration 27/1000 | Loss: 0.00001695
Iteration 28/1000 | Loss: 0.00001694
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001692
Iteration 35/1000 | Loss: 0.00001691
Iteration 36/1000 | Loss: 0.00001691
Iteration 37/1000 | Loss: 0.00001691
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001690
Iteration 41/1000 | Loss: 0.00001689
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001687
Iteration 45/1000 | Loss: 0.00001687
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00004072
Iteration 49/1000 | Loss: 0.00001887
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001679
Iteration 68/1000 | Loss: 0.00001679
Iteration 69/1000 | Loss: 0.00001678
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001678
Iteration 72/1000 | Loss: 0.00001678
Iteration 73/1000 | Loss: 0.00001678
Iteration 74/1000 | Loss: 0.00001678
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001676
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001673
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001671
Iteration 111/1000 | Loss: 0.00001671
Iteration 112/1000 | Loss: 0.00001671
Iteration 113/1000 | Loss: 0.00001671
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001671
Iteration 120/1000 | Loss: 0.00001671
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001670
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Iteration 136/1000 | Loss: 0.00001670
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001670
Iteration 140/1000 | Loss: 0.00001670
Iteration 141/1000 | Loss: 0.00001670
Iteration 142/1000 | Loss: 0.00001670
Iteration 143/1000 | Loss: 0.00001670
Iteration 144/1000 | Loss: 0.00001670
Iteration 145/1000 | Loss: 0.00001670
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001670
Iteration 149/1000 | Loss: 0.00001670
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001670
Iteration 152/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.6702777429600246e-05, 1.6702777429600246e-05, 1.6702777429600246e-05, 1.6702777429600246e-05, 1.6702777429600246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6702777429600246e-05

Optimization complete. Final v2v error: 3.4537205696105957 mm

Highest mean error: 4.13651704788208 mm for frame 223

Lowest mean error: 2.982846975326538 mm for frame 99

Saving results

Total time: 88.05792164802551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379490
Iteration 2/25 | Loss: 0.00084743
Iteration 3/25 | Loss: 0.00066008
Iteration 4/25 | Loss: 0.00063173
Iteration 5/25 | Loss: 0.00062510
Iteration 6/25 | Loss: 0.00062319
Iteration 7/25 | Loss: 0.00062275
Iteration 8/25 | Loss: 0.00062275
Iteration 9/25 | Loss: 0.00062275
Iteration 10/25 | Loss: 0.00062275
Iteration 11/25 | Loss: 0.00062275
Iteration 12/25 | Loss: 0.00062275
Iteration 13/25 | Loss: 0.00062275
Iteration 14/25 | Loss: 0.00062275
Iteration 15/25 | Loss: 0.00062275
Iteration 16/25 | Loss: 0.00062275
Iteration 17/25 | Loss: 0.00062275
Iteration 18/25 | Loss: 0.00062275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006227520643733442, 0.0006227520643733442, 0.0006227520643733442, 0.0006227520643733442, 0.0006227520643733442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006227520643733442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47061932
Iteration 2/25 | Loss: 0.00033736
Iteration 3/25 | Loss: 0.00033736
Iteration 4/25 | Loss: 0.00033736
Iteration 5/25 | Loss: 0.00033735
Iteration 6/25 | Loss: 0.00033735
Iteration 7/25 | Loss: 0.00033735
Iteration 8/25 | Loss: 0.00033735
Iteration 9/25 | Loss: 0.00033735
Iteration 10/25 | Loss: 0.00033735
Iteration 11/25 | Loss: 0.00033735
Iteration 12/25 | Loss: 0.00033735
Iteration 13/25 | Loss: 0.00033735
Iteration 14/25 | Loss: 0.00033735
Iteration 15/25 | Loss: 0.00033735
Iteration 16/25 | Loss: 0.00033735
Iteration 17/25 | Loss: 0.00033735
Iteration 18/25 | Loss: 0.00033735
Iteration 19/25 | Loss: 0.00033735
Iteration 20/25 | Loss: 0.00033735
Iteration 21/25 | Loss: 0.00033735
Iteration 22/25 | Loss: 0.00033735
Iteration 23/25 | Loss: 0.00033735
Iteration 24/25 | Loss: 0.00033735
Iteration 25/25 | Loss: 0.00033735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033735
Iteration 2/1000 | Loss: 0.00002701
Iteration 3/1000 | Loss: 0.00001806
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001346
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001278
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001241
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001240
Iteration 37/1000 | Loss: 0.00001240
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.2253669410711154e-05, 1.2253669410711154e-05, 1.2253669410711154e-05, 1.2253669410711154e-05, 1.2253669410711154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2253669410711154e-05

Optimization complete. Final v2v error: 2.898519277572632 mm

Highest mean error: 3.5109825134277344 mm for frame 73

Lowest mean error: 2.4446678161621094 mm for frame 53

Saving results

Total time: 35.36831307411194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00258962
Iteration 2/25 | Loss: 0.00096130
Iteration 3/25 | Loss: 0.00073658
Iteration 4/25 | Loss: 0.00068859
Iteration 5/25 | Loss: 0.00067729
Iteration 6/25 | Loss: 0.00067322
Iteration 7/25 | Loss: 0.00067233
Iteration 8/25 | Loss: 0.00067186
Iteration 9/25 | Loss: 0.00067174
Iteration 10/25 | Loss: 0.00067174
Iteration 11/25 | Loss: 0.00067174
Iteration 12/25 | Loss: 0.00067174
Iteration 13/25 | Loss: 0.00067174
Iteration 14/25 | Loss: 0.00067174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000671735149808228, 0.000671735149808228, 0.000671735149808228, 0.000671735149808228, 0.000671735149808228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000671735149808228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46345222
Iteration 2/25 | Loss: 0.00034010
Iteration 3/25 | Loss: 0.00034010
Iteration 4/25 | Loss: 0.00034010
Iteration 5/25 | Loss: 0.00034010
Iteration 6/25 | Loss: 0.00034010
Iteration 7/25 | Loss: 0.00034010
Iteration 8/25 | Loss: 0.00034010
Iteration 9/25 | Loss: 0.00034010
Iteration 10/25 | Loss: 0.00034010
Iteration 11/25 | Loss: 0.00034010
Iteration 12/25 | Loss: 0.00034010
Iteration 13/25 | Loss: 0.00034010
Iteration 14/25 | Loss: 0.00034010
Iteration 15/25 | Loss: 0.00034010
Iteration 16/25 | Loss: 0.00034010
Iteration 17/25 | Loss: 0.00034010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003400961577426642, 0.0003400961577426642, 0.0003400961577426642, 0.0003400961577426642, 0.0003400961577426642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003400961577426642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034010
Iteration 2/1000 | Loss: 0.00003816
Iteration 3/1000 | Loss: 0.00002427
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001879
Iteration 6/1000 | Loss: 0.00001804
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001711
Iteration 9/1000 | Loss: 0.00001683
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001644
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001624
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001616
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001612
Iteration 19/1000 | Loss: 0.00001611
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001609
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001607
Iteration 27/1000 | Loss: 0.00001607
Iteration 28/1000 | Loss: 0.00001606
Iteration 29/1000 | Loss: 0.00001606
Iteration 30/1000 | Loss: 0.00001605
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001605
Iteration 33/1000 | Loss: 0.00001604
Iteration 34/1000 | Loss: 0.00001604
Iteration 35/1000 | Loss: 0.00001603
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001602
Iteration 38/1000 | Loss: 0.00001602
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001597
Iteration 41/1000 | Loss: 0.00001597
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001596
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001595
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001593
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001584
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001583
Iteration 97/1000 | Loss: 0.00001583
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001582
Iteration 104/1000 | Loss: 0.00001582
Iteration 105/1000 | Loss: 0.00001582
Iteration 106/1000 | Loss: 0.00001582
Iteration 107/1000 | Loss: 0.00001582
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001580
Iteration 123/1000 | Loss: 0.00001580
Iteration 124/1000 | Loss: 0.00001580
Iteration 125/1000 | Loss: 0.00001580
Iteration 126/1000 | Loss: 0.00001580
Iteration 127/1000 | Loss: 0.00001580
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001580
Iteration 136/1000 | Loss: 0.00001580
Iteration 137/1000 | Loss: 0.00001580
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Iteration 149/1000 | Loss: 0.00001579
Iteration 150/1000 | Loss: 0.00001579
Iteration 151/1000 | Loss: 0.00001579
Iteration 152/1000 | Loss: 0.00001579
Iteration 153/1000 | Loss: 0.00001579
Iteration 154/1000 | Loss: 0.00001579
Iteration 155/1000 | Loss: 0.00001579
Iteration 156/1000 | Loss: 0.00001579
Iteration 157/1000 | Loss: 0.00001579
Iteration 158/1000 | Loss: 0.00001579
Iteration 159/1000 | Loss: 0.00001579
Iteration 160/1000 | Loss: 0.00001579
Iteration 161/1000 | Loss: 0.00001579
Iteration 162/1000 | Loss: 0.00001579
Iteration 163/1000 | Loss: 0.00001579
Iteration 164/1000 | Loss: 0.00001579
Iteration 165/1000 | Loss: 0.00001579
Iteration 166/1000 | Loss: 0.00001579
Iteration 167/1000 | Loss: 0.00001579
Iteration 168/1000 | Loss: 0.00001579
Iteration 169/1000 | Loss: 0.00001579
Iteration 170/1000 | Loss: 0.00001579
Iteration 171/1000 | Loss: 0.00001579
Iteration 172/1000 | Loss: 0.00001579
Iteration 173/1000 | Loss: 0.00001579
Iteration 174/1000 | Loss: 0.00001579
Iteration 175/1000 | Loss: 0.00001579
Iteration 176/1000 | Loss: 0.00001579
Iteration 177/1000 | Loss: 0.00001579
Iteration 178/1000 | Loss: 0.00001579
Iteration 179/1000 | Loss: 0.00001579
Iteration 180/1000 | Loss: 0.00001579
Iteration 181/1000 | Loss: 0.00001579
Iteration 182/1000 | Loss: 0.00001579
Iteration 183/1000 | Loss: 0.00001579
Iteration 184/1000 | Loss: 0.00001579
Iteration 185/1000 | Loss: 0.00001579
Iteration 186/1000 | Loss: 0.00001579
Iteration 187/1000 | Loss: 0.00001579
Iteration 188/1000 | Loss: 0.00001579
Iteration 189/1000 | Loss: 0.00001579
Iteration 190/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.5790124962222762e-05, 1.5790124962222762e-05, 1.5790124962222762e-05, 1.5790124962222762e-05, 1.5790124962222762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5790124962222762e-05

Optimization complete. Final v2v error: 3.339446783065796 mm

Highest mean error: 3.578310966491699 mm for frame 103

Lowest mean error: 3.161442756652832 mm for frame 2

Saving results

Total time: 43.37681603431702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878927
Iteration 2/25 | Loss: 0.00076655
Iteration 3/25 | Loss: 0.00063338
Iteration 4/25 | Loss: 0.00061287
Iteration 5/25 | Loss: 0.00060480
Iteration 6/25 | Loss: 0.00060357
Iteration 7/25 | Loss: 0.00060335
Iteration 8/25 | Loss: 0.00060335
Iteration 9/25 | Loss: 0.00060335
Iteration 10/25 | Loss: 0.00060335
Iteration 11/25 | Loss: 0.00060335
Iteration 12/25 | Loss: 0.00060335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006033456302247941, 0.0006033456302247941, 0.0006033456302247941, 0.0006033456302247941, 0.0006033456302247941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006033456302247941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79825532
Iteration 2/25 | Loss: 0.00029259
Iteration 3/25 | Loss: 0.00029259
Iteration 4/25 | Loss: 0.00029259
Iteration 5/25 | Loss: 0.00029259
Iteration 6/25 | Loss: 0.00029259
Iteration 7/25 | Loss: 0.00029259
Iteration 8/25 | Loss: 0.00029259
Iteration 9/25 | Loss: 0.00029258
Iteration 10/25 | Loss: 0.00029258
Iteration 11/25 | Loss: 0.00029258
Iteration 12/25 | Loss: 0.00029258
Iteration 13/25 | Loss: 0.00029258
Iteration 14/25 | Loss: 0.00029258
Iteration 15/25 | Loss: 0.00029258
Iteration 16/25 | Loss: 0.00029258
Iteration 17/25 | Loss: 0.00029258
Iteration 18/25 | Loss: 0.00029258
Iteration 19/25 | Loss: 0.00029258
Iteration 20/25 | Loss: 0.00029258
Iteration 21/25 | Loss: 0.00029258
Iteration 22/25 | Loss: 0.00029258
Iteration 23/25 | Loss: 0.00029258
Iteration 24/25 | Loss: 0.00029258
Iteration 25/25 | Loss: 0.00029258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029258
Iteration 2/1000 | Loss: 0.00002137
Iteration 3/1000 | Loss: 0.00001503
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001282
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001222
Iteration 40/1000 | Loss: 0.00001222
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001211
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001206
Iteration 110/1000 | Loss: 0.00001206
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.205845364893321e-05, 1.205845364893321e-05, 1.205845364893321e-05, 1.205845364893321e-05, 1.205845364893321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.205845364893321e-05

Optimization complete. Final v2v error: 2.948916435241699 mm

Highest mean error: 3.2992920875549316 mm for frame 91

Lowest mean error: 2.825676918029785 mm for frame 109

Saving results

Total time: 32.2676796913147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071282
Iteration 2/25 | Loss: 0.00144604
Iteration 3/25 | Loss: 0.00106507
Iteration 4/25 | Loss: 0.00090199
Iteration 5/25 | Loss: 0.00085480
Iteration 6/25 | Loss: 0.00081331
Iteration 7/25 | Loss: 0.00079556
Iteration 8/25 | Loss: 0.00078100
Iteration 9/25 | Loss: 0.00076445
Iteration 10/25 | Loss: 0.00075001
Iteration 11/25 | Loss: 0.00074176
Iteration 12/25 | Loss: 0.00073451
Iteration 13/25 | Loss: 0.00073179
Iteration 14/25 | Loss: 0.00072987
Iteration 15/25 | Loss: 0.00072946
Iteration 16/25 | Loss: 0.00072922
Iteration 17/25 | Loss: 0.00072919
Iteration 18/25 | Loss: 0.00072919
Iteration 19/25 | Loss: 0.00072919
Iteration 20/25 | Loss: 0.00072919
Iteration 21/25 | Loss: 0.00072919
Iteration 22/25 | Loss: 0.00072919
Iteration 23/25 | Loss: 0.00072919
Iteration 24/25 | Loss: 0.00072919
Iteration 25/25 | Loss: 0.00072919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47704744
Iteration 2/25 | Loss: 0.00042293
Iteration 3/25 | Loss: 0.00042293
Iteration 4/25 | Loss: 0.00042293
Iteration 5/25 | Loss: 0.00042293
Iteration 6/25 | Loss: 0.00042293
Iteration 7/25 | Loss: 0.00042293
Iteration 8/25 | Loss: 0.00042293
Iteration 9/25 | Loss: 0.00042292
Iteration 10/25 | Loss: 0.00042292
Iteration 11/25 | Loss: 0.00042292
Iteration 12/25 | Loss: 0.00042292
Iteration 13/25 | Loss: 0.00042292
Iteration 14/25 | Loss: 0.00042292
Iteration 15/25 | Loss: 0.00042292
Iteration 16/25 | Loss: 0.00042292
Iteration 17/25 | Loss: 0.00042292
Iteration 18/25 | Loss: 0.00042292
Iteration 19/25 | Loss: 0.00042292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004229246696922928, 0.0004229246696922928, 0.0004229246696922928, 0.0004229246696922928, 0.0004229246696922928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004229246696922928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042292
Iteration 2/1000 | Loss: 0.00005616
Iteration 3/1000 | Loss: 0.00004015
Iteration 4/1000 | Loss: 0.00003502
Iteration 5/1000 | Loss: 0.00003176
Iteration 6/1000 | Loss: 0.00003064
Iteration 7/1000 | Loss: 0.00002972
Iteration 8/1000 | Loss: 0.00002864
Iteration 9/1000 | Loss: 0.00081427
Iteration 10/1000 | Loss: 0.00048034
Iteration 11/1000 | Loss: 0.00046656
Iteration 12/1000 | Loss: 0.00005084
Iteration 13/1000 | Loss: 0.00012039
Iteration 14/1000 | Loss: 0.00002642
Iteration 15/1000 | Loss: 0.00002287
Iteration 16/1000 | Loss: 0.00002102
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001882
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001853
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001844
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001843
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001840
Iteration 49/1000 | Loss: 0.00001840
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001839
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00001839
Iteration 54/1000 | Loss: 0.00001838
Iteration 55/1000 | Loss: 0.00001838
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001836
Iteration 62/1000 | Loss: 0.00001836
Iteration 63/1000 | Loss: 0.00001836
Iteration 64/1000 | Loss: 0.00001836
Iteration 65/1000 | Loss: 0.00001836
Iteration 66/1000 | Loss: 0.00001836
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001835
Iteration 69/1000 | Loss: 0.00001835
Iteration 70/1000 | Loss: 0.00001835
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001835
Iteration 74/1000 | Loss: 0.00001835
Iteration 75/1000 | Loss: 0.00001835
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001834
Iteration 85/1000 | Loss: 0.00001834
Iteration 86/1000 | Loss: 0.00001833
Iteration 87/1000 | Loss: 0.00001833
Iteration 88/1000 | Loss: 0.00001833
Iteration 89/1000 | Loss: 0.00001833
Iteration 90/1000 | Loss: 0.00001833
Iteration 91/1000 | Loss: 0.00001833
Iteration 92/1000 | Loss: 0.00001833
Iteration 93/1000 | Loss: 0.00001833
Iteration 94/1000 | Loss: 0.00001833
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001833
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001833
Iteration 101/1000 | Loss: 0.00001833
Iteration 102/1000 | Loss: 0.00001833
Iteration 103/1000 | Loss: 0.00001833
Iteration 104/1000 | Loss: 0.00001833
Iteration 105/1000 | Loss: 0.00001833
Iteration 106/1000 | Loss: 0.00001833
Iteration 107/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.8328697478864342e-05, 1.8328697478864342e-05, 1.8328697478864342e-05, 1.8328697478864342e-05, 1.8328697478864342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8328697478864342e-05

Optimization complete. Final v2v error: 3.5922799110412598 mm

Highest mean error: 4.035042762756348 mm for frame 184

Lowest mean error: 3.254004716873169 mm for frame 37

Saving results

Total time: 69.95557570457458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032774
Iteration 2/25 | Loss: 0.00112205
Iteration 3/25 | Loss: 0.00077826
Iteration 4/25 | Loss: 0.00067170
Iteration 5/25 | Loss: 0.00066197
Iteration 6/25 | Loss: 0.00065653
Iteration 7/25 | Loss: 0.00065804
Iteration 8/25 | Loss: 0.00065389
Iteration 9/25 | Loss: 0.00065381
Iteration 10/25 | Loss: 0.00065381
Iteration 11/25 | Loss: 0.00065381
Iteration 12/25 | Loss: 0.00065381
Iteration 13/25 | Loss: 0.00065381
Iteration 14/25 | Loss: 0.00065381
Iteration 15/25 | Loss: 0.00065380
Iteration 16/25 | Loss: 0.00065380
Iteration 17/25 | Loss: 0.00065380
Iteration 18/25 | Loss: 0.00065380
Iteration 19/25 | Loss: 0.00065380
Iteration 20/25 | Loss: 0.00065380
Iteration 21/25 | Loss: 0.00065380
Iteration 22/25 | Loss: 0.00065380
Iteration 23/25 | Loss: 0.00065380
Iteration 24/25 | Loss: 0.00065380
Iteration 25/25 | Loss: 0.00065380

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.20235491
Iteration 2/25 | Loss: 0.00027368
Iteration 3/25 | Loss: 0.00027368
Iteration 4/25 | Loss: 0.00027368
Iteration 5/25 | Loss: 0.00027368
Iteration 6/25 | Loss: 0.00027368
Iteration 7/25 | Loss: 0.00027368
Iteration 8/25 | Loss: 0.00027368
Iteration 9/25 | Loss: 0.00027368
Iteration 10/25 | Loss: 0.00027368
Iteration 11/25 | Loss: 0.00027368
Iteration 12/25 | Loss: 0.00027368
Iteration 13/25 | Loss: 0.00027368
Iteration 14/25 | Loss: 0.00027368
Iteration 15/25 | Loss: 0.00027368
Iteration 16/25 | Loss: 0.00027368
Iteration 17/25 | Loss: 0.00027368
Iteration 18/25 | Loss: 0.00027368
Iteration 19/25 | Loss: 0.00027368
Iteration 20/25 | Loss: 0.00027368
Iteration 21/25 | Loss: 0.00027368
Iteration 22/25 | Loss: 0.00027368
Iteration 23/25 | Loss: 0.00027368
Iteration 24/25 | Loss: 0.00027368
Iteration 25/25 | Loss: 0.00027368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027368
Iteration 2/1000 | Loss: 0.00002482
Iteration 3/1000 | Loss: 0.00006794
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001389
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001320
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001313
Iteration 13/1000 | Loss: 0.00001313
Iteration 14/1000 | Loss: 0.00007997
Iteration 15/1000 | Loss: 0.00018106
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001302
Iteration 18/1000 | Loss: 0.00001301
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001296
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001295
Iteration 31/1000 | Loss: 0.00001294
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001289
Iteration 35/1000 | Loss: 0.00001289
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00001288
Iteration 38/1000 | Loss: 0.00001287
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00001286
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00006435
Iteration 44/1000 | Loss: 0.00009662
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001276
Iteration 49/1000 | Loss: 0.00001276
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001275
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001269
Iteration 60/1000 | Loss: 0.00001269
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001267
Iteration 86/1000 | Loss: 0.00001267
Iteration 87/1000 | Loss: 0.00001267
Iteration 88/1000 | Loss: 0.00001267
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001266
Iteration 91/1000 | Loss: 0.00001266
Iteration 92/1000 | Loss: 0.00001266
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001266
Iteration 95/1000 | Loss: 0.00001266
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001266
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001265
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00007552
Iteration 111/1000 | Loss: 0.00002554
Iteration 112/1000 | Loss: 0.00005120
Iteration 113/1000 | Loss: 0.00002172
Iteration 114/1000 | Loss: 0.00002404
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001267
Iteration 117/1000 | Loss: 0.00004294
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001260
Iteration 134/1000 | Loss: 0.00001260
Iteration 135/1000 | Loss: 0.00001260
Iteration 136/1000 | Loss: 0.00001260
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001259
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001258
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001256
Iteration 150/1000 | Loss: 0.00001256
Iteration 151/1000 | Loss: 0.00001256
Iteration 152/1000 | Loss: 0.00001256
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Iteration 155/1000 | Loss: 0.00001255
Iteration 156/1000 | Loss: 0.00001255
Iteration 157/1000 | Loss: 0.00001255
Iteration 158/1000 | Loss: 0.00001254
Iteration 159/1000 | Loss: 0.00001254
Iteration 160/1000 | Loss: 0.00001254
Iteration 161/1000 | Loss: 0.00001254
Iteration 162/1000 | Loss: 0.00001254
Iteration 163/1000 | Loss: 0.00001253
Iteration 164/1000 | Loss: 0.00001253
Iteration 165/1000 | Loss: 0.00001253
Iteration 166/1000 | Loss: 0.00001253
Iteration 167/1000 | Loss: 0.00001253
Iteration 168/1000 | Loss: 0.00001253
Iteration 169/1000 | Loss: 0.00001253
Iteration 170/1000 | Loss: 0.00001253
Iteration 171/1000 | Loss: 0.00001253
Iteration 172/1000 | Loss: 0.00001253
Iteration 173/1000 | Loss: 0.00001252
Iteration 174/1000 | Loss: 0.00001252
Iteration 175/1000 | Loss: 0.00001252
Iteration 176/1000 | Loss: 0.00001252
Iteration 177/1000 | Loss: 0.00001252
Iteration 178/1000 | Loss: 0.00001252
Iteration 179/1000 | Loss: 0.00001251
Iteration 180/1000 | Loss: 0.00001251
Iteration 181/1000 | Loss: 0.00001251
Iteration 182/1000 | Loss: 0.00001251
Iteration 183/1000 | Loss: 0.00001251
Iteration 184/1000 | Loss: 0.00001251
Iteration 185/1000 | Loss: 0.00001251
Iteration 186/1000 | Loss: 0.00001251
Iteration 187/1000 | Loss: 0.00001251
Iteration 188/1000 | Loss: 0.00001251
Iteration 189/1000 | Loss: 0.00001251
Iteration 190/1000 | Loss: 0.00001251
Iteration 191/1000 | Loss: 0.00001251
Iteration 192/1000 | Loss: 0.00001251
Iteration 193/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.2508546205936e-05, 1.2508546205936e-05, 1.2508546205936e-05, 1.2508546205936e-05, 1.2508546205936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2508546205936e-05

Optimization complete. Final v2v error: 2.9817793369293213 mm

Highest mean error: 3.6037919521331787 mm for frame 172

Lowest mean error: 2.727377414703369 mm for frame 64

Saving results

Total time: 66.08966445922852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044091
Iteration 2/25 | Loss: 0.01044090
Iteration 3/25 | Loss: 0.01044090
Iteration 4/25 | Loss: 0.01044090
Iteration 5/25 | Loss: 0.01044089
Iteration 6/25 | Loss: 0.01044089
Iteration 7/25 | Loss: 0.01044089
Iteration 8/25 | Loss: 0.01044089
Iteration 9/25 | Loss: 0.01044088
Iteration 10/25 | Loss: 0.01044088
Iteration 11/25 | Loss: 0.01044088
Iteration 12/25 | Loss: 0.01044088
Iteration 13/25 | Loss: 0.01044087
Iteration 14/25 | Loss: 0.00287151
Iteration 15/25 | Loss: 0.00175492
Iteration 16/25 | Loss: 0.00168855
Iteration 17/25 | Loss: 0.00168124
Iteration 18/25 | Loss: 0.00161858
Iteration 19/25 | Loss: 0.00150996
Iteration 20/25 | Loss: 0.00147103
Iteration 21/25 | Loss: 0.00142587
Iteration 22/25 | Loss: 0.00139955
Iteration 23/25 | Loss: 0.00137603
Iteration 24/25 | Loss: 0.00136250
Iteration 25/25 | Loss: 0.00133959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47398913
Iteration 2/25 | Loss: 0.01351781
Iteration 3/25 | Loss: 0.00477863
Iteration 4/25 | Loss: 0.00477830
Iteration 5/25 | Loss: 0.00477830
Iteration 6/25 | Loss: 0.00477830
Iteration 7/25 | Loss: 0.00477830
Iteration 8/25 | Loss: 0.00477830
Iteration 9/25 | Loss: 0.00477830
Iteration 10/25 | Loss: 0.00477830
Iteration 11/25 | Loss: 0.00477830
Iteration 12/25 | Loss: 0.00477830
Iteration 13/25 | Loss: 0.00477830
Iteration 14/25 | Loss: 0.00477830
Iteration 15/25 | Loss: 0.00477830
Iteration 16/25 | Loss: 0.00477830
Iteration 17/25 | Loss: 0.00477830
Iteration 18/25 | Loss: 0.00477830
Iteration 19/25 | Loss: 0.00477830
Iteration 20/25 | Loss: 0.00477830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004778299480676651, 0.004778299480676651, 0.004778299480676651, 0.004778299480676651, 0.004778299480676651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004778299480676651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00477830
Iteration 2/1000 | Loss: 0.01098023
Iteration 3/1000 | Loss: 0.00336957
Iteration 4/1000 | Loss: 0.00208046
Iteration 5/1000 | Loss: 0.00295227
Iteration 6/1000 | Loss: 0.00462274
Iteration 7/1000 | Loss: 0.00890527
Iteration 8/1000 | Loss: 0.00413736
Iteration 9/1000 | Loss: 0.00589114
Iteration 10/1000 | Loss: 0.00421318
Iteration 11/1000 | Loss: 0.00554862
Iteration 12/1000 | Loss: 0.00218385
Iteration 13/1000 | Loss: 0.00519324
Iteration 14/1000 | Loss: 0.00190491
Iteration 15/1000 | Loss: 0.00125100
Iteration 16/1000 | Loss: 0.00189487
Iteration 17/1000 | Loss: 0.00109328
Iteration 18/1000 | Loss: 0.00193809
Iteration 19/1000 | Loss: 0.00163394
Iteration 20/1000 | Loss: 0.00111217
Iteration 21/1000 | Loss: 0.00055580
Iteration 22/1000 | Loss: 0.00315350
Iteration 23/1000 | Loss: 0.00170157
Iteration 24/1000 | Loss: 0.00291668
Iteration 25/1000 | Loss: 0.00111375
Iteration 26/1000 | Loss: 0.00083311
Iteration 27/1000 | Loss: 0.00073506
Iteration 28/1000 | Loss: 0.00386619
Iteration 29/1000 | Loss: 0.00061549
Iteration 30/1000 | Loss: 0.00049405
Iteration 31/1000 | Loss: 0.00042907
Iteration 32/1000 | Loss: 0.00115020
Iteration 33/1000 | Loss: 0.00075403
Iteration 34/1000 | Loss: 0.00099802
Iteration 35/1000 | Loss: 0.00020409
Iteration 36/1000 | Loss: 0.00008766
Iteration 37/1000 | Loss: 0.00010990
Iteration 38/1000 | Loss: 0.00011316
Iteration 39/1000 | Loss: 0.00091908
Iteration 40/1000 | Loss: 0.00007941
Iteration 41/1000 | Loss: 0.00023000
Iteration 42/1000 | Loss: 0.00054231
Iteration 43/1000 | Loss: 0.00017101
Iteration 44/1000 | Loss: 0.00055615
Iteration 45/1000 | Loss: 0.00006541
Iteration 46/1000 | Loss: 0.00047863
Iteration 47/1000 | Loss: 0.00258069
Iteration 48/1000 | Loss: 0.00078721
Iteration 49/1000 | Loss: 0.00030404
Iteration 50/1000 | Loss: 0.00006162
Iteration 51/1000 | Loss: 0.00047853
Iteration 52/1000 | Loss: 0.00009909
Iteration 53/1000 | Loss: 0.00023114
Iteration 54/1000 | Loss: 0.00038763
Iteration 55/1000 | Loss: 0.00037227
Iteration 56/1000 | Loss: 0.00015930
Iteration 57/1000 | Loss: 0.00049547
Iteration 58/1000 | Loss: 0.00013830
Iteration 59/1000 | Loss: 0.00007546
Iteration 60/1000 | Loss: 0.00005259
Iteration 61/1000 | Loss: 0.00004290
Iteration 62/1000 | Loss: 0.00053746
Iteration 63/1000 | Loss: 0.00074261
Iteration 64/1000 | Loss: 0.00043141
Iteration 65/1000 | Loss: 0.00004889
Iteration 66/1000 | Loss: 0.00035420
Iteration 67/1000 | Loss: 0.00014174
Iteration 68/1000 | Loss: 0.00004046
Iteration 69/1000 | Loss: 0.00013559
Iteration 70/1000 | Loss: 0.00187305
Iteration 71/1000 | Loss: 0.00047223
Iteration 72/1000 | Loss: 0.00048412
Iteration 73/1000 | Loss: 0.00031366
Iteration 74/1000 | Loss: 0.00007853
Iteration 75/1000 | Loss: 0.00059893
Iteration 76/1000 | Loss: 0.00044792
Iteration 77/1000 | Loss: 0.00070586
Iteration 78/1000 | Loss: 0.00057110
Iteration 79/1000 | Loss: 0.00063090
Iteration 80/1000 | Loss: 0.00090992
Iteration 81/1000 | Loss: 0.00055123
Iteration 82/1000 | Loss: 0.00022049
Iteration 83/1000 | Loss: 0.00016634
Iteration 84/1000 | Loss: 0.00029435
Iteration 85/1000 | Loss: 0.00028116
Iteration 86/1000 | Loss: 0.00064386
Iteration 87/1000 | Loss: 0.00035103
Iteration 88/1000 | Loss: 0.00018418
Iteration 89/1000 | Loss: 0.00027501
Iteration 90/1000 | Loss: 0.00017524
Iteration 91/1000 | Loss: 0.00031349
Iteration 92/1000 | Loss: 0.00003058
Iteration 93/1000 | Loss: 0.00002652
Iteration 94/1000 | Loss: 0.00008087
Iteration 95/1000 | Loss: 0.00002797
Iteration 96/1000 | Loss: 0.00002116
Iteration 97/1000 | Loss: 0.00002001
Iteration 98/1000 | Loss: 0.00001948
Iteration 99/1000 | Loss: 0.00001899
Iteration 100/1000 | Loss: 0.00029566
Iteration 101/1000 | Loss: 0.00002487
Iteration 102/1000 | Loss: 0.00002144
Iteration 103/1000 | Loss: 0.00001923
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001758
Iteration 106/1000 | Loss: 0.00001730
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001681
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001678
Iteration 124/1000 | Loss: 0.00001678
Iteration 125/1000 | Loss: 0.00001677
Iteration 126/1000 | Loss: 0.00001677
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00001676
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001675
Iteration 136/1000 | Loss: 0.00001675
Iteration 137/1000 | Loss: 0.00001675
Iteration 138/1000 | Loss: 0.00001675
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001675
Iteration 141/1000 | Loss: 0.00001674
Iteration 142/1000 | Loss: 0.00001674
Iteration 143/1000 | Loss: 0.00001674
Iteration 144/1000 | Loss: 0.00001674
Iteration 145/1000 | Loss: 0.00001674
Iteration 146/1000 | Loss: 0.00001673
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001673
Iteration 149/1000 | Loss: 0.00001673
Iteration 150/1000 | Loss: 0.00001672
Iteration 151/1000 | Loss: 0.00001672
Iteration 152/1000 | Loss: 0.00001672
Iteration 153/1000 | Loss: 0.00001672
Iteration 154/1000 | Loss: 0.00001672
Iteration 155/1000 | Loss: 0.00001671
Iteration 156/1000 | Loss: 0.00001671
Iteration 157/1000 | Loss: 0.00001671
Iteration 158/1000 | Loss: 0.00001670
Iteration 159/1000 | Loss: 0.00001670
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001669
Iteration 162/1000 | Loss: 0.00001669
Iteration 163/1000 | Loss: 0.00001669
Iteration 164/1000 | Loss: 0.00001669
Iteration 165/1000 | Loss: 0.00001668
Iteration 166/1000 | Loss: 0.00001668
Iteration 167/1000 | Loss: 0.00001668
Iteration 168/1000 | Loss: 0.00001668
Iteration 169/1000 | Loss: 0.00001668
Iteration 170/1000 | Loss: 0.00001667
Iteration 171/1000 | Loss: 0.00001667
Iteration 172/1000 | Loss: 0.00001667
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Iteration 175/1000 | Loss: 0.00001666
Iteration 176/1000 | Loss: 0.00001666
Iteration 177/1000 | Loss: 0.00001666
Iteration 178/1000 | Loss: 0.00001666
Iteration 179/1000 | Loss: 0.00001666
Iteration 180/1000 | Loss: 0.00001666
Iteration 181/1000 | Loss: 0.00001666
Iteration 182/1000 | Loss: 0.00001666
Iteration 183/1000 | Loss: 0.00001665
Iteration 184/1000 | Loss: 0.00001665
Iteration 185/1000 | Loss: 0.00001665
Iteration 186/1000 | Loss: 0.00001665
Iteration 187/1000 | Loss: 0.00001664
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001663
Iteration 190/1000 | Loss: 0.00001663
Iteration 191/1000 | Loss: 0.00001663
Iteration 192/1000 | Loss: 0.00001663
Iteration 193/1000 | Loss: 0.00001663
Iteration 194/1000 | Loss: 0.00001663
Iteration 195/1000 | Loss: 0.00001662
Iteration 196/1000 | Loss: 0.00001662
Iteration 197/1000 | Loss: 0.00001662
Iteration 198/1000 | Loss: 0.00001662
Iteration 199/1000 | Loss: 0.00001662
Iteration 200/1000 | Loss: 0.00001662
Iteration 201/1000 | Loss: 0.00001662
Iteration 202/1000 | Loss: 0.00001661
Iteration 203/1000 | Loss: 0.00001661
Iteration 204/1000 | Loss: 0.00001661
Iteration 205/1000 | Loss: 0.00001661
Iteration 206/1000 | Loss: 0.00001661
Iteration 207/1000 | Loss: 0.00001661
Iteration 208/1000 | Loss: 0.00001661
Iteration 209/1000 | Loss: 0.00001661
Iteration 210/1000 | Loss: 0.00001660
Iteration 211/1000 | Loss: 0.00001660
Iteration 212/1000 | Loss: 0.00001660
Iteration 213/1000 | Loss: 0.00001659
Iteration 214/1000 | Loss: 0.00001659
Iteration 215/1000 | Loss: 0.00001659
Iteration 216/1000 | Loss: 0.00001659
Iteration 217/1000 | Loss: 0.00001658
Iteration 218/1000 | Loss: 0.00001658
Iteration 219/1000 | Loss: 0.00001658
Iteration 220/1000 | Loss: 0.00001657
Iteration 221/1000 | Loss: 0.00001657
Iteration 222/1000 | Loss: 0.00001657
Iteration 223/1000 | Loss: 0.00001657
Iteration 224/1000 | Loss: 0.00001657
Iteration 225/1000 | Loss: 0.00001657
Iteration 226/1000 | Loss: 0.00001656
Iteration 227/1000 | Loss: 0.00001656
Iteration 228/1000 | Loss: 0.00001656
Iteration 229/1000 | Loss: 0.00001656
Iteration 230/1000 | Loss: 0.00001656
Iteration 231/1000 | Loss: 0.00001656
Iteration 232/1000 | Loss: 0.00001655
Iteration 233/1000 | Loss: 0.00001655
Iteration 234/1000 | Loss: 0.00001655
Iteration 235/1000 | Loss: 0.00001654
Iteration 236/1000 | Loss: 0.00001654
Iteration 237/1000 | Loss: 0.00001654
Iteration 238/1000 | Loss: 0.00001653
Iteration 239/1000 | Loss: 0.00001653
Iteration 240/1000 | Loss: 0.00001653
Iteration 241/1000 | Loss: 0.00001653
Iteration 242/1000 | Loss: 0.00001653
Iteration 243/1000 | Loss: 0.00001653
Iteration 244/1000 | Loss: 0.00001653
Iteration 245/1000 | Loss: 0.00001652
Iteration 246/1000 | Loss: 0.00001652
Iteration 247/1000 | Loss: 0.00001652
Iteration 248/1000 | Loss: 0.00001652
Iteration 249/1000 | Loss: 0.00001652
Iteration 250/1000 | Loss: 0.00001652
Iteration 251/1000 | Loss: 0.00001652
Iteration 252/1000 | Loss: 0.00001652
Iteration 253/1000 | Loss: 0.00001651
Iteration 254/1000 | Loss: 0.00001651
Iteration 255/1000 | Loss: 0.00001651
Iteration 256/1000 | Loss: 0.00001651
Iteration 257/1000 | Loss: 0.00001651
Iteration 258/1000 | Loss: 0.00001651
Iteration 259/1000 | Loss: 0.00001651
Iteration 260/1000 | Loss: 0.00001651
Iteration 261/1000 | Loss: 0.00001651
Iteration 262/1000 | Loss: 0.00001651
Iteration 263/1000 | Loss: 0.00001651
Iteration 264/1000 | Loss: 0.00001651
Iteration 265/1000 | Loss: 0.00001651
Iteration 266/1000 | Loss: 0.00001651
Iteration 267/1000 | Loss: 0.00001651
Iteration 268/1000 | Loss: 0.00001650
Iteration 269/1000 | Loss: 0.00001650
Iteration 270/1000 | Loss: 0.00001650
Iteration 271/1000 | Loss: 0.00001650
Iteration 272/1000 | Loss: 0.00001650
Iteration 273/1000 | Loss: 0.00001650
Iteration 274/1000 | Loss: 0.00001650
Iteration 275/1000 | Loss: 0.00001650
Iteration 276/1000 | Loss: 0.00001650
Iteration 277/1000 | Loss: 0.00001650
Iteration 278/1000 | Loss: 0.00001650
Iteration 279/1000 | Loss: 0.00001650
Iteration 280/1000 | Loss: 0.00001650
Iteration 281/1000 | Loss: 0.00001649
Iteration 282/1000 | Loss: 0.00001649
Iteration 283/1000 | Loss: 0.00001649
Iteration 284/1000 | Loss: 0.00001649
Iteration 285/1000 | Loss: 0.00001649
Iteration 286/1000 | Loss: 0.00001649
Iteration 287/1000 | Loss: 0.00001649
Iteration 288/1000 | Loss: 0.00001649
Iteration 289/1000 | Loss: 0.00001649
Iteration 290/1000 | Loss: 0.00001649
Iteration 291/1000 | Loss: 0.00001649
Iteration 292/1000 | Loss: 0.00001649
Iteration 293/1000 | Loss: 0.00001649
Iteration 294/1000 | Loss: 0.00001649
Iteration 295/1000 | Loss: 0.00001649
Iteration 296/1000 | Loss: 0.00001649
Iteration 297/1000 | Loss: 0.00001649
Iteration 298/1000 | Loss: 0.00001649
Iteration 299/1000 | Loss: 0.00001649
Iteration 300/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.6488480468979105e-05, 1.6488480468979105e-05, 1.6488480468979105e-05, 1.6488480468979105e-05, 1.6488480468979105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6488480468979105e-05

Optimization complete. Final v2v error: 3.394052028656006 mm

Highest mean error: 6.302656173706055 mm for frame 8

Lowest mean error: 3.0359585285186768 mm for frame 93

Saving results

Total time: 221.05096697807312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843779
Iteration 2/25 | Loss: 0.00085572
Iteration 3/25 | Loss: 0.00064326
Iteration 4/25 | Loss: 0.00061370
Iteration 5/25 | Loss: 0.00060616
Iteration 6/25 | Loss: 0.00060336
Iteration 7/25 | Loss: 0.00060251
Iteration 8/25 | Loss: 0.00060244
Iteration 9/25 | Loss: 0.00060244
Iteration 10/25 | Loss: 0.00060244
Iteration 11/25 | Loss: 0.00060244
Iteration 12/25 | Loss: 0.00060244
Iteration 13/25 | Loss: 0.00060244
Iteration 14/25 | Loss: 0.00060244
Iteration 15/25 | Loss: 0.00060244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006024377071298659, 0.0006024377071298659, 0.0006024377071298659, 0.0006024377071298659, 0.0006024377071298659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006024377071298659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45512009
Iteration 2/25 | Loss: 0.00027388
Iteration 3/25 | Loss: 0.00027387
Iteration 4/25 | Loss: 0.00027387
Iteration 5/25 | Loss: 0.00027387
Iteration 6/25 | Loss: 0.00027387
Iteration 7/25 | Loss: 0.00027387
Iteration 8/25 | Loss: 0.00027387
Iteration 9/25 | Loss: 0.00027387
Iteration 10/25 | Loss: 0.00027387
Iteration 11/25 | Loss: 0.00027387
Iteration 12/25 | Loss: 0.00027387
Iteration 13/25 | Loss: 0.00027387
Iteration 14/25 | Loss: 0.00027387
Iteration 15/25 | Loss: 0.00027387
Iteration 16/25 | Loss: 0.00027387
Iteration 17/25 | Loss: 0.00027387
Iteration 18/25 | Loss: 0.00027387
Iteration 19/25 | Loss: 0.00027387
Iteration 20/25 | Loss: 0.00027387
Iteration 21/25 | Loss: 0.00027387
Iteration 22/25 | Loss: 0.00027387
Iteration 23/25 | Loss: 0.00027387
Iteration 24/25 | Loss: 0.00027387
Iteration 25/25 | Loss: 0.00027387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027387
Iteration 2/1000 | Loss: 0.00002550
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001308
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001203
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001165
Iteration 10/1000 | Loss: 0.00001164
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001158
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001143
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001141
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001138
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001129
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001126
Iteration 41/1000 | Loss: 0.00001126
Iteration 42/1000 | Loss: 0.00001125
Iteration 43/1000 | Loss: 0.00001125
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001122
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001120
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001115
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001114
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001112
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001111
Iteration 78/1000 | Loss: 0.00001111
Iteration 79/1000 | Loss: 0.00001111
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001106
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001106
Iteration 140/1000 | Loss: 0.00001106
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001105
Iteration 148/1000 | Loss: 0.00001105
Iteration 149/1000 | Loss: 0.00001105
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001104
Iteration 158/1000 | Loss: 0.00001104
Iteration 159/1000 | Loss: 0.00001104
Iteration 160/1000 | Loss: 0.00001104
Iteration 161/1000 | Loss: 0.00001104
Iteration 162/1000 | Loss: 0.00001104
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001104
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001104
Iteration 168/1000 | Loss: 0.00001104
Iteration 169/1000 | Loss: 0.00001104
Iteration 170/1000 | Loss: 0.00001104
Iteration 171/1000 | Loss: 0.00001104
Iteration 172/1000 | Loss: 0.00001103
Iteration 173/1000 | Loss: 0.00001103
Iteration 174/1000 | Loss: 0.00001103
Iteration 175/1000 | Loss: 0.00001103
Iteration 176/1000 | Loss: 0.00001103
Iteration 177/1000 | Loss: 0.00001103
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001103
Iteration 181/1000 | Loss: 0.00001103
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001102
Iteration 188/1000 | Loss: 0.00001102
Iteration 189/1000 | Loss: 0.00001102
Iteration 190/1000 | Loss: 0.00001102
Iteration 191/1000 | Loss: 0.00001102
Iteration 192/1000 | Loss: 0.00001102
Iteration 193/1000 | Loss: 0.00001102
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001101
Iteration 200/1000 | Loss: 0.00001101
Iteration 201/1000 | Loss: 0.00001101
Iteration 202/1000 | Loss: 0.00001101
Iteration 203/1000 | Loss: 0.00001101
Iteration 204/1000 | Loss: 0.00001101
Iteration 205/1000 | Loss: 0.00001101
Iteration 206/1000 | Loss: 0.00001101
Iteration 207/1000 | Loss: 0.00001101
Iteration 208/1000 | Loss: 0.00001101
Iteration 209/1000 | Loss: 0.00001101
Iteration 210/1000 | Loss: 0.00001101
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.101422640203964e-05, 1.101422640203964e-05, 1.101422640203964e-05, 1.101422640203964e-05, 1.101422640203964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.101422640203964e-05

Optimization complete. Final v2v error: 2.7579853534698486 mm

Highest mean error: 3.682806968688965 mm for frame 59

Lowest mean error: 2.4232590198516846 mm for frame 129

Saving results

Total time: 39.62214231491089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441877
Iteration 2/25 | Loss: 0.00104736
Iteration 3/25 | Loss: 0.00071833
Iteration 4/25 | Loss: 0.00064031
Iteration 5/25 | Loss: 0.00061960
Iteration 6/25 | Loss: 0.00061573
Iteration 7/25 | Loss: 0.00061499
Iteration 8/25 | Loss: 0.00061499
Iteration 9/25 | Loss: 0.00061499
Iteration 10/25 | Loss: 0.00061499
Iteration 11/25 | Loss: 0.00061499
Iteration 12/25 | Loss: 0.00061499
Iteration 13/25 | Loss: 0.00061499
Iteration 14/25 | Loss: 0.00061499
Iteration 15/25 | Loss: 0.00061499
Iteration 16/25 | Loss: 0.00061499
Iteration 17/25 | Loss: 0.00061499
Iteration 18/25 | Loss: 0.00061499
Iteration 19/25 | Loss: 0.00061499
Iteration 20/25 | Loss: 0.00061499
Iteration 21/25 | Loss: 0.00061499
Iteration 22/25 | Loss: 0.00061499
Iteration 23/25 | Loss: 0.00061499
Iteration 24/25 | Loss: 0.00061499
Iteration 25/25 | Loss: 0.00061499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56775165
Iteration 2/25 | Loss: 0.00027577
Iteration 3/25 | Loss: 0.00027577
Iteration 4/25 | Loss: 0.00027577
Iteration 5/25 | Loss: 0.00027577
Iteration 6/25 | Loss: 0.00027577
Iteration 7/25 | Loss: 0.00027577
Iteration 8/25 | Loss: 0.00027577
Iteration 9/25 | Loss: 0.00027577
Iteration 10/25 | Loss: 0.00027577
Iteration 11/25 | Loss: 0.00027577
Iteration 12/25 | Loss: 0.00027577
Iteration 13/25 | Loss: 0.00027577
Iteration 14/25 | Loss: 0.00027577
Iteration 15/25 | Loss: 0.00027577
Iteration 16/25 | Loss: 0.00027577
Iteration 17/25 | Loss: 0.00027577
Iteration 18/25 | Loss: 0.00027577
Iteration 19/25 | Loss: 0.00027577
Iteration 20/25 | Loss: 0.00027577
Iteration 21/25 | Loss: 0.00027577
Iteration 22/25 | Loss: 0.00027577
Iteration 23/25 | Loss: 0.00027577
Iteration 24/25 | Loss: 0.00027577
Iteration 25/25 | Loss: 0.00027577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027577
Iteration 2/1000 | Loss: 0.00002373
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001587
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001361
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001319
Iteration 14/1000 | Loss: 0.00001312
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001296
Iteration 20/1000 | Loss: 0.00001296
Iteration 21/1000 | Loss: 0.00001295
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001294
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001293
Iteration 26/1000 | Loss: 0.00001293
Iteration 27/1000 | Loss: 0.00001292
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001291
Iteration 30/1000 | Loss: 0.00001291
Iteration 31/1000 | Loss: 0.00001291
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001290
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001288
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001284
Iteration 70/1000 | Loss: 0.00001284
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001284
Iteration 73/1000 | Loss: 0.00001284
Iteration 74/1000 | Loss: 0.00001284
Iteration 75/1000 | Loss: 0.00001284
Iteration 76/1000 | Loss: 0.00001284
Iteration 77/1000 | Loss: 0.00001283
Iteration 78/1000 | Loss: 0.00001283
Iteration 79/1000 | Loss: 0.00001283
Iteration 80/1000 | Loss: 0.00001283
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001282
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001280
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001277
Iteration 125/1000 | Loss: 0.00001277
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001277
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001276
Iteration 134/1000 | Loss: 0.00001276
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001275
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001275
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001275
Iteration 151/1000 | Loss: 0.00001275
Iteration 152/1000 | Loss: 0.00001275
Iteration 153/1000 | Loss: 0.00001274
Iteration 154/1000 | Loss: 0.00001274
Iteration 155/1000 | Loss: 0.00001274
Iteration 156/1000 | Loss: 0.00001274
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001274
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Iteration 164/1000 | Loss: 0.00001273
Iteration 165/1000 | Loss: 0.00001273
Iteration 166/1000 | Loss: 0.00001273
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001272
Iteration 171/1000 | Loss: 0.00001272
Iteration 172/1000 | Loss: 0.00001272
Iteration 173/1000 | Loss: 0.00001272
Iteration 174/1000 | Loss: 0.00001272
Iteration 175/1000 | Loss: 0.00001272
Iteration 176/1000 | Loss: 0.00001272
Iteration 177/1000 | Loss: 0.00001272
Iteration 178/1000 | Loss: 0.00001272
Iteration 179/1000 | Loss: 0.00001272
Iteration 180/1000 | Loss: 0.00001272
Iteration 181/1000 | Loss: 0.00001271
Iteration 182/1000 | Loss: 0.00001271
Iteration 183/1000 | Loss: 0.00001271
Iteration 184/1000 | Loss: 0.00001271
Iteration 185/1000 | Loss: 0.00001271
Iteration 186/1000 | Loss: 0.00001271
Iteration 187/1000 | Loss: 0.00001271
Iteration 188/1000 | Loss: 0.00001271
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001271
Iteration 205/1000 | Loss: 0.00001271
Iteration 206/1000 | Loss: 0.00001271
Iteration 207/1000 | Loss: 0.00001271
Iteration 208/1000 | Loss: 0.00001271
Iteration 209/1000 | Loss: 0.00001271
Iteration 210/1000 | Loss: 0.00001271
Iteration 211/1000 | Loss: 0.00001271
Iteration 212/1000 | Loss: 0.00001271
Iteration 213/1000 | Loss: 0.00001271
Iteration 214/1000 | Loss: 0.00001271
Iteration 215/1000 | Loss: 0.00001271
Iteration 216/1000 | Loss: 0.00001271
Iteration 217/1000 | Loss: 0.00001271
Iteration 218/1000 | Loss: 0.00001271
Iteration 219/1000 | Loss: 0.00001271
Iteration 220/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.2710080227407161e-05, 1.2710080227407161e-05, 1.2710080227407161e-05, 1.2710080227407161e-05, 1.2710080227407161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2710080227407161e-05

Optimization complete. Final v2v error: 2.9932453632354736 mm

Highest mean error: 4.013213634490967 mm for frame 105

Lowest mean error: 2.815539598464966 mm for frame 176

Saving results

Total time: 46.39616298675537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862906
Iteration 2/25 | Loss: 0.00170142
Iteration 3/25 | Loss: 0.00096906
Iteration 4/25 | Loss: 0.00082354
Iteration 5/25 | Loss: 0.00080265
Iteration 6/25 | Loss: 0.00080220
Iteration 7/25 | Loss: 0.00080077
Iteration 8/25 | Loss: 0.00080173
Iteration 9/25 | Loss: 0.00079751
Iteration 10/25 | Loss: 0.00079620
Iteration 11/25 | Loss: 0.00079606
Iteration 12/25 | Loss: 0.00079679
Iteration 13/25 | Loss: 0.00079607
Iteration 14/25 | Loss: 0.00079545
Iteration 15/25 | Loss: 0.00079643
Iteration 16/25 | Loss: 0.00079595
Iteration 17/25 | Loss: 0.00079629
Iteration 18/25 | Loss: 0.00079578
Iteration 19/25 | Loss: 0.00079580
Iteration 20/25 | Loss: 0.00079626
Iteration 21/25 | Loss: 0.00079611
Iteration 22/25 | Loss: 0.00079615
Iteration 23/25 | Loss: 0.00079599
Iteration 24/25 | Loss: 0.00079612
Iteration 25/25 | Loss: 0.00079589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28782821
Iteration 2/25 | Loss: 0.00038498
Iteration 3/25 | Loss: 0.00038495
Iteration 4/25 | Loss: 0.00038495
Iteration 5/25 | Loss: 0.00038495
Iteration 6/25 | Loss: 0.00038495
Iteration 7/25 | Loss: 0.00038494
Iteration 8/25 | Loss: 0.00038494
Iteration 9/25 | Loss: 0.00038494
Iteration 10/25 | Loss: 0.00038494
Iteration 11/25 | Loss: 0.00038494
Iteration 12/25 | Loss: 0.00038494
Iteration 13/25 | Loss: 0.00038494
Iteration 14/25 | Loss: 0.00038494
Iteration 15/25 | Loss: 0.00038494
Iteration 16/25 | Loss: 0.00038494
Iteration 17/25 | Loss: 0.00038494
Iteration 18/25 | Loss: 0.00038494
Iteration 19/25 | Loss: 0.00038494
Iteration 20/25 | Loss: 0.00038494
Iteration 21/25 | Loss: 0.00038494
Iteration 22/25 | Loss: 0.00038494
Iteration 23/25 | Loss: 0.00038494
Iteration 24/25 | Loss: 0.00038494
Iteration 25/25 | Loss: 0.00038494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038494
Iteration 2/1000 | Loss: 0.00005281
Iteration 3/1000 | Loss: 0.00002732
Iteration 4/1000 | Loss: 0.00003716
Iteration 5/1000 | Loss: 0.00002617
Iteration 6/1000 | Loss: 0.00003035
Iteration 7/1000 | Loss: 0.00003050
Iteration 8/1000 | Loss: 0.00002385
Iteration 9/1000 | Loss: 0.00003515
Iteration 10/1000 | Loss: 0.00003353
Iteration 11/1000 | Loss: 0.00002806
Iteration 12/1000 | Loss: 0.00003056
Iteration 13/1000 | Loss: 0.00003792
Iteration 14/1000 | Loss: 0.00003099
Iteration 15/1000 | Loss: 0.00003132
Iteration 16/1000 | Loss: 0.00002788
Iteration 17/1000 | Loss: 0.00002893
Iteration 18/1000 | Loss: 0.00002919
Iteration 19/1000 | Loss: 0.00003248
Iteration 20/1000 | Loss: 0.00003377
Iteration 21/1000 | Loss: 0.00002678
Iteration 22/1000 | Loss: 0.00003020
Iteration 23/1000 | Loss: 0.00002489
Iteration 24/1000 | Loss: 0.00003075
Iteration 25/1000 | Loss: 0.00002991
Iteration 26/1000 | Loss: 0.00003021
Iteration 27/1000 | Loss: 0.00003571
Iteration 28/1000 | Loss: 0.00003059
Iteration 29/1000 | Loss: 0.00002967
Iteration 30/1000 | Loss: 0.00003097
Iteration 31/1000 | Loss: 0.00002991
Iteration 32/1000 | Loss: 0.00003058
Iteration 33/1000 | Loss: 0.00003795
Iteration 34/1000 | Loss: 0.00002897
Iteration 35/1000 | Loss: 0.00002989
Iteration 36/1000 | Loss: 0.00003048
Iteration 37/1000 | Loss: 0.00002993
Iteration 38/1000 | Loss: 0.00003045
Iteration 39/1000 | Loss: 0.00004568
Iteration 40/1000 | Loss: 0.00003209
Iteration 41/1000 | Loss: 0.00002886
Iteration 42/1000 | Loss: 0.00003453
Iteration 43/1000 | Loss: 0.00003493
Iteration 44/1000 | Loss: 0.00003492
Iteration 45/1000 | Loss: 0.00003584
Iteration 46/1000 | Loss: 0.00004482
Iteration 47/1000 | Loss: 0.00003684
Iteration 48/1000 | Loss: 0.00004597
Iteration 49/1000 | Loss: 0.00003548
Iteration 50/1000 | Loss: 0.00004473
Iteration 51/1000 | Loss: 0.00004167
Iteration 52/1000 | Loss: 0.00003505
Iteration 53/1000 | Loss: 0.00003839
Iteration 54/1000 | Loss: 0.00002430
Iteration 55/1000 | Loss: 0.00003249
Iteration 56/1000 | Loss: 0.00002873
Iteration 57/1000 | Loss: 0.00003858
Iteration 58/1000 | Loss: 0.00002813
Iteration 59/1000 | Loss: 0.00002788
Iteration 60/1000 | Loss: 0.00002172
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002053
Iteration 65/1000 | Loss: 0.00002044
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002043
Iteration 69/1000 | Loss: 0.00002042
Iteration 70/1000 | Loss: 0.00002040
Iteration 71/1000 | Loss: 0.00002040
Iteration 72/1000 | Loss: 0.00002040
Iteration 73/1000 | Loss: 0.00002040
Iteration 74/1000 | Loss: 0.00002040
Iteration 75/1000 | Loss: 0.00002040
Iteration 76/1000 | Loss: 0.00002039
Iteration 77/1000 | Loss: 0.00002039
Iteration 78/1000 | Loss: 0.00002039
Iteration 79/1000 | Loss: 0.00002039
Iteration 80/1000 | Loss: 0.00002039
Iteration 81/1000 | Loss: 0.00002039
Iteration 82/1000 | Loss: 0.00002039
Iteration 83/1000 | Loss: 0.00002039
Iteration 84/1000 | Loss: 0.00002039
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002039
Iteration 87/1000 | Loss: 0.00002038
Iteration 88/1000 | Loss: 0.00002038
Iteration 89/1000 | Loss: 0.00002038
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002038
Iteration 93/1000 | Loss: 0.00002036
Iteration 94/1000 | Loss: 0.00002036
Iteration 95/1000 | Loss: 0.00002036
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002035
Iteration 100/1000 | Loss: 0.00002035
Iteration 101/1000 | Loss: 0.00002035
Iteration 102/1000 | Loss: 0.00002035
Iteration 103/1000 | Loss: 0.00002035
Iteration 104/1000 | Loss: 0.00002035
Iteration 105/1000 | Loss: 0.00002035
Iteration 106/1000 | Loss: 0.00002034
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002032
Iteration 110/1000 | Loss: 0.00002031
Iteration 111/1000 | Loss: 0.00002031
Iteration 112/1000 | Loss: 0.00002029
Iteration 113/1000 | Loss: 0.00002029
Iteration 114/1000 | Loss: 0.00002029
Iteration 115/1000 | Loss: 0.00002029
Iteration 116/1000 | Loss: 0.00002029
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002028
Iteration 119/1000 | Loss: 0.00002028
Iteration 120/1000 | Loss: 0.00002028
Iteration 121/1000 | Loss: 0.00002028
Iteration 122/1000 | Loss: 0.00002028
Iteration 123/1000 | Loss: 0.00002027
Iteration 124/1000 | Loss: 0.00002027
Iteration 125/1000 | Loss: 0.00002027
Iteration 126/1000 | Loss: 0.00002027
Iteration 127/1000 | Loss: 0.00002027
Iteration 128/1000 | Loss: 0.00002027
Iteration 129/1000 | Loss: 0.00002027
Iteration 130/1000 | Loss: 0.00002027
Iteration 131/1000 | Loss: 0.00002026
Iteration 132/1000 | Loss: 0.00002026
Iteration 133/1000 | Loss: 0.00002026
Iteration 134/1000 | Loss: 0.00002026
Iteration 135/1000 | Loss: 0.00002026
Iteration 136/1000 | Loss: 0.00002026
Iteration 137/1000 | Loss: 0.00002026
Iteration 138/1000 | Loss: 0.00002026
Iteration 139/1000 | Loss: 0.00002026
Iteration 140/1000 | Loss: 0.00002026
Iteration 141/1000 | Loss: 0.00002026
Iteration 142/1000 | Loss: 0.00002026
Iteration 143/1000 | Loss: 0.00002025
Iteration 144/1000 | Loss: 0.00002025
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Iteration 148/1000 | Loss: 0.00002024
Iteration 149/1000 | Loss: 0.00002024
Iteration 150/1000 | Loss: 0.00002024
Iteration 151/1000 | Loss: 0.00002024
Iteration 152/1000 | Loss: 0.00002024
Iteration 153/1000 | Loss: 0.00002024
Iteration 154/1000 | Loss: 0.00002023
Iteration 155/1000 | Loss: 0.00002023
Iteration 156/1000 | Loss: 0.00002023
Iteration 157/1000 | Loss: 0.00002023
Iteration 158/1000 | Loss: 0.00002023
Iteration 159/1000 | Loss: 0.00002023
Iteration 160/1000 | Loss: 0.00002023
Iteration 161/1000 | Loss: 0.00002022
Iteration 162/1000 | Loss: 0.00002022
Iteration 163/1000 | Loss: 0.00002022
Iteration 164/1000 | Loss: 0.00002022
Iteration 165/1000 | Loss: 0.00002022
Iteration 166/1000 | Loss: 0.00002022
Iteration 167/1000 | Loss: 0.00002022
Iteration 168/1000 | Loss: 0.00002022
Iteration 169/1000 | Loss: 0.00002022
Iteration 170/1000 | Loss: 0.00002022
Iteration 171/1000 | Loss: 0.00002022
Iteration 172/1000 | Loss: 0.00002022
Iteration 173/1000 | Loss: 0.00002022
Iteration 174/1000 | Loss: 0.00002022
Iteration 175/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.022168155235704e-05, 2.022168155235704e-05, 2.022168155235704e-05, 2.022168155235704e-05, 2.022168155235704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.022168155235704e-05

Optimization complete. Final v2v error: 3.7165944576263428 mm

Highest mean error: 4.991937160491943 mm for frame 175

Lowest mean error: 3.5755252838134766 mm for frame 68

Saving results

Total time: 162.28965282440186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720255
Iteration 2/25 | Loss: 0.00110410
Iteration 3/25 | Loss: 0.00076623
Iteration 4/25 | Loss: 0.00069446
Iteration 5/25 | Loss: 0.00068213
Iteration 6/25 | Loss: 0.00068053
Iteration 7/25 | Loss: 0.00068053
Iteration 8/25 | Loss: 0.00068053
Iteration 9/25 | Loss: 0.00068053
Iteration 10/25 | Loss: 0.00068053
Iteration 11/25 | Loss: 0.00068053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006805342272855341, 0.0006805342272855341, 0.0006805342272855341, 0.0006805342272855341, 0.0006805342272855341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006805342272855341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42480087
Iteration 2/25 | Loss: 0.00020836
Iteration 3/25 | Loss: 0.00020834
Iteration 4/25 | Loss: 0.00020834
Iteration 5/25 | Loss: 0.00020834
Iteration 6/25 | Loss: 0.00020834
Iteration 7/25 | Loss: 0.00020834
Iteration 8/25 | Loss: 0.00020834
Iteration 9/25 | Loss: 0.00020834
Iteration 10/25 | Loss: 0.00020834
Iteration 11/25 | Loss: 0.00020834
Iteration 12/25 | Loss: 0.00020834
Iteration 13/25 | Loss: 0.00020834
Iteration 14/25 | Loss: 0.00020834
Iteration 15/25 | Loss: 0.00020834
Iteration 16/25 | Loss: 0.00020834
Iteration 17/25 | Loss: 0.00020834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00020833784947171807, 0.00020833784947171807, 0.00020833784947171807, 0.00020833784947171807, 0.00020833784947171807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020833784947171807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020834
Iteration 2/1000 | Loss: 0.00002445
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001871
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001707
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001676
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001650
Iteration 16/1000 | Loss: 0.00001643
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001628
Iteration 25/1000 | Loss: 0.00001627
Iteration 26/1000 | Loss: 0.00001627
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001623
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001622
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001621
Iteration 37/1000 | Loss: 0.00001621
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001620
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001619
Iteration 45/1000 | Loss: 0.00001619
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001619
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001618
Iteration 50/1000 | Loss: 0.00001618
Iteration 51/1000 | Loss: 0.00001618
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001617
Iteration 54/1000 | Loss: 0.00001617
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001616
Iteration 57/1000 | Loss: 0.00001616
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001616
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001613
Iteration 73/1000 | Loss: 0.00001613
Iteration 74/1000 | Loss: 0.00001613
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001611
Iteration 81/1000 | Loss: 0.00001611
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001611
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001609
Iteration 93/1000 | Loss: 0.00001609
Iteration 94/1000 | Loss: 0.00001609
Iteration 95/1000 | Loss: 0.00001609
Iteration 96/1000 | Loss: 0.00001609
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001608
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001608
Iteration 128/1000 | Loss: 0.00001608
Iteration 129/1000 | Loss: 0.00001608
Iteration 130/1000 | Loss: 0.00001608
Iteration 131/1000 | Loss: 0.00001608
Iteration 132/1000 | Loss: 0.00001608
Iteration 133/1000 | Loss: 0.00001608
Iteration 134/1000 | Loss: 0.00001608
Iteration 135/1000 | Loss: 0.00001608
Iteration 136/1000 | Loss: 0.00001608
Iteration 137/1000 | Loss: 0.00001608
Iteration 138/1000 | Loss: 0.00001608
Iteration 139/1000 | Loss: 0.00001608
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001608
Iteration 143/1000 | Loss: 0.00001608
Iteration 144/1000 | Loss: 0.00001608
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001608
Iteration 150/1000 | Loss: 0.00001608
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001608
Iteration 154/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.60752642841544e-05, 1.60752642841544e-05, 1.60752642841544e-05, 1.60752642841544e-05, 1.60752642841544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.60752642841544e-05

Optimization complete. Final v2v error: 3.458129644393921 mm

Highest mean error: 3.6532142162323 mm for frame 40

Lowest mean error: 3.236461639404297 mm for frame 193

Saving results

Total time: 40.945467472076416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390921
Iteration 2/25 | Loss: 0.00074195
Iteration 3/25 | Loss: 0.00063842
Iteration 4/25 | Loss: 0.00061744
Iteration 5/25 | Loss: 0.00060943
Iteration 6/25 | Loss: 0.00060799
Iteration 7/25 | Loss: 0.00060757
Iteration 8/25 | Loss: 0.00060753
Iteration 9/25 | Loss: 0.00060753
Iteration 10/25 | Loss: 0.00060753
Iteration 11/25 | Loss: 0.00060753
Iteration 12/25 | Loss: 0.00060753
Iteration 13/25 | Loss: 0.00060753
Iteration 14/25 | Loss: 0.00060753
Iteration 15/25 | Loss: 0.00060753
Iteration 16/25 | Loss: 0.00060753
Iteration 17/25 | Loss: 0.00060753
Iteration 18/25 | Loss: 0.00060753
Iteration 19/25 | Loss: 0.00060753
Iteration 20/25 | Loss: 0.00060753
Iteration 21/25 | Loss: 0.00060753
Iteration 22/25 | Loss: 0.00060753
Iteration 23/25 | Loss: 0.00060753
Iteration 24/25 | Loss: 0.00060753
Iteration 25/25 | Loss: 0.00060753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94918358
Iteration 2/25 | Loss: 0.00027237
Iteration 3/25 | Loss: 0.00027237
Iteration 4/25 | Loss: 0.00027237
Iteration 5/25 | Loss: 0.00027237
Iteration 6/25 | Loss: 0.00027237
Iteration 7/25 | Loss: 0.00027237
Iteration 8/25 | Loss: 0.00027237
Iteration 9/25 | Loss: 0.00027237
Iteration 10/25 | Loss: 0.00027237
Iteration 11/25 | Loss: 0.00027237
Iteration 12/25 | Loss: 0.00027237
Iteration 13/25 | Loss: 0.00027237
Iteration 14/25 | Loss: 0.00027237
Iteration 15/25 | Loss: 0.00027237
Iteration 16/25 | Loss: 0.00027237
Iteration 17/25 | Loss: 0.00027237
Iteration 18/25 | Loss: 0.00027237
Iteration 19/25 | Loss: 0.00027237
Iteration 20/25 | Loss: 0.00027237
Iteration 21/25 | Loss: 0.00027237
Iteration 22/25 | Loss: 0.00027237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002723654906731099, 0.0002723654906731099, 0.0002723654906731099, 0.0002723654906731099, 0.0002723654906731099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002723654906731099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027237
Iteration 2/1000 | Loss: 0.00002974
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001562
Iteration 7/1000 | Loss: 0.00001514
Iteration 8/1000 | Loss: 0.00001486
Iteration 9/1000 | Loss: 0.00001467
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001443
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001438
Iteration 25/1000 | Loss: 0.00001437
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001432
Iteration 30/1000 | Loss: 0.00001432
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001431
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001430
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001428
Iteration 41/1000 | Loss: 0.00001428
Iteration 42/1000 | Loss: 0.00001427
Iteration 43/1000 | Loss: 0.00001427
Iteration 44/1000 | Loss: 0.00001427
Iteration 45/1000 | Loss: 0.00001427
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001426
Iteration 48/1000 | Loss: 0.00001426
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001422
Iteration 62/1000 | Loss: 0.00001422
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001421
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001420
Iteration 71/1000 | Loss: 0.00001420
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001419
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001409
Iteration 95/1000 | Loss: 0.00001409
Iteration 96/1000 | Loss: 0.00001409
Iteration 97/1000 | Loss: 0.00001408
Iteration 98/1000 | Loss: 0.00001408
Iteration 99/1000 | Loss: 0.00001408
Iteration 100/1000 | Loss: 0.00001408
Iteration 101/1000 | Loss: 0.00001408
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001408
Iteration 105/1000 | Loss: 0.00001408
Iteration 106/1000 | Loss: 0.00001408
Iteration 107/1000 | Loss: 0.00001408
Iteration 108/1000 | Loss: 0.00001408
Iteration 109/1000 | Loss: 0.00001408
Iteration 110/1000 | Loss: 0.00001407
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001407
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001405
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001405
Iteration 133/1000 | Loss: 0.00001405
Iteration 134/1000 | Loss: 0.00001405
Iteration 135/1000 | Loss: 0.00001405
Iteration 136/1000 | Loss: 0.00001404
Iteration 137/1000 | Loss: 0.00001404
Iteration 138/1000 | Loss: 0.00001404
Iteration 139/1000 | Loss: 0.00001404
Iteration 140/1000 | Loss: 0.00001404
Iteration 141/1000 | Loss: 0.00001404
Iteration 142/1000 | Loss: 0.00001404
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001404
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001403
Iteration 156/1000 | Loss: 0.00001403
Iteration 157/1000 | Loss: 0.00001403
Iteration 158/1000 | Loss: 0.00001403
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001403
Iteration 176/1000 | Loss: 0.00001403
Iteration 177/1000 | Loss: 0.00001403
Iteration 178/1000 | Loss: 0.00001403
Iteration 179/1000 | Loss: 0.00001403
Iteration 180/1000 | Loss: 0.00001403
Iteration 181/1000 | Loss: 0.00001403
Iteration 182/1000 | Loss: 0.00001403
Iteration 183/1000 | Loss: 0.00001403
Iteration 184/1000 | Loss: 0.00001403
Iteration 185/1000 | Loss: 0.00001403
Iteration 186/1000 | Loss: 0.00001403
Iteration 187/1000 | Loss: 0.00001403
Iteration 188/1000 | Loss: 0.00001403
Iteration 189/1000 | Loss: 0.00001403
Iteration 190/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.402591442456469e-05, 1.402591442456469e-05, 1.402591442456469e-05, 1.402591442456469e-05, 1.402591442456469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.402591442456469e-05

Optimization complete. Final v2v error: 3.1671347618103027 mm

Highest mean error: 3.785536527633667 mm for frame 55

Lowest mean error: 2.9270219802856445 mm for frame 82

Saving results

Total time: 39.16389751434326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450222
Iteration 2/25 | Loss: 0.00092401
Iteration 3/25 | Loss: 0.00073268
Iteration 4/25 | Loss: 0.00067258
Iteration 5/25 | Loss: 0.00066256
Iteration 6/25 | Loss: 0.00066101
Iteration 7/25 | Loss: 0.00066071
Iteration 8/25 | Loss: 0.00066071
Iteration 9/25 | Loss: 0.00066071
Iteration 10/25 | Loss: 0.00066071
Iteration 11/25 | Loss: 0.00066071
Iteration 12/25 | Loss: 0.00066071
Iteration 13/25 | Loss: 0.00066071
Iteration 14/25 | Loss: 0.00066071
Iteration 15/25 | Loss: 0.00066071
Iteration 16/25 | Loss: 0.00066071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006607133545912802, 0.0006607133545912802, 0.0006607133545912802, 0.0006607133545912802, 0.0006607133545912802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006607133545912802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46767163
Iteration 2/25 | Loss: 0.00030439
Iteration 3/25 | Loss: 0.00030438
Iteration 4/25 | Loss: 0.00030438
Iteration 5/25 | Loss: 0.00030438
Iteration 6/25 | Loss: 0.00030438
Iteration 7/25 | Loss: 0.00030438
Iteration 8/25 | Loss: 0.00030438
Iteration 9/25 | Loss: 0.00030438
Iteration 10/25 | Loss: 0.00030438
Iteration 11/25 | Loss: 0.00030438
Iteration 12/25 | Loss: 0.00030438
Iteration 13/25 | Loss: 0.00030438
Iteration 14/25 | Loss: 0.00030438
Iteration 15/25 | Loss: 0.00030438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0003043771139346063, 0.0003043771139346063, 0.0003043771139346063, 0.0003043771139346063, 0.0003043771139346063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003043771139346063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030438
Iteration 2/1000 | Loss: 0.00003488
Iteration 3/1000 | Loss: 0.00002467
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001946
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001804
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001665
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001636
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001630
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001628
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001614
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001611
Iteration 30/1000 | Loss: 0.00001611
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001610
Iteration 38/1000 | Loss: 0.00001610
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001607
Iteration 50/1000 | Loss: 0.00001607
Iteration 51/1000 | Loss: 0.00001607
Iteration 52/1000 | Loss: 0.00001607
Iteration 53/1000 | Loss: 0.00001607
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001605
Iteration 65/1000 | Loss: 0.00001605
Iteration 66/1000 | Loss: 0.00001605
Iteration 67/1000 | Loss: 0.00001605
Iteration 68/1000 | Loss: 0.00001605
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001605
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001605
Iteration 90/1000 | Loss: 0.00001605
Iteration 91/1000 | Loss: 0.00001605
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001605
Iteration 103/1000 | Loss: 0.00001605
Iteration 104/1000 | Loss: 0.00001605
Iteration 105/1000 | Loss: 0.00001605
Iteration 106/1000 | Loss: 0.00001605
Iteration 107/1000 | Loss: 0.00001605
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001605
Iteration 116/1000 | Loss: 0.00001605
Iteration 117/1000 | Loss: 0.00001605
Iteration 118/1000 | Loss: 0.00001605
Iteration 119/1000 | Loss: 0.00001605
Iteration 120/1000 | Loss: 0.00001605
Iteration 121/1000 | Loss: 0.00001605
Iteration 122/1000 | Loss: 0.00001605
Iteration 123/1000 | Loss: 0.00001605
Iteration 124/1000 | Loss: 0.00001605
Iteration 125/1000 | Loss: 0.00001605
Iteration 126/1000 | Loss: 0.00001605
Iteration 127/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.6045865777414292e-05, 1.6045865777414292e-05, 1.6045865777414292e-05, 1.6045865777414292e-05, 1.6045865777414292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6045865777414292e-05

Optimization complete. Final v2v error: 3.3336236476898193 mm

Highest mean error: 3.8998122215270996 mm for frame 112

Lowest mean error: 2.8548498153686523 mm for frame 0

Saving results

Total time: 37.778361082077026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855872
Iteration 2/25 | Loss: 0.00132397
Iteration 3/25 | Loss: 0.00076280
Iteration 4/25 | Loss: 0.00069491
Iteration 5/25 | Loss: 0.00068229
Iteration 6/25 | Loss: 0.00068078
Iteration 7/25 | Loss: 0.00068055
Iteration 8/25 | Loss: 0.00068055
Iteration 9/25 | Loss: 0.00068055
Iteration 10/25 | Loss: 0.00068055
Iteration 11/25 | Loss: 0.00068055
Iteration 12/25 | Loss: 0.00068055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006805535522289574, 0.0006805535522289574, 0.0006805535522289574, 0.0006805535522289574, 0.0006805535522289574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006805535522289574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44918263
Iteration 2/25 | Loss: 0.00028442
Iteration 3/25 | Loss: 0.00028441
Iteration 4/25 | Loss: 0.00028441
Iteration 5/25 | Loss: 0.00028441
Iteration 6/25 | Loss: 0.00028441
Iteration 7/25 | Loss: 0.00028441
Iteration 8/25 | Loss: 0.00028441
Iteration 9/25 | Loss: 0.00028441
Iteration 10/25 | Loss: 0.00028441
Iteration 11/25 | Loss: 0.00028441
Iteration 12/25 | Loss: 0.00028441
Iteration 13/25 | Loss: 0.00028441
Iteration 14/25 | Loss: 0.00028441
Iteration 15/25 | Loss: 0.00028441
Iteration 16/25 | Loss: 0.00028441
Iteration 17/25 | Loss: 0.00028441
Iteration 18/25 | Loss: 0.00028441
Iteration 19/25 | Loss: 0.00028441
Iteration 20/25 | Loss: 0.00028441
Iteration 21/25 | Loss: 0.00028441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002844068221747875, 0.0002844068221747875, 0.0002844068221747875, 0.0002844068221747875, 0.0002844068221747875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002844068221747875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028441
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00002398
Iteration 4/1000 | Loss: 0.00002245
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00002038
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001961
Iteration 9/1000 | Loss: 0.00001949
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001900
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001897
Iteration 24/1000 | Loss: 0.00001896
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001895
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001895
Iteration 36/1000 | Loss: 0.00001895
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001895
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001894
Iteration 43/1000 | Loss: 0.00001894
Iteration 44/1000 | Loss: 0.00001893
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001892
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001892
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001892
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001889
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001888
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001887
Iteration 88/1000 | Loss: 0.00001887
Iteration 89/1000 | Loss: 0.00001887
Iteration 90/1000 | Loss: 0.00001887
Iteration 91/1000 | Loss: 0.00001886
Iteration 92/1000 | Loss: 0.00001886
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001885
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00001882
Iteration 104/1000 | Loss: 0.00001882
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001880
Iteration 111/1000 | Loss: 0.00001880
Iteration 112/1000 | Loss: 0.00001880
Iteration 113/1000 | Loss: 0.00001880
Iteration 114/1000 | Loss: 0.00001879
Iteration 115/1000 | Loss: 0.00001879
Iteration 116/1000 | Loss: 0.00001879
Iteration 117/1000 | Loss: 0.00001879
Iteration 118/1000 | Loss: 0.00001879
Iteration 119/1000 | Loss: 0.00001879
Iteration 120/1000 | Loss: 0.00001879
Iteration 121/1000 | Loss: 0.00001878
Iteration 122/1000 | Loss: 0.00001878
Iteration 123/1000 | Loss: 0.00001878
Iteration 124/1000 | Loss: 0.00001878
Iteration 125/1000 | Loss: 0.00001878
Iteration 126/1000 | Loss: 0.00001878
Iteration 127/1000 | Loss: 0.00001878
Iteration 128/1000 | Loss: 0.00001878
Iteration 129/1000 | Loss: 0.00001878
Iteration 130/1000 | Loss: 0.00001878
Iteration 131/1000 | Loss: 0.00001878
Iteration 132/1000 | Loss: 0.00001878
Iteration 133/1000 | Loss: 0.00001878
Iteration 134/1000 | Loss: 0.00001878
Iteration 135/1000 | Loss: 0.00001877
Iteration 136/1000 | Loss: 0.00001877
Iteration 137/1000 | Loss: 0.00001877
Iteration 138/1000 | Loss: 0.00001877
Iteration 139/1000 | Loss: 0.00001876
Iteration 140/1000 | Loss: 0.00001876
Iteration 141/1000 | Loss: 0.00001876
Iteration 142/1000 | Loss: 0.00001876
Iteration 143/1000 | Loss: 0.00001876
Iteration 144/1000 | Loss: 0.00001876
Iteration 145/1000 | Loss: 0.00001876
Iteration 146/1000 | Loss: 0.00001876
Iteration 147/1000 | Loss: 0.00001876
Iteration 148/1000 | Loss: 0.00001876
Iteration 149/1000 | Loss: 0.00001876
Iteration 150/1000 | Loss: 0.00001876
Iteration 151/1000 | Loss: 0.00001876
Iteration 152/1000 | Loss: 0.00001875
Iteration 153/1000 | Loss: 0.00001875
Iteration 154/1000 | Loss: 0.00001875
Iteration 155/1000 | Loss: 0.00001875
Iteration 156/1000 | Loss: 0.00001875
Iteration 157/1000 | Loss: 0.00001875
Iteration 158/1000 | Loss: 0.00001875
Iteration 159/1000 | Loss: 0.00001875
Iteration 160/1000 | Loss: 0.00001875
Iteration 161/1000 | Loss: 0.00001875
Iteration 162/1000 | Loss: 0.00001875
Iteration 163/1000 | Loss: 0.00001875
Iteration 164/1000 | Loss: 0.00001875
Iteration 165/1000 | Loss: 0.00001875
Iteration 166/1000 | Loss: 0.00001875
Iteration 167/1000 | Loss: 0.00001875
Iteration 168/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.8751952666207217e-05, 1.8751952666207217e-05, 1.8751952666207217e-05, 1.8751952666207217e-05, 1.8751952666207217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8751952666207217e-05

Optimization complete. Final v2v error: 3.631572961807251 mm

Highest mean error: 3.7271499633789062 mm for frame 48

Lowest mean error: 3.5380496978759766 mm for frame 141

Saving results

Total time: 35.737387895584106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827050
Iteration 2/25 | Loss: 0.00106410
Iteration 3/25 | Loss: 0.00078292
Iteration 4/25 | Loss: 0.00074100
Iteration 5/25 | Loss: 0.00072275
Iteration 6/25 | Loss: 0.00071951
Iteration 7/25 | Loss: 0.00071885
Iteration 8/25 | Loss: 0.00071884
Iteration 9/25 | Loss: 0.00071884
Iteration 10/25 | Loss: 0.00071884
Iteration 11/25 | Loss: 0.00071884
Iteration 12/25 | Loss: 0.00071884
Iteration 13/25 | Loss: 0.00071884
Iteration 14/25 | Loss: 0.00071884
Iteration 15/25 | Loss: 0.00071884
Iteration 16/25 | Loss: 0.00071884
Iteration 17/25 | Loss: 0.00071884
Iteration 18/25 | Loss: 0.00071884
Iteration 19/25 | Loss: 0.00071884
Iteration 20/25 | Loss: 0.00071884
Iteration 21/25 | Loss: 0.00071884
Iteration 22/25 | Loss: 0.00071884
Iteration 23/25 | Loss: 0.00071884
Iteration 24/25 | Loss: 0.00071884
Iteration 25/25 | Loss: 0.00071884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32705736
Iteration 2/25 | Loss: 0.00028350
Iteration 3/25 | Loss: 0.00028345
Iteration 4/25 | Loss: 0.00028345
Iteration 5/25 | Loss: 0.00028345
Iteration 6/25 | Loss: 0.00028345
Iteration 7/25 | Loss: 0.00028345
Iteration 8/25 | Loss: 0.00028344
Iteration 9/25 | Loss: 0.00028344
Iteration 10/25 | Loss: 0.00028344
Iteration 11/25 | Loss: 0.00028344
Iteration 12/25 | Loss: 0.00028344
Iteration 13/25 | Loss: 0.00028344
Iteration 14/25 | Loss: 0.00028344
Iteration 15/25 | Loss: 0.00028344
Iteration 16/25 | Loss: 0.00028344
Iteration 17/25 | Loss: 0.00028344
Iteration 18/25 | Loss: 0.00028344
Iteration 19/25 | Loss: 0.00028344
Iteration 20/25 | Loss: 0.00028344
Iteration 21/25 | Loss: 0.00028344
Iteration 22/25 | Loss: 0.00028344
Iteration 23/25 | Loss: 0.00028344
Iteration 24/25 | Loss: 0.00028344
Iteration 25/25 | Loss: 0.00028344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028344
Iteration 2/1000 | Loss: 0.00004269
Iteration 3/1000 | Loss: 0.00003118
Iteration 4/1000 | Loss: 0.00002790
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00002490
Iteration 7/1000 | Loss: 0.00002404
Iteration 8/1000 | Loss: 0.00002347
Iteration 9/1000 | Loss: 0.00002305
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002264
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002252
Iteration 14/1000 | Loss: 0.00002245
Iteration 15/1000 | Loss: 0.00002242
Iteration 16/1000 | Loss: 0.00002241
Iteration 17/1000 | Loss: 0.00002235
Iteration 18/1000 | Loss: 0.00002233
Iteration 19/1000 | Loss: 0.00002232
Iteration 20/1000 | Loss: 0.00002231
Iteration 21/1000 | Loss: 0.00002231
Iteration 22/1000 | Loss: 0.00002231
Iteration 23/1000 | Loss: 0.00002231
Iteration 24/1000 | Loss: 0.00002230
Iteration 25/1000 | Loss: 0.00002230
Iteration 26/1000 | Loss: 0.00002230
Iteration 27/1000 | Loss: 0.00002230
Iteration 28/1000 | Loss: 0.00002230
Iteration 29/1000 | Loss: 0.00002229
Iteration 30/1000 | Loss: 0.00002229
Iteration 31/1000 | Loss: 0.00002228
Iteration 32/1000 | Loss: 0.00002228
Iteration 33/1000 | Loss: 0.00002227
Iteration 34/1000 | Loss: 0.00002227
Iteration 35/1000 | Loss: 0.00002227
Iteration 36/1000 | Loss: 0.00002227
Iteration 37/1000 | Loss: 0.00002227
Iteration 38/1000 | Loss: 0.00002226
Iteration 39/1000 | Loss: 0.00002226
Iteration 40/1000 | Loss: 0.00002225
Iteration 41/1000 | Loss: 0.00002225
Iteration 42/1000 | Loss: 0.00002225
Iteration 43/1000 | Loss: 0.00002224
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00002223
Iteration 46/1000 | Loss: 0.00002223
Iteration 47/1000 | Loss: 0.00002223
Iteration 48/1000 | Loss: 0.00002223
Iteration 49/1000 | Loss: 0.00002223
Iteration 50/1000 | Loss: 0.00002223
Iteration 51/1000 | Loss: 0.00002223
Iteration 52/1000 | Loss: 0.00002223
Iteration 53/1000 | Loss: 0.00002222
Iteration 54/1000 | Loss: 0.00002222
Iteration 55/1000 | Loss: 0.00002222
Iteration 56/1000 | Loss: 0.00002222
Iteration 57/1000 | Loss: 0.00002222
Iteration 58/1000 | Loss: 0.00002222
Iteration 59/1000 | Loss: 0.00002221
Iteration 60/1000 | Loss: 0.00002221
Iteration 61/1000 | Loss: 0.00002220
Iteration 62/1000 | Loss: 0.00002220
Iteration 63/1000 | Loss: 0.00002220
Iteration 64/1000 | Loss: 0.00002220
Iteration 65/1000 | Loss: 0.00002220
Iteration 66/1000 | Loss: 0.00002220
Iteration 67/1000 | Loss: 0.00002220
Iteration 68/1000 | Loss: 0.00002220
Iteration 69/1000 | Loss: 0.00002219
Iteration 70/1000 | Loss: 0.00002219
Iteration 71/1000 | Loss: 0.00002219
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002218
Iteration 74/1000 | Loss: 0.00002218
Iteration 75/1000 | Loss: 0.00002218
Iteration 76/1000 | Loss: 0.00002217
Iteration 77/1000 | Loss: 0.00002217
Iteration 78/1000 | Loss: 0.00002217
Iteration 79/1000 | Loss: 0.00002217
Iteration 80/1000 | Loss: 0.00002217
Iteration 81/1000 | Loss: 0.00002217
Iteration 82/1000 | Loss: 0.00002217
Iteration 83/1000 | Loss: 0.00002216
Iteration 84/1000 | Loss: 0.00002216
Iteration 85/1000 | Loss: 0.00002216
Iteration 86/1000 | Loss: 0.00002216
Iteration 87/1000 | Loss: 0.00002216
Iteration 88/1000 | Loss: 0.00002216
Iteration 89/1000 | Loss: 0.00002216
Iteration 90/1000 | Loss: 0.00002216
Iteration 91/1000 | Loss: 0.00002216
Iteration 92/1000 | Loss: 0.00002216
Iteration 93/1000 | Loss: 0.00002216
Iteration 94/1000 | Loss: 0.00002215
Iteration 95/1000 | Loss: 0.00002215
Iteration 96/1000 | Loss: 0.00002215
Iteration 97/1000 | Loss: 0.00002215
Iteration 98/1000 | Loss: 0.00002215
Iteration 99/1000 | Loss: 0.00002215
Iteration 100/1000 | Loss: 0.00002215
Iteration 101/1000 | Loss: 0.00002215
Iteration 102/1000 | Loss: 0.00002215
Iteration 103/1000 | Loss: 0.00002215
Iteration 104/1000 | Loss: 0.00002215
Iteration 105/1000 | Loss: 0.00002214
Iteration 106/1000 | Loss: 0.00002214
Iteration 107/1000 | Loss: 0.00002214
Iteration 108/1000 | Loss: 0.00002214
Iteration 109/1000 | Loss: 0.00002214
Iteration 110/1000 | Loss: 0.00002214
Iteration 111/1000 | Loss: 0.00002214
Iteration 112/1000 | Loss: 0.00002214
Iteration 113/1000 | Loss: 0.00002214
Iteration 114/1000 | Loss: 0.00002214
Iteration 115/1000 | Loss: 0.00002214
Iteration 116/1000 | Loss: 0.00002214
Iteration 117/1000 | Loss: 0.00002213
Iteration 118/1000 | Loss: 0.00002213
Iteration 119/1000 | Loss: 0.00002213
Iteration 120/1000 | Loss: 0.00002213
Iteration 121/1000 | Loss: 0.00002213
Iteration 122/1000 | Loss: 0.00002213
Iteration 123/1000 | Loss: 0.00002213
Iteration 124/1000 | Loss: 0.00002213
Iteration 125/1000 | Loss: 0.00002212
Iteration 126/1000 | Loss: 0.00002212
Iteration 127/1000 | Loss: 0.00002212
Iteration 128/1000 | Loss: 0.00002212
Iteration 129/1000 | Loss: 0.00002212
Iteration 130/1000 | Loss: 0.00002212
Iteration 131/1000 | Loss: 0.00002212
Iteration 132/1000 | Loss: 0.00002212
Iteration 133/1000 | Loss: 0.00002212
Iteration 134/1000 | Loss: 0.00002212
Iteration 135/1000 | Loss: 0.00002212
Iteration 136/1000 | Loss: 0.00002212
Iteration 137/1000 | Loss: 0.00002212
Iteration 138/1000 | Loss: 0.00002212
Iteration 139/1000 | Loss: 0.00002212
Iteration 140/1000 | Loss: 0.00002212
Iteration 141/1000 | Loss: 0.00002211
Iteration 142/1000 | Loss: 0.00002211
Iteration 143/1000 | Loss: 0.00002211
Iteration 144/1000 | Loss: 0.00002211
Iteration 145/1000 | Loss: 0.00002211
Iteration 146/1000 | Loss: 0.00002211
Iteration 147/1000 | Loss: 0.00002211
Iteration 148/1000 | Loss: 0.00002211
Iteration 149/1000 | Loss: 0.00002211
Iteration 150/1000 | Loss: 0.00002211
Iteration 151/1000 | Loss: 0.00002210
Iteration 152/1000 | Loss: 0.00002210
Iteration 153/1000 | Loss: 0.00002210
Iteration 154/1000 | Loss: 0.00002210
Iteration 155/1000 | Loss: 0.00002210
Iteration 156/1000 | Loss: 0.00002210
Iteration 157/1000 | Loss: 0.00002210
Iteration 158/1000 | Loss: 0.00002210
Iteration 159/1000 | Loss: 0.00002210
Iteration 160/1000 | Loss: 0.00002210
Iteration 161/1000 | Loss: 0.00002210
Iteration 162/1000 | Loss: 0.00002209
Iteration 163/1000 | Loss: 0.00002209
Iteration 164/1000 | Loss: 0.00002209
Iteration 165/1000 | Loss: 0.00002208
Iteration 166/1000 | Loss: 0.00002208
Iteration 167/1000 | Loss: 0.00002208
Iteration 168/1000 | Loss: 0.00002208
Iteration 169/1000 | Loss: 0.00002208
Iteration 170/1000 | Loss: 0.00002208
Iteration 171/1000 | Loss: 0.00002208
Iteration 172/1000 | Loss: 0.00002207
Iteration 173/1000 | Loss: 0.00002207
Iteration 174/1000 | Loss: 0.00002207
Iteration 175/1000 | Loss: 0.00002207
Iteration 176/1000 | Loss: 0.00002207
Iteration 177/1000 | Loss: 0.00002207
Iteration 178/1000 | Loss: 0.00002207
Iteration 179/1000 | Loss: 0.00002207
Iteration 180/1000 | Loss: 0.00002207
Iteration 181/1000 | Loss: 0.00002207
Iteration 182/1000 | Loss: 0.00002207
Iteration 183/1000 | Loss: 0.00002207
Iteration 184/1000 | Loss: 0.00002207
Iteration 185/1000 | Loss: 0.00002207
Iteration 186/1000 | Loss: 0.00002207
Iteration 187/1000 | Loss: 0.00002207
Iteration 188/1000 | Loss: 0.00002207
Iteration 189/1000 | Loss: 0.00002207
Iteration 190/1000 | Loss: 0.00002207
Iteration 191/1000 | Loss: 0.00002207
Iteration 192/1000 | Loss: 0.00002207
Iteration 193/1000 | Loss: 0.00002207
Iteration 194/1000 | Loss: 0.00002207
Iteration 195/1000 | Loss: 0.00002207
Iteration 196/1000 | Loss: 0.00002207
Iteration 197/1000 | Loss: 0.00002207
Iteration 198/1000 | Loss: 0.00002207
Iteration 199/1000 | Loss: 0.00002207
Iteration 200/1000 | Loss: 0.00002207
Iteration 201/1000 | Loss: 0.00002207
Iteration 202/1000 | Loss: 0.00002207
Iteration 203/1000 | Loss: 0.00002207
Iteration 204/1000 | Loss: 0.00002207
Iteration 205/1000 | Loss: 0.00002207
Iteration 206/1000 | Loss: 0.00002207
Iteration 207/1000 | Loss: 0.00002207
Iteration 208/1000 | Loss: 0.00002207
Iteration 209/1000 | Loss: 0.00002207
Iteration 210/1000 | Loss: 0.00002207
Iteration 211/1000 | Loss: 0.00002207
Iteration 212/1000 | Loss: 0.00002207
Iteration 213/1000 | Loss: 0.00002207
Iteration 214/1000 | Loss: 0.00002207
Iteration 215/1000 | Loss: 0.00002207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.2066957171773538e-05, 2.2066957171773538e-05, 2.2066957171773538e-05, 2.2066957171773538e-05, 2.2066957171773538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2066957171773538e-05

Optimization complete. Final v2v error: 3.94648814201355 mm

Highest mean error: 5.324010372161865 mm for frame 52

Lowest mean error: 3.267883777618408 mm for frame 81

Saving results

Total time: 41.66747260093689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00687444
Iteration 2/25 | Loss: 0.00134233
Iteration 3/25 | Loss: 0.00084348
Iteration 4/25 | Loss: 0.00070139
Iteration 5/25 | Loss: 0.00067421
Iteration 6/25 | Loss: 0.00066361
Iteration 7/25 | Loss: 0.00065201
Iteration 8/25 | Loss: 0.00065075
Iteration 9/25 | Loss: 0.00064802
Iteration 10/25 | Loss: 0.00064462
Iteration 11/25 | Loss: 0.00064311
Iteration 12/25 | Loss: 0.00064215
Iteration 13/25 | Loss: 0.00064172
Iteration 14/25 | Loss: 0.00064160
Iteration 15/25 | Loss: 0.00064152
Iteration 16/25 | Loss: 0.00064147
Iteration 17/25 | Loss: 0.00064147
Iteration 18/25 | Loss: 0.00064146
Iteration 19/25 | Loss: 0.00064146
Iteration 20/25 | Loss: 0.00064146
Iteration 21/25 | Loss: 0.00064146
Iteration 22/25 | Loss: 0.00064146
Iteration 23/25 | Loss: 0.00064145
Iteration 24/25 | Loss: 0.00064145
Iteration 25/25 | Loss: 0.00064145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62955999
Iteration 2/25 | Loss: 0.00018403
Iteration 3/25 | Loss: 0.00018386
Iteration 4/25 | Loss: 0.00018386
Iteration 5/25 | Loss: 0.00018386
Iteration 6/25 | Loss: 0.00018386
Iteration 7/25 | Loss: 0.00018386
Iteration 8/25 | Loss: 0.00018386
Iteration 9/25 | Loss: 0.00018386
Iteration 10/25 | Loss: 0.00018386
Iteration 11/25 | Loss: 0.00018386
Iteration 12/25 | Loss: 0.00018386
Iteration 13/25 | Loss: 0.00018386
Iteration 14/25 | Loss: 0.00018386
Iteration 15/25 | Loss: 0.00018386
Iteration 16/25 | Loss: 0.00018386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00018385988369118422, 0.00018385988369118422, 0.00018385988369118422, 0.00018385988369118422, 0.00018385988369118422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018385988369118422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018386
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00002020
Iteration 5/1000 | Loss: 0.00008378
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00005257
Iteration 8/1000 | Loss: 0.00002444
Iteration 9/1000 | Loss: 0.00001771
Iteration 10/1000 | Loss: 0.00001744
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001700
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001693
Iteration 15/1000 | Loss: 0.00001687
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001679
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001674
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001670
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001668
Iteration 26/1000 | Loss: 0.00001668
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001668
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001667
Iteration 31/1000 | Loss: 0.00001667
Iteration 32/1000 | Loss: 0.00001667
Iteration 33/1000 | Loss: 0.00001667
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001666
Iteration 38/1000 | Loss: 0.00001666
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001665
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001665
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001664
Iteration 51/1000 | Loss: 0.00001664
Iteration 52/1000 | Loss: 0.00001664
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001663
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001661
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001660
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001660
Iteration 81/1000 | Loss: 0.00001660
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001659
Iteration 86/1000 | Loss: 0.00001659
Iteration 87/1000 | Loss: 0.00001658
Iteration 88/1000 | Loss: 0.00001658
Iteration 89/1000 | Loss: 0.00001658
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001655
Iteration 102/1000 | Loss: 0.00001655
Iteration 103/1000 | Loss: 0.00001655
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001654
Iteration 107/1000 | Loss: 0.00001654
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001653
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001648
Iteration 134/1000 | Loss: 0.00001648
Iteration 135/1000 | Loss: 0.00001648
Iteration 136/1000 | Loss: 0.00001648
Iteration 137/1000 | Loss: 0.00001647
Iteration 138/1000 | Loss: 0.00001647
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001647
Iteration 141/1000 | Loss: 0.00001647
Iteration 142/1000 | Loss: 0.00001647
Iteration 143/1000 | Loss: 0.00001647
Iteration 144/1000 | Loss: 0.00001647
Iteration 145/1000 | Loss: 0.00001646
Iteration 146/1000 | Loss: 0.00001646
Iteration 147/1000 | Loss: 0.00001646
Iteration 148/1000 | Loss: 0.00001646
Iteration 149/1000 | Loss: 0.00001645
Iteration 150/1000 | Loss: 0.00001645
Iteration 151/1000 | Loss: 0.00001645
Iteration 152/1000 | Loss: 0.00001645
Iteration 153/1000 | Loss: 0.00001645
Iteration 154/1000 | Loss: 0.00001645
Iteration 155/1000 | Loss: 0.00001645
Iteration 156/1000 | Loss: 0.00001645
Iteration 157/1000 | Loss: 0.00001645
Iteration 158/1000 | Loss: 0.00001644
Iteration 159/1000 | Loss: 0.00001644
Iteration 160/1000 | Loss: 0.00001644
Iteration 161/1000 | Loss: 0.00001644
Iteration 162/1000 | Loss: 0.00001644
Iteration 163/1000 | Loss: 0.00001644
Iteration 164/1000 | Loss: 0.00001644
Iteration 165/1000 | Loss: 0.00001644
Iteration 166/1000 | Loss: 0.00001644
Iteration 167/1000 | Loss: 0.00001644
Iteration 168/1000 | Loss: 0.00001644
Iteration 169/1000 | Loss: 0.00001644
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001643
Iteration 172/1000 | Loss: 0.00001643
Iteration 173/1000 | Loss: 0.00001643
Iteration 174/1000 | Loss: 0.00001643
Iteration 175/1000 | Loss: 0.00001643
Iteration 176/1000 | Loss: 0.00001642
Iteration 177/1000 | Loss: 0.00001642
Iteration 178/1000 | Loss: 0.00001642
Iteration 179/1000 | Loss: 0.00001642
Iteration 180/1000 | Loss: 0.00001642
Iteration 181/1000 | Loss: 0.00001642
Iteration 182/1000 | Loss: 0.00001642
Iteration 183/1000 | Loss: 0.00001642
Iteration 184/1000 | Loss: 0.00001642
Iteration 185/1000 | Loss: 0.00001642
Iteration 186/1000 | Loss: 0.00001642
Iteration 187/1000 | Loss: 0.00001642
Iteration 188/1000 | Loss: 0.00001642
Iteration 189/1000 | Loss: 0.00001642
Iteration 190/1000 | Loss: 0.00001642
Iteration 191/1000 | Loss: 0.00001642
Iteration 192/1000 | Loss: 0.00001642
Iteration 193/1000 | Loss: 0.00001642
Iteration 194/1000 | Loss: 0.00001642
Iteration 195/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.641810558794532e-05, 1.641810558794532e-05, 1.641810558794532e-05, 1.641810558794532e-05, 1.641810558794532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.641810558794532e-05

Optimization complete. Final v2v error: 3.4687435626983643 mm

Highest mean error: 3.793924570083618 mm for frame 194

Lowest mean error: 3.0026400089263916 mm for frame 79

Saving results

Total time: 68.2325165271759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042658
Iteration 2/25 | Loss: 0.00229755
Iteration 3/25 | Loss: 0.00148772
Iteration 4/25 | Loss: 0.00126904
Iteration 5/25 | Loss: 0.00116426
Iteration 6/25 | Loss: 0.00096322
Iteration 7/25 | Loss: 0.00090627
Iteration 8/25 | Loss: 0.00089193
Iteration 9/25 | Loss: 0.00085747
Iteration 10/25 | Loss: 0.00083884
Iteration 11/25 | Loss: 0.00082917
Iteration 12/25 | Loss: 0.00082497
Iteration 13/25 | Loss: 0.00080314
Iteration 14/25 | Loss: 0.00078753
Iteration 15/25 | Loss: 0.00078262
Iteration 16/25 | Loss: 0.00077998
Iteration 17/25 | Loss: 0.00077731
Iteration 18/25 | Loss: 0.00077639
Iteration 19/25 | Loss: 0.00077495
Iteration 20/25 | Loss: 0.00077388
Iteration 21/25 | Loss: 0.00077562
Iteration 22/25 | Loss: 0.00077250
Iteration 23/25 | Loss: 0.00077199
Iteration 24/25 | Loss: 0.00077172
Iteration 25/25 | Loss: 0.00077161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47876024
Iteration 2/25 | Loss: 0.00056000
Iteration 3/25 | Loss: 0.00055999
Iteration 4/25 | Loss: 0.00055999
Iteration 5/25 | Loss: 0.00055999
Iteration 6/25 | Loss: 0.00055999
Iteration 7/25 | Loss: 0.00055591
Iteration 8/25 | Loss: 0.00055591
Iteration 9/25 | Loss: 0.00055591
Iteration 10/25 | Loss: 0.00055591
Iteration 11/25 | Loss: 0.00055591
Iteration 12/25 | Loss: 0.00055591
Iteration 13/25 | Loss: 0.00055591
Iteration 14/25 | Loss: 0.00055591
Iteration 15/25 | Loss: 0.00055591
Iteration 16/25 | Loss: 0.00055591
Iteration 17/25 | Loss: 0.00055591
Iteration 18/25 | Loss: 0.00055591
Iteration 19/25 | Loss: 0.00055591
Iteration 20/25 | Loss: 0.00055591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005559095880016685, 0.0005559095880016685, 0.0005559095880016685, 0.0005559095880016685, 0.0005559095880016685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005559095880016685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055591
Iteration 2/1000 | Loss: 0.00006949
Iteration 3/1000 | Loss: 0.00007006
Iteration 4/1000 | Loss: 0.00011207
Iteration 5/1000 | Loss: 0.00008875
Iteration 6/1000 | Loss: 0.00010920
Iteration 7/1000 | Loss: 0.00007604
Iteration 8/1000 | Loss: 0.00004150
Iteration 9/1000 | Loss: 0.00006322
Iteration 10/1000 | Loss: 0.00003450
Iteration 11/1000 | Loss: 0.00015196
Iteration 12/1000 | Loss: 0.00003340
Iteration 13/1000 | Loss: 0.00037627
Iteration 14/1000 | Loss: 0.00003268
Iteration 15/1000 | Loss: 0.00003250
Iteration 16/1000 | Loss: 0.00002975
Iteration 17/1000 | Loss: 0.00002897
Iteration 18/1000 | Loss: 0.00002878
Iteration 19/1000 | Loss: 0.00002737
Iteration 20/1000 | Loss: 0.00002688
Iteration 21/1000 | Loss: 0.00002659
Iteration 22/1000 | Loss: 0.00057645
Iteration 23/1000 | Loss: 0.00006575
Iteration 24/1000 | Loss: 0.00003700
Iteration 25/1000 | Loss: 0.00004304
Iteration 26/1000 | Loss: 0.00003992
Iteration 27/1000 | Loss: 0.00011761
Iteration 28/1000 | Loss: 0.00012857
Iteration 29/1000 | Loss: 0.00002808
Iteration 30/1000 | Loss: 0.00002597
Iteration 31/1000 | Loss: 0.00010254
Iteration 32/1000 | Loss: 0.00002529
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002461
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002444
Iteration 37/1000 | Loss: 0.00002436
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002431
Iteration 40/1000 | Loss: 0.00002430
Iteration 41/1000 | Loss: 0.00002427
Iteration 42/1000 | Loss: 0.00002422
Iteration 43/1000 | Loss: 0.00002421
Iteration 44/1000 | Loss: 0.00002419
Iteration 45/1000 | Loss: 0.00002419
Iteration 46/1000 | Loss: 0.00002418
Iteration 47/1000 | Loss: 0.00002418
Iteration 48/1000 | Loss: 0.00002418
Iteration 49/1000 | Loss: 0.00002417
Iteration 50/1000 | Loss: 0.00002417
Iteration 51/1000 | Loss: 0.00002417
Iteration 52/1000 | Loss: 0.00002415
Iteration 53/1000 | Loss: 0.00002415
Iteration 54/1000 | Loss: 0.00002412
Iteration 55/1000 | Loss: 0.00002411
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002403
Iteration 59/1000 | Loss: 0.00002403
Iteration 60/1000 | Loss: 0.00002403
Iteration 61/1000 | Loss: 0.00002402
Iteration 62/1000 | Loss: 0.00003948
Iteration 63/1000 | Loss: 0.00002483
Iteration 64/1000 | Loss: 0.00002506
Iteration 65/1000 | Loss: 0.00002396
Iteration 66/1000 | Loss: 0.00002396
Iteration 67/1000 | Loss: 0.00002396
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002396
Iteration 72/1000 | Loss: 0.00002396
Iteration 73/1000 | Loss: 0.00002396
Iteration 74/1000 | Loss: 0.00002396
Iteration 75/1000 | Loss: 0.00002396
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002395
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002395
Iteration 82/1000 | Loss: 0.00002395
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002395
Iteration 85/1000 | Loss: 0.00002394
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002394
Iteration 88/1000 | Loss: 0.00002393
Iteration 89/1000 | Loss: 0.00002393
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002392
Iteration 95/1000 | Loss: 0.00002392
Iteration 96/1000 | Loss: 0.00002392
Iteration 97/1000 | Loss: 0.00002392
Iteration 98/1000 | Loss: 0.00002392
Iteration 99/1000 | Loss: 0.00002391
Iteration 100/1000 | Loss: 0.00002391
Iteration 101/1000 | Loss: 0.00002391
Iteration 102/1000 | Loss: 0.00002391
Iteration 103/1000 | Loss: 0.00002391
Iteration 104/1000 | Loss: 0.00002390
Iteration 105/1000 | Loss: 0.00002390
Iteration 106/1000 | Loss: 0.00002390
Iteration 107/1000 | Loss: 0.00002389
Iteration 108/1000 | Loss: 0.00002389
Iteration 109/1000 | Loss: 0.00002389
Iteration 110/1000 | Loss: 0.00002389
Iteration 111/1000 | Loss: 0.00002389
Iteration 112/1000 | Loss: 0.00002848
Iteration 113/1000 | Loss: 0.00003370
Iteration 114/1000 | Loss: 0.00002387
Iteration 115/1000 | Loss: 0.00002386
Iteration 116/1000 | Loss: 0.00002386
Iteration 117/1000 | Loss: 0.00002385
Iteration 118/1000 | Loss: 0.00002385
Iteration 119/1000 | Loss: 0.00002385
Iteration 120/1000 | Loss: 0.00002384
Iteration 121/1000 | Loss: 0.00002384
Iteration 122/1000 | Loss: 0.00002384
Iteration 123/1000 | Loss: 0.00002384
Iteration 124/1000 | Loss: 0.00002384
Iteration 125/1000 | Loss: 0.00002384
Iteration 126/1000 | Loss: 0.00002384
Iteration 127/1000 | Loss: 0.00002384
Iteration 128/1000 | Loss: 0.00002384
Iteration 129/1000 | Loss: 0.00002384
Iteration 130/1000 | Loss: 0.00002384
Iteration 131/1000 | Loss: 0.00002384
Iteration 132/1000 | Loss: 0.00002384
Iteration 133/1000 | Loss: 0.00002384
Iteration 134/1000 | Loss: 0.00002383
Iteration 135/1000 | Loss: 0.00002383
Iteration 136/1000 | Loss: 0.00002383
Iteration 137/1000 | Loss: 0.00002383
Iteration 138/1000 | Loss: 0.00002383
Iteration 139/1000 | Loss: 0.00002383
Iteration 140/1000 | Loss: 0.00002383
Iteration 141/1000 | Loss: 0.00002383
Iteration 142/1000 | Loss: 0.00002383
Iteration 143/1000 | Loss: 0.00002383
Iteration 144/1000 | Loss: 0.00002383
Iteration 145/1000 | Loss: 0.00002382
Iteration 146/1000 | Loss: 0.00002382
Iteration 147/1000 | Loss: 0.00002382
Iteration 148/1000 | Loss: 0.00002382
Iteration 149/1000 | Loss: 0.00002382
Iteration 150/1000 | Loss: 0.00002382
Iteration 151/1000 | Loss: 0.00002382
Iteration 152/1000 | Loss: 0.00002382
Iteration 153/1000 | Loss: 0.00002382
Iteration 154/1000 | Loss: 0.00002382
Iteration 155/1000 | Loss: 0.00002382
Iteration 156/1000 | Loss: 0.00002382
Iteration 157/1000 | Loss: 0.00002382
Iteration 158/1000 | Loss: 0.00002382
Iteration 159/1000 | Loss: 0.00002382
Iteration 160/1000 | Loss: 0.00002382
Iteration 161/1000 | Loss: 0.00002382
Iteration 162/1000 | Loss: 0.00002382
Iteration 163/1000 | Loss: 0.00002382
Iteration 164/1000 | Loss: 0.00002382
Iteration 165/1000 | Loss: 0.00002382
Iteration 166/1000 | Loss: 0.00002382
Iteration 167/1000 | Loss: 0.00002382
Iteration 168/1000 | Loss: 0.00002382
Iteration 169/1000 | Loss: 0.00002382
Iteration 170/1000 | Loss: 0.00002382
Iteration 171/1000 | Loss: 0.00002382
Iteration 172/1000 | Loss: 0.00002382
Iteration 173/1000 | Loss: 0.00002382
Iteration 174/1000 | Loss: 0.00002382
Iteration 175/1000 | Loss: 0.00002382
Iteration 176/1000 | Loss: 0.00002382
Iteration 177/1000 | Loss: 0.00002382
Iteration 178/1000 | Loss: 0.00002382
Iteration 179/1000 | Loss: 0.00002382
Iteration 180/1000 | Loss: 0.00002382
Iteration 181/1000 | Loss: 0.00002382
Iteration 182/1000 | Loss: 0.00002382
Iteration 183/1000 | Loss: 0.00002382
Iteration 184/1000 | Loss: 0.00002382
Iteration 185/1000 | Loss: 0.00002382
Iteration 186/1000 | Loss: 0.00002382
Iteration 187/1000 | Loss: 0.00002382
Iteration 188/1000 | Loss: 0.00002382
Iteration 189/1000 | Loss: 0.00002382
Iteration 190/1000 | Loss: 0.00002382
Iteration 191/1000 | Loss: 0.00002382
Iteration 192/1000 | Loss: 0.00002382
Iteration 193/1000 | Loss: 0.00002382
Iteration 194/1000 | Loss: 0.00002382
Iteration 195/1000 | Loss: 0.00002382
Iteration 196/1000 | Loss: 0.00002382
Iteration 197/1000 | Loss: 0.00002382
Iteration 198/1000 | Loss: 0.00002382
Iteration 199/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.3820490241632797e-05, 2.3820490241632797e-05, 2.3820490241632797e-05, 2.3820490241632797e-05, 2.3820490241632797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3820490241632797e-05

Optimization complete. Final v2v error: 4.032670497894287 mm

Highest mean error: 5.369166851043701 mm for frame 40

Lowest mean error: 3.4665348529815674 mm for frame 193

Saving results

Total time: 127.96069121360779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405522
Iteration 2/25 | Loss: 0.00087062
Iteration 3/25 | Loss: 0.00068760
Iteration 4/25 | Loss: 0.00065214
Iteration 5/25 | Loss: 0.00064116
Iteration 6/25 | Loss: 0.00063816
Iteration 7/25 | Loss: 0.00063736
Iteration 8/25 | Loss: 0.00063713
Iteration 9/25 | Loss: 0.00063709
Iteration 10/25 | Loss: 0.00063709
Iteration 11/25 | Loss: 0.00063709
Iteration 12/25 | Loss: 0.00063709
Iteration 13/25 | Loss: 0.00063709
Iteration 14/25 | Loss: 0.00063709
Iteration 15/25 | Loss: 0.00063709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006370889605022967, 0.0006370889605022967, 0.0006370889605022967, 0.0006370889605022967, 0.0006370889605022967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006370889605022967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50774395
Iteration 2/25 | Loss: 0.00032316
Iteration 3/25 | Loss: 0.00032315
Iteration 4/25 | Loss: 0.00032315
Iteration 5/25 | Loss: 0.00032315
Iteration 6/25 | Loss: 0.00032315
Iteration 7/25 | Loss: 0.00032315
Iteration 8/25 | Loss: 0.00032315
Iteration 9/25 | Loss: 0.00032315
Iteration 10/25 | Loss: 0.00032315
Iteration 11/25 | Loss: 0.00032315
Iteration 12/25 | Loss: 0.00032315
Iteration 13/25 | Loss: 0.00032315
Iteration 14/25 | Loss: 0.00032315
Iteration 15/25 | Loss: 0.00032315
Iteration 16/25 | Loss: 0.00032315
Iteration 17/25 | Loss: 0.00032315
Iteration 18/25 | Loss: 0.00032315
Iteration 19/25 | Loss: 0.00032315
Iteration 20/25 | Loss: 0.00032315
Iteration 21/25 | Loss: 0.00032315
Iteration 22/25 | Loss: 0.00032315
Iteration 23/25 | Loss: 0.00032315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000323145417496562, 0.000323145417496562, 0.000323145417496562, 0.000323145417496562, 0.000323145417496562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000323145417496562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032315
Iteration 2/1000 | Loss: 0.00003531
Iteration 3/1000 | Loss: 0.00002326
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001885
Iteration 6/1000 | Loss: 0.00001809
Iteration 7/1000 | Loss: 0.00001755
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001657
Iteration 13/1000 | Loss: 0.00001654
Iteration 14/1000 | Loss: 0.00001653
Iteration 15/1000 | Loss: 0.00001653
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001649
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001646
Iteration 23/1000 | Loss: 0.00001643
Iteration 24/1000 | Loss: 0.00001643
Iteration 25/1000 | Loss: 0.00001642
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001641
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001633
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001625
Iteration 54/1000 | Loss: 0.00001625
Iteration 55/1000 | Loss: 0.00001625
Iteration 56/1000 | Loss: 0.00001625
Iteration 57/1000 | Loss: 0.00001625
Iteration 58/1000 | Loss: 0.00001625
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001625
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001616
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001615
Iteration 105/1000 | Loss: 0.00001615
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001614
Iteration 110/1000 | Loss: 0.00001614
Iteration 111/1000 | Loss: 0.00001614
Iteration 112/1000 | Loss: 0.00001614
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001614
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001613
Iteration 119/1000 | Loss: 0.00001613
Iteration 120/1000 | Loss: 0.00001613
Iteration 121/1000 | Loss: 0.00001613
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001612
Iteration 124/1000 | Loss: 0.00001612
Iteration 125/1000 | Loss: 0.00001612
Iteration 126/1000 | Loss: 0.00001611
Iteration 127/1000 | Loss: 0.00001611
Iteration 128/1000 | Loss: 0.00001611
Iteration 129/1000 | Loss: 0.00001611
Iteration 130/1000 | Loss: 0.00001611
Iteration 131/1000 | Loss: 0.00001611
Iteration 132/1000 | Loss: 0.00001611
Iteration 133/1000 | Loss: 0.00001611
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001610
Iteration 140/1000 | Loss: 0.00001610
Iteration 141/1000 | Loss: 0.00001609
Iteration 142/1000 | Loss: 0.00001609
Iteration 143/1000 | Loss: 0.00001609
Iteration 144/1000 | Loss: 0.00001609
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001608
Iteration 150/1000 | Loss: 0.00001608
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001607
Iteration 154/1000 | Loss: 0.00001607
Iteration 155/1000 | Loss: 0.00001607
Iteration 156/1000 | Loss: 0.00001607
Iteration 157/1000 | Loss: 0.00001607
Iteration 158/1000 | Loss: 0.00001607
Iteration 159/1000 | Loss: 0.00001606
Iteration 160/1000 | Loss: 0.00001606
Iteration 161/1000 | Loss: 0.00001606
Iteration 162/1000 | Loss: 0.00001606
Iteration 163/1000 | Loss: 0.00001606
Iteration 164/1000 | Loss: 0.00001606
Iteration 165/1000 | Loss: 0.00001606
Iteration 166/1000 | Loss: 0.00001605
Iteration 167/1000 | Loss: 0.00001605
Iteration 168/1000 | Loss: 0.00001605
Iteration 169/1000 | Loss: 0.00001605
Iteration 170/1000 | Loss: 0.00001605
Iteration 171/1000 | Loss: 0.00001605
Iteration 172/1000 | Loss: 0.00001605
Iteration 173/1000 | Loss: 0.00001605
Iteration 174/1000 | Loss: 0.00001605
Iteration 175/1000 | Loss: 0.00001605
Iteration 176/1000 | Loss: 0.00001605
Iteration 177/1000 | Loss: 0.00001605
Iteration 178/1000 | Loss: 0.00001605
Iteration 179/1000 | Loss: 0.00001604
Iteration 180/1000 | Loss: 0.00001604
Iteration 181/1000 | Loss: 0.00001604
Iteration 182/1000 | Loss: 0.00001604
Iteration 183/1000 | Loss: 0.00001604
Iteration 184/1000 | Loss: 0.00001604
Iteration 185/1000 | Loss: 0.00001604
Iteration 186/1000 | Loss: 0.00001604
Iteration 187/1000 | Loss: 0.00001604
Iteration 188/1000 | Loss: 0.00001603
Iteration 189/1000 | Loss: 0.00001603
Iteration 190/1000 | Loss: 0.00001603
Iteration 191/1000 | Loss: 0.00001603
Iteration 192/1000 | Loss: 0.00001603
Iteration 193/1000 | Loss: 0.00001603
Iteration 194/1000 | Loss: 0.00001603
Iteration 195/1000 | Loss: 0.00001603
Iteration 196/1000 | Loss: 0.00001603
Iteration 197/1000 | Loss: 0.00001603
Iteration 198/1000 | Loss: 0.00001603
Iteration 199/1000 | Loss: 0.00001603
Iteration 200/1000 | Loss: 0.00001603
Iteration 201/1000 | Loss: 0.00001603
Iteration 202/1000 | Loss: 0.00001603
Iteration 203/1000 | Loss: 0.00001603
Iteration 204/1000 | Loss: 0.00001603
Iteration 205/1000 | Loss: 0.00001602
Iteration 206/1000 | Loss: 0.00001602
Iteration 207/1000 | Loss: 0.00001602
Iteration 208/1000 | Loss: 0.00001602
Iteration 209/1000 | Loss: 0.00001602
Iteration 210/1000 | Loss: 0.00001602
Iteration 211/1000 | Loss: 0.00001602
Iteration 212/1000 | Loss: 0.00001602
Iteration 213/1000 | Loss: 0.00001602
Iteration 214/1000 | Loss: 0.00001602
Iteration 215/1000 | Loss: 0.00001602
Iteration 216/1000 | Loss: 0.00001602
Iteration 217/1000 | Loss: 0.00001602
Iteration 218/1000 | Loss: 0.00001602
Iteration 219/1000 | Loss: 0.00001602
Iteration 220/1000 | Loss: 0.00001602
Iteration 221/1000 | Loss: 0.00001602
Iteration 222/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.602225711394567e-05, 1.602225711394567e-05, 1.602225711394567e-05, 1.602225711394567e-05, 1.602225711394567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.602225711394567e-05

Optimization complete. Final v2v error: 3.3786227703094482 mm

Highest mean error: 4.331379413604736 mm for frame 46

Lowest mean error: 2.961793899536133 mm for frame 92

Saving results

Total time: 43.19508600234985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052673
Iteration 2/25 | Loss: 0.01052673
Iteration 3/25 | Loss: 0.01052673
Iteration 4/25 | Loss: 0.01052673
Iteration 5/25 | Loss: 0.01052672
Iteration 6/25 | Loss: 0.00486994
Iteration 7/25 | Loss: 0.00327598
Iteration 8/25 | Loss: 0.00217427
Iteration 9/25 | Loss: 0.00188006
Iteration 10/25 | Loss: 0.00180350
Iteration 11/25 | Loss: 0.00174589
Iteration 12/25 | Loss: 0.00176828
Iteration 13/25 | Loss: 0.00164424
Iteration 14/25 | Loss: 0.00158686
Iteration 15/25 | Loss: 0.00155654
Iteration 16/25 | Loss: 0.00149714
Iteration 17/25 | Loss: 0.00142933
Iteration 18/25 | Loss: 0.00133372
Iteration 19/25 | Loss: 0.00129885
Iteration 20/25 | Loss: 0.00125234
Iteration 21/25 | Loss: 0.00122827
Iteration 22/25 | Loss: 0.00120488
Iteration 23/25 | Loss: 0.00119228
Iteration 24/25 | Loss: 0.00117455
Iteration 25/25 | Loss: 0.00115990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41864276
Iteration 2/25 | Loss: 0.00573203
Iteration 3/25 | Loss: 0.00448876
Iteration 4/25 | Loss: 0.00448876
Iteration 5/25 | Loss: 0.00448875
Iteration 6/25 | Loss: 0.00448875
Iteration 7/25 | Loss: 0.00448875
Iteration 8/25 | Loss: 0.00448875
Iteration 9/25 | Loss: 0.00448875
Iteration 10/25 | Loss: 0.00448875
Iteration 11/25 | Loss: 0.00448875
Iteration 12/25 | Loss: 0.00448875
Iteration 13/25 | Loss: 0.00448875
Iteration 14/25 | Loss: 0.00448875
Iteration 15/25 | Loss: 0.00448875
Iteration 16/25 | Loss: 0.00448875
Iteration 17/25 | Loss: 0.00448875
Iteration 18/25 | Loss: 0.00448875
Iteration 19/25 | Loss: 0.00448875
Iteration 20/25 | Loss: 0.00448875
Iteration 21/25 | Loss: 0.00448875
Iteration 22/25 | Loss: 0.00448875
Iteration 23/25 | Loss: 0.00448875
Iteration 24/25 | Loss: 0.00448875
Iteration 25/25 | Loss: 0.00448875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004488752223551273, 0.004488752223551273, 0.004488752223551273, 0.004488752223551273, 0.004488752223551273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004488752223551273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00448875
Iteration 2/1000 | Loss: 0.00525249
Iteration 3/1000 | Loss: 0.00930521
Iteration 4/1000 | Loss: 0.00505370
Iteration 5/1000 | Loss: 0.00347513
Iteration 6/1000 | Loss: 0.00492115
Iteration 7/1000 | Loss: 0.00306850
Iteration 8/1000 | Loss: 0.00194830
Iteration 9/1000 | Loss: 0.00079333
Iteration 10/1000 | Loss: 0.00143638
Iteration 11/1000 | Loss: 0.00328332
Iteration 12/1000 | Loss: 0.00195762
Iteration 13/1000 | Loss: 0.00150387
Iteration 14/1000 | Loss: 0.00160425
Iteration 15/1000 | Loss: 0.00131968
Iteration 16/1000 | Loss: 0.00098457
Iteration 17/1000 | Loss: 0.00096274
Iteration 18/1000 | Loss: 0.00052132
Iteration 19/1000 | Loss: 0.00039226
Iteration 20/1000 | Loss: 0.00108763
Iteration 21/1000 | Loss: 0.00040648
Iteration 22/1000 | Loss: 0.00015878
Iteration 23/1000 | Loss: 0.00045587
Iteration 24/1000 | Loss: 0.00125941
Iteration 25/1000 | Loss: 0.00214521
Iteration 26/1000 | Loss: 0.00034719
Iteration 27/1000 | Loss: 0.00025512
Iteration 28/1000 | Loss: 0.00025102
Iteration 29/1000 | Loss: 0.00014478
Iteration 30/1000 | Loss: 0.00137605
Iteration 31/1000 | Loss: 0.00279768
Iteration 32/1000 | Loss: 0.00113590
Iteration 33/1000 | Loss: 0.00077180
Iteration 34/1000 | Loss: 0.00084918
Iteration 35/1000 | Loss: 0.00052249
Iteration 36/1000 | Loss: 0.00056536
Iteration 37/1000 | Loss: 0.00020425
Iteration 38/1000 | Loss: 0.00025959
Iteration 39/1000 | Loss: 0.00044288
Iteration 40/1000 | Loss: 0.00012602
Iteration 41/1000 | Loss: 0.00029016
Iteration 42/1000 | Loss: 0.00020865
Iteration 43/1000 | Loss: 0.00032922
Iteration 44/1000 | Loss: 0.00017850
Iteration 45/1000 | Loss: 0.00011130
Iteration 46/1000 | Loss: 0.00009568
Iteration 47/1000 | Loss: 0.00011780
Iteration 48/1000 | Loss: 0.00032393
Iteration 49/1000 | Loss: 0.00027213
Iteration 50/1000 | Loss: 0.00021295
Iteration 51/1000 | Loss: 0.00016884
Iteration 52/1000 | Loss: 0.00019993
Iteration 53/1000 | Loss: 0.00006911
Iteration 54/1000 | Loss: 0.00006019
Iteration 55/1000 | Loss: 0.00021663
Iteration 56/1000 | Loss: 0.00003985
Iteration 57/1000 | Loss: 0.00013004
Iteration 58/1000 | Loss: 0.00035651
Iteration 59/1000 | Loss: 0.00039863
Iteration 60/1000 | Loss: 0.00018107
Iteration 61/1000 | Loss: 0.00029018
Iteration 62/1000 | Loss: 0.00009795
Iteration 63/1000 | Loss: 0.00012952
Iteration 64/1000 | Loss: 0.00004182
Iteration 65/1000 | Loss: 0.00003742
Iteration 66/1000 | Loss: 0.00008854
Iteration 67/1000 | Loss: 0.00016418
Iteration 68/1000 | Loss: 0.00006365
Iteration 69/1000 | Loss: 0.00007266
Iteration 70/1000 | Loss: 0.00020769
Iteration 71/1000 | Loss: 0.00016422
Iteration 72/1000 | Loss: 0.00012609
Iteration 73/1000 | Loss: 0.00018141
Iteration 74/1000 | Loss: 0.00013853
Iteration 75/1000 | Loss: 0.00018531
Iteration 76/1000 | Loss: 0.00014156
Iteration 77/1000 | Loss: 0.00010782
Iteration 78/1000 | Loss: 0.00015922
Iteration 79/1000 | Loss: 0.00032036
Iteration 80/1000 | Loss: 0.00005686
Iteration 81/1000 | Loss: 0.00006480
Iteration 82/1000 | Loss: 0.00003659
Iteration 83/1000 | Loss: 0.00007116
Iteration 84/1000 | Loss: 0.00003075
Iteration 85/1000 | Loss: 0.00015505
Iteration 86/1000 | Loss: 0.00005734
Iteration 87/1000 | Loss: 0.00036067
Iteration 88/1000 | Loss: 0.00042781
Iteration 89/1000 | Loss: 0.00012726
Iteration 90/1000 | Loss: 0.00031136
Iteration 91/1000 | Loss: 0.00026435
Iteration 92/1000 | Loss: 0.00006384
Iteration 93/1000 | Loss: 0.00023435
Iteration 94/1000 | Loss: 0.00027654
Iteration 95/1000 | Loss: 0.00023681
Iteration 96/1000 | Loss: 0.00021827
Iteration 97/1000 | Loss: 0.00005612
Iteration 98/1000 | Loss: 0.00041206
Iteration 99/1000 | Loss: 0.00012573
Iteration 100/1000 | Loss: 0.00023799
Iteration 101/1000 | Loss: 0.00011821
Iteration 102/1000 | Loss: 0.00006741
Iteration 103/1000 | Loss: 0.00006966
Iteration 104/1000 | Loss: 0.00011975
Iteration 105/1000 | Loss: 0.00003835
Iteration 106/1000 | Loss: 0.00006562
Iteration 107/1000 | Loss: 0.00020309
Iteration 108/1000 | Loss: 0.00004132
Iteration 109/1000 | Loss: 0.00008118
Iteration 110/1000 | Loss: 0.00003114
Iteration 111/1000 | Loss: 0.00003569
Iteration 112/1000 | Loss: 0.00006770
Iteration 113/1000 | Loss: 0.00003816
Iteration 114/1000 | Loss: 0.00004352
Iteration 115/1000 | Loss: 0.00002920
Iteration 116/1000 | Loss: 0.00006570
Iteration 117/1000 | Loss: 0.00011747
Iteration 118/1000 | Loss: 0.00075093
Iteration 119/1000 | Loss: 0.00007135
Iteration 120/1000 | Loss: 0.00012132
Iteration 121/1000 | Loss: 0.00004616
Iteration 122/1000 | Loss: 0.00008165
Iteration 123/1000 | Loss: 0.00025717
Iteration 124/1000 | Loss: 0.00005584
Iteration 125/1000 | Loss: 0.00004953
Iteration 126/1000 | Loss: 0.00005275
Iteration 127/1000 | Loss: 0.00002993
Iteration 128/1000 | Loss: 0.00002829
Iteration 129/1000 | Loss: 0.00005042
Iteration 130/1000 | Loss: 0.00003277
Iteration 131/1000 | Loss: 0.00005780
Iteration 132/1000 | Loss: 0.00002590
Iteration 133/1000 | Loss: 0.00004136
Iteration 134/1000 | Loss: 0.00015851
Iteration 135/1000 | Loss: 0.00027758
Iteration 136/1000 | Loss: 0.00007932
Iteration 137/1000 | Loss: 0.00005680
Iteration 138/1000 | Loss: 0.00002968
Iteration 139/1000 | Loss: 0.00002681
Iteration 140/1000 | Loss: 0.00008379
Iteration 141/1000 | Loss: 0.00006092
Iteration 142/1000 | Loss: 0.00003517
Iteration 143/1000 | Loss: 0.00003744
Iteration 144/1000 | Loss: 0.00003797
Iteration 145/1000 | Loss: 0.00002251
Iteration 146/1000 | Loss: 0.00004485
Iteration 147/1000 | Loss: 0.00012151
Iteration 148/1000 | Loss: 0.00007785
Iteration 149/1000 | Loss: 0.00002678
Iteration 150/1000 | Loss: 0.00002640
Iteration 151/1000 | Loss: 0.00002207
Iteration 152/1000 | Loss: 0.00002207
Iteration 153/1000 | Loss: 0.00002207
Iteration 154/1000 | Loss: 0.00002207
Iteration 155/1000 | Loss: 0.00002207
Iteration 156/1000 | Loss: 0.00002207
Iteration 157/1000 | Loss: 0.00002207
Iteration 158/1000 | Loss: 0.00002207
Iteration 159/1000 | Loss: 0.00002207
Iteration 160/1000 | Loss: 0.00002207
Iteration 161/1000 | Loss: 0.00002207
Iteration 162/1000 | Loss: 0.00003218
Iteration 163/1000 | Loss: 0.00002204
Iteration 164/1000 | Loss: 0.00002202
Iteration 165/1000 | Loss: 0.00002201
Iteration 166/1000 | Loss: 0.00002201
Iteration 167/1000 | Loss: 0.00002201
Iteration 168/1000 | Loss: 0.00002200
Iteration 169/1000 | Loss: 0.00002200
Iteration 170/1000 | Loss: 0.00002200
Iteration 171/1000 | Loss: 0.00002200
Iteration 172/1000 | Loss: 0.00002200
Iteration 173/1000 | Loss: 0.00002200
Iteration 174/1000 | Loss: 0.00002200
Iteration 175/1000 | Loss: 0.00002200
Iteration 176/1000 | Loss: 0.00002200
Iteration 177/1000 | Loss: 0.00002199
Iteration 178/1000 | Loss: 0.00002199
Iteration 179/1000 | Loss: 0.00002199
Iteration 180/1000 | Loss: 0.00002198
Iteration 181/1000 | Loss: 0.00002198
Iteration 182/1000 | Loss: 0.00002198
Iteration 183/1000 | Loss: 0.00002197
Iteration 184/1000 | Loss: 0.00002197
Iteration 185/1000 | Loss: 0.00002197
Iteration 186/1000 | Loss: 0.00002197
Iteration 187/1000 | Loss: 0.00002197
Iteration 188/1000 | Loss: 0.00002197
Iteration 189/1000 | Loss: 0.00002197
Iteration 190/1000 | Loss: 0.00002197
Iteration 191/1000 | Loss: 0.00002197
Iteration 192/1000 | Loss: 0.00002196
Iteration 193/1000 | Loss: 0.00002196
Iteration 194/1000 | Loss: 0.00002196
Iteration 195/1000 | Loss: 0.00002196
Iteration 196/1000 | Loss: 0.00003490
Iteration 197/1000 | Loss: 0.00002195
Iteration 198/1000 | Loss: 0.00002193
Iteration 199/1000 | Loss: 0.00002193
Iteration 200/1000 | Loss: 0.00002193
Iteration 201/1000 | Loss: 0.00002193
Iteration 202/1000 | Loss: 0.00002193
Iteration 203/1000 | Loss: 0.00002193
Iteration 204/1000 | Loss: 0.00002193
Iteration 205/1000 | Loss: 0.00002193
Iteration 206/1000 | Loss: 0.00002193
Iteration 207/1000 | Loss: 0.00002193
Iteration 208/1000 | Loss: 0.00002192
Iteration 209/1000 | Loss: 0.00002192
Iteration 210/1000 | Loss: 0.00002192
Iteration 211/1000 | Loss: 0.00002192
Iteration 212/1000 | Loss: 0.00002192
Iteration 213/1000 | Loss: 0.00002192
Iteration 214/1000 | Loss: 0.00002192
Iteration 215/1000 | Loss: 0.00002191
Iteration 216/1000 | Loss: 0.00002191
Iteration 217/1000 | Loss: 0.00002191
Iteration 218/1000 | Loss: 0.00002191
Iteration 219/1000 | Loss: 0.00002191
Iteration 220/1000 | Loss: 0.00002191
Iteration 221/1000 | Loss: 0.00002191
Iteration 222/1000 | Loss: 0.00002191
Iteration 223/1000 | Loss: 0.00002191
Iteration 224/1000 | Loss: 0.00002191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.1913700038567185e-05, 2.1913700038567185e-05, 2.1913700038567185e-05, 2.1913700038567185e-05, 2.1913700038567185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1913700038567185e-05

Optimization complete. Final v2v error: 3.895613193511963 mm

Highest mean error: 6.879239559173584 mm for frame 228

Lowest mean error: 3.549753427505493 mm for frame 29

Saving results

Total time: 295.95400047302246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821735
Iteration 2/25 | Loss: 0.00163383
Iteration 3/25 | Loss: 0.00128468
Iteration 4/25 | Loss: 0.00095325
Iteration 5/25 | Loss: 0.00086220
Iteration 6/25 | Loss: 0.00083289
Iteration 7/25 | Loss: 0.00081770
Iteration 8/25 | Loss: 0.00082269
Iteration 9/25 | Loss: 0.00080749
Iteration 10/25 | Loss: 0.00080770
Iteration 11/25 | Loss: 0.00080858
Iteration 12/25 | Loss: 0.00079993
Iteration 13/25 | Loss: 0.00079900
Iteration 14/25 | Loss: 0.00079539
Iteration 15/25 | Loss: 0.00079217
Iteration 16/25 | Loss: 0.00078664
Iteration 17/25 | Loss: 0.00078489
Iteration 18/25 | Loss: 0.00078408
Iteration 19/25 | Loss: 0.00078380
Iteration 20/25 | Loss: 0.00078363
Iteration 21/25 | Loss: 0.00078359
Iteration 22/25 | Loss: 0.00078359
Iteration 23/25 | Loss: 0.00078359
Iteration 24/25 | Loss: 0.00078359
Iteration 25/25 | Loss: 0.00078359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.08037949
Iteration 2/25 | Loss: 0.00038043
Iteration 3/25 | Loss: 0.00038039
Iteration 4/25 | Loss: 0.00038038
Iteration 5/25 | Loss: 0.00038038
Iteration 6/25 | Loss: 0.00038038
Iteration 7/25 | Loss: 0.00038038
Iteration 8/25 | Loss: 0.00038038
Iteration 9/25 | Loss: 0.00038038
Iteration 10/25 | Loss: 0.00038038
Iteration 11/25 | Loss: 0.00038038
Iteration 12/25 | Loss: 0.00038038
Iteration 13/25 | Loss: 0.00038038
Iteration 14/25 | Loss: 0.00038038
Iteration 15/25 | Loss: 0.00038038
Iteration 16/25 | Loss: 0.00038038
Iteration 17/25 | Loss: 0.00038038
Iteration 18/25 | Loss: 0.00038038
Iteration 19/25 | Loss: 0.00038038
Iteration 20/25 | Loss: 0.00038038
Iteration 21/25 | Loss: 0.00038038
Iteration 22/25 | Loss: 0.00038038
Iteration 23/25 | Loss: 0.00038038
Iteration 24/25 | Loss: 0.00038038
Iteration 25/25 | Loss: 0.00038038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038038
Iteration 2/1000 | Loss: 0.00005970
Iteration 3/1000 | Loss: 0.00003913
Iteration 4/1000 | Loss: 0.00017823
Iteration 5/1000 | Loss: 0.00012761
Iteration 6/1000 | Loss: 0.00007658
Iteration 7/1000 | Loss: 0.00011871
Iteration 8/1000 | Loss: 0.00004514
Iteration 9/1000 | Loss: 0.00003455
Iteration 10/1000 | Loss: 0.00003219
Iteration 11/1000 | Loss: 0.00020821
Iteration 12/1000 | Loss: 0.00003448
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002792
Iteration 15/1000 | Loss: 0.00016240
Iteration 16/1000 | Loss: 0.00008307
Iteration 17/1000 | Loss: 0.00016021
Iteration 18/1000 | Loss: 0.00005109
Iteration 19/1000 | Loss: 0.00003557
Iteration 20/1000 | Loss: 0.00002876
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002374
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002928
Iteration 25/1000 | Loss: 0.00002214
Iteration 26/1000 | Loss: 0.00002121
Iteration 27/1000 | Loss: 0.00002063
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002021
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002011
Iteration 32/1000 | Loss: 0.00002011
Iteration 33/1000 | Loss: 0.00002011
Iteration 34/1000 | Loss: 0.00002011
Iteration 35/1000 | Loss: 0.00002010
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002009
Iteration 39/1000 | Loss: 0.00002008
Iteration 40/1000 | Loss: 0.00001997
Iteration 41/1000 | Loss: 0.00001994
Iteration 42/1000 | Loss: 0.00001991
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001985
Iteration 48/1000 | Loss: 0.00001984
Iteration 49/1000 | Loss: 0.00001983
Iteration 50/1000 | Loss: 0.00001982
Iteration 51/1000 | Loss: 0.00001982
Iteration 52/1000 | Loss: 0.00001981
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00001980
Iteration 55/1000 | Loss: 0.00001980
Iteration 56/1000 | Loss: 0.00001980
Iteration 57/1000 | Loss: 0.00001979
Iteration 58/1000 | Loss: 0.00001979
Iteration 59/1000 | Loss: 0.00001979
Iteration 60/1000 | Loss: 0.00001978
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001977
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001976
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001976
Iteration 71/1000 | Loss: 0.00001976
Iteration 72/1000 | Loss: 0.00001976
Iteration 73/1000 | Loss: 0.00001976
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001973
Iteration 79/1000 | Loss: 0.00001973
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001973
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001972
Iteration 99/1000 | Loss: 0.00001972
Iteration 100/1000 | Loss: 0.00001972
Iteration 101/1000 | Loss: 0.00001972
Iteration 102/1000 | Loss: 0.00001972
Iteration 103/1000 | Loss: 0.00001972
Iteration 104/1000 | Loss: 0.00001972
Iteration 105/1000 | Loss: 0.00001972
Iteration 106/1000 | Loss: 0.00001972
Iteration 107/1000 | Loss: 0.00001972
Iteration 108/1000 | Loss: 0.00001972
Iteration 109/1000 | Loss: 0.00001972
Iteration 110/1000 | Loss: 0.00001972
Iteration 111/1000 | Loss: 0.00001972
Iteration 112/1000 | Loss: 0.00001972
Iteration 113/1000 | Loss: 0.00001972
Iteration 114/1000 | Loss: 0.00001972
Iteration 115/1000 | Loss: 0.00001972
Iteration 116/1000 | Loss: 0.00001972
Iteration 117/1000 | Loss: 0.00001972
Iteration 118/1000 | Loss: 0.00001972
Iteration 119/1000 | Loss: 0.00001972
Iteration 120/1000 | Loss: 0.00001972
Iteration 121/1000 | Loss: 0.00001972
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001972
Iteration 131/1000 | Loss: 0.00001972
Iteration 132/1000 | Loss: 0.00001972
Iteration 133/1000 | Loss: 0.00001972
Iteration 134/1000 | Loss: 0.00001972
Iteration 135/1000 | Loss: 0.00001972
Iteration 136/1000 | Loss: 0.00001972
Iteration 137/1000 | Loss: 0.00001972
Iteration 138/1000 | Loss: 0.00001972
Iteration 139/1000 | Loss: 0.00001972
Iteration 140/1000 | Loss: 0.00001972
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.9724549929378554e-05, 1.9724549929378554e-05, 1.9724549929378554e-05, 1.9724549929378554e-05, 1.9724549929378554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9724549929378554e-05

Optimization complete. Final v2v error: 3.675687551498413 mm

Highest mean error: 4.73551082611084 mm for frame 225

Lowest mean error: 3.477247714996338 mm for frame 144

Saving results

Total time: 97.1816828250885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075232
Iteration 2/25 | Loss: 0.00221414
Iteration 3/25 | Loss: 0.00140633
Iteration 4/25 | Loss: 0.00140616
Iteration 5/25 | Loss: 0.00104807
Iteration 6/25 | Loss: 0.00097448
Iteration 7/25 | Loss: 0.00099775
Iteration 8/25 | Loss: 0.00092224
Iteration 9/25 | Loss: 0.00089958
Iteration 10/25 | Loss: 0.00082756
Iteration 11/25 | Loss: 0.00080171
Iteration 12/25 | Loss: 0.00078923
Iteration 13/25 | Loss: 0.00077615
Iteration 14/25 | Loss: 0.00077857
Iteration 15/25 | Loss: 0.00075454
Iteration 16/25 | Loss: 0.00076217
Iteration 17/25 | Loss: 0.00075698
Iteration 18/25 | Loss: 0.00075003
Iteration 19/25 | Loss: 0.00074604
Iteration 20/25 | Loss: 0.00074210
Iteration 21/25 | Loss: 0.00074999
Iteration 22/25 | Loss: 0.00074550
Iteration 23/25 | Loss: 0.00073898
Iteration 24/25 | Loss: 0.00072879
Iteration 25/25 | Loss: 0.00074359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65213859
Iteration 2/25 | Loss: 0.00124992
Iteration 3/25 | Loss: 0.00105569
Iteration 4/25 | Loss: 0.00105568
Iteration 5/25 | Loss: 0.00105568
Iteration 6/25 | Loss: 0.00105568
Iteration 7/25 | Loss: 0.00105568
Iteration 8/25 | Loss: 0.00105568
Iteration 9/25 | Loss: 0.00105567
Iteration 10/25 | Loss: 0.00105567
Iteration 11/25 | Loss: 0.00105567
Iteration 12/25 | Loss: 0.00105567
Iteration 13/25 | Loss: 0.00105567
Iteration 14/25 | Loss: 0.00105567
Iteration 15/25 | Loss: 0.00105567
Iteration 16/25 | Loss: 0.00105567
Iteration 17/25 | Loss: 0.00105567
Iteration 18/25 | Loss: 0.00105567
Iteration 19/25 | Loss: 0.00105567
Iteration 20/25 | Loss: 0.00105567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010556741617619991, 0.0010556741617619991, 0.0010556741617619991, 0.0010556741617619991, 0.0010556741617619991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010556741617619991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105567
Iteration 2/1000 | Loss: 0.00155809
Iteration 3/1000 | Loss: 0.00084984
Iteration 4/1000 | Loss: 0.00120020
Iteration 5/1000 | Loss: 0.00106980
Iteration 6/1000 | Loss: 0.00127763
Iteration 7/1000 | Loss: 0.00126530
Iteration 8/1000 | Loss: 0.00095510
Iteration 9/1000 | Loss: 0.00099572
Iteration 10/1000 | Loss: 0.00089887
Iteration 11/1000 | Loss: 0.00120586
Iteration 12/1000 | Loss: 0.00090004
Iteration 13/1000 | Loss: 0.00084347
Iteration 14/1000 | Loss: 0.00113443
Iteration 15/1000 | Loss: 0.00070071
Iteration 16/1000 | Loss: 0.00053203
Iteration 17/1000 | Loss: 0.00051487
Iteration 18/1000 | Loss: 0.00070912
Iteration 19/1000 | Loss: 0.00121349
Iteration 20/1000 | Loss: 0.00115170
Iteration 21/1000 | Loss: 0.00118505
Iteration 22/1000 | Loss: 0.00123581
Iteration 23/1000 | Loss: 0.00090748
Iteration 24/1000 | Loss: 0.00117200
Iteration 25/1000 | Loss: 0.00126561
Iteration 26/1000 | Loss: 0.00065442
Iteration 27/1000 | Loss: 0.00124920
Iteration 28/1000 | Loss: 0.00168849
Iteration 29/1000 | Loss: 0.00170362
Iteration 30/1000 | Loss: 0.00126144
Iteration 31/1000 | Loss: 0.00106860
Iteration 32/1000 | Loss: 0.00126295
Iteration 33/1000 | Loss: 0.00161533
Iteration 34/1000 | Loss: 0.00078427
Iteration 35/1000 | Loss: 0.00095887
Iteration 36/1000 | Loss: 0.00074738
Iteration 37/1000 | Loss: 0.00121888
Iteration 38/1000 | Loss: 0.00089131
Iteration 39/1000 | Loss: 0.00085339
Iteration 40/1000 | Loss: 0.00065730
Iteration 41/1000 | Loss: 0.00096760
Iteration 42/1000 | Loss: 0.00115051
Iteration 43/1000 | Loss: 0.00082072
Iteration 44/1000 | Loss: 0.00057704
Iteration 45/1000 | Loss: 0.00055073
Iteration 46/1000 | Loss: 0.00053854
Iteration 47/1000 | Loss: 0.00069653
Iteration 48/1000 | Loss: 0.00056272
Iteration 49/1000 | Loss: 0.00093906
Iteration 50/1000 | Loss: 0.00065417
Iteration 51/1000 | Loss: 0.00067293
Iteration 52/1000 | Loss: 0.00144500
Iteration 53/1000 | Loss: 0.00054263
Iteration 54/1000 | Loss: 0.00093430
Iteration 55/1000 | Loss: 0.00117974
Iteration 56/1000 | Loss: 0.00096811
Iteration 57/1000 | Loss: 0.00085298
Iteration 58/1000 | Loss: 0.00233734
Iteration 59/1000 | Loss: 0.00068543
Iteration 60/1000 | Loss: 0.00087053
Iteration 61/1000 | Loss: 0.00053058
Iteration 62/1000 | Loss: 0.00069046
Iteration 63/1000 | Loss: 0.00070679
Iteration 64/1000 | Loss: 0.00055333
Iteration 65/1000 | Loss: 0.00049833
Iteration 66/1000 | Loss: 0.00030584
Iteration 67/1000 | Loss: 0.00029512
Iteration 68/1000 | Loss: 0.00088472
Iteration 69/1000 | Loss: 0.00061389
Iteration 70/1000 | Loss: 0.00018408
Iteration 71/1000 | Loss: 0.00054092
Iteration 72/1000 | Loss: 0.00035295
Iteration 73/1000 | Loss: 0.00047895
Iteration 74/1000 | Loss: 0.00036454
Iteration 75/1000 | Loss: 0.00037744
Iteration 76/1000 | Loss: 0.00039380
Iteration 77/1000 | Loss: 0.00021119
Iteration 78/1000 | Loss: 0.00139510
Iteration 79/1000 | Loss: 0.00035665
Iteration 80/1000 | Loss: 0.00182005
Iteration 81/1000 | Loss: 0.00193707
Iteration 82/1000 | Loss: 0.00069345
Iteration 83/1000 | Loss: 0.00063471
Iteration 84/1000 | Loss: 0.00104825
Iteration 85/1000 | Loss: 0.00084476
Iteration 86/1000 | Loss: 0.00131934
Iteration 87/1000 | Loss: 0.00075545
Iteration 88/1000 | Loss: 0.00059713
Iteration 89/1000 | Loss: 0.00044925
Iteration 90/1000 | Loss: 0.00055096
Iteration 91/1000 | Loss: 0.00030921
Iteration 92/1000 | Loss: 0.00021787
Iteration 93/1000 | Loss: 0.00020209
Iteration 94/1000 | Loss: 0.00034714
Iteration 95/1000 | Loss: 0.00051999
Iteration 96/1000 | Loss: 0.00173776
Iteration 97/1000 | Loss: 0.00089153
Iteration 98/1000 | Loss: 0.00081678
Iteration 99/1000 | Loss: 0.00109219
Iteration 100/1000 | Loss: 0.00079870
Iteration 101/1000 | Loss: 0.00019757
Iteration 102/1000 | Loss: 0.00027561
Iteration 103/1000 | Loss: 0.00015481
Iteration 104/1000 | Loss: 0.00047795
Iteration 105/1000 | Loss: 0.00019730
Iteration 106/1000 | Loss: 0.00039553
Iteration 107/1000 | Loss: 0.00036439
Iteration 108/1000 | Loss: 0.00032240
Iteration 109/1000 | Loss: 0.00036160
Iteration 110/1000 | Loss: 0.00024838
Iteration 111/1000 | Loss: 0.00115661
Iteration 112/1000 | Loss: 0.00123446
Iteration 113/1000 | Loss: 0.00064119
Iteration 114/1000 | Loss: 0.00070186
Iteration 115/1000 | Loss: 0.00111959
Iteration 116/1000 | Loss: 0.00126605
Iteration 117/1000 | Loss: 0.00091085
Iteration 118/1000 | Loss: 0.00083029
Iteration 119/1000 | Loss: 0.00038762
Iteration 120/1000 | Loss: 0.00092499
Iteration 121/1000 | Loss: 0.00145419
Iteration 122/1000 | Loss: 0.00236878
Iteration 123/1000 | Loss: 0.00169399
Iteration 124/1000 | Loss: 0.00064540
Iteration 125/1000 | Loss: 0.00071720
Iteration 126/1000 | Loss: 0.00052473
Iteration 127/1000 | Loss: 0.00075094
Iteration 128/1000 | Loss: 0.00041373
Iteration 129/1000 | Loss: 0.00039501
Iteration 130/1000 | Loss: 0.00037241
Iteration 131/1000 | Loss: 0.00035251
Iteration 132/1000 | Loss: 0.00048122
Iteration 133/1000 | Loss: 0.00049364
Iteration 134/1000 | Loss: 0.00055731
Iteration 135/1000 | Loss: 0.00069293
Iteration 136/1000 | Loss: 0.00129331
Iteration 137/1000 | Loss: 0.00077980
Iteration 138/1000 | Loss: 0.00202691
Iteration 139/1000 | Loss: 0.00203104
Iteration 140/1000 | Loss: 0.00109370
Iteration 141/1000 | Loss: 0.00080276
Iteration 142/1000 | Loss: 0.00072875
Iteration 143/1000 | Loss: 0.00060465
Iteration 144/1000 | Loss: 0.00067031
Iteration 145/1000 | Loss: 0.00063066
Iteration 146/1000 | Loss: 0.00065755
Iteration 147/1000 | Loss: 0.00256183
Iteration 148/1000 | Loss: 0.00223773
Iteration 149/1000 | Loss: 0.00149613
Iteration 150/1000 | Loss: 0.00181989
Iteration 151/1000 | Loss: 0.00253772
Iteration 152/1000 | Loss: 0.00163484
Iteration 153/1000 | Loss: 0.00051288
Iteration 154/1000 | Loss: 0.00123981
Iteration 155/1000 | Loss: 0.00028134
Iteration 156/1000 | Loss: 0.00045302
Iteration 157/1000 | Loss: 0.00039387
Iteration 158/1000 | Loss: 0.00040922
Iteration 159/1000 | Loss: 0.00037719
Iteration 160/1000 | Loss: 0.00026839
Iteration 161/1000 | Loss: 0.00033860
Iteration 162/1000 | Loss: 0.00167580
Iteration 163/1000 | Loss: 0.00043299
Iteration 164/1000 | Loss: 0.00049035
Iteration 165/1000 | Loss: 0.00071629
Iteration 166/1000 | Loss: 0.00040579
Iteration 167/1000 | Loss: 0.00048633
Iteration 168/1000 | Loss: 0.00088385
Iteration 169/1000 | Loss: 0.00060113
Iteration 170/1000 | Loss: 0.00066177
Iteration 171/1000 | Loss: 0.00063462
Iteration 172/1000 | Loss: 0.00073272
Iteration 173/1000 | Loss: 0.00049241
Iteration 174/1000 | Loss: 0.00009991
Iteration 175/1000 | Loss: 0.00012388
Iteration 176/1000 | Loss: 0.00041342
Iteration 177/1000 | Loss: 0.00015289
Iteration 178/1000 | Loss: 0.00006125
Iteration 179/1000 | Loss: 0.00020872
Iteration 180/1000 | Loss: 0.00005616
Iteration 181/1000 | Loss: 0.00027508
Iteration 182/1000 | Loss: 0.00037131
Iteration 183/1000 | Loss: 0.00020555
Iteration 184/1000 | Loss: 0.00024265
Iteration 185/1000 | Loss: 0.00036280
Iteration 186/1000 | Loss: 0.00019488
Iteration 187/1000 | Loss: 0.00017570
Iteration 188/1000 | Loss: 0.00033112
Iteration 189/1000 | Loss: 0.00017921
Iteration 190/1000 | Loss: 0.00036303
Iteration 191/1000 | Loss: 0.00049706
Iteration 192/1000 | Loss: 0.00009029
Iteration 193/1000 | Loss: 0.00026784
Iteration 194/1000 | Loss: 0.00024144
Iteration 195/1000 | Loss: 0.00036798
Iteration 196/1000 | Loss: 0.00041111
Iteration 197/1000 | Loss: 0.00026863
Iteration 198/1000 | Loss: 0.00018310
Iteration 199/1000 | Loss: 0.00022472
Iteration 200/1000 | Loss: 0.00026165
Iteration 201/1000 | Loss: 0.00031295
Iteration 202/1000 | Loss: 0.00024290
Iteration 203/1000 | Loss: 0.00031780
Iteration 204/1000 | Loss: 0.00023582
Iteration 205/1000 | Loss: 0.00033732
Iteration 206/1000 | Loss: 0.00015142
Iteration 207/1000 | Loss: 0.00022567
Iteration 208/1000 | Loss: 0.00026093
Iteration 209/1000 | Loss: 0.00018887
Iteration 210/1000 | Loss: 0.00014350
Iteration 211/1000 | Loss: 0.00010541
Iteration 212/1000 | Loss: 0.00022728
Iteration 213/1000 | Loss: 0.00016721
Iteration 214/1000 | Loss: 0.00020518
Iteration 215/1000 | Loss: 0.00011695
Iteration 216/1000 | Loss: 0.00008610
Iteration 217/1000 | Loss: 0.00013883
Iteration 218/1000 | Loss: 0.00017853
Iteration 219/1000 | Loss: 0.00013359
Iteration 220/1000 | Loss: 0.00021387
Iteration 221/1000 | Loss: 0.00020569
Iteration 222/1000 | Loss: 0.00021054
Iteration 223/1000 | Loss: 0.00016315
Iteration 224/1000 | Loss: 0.00022818
Iteration 225/1000 | Loss: 0.00014169
Iteration 226/1000 | Loss: 0.00014463
Iteration 227/1000 | Loss: 0.00012409
Iteration 228/1000 | Loss: 0.00014661
Iteration 229/1000 | Loss: 0.00012407
Iteration 230/1000 | Loss: 0.00010978
Iteration 231/1000 | Loss: 0.00040924
Iteration 232/1000 | Loss: 0.00023546
Iteration 233/1000 | Loss: 0.00007570
Iteration 234/1000 | Loss: 0.00007836
Iteration 235/1000 | Loss: 0.00005878
Iteration 236/1000 | Loss: 0.00005886
Iteration 237/1000 | Loss: 0.00006517
Iteration 238/1000 | Loss: 0.00022122
Iteration 239/1000 | Loss: 0.00041051
Iteration 240/1000 | Loss: 0.00034111
Iteration 241/1000 | Loss: 0.00009766
Iteration 242/1000 | Loss: 0.00049021
Iteration 243/1000 | Loss: 0.00028111
Iteration 244/1000 | Loss: 0.00015978
Iteration 245/1000 | Loss: 0.00021023
Iteration 246/1000 | Loss: 0.00033545
Iteration 247/1000 | Loss: 0.00003550
Iteration 248/1000 | Loss: 0.00003156
Iteration 249/1000 | Loss: 0.00003446
Iteration 250/1000 | Loss: 0.00002699
Iteration 251/1000 | Loss: 0.00002561
Iteration 252/1000 | Loss: 0.00002436
Iteration 253/1000 | Loss: 0.00003014
Iteration 254/1000 | Loss: 0.00002390
Iteration 255/1000 | Loss: 0.00002204
Iteration 256/1000 | Loss: 0.00002047
Iteration 257/1000 | Loss: 0.00001956
Iteration 258/1000 | Loss: 0.00001891
Iteration 259/1000 | Loss: 0.00002760
Iteration 260/1000 | Loss: 0.00002184
Iteration 261/1000 | Loss: 0.00001832
Iteration 262/1000 | Loss: 0.00001718
Iteration 263/1000 | Loss: 0.00001648
Iteration 264/1000 | Loss: 0.00001613
Iteration 265/1000 | Loss: 0.00001591
Iteration 266/1000 | Loss: 0.00001588
Iteration 267/1000 | Loss: 0.00001584
Iteration 268/1000 | Loss: 0.00001583
Iteration 269/1000 | Loss: 0.00001579
Iteration 270/1000 | Loss: 0.00001579
Iteration 271/1000 | Loss: 0.00001578
Iteration 272/1000 | Loss: 0.00001577
Iteration 273/1000 | Loss: 0.00001576
Iteration 274/1000 | Loss: 0.00001576
Iteration 275/1000 | Loss: 0.00001575
Iteration 276/1000 | Loss: 0.00001575
Iteration 277/1000 | Loss: 0.00001574
Iteration 278/1000 | Loss: 0.00001574
Iteration 279/1000 | Loss: 0.00001573
Iteration 280/1000 | Loss: 0.00001573
Iteration 281/1000 | Loss: 0.00001572
Iteration 282/1000 | Loss: 0.00001571
Iteration 283/1000 | Loss: 0.00001570
Iteration 284/1000 | Loss: 0.00001570
Iteration 285/1000 | Loss: 0.00001570
Iteration 286/1000 | Loss: 0.00001570
Iteration 287/1000 | Loss: 0.00001570
Iteration 288/1000 | Loss: 0.00001570
Iteration 289/1000 | Loss: 0.00001569
Iteration 290/1000 | Loss: 0.00001569
Iteration 291/1000 | Loss: 0.00001569
Iteration 292/1000 | Loss: 0.00001569
Iteration 293/1000 | Loss: 0.00001569
Iteration 294/1000 | Loss: 0.00001569
Iteration 295/1000 | Loss: 0.00001569
Iteration 296/1000 | Loss: 0.00001568
Iteration 297/1000 | Loss: 0.00001568
Iteration 298/1000 | Loss: 0.00001567
Iteration 299/1000 | Loss: 0.00001567
Iteration 300/1000 | Loss: 0.00001567
Iteration 301/1000 | Loss: 0.00001566
Iteration 302/1000 | Loss: 0.00001566
Iteration 303/1000 | Loss: 0.00001566
Iteration 304/1000 | Loss: 0.00001566
Iteration 305/1000 | Loss: 0.00001566
Iteration 306/1000 | Loss: 0.00001566
Iteration 307/1000 | Loss: 0.00001566
Iteration 308/1000 | Loss: 0.00001566
Iteration 309/1000 | Loss: 0.00001566
Iteration 310/1000 | Loss: 0.00001566
Iteration 311/1000 | Loss: 0.00001566
Iteration 312/1000 | Loss: 0.00001566
Iteration 313/1000 | Loss: 0.00001566
Iteration 314/1000 | Loss: 0.00001566
Iteration 315/1000 | Loss: 0.00001566
Iteration 316/1000 | Loss: 0.00001565
Iteration 317/1000 | Loss: 0.00001565
Iteration 318/1000 | Loss: 0.00001565
Iteration 319/1000 | Loss: 0.00001565
Iteration 320/1000 | Loss: 0.00001564
Iteration 321/1000 | Loss: 0.00001564
Iteration 322/1000 | Loss: 0.00001564
Iteration 323/1000 | Loss: 0.00001564
Iteration 324/1000 | Loss: 0.00001564
Iteration 325/1000 | Loss: 0.00001564
Iteration 326/1000 | Loss: 0.00001563
Iteration 327/1000 | Loss: 0.00001563
Iteration 328/1000 | Loss: 0.00001563
Iteration 329/1000 | Loss: 0.00001562
Iteration 330/1000 | Loss: 0.00001562
Iteration 331/1000 | Loss: 0.00001562
Iteration 332/1000 | Loss: 0.00001561
Iteration 333/1000 | Loss: 0.00001561
Iteration 334/1000 | Loss: 0.00001561
Iteration 335/1000 | Loss: 0.00001561
Iteration 336/1000 | Loss: 0.00001561
Iteration 337/1000 | Loss: 0.00001560
Iteration 338/1000 | Loss: 0.00001560
Iteration 339/1000 | Loss: 0.00001560
Iteration 340/1000 | Loss: 0.00001560
Iteration 341/1000 | Loss: 0.00001560
Iteration 342/1000 | Loss: 0.00001560
Iteration 343/1000 | Loss: 0.00001560
Iteration 344/1000 | Loss: 0.00001559
Iteration 345/1000 | Loss: 0.00001559
Iteration 346/1000 | Loss: 0.00001559
Iteration 347/1000 | Loss: 0.00001559
Iteration 348/1000 | Loss: 0.00001559
Iteration 349/1000 | Loss: 0.00001559
Iteration 350/1000 | Loss: 0.00001559
Iteration 351/1000 | Loss: 0.00001559
Iteration 352/1000 | Loss: 0.00001559
Iteration 353/1000 | Loss: 0.00001559
Iteration 354/1000 | Loss: 0.00001558
Iteration 355/1000 | Loss: 0.00001558
Iteration 356/1000 | Loss: 0.00001558
Iteration 357/1000 | Loss: 0.00001558
Iteration 358/1000 | Loss: 0.00001558
Iteration 359/1000 | Loss: 0.00001558
Iteration 360/1000 | Loss: 0.00001558
Iteration 361/1000 | Loss: 0.00001558
Iteration 362/1000 | Loss: 0.00001558
Iteration 363/1000 | Loss: 0.00001558
Iteration 364/1000 | Loss: 0.00001558
Iteration 365/1000 | Loss: 0.00001557
Iteration 366/1000 | Loss: 0.00001557
Iteration 367/1000 | Loss: 0.00001557
Iteration 368/1000 | Loss: 0.00001557
Iteration 369/1000 | Loss: 0.00001557
Iteration 370/1000 | Loss: 0.00001557
Iteration 371/1000 | Loss: 0.00001557
Iteration 372/1000 | Loss: 0.00001556
Iteration 373/1000 | Loss: 0.00001556
Iteration 374/1000 | Loss: 0.00001556
Iteration 375/1000 | Loss: 0.00001556
Iteration 376/1000 | Loss: 0.00001556
Iteration 377/1000 | Loss: 0.00001556
Iteration 378/1000 | Loss: 0.00001556
Iteration 379/1000 | Loss: 0.00001556
Iteration 380/1000 | Loss: 0.00001556
Iteration 381/1000 | Loss: 0.00001556
Iteration 382/1000 | Loss: 0.00001556
Iteration 383/1000 | Loss: 0.00001556
Iteration 384/1000 | Loss: 0.00001555
Iteration 385/1000 | Loss: 0.00001555
Iteration 386/1000 | Loss: 0.00001554
Iteration 387/1000 | Loss: 0.00001554
Iteration 388/1000 | Loss: 0.00001554
Iteration 389/1000 | Loss: 0.00001554
Iteration 390/1000 | Loss: 0.00001554
Iteration 391/1000 | Loss: 0.00001554
Iteration 392/1000 | Loss: 0.00001553
Iteration 393/1000 | Loss: 0.00001553
Iteration 394/1000 | Loss: 0.00001553
Iteration 395/1000 | Loss: 0.00001553
Iteration 396/1000 | Loss: 0.00001553
Iteration 397/1000 | Loss: 0.00001553
Iteration 398/1000 | Loss: 0.00001553
Iteration 399/1000 | Loss: 0.00001553
Iteration 400/1000 | Loss: 0.00001553
Iteration 401/1000 | Loss: 0.00001553
Iteration 402/1000 | Loss: 0.00001552
Iteration 403/1000 | Loss: 0.00001552
Iteration 404/1000 | Loss: 0.00001552
Iteration 405/1000 | Loss: 0.00001552
Iteration 406/1000 | Loss: 0.00001552
Iteration 407/1000 | Loss: 0.00001552
Iteration 408/1000 | Loss: 0.00001552
Iteration 409/1000 | Loss: 0.00001552
Iteration 410/1000 | Loss: 0.00001552
Iteration 411/1000 | Loss: 0.00001552
Iteration 412/1000 | Loss: 0.00001552
Iteration 413/1000 | Loss: 0.00001552
Iteration 414/1000 | Loss: 0.00001552
Iteration 415/1000 | Loss: 0.00001552
Iteration 416/1000 | Loss: 0.00001552
Iteration 417/1000 | Loss: 0.00001552
Iteration 418/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 418. Stopping optimization.
Last 5 losses: [1.5517965948674828e-05, 1.5517965948674828e-05, 1.5517965948674828e-05, 1.5517965948674828e-05, 1.5517965948674828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5517965948674828e-05

Optimization complete. Final v2v error: 3.3374884128570557 mm

Highest mean error: 3.9243361949920654 mm for frame 76

Lowest mean error: 3.000779390335083 mm for frame 90

Saving results

Total time: 423.8658721446991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866232
Iteration 2/25 | Loss: 0.00121692
Iteration 3/25 | Loss: 0.00078246
Iteration 4/25 | Loss: 0.00066694
Iteration 5/25 | Loss: 0.00065056
Iteration 6/25 | Loss: 0.00064505
Iteration 7/25 | Loss: 0.00064369
Iteration 8/25 | Loss: 0.00064361
Iteration 9/25 | Loss: 0.00064361
Iteration 10/25 | Loss: 0.00064361
Iteration 11/25 | Loss: 0.00064361
Iteration 12/25 | Loss: 0.00064361
Iteration 13/25 | Loss: 0.00064361
Iteration 14/25 | Loss: 0.00064361
Iteration 15/25 | Loss: 0.00064361
Iteration 16/25 | Loss: 0.00064361
Iteration 17/25 | Loss: 0.00064361
Iteration 18/25 | Loss: 0.00064361
Iteration 19/25 | Loss: 0.00064361
Iteration 20/25 | Loss: 0.00064361
Iteration 21/25 | Loss: 0.00064361
Iteration 22/25 | Loss: 0.00064361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006436139810830355, 0.0006436139810830355, 0.0006436139810830355, 0.0006436139810830355, 0.0006436139810830355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006436139810830355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45859313
Iteration 2/25 | Loss: 0.00025138
Iteration 3/25 | Loss: 0.00025136
Iteration 4/25 | Loss: 0.00025136
Iteration 5/25 | Loss: 0.00025136
Iteration 6/25 | Loss: 0.00025136
Iteration 7/25 | Loss: 0.00025136
Iteration 8/25 | Loss: 0.00025136
Iteration 9/25 | Loss: 0.00025136
Iteration 10/25 | Loss: 0.00025136
Iteration 11/25 | Loss: 0.00025136
Iteration 12/25 | Loss: 0.00025136
Iteration 13/25 | Loss: 0.00025136
Iteration 14/25 | Loss: 0.00025136
Iteration 15/25 | Loss: 0.00025136
Iteration 16/25 | Loss: 0.00025136
Iteration 17/25 | Loss: 0.00025136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00025136134354397655, 0.00025136134354397655, 0.00025136134354397655, 0.00025136134354397655, 0.00025136134354397655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025136134354397655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025136
Iteration 2/1000 | Loss: 0.00002039
Iteration 3/1000 | Loss: 0.00001685
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001418
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001356
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001355
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001350
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001348
Iteration 25/1000 | Loss: 0.00001348
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001338
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001331
Iteration 105/1000 | Loss: 0.00001331
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001331
Iteration 109/1000 | Loss: 0.00001331
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001331
Iteration 117/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.3314921488927212e-05, 1.3314921488927212e-05, 1.3314921488927212e-05, 1.3314921488927212e-05, 1.3314921488927212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3314921488927212e-05

Optimization complete. Final v2v error: 3.0942223072052 mm

Highest mean error: 3.3035426139831543 mm for frame 220

Lowest mean error: 2.7870919704437256 mm for frame 37

Saving results

Total time: 36.56863498687744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009623
Iteration 2/25 | Loss: 0.00242446
Iteration 3/25 | Loss: 0.00179579
Iteration 4/25 | Loss: 0.00164617
Iteration 5/25 | Loss: 0.00160635
Iteration 6/25 | Loss: 0.00158189
Iteration 7/25 | Loss: 0.00155804
Iteration 8/25 | Loss: 0.00154484
Iteration 9/25 | Loss: 0.00153609
Iteration 10/25 | Loss: 0.00153051
Iteration 11/25 | Loss: 0.00152788
Iteration 12/25 | Loss: 0.00152713
Iteration 13/25 | Loss: 0.00152693
Iteration 14/25 | Loss: 0.00152692
Iteration 15/25 | Loss: 0.00152692
Iteration 16/25 | Loss: 0.00152692
Iteration 17/25 | Loss: 0.00152692
Iteration 18/25 | Loss: 0.00152692
Iteration 19/25 | Loss: 0.00152692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001526923500932753, 0.001526923500932753, 0.001526923500932753, 0.001526923500932753, 0.001526923500932753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001526923500932753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43624043
Iteration 2/25 | Loss: 0.00719118
Iteration 3/25 | Loss: 0.00719117
Iteration 4/25 | Loss: 0.00719117
Iteration 5/25 | Loss: 0.00719117
Iteration 6/25 | Loss: 0.00719116
Iteration 7/25 | Loss: 0.00719116
Iteration 8/25 | Loss: 0.00719116
Iteration 9/25 | Loss: 0.00719116
Iteration 10/25 | Loss: 0.00719116
Iteration 11/25 | Loss: 0.00719116
Iteration 12/25 | Loss: 0.00719116
Iteration 13/25 | Loss: 0.00719116
Iteration 14/25 | Loss: 0.00719116
Iteration 15/25 | Loss: 0.00719116
Iteration 16/25 | Loss: 0.00719116
Iteration 17/25 | Loss: 0.00719116
Iteration 18/25 | Loss: 0.00719116
Iteration 19/25 | Loss: 0.00719116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.007191163953393698, 0.007191163953393698, 0.007191163953393698, 0.007191163953393698, 0.007191163953393698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007191163953393698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00719116
Iteration 2/1000 | Loss: 0.00100467
Iteration 3/1000 | Loss: 0.00074822
Iteration 4/1000 | Loss: 0.00062834
Iteration 5/1000 | Loss: 0.00054384
Iteration 6/1000 | Loss: 0.00049907
Iteration 7/1000 | Loss: 0.00043554
Iteration 8/1000 | Loss: 0.00066899
Iteration 9/1000 | Loss: 0.01686461
Iteration 10/1000 | Loss: 0.01754151
Iteration 11/1000 | Loss: 0.00396604
Iteration 12/1000 | Loss: 0.00182530
Iteration 13/1000 | Loss: 0.00099704
Iteration 14/1000 | Loss: 0.00046254
Iteration 15/1000 | Loss: 0.00030315
Iteration 16/1000 | Loss: 0.00020309
Iteration 17/1000 | Loss: 0.00013926
Iteration 18/1000 | Loss: 0.00009795
Iteration 19/1000 | Loss: 0.00007090
Iteration 20/1000 | Loss: 0.00005258
Iteration 21/1000 | Loss: 0.00004096
Iteration 22/1000 | Loss: 0.00003396
Iteration 23/1000 | Loss: 0.00002903
Iteration 24/1000 | Loss: 0.00002594
Iteration 25/1000 | Loss: 0.00002421
Iteration 26/1000 | Loss: 0.00002284
Iteration 27/1000 | Loss: 0.00002157
Iteration 28/1000 | Loss: 0.00002032
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001833
Iteration 33/1000 | Loss: 0.00001828
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001813
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001743
Iteration 44/1000 | Loss: 0.00001742
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001740
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001736
Iteration 55/1000 | Loss: 0.00001736
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001731
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001730
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001729
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001728
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001726
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001725
Iteration 84/1000 | Loss: 0.00001725
Iteration 85/1000 | Loss: 0.00001725
Iteration 86/1000 | Loss: 0.00001724
Iteration 87/1000 | Loss: 0.00001724
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00001724
Iteration 91/1000 | Loss: 0.00001724
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001723
Iteration 94/1000 | Loss: 0.00001723
Iteration 95/1000 | Loss: 0.00001723
Iteration 96/1000 | Loss: 0.00001723
Iteration 97/1000 | Loss: 0.00001723
Iteration 98/1000 | Loss: 0.00001723
Iteration 99/1000 | Loss: 0.00001723
Iteration 100/1000 | Loss: 0.00001723
Iteration 101/1000 | Loss: 0.00001723
Iteration 102/1000 | Loss: 0.00001722
Iteration 103/1000 | Loss: 0.00001722
Iteration 104/1000 | Loss: 0.00001722
Iteration 105/1000 | Loss: 0.00001722
Iteration 106/1000 | Loss: 0.00001722
Iteration 107/1000 | Loss: 0.00001722
Iteration 108/1000 | Loss: 0.00001721
Iteration 109/1000 | Loss: 0.00001721
Iteration 110/1000 | Loss: 0.00001721
Iteration 111/1000 | Loss: 0.00001721
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001720
Iteration 114/1000 | Loss: 0.00001720
Iteration 115/1000 | Loss: 0.00001720
Iteration 116/1000 | Loss: 0.00001720
Iteration 117/1000 | Loss: 0.00001720
Iteration 118/1000 | Loss: 0.00001720
Iteration 119/1000 | Loss: 0.00001720
Iteration 120/1000 | Loss: 0.00001720
Iteration 121/1000 | Loss: 0.00001720
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001719
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001719
Iteration 131/1000 | Loss: 0.00001719
Iteration 132/1000 | Loss: 0.00001719
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001717
Iteration 139/1000 | Loss: 0.00001717
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001717
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001716
Iteration 169/1000 | Loss: 0.00001716
Iteration 170/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.7160153220174834e-05, 1.7160153220174834e-05, 1.7160153220174834e-05, 1.7160153220174834e-05, 1.7160153220174834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7160153220174834e-05

Optimization complete. Final v2v error: 3.58398175239563 mm

Highest mean error: 3.8445632457733154 mm for frame 119

Lowest mean error: 3.494204044342041 mm for frame 94

Saving results

Total time: 81.78050756454468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378672
Iteration 2/25 | Loss: 0.00077563
Iteration 3/25 | Loss: 0.00061263
Iteration 4/25 | Loss: 0.00058691
Iteration 5/25 | Loss: 0.00058009
Iteration 6/25 | Loss: 0.00057837
Iteration 7/25 | Loss: 0.00057785
Iteration 8/25 | Loss: 0.00057785
Iteration 9/25 | Loss: 0.00057785
Iteration 10/25 | Loss: 0.00057785
Iteration 11/25 | Loss: 0.00057785
Iteration 12/25 | Loss: 0.00057785
Iteration 13/25 | Loss: 0.00057785
Iteration 14/25 | Loss: 0.00057785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005778493359684944, 0.0005778493359684944, 0.0005778493359684944, 0.0005778493359684944, 0.0005778493359684944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005778493359684944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46047199
Iteration 2/25 | Loss: 0.00024811
Iteration 3/25 | Loss: 0.00024811
Iteration 4/25 | Loss: 0.00024811
Iteration 5/25 | Loss: 0.00024811
Iteration 6/25 | Loss: 0.00024811
Iteration 7/25 | Loss: 0.00024811
Iteration 8/25 | Loss: 0.00024811
Iteration 9/25 | Loss: 0.00024811
Iteration 10/25 | Loss: 0.00024811
Iteration 11/25 | Loss: 0.00024811
Iteration 12/25 | Loss: 0.00024811
Iteration 13/25 | Loss: 0.00024811
Iteration 14/25 | Loss: 0.00024811
Iteration 15/25 | Loss: 0.00024811
Iteration 16/25 | Loss: 0.00024811
Iteration 17/25 | Loss: 0.00024811
Iteration 18/25 | Loss: 0.00024811
Iteration 19/25 | Loss: 0.00024811
Iteration 20/25 | Loss: 0.00024811
Iteration 21/25 | Loss: 0.00024811
Iteration 22/25 | Loss: 0.00024811
Iteration 23/25 | Loss: 0.00024811
Iteration 24/25 | Loss: 0.00024811
Iteration 25/25 | Loss: 0.00024811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024811
Iteration 2/1000 | Loss: 0.00001779
Iteration 3/1000 | Loss: 0.00001197
Iteration 4/1000 | Loss: 0.00001126
Iteration 5/1000 | Loss: 0.00001068
Iteration 6/1000 | Loss: 0.00001037
Iteration 7/1000 | Loss: 0.00001016
Iteration 8/1000 | Loss: 0.00001014
Iteration 9/1000 | Loss: 0.00001014
Iteration 10/1000 | Loss: 0.00001013
Iteration 11/1000 | Loss: 0.00001012
Iteration 12/1000 | Loss: 0.00001008
Iteration 13/1000 | Loss: 0.00001005
Iteration 14/1000 | Loss: 0.00001000
Iteration 15/1000 | Loss: 0.00000997
Iteration 16/1000 | Loss: 0.00000995
Iteration 17/1000 | Loss: 0.00000995
Iteration 18/1000 | Loss: 0.00000991
Iteration 19/1000 | Loss: 0.00000990
Iteration 20/1000 | Loss: 0.00000990
Iteration 21/1000 | Loss: 0.00000989
Iteration 22/1000 | Loss: 0.00000989
Iteration 23/1000 | Loss: 0.00000988
Iteration 24/1000 | Loss: 0.00000988
Iteration 25/1000 | Loss: 0.00000987
Iteration 26/1000 | Loss: 0.00000987
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000986
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000985
Iteration 33/1000 | Loss: 0.00000984
Iteration 34/1000 | Loss: 0.00000984
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000980
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000976
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000974
Iteration 60/1000 | Loss: 0.00000974
Iteration 61/1000 | Loss: 0.00000974
Iteration 62/1000 | Loss: 0.00000973
Iteration 63/1000 | Loss: 0.00000973
Iteration 64/1000 | Loss: 0.00000973
Iteration 65/1000 | Loss: 0.00000973
Iteration 66/1000 | Loss: 0.00000973
Iteration 67/1000 | Loss: 0.00000973
Iteration 68/1000 | Loss: 0.00000972
Iteration 69/1000 | Loss: 0.00000972
Iteration 70/1000 | Loss: 0.00000972
Iteration 71/1000 | Loss: 0.00000972
Iteration 72/1000 | Loss: 0.00000972
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000972
Iteration 75/1000 | Loss: 0.00000972
Iteration 76/1000 | Loss: 0.00000972
Iteration 77/1000 | Loss: 0.00000972
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000972
Iteration 83/1000 | Loss: 0.00000972
Iteration 84/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [9.719591616885737e-06, 9.719591616885737e-06, 9.719591616885737e-06, 9.719591616885737e-06, 9.719591616885737e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.719591616885737e-06

Optimization complete. Final v2v error: 2.623812675476074 mm

Highest mean error: 3.5992283821105957 mm for frame 70

Lowest mean error: 2.4742486476898193 mm for frame 103

Saving results

Total time: 29.69208526611328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942042
Iteration 2/25 | Loss: 0.00160712
Iteration 3/25 | Loss: 0.00115329
Iteration 4/25 | Loss: 0.00102956
Iteration 5/25 | Loss: 0.00097957
Iteration 6/25 | Loss: 0.00096697
Iteration 7/25 | Loss: 0.00091386
Iteration 8/25 | Loss: 0.00087016
Iteration 9/25 | Loss: 0.00084692
Iteration 10/25 | Loss: 0.00083648
Iteration 11/25 | Loss: 0.00082427
Iteration 12/25 | Loss: 0.00082347
Iteration 13/25 | Loss: 0.00081612
Iteration 14/25 | Loss: 0.00081312
Iteration 15/25 | Loss: 0.00081253
Iteration 16/25 | Loss: 0.00081244
Iteration 17/25 | Loss: 0.00081244
Iteration 18/25 | Loss: 0.00081244
Iteration 19/25 | Loss: 0.00081244
Iteration 20/25 | Loss: 0.00081244
Iteration 21/25 | Loss: 0.00081244
Iteration 22/25 | Loss: 0.00081244
Iteration 23/25 | Loss: 0.00081244
Iteration 24/25 | Loss: 0.00081244
Iteration 25/25 | Loss: 0.00081244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.35875964
Iteration 2/25 | Loss: 0.00052846
Iteration 3/25 | Loss: 0.00052703
Iteration 4/25 | Loss: 0.00052703
Iteration 5/25 | Loss: 0.00052703
Iteration 6/25 | Loss: 0.00052703
Iteration 7/25 | Loss: 0.00052703
Iteration 8/25 | Loss: 0.00052703
Iteration 9/25 | Loss: 0.00052703
Iteration 10/25 | Loss: 0.00052703
Iteration 11/25 | Loss: 0.00052702
Iteration 12/25 | Loss: 0.00052702
Iteration 13/25 | Loss: 0.00052702
Iteration 14/25 | Loss: 0.00052702
Iteration 15/25 | Loss: 0.00052702
Iteration 16/25 | Loss: 0.00052702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000527024909388274, 0.000527024909388274, 0.000527024909388274, 0.000527024909388274, 0.000527024909388274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000527024909388274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052702
Iteration 2/1000 | Loss: 0.00087936
Iteration 3/1000 | Loss: 0.00047208
Iteration 4/1000 | Loss: 0.00031333
Iteration 5/1000 | Loss: 0.00019878
Iteration 6/1000 | Loss: 0.00013257
Iteration 7/1000 | Loss: 0.00010542
Iteration 8/1000 | Loss: 0.00015748
Iteration 9/1000 | Loss: 0.00024453
Iteration 10/1000 | Loss: 0.00045858
Iteration 11/1000 | Loss: 0.00020845
Iteration 12/1000 | Loss: 0.00016696
Iteration 13/1000 | Loss: 0.00013460
Iteration 14/1000 | Loss: 0.00025183
Iteration 15/1000 | Loss: 0.00018254
Iteration 16/1000 | Loss: 0.00011531
Iteration 17/1000 | Loss: 0.00014566
Iteration 18/1000 | Loss: 0.00009587
Iteration 19/1000 | Loss: 0.00016856
Iteration 20/1000 | Loss: 0.00008867
Iteration 21/1000 | Loss: 0.00008169
Iteration 22/1000 | Loss: 0.00007138
Iteration 23/1000 | Loss: 0.00006354
Iteration 24/1000 | Loss: 0.00005639
Iteration 25/1000 | Loss: 0.00005117
Iteration 26/1000 | Loss: 0.00004747
Iteration 27/1000 | Loss: 0.00005986
Iteration 28/1000 | Loss: 0.00005208
Iteration 29/1000 | Loss: 0.00006616
Iteration 30/1000 | Loss: 0.00005855
Iteration 31/1000 | Loss: 0.00006316
Iteration 32/1000 | Loss: 0.00005443
Iteration 33/1000 | Loss: 0.00005730
Iteration 34/1000 | Loss: 0.00005843
Iteration 35/1000 | Loss: 0.00006409
Iteration 36/1000 | Loss: 0.00006457
Iteration 37/1000 | Loss: 0.00006563
Iteration 38/1000 | Loss: 0.00006435
Iteration 39/1000 | Loss: 0.00006636
Iteration 40/1000 | Loss: 0.00005425
Iteration 41/1000 | Loss: 0.00005763
Iteration 42/1000 | Loss: 0.00006544
Iteration 43/1000 | Loss: 0.00004915
Iteration 44/1000 | Loss: 0.00004026
Iteration 45/1000 | Loss: 0.00005003
Iteration 46/1000 | Loss: 0.00006017
Iteration 47/1000 | Loss: 0.00004876
Iteration 48/1000 | Loss: 0.00004076
Iteration 49/1000 | Loss: 0.00005177
Iteration 50/1000 | Loss: 0.00004549
Iteration 51/1000 | Loss: 0.00004959
Iteration 52/1000 | Loss: 0.00004604
Iteration 53/1000 | Loss: 0.00004044
Iteration 54/1000 | Loss: 0.00004438
Iteration 55/1000 | Loss: 0.00003773
Iteration 56/1000 | Loss: 0.00003525
Iteration 57/1000 | Loss: 0.00004912
Iteration 58/1000 | Loss: 0.00004632
Iteration 59/1000 | Loss: 0.00003415
Iteration 60/1000 | Loss: 0.00004772
Iteration 61/1000 | Loss: 0.00004653
Iteration 62/1000 | Loss: 0.00004622
Iteration 63/1000 | Loss: 0.00003771
Iteration 64/1000 | Loss: 0.00004376
Iteration 65/1000 | Loss: 0.00003907
Iteration 66/1000 | Loss: 0.00004244
Iteration 67/1000 | Loss: 0.00003914
Iteration 68/1000 | Loss: 0.00004078
Iteration 69/1000 | Loss: 0.00003787
Iteration 70/1000 | Loss: 0.00003912
Iteration 71/1000 | Loss: 0.00003670
Iteration 72/1000 | Loss: 0.00004059
Iteration 73/1000 | Loss: 0.00004624
Iteration 74/1000 | Loss: 0.00008428
Iteration 75/1000 | Loss: 0.00005520
Iteration 76/1000 | Loss: 0.00005450
Iteration 77/1000 | Loss: 0.00005244
Iteration 78/1000 | Loss: 0.00005083
Iteration 79/1000 | Loss: 0.00004248
Iteration 80/1000 | Loss: 0.00005224
Iteration 81/1000 | Loss: 0.00003623
Iteration 82/1000 | Loss: 0.00004312
Iteration 83/1000 | Loss: 0.00004270
Iteration 84/1000 | Loss: 0.00004244
Iteration 85/1000 | Loss: 0.00004248
Iteration 86/1000 | Loss: 0.00004304
Iteration 87/1000 | Loss: 0.00004282
Iteration 88/1000 | Loss: 0.00004254
Iteration 89/1000 | Loss: 0.00007986
Iteration 90/1000 | Loss: 0.00005132
Iteration 91/1000 | Loss: 0.00005052
Iteration 92/1000 | Loss: 0.00004219
Iteration 93/1000 | Loss: 0.00004080
Iteration 94/1000 | Loss: 0.00004631
Iteration 95/1000 | Loss: 0.00005186
Iteration 96/1000 | Loss: 0.00009178
Iteration 97/1000 | Loss: 0.00004351
Iteration 98/1000 | Loss: 0.00004911
Iteration 99/1000 | Loss: 0.00004185
Iteration 100/1000 | Loss: 0.00005179
Iteration 101/1000 | Loss: 0.00005763
Iteration 102/1000 | Loss: 0.00004870
Iteration 103/1000 | Loss: 0.00010524
Iteration 104/1000 | Loss: 0.00020020
Iteration 105/1000 | Loss: 0.00003989
Iteration 106/1000 | Loss: 0.00006351
Iteration 107/1000 | Loss: 0.00003659
Iteration 108/1000 | Loss: 0.00003424
Iteration 109/1000 | Loss: 0.00003338
Iteration 110/1000 | Loss: 0.00003294
Iteration 111/1000 | Loss: 0.00003248
Iteration 112/1000 | Loss: 0.00003216
Iteration 113/1000 | Loss: 0.00003189
Iteration 114/1000 | Loss: 0.00003167
Iteration 115/1000 | Loss: 0.00003148
Iteration 116/1000 | Loss: 0.00003140
Iteration 117/1000 | Loss: 0.00003133
Iteration 118/1000 | Loss: 0.00003130
Iteration 119/1000 | Loss: 0.00003129
Iteration 120/1000 | Loss: 0.00003110
Iteration 121/1000 | Loss: 0.00003091
Iteration 122/1000 | Loss: 0.00003084
Iteration 123/1000 | Loss: 0.00003065
Iteration 124/1000 | Loss: 0.00003045
Iteration 125/1000 | Loss: 0.00003028
Iteration 126/1000 | Loss: 0.00003021
Iteration 127/1000 | Loss: 0.00003020
Iteration 128/1000 | Loss: 0.00003020
Iteration 129/1000 | Loss: 0.00003018
Iteration 130/1000 | Loss: 0.00003018
Iteration 131/1000 | Loss: 0.00003018
Iteration 132/1000 | Loss: 0.00003018
Iteration 133/1000 | Loss: 0.00003018
Iteration 134/1000 | Loss: 0.00003018
Iteration 135/1000 | Loss: 0.00003017
Iteration 136/1000 | Loss: 0.00003017
Iteration 137/1000 | Loss: 0.00003016
Iteration 138/1000 | Loss: 0.00003016
Iteration 139/1000 | Loss: 0.00003015
Iteration 140/1000 | Loss: 0.00003015
Iteration 141/1000 | Loss: 0.00003015
Iteration 142/1000 | Loss: 0.00003015
Iteration 143/1000 | Loss: 0.00003015
Iteration 144/1000 | Loss: 0.00003015
Iteration 145/1000 | Loss: 0.00003015
Iteration 146/1000 | Loss: 0.00003015
Iteration 147/1000 | Loss: 0.00003014
Iteration 148/1000 | Loss: 0.00003014
Iteration 149/1000 | Loss: 0.00003013
Iteration 150/1000 | Loss: 0.00003013
Iteration 151/1000 | Loss: 0.00003013
Iteration 152/1000 | Loss: 0.00003012
Iteration 153/1000 | Loss: 0.00003012
Iteration 154/1000 | Loss: 0.00003012
Iteration 155/1000 | Loss: 0.00003011
Iteration 156/1000 | Loss: 0.00003011
Iteration 157/1000 | Loss: 0.00003011
Iteration 158/1000 | Loss: 0.00003010
Iteration 159/1000 | Loss: 0.00003010
Iteration 160/1000 | Loss: 0.00003009
Iteration 161/1000 | Loss: 0.00003009
Iteration 162/1000 | Loss: 0.00003009
Iteration 163/1000 | Loss: 0.00003009
Iteration 164/1000 | Loss: 0.00003009
Iteration 165/1000 | Loss: 0.00003008
Iteration 166/1000 | Loss: 0.00003008
Iteration 167/1000 | Loss: 0.00003008
Iteration 168/1000 | Loss: 0.00003008
Iteration 169/1000 | Loss: 0.00003008
Iteration 170/1000 | Loss: 0.00003008
Iteration 171/1000 | Loss: 0.00003008
Iteration 172/1000 | Loss: 0.00003007
Iteration 173/1000 | Loss: 0.00003007
Iteration 174/1000 | Loss: 0.00003007
Iteration 175/1000 | Loss: 0.00003007
Iteration 176/1000 | Loss: 0.00003007
Iteration 177/1000 | Loss: 0.00003007
Iteration 178/1000 | Loss: 0.00003007
Iteration 179/1000 | Loss: 0.00003007
Iteration 180/1000 | Loss: 0.00003007
Iteration 181/1000 | Loss: 0.00003006
Iteration 182/1000 | Loss: 0.00003006
Iteration 183/1000 | Loss: 0.00003006
Iteration 184/1000 | Loss: 0.00003006
Iteration 185/1000 | Loss: 0.00003006
Iteration 186/1000 | Loss: 0.00003005
Iteration 187/1000 | Loss: 0.00003005
Iteration 188/1000 | Loss: 0.00003005
Iteration 189/1000 | Loss: 0.00003005
Iteration 190/1000 | Loss: 0.00003005
Iteration 191/1000 | Loss: 0.00003005
Iteration 192/1000 | Loss: 0.00003005
Iteration 193/1000 | Loss: 0.00003005
Iteration 194/1000 | Loss: 0.00003005
Iteration 195/1000 | Loss: 0.00003005
Iteration 196/1000 | Loss: 0.00003005
Iteration 197/1000 | Loss: 0.00003005
Iteration 198/1000 | Loss: 0.00003005
Iteration 199/1000 | Loss: 0.00003004
Iteration 200/1000 | Loss: 0.00003004
Iteration 201/1000 | Loss: 0.00003004
Iteration 202/1000 | Loss: 0.00003004
Iteration 203/1000 | Loss: 0.00003004
Iteration 204/1000 | Loss: 0.00003003
Iteration 205/1000 | Loss: 0.00003003
Iteration 206/1000 | Loss: 0.00003003
Iteration 207/1000 | Loss: 0.00003003
Iteration 208/1000 | Loss: 0.00003003
Iteration 209/1000 | Loss: 0.00003003
Iteration 210/1000 | Loss: 0.00003003
Iteration 211/1000 | Loss: 0.00003003
Iteration 212/1000 | Loss: 0.00003003
Iteration 213/1000 | Loss: 0.00003003
Iteration 214/1000 | Loss: 0.00003003
Iteration 215/1000 | Loss: 0.00003002
Iteration 216/1000 | Loss: 0.00003002
Iteration 217/1000 | Loss: 0.00003002
Iteration 218/1000 | Loss: 0.00003002
Iteration 219/1000 | Loss: 0.00003002
Iteration 220/1000 | Loss: 0.00003002
Iteration 221/1000 | Loss: 0.00003002
Iteration 222/1000 | Loss: 0.00003002
Iteration 223/1000 | Loss: 0.00003002
Iteration 224/1000 | Loss: 0.00003002
Iteration 225/1000 | Loss: 0.00003002
Iteration 226/1000 | Loss: 0.00003002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [3.0022636565263383e-05, 3.0022636565263383e-05, 3.0022636565263383e-05, 3.0022636565263383e-05, 3.0022636565263383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0022636565263383e-05

Optimization complete. Final v2v error: 4.316156387329102 mm

Highest mean error: 8.112958908081055 mm for frame 105

Lowest mean error: 3.1958630084991455 mm for frame 75

Saving results

Total time: 217.93216490745544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772436
Iteration 2/25 | Loss: 0.00109885
Iteration 3/25 | Loss: 0.00083241
Iteration 4/25 | Loss: 0.00075344
Iteration 5/25 | Loss: 0.00072131
Iteration 6/25 | Loss: 0.00072733
Iteration 7/25 | Loss: 0.00071125
Iteration 8/25 | Loss: 0.00070659
Iteration 9/25 | Loss: 0.00070465
Iteration 10/25 | Loss: 0.00070340
Iteration 11/25 | Loss: 0.00070240
Iteration 12/25 | Loss: 0.00070159
Iteration 13/25 | Loss: 0.00070119
Iteration 14/25 | Loss: 0.00070093
Iteration 15/25 | Loss: 0.00070103
Iteration 16/25 | Loss: 0.00070080
Iteration 17/25 | Loss: 0.00070049
Iteration 18/25 | Loss: 0.00070034
Iteration 19/25 | Loss: 0.00070024
Iteration 20/25 | Loss: 0.00070049
Iteration 21/25 | Loss: 0.00070026
Iteration 22/25 | Loss: 0.00070039
Iteration 23/25 | Loss: 0.00070037
Iteration 24/25 | Loss: 0.00070037
Iteration 25/25 | Loss: 0.00070018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.05132198
Iteration 2/25 | Loss: 0.00040050
Iteration 3/25 | Loss: 0.00040043
Iteration 4/25 | Loss: 0.00040043
Iteration 5/25 | Loss: 0.00040043
Iteration 6/25 | Loss: 0.00040043
Iteration 7/25 | Loss: 0.00040043
Iteration 8/25 | Loss: 0.00040043
Iteration 9/25 | Loss: 0.00040043
Iteration 10/25 | Loss: 0.00040043
Iteration 11/25 | Loss: 0.00040043
Iteration 12/25 | Loss: 0.00040043
Iteration 13/25 | Loss: 0.00040043
Iteration 14/25 | Loss: 0.00040043
Iteration 15/25 | Loss: 0.00040043
Iteration 16/25 | Loss: 0.00040043
Iteration 17/25 | Loss: 0.00040043
Iteration 18/25 | Loss: 0.00040043
Iteration 19/25 | Loss: 0.00040043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00040042915497906506, 0.00040042915497906506, 0.00040042915497906506, 0.00040042915497906506, 0.00040042915497906506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040042915497906506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040043
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00002208
Iteration 5/1000 | Loss: 0.00006894
Iteration 6/1000 | Loss: 0.00002065
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001963
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001922
Iteration 11/1000 | Loss: 0.00001907
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001885
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001865
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001865
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00001846
Iteration 24/1000 | Loss: 0.00001845
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001842
Iteration 30/1000 | Loss: 0.00001841
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00001839
Iteration 33/1000 | Loss: 0.00001839
Iteration 34/1000 | Loss: 0.00001838
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001835
Iteration 43/1000 | Loss: 0.00001832
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001830
Iteration 47/1000 | Loss: 0.00001830
Iteration 48/1000 | Loss: 0.00001829
Iteration 49/1000 | Loss: 0.00001829
Iteration 50/1000 | Loss: 0.00001829
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001828
Iteration 55/1000 | Loss: 0.00001828
Iteration 56/1000 | Loss: 0.00001828
Iteration 57/1000 | Loss: 0.00001830
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001829
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00001828
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001825
Iteration 69/1000 | Loss: 0.00001825
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001824
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001824
Iteration 79/1000 | Loss: 0.00001823
Iteration 80/1000 | Loss: 0.00001823
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001822
Iteration 88/1000 | Loss: 0.00001822
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001819
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001819
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001819
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00001819
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001818
Iteration 114/1000 | Loss: 0.00001818
Iteration 115/1000 | Loss: 0.00001818
Iteration 116/1000 | Loss: 0.00001818
Iteration 117/1000 | Loss: 0.00001817
Iteration 118/1000 | Loss: 0.00001817
Iteration 119/1000 | Loss: 0.00001817
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001816
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001815
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001814
Iteration 130/1000 | Loss: 0.00001814
Iteration 131/1000 | Loss: 0.00001814
Iteration 132/1000 | Loss: 0.00001814
Iteration 133/1000 | Loss: 0.00001813
Iteration 134/1000 | Loss: 0.00001813
Iteration 135/1000 | Loss: 0.00001813
Iteration 136/1000 | Loss: 0.00001813
Iteration 137/1000 | Loss: 0.00001813
Iteration 138/1000 | Loss: 0.00001813
Iteration 139/1000 | Loss: 0.00001813
Iteration 140/1000 | Loss: 0.00001813
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001813
Iteration 143/1000 | Loss: 0.00001812
Iteration 144/1000 | Loss: 0.00001812
Iteration 145/1000 | Loss: 0.00001812
Iteration 146/1000 | Loss: 0.00001812
Iteration 147/1000 | Loss: 0.00001812
Iteration 148/1000 | Loss: 0.00001812
Iteration 149/1000 | Loss: 0.00001812
Iteration 150/1000 | Loss: 0.00001811
Iteration 151/1000 | Loss: 0.00001811
Iteration 152/1000 | Loss: 0.00001811
Iteration 153/1000 | Loss: 0.00001811
Iteration 154/1000 | Loss: 0.00001811
Iteration 155/1000 | Loss: 0.00001811
Iteration 156/1000 | Loss: 0.00001811
Iteration 157/1000 | Loss: 0.00001811
Iteration 158/1000 | Loss: 0.00001811
Iteration 159/1000 | Loss: 0.00001811
Iteration 160/1000 | Loss: 0.00001811
Iteration 161/1000 | Loss: 0.00001811
Iteration 162/1000 | Loss: 0.00001811
Iteration 163/1000 | Loss: 0.00001811
Iteration 164/1000 | Loss: 0.00001811
Iteration 165/1000 | Loss: 0.00001811
Iteration 166/1000 | Loss: 0.00001811
Iteration 167/1000 | Loss: 0.00001810
Iteration 168/1000 | Loss: 0.00001810
Iteration 169/1000 | Loss: 0.00001810
Iteration 170/1000 | Loss: 0.00001810
Iteration 171/1000 | Loss: 0.00001810
Iteration 172/1000 | Loss: 0.00001810
Iteration 173/1000 | Loss: 0.00001810
Iteration 174/1000 | Loss: 0.00001810
Iteration 175/1000 | Loss: 0.00001810
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001810
Iteration 179/1000 | Loss: 0.00001810
Iteration 180/1000 | Loss: 0.00001810
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001810
Iteration 183/1000 | Loss: 0.00001809
Iteration 184/1000 | Loss: 0.00001809
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001809
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00001809
Iteration 189/1000 | Loss: 0.00001809
Iteration 190/1000 | Loss: 0.00001809
Iteration 191/1000 | Loss: 0.00001809
Iteration 192/1000 | Loss: 0.00001809
Iteration 193/1000 | Loss: 0.00001809
Iteration 194/1000 | Loss: 0.00001809
Iteration 195/1000 | Loss: 0.00001809
Iteration 196/1000 | Loss: 0.00001809
Iteration 197/1000 | Loss: 0.00001809
Iteration 198/1000 | Loss: 0.00001809
Iteration 199/1000 | Loss: 0.00001809
Iteration 200/1000 | Loss: 0.00001809
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Iteration 203/1000 | Loss: 0.00001809
Iteration 204/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.8090368030243553e-05, 1.8090368030243553e-05, 1.8090368030243553e-05, 1.8090368030243553e-05, 1.8090368030243553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8090368030243553e-05

Optimization complete. Final v2v error: 3.458235740661621 mm

Highest mean error: 13.074616432189941 mm for frame 159

Lowest mean error: 2.8359522819519043 mm for frame 231

Saving results

Total time: 86.89019823074341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540889
Iteration 2/25 | Loss: 0.00081814
Iteration 3/25 | Loss: 0.00065730
Iteration 4/25 | Loss: 0.00062836
Iteration 5/25 | Loss: 0.00061961
Iteration 6/25 | Loss: 0.00061743
Iteration 7/25 | Loss: 0.00061689
Iteration 8/25 | Loss: 0.00061689
Iteration 9/25 | Loss: 0.00061689
Iteration 10/25 | Loss: 0.00061689
Iteration 11/25 | Loss: 0.00061689
Iteration 12/25 | Loss: 0.00061689
Iteration 13/25 | Loss: 0.00061689
Iteration 14/25 | Loss: 0.00061689
Iteration 15/25 | Loss: 0.00061689
Iteration 16/25 | Loss: 0.00061689
Iteration 17/25 | Loss: 0.00061689
Iteration 18/25 | Loss: 0.00061689
Iteration 19/25 | Loss: 0.00061689
Iteration 20/25 | Loss: 0.00061689
Iteration 21/25 | Loss: 0.00061689
Iteration 22/25 | Loss: 0.00061689
Iteration 23/25 | Loss: 0.00061689
Iteration 24/25 | Loss: 0.00061689
Iteration 25/25 | Loss: 0.00061689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.67981815
Iteration 2/25 | Loss: 0.00025635
Iteration 3/25 | Loss: 0.00025632
Iteration 4/25 | Loss: 0.00025632
Iteration 5/25 | Loss: 0.00025632
Iteration 6/25 | Loss: 0.00025632
Iteration 7/25 | Loss: 0.00025632
Iteration 8/25 | Loss: 0.00025632
Iteration 9/25 | Loss: 0.00025632
Iteration 10/25 | Loss: 0.00025632
Iteration 11/25 | Loss: 0.00025632
Iteration 12/25 | Loss: 0.00025632
Iteration 13/25 | Loss: 0.00025632
Iteration 14/25 | Loss: 0.00025632
Iteration 15/25 | Loss: 0.00025632
Iteration 16/25 | Loss: 0.00025632
Iteration 17/25 | Loss: 0.00025632
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00025632113101892173, 0.00025632113101892173, 0.00025632113101892173, 0.00025632113101892173, 0.00025632113101892173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025632113101892173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025632
Iteration 2/1000 | Loss: 0.00002924
Iteration 3/1000 | Loss: 0.00001891
Iteration 4/1000 | Loss: 0.00001767
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001617
Iteration 7/1000 | Loss: 0.00001572
Iteration 8/1000 | Loss: 0.00001556
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001507
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001494
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001493
Iteration 22/1000 | Loss: 0.00001492
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001490
Iteration 27/1000 | Loss: 0.00001489
Iteration 28/1000 | Loss: 0.00001488
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001485
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001481
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001479
Iteration 72/1000 | Loss: 0.00001479
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.47940199894947e-05, 1.47940199894947e-05, 1.47940199894947e-05, 1.47940199894947e-05, 1.47940199894947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.47940199894947e-05

Optimization complete. Final v2v error: 3.2650768756866455 mm

Highest mean error: 3.826666831970215 mm for frame 66

Lowest mean error: 2.9893083572387695 mm for frame 174

Saving results

Total time: 31.09390354156494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074609
Iteration 2/25 | Loss: 0.00256732
Iteration 3/25 | Loss: 0.00212152
Iteration 4/25 | Loss: 0.00156015
Iteration 5/25 | Loss: 0.00189030
Iteration 6/25 | Loss: 0.00193882
Iteration 7/25 | Loss: 0.00183708
Iteration 8/25 | Loss: 0.00161900
Iteration 9/25 | Loss: 0.00168117
Iteration 10/25 | Loss: 0.00157874
Iteration 11/25 | Loss: 0.00146316
Iteration 12/25 | Loss: 0.00141240
Iteration 13/25 | Loss: 0.00136928
Iteration 14/25 | Loss: 0.00137547
Iteration 15/25 | Loss: 0.00129691
Iteration 16/25 | Loss: 0.00128199
Iteration 17/25 | Loss: 0.00125505
Iteration 18/25 | Loss: 0.00121330
Iteration 19/25 | Loss: 0.00122133
Iteration 20/25 | Loss: 0.00117841
Iteration 21/25 | Loss: 0.00114483
Iteration 22/25 | Loss: 0.00113203
Iteration 23/25 | Loss: 0.00114574
Iteration 24/25 | Loss: 0.00114884
Iteration 25/25 | Loss: 0.00114814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68613255
Iteration 2/25 | Loss: 0.00486046
Iteration 3/25 | Loss: 0.00481542
Iteration 4/25 | Loss: 0.00481540
Iteration 5/25 | Loss: 0.00481540
Iteration 6/25 | Loss: 0.00481540
Iteration 7/25 | Loss: 0.00481540
Iteration 8/25 | Loss: 0.00481540
Iteration 9/25 | Loss: 0.00481540
Iteration 10/25 | Loss: 0.00481540
Iteration 11/25 | Loss: 0.00481540
Iteration 12/25 | Loss: 0.00481540
Iteration 13/25 | Loss: 0.00481540
Iteration 14/25 | Loss: 0.00481540
Iteration 15/25 | Loss: 0.00481540
Iteration 16/25 | Loss: 0.00481540
Iteration 17/25 | Loss: 0.00481540
Iteration 18/25 | Loss: 0.00481540
Iteration 19/25 | Loss: 0.00481540
Iteration 20/25 | Loss: 0.00481540
Iteration 21/25 | Loss: 0.00481540
Iteration 22/25 | Loss: 0.00481540
Iteration 23/25 | Loss: 0.00481540
Iteration 24/25 | Loss: 0.00481540
Iteration 25/25 | Loss: 0.00481540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004815397784113884, 0.004815397784113884, 0.004815397784113884, 0.004815397784113884, 0.004815397784113884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004815397784113884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00481540
Iteration 2/1000 | Loss: 0.00217080
Iteration 3/1000 | Loss: 0.00205886
Iteration 4/1000 | Loss: 0.00222563
Iteration 5/1000 | Loss: 0.00174565
Iteration 6/1000 | Loss: 0.00286028
Iteration 7/1000 | Loss: 0.00307640
Iteration 8/1000 | Loss: 0.00167119
Iteration 9/1000 | Loss: 0.00235727
Iteration 10/1000 | Loss: 0.00251715
Iteration 11/1000 | Loss: 0.00219247
Iteration 12/1000 | Loss: 0.00201241
Iteration 13/1000 | Loss: 0.00204550
Iteration 14/1000 | Loss: 0.00175567
Iteration 15/1000 | Loss: 0.00166016
Iteration 16/1000 | Loss: 0.00203804
Iteration 17/1000 | Loss: 0.00165024
Iteration 18/1000 | Loss: 0.00143364
Iteration 19/1000 | Loss: 0.00131090
Iteration 20/1000 | Loss: 0.00108257
Iteration 21/1000 | Loss: 0.00104448
Iteration 22/1000 | Loss: 0.00143244
Iteration 23/1000 | Loss: 0.00171511
Iteration 24/1000 | Loss: 0.00106054
Iteration 25/1000 | Loss: 0.00127223
Iteration 26/1000 | Loss: 0.00148172
Iteration 27/1000 | Loss: 0.00102890
Iteration 28/1000 | Loss: 0.00089588
Iteration 29/1000 | Loss: 0.00091313
Iteration 30/1000 | Loss: 0.00130436
Iteration 31/1000 | Loss: 0.00119798
Iteration 32/1000 | Loss: 0.00242179
Iteration 33/1000 | Loss: 0.00222096
Iteration 34/1000 | Loss: 0.00168947
Iteration 35/1000 | Loss: 0.00153410
Iteration 36/1000 | Loss: 0.00164635
Iteration 37/1000 | Loss: 0.00100949
Iteration 38/1000 | Loss: 0.00108089
Iteration 39/1000 | Loss: 0.00139248
Iteration 40/1000 | Loss: 0.00175862
Iteration 41/1000 | Loss: 0.00175312
Iteration 42/1000 | Loss: 0.00203850
Iteration 43/1000 | Loss: 0.00162063
Iteration 44/1000 | Loss: 0.00108743
Iteration 45/1000 | Loss: 0.00178188
Iteration 46/1000 | Loss: 0.00064843
Iteration 47/1000 | Loss: 0.00114586
Iteration 48/1000 | Loss: 0.00194497
Iteration 49/1000 | Loss: 0.00107656
Iteration 50/1000 | Loss: 0.00091233
Iteration 51/1000 | Loss: 0.00089468
Iteration 52/1000 | Loss: 0.00189186
Iteration 53/1000 | Loss: 0.00100371
Iteration 54/1000 | Loss: 0.00174705
Iteration 55/1000 | Loss: 0.00157912
Iteration 56/1000 | Loss: 0.00191102
Iteration 57/1000 | Loss: 0.00119031
Iteration 58/1000 | Loss: 0.00122176
Iteration 59/1000 | Loss: 0.00059197
Iteration 60/1000 | Loss: 0.00048000
Iteration 61/1000 | Loss: 0.00134997
Iteration 62/1000 | Loss: 0.00108016
Iteration 63/1000 | Loss: 0.00095484
Iteration 64/1000 | Loss: 0.00143258
Iteration 65/1000 | Loss: 0.00105968
Iteration 66/1000 | Loss: 0.00158678
Iteration 67/1000 | Loss: 0.00204756
Iteration 68/1000 | Loss: 0.00127871
Iteration 69/1000 | Loss: 0.00137053
Iteration 70/1000 | Loss: 0.00119442
Iteration 71/1000 | Loss: 0.00091349
Iteration 72/1000 | Loss: 0.00094556
Iteration 73/1000 | Loss: 0.00105631
Iteration 74/1000 | Loss: 0.00094404
Iteration 75/1000 | Loss: 0.00122452
Iteration 76/1000 | Loss: 0.00125098
Iteration 77/1000 | Loss: 0.00110242
Iteration 78/1000 | Loss: 0.00065768
Iteration 79/1000 | Loss: 0.00097139
Iteration 80/1000 | Loss: 0.00082039
Iteration 81/1000 | Loss: 0.00085696
Iteration 82/1000 | Loss: 0.00155553
Iteration 83/1000 | Loss: 0.00087230
Iteration 84/1000 | Loss: 0.00074508
Iteration 85/1000 | Loss: 0.00125127
Iteration 86/1000 | Loss: 0.00149336
Iteration 87/1000 | Loss: 0.00123316
Iteration 88/1000 | Loss: 0.00055240
Iteration 89/1000 | Loss: 0.00102393
Iteration 90/1000 | Loss: 0.00077826
Iteration 91/1000 | Loss: 0.00050503
Iteration 92/1000 | Loss: 0.00041541
Iteration 93/1000 | Loss: 0.00039105
Iteration 94/1000 | Loss: 0.00023060
Iteration 95/1000 | Loss: 0.00049874
Iteration 96/1000 | Loss: 0.00024192
Iteration 97/1000 | Loss: 0.00029198
Iteration 98/1000 | Loss: 0.00043930
Iteration 99/1000 | Loss: 0.00059464
Iteration 100/1000 | Loss: 0.00038101
Iteration 101/1000 | Loss: 0.00059189
Iteration 102/1000 | Loss: 0.00079267
Iteration 103/1000 | Loss: 0.00051694
Iteration 104/1000 | Loss: 0.00044760
Iteration 105/1000 | Loss: 0.00033290
Iteration 106/1000 | Loss: 0.00060745
Iteration 107/1000 | Loss: 0.00033894
Iteration 108/1000 | Loss: 0.00051767
Iteration 109/1000 | Loss: 0.00221356
Iteration 110/1000 | Loss: 0.00051349
Iteration 111/1000 | Loss: 0.00110744
Iteration 112/1000 | Loss: 0.00035860
Iteration 113/1000 | Loss: 0.00032254
Iteration 114/1000 | Loss: 0.00071270
Iteration 115/1000 | Loss: 0.00071826
Iteration 116/1000 | Loss: 0.00058976
Iteration 117/1000 | Loss: 0.00084907
Iteration 118/1000 | Loss: 0.00049938
Iteration 119/1000 | Loss: 0.00052773
Iteration 120/1000 | Loss: 0.00054777
Iteration 121/1000 | Loss: 0.00033145
Iteration 122/1000 | Loss: 0.00017510
Iteration 123/1000 | Loss: 0.00034577
Iteration 124/1000 | Loss: 0.00054312
Iteration 125/1000 | Loss: 0.00017109
Iteration 126/1000 | Loss: 0.00088489
Iteration 127/1000 | Loss: 0.00028891
Iteration 128/1000 | Loss: 0.00031751
Iteration 129/1000 | Loss: 0.00031572
Iteration 130/1000 | Loss: 0.00016861
Iteration 131/1000 | Loss: 0.00016945
Iteration 132/1000 | Loss: 0.00015763
Iteration 133/1000 | Loss: 0.00013314
Iteration 134/1000 | Loss: 0.00091292
Iteration 135/1000 | Loss: 0.00188737
Iteration 136/1000 | Loss: 0.00066928
Iteration 137/1000 | Loss: 0.00069758
Iteration 138/1000 | Loss: 0.00030706
Iteration 139/1000 | Loss: 0.00027753
Iteration 140/1000 | Loss: 0.00032373
Iteration 141/1000 | Loss: 0.00014834
Iteration 142/1000 | Loss: 0.00031474
Iteration 143/1000 | Loss: 0.00013868
Iteration 144/1000 | Loss: 0.00027438
Iteration 145/1000 | Loss: 0.00012496
Iteration 146/1000 | Loss: 0.00087845
Iteration 147/1000 | Loss: 0.00117734
Iteration 148/1000 | Loss: 0.00047000
Iteration 149/1000 | Loss: 0.00028175
Iteration 150/1000 | Loss: 0.00059448
Iteration 151/1000 | Loss: 0.00016147
Iteration 152/1000 | Loss: 0.00033381
Iteration 153/1000 | Loss: 0.00070456
Iteration 154/1000 | Loss: 0.00037854
Iteration 155/1000 | Loss: 0.00014233
Iteration 156/1000 | Loss: 0.00029450
Iteration 157/1000 | Loss: 0.00027120
Iteration 158/1000 | Loss: 0.00016698
Iteration 159/1000 | Loss: 0.00028650
Iteration 160/1000 | Loss: 0.00012438
Iteration 161/1000 | Loss: 0.00013870
Iteration 162/1000 | Loss: 0.00014120
Iteration 163/1000 | Loss: 0.00017814
Iteration 164/1000 | Loss: 0.00033537
Iteration 165/1000 | Loss: 0.00034147
Iteration 166/1000 | Loss: 0.00035876
Iteration 167/1000 | Loss: 0.00019611
Iteration 168/1000 | Loss: 0.00011256
Iteration 169/1000 | Loss: 0.00010658
Iteration 170/1000 | Loss: 0.00039674
Iteration 171/1000 | Loss: 0.00010680
Iteration 172/1000 | Loss: 0.00010326
Iteration 173/1000 | Loss: 0.00029331
Iteration 174/1000 | Loss: 0.00033052
Iteration 175/1000 | Loss: 0.00027055
Iteration 176/1000 | Loss: 0.00030617
Iteration 177/1000 | Loss: 0.00012862
Iteration 178/1000 | Loss: 0.00011276
Iteration 179/1000 | Loss: 0.00012496
Iteration 180/1000 | Loss: 0.00011157
Iteration 181/1000 | Loss: 0.00009670
Iteration 182/1000 | Loss: 0.00009159
Iteration 183/1000 | Loss: 0.00008565
Iteration 184/1000 | Loss: 0.00009250
Iteration 185/1000 | Loss: 0.00008126
Iteration 186/1000 | Loss: 0.00008369
Iteration 187/1000 | Loss: 0.00008681
Iteration 188/1000 | Loss: 0.00009631
Iteration 189/1000 | Loss: 0.00010481
Iteration 190/1000 | Loss: 0.00008543
Iteration 191/1000 | Loss: 0.00008653
Iteration 192/1000 | Loss: 0.00008948
Iteration 193/1000 | Loss: 0.00008411
Iteration 194/1000 | Loss: 0.00025130
Iteration 195/1000 | Loss: 0.00173955
Iteration 196/1000 | Loss: 0.00131564
Iteration 197/1000 | Loss: 0.00118251
Iteration 198/1000 | Loss: 0.00026992
Iteration 199/1000 | Loss: 0.00029510
Iteration 200/1000 | Loss: 0.00011043
Iteration 201/1000 | Loss: 0.00017751
Iteration 202/1000 | Loss: 0.00008455
Iteration 203/1000 | Loss: 0.00007708
Iteration 204/1000 | Loss: 0.00007609
Iteration 205/1000 | Loss: 0.00007137
Iteration 206/1000 | Loss: 0.00008377
Iteration 207/1000 | Loss: 0.00057396
Iteration 208/1000 | Loss: 0.00028072
Iteration 209/1000 | Loss: 0.00057035
Iteration 210/1000 | Loss: 0.00110168
Iteration 211/1000 | Loss: 0.00080655
Iteration 212/1000 | Loss: 0.00085469
Iteration 213/1000 | Loss: 0.00049508
Iteration 214/1000 | Loss: 0.00053696
Iteration 215/1000 | Loss: 0.00066486
Iteration 216/1000 | Loss: 0.00024678
Iteration 217/1000 | Loss: 0.00024174
Iteration 218/1000 | Loss: 0.00048717
Iteration 219/1000 | Loss: 0.00022426
Iteration 220/1000 | Loss: 0.00009595
Iteration 221/1000 | Loss: 0.00016428
Iteration 222/1000 | Loss: 0.00013318
Iteration 223/1000 | Loss: 0.00008352
Iteration 224/1000 | Loss: 0.00008934
Iteration 225/1000 | Loss: 0.00036738
Iteration 226/1000 | Loss: 0.00071428
Iteration 227/1000 | Loss: 0.00009771
Iteration 228/1000 | Loss: 0.00114228
Iteration 229/1000 | Loss: 0.00018904
Iteration 230/1000 | Loss: 0.00018134
Iteration 231/1000 | Loss: 0.00011307
Iteration 232/1000 | Loss: 0.00056068
Iteration 233/1000 | Loss: 0.00008603
Iteration 234/1000 | Loss: 0.00036525
Iteration 235/1000 | Loss: 0.00025352
Iteration 236/1000 | Loss: 0.00008618
Iteration 237/1000 | Loss: 0.00019147
Iteration 238/1000 | Loss: 0.00070214
Iteration 239/1000 | Loss: 0.00062503
Iteration 240/1000 | Loss: 0.00042116
Iteration 241/1000 | Loss: 0.00018728
Iteration 242/1000 | Loss: 0.00053052
Iteration 243/1000 | Loss: 0.00022367
Iteration 244/1000 | Loss: 0.00031178
Iteration 245/1000 | Loss: 0.00027710
Iteration 246/1000 | Loss: 0.00009034
Iteration 247/1000 | Loss: 0.00008328
Iteration 248/1000 | Loss: 0.00019442
Iteration 249/1000 | Loss: 0.00012239
Iteration 250/1000 | Loss: 0.00068536
Iteration 251/1000 | Loss: 0.00082722
Iteration 252/1000 | Loss: 0.00043766
Iteration 253/1000 | Loss: 0.00099132
Iteration 254/1000 | Loss: 0.00027068
Iteration 255/1000 | Loss: 0.00072566
Iteration 256/1000 | Loss: 0.00022920
Iteration 257/1000 | Loss: 0.00025508
Iteration 258/1000 | Loss: 0.00026980
Iteration 259/1000 | Loss: 0.00037120
Iteration 260/1000 | Loss: 0.00012455
Iteration 261/1000 | Loss: 0.00023112
Iteration 262/1000 | Loss: 0.00009374
Iteration 263/1000 | Loss: 0.00091595
Iteration 264/1000 | Loss: 0.00152482
Iteration 265/1000 | Loss: 0.00062292
Iteration 266/1000 | Loss: 0.00033295
Iteration 267/1000 | Loss: 0.00011480
Iteration 268/1000 | Loss: 0.00037845
Iteration 269/1000 | Loss: 0.00008876
Iteration 270/1000 | Loss: 0.00063113
Iteration 271/1000 | Loss: 0.00012053
Iteration 272/1000 | Loss: 0.00009812
Iteration 273/1000 | Loss: 0.00057788
Iteration 274/1000 | Loss: 0.00072883
Iteration 275/1000 | Loss: 0.00044285
Iteration 276/1000 | Loss: 0.00027337
Iteration 277/1000 | Loss: 0.00010373
Iteration 278/1000 | Loss: 0.00008741
Iteration 279/1000 | Loss: 0.00108911
Iteration 280/1000 | Loss: 0.00035440
Iteration 281/1000 | Loss: 0.00048245
Iteration 282/1000 | Loss: 0.00028475
Iteration 283/1000 | Loss: 0.00009436
Iteration 284/1000 | Loss: 0.00008453
Iteration 285/1000 | Loss: 0.00007723
Iteration 286/1000 | Loss: 0.00007408
Iteration 287/1000 | Loss: 0.00059147
Iteration 288/1000 | Loss: 0.00163445
Iteration 289/1000 | Loss: 0.00017775
Iteration 290/1000 | Loss: 0.00011449
Iteration 291/1000 | Loss: 0.00009794
Iteration 292/1000 | Loss: 0.00008376
Iteration 293/1000 | Loss: 0.00023708
Iteration 294/1000 | Loss: 0.00008030
Iteration 295/1000 | Loss: 0.00007524
Iteration 296/1000 | Loss: 0.00022900
Iteration 297/1000 | Loss: 0.00023605
Iteration 298/1000 | Loss: 0.00017183
Iteration 299/1000 | Loss: 0.00016763
Iteration 300/1000 | Loss: 0.00006880
Iteration 301/1000 | Loss: 0.00006454
Iteration 302/1000 | Loss: 0.00006193
Iteration 303/1000 | Loss: 0.00006008
Iteration 304/1000 | Loss: 0.00005812
Iteration 305/1000 | Loss: 0.00005627
Iteration 306/1000 | Loss: 0.00005503
Iteration 307/1000 | Loss: 0.00036697
Iteration 308/1000 | Loss: 0.00037649
Iteration 309/1000 | Loss: 0.00071084
Iteration 310/1000 | Loss: 0.00051999
Iteration 311/1000 | Loss: 0.00033249
Iteration 312/1000 | Loss: 0.00014007
Iteration 313/1000 | Loss: 0.00022882
Iteration 314/1000 | Loss: 0.00029266
Iteration 315/1000 | Loss: 0.00023462
Iteration 316/1000 | Loss: 0.00029356
Iteration 317/1000 | Loss: 0.00008083
Iteration 318/1000 | Loss: 0.00024616
Iteration 319/1000 | Loss: 0.00054988
Iteration 320/1000 | Loss: 0.00020522
Iteration 321/1000 | Loss: 0.00014278
Iteration 322/1000 | Loss: 0.00006959
Iteration 323/1000 | Loss: 0.00023554
Iteration 324/1000 | Loss: 0.00006924
Iteration 325/1000 | Loss: 0.00006417
Iteration 326/1000 | Loss: 0.00025953
Iteration 327/1000 | Loss: 0.00077489
Iteration 328/1000 | Loss: 0.00165925
Iteration 329/1000 | Loss: 0.00047241
Iteration 330/1000 | Loss: 0.00030124
Iteration 331/1000 | Loss: 0.00032870
Iteration 332/1000 | Loss: 0.00017076
Iteration 333/1000 | Loss: 0.00008215
Iteration 334/1000 | Loss: 0.00007337
Iteration 335/1000 | Loss: 0.00007065
Iteration 336/1000 | Loss: 0.00023987
Iteration 337/1000 | Loss: 0.00007658
Iteration 338/1000 | Loss: 0.00006976
Iteration 339/1000 | Loss: 0.00055296
Iteration 340/1000 | Loss: 0.00007349
Iteration 341/1000 | Loss: 0.00017182
Iteration 342/1000 | Loss: 0.00051054
Iteration 343/1000 | Loss: 0.00014610
Iteration 344/1000 | Loss: 0.00021521
Iteration 345/1000 | Loss: 0.00092538
Iteration 346/1000 | Loss: 0.00072702
Iteration 347/1000 | Loss: 0.00054639
Iteration 348/1000 | Loss: 0.00041003
Iteration 349/1000 | Loss: 0.00020091
Iteration 350/1000 | Loss: 0.00022645
Iteration 351/1000 | Loss: 0.00011584
Iteration 352/1000 | Loss: 0.00076645
Iteration 353/1000 | Loss: 0.00013505
Iteration 354/1000 | Loss: 0.00012632
Iteration 355/1000 | Loss: 0.00009293
Iteration 356/1000 | Loss: 0.00031586
Iteration 357/1000 | Loss: 0.00067763
Iteration 358/1000 | Loss: 0.00052680
Iteration 359/1000 | Loss: 0.00024594
Iteration 360/1000 | Loss: 0.00014738
Iteration 361/1000 | Loss: 0.00068396
Iteration 362/1000 | Loss: 0.00068184
Iteration 363/1000 | Loss: 0.00126489
Iteration 364/1000 | Loss: 0.00046490
Iteration 365/1000 | Loss: 0.00065456
Iteration 366/1000 | Loss: 0.00017655
Iteration 367/1000 | Loss: 0.00047044
Iteration 368/1000 | Loss: 0.00069032
Iteration 369/1000 | Loss: 0.00024408
Iteration 370/1000 | Loss: 0.00024739
Iteration 371/1000 | Loss: 0.00009059
Iteration 372/1000 | Loss: 0.00007844
Iteration 373/1000 | Loss: 0.00047984
Iteration 374/1000 | Loss: 0.00029575
Iteration 375/1000 | Loss: 0.00032102
Iteration 376/1000 | Loss: 0.00022355
Iteration 377/1000 | Loss: 0.00040808
Iteration 378/1000 | Loss: 0.00020055
Iteration 379/1000 | Loss: 0.00109263
Iteration 380/1000 | Loss: 0.00073928
Iteration 381/1000 | Loss: 0.00033228
Iteration 382/1000 | Loss: 0.00022465
Iteration 383/1000 | Loss: 0.00022758
Iteration 384/1000 | Loss: 0.00018079
Iteration 385/1000 | Loss: 0.00007546
Iteration 386/1000 | Loss: 0.00031085
Iteration 387/1000 | Loss: 0.00022005
Iteration 388/1000 | Loss: 0.00020522
Iteration 389/1000 | Loss: 0.00006914
Iteration 390/1000 | Loss: 0.00006373
Iteration 391/1000 | Loss: 0.00006093
Iteration 392/1000 | Loss: 0.00021500
Iteration 393/1000 | Loss: 0.00132385
Iteration 394/1000 | Loss: 0.00032281
Iteration 395/1000 | Loss: 0.00009936
Iteration 396/1000 | Loss: 0.00007976
Iteration 397/1000 | Loss: 0.00006939
Iteration 398/1000 | Loss: 0.00022325
Iteration 399/1000 | Loss: 0.00022214
Iteration 400/1000 | Loss: 0.00007246
Iteration 401/1000 | Loss: 0.00006507
Iteration 402/1000 | Loss: 0.00006028
Iteration 403/1000 | Loss: 0.00005814
Iteration 404/1000 | Loss: 0.00052412
Iteration 405/1000 | Loss: 0.00007011
Iteration 406/1000 | Loss: 0.00038207
Iteration 407/1000 | Loss: 0.00054544
Iteration 408/1000 | Loss: 0.00024086
Iteration 409/1000 | Loss: 0.00008093
Iteration 410/1000 | Loss: 0.00040588
Iteration 411/1000 | Loss: 0.00009038
Iteration 412/1000 | Loss: 0.00007397
Iteration 413/1000 | Loss: 0.00008196
Iteration 414/1000 | Loss: 0.00007106
Iteration 415/1000 | Loss: 0.00006132
Iteration 416/1000 | Loss: 0.00005879
Iteration 417/1000 | Loss: 0.00005610
Iteration 418/1000 | Loss: 0.00037244
Iteration 419/1000 | Loss: 0.00006315
Iteration 420/1000 | Loss: 0.00005883
Iteration 421/1000 | Loss: 0.00005429
Iteration 422/1000 | Loss: 0.00005309
Iteration 423/1000 | Loss: 0.00005190
Iteration 424/1000 | Loss: 0.00005050
Iteration 425/1000 | Loss: 0.00035512
Iteration 426/1000 | Loss: 0.00082096
Iteration 427/1000 | Loss: 0.00162936
Iteration 428/1000 | Loss: 0.00104751
Iteration 429/1000 | Loss: 0.00086971
Iteration 430/1000 | Loss: 0.00037902
Iteration 431/1000 | Loss: 0.00007174
Iteration 432/1000 | Loss: 0.00006115
Iteration 433/1000 | Loss: 0.00020157
Iteration 434/1000 | Loss: 0.00093243
Iteration 435/1000 | Loss: 0.00080691
Iteration 436/1000 | Loss: 0.00104216
Iteration 437/1000 | Loss: 0.00071181
Iteration 438/1000 | Loss: 0.00017652
Iteration 439/1000 | Loss: 0.00068879
Iteration 440/1000 | Loss: 0.00048643
Iteration 441/1000 | Loss: 0.00062734
Iteration 442/1000 | Loss: 0.00038027
Iteration 443/1000 | Loss: 0.00036336
Iteration 444/1000 | Loss: 0.00031203
Iteration 445/1000 | Loss: 0.00013665
Iteration 446/1000 | Loss: 0.00030039
Iteration 447/1000 | Loss: 0.00042896
Iteration 448/1000 | Loss: 0.00016609
Iteration 449/1000 | Loss: 0.00019135
Iteration 450/1000 | Loss: 0.00058642
Iteration 451/1000 | Loss: 0.00020164
Iteration 452/1000 | Loss: 0.00024383
Iteration 453/1000 | Loss: 0.00020932
Iteration 454/1000 | Loss: 0.00029864
Iteration 455/1000 | Loss: 0.00016804
Iteration 456/1000 | Loss: 0.00037556
Iteration 457/1000 | Loss: 0.00027746
Iteration 458/1000 | Loss: 0.00008404
Iteration 459/1000 | Loss: 0.00061667
Iteration 460/1000 | Loss: 0.00028813
Iteration 461/1000 | Loss: 0.00080180
Iteration 462/1000 | Loss: 0.00026491
Iteration 463/1000 | Loss: 0.00009991
Iteration 464/1000 | Loss: 0.00048395
Iteration 465/1000 | Loss: 0.00054164
Iteration 466/1000 | Loss: 0.00008210
Iteration 467/1000 | Loss: 0.00077751
Iteration 468/1000 | Loss: 0.00008471
Iteration 469/1000 | Loss: 0.00026681
Iteration 470/1000 | Loss: 0.00051290
Iteration 471/1000 | Loss: 0.00062150
Iteration 472/1000 | Loss: 0.00061217
Iteration 473/1000 | Loss: 0.00052729
Iteration 474/1000 | Loss: 0.00114390
Iteration 475/1000 | Loss: 0.00051915
Iteration 476/1000 | Loss: 0.00010248
Iteration 477/1000 | Loss: 0.00007563
Iteration 478/1000 | Loss: 0.00014906
Iteration 479/1000 | Loss: 0.00022227
Iteration 480/1000 | Loss: 0.00007111
Iteration 481/1000 | Loss: 0.00006457
Iteration 482/1000 | Loss: 0.00021237
Iteration 483/1000 | Loss: 0.00006347
Iteration 484/1000 | Loss: 0.00021405
Iteration 485/1000 | Loss: 0.00021362
Iteration 486/1000 | Loss: 0.00060841
Iteration 487/1000 | Loss: 0.00095156
Iteration 488/1000 | Loss: 0.00043826
Iteration 489/1000 | Loss: 0.00008253
Iteration 490/1000 | Loss: 0.00012456
Iteration 491/1000 | Loss: 0.00028448
Iteration 492/1000 | Loss: 0.00018898
Iteration 493/1000 | Loss: 0.00008036
Iteration 494/1000 | Loss: 0.00006162
Iteration 495/1000 | Loss: 0.00036580
Iteration 496/1000 | Loss: 0.00007870
Iteration 497/1000 | Loss: 0.00005979
Iteration 498/1000 | Loss: 0.00021051
Iteration 499/1000 | Loss: 0.00022897
Iteration 500/1000 | Loss: 0.00016860
Iteration 501/1000 | Loss: 0.00013626
Iteration 502/1000 | Loss: 0.00005902
Iteration 503/1000 | Loss: 0.00005331
Iteration 504/1000 | Loss: 0.00021854
Iteration 505/1000 | Loss: 0.00005825
Iteration 506/1000 | Loss: 0.00005272
Iteration 507/1000 | Loss: 0.00018750
Iteration 508/1000 | Loss: 0.00019677
Iteration 509/1000 | Loss: 0.00011766
Iteration 510/1000 | Loss: 0.00017034
Iteration 511/1000 | Loss: 0.00015893
Iteration 512/1000 | Loss: 0.00015773
Iteration 513/1000 | Loss: 0.00013398
Iteration 514/1000 | Loss: 0.00015121
Iteration 515/1000 | Loss: 0.00012250
Iteration 516/1000 | Loss: 0.00021029
Iteration 517/1000 | Loss: 0.00045234
Iteration 518/1000 | Loss: 0.00023118
Iteration 519/1000 | Loss: 0.00023656
Iteration 520/1000 | Loss: 0.00005611
Iteration 521/1000 | Loss: 0.00005032
Iteration 522/1000 | Loss: 0.00021757
Iteration 523/1000 | Loss: 0.00015244
Iteration 524/1000 | Loss: 0.00019988
Iteration 525/1000 | Loss: 0.00012348
Iteration 526/1000 | Loss: 0.00004982
Iteration 527/1000 | Loss: 0.00051516
Iteration 528/1000 | Loss: 0.00060563
Iteration 529/1000 | Loss: 0.00015578
Iteration 530/1000 | Loss: 0.00006643
Iteration 531/1000 | Loss: 0.00052324
Iteration 532/1000 | Loss: 0.00022707
Iteration 533/1000 | Loss: 0.00007833
Iteration 534/1000 | Loss: 0.00006404
Iteration 535/1000 | Loss: 0.00020714
Iteration 536/1000 | Loss: 0.00017489
Iteration 537/1000 | Loss: 0.00005604
Iteration 538/1000 | Loss: 0.00005388
Iteration 539/1000 | Loss: 0.00036375
Iteration 540/1000 | Loss: 0.00015581
Iteration 541/1000 | Loss: 0.00021085
Iteration 542/1000 | Loss: 0.00055267
Iteration 543/1000 | Loss: 0.00006670
Iteration 544/1000 | Loss: 0.00005967
Iteration 545/1000 | Loss: 0.00020335
Iteration 546/1000 | Loss: 0.00021692
Iteration 547/1000 | Loss: 0.00101793
Iteration 548/1000 | Loss: 0.00008053
Iteration 549/1000 | Loss: 0.00038528
Iteration 550/1000 | Loss: 0.00015878
Iteration 551/1000 | Loss: 0.00015886
Iteration 552/1000 | Loss: 0.00010761
Iteration 553/1000 | Loss: 0.00006447
Iteration 554/1000 | Loss: 0.00016235
Iteration 555/1000 | Loss: 0.00006640
Iteration 556/1000 | Loss: 0.00011543
Iteration 557/1000 | Loss: 0.00036163
Iteration 558/1000 | Loss: 0.00024496
Iteration 559/1000 | Loss: 0.00034717
Iteration 560/1000 | Loss: 0.00034829
Iteration 561/1000 | Loss: 0.00037600
Iteration 562/1000 | Loss: 0.00007100
Iteration 563/1000 | Loss: 0.00006130
Iteration 564/1000 | Loss: 0.00005691
Iteration 565/1000 | Loss: 0.00005421
Iteration 566/1000 | Loss: 0.00005148
Iteration 567/1000 | Loss: 0.00036747
Iteration 568/1000 | Loss: 0.00005891
Iteration 569/1000 | Loss: 0.00005426
Iteration 570/1000 | Loss: 0.00022574
Iteration 571/1000 | Loss: 0.00005254
Iteration 572/1000 | Loss: 0.00005032
Iteration 573/1000 | Loss: 0.00004834
Iteration 574/1000 | Loss: 0.00004742
Iteration 575/1000 | Loss: 0.00019724
Iteration 576/1000 | Loss: 0.00065557
Iteration 577/1000 | Loss: 0.00104373
Iteration 578/1000 | Loss: 0.00010755
Iteration 579/1000 | Loss: 0.00007834
Iteration 580/1000 | Loss: 0.00006630
Iteration 581/1000 | Loss: 0.00005845
Iteration 582/1000 | Loss: 0.00005473
Iteration 583/1000 | Loss: 0.00064701
Iteration 584/1000 | Loss: 0.00022110
Iteration 585/1000 | Loss: 0.00018392
Iteration 586/1000 | Loss: 0.00012108
Iteration 587/1000 | Loss: 0.00056316
Iteration 588/1000 | Loss: 0.00014110
Iteration 589/1000 | Loss: 0.00010629
Iteration 590/1000 | Loss: 0.00006686
Iteration 591/1000 | Loss: 0.00025364
Iteration 592/1000 | Loss: 0.00015325
Iteration 593/1000 | Loss: 0.00066473
Iteration 594/1000 | Loss: 0.00054767
Iteration 595/1000 | Loss: 0.00023943
Iteration 596/1000 | Loss: 0.00006526
Iteration 597/1000 | Loss: 0.00005847
Iteration 598/1000 | Loss: 0.00079575
Iteration 599/1000 | Loss: 0.00053914
Iteration 600/1000 | Loss: 0.00057104
Iteration 601/1000 | Loss: 0.00043487
Iteration 602/1000 | Loss: 0.00049775
Iteration 603/1000 | Loss: 0.00017324
Iteration 604/1000 | Loss: 0.00008954
Iteration 605/1000 | Loss: 0.00018623
Iteration 606/1000 | Loss: 0.00018160
Iteration 607/1000 | Loss: 0.00026585
Iteration 608/1000 | Loss: 0.00007898
Iteration 609/1000 | Loss: 0.00012419
Iteration 610/1000 | Loss: 0.00039650
Iteration 611/1000 | Loss: 0.00007917
Iteration 612/1000 | Loss: 0.00057321
Iteration 613/1000 | Loss: 0.00043006
Iteration 614/1000 | Loss: 0.00025022
Iteration 615/1000 | Loss: 0.00035088
Iteration 616/1000 | Loss: 0.00014456
Iteration 617/1000 | Loss: 0.00027351
Iteration 618/1000 | Loss: 0.00024398
Iteration 619/1000 | Loss: 0.00011923
Iteration 620/1000 | Loss: 0.00014015
Iteration 621/1000 | Loss: 0.00010904
Iteration 622/1000 | Loss: 0.00038152
Iteration 623/1000 | Loss: 0.00023806
Iteration 624/1000 | Loss: 0.00018816
Iteration 625/1000 | Loss: 0.00006893
Iteration 626/1000 | Loss: 0.00005321
Iteration 627/1000 | Loss: 0.00005161
Iteration 628/1000 | Loss: 0.00005002
Iteration 629/1000 | Loss: 0.00020169
Iteration 630/1000 | Loss: 0.00005174
Iteration 631/1000 | Loss: 0.00004843
Iteration 632/1000 | Loss: 0.00004720
Iteration 633/1000 | Loss: 0.00053222
Iteration 634/1000 | Loss: 0.00022299
Iteration 635/1000 | Loss: 0.00006580
Iteration 636/1000 | Loss: 0.00021688
Iteration 637/1000 | Loss: 0.00051823
Iteration 638/1000 | Loss: 0.00039156
Iteration 639/1000 | Loss: 0.00042937
Iteration 640/1000 | Loss: 0.00031763
Iteration 641/1000 | Loss: 0.00020787
Iteration 642/1000 | Loss: 0.00018929
Iteration 643/1000 | Loss: 0.00010337
Iteration 644/1000 | Loss: 0.00017724
Iteration 645/1000 | Loss: 0.00015492
Iteration 646/1000 | Loss: 0.00030139
Iteration 647/1000 | Loss: 0.00018287
Iteration 648/1000 | Loss: 0.00014310
Iteration 649/1000 | Loss: 0.00015502
Iteration 650/1000 | Loss: 0.00014164
Iteration 651/1000 | Loss: 0.00015439
Iteration 652/1000 | Loss: 0.00014795
Iteration 653/1000 | Loss: 0.00011593
Iteration 654/1000 | Loss: 0.00016075
Iteration 655/1000 | Loss: 0.00014238
Iteration 656/1000 | Loss: 0.00031624
Iteration 657/1000 | Loss: 0.00033722
Iteration 658/1000 | Loss: 0.00018506
Iteration 659/1000 | Loss: 0.00005834
Iteration 660/1000 | Loss: 0.00005363
Iteration 661/1000 | Loss: 0.00021133
Iteration 662/1000 | Loss: 0.00067468
Iteration 663/1000 | Loss: 0.00033835
Iteration 664/1000 | Loss: 0.00034553
Iteration 665/1000 | Loss: 0.00007413
Iteration 666/1000 | Loss: 0.00025141
Iteration 667/1000 | Loss: 0.00038237
Iteration 668/1000 | Loss: 0.00041819
Iteration 669/1000 | Loss: 0.00024960
Iteration 670/1000 | Loss: 0.00006029
Iteration 671/1000 | Loss: 0.00016771
Iteration 672/1000 | Loss: 0.00006107
Iteration 673/1000 | Loss: 0.00021058
Iteration 674/1000 | Loss: 0.00010638
Iteration 675/1000 | Loss: 0.00041700
Iteration 676/1000 | Loss: 0.00006374
Iteration 677/1000 | Loss: 0.00005120
Iteration 678/1000 | Loss: 0.00005518
Iteration 679/1000 | Loss: 0.00004761
Iteration 680/1000 | Loss: 0.00004829
Iteration 681/1000 | Loss: 0.00004508
Iteration 682/1000 | Loss: 0.00004422
Iteration 683/1000 | Loss: 0.00020295
Iteration 684/1000 | Loss: 0.00004624
Iteration 685/1000 | Loss: 0.00004405
Iteration 686/1000 | Loss: 0.00004258
Iteration 687/1000 | Loss: 0.00004181
Iteration 688/1000 | Loss: 0.00019732
Iteration 689/1000 | Loss: 0.00004590
Iteration 690/1000 | Loss: 0.00004378
Iteration 691/1000 | Loss: 0.00004161
Iteration 692/1000 | Loss: 0.00004077
Iteration 693/1000 | Loss: 0.00034350
Iteration 694/1000 | Loss: 0.00020863
Iteration 695/1000 | Loss: 0.00016349
Iteration 696/1000 | Loss: 0.00009696
Iteration 697/1000 | Loss: 0.00011197
Iteration 698/1000 | Loss: 0.00018766
Iteration 699/1000 | Loss: 0.00013202
Iteration 700/1000 | Loss: 0.00018577
Iteration 701/1000 | Loss: 0.00021609
Iteration 702/1000 | Loss: 0.00022709
Iteration 703/1000 | Loss: 0.00021372
Iteration 704/1000 | Loss: 0.00098845
Iteration 705/1000 | Loss: 0.00066327
Iteration 706/1000 | Loss: 0.00109589
Iteration 707/1000 | Loss: 0.00015474
Iteration 708/1000 | Loss: 0.00005232
Iteration 709/1000 | Loss: 0.00004699
Iteration 710/1000 | Loss: 0.00035151
Iteration 711/1000 | Loss: 0.00020319
Iteration 712/1000 | Loss: 0.00038102
Iteration 713/1000 | Loss: 0.00028210
Iteration 714/1000 | Loss: 0.00026969
Iteration 715/1000 | Loss: 0.00014308
Iteration 716/1000 | Loss: 0.00012785
Iteration 717/1000 | Loss: 0.00005107
Iteration 718/1000 | Loss: 0.00004740
Iteration 719/1000 | Loss: 0.00017235
Iteration 720/1000 | Loss: 0.00014413
Iteration 721/1000 | Loss: 0.00008741
Iteration 722/1000 | Loss: 0.00046426
Iteration 723/1000 | Loss: 0.00089675
Iteration 724/1000 | Loss: 0.00011939
Iteration 725/1000 | Loss: 0.00007567
Iteration 726/1000 | Loss: 0.00006216
Iteration 727/1000 | Loss: 0.00021282
Iteration 728/1000 | Loss: 0.00039198
Iteration 729/1000 | Loss: 0.00053327
Iteration 730/1000 | Loss: 0.00053628
Iteration 731/1000 | Loss: 0.00071224
Iteration 732/1000 | Loss: 0.00038699
Iteration 733/1000 | Loss: 0.00017747
Iteration 734/1000 | Loss: 0.00010315
Iteration 735/1000 | Loss: 0.00013248
Iteration 736/1000 | Loss: 0.00006476
Iteration 737/1000 | Loss: 0.00022748
Iteration 738/1000 | Loss: 0.00006201
Iteration 739/1000 | Loss: 0.00032948
Iteration 740/1000 | Loss: 0.00011232
Iteration 741/1000 | Loss: 0.00021350
Iteration 742/1000 | Loss: 0.00037021
Iteration 743/1000 | Loss: 0.00011540
Iteration 744/1000 | Loss: 0.00005973
Iteration 745/1000 | Loss: 0.00005460
Iteration 746/1000 | Loss: 0.00020225
Iteration 747/1000 | Loss: 0.00021282
Iteration 748/1000 | Loss: 0.00011826
Iteration 749/1000 | Loss: 0.00005290
Iteration 750/1000 | Loss: 0.00004970
Iteration 751/1000 | Loss: 0.00004791
Iteration 752/1000 | Loss: 0.00028745
Iteration 753/1000 | Loss: 0.00009928
Iteration 754/1000 | Loss: 0.00019954
Iteration 755/1000 | Loss: 0.00014110
Iteration 756/1000 | Loss: 0.00006096
Iteration 757/1000 | Loss: 0.00005128
Iteration 758/1000 | Loss: 0.00019604
Iteration 759/1000 | Loss: 0.00038224
Iteration 760/1000 | Loss: 0.00005331
Iteration 761/1000 | Loss: 0.00004801
Iteration 762/1000 | Loss: 0.00004532
Iteration 763/1000 | Loss: 0.00004423
Iteration 764/1000 | Loss: 0.00019292
Iteration 765/1000 | Loss: 0.00004784
Iteration 766/1000 | Loss: 0.00004542
Iteration 767/1000 | Loss: 0.00067896
Iteration 768/1000 | Loss: 0.00037781
Iteration 769/1000 | Loss: 0.00025912
Iteration 770/1000 | Loss: 0.00013138
Iteration 771/1000 | Loss: 0.00009862
Iteration 772/1000 | Loss: 0.00046737
Iteration 773/1000 | Loss: 0.00034019
Iteration 774/1000 | Loss: 0.00006849
Iteration 775/1000 | Loss: 0.00005471
Iteration 776/1000 | Loss: 0.00004781
Iteration 777/1000 | Loss: 0.00020379
Iteration 778/1000 | Loss: 0.00004714
Iteration 779/1000 | Loss: 0.00020273
Iteration 780/1000 | Loss: 0.00005515
Iteration 781/1000 | Loss: 0.00004673
Iteration 782/1000 | Loss: 0.00022248
Iteration 783/1000 | Loss: 0.00005163
Iteration 784/1000 | Loss: 0.00004580
Iteration 785/1000 | Loss: 0.00004348
Iteration 786/1000 | Loss: 0.00019398
Iteration 787/1000 | Loss: 0.00020533
Iteration 788/1000 | Loss: 0.00016238
Iteration 789/1000 | Loss: 0.00008984
Iteration 790/1000 | Loss: 0.00005985
Iteration 791/1000 | Loss: 0.00012467
Iteration 792/1000 | Loss: 0.00016454
Iteration 793/1000 | Loss: 0.00004632
Iteration 794/1000 | Loss: 0.00004462
Iteration 795/1000 | Loss: 0.00027091
Iteration 796/1000 | Loss: 0.00035066
Iteration 797/1000 | Loss: 0.00028842
Iteration 798/1000 | Loss: 0.00010746
Iteration 799/1000 | Loss: 0.00019596
Iteration 800/1000 | Loss: 0.00025857
Iteration 801/1000 | Loss: 0.00060414
Iteration 802/1000 | Loss: 0.00041945
Iteration 803/1000 | Loss: 0.00012648
Iteration 804/1000 | Loss: 0.00025142
Iteration 805/1000 | Loss: 0.00023268
Iteration 806/1000 | Loss: 0.00050023
Iteration 807/1000 | Loss: 0.00007334
Iteration 808/1000 | Loss: 0.00005784
Iteration 809/1000 | Loss: 0.00045406
Iteration 810/1000 | Loss: 0.00006958
Iteration 811/1000 | Loss: 0.00020849
Iteration 812/1000 | Loss: 0.00021798
Iteration 813/1000 | Loss: 0.00005439
Iteration 814/1000 | Loss: 0.00004819
Iteration 815/1000 | Loss: 0.00006626
Iteration 816/1000 | Loss: 0.00004768
Iteration 817/1000 | Loss: 0.00004447
Iteration 818/1000 | Loss: 0.00035264
Iteration 819/1000 | Loss: 0.00036196
Iteration 820/1000 | Loss: 0.00287666
Iteration 821/1000 | Loss: 0.00088174
Iteration 822/1000 | Loss: 0.00013198
Iteration 823/1000 | Loss: 0.00007845
Iteration 824/1000 | Loss: 0.00027001
Iteration 825/1000 | Loss: 0.00006729
Iteration 826/1000 | Loss: 0.00060999
Iteration 827/1000 | Loss: 0.00006007
Iteration 828/1000 | Loss: 0.00005938
Iteration 829/1000 | Loss: 0.00005065
Iteration 830/1000 | Loss: 0.00008013
Iteration 831/1000 | Loss: 0.00052977
Iteration 832/1000 | Loss: 0.00047833
Iteration 833/1000 | Loss: 0.00022253
Iteration 834/1000 | Loss: 0.00024943
Iteration 835/1000 | Loss: 0.00025381
Iteration 836/1000 | Loss: 0.00018614
Iteration 837/1000 | Loss: 0.00031528
Iteration 838/1000 | Loss: 0.00018171
Iteration 839/1000 | Loss: 0.00018849
Iteration 840/1000 | Loss: 0.00035029
Iteration 841/1000 | Loss: 0.00041439
Iteration 842/1000 | Loss: 0.00028665
Iteration 843/1000 | Loss: 0.00007460
Iteration 844/1000 | Loss: 0.00011798
Iteration 845/1000 | Loss: 0.00015467
Iteration 846/1000 | Loss: 0.00012102
Iteration 847/1000 | Loss: 0.00029599
Iteration 848/1000 | Loss: 0.00013215
Iteration 849/1000 | Loss: 0.00023912
Iteration 850/1000 | Loss: 0.00018161
Iteration 851/1000 | Loss: 0.00019093
Iteration 852/1000 | Loss: 0.00011741
Iteration 853/1000 | Loss: 0.00009856
Iteration 854/1000 | Loss: 0.00044421
Iteration 855/1000 | Loss: 0.00026932
Iteration 856/1000 | Loss: 0.00040957
Iteration 857/1000 | Loss: 0.00014188
Iteration 858/1000 | Loss: 0.00009246
Iteration 859/1000 | Loss: 0.00008770
Iteration 860/1000 | Loss: 0.00020878
Iteration 861/1000 | Loss: 0.00043137
Iteration 862/1000 | Loss: 0.00026649
Iteration 863/1000 | Loss: 0.00024681
Iteration 864/1000 | Loss: 0.00019912
Iteration 865/1000 | Loss: 0.00013238
Iteration 866/1000 | Loss: 0.00018179
Iteration 867/1000 | Loss: 0.00028576
Iteration 868/1000 | Loss: 0.00039455
Iteration 869/1000 | Loss: 0.00085089
Iteration 870/1000 | Loss: 0.00012974
Iteration 871/1000 | Loss: 0.00007010
Iteration 872/1000 | Loss: 0.00007465
Iteration 873/1000 | Loss: 0.00006078
Iteration 874/1000 | Loss: 0.00007307
Iteration 875/1000 | Loss: 0.00005398
Iteration 876/1000 | Loss: 0.00050818
Iteration 877/1000 | Loss: 0.00054879
Iteration 878/1000 | Loss: 0.00031835
Iteration 879/1000 | Loss: 0.00007408
Iteration 880/1000 | Loss: 0.00024605
Iteration 881/1000 | Loss: 0.00021893
Iteration 882/1000 | Loss: 0.00024498
Iteration 883/1000 | Loss: 0.00012577
Iteration 884/1000 | Loss: 0.00011415
Iteration 885/1000 | Loss: 0.00004918
Iteration 886/1000 | Loss: 0.00006441
Iteration 887/1000 | Loss: 0.00006153
Iteration 888/1000 | Loss: 0.00004418
Iteration 889/1000 | Loss: 0.00042159
Iteration 890/1000 | Loss: 0.00032157
Iteration 891/1000 | Loss: 0.00015955
Iteration 892/1000 | Loss: 0.00016279
Iteration 893/1000 | Loss: 0.00005090
Iteration 894/1000 | Loss: 0.00004556
Iteration 895/1000 | Loss: 0.00006249
Iteration 896/1000 | Loss: 0.00004687
Iteration 897/1000 | Loss: 0.00004115
Iteration 898/1000 | Loss: 0.00003905
Iteration 899/1000 | Loss: 0.00003769
Iteration 900/1000 | Loss: 0.00003683
Iteration 901/1000 | Loss: 0.00003571
Iteration 902/1000 | Loss: 0.00003454
Iteration 903/1000 | Loss: 0.00003365
Iteration 904/1000 | Loss: 0.00003292
Iteration 905/1000 | Loss: 0.00019154
Iteration 906/1000 | Loss: 0.00034505
Iteration 907/1000 | Loss: 0.00021192
Iteration 908/1000 | Loss: 0.00040546
Iteration 909/1000 | Loss: 0.00038535
Iteration 910/1000 | Loss: 0.00009658
Iteration 911/1000 | Loss: 0.00013594
Iteration 912/1000 | Loss: 0.00004783
Iteration 913/1000 | Loss: 0.00003913
Iteration 914/1000 | Loss: 0.00003545
Iteration 915/1000 | Loss: 0.00003361
Iteration 916/1000 | Loss: 0.00016972
Iteration 917/1000 | Loss: 0.00023396
Iteration 918/1000 | Loss: 0.00015744
Iteration 919/1000 | Loss: 0.00003714
Iteration 920/1000 | Loss: 0.00003547
Iteration 921/1000 | Loss: 0.00017933
Iteration 922/1000 | Loss: 0.00017615
Iteration 923/1000 | Loss: 0.00008190
Iteration 924/1000 | Loss: 0.00012722
Iteration 925/1000 | Loss: 0.00007101
Iteration 926/1000 | Loss: 0.00003871
Iteration 927/1000 | Loss: 0.00019308
Iteration 928/1000 | Loss: 0.00020102
Iteration 929/1000 | Loss: 0.00030826
Iteration 930/1000 | Loss: 0.00007859
Iteration 931/1000 | Loss: 0.00003902
Iteration 932/1000 | Loss: 0.00034805
Iteration 933/1000 | Loss: 0.00004223
Iteration 934/1000 | Loss: 0.00003731
Iteration 935/1000 | Loss: 0.00005340
Iteration 936/1000 | Loss: 0.00004604
Iteration 937/1000 | Loss: 0.00003667
Iteration 938/1000 | Loss: 0.00003453
Iteration 939/1000 | Loss: 0.00003359
Iteration 940/1000 | Loss: 0.00034840
Iteration 941/1000 | Loss: 0.00019531
Iteration 942/1000 | Loss: 0.00014983
Iteration 943/1000 | Loss: 0.00005839
Iteration 944/1000 | Loss: 0.00004703
Iteration 945/1000 | Loss: 0.00018978
Iteration 946/1000 | Loss: 0.00004057
Iteration 947/1000 | Loss: 0.00004741
Iteration 948/1000 | Loss: 0.00003733
Iteration 949/1000 | Loss: 0.00003961
Iteration 950/1000 | Loss: 0.00003466
Iteration 951/1000 | Loss: 0.00003403
Iteration 952/1000 | Loss: 0.00003375
Iteration 953/1000 | Loss: 0.00033676
Iteration 954/1000 | Loss: 0.00004091
Iteration 955/1000 | Loss: 0.00003637
Iteration 956/1000 | Loss: 0.00003443
Iteration 957/1000 | Loss: 0.00018490
Iteration 958/1000 | Loss: 0.00034683
Iteration 959/1000 | Loss: 0.00020336
Iteration 960/1000 | Loss: 0.00020076
Iteration 961/1000 | Loss: 0.00022128
Iteration 962/1000 | Loss: 0.00004927
Iteration 963/1000 | Loss: 0.00004370
Iteration 964/1000 | Loss: 0.00019926
Iteration 965/1000 | Loss: 0.00004453
Iteration 966/1000 | Loss: 0.00004196
Iteration 967/1000 | Loss: 0.00019673
Iteration 968/1000 | Loss: 0.00004202
Iteration 969/1000 | Loss: 0.00019722
Iteration 970/1000 | Loss: 0.00046993
Iteration 971/1000 | Loss: 0.00007887
Iteration 972/1000 | Loss: 0.00005063
Iteration 973/1000 | Loss: 0.00004398
Iteration 974/1000 | Loss: 0.00004096
Iteration 975/1000 | Loss: 0.00018903
Iteration 976/1000 | Loss: 0.00020665
Iteration 977/1000 | Loss: 0.00016306
Iteration 978/1000 | Loss: 0.00004155
Iteration 979/1000 | Loss: 0.00003873
Iteration 980/1000 | Loss: 0.00003688
Iteration 981/1000 | Loss: 0.00003582
Iteration 982/1000 | Loss: 0.00003477
Iteration 983/1000 | Loss: 0.00003367
Iteration 984/1000 | Loss: 0.00003279
Iteration 985/1000 | Loss: 0.00019808
Iteration 986/1000 | Loss: 0.00003766
Iteration 987/1000 | Loss: 0.00003510
Iteration 988/1000 | Loss: 0.00018236
Iteration 989/1000 | Loss: 0.00003615
Iteration 990/1000 | Loss: 0.00003425
Iteration 991/1000 | Loss: 0.00019156
Iteration 992/1000 | Loss: 0.00003639
Iteration 993/1000 | Loss: 0.00003403
Iteration 994/1000 | Loss: 0.00003288
Iteration 995/1000 | Loss: 0.00003231
Iteration 996/1000 | Loss: 0.00003137
Iteration 997/1000 | Loss: 0.00081356
Iteration 998/1000 | Loss: 0.00020722
Iteration 999/1000 | Loss: 0.00004882
Iteration 1000/1000 | Loss: 0.00004245

Optimization complete. Final v2v error: 4.083362102508545 mm

Highest mean error: 12.34219741821289 mm for frame 116

Lowest mean error: 3.5074515342712402 mm for frame 126

Saving results

Total time: 1671.057314157486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864534
Iteration 2/25 | Loss: 0.00101321
Iteration 3/25 | Loss: 0.00081067
Iteration 4/25 | Loss: 0.00075408
Iteration 5/25 | Loss: 0.00074227
Iteration 6/25 | Loss: 0.00074115
Iteration 7/25 | Loss: 0.00074115
Iteration 8/25 | Loss: 0.00074115
Iteration 9/25 | Loss: 0.00074115
Iteration 10/25 | Loss: 0.00074115
Iteration 11/25 | Loss: 0.00074115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007411518017761409, 0.0007411518017761409, 0.0007411518017761409, 0.0007411518017761409, 0.0007411518017761409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007411518017761409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03371215
Iteration 2/25 | Loss: 0.00035428
Iteration 3/25 | Loss: 0.00035426
Iteration 4/25 | Loss: 0.00035426
Iteration 5/25 | Loss: 0.00035426
Iteration 6/25 | Loss: 0.00035426
Iteration 7/25 | Loss: 0.00035426
Iteration 8/25 | Loss: 0.00035426
Iteration 9/25 | Loss: 0.00035426
Iteration 10/25 | Loss: 0.00035426
Iteration 11/25 | Loss: 0.00035426
Iteration 12/25 | Loss: 0.00035426
Iteration 13/25 | Loss: 0.00035426
Iteration 14/25 | Loss: 0.00035426
Iteration 15/25 | Loss: 0.00035426
Iteration 16/25 | Loss: 0.00035426
Iteration 17/25 | Loss: 0.00035426
Iteration 18/25 | Loss: 0.00035426
Iteration 19/25 | Loss: 0.00035426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003542609338182956, 0.0003542609338182956, 0.0003542609338182956, 0.0003542609338182956, 0.0003542609338182956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003542609338182956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035426
Iteration 2/1000 | Loss: 0.00005410
Iteration 3/1000 | Loss: 0.00004027
Iteration 4/1000 | Loss: 0.00003706
Iteration 5/1000 | Loss: 0.00003438
Iteration 6/1000 | Loss: 0.00003238
Iteration 7/1000 | Loss: 0.00003106
Iteration 8/1000 | Loss: 0.00003010
Iteration 9/1000 | Loss: 0.00002971
Iteration 10/1000 | Loss: 0.00002940
Iteration 11/1000 | Loss: 0.00002914
Iteration 12/1000 | Loss: 0.00002909
Iteration 13/1000 | Loss: 0.00002903
Iteration 14/1000 | Loss: 0.00002890
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00002884
Iteration 17/1000 | Loss: 0.00002884
Iteration 18/1000 | Loss: 0.00002883
Iteration 19/1000 | Loss: 0.00002883
Iteration 20/1000 | Loss: 0.00002883
Iteration 21/1000 | Loss: 0.00002883
Iteration 22/1000 | Loss: 0.00002883
Iteration 23/1000 | Loss: 0.00002883
Iteration 24/1000 | Loss: 0.00002883
Iteration 25/1000 | Loss: 0.00002883
Iteration 26/1000 | Loss: 0.00002883
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00002883
Iteration 29/1000 | Loss: 0.00002883
Iteration 30/1000 | Loss: 0.00002883
Iteration 31/1000 | Loss: 0.00002883
Iteration 32/1000 | Loss: 0.00002883
Iteration 33/1000 | Loss: 0.00002883
Iteration 34/1000 | Loss: 0.00002883
Iteration 35/1000 | Loss: 0.00002883
Iteration 36/1000 | Loss: 0.00002883
Iteration 37/1000 | Loss: 0.00002883
Iteration 38/1000 | Loss: 0.00002883
Iteration 39/1000 | Loss: 0.00002883
Iteration 40/1000 | Loss: 0.00002883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [2.8827202186221257e-05, 2.8827202186221257e-05, 2.8827202186221257e-05, 2.8827202186221257e-05, 2.8827202186221257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8827202186221257e-05

Optimization complete. Final v2v error: 4.495218753814697 mm

Highest mean error: 4.871890068054199 mm for frame 239

Lowest mean error: 4.129115581512451 mm for frame 1

Saving results

Total time: 31.779943227767944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415579
Iteration 2/25 | Loss: 0.00086918
Iteration 3/25 | Loss: 0.00065485
Iteration 4/25 | Loss: 0.00062371
Iteration 5/25 | Loss: 0.00061780
Iteration 6/25 | Loss: 0.00061644
Iteration 7/25 | Loss: 0.00061625
Iteration 8/25 | Loss: 0.00061625
Iteration 9/25 | Loss: 0.00061625
Iteration 10/25 | Loss: 0.00061625
Iteration 11/25 | Loss: 0.00061625
Iteration 12/25 | Loss: 0.00061625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006162471836432815, 0.0006162471836432815, 0.0006162471836432815, 0.0006162471836432815, 0.0006162471836432815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006162471836432815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44110250
Iteration 2/25 | Loss: 0.00036563
Iteration 3/25 | Loss: 0.00036563
Iteration 4/25 | Loss: 0.00036563
Iteration 5/25 | Loss: 0.00036563
Iteration 6/25 | Loss: 0.00036563
Iteration 7/25 | Loss: 0.00036563
Iteration 8/25 | Loss: 0.00036563
Iteration 9/25 | Loss: 0.00036563
Iteration 10/25 | Loss: 0.00036563
Iteration 11/25 | Loss: 0.00036563
Iteration 12/25 | Loss: 0.00036563
Iteration 13/25 | Loss: 0.00036563
Iteration 14/25 | Loss: 0.00036563
Iteration 15/25 | Loss: 0.00036563
Iteration 16/25 | Loss: 0.00036563
Iteration 17/25 | Loss: 0.00036563
Iteration 18/25 | Loss: 0.00036563
Iteration 19/25 | Loss: 0.00036563
Iteration 20/25 | Loss: 0.00036563
Iteration 21/25 | Loss: 0.00036563
Iteration 22/25 | Loss: 0.00036563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003656269982457161, 0.0003656269982457161, 0.0003656269982457161, 0.0003656269982457161, 0.0003656269982457161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003656269982457161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036563
Iteration 2/1000 | Loss: 0.00002701
Iteration 3/1000 | Loss: 0.00001433
Iteration 4/1000 | Loss: 0.00001304
Iteration 5/1000 | Loss: 0.00001229
Iteration 6/1000 | Loss: 0.00001189
Iteration 7/1000 | Loss: 0.00001160
Iteration 8/1000 | Loss: 0.00001142
Iteration 9/1000 | Loss: 0.00001139
Iteration 10/1000 | Loss: 0.00001137
Iteration 11/1000 | Loss: 0.00001126
Iteration 12/1000 | Loss: 0.00001122
Iteration 13/1000 | Loss: 0.00001122
Iteration 14/1000 | Loss: 0.00001119
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001114
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001112
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001111
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001108
Iteration 31/1000 | Loss: 0.00001107
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001102
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001101
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001097
Iteration 55/1000 | Loss: 0.00001097
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001092
Iteration 76/1000 | Loss: 0.00001092
Iteration 77/1000 | Loss: 0.00001092
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001092
Iteration 82/1000 | Loss: 0.00001092
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001091
Iteration 87/1000 | Loss: 0.00001091
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001090
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001088
Iteration 98/1000 | Loss: 0.00001088
Iteration 99/1000 | Loss: 0.00001088
Iteration 100/1000 | Loss: 0.00001088
Iteration 101/1000 | Loss: 0.00001088
Iteration 102/1000 | Loss: 0.00001087
Iteration 103/1000 | Loss: 0.00001087
Iteration 104/1000 | Loss: 0.00001086
Iteration 105/1000 | Loss: 0.00001086
Iteration 106/1000 | Loss: 0.00001086
Iteration 107/1000 | Loss: 0.00001086
Iteration 108/1000 | Loss: 0.00001086
Iteration 109/1000 | Loss: 0.00001086
Iteration 110/1000 | Loss: 0.00001086
Iteration 111/1000 | Loss: 0.00001086
Iteration 112/1000 | Loss: 0.00001086
Iteration 113/1000 | Loss: 0.00001086
Iteration 114/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.0863681382033974e-05, 1.0863681382033974e-05, 1.0863681382033974e-05, 1.0863681382033974e-05, 1.0863681382033974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0863681382033974e-05

Optimization complete. Final v2v error: 2.743370771408081 mm

Highest mean error: 3.0432167053222656 mm for frame 227

Lowest mean error: 2.5720486640930176 mm for frame 19

Saving results

Total time: 37.063626766204834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587622
Iteration 2/25 | Loss: 0.00112421
Iteration 3/25 | Loss: 0.00085995
Iteration 4/25 | Loss: 0.00080957
Iteration 5/25 | Loss: 0.00078217
Iteration 6/25 | Loss: 0.00077249
Iteration 7/25 | Loss: 0.00077009
Iteration 8/25 | Loss: 0.00076687
Iteration 9/25 | Loss: 0.00077160
Iteration 10/25 | Loss: 0.00076587
Iteration 11/25 | Loss: 0.00076432
Iteration 12/25 | Loss: 0.00076356
Iteration 13/25 | Loss: 0.00076306
Iteration 14/25 | Loss: 0.00076253
Iteration 15/25 | Loss: 0.00076226
Iteration 16/25 | Loss: 0.00076207
Iteration 17/25 | Loss: 0.00076197
Iteration 18/25 | Loss: 0.00076188
Iteration 19/25 | Loss: 0.00076170
Iteration 20/25 | Loss: 0.00076153
Iteration 21/25 | Loss: 0.00076146
Iteration 22/25 | Loss: 0.00076146
Iteration 23/25 | Loss: 0.00076146
Iteration 24/25 | Loss: 0.00076146
Iteration 25/25 | Loss: 0.00076145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18740940
Iteration 2/25 | Loss: 0.00038815
Iteration 3/25 | Loss: 0.00038813
Iteration 4/25 | Loss: 0.00038813
Iteration 5/25 | Loss: 0.00038813
Iteration 6/25 | Loss: 0.00038813
Iteration 7/25 | Loss: 0.00038813
Iteration 8/25 | Loss: 0.00038813
Iteration 9/25 | Loss: 0.00038813
Iteration 10/25 | Loss: 0.00038813
Iteration 11/25 | Loss: 0.00038813
Iteration 12/25 | Loss: 0.00038813
Iteration 13/25 | Loss: 0.00038813
Iteration 14/25 | Loss: 0.00038813
Iteration 15/25 | Loss: 0.00038813
Iteration 16/25 | Loss: 0.00038813
Iteration 17/25 | Loss: 0.00038813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00038813005085103214, 0.00038813005085103214, 0.00038813005085103214, 0.00038813005085103214, 0.00038813005085103214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038813005085103214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038813
Iteration 2/1000 | Loss: 0.00006180
Iteration 3/1000 | Loss: 0.00004402
Iteration 4/1000 | Loss: 0.00003899
Iteration 5/1000 | Loss: 0.00003767
Iteration 6/1000 | Loss: 0.00003639
Iteration 7/1000 | Loss: 0.00003544
Iteration 8/1000 | Loss: 0.00005113
Iteration 9/1000 | Loss: 0.00003413
Iteration 10/1000 | Loss: 0.00003305
Iteration 11/1000 | Loss: 0.00003234
Iteration 12/1000 | Loss: 0.00003175
Iteration 13/1000 | Loss: 0.00003139
Iteration 14/1000 | Loss: 0.00003115
Iteration 15/1000 | Loss: 0.00003104
Iteration 16/1000 | Loss: 0.00003097
Iteration 17/1000 | Loss: 0.00003093
Iteration 18/1000 | Loss: 0.00003088
Iteration 19/1000 | Loss: 0.00003087
Iteration 20/1000 | Loss: 0.00003086
Iteration 21/1000 | Loss: 0.00003086
Iteration 22/1000 | Loss: 0.00003073
Iteration 23/1000 | Loss: 0.00003073
Iteration 24/1000 | Loss: 0.00003067
Iteration 25/1000 | Loss: 0.00003067
Iteration 26/1000 | Loss: 0.00003060
Iteration 27/1000 | Loss: 0.00003060
Iteration 28/1000 | Loss: 0.00003058
Iteration 29/1000 | Loss: 0.00003058
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003057
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003057
Iteration 34/1000 | Loss: 0.00003056
Iteration 35/1000 | Loss: 0.00003054
Iteration 36/1000 | Loss: 0.00003054
Iteration 37/1000 | Loss: 0.00003054
Iteration 38/1000 | Loss: 0.00003054
Iteration 39/1000 | Loss: 0.00003054
Iteration 40/1000 | Loss: 0.00003054
Iteration 41/1000 | Loss: 0.00003054
Iteration 42/1000 | Loss: 0.00003054
Iteration 43/1000 | Loss: 0.00003054
Iteration 44/1000 | Loss: 0.00003054
Iteration 45/1000 | Loss: 0.00003054
Iteration 46/1000 | Loss: 0.00003054
Iteration 47/1000 | Loss: 0.00003054
Iteration 48/1000 | Loss: 0.00003053
Iteration 49/1000 | Loss: 0.00003053
Iteration 50/1000 | Loss: 0.00003052
Iteration 51/1000 | Loss: 0.00003052
Iteration 52/1000 | Loss: 0.00003052
Iteration 53/1000 | Loss: 0.00003052
Iteration 54/1000 | Loss: 0.00003051
Iteration 55/1000 | Loss: 0.00003051
Iteration 56/1000 | Loss: 0.00003051
Iteration 57/1000 | Loss: 0.00003050
Iteration 58/1000 | Loss: 0.00003050
Iteration 59/1000 | Loss: 0.00003050
Iteration 60/1000 | Loss: 0.00003050
Iteration 61/1000 | Loss: 0.00003050
Iteration 62/1000 | Loss: 0.00003049
Iteration 63/1000 | Loss: 0.00003049
Iteration 64/1000 | Loss: 0.00003049
Iteration 65/1000 | Loss: 0.00003049
Iteration 66/1000 | Loss: 0.00003049
Iteration 67/1000 | Loss: 0.00003048
Iteration 68/1000 | Loss: 0.00003048
Iteration 69/1000 | Loss: 0.00003048
Iteration 70/1000 | Loss: 0.00003047
Iteration 71/1000 | Loss: 0.00003047
Iteration 72/1000 | Loss: 0.00003047
Iteration 73/1000 | Loss: 0.00003046
Iteration 74/1000 | Loss: 0.00003046
Iteration 75/1000 | Loss: 0.00003046
Iteration 76/1000 | Loss: 0.00003046
Iteration 77/1000 | Loss: 0.00003046
Iteration 78/1000 | Loss: 0.00003046
Iteration 79/1000 | Loss: 0.00003046
Iteration 80/1000 | Loss: 0.00003046
Iteration 81/1000 | Loss: 0.00003046
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003045
Iteration 85/1000 | Loss: 0.00003045
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003045
Iteration 89/1000 | Loss: 0.00003045
Iteration 90/1000 | Loss: 0.00003045
Iteration 91/1000 | Loss: 0.00003045
Iteration 92/1000 | Loss: 0.00003044
Iteration 93/1000 | Loss: 0.00003044
Iteration 94/1000 | Loss: 0.00003044
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003044
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003044
Iteration 102/1000 | Loss: 0.00003044
Iteration 103/1000 | Loss: 0.00003044
Iteration 104/1000 | Loss: 0.00003044
Iteration 105/1000 | Loss: 0.00003044
Iteration 106/1000 | Loss: 0.00003043
Iteration 107/1000 | Loss: 0.00003043
Iteration 108/1000 | Loss: 0.00003043
Iteration 109/1000 | Loss: 0.00003043
Iteration 110/1000 | Loss: 0.00003043
Iteration 111/1000 | Loss: 0.00003043
Iteration 112/1000 | Loss: 0.00003043
Iteration 113/1000 | Loss: 0.00003042
Iteration 114/1000 | Loss: 0.00003042
Iteration 115/1000 | Loss: 0.00003042
Iteration 116/1000 | Loss: 0.00003042
Iteration 117/1000 | Loss: 0.00003042
Iteration 118/1000 | Loss: 0.00003042
Iteration 119/1000 | Loss: 0.00003042
Iteration 120/1000 | Loss: 0.00003042
Iteration 121/1000 | Loss: 0.00003042
Iteration 122/1000 | Loss: 0.00003042
Iteration 123/1000 | Loss: 0.00003042
Iteration 124/1000 | Loss: 0.00003042
Iteration 125/1000 | Loss: 0.00003042
Iteration 126/1000 | Loss: 0.00003042
Iteration 127/1000 | Loss: 0.00003042
Iteration 128/1000 | Loss: 0.00003042
Iteration 129/1000 | Loss: 0.00003042
Iteration 130/1000 | Loss: 0.00003042
Iteration 131/1000 | Loss: 0.00003042
Iteration 132/1000 | Loss: 0.00003042
Iteration 133/1000 | Loss: 0.00003042
Iteration 134/1000 | Loss: 0.00003042
Iteration 135/1000 | Loss: 0.00003042
Iteration 136/1000 | Loss: 0.00003042
Iteration 137/1000 | Loss: 0.00003042
Iteration 138/1000 | Loss: 0.00003042
Iteration 139/1000 | Loss: 0.00003042
Iteration 140/1000 | Loss: 0.00003042
Iteration 141/1000 | Loss: 0.00003042
Iteration 142/1000 | Loss: 0.00003042
Iteration 143/1000 | Loss: 0.00003042
Iteration 144/1000 | Loss: 0.00003042
Iteration 145/1000 | Loss: 0.00003042
Iteration 146/1000 | Loss: 0.00003042
Iteration 147/1000 | Loss: 0.00003042
Iteration 148/1000 | Loss: 0.00003042
Iteration 149/1000 | Loss: 0.00003042
Iteration 150/1000 | Loss: 0.00003042
Iteration 151/1000 | Loss: 0.00003042
Iteration 152/1000 | Loss: 0.00003042
Iteration 153/1000 | Loss: 0.00003042
Iteration 154/1000 | Loss: 0.00003042
Iteration 155/1000 | Loss: 0.00003042
Iteration 156/1000 | Loss: 0.00003042
Iteration 157/1000 | Loss: 0.00003042
Iteration 158/1000 | Loss: 0.00003042
Iteration 159/1000 | Loss: 0.00003042
Iteration 160/1000 | Loss: 0.00003042
Iteration 161/1000 | Loss: 0.00003042
Iteration 162/1000 | Loss: 0.00003042
Iteration 163/1000 | Loss: 0.00003042
Iteration 164/1000 | Loss: 0.00003042
Iteration 165/1000 | Loss: 0.00003042
Iteration 166/1000 | Loss: 0.00003042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.042177195311524e-05, 3.042177195311524e-05, 3.042177195311524e-05, 3.042177195311524e-05, 3.042177195311524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.042177195311524e-05

Optimization complete. Final v2v error: 4.462237358093262 mm

Highest mean error: 6.482126235961914 mm for frame 114

Lowest mean error: 3.707136631011963 mm for frame 132

Saving results

Total time: 70.29315280914307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486515
Iteration 2/25 | Loss: 0.00081021
Iteration 3/25 | Loss: 0.00067663
Iteration 4/25 | Loss: 0.00064353
Iteration 5/25 | Loss: 0.00063419
Iteration 6/25 | Loss: 0.00063225
Iteration 7/25 | Loss: 0.00063163
Iteration 8/25 | Loss: 0.00063163
Iteration 9/25 | Loss: 0.00063163
Iteration 10/25 | Loss: 0.00063163
Iteration 11/25 | Loss: 0.00063163
Iteration 12/25 | Loss: 0.00063163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006316262879408896, 0.0006316262879408896, 0.0006316262879408896, 0.0006316262879408896, 0.0006316262879408896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006316262879408896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51224434
Iteration 2/25 | Loss: 0.00030346
Iteration 3/25 | Loss: 0.00030345
Iteration 4/25 | Loss: 0.00030345
Iteration 5/25 | Loss: 0.00030345
Iteration 6/25 | Loss: 0.00030345
Iteration 7/25 | Loss: 0.00030345
Iteration 8/25 | Loss: 0.00030344
Iteration 9/25 | Loss: 0.00030344
Iteration 10/25 | Loss: 0.00030344
Iteration 11/25 | Loss: 0.00030344
Iteration 12/25 | Loss: 0.00030344
Iteration 13/25 | Loss: 0.00030344
Iteration 14/25 | Loss: 0.00030344
Iteration 15/25 | Loss: 0.00030344
Iteration 16/25 | Loss: 0.00030344
Iteration 17/25 | Loss: 0.00030344
Iteration 18/25 | Loss: 0.00030344
Iteration 19/25 | Loss: 0.00030344
Iteration 20/25 | Loss: 0.00030344
Iteration 21/25 | Loss: 0.00030344
Iteration 22/25 | Loss: 0.00030344
Iteration 23/25 | Loss: 0.00030344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000303444016026333, 0.000303444016026333, 0.000303444016026333, 0.000303444016026333, 0.000303444016026333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000303444016026333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030344
Iteration 2/1000 | Loss: 0.00002905
Iteration 3/1000 | Loss: 0.00001874
Iteration 4/1000 | Loss: 0.00001735
Iteration 5/1000 | Loss: 0.00001649
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001539
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001503
Iteration 14/1000 | Loss: 0.00001500
Iteration 15/1000 | Loss: 0.00001497
Iteration 16/1000 | Loss: 0.00001497
Iteration 17/1000 | Loss: 0.00001495
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001493
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001490
Iteration 27/1000 | Loss: 0.00001490
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001489
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001488
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001487
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001484
Iteration 47/1000 | Loss: 0.00001484
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001473
Iteration 93/1000 | Loss: 0.00001473
Iteration 94/1000 | Loss: 0.00001473
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001471
Iteration 106/1000 | Loss: 0.00001471
Iteration 107/1000 | Loss: 0.00001471
Iteration 108/1000 | Loss: 0.00001471
Iteration 109/1000 | Loss: 0.00001471
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001470
Iteration 113/1000 | Loss: 0.00001470
Iteration 114/1000 | Loss: 0.00001470
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001469
Iteration 122/1000 | Loss: 0.00001469
Iteration 123/1000 | Loss: 0.00001469
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00001469
Iteration 126/1000 | Loss: 0.00001469
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001468
Iteration 131/1000 | Loss: 0.00001468
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Iteration 146/1000 | Loss: 0.00001468
Iteration 147/1000 | Loss: 0.00001468
Iteration 148/1000 | Loss: 0.00001468
Iteration 149/1000 | Loss: 0.00001468
Iteration 150/1000 | Loss: 0.00001468
Iteration 151/1000 | Loss: 0.00001468
Iteration 152/1000 | Loss: 0.00001468
Iteration 153/1000 | Loss: 0.00001468
Iteration 154/1000 | Loss: 0.00001468
Iteration 155/1000 | Loss: 0.00001468
Iteration 156/1000 | Loss: 0.00001468
Iteration 157/1000 | Loss: 0.00001468
Iteration 158/1000 | Loss: 0.00001468
Iteration 159/1000 | Loss: 0.00001468
Iteration 160/1000 | Loss: 0.00001468
Iteration 161/1000 | Loss: 0.00001468
Iteration 162/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.4682970686408225e-05, 1.4682970686408225e-05, 1.4682970686408225e-05, 1.4682970686408225e-05, 1.4682970686408225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4682970686408225e-05

Optimization complete. Final v2v error: 3.2589573860168457 mm

Highest mean error: 4.1859893798828125 mm for frame 47

Lowest mean error: 2.861067771911621 mm for frame 2

Saving results

Total time: 41.01907467842102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01261047
Iteration 2/25 | Loss: 0.00393437
Iteration 3/25 | Loss: 0.00259090
Iteration 4/25 | Loss: 0.00232234
Iteration 5/25 | Loss: 0.00196834
Iteration 6/25 | Loss: 0.00178399
Iteration 7/25 | Loss: 0.00157364
Iteration 8/25 | Loss: 0.00148772
Iteration 9/25 | Loss: 0.00140437
Iteration 10/25 | Loss: 0.00137417
Iteration 11/25 | Loss: 0.00135552
Iteration 12/25 | Loss: 0.00134662
Iteration 13/25 | Loss: 0.00134816
Iteration 14/25 | Loss: 0.00133746
Iteration 15/25 | Loss: 0.00133091
Iteration 16/25 | Loss: 0.00133726
Iteration 17/25 | Loss: 0.00132796
Iteration 18/25 | Loss: 0.00133007
Iteration 19/25 | Loss: 0.00131565
Iteration 20/25 | Loss: 0.00130508
Iteration 21/25 | Loss: 0.00130057
Iteration 22/25 | Loss: 0.00129720
Iteration 23/25 | Loss: 0.00129732
Iteration 24/25 | Loss: 0.00130413
Iteration 25/25 | Loss: 0.00130186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59626925
Iteration 2/25 | Loss: 0.00476473
Iteration 3/25 | Loss: 0.00476473
Iteration 4/25 | Loss: 0.00476473
Iteration 5/25 | Loss: 0.00476473
Iteration 6/25 | Loss: 0.00476473
Iteration 7/25 | Loss: 0.00476473
Iteration 8/25 | Loss: 0.00476473
Iteration 9/25 | Loss: 0.00476473
Iteration 10/25 | Loss: 0.00476473
Iteration 11/25 | Loss: 0.00476473
Iteration 12/25 | Loss: 0.00476473
Iteration 13/25 | Loss: 0.00476473
Iteration 14/25 | Loss: 0.00476473
Iteration 15/25 | Loss: 0.00476473
Iteration 16/25 | Loss: 0.00476473
Iteration 17/25 | Loss: 0.00476473
Iteration 18/25 | Loss: 0.00476473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004764726851135492, 0.004764726851135492, 0.004764726851135492, 0.004764726851135492, 0.004764726851135492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004764726851135492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00476473
Iteration 2/1000 | Loss: 0.00282438
Iteration 3/1000 | Loss: 0.00236368
Iteration 4/1000 | Loss: 0.00264175
Iteration 5/1000 | Loss: 0.00250689
Iteration 6/1000 | Loss: 0.00210871
Iteration 7/1000 | Loss: 0.00202114
Iteration 8/1000 | Loss: 0.00196835
Iteration 9/1000 | Loss: 0.00163215
Iteration 10/1000 | Loss: 0.00173486
Iteration 11/1000 | Loss: 0.00681650
Iteration 12/1000 | Loss: 0.00315512
Iteration 13/1000 | Loss: 0.00156770
Iteration 14/1000 | Loss: 0.00514655
Iteration 15/1000 | Loss: 0.00551360
Iteration 16/1000 | Loss: 0.00429680
Iteration 17/1000 | Loss: 0.00324503
Iteration 18/1000 | Loss: 0.00208873
Iteration 19/1000 | Loss: 0.00282869
Iteration 20/1000 | Loss: 0.00155782
Iteration 21/1000 | Loss: 0.00103832
Iteration 22/1000 | Loss: 0.00092590
Iteration 23/1000 | Loss: 0.00060487
Iteration 24/1000 | Loss: 0.00156968
Iteration 25/1000 | Loss: 0.00147047
Iteration 26/1000 | Loss: 0.00434161
Iteration 27/1000 | Loss: 0.00116215
Iteration 28/1000 | Loss: 0.00065845
Iteration 29/1000 | Loss: 0.00122300
Iteration 30/1000 | Loss: 0.00127629
Iteration 31/1000 | Loss: 0.00221302
Iteration 32/1000 | Loss: 0.00169093
Iteration 33/1000 | Loss: 0.00200249
Iteration 34/1000 | Loss: 0.00236891
Iteration 35/1000 | Loss: 0.00092101
Iteration 36/1000 | Loss: 0.00139353
Iteration 37/1000 | Loss: 0.00106209
Iteration 38/1000 | Loss: 0.00075560
Iteration 39/1000 | Loss: 0.00165709
Iteration 40/1000 | Loss: 0.00177389
Iteration 41/1000 | Loss: 0.00158768
Iteration 42/1000 | Loss: 0.00197277
Iteration 43/1000 | Loss: 0.00136082
Iteration 44/1000 | Loss: 0.00052892
Iteration 45/1000 | Loss: 0.00070830
Iteration 46/1000 | Loss: 0.00044102
Iteration 47/1000 | Loss: 0.00113065
Iteration 48/1000 | Loss: 0.00072050
Iteration 49/1000 | Loss: 0.00048475
Iteration 50/1000 | Loss: 0.00084979
Iteration 51/1000 | Loss: 0.00094385
Iteration 52/1000 | Loss: 0.00112538
Iteration 53/1000 | Loss: 0.00061082
Iteration 54/1000 | Loss: 0.00050969
Iteration 55/1000 | Loss: 0.00027558
Iteration 56/1000 | Loss: 0.00077227
Iteration 57/1000 | Loss: 0.00075725
Iteration 58/1000 | Loss: 0.00068922
Iteration 59/1000 | Loss: 0.00137400
Iteration 60/1000 | Loss: 0.00101309
Iteration 61/1000 | Loss: 0.00071144
Iteration 62/1000 | Loss: 0.00031129
Iteration 63/1000 | Loss: 0.00052175
Iteration 64/1000 | Loss: 0.00052552
Iteration 65/1000 | Loss: 0.00052291
Iteration 66/1000 | Loss: 0.00045888
Iteration 67/1000 | Loss: 0.00069494
Iteration 68/1000 | Loss: 0.00064572
Iteration 69/1000 | Loss: 0.00072735
Iteration 70/1000 | Loss: 0.00069200
Iteration 71/1000 | Loss: 0.00097133
Iteration 72/1000 | Loss: 0.00059627
Iteration 73/1000 | Loss: 0.00063060
Iteration 74/1000 | Loss: 0.00039878
Iteration 75/1000 | Loss: 0.00046134
Iteration 76/1000 | Loss: 0.00090971
Iteration 77/1000 | Loss: 0.00071659
Iteration 78/1000 | Loss: 0.00082405
Iteration 79/1000 | Loss: 0.00068141
Iteration 80/1000 | Loss: 0.00081625
Iteration 81/1000 | Loss: 0.00078048
Iteration 82/1000 | Loss: 0.00080638
Iteration 83/1000 | Loss: 0.00043900
Iteration 84/1000 | Loss: 0.00074291
Iteration 85/1000 | Loss: 0.00215805
Iteration 86/1000 | Loss: 0.00099676
Iteration 87/1000 | Loss: 0.00139691
Iteration 88/1000 | Loss: 0.00101369
Iteration 89/1000 | Loss: 0.00125507
Iteration 90/1000 | Loss: 0.00038865
Iteration 91/1000 | Loss: 0.00039955
Iteration 92/1000 | Loss: 0.00032714
Iteration 93/1000 | Loss: 0.00028290
Iteration 94/1000 | Loss: 0.00030661
Iteration 95/1000 | Loss: 0.00054178
Iteration 96/1000 | Loss: 0.00044682
Iteration 97/1000 | Loss: 0.00046678
Iteration 98/1000 | Loss: 0.00047604
Iteration 99/1000 | Loss: 0.00038744
Iteration 100/1000 | Loss: 0.00044999
Iteration 101/1000 | Loss: 0.00035183
Iteration 102/1000 | Loss: 0.00046298
Iteration 103/1000 | Loss: 0.00092298
Iteration 104/1000 | Loss: 0.00045919
Iteration 105/1000 | Loss: 0.00040115
Iteration 106/1000 | Loss: 0.00021231
Iteration 107/1000 | Loss: 0.00032140
Iteration 108/1000 | Loss: 0.00026707
Iteration 109/1000 | Loss: 0.00044537
Iteration 110/1000 | Loss: 0.00044522
Iteration 111/1000 | Loss: 0.00049596
Iteration 112/1000 | Loss: 0.00042508
Iteration 113/1000 | Loss: 0.00034770
Iteration 114/1000 | Loss: 0.00041551
Iteration 115/1000 | Loss: 0.00046730
Iteration 116/1000 | Loss: 0.00049501
Iteration 117/1000 | Loss: 0.00056199
Iteration 118/1000 | Loss: 0.00043900
Iteration 119/1000 | Loss: 0.00048450
Iteration 120/1000 | Loss: 0.00047279
Iteration 121/1000 | Loss: 0.00047348
Iteration 122/1000 | Loss: 0.00039115
Iteration 123/1000 | Loss: 0.00044630
Iteration 124/1000 | Loss: 0.00039359
Iteration 125/1000 | Loss: 0.00041710
Iteration 126/1000 | Loss: 0.00046591
Iteration 127/1000 | Loss: 0.00052756
Iteration 128/1000 | Loss: 0.00088009
Iteration 129/1000 | Loss: 0.00054514
Iteration 130/1000 | Loss: 0.00034271
Iteration 131/1000 | Loss: 0.00043452
Iteration 132/1000 | Loss: 0.00065009
Iteration 133/1000 | Loss: 0.00054087
Iteration 134/1000 | Loss: 0.00020006
Iteration 135/1000 | Loss: 0.00052956
Iteration 136/1000 | Loss: 0.00048265
Iteration 137/1000 | Loss: 0.00045006
Iteration 138/1000 | Loss: 0.00029066
Iteration 139/1000 | Loss: 0.00036907
Iteration 140/1000 | Loss: 0.00034380
Iteration 141/1000 | Loss: 0.00048042
Iteration 142/1000 | Loss: 0.00051235
Iteration 143/1000 | Loss: 0.00039957
Iteration 144/1000 | Loss: 0.00042197
Iteration 145/1000 | Loss: 0.00042733
Iteration 146/1000 | Loss: 0.00055245
Iteration 147/1000 | Loss: 0.00018561
Iteration 148/1000 | Loss: 0.00022665
Iteration 149/1000 | Loss: 0.00046913
Iteration 150/1000 | Loss: 0.00039167
Iteration 151/1000 | Loss: 0.00047019
Iteration 152/1000 | Loss: 0.00057988
Iteration 153/1000 | Loss: 0.00050734
Iteration 154/1000 | Loss: 0.00050876
Iteration 155/1000 | Loss: 0.00079335
Iteration 156/1000 | Loss: 0.00050249
Iteration 157/1000 | Loss: 0.00046987
Iteration 158/1000 | Loss: 0.00045071
Iteration 159/1000 | Loss: 0.00041826
Iteration 160/1000 | Loss: 0.00059454
Iteration 161/1000 | Loss: 0.00037162
Iteration 162/1000 | Loss: 0.00041623
Iteration 163/1000 | Loss: 0.00036926
Iteration 164/1000 | Loss: 0.00040090
Iteration 165/1000 | Loss: 0.00027622
Iteration 166/1000 | Loss: 0.00035303
Iteration 167/1000 | Loss: 0.00045854
Iteration 168/1000 | Loss: 0.00040896
Iteration 169/1000 | Loss: 0.00044486
Iteration 170/1000 | Loss: 0.00041842
Iteration 171/1000 | Loss: 0.00039679
Iteration 172/1000 | Loss: 0.00032363
Iteration 173/1000 | Loss: 0.00045341
Iteration 174/1000 | Loss: 0.00027780
Iteration 175/1000 | Loss: 0.00046701
Iteration 176/1000 | Loss: 0.00036190
Iteration 177/1000 | Loss: 0.00050564
Iteration 178/1000 | Loss: 0.00048597
Iteration 179/1000 | Loss: 0.00039692
Iteration 180/1000 | Loss: 0.00042839
Iteration 181/1000 | Loss: 0.00047045
Iteration 182/1000 | Loss: 0.00042954
Iteration 183/1000 | Loss: 0.00048586
Iteration 184/1000 | Loss: 0.00048625
Iteration 185/1000 | Loss: 0.00047288
Iteration 186/1000 | Loss: 0.00042838
Iteration 187/1000 | Loss: 0.00046177
Iteration 188/1000 | Loss: 0.00045117
Iteration 189/1000 | Loss: 0.00040347
Iteration 190/1000 | Loss: 0.00034948
Iteration 191/1000 | Loss: 0.00049482
Iteration 192/1000 | Loss: 0.00045411
Iteration 193/1000 | Loss: 0.00045819
Iteration 194/1000 | Loss: 0.00040727
Iteration 195/1000 | Loss: 0.00019358
Iteration 196/1000 | Loss: 0.00035789
Iteration 197/1000 | Loss: 0.00055850
Iteration 198/1000 | Loss: 0.00033852
Iteration 199/1000 | Loss: 0.00044911
Iteration 200/1000 | Loss: 0.00029021
Iteration 201/1000 | Loss: 0.00044433
Iteration 202/1000 | Loss: 0.00034505
Iteration 203/1000 | Loss: 0.00044219
Iteration 204/1000 | Loss: 0.00032421
Iteration 205/1000 | Loss: 0.00035208
Iteration 206/1000 | Loss: 0.00028168
Iteration 207/1000 | Loss: 0.00038964
Iteration 208/1000 | Loss: 0.00034058
Iteration 209/1000 | Loss: 0.00027214
Iteration 210/1000 | Loss: 0.00019281
Iteration 211/1000 | Loss: 0.00020275
Iteration 212/1000 | Loss: 0.00033443
Iteration 213/1000 | Loss: 0.00032559
Iteration 214/1000 | Loss: 0.00031969
Iteration 215/1000 | Loss: 0.00026339
Iteration 216/1000 | Loss: 0.00034212
Iteration 217/1000 | Loss: 0.00032220
Iteration 218/1000 | Loss: 0.00035957
Iteration 219/1000 | Loss: 0.00032422
Iteration 220/1000 | Loss: 0.00031195
Iteration 221/1000 | Loss: 0.00027378
Iteration 222/1000 | Loss: 0.00028815
Iteration 223/1000 | Loss: 0.00029037
Iteration 224/1000 | Loss: 0.00029515
Iteration 225/1000 | Loss: 0.00030325
Iteration 226/1000 | Loss: 0.00183452
Iteration 227/1000 | Loss: 0.00079124
Iteration 228/1000 | Loss: 0.00091137
Iteration 229/1000 | Loss: 0.00054396
Iteration 230/1000 | Loss: 0.00017576
Iteration 231/1000 | Loss: 0.00030156
Iteration 232/1000 | Loss: 0.00025839
Iteration 233/1000 | Loss: 0.00031142
Iteration 234/1000 | Loss: 0.00029636
Iteration 235/1000 | Loss: 0.00016590
Iteration 236/1000 | Loss: 0.00027797
Iteration 237/1000 | Loss: 0.00033591
Iteration 238/1000 | Loss: 0.00032969
Iteration 239/1000 | Loss: 0.00038701
Iteration 240/1000 | Loss: 0.00036313
Iteration 241/1000 | Loss: 0.00026666
Iteration 242/1000 | Loss: 0.00020730
Iteration 243/1000 | Loss: 0.00029215
Iteration 244/1000 | Loss: 0.00028992
Iteration 245/1000 | Loss: 0.00026300
Iteration 246/1000 | Loss: 0.00026136
Iteration 247/1000 | Loss: 0.00017018
Iteration 248/1000 | Loss: 0.00036271
Iteration 249/1000 | Loss: 0.00033244
Iteration 250/1000 | Loss: 0.00032613
Iteration 251/1000 | Loss: 0.00032296
Iteration 252/1000 | Loss: 0.00032224
Iteration 253/1000 | Loss: 0.00034323
Iteration 254/1000 | Loss: 0.00033235
Iteration 255/1000 | Loss: 0.00033453
Iteration 256/1000 | Loss: 0.00033677
Iteration 257/1000 | Loss: 0.00018491
Iteration 258/1000 | Loss: 0.00022403
Iteration 259/1000 | Loss: 0.00026354
Iteration 260/1000 | Loss: 0.00026067
Iteration 261/1000 | Loss: 0.00022978
Iteration 262/1000 | Loss: 0.00028382
Iteration 263/1000 | Loss: 0.00032949
Iteration 264/1000 | Loss: 0.00032548
Iteration 265/1000 | Loss: 0.00031161
Iteration 266/1000 | Loss: 0.00027728
Iteration 267/1000 | Loss: 0.00019296
Iteration 268/1000 | Loss: 0.00014701
Iteration 269/1000 | Loss: 0.00018405
Iteration 270/1000 | Loss: 0.00029662
Iteration 271/1000 | Loss: 0.00028608
Iteration 272/1000 | Loss: 0.00031377
Iteration 273/1000 | Loss: 0.00025388
Iteration 274/1000 | Loss: 0.00031837
Iteration 275/1000 | Loss: 0.00016237
Iteration 276/1000 | Loss: 0.00060860
Iteration 277/1000 | Loss: 0.00108047
Iteration 278/1000 | Loss: 0.00107124
Iteration 279/1000 | Loss: 0.00100069
Iteration 280/1000 | Loss: 0.00078397
Iteration 281/1000 | Loss: 0.00033153
Iteration 282/1000 | Loss: 0.00013278
Iteration 283/1000 | Loss: 0.00022072
Iteration 284/1000 | Loss: 0.00028835
Iteration 285/1000 | Loss: 0.00035634
Iteration 286/1000 | Loss: 0.00031927
Iteration 287/1000 | Loss: 0.00016843
Iteration 288/1000 | Loss: 0.00025702
Iteration 289/1000 | Loss: 0.00022053
Iteration 290/1000 | Loss: 0.00017543
Iteration 291/1000 | Loss: 0.00018240
Iteration 292/1000 | Loss: 0.00022039
Iteration 293/1000 | Loss: 0.00026863
Iteration 294/1000 | Loss: 0.00041275
Iteration 295/1000 | Loss: 0.00041982
Iteration 296/1000 | Loss: 0.00026575
Iteration 297/1000 | Loss: 0.00040659
Iteration 298/1000 | Loss: 0.00024356
Iteration 299/1000 | Loss: 0.00010926
Iteration 300/1000 | Loss: 0.00013849
Iteration 301/1000 | Loss: 0.00012693
Iteration 302/1000 | Loss: 0.00022577
Iteration 303/1000 | Loss: 0.00010353
Iteration 304/1000 | Loss: 0.00021936
Iteration 305/1000 | Loss: 0.00023797
Iteration 306/1000 | Loss: 0.00029327
Iteration 307/1000 | Loss: 0.00027229
Iteration 308/1000 | Loss: 0.00020628
Iteration 309/1000 | Loss: 0.00020338
Iteration 310/1000 | Loss: 0.00024527
Iteration 311/1000 | Loss: 0.00026775
Iteration 312/1000 | Loss: 0.00028235
Iteration 313/1000 | Loss: 0.00166551
Iteration 314/1000 | Loss: 0.00102340
Iteration 315/1000 | Loss: 0.00103955
Iteration 316/1000 | Loss: 0.00031329
Iteration 317/1000 | Loss: 0.00013052
Iteration 318/1000 | Loss: 0.00028460
Iteration 319/1000 | Loss: 0.00011769
Iteration 320/1000 | Loss: 0.00078611
Iteration 321/1000 | Loss: 0.00008974
Iteration 322/1000 | Loss: 0.00082177
Iteration 323/1000 | Loss: 0.00015253
Iteration 324/1000 | Loss: 0.00010168
Iteration 325/1000 | Loss: 0.00008091
Iteration 326/1000 | Loss: 0.00007929
Iteration 327/1000 | Loss: 0.00083025
Iteration 328/1000 | Loss: 0.00008327
Iteration 329/1000 | Loss: 0.00007371
Iteration 330/1000 | Loss: 0.00024348
Iteration 331/1000 | Loss: 0.00182182
Iteration 332/1000 | Loss: 0.00103071
Iteration 333/1000 | Loss: 0.00017125
Iteration 334/1000 | Loss: 0.00018784
Iteration 335/1000 | Loss: 0.00008765
Iteration 336/1000 | Loss: 0.00086875
Iteration 337/1000 | Loss: 0.00051340
Iteration 338/1000 | Loss: 0.00084395
Iteration 339/1000 | Loss: 0.00009507
Iteration 340/1000 | Loss: 0.00008022
Iteration 341/1000 | Loss: 0.00041948
Iteration 342/1000 | Loss: 0.00038200
Iteration 343/1000 | Loss: 0.00034858
Iteration 344/1000 | Loss: 0.00043904
Iteration 345/1000 | Loss: 0.00030773
Iteration 346/1000 | Loss: 0.00039312
Iteration 347/1000 | Loss: 0.00044802
Iteration 348/1000 | Loss: 0.00027871
Iteration 349/1000 | Loss: 0.00012187
Iteration 350/1000 | Loss: 0.00007769
Iteration 351/1000 | Loss: 0.00023686
Iteration 352/1000 | Loss: 0.00009314
Iteration 353/1000 | Loss: 0.00007383
Iteration 354/1000 | Loss: 0.00006800
Iteration 355/1000 | Loss: 0.00017543
Iteration 356/1000 | Loss: 0.00007891
Iteration 357/1000 | Loss: 0.00008793
Iteration 358/1000 | Loss: 0.00012695
Iteration 359/1000 | Loss: 0.00008202
Iteration 360/1000 | Loss: 0.00008433
Iteration 361/1000 | Loss: 0.00008454
Iteration 362/1000 | Loss: 0.00014865
Iteration 363/1000 | Loss: 0.00012303
Iteration 364/1000 | Loss: 0.00016155
Iteration 365/1000 | Loss: 0.00007919
Iteration 366/1000 | Loss: 0.00017410
Iteration 367/1000 | Loss: 0.00008223
Iteration 368/1000 | Loss: 0.00008344
Iteration 369/1000 | Loss: 0.00020974
Iteration 370/1000 | Loss: 0.00019334
Iteration 371/1000 | Loss: 0.00022706
Iteration 372/1000 | Loss: 0.00016024
Iteration 373/1000 | Loss: 0.00007990
Iteration 374/1000 | Loss: 0.00020150
Iteration 375/1000 | Loss: 0.00010226
Iteration 376/1000 | Loss: 0.00007815
Iteration 377/1000 | Loss: 0.00016562
Iteration 378/1000 | Loss: 0.00018663
Iteration 379/1000 | Loss: 0.00015526
Iteration 380/1000 | Loss: 0.00009894
Iteration 381/1000 | Loss: 0.00010435
Iteration 382/1000 | Loss: 0.00015831
Iteration 383/1000 | Loss: 0.00009511
Iteration 384/1000 | Loss: 0.00008164
Iteration 385/1000 | Loss: 0.00006641
Iteration 386/1000 | Loss: 0.00007292
Iteration 387/1000 | Loss: 0.00013809
Iteration 388/1000 | Loss: 0.00013175
Iteration 389/1000 | Loss: 0.00015024
Iteration 390/1000 | Loss: 0.00062983
Iteration 391/1000 | Loss: 0.00020400
Iteration 392/1000 | Loss: 0.00029268
Iteration 393/1000 | Loss: 0.00033366
Iteration 394/1000 | Loss: 0.00016731
Iteration 395/1000 | Loss: 0.00021875
Iteration 396/1000 | Loss: 0.00027220
Iteration 397/1000 | Loss: 0.00014817
Iteration 398/1000 | Loss: 0.00016911
Iteration 399/1000 | Loss: 0.00020072
Iteration 400/1000 | Loss: 0.00032395
Iteration 401/1000 | Loss: 0.00010378
Iteration 402/1000 | Loss: 0.00020960
Iteration 403/1000 | Loss: 0.00013595
Iteration 404/1000 | Loss: 0.00013257
Iteration 405/1000 | Loss: 0.00007234
Iteration 406/1000 | Loss: 0.00014943
Iteration 407/1000 | Loss: 0.00010115
Iteration 408/1000 | Loss: 0.00013028
Iteration 409/1000 | Loss: 0.00008149
Iteration 410/1000 | Loss: 0.00015496
Iteration 411/1000 | Loss: 0.00097032
Iteration 412/1000 | Loss: 0.00016254
Iteration 413/1000 | Loss: 0.00095957
Iteration 414/1000 | Loss: 0.00030184
Iteration 415/1000 | Loss: 0.00008110
Iteration 416/1000 | Loss: 0.00014382
Iteration 417/1000 | Loss: 0.00006815
Iteration 418/1000 | Loss: 0.00006469
Iteration 419/1000 | Loss: 0.00006233
Iteration 420/1000 | Loss: 0.00006096
Iteration 421/1000 | Loss: 0.00005976
Iteration 422/1000 | Loss: 0.00065945
Iteration 423/1000 | Loss: 0.00020271
Iteration 424/1000 | Loss: 0.00036059
Iteration 425/1000 | Loss: 0.00045108
Iteration 426/1000 | Loss: 0.00029058
Iteration 427/1000 | Loss: 0.00036368
Iteration 428/1000 | Loss: 0.00026589
Iteration 429/1000 | Loss: 0.00028895
Iteration 430/1000 | Loss: 0.00049194
Iteration 431/1000 | Loss: 0.00061645
Iteration 432/1000 | Loss: 0.00028412
Iteration 433/1000 | Loss: 0.00010219
Iteration 434/1000 | Loss: 0.00007147
Iteration 435/1000 | Loss: 0.00052300
Iteration 436/1000 | Loss: 0.00007919
Iteration 437/1000 | Loss: 0.00006783
Iteration 438/1000 | Loss: 0.00006277
Iteration 439/1000 | Loss: 0.00006163
Iteration 440/1000 | Loss: 0.00081635
Iteration 441/1000 | Loss: 0.00006302
Iteration 442/1000 | Loss: 0.00006071
Iteration 443/1000 | Loss: 0.00005851
Iteration 444/1000 | Loss: 0.00005656
Iteration 445/1000 | Loss: 0.00006866
Iteration 446/1000 | Loss: 0.00035091
Iteration 447/1000 | Loss: 0.00007538
Iteration 448/1000 | Loss: 0.00032301
Iteration 449/1000 | Loss: 0.00013825
Iteration 450/1000 | Loss: 0.00025915
Iteration 451/1000 | Loss: 0.00032843
Iteration 452/1000 | Loss: 0.00014124
Iteration 453/1000 | Loss: 0.00048652
Iteration 454/1000 | Loss: 0.00026158
Iteration 455/1000 | Loss: 0.00007945
Iteration 456/1000 | Loss: 0.00028609
Iteration 457/1000 | Loss: 0.00030539
Iteration 458/1000 | Loss: 0.00064621
Iteration 459/1000 | Loss: 0.00077695
Iteration 460/1000 | Loss: 0.00021818
Iteration 461/1000 | Loss: 0.00007357
Iteration 462/1000 | Loss: 0.00005948
Iteration 463/1000 | Loss: 0.00005639
Iteration 464/1000 | Loss: 0.00005450
Iteration 465/1000 | Loss: 0.00079298
Iteration 466/1000 | Loss: 0.00063183
Iteration 467/1000 | Loss: 0.00076002
Iteration 468/1000 | Loss: 0.00040416
Iteration 469/1000 | Loss: 0.00064215
Iteration 470/1000 | Loss: 0.00006172
Iteration 471/1000 | Loss: 0.00005382
Iteration 472/1000 | Loss: 0.00005189
Iteration 473/1000 | Loss: 0.00005050
Iteration 474/1000 | Loss: 0.00004923
Iteration 475/1000 | Loss: 0.00004852
Iteration 476/1000 | Loss: 0.00076987
Iteration 477/1000 | Loss: 0.00005235
Iteration 478/1000 | Loss: 0.00004915
Iteration 479/1000 | Loss: 0.00004796
Iteration 480/1000 | Loss: 0.00004618
Iteration 481/1000 | Loss: 0.00004519
Iteration 482/1000 | Loss: 0.00004486
Iteration 483/1000 | Loss: 0.00004468
Iteration 484/1000 | Loss: 0.00004449
Iteration 485/1000 | Loss: 0.00004447
Iteration 486/1000 | Loss: 0.00004434
Iteration 487/1000 | Loss: 0.00004425
Iteration 488/1000 | Loss: 0.00004425
Iteration 489/1000 | Loss: 0.00004424
Iteration 490/1000 | Loss: 0.00004424
Iteration 491/1000 | Loss: 0.00004420
Iteration 492/1000 | Loss: 0.00004420
Iteration 493/1000 | Loss: 0.00068525
Iteration 494/1000 | Loss: 0.00203783
Iteration 495/1000 | Loss: 0.00006311
Iteration 496/1000 | Loss: 0.00004550
Iteration 497/1000 | Loss: 0.00004155
Iteration 498/1000 | Loss: 0.00003854
Iteration 499/1000 | Loss: 0.00003626
Iteration 500/1000 | Loss: 0.00003479
Iteration 501/1000 | Loss: 0.00003420
Iteration 502/1000 | Loss: 0.00003372
Iteration 503/1000 | Loss: 0.00003331
Iteration 504/1000 | Loss: 0.00003308
Iteration 505/1000 | Loss: 0.00003297
Iteration 506/1000 | Loss: 0.00003289
Iteration 507/1000 | Loss: 0.00003285
Iteration 508/1000 | Loss: 0.00003281
Iteration 509/1000 | Loss: 0.00003280
Iteration 510/1000 | Loss: 0.00003279
Iteration 511/1000 | Loss: 0.00003277
Iteration 512/1000 | Loss: 0.00003276
Iteration 513/1000 | Loss: 0.00003275
Iteration 514/1000 | Loss: 0.00003274
Iteration 515/1000 | Loss: 0.00003273
Iteration 516/1000 | Loss: 0.00003273
Iteration 517/1000 | Loss: 0.00003272
Iteration 518/1000 | Loss: 0.00003272
Iteration 519/1000 | Loss: 0.00003272
Iteration 520/1000 | Loss: 0.00003271
Iteration 521/1000 | Loss: 0.00003271
Iteration 522/1000 | Loss: 0.00003270
Iteration 523/1000 | Loss: 0.00003268
Iteration 524/1000 | Loss: 0.00003267
Iteration 525/1000 | Loss: 0.00003267
Iteration 526/1000 | Loss: 0.00003267
Iteration 527/1000 | Loss: 0.00003267
Iteration 528/1000 | Loss: 0.00003266
Iteration 529/1000 | Loss: 0.00003266
Iteration 530/1000 | Loss: 0.00003266
Iteration 531/1000 | Loss: 0.00003265
Iteration 532/1000 | Loss: 0.00003265
Iteration 533/1000 | Loss: 0.00003265
Iteration 534/1000 | Loss: 0.00003265
Iteration 535/1000 | Loss: 0.00003264
Iteration 536/1000 | Loss: 0.00003264
Iteration 537/1000 | Loss: 0.00003264
Iteration 538/1000 | Loss: 0.00003264
Iteration 539/1000 | Loss: 0.00003264
Iteration 540/1000 | Loss: 0.00003263
Iteration 541/1000 | Loss: 0.00003263
Iteration 542/1000 | Loss: 0.00003263
Iteration 543/1000 | Loss: 0.00003263
Iteration 544/1000 | Loss: 0.00003262
Iteration 545/1000 | Loss: 0.00003262
Iteration 546/1000 | Loss: 0.00003262
Iteration 547/1000 | Loss: 0.00003261
Iteration 548/1000 | Loss: 0.00003261
Iteration 549/1000 | Loss: 0.00003261
Iteration 550/1000 | Loss: 0.00003260
Iteration 551/1000 | Loss: 0.00003260
Iteration 552/1000 | Loss: 0.00003260
Iteration 553/1000 | Loss: 0.00003260
Iteration 554/1000 | Loss: 0.00003260
Iteration 555/1000 | Loss: 0.00003259
Iteration 556/1000 | Loss: 0.00003259
Iteration 557/1000 | Loss: 0.00003259
Iteration 558/1000 | Loss: 0.00003259
Iteration 559/1000 | Loss: 0.00003259
Iteration 560/1000 | Loss: 0.00003259
Iteration 561/1000 | Loss: 0.00003258
Iteration 562/1000 | Loss: 0.00003258
Iteration 563/1000 | Loss: 0.00003258
Iteration 564/1000 | Loss: 0.00003258
Iteration 565/1000 | Loss: 0.00003258
Iteration 566/1000 | Loss: 0.00003258
Iteration 567/1000 | Loss: 0.00003258
Iteration 568/1000 | Loss: 0.00003258
Iteration 569/1000 | Loss: 0.00003258
Iteration 570/1000 | Loss: 0.00003258
Iteration 571/1000 | Loss: 0.00003258
Iteration 572/1000 | Loss: 0.00003258
Iteration 573/1000 | Loss: 0.00003258
Iteration 574/1000 | Loss: 0.00003257
Iteration 575/1000 | Loss: 0.00003257
Iteration 576/1000 | Loss: 0.00003257
Iteration 577/1000 | Loss: 0.00003257
Iteration 578/1000 | Loss: 0.00003257
Iteration 579/1000 | Loss: 0.00003257
Iteration 580/1000 | Loss: 0.00003257
Iteration 581/1000 | Loss: 0.00003257
Iteration 582/1000 | Loss: 0.00003257
Iteration 583/1000 | Loss: 0.00003257
Iteration 584/1000 | Loss: 0.00003257
Iteration 585/1000 | Loss: 0.00003256
Iteration 586/1000 | Loss: 0.00003256
Iteration 587/1000 | Loss: 0.00003256
Iteration 588/1000 | Loss: 0.00003256
Iteration 589/1000 | Loss: 0.00003256
Iteration 590/1000 | Loss: 0.00003256
Iteration 591/1000 | Loss: 0.00003255
Iteration 592/1000 | Loss: 0.00003255
Iteration 593/1000 | Loss: 0.00003255
Iteration 594/1000 | Loss: 0.00003255
Iteration 595/1000 | Loss: 0.00003255
Iteration 596/1000 | Loss: 0.00003255
Iteration 597/1000 | Loss: 0.00003255
Iteration 598/1000 | Loss: 0.00003254
Iteration 599/1000 | Loss: 0.00003254
Iteration 600/1000 | Loss: 0.00003254
Iteration 601/1000 | Loss: 0.00003254
Iteration 602/1000 | Loss: 0.00003253
Iteration 603/1000 | Loss: 0.00003253
Iteration 604/1000 | Loss: 0.00003253
Iteration 605/1000 | Loss: 0.00003252
Iteration 606/1000 | Loss: 0.00003252
Iteration 607/1000 | Loss: 0.00003252
Iteration 608/1000 | Loss: 0.00003252
Iteration 609/1000 | Loss: 0.00003252
Iteration 610/1000 | Loss: 0.00003251
Iteration 611/1000 | Loss: 0.00003251
Iteration 612/1000 | Loss: 0.00003251
Iteration 613/1000 | Loss: 0.00003250
Iteration 614/1000 | Loss: 0.00003250
Iteration 615/1000 | Loss: 0.00003249
Iteration 616/1000 | Loss: 0.00003249
Iteration 617/1000 | Loss: 0.00003249
Iteration 618/1000 | Loss: 0.00003249
Iteration 619/1000 | Loss: 0.00003249
Iteration 620/1000 | Loss: 0.00003249
Iteration 621/1000 | Loss: 0.00003248
Iteration 622/1000 | Loss: 0.00003248
Iteration 623/1000 | Loss: 0.00003248
Iteration 624/1000 | Loss: 0.00003248
Iteration 625/1000 | Loss: 0.00003248
Iteration 626/1000 | Loss: 0.00003248
Iteration 627/1000 | Loss: 0.00003248
Iteration 628/1000 | Loss: 0.00003247
Iteration 629/1000 | Loss: 0.00003247
Iteration 630/1000 | Loss: 0.00003247
Iteration 631/1000 | Loss: 0.00003247
Iteration 632/1000 | Loss: 0.00003247
Iteration 633/1000 | Loss: 0.00003247
Iteration 634/1000 | Loss: 0.00003247
Iteration 635/1000 | Loss: 0.00003247
Iteration 636/1000 | Loss: 0.00003247
Iteration 637/1000 | Loss: 0.00003247
Iteration 638/1000 | Loss: 0.00003247
Iteration 639/1000 | Loss: 0.00003247
Iteration 640/1000 | Loss: 0.00003247
Iteration 641/1000 | Loss: 0.00003247
Iteration 642/1000 | Loss: 0.00003247
Iteration 643/1000 | Loss: 0.00003247
Iteration 644/1000 | Loss: 0.00003246
Iteration 645/1000 | Loss: 0.00003246
Iteration 646/1000 | Loss: 0.00003246
Iteration 647/1000 | Loss: 0.00003246
Iteration 648/1000 | Loss: 0.00003246
Iteration 649/1000 | Loss: 0.00003246
Iteration 650/1000 | Loss: 0.00003246
Iteration 651/1000 | Loss: 0.00003246
Iteration 652/1000 | Loss: 0.00003246
Iteration 653/1000 | Loss: 0.00003245
Iteration 654/1000 | Loss: 0.00003245
Iteration 655/1000 | Loss: 0.00003245
Iteration 656/1000 | Loss: 0.00003245
Iteration 657/1000 | Loss: 0.00003245
Iteration 658/1000 | Loss: 0.00003245
Iteration 659/1000 | Loss: 0.00003244
Iteration 660/1000 | Loss: 0.00003244
Iteration 661/1000 | Loss: 0.00003244
Iteration 662/1000 | Loss: 0.00003244
Iteration 663/1000 | Loss: 0.00003244
Iteration 664/1000 | Loss: 0.00003244
Iteration 665/1000 | Loss: 0.00003244
Iteration 666/1000 | Loss: 0.00003244
Iteration 667/1000 | Loss: 0.00003244
Iteration 668/1000 | Loss: 0.00003244
Iteration 669/1000 | Loss: 0.00003243
Iteration 670/1000 | Loss: 0.00003243
Iteration 671/1000 | Loss: 0.00003243
Iteration 672/1000 | Loss: 0.00003243
Iteration 673/1000 | Loss: 0.00003243
Iteration 674/1000 | Loss: 0.00003243
Iteration 675/1000 | Loss: 0.00003243
Iteration 676/1000 | Loss: 0.00003243
Iteration 677/1000 | Loss: 0.00003243
Iteration 678/1000 | Loss: 0.00003243
Iteration 679/1000 | Loss: 0.00003243
Iteration 680/1000 | Loss: 0.00003242
Iteration 681/1000 | Loss: 0.00003242
Iteration 682/1000 | Loss: 0.00003242
Iteration 683/1000 | Loss: 0.00003242
Iteration 684/1000 | Loss: 0.00003242
Iteration 685/1000 | Loss: 0.00003242
Iteration 686/1000 | Loss: 0.00003242
Iteration 687/1000 | Loss: 0.00003242
Iteration 688/1000 | Loss: 0.00003242
Iteration 689/1000 | Loss: 0.00003242
Iteration 690/1000 | Loss: 0.00003242
Iteration 691/1000 | Loss: 0.00003242
Iteration 692/1000 | Loss: 0.00003242
Iteration 693/1000 | Loss: 0.00003242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 693. Stopping optimization.
Last 5 losses: [3.242058301111683e-05, 3.242058301111683e-05, 3.242058301111683e-05, 3.242058301111683e-05, 3.242058301111683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.242058301111683e-05

Optimization complete. Final v2v error: 4.496801376342773 mm

Highest mean error: 5.132549285888672 mm for frame 150

Lowest mean error: 3.9037418365478516 mm for frame 66

Saving results

Total time: 763.6895682811737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409942
Iteration 2/25 | Loss: 0.00078201
Iteration 3/25 | Loss: 0.00068043
Iteration 4/25 | Loss: 0.00064684
Iteration 5/25 | Loss: 0.00063826
Iteration 6/25 | Loss: 0.00063694
Iteration 7/25 | Loss: 0.00063650
Iteration 8/25 | Loss: 0.00063650
Iteration 9/25 | Loss: 0.00063650
Iteration 10/25 | Loss: 0.00063650
Iteration 11/25 | Loss: 0.00063650
Iteration 12/25 | Loss: 0.00063650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006364966393448412, 0.0006364966393448412, 0.0006364966393448412, 0.0006364966393448412, 0.0006364966393448412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006364966393448412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47104096
Iteration 2/25 | Loss: 0.00026707
Iteration 3/25 | Loss: 0.00026707
Iteration 4/25 | Loss: 0.00026706
Iteration 5/25 | Loss: 0.00026706
Iteration 6/25 | Loss: 0.00026706
Iteration 7/25 | Loss: 0.00026706
Iteration 8/25 | Loss: 0.00026706
Iteration 9/25 | Loss: 0.00026706
Iteration 10/25 | Loss: 0.00026706
Iteration 11/25 | Loss: 0.00026706
Iteration 12/25 | Loss: 0.00026706
Iteration 13/25 | Loss: 0.00026706
Iteration 14/25 | Loss: 0.00026706
Iteration 15/25 | Loss: 0.00026706
Iteration 16/25 | Loss: 0.00026706
Iteration 17/25 | Loss: 0.00026706
Iteration 18/25 | Loss: 0.00026706
Iteration 19/25 | Loss: 0.00026706
Iteration 20/25 | Loss: 0.00026706
Iteration 21/25 | Loss: 0.00026706
Iteration 22/25 | Loss: 0.00026706
Iteration 23/25 | Loss: 0.00026706
Iteration 24/25 | Loss: 0.00026706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002670622488949448, 0.0002670622488949448, 0.0002670622488949448, 0.0002670622488949448, 0.0002670622488949448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002670622488949448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026706
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001922
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001685
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001652
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001651
Iteration 22/1000 | Loss: 0.00001651
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001644
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001640
Iteration 40/1000 | Loss: 0.00001639
Iteration 41/1000 | Loss: 0.00001639
Iteration 42/1000 | Loss: 0.00001639
Iteration 43/1000 | Loss: 0.00001639
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001638
Iteration 47/1000 | Loss: 0.00001638
Iteration 48/1000 | Loss: 0.00001638
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001636
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001634
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001634
Iteration 61/1000 | Loss: 0.00001634
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001632
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001630
Iteration 82/1000 | Loss: 0.00001630
Iteration 83/1000 | Loss: 0.00001630
Iteration 84/1000 | Loss: 0.00001630
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.6300811694236472e-05, 1.6300811694236472e-05, 1.6300811694236472e-05, 1.6300811694236472e-05, 1.6300811694236472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6300811694236472e-05

Optimization complete. Final v2v error: 3.403480291366577 mm

Highest mean error: 3.693690061569214 mm for frame 89

Lowest mean error: 3.1677958965301514 mm for frame 116

Saving results

Total time: 31.133259534835815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888403
Iteration 2/25 | Loss: 0.00146008
Iteration 3/25 | Loss: 0.00090485
Iteration 4/25 | Loss: 0.00076585
Iteration 5/25 | Loss: 0.00075081
Iteration 6/25 | Loss: 0.00074940
Iteration 7/25 | Loss: 0.00074933
Iteration 8/25 | Loss: 0.00074933
Iteration 9/25 | Loss: 0.00074933
Iteration 10/25 | Loss: 0.00074933
Iteration 11/25 | Loss: 0.00074933
Iteration 12/25 | Loss: 0.00074933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007493346347473562, 0.0007493346347473562, 0.0007493346347473562, 0.0007493346347473562, 0.0007493346347473562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007493346347473562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30474210
Iteration 2/25 | Loss: 0.00028215
Iteration 3/25 | Loss: 0.00028215
Iteration 4/25 | Loss: 0.00028215
Iteration 5/25 | Loss: 0.00028215
Iteration 6/25 | Loss: 0.00028215
Iteration 7/25 | Loss: 0.00028215
Iteration 8/25 | Loss: 0.00028215
Iteration 9/25 | Loss: 0.00028215
Iteration 10/25 | Loss: 0.00028215
Iteration 11/25 | Loss: 0.00028215
Iteration 12/25 | Loss: 0.00028215
Iteration 13/25 | Loss: 0.00028215
Iteration 14/25 | Loss: 0.00028215
Iteration 15/25 | Loss: 0.00028215
Iteration 16/25 | Loss: 0.00028215
Iteration 17/25 | Loss: 0.00028215
Iteration 18/25 | Loss: 0.00028215
Iteration 19/25 | Loss: 0.00028215
Iteration 20/25 | Loss: 0.00028215
Iteration 21/25 | Loss: 0.00028215
Iteration 22/25 | Loss: 0.00028215
Iteration 23/25 | Loss: 0.00028215
Iteration 24/25 | Loss: 0.00028215
Iteration 25/25 | Loss: 0.00028215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028215
Iteration 2/1000 | Loss: 0.00003467
Iteration 3/1000 | Loss: 0.00002434
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001831
Iteration 9/1000 | Loss: 0.00001808
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001769
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001742
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001741
Iteration 24/1000 | Loss: 0.00001740
Iteration 25/1000 | Loss: 0.00001739
Iteration 26/1000 | Loss: 0.00001739
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001735
Iteration 34/1000 | Loss: 0.00001735
Iteration 35/1000 | Loss: 0.00001735
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001733
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001733
Iteration 52/1000 | Loss: 0.00001733
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001730
Iteration 97/1000 | Loss: 0.00001730
Iteration 98/1000 | Loss: 0.00001730
Iteration 99/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.729914038151037e-05, 1.729914038151037e-05, 1.729914038151037e-05, 1.729914038151037e-05, 1.729914038151037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.729914038151037e-05

Optimization complete. Final v2v error: 3.507324457168579 mm

Highest mean error: 3.7511556148529053 mm for frame 105

Lowest mean error: 3.1301026344299316 mm for frame 30

Saving results

Total time: 31.729612588882446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044461
Iteration 2/25 | Loss: 0.00327336
Iteration 3/25 | Loss: 0.00232495
Iteration 4/25 | Loss: 0.00213298
Iteration 5/25 | Loss: 0.00207820
Iteration 6/25 | Loss: 0.00180017
Iteration 7/25 | Loss: 0.00161481
Iteration 8/25 | Loss: 0.00162863
Iteration 9/25 | Loss: 0.00148011
Iteration 10/25 | Loss: 0.00137126
Iteration 11/25 | Loss: 0.00135829
Iteration 12/25 | Loss: 0.00134287
Iteration 13/25 | Loss: 0.00131381
Iteration 14/25 | Loss: 0.00129865
Iteration 15/25 | Loss: 0.00128994
Iteration 16/25 | Loss: 0.00127830
Iteration 17/25 | Loss: 0.00127776
Iteration 18/25 | Loss: 0.00126634
Iteration 19/25 | Loss: 0.00126058
Iteration 20/25 | Loss: 0.00125883
Iteration 21/25 | Loss: 0.00125820
Iteration 22/25 | Loss: 0.00126149
Iteration 23/25 | Loss: 0.00125824
Iteration 24/25 | Loss: 0.00125716
Iteration 25/25 | Loss: 0.00125681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43474507
Iteration 2/25 | Loss: 0.00403882
Iteration 3/25 | Loss: 0.00403881
Iteration 4/25 | Loss: 0.00403881
Iteration 5/25 | Loss: 0.00403881
Iteration 6/25 | Loss: 0.00403881
Iteration 7/25 | Loss: 0.00403881
Iteration 8/25 | Loss: 0.00403881
Iteration 9/25 | Loss: 0.00403881
Iteration 10/25 | Loss: 0.00403881
Iteration 11/25 | Loss: 0.00403881
Iteration 12/25 | Loss: 0.00403881
Iteration 13/25 | Loss: 0.00403881
Iteration 14/25 | Loss: 0.00403881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0040388088673353195, 0.0040388088673353195, 0.0040388088673353195, 0.0040388088673353195, 0.0040388088673353195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0040388088673353195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00403881
Iteration 2/1000 | Loss: 0.00859803
Iteration 3/1000 | Loss: 0.00079993
Iteration 4/1000 | Loss: 0.00035395
Iteration 5/1000 | Loss: 0.00021091
Iteration 6/1000 | Loss: 0.00012007
Iteration 7/1000 | Loss: 0.00007440
Iteration 8/1000 | Loss: 0.00005073
Iteration 9/1000 | Loss: 0.00004100
Iteration 10/1000 | Loss: 0.00003378
Iteration 11/1000 | Loss: 0.00002902
Iteration 12/1000 | Loss: 0.00002564
Iteration 13/1000 | Loss: 0.00002369
Iteration 14/1000 | Loss: 0.00002220
Iteration 15/1000 | Loss: 0.00002148
Iteration 16/1000 | Loss: 0.00002082
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00001998
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001930
Iteration 22/1000 | Loss: 0.00001915
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001910
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001908
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001896
Iteration 33/1000 | Loss: 0.00001895
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001895
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001887
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001886
Iteration 42/1000 | Loss: 0.00001885
Iteration 43/1000 | Loss: 0.00001884
Iteration 44/1000 | Loss: 0.00001884
Iteration 45/1000 | Loss: 0.00001884
Iteration 46/1000 | Loss: 0.00001884
Iteration 47/1000 | Loss: 0.00001884
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00001883
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001878
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001877
Iteration 67/1000 | Loss: 0.00001877
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001877
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001876
Iteration 73/1000 | Loss: 0.00001876
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001875
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001873
Iteration 82/1000 | Loss: 0.00001873
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001872
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001872
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00001870
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00001870
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001869
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001868
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001868
Iteration 107/1000 | Loss: 0.00001868
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Iteration 123/1000 | Loss: 0.00001868
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.8679857021197677e-05, 1.8679857021197677e-05, 1.8679857021197677e-05, 1.8679857021197677e-05, 1.8679857021197677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8679857021197677e-05

Optimization complete. Final v2v error: 3.6411736011505127 mm

Highest mean error: 3.8534278869628906 mm for frame 231

Lowest mean error: 3.504225015640259 mm for frame 10

Saving results

Total time: 95.35925674438477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955328
Iteration 2/25 | Loss: 0.00148137
Iteration 3/25 | Loss: 0.00089019
Iteration 4/25 | Loss: 0.00080792
Iteration 5/25 | Loss: 0.00078499
Iteration 6/25 | Loss: 0.00077602
Iteration 7/25 | Loss: 0.00077469
Iteration 8/25 | Loss: 0.00077446
Iteration 9/25 | Loss: 0.00077446
Iteration 10/25 | Loss: 0.00077446
Iteration 11/25 | Loss: 0.00077446
Iteration 12/25 | Loss: 0.00077446
Iteration 13/25 | Loss: 0.00077446
Iteration 14/25 | Loss: 0.00077446
Iteration 15/25 | Loss: 0.00077446
Iteration 16/25 | Loss: 0.00077446
Iteration 17/25 | Loss: 0.00077446
Iteration 18/25 | Loss: 0.00077446
Iteration 19/25 | Loss: 0.00077446
Iteration 20/25 | Loss: 0.00077446
Iteration 21/25 | Loss: 0.00077446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007744551403447986, 0.0007744551403447986, 0.0007744551403447986, 0.0007744551403447986, 0.0007744551403447986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007744551403447986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13811326
Iteration 2/25 | Loss: 0.00025959
Iteration 3/25 | Loss: 0.00025958
Iteration 4/25 | Loss: 0.00025958
Iteration 5/25 | Loss: 0.00025958
Iteration 6/25 | Loss: 0.00025958
Iteration 7/25 | Loss: 0.00025958
Iteration 8/25 | Loss: 0.00025958
Iteration 9/25 | Loss: 0.00025958
Iteration 10/25 | Loss: 0.00025958
Iteration 11/25 | Loss: 0.00025958
Iteration 12/25 | Loss: 0.00025958
Iteration 13/25 | Loss: 0.00025958
Iteration 14/25 | Loss: 0.00025958
Iteration 15/25 | Loss: 0.00025958
Iteration 16/25 | Loss: 0.00025958
Iteration 17/25 | Loss: 0.00025958
Iteration 18/25 | Loss: 0.00025958
Iteration 19/25 | Loss: 0.00025958
Iteration 20/25 | Loss: 0.00025958
Iteration 21/25 | Loss: 0.00025958
Iteration 22/25 | Loss: 0.00025958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000259577325778082, 0.000259577325778082, 0.000259577325778082, 0.000259577325778082, 0.000259577325778082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000259577325778082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025958
Iteration 2/1000 | Loss: 0.00005511
Iteration 3/1000 | Loss: 0.00003851
Iteration 4/1000 | Loss: 0.00003535
Iteration 5/1000 | Loss: 0.00003393
Iteration 6/1000 | Loss: 0.00003249
Iteration 7/1000 | Loss: 0.00003160
Iteration 8/1000 | Loss: 0.00003065
Iteration 9/1000 | Loss: 0.00003007
Iteration 10/1000 | Loss: 0.00002976
Iteration 11/1000 | Loss: 0.00002956
Iteration 12/1000 | Loss: 0.00002939
Iteration 13/1000 | Loss: 0.00002923
Iteration 14/1000 | Loss: 0.00002918
Iteration 15/1000 | Loss: 0.00002918
Iteration 16/1000 | Loss: 0.00002913
Iteration 17/1000 | Loss: 0.00002913
Iteration 18/1000 | Loss: 0.00002912
Iteration 19/1000 | Loss: 0.00002911
Iteration 20/1000 | Loss: 0.00002911
Iteration 21/1000 | Loss: 0.00002911
Iteration 22/1000 | Loss: 0.00002909
Iteration 23/1000 | Loss: 0.00002909
Iteration 24/1000 | Loss: 0.00002908
Iteration 25/1000 | Loss: 0.00002908
Iteration 26/1000 | Loss: 0.00002907
Iteration 27/1000 | Loss: 0.00002907
Iteration 28/1000 | Loss: 0.00002905
Iteration 29/1000 | Loss: 0.00002905
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002903
Iteration 32/1000 | Loss: 0.00002900
Iteration 33/1000 | Loss: 0.00002899
Iteration 34/1000 | Loss: 0.00002899
Iteration 35/1000 | Loss: 0.00002894
Iteration 36/1000 | Loss: 0.00002890
Iteration 37/1000 | Loss: 0.00002890
Iteration 38/1000 | Loss: 0.00002887
Iteration 39/1000 | Loss: 0.00002886
Iteration 40/1000 | Loss: 0.00002882
Iteration 41/1000 | Loss: 0.00002882
Iteration 42/1000 | Loss: 0.00002882
Iteration 43/1000 | Loss: 0.00002882
Iteration 44/1000 | Loss: 0.00002882
Iteration 45/1000 | Loss: 0.00002881
Iteration 46/1000 | Loss: 0.00002881
Iteration 47/1000 | Loss: 0.00002881
Iteration 48/1000 | Loss: 0.00002877
Iteration 49/1000 | Loss: 0.00002877
Iteration 50/1000 | Loss: 0.00002877
Iteration 51/1000 | Loss: 0.00002877
Iteration 52/1000 | Loss: 0.00002876
Iteration 53/1000 | Loss: 0.00002876
Iteration 54/1000 | Loss: 0.00002876
Iteration 55/1000 | Loss: 0.00002876
Iteration 56/1000 | Loss: 0.00002875
Iteration 57/1000 | Loss: 0.00002873
Iteration 58/1000 | Loss: 0.00002873
Iteration 59/1000 | Loss: 0.00002873
Iteration 60/1000 | Loss: 0.00002873
Iteration 61/1000 | Loss: 0.00002873
Iteration 62/1000 | Loss: 0.00002872
Iteration 63/1000 | Loss: 0.00002872
Iteration 64/1000 | Loss: 0.00002872
Iteration 65/1000 | Loss: 0.00002872
Iteration 66/1000 | Loss: 0.00002872
Iteration 67/1000 | Loss: 0.00002872
Iteration 68/1000 | Loss: 0.00002872
Iteration 69/1000 | Loss: 0.00002872
Iteration 70/1000 | Loss: 0.00002872
Iteration 71/1000 | Loss: 0.00002872
Iteration 72/1000 | Loss: 0.00002871
Iteration 73/1000 | Loss: 0.00002870
Iteration 74/1000 | Loss: 0.00002870
Iteration 75/1000 | Loss: 0.00002869
Iteration 76/1000 | Loss: 0.00002869
Iteration 77/1000 | Loss: 0.00002869
Iteration 78/1000 | Loss: 0.00002869
Iteration 79/1000 | Loss: 0.00002869
Iteration 80/1000 | Loss: 0.00002869
Iteration 81/1000 | Loss: 0.00002869
Iteration 82/1000 | Loss: 0.00002868
Iteration 83/1000 | Loss: 0.00002868
Iteration 84/1000 | Loss: 0.00002867
Iteration 85/1000 | Loss: 0.00002867
Iteration 86/1000 | Loss: 0.00002866
Iteration 87/1000 | Loss: 0.00002866
Iteration 88/1000 | Loss: 0.00002866
Iteration 89/1000 | Loss: 0.00002864
Iteration 90/1000 | Loss: 0.00002864
Iteration 91/1000 | Loss: 0.00002864
Iteration 92/1000 | Loss: 0.00002864
Iteration 93/1000 | Loss: 0.00002864
Iteration 94/1000 | Loss: 0.00002864
Iteration 95/1000 | Loss: 0.00002864
Iteration 96/1000 | Loss: 0.00002863
Iteration 97/1000 | Loss: 0.00002862
Iteration 98/1000 | Loss: 0.00002862
Iteration 99/1000 | Loss: 0.00002861
Iteration 100/1000 | Loss: 0.00002861
Iteration 101/1000 | Loss: 0.00002861
Iteration 102/1000 | Loss: 0.00002861
Iteration 103/1000 | Loss: 0.00002861
Iteration 104/1000 | Loss: 0.00002861
Iteration 105/1000 | Loss: 0.00002860
Iteration 106/1000 | Loss: 0.00002860
Iteration 107/1000 | Loss: 0.00002860
Iteration 108/1000 | Loss: 0.00002858
Iteration 109/1000 | Loss: 0.00002858
Iteration 110/1000 | Loss: 0.00002858
Iteration 111/1000 | Loss: 0.00002858
Iteration 112/1000 | Loss: 0.00002858
Iteration 113/1000 | Loss: 0.00002858
Iteration 114/1000 | Loss: 0.00002858
Iteration 115/1000 | Loss: 0.00002858
Iteration 116/1000 | Loss: 0.00002858
Iteration 117/1000 | Loss: 0.00002858
Iteration 118/1000 | Loss: 0.00002857
Iteration 119/1000 | Loss: 0.00002857
Iteration 120/1000 | Loss: 0.00002857
Iteration 121/1000 | Loss: 0.00002857
Iteration 122/1000 | Loss: 0.00002857
Iteration 123/1000 | Loss: 0.00002856
Iteration 124/1000 | Loss: 0.00002856
Iteration 125/1000 | Loss: 0.00002856
Iteration 126/1000 | Loss: 0.00002855
Iteration 127/1000 | Loss: 0.00002855
Iteration 128/1000 | Loss: 0.00002855
Iteration 129/1000 | Loss: 0.00002855
Iteration 130/1000 | Loss: 0.00002855
Iteration 131/1000 | Loss: 0.00002854
Iteration 132/1000 | Loss: 0.00002854
Iteration 133/1000 | Loss: 0.00002854
Iteration 134/1000 | Loss: 0.00002854
Iteration 135/1000 | Loss: 0.00002853
Iteration 136/1000 | Loss: 0.00002853
Iteration 137/1000 | Loss: 0.00002853
Iteration 138/1000 | Loss: 0.00002853
Iteration 139/1000 | Loss: 0.00002853
Iteration 140/1000 | Loss: 0.00002853
Iteration 141/1000 | Loss: 0.00002853
Iteration 142/1000 | Loss: 0.00002853
Iteration 143/1000 | Loss: 0.00002853
Iteration 144/1000 | Loss: 0.00002853
Iteration 145/1000 | Loss: 0.00002853
Iteration 146/1000 | Loss: 0.00002853
Iteration 147/1000 | Loss: 0.00002853
Iteration 148/1000 | Loss: 0.00002853
Iteration 149/1000 | Loss: 0.00002853
Iteration 150/1000 | Loss: 0.00002853
Iteration 151/1000 | Loss: 0.00002853
Iteration 152/1000 | Loss: 0.00002853
Iteration 153/1000 | Loss: 0.00002853
Iteration 154/1000 | Loss: 0.00002853
Iteration 155/1000 | Loss: 0.00002853
Iteration 156/1000 | Loss: 0.00002853
Iteration 157/1000 | Loss: 0.00002853
Iteration 158/1000 | Loss: 0.00002853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.8525946618174203e-05, 2.8525946618174203e-05, 2.8525946618174203e-05, 2.8525946618174203e-05, 2.8525946618174203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8525946618174203e-05

Optimization complete. Final v2v error: 4.304201602935791 mm

Highest mean error: 5.421648979187012 mm for frame 85

Lowest mean error: 3.577773094177246 mm for frame 113

Saving results

Total time: 45.67507576942444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038604
Iteration 2/25 | Loss: 0.00217952
Iteration 3/25 | Loss: 0.00156490
Iteration 4/25 | Loss: 0.00135028
Iteration 5/25 | Loss: 0.00111965
Iteration 6/25 | Loss: 0.00093881
Iteration 7/25 | Loss: 0.00086215
Iteration 8/25 | Loss: 0.00083199
Iteration 9/25 | Loss: 0.00082815
Iteration 10/25 | Loss: 0.00082751
Iteration 11/25 | Loss: 0.00082730
Iteration 12/25 | Loss: 0.00082726
Iteration 13/25 | Loss: 0.00082725
Iteration 14/25 | Loss: 0.00082725
Iteration 15/25 | Loss: 0.00082725
Iteration 16/25 | Loss: 0.00082725
Iteration 17/25 | Loss: 0.00082725
Iteration 18/25 | Loss: 0.00082725
Iteration 19/25 | Loss: 0.00082725
Iteration 20/25 | Loss: 0.00082725
Iteration 21/25 | Loss: 0.00082725
Iteration 22/25 | Loss: 0.00082725
Iteration 23/25 | Loss: 0.00082725
Iteration 24/25 | Loss: 0.00082725
Iteration 25/25 | Loss: 0.00082725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44300604
Iteration 2/25 | Loss: 0.00050894
Iteration 3/25 | Loss: 0.00050894
Iteration 4/25 | Loss: 0.00050894
Iteration 5/25 | Loss: 0.00050894
Iteration 6/25 | Loss: 0.00050894
Iteration 7/25 | Loss: 0.00050894
Iteration 8/25 | Loss: 0.00050894
Iteration 9/25 | Loss: 0.00050894
Iteration 10/25 | Loss: 0.00050894
Iteration 11/25 | Loss: 0.00050894
Iteration 12/25 | Loss: 0.00050894
Iteration 13/25 | Loss: 0.00050894
Iteration 14/25 | Loss: 0.00050894
Iteration 15/25 | Loss: 0.00050894
Iteration 16/25 | Loss: 0.00050894
Iteration 17/25 | Loss: 0.00050894
Iteration 18/25 | Loss: 0.00050894
Iteration 19/25 | Loss: 0.00050894
Iteration 20/25 | Loss: 0.00050894
Iteration 21/25 | Loss: 0.00050894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000508940895088017, 0.000508940895088017, 0.000508940895088017, 0.000508940895088017, 0.000508940895088017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000508940895088017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050894
Iteration 2/1000 | Loss: 0.00003248
Iteration 3/1000 | Loss: 0.00002390
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00002165
Iteration 6/1000 | Loss: 0.00002120
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00002070
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002051
Iteration 11/1000 | Loss: 0.00002047
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00002036
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002033
Iteration 16/1000 | Loss: 0.00002032
Iteration 17/1000 | Loss: 0.00002027
Iteration 18/1000 | Loss: 0.00002022
Iteration 19/1000 | Loss: 0.00002021
Iteration 20/1000 | Loss: 0.00002021
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002019
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002018
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002018
Iteration 34/1000 | Loss: 0.00002018
Iteration 35/1000 | Loss: 0.00002017
Iteration 36/1000 | Loss: 0.00002017
Iteration 37/1000 | Loss: 0.00002017
Iteration 38/1000 | Loss: 0.00002017
Iteration 39/1000 | Loss: 0.00002017
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002015
Iteration 44/1000 | Loss: 0.00002014
Iteration 45/1000 | Loss: 0.00002014
Iteration 46/1000 | Loss: 0.00002014
Iteration 47/1000 | Loss: 0.00002014
Iteration 48/1000 | Loss: 0.00002013
Iteration 49/1000 | Loss: 0.00002013
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002013
Iteration 53/1000 | Loss: 0.00002013
Iteration 54/1000 | Loss: 0.00002013
Iteration 55/1000 | Loss: 0.00002013
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002012
Iteration 59/1000 | Loss: 0.00002012
Iteration 60/1000 | Loss: 0.00002012
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002102
Iteration 63/1000 | Loss: 0.00002102
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002101
Iteration 66/1000 | Loss: 0.00002101
Iteration 67/1000 | Loss: 0.00002101
Iteration 68/1000 | Loss: 0.00002101
Iteration 69/1000 | Loss: 0.00002101
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00002100
Iteration 73/1000 | Loss: 0.00002084
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002022
Iteration 76/1000 | Loss: 0.00002019
Iteration 77/1000 | Loss: 0.00002019
Iteration 78/1000 | Loss: 0.00002018
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002016
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00002016
Iteration 86/1000 | Loss: 0.00002016
Iteration 87/1000 | Loss: 0.00002015
Iteration 88/1000 | Loss: 0.00002015
Iteration 89/1000 | Loss: 0.00002015
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002014
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002013
Iteration 100/1000 | Loss: 0.00002013
Iteration 101/1000 | Loss: 0.00002012
Iteration 102/1000 | Loss: 0.00002012
Iteration 103/1000 | Loss: 0.00002012
Iteration 104/1000 | Loss: 0.00002012
Iteration 105/1000 | Loss: 0.00002012
Iteration 106/1000 | Loss: 0.00002011
Iteration 107/1000 | Loss: 0.00002011
Iteration 108/1000 | Loss: 0.00002011
Iteration 109/1000 | Loss: 0.00002011
Iteration 110/1000 | Loss: 0.00002010
Iteration 111/1000 | Loss: 0.00002010
Iteration 112/1000 | Loss: 0.00002008
Iteration 113/1000 | Loss: 0.00002008
Iteration 114/1000 | Loss: 0.00002008
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002006
Iteration 119/1000 | Loss: 0.00002006
Iteration 120/1000 | Loss: 0.00002006
Iteration 121/1000 | Loss: 0.00002005
Iteration 122/1000 | Loss: 0.00002005
Iteration 123/1000 | Loss: 0.00002005
Iteration 124/1000 | Loss: 0.00002005
Iteration 125/1000 | Loss: 0.00002005
Iteration 126/1000 | Loss: 0.00002005
Iteration 127/1000 | Loss: 0.00002005
Iteration 128/1000 | Loss: 0.00002005
Iteration 129/1000 | Loss: 0.00002004
Iteration 130/1000 | Loss: 0.00002004
Iteration 131/1000 | Loss: 0.00002004
Iteration 132/1000 | Loss: 0.00002004
Iteration 133/1000 | Loss: 0.00002004
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002003
Iteration 136/1000 | Loss: 0.00002003
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002001
Iteration 141/1000 | Loss: 0.00002001
Iteration 142/1000 | Loss: 0.00002001
Iteration 143/1000 | Loss: 0.00002001
Iteration 144/1000 | Loss: 0.00002001
Iteration 145/1000 | Loss: 0.00002000
Iteration 146/1000 | Loss: 0.00002000
Iteration 147/1000 | Loss: 0.00002000
Iteration 148/1000 | Loss: 0.00002000
Iteration 149/1000 | Loss: 0.00002000
Iteration 150/1000 | Loss: 0.00002000
Iteration 151/1000 | Loss: 0.00001999
Iteration 152/1000 | Loss: 0.00001999
Iteration 153/1000 | Loss: 0.00001999
Iteration 154/1000 | Loss: 0.00001999
Iteration 155/1000 | Loss: 0.00001998
Iteration 156/1000 | Loss: 0.00001998
Iteration 157/1000 | Loss: 0.00001998
Iteration 158/1000 | Loss: 0.00001998
Iteration 159/1000 | Loss: 0.00001998
Iteration 160/1000 | Loss: 0.00001998
Iteration 161/1000 | Loss: 0.00001998
Iteration 162/1000 | Loss: 0.00001998
Iteration 163/1000 | Loss: 0.00001998
Iteration 164/1000 | Loss: 0.00001998
Iteration 165/1000 | Loss: 0.00001998
Iteration 166/1000 | Loss: 0.00001998
Iteration 167/1000 | Loss: 0.00001997
Iteration 168/1000 | Loss: 0.00001997
Iteration 169/1000 | Loss: 0.00001997
Iteration 170/1000 | Loss: 0.00001997
Iteration 171/1000 | Loss: 0.00001997
Iteration 172/1000 | Loss: 0.00001997
Iteration 173/1000 | Loss: 0.00001997
Iteration 174/1000 | Loss: 0.00001997
Iteration 175/1000 | Loss: 0.00001997
Iteration 176/1000 | Loss: 0.00001997
Iteration 177/1000 | Loss: 0.00001997
Iteration 178/1000 | Loss: 0.00001997
Iteration 179/1000 | Loss: 0.00001997
Iteration 180/1000 | Loss: 0.00001997
Iteration 181/1000 | Loss: 0.00001997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.997386607399676e-05, 1.997386607399676e-05, 1.997386607399676e-05, 1.997386607399676e-05, 1.997386607399676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.997386607399676e-05

Optimization complete. Final v2v error: 3.6383931636810303 mm

Highest mean error: 10.446806907653809 mm for frame 7

Lowest mean error: 3.4579083919525146 mm for frame 51

Saving results

Total time: 58.66868495941162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449568
Iteration 2/25 | Loss: 0.00092181
Iteration 3/25 | Loss: 0.00070144
Iteration 4/25 | Loss: 0.00067600
Iteration 5/25 | Loss: 0.00066707
Iteration 6/25 | Loss: 0.00066539
Iteration 7/25 | Loss: 0.00066531
Iteration 8/25 | Loss: 0.00066528
Iteration 9/25 | Loss: 0.00066528
Iteration 10/25 | Loss: 0.00066528
Iteration 11/25 | Loss: 0.00066528
Iteration 12/25 | Loss: 0.00066528
Iteration 13/25 | Loss: 0.00066528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006652810843661427, 0.0006652810843661427, 0.0006652810843661427, 0.0006652810843661427, 0.0006652810843661427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006652810843661427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45051372
Iteration 2/25 | Loss: 0.00028048
Iteration 3/25 | Loss: 0.00028048
Iteration 4/25 | Loss: 0.00028048
Iteration 5/25 | Loss: 0.00028048
Iteration 6/25 | Loss: 0.00028048
Iteration 7/25 | Loss: 0.00028048
Iteration 8/25 | Loss: 0.00028048
Iteration 9/25 | Loss: 0.00028048
Iteration 10/25 | Loss: 0.00028048
Iteration 11/25 | Loss: 0.00028048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00028047888190485537, 0.00028047888190485537, 0.00028047888190485537, 0.00028047888190485537, 0.00028047888190485537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028047888190485537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028048
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001807
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001771
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001764
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001763
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001759
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001752
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001748
Iteration 31/1000 | Loss: 0.00001748
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001747
Iteration 37/1000 | Loss: 0.00001747
Iteration 38/1000 | Loss: 0.00001747
Iteration 39/1000 | Loss: 0.00001747
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001746
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001746
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001746
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001745
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001745
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001744
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.7441825548303314e-05, 1.7441825548303314e-05, 1.7441825548303314e-05, 1.7441825548303314e-05, 1.7441825548303314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7441825548303314e-05

Optimization complete. Final v2v error: 3.5207407474517822 mm

Highest mean error: 3.954261064529419 mm for frame 23

Lowest mean error: 3.2138540744781494 mm for frame 108

Saving results

Total time: 31.020339250564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463693
Iteration 2/25 | Loss: 0.00106054
Iteration 3/25 | Loss: 0.00075890
Iteration 4/25 | Loss: 0.00069321
Iteration 5/25 | Loss: 0.00068230
Iteration 6/25 | Loss: 0.00067901
Iteration 7/25 | Loss: 0.00067868
Iteration 8/25 | Loss: 0.00067868
Iteration 9/25 | Loss: 0.00067868
Iteration 10/25 | Loss: 0.00067868
Iteration 11/25 | Loss: 0.00067868
Iteration 12/25 | Loss: 0.00067868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006786779849790037, 0.0006786779849790037, 0.0006786779849790037, 0.0006786779849790037, 0.0006786779849790037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006786779849790037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47746003
Iteration 2/25 | Loss: 0.00030096
Iteration 3/25 | Loss: 0.00030094
Iteration 4/25 | Loss: 0.00030094
Iteration 5/25 | Loss: 0.00030094
Iteration 6/25 | Loss: 0.00030094
Iteration 7/25 | Loss: 0.00030094
Iteration 8/25 | Loss: 0.00030094
Iteration 9/25 | Loss: 0.00030094
Iteration 10/25 | Loss: 0.00030094
Iteration 11/25 | Loss: 0.00030094
Iteration 12/25 | Loss: 0.00030094
Iteration 13/25 | Loss: 0.00030094
Iteration 14/25 | Loss: 0.00030094
Iteration 15/25 | Loss: 0.00030094
Iteration 16/25 | Loss: 0.00030094
Iteration 17/25 | Loss: 0.00030094
Iteration 18/25 | Loss: 0.00030094
Iteration 19/25 | Loss: 0.00030094
Iteration 20/25 | Loss: 0.00030094
Iteration 21/25 | Loss: 0.00030094
Iteration 22/25 | Loss: 0.00030094
Iteration 23/25 | Loss: 0.00030094
Iteration 24/25 | Loss: 0.00030094
Iteration 25/25 | Loss: 0.00030094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030094
Iteration 2/1000 | Loss: 0.00002762
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001892
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001719
Iteration 7/1000 | Loss: 0.00001679
Iteration 8/1000 | Loss: 0.00001642
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001610
Iteration 11/1000 | Loss: 0.00001608
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001568
Iteration 31/1000 | Loss: 0.00001568
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001566
Iteration 34/1000 | Loss: 0.00001566
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001555
Iteration 47/1000 | Loss: 0.00001554
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001550
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001544
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001541
Iteration 94/1000 | Loss: 0.00001541
Iteration 95/1000 | Loss: 0.00001540
Iteration 96/1000 | Loss: 0.00001540
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.5395484297187068e-05, 1.5395484297187068e-05, 1.5395484297187068e-05, 1.5395484297187068e-05, 1.5395484297187068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5395484297187068e-05

Optimization complete. Final v2v error: 3.2680768966674805 mm

Highest mean error: 3.5116758346557617 mm for frame 172

Lowest mean error: 2.9283366203308105 mm for frame 157

Saving results

Total time: 39.341365337371826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866473
Iteration 2/25 | Loss: 0.00091813
Iteration 3/25 | Loss: 0.00071599
Iteration 4/25 | Loss: 0.00068994
Iteration 5/25 | Loss: 0.00068063
Iteration 6/25 | Loss: 0.00067775
Iteration 7/25 | Loss: 0.00067681
Iteration 8/25 | Loss: 0.00067632
Iteration 9/25 | Loss: 0.00067823
Iteration 10/25 | Loss: 0.00067804
Iteration 11/25 | Loss: 0.00067549
Iteration 12/25 | Loss: 0.00067780
Iteration 13/25 | Loss: 0.00067847
Iteration 14/25 | Loss: 0.00067530
Iteration 15/25 | Loss: 0.00067440
Iteration 16/25 | Loss: 0.00067370
Iteration 17/25 | Loss: 0.00067322
Iteration 18/25 | Loss: 0.00067275
Iteration 19/25 | Loss: 0.00067246
Iteration 20/25 | Loss: 0.00067225
Iteration 21/25 | Loss: 0.00067217
Iteration 22/25 | Loss: 0.00067217
Iteration 23/25 | Loss: 0.00067217
Iteration 24/25 | Loss: 0.00067217
Iteration 25/25 | Loss: 0.00067217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46332121
Iteration 2/25 | Loss: 0.00109024
Iteration 3/25 | Loss: 0.00109023
Iteration 4/25 | Loss: 0.00109023
Iteration 5/25 | Loss: 0.00109023
Iteration 6/25 | Loss: 0.00109023
Iteration 7/25 | Loss: 0.00109023
Iteration 8/25 | Loss: 0.00109023
Iteration 9/25 | Loss: 0.00109023
Iteration 10/25 | Loss: 0.00109023
Iteration 11/25 | Loss: 0.00109023
Iteration 12/25 | Loss: 0.00109023
Iteration 13/25 | Loss: 0.00109023
Iteration 14/25 | Loss: 0.00109023
Iteration 15/25 | Loss: 0.00109023
Iteration 16/25 | Loss: 0.00109023
Iteration 17/25 | Loss: 0.00109023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010902303038164973, 0.0010902303038164973, 0.0010902303038164973, 0.0010902303038164973, 0.0010902303038164973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010902303038164973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109023
Iteration 2/1000 | Loss: 0.00011057
Iteration 3/1000 | Loss: 0.00008057
Iteration 4/1000 | Loss: 0.00006957
Iteration 5/1000 | Loss: 0.00006416
Iteration 6/1000 | Loss: 0.00006042
Iteration 7/1000 | Loss: 0.00005871
Iteration 8/1000 | Loss: 0.00005647
Iteration 9/1000 | Loss: 0.00005483
Iteration 10/1000 | Loss: 0.00005421
Iteration 11/1000 | Loss: 0.00005364
Iteration 12/1000 | Loss: 0.00005305
Iteration 13/1000 | Loss: 0.00041980
Iteration 14/1000 | Loss: 0.00005638
Iteration 15/1000 | Loss: 0.00005287
Iteration 16/1000 | Loss: 0.00005159
Iteration 17/1000 | Loss: 0.00005084
Iteration 18/1000 | Loss: 0.00054518
Iteration 19/1000 | Loss: 0.00037453
Iteration 20/1000 | Loss: 0.00008798
Iteration 21/1000 | Loss: 0.00005892
Iteration 22/1000 | Loss: 0.00005511
Iteration 23/1000 | Loss: 0.00149103
Iteration 24/1000 | Loss: 0.00060057
Iteration 25/1000 | Loss: 0.00007424
Iteration 26/1000 | Loss: 0.00151352
Iteration 27/1000 | Loss: 0.00037685
Iteration 28/1000 | Loss: 0.00249134
Iteration 29/1000 | Loss: 0.00089543
Iteration 30/1000 | Loss: 0.00009623
Iteration 31/1000 | Loss: 0.00006785
Iteration 32/1000 | Loss: 0.00006259
Iteration 33/1000 | Loss: 0.00105947
Iteration 34/1000 | Loss: 0.00009971
Iteration 35/1000 | Loss: 0.00005749
Iteration 36/1000 | Loss: 0.00005215
Iteration 37/1000 | Loss: 0.00004923
Iteration 38/1000 | Loss: 0.00004721
Iteration 39/1000 | Loss: 0.00040773
Iteration 40/1000 | Loss: 0.00004678
Iteration 41/1000 | Loss: 0.00004421
Iteration 42/1000 | Loss: 0.00004256
Iteration 43/1000 | Loss: 0.00004112
Iteration 44/1000 | Loss: 0.00004006
Iteration 45/1000 | Loss: 0.00003923
Iteration 46/1000 | Loss: 0.00003867
Iteration 47/1000 | Loss: 0.00042005
Iteration 48/1000 | Loss: 0.00004319
Iteration 49/1000 | Loss: 0.00003777
Iteration 50/1000 | Loss: 0.00016487
Iteration 51/1000 | Loss: 0.00013948
Iteration 52/1000 | Loss: 0.00003705
Iteration 53/1000 | Loss: 0.00015553
Iteration 54/1000 | Loss: 0.00016781
Iteration 55/1000 | Loss: 0.00013289
Iteration 56/1000 | Loss: 0.00015344
Iteration 57/1000 | Loss: 0.00012968
Iteration 58/1000 | Loss: 0.00014427
Iteration 59/1000 | Loss: 0.00012175
Iteration 60/1000 | Loss: 0.00014831
Iteration 61/1000 | Loss: 0.00003804
Iteration 62/1000 | Loss: 0.00003693
Iteration 63/1000 | Loss: 0.00014412
Iteration 64/1000 | Loss: 0.00015810
Iteration 65/1000 | Loss: 0.00004761
Iteration 66/1000 | Loss: 0.00008054
Iteration 67/1000 | Loss: 0.00003797
Iteration 68/1000 | Loss: 0.00003680
Iteration 69/1000 | Loss: 0.00004199
Iteration 70/1000 | Loss: 0.00004002
Iteration 71/1000 | Loss: 0.00003799
Iteration 72/1000 | Loss: 0.00003719
Iteration 73/1000 | Loss: 0.00004140
Iteration 74/1000 | Loss: 0.00003581
Iteration 75/1000 | Loss: 0.00003592
Iteration 76/1000 | Loss: 0.00003532
Iteration 77/1000 | Loss: 0.00038539
Iteration 78/1000 | Loss: 0.00039851
Iteration 79/1000 | Loss: 0.00014075
Iteration 80/1000 | Loss: 0.00010357
Iteration 81/1000 | Loss: 0.00003537
Iteration 82/1000 | Loss: 0.00003387
Iteration 83/1000 | Loss: 0.00003320
Iteration 84/1000 | Loss: 0.00003260
Iteration 85/1000 | Loss: 0.00003207
Iteration 86/1000 | Loss: 0.00003169
Iteration 87/1000 | Loss: 0.00003145
Iteration 88/1000 | Loss: 0.00003129
Iteration 89/1000 | Loss: 0.00003127
Iteration 90/1000 | Loss: 0.00003122
Iteration 91/1000 | Loss: 0.00003120
Iteration 92/1000 | Loss: 0.00003105
Iteration 93/1000 | Loss: 0.00035746
Iteration 94/1000 | Loss: 0.00005871
Iteration 95/1000 | Loss: 0.00003267
Iteration 96/1000 | Loss: 0.00003072
Iteration 97/1000 | Loss: 0.00002988
Iteration 98/1000 | Loss: 0.00002958
Iteration 99/1000 | Loss: 0.00002950
Iteration 100/1000 | Loss: 0.00002948
Iteration 101/1000 | Loss: 0.00002947
Iteration 102/1000 | Loss: 0.00002936
Iteration 103/1000 | Loss: 0.00002935
Iteration 104/1000 | Loss: 0.00002930
Iteration 105/1000 | Loss: 0.00002927
Iteration 106/1000 | Loss: 0.00002926
Iteration 107/1000 | Loss: 0.00002925
Iteration 108/1000 | Loss: 0.00002924
Iteration 109/1000 | Loss: 0.00002923
Iteration 110/1000 | Loss: 0.00002923
Iteration 111/1000 | Loss: 0.00002923
Iteration 112/1000 | Loss: 0.00002922
Iteration 113/1000 | Loss: 0.00002922
Iteration 114/1000 | Loss: 0.00002922
Iteration 115/1000 | Loss: 0.00002921
Iteration 116/1000 | Loss: 0.00002920
Iteration 117/1000 | Loss: 0.00002920
Iteration 118/1000 | Loss: 0.00002919
Iteration 119/1000 | Loss: 0.00002919
Iteration 120/1000 | Loss: 0.00002918
Iteration 121/1000 | Loss: 0.00002918
Iteration 122/1000 | Loss: 0.00002918
Iteration 123/1000 | Loss: 0.00002918
Iteration 124/1000 | Loss: 0.00002917
Iteration 125/1000 | Loss: 0.00002917
Iteration 126/1000 | Loss: 0.00002916
Iteration 127/1000 | Loss: 0.00002916
Iteration 128/1000 | Loss: 0.00002915
Iteration 129/1000 | Loss: 0.00002915
Iteration 130/1000 | Loss: 0.00002915
Iteration 131/1000 | Loss: 0.00002914
Iteration 132/1000 | Loss: 0.00002914
Iteration 133/1000 | Loss: 0.00002914
Iteration 134/1000 | Loss: 0.00002913
Iteration 135/1000 | Loss: 0.00002913
Iteration 136/1000 | Loss: 0.00002913
Iteration 137/1000 | Loss: 0.00002913
Iteration 138/1000 | Loss: 0.00002913
Iteration 139/1000 | Loss: 0.00002913
Iteration 140/1000 | Loss: 0.00002912
Iteration 141/1000 | Loss: 0.00002912
Iteration 142/1000 | Loss: 0.00002912
Iteration 143/1000 | Loss: 0.00002912
Iteration 144/1000 | Loss: 0.00002912
Iteration 145/1000 | Loss: 0.00002912
Iteration 146/1000 | Loss: 0.00002911
Iteration 147/1000 | Loss: 0.00002911
Iteration 148/1000 | Loss: 0.00002911
Iteration 149/1000 | Loss: 0.00002911
Iteration 150/1000 | Loss: 0.00002910
Iteration 151/1000 | Loss: 0.00002910
Iteration 152/1000 | Loss: 0.00002910
Iteration 153/1000 | Loss: 0.00002910
Iteration 154/1000 | Loss: 0.00002910
Iteration 155/1000 | Loss: 0.00002909
Iteration 156/1000 | Loss: 0.00002909
Iteration 157/1000 | Loss: 0.00002909
Iteration 158/1000 | Loss: 0.00002908
Iteration 159/1000 | Loss: 0.00002908
Iteration 160/1000 | Loss: 0.00002908
Iteration 161/1000 | Loss: 0.00002908
Iteration 162/1000 | Loss: 0.00002907
Iteration 163/1000 | Loss: 0.00002907
Iteration 164/1000 | Loss: 0.00002907
Iteration 165/1000 | Loss: 0.00002907
Iteration 166/1000 | Loss: 0.00002907
Iteration 167/1000 | Loss: 0.00002906
Iteration 168/1000 | Loss: 0.00002906
Iteration 169/1000 | Loss: 0.00002906
Iteration 170/1000 | Loss: 0.00002906
Iteration 171/1000 | Loss: 0.00002906
Iteration 172/1000 | Loss: 0.00002906
Iteration 173/1000 | Loss: 0.00002906
Iteration 174/1000 | Loss: 0.00002905
Iteration 175/1000 | Loss: 0.00002905
Iteration 176/1000 | Loss: 0.00002905
Iteration 177/1000 | Loss: 0.00002905
Iteration 178/1000 | Loss: 0.00002905
Iteration 179/1000 | Loss: 0.00002905
Iteration 180/1000 | Loss: 0.00002905
Iteration 181/1000 | Loss: 0.00002905
Iteration 182/1000 | Loss: 0.00002904
Iteration 183/1000 | Loss: 0.00002904
Iteration 184/1000 | Loss: 0.00002904
Iteration 185/1000 | Loss: 0.00002903
Iteration 186/1000 | Loss: 0.00002903
Iteration 187/1000 | Loss: 0.00002903
Iteration 188/1000 | Loss: 0.00002903
Iteration 189/1000 | Loss: 0.00002903
Iteration 190/1000 | Loss: 0.00002903
Iteration 191/1000 | Loss: 0.00002903
Iteration 192/1000 | Loss: 0.00002903
Iteration 193/1000 | Loss: 0.00002902
Iteration 194/1000 | Loss: 0.00002902
Iteration 195/1000 | Loss: 0.00002902
Iteration 196/1000 | Loss: 0.00002902
Iteration 197/1000 | Loss: 0.00002902
Iteration 198/1000 | Loss: 0.00002902
Iteration 199/1000 | Loss: 0.00002902
Iteration 200/1000 | Loss: 0.00002901
Iteration 201/1000 | Loss: 0.00002901
Iteration 202/1000 | Loss: 0.00002901
Iteration 203/1000 | Loss: 0.00002901
Iteration 204/1000 | Loss: 0.00002900
Iteration 205/1000 | Loss: 0.00002900
Iteration 206/1000 | Loss: 0.00002900
Iteration 207/1000 | Loss: 0.00002900
Iteration 208/1000 | Loss: 0.00002900
Iteration 209/1000 | Loss: 0.00002899
Iteration 210/1000 | Loss: 0.00002899
Iteration 211/1000 | Loss: 0.00002899
Iteration 212/1000 | Loss: 0.00002899
Iteration 213/1000 | Loss: 0.00002899
Iteration 214/1000 | Loss: 0.00002899
Iteration 215/1000 | Loss: 0.00002899
Iteration 216/1000 | Loss: 0.00002899
Iteration 217/1000 | Loss: 0.00002899
Iteration 218/1000 | Loss: 0.00002899
Iteration 219/1000 | Loss: 0.00002899
Iteration 220/1000 | Loss: 0.00002898
Iteration 221/1000 | Loss: 0.00002898
Iteration 222/1000 | Loss: 0.00002898
Iteration 223/1000 | Loss: 0.00002898
Iteration 224/1000 | Loss: 0.00002898
Iteration 225/1000 | Loss: 0.00002898
Iteration 226/1000 | Loss: 0.00002898
Iteration 227/1000 | Loss: 0.00002898
Iteration 228/1000 | Loss: 0.00002898
Iteration 229/1000 | Loss: 0.00002898
Iteration 230/1000 | Loss: 0.00002897
Iteration 231/1000 | Loss: 0.00002897
Iteration 232/1000 | Loss: 0.00002897
Iteration 233/1000 | Loss: 0.00002897
Iteration 234/1000 | Loss: 0.00002897
Iteration 235/1000 | Loss: 0.00002897
Iteration 236/1000 | Loss: 0.00002897
Iteration 237/1000 | Loss: 0.00002897
Iteration 238/1000 | Loss: 0.00002897
Iteration 239/1000 | Loss: 0.00002897
Iteration 240/1000 | Loss: 0.00002897
Iteration 241/1000 | Loss: 0.00002897
Iteration 242/1000 | Loss: 0.00002897
Iteration 243/1000 | Loss: 0.00002896
Iteration 244/1000 | Loss: 0.00002896
Iteration 245/1000 | Loss: 0.00002896
Iteration 246/1000 | Loss: 0.00002896
Iteration 247/1000 | Loss: 0.00002896
Iteration 248/1000 | Loss: 0.00002896
Iteration 249/1000 | Loss: 0.00002896
Iteration 250/1000 | Loss: 0.00002896
Iteration 251/1000 | Loss: 0.00002896
Iteration 252/1000 | Loss: 0.00002896
Iteration 253/1000 | Loss: 0.00002896
Iteration 254/1000 | Loss: 0.00002896
Iteration 255/1000 | Loss: 0.00002896
Iteration 256/1000 | Loss: 0.00002896
Iteration 257/1000 | Loss: 0.00002896
Iteration 258/1000 | Loss: 0.00002896
Iteration 259/1000 | Loss: 0.00002896
Iteration 260/1000 | Loss: 0.00002896
Iteration 261/1000 | Loss: 0.00002896
Iteration 262/1000 | Loss: 0.00002896
Iteration 263/1000 | Loss: 0.00002896
Iteration 264/1000 | Loss: 0.00002896
Iteration 265/1000 | Loss: 0.00002896
Iteration 266/1000 | Loss: 0.00002896
Iteration 267/1000 | Loss: 0.00002896
Iteration 268/1000 | Loss: 0.00002896
Iteration 269/1000 | Loss: 0.00002896
Iteration 270/1000 | Loss: 0.00002896
Iteration 271/1000 | Loss: 0.00002896
Iteration 272/1000 | Loss: 0.00002896
Iteration 273/1000 | Loss: 0.00002896
Iteration 274/1000 | Loss: 0.00002896
Iteration 275/1000 | Loss: 0.00002896
Iteration 276/1000 | Loss: 0.00002896
Iteration 277/1000 | Loss: 0.00002896
Iteration 278/1000 | Loss: 0.00002896
Iteration 279/1000 | Loss: 0.00002896
Iteration 280/1000 | Loss: 0.00002896
Iteration 281/1000 | Loss: 0.00002896
Iteration 282/1000 | Loss: 0.00002896
Iteration 283/1000 | Loss: 0.00002896
Iteration 284/1000 | Loss: 0.00002896
Iteration 285/1000 | Loss: 0.00002896
Iteration 286/1000 | Loss: 0.00002896
Iteration 287/1000 | Loss: 0.00002896
Iteration 288/1000 | Loss: 0.00002896
Iteration 289/1000 | Loss: 0.00002896
Iteration 290/1000 | Loss: 0.00002896
Iteration 291/1000 | Loss: 0.00002896
Iteration 292/1000 | Loss: 0.00002896
Iteration 293/1000 | Loss: 0.00002896
Iteration 294/1000 | Loss: 0.00002896
Iteration 295/1000 | Loss: 0.00002896
Iteration 296/1000 | Loss: 0.00002896
Iteration 297/1000 | Loss: 0.00002896
Iteration 298/1000 | Loss: 0.00002896
Iteration 299/1000 | Loss: 0.00002896
Iteration 300/1000 | Loss: 0.00002896
Iteration 301/1000 | Loss: 0.00002896
Iteration 302/1000 | Loss: 0.00002896
Iteration 303/1000 | Loss: 0.00002896
Iteration 304/1000 | Loss: 0.00002896
Iteration 305/1000 | Loss: 0.00002896
Iteration 306/1000 | Loss: 0.00002896
Iteration 307/1000 | Loss: 0.00002896
Iteration 308/1000 | Loss: 0.00002896
Iteration 309/1000 | Loss: 0.00002896
Iteration 310/1000 | Loss: 0.00002896
Iteration 311/1000 | Loss: 0.00002896
Iteration 312/1000 | Loss: 0.00002896
Iteration 313/1000 | Loss: 0.00002896
Iteration 314/1000 | Loss: 0.00002896
Iteration 315/1000 | Loss: 0.00002896
Iteration 316/1000 | Loss: 0.00002896
Iteration 317/1000 | Loss: 0.00002896
Iteration 318/1000 | Loss: 0.00002896
Iteration 319/1000 | Loss: 0.00002896
Iteration 320/1000 | Loss: 0.00002896
Iteration 321/1000 | Loss: 0.00002896
Iteration 322/1000 | Loss: 0.00002896
Iteration 323/1000 | Loss: 0.00002896
Iteration 324/1000 | Loss: 0.00002896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [2.895512807299383e-05, 2.895512807299383e-05, 2.895512807299383e-05, 2.895512807299383e-05, 2.895512807299383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.895512807299383e-05

Optimization complete. Final v2v error: 2.988312244415283 mm

Highest mean error: 11.973267555236816 mm for frame 126

Lowest mean error: 2.422957181930542 mm for frame 82

Saving results

Total time: 215.05107259750366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459876
Iteration 2/25 | Loss: 0.00085833
Iteration 3/25 | Loss: 0.00068289
Iteration 4/25 | Loss: 0.00066314
Iteration 5/25 | Loss: 0.00065742
Iteration 6/25 | Loss: 0.00065662
Iteration 7/25 | Loss: 0.00065662
Iteration 8/25 | Loss: 0.00065662
Iteration 9/25 | Loss: 0.00065662
Iteration 10/25 | Loss: 0.00065662
Iteration 11/25 | Loss: 0.00065662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006566194351762533, 0.0006566194351762533, 0.0006566194351762533, 0.0006566194351762533, 0.0006566194351762533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006566194351762533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02135205
Iteration 2/25 | Loss: 0.00035259
Iteration 3/25 | Loss: 0.00035257
Iteration 4/25 | Loss: 0.00035257
Iteration 5/25 | Loss: 0.00035257
Iteration 6/25 | Loss: 0.00035257
Iteration 7/25 | Loss: 0.00035257
Iteration 8/25 | Loss: 0.00035257
Iteration 9/25 | Loss: 0.00035257
Iteration 10/25 | Loss: 0.00035257
Iteration 11/25 | Loss: 0.00035257
Iteration 12/25 | Loss: 0.00035257
Iteration 13/25 | Loss: 0.00035257
Iteration 14/25 | Loss: 0.00035257
Iteration 15/25 | Loss: 0.00035257
Iteration 16/25 | Loss: 0.00035257
Iteration 17/25 | Loss: 0.00035257
Iteration 18/25 | Loss: 0.00035257
Iteration 19/25 | Loss: 0.00035257
Iteration 20/25 | Loss: 0.00035257
Iteration 21/25 | Loss: 0.00035257
Iteration 22/25 | Loss: 0.00035257
Iteration 23/25 | Loss: 0.00035257
Iteration 24/25 | Loss: 0.00035257
Iteration 25/25 | Loss: 0.00035257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035257
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00001839
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001578
Iteration 6/1000 | Loss: 0.00001498
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001385
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001382
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001379
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001366
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001365
Iteration 47/1000 | Loss: 0.00001364
Iteration 48/1000 | Loss: 0.00001364
Iteration 49/1000 | Loss: 0.00001364
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001363
Iteration 55/1000 | Loss: 0.00001362
Iteration 56/1000 | Loss: 0.00001362
Iteration 57/1000 | Loss: 0.00001362
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001362
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001362
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001361
Iteration 72/1000 | Loss: 0.00001361
Iteration 73/1000 | Loss: 0.00001361
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001360
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001360
Iteration 78/1000 | Loss: 0.00001360
Iteration 79/1000 | Loss: 0.00001360
Iteration 80/1000 | Loss: 0.00001360
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001360
Iteration 83/1000 | Loss: 0.00001360
Iteration 84/1000 | Loss: 0.00001360
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001359
Iteration 96/1000 | Loss: 0.00001359
Iteration 97/1000 | Loss: 0.00001359
Iteration 98/1000 | Loss: 0.00001359
Iteration 99/1000 | Loss: 0.00001359
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001357
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001357
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001356
Iteration 132/1000 | Loss: 0.00001356
Iteration 133/1000 | Loss: 0.00001356
Iteration 134/1000 | Loss: 0.00001356
Iteration 135/1000 | Loss: 0.00001356
Iteration 136/1000 | Loss: 0.00001356
Iteration 137/1000 | Loss: 0.00001356
Iteration 138/1000 | Loss: 0.00001356
Iteration 139/1000 | Loss: 0.00001356
Iteration 140/1000 | Loss: 0.00001355
Iteration 141/1000 | Loss: 0.00001355
Iteration 142/1000 | Loss: 0.00001355
Iteration 143/1000 | Loss: 0.00001355
Iteration 144/1000 | Loss: 0.00001355
Iteration 145/1000 | Loss: 0.00001355
Iteration 146/1000 | Loss: 0.00001355
Iteration 147/1000 | Loss: 0.00001355
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001355
Iteration 150/1000 | Loss: 0.00001355
Iteration 151/1000 | Loss: 0.00001355
Iteration 152/1000 | Loss: 0.00001355
Iteration 153/1000 | Loss: 0.00001355
Iteration 154/1000 | Loss: 0.00001355
Iteration 155/1000 | Loss: 0.00001355
Iteration 156/1000 | Loss: 0.00001355
Iteration 157/1000 | Loss: 0.00001355
Iteration 158/1000 | Loss: 0.00001354
Iteration 159/1000 | Loss: 0.00001354
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001354
Iteration 163/1000 | Loss: 0.00001354
Iteration 164/1000 | Loss: 0.00001354
Iteration 165/1000 | Loss: 0.00001354
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001354
Iteration 184/1000 | Loss: 0.00001354
Iteration 185/1000 | Loss: 0.00001354
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001354
Iteration 190/1000 | Loss: 0.00001354
Iteration 191/1000 | Loss: 0.00001354
Iteration 192/1000 | Loss: 0.00001354
Iteration 193/1000 | Loss: 0.00001354
Iteration 194/1000 | Loss: 0.00001354
Iteration 195/1000 | Loss: 0.00001354
Iteration 196/1000 | Loss: 0.00001354
Iteration 197/1000 | Loss: 0.00001354
Iteration 198/1000 | Loss: 0.00001354
Iteration 199/1000 | Loss: 0.00001354
Iteration 200/1000 | Loss: 0.00001354
Iteration 201/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.3536001461034175e-05, 1.3536001461034175e-05, 1.3536001461034175e-05, 1.3536001461034175e-05, 1.3536001461034175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3536001461034175e-05

Optimization complete. Final v2v error: 3.1198301315307617 mm

Highest mean error: 3.878730297088623 mm for frame 171

Lowest mean error: 2.8185815811157227 mm for frame 239

Saving results

Total time: 40.9066424369812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084600
Iteration 2/25 | Loss: 0.01084600
Iteration 3/25 | Loss: 0.01084599
Iteration 4/25 | Loss: 0.01084599
Iteration 5/25 | Loss: 0.01084599
Iteration 6/25 | Loss: 0.01084599
Iteration 7/25 | Loss: 0.01084599
Iteration 8/25 | Loss: 0.01084599
Iteration 9/25 | Loss: 0.01084598
Iteration 10/25 | Loss: 0.01084598
Iteration 11/25 | Loss: 0.01084598
Iteration 12/25 | Loss: 0.01084598
Iteration 13/25 | Loss: 0.01084598
Iteration 14/25 | Loss: 0.01084597
Iteration 15/25 | Loss: 0.01084597
Iteration 16/25 | Loss: 0.01084597
Iteration 17/25 | Loss: 0.01084597
Iteration 18/25 | Loss: 0.01084597
Iteration 19/25 | Loss: 0.01084596
Iteration 20/25 | Loss: 0.01084596
Iteration 21/25 | Loss: 0.01084596
Iteration 22/25 | Loss: 0.01084596
Iteration 23/25 | Loss: 0.01084596
Iteration 24/25 | Loss: 0.01084595
Iteration 25/25 | Loss: 0.01084595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43933535
Iteration 2/25 | Loss: 0.17315832
Iteration 3/25 | Loss: 0.17040929
Iteration 4/25 | Loss: 0.16559826
Iteration 5/25 | Loss: 0.16559507
Iteration 6/25 | Loss: 0.16556373
Iteration 7/25 | Loss: 0.16556366
Iteration 8/25 | Loss: 0.16556366
Iteration 9/25 | Loss: 0.16556366
Iteration 10/25 | Loss: 0.16556366
Iteration 11/25 | Loss: 0.16556366
Iteration 12/25 | Loss: 0.16556366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.1655636578798294, 0.1655636578798294, 0.1655636578798294, 0.1655636578798294, 0.1655636578798294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1655636578798294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16556366
Iteration 2/1000 | Loss: 0.00425443
Iteration 3/1000 | Loss: 0.00063943
Iteration 4/1000 | Loss: 0.00049322
Iteration 5/1000 | Loss: 0.00059113
Iteration 6/1000 | Loss: 0.00032223
Iteration 7/1000 | Loss: 0.00005689
Iteration 8/1000 | Loss: 0.00006533
Iteration 9/1000 | Loss: 0.00006889
Iteration 10/1000 | Loss: 0.00024381
Iteration 11/1000 | Loss: 0.00025287
Iteration 12/1000 | Loss: 0.00003322
Iteration 13/1000 | Loss: 0.00167157
Iteration 14/1000 | Loss: 0.00073792
Iteration 15/1000 | Loss: 0.00025922
Iteration 16/1000 | Loss: 0.00046054
Iteration 17/1000 | Loss: 0.00010105
Iteration 18/1000 | Loss: 0.00004322
Iteration 19/1000 | Loss: 0.00019916
Iteration 20/1000 | Loss: 0.00024859
Iteration 21/1000 | Loss: 0.00086016
Iteration 22/1000 | Loss: 0.00028986
Iteration 23/1000 | Loss: 0.00008453
Iteration 24/1000 | Loss: 0.00029000
Iteration 25/1000 | Loss: 0.00002803
Iteration 26/1000 | Loss: 0.00003298
Iteration 27/1000 | Loss: 0.00002750
Iteration 28/1000 | Loss: 0.00003223
Iteration 29/1000 | Loss: 0.00002524
Iteration 30/1000 | Loss: 0.00009341
Iteration 31/1000 | Loss: 0.00002462
Iteration 32/1000 | Loss: 0.00002355
Iteration 33/1000 | Loss: 0.00057873
Iteration 34/1000 | Loss: 0.00005072
Iteration 35/1000 | Loss: 0.00050054
Iteration 36/1000 | Loss: 0.00140728
Iteration 37/1000 | Loss: 0.00034084
Iteration 38/1000 | Loss: 0.00002294
Iteration 39/1000 | Loss: 0.00023653
Iteration 40/1000 | Loss: 0.00050967
Iteration 41/1000 | Loss: 0.00004375
Iteration 42/1000 | Loss: 0.00002190
Iteration 43/1000 | Loss: 0.00031421
Iteration 44/1000 | Loss: 0.00038530
Iteration 45/1000 | Loss: 0.00029689
Iteration 46/1000 | Loss: 0.00025841
Iteration 47/1000 | Loss: 0.00002188
Iteration 48/1000 | Loss: 0.00016670
Iteration 49/1000 | Loss: 0.00002103
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002007
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001969
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001964
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001955
Iteration 60/1000 | Loss: 0.00001955
Iteration 61/1000 | Loss: 0.00001954
Iteration 62/1000 | Loss: 0.00001951
Iteration 63/1000 | Loss: 0.00001951
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001950
Iteration 66/1000 | Loss: 0.00001950
Iteration 67/1000 | Loss: 0.00001950
Iteration 68/1000 | Loss: 0.00001950
Iteration 69/1000 | Loss: 0.00001950
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001949
Iteration 74/1000 | Loss: 0.00001949
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001945
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001944
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001944
Iteration 88/1000 | Loss: 0.00001944
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00001944
Iteration 96/1000 | Loss: 0.00001944
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001943
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001940
Iteration 110/1000 | Loss: 0.00001940
Iteration 111/1000 | Loss: 0.00001940
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001939
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001938
Iteration 117/1000 | Loss: 0.00001938
Iteration 118/1000 | Loss: 0.00001938
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001937
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001937
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001936
Iteration 131/1000 | Loss: 0.00001936
Iteration 132/1000 | Loss: 0.00001936
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001935
Iteration 140/1000 | Loss: 0.00001935
Iteration 141/1000 | Loss: 0.00001935
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001933
Iteration 148/1000 | Loss: 0.00001933
Iteration 149/1000 | Loss: 0.00001933
Iteration 150/1000 | Loss: 0.00001933
Iteration 151/1000 | Loss: 0.00001933
Iteration 152/1000 | Loss: 0.00001933
Iteration 153/1000 | Loss: 0.00001933
Iteration 154/1000 | Loss: 0.00001933
Iteration 155/1000 | Loss: 0.00001933
Iteration 156/1000 | Loss: 0.00001933
Iteration 157/1000 | Loss: 0.00001933
Iteration 158/1000 | Loss: 0.00001933
Iteration 159/1000 | Loss: 0.00001933
Iteration 160/1000 | Loss: 0.00001932
Iteration 161/1000 | Loss: 0.00001932
Iteration 162/1000 | Loss: 0.00001932
Iteration 163/1000 | Loss: 0.00001932
Iteration 164/1000 | Loss: 0.00001932
Iteration 165/1000 | Loss: 0.00001932
Iteration 166/1000 | Loss: 0.00001932
Iteration 167/1000 | Loss: 0.00001932
Iteration 168/1000 | Loss: 0.00001931
Iteration 169/1000 | Loss: 0.00001931
Iteration 170/1000 | Loss: 0.00001931
Iteration 171/1000 | Loss: 0.00001931
Iteration 172/1000 | Loss: 0.00001931
Iteration 173/1000 | Loss: 0.00001931
Iteration 174/1000 | Loss: 0.00001931
Iteration 175/1000 | Loss: 0.00001931
Iteration 176/1000 | Loss: 0.00001931
Iteration 177/1000 | Loss: 0.00001931
Iteration 178/1000 | Loss: 0.00001931
Iteration 179/1000 | Loss: 0.00001931
Iteration 180/1000 | Loss: 0.00001931
Iteration 181/1000 | Loss: 0.00001931
Iteration 182/1000 | Loss: 0.00001931
Iteration 183/1000 | Loss: 0.00001931
Iteration 184/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.930502003233414e-05, 1.930502003233414e-05, 1.930502003233414e-05, 1.930502003233414e-05, 1.930502003233414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.930502003233414e-05

Optimization complete. Final v2v error: 3.647244930267334 mm

Highest mean error: 9.116039276123047 mm for frame 186

Lowest mean error: 3.3438456058502197 mm for frame 221

Saving results

Total time: 106.54890060424805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033015
Iteration 2/25 | Loss: 0.01033015
Iteration 3/25 | Loss: 0.01033014
Iteration 4/25 | Loss: 0.01033014
Iteration 5/25 | Loss: 0.01033014
Iteration 6/25 | Loss: 0.01033014
Iteration 7/25 | Loss: 0.00200023
Iteration 8/25 | Loss: 0.00138433
Iteration 9/25 | Loss: 0.00124443
Iteration 10/25 | Loss: 0.00125594
Iteration 11/25 | Loss: 0.00123177
Iteration 12/25 | Loss: 0.00106113
Iteration 13/25 | Loss: 0.00089832
Iteration 14/25 | Loss: 0.00080251
Iteration 15/25 | Loss: 0.00077557
Iteration 16/25 | Loss: 0.00076967
Iteration 17/25 | Loss: 0.00076909
Iteration 18/25 | Loss: 0.00076909
Iteration 19/25 | Loss: 0.00076909
Iteration 20/25 | Loss: 0.00076909
Iteration 21/25 | Loss: 0.00076909
Iteration 22/25 | Loss: 0.00076909
Iteration 23/25 | Loss: 0.00076909
Iteration 24/25 | Loss: 0.00076909
Iteration 25/25 | Loss: 0.00076909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45342958
Iteration 2/25 | Loss: 0.00032962
Iteration 3/25 | Loss: 0.00032962
Iteration 4/25 | Loss: 0.00032962
Iteration 5/25 | Loss: 0.00032962
Iteration 6/25 | Loss: 0.00032962
Iteration 7/25 | Loss: 0.00032962
Iteration 8/25 | Loss: 0.00032962
Iteration 9/25 | Loss: 0.00032962
Iteration 10/25 | Loss: 0.00032962
Iteration 11/25 | Loss: 0.00032962
Iteration 12/25 | Loss: 0.00032962
Iteration 13/25 | Loss: 0.00032962
Iteration 14/25 | Loss: 0.00032962
Iteration 15/25 | Loss: 0.00032962
Iteration 16/25 | Loss: 0.00032962
Iteration 17/25 | Loss: 0.00032962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00032961677061393857, 0.00032961677061393857, 0.00032961677061393857, 0.00032961677061393857, 0.00032961677061393857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032961677061393857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032962
Iteration 2/1000 | Loss: 0.00003076
Iteration 3/1000 | Loss: 0.00002534
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002309
Iteration 6/1000 | Loss: 0.00002280
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002261
Iteration 9/1000 | Loss: 0.00002261
Iteration 10/1000 | Loss: 0.00002250
Iteration 11/1000 | Loss: 0.00002249
Iteration 12/1000 | Loss: 0.00002248
Iteration 13/1000 | Loss: 0.00002248
Iteration 14/1000 | Loss: 0.00002232
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00002203
Iteration 18/1000 | Loss: 0.00002203
Iteration 19/1000 | Loss: 0.00002203
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002198
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00002197
Iteration 26/1000 | Loss: 0.00002195
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002195
Iteration 29/1000 | Loss: 0.00002195
Iteration 30/1000 | Loss: 0.00002195
Iteration 31/1000 | Loss: 0.00002195
Iteration 32/1000 | Loss: 0.00002194
Iteration 33/1000 | Loss: 0.00002194
Iteration 34/1000 | Loss: 0.00002194
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002194
Iteration 37/1000 | Loss: 0.00002193
Iteration 38/1000 | Loss: 0.00002192
Iteration 39/1000 | Loss: 0.00002191
Iteration 40/1000 | Loss: 0.00002187
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002186
Iteration 43/1000 | Loss: 0.00002186
Iteration 44/1000 | Loss: 0.00002185
Iteration 45/1000 | Loss: 0.00002185
Iteration 46/1000 | Loss: 0.00002185
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002184
Iteration 49/1000 | Loss: 0.00002183
Iteration 50/1000 | Loss: 0.00002183
Iteration 51/1000 | Loss: 0.00002183
Iteration 52/1000 | Loss: 0.00002183
Iteration 53/1000 | Loss: 0.00002182
Iteration 54/1000 | Loss: 0.00002182
Iteration 55/1000 | Loss: 0.00002182
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002182
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002181
Iteration 61/1000 | Loss: 0.00002181
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002179
Iteration 68/1000 | Loss: 0.00002179
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002179
Iteration 74/1000 | Loss: 0.00002179
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002178
Iteration 79/1000 | Loss: 0.00002178
Iteration 80/1000 | Loss: 0.00002178
Iteration 81/1000 | Loss: 0.00002178
Iteration 82/1000 | Loss: 0.00002178
Iteration 83/1000 | Loss: 0.00002178
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00002178
Iteration 86/1000 | Loss: 0.00002178
Iteration 87/1000 | Loss: 0.00002178
Iteration 88/1000 | Loss: 0.00002178
Iteration 89/1000 | Loss: 0.00002178
Iteration 90/1000 | Loss: 0.00002178
Iteration 91/1000 | Loss: 0.00002178
Iteration 92/1000 | Loss: 0.00002178
Iteration 93/1000 | Loss: 0.00002178
Iteration 94/1000 | Loss: 0.00002178
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002178
Iteration 102/1000 | Loss: 0.00002178
Iteration 103/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.1779840608360246e-05, 2.1779840608360246e-05, 2.1779840608360246e-05, 2.1779840608360246e-05, 2.1779840608360246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1779840608360246e-05

Optimization complete. Final v2v error: 3.8213396072387695 mm

Highest mean error: 3.895287275314331 mm for frame 25

Lowest mean error: 3.761805534362793 mm for frame 93

Saving results

Total time: 48.11211681365967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981718
Iteration 2/25 | Loss: 0.00189022
Iteration 3/25 | Loss: 0.00125823
Iteration 4/25 | Loss: 0.00103923
Iteration 5/25 | Loss: 0.00102236
Iteration 6/25 | Loss: 0.00106122
Iteration 7/25 | Loss: 0.00101529
Iteration 8/25 | Loss: 0.00095920
Iteration 9/25 | Loss: 0.00102738
Iteration 10/25 | Loss: 0.00101527
Iteration 11/25 | Loss: 0.00093963
Iteration 12/25 | Loss: 0.00092135
Iteration 13/25 | Loss: 0.00090106
Iteration 14/25 | Loss: 0.00089014
Iteration 15/25 | Loss: 0.00088302
Iteration 16/25 | Loss: 0.00088234
Iteration 17/25 | Loss: 0.00088208
Iteration 18/25 | Loss: 0.00088207
Iteration 19/25 | Loss: 0.00088207
Iteration 20/25 | Loss: 0.00088206
Iteration 21/25 | Loss: 0.00088206
Iteration 22/25 | Loss: 0.00088206
Iteration 23/25 | Loss: 0.00088206
Iteration 24/25 | Loss: 0.00088206
Iteration 25/25 | Loss: 0.00088206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37632120
Iteration 2/25 | Loss: 0.00050815
Iteration 3/25 | Loss: 0.00050812
Iteration 4/25 | Loss: 0.00050812
Iteration 5/25 | Loss: 0.00050812
Iteration 6/25 | Loss: 0.00050812
Iteration 7/25 | Loss: 0.00050812
Iteration 8/25 | Loss: 0.00050811
Iteration 9/25 | Loss: 0.00050811
Iteration 10/25 | Loss: 0.00050811
Iteration 11/25 | Loss: 0.00050811
Iteration 12/25 | Loss: 0.00050811
Iteration 13/25 | Loss: 0.00050811
Iteration 14/25 | Loss: 0.00050811
Iteration 15/25 | Loss: 0.00050811
Iteration 16/25 | Loss: 0.00050811
Iteration 17/25 | Loss: 0.00050811
Iteration 18/25 | Loss: 0.00050811
Iteration 19/25 | Loss: 0.00050811
Iteration 20/25 | Loss: 0.00050811
Iteration 21/25 | Loss: 0.00050811
Iteration 22/25 | Loss: 0.00050811
Iteration 23/25 | Loss: 0.00050811
Iteration 24/25 | Loss: 0.00050811
Iteration 25/25 | Loss: 0.00050811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050811
Iteration 2/1000 | Loss: 0.01014919
Iteration 3/1000 | Loss: 0.00177959
Iteration 4/1000 | Loss: 0.00051879
Iteration 5/1000 | Loss: 0.00028636
Iteration 6/1000 | Loss: 0.00017384
Iteration 7/1000 | Loss: 0.00013520
Iteration 8/1000 | Loss: 0.00012591
Iteration 9/1000 | Loss: 0.01314535
Iteration 10/1000 | Loss: 0.00876926
Iteration 11/1000 | Loss: 0.01275787
Iteration 12/1000 | Loss: 0.00777240
Iteration 13/1000 | Loss: 0.01206003
Iteration 14/1000 | Loss: 0.00711383
Iteration 15/1000 | Loss: 0.01172082
Iteration 16/1000 | Loss: 0.00093218
Iteration 17/1000 | Loss: 0.00041904
Iteration 18/1000 | Loss: 0.00027952
Iteration 19/1000 | Loss: 0.00018063
Iteration 20/1000 | Loss: 0.00013081
Iteration 21/1000 | Loss: 0.00009652
Iteration 22/1000 | Loss: 0.00007157
Iteration 23/1000 | Loss: 0.00006060
Iteration 24/1000 | Loss: 0.00005461
Iteration 25/1000 | Loss: 0.00004820
Iteration 26/1000 | Loss: 0.00004254
Iteration 27/1000 | Loss: 0.00003937
Iteration 28/1000 | Loss: 0.00003617
Iteration 29/1000 | Loss: 0.00003318
Iteration 30/1000 | Loss: 0.00003092
Iteration 31/1000 | Loss: 0.00002906
Iteration 32/1000 | Loss: 0.00002786
Iteration 33/1000 | Loss: 0.00002735
Iteration 34/1000 | Loss: 0.00002693
Iteration 35/1000 | Loss: 0.00002656
Iteration 36/1000 | Loss: 0.00002628
Iteration 37/1000 | Loss: 0.00002606
Iteration 38/1000 | Loss: 0.00002586
Iteration 39/1000 | Loss: 0.00002573
Iteration 40/1000 | Loss: 0.00002566
Iteration 41/1000 | Loss: 0.00002559
Iteration 42/1000 | Loss: 0.00002555
Iteration 43/1000 | Loss: 0.00002554
Iteration 44/1000 | Loss: 0.00002553
Iteration 45/1000 | Loss: 0.00002552
Iteration 46/1000 | Loss: 0.00002551
Iteration 47/1000 | Loss: 0.00002551
Iteration 48/1000 | Loss: 0.00002550
Iteration 49/1000 | Loss: 0.00002549
Iteration 50/1000 | Loss: 0.00002549
Iteration 51/1000 | Loss: 0.00002548
Iteration 52/1000 | Loss: 0.00002548
Iteration 53/1000 | Loss: 0.00002547
Iteration 54/1000 | Loss: 0.00002546
Iteration 55/1000 | Loss: 0.00002546
Iteration 56/1000 | Loss: 0.00002545
Iteration 57/1000 | Loss: 0.00002544
Iteration 58/1000 | Loss: 0.00002544
Iteration 59/1000 | Loss: 0.00002544
Iteration 60/1000 | Loss: 0.00002544
Iteration 61/1000 | Loss: 0.00002544
Iteration 62/1000 | Loss: 0.00002544
Iteration 63/1000 | Loss: 0.00002543
Iteration 64/1000 | Loss: 0.00002543
Iteration 65/1000 | Loss: 0.00002543
Iteration 66/1000 | Loss: 0.00002543
Iteration 67/1000 | Loss: 0.00002542
Iteration 68/1000 | Loss: 0.00002542
Iteration 69/1000 | Loss: 0.00002542
Iteration 70/1000 | Loss: 0.00002542
Iteration 71/1000 | Loss: 0.00002542
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002542
Iteration 74/1000 | Loss: 0.00002542
Iteration 75/1000 | Loss: 0.00002542
Iteration 76/1000 | Loss: 0.00002542
Iteration 77/1000 | Loss: 0.00002542
Iteration 78/1000 | Loss: 0.00002541
Iteration 79/1000 | Loss: 0.00002541
Iteration 80/1000 | Loss: 0.00002541
Iteration 81/1000 | Loss: 0.00002541
Iteration 82/1000 | Loss: 0.00002541
Iteration 83/1000 | Loss: 0.00002541
Iteration 84/1000 | Loss: 0.00002541
Iteration 85/1000 | Loss: 0.00002541
Iteration 86/1000 | Loss: 0.00002541
Iteration 87/1000 | Loss: 0.00002541
Iteration 88/1000 | Loss: 0.00002541
Iteration 89/1000 | Loss: 0.00002541
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002541
Iteration 92/1000 | Loss: 0.00002541
Iteration 93/1000 | Loss: 0.00002541
Iteration 94/1000 | Loss: 0.00002541
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002541
Iteration 97/1000 | Loss: 0.00002541
Iteration 98/1000 | Loss: 0.00002541
Iteration 99/1000 | Loss: 0.00002541
Iteration 100/1000 | Loss: 0.00002541
Iteration 101/1000 | Loss: 0.00002541
Iteration 102/1000 | Loss: 0.00002541
Iteration 103/1000 | Loss: 0.00002541
Iteration 104/1000 | Loss: 0.00002541
Iteration 105/1000 | Loss: 0.00002541
Iteration 106/1000 | Loss: 0.00002541
Iteration 107/1000 | Loss: 0.00002541
Iteration 108/1000 | Loss: 0.00002541
Iteration 109/1000 | Loss: 0.00002541
Iteration 110/1000 | Loss: 0.00002541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.540879722801037e-05, 2.540879722801037e-05, 2.540879722801037e-05, 2.540879722801037e-05, 2.540879722801037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.540879722801037e-05

Optimization complete. Final v2v error: 4.177164077758789 mm

Highest mean error: 4.450369834899902 mm for frame 0

Lowest mean error: 4.00147008895874 mm for frame 124

Saving results

Total time: 91.25892329216003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006910
Iteration 2/25 | Loss: 0.00157646
Iteration 3/25 | Loss: 0.00095693
Iteration 4/25 | Loss: 0.00090806
Iteration 5/25 | Loss: 0.00089343
Iteration 6/25 | Loss: 0.00088888
Iteration 7/25 | Loss: 0.00088793
Iteration 8/25 | Loss: 0.00088784
Iteration 9/25 | Loss: 0.00088784
Iteration 10/25 | Loss: 0.00088784
Iteration 11/25 | Loss: 0.00088784
Iteration 12/25 | Loss: 0.00088784
Iteration 13/25 | Loss: 0.00088784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008878427324816585, 0.0008878427324816585, 0.0008878427324816585, 0.0008878427324816585, 0.0008878427324816585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008878427324816585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83488321
Iteration 2/25 | Loss: 0.00023717
Iteration 3/25 | Loss: 0.00023717
Iteration 4/25 | Loss: 0.00023717
Iteration 5/25 | Loss: 0.00023717
Iteration 6/25 | Loss: 0.00023717
Iteration 7/25 | Loss: 0.00023717
Iteration 8/25 | Loss: 0.00023717
Iteration 9/25 | Loss: 0.00023716
Iteration 10/25 | Loss: 0.00023716
Iteration 11/25 | Loss: 0.00023716
Iteration 12/25 | Loss: 0.00023716
Iteration 13/25 | Loss: 0.00023716
Iteration 14/25 | Loss: 0.00023716
Iteration 15/25 | Loss: 0.00023716
Iteration 16/25 | Loss: 0.00023716
Iteration 17/25 | Loss: 0.00023716
Iteration 18/25 | Loss: 0.00023716
Iteration 19/25 | Loss: 0.00023716
Iteration 20/25 | Loss: 0.00023716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00023716475698165596, 0.00023716475698165596, 0.00023716475698165596, 0.00023716475698165596, 0.00023716475698165596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023716475698165596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023716
Iteration 2/1000 | Loss: 0.00006410
Iteration 3/1000 | Loss: 0.00004342
Iteration 4/1000 | Loss: 0.00003989
Iteration 5/1000 | Loss: 0.00003803
Iteration 6/1000 | Loss: 0.00003713
Iteration 7/1000 | Loss: 0.00003641
Iteration 8/1000 | Loss: 0.00003584
Iteration 9/1000 | Loss: 0.00003545
Iteration 10/1000 | Loss: 0.00003513
Iteration 11/1000 | Loss: 0.00003490
Iteration 12/1000 | Loss: 0.00003468
Iteration 13/1000 | Loss: 0.00003448
Iteration 14/1000 | Loss: 0.00003443
Iteration 15/1000 | Loss: 0.00003443
Iteration 16/1000 | Loss: 0.00003441
Iteration 17/1000 | Loss: 0.00003440
Iteration 18/1000 | Loss: 0.00003429
Iteration 19/1000 | Loss: 0.00003418
Iteration 20/1000 | Loss: 0.00003406
Iteration 21/1000 | Loss: 0.00003401
Iteration 22/1000 | Loss: 0.00003400
Iteration 23/1000 | Loss: 0.00003397
Iteration 24/1000 | Loss: 0.00003397
Iteration 25/1000 | Loss: 0.00003396
Iteration 26/1000 | Loss: 0.00003396
Iteration 27/1000 | Loss: 0.00003392
Iteration 28/1000 | Loss: 0.00003392
Iteration 29/1000 | Loss: 0.00003392
Iteration 30/1000 | Loss: 0.00003392
Iteration 31/1000 | Loss: 0.00003392
Iteration 32/1000 | Loss: 0.00003391
Iteration 33/1000 | Loss: 0.00003391
Iteration 34/1000 | Loss: 0.00003391
Iteration 35/1000 | Loss: 0.00003390
Iteration 36/1000 | Loss: 0.00003389
Iteration 37/1000 | Loss: 0.00003388
Iteration 38/1000 | Loss: 0.00003386
Iteration 39/1000 | Loss: 0.00003386
Iteration 40/1000 | Loss: 0.00003384
Iteration 41/1000 | Loss: 0.00003384
Iteration 42/1000 | Loss: 0.00003383
Iteration 43/1000 | Loss: 0.00003382
Iteration 44/1000 | Loss: 0.00003381
Iteration 45/1000 | Loss: 0.00003381
Iteration 46/1000 | Loss: 0.00003381
Iteration 47/1000 | Loss: 0.00003380
Iteration 48/1000 | Loss: 0.00003380
Iteration 49/1000 | Loss: 0.00003380
Iteration 50/1000 | Loss: 0.00003379
Iteration 51/1000 | Loss: 0.00003379
Iteration 52/1000 | Loss: 0.00003379
Iteration 53/1000 | Loss: 0.00003379
Iteration 54/1000 | Loss: 0.00003379
Iteration 55/1000 | Loss: 0.00003379
Iteration 56/1000 | Loss: 0.00003378
Iteration 57/1000 | Loss: 0.00003378
Iteration 58/1000 | Loss: 0.00003378
Iteration 59/1000 | Loss: 0.00003378
Iteration 60/1000 | Loss: 0.00003378
Iteration 61/1000 | Loss: 0.00003377
Iteration 62/1000 | Loss: 0.00003377
Iteration 63/1000 | Loss: 0.00003376
Iteration 64/1000 | Loss: 0.00003376
Iteration 65/1000 | Loss: 0.00003376
Iteration 66/1000 | Loss: 0.00003376
Iteration 67/1000 | Loss: 0.00003376
Iteration 68/1000 | Loss: 0.00003376
Iteration 69/1000 | Loss: 0.00003376
Iteration 70/1000 | Loss: 0.00003376
Iteration 71/1000 | Loss: 0.00003376
Iteration 72/1000 | Loss: 0.00003376
Iteration 73/1000 | Loss: 0.00003376
Iteration 74/1000 | Loss: 0.00003374
Iteration 75/1000 | Loss: 0.00003374
Iteration 76/1000 | Loss: 0.00003374
Iteration 77/1000 | Loss: 0.00003374
Iteration 78/1000 | Loss: 0.00003374
Iteration 79/1000 | Loss: 0.00003374
Iteration 80/1000 | Loss: 0.00003374
Iteration 81/1000 | Loss: 0.00003374
Iteration 82/1000 | Loss: 0.00003374
Iteration 83/1000 | Loss: 0.00003374
Iteration 84/1000 | Loss: 0.00003374
Iteration 85/1000 | Loss: 0.00003373
Iteration 86/1000 | Loss: 0.00003373
Iteration 87/1000 | Loss: 0.00003373
Iteration 88/1000 | Loss: 0.00003373
Iteration 89/1000 | Loss: 0.00003372
Iteration 90/1000 | Loss: 0.00003372
Iteration 91/1000 | Loss: 0.00003372
Iteration 92/1000 | Loss: 0.00003372
Iteration 93/1000 | Loss: 0.00003372
Iteration 94/1000 | Loss: 0.00003372
Iteration 95/1000 | Loss: 0.00003372
Iteration 96/1000 | Loss: 0.00003372
Iteration 97/1000 | Loss: 0.00003372
Iteration 98/1000 | Loss: 0.00003372
Iteration 99/1000 | Loss: 0.00003371
Iteration 100/1000 | Loss: 0.00003371
Iteration 101/1000 | Loss: 0.00003371
Iteration 102/1000 | Loss: 0.00003371
Iteration 103/1000 | Loss: 0.00003370
Iteration 104/1000 | Loss: 0.00003370
Iteration 105/1000 | Loss: 0.00003370
Iteration 106/1000 | Loss: 0.00003369
Iteration 107/1000 | Loss: 0.00003369
Iteration 108/1000 | Loss: 0.00003369
Iteration 109/1000 | Loss: 0.00003368
Iteration 110/1000 | Loss: 0.00003368
Iteration 111/1000 | Loss: 0.00003368
Iteration 112/1000 | Loss: 0.00003368
Iteration 113/1000 | Loss: 0.00003368
Iteration 114/1000 | Loss: 0.00003368
Iteration 115/1000 | Loss: 0.00003368
Iteration 116/1000 | Loss: 0.00003368
Iteration 117/1000 | Loss: 0.00003368
Iteration 118/1000 | Loss: 0.00003368
Iteration 119/1000 | Loss: 0.00003367
Iteration 120/1000 | Loss: 0.00003367
Iteration 121/1000 | Loss: 0.00003367
Iteration 122/1000 | Loss: 0.00003367
Iteration 123/1000 | Loss: 0.00003367
Iteration 124/1000 | Loss: 0.00003367
Iteration 125/1000 | Loss: 0.00003367
Iteration 126/1000 | Loss: 0.00003366
Iteration 127/1000 | Loss: 0.00003366
Iteration 128/1000 | Loss: 0.00003366
Iteration 129/1000 | Loss: 0.00003366
Iteration 130/1000 | Loss: 0.00003366
Iteration 131/1000 | Loss: 0.00003366
Iteration 132/1000 | Loss: 0.00003366
Iteration 133/1000 | Loss: 0.00003365
Iteration 134/1000 | Loss: 0.00003365
Iteration 135/1000 | Loss: 0.00003365
Iteration 136/1000 | Loss: 0.00003365
Iteration 137/1000 | Loss: 0.00003365
Iteration 138/1000 | Loss: 0.00003365
Iteration 139/1000 | Loss: 0.00003365
Iteration 140/1000 | Loss: 0.00003364
Iteration 141/1000 | Loss: 0.00003364
Iteration 142/1000 | Loss: 0.00003364
Iteration 143/1000 | Loss: 0.00003364
Iteration 144/1000 | Loss: 0.00003364
Iteration 145/1000 | Loss: 0.00003363
Iteration 146/1000 | Loss: 0.00003363
Iteration 147/1000 | Loss: 0.00003363
Iteration 148/1000 | Loss: 0.00003363
Iteration 149/1000 | Loss: 0.00003363
Iteration 150/1000 | Loss: 0.00003363
Iteration 151/1000 | Loss: 0.00003363
Iteration 152/1000 | Loss: 0.00003363
Iteration 153/1000 | Loss: 0.00003363
Iteration 154/1000 | Loss: 0.00003363
Iteration 155/1000 | Loss: 0.00003363
Iteration 156/1000 | Loss: 0.00003363
Iteration 157/1000 | Loss: 0.00003363
Iteration 158/1000 | Loss: 0.00003363
Iteration 159/1000 | Loss: 0.00003362
Iteration 160/1000 | Loss: 0.00003362
Iteration 161/1000 | Loss: 0.00003362
Iteration 162/1000 | Loss: 0.00003362
Iteration 163/1000 | Loss: 0.00003362
Iteration 164/1000 | Loss: 0.00003362
Iteration 165/1000 | Loss: 0.00003362
Iteration 166/1000 | Loss: 0.00003362
Iteration 167/1000 | Loss: 0.00003361
Iteration 168/1000 | Loss: 0.00003361
Iteration 169/1000 | Loss: 0.00003361
Iteration 170/1000 | Loss: 0.00003361
Iteration 171/1000 | Loss: 0.00003361
Iteration 172/1000 | Loss: 0.00003361
Iteration 173/1000 | Loss: 0.00003361
Iteration 174/1000 | Loss: 0.00003361
Iteration 175/1000 | Loss: 0.00003361
Iteration 176/1000 | Loss: 0.00003361
Iteration 177/1000 | Loss: 0.00003361
Iteration 178/1000 | Loss: 0.00003361
Iteration 179/1000 | Loss: 0.00003361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.361056951689534e-05, 3.361056951689534e-05, 3.361056951689534e-05, 3.361056951689534e-05, 3.361056951689534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.361056951689534e-05

Optimization complete. Final v2v error: 4.686463356018066 mm

Highest mean error: 5.728809356689453 mm for frame 106

Lowest mean error: 3.7966909408569336 mm for frame 32

Saving results

Total time: 49.13891005516052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079688
Iteration 2/25 | Loss: 0.00228489
Iteration 3/25 | Loss: 0.00236819
Iteration 4/25 | Loss: 0.00192495
Iteration 5/25 | Loss: 0.00171621
Iteration 6/25 | Loss: 0.00160906
Iteration 7/25 | Loss: 0.00109745
Iteration 8/25 | Loss: 0.00097971
Iteration 9/25 | Loss: 0.00100124
Iteration 10/25 | Loss: 0.00090614
Iteration 11/25 | Loss: 0.00089585
Iteration 12/25 | Loss: 0.00089929
Iteration 13/25 | Loss: 0.00089016
Iteration 14/25 | Loss: 0.00088738
Iteration 15/25 | Loss: 0.00088818
Iteration 16/25 | Loss: 0.00087719
Iteration 17/25 | Loss: 0.00087687
Iteration 18/25 | Loss: 0.00087669
Iteration 19/25 | Loss: 0.00087657
Iteration 20/25 | Loss: 0.00087651
Iteration 21/25 | Loss: 0.00087650
Iteration 22/25 | Loss: 0.00087650
Iteration 23/25 | Loss: 0.00087641
Iteration 24/25 | Loss: 0.00087634
Iteration 25/25 | Loss: 0.00087634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37812185
Iteration 2/25 | Loss: 0.00166938
Iteration 3/25 | Loss: 0.00102855
Iteration 4/25 | Loss: 0.00102855
Iteration 5/25 | Loss: 0.00102855
Iteration 6/25 | Loss: 0.00102855
Iteration 7/25 | Loss: 0.00102855
Iteration 8/25 | Loss: 0.00102855
Iteration 9/25 | Loss: 0.00102855
Iteration 10/25 | Loss: 0.00102855
Iteration 11/25 | Loss: 0.00102855
Iteration 12/25 | Loss: 0.00102855
Iteration 13/25 | Loss: 0.00102855
Iteration 14/25 | Loss: 0.00102855
Iteration 15/25 | Loss: 0.00102855
Iteration 16/25 | Loss: 0.00102855
Iteration 17/25 | Loss: 0.00102855
Iteration 18/25 | Loss: 0.00102855
Iteration 19/25 | Loss: 0.00102855
Iteration 20/25 | Loss: 0.00102855
Iteration 21/25 | Loss: 0.00102855
Iteration 22/25 | Loss: 0.00102855
Iteration 23/25 | Loss: 0.00102855
Iteration 24/25 | Loss: 0.00102855
Iteration 25/25 | Loss: 0.00102855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102855
Iteration 2/1000 | Loss: 0.00133014
Iteration 3/1000 | Loss: 0.00263265
Iteration 4/1000 | Loss: 0.00176977
Iteration 5/1000 | Loss: 0.00028162
Iteration 6/1000 | Loss: 0.00071347
Iteration 7/1000 | Loss: 0.00179277
Iteration 8/1000 | Loss: 0.00028652
Iteration 9/1000 | Loss: 0.00014420
Iteration 10/1000 | Loss: 0.00025699
Iteration 11/1000 | Loss: 0.00019563
Iteration 12/1000 | Loss: 0.00007130
Iteration 13/1000 | Loss: 0.00025709
Iteration 14/1000 | Loss: 0.00005051
Iteration 15/1000 | Loss: 0.00009763
Iteration 16/1000 | Loss: 0.00034109
Iteration 17/1000 | Loss: 0.00174549
Iteration 18/1000 | Loss: 0.00046146
Iteration 19/1000 | Loss: 0.00028511
Iteration 20/1000 | Loss: 0.00004292
Iteration 21/1000 | Loss: 0.00023780
Iteration 22/1000 | Loss: 0.00024021
Iteration 23/1000 | Loss: 0.00003030
Iteration 24/1000 | Loss: 0.00015193
Iteration 25/1000 | Loss: 0.00002682
Iteration 26/1000 | Loss: 0.00017410
Iteration 27/1000 | Loss: 0.00097056
Iteration 28/1000 | Loss: 0.00014798
Iteration 29/1000 | Loss: 0.00002563
Iteration 30/1000 | Loss: 0.00020745
Iteration 31/1000 | Loss: 0.00002509
Iteration 32/1000 | Loss: 0.00002406
Iteration 33/1000 | Loss: 0.00002355
Iteration 34/1000 | Loss: 0.00015601
Iteration 35/1000 | Loss: 0.00077989
Iteration 36/1000 | Loss: 0.00002891
Iteration 37/1000 | Loss: 0.00009106
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002288
Iteration 41/1000 | Loss: 0.00002284
Iteration 42/1000 | Loss: 0.00002284
Iteration 43/1000 | Loss: 0.00002270
Iteration 44/1000 | Loss: 0.00002261
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002255
Iteration 47/1000 | Loss: 0.00002255
Iteration 48/1000 | Loss: 0.00002254
Iteration 49/1000 | Loss: 0.00002253
Iteration 50/1000 | Loss: 0.00002250
Iteration 51/1000 | Loss: 0.00002249
Iteration 52/1000 | Loss: 0.00002249
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002247
Iteration 56/1000 | Loss: 0.00002246
Iteration 57/1000 | Loss: 0.00002246
Iteration 58/1000 | Loss: 0.00002246
Iteration 59/1000 | Loss: 0.00002245
Iteration 60/1000 | Loss: 0.00002245
Iteration 61/1000 | Loss: 0.00002245
Iteration 62/1000 | Loss: 0.00002245
Iteration 63/1000 | Loss: 0.00002245
Iteration 64/1000 | Loss: 0.00002245
Iteration 65/1000 | Loss: 0.00002245
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002244
Iteration 68/1000 | Loss: 0.00002244
Iteration 69/1000 | Loss: 0.00002244
Iteration 70/1000 | Loss: 0.00002243
Iteration 71/1000 | Loss: 0.00002243
Iteration 72/1000 | Loss: 0.00002243
Iteration 73/1000 | Loss: 0.00002242
Iteration 74/1000 | Loss: 0.00002242
Iteration 75/1000 | Loss: 0.00002242
Iteration 76/1000 | Loss: 0.00002242
Iteration 77/1000 | Loss: 0.00002242
Iteration 78/1000 | Loss: 0.00002242
Iteration 79/1000 | Loss: 0.00002242
Iteration 80/1000 | Loss: 0.00002241
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00002241
Iteration 84/1000 | Loss: 0.00002241
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002241
Iteration 87/1000 | Loss: 0.00002240
Iteration 88/1000 | Loss: 0.00002240
Iteration 89/1000 | Loss: 0.00002240
Iteration 90/1000 | Loss: 0.00002240
Iteration 91/1000 | Loss: 0.00002240
Iteration 92/1000 | Loss: 0.00002240
Iteration 93/1000 | Loss: 0.00002240
Iteration 94/1000 | Loss: 0.00002240
Iteration 95/1000 | Loss: 0.00002240
Iteration 96/1000 | Loss: 0.00002240
Iteration 97/1000 | Loss: 0.00002240
Iteration 98/1000 | Loss: 0.00002239
Iteration 99/1000 | Loss: 0.00002239
Iteration 100/1000 | Loss: 0.00002239
Iteration 101/1000 | Loss: 0.00002239
Iteration 102/1000 | Loss: 0.00002239
Iteration 103/1000 | Loss: 0.00002239
Iteration 104/1000 | Loss: 0.00002239
Iteration 105/1000 | Loss: 0.00002239
Iteration 106/1000 | Loss: 0.00002239
Iteration 107/1000 | Loss: 0.00002239
Iteration 108/1000 | Loss: 0.00002238
Iteration 109/1000 | Loss: 0.00002238
Iteration 110/1000 | Loss: 0.00002238
Iteration 111/1000 | Loss: 0.00002238
Iteration 112/1000 | Loss: 0.00002238
Iteration 113/1000 | Loss: 0.00002237
Iteration 114/1000 | Loss: 0.00002237
Iteration 115/1000 | Loss: 0.00002237
Iteration 116/1000 | Loss: 0.00002237
Iteration 117/1000 | Loss: 0.00002237
Iteration 118/1000 | Loss: 0.00002237
Iteration 119/1000 | Loss: 0.00002237
Iteration 120/1000 | Loss: 0.00002237
Iteration 121/1000 | Loss: 0.00002237
Iteration 122/1000 | Loss: 0.00002237
Iteration 123/1000 | Loss: 0.00002237
Iteration 124/1000 | Loss: 0.00002236
Iteration 125/1000 | Loss: 0.00002236
Iteration 126/1000 | Loss: 0.00002236
Iteration 127/1000 | Loss: 0.00002236
Iteration 128/1000 | Loss: 0.00002236
Iteration 129/1000 | Loss: 0.00002236
Iteration 130/1000 | Loss: 0.00002236
Iteration 131/1000 | Loss: 0.00002236
Iteration 132/1000 | Loss: 0.00002236
Iteration 133/1000 | Loss: 0.00002236
Iteration 134/1000 | Loss: 0.00002236
Iteration 135/1000 | Loss: 0.00002236
Iteration 136/1000 | Loss: 0.00002236
Iteration 137/1000 | Loss: 0.00002236
Iteration 138/1000 | Loss: 0.00002236
Iteration 139/1000 | Loss: 0.00002236
Iteration 140/1000 | Loss: 0.00002236
Iteration 141/1000 | Loss: 0.00002236
Iteration 142/1000 | Loss: 0.00002236
Iteration 143/1000 | Loss: 0.00002236
Iteration 144/1000 | Loss: 0.00002236
Iteration 145/1000 | Loss: 0.00002236
Iteration 146/1000 | Loss: 0.00002236
Iteration 147/1000 | Loss: 0.00002236
Iteration 148/1000 | Loss: 0.00002236
Iteration 149/1000 | Loss: 0.00002236
Iteration 150/1000 | Loss: 0.00002236
Iteration 151/1000 | Loss: 0.00002236
Iteration 152/1000 | Loss: 0.00002236
Iteration 153/1000 | Loss: 0.00002236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.2360720322467387e-05, 2.2360720322467387e-05, 2.2360720322467387e-05, 2.2360720322467387e-05, 2.2360720322467387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2360720322467387e-05

Optimization complete. Final v2v error: 3.964594602584839 mm

Highest mean error: 9.4571533203125 mm for frame 17

Lowest mean error: 3.5476937294006348 mm for frame 41

Saving results

Total time: 104.3507125377655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856609
Iteration 2/25 | Loss: 0.00116133
Iteration 3/25 | Loss: 0.00079428
Iteration 4/25 | Loss: 0.00076106
Iteration 5/25 | Loss: 0.00075788
Iteration 6/25 | Loss: 0.00075788
Iteration 7/25 | Loss: 0.00075788
Iteration 8/25 | Loss: 0.00075788
Iteration 9/25 | Loss: 0.00075788
Iteration 10/25 | Loss: 0.00075788
Iteration 11/25 | Loss: 0.00075788
Iteration 12/25 | Loss: 0.00075788
Iteration 13/25 | Loss: 0.00075788
Iteration 14/25 | Loss: 0.00075788
Iteration 15/25 | Loss: 0.00075788
Iteration 16/25 | Loss: 0.00075788
Iteration 17/25 | Loss: 0.00075788
Iteration 18/25 | Loss: 0.00075788
Iteration 19/25 | Loss: 0.00075788
Iteration 20/25 | Loss: 0.00075788
Iteration 21/25 | Loss: 0.00075788
Iteration 22/25 | Loss: 0.00075788
Iteration 23/25 | Loss: 0.00075788
Iteration 24/25 | Loss: 0.00075788
Iteration 25/25 | Loss: 0.00075788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04424322
Iteration 2/25 | Loss: 0.00025548
Iteration 3/25 | Loss: 0.00025548
Iteration 4/25 | Loss: 0.00025547
Iteration 5/25 | Loss: 0.00025547
Iteration 6/25 | Loss: 0.00025547
Iteration 7/25 | Loss: 0.00025547
Iteration 8/25 | Loss: 0.00025547
Iteration 9/25 | Loss: 0.00025547
Iteration 10/25 | Loss: 0.00025547
Iteration 11/25 | Loss: 0.00025547
Iteration 12/25 | Loss: 0.00025547
Iteration 13/25 | Loss: 0.00025547
Iteration 14/25 | Loss: 0.00025547
Iteration 15/25 | Loss: 0.00025547
Iteration 16/25 | Loss: 0.00025547
Iteration 17/25 | Loss: 0.00025547
Iteration 18/25 | Loss: 0.00025547
Iteration 19/25 | Loss: 0.00025547
Iteration 20/25 | Loss: 0.00025547
Iteration 21/25 | Loss: 0.00025547
Iteration 22/25 | Loss: 0.00025547
Iteration 23/25 | Loss: 0.00025547
Iteration 24/25 | Loss: 0.00025547
Iteration 25/25 | Loss: 0.00025547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025547
Iteration 2/1000 | Loss: 0.00003908
Iteration 3/1000 | Loss: 0.00003218
Iteration 4/1000 | Loss: 0.00002965
Iteration 5/1000 | Loss: 0.00002863
Iteration 6/1000 | Loss: 0.00002780
Iteration 7/1000 | Loss: 0.00002723
Iteration 8/1000 | Loss: 0.00002674
Iteration 9/1000 | Loss: 0.00002654
Iteration 10/1000 | Loss: 0.00002623
Iteration 11/1000 | Loss: 0.00002610
Iteration 12/1000 | Loss: 0.00002593
Iteration 13/1000 | Loss: 0.00002592
Iteration 14/1000 | Loss: 0.00002585
Iteration 15/1000 | Loss: 0.00002585
Iteration 16/1000 | Loss: 0.00002585
Iteration 17/1000 | Loss: 0.00002585
Iteration 18/1000 | Loss: 0.00002581
Iteration 19/1000 | Loss: 0.00002581
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00002579
Iteration 22/1000 | Loss: 0.00002579
Iteration 23/1000 | Loss: 0.00002578
Iteration 24/1000 | Loss: 0.00002578
Iteration 25/1000 | Loss: 0.00002578
Iteration 26/1000 | Loss: 0.00002577
Iteration 27/1000 | Loss: 0.00002577
Iteration 28/1000 | Loss: 0.00002577
Iteration 29/1000 | Loss: 0.00002577
Iteration 30/1000 | Loss: 0.00002577
Iteration 31/1000 | Loss: 0.00002577
Iteration 32/1000 | Loss: 0.00002576
Iteration 33/1000 | Loss: 0.00002576
Iteration 34/1000 | Loss: 0.00002576
Iteration 35/1000 | Loss: 0.00002575
Iteration 36/1000 | Loss: 0.00002575
Iteration 37/1000 | Loss: 0.00002575
Iteration 38/1000 | Loss: 0.00002574
Iteration 39/1000 | Loss: 0.00002574
Iteration 40/1000 | Loss: 0.00002574
Iteration 41/1000 | Loss: 0.00002574
Iteration 42/1000 | Loss: 0.00002574
Iteration 43/1000 | Loss: 0.00002573
Iteration 44/1000 | Loss: 0.00002573
Iteration 45/1000 | Loss: 0.00002573
Iteration 46/1000 | Loss: 0.00002573
Iteration 47/1000 | Loss: 0.00002572
Iteration 48/1000 | Loss: 0.00002572
Iteration 49/1000 | Loss: 0.00002572
Iteration 50/1000 | Loss: 0.00002569
Iteration 51/1000 | Loss: 0.00002569
Iteration 52/1000 | Loss: 0.00002568
Iteration 53/1000 | Loss: 0.00002568
Iteration 54/1000 | Loss: 0.00002568
Iteration 55/1000 | Loss: 0.00002567
Iteration 56/1000 | Loss: 0.00002567
Iteration 57/1000 | Loss: 0.00002567
Iteration 58/1000 | Loss: 0.00002567
Iteration 59/1000 | Loss: 0.00002566
Iteration 60/1000 | Loss: 0.00002566
Iteration 61/1000 | Loss: 0.00002566
Iteration 62/1000 | Loss: 0.00002565
Iteration 63/1000 | Loss: 0.00002563
Iteration 64/1000 | Loss: 0.00002563
Iteration 65/1000 | Loss: 0.00002563
Iteration 66/1000 | Loss: 0.00002563
Iteration 67/1000 | Loss: 0.00002563
Iteration 68/1000 | Loss: 0.00002563
Iteration 69/1000 | Loss: 0.00002563
Iteration 70/1000 | Loss: 0.00002563
Iteration 71/1000 | Loss: 0.00002563
Iteration 72/1000 | Loss: 0.00002562
Iteration 73/1000 | Loss: 0.00002562
Iteration 74/1000 | Loss: 0.00002562
Iteration 75/1000 | Loss: 0.00002562
Iteration 76/1000 | Loss: 0.00002562
Iteration 77/1000 | Loss: 0.00002559
Iteration 78/1000 | Loss: 0.00002559
Iteration 79/1000 | Loss: 0.00002559
Iteration 80/1000 | Loss: 0.00002559
Iteration 81/1000 | Loss: 0.00002559
Iteration 82/1000 | Loss: 0.00002559
Iteration 83/1000 | Loss: 0.00002559
Iteration 84/1000 | Loss: 0.00002559
Iteration 85/1000 | Loss: 0.00002559
Iteration 86/1000 | Loss: 0.00002559
Iteration 87/1000 | Loss: 0.00002559
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002559
Iteration 90/1000 | Loss: 0.00002559
Iteration 91/1000 | Loss: 0.00002558
Iteration 92/1000 | Loss: 0.00002558
Iteration 93/1000 | Loss: 0.00002558
Iteration 94/1000 | Loss: 0.00002558
Iteration 95/1000 | Loss: 0.00002558
Iteration 96/1000 | Loss: 0.00002558
Iteration 97/1000 | Loss: 0.00002558
Iteration 98/1000 | Loss: 0.00002557
Iteration 99/1000 | Loss: 0.00002557
Iteration 100/1000 | Loss: 0.00002557
Iteration 101/1000 | Loss: 0.00002557
Iteration 102/1000 | Loss: 0.00002557
Iteration 103/1000 | Loss: 0.00002557
Iteration 104/1000 | Loss: 0.00002557
Iteration 105/1000 | Loss: 0.00002557
Iteration 106/1000 | Loss: 0.00002557
Iteration 107/1000 | Loss: 0.00002557
Iteration 108/1000 | Loss: 0.00002557
Iteration 109/1000 | Loss: 0.00002556
Iteration 110/1000 | Loss: 0.00002556
Iteration 111/1000 | Loss: 0.00002556
Iteration 112/1000 | Loss: 0.00002555
Iteration 113/1000 | Loss: 0.00002555
Iteration 114/1000 | Loss: 0.00002555
Iteration 115/1000 | Loss: 0.00002555
Iteration 116/1000 | Loss: 0.00002555
Iteration 117/1000 | Loss: 0.00002554
Iteration 118/1000 | Loss: 0.00002554
Iteration 119/1000 | Loss: 0.00002554
Iteration 120/1000 | Loss: 0.00002554
Iteration 121/1000 | Loss: 0.00002554
Iteration 122/1000 | Loss: 0.00002554
Iteration 123/1000 | Loss: 0.00002554
Iteration 124/1000 | Loss: 0.00002554
Iteration 125/1000 | Loss: 0.00002554
Iteration 126/1000 | Loss: 0.00002554
Iteration 127/1000 | Loss: 0.00002554
Iteration 128/1000 | Loss: 0.00002554
Iteration 129/1000 | Loss: 0.00002554
Iteration 130/1000 | Loss: 0.00002554
Iteration 131/1000 | Loss: 0.00002554
Iteration 132/1000 | Loss: 0.00002554
Iteration 133/1000 | Loss: 0.00002553
Iteration 134/1000 | Loss: 0.00002553
Iteration 135/1000 | Loss: 0.00002553
Iteration 136/1000 | Loss: 0.00002553
Iteration 137/1000 | Loss: 0.00002553
Iteration 138/1000 | Loss: 0.00002553
Iteration 139/1000 | Loss: 0.00002553
Iteration 140/1000 | Loss: 0.00002553
Iteration 141/1000 | Loss: 0.00002553
Iteration 142/1000 | Loss: 0.00002553
Iteration 143/1000 | Loss: 0.00002553
Iteration 144/1000 | Loss: 0.00002552
Iteration 145/1000 | Loss: 0.00002552
Iteration 146/1000 | Loss: 0.00002552
Iteration 147/1000 | Loss: 0.00002552
Iteration 148/1000 | Loss: 0.00002552
Iteration 149/1000 | Loss: 0.00002552
Iteration 150/1000 | Loss: 0.00002552
Iteration 151/1000 | Loss: 0.00002552
Iteration 152/1000 | Loss: 0.00002552
Iteration 153/1000 | Loss: 0.00002552
Iteration 154/1000 | Loss: 0.00002552
Iteration 155/1000 | Loss: 0.00002552
Iteration 156/1000 | Loss: 0.00002551
Iteration 157/1000 | Loss: 0.00002551
Iteration 158/1000 | Loss: 0.00002551
Iteration 159/1000 | Loss: 0.00002551
Iteration 160/1000 | Loss: 0.00002551
Iteration 161/1000 | Loss: 0.00002551
Iteration 162/1000 | Loss: 0.00002551
Iteration 163/1000 | Loss: 0.00002551
Iteration 164/1000 | Loss: 0.00002551
Iteration 165/1000 | Loss: 0.00002551
Iteration 166/1000 | Loss: 0.00002551
Iteration 167/1000 | Loss: 0.00002551
Iteration 168/1000 | Loss: 0.00002551
Iteration 169/1000 | Loss: 0.00002551
Iteration 170/1000 | Loss: 0.00002550
Iteration 171/1000 | Loss: 0.00002550
Iteration 172/1000 | Loss: 0.00002550
Iteration 173/1000 | Loss: 0.00002550
Iteration 174/1000 | Loss: 0.00002550
Iteration 175/1000 | Loss: 0.00002550
Iteration 176/1000 | Loss: 0.00002550
Iteration 177/1000 | Loss: 0.00002550
Iteration 178/1000 | Loss: 0.00002550
Iteration 179/1000 | Loss: 0.00002550
Iteration 180/1000 | Loss: 0.00002550
Iteration 181/1000 | Loss: 0.00002550
Iteration 182/1000 | Loss: 0.00002550
Iteration 183/1000 | Loss: 0.00002550
Iteration 184/1000 | Loss: 0.00002550
Iteration 185/1000 | Loss: 0.00002550
Iteration 186/1000 | Loss: 0.00002550
Iteration 187/1000 | Loss: 0.00002550
Iteration 188/1000 | Loss: 0.00002550
Iteration 189/1000 | Loss: 0.00002550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.5496879970887676e-05, 2.5496879970887676e-05, 2.5496879970887676e-05, 2.5496879970887676e-05, 2.5496879970887676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5496879970887676e-05

Optimization complete. Final v2v error: 4.2191314697265625 mm

Highest mean error: 4.262511730194092 mm for frame 21

Lowest mean error: 4.182692050933838 mm for frame 68

Saving results

Total time: 36.395066022872925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867500
Iteration 2/25 | Loss: 0.00081413
Iteration 3/25 | Loss: 0.00066010
Iteration 4/25 | Loss: 0.00063868
Iteration 5/25 | Loss: 0.00063205
Iteration 6/25 | Loss: 0.00063093
Iteration 7/25 | Loss: 0.00063060
Iteration 8/25 | Loss: 0.00063060
Iteration 9/25 | Loss: 0.00063060
Iteration 10/25 | Loss: 0.00063060
Iteration 11/25 | Loss: 0.00063060
Iteration 12/25 | Loss: 0.00063060
Iteration 13/25 | Loss: 0.00063060
Iteration 14/25 | Loss: 0.00063060
Iteration 15/25 | Loss: 0.00063060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006306019495241344, 0.0006306019495241344, 0.0006306019495241344, 0.0006306019495241344, 0.0006306019495241344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006306019495241344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.26573372
Iteration 2/25 | Loss: 0.00027918
Iteration 3/25 | Loss: 0.00027917
Iteration 4/25 | Loss: 0.00027917
Iteration 5/25 | Loss: 0.00027917
Iteration 6/25 | Loss: 0.00027917
Iteration 7/25 | Loss: 0.00027916
Iteration 8/25 | Loss: 0.00027916
Iteration 9/25 | Loss: 0.00027916
Iteration 10/25 | Loss: 0.00027916
Iteration 11/25 | Loss: 0.00027916
Iteration 12/25 | Loss: 0.00027916
Iteration 13/25 | Loss: 0.00027916
Iteration 14/25 | Loss: 0.00027916
Iteration 15/25 | Loss: 0.00027916
Iteration 16/25 | Loss: 0.00027916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002791640581563115, 0.0002791640581563115, 0.0002791640581563115, 0.0002791640581563115, 0.0002791640581563115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002791640581563115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027916
Iteration 2/1000 | Loss: 0.00002881
Iteration 3/1000 | Loss: 0.00001971
Iteration 4/1000 | Loss: 0.00001831
Iteration 5/1000 | Loss: 0.00001745
Iteration 6/1000 | Loss: 0.00001687
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001607
Iteration 9/1000 | Loss: 0.00001593
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001544
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001542
Iteration 25/1000 | Loss: 0.00001541
Iteration 26/1000 | Loss: 0.00001541
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001532
Iteration 51/1000 | Loss: 0.00001532
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001525
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001518
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001515
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001515
Iteration 152/1000 | Loss: 0.00001515
Iteration 153/1000 | Loss: 0.00001515
Iteration 154/1000 | Loss: 0.00001515
Iteration 155/1000 | Loss: 0.00001515
Iteration 156/1000 | Loss: 0.00001515
Iteration 157/1000 | Loss: 0.00001515
Iteration 158/1000 | Loss: 0.00001515
Iteration 159/1000 | Loss: 0.00001515
Iteration 160/1000 | Loss: 0.00001515
Iteration 161/1000 | Loss: 0.00001515
Iteration 162/1000 | Loss: 0.00001515
Iteration 163/1000 | Loss: 0.00001515
Iteration 164/1000 | Loss: 0.00001515
Iteration 165/1000 | Loss: 0.00001515
Iteration 166/1000 | Loss: 0.00001515
Iteration 167/1000 | Loss: 0.00001515
Iteration 168/1000 | Loss: 0.00001515
Iteration 169/1000 | Loss: 0.00001515
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.5154386346694082e-05, 1.5154386346694082e-05, 1.5154386346694082e-05, 1.5154386346694082e-05, 1.5154386346694082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5154386346694082e-05

Optimization complete. Final v2v error: 3.306436777114868 mm

Highest mean error: 3.957735300064087 mm for frame 63

Lowest mean error: 2.9632296562194824 mm for frame 85

Saving results

Total time: 37.33874988555908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827530
Iteration 2/25 | Loss: 0.00098491
Iteration 3/25 | Loss: 0.00069428
Iteration 4/25 | Loss: 0.00064957
Iteration 5/25 | Loss: 0.00063953
Iteration 6/25 | Loss: 0.00063798
Iteration 7/25 | Loss: 0.00062664
Iteration 8/25 | Loss: 0.00062134
Iteration 9/25 | Loss: 0.00062003
Iteration 10/25 | Loss: 0.00061990
Iteration 11/25 | Loss: 0.00061990
Iteration 12/25 | Loss: 0.00061990
Iteration 13/25 | Loss: 0.00061990
Iteration 14/25 | Loss: 0.00061990
Iteration 15/25 | Loss: 0.00061989
Iteration 16/25 | Loss: 0.00061989
Iteration 17/25 | Loss: 0.00061989
Iteration 18/25 | Loss: 0.00061988
Iteration 19/25 | Loss: 0.00061988
Iteration 20/25 | Loss: 0.00061988
Iteration 21/25 | Loss: 0.00061988
Iteration 22/25 | Loss: 0.00061988
Iteration 23/25 | Loss: 0.00061988
Iteration 24/25 | Loss: 0.00061987
Iteration 25/25 | Loss: 0.00061987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62709212
Iteration 2/25 | Loss: 0.00031648
Iteration 3/25 | Loss: 0.00031648
Iteration 4/25 | Loss: 0.00031648
Iteration 5/25 | Loss: 0.00031648
Iteration 6/25 | Loss: 0.00031648
Iteration 7/25 | Loss: 0.00031648
Iteration 8/25 | Loss: 0.00031648
Iteration 9/25 | Loss: 0.00031648
Iteration 10/25 | Loss: 0.00031648
Iteration 11/25 | Loss: 0.00031648
Iteration 12/25 | Loss: 0.00031648
Iteration 13/25 | Loss: 0.00031648
Iteration 14/25 | Loss: 0.00031648
Iteration 15/25 | Loss: 0.00031648
Iteration 16/25 | Loss: 0.00031648
Iteration 17/25 | Loss: 0.00031648
Iteration 18/25 | Loss: 0.00031648
Iteration 19/25 | Loss: 0.00031648
Iteration 20/25 | Loss: 0.00031648
Iteration 21/25 | Loss: 0.00031648
Iteration 22/25 | Loss: 0.00031648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00031647857395000756, 0.00031647857395000756, 0.00031647857395000756, 0.00031647857395000756, 0.00031647857395000756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031647857395000756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031648
Iteration 2/1000 | Loss: 0.00002094
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001474
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001386
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001382
Iteration 29/1000 | Loss: 0.00001382
Iteration 30/1000 | Loss: 0.00001382
Iteration 31/1000 | Loss: 0.00001380
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001376
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001370
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001369
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001369
Iteration 59/1000 | Loss: 0.00001368
Iteration 60/1000 | Loss: 0.00001368
Iteration 61/1000 | Loss: 0.00001368
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001367
Iteration 64/1000 | Loss: 0.00001367
Iteration 65/1000 | Loss: 0.00001367
Iteration 66/1000 | Loss: 0.00001367
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001366
Iteration 70/1000 | Loss: 0.00001366
Iteration 71/1000 | Loss: 0.00001365
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001365
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001364
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001364
Iteration 78/1000 | Loss: 0.00001364
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001362
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001362
Iteration 84/1000 | Loss: 0.00001362
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001362
Iteration 99/1000 | Loss: 0.00001362
Iteration 100/1000 | Loss: 0.00001362
Iteration 101/1000 | Loss: 0.00001362
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.3622783626487944e-05, 1.3622783626487944e-05, 1.3622783626487944e-05, 1.3622783626487944e-05, 1.3622783626487944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3622783626487944e-05

Optimization complete. Final v2v error: 3.1388731002807617 mm

Highest mean error: 3.43625807762146 mm for frame 125

Lowest mean error: 2.7259297370910645 mm for frame 237

Saving results

Total time: 42.585853815078735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00335009
Iteration 2/25 | Loss: 0.00109941
Iteration 3/25 | Loss: 0.00074629
Iteration 4/25 | Loss: 0.00064528
Iteration 5/25 | Loss: 0.00062680
Iteration 6/25 | Loss: 0.00062205
Iteration 7/25 | Loss: 0.00062092
Iteration 8/25 | Loss: 0.00062058
Iteration 9/25 | Loss: 0.00062058
Iteration 10/25 | Loss: 0.00062058
Iteration 11/25 | Loss: 0.00062058
Iteration 12/25 | Loss: 0.00062058
Iteration 13/25 | Loss: 0.00062058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006205761455930769, 0.0006205761455930769, 0.0006205761455930769, 0.0006205761455930769, 0.0006205761455930769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006205761455930769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50573409
Iteration 2/25 | Loss: 0.00033208
Iteration 3/25 | Loss: 0.00033208
Iteration 4/25 | Loss: 0.00033208
Iteration 5/25 | Loss: 0.00033208
Iteration 6/25 | Loss: 0.00033208
Iteration 7/25 | Loss: 0.00033208
Iteration 8/25 | Loss: 0.00033208
Iteration 9/25 | Loss: 0.00033208
Iteration 10/25 | Loss: 0.00033208
Iteration 11/25 | Loss: 0.00033208
Iteration 12/25 | Loss: 0.00033208
Iteration 13/25 | Loss: 0.00033208
Iteration 14/25 | Loss: 0.00033208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000332076771883294, 0.000332076771883294, 0.000332076771883294, 0.000332076771883294, 0.000332076771883294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000332076771883294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033208
Iteration 2/1000 | Loss: 0.00002928
Iteration 3/1000 | Loss: 0.00002036
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001200
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001166
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001163
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001160
Iteration 33/1000 | Loss: 0.00001160
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001159
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001157
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001152
Iteration 82/1000 | Loss: 0.00001152
Iteration 83/1000 | Loss: 0.00001152
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001150
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001148
Iteration 108/1000 | Loss: 0.00001148
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001147
Iteration 112/1000 | Loss: 0.00001147
Iteration 113/1000 | Loss: 0.00001147
Iteration 114/1000 | Loss: 0.00001147
Iteration 115/1000 | Loss: 0.00001147
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001145
Iteration 142/1000 | Loss: 0.00001145
Iteration 143/1000 | Loss: 0.00001145
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001145
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001144
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001143
Iteration 169/1000 | Loss: 0.00001143
Iteration 170/1000 | Loss: 0.00001143
Iteration 171/1000 | Loss: 0.00001143
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001142
Iteration 175/1000 | Loss: 0.00001142
Iteration 176/1000 | Loss: 0.00001142
Iteration 177/1000 | Loss: 0.00001142
Iteration 178/1000 | Loss: 0.00001142
Iteration 179/1000 | Loss: 0.00001142
Iteration 180/1000 | Loss: 0.00001142
Iteration 181/1000 | Loss: 0.00001142
Iteration 182/1000 | Loss: 0.00001142
Iteration 183/1000 | Loss: 0.00001142
Iteration 184/1000 | Loss: 0.00001142
Iteration 185/1000 | Loss: 0.00001142
Iteration 186/1000 | Loss: 0.00001142
Iteration 187/1000 | Loss: 0.00001142
Iteration 188/1000 | Loss: 0.00001142
Iteration 189/1000 | Loss: 0.00001141
Iteration 190/1000 | Loss: 0.00001141
Iteration 191/1000 | Loss: 0.00001141
Iteration 192/1000 | Loss: 0.00001141
Iteration 193/1000 | Loss: 0.00001141
Iteration 194/1000 | Loss: 0.00001141
Iteration 195/1000 | Loss: 0.00001141
Iteration 196/1000 | Loss: 0.00001141
Iteration 197/1000 | Loss: 0.00001141
Iteration 198/1000 | Loss: 0.00001141
Iteration 199/1000 | Loss: 0.00001140
Iteration 200/1000 | Loss: 0.00001140
Iteration 201/1000 | Loss: 0.00001140
Iteration 202/1000 | Loss: 0.00001140
Iteration 203/1000 | Loss: 0.00001140
Iteration 204/1000 | Loss: 0.00001140
Iteration 205/1000 | Loss: 0.00001140
Iteration 206/1000 | Loss: 0.00001140
Iteration 207/1000 | Loss: 0.00001140
Iteration 208/1000 | Loss: 0.00001140
Iteration 209/1000 | Loss: 0.00001140
Iteration 210/1000 | Loss: 0.00001140
Iteration 211/1000 | Loss: 0.00001140
Iteration 212/1000 | Loss: 0.00001140
Iteration 213/1000 | Loss: 0.00001140
Iteration 214/1000 | Loss: 0.00001140
Iteration 215/1000 | Loss: 0.00001140
Iteration 216/1000 | Loss: 0.00001140
Iteration 217/1000 | Loss: 0.00001140
Iteration 218/1000 | Loss: 0.00001140
Iteration 219/1000 | Loss: 0.00001140
Iteration 220/1000 | Loss: 0.00001140
Iteration 221/1000 | Loss: 0.00001140
Iteration 222/1000 | Loss: 0.00001140
Iteration 223/1000 | Loss: 0.00001140
Iteration 224/1000 | Loss: 0.00001140
Iteration 225/1000 | Loss: 0.00001140
Iteration 226/1000 | Loss: 0.00001140
Iteration 227/1000 | Loss: 0.00001140
Iteration 228/1000 | Loss: 0.00001140
Iteration 229/1000 | Loss: 0.00001140
Iteration 230/1000 | Loss: 0.00001140
Iteration 231/1000 | Loss: 0.00001140
Iteration 232/1000 | Loss: 0.00001140
Iteration 233/1000 | Loss: 0.00001140
Iteration 234/1000 | Loss: 0.00001140
Iteration 235/1000 | Loss: 0.00001140
Iteration 236/1000 | Loss: 0.00001140
Iteration 237/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.1397828529879916e-05, 1.1397828529879916e-05, 1.1397828529879916e-05, 1.1397828529879916e-05, 1.1397828529879916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1397828529879916e-05

Optimization complete. Final v2v error: 2.88308048248291 mm

Highest mean error: 3.1678385734558105 mm for frame 28

Lowest mean error: 2.7336738109588623 mm for frame 78

Saving results

Total time: 42.860989570617676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874697
Iteration 2/25 | Loss: 0.00112825
Iteration 3/25 | Loss: 0.00082183
Iteration 4/25 | Loss: 0.00078014
Iteration 5/25 | Loss: 0.00077533
Iteration 6/25 | Loss: 0.00077478
Iteration 7/25 | Loss: 0.00077478
Iteration 8/25 | Loss: 0.00077478
Iteration 9/25 | Loss: 0.00077478
Iteration 10/25 | Loss: 0.00077478
Iteration 11/25 | Loss: 0.00077478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007747841882519424, 0.0007747841882519424, 0.0007747841882519424, 0.0007747841882519424, 0.0007747841882519424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007747841882519424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03326774
Iteration 2/25 | Loss: 0.00023343
Iteration 3/25 | Loss: 0.00023342
Iteration 4/25 | Loss: 0.00023342
Iteration 5/25 | Loss: 0.00023342
Iteration 6/25 | Loss: 0.00023342
Iteration 7/25 | Loss: 0.00023342
Iteration 8/25 | Loss: 0.00023342
Iteration 9/25 | Loss: 0.00023342
Iteration 10/25 | Loss: 0.00023342
Iteration 11/25 | Loss: 0.00023342
Iteration 12/25 | Loss: 0.00023342
Iteration 13/25 | Loss: 0.00023342
Iteration 14/25 | Loss: 0.00023342
Iteration 15/25 | Loss: 0.00023342
Iteration 16/25 | Loss: 0.00023342
Iteration 17/25 | Loss: 0.00023342
Iteration 18/25 | Loss: 0.00023342
Iteration 19/25 | Loss: 0.00023342
Iteration 20/25 | Loss: 0.00023342
Iteration 21/25 | Loss: 0.00023342
Iteration 22/25 | Loss: 0.00023342
Iteration 23/25 | Loss: 0.00023342
Iteration 24/25 | Loss: 0.00023342
Iteration 25/25 | Loss: 0.00023342

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023342
Iteration 2/1000 | Loss: 0.00003959
Iteration 3/1000 | Loss: 0.00003145
Iteration 4/1000 | Loss: 0.00002958
Iteration 5/1000 | Loss: 0.00002784
Iteration 6/1000 | Loss: 0.00002715
Iteration 7/1000 | Loss: 0.00002646
Iteration 8/1000 | Loss: 0.00002616
Iteration 9/1000 | Loss: 0.00002576
Iteration 10/1000 | Loss: 0.00002571
Iteration 11/1000 | Loss: 0.00002542
Iteration 12/1000 | Loss: 0.00002538
Iteration 13/1000 | Loss: 0.00002520
Iteration 14/1000 | Loss: 0.00002519
Iteration 15/1000 | Loss: 0.00002518
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00002512
Iteration 19/1000 | Loss: 0.00002511
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002511
Iteration 22/1000 | Loss: 0.00002509
Iteration 23/1000 | Loss: 0.00002506
Iteration 24/1000 | Loss: 0.00002506
Iteration 25/1000 | Loss: 0.00002506
Iteration 26/1000 | Loss: 0.00002505
Iteration 27/1000 | Loss: 0.00002504
Iteration 28/1000 | Loss: 0.00002504
Iteration 29/1000 | Loss: 0.00002504
Iteration 30/1000 | Loss: 0.00002502
Iteration 31/1000 | Loss: 0.00002498
Iteration 32/1000 | Loss: 0.00002498
Iteration 33/1000 | Loss: 0.00002498
Iteration 34/1000 | Loss: 0.00002498
Iteration 35/1000 | Loss: 0.00002498
Iteration 36/1000 | Loss: 0.00002498
Iteration 37/1000 | Loss: 0.00002498
Iteration 38/1000 | Loss: 0.00002498
Iteration 39/1000 | Loss: 0.00002498
Iteration 40/1000 | Loss: 0.00002498
Iteration 41/1000 | Loss: 0.00002498
Iteration 42/1000 | Loss: 0.00002498
Iteration 43/1000 | Loss: 0.00002498
Iteration 44/1000 | Loss: 0.00002498
Iteration 45/1000 | Loss: 0.00002498
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00002498
Iteration 49/1000 | Loss: 0.00002498
Iteration 50/1000 | Loss: 0.00002498
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00002498
Iteration 53/1000 | Loss: 0.00002498
Iteration 54/1000 | Loss: 0.00002498
Iteration 55/1000 | Loss: 0.00002498
Iteration 56/1000 | Loss: 0.00002498
Iteration 57/1000 | Loss: 0.00002498
Iteration 58/1000 | Loss: 0.00002498
Iteration 59/1000 | Loss: 0.00002498
Iteration 60/1000 | Loss: 0.00002498
Iteration 61/1000 | Loss: 0.00002498
Iteration 62/1000 | Loss: 0.00002498
Iteration 63/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [2.497766399756074e-05, 2.497766399756074e-05, 2.497766399756074e-05, 2.497766399756074e-05, 2.497766399756074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.497766399756074e-05

Optimization complete. Final v2v error: 4.196960926055908 mm

Highest mean error: 4.258193016052246 mm for frame 78

Lowest mean error: 4.1364054679870605 mm for frame 125

Saving results

Total time: 28.15098476409912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_010/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_010/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362280
Iteration 2/25 | Loss: 0.00072575
Iteration 3/25 | Loss: 0.00062364
Iteration 4/25 | Loss: 0.00060522
Iteration 5/25 | Loss: 0.00059768
Iteration 6/25 | Loss: 0.00059626
Iteration 7/25 | Loss: 0.00059593
Iteration 8/25 | Loss: 0.00059593
Iteration 9/25 | Loss: 0.00059593
Iteration 10/25 | Loss: 0.00059593
Iteration 11/25 | Loss: 0.00059593
Iteration 12/25 | Loss: 0.00059593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005959332920610905, 0.0005959332920610905, 0.0005959332920610905, 0.0005959332920610905, 0.0005959332920610905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005959332920610905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02201843
Iteration 2/25 | Loss: 0.00028623
Iteration 3/25 | Loss: 0.00028622
Iteration 4/25 | Loss: 0.00028622
Iteration 5/25 | Loss: 0.00028622
Iteration 6/25 | Loss: 0.00028622
Iteration 7/25 | Loss: 0.00028622
Iteration 8/25 | Loss: 0.00028622
Iteration 9/25 | Loss: 0.00028622
Iteration 10/25 | Loss: 0.00028622
Iteration 11/25 | Loss: 0.00028622
Iteration 12/25 | Loss: 0.00028622
Iteration 13/25 | Loss: 0.00028622
Iteration 14/25 | Loss: 0.00028622
Iteration 15/25 | Loss: 0.00028622
Iteration 16/25 | Loss: 0.00028622
Iteration 17/25 | Loss: 0.00028622
Iteration 18/25 | Loss: 0.00028622
Iteration 19/25 | Loss: 0.00028622
Iteration 20/25 | Loss: 0.00028622
Iteration 21/25 | Loss: 0.00028622
Iteration 22/25 | Loss: 0.00028622
Iteration 23/25 | Loss: 0.00028622
Iteration 24/25 | Loss: 0.00028622
Iteration 25/25 | Loss: 0.00028622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028622
Iteration 2/1000 | Loss: 0.00002573
Iteration 3/1000 | Loss: 0.00001645
Iteration 4/1000 | Loss: 0.00001509
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001294
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001283
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001282
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001281
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001276
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001275
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001275
Iteration 46/1000 | Loss: 0.00001274
Iteration 47/1000 | Loss: 0.00001274
Iteration 48/1000 | Loss: 0.00001274
Iteration 49/1000 | Loss: 0.00001273
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001270
Iteration 56/1000 | Loss: 0.00001270
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001269
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001268
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001267
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001266
Iteration 66/1000 | Loss: 0.00001266
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001257
Iteration 97/1000 | Loss: 0.00001257
Iteration 98/1000 | Loss: 0.00001257
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001255
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001255
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001253
Iteration 147/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.2531296306406148e-05, 1.2531296306406148e-05, 1.2531296306406148e-05, 1.2531296306406148e-05, 1.2531296306406148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2531296306406148e-05

Optimization complete. Final v2v error: 3.006355047225952 mm

Highest mean error: 3.28547739982605 mm for frame 94

Lowest mean error: 2.767674207687378 mm for frame 123

Saving results

Total time: 37.419188261032104
