Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=64, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3584-3639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385530
Iteration 2/25 | Loss: 0.00135728
Iteration 3/25 | Loss: 0.00125058
Iteration 4/25 | Loss: 0.00122704
Iteration 5/25 | Loss: 0.00122141
Iteration 6/25 | Loss: 0.00121949
Iteration 7/25 | Loss: 0.00121917
Iteration 8/25 | Loss: 0.00121917
Iteration 9/25 | Loss: 0.00121917
Iteration 10/25 | Loss: 0.00121917
Iteration 11/25 | Loss: 0.00121917
Iteration 12/25 | Loss: 0.00121917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001219169469550252, 0.001219169469550252, 0.001219169469550252, 0.001219169469550252, 0.001219169469550252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001219169469550252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32682610
Iteration 2/25 | Loss: 0.00088762
Iteration 3/25 | Loss: 0.00088761
Iteration 4/25 | Loss: 0.00088761
Iteration 5/25 | Loss: 0.00088761
Iteration 6/25 | Loss: 0.00088761
Iteration 7/25 | Loss: 0.00088761
Iteration 8/25 | Loss: 0.00088761
Iteration 9/25 | Loss: 0.00088761
Iteration 10/25 | Loss: 0.00088761
Iteration 11/25 | Loss: 0.00088761
Iteration 12/25 | Loss: 0.00088761
Iteration 13/25 | Loss: 0.00088761
Iteration 14/25 | Loss: 0.00088761
Iteration 15/25 | Loss: 0.00088761
Iteration 16/25 | Loss: 0.00088761
Iteration 17/25 | Loss: 0.00088761
Iteration 18/25 | Loss: 0.00088761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008876051288098097, 0.0008876051288098097, 0.0008876051288098097, 0.0008876051288098097, 0.0008876051288098097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008876051288098097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088761
Iteration 2/1000 | Loss: 0.00003926
Iteration 3/1000 | Loss: 0.00002770
Iteration 4/1000 | Loss: 0.00002262
Iteration 5/1000 | Loss: 0.00002110
Iteration 6/1000 | Loss: 0.00001987
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001782
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001759
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001749
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001743
Iteration 22/1000 | Loss: 0.00001742
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001737
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001733
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001707
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001706
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001703
Iteration 85/1000 | Loss: 0.00001703
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001698
Iteration 117/1000 | Loss: 0.00001698
Iteration 118/1000 | Loss: 0.00001698
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001697
Iteration 121/1000 | Loss: 0.00001697
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001697
Iteration 124/1000 | Loss: 0.00001697
Iteration 125/1000 | Loss: 0.00001697
Iteration 126/1000 | Loss: 0.00001697
Iteration 127/1000 | Loss: 0.00001697
Iteration 128/1000 | Loss: 0.00001697
Iteration 129/1000 | Loss: 0.00001697
Iteration 130/1000 | Loss: 0.00001697
Iteration 131/1000 | Loss: 0.00001697
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001696
Iteration 138/1000 | Loss: 0.00001696
Iteration 139/1000 | Loss: 0.00001696
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001695
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001695
Iteration 149/1000 | Loss: 0.00001695
Iteration 150/1000 | Loss: 0.00001695
Iteration 151/1000 | Loss: 0.00001695
Iteration 152/1000 | Loss: 0.00001695
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001695
Iteration 156/1000 | Loss: 0.00001695
Iteration 157/1000 | Loss: 0.00001695
Iteration 158/1000 | Loss: 0.00001695
Iteration 159/1000 | Loss: 0.00001694
Iteration 160/1000 | Loss: 0.00001694
Iteration 161/1000 | Loss: 0.00001694
Iteration 162/1000 | Loss: 0.00001694
Iteration 163/1000 | Loss: 0.00001694
Iteration 164/1000 | Loss: 0.00001694
Iteration 165/1000 | Loss: 0.00001694
Iteration 166/1000 | Loss: 0.00001694
Iteration 167/1000 | Loss: 0.00001694
Iteration 168/1000 | Loss: 0.00001694
Iteration 169/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.6940537534537725e-05, 1.6940537534537725e-05, 1.6940537534537725e-05, 1.6940537534537725e-05, 1.6940537534537725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6940537534537725e-05

Optimization complete. Final v2v error: 3.4980316162109375 mm

Highest mean error: 3.999857187271118 mm for frame 4

Lowest mean error: 3.24991774559021 mm for frame 104

Saving results

Total time: 43.33572959899902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425675
Iteration 2/25 | Loss: 0.00137008
Iteration 3/25 | Loss: 0.00129558
Iteration 4/25 | Loss: 0.00129259
Iteration 5/25 | Loss: 0.00129259
Iteration 6/25 | Loss: 0.00129259
Iteration 7/25 | Loss: 0.00129259
Iteration 8/25 | Loss: 0.00129259
Iteration 9/25 | Loss: 0.00129259
Iteration 10/25 | Loss: 0.00129259
Iteration 11/25 | Loss: 0.00129259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012925852788612247, 0.0012925852788612247, 0.0012925852788612247, 0.0012925852788612247, 0.0012925852788612247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012925852788612247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35908294
Iteration 2/25 | Loss: 0.00075555
Iteration 3/25 | Loss: 0.00075555
Iteration 4/25 | Loss: 0.00075555
Iteration 5/25 | Loss: 0.00075555
Iteration 6/25 | Loss: 0.00075555
Iteration 7/25 | Loss: 0.00075555
Iteration 8/25 | Loss: 0.00075555
Iteration 9/25 | Loss: 0.00075555
Iteration 10/25 | Loss: 0.00075555
Iteration 11/25 | Loss: 0.00075555
Iteration 12/25 | Loss: 0.00075555
Iteration 13/25 | Loss: 0.00075555
Iteration 14/25 | Loss: 0.00075555
Iteration 15/25 | Loss: 0.00075555
Iteration 16/25 | Loss: 0.00075555
Iteration 17/25 | Loss: 0.00075555
Iteration 18/25 | Loss: 0.00075555
Iteration 19/25 | Loss: 0.00075555
Iteration 20/25 | Loss: 0.00075555
Iteration 21/25 | Loss: 0.00075555
Iteration 22/25 | Loss: 0.00075555
Iteration 23/25 | Loss: 0.00075555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007555464399047196, 0.0007555464399047196, 0.0007555464399047196, 0.0007555464399047196, 0.0007555464399047196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007555464399047196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075555
Iteration 2/1000 | Loss: 0.00002548
Iteration 3/1000 | Loss: 0.00002221
Iteration 4/1000 | Loss: 0.00002114
Iteration 5/1000 | Loss: 0.00001999
Iteration 6/1000 | Loss: 0.00001940
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001839
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001792
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001776
Iteration 16/1000 | Loss: 0.00001774
Iteration 17/1000 | Loss: 0.00001771
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001770
Iteration 20/1000 | Loss: 0.00001770
Iteration 21/1000 | Loss: 0.00001769
Iteration 22/1000 | Loss: 0.00001755
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001739
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001738
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001735
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001731
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001729
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001729
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001728
Iteration 48/1000 | Loss: 0.00001728
Iteration 49/1000 | Loss: 0.00001728
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001725
Iteration 53/1000 | Loss: 0.00001725
Iteration 54/1000 | Loss: 0.00001725
Iteration 55/1000 | Loss: 0.00001725
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001718
Iteration 67/1000 | Loss: 0.00001718
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001716
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Iteration 84/1000 | Loss: 0.00001715
Iteration 85/1000 | Loss: 0.00001715
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001712
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001711
Iteration 107/1000 | Loss: 0.00001711
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001709
Iteration 114/1000 | Loss: 0.00001709
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001709
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001706
Iteration 126/1000 | Loss: 0.00001706
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001705
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00001703
Iteration 150/1000 | Loss: 0.00001703
Iteration 151/1000 | Loss: 0.00001703
Iteration 152/1000 | Loss: 0.00001703
Iteration 153/1000 | Loss: 0.00001703
Iteration 154/1000 | Loss: 0.00001703
Iteration 155/1000 | Loss: 0.00001703
Iteration 156/1000 | Loss: 0.00001703
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.7031998140737414e-05, 1.7031998140737414e-05, 1.7031998140737414e-05, 1.7031998140737414e-05, 1.7031998140737414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7031998140737414e-05

Optimization complete. Final v2v error: 3.46525502204895 mm

Highest mean error: 3.4992282390594482 mm for frame 81

Lowest mean error: 3.433608055114746 mm for frame 14

Saving results

Total time: 41.64148187637329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993171
Iteration 2/25 | Loss: 0.00224031
Iteration 3/25 | Loss: 0.00188112
Iteration 4/25 | Loss: 0.00172394
Iteration 5/25 | Loss: 0.00163027
Iteration 6/25 | Loss: 0.00157866
Iteration 7/25 | Loss: 0.00147176
Iteration 8/25 | Loss: 0.00145778
Iteration 9/25 | Loss: 0.00140004
Iteration 10/25 | Loss: 0.00139792
Iteration 11/25 | Loss: 0.00137281
Iteration 12/25 | Loss: 0.00136941
Iteration 13/25 | Loss: 0.00134837
Iteration 14/25 | Loss: 0.00134303
Iteration 15/25 | Loss: 0.00134251
Iteration 16/25 | Loss: 0.00133440
Iteration 17/25 | Loss: 0.00133413
Iteration 18/25 | Loss: 0.00134040
Iteration 19/25 | Loss: 0.00133458
Iteration 20/25 | Loss: 0.00133724
Iteration 21/25 | Loss: 0.00132709
Iteration 22/25 | Loss: 0.00132081
Iteration 23/25 | Loss: 0.00132282
Iteration 24/25 | Loss: 0.00131701
Iteration 25/25 | Loss: 0.00131824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45314550
Iteration 2/25 | Loss: 0.00149132
Iteration 3/25 | Loss: 0.00149132
Iteration 4/25 | Loss: 0.00149132
Iteration 5/25 | Loss: 0.00142376
Iteration 6/25 | Loss: 0.00142370
Iteration 7/25 | Loss: 0.00142370
Iteration 8/25 | Loss: 0.00142370
Iteration 9/25 | Loss: 0.00142370
Iteration 10/25 | Loss: 0.00142370
Iteration 11/25 | Loss: 0.00142370
Iteration 12/25 | Loss: 0.00142370
Iteration 13/25 | Loss: 0.00142370
Iteration 14/25 | Loss: 0.00142370
Iteration 15/25 | Loss: 0.00142370
Iteration 16/25 | Loss: 0.00142370
Iteration 17/25 | Loss: 0.00142370
Iteration 18/25 | Loss: 0.00142370
Iteration 19/25 | Loss: 0.00142370
Iteration 20/25 | Loss: 0.00142370
Iteration 21/25 | Loss: 0.00142370
Iteration 22/25 | Loss: 0.00142370
Iteration 23/25 | Loss: 0.00142370
Iteration 24/25 | Loss: 0.00142370
Iteration 25/25 | Loss: 0.00142370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142370
Iteration 2/1000 | Loss: 0.00301185
Iteration 3/1000 | Loss: 0.00020373
Iteration 4/1000 | Loss: 0.00016551
Iteration 5/1000 | Loss: 0.00100399
Iteration 6/1000 | Loss: 0.00007677
Iteration 7/1000 | Loss: 0.00012209
Iteration 8/1000 | Loss: 0.00276982
Iteration 9/1000 | Loss: 0.00298937
Iteration 10/1000 | Loss: 0.00031409
Iteration 11/1000 | Loss: 0.00039937
Iteration 12/1000 | Loss: 0.00139043
Iteration 13/1000 | Loss: 0.00050306
Iteration 14/1000 | Loss: 0.00019848
Iteration 15/1000 | Loss: 0.00012592
Iteration 16/1000 | Loss: 0.00307883
Iteration 17/1000 | Loss: 0.00033087
Iteration 18/1000 | Loss: 0.00015436
Iteration 19/1000 | Loss: 0.00006110
Iteration 20/1000 | Loss: 0.00004381
Iteration 21/1000 | Loss: 0.00003708
Iteration 22/1000 | Loss: 0.00003417
Iteration 23/1000 | Loss: 0.00017991
Iteration 24/1000 | Loss: 0.00011733
Iteration 25/1000 | Loss: 0.00021100
Iteration 26/1000 | Loss: 0.00009573
Iteration 27/1000 | Loss: 0.00005201
Iteration 28/1000 | Loss: 0.00021211
Iteration 29/1000 | Loss: 0.00029200
Iteration 30/1000 | Loss: 0.00022225
Iteration 31/1000 | Loss: 0.00020450
Iteration 32/1000 | Loss: 0.00005704
Iteration 33/1000 | Loss: 0.00013578
Iteration 34/1000 | Loss: 0.00010651
Iteration 35/1000 | Loss: 0.00066798
Iteration 36/1000 | Loss: 0.00033514
Iteration 37/1000 | Loss: 0.00024884
Iteration 38/1000 | Loss: 0.00013496
Iteration 39/1000 | Loss: 0.00005462
Iteration 40/1000 | Loss: 0.00010128
Iteration 41/1000 | Loss: 0.00038264
Iteration 42/1000 | Loss: 0.00058741
Iteration 43/1000 | Loss: 0.00029774
Iteration 44/1000 | Loss: 0.00004432
Iteration 45/1000 | Loss: 0.00011781
Iteration 46/1000 | Loss: 0.00015848
Iteration 47/1000 | Loss: 0.00020413
Iteration 48/1000 | Loss: 0.00033617
Iteration 49/1000 | Loss: 0.00018728
Iteration 50/1000 | Loss: 0.00017758
Iteration 51/1000 | Loss: 0.00040157
Iteration 52/1000 | Loss: 0.00016749
Iteration 53/1000 | Loss: 0.00017141
Iteration 54/1000 | Loss: 0.00004303
Iteration 55/1000 | Loss: 0.00007731
Iteration 56/1000 | Loss: 0.00002546
Iteration 57/1000 | Loss: 0.00054052
Iteration 58/1000 | Loss: 0.00055495
Iteration 59/1000 | Loss: 0.00054917
Iteration 60/1000 | Loss: 0.00054452
Iteration 61/1000 | Loss: 0.00034696
Iteration 62/1000 | Loss: 0.00034651
Iteration 63/1000 | Loss: 0.00040002
Iteration 64/1000 | Loss: 0.00048516
Iteration 65/1000 | Loss: 0.00039687
Iteration 66/1000 | Loss: 0.00041338
Iteration 67/1000 | Loss: 0.00025385
Iteration 68/1000 | Loss: 0.00005790
Iteration 69/1000 | Loss: 0.00004091
Iteration 70/1000 | Loss: 0.00011020
Iteration 71/1000 | Loss: 0.00026555
Iteration 72/1000 | Loss: 0.00012021
Iteration 73/1000 | Loss: 0.00003880
Iteration 74/1000 | Loss: 0.00018879
Iteration 75/1000 | Loss: 0.00008285
Iteration 76/1000 | Loss: 0.00003329
Iteration 77/1000 | Loss: 0.00003056
Iteration 78/1000 | Loss: 0.00005157
Iteration 79/1000 | Loss: 0.00006172
Iteration 80/1000 | Loss: 0.00008312
Iteration 81/1000 | Loss: 0.00009261
Iteration 82/1000 | Loss: 0.00003587
Iteration 83/1000 | Loss: 0.00004659
Iteration 84/1000 | Loss: 0.00002697
Iteration 85/1000 | Loss: 0.00005903
Iteration 86/1000 | Loss: 0.00002635
Iteration 87/1000 | Loss: 0.00002514
Iteration 88/1000 | Loss: 0.00009023
Iteration 89/1000 | Loss: 0.00020200
Iteration 90/1000 | Loss: 0.00013876
Iteration 91/1000 | Loss: 0.00003835
Iteration 92/1000 | Loss: 0.00016661
Iteration 93/1000 | Loss: 0.00040928
Iteration 94/1000 | Loss: 0.00009351
Iteration 95/1000 | Loss: 0.00014143
Iteration 96/1000 | Loss: 0.00012647
Iteration 97/1000 | Loss: 0.00013897
Iteration 98/1000 | Loss: 0.00022021
Iteration 99/1000 | Loss: 0.00026804
Iteration 100/1000 | Loss: 0.00038114
Iteration 101/1000 | Loss: 0.00014591
Iteration 102/1000 | Loss: 0.00028877
Iteration 103/1000 | Loss: 0.00026871
Iteration 104/1000 | Loss: 0.00021558
Iteration 105/1000 | Loss: 0.00029986
Iteration 106/1000 | Loss: 0.00024253
Iteration 107/1000 | Loss: 0.00019009
Iteration 108/1000 | Loss: 0.00021936
Iteration 109/1000 | Loss: 0.00007625
Iteration 110/1000 | Loss: 0.00007738
Iteration 111/1000 | Loss: 0.00002188
Iteration 112/1000 | Loss: 0.00002072
Iteration 113/1000 | Loss: 0.00001985
Iteration 114/1000 | Loss: 0.00001897
Iteration 115/1000 | Loss: 0.00005321
Iteration 116/1000 | Loss: 0.00001863
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00002874
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001701
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001697
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00009469
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001679
Iteration 133/1000 | Loss: 0.00001679
Iteration 134/1000 | Loss: 0.00001678
Iteration 135/1000 | Loss: 0.00001678
Iteration 136/1000 | Loss: 0.00001677
Iteration 137/1000 | Loss: 0.00001677
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001674
Iteration 141/1000 | Loss: 0.00001674
Iteration 142/1000 | Loss: 0.00001674
Iteration 143/1000 | Loss: 0.00001674
Iteration 144/1000 | Loss: 0.00001673
Iteration 145/1000 | Loss: 0.00001673
Iteration 146/1000 | Loss: 0.00001673
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001672
Iteration 149/1000 | Loss: 0.00001671
Iteration 150/1000 | Loss: 0.00001671
Iteration 151/1000 | Loss: 0.00001671
Iteration 152/1000 | Loss: 0.00001670
Iteration 153/1000 | Loss: 0.00001670
Iteration 154/1000 | Loss: 0.00001670
Iteration 155/1000 | Loss: 0.00001670
Iteration 156/1000 | Loss: 0.00001669
Iteration 157/1000 | Loss: 0.00001668
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001665
Iteration 160/1000 | Loss: 0.00001665
Iteration 161/1000 | Loss: 0.00001664
Iteration 162/1000 | Loss: 0.00001664
Iteration 163/1000 | Loss: 0.00001663
Iteration 164/1000 | Loss: 0.00001663
Iteration 165/1000 | Loss: 0.00001662
Iteration 166/1000 | Loss: 0.00001662
Iteration 167/1000 | Loss: 0.00001662
Iteration 168/1000 | Loss: 0.00001661
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001661
Iteration 171/1000 | Loss: 0.00001660
Iteration 172/1000 | Loss: 0.00001659
Iteration 173/1000 | Loss: 0.00001659
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001658
Iteration 177/1000 | Loss: 0.00001658
Iteration 178/1000 | Loss: 0.00001658
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001657
Iteration 182/1000 | Loss: 0.00001656
Iteration 183/1000 | Loss: 0.00001656
Iteration 184/1000 | Loss: 0.00001656
Iteration 185/1000 | Loss: 0.00001656
Iteration 186/1000 | Loss: 0.00001656
Iteration 187/1000 | Loss: 0.00001655
Iteration 188/1000 | Loss: 0.00001655
Iteration 189/1000 | Loss: 0.00001655
Iteration 190/1000 | Loss: 0.00001655
Iteration 191/1000 | Loss: 0.00001655
Iteration 192/1000 | Loss: 0.00001655
Iteration 193/1000 | Loss: 0.00001655
Iteration 194/1000 | Loss: 0.00001655
Iteration 195/1000 | Loss: 0.00001654
Iteration 196/1000 | Loss: 0.00001654
Iteration 197/1000 | Loss: 0.00001653
Iteration 198/1000 | Loss: 0.00001653
Iteration 199/1000 | Loss: 0.00001653
Iteration 200/1000 | Loss: 0.00001653
Iteration 201/1000 | Loss: 0.00001653
Iteration 202/1000 | Loss: 0.00001653
Iteration 203/1000 | Loss: 0.00001653
Iteration 204/1000 | Loss: 0.00001653
Iteration 205/1000 | Loss: 0.00001653
Iteration 206/1000 | Loss: 0.00001653
Iteration 207/1000 | Loss: 0.00001653
Iteration 208/1000 | Loss: 0.00001653
Iteration 209/1000 | Loss: 0.00001653
Iteration 210/1000 | Loss: 0.00001652
Iteration 211/1000 | Loss: 0.00001652
Iteration 212/1000 | Loss: 0.00001652
Iteration 213/1000 | Loss: 0.00001652
Iteration 214/1000 | Loss: 0.00001652
Iteration 215/1000 | Loss: 0.00001652
Iteration 216/1000 | Loss: 0.00001652
Iteration 217/1000 | Loss: 0.00001652
Iteration 218/1000 | Loss: 0.00001652
Iteration 219/1000 | Loss: 0.00001652
Iteration 220/1000 | Loss: 0.00001652
Iteration 221/1000 | Loss: 0.00001652
Iteration 222/1000 | Loss: 0.00001652
Iteration 223/1000 | Loss: 0.00001652
Iteration 224/1000 | Loss: 0.00001652
Iteration 225/1000 | Loss: 0.00001651
Iteration 226/1000 | Loss: 0.00001651
Iteration 227/1000 | Loss: 0.00001651
Iteration 228/1000 | Loss: 0.00001651
Iteration 229/1000 | Loss: 0.00001651
Iteration 230/1000 | Loss: 0.00001651
Iteration 231/1000 | Loss: 0.00001651
Iteration 232/1000 | Loss: 0.00001651
Iteration 233/1000 | Loss: 0.00001651
Iteration 234/1000 | Loss: 0.00001651
Iteration 235/1000 | Loss: 0.00001651
Iteration 236/1000 | Loss: 0.00001651
Iteration 237/1000 | Loss: 0.00001651
Iteration 238/1000 | Loss: 0.00001651
Iteration 239/1000 | Loss: 0.00001651
Iteration 240/1000 | Loss: 0.00001651
Iteration 241/1000 | Loss: 0.00001651
Iteration 242/1000 | Loss: 0.00001651
Iteration 243/1000 | Loss: 0.00001651
Iteration 244/1000 | Loss: 0.00001651
Iteration 245/1000 | Loss: 0.00001651
Iteration 246/1000 | Loss: 0.00001651
Iteration 247/1000 | Loss: 0.00001651
Iteration 248/1000 | Loss: 0.00001651
Iteration 249/1000 | Loss: 0.00001651
Iteration 250/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.651142883929424e-05, 1.651142883929424e-05, 1.651142883929424e-05, 1.651142883929424e-05, 1.651142883929424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.651142883929424e-05

Optimization complete. Final v2v error: 3.395319700241089 mm

Highest mean error: 4.5552496910095215 mm for frame 55

Lowest mean error: 2.893237352371216 mm for frame 31

Saving results

Total time: 224.4539816379547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648391
Iteration 2/25 | Loss: 0.00132340
Iteration 3/25 | Loss: 0.00126907
Iteration 4/25 | Loss: 0.00126460
Iteration 5/25 | Loss: 0.00126294
Iteration 6/25 | Loss: 0.00126294
Iteration 7/25 | Loss: 0.00126294
Iteration 8/25 | Loss: 0.00126294
Iteration 9/25 | Loss: 0.00126294
Iteration 10/25 | Loss: 0.00126294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012629387201741338, 0.0012629387201741338, 0.0012629387201741338, 0.0012629387201741338, 0.0012629387201741338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012629387201741338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06714106
Iteration 2/25 | Loss: 0.00097597
Iteration 3/25 | Loss: 0.00097591
Iteration 4/25 | Loss: 0.00097591
Iteration 5/25 | Loss: 0.00097591
Iteration 6/25 | Loss: 0.00097591
Iteration 7/25 | Loss: 0.00097591
Iteration 8/25 | Loss: 0.00097591
Iteration 9/25 | Loss: 0.00097591
Iteration 10/25 | Loss: 0.00097591
Iteration 11/25 | Loss: 0.00097591
Iteration 12/25 | Loss: 0.00097591
Iteration 13/25 | Loss: 0.00097591
Iteration 14/25 | Loss: 0.00097591
Iteration 15/25 | Loss: 0.00097591
Iteration 16/25 | Loss: 0.00097591
Iteration 17/25 | Loss: 0.00097591
Iteration 18/25 | Loss: 0.00097591
Iteration 19/25 | Loss: 0.00097591
Iteration 20/25 | Loss: 0.00097591
Iteration 21/25 | Loss: 0.00097591
Iteration 22/25 | Loss: 0.00097591
Iteration 23/25 | Loss: 0.00097591
Iteration 24/25 | Loss: 0.00097591
Iteration 25/25 | Loss: 0.00097591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097591
Iteration 2/1000 | Loss: 0.00002890
Iteration 3/1000 | Loss: 0.00002194
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001969
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001821
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001689
Iteration 13/1000 | Loss: 0.00001673
Iteration 14/1000 | Loss: 0.00001671
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001665
Iteration 21/1000 | Loss: 0.00001664
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001658
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001652
Iteration 29/1000 | Loss: 0.00001652
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001649
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001645
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001641
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001635
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001632
Iteration 81/1000 | Loss: 0.00001632
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001630
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001627
Iteration 93/1000 | Loss: 0.00001627
Iteration 94/1000 | Loss: 0.00001627
Iteration 95/1000 | Loss: 0.00001627
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001626
Iteration 99/1000 | Loss: 0.00001626
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00001625
Iteration 103/1000 | Loss: 0.00001625
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001625
Iteration 106/1000 | Loss: 0.00001625
Iteration 107/1000 | Loss: 0.00001624
Iteration 108/1000 | Loss: 0.00001624
Iteration 109/1000 | Loss: 0.00001624
Iteration 110/1000 | Loss: 0.00001624
Iteration 111/1000 | Loss: 0.00001624
Iteration 112/1000 | Loss: 0.00001623
Iteration 113/1000 | Loss: 0.00001623
Iteration 114/1000 | Loss: 0.00001623
Iteration 115/1000 | Loss: 0.00001623
Iteration 116/1000 | Loss: 0.00001623
Iteration 117/1000 | Loss: 0.00001622
Iteration 118/1000 | Loss: 0.00001622
Iteration 119/1000 | Loss: 0.00001622
Iteration 120/1000 | Loss: 0.00001622
Iteration 121/1000 | Loss: 0.00001622
Iteration 122/1000 | Loss: 0.00001622
Iteration 123/1000 | Loss: 0.00001622
Iteration 124/1000 | Loss: 0.00001622
Iteration 125/1000 | Loss: 0.00001622
Iteration 126/1000 | Loss: 0.00001622
Iteration 127/1000 | Loss: 0.00001622
Iteration 128/1000 | Loss: 0.00001622
Iteration 129/1000 | Loss: 0.00001622
Iteration 130/1000 | Loss: 0.00001621
Iteration 131/1000 | Loss: 0.00001621
Iteration 132/1000 | Loss: 0.00001621
Iteration 133/1000 | Loss: 0.00001621
Iteration 134/1000 | Loss: 0.00001621
Iteration 135/1000 | Loss: 0.00001621
Iteration 136/1000 | Loss: 0.00001621
Iteration 137/1000 | Loss: 0.00001621
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001620
Iteration 140/1000 | Loss: 0.00001620
Iteration 141/1000 | Loss: 0.00001620
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001619
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001617
Iteration 173/1000 | Loss: 0.00001617
Iteration 174/1000 | Loss: 0.00001617
Iteration 175/1000 | Loss: 0.00001617
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001617
Iteration 180/1000 | Loss: 0.00001616
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001616
Iteration 187/1000 | Loss: 0.00001616
Iteration 188/1000 | Loss: 0.00001616
Iteration 189/1000 | Loss: 0.00001616
Iteration 190/1000 | Loss: 0.00001616
Iteration 191/1000 | Loss: 0.00001616
Iteration 192/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.6157888239831664e-05, 1.6157888239831664e-05, 1.6157888239831664e-05, 1.6157888239831664e-05, 1.6157888239831664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6157888239831664e-05

Optimization complete. Final v2v error: 3.4623286724090576 mm

Highest mean error: 3.8284811973571777 mm for frame 54

Lowest mean error: 3.2192068099975586 mm for frame 154

Saving results

Total time: 40.223920583724976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994742
Iteration 2/25 | Loss: 0.00994742
Iteration 3/25 | Loss: 0.00994742
Iteration 4/25 | Loss: 0.00994742
Iteration 5/25 | Loss: 0.00994741
Iteration 6/25 | Loss: 0.00994741
Iteration 7/25 | Loss: 0.00994741
Iteration 8/25 | Loss: 0.00994741
Iteration 9/25 | Loss: 0.00994741
Iteration 10/25 | Loss: 0.00994741
Iteration 11/25 | Loss: 0.00994741
Iteration 12/25 | Loss: 0.00994741
Iteration 13/25 | Loss: 0.00994741
Iteration 14/25 | Loss: 0.00994741
Iteration 15/25 | Loss: 0.00994741
Iteration 16/25 | Loss: 0.00994740
Iteration 17/25 | Loss: 0.00994740
Iteration 18/25 | Loss: 0.00994740
Iteration 19/25 | Loss: 0.00994740
Iteration 20/25 | Loss: 0.00994740
Iteration 21/25 | Loss: 0.00994740
Iteration 22/25 | Loss: 0.00994740
Iteration 23/25 | Loss: 0.00994740
Iteration 24/25 | Loss: 0.00994740
Iteration 25/25 | Loss: 0.00994740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59224081
Iteration 2/25 | Loss: 0.11971103
Iteration 3/25 | Loss: 0.11918362
Iteration 4/25 | Loss: 0.11799571
Iteration 5/25 | Loss: 0.11799569
Iteration 6/25 | Loss: 0.11799569
Iteration 7/25 | Loss: 0.11799568
Iteration 8/25 | Loss: 0.11799568
Iteration 9/25 | Loss: 0.11799568
Iteration 10/25 | Loss: 0.11799568
Iteration 11/25 | Loss: 0.11799568
Iteration 12/25 | Loss: 0.11799568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.11799567937850952, 0.11799567937850952, 0.11799567937850952, 0.11799567937850952, 0.11799567937850952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11799567937850952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11799568
Iteration 2/1000 | Loss: 0.00306022
Iteration 3/1000 | Loss: 0.00692161
Iteration 4/1000 | Loss: 0.00563861
Iteration 5/1000 | Loss: 0.00097247
Iteration 6/1000 | Loss: 0.00122158
Iteration 7/1000 | Loss: 0.00041101
Iteration 8/1000 | Loss: 0.00181108
Iteration 9/1000 | Loss: 0.00072650
Iteration 10/1000 | Loss: 0.00027523
Iteration 11/1000 | Loss: 0.00007889
Iteration 12/1000 | Loss: 0.00074434
Iteration 13/1000 | Loss: 0.00013919
Iteration 14/1000 | Loss: 0.00021385
Iteration 15/1000 | Loss: 0.00005183
Iteration 16/1000 | Loss: 0.00020394
Iteration 17/1000 | Loss: 0.00007437
Iteration 18/1000 | Loss: 0.00005881
Iteration 19/1000 | Loss: 0.00003815
Iteration 20/1000 | Loss: 0.00006056
Iteration 21/1000 | Loss: 0.00003790
Iteration 22/1000 | Loss: 0.00005125
Iteration 23/1000 | Loss: 0.00008541
Iteration 24/1000 | Loss: 0.00003852
Iteration 25/1000 | Loss: 0.00005300
Iteration 26/1000 | Loss: 0.00007711
Iteration 27/1000 | Loss: 0.00003710
Iteration 28/1000 | Loss: 0.00014945
Iteration 29/1000 | Loss: 0.00002103
Iteration 30/1000 | Loss: 0.00003242
Iteration 31/1000 | Loss: 0.00009615
Iteration 32/1000 | Loss: 0.00006286
Iteration 33/1000 | Loss: 0.00025680
Iteration 34/1000 | Loss: 0.00010641
Iteration 35/1000 | Loss: 0.00015657
Iteration 36/1000 | Loss: 0.00011131
Iteration 37/1000 | Loss: 0.00010696
Iteration 38/1000 | Loss: 0.00005970
Iteration 39/1000 | Loss: 0.00009778
Iteration 40/1000 | Loss: 0.00005895
Iteration 41/1000 | Loss: 0.00091577
Iteration 42/1000 | Loss: 0.00003483
Iteration 43/1000 | Loss: 0.00003901
Iteration 44/1000 | Loss: 0.00002365
Iteration 45/1000 | Loss: 0.00002656
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00003228
Iteration 48/1000 | Loss: 0.00004103
Iteration 49/1000 | Loss: 0.00002508
Iteration 50/1000 | Loss: 0.00002565
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00006328
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00004003
Iteration 55/1000 | Loss: 0.00004496
Iteration 56/1000 | Loss: 0.00011110
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00001810
Iteration 60/1000 | Loss: 0.00001790
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001659
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001659
Iteration 65/1000 | Loss: 0.00001659
Iteration 66/1000 | Loss: 0.00001658
Iteration 67/1000 | Loss: 0.00001658
Iteration 68/1000 | Loss: 0.00001658
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001657
Iteration 73/1000 | Loss: 0.00001657
Iteration 74/1000 | Loss: 0.00001657
Iteration 75/1000 | Loss: 0.00001657
Iteration 76/1000 | Loss: 0.00001656
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001652
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001651
Iteration 85/1000 | Loss: 0.00001648
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001647
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001645
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00002756
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001642
Iteration 103/1000 | Loss: 0.00001642
Iteration 104/1000 | Loss: 0.00001642
Iteration 105/1000 | Loss: 0.00001641
Iteration 106/1000 | Loss: 0.00001641
Iteration 107/1000 | Loss: 0.00001641
Iteration 108/1000 | Loss: 0.00001641
Iteration 109/1000 | Loss: 0.00001641
Iteration 110/1000 | Loss: 0.00001641
Iteration 111/1000 | Loss: 0.00002864
Iteration 112/1000 | Loss: 0.00002072
Iteration 113/1000 | Loss: 0.00010477
Iteration 114/1000 | Loss: 0.00001894
Iteration 115/1000 | Loss: 0.00010746
Iteration 116/1000 | Loss: 0.00002601
Iteration 117/1000 | Loss: 0.00005050
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001637
Iteration 120/1000 | Loss: 0.00001637
Iteration 121/1000 | Loss: 0.00001637
Iteration 122/1000 | Loss: 0.00002148
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001637
Iteration 130/1000 | Loss: 0.00001637
Iteration 131/1000 | Loss: 0.00001637
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Iteration 135/1000 | Loss: 0.00001636
Iteration 136/1000 | Loss: 0.00001636
Iteration 137/1000 | Loss: 0.00001636
Iteration 138/1000 | Loss: 0.00001636
Iteration 139/1000 | Loss: 0.00001636
Iteration 140/1000 | Loss: 0.00001636
Iteration 141/1000 | Loss: 0.00001636
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001635
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001635
Iteration 149/1000 | Loss: 0.00001635
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001635
Iteration 154/1000 | Loss: 0.00001635
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001634
Iteration 157/1000 | Loss: 0.00001634
Iteration 158/1000 | Loss: 0.00001634
Iteration 159/1000 | Loss: 0.00001634
Iteration 160/1000 | Loss: 0.00001634
Iteration 161/1000 | Loss: 0.00001634
Iteration 162/1000 | Loss: 0.00001634
Iteration 163/1000 | Loss: 0.00001634
Iteration 164/1000 | Loss: 0.00001634
Iteration 165/1000 | Loss: 0.00001634
Iteration 166/1000 | Loss: 0.00001634
Iteration 167/1000 | Loss: 0.00001634
Iteration 168/1000 | Loss: 0.00001634
Iteration 169/1000 | Loss: 0.00001634
Iteration 170/1000 | Loss: 0.00001634
Iteration 171/1000 | Loss: 0.00001634
Iteration 172/1000 | Loss: 0.00001633
Iteration 173/1000 | Loss: 0.00001633
Iteration 174/1000 | Loss: 0.00001633
Iteration 175/1000 | Loss: 0.00001633
Iteration 176/1000 | Loss: 0.00001633
Iteration 177/1000 | Loss: 0.00001633
Iteration 178/1000 | Loss: 0.00001633
Iteration 179/1000 | Loss: 0.00001633
Iteration 180/1000 | Loss: 0.00001633
Iteration 181/1000 | Loss: 0.00001633
Iteration 182/1000 | Loss: 0.00001633
Iteration 183/1000 | Loss: 0.00001633
Iteration 184/1000 | Loss: 0.00001633
Iteration 185/1000 | Loss: 0.00001633
Iteration 186/1000 | Loss: 0.00001633
Iteration 187/1000 | Loss: 0.00001633
Iteration 188/1000 | Loss: 0.00001633
Iteration 189/1000 | Loss: 0.00001633
Iteration 190/1000 | Loss: 0.00001633
Iteration 191/1000 | Loss: 0.00001633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6329100617440417e-05, 1.6329100617440417e-05, 1.6329100617440417e-05, 1.6329100617440417e-05, 1.6329100617440417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6329100617440417e-05

Optimization complete. Final v2v error: 3.4240474700927734 mm

Highest mean error: 3.570464849472046 mm for frame 226

Lowest mean error: 3.3429481983184814 mm for frame 230

Saving results

Total time: 126.27216053009033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402287
Iteration 2/25 | Loss: 0.00136338
Iteration 3/25 | Loss: 0.00122858
Iteration 4/25 | Loss: 0.00121194
Iteration 5/25 | Loss: 0.00120943
Iteration 6/25 | Loss: 0.00120911
Iteration 7/25 | Loss: 0.00120911
Iteration 8/25 | Loss: 0.00120906
Iteration 9/25 | Loss: 0.00120906
Iteration 10/25 | Loss: 0.00120906
Iteration 11/25 | Loss: 0.00120906
Iteration 12/25 | Loss: 0.00120906
Iteration 13/25 | Loss: 0.00120906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012090569362044334, 0.0012090569362044334, 0.0012090569362044334, 0.0012090569362044334, 0.0012090569362044334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012090569362044334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34539413
Iteration 2/25 | Loss: 0.00077828
Iteration 3/25 | Loss: 0.00077828
Iteration 4/25 | Loss: 0.00077828
Iteration 5/25 | Loss: 0.00077828
Iteration 6/25 | Loss: 0.00077828
Iteration 7/25 | Loss: 0.00077828
Iteration 8/25 | Loss: 0.00077828
Iteration 9/25 | Loss: 0.00077828
Iteration 10/25 | Loss: 0.00077827
Iteration 11/25 | Loss: 0.00077827
Iteration 12/25 | Loss: 0.00077827
Iteration 13/25 | Loss: 0.00077827
Iteration 14/25 | Loss: 0.00077827
Iteration 15/25 | Loss: 0.00077827
Iteration 16/25 | Loss: 0.00077827
Iteration 17/25 | Loss: 0.00077827
Iteration 18/25 | Loss: 0.00077827
Iteration 19/25 | Loss: 0.00077827
Iteration 20/25 | Loss: 0.00077827
Iteration 21/25 | Loss: 0.00077827
Iteration 22/25 | Loss: 0.00077827
Iteration 23/25 | Loss: 0.00077827
Iteration 24/25 | Loss: 0.00077827
Iteration 25/25 | Loss: 0.00077827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077827
Iteration 2/1000 | Loss: 0.00002635
Iteration 3/1000 | Loss: 0.00001874
Iteration 4/1000 | Loss: 0.00001687
Iteration 5/1000 | Loss: 0.00001576
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001388
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001357
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001302
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001287
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001264
Iteration 24/1000 | Loss: 0.00001263
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001241
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001239
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001238
Iteration 101/1000 | Loss: 0.00001238
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001237
Iteration 106/1000 | Loss: 0.00001237
Iteration 107/1000 | Loss: 0.00001237
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001236
Iteration 111/1000 | Loss: 0.00001236
Iteration 112/1000 | Loss: 0.00001236
Iteration 113/1000 | Loss: 0.00001236
Iteration 114/1000 | Loss: 0.00001236
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001235
Iteration 121/1000 | Loss: 0.00001235
Iteration 122/1000 | Loss: 0.00001235
Iteration 123/1000 | Loss: 0.00001234
Iteration 124/1000 | Loss: 0.00001234
Iteration 125/1000 | Loss: 0.00001234
Iteration 126/1000 | Loss: 0.00001234
Iteration 127/1000 | Loss: 0.00001234
Iteration 128/1000 | Loss: 0.00001234
Iteration 129/1000 | Loss: 0.00001234
Iteration 130/1000 | Loss: 0.00001234
Iteration 131/1000 | Loss: 0.00001234
Iteration 132/1000 | Loss: 0.00001234
Iteration 133/1000 | Loss: 0.00001234
Iteration 134/1000 | Loss: 0.00001234
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001233
Iteration 137/1000 | Loss: 0.00001233
Iteration 138/1000 | Loss: 0.00001233
Iteration 139/1000 | Loss: 0.00001233
Iteration 140/1000 | Loss: 0.00001233
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Iteration 149/1000 | Loss: 0.00001233
Iteration 150/1000 | Loss: 0.00001233
Iteration 151/1000 | Loss: 0.00001233
Iteration 152/1000 | Loss: 0.00001233
Iteration 153/1000 | Loss: 0.00001233
Iteration 154/1000 | Loss: 0.00001233
Iteration 155/1000 | Loss: 0.00001233
Iteration 156/1000 | Loss: 0.00001233
Iteration 157/1000 | Loss: 0.00001233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2327218428254128e-05, 1.2327218428254128e-05, 1.2327218428254128e-05, 1.2327218428254128e-05, 1.2327218428254128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2327218428254128e-05

Optimization complete. Final v2v error: 3.012972354888916 mm

Highest mean error: 3.3025095462799072 mm for frame 72

Lowest mean error: 2.7845511436462402 mm for frame 16

Saving results

Total time: 40.22483491897583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695718
Iteration 2/25 | Loss: 0.00166501
Iteration 3/25 | Loss: 0.00134710
Iteration 4/25 | Loss: 0.00130180
Iteration 5/25 | Loss: 0.00129830
Iteration 6/25 | Loss: 0.00129011
Iteration 7/25 | Loss: 0.00128762
Iteration 8/25 | Loss: 0.00128437
Iteration 9/25 | Loss: 0.00128155
Iteration 10/25 | Loss: 0.00127983
Iteration 11/25 | Loss: 0.00127954
Iteration 12/25 | Loss: 0.00127940
Iteration 13/25 | Loss: 0.00127956
Iteration 14/25 | Loss: 0.00127885
Iteration 15/25 | Loss: 0.00127845
Iteration 16/25 | Loss: 0.00127826
Iteration 17/25 | Loss: 0.00127825
Iteration 18/25 | Loss: 0.00127825
Iteration 19/25 | Loss: 0.00127825
Iteration 20/25 | Loss: 0.00127825
Iteration 21/25 | Loss: 0.00127824
Iteration 22/25 | Loss: 0.00127824
Iteration 23/25 | Loss: 0.00127824
Iteration 24/25 | Loss: 0.00127824
Iteration 25/25 | Loss: 0.00127824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50346804
Iteration 2/25 | Loss: 0.00103098
Iteration 3/25 | Loss: 0.00103097
Iteration 4/25 | Loss: 0.00103096
Iteration 5/25 | Loss: 0.00103096
Iteration 6/25 | Loss: 0.00103096
Iteration 7/25 | Loss: 0.00103096
Iteration 8/25 | Loss: 0.00103096
Iteration 9/25 | Loss: 0.00103096
Iteration 10/25 | Loss: 0.00103096
Iteration 11/25 | Loss: 0.00103096
Iteration 12/25 | Loss: 0.00103096
Iteration 13/25 | Loss: 0.00103096
Iteration 14/25 | Loss: 0.00103096
Iteration 15/25 | Loss: 0.00103096
Iteration 16/25 | Loss: 0.00103096
Iteration 17/25 | Loss: 0.00103096
Iteration 18/25 | Loss: 0.00103096
Iteration 19/25 | Loss: 0.00103096
Iteration 20/25 | Loss: 0.00103096
Iteration 21/25 | Loss: 0.00103096
Iteration 22/25 | Loss: 0.00103096
Iteration 23/25 | Loss: 0.00103096
Iteration 24/25 | Loss: 0.00103096
Iteration 25/25 | Loss: 0.00103096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103096
Iteration 2/1000 | Loss: 0.00013120
Iteration 3/1000 | Loss: 0.00004740
Iteration 4/1000 | Loss: 0.00003185
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00012000
Iteration 7/1000 | Loss: 0.00003386
Iteration 8/1000 | Loss: 0.00002944
Iteration 9/1000 | Loss: 0.00002790
Iteration 10/1000 | Loss: 0.00002696
Iteration 11/1000 | Loss: 0.00002614
Iteration 12/1000 | Loss: 0.00011083
Iteration 13/1000 | Loss: 0.00008001
Iteration 14/1000 | Loss: 0.00005473
Iteration 15/1000 | Loss: 0.00002770
Iteration 16/1000 | Loss: 0.00002652
Iteration 17/1000 | Loss: 0.00002590
Iteration 18/1000 | Loss: 0.00007070
Iteration 19/1000 | Loss: 0.00002909
Iteration 20/1000 | Loss: 0.00012760
Iteration 21/1000 | Loss: 0.00018280
Iteration 22/1000 | Loss: 0.00004157
Iteration 23/1000 | Loss: 0.00003921
Iteration 24/1000 | Loss: 0.00016055
Iteration 25/1000 | Loss: 0.00016859
Iteration 26/1000 | Loss: 0.00007710
Iteration 27/1000 | Loss: 0.00012985
Iteration 28/1000 | Loss: 0.00005324
Iteration 29/1000 | Loss: 0.00015640
Iteration 30/1000 | Loss: 0.00010136
Iteration 31/1000 | Loss: 0.00002862
Iteration 32/1000 | Loss: 0.00003887
Iteration 33/1000 | Loss: 0.00007349
Iteration 34/1000 | Loss: 0.00002570
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002250
Iteration 37/1000 | Loss: 0.00006406
Iteration 38/1000 | Loss: 0.00002774
Iteration 39/1000 | Loss: 0.00002613
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00006955
Iteration 42/1000 | Loss: 0.00002567
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002280
Iteration 45/1000 | Loss: 0.00002231
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002147
Iteration 48/1000 | Loss: 0.00011956
Iteration 49/1000 | Loss: 0.00003681
Iteration 50/1000 | Loss: 0.00003648
Iteration 51/1000 | Loss: 0.00002913
Iteration 52/1000 | Loss: 0.00003193
Iteration 53/1000 | Loss: 0.00004054
Iteration 54/1000 | Loss: 0.00002403
Iteration 55/1000 | Loss: 0.00002299
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002096
Iteration 59/1000 | Loss: 0.00006869
Iteration 60/1000 | Loss: 0.00002603
Iteration 61/1000 | Loss: 0.00002064
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002002
Iteration 65/1000 | Loss: 0.00001986
Iteration 66/1000 | Loss: 0.00001986
Iteration 67/1000 | Loss: 0.00001984
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001981
Iteration 70/1000 | Loss: 0.00001970
Iteration 71/1000 | Loss: 0.00001963
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001958
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001950
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00012902
Iteration 81/1000 | Loss: 0.00002124
Iteration 82/1000 | Loss: 0.00002155
Iteration 83/1000 | Loss: 0.00001920
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001890
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00006040
Iteration 93/1000 | Loss: 0.00005998
Iteration 94/1000 | Loss: 0.00002181
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00007099
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001852
Iteration 101/1000 | Loss: 0.00001851
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001844
Iteration 117/1000 | Loss: 0.00001844
Iteration 118/1000 | Loss: 0.00001843
Iteration 119/1000 | Loss: 0.00001843
Iteration 120/1000 | Loss: 0.00001842
Iteration 121/1000 | Loss: 0.00001842
Iteration 122/1000 | Loss: 0.00001842
Iteration 123/1000 | Loss: 0.00001842
Iteration 124/1000 | Loss: 0.00001842
Iteration 125/1000 | Loss: 0.00001841
Iteration 126/1000 | Loss: 0.00001841
Iteration 127/1000 | Loss: 0.00001841
Iteration 128/1000 | Loss: 0.00001841
Iteration 129/1000 | Loss: 0.00001840
Iteration 130/1000 | Loss: 0.00001840
Iteration 131/1000 | Loss: 0.00001840
Iteration 132/1000 | Loss: 0.00001839
Iteration 133/1000 | Loss: 0.00001839
Iteration 134/1000 | Loss: 0.00001839
Iteration 135/1000 | Loss: 0.00001839
Iteration 136/1000 | Loss: 0.00001838
Iteration 137/1000 | Loss: 0.00001838
Iteration 138/1000 | Loss: 0.00001838
Iteration 139/1000 | Loss: 0.00001838
Iteration 140/1000 | Loss: 0.00001838
Iteration 141/1000 | Loss: 0.00001838
Iteration 142/1000 | Loss: 0.00001838
Iteration 143/1000 | Loss: 0.00001838
Iteration 144/1000 | Loss: 0.00001838
Iteration 145/1000 | Loss: 0.00001837
Iteration 146/1000 | Loss: 0.00001837
Iteration 147/1000 | Loss: 0.00001837
Iteration 148/1000 | Loss: 0.00001837
Iteration 149/1000 | Loss: 0.00001837
Iteration 150/1000 | Loss: 0.00001837
Iteration 151/1000 | Loss: 0.00001837
Iteration 152/1000 | Loss: 0.00001837
Iteration 153/1000 | Loss: 0.00001836
Iteration 154/1000 | Loss: 0.00001836
Iteration 155/1000 | Loss: 0.00001836
Iteration 156/1000 | Loss: 0.00001835
Iteration 157/1000 | Loss: 0.00001835
Iteration 158/1000 | Loss: 0.00001835
Iteration 159/1000 | Loss: 0.00001834
Iteration 160/1000 | Loss: 0.00001834
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001832
Iteration 165/1000 | Loss: 0.00001832
Iteration 166/1000 | Loss: 0.00001832
Iteration 167/1000 | Loss: 0.00001832
Iteration 168/1000 | Loss: 0.00001831
Iteration 169/1000 | Loss: 0.00001831
Iteration 170/1000 | Loss: 0.00001830
Iteration 171/1000 | Loss: 0.00001830
Iteration 172/1000 | Loss: 0.00001830
Iteration 173/1000 | Loss: 0.00001829
Iteration 174/1000 | Loss: 0.00001829
Iteration 175/1000 | Loss: 0.00001829
Iteration 176/1000 | Loss: 0.00001829
Iteration 177/1000 | Loss: 0.00001828
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001828
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001827
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Iteration 185/1000 | Loss: 0.00001826
Iteration 186/1000 | Loss: 0.00001826
Iteration 187/1000 | Loss: 0.00001826
Iteration 188/1000 | Loss: 0.00001826
Iteration 189/1000 | Loss: 0.00001826
Iteration 190/1000 | Loss: 0.00001825
Iteration 191/1000 | Loss: 0.00001825
Iteration 192/1000 | Loss: 0.00001825
Iteration 193/1000 | Loss: 0.00001825
Iteration 194/1000 | Loss: 0.00001824
Iteration 195/1000 | Loss: 0.00001824
Iteration 196/1000 | Loss: 0.00001824
Iteration 197/1000 | Loss: 0.00001824
Iteration 198/1000 | Loss: 0.00001824
Iteration 199/1000 | Loss: 0.00001824
Iteration 200/1000 | Loss: 0.00001823
Iteration 201/1000 | Loss: 0.00001823
Iteration 202/1000 | Loss: 0.00001823
Iteration 203/1000 | Loss: 0.00001823
Iteration 204/1000 | Loss: 0.00001822
Iteration 205/1000 | Loss: 0.00001820
Iteration 206/1000 | Loss: 0.00001820
Iteration 207/1000 | Loss: 0.00001820
Iteration 208/1000 | Loss: 0.00001820
Iteration 209/1000 | Loss: 0.00001820
Iteration 210/1000 | Loss: 0.00001820
Iteration 211/1000 | Loss: 0.00001820
Iteration 212/1000 | Loss: 0.00001819
Iteration 213/1000 | Loss: 0.00001819
Iteration 214/1000 | Loss: 0.00001819
Iteration 215/1000 | Loss: 0.00001819
Iteration 216/1000 | Loss: 0.00011408
Iteration 217/1000 | Loss: 0.00001829
Iteration 218/1000 | Loss: 0.00002922
Iteration 219/1000 | Loss: 0.00002217
Iteration 220/1000 | Loss: 0.00002162
Iteration 221/1000 | Loss: 0.00001820
Iteration 222/1000 | Loss: 0.00001820
Iteration 223/1000 | Loss: 0.00001820
Iteration 224/1000 | Loss: 0.00001819
Iteration 225/1000 | Loss: 0.00001819
Iteration 226/1000 | Loss: 0.00001819
Iteration 227/1000 | Loss: 0.00001819
Iteration 228/1000 | Loss: 0.00001819
Iteration 229/1000 | Loss: 0.00001819
Iteration 230/1000 | Loss: 0.00001819
Iteration 231/1000 | Loss: 0.00001819
Iteration 232/1000 | Loss: 0.00001819
Iteration 233/1000 | Loss: 0.00001819
Iteration 234/1000 | Loss: 0.00001819
Iteration 235/1000 | Loss: 0.00001819
Iteration 236/1000 | Loss: 0.00001819
Iteration 237/1000 | Loss: 0.00001818
Iteration 238/1000 | Loss: 0.00001818
Iteration 239/1000 | Loss: 0.00001818
Iteration 240/1000 | Loss: 0.00001818
Iteration 241/1000 | Loss: 0.00001817
Iteration 242/1000 | Loss: 0.00001817
Iteration 243/1000 | Loss: 0.00001817
Iteration 244/1000 | Loss: 0.00001817
Iteration 245/1000 | Loss: 0.00001817
Iteration 246/1000 | Loss: 0.00001817
Iteration 247/1000 | Loss: 0.00001817
Iteration 248/1000 | Loss: 0.00001817
Iteration 249/1000 | Loss: 0.00001817
Iteration 250/1000 | Loss: 0.00001817
Iteration 251/1000 | Loss: 0.00001816
Iteration 252/1000 | Loss: 0.00001816
Iteration 253/1000 | Loss: 0.00001816
Iteration 254/1000 | Loss: 0.00001816
Iteration 255/1000 | Loss: 0.00001816
Iteration 256/1000 | Loss: 0.00001816
Iteration 257/1000 | Loss: 0.00001816
Iteration 258/1000 | Loss: 0.00001816
Iteration 259/1000 | Loss: 0.00001816
Iteration 260/1000 | Loss: 0.00001815
Iteration 261/1000 | Loss: 0.00001815
Iteration 262/1000 | Loss: 0.00001815
Iteration 263/1000 | Loss: 0.00001815
Iteration 264/1000 | Loss: 0.00001815
Iteration 265/1000 | Loss: 0.00001815
Iteration 266/1000 | Loss: 0.00001814
Iteration 267/1000 | Loss: 0.00001814
Iteration 268/1000 | Loss: 0.00001814
Iteration 269/1000 | Loss: 0.00001814
Iteration 270/1000 | Loss: 0.00001814
Iteration 271/1000 | Loss: 0.00001814
Iteration 272/1000 | Loss: 0.00003656
Iteration 273/1000 | Loss: 0.00001932
Iteration 274/1000 | Loss: 0.00001821
Iteration 275/1000 | Loss: 0.00001821
Iteration 276/1000 | Loss: 0.00001820
Iteration 277/1000 | Loss: 0.00001820
Iteration 278/1000 | Loss: 0.00001820
Iteration 279/1000 | Loss: 0.00001820
Iteration 280/1000 | Loss: 0.00001820
Iteration 281/1000 | Loss: 0.00001820
Iteration 282/1000 | Loss: 0.00001819
Iteration 283/1000 | Loss: 0.00001819
Iteration 284/1000 | Loss: 0.00001819
Iteration 285/1000 | Loss: 0.00001818
Iteration 286/1000 | Loss: 0.00001818
Iteration 287/1000 | Loss: 0.00001818
Iteration 288/1000 | Loss: 0.00001818
Iteration 289/1000 | Loss: 0.00001923
Iteration 290/1000 | Loss: 0.00002284
Iteration 291/1000 | Loss: 0.00002449
Iteration 292/1000 | Loss: 0.00001815
Iteration 293/1000 | Loss: 0.00001815
Iteration 294/1000 | Loss: 0.00001815
Iteration 295/1000 | Loss: 0.00001815
Iteration 296/1000 | Loss: 0.00001815
Iteration 297/1000 | Loss: 0.00001815
Iteration 298/1000 | Loss: 0.00001815
Iteration 299/1000 | Loss: 0.00001815
Iteration 300/1000 | Loss: 0.00001815
Iteration 301/1000 | Loss: 0.00001815
Iteration 302/1000 | Loss: 0.00001815
Iteration 303/1000 | Loss: 0.00001815
Iteration 304/1000 | Loss: 0.00001815
Iteration 305/1000 | Loss: 0.00001815
Iteration 306/1000 | Loss: 0.00001815
Iteration 307/1000 | Loss: 0.00001815
Iteration 308/1000 | Loss: 0.00001815
Iteration 309/1000 | Loss: 0.00001815
Iteration 310/1000 | Loss: 0.00001815
Iteration 311/1000 | Loss: 0.00001815
Iteration 312/1000 | Loss: 0.00001815
Iteration 313/1000 | Loss: 0.00001815
Iteration 314/1000 | Loss: 0.00001815
Iteration 315/1000 | Loss: 0.00001815
Iteration 316/1000 | Loss: 0.00001815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [1.8146552974940278e-05, 1.8146552974940278e-05, 1.8146552974940278e-05, 1.8146552974940278e-05, 1.8146552974940278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8146552974940278e-05

Optimization complete. Final v2v error: 3.5441691875457764 mm

Highest mean error: 5.6471848487854 mm for frame 67

Lowest mean error: 2.7518422603607178 mm for frame 197

Saving results

Total time: 187.53930687904358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020255
Iteration 2/25 | Loss: 0.00200354
Iteration 3/25 | Loss: 0.00162525
Iteration 4/25 | Loss: 0.00175782
Iteration 5/25 | Loss: 0.00148560
Iteration 6/25 | Loss: 0.00144414
Iteration 7/25 | Loss: 0.00137295
Iteration 8/25 | Loss: 0.00138510
Iteration 9/25 | Loss: 0.00134036
Iteration 10/25 | Loss: 0.00133302
Iteration 11/25 | Loss: 0.00133656
Iteration 12/25 | Loss: 0.00132131
Iteration 13/25 | Loss: 0.00146693
Iteration 14/25 | Loss: 0.00134552
Iteration 15/25 | Loss: 0.00128219
Iteration 16/25 | Loss: 0.00127373
Iteration 17/25 | Loss: 0.00127845
Iteration 18/25 | Loss: 0.00127776
Iteration 19/25 | Loss: 0.00127237
Iteration 20/25 | Loss: 0.00127032
Iteration 21/25 | Loss: 0.00127000
Iteration 22/25 | Loss: 0.00126999
Iteration 23/25 | Loss: 0.00126998
Iteration 24/25 | Loss: 0.00126998
Iteration 25/25 | Loss: 0.00126998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53612947
Iteration 2/25 | Loss: 0.00131228
Iteration 3/25 | Loss: 0.00131228
Iteration 4/25 | Loss: 0.00131228
Iteration 5/25 | Loss: 0.00131228
Iteration 6/25 | Loss: 0.00131228
Iteration 7/25 | Loss: 0.00131228
Iteration 8/25 | Loss: 0.00131228
Iteration 9/25 | Loss: 0.00131228
Iteration 10/25 | Loss: 0.00131228
Iteration 11/25 | Loss: 0.00131228
Iteration 12/25 | Loss: 0.00131227
Iteration 13/25 | Loss: 0.00131227
Iteration 14/25 | Loss: 0.00131227
Iteration 15/25 | Loss: 0.00131227
Iteration 16/25 | Loss: 0.00131227
Iteration 17/25 | Loss: 0.00131227
Iteration 18/25 | Loss: 0.00131227
Iteration 19/25 | Loss: 0.00131227
Iteration 20/25 | Loss: 0.00131227
Iteration 21/25 | Loss: 0.00131227
Iteration 22/25 | Loss: 0.00131227
Iteration 23/25 | Loss: 0.00131227
Iteration 24/25 | Loss: 0.00131227
Iteration 25/25 | Loss: 0.00131227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131227
Iteration 2/1000 | Loss: 0.00006889
Iteration 3/1000 | Loss: 0.00172295
Iteration 4/1000 | Loss: 0.00004464
Iteration 5/1000 | Loss: 0.00003967
Iteration 6/1000 | Loss: 0.00033154
Iteration 7/1000 | Loss: 0.00005029
Iteration 8/1000 | Loss: 0.00016387
Iteration 9/1000 | Loss: 0.00003900
Iteration 10/1000 | Loss: 0.00003493
Iteration 11/1000 | Loss: 0.00144425
Iteration 12/1000 | Loss: 0.00005201
Iteration 13/1000 | Loss: 0.00005606
Iteration 14/1000 | Loss: 0.00003206
Iteration 15/1000 | Loss: 0.00002602
Iteration 16/1000 | Loss: 0.00002481
Iteration 17/1000 | Loss: 0.00006812
Iteration 18/1000 | Loss: 0.00002409
Iteration 19/1000 | Loss: 0.00003538
Iteration 20/1000 | Loss: 0.00003454
Iteration 21/1000 | Loss: 0.00039253
Iteration 22/1000 | Loss: 0.00078648
Iteration 23/1000 | Loss: 0.00140542
Iteration 24/1000 | Loss: 0.00051443
Iteration 25/1000 | Loss: 0.00005360
Iteration 26/1000 | Loss: 0.00007647
Iteration 27/1000 | Loss: 0.00003141
Iteration 28/1000 | Loss: 0.00004658
Iteration 29/1000 | Loss: 0.00056461
Iteration 30/1000 | Loss: 0.00006657
Iteration 31/1000 | Loss: 0.00002459
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00028733
Iteration 34/1000 | Loss: 0.00002442
Iteration 35/1000 | Loss: 0.00019124
Iteration 36/1000 | Loss: 0.00020436
Iteration 37/1000 | Loss: 0.00011560
Iteration 38/1000 | Loss: 0.00028669
Iteration 39/1000 | Loss: 0.00027774
Iteration 40/1000 | Loss: 0.00024442
Iteration 41/1000 | Loss: 0.00002355
Iteration 42/1000 | Loss: 0.00003253
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001615
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00030340
Iteration 50/1000 | Loss: 0.00025647
Iteration 51/1000 | Loss: 0.00026775
Iteration 52/1000 | Loss: 0.00021400
Iteration 53/1000 | Loss: 0.00023357
Iteration 54/1000 | Loss: 0.00018017
Iteration 55/1000 | Loss: 0.00023084
Iteration 56/1000 | Loss: 0.00018294
Iteration 57/1000 | Loss: 0.00022996
Iteration 58/1000 | Loss: 0.00014632
Iteration 59/1000 | Loss: 0.00016361
Iteration 60/1000 | Loss: 0.00020791
Iteration 61/1000 | Loss: 0.00016435
Iteration 62/1000 | Loss: 0.00020259
Iteration 63/1000 | Loss: 0.00014513
Iteration 64/1000 | Loss: 0.00020335
Iteration 65/1000 | Loss: 0.00019502
Iteration 66/1000 | Loss: 0.00013480
Iteration 67/1000 | Loss: 0.00012071
Iteration 68/1000 | Loss: 0.00016165
Iteration 69/1000 | Loss: 0.00020304
Iteration 70/1000 | Loss: 0.00025780
Iteration 71/1000 | Loss: 0.00021616
Iteration 72/1000 | Loss: 0.00003321
Iteration 73/1000 | Loss: 0.00003929
Iteration 74/1000 | Loss: 0.00002170
Iteration 75/1000 | Loss: 0.00001853
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00002162
Iteration 85/1000 | Loss: 0.00001246
Iteration 86/1000 | Loss: 0.00002485
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001219
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001218
Iteration 119/1000 | Loss: 0.00001218
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001215
Iteration 126/1000 | Loss: 0.00001215
Iteration 127/1000 | Loss: 0.00001215
Iteration 128/1000 | Loss: 0.00001215
Iteration 129/1000 | Loss: 0.00001215
Iteration 130/1000 | Loss: 0.00001215
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001214
Iteration 144/1000 | Loss: 0.00001214
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Iteration 147/1000 | Loss: 0.00001213
Iteration 148/1000 | Loss: 0.00001213
Iteration 149/1000 | Loss: 0.00001213
Iteration 150/1000 | Loss: 0.00001213
Iteration 151/1000 | Loss: 0.00001212
Iteration 152/1000 | Loss: 0.00001212
Iteration 153/1000 | Loss: 0.00001212
Iteration 154/1000 | Loss: 0.00001212
Iteration 155/1000 | Loss: 0.00001212
Iteration 156/1000 | Loss: 0.00001212
Iteration 157/1000 | Loss: 0.00001212
Iteration 158/1000 | Loss: 0.00001212
Iteration 159/1000 | Loss: 0.00001212
Iteration 160/1000 | Loss: 0.00001212
Iteration 161/1000 | Loss: 0.00001212
Iteration 162/1000 | Loss: 0.00001212
Iteration 163/1000 | Loss: 0.00001212
Iteration 164/1000 | Loss: 0.00001212
Iteration 165/1000 | Loss: 0.00001212
Iteration 166/1000 | Loss: 0.00001212
Iteration 167/1000 | Loss: 0.00001211
Iteration 168/1000 | Loss: 0.00001211
Iteration 169/1000 | Loss: 0.00001211
Iteration 170/1000 | Loss: 0.00001211
Iteration 171/1000 | Loss: 0.00001211
Iteration 172/1000 | Loss: 0.00001211
Iteration 173/1000 | Loss: 0.00001211
Iteration 174/1000 | Loss: 0.00001211
Iteration 175/1000 | Loss: 0.00001211
Iteration 176/1000 | Loss: 0.00001211
Iteration 177/1000 | Loss: 0.00001211
Iteration 178/1000 | Loss: 0.00001211
Iteration 179/1000 | Loss: 0.00001211
Iteration 180/1000 | Loss: 0.00001211
Iteration 181/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.2111116120649967e-05, 1.2111116120649967e-05, 1.2111116120649967e-05, 1.2111116120649967e-05, 1.2111116120649967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2111116120649967e-05

Optimization complete. Final v2v error: 2.9506306648254395 mm

Highest mean error: 4.525087356567383 mm for frame 48

Lowest mean error: 2.6848952770233154 mm for frame 135

Saving results

Total time: 156.19585394859314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499145
Iteration 2/25 | Loss: 0.00130200
Iteration 3/25 | Loss: 0.00121669
Iteration 4/25 | Loss: 0.00120143
Iteration 5/25 | Loss: 0.00119689
Iteration 6/25 | Loss: 0.00119586
Iteration 7/25 | Loss: 0.00119586
Iteration 8/25 | Loss: 0.00119586
Iteration 9/25 | Loss: 0.00119586
Iteration 10/25 | Loss: 0.00119586
Iteration 11/25 | Loss: 0.00119586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011958579998463392, 0.0011958579998463392, 0.0011958579998463392, 0.0011958579998463392, 0.0011958579998463392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011958579998463392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49591243
Iteration 2/25 | Loss: 0.00090027
Iteration 3/25 | Loss: 0.00090026
Iteration 4/25 | Loss: 0.00090026
Iteration 5/25 | Loss: 0.00090026
Iteration 6/25 | Loss: 0.00090026
Iteration 7/25 | Loss: 0.00090026
Iteration 8/25 | Loss: 0.00090026
Iteration 9/25 | Loss: 0.00090026
Iteration 10/25 | Loss: 0.00090026
Iteration 11/25 | Loss: 0.00090026
Iteration 12/25 | Loss: 0.00090026
Iteration 13/25 | Loss: 0.00090026
Iteration 14/25 | Loss: 0.00090026
Iteration 15/25 | Loss: 0.00090026
Iteration 16/25 | Loss: 0.00090026
Iteration 17/25 | Loss: 0.00090026
Iteration 18/25 | Loss: 0.00090026
Iteration 19/25 | Loss: 0.00090026
Iteration 20/25 | Loss: 0.00090026
Iteration 21/25 | Loss: 0.00090026
Iteration 22/25 | Loss: 0.00090026
Iteration 23/25 | Loss: 0.00090026
Iteration 24/25 | Loss: 0.00090026
Iteration 25/25 | Loss: 0.00090026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090026
Iteration 2/1000 | Loss: 0.00002175
Iteration 3/1000 | Loss: 0.00001604
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001394
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001270
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001207
Iteration 12/1000 | Loss: 0.00001197
Iteration 13/1000 | Loss: 0.00001184
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001151
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001145
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001144
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001125
Iteration 38/1000 | Loss: 0.00001125
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001124
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001120
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001118
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001118
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001114
Iteration 72/1000 | Loss: 0.00001114
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001106
Iteration 117/1000 | Loss: 0.00001106
Iteration 118/1000 | Loss: 0.00001106
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001106
Iteration 123/1000 | Loss: 0.00001106
Iteration 124/1000 | Loss: 0.00001106
Iteration 125/1000 | Loss: 0.00001106
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001106
Iteration 129/1000 | Loss: 0.00001106
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001105
Iteration 134/1000 | Loss: 0.00001105
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001104
Iteration 145/1000 | Loss: 0.00001104
Iteration 146/1000 | Loss: 0.00001104
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001104
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001104
Iteration 155/1000 | Loss: 0.00001104
Iteration 156/1000 | Loss: 0.00001104
Iteration 157/1000 | Loss: 0.00001104
Iteration 158/1000 | Loss: 0.00001104
Iteration 159/1000 | Loss: 0.00001104
Iteration 160/1000 | Loss: 0.00001104
Iteration 161/1000 | Loss: 0.00001104
Iteration 162/1000 | Loss: 0.00001104
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001104
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001104
Iteration 168/1000 | Loss: 0.00001104
Iteration 169/1000 | Loss: 0.00001104
Iteration 170/1000 | Loss: 0.00001104
Iteration 171/1000 | Loss: 0.00001104
Iteration 172/1000 | Loss: 0.00001104
Iteration 173/1000 | Loss: 0.00001104
Iteration 174/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.104128659790149e-05, 1.104128659790149e-05, 1.104128659790149e-05, 1.104128659790149e-05, 1.104128659790149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.104128659790149e-05

Optimization complete. Final v2v error: 2.8850250244140625 mm

Highest mean error: 3.1263368129730225 mm for frame 78

Lowest mean error: 2.7110559940338135 mm for frame 97

Saving results

Total time: 39.299818992614746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026896
Iteration 2/25 | Loss: 0.00224011
Iteration 3/25 | Loss: 0.00221294
Iteration 4/25 | Loss: 0.00146199
Iteration 5/25 | Loss: 0.00161654
Iteration 6/25 | Loss: 0.00135991
Iteration 7/25 | Loss: 0.00130884
Iteration 8/25 | Loss: 0.00131348
Iteration 9/25 | Loss: 0.00129257
Iteration 10/25 | Loss: 0.00124510
Iteration 11/25 | Loss: 0.00124766
Iteration 12/25 | Loss: 0.00124122
Iteration 13/25 | Loss: 0.00123881
Iteration 14/25 | Loss: 0.00124135
Iteration 15/25 | Loss: 0.00124028
Iteration 16/25 | Loss: 0.00123906
Iteration 17/25 | Loss: 0.00124126
Iteration 18/25 | Loss: 0.00123786
Iteration 19/25 | Loss: 0.00124783
Iteration 20/25 | Loss: 0.00124666
Iteration 21/25 | Loss: 0.00123918
Iteration 22/25 | Loss: 0.00123702
Iteration 23/25 | Loss: 0.00123501
Iteration 24/25 | Loss: 0.00123424
Iteration 25/25 | Loss: 0.00123498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38786221
Iteration 2/25 | Loss: 0.00134734
Iteration 3/25 | Loss: 0.00121394
Iteration 4/25 | Loss: 0.00121394
Iteration 5/25 | Loss: 0.00121394
Iteration 6/25 | Loss: 0.00121394
Iteration 7/25 | Loss: 0.00121394
Iteration 8/25 | Loss: 0.00121394
Iteration 9/25 | Loss: 0.00121394
Iteration 10/25 | Loss: 0.00121394
Iteration 11/25 | Loss: 0.00121394
Iteration 12/25 | Loss: 0.00121394
Iteration 13/25 | Loss: 0.00121394
Iteration 14/25 | Loss: 0.00121394
Iteration 15/25 | Loss: 0.00121394
Iteration 16/25 | Loss: 0.00121394
Iteration 17/25 | Loss: 0.00121394
Iteration 18/25 | Loss: 0.00121394
Iteration 19/25 | Loss: 0.00121394
Iteration 20/25 | Loss: 0.00121394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001213936018757522, 0.001213936018757522, 0.001213936018757522, 0.001213936018757522, 0.001213936018757522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001213936018757522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121394
Iteration 2/1000 | Loss: 0.00021547
Iteration 3/1000 | Loss: 0.00032476
Iteration 4/1000 | Loss: 0.00016626
Iteration 5/1000 | Loss: 0.00020839
Iteration 6/1000 | Loss: 0.00012702
Iteration 7/1000 | Loss: 0.00016429
Iteration 8/1000 | Loss: 0.00027603
Iteration 9/1000 | Loss: 0.00010167
Iteration 10/1000 | Loss: 0.00019456
Iteration 11/1000 | Loss: 0.00017873
Iteration 12/1000 | Loss: 0.00017536
Iteration 13/1000 | Loss: 0.00028891
Iteration 14/1000 | Loss: 0.00016199
Iteration 15/1000 | Loss: 0.00030152
Iteration 16/1000 | Loss: 0.00028106
Iteration 17/1000 | Loss: 0.00013391
Iteration 18/1000 | Loss: 0.00008259
Iteration 19/1000 | Loss: 0.00007658
Iteration 20/1000 | Loss: 0.00016366
Iteration 21/1000 | Loss: 0.00005169
Iteration 22/1000 | Loss: 0.00019227
Iteration 23/1000 | Loss: 0.00427033
Iteration 24/1000 | Loss: 0.00271621
Iteration 25/1000 | Loss: 0.00248655
Iteration 26/1000 | Loss: 0.00085474
Iteration 27/1000 | Loss: 0.00037008
Iteration 28/1000 | Loss: 0.00017694
Iteration 29/1000 | Loss: 0.00017382
Iteration 30/1000 | Loss: 0.00013680
Iteration 31/1000 | Loss: 0.00006709
Iteration 32/1000 | Loss: 0.00019776
Iteration 33/1000 | Loss: 0.00016598
Iteration 34/1000 | Loss: 0.00024875
Iteration 35/1000 | Loss: 0.00027161
Iteration 36/1000 | Loss: 0.00006329
Iteration 37/1000 | Loss: 0.00012898
Iteration 38/1000 | Loss: 0.00018370
Iteration 39/1000 | Loss: 0.00018049
Iteration 40/1000 | Loss: 0.00022682
Iteration 41/1000 | Loss: 0.00017001
Iteration 42/1000 | Loss: 0.00005812
Iteration 43/1000 | Loss: 0.00005131
Iteration 44/1000 | Loss: 0.00004544
Iteration 45/1000 | Loss: 0.00014222
Iteration 46/1000 | Loss: 0.00004211
Iteration 47/1000 | Loss: 0.00006785
Iteration 48/1000 | Loss: 0.00073797
Iteration 49/1000 | Loss: 0.00018835
Iteration 50/1000 | Loss: 0.00030156
Iteration 51/1000 | Loss: 0.00042828
Iteration 52/1000 | Loss: 0.00074820
Iteration 53/1000 | Loss: 0.00042768
Iteration 54/1000 | Loss: 0.00010477
Iteration 55/1000 | Loss: 0.00006555
Iteration 56/1000 | Loss: 0.00002482
Iteration 57/1000 | Loss: 0.00004176
Iteration 58/1000 | Loss: 0.00003635
Iteration 59/1000 | Loss: 0.00004372
Iteration 60/1000 | Loss: 0.00008436
Iteration 61/1000 | Loss: 0.00007248
Iteration 62/1000 | Loss: 0.00004479
Iteration 63/1000 | Loss: 0.00005127
Iteration 64/1000 | Loss: 0.00004272
Iteration 65/1000 | Loss: 0.00004719
Iteration 66/1000 | Loss: 0.00009195
Iteration 67/1000 | Loss: 0.00004249
Iteration 68/1000 | Loss: 0.00006326
Iteration 69/1000 | Loss: 0.00006731
Iteration 70/1000 | Loss: 0.00005695
Iteration 71/1000 | Loss: 0.00003697
Iteration 72/1000 | Loss: 0.00007309
Iteration 73/1000 | Loss: 0.00002464
Iteration 74/1000 | Loss: 0.00004817
Iteration 75/1000 | Loss: 0.00010612
Iteration 76/1000 | Loss: 0.00004364
Iteration 77/1000 | Loss: 0.00004743
Iteration 78/1000 | Loss: 0.00008888
Iteration 79/1000 | Loss: 0.00005822
Iteration 80/1000 | Loss: 0.00004169
Iteration 81/1000 | Loss: 0.00003490
Iteration 82/1000 | Loss: 0.00005403
Iteration 83/1000 | Loss: 0.00003717
Iteration 84/1000 | Loss: 0.00022362
Iteration 85/1000 | Loss: 0.00017074
Iteration 86/1000 | Loss: 0.00005852
Iteration 87/1000 | Loss: 0.00024007
Iteration 88/1000 | Loss: 0.00016709
Iteration 89/1000 | Loss: 0.00019544
Iteration 90/1000 | Loss: 0.00012676
Iteration 91/1000 | Loss: 0.00016026
Iteration 92/1000 | Loss: 0.00010240
Iteration 93/1000 | Loss: 0.00004319
Iteration 94/1000 | Loss: 0.00004719
Iteration 95/1000 | Loss: 0.00015279
Iteration 96/1000 | Loss: 0.00005096
Iteration 97/1000 | Loss: 0.00015492
Iteration 98/1000 | Loss: 0.00018607
Iteration 99/1000 | Loss: 0.00006456
Iteration 100/1000 | Loss: 0.00010579
Iteration 101/1000 | Loss: 0.00013047
Iteration 102/1000 | Loss: 0.00009984
Iteration 103/1000 | Loss: 0.00014077
Iteration 104/1000 | Loss: 0.00008938
Iteration 105/1000 | Loss: 0.00005904
Iteration 106/1000 | Loss: 0.00005071
Iteration 107/1000 | Loss: 0.00004871
Iteration 108/1000 | Loss: 0.00003831
Iteration 109/1000 | Loss: 0.00005186
Iteration 110/1000 | Loss: 0.00004385
Iteration 111/1000 | Loss: 0.00003557
Iteration 112/1000 | Loss: 0.00003548
Iteration 113/1000 | Loss: 0.00006633
Iteration 114/1000 | Loss: 0.00042793
Iteration 115/1000 | Loss: 0.00004166
Iteration 116/1000 | Loss: 0.00005861
Iteration 117/1000 | Loss: 0.00003806
Iteration 118/1000 | Loss: 0.00005512
Iteration 119/1000 | Loss: 0.00003967
Iteration 120/1000 | Loss: 0.00005426
Iteration 121/1000 | Loss: 0.00005331
Iteration 122/1000 | Loss: 0.00007109
Iteration 123/1000 | Loss: 0.00008780
Iteration 124/1000 | Loss: 0.00004225
Iteration 125/1000 | Loss: 0.00003551
Iteration 126/1000 | Loss: 0.00003708
Iteration 127/1000 | Loss: 0.00006505
Iteration 128/1000 | Loss: 0.00003753
Iteration 129/1000 | Loss: 0.00003846
Iteration 130/1000 | Loss: 0.00007591
Iteration 131/1000 | Loss: 0.00004024
Iteration 132/1000 | Loss: 0.00014526
Iteration 133/1000 | Loss: 0.00010475
Iteration 134/1000 | Loss: 0.00003789
Iteration 135/1000 | Loss: 0.00003545
Iteration 136/1000 | Loss: 0.00004248
Iteration 137/1000 | Loss: 0.00004624
Iteration 138/1000 | Loss: 0.00003847
Iteration 139/1000 | Loss: 0.00005423
Iteration 140/1000 | Loss: 0.00003832
Iteration 141/1000 | Loss: 0.00005180
Iteration 142/1000 | Loss: 0.00005417
Iteration 143/1000 | Loss: 0.00005684
Iteration 144/1000 | Loss: 0.00003644
Iteration 145/1000 | Loss: 0.00003469
Iteration 146/1000 | Loss: 0.00003901
Iteration 147/1000 | Loss: 0.00008131
Iteration 148/1000 | Loss: 0.00004143
Iteration 149/1000 | Loss: 0.00003707
Iteration 150/1000 | Loss: 0.00003662
Iteration 151/1000 | Loss: 0.00004200
Iteration 152/1000 | Loss: 0.00003453
Iteration 153/1000 | Loss: 0.00003337
Iteration 154/1000 | Loss: 0.00005312
Iteration 155/1000 | Loss: 0.00003320
Iteration 156/1000 | Loss: 0.00004210
Iteration 157/1000 | Loss: 0.00002464
Iteration 158/1000 | Loss: 0.00003379
Iteration 159/1000 | Loss: 0.00004559
Iteration 160/1000 | Loss: 0.00003416
Iteration 161/1000 | Loss: 0.00003451
Iteration 162/1000 | Loss: 0.00003359
Iteration 163/1000 | Loss: 0.00003634
Iteration 164/1000 | Loss: 0.00003702
Iteration 165/1000 | Loss: 0.00004124
Iteration 166/1000 | Loss: 0.00004551
Iteration 167/1000 | Loss: 0.00003483
Iteration 168/1000 | Loss: 0.00003960
Iteration 169/1000 | Loss: 0.00003596
Iteration 170/1000 | Loss: 0.00003618
Iteration 171/1000 | Loss: 0.00003520
Iteration 172/1000 | Loss: 0.00005783
Iteration 173/1000 | Loss: 0.00003564
Iteration 174/1000 | Loss: 0.00003354
Iteration 175/1000 | Loss: 0.00003219
Iteration 176/1000 | Loss: 0.00003423
Iteration 177/1000 | Loss: 0.00004552
Iteration 178/1000 | Loss: 0.00003546
Iteration 179/1000 | Loss: 0.00003230
Iteration 180/1000 | Loss: 0.00003458
Iteration 181/1000 | Loss: 0.00003559
Iteration 182/1000 | Loss: 0.00003464
Iteration 183/1000 | Loss: 0.00003308
Iteration 184/1000 | Loss: 0.00003697
Iteration 185/1000 | Loss: 0.00003349
Iteration 186/1000 | Loss: 0.00003364
Iteration 187/1000 | Loss: 0.00004597
Iteration 188/1000 | Loss: 0.00003393
Iteration 189/1000 | Loss: 0.00006215
Iteration 190/1000 | Loss: 0.00003916
Iteration 191/1000 | Loss: 0.00002929
Iteration 192/1000 | Loss: 0.00004762
Iteration 193/1000 | Loss: 0.00003627
Iteration 194/1000 | Loss: 0.00005895
Iteration 195/1000 | Loss: 0.00003844
Iteration 196/1000 | Loss: 0.00005173
Iteration 197/1000 | Loss: 0.00001930
Iteration 198/1000 | Loss: 0.00002272
Iteration 199/1000 | Loss: 0.00003346
Iteration 200/1000 | Loss: 0.00012041
Iteration 201/1000 | Loss: 0.00001792
Iteration 202/1000 | Loss: 0.00001733
Iteration 203/1000 | Loss: 0.00001296
Iteration 204/1000 | Loss: 0.00001275
Iteration 205/1000 | Loss: 0.00001254
Iteration 206/1000 | Loss: 0.00001254
Iteration 207/1000 | Loss: 0.00001253
Iteration 208/1000 | Loss: 0.00001252
Iteration 209/1000 | Loss: 0.00001252
Iteration 210/1000 | Loss: 0.00001248
Iteration 211/1000 | Loss: 0.00001248
Iteration 212/1000 | Loss: 0.00001247
Iteration 213/1000 | Loss: 0.00001246
Iteration 214/1000 | Loss: 0.00001246
Iteration 215/1000 | Loss: 0.00002222
Iteration 216/1000 | Loss: 0.00002459
Iteration 217/1000 | Loss: 0.00001486
Iteration 218/1000 | Loss: 0.00002288
Iteration 219/1000 | Loss: 0.00001372
Iteration 220/1000 | Loss: 0.00001226
Iteration 221/1000 | Loss: 0.00001226
Iteration 222/1000 | Loss: 0.00001226
Iteration 223/1000 | Loss: 0.00001226
Iteration 224/1000 | Loss: 0.00001225
Iteration 225/1000 | Loss: 0.00001225
Iteration 226/1000 | Loss: 0.00001225
Iteration 227/1000 | Loss: 0.00001225
Iteration 228/1000 | Loss: 0.00001225
Iteration 229/1000 | Loss: 0.00001225
Iteration 230/1000 | Loss: 0.00001224
Iteration 231/1000 | Loss: 0.00001224
Iteration 232/1000 | Loss: 0.00001224
Iteration 233/1000 | Loss: 0.00001224
Iteration 234/1000 | Loss: 0.00001224
Iteration 235/1000 | Loss: 0.00001223
Iteration 236/1000 | Loss: 0.00001223
Iteration 237/1000 | Loss: 0.00001223
Iteration 238/1000 | Loss: 0.00001222
Iteration 239/1000 | Loss: 0.00001392
Iteration 240/1000 | Loss: 0.00002306
Iteration 241/1000 | Loss: 0.00003875
Iteration 242/1000 | Loss: 0.00001467
Iteration 243/1000 | Loss: 0.00001218
Iteration 244/1000 | Loss: 0.00001214
Iteration 245/1000 | Loss: 0.00001214
Iteration 246/1000 | Loss: 0.00001214
Iteration 247/1000 | Loss: 0.00001214
Iteration 248/1000 | Loss: 0.00001214
Iteration 249/1000 | Loss: 0.00001214
Iteration 250/1000 | Loss: 0.00001214
Iteration 251/1000 | Loss: 0.00001214
Iteration 252/1000 | Loss: 0.00001214
Iteration 253/1000 | Loss: 0.00001214
Iteration 254/1000 | Loss: 0.00001214
Iteration 255/1000 | Loss: 0.00001214
Iteration 256/1000 | Loss: 0.00001214
Iteration 257/1000 | Loss: 0.00001213
Iteration 258/1000 | Loss: 0.00001213
Iteration 259/1000 | Loss: 0.00001213
Iteration 260/1000 | Loss: 0.00001212
Iteration 261/1000 | Loss: 0.00001295
Iteration 262/1000 | Loss: 0.00001210
Iteration 263/1000 | Loss: 0.00001210
Iteration 264/1000 | Loss: 0.00001210
Iteration 265/1000 | Loss: 0.00001210
Iteration 266/1000 | Loss: 0.00001210
Iteration 267/1000 | Loss: 0.00001210
Iteration 268/1000 | Loss: 0.00001210
Iteration 269/1000 | Loss: 0.00001210
Iteration 270/1000 | Loss: 0.00001210
Iteration 271/1000 | Loss: 0.00001210
Iteration 272/1000 | Loss: 0.00001209
Iteration 273/1000 | Loss: 0.00001208
Iteration 274/1000 | Loss: 0.00001208
Iteration 275/1000 | Loss: 0.00001208
Iteration 276/1000 | Loss: 0.00001531
Iteration 277/1000 | Loss: 0.00001597
Iteration 278/1000 | Loss: 0.00001205
Iteration 279/1000 | Loss: 0.00001203
Iteration 280/1000 | Loss: 0.00001203
Iteration 281/1000 | Loss: 0.00001203
Iteration 282/1000 | Loss: 0.00001203
Iteration 283/1000 | Loss: 0.00001203
Iteration 284/1000 | Loss: 0.00001203
Iteration 285/1000 | Loss: 0.00001203
Iteration 286/1000 | Loss: 0.00001203
Iteration 287/1000 | Loss: 0.00001203
Iteration 288/1000 | Loss: 0.00001203
Iteration 289/1000 | Loss: 0.00001202
Iteration 290/1000 | Loss: 0.00001202
Iteration 291/1000 | Loss: 0.00001202
Iteration 292/1000 | Loss: 0.00001202
Iteration 293/1000 | Loss: 0.00001202
Iteration 294/1000 | Loss: 0.00001201
Iteration 295/1000 | Loss: 0.00001201
Iteration 296/1000 | Loss: 0.00001385
Iteration 297/1000 | Loss: 0.00002548
Iteration 298/1000 | Loss: 0.00001367
Iteration 299/1000 | Loss: 0.00001212
Iteration 300/1000 | Loss: 0.00001560
Iteration 301/1000 | Loss: 0.00006680
Iteration 302/1000 | Loss: 0.00001482
Iteration 303/1000 | Loss: 0.00001198
Iteration 304/1000 | Loss: 0.00001198
Iteration 305/1000 | Loss: 0.00001197
Iteration 306/1000 | Loss: 0.00001721
Iteration 307/1000 | Loss: 0.00005507
Iteration 308/1000 | Loss: 0.00001522
Iteration 309/1000 | Loss: 0.00001198
Iteration 310/1000 | Loss: 0.00001197
Iteration 311/1000 | Loss: 0.00001189
Iteration 312/1000 | Loss: 0.00001188
Iteration 313/1000 | Loss: 0.00001188
Iteration 314/1000 | Loss: 0.00001188
Iteration 315/1000 | Loss: 0.00001188
Iteration 316/1000 | Loss: 0.00001188
Iteration 317/1000 | Loss: 0.00001188
Iteration 318/1000 | Loss: 0.00001188
Iteration 319/1000 | Loss: 0.00001188
Iteration 320/1000 | Loss: 0.00001188
Iteration 321/1000 | Loss: 0.00001188
Iteration 322/1000 | Loss: 0.00001188
Iteration 323/1000 | Loss: 0.00001188
Iteration 324/1000 | Loss: 0.00001188
Iteration 325/1000 | Loss: 0.00001188
Iteration 326/1000 | Loss: 0.00001187
Iteration 327/1000 | Loss: 0.00001187
Iteration 328/1000 | Loss: 0.00001187
Iteration 329/1000 | Loss: 0.00001187
Iteration 330/1000 | Loss: 0.00001187
Iteration 331/1000 | Loss: 0.00001187
Iteration 332/1000 | Loss: 0.00001187
Iteration 333/1000 | Loss: 0.00001187
Iteration 334/1000 | Loss: 0.00001187
Iteration 335/1000 | Loss: 0.00001187
Iteration 336/1000 | Loss: 0.00001187
Iteration 337/1000 | Loss: 0.00001187
Iteration 338/1000 | Loss: 0.00001187
Iteration 339/1000 | Loss: 0.00001187
Iteration 340/1000 | Loss: 0.00001187
Iteration 341/1000 | Loss: 0.00001187
Iteration 342/1000 | Loss: 0.00001187
Iteration 343/1000 | Loss: 0.00001187
Iteration 344/1000 | Loss: 0.00001187
Iteration 345/1000 | Loss: 0.00001187
Iteration 346/1000 | Loss: 0.00001187
Iteration 347/1000 | Loss: 0.00001187
Iteration 348/1000 | Loss: 0.00001187
Iteration 349/1000 | Loss: 0.00001187
Iteration 350/1000 | Loss: 0.00001187
Iteration 351/1000 | Loss: 0.00001187
Iteration 352/1000 | Loss: 0.00001187
Iteration 353/1000 | Loss: 0.00001187
Iteration 354/1000 | Loss: 0.00001187
Iteration 355/1000 | Loss: 0.00001187
Iteration 356/1000 | Loss: 0.00001187
Iteration 357/1000 | Loss: 0.00001187
Iteration 358/1000 | Loss: 0.00001187
Iteration 359/1000 | Loss: 0.00001187
Iteration 360/1000 | Loss: 0.00001187
Iteration 361/1000 | Loss: 0.00001187
Iteration 362/1000 | Loss: 0.00001187
Iteration 363/1000 | Loss: 0.00001187
Iteration 364/1000 | Loss: 0.00001187
Iteration 365/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [1.1873948096763343e-05, 1.1873948096763343e-05, 1.1873948096763343e-05, 1.1873948096763343e-05, 1.1873948096763343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1873948096763343e-05

Optimization complete. Final v2v error: 2.850177764892578 mm

Highest mean error: 6.885144233703613 mm for frame 77

Lowest mean error: 2.4911956787109375 mm for frame 38

Saving results

Total time: 355.6253516674042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015841
Iteration 2/25 | Loss: 0.00196129
Iteration 3/25 | Loss: 0.00201681
Iteration 4/25 | Loss: 0.00127722
Iteration 5/25 | Loss: 0.00126396
Iteration 6/25 | Loss: 0.00124799
Iteration 7/25 | Loss: 0.00124729
Iteration 8/25 | Loss: 0.00124688
Iteration 9/25 | Loss: 0.00124858
Iteration 10/25 | Loss: 0.00125363
Iteration 11/25 | Loss: 0.00124909
Iteration 12/25 | Loss: 0.00124034
Iteration 13/25 | Loss: 0.00123933
Iteration 14/25 | Loss: 0.00123910
Iteration 15/25 | Loss: 0.00123841
Iteration 16/25 | Loss: 0.00123840
Iteration 17/25 | Loss: 0.00123916
Iteration 18/25 | Loss: 0.00123840
Iteration 19/25 | Loss: 0.00123840
Iteration 20/25 | Loss: 0.00123840
Iteration 21/25 | Loss: 0.00123840
Iteration 22/25 | Loss: 0.00123840
Iteration 23/25 | Loss: 0.00123839
Iteration 24/25 | Loss: 0.00123839
Iteration 25/25 | Loss: 0.00123839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30854762
Iteration 2/25 | Loss: 0.00103664
Iteration 3/25 | Loss: 0.00101182
Iteration 4/25 | Loss: 0.00101182
Iteration 5/25 | Loss: 0.00101182
Iteration 6/25 | Loss: 0.00101182
Iteration 7/25 | Loss: 0.00101182
Iteration 8/25 | Loss: 0.00101182
Iteration 9/25 | Loss: 0.00101182
Iteration 10/25 | Loss: 0.00101182
Iteration 11/25 | Loss: 0.00101182
Iteration 12/25 | Loss: 0.00101182
Iteration 13/25 | Loss: 0.00101182
Iteration 14/25 | Loss: 0.00101182
Iteration 15/25 | Loss: 0.00101182
Iteration 16/25 | Loss: 0.00101182
Iteration 17/25 | Loss: 0.00101182
Iteration 18/25 | Loss: 0.00101182
Iteration 19/25 | Loss: 0.00101182
Iteration 20/25 | Loss: 0.00101182
Iteration 21/25 | Loss: 0.00101182
Iteration 22/25 | Loss: 0.00101182
Iteration 23/25 | Loss: 0.00101182
Iteration 24/25 | Loss: 0.00101182
Iteration 25/25 | Loss: 0.00101182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101182
Iteration 2/1000 | Loss: 0.00004667
Iteration 3/1000 | Loss: 0.00024700
Iteration 4/1000 | Loss: 0.00004583
Iteration 5/1000 | Loss: 0.00002995
Iteration 6/1000 | Loss: 0.00002474
Iteration 7/1000 | Loss: 0.00011219
Iteration 8/1000 | Loss: 0.00017227
Iteration 9/1000 | Loss: 0.00002401
Iteration 10/1000 | Loss: 0.00011794
Iteration 11/1000 | Loss: 0.00018685
Iteration 12/1000 | Loss: 0.00015586
Iteration 13/1000 | Loss: 0.00003133
Iteration 14/1000 | Loss: 0.00005378
Iteration 15/1000 | Loss: 0.00002380
Iteration 16/1000 | Loss: 0.00002058
Iteration 17/1000 | Loss: 0.00011353
Iteration 18/1000 | Loss: 0.00004482
Iteration 19/1000 | Loss: 0.00004646
Iteration 20/1000 | Loss: 0.00003232
Iteration 21/1000 | Loss: 0.00004663
Iteration 22/1000 | Loss: 0.00026344
Iteration 23/1000 | Loss: 0.00008998
Iteration 24/1000 | Loss: 0.00002597
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00008394
Iteration 27/1000 | Loss: 0.00004087
Iteration 28/1000 | Loss: 0.00004031
Iteration 29/1000 | Loss: 0.00005491
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002930
Iteration 32/1000 | Loss: 0.00007115
Iteration 33/1000 | Loss: 0.00023027
Iteration 34/1000 | Loss: 0.00002147
Iteration 35/1000 | Loss: 0.00009607
Iteration 36/1000 | Loss: 0.00002824
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00006420
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001800
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001798
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001792
Iteration 54/1000 | Loss: 0.00001789
Iteration 55/1000 | Loss: 0.00001786
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00013350
Iteration 67/1000 | Loss: 0.00002354
Iteration 68/1000 | Loss: 0.00007969
Iteration 69/1000 | Loss: 0.00002891
Iteration 70/1000 | Loss: 0.00001804
Iteration 71/1000 | Loss: 0.00037543
Iteration 72/1000 | Loss: 0.00007228
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00003976
Iteration 75/1000 | Loss: 0.00005634
Iteration 76/1000 | Loss: 0.00003510
Iteration 77/1000 | Loss: 0.00002862
Iteration 78/1000 | Loss: 0.00004060
Iteration 79/1000 | Loss: 0.00006754
Iteration 80/1000 | Loss: 0.00004009
Iteration 81/1000 | Loss: 0.00005523
Iteration 82/1000 | Loss: 0.00001790
Iteration 83/1000 | Loss: 0.00001785
Iteration 84/1000 | Loss: 0.00001781
Iteration 85/1000 | Loss: 0.00001781
Iteration 86/1000 | Loss: 0.00001781
Iteration 87/1000 | Loss: 0.00001780
Iteration 88/1000 | Loss: 0.00002693
Iteration 89/1000 | Loss: 0.00001777
Iteration 90/1000 | Loss: 0.00005818
Iteration 91/1000 | Loss: 0.00002193
Iteration 92/1000 | Loss: 0.00002938
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001768
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00003726
Iteration 113/1000 | Loss: 0.00002632
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00002733
Iteration 118/1000 | Loss: 0.00001767
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001766
Iteration 133/1000 | Loss: 0.00001766
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001764
Iteration 140/1000 | Loss: 0.00001764
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.7640091755311005e-05, 1.7640091755311005e-05, 1.7640091755311005e-05, 1.7640091755311005e-05, 1.7640091755311005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7640091755311005e-05

Optimization complete. Final v2v error: 3.5887646675109863 mm

Highest mean error: 3.956557273864746 mm for frame 24

Lowest mean error: 3.246455430984497 mm for frame 121

Saving results

Total time: 125.98878073692322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652198
Iteration 2/25 | Loss: 0.00149072
Iteration 3/25 | Loss: 0.00135929
Iteration 4/25 | Loss: 0.00135030
Iteration 5/25 | Loss: 0.00134835
Iteration 6/25 | Loss: 0.00134826
Iteration 7/25 | Loss: 0.00134826
Iteration 8/25 | Loss: 0.00134826
Iteration 9/25 | Loss: 0.00134826
Iteration 10/25 | Loss: 0.00134826
Iteration 11/25 | Loss: 0.00134826
Iteration 12/25 | Loss: 0.00134826
Iteration 13/25 | Loss: 0.00134826
Iteration 14/25 | Loss: 0.00134826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013482559006661177, 0.0013482559006661177, 0.0013482559006661177, 0.0013482559006661177, 0.0013482559006661177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013482559006661177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.96160507
Iteration 2/25 | Loss: 0.00091195
Iteration 3/25 | Loss: 0.00091181
Iteration 4/25 | Loss: 0.00091181
Iteration 5/25 | Loss: 0.00091181
Iteration 6/25 | Loss: 0.00091180
Iteration 7/25 | Loss: 0.00091180
Iteration 8/25 | Loss: 0.00091180
Iteration 9/25 | Loss: 0.00091180
Iteration 10/25 | Loss: 0.00091180
Iteration 11/25 | Loss: 0.00091180
Iteration 12/25 | Loss: 0.00091180
Iteration 13/25 | Loss: 0.00091180
Iteration 14/25 | Loss: 0.00091180
Iteration 15/25 | Loss: 0.00091180
Iteration 16/25 | Loss: 0.00091180
Iteration 17/25 | Loss: 0.00091180
Iteration 18/25 | Loss: 0.00091180
Iteration 19/25 | Loss: 0.00091180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009118039160966873, 0.0009118039160966873, 0.0009118039160966873, 0.0009118039160966873, 0.0009118039160966873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009118039160966873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091180
Iteration 2/1000 | Loss: 0.00005202
Iteration 3/1000 | Loss: 0.00003199
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00002471
Iteration 6/1000 | Loss: 0.00002363
Iteration 7/1000 | Loss: 0.00002305
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002199
Iteration 10/1000 | Loss: 0.00002150
Iteration 11/1000 | Loss: 0.00002118
Iteration 12/1000 | Loss: 0.00002092
Iteration 13/1000 | Loss: 0.00002073
Iteration 14/1000 | Loss: 0.00002054
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002033
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002028
Iteration 22/1000 | Loss: 0.00002027
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00002020
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002015
Iteration 27/1000 | Loss: 0.00002011
Iteration 28/1000 | Loss: 0.00002010
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002009
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002008
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002005
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002004
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002002
Iteration 40/1000 | Loss: 0.00002002
Iteration 41/1000 | Loss: 0.00002002
Iteration 42/1000 | Loss: 0.00002002
Iteration 43/1000 | Loss: 0.00002002
Iteration 44/1000 | Loss: 0.00002001
Iteration 45/1000 | Loss: 0.00002001
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00002001
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00002001
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002000
Iteration 52/1000 | Loss: 0.00002000
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001997
Iteration 56/1000 | Loss: 0.00001997
Iteration 57/1000 | Loss: 0.00001997
Iteration 58/1000 | Loss: 0.00001997
Iteration 59/1000 | Loss: 0.00001997
Iteration 60/1000 | Loss: 0.00001997
Iteration 61/1000 | Loss: 0.00001997
Iteration 62/1000 | Loss: 0.00001996
Iteration 63/1000 | Loss: 0.00001996
Iteration 64/1000 | Loss: 0.00001995
Iteration 65/1000 | Loss: 0.00001995
Iteration 66/1000 | Loss: 0.00001995
Iteration 67/1000 | Loss: 0.00001995
Iteration 68/1000 | Loss: 0.00001994
Iteration 69/1000 | Loss: 0.00001994
Iteration 70/1000 | Loss: 0.00001994
Iteration 71/1000 | Loss: 0.00001994
Iteration 72/1000 | Loss: 0.00001994
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00001993
Iteration 78/1000 | Loss: 0.00001992
Iteration 79/1000 | Loss: 0.00001992
Iteration 80/1000 | Loss: 0.00001992
Iteration 81/1000 | Loss: 0.00001991
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001991
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001991
Iteration 87/1000 | Loss: 0.00001991
Iteration 88/1000 | Loss: 0.00001991
Iteration 89/1000 | Loss: 0.00001991
Iteration 90/1000 | Loss: 0.00001991
Iteration 91/1000 | Loss: 0.00001990
Iteration 92/1000 | Loss: 0.00001990
Iteration 93/1000 | Loss: 0.00001990
Iteration 94/1000 | Loss: 0.00001990
Iteration 95/1000 | Loss: 0.00001989
Iteration 96/1000 | Loss: 0.00001989
Iteration 97/1000 | Loss: 0.00001989
Iteration 98/1000 | Loss: 0.00001989
Iteration 99/1000 | Loss: 0.00001989
Iteration 100/1000 | Loss: 0.00001988
Iteration 101/1000 | Loss: 0.00001988
Iteration 102/1000 | Loss: 0.00001988
Iteration 103/1000 | Loss: 0.00001988
Iteration 104/1000 | Loss: 0.00001988
Iteration 105/1000 | Loss: 0.00001988
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001987
Iteration 110/1000 | Loss: 0.00001987
Iteration 111/1000 | Loss: 0.00001987
Iteration 112/1000 | Loss: 0.00001987
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001986
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001986
Iteration 119/1000 | Loss: 0.00001986
Iteration 120/1000 | Loss: 0.00001986
Iteration 121/1000 | Loss: 0.00001986
Iteration 122/1000 | Loss: 0.00001986
Iteration 123/1000 | Loss: 0.00001986
Iteration 124/1000 | Loss: 0.00001986
Iteration 125/1000 | Loss: 0.00001986
Iteration 126/1000 | Loss: 0.00001986
Iteration 127/1000 | Loss: 0.00001986
Iteration 128/1000 | Loss: 0.00001986
Iteration 129/1000 | Loss: 0.00001986
Iteration 130/1000 | Loss: 0.00001986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.9858624000335112e-05, 1.9858624000335112e-05, 1.9858624000335112e-05, 1.9858624000335112e-05, 1.9858624000335112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9858624000335112e-05

Optimization complete. Final v2v error: 3.713554620742798 mm

Highest mean error: 4.480752944946289 mm for frame 124

Lowest mean error: 2.975454330444336 mm for frame 21

Saving results

Total time: 38.58453822135925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797090
Iteration 2/25 | Loss: 0.00169014
Iteration 3/25 | Loss: 0.00143442
Iteration 4/25 | Loss: 0.00134404
Iteration 5/25 | Loss: 0.00134044
Iteration 6/25 | Loss: 0.00133170
Iteration 7/25 | Loss: 0.00133085
Iteration 8/25 | Loss: 0.00133058
Iteration 9/25 | Loss: 0.00133538
Iteration 10/25 | Loss: 0.00133895
Iteration 11/25 | Loss: 0.00133096
Iteration 12/25 | Loss: 0.00133424
Iteration 13/25 | Loss: 0.00133048
Iteration 14/25 | Loss: 0.00133041
Iteration 15/25 | Loss: 0.00133041
Iteration 16/25 | Loss: 0.00133041
Iteration 17/25 | Loss: 0.00133041
Iteration 18/25 | Loss: 0.00133041
Iteration 19/25 | Loss: 0.00133041
Iteration 20/25 | Loss: 0.00133041
Iteration 21/25 | Loss: 0.00133041
Iteration 22/25 | Loss: 0.00133041
Iteration 23/25 | Loss: 0.00133041
Iteration 24/25 | Loss: 0.00133041
Iteration 25/25 | Loss: 0.00133041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.13213158
Iteration 2/25 | Loss: 0.00133641
Iteration 3/25 | Loss: 0.00128249
Iteration 4/25 | Loss: 0.00128249
Iteration 5/25 | Loss: 0.00128249
Iteration 6/25 | Loss: 0.00128249
Iteration 7/25 | Loss: 0.00128249
Iteration 8/25 | Loss: 0.00128249
Iteration 9/25 | Loss: 0.00128249
Iteration 10/25 | Loss: 0.00128249
Iteration 11/25 | Loss: 0.00128249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012824888108298182, 0.0012824888108298182, 0.0012824888108298182, 0.0012824888108298182, 0.0012824888108298182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012824888108298182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128249
Iteration 2/1000 | Loss: 0.00010134
Iteration 3/1000 | Loss: 0.00002727
Iteration 4/1000 | Loss: 0.00002510
Iteration 5/1000 | Loss: 0.00021605
Iteration 6/1000 | Loss: 0.00004247
Iteration 7/1000 | Loss: 0.00002839
Iteration 8/1000 | Loss: 0.00002373
Iteration 9/1000 | Loss: 0.00002324
Iteration 10/1000 | Loss: 0.00002296
Iteration 11/1000 | Loss: 0.00002275
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002230
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002209
Iteration 17/1000 | Loss: 0.00002197
Iteration 18/1000 | Loss: 0.00002190
Iteration 19/1000 | Loss: 0.00002179
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002170
Iteration 23/1000 | Loss: 0.00002170
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002164
Iteration 28/1000 | Loss: 0.00002161
Iteration 29/1000 | Loss: 0.00002159
Iteration 30/1000 | Loss: 0.00002154
Iteration 31/1000 | Loss: 0.00002154
Iteration 32/1000 | Loss: 0.00002152
Iteration 33/1000 | Loss: 0.00002152
Iteration 34/1000 | Loss: 0.00002151
Iteration 35/1000 | Loss: 0.00002151
Iteration 36/1000 | Loss: 0.00002149
Iteration 37/1000 | Loss: 0.00002149
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002148
Iteration 40/1000 | Loss: 0.00002147
Iteration 41/1000 | Loss: 0.00002147
Iteration 42/1000 | Loss: 0.00002146
Iteration 43/1000 | Loss: 0.00002146
Iteration 44/1000 | Loss: 0.00002145
Iteration 45/1000 | Loss: 0.00002143
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002140
Iteration 48/1000 | Loss: 0.00002139
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002135
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002133
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002133
Iteration 66/1000 | Loss: 0.00002132
Iteration 67/1000 | Loss: 0.00002132
Iteration 68/1000 | Loss: 0.00002132
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002132
Iteration 71/1000 | Loss: 0.00002132
Iteration 72/1000 | Loss: 0.00002132
Iteration 73/1000 | Loss: 0.00002131
Iteration 74/1000 | Loss: 0.00002131
Iteration 75/1000 | Loss: 0.00002131
Iteration 76/1000 | Loss: 0.00002131
Iteration 77/1000 | Loss: 0.00002131
Iteration 78/1000 | Loss: 0.00002131
Iteration 79/1000 | Loss: 0.00002130
Iteration 80/1000 | Loss: 0.00002130
Iteration 81/1000 | Loss: 0.00002130
Iteration 82/1000 | Loss: 0.00002130
Iteration 83/1000 | Loss: 0.00002130
Iteration 84/1000 | Loss: 0.00002130
Iteration 85/1000 | Loss: 0.00002129
Iteration 86/1000 | Loss: 0.00002129
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002129
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002128
Iteration 91/1000 | Loss: 0.00002128
Iteration 92/1000 | Loss: 0.00002128
Iteration 93/1000 | Loss: 0.00002128
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002128
Iteration 96/1000 | Loss: 0.00002127
Iteration 97/1000 | Loss: 0.00002127
Iteration 98/1000 | Loss: 0.00002127
Iteration 99/1000 | Loss: 0.00002127
Iteration 100/1000 | Loss: 0.00002127
Iteration 101/1000 | Loss: 0.00002127
Iteration 102/1000 | Loss: 0.00002126
Iteration 103/1000 | Loss: 0.00002126
Iteration 104/1000 | Loss: 0.00002126
Iteration 105/1000 | Loss: 0.00002126
Iteration 106/1000 | Loss: 0.00002126
Iteration 107/1000 | Loss: 0.00002126
Iteration 108/1000 | Loss: 0.00002126
Iteration 109/1000 | Loss: 0.00002125
Iteration 110/1000 | Loss: 0.00002125
Iteration 111/1000 | Loss: 0.00002125
Iteration 112/1000 | Loss: 0.00002125
Iteration 113/1000 | Loss: 0.00002125
Iteration 114/1000 | Loss: 0.00002125
Iteration 115/1000 | Loss: 0.00002125
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002124
Iteration 127/1000 | Loss: 0.00002124
Iteration 128/1000 | Loss: 0.00002124
Iteration 129/1000 | Loss: 0.00002124
Iteration 130/1000 | Loss: 0.00002124
Iteration 131/1000 | Loss: 0.00002124
Iteration 132/1000 | Loss: 0.00002123
Iteration 133/1000 | Loss: 0.00002123
Iteration 134/1000 | Loss: 0.00002123
Iteration 135/1000 | Loss: 0.00002123
Iteration 136/1000 | Loss: 0.00002123
Iteration 137/1000 | Loss: 0.00002123
Iteration 138/1000 | Loss: 0.00002123
Iteration 139/1000 | Loss: 0.00002123
Iteration 140/1000 | Loss: 0.00002123
Iteration 141/1000 | Loss: 0.00002123
Iteration 142/1000 | Loss: 0.00002123
Iteration 143/1000 | Loss: 0.00002123
Iteration 144/1000 | Loss: 0.00002123
Iteration 145/1000 | Loss: 0.00002123
Iteration 146/1000 | Loss: 0.00002123
Iteration 147/1000 | Loss: 0.00002123
Iteration 148/1000 | Loss: 0.00002123
Iteration 149/1000 | Loss: 0.00002123
Iteration 150/1000 | Loss: 0.00002123
Iteration 151/1000 | Loss: 0.00002123
Iteration 152/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.1229561752988957e-05, 2.1229561752988957e-05, 2.1229561752988957e-05, 2.1229561752988957e-05, 2.1229561752988957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1229561752988957e-05

Optimization complete. Final v2v error: 3.8717472553253174 mm

Highest mean error: 4.347115993499756 mm for frame 220

Lowest mean error: 3.279331922531128 mm for frame 177

Saving results

Total time: 65.48463320732117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846163
Iteration 2/25 | Loss: 0.00152054
Iteration 3/25 | Loss: 0.00136042
Iteration 4/25 | Loss: 0.00133956
Iteration 5/25 | Loss: 0.00133496
Iteration 6/25 | Loss: 0.00133436
Iteration 7/25 | Loss: 0.00133436
Iteration 8/25 | Loss: 0.00133436
Iteration 9/25 | Loss: 0.00133436
Iteration 10/25 | Loss: 0.00133436
Iteration 11/25 | Loss: 0.00133436
Iteration 12/25 | Loss: 0.00133436
Iteration 13/25 | Loss: 0.00133436
Iteration 14/25 | Loss: 0.00133436
Iteration 15/25 | Loss: 0.00133436
Iteration 16/25 | Loss: 0.00133436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013343588216230273, 0.0013343588216230273, 0.0013343588216230273, 0.0013343588216230273, 0.0013343588216230273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013343588216230273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30961990
Iteration 2/25 | Loss: 0.00100040
Iteration 3/25 | Loss: 0.00100040
Iteration 4/25 | Loss: 0.00100040
Iteration 5/25 | Loss: 0.00100040
Iteration 6/25 | Loss: 0.00100040
Iteration 7/25 | Loss: 0.00100040
Iteration 8/25 | Loss: 0.00100040
Iteration 9/25 | Loss: 0.00100040
Iteration 10/25 | Loss: 0.00100040
Iteration 11/25 | Loss: 0.00100040
Iteration 12/25 | Loss: 0.00100040
Iteration 13/25 | Loss: 0.00100040
Iteration 14/25 | Loss: 0.00100040
Iteration 15/25 | Loss: 0.00100040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001000395161099732, 0.001000395161099732, 0.001000395161099732, 0.001000395161099732, 0.001000395161099732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001000395161099732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100040
Iteration 2/1000 | Loss: 0.00005906
Iteration 3/1000 | Loss: 0.00004875
Iteration 4/1000 | Loss: 0.00004198
Iteration 5/1000 | Loss: 0.00004005
Iteration 6/1000 | Loss: 0.00003823
Iteration 7/1000 | Loss: 0.00003713
Iteration 8/1000 | Loss: 0.00003644
Iteration 9/1000 | Loss: 0.00003589
Iteration 10/1000 | Loss: 0.00003548
Iteration 11/1000 | Loss: 0.00003516
Iteration 12/1000 | Loss: 0.00003486
Iteration 13/1000 | Loss: 0.00003460
Iteration 14/1000 | Loss: 0.00003443
Iteration 15/1000 | Loss: 0.00003438
Iteration 16/1000 | Loss: 0.00003438
Iteration 17/1000 | Loss: 0.00003436
Iteration 18/1000 | Loss: 0.00003429
Iteration 19/1000 | Loss: 0.00003424
Iteration 20/1000 | Loss: 0.00003423
Iteration 21/1000 | Loss: 0.00003422
Iteration 22/1000 | Loss: 0.00003421
Iteration 23/1000 | Loss: 0.00003415
Iteration 24/1000 | Loss: 0.00003413
Iteration 25/1000 | Loss: 0.00003408
Iteration 26/1000 | Loss: 0.00003407
Iteration 27/1000 | Loss: 0.00003407
Iteration 28/1000 | Loss: 0.00003403
Iteration 29/1000 | Loss: 0.00003403
Iteration 30/1000 | Loss: 0.00003403
Iteration 31/1000 | Loss: 0.00003403
Iteration 32/1000 | Loss: 0.00003402
Iteration 33/1000 | Loss: 0.00003402
Iteration 34/1000 | Loss: 0.00003402
Iteration 35/1000 | Loss: 0.00003401
Iteration 36/1000 | Loss: 0.00003401
Iteration 37/1000 | Loss: 0.00003399
Iteration 38/1000 | Loss: 0.00003399
Iteration 39/1000 | Loss: 0.00003399
Iteration 40/1000 | Loss: 0.00003399
Iteration 41/1000 | Loss: 0.00003398
Iteration 42/1000 | Loss: 0.00003398
Iteration 43/1000 | Loss: 0.00003398
Iteration 44/1000 | Loss: 0.00003397
Iteration 45/1000 | Loss: 0.00003397
Iteration 46/1000 | Loss: 0.00003397
Iteration 47/1000 | Loss: 0.00003396
Iteration 48/1000 | Loss: 0.00003395
Iteration 49/1000 | Loss: 0.00003395
Iteration 50/1000 | Loss: 0.00003395
Iteration 51/1000 | Loss: 0.00003395
Iteration 52/1000 | Loss: 0.00003395
Iteration 53/1000 | Loss: 0.00003395
Iteration 54/1000 | Loss: 0.00003395
Iteration 55/1000 | Loss: 0.00003395
Iteration 56/1000 | Loss: 0.00003394
Iteration 57/1000 | Loss: 0.00003394
Iteration 58/1000 | Loss: 0.00003394
Iteration 59/1000 | Loss: 0.00003394
Iteration 60/1000 | Loss: 0.00003394
Iteration 61/1000 | Loss: 0.00003394
Iteration 62/1000 | Loss: 0.00003394
Iteration 63/1000 | Loss: 0.00003394
Iteration 64/1000 | Loss: 0.00003393
Iteration 65/1000 | Loss: 0.00003393
Iteration 66/1000 | Loss: 0.00003393
Iteration 67/1000 | Loss: 0.00003393
Iteration 68/1000 | Loss: 0.00003393
Iteration 69/1000 | Loss: 0.00003393
Iteration 70/1000 | Loss: 0.00003393
Iteration 71/1000 | Loss: 0.00003393
Iteration 72/1000 | Loss: 0.00003393
Iteration 73/1000 | Loss: 0.00003393
Iteration 74/1000 | Loss: 0.00003392
Iteration 75/1000 | Loss: 0.00003392
Iteration 76/1000 | Loss: 0.00003392
Iteration 77/1000 | Loss: 0.00003392
Iteration 78/1000 | Loss: 0.00003392
Iteration 79/1000 | Loss: 0.00003392
Iteration 80/1000 | Loss: 0.00003392
Iteration 81/1000 | Loss: 0.00003392
Iteration 82/1000 | Loss: 0.00003391
Iteration 83/1000 | Loss: 0.00003391
Iteration 84/1000 | Loss: 0.00003391
Iteration 85/1000 | Loss: 0.00003391
Iteration 86/1000 | Loss: 0.00003391
Iteration 87/1000 | Loss: 0.00003391
Iteration 88/1000 | Loss: 0.00003391
Iteration 89/1000 | Loss: 0.00003390
Iteration 90/1000 | Loss: 0.00003390
Iteration 91/1000 | Loss: 0.00003390
Iteration 92/1000 | Loss: 0.00003390
Iteration 93/1000 | Loss: 0.00003389
Iteration 94/1000 | Loss: 0.00003389
Iteration 95/1000 | Loss: 0.00003389
Iteration 96/1000 | Loss: 0.00003388
Iteration 97/1000 | Loss: 0.00003388
Iteration 98/1000 | Loss: 0.00003388
Iteration 99/1000 | Loss: 0.00003388
Iteration 100/1000 | Loss: 0.00003388
Iteration 101/1000 | Loss: 0.00003388
Iteration 102/1000 | Loss: 0.00003387
Iteration 103/1000 | Loss: 0.00003387
Iteration 104/1000 | Loss: 0.00003387
Iteration 105/1000 | Loss: 0.00003387
Iteration 106/1000 | Loss: 0.00003387
Iteration 107/1000 | Loss: 0.00003386
Iteration 108/1000 | Loss: 0.00003386
Iteration 109/1000 | Loss: 0.00003386
Iteration 110/1000 | Loss: 0.00003386
Iteration 111/1000 | Loss: 0.00003385
Iteration 112/1000 | Loss: 0.00003385
Iteration 113/1000 | Loss: 0.00003385
Iteration 114/1000 | Loss: 0.00003385
Iteration 115/1000 | Loss: 0.00003385
Iteration 116/1000 | Loss: 0.00003385
Iteration 117/1000 | Loss: 0.00003385
Iteration 118/1000 | Loss: 0.00003385
Iteration 119/1000 | Loss: 0.00003384
Iteration 120/1000 | Loss: 0.00003384
Iteration 121/1000 | Loss: 0.00003384
Iteration 122/1000 | Loss: 0.00003383
Iteration 123/1000 | Loss: 0.00003383
Iteration 124/1000 | Loss: 0.00003383
Iteration 125/1000 | Loss: 0.00003383
Iteration 126/1000 | Loss: 0.00003383
Iteration 127/1000 | Loss: 0.00003382
Iteration 128/1000 | Loss: 0.00003382
Iteration 129/1000 | Loss: 0.00003382
Iteration 130/1000 | Loss: 0.00003382
Iteration 131/1000 | Loss: 0.00003382
Iteration 132/1000 | Loss: 0.00003382
Iteration 133/1000 | Loss: 0.00003381
Iteration 134/1000 | Loss: 0.00003381
Iteration 135/1000 | Loss: 0.00003381
Iteration 136/1000 | Loss: 0.00003381
Iteration 137/1000 | Loss: 0.00003380
Iteration 138/1000 | Loss: 0.00003380
Iteration 139/1000 | Loss: 0.00003380
Iteration 140/1000 | Loss: 0.00003380
Iteration 141/1000 | Loss: 0.00003380
Iteration 142/1000 | Loss: 0.00003380
Iteration 143/1000 | Loss: 0.00003380
Iteration 144/1000 | Loss: 0.00003380
Iteration 145/1000 | Loss: 0.00003380
Iteration 146/1000 | Loss: 0.00003380
Iteration 147/1000 | Loss: 0.00003380
Iteration 148/1000 | Loss: 0.00003380
Iteration 149/1000 | Loss: 0.00003379
Iteration 150/1000 | Loss: 0.00003379
Iteration 151/1000 | Loss: 0.00003379
Iteration 152/1000 | Loss: 0.00003378
Iteration 153/1000 | Loss: 0.00003378
Iteration 154/1000 | Loss: 0.00003378
Iteration 155/1000 | Loss: 0.00003378
Iteration 156/1000 | Loss: 0.00003378
Iteration 157/1000 | Loss: 0.00003378
Iteration 158/1000 | Loss: 0.00003378
Iteration 159/1000 | Loss: 0.00003378
Iteration 160/1000 | Loss: 0.00003378
Iteration 161/1000 | Loss: 0.00003378
Iteration 162/1000 | Loss: 0.00003378
Iteration 163/1000 | Loss: 0.00003378
Iteration 164/1000 | Loss: 0.00003378
Iteration 165/1000 | Loss: 0.00003378
Iteration 166/1000 | Loss: 0.00003378
Iteration 167/1000 | Loss: 0.00003378
Iteration 168/1000 | Loss: 0.00003378
Iteration 169/1000 | Loss: 0.00003378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.3777381759136915e-05, 3.3777381759136915e-05, 3.3777381759136915e-05, 3.3777381759136915e-05, 3.3777381759136915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3777381759136915e-05

Optimization complete. Final v2v error: 4.817739009857178 mm

Highest mean error: 5.383227348327637 mm for frame 22

Lowest mean error: 4.270137310028076 mm for frame 87

Saving results

Total time: 39.92662453651428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392108
Iteration 2/25 | Loss: 0.00131201
Iteration 3/25 | Loss: 0.00123368
Iteration 4/25 | Loss: 0.00122833
Iteration 5/25 | Loss: 0.00122643
Iteration 6/25 | Loss: 0.00122643
Iteration 7/25 | Loss: 0.00122643
Iteration 8/25 | Loss: 0.00122643
Iteration 9/25 | Loss: 0.00122643
Iteration 10/25 | Loss: 0.00122643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012264319229871035, 0.0012264319229871035, 0.0012264319229871035, 0.0012264319229871035, 0.0012264319229871035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012264319229871035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63862252
Iteration 2/25 | Loss: 0.00092181
Iteration 3/25 | Loss: 0.00092180
Iteration 4/25 | Loss: 0.00092180
Iteration 5/25 | Loss: 0.00092180
Iteration 6/25 | Loss: 0.00092180
Iteration 7/25 | Loss: 0.00092179
Iteration 8/25 | Loss: 0.00092179
Iteration 9/25 | Loss: 0.00092179
Iteration 10/25 | Loss: 0.00092179
Iteration 11/25 | Loss: 0.00092179
Iteration 12/25 | Loss: 0.00092179
Iteration 13/25 | Loss: 0.00092179
Iteration 14/25 | Loss: 0.00092179
Iteration 15/25 | Loss: 0.00092179
Iteration 16/25 | Loss: 0.00092179
Iteration 17/25 | Loss: 0.00092179
Iteration 18/25 | Loss: 0.00092179
Iteration 19/25 | Loss: 0.00092179
Iteration 20/25 | Loss: 0.00092179
Iteration 21/25 | Loss: 0.00092179
Iteration 22/25 | Loss: 0.00092179
Iteration 23/25 | Loss: 0.00092179
Iteration 24/25 | Loss: 0.00092179
Iteration 25/25 | Loss: 0.00092179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092179
Iteration 2/1000 | Loss: 0.00002773
Iteration 3/1000 | Loss: 0.00001784
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001211
Iteration 10/1000 | Loss: 0.00001208
Iteration 11/1000 | Loss: 0.00001162
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001126
Iteration 15/1000 | Loss: 0.00001125
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001110
Iteration 19/1000 | Loss: 0.00001110
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001107
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001100
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001097
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001096
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001092
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001089
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001081
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001081
Iteration 72/1000 | Loss: 0.00001081
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001080
Iteration 76/1000 | Loss: 0.00001080
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001073
Iteration 95/1000 | Loss: 0.00001073
Iteration 96/1000 | Loss: 0.00001073
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001072
Iteration 99/1000 | Loss: 0.00001072
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001072
Iteration 104/1000 | Loss: 0.00001072
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001070
Iteration 113/1000 | Loss: 0.00001070
Iteration 114/1000 | Loss: 0.00001070
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001069
Iteration 117/1000 | Loss: 0.00001069
Iteration 118/1000 | Loss: 0.00001068
Iteration 119/1000 | Loss: 0.00001068
Iteration 120/1000 | Loss: 0.00001068
Iteration 121/1000 | Loss: 0.00001068
Iteration 122/1000 | Loss: 0.00001068
Iteration 123/1000 | Loss: 0.00001068
Iteration 124/1000 | Loss: 0.00001068
Iteration 125/1000 | Loss: 0.00001067
Iteration 126/1000 | Loss: 0.00001067
Iteration 127/1000 | Loss: 0.00001067
Iteration 128/1000 | Loss: 0.00001067
Iteration 129/1000 | Loss: 0.00001067
Iteration 130/1000 | Loss: 0.00001067
Iteration 131/1000 | Loss: 0.00001066
Iteration 132/1000 | Loss: 0.00001066
Iteration 133/1000 | Loss: 0.00001066
Iteration 134/1000 | Loss: 0.00001066
Iteration 135/1000 | Loss: 0.00001065
Iteration 136/1000 | Loss: 0.00001065
Iteration 137/1000 | Loss: 0.00001065
Iteration 138/1000 | Loss: 0.00001065
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001064
Iteration 141/1000 | Loss: 0.00001064
Iteration 142/1000 | Loss: 0.00001064
Iteration 143/1000 | Loss: 0.00001064
Iteration 144/1000 | Loss: 0.00001064
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001062
Iteration 154/1000 | Loss: 0.00001062
Iteration 155/1000 | Loss: 0.00001062
Iteration 156/1000 | Loss: 0.00001061
Iteration 157/1000 | Loss: 0.00001061
Iteration 158/1000 | Loss: 0.00001061
Iteration 159/1000 | Loss: 0.00001060
Iteration 160/1000 | Loss: 0.00001060
Iteration 161/1000 | Loss: 0.00001060
Iteration 162/1000 | Loss: 0.00001060
Iteration 163/1000 | Loss: 0.00001059
Iteration 164/1000 | Loss: 0.00001059
Iteration 165/1000 | Loss: 0.00001059
Iteration 166/1000 | Loss: 0.00001058
Iteration 167/1000 | Loss: 0.00001058
Iteration 168/1000 | Loss: 0.00001058
Iteration 169/1000 | Loss: 0.00001058
Iteration 170/1000 | Loss: 0.00001058
Iteration 171/1000 | Loss: 0.00001058
Iteration 172/1000 | Loss: 0.00001058
Iteration 173/1000 | Loss: 0.00001058
Iteration 174/1000 | Loss: 0.00001057
Iteration 175/1000 | Loss: 0.00001057
Iteration 176/1000 | Loss: 0.00001057
Iteration 177/1000 | Loss: 0.00001056
Iteration 178/1000 | Loss: 0.00001056
Iteration 179/1000 | Loss: 0.00001056
Iteration 180/1000 | Loss: 0.00001056
Iteration 181/1000 | Loss: 0.00001056
Iteration 182/1000 | Loss: 0.00001056
Iteration 183/1000 | Loss: 0.00001055
Iteration 184/1000 | Loss: 0.00001055
Iteration 185/1000 | Loss: 0.00001055
Iteration 186/1000 | Loss: 0.00001055
Iteration 187/1000 | Loss: 0.00001055
Iteration 188/1000 | Loss: 0.00001055
Iteration 189/1000 | Loss: 0.00001055
Iteration 190/1000 | Loss: 0.00001054
Iteration 191/1000 | Loss: 0.00001054
Iteration 192/1000 | Loss: 0.00001054
Iteration 193/1000 | Loss: 0.00001054
Iteration 194/1000 | Loss: 0.00001054
Iteration 195/1000 | Loss: 0.00001054
Iteration 196/1000 | Loss: 0.00001054
Iteration 197/1000 | Loss: 0.00001054
Iteration 198/1000 | Loss: 0.00001054
Iteration 199/1000 | Loss: 0.00001054
Iteration 200/1000 | Loss: 0.00001054
Iteration 201/1000 | Loss: 0.00001054
Iteration 202/1000 | Loss: 0.00001054
Iteration 203/1000 | Loss: 0.00001054
Iteration 204/1000 | Loss: 0.00001054
Iteration 205/1000 | Loss: 0.00001054
Iteration 206/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.0538121387071442e-05, 1.0538121387071442e-05, 1.0538121387071442e-05, 1.0538121387071442e-05, 1.0538121387071442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0538121387071442e-05

Optimization complete. Final v2v error: 2.794583320617676 mm

Highest mean error: 2.9411468505859375 mm for frame 106

Lowest mean error: 2.6805546283721924 mm for frame 199

Saving results

Total time: 46.29441261291504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_belle_posed_007/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_belle_posed_007/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816856
Iteration 2/25 | Loss: 0.00154882
Iteration 3/25 | Loss: 0.00132826
Iteration 4/25 | Loss: 0.00131163
Iteration 5/25 | Loss: 0.00131123
Iteration 6/25 | Loss: 0.00130782
Iteration 7/25 | Loss: 0.00130622
Iteration 8/25 | Loss: 0.00130435
Iteration 9/25 | Loss: 0.00130684
Iteration 10/25 | Loss: 0.00130439
Iteration 11/25 | Loss: 0.00130235
Iteration 12/25 | Loss: 0.00130815
Iteration 13/25 | Loss: 0.00130173
Iteration 14/25 | Loss: 0.00130016
Iteration 15/25 | Loss: 0.00129985
Iteration 16/25 | Loss: 0.00130283
Iteration 17/25 | Loss: 0.00130320
Iteration 18/25 | Loss: 0.00130407
Iteration 19/25 | Loss: 0.00129963
Iteration 20/25 | Loss: 0.00129920
Iteration 21/25 | Loss: 0.00129881
Iteration 22/25 | Loss: 0.00129862
Iteration 23/25 | Loss: 0.00129763
Iteration 24/25 | Loss: 0.00129915
Iteration 25/25 | Loss: 0.00129765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29071927
Iteration 2/25 | Loss: 0.00102446
Iteration 3/25 | Loss: 0.00102445
Iteration 4/25 | Loss: 0.00102445
Iteration 5/25 | Loss: 0.00102445
Iteration 6/25 | Loss: 0.00102445
Iteration 7/25 | Loss: 0.00102445
Iteration 8/25 | Loss: 0.00102445
Iteration 9/25 | Loss: 0.00102445
Iteration 10/25 | Loss: 0.00102445
Iteration 11/25 | Loss: 0.00102445
Iteration 12/25 | Loss: 0.00102445
Iteration 13/25 | Loss: 0.00102445
Iteration 14/25 | Loss: 0.00102445
Iteration 15/25 | Loss: 0.00102445
Iteration 16/25 | Loss: 0.00102445
Iteration 17/25 | Loss: 0.00102445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010244458680972457, 0.0010244458680972457, 0.0010244458680972457, 0.0010244458680972457, 0.0010244458680972457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010244458680972457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102445
Iteration 2/1000 | Loss: 0.00008396
Iteration 3/1000 | Loss: 0.00014433
Iteration 4/1000 | Loss: 0.00025771
Iteration 5/1000 | Loss: 0.00029816
Iteration 6/1000 | Loss: 0.00004943
Iteration 7/1000 | Loss: 0.00004007
Iteration 8/1000 | Loss: 0.00003233
Iteration 9/1000 | Loss: 0.00002672
Iteration 10/1000 | Loss: 0.00002323
Iteration 11/1000 | Loss: 0.00002064
Iteration 12/1000 | Loss: 0.00001936
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001739
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001660
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001643
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001620
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001615
Iteration 84/1000 | Loss: 0.00001615
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001610
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001607
Iteration 111/1000 | Loss: 0.00001607
Iteration 112/1000 | Loss: 0.00001607
Iteration 113/1000 | Loss: 0.00001607
Iteration 114/1000 | Loss: 0.00001606
Iteration 115/1000 | Loss: 0.00001606
Iteration 116/1000 | Loss: 0.00001606
Iteration 117/1000 | Loss: 0.00001606
Iteration 118/1000 | Loss: 0.00001606
Iteration 119/1000 | Loss: 0.00001606
Iteration 120/1000 | Loss: 0.00001606
Iteration 121/1000 | Loss: 0.00001605
Iteration 122/1000 | Loss: 0.00001605
Iteration 123/1000 | Loss: 0.00001604
Iteration 124/1000 | Loss: 0.00001604
Iteration 125/1000 | Loss: 0.00001604
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001601
Iteration 140/1000 | Loss: 0.00001601
Iteration 141/1000 | Loss: 0.00001601
Iteration 142/1000 | Loss: 0.00001601
Iteration 143/1000 | Loss: 0.00001601
Iteration 144/1000 | Loss: 0.00001600
Iteration 145/1000 | Loss: 0.00001600
Iteration 146/1000 | Loss: 0.00001600
Iteration 147/1000 | Loss: 0.00001600
Iteration 148/1000 | Loss: 0.00001600
Iteration 149/1000 | Loss: 0.00001600
Iteration 150/1000 | Loss: 0.00001600
Iteration 151/1000 | Loss: 0.00001600
Iteration 152/1000 | Loss: 0.00001600
Iteration 153/1000 | Loss: 0.00001600
Iteration 154/1000 | Loss: 0.00001600
Iteration 155/1000 | Loss: 0.00001600
Iteration 156/1000 | Loss: 0.00001600
Iteration 157/1000 | Loss: 0.00001600
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001600
Iteration 160/1000 | Loss: 0.00001600
Iteration 161/1000 | Loss: 0.00001599
Iteration 162/1000 | Loss: 0.00001599
Iteration 163/1000 | Loss: 0.00001599
Iteration 164/1000 | Loss: 0.00001599
Iteration 165/1000 | Loss: 0.00001599
Iteration 166/1000 | Loss: 0.00001598
Iteration 167/1000 | Loss: 0.00001598
Iteration 168/1000 | Loss: 0.00001598
Iteration 169/1000 | Loss: 0.00001598
Iteration 170/1000 | Loss: 0.00001598
Iteration 171/1000 | Loss: 0.00001598
Iteration 172/1000 | Loss: 0.00001598
Iteration 173/1000 | Loss: 0.00001597
Iteration 174/1000 | Loss: 0.00001597
Iteration 175/1000 | Loss: 0.00001597
Iteration 176/1000 | Loss: 0.00001597
Iteration 177/1000 | Loss: 0.00001597
Iteration 178/1000 | Loss: 0.00001597
Iteration 179/1000 | Loss: 0.00001597
Iteration 180/1000 | Loss: 0.00001597
Iteration 181/1000 | Loss: 0.00001597
Iteration 182/1000 | Loss: 0.00001596
Iteration 183/1000 | Loss: 0.00001596
Iteration 184/1000 | Loss: 0.00001596
Iteration 185/1000 | Loss: 0.00001595
Iteration 186/1000 | Loss: 0.00001595
Iteration 187/1000 | Loss: 0.00001595
Iteration 188/1000 | Loss: 0.00001595
Iteration 189/1000 | Loss: 0.00001595
Iteration 190/1000 | Loss: 0.00001595
Iteration 191/1000 | Loss: 0.00001595
Iteration 192/1000 | Loss: 0.00001595
Iteration 193/1000 | Loss: 0.00001595
Iteration 194/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.595340836502146e-05, 1.595340836502146e-05, 1.595340836502146e-05, 1.595340836502146e-05, 1.595340836502146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.595340836502146e-05

Optimization complete. Final v2v error: 3.3851046562194824 mm

Highest mean error: 4.100430965423584 mm for frame 153

Lowest mean error: 2.950331211090088 mm for frame 115

Saving results

Total time: 98.63941860198975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598925
Iteration 2/25 | Loss: 0.00123541
Iteration 3/25 | Loss: 0.00099010
Iteration 4/25 | Loss: 0.00096561
Iteration 5/25 | Loss: 0.00095888
Iteration 6/25 | Loss: 0.00095681
Iteration 7/25 | Loss: 0.00095673
Iteration 8/25 | Loss: 0.00095673
Iteration 9/25 | Loss: 0.00095673
Iteration 10/25 | Loss: 0.00095673
Iteration 11/25 | Loss: 0.00095673
Iteration 12/25 | Loss: 0.00095673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009567312081344426, 0.0009567312081344426, 0.0009567312081344426, 0.0009567312081344426, 0.0009567312081344426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009567312081344426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69936645
Iteration 2/25 | Loss: 0.00073561
Iteration 3/25 | Loss: 0.00073560
Iteration 4/25 | Loss: 0.00073560
Iteration 5/25 | Loss: 0.00073560
Iteration 6/25 | Loss: 0.00073560
Iteration 7/25 | Loss: 0.00073560
Iteration 8/25 | Loss: 0.00073560
Iteration 9/25 | Loss: 0.00073560
Iteration 10/25 | Loss: 0.00073560
Iteration 11/25 | Loss: 0.00073560
Iteration 12/25 | Loss: 0.00073560
Iteration 13/25 | Loss: 0.00073560
Iteration 14/25 | Loss: 0.00073560
Iteration 15/25 | Loss: 0.00073560
Iteration 16/25 | Loss: 0.00073560
Iteration 17/25 | Loss: 0.00073560
Iteration 18/25 | Loss: 0.00073560
Iteration 19/25 | Loss: 0.00073560
Iteration 20/25 | Loss: 0.00073560
Iteration 21/25 | Loss: 0.00073560
Iteration 22/25 | Loss: 0.00073560
Iteration 23/25 | Loss: 0.00073560
Iteration 24/25 | Loss: 0.00073560
Iteration 25/25 | Loss: 0.00073560

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073560
Iteration 2/1000 | Loss: 0.00004665
Iteration 3/1000 | Loss: 0.00003485
Iteration 4/1000 | Loss: 0.00003015
Iteration 5/1000 | Loss: 0.00002832
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00002688
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00002608
Iteration 11/1000 | Loss: 0.00002605
Iteration 12/1000 | Loss: 0.00002598
Iteration 13/1000 | Loss: 0.00002598
Iteration 14/1000 | Loss: 0.00002597
Iteration 15/1000 | Loss: 0.00002593
Iteration 16/1000 | Loss: 0.00002592
Iteration 17/1000 | Loss: 0.00002591
Iteration 18/1000 | Loss: 0.00002590
Iteration 19/1000 | Loss: 0.00002589
Iteration 20/1000 | Loss: 0.00002585
Iteration 21/1000 | Loss: 0.00002585
Iteration 22/1000 | Loss: 0.00002582
Iteration 23/1000 | Loss: 0.00002582
Iteration 24/1000 | Loss: 0.00002582
Iteration 25/1000 | Loss: 0.00002581
Iteration 26/1000 | Loss: 0.00002581
Iteration 27/1000 | Loss: 0.00002579
Iteration 28/1000 | Loss: 0.00002579
Iteration 29/1000 | Loss: 0.00002579
Iteration 30/1000 | Loss: 0.00002579
Iteration 31/1000 | Loss: 0.00002576
Iteration 32/1000 | Loss: 0.00002576
Iteration 33/1000 | Loss: 0.00002576
Iteration 34/1000 | Loss: 0.00002576
Iteration 35/1000 | Loss: 0.00002575
Iteration 36/1000 | Loss: 0.00002574
Iteration 37/1000 | Loss: 0.00002572
Iteration 38/1000 | Loss: 0.00002572
Iteration 39/1000 | Loss: 0.00002572
Iteration 40/1000 | Loss: 0.00002571
Iteration 41/1000 | Loss: 0.00002571
Iteration 42/1000 | Loss: 0.00002570
Iteration 43/1000 | Loss: 0.00002570
Iteration 44/1000 | Loss: 0.00002569
Iteration 45/1000 | Loss: 0.00002569
Iteration 46/1000 | Loss: 0.00002569
Iteration 47/1000 | Loss: 0.00002567
Iteration 48/1000 | Loss: 0.00002567
Iteration 49/1000 | Loss: 0.00002566
Iteration 50/1000 | Loss: 0.00002566
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002565
Iteration 53/1000 | Loss: 0.00002564
Iteration 54/1000 | Loss: 0.00002563
Iteration 55/1000 | Loss: 0.00002563
Iteration 56/1000 | Loss: 0.00002562
Iteration 57/1000 | Loss: 0.00002562
Iteration 58/1000 | Loss: 0.00002562
Iteration 59/1000 | Loss: 0.00002562
Iteration 60/1000 | Loss: 0.00002562
Iteration 61/1000 | Loss: 0.00002562
Iteration 62/1000 | Loss: 0.00002562
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002562
Iteration 65/1000 | Loss: 0.00002562
Iteration 66/1000 | Loss: 0.00002562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.562491863500327e-05, 2.562491863500327e-05, 2.562491863500327e-05, 2.562491863500327e-05, 2.562491863500327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.562491863500327e-05

Optimization complete. Final v2v error: 4.426918029785156 mm

Highest mean error: 5.345329284667969 mm for frame 70

Lowest mean error: 3.8772976398468018 mm for frame 171

Saving results

Total time: 35.17995762825012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508260
Iteration 2/25 | Loss: 0.00122216
Iteration 3/25 | Loss: 0.00092719
Iteration 4/25 | Loss: 0.00088448
Iteration 5/25 | Loss: 0.00087675
Iteration 6/25 | Loss: 0.00087395
Iteration 7/25 | Loss: 0.00087293
Iteration 8/25 | Loss: 0.00087266
Iteration 9/25 | Loss: 0.00087266
Iteration 10/25 | Loss: 0.00087266
Iteration 11/25 | Loss: 0.00087266
Iteration 12/25 | Loss: 0.00087266
Iteration 13/25 | Loss: 0.00087266
Iteration 14/25 | Loss: 0.00087266
Iteration 15/25 | Loss: 0.00087266
Iteration 16/25 | Loss: 0.00087266
Iteration 17/25 | Loss: 0.00087266
Iteration 18/25 | Loss: 0.00087266
Iteration 19/25 | Loss: 0.00087266
Iteration 20/25 | Loss: 0.00087266
Iteration 21/25 | Loss: 0.00087266
Iteration 22/25 | Loss: 0.00087266
Iteration 23/25 | Loss: 0.00087266
Iteration 24/25 | Loss: 0.00087266
Iteration 25/25 | Loss: 0.00087266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50932074
Iteration 2/25 | Loss: 0.00056584
Iteration 3/25 | Loss: 0.00056584
Iteration 4/25 | Loss: 0.00056584
Iteration 5/25 | Loss: 0.00056584
Iteration 6/25 | Loss: 0.00056584
Iteration 7/25 | Loss: 0.00056584
Iteration 8/25 | Loss: 0.00056584
Iteration 9/25 | Loss: 0.00056584
Iteration 10/25 | Loss: 0.00056584
Iteration 11/25 | Loss: 0.00056584
Iteration 12/25 | Loss: 0.00056584
Iteration 13/25 | Loss: 0.00056584
Iteration 14/25 | Loss: 0.00056584
Iteration 15/25 | Loss: 0.00056584
Iteration 16/25 | Loss: 0.00056584
Iteration 17/25 | Loss: 0.00056584
Iteration 18/25 | Loss: 0.00056584
Iteration 19/25 | Loss: 0.00056584
Iteration 20/25 | Loss: 0.00056584
Iteration 21/25 | Loss: 0.00056584
Iteration 22/25 | Loss: 0.00056584
Iteration 23/25 | Loss: 0.00056584
Iteration 24/25 | Loss: 0.00056584
Iteration 25/25 | Loss: 0.00056584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056584
Iteration 2/1000 | Loss: 0.00003710
Iteration 3/1000 | Loss: 0.00002466
Iteration 4/1000 | Loss: 0.00002169
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001856
Iteration 9/1000 | Loss: 0.00001837
Iteration 10/1000 | Loss: 0.00001824
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001810
Iteration 16/1000 | Loss: 0.00001810
Iteration 17/1000 | Loss: 0.00001810
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001809
Iteration 22/1000 | Loss: 0.00001809
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001807
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001806
Iteration 31/1000 | Loss: 0.00001806
Iteration 32/1000 | Loss: 0.00001806
Iteration 33/1000 | Loss: 0.00001806
Iteration 34/1000 | Loss: 0.00001805
Iteration 35/1000 | Loss: 0.00001804
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001803
Iteration 38/1000 | Loss: 0.00001803
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001803
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001802
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001802
Iteration 46/1000 | Loss: 0.00001802
Iteration 47/1000 | Loss: 0.00001802
Iteration 48/1000 | Loss: 0.00001802
Iteration 49/1000 | Loss: 0.00001801
Iteration 50/1000 | Loss: 0.00001801
Iteration 51/1000 | Loss: 0.00001800
Iteration 52/1000 | Loss: 0.00001800
Iteration 53/1000 | Loss: 0.00001800
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001800
Iteration 57/1000 | Loss: 0.00001799
Iteration 58/1000 | Loss: 0.00001799
Iteration 59/1000 | Loss: 0.00001799
Iteration 60/1000 | Loss: 0.00001799
Iteration 61/1000 | Loss: 0.00001798
Iteration 62/1000 | Loss: 0.00001798
Iteration 63/1000 | Loss: 0.00001798
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001796
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001795
Iteration 77/1000 | Loss: 0.00001795
Iteration 78/1000 | Loss: 0.00001795
Iteration 79/1000 | Loss: 0.00001795
Iteration 80/1000 | Loss: 0.00001795
Iteration 81/1000 | Loss: 0.00001795
Iteration 82/1000 | Loss: 0.00001795
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001795
Iteration 87/1000 | Loss: 0.00001794
Iteration 88/1000 | Loss: 0.00001794
Iteration 89/1000 | Loss: 0.00001794
Iteration 90/1000 | Loss: 0.00001793
Iteration 91/1000 | Loss: 0.00001793
Iteration 92/1000 | Loss: 0.00001793
Iteration 93/1000 | Loss: 0.00001793
Iteration 94/1000 | Loss: 0.00001793
Iteration 95/1000 | Loss: 0.00001792
Iteration 96/1000 | Loss: 0.00001792
Iteration 97/1000 | Loss: 0.00001792
Iteration 98/1000 | Loss: 0.00001792
Iteration 99/1000 | Loss: 0.00001792
Iteration 100/1000 | Loss: 0.00001792
Iteration 101/1000 | Loss: 0.00001791
Iteration 102/1000 | Loss: 0.00001791
Iteration 103/1000 | Loss: 0.00001791
Iteration 104/1000 | Loss: 0.00001791
Iteration 105/1000 | Loss: 0.00001790
Iteration 106/1000 | Loss: 0.00001790
Iteration 107/1000 | Loss: 0.00001790
Iteration 108/1000 | Loss: 0.00001790
Iteration 109/1000 | Loss: 0.00001789
Iteration 110/1000 | Loss: 0.00001789
Iteration 111/1000 | Loss: 0.00001789
Iteration 112/1000 | Loss: 0.00001788
Iteration 113/1000 | Loss: 0.00001788
Iteration 114/1000 | Loss: 0.00001787
Iteration 115/1000 | Loss: 0.00001787
Iteration 116/1000 | Loss: 0.00001787
Iteration 117/1000 | Loss: 0.00001787
Iteration 118/1000 | Loss: 0.00001787
Iteration 119/1000 | Loss: 0.00001786
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001786
Iteration 123/1000 | Loss: 0.00001786
Iteration 124/1000 | Loss: 0.00001786
Iteration 125/1000 | Loss: 0.00001786
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001785
Iteration 128/1000 | Loss: 0.00001785
Iteration 129/1000 | Loss: 0.00001785
Iteration 130/1000 | Loss: 0.00001785
Iteration 131/1000 | Loss: 0.00001785
Iteration 132/1000 | Loss: 0.00001785
Iteration 133/1000 | Loss: 0.00001785
Iteration 134/1000 | Loss: 0.00001785
Iteration 135/1000 | Loss: 0.00001785
Iteration 136/1000 | Loss: 0.00001784
Iteration 137/1000 | Loss: 0.00001784
Iteration 138/1000 | Loss: 0.00001784
Iteration 139/1000 | Loss: 0.00001784
Iteration 140/1000 | Loss: 0.00001783
Iteration 141/1000 | Loss: 0.00001783
Iteration 142/1000 | Loss: 0.00001783
Iteration 143/1000 | Loss: 0.00001783
Iteration 144/1000 | Loss: 0.00001783
Iteration 145/1000 | Loss: 0.00001783
Iteration 146/1000 | Loss: 0.00001783
Iteration 147/1000 | Loss: 0.00001783
Iteration 148/1000 | Loss: 0.00001783
Iteration 149/1000 | Loss: 0.00001783
Iteration 150/1000 | Loss: 0.00001782
Iteration 151/1000 | Loss: 0.00001782
Iteration 152/1000 | Loss: 0.00001782
Iteration 153/1000 | Loss: 0.00001782
Iteration 154/1000 | Loss: 0.00001782
Iteration 155/1000 | Loss: 0.00001782
Iteration 156/1000 | Loss: 0.00001782
Iteration 157/1000 | Loss: 0.00001782
Iteration 158/1000 | Loss: 0.00001782
Iteration 159/1000 | Loss: 0.00001782
Iteration 160/1000 | Loss: 0.00001782
Iteration 161/1000 | Loss: 0.00001782
Iteration 162/1000 | Loss: 0.00001782
Iteration 163/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7820326320361346e-05, 1.7820326320361346e-05, 1.7820326320361346e-05, 1.7820326320361346e-05, 1.7820326320361346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7820326320361346e-05

Optimization complete. Final v2v error: 3.4356749057769775 mm

Highest mean error: 5.306759834289551 mm for frame 83

Lowest mean error: 2.8881425857543945 mm for frame 194

Saving results

Total time: 39.42674541473389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860167
Iteration 2/25 | Loss: 0.00109648
Iteration 3/25 | Loss: 0.00091684
Iteration 4/25 | Loss: 0.00088231
Iteration 5/25 | Loss: 0.00087051
Iteration 6/25 | Loss: 0.00086711
Iteration 7/25 | Loss: 0.00086634
Iteration 8/25 | Loss: 0.00086634
Iteration 9/25 | Loss: 0.00086634
Iteration 10/25 | Loss: 0.00086634
Iteration 11/25 | Loss: 0.00086634
Iteration 12/25 | Loss: 0.00086634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008663350017741323, 0.0008663350017741323, 0.0008663350017741323, 0.0008663350017741323, 0.0008663350017741323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008663350017741323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49209571
Iteration 2/25 | Loss: 0.00058283
Iteration 3/25 | Loss: 0.00058283
Iteration 4/25 | Loss: 0.00058283
Iteration 5/25 | Loss: 0.00058283
Iteration 6/25 | Loss: 0.00058283
Iteration 7/25 | Loss: 0.00058283
Iteration 8/25 | Loss: 0.00058283
Iteration 9/25 | Loss: 0.00058283
Iteration 10/25 | Loss: 0.00058283
Iteration 11/25 | Loss: 0.00058283
Iteration 12/25 | Loss: 0.00058283
Iteration 13/25 | Loss: 0.00058283
Iteration 14/25 | Loss: 0.00058283
Iteration 15/25 | Loss: 0.00058283
Iteration 16/25 | Loss: 0.00058283
Iteration 17/25 | Loss: 0.00058283
Iteration 18/25 | Loss: 0.00058283
Iteration 19/25 | Loss: 0.00058283
Iteration 20/25 | Loss: 0.00058283
Iteration 21/25 | Loss: 0.00058283
Iteration 22/25 | Loss: 0.00058283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005828279536217451, 0.0005828279536217451, 0.0005828279536217451, 0.0005828279536217451, 0.0005828279536217451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005828279536217451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058283
Iteration 2/1000 | Loss: 0.00002952
Iteration 3/1000 | Loss: 0.00001991
Iteration 4/1000 | Loss: 0.00001627
Iteration 5/1000 | Loss: 0.00001508
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001393
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001374
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001368
Iteration 56/1000 | Loss: 0.00001368
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001368
Iteration 59/1000 | Loss: 0.00001368
Iteration 60/1000 | Loss: 0.00001368
Iteration 61/1000 | Loss: 0.00001367
Iteration 62/1000 | Loss: 0.00001367
Iteration 63/1000 | Loss: 0.00001367
Iteration 64/1000 | Loss: 0.00001366
Iteration 65/1000 | Loss: 0.00001366
Iteration 66/1000 | Loss: 0.00001366
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001366
Iteration 70/1000 | Loss: 0.00001366
Iteration 71/1000 | Loss: 0.00001366
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001366
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.3656391274707858e-05, 1.3656391274707858e-05, 1.3656391274707858e-05, 1.3656391274707858e-05, 1.3656391274707858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3656391274707858e-05

Optimization complete. Final v2v error: 3.1368401050567627 mm

Highest mean error: 3.4723665714263916 mm for frame 154

Lowest mean error: 2.797574043273926 mm for frame 206

Saving results

Total time: 29.643233060836792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154920
Iteration 2/25 | Loss: 0.00553030
Iteration 3/25 | Loss: 0.00337971
Iteration 4/25 | Loss: 0.00324322
Iteration 5/25 | Loss: 0.00259945
Iteration 6/25 | Loss: 0.00248700
Iteration 7/25 | Loss: 0.00220134
Iteration 8/25 | Loss: 0.00208932
Iteration 9/25 | Loss: 0.00196984
Iteration 10/25 | Loss: 0.00190418
Iteration 11/25 | Loss: 0.00185541
Iteration 12/25 | Loss: 0.00180877
Iteration 13/25 | Loss: 0.00178465
Iteration 14/25 | Loss: 0.00178519
Iteration 15/25 | Loss: 0.00175927
Iteration 16/25 | Loss: 0.00173944
Iteration 17/25 | Loss: 0.00173126
Iteration 18/25 | Loss: 0.00172663
Iteration 19/25 | Loss: 0.00173229
Iteration 20/25 | Loss: 0.00172916
Iteration 21/25 | Loss: 0.00172335
Iteration 22/25 | Loss: 0.00172373
Iteration 23/25 | Loss: 0.00172171
Iteration 24/25 | Loss: 0.00172077
Iteration 25/25 | Loss: 0.00171979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.47245517
Iteration 2/25 | Loss: 0.00565456
Iteration 3/25 | Loss: 0.00413540
Iteration 4/25 | Loss: 0.00413538
Iteration 5/25 | Loss: 0.00413538
Iteration 6/25 | Loss: 0.00413538
Iteration 7/25 | Loss: 0.00413538
Iteration 8/25 | Loss: 0.00413538
Iteration 9/25 | Loss: 0.00413538
Iteration 10/25 | Loss: 0.00413538
Iteration 11/25 | Loss: 0.00413538
Iteration 12/25 | Loss: 0.00413538
Iteration 13/25 | Loss: 0.00413538
Iteration 14/25 | Loss: 0.00413538
Iteration 15/25 | Loss: 0.00413538
Iteration 16/25 | Loss: 0.00413538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004135380499064922, 0.004135380499064922, 0.004135380499064922, 0.004135380499064922, 0.004135380499064922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004135380499064922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00413538
Iteration 2/1000 | Loss: 0.00091203
Iteration 3/1000 | Loss: 0.00174455
Iteration 4/1000 | Loss: 0.00278725
Iteration 5/1000 | Loss: 0.00113903
Iteration 6/1000 | Loss: 0.00217315
Iteration 7/1000 | Loss: 0.00384258
Iteration 8/1000 | Loss: 0.00246659
Iteration 9/1000 | Loss: 0.00167860
Iteration 10/1000 | Loss: 0.00052885
Iteration 11/1000 | Loss: 0.00184870
Iteration 12/1000 | Loss: 0.00607908
Iteration 13/1000 | Loss: 0.01604601
Iteration 14/1000 | Loss: 0.00418456
Iteration 15/1000 | Loss: 0.00306366
Iteration 16/1000 | Loss: 0.00676652
Iteration 17/1000 | Loss: 0.00190901
Iteration 18/1000 | Loss: 0.00151439
Iteration 19/1000 | Loss: 0.00091761
Iteration 20/1000 | Loss: 0.00128306
Iteration 21/1000 | Loss: 0.00310939
Iteration 22/1000 | Loss: 0.00143175
Iteration 23/1000 | Loss: 0.00147937
Iteration 24/1000 | Loss: 0.00218543
Iteration 25/1000 | Loss: 0.00152802
Iteration 26/1000 | Loss: 0.00133227
Iteration 27/1000 | Loss: 0.00210424
Iteration 28/1000 | Loss: 0.00198295
Iteration 29/1000 | Loss: 0.00067050
Iteration 30/1000 | Loss: 0.00220587
Iteration 31/1000 | Loss: 0.00056949
Iteration 32/1000 | Loss: 0.00037477
Iteration 33/1000 | Loss: 0.00112740
Iteration 34/1000 | Loss: 0.00378427
Iteration 35/1000 | Loss: 0.00145533
Iteration 36/1000 | Loss: 0.00228089
Iteration 37/1000 | Loss: 0.00042997
Iteration 38/1000 | Loss: 0.00107124
Iteration 39/1000 | Loss: 0.00083910
Iteration 40/1000 | Loss: 0.00103618
Iteration 41/1000 | Loss: 0.00090987
Iteration 42/1000 | Loss: 0.00096794
Iteration 43/1000 | Loss: 0.00245335
Iteration 44/1000 | Loss: 0.00105155
Iteration 45/1000 | Loss: 0.00176328
Iteration 46/1000 | Loss: 0.00048687
Iteration 47/1000 | Loss: 0.00031099
Iteration 48/1000 | Loss: 0.00026643
Iteration 49/1000 | Loss: 0.00039829
Iteration 50/1000 | Loss: 0.00040390
Iteration 51/1000 | Loss: 0.00022797
Iteration 52/1000 | Loss: 0.00028253
Iteration 53/1000 | Loss: 0.00073358
Iteration 54/1000 | Loss: 0.00090317
Iteration 55/1000 | Loss: 0.00033622
Iteration 56/1000 | Loss: 0.00054566
Iteration 57/1000 | Loss: 0.00028297
Iteration 58/1000 | Loss: 0.00026934
Iteration 59/1000 | Loss: 0.00025398
Iteration 60/1000 | Loss: 0.00024904
Iteration 61/1000 | Loss: 0.00014092
Iteration 62/1000 | Loss: 0.00058627
Iteration 63/1000 | Loss: 0.00079439
Iteration 64/1000 | Loss: 0.00040722
Iteration 65/1000 | Loss: 0.00034955
Iteration 66/1000 | Loss: 0.00013931
Iteration 67/1000 | Loss: 0.00066974
Iteration 68/1000 | Loss: 0.00048952
Iteration 69/1000 | Loss: 0.00082096
Iteration 70/1000 | Loss: 0.00033462
Iteration 71/1000 | Loss: 0.00021815
Iteration 72/1000 | Loss: 0.00019887
Iteration 73/1000 | Loss: 0.00059669
Iteration 74/1000 | Loss: 0.00056914
Iteration 75/1000 | Loss: 0.00013136
Iteration 76/1000 | Loss: 0.00011976
Iteration 77/1000 | Loss: 0.00029749
Iteration 78/1000 | Loss: 0.00107416
Iteration 79/1000 | Loss: 0.00083809
Iteration 80/1000 | Loss: 0.00028105
Iteration 81/1000 | Loss: 0.00146589
Iteration 82/1000 | Loss: 0.00081023
Iteration 83/1000 | Loss: 0.00043004
Iteration 84/1000 | Loss: 0.00054989
Iteration 85/1000 | Loss: 0.00024148
Iteration 86/1000 | Loss: 0.00026345
Iteration 87/1000 | Loss: 0.00050471
Iteration 88/1000 | Loss: 0.00035561
Iteration 89/1000 | Loss: 0.00033677
Iteration 90/1000 | Loss: 0.00046586
Iteration 91/1000 | Loss: 0.00126549
Iteration 92/1000 | Loss: 0.00043884
Iteration 93/1000 | Loss: 0.00063742
Iteration 94/1000 | Loss: 0.00021418
Iteration 95/1000 | Loss: 0.00030173
Iteration 96/1000 | Loss: 0.00031065
Iteration 97/1000 | Loss: 0.00026597
Iteration 98/1000 | Loss: 0.00027117
Iteration 99/1000 | Loss: 0.00024009
Iteration 100/1000 | Loss: 0.00023054
Iteration 101/1000 | Loss: 0.00010800
Iteration 102/1000 | Loss: 0.00010239
Iteration 103/1000 | Loss: 0.00052938
Iteration 104/1000 | Loss: 0.00026574
Iteration 105/1000 | Loss: 0.00010245
Iteration 106/1000 | Loss: 0.00027794
Iteration 107/1000 | Loss: 0.00010328
Iteration 108/1000 | Loss: 0.00009766
Iteration 109/1000 | Loss: 0.00009473
Iteration 110/1000 | Loss: 0.00009541
Iteration 111/1000 | Loss: 0.00029704
Iteration 112/1000 | Loss: 0.00028326
Iteration 113/1000 | Loss: 0.00029112
Iteration 114/1000 | Loss: 0.00022397
Iteration 115/1000 | Loss: 0.00026366
Iteration 116/1000 | Loss: 0.00030231
Iteration 117/1000 | Loss: 0.00011757
Iteration 118/1000 | Loss: 0.00009902
Iteration 119/1000 | Loss: 0.00009523
Iteration 120/1000 | Loss: 0.00009569
Iteration 121/1000 | Loss: 0.00009348
Iteration 122/1000 | Loss: 0.00009210
Iteration 123/1000 | Loss: 0.00009270
Iteration 124/1000 | Loss: 0.00009071
Iteration 125/1000 | Loss: 0.00016504
Iteration 126/1000 | Loss: 0.00035444
Iteration 127/1000 | Loss: 0.00067901
Iteration 128/1000 | Loss: 0.00054462
Iteration 129/1000 | Loss: 0.00016898
Iteration 130/1000 | Loss: 0.00039013
Iteration 131/1000 | Loss: 0.00028269
Iteration 132/1000 | Loss: 0.00011815
Iteration 133/1000 | Loss: 0.00009717
Iteration 134/1000 | Loss: 0.00009239
Iteration 135/1000 | Loss: 0.00009143
Iteration 136/1000 | Loss: 0.00008945
Iteration 137/1000 | Loss: 0.00008869
Iteration 138/1000 | Loss: 0.00008646
Iteration 139/1000 | Loss: 0.00008627
Iteration 140/1000 | Loss: 0.00008911
Iteration 141/1000 | Loss: 0.00008819
Iteration 142/1000 | Loss: 0.00008847
Iteration 143/1000 | Loss: 0.00043792
Iteration 144/1000 | Loss: 0.00013317
Iteration 145/1000 | Loss: 0.00032135
Iteration 146/1000 | Loss: 0.00015183
Iteration 147/1000 | Loss: 0.00010241
Iteration 148/1000 | Loss: 0.00009428
Iteration 149/1000 | Loss: 0.00009216
Iteration 150/1000 | Loss: 0.00009326
Iteration 151/1000 | Loss: 0.00009087
Iteration 152/1000 | Loss: 0.00009335
Iteration 153/1000 | Loss: 0.00022947
Iteration 154/1000 | Loss: 0.00016608
Iteration 155/1000 | Loss: 0.00008908
Iteration 156/1000 | Loss: 0.00008833
Iteration 157/1000 | Loss: 0.00041197
Iteration 158/1000 | Loss: 0.00016977
Iteration 159/1000 | Loss: 0.00010842
Iteration 160/1000 | Loss: 0.00008871
Iteration 161/1000 | Loss: 0.00008773
Iteration 162/1000 | Loss: 0.00008561
Iteration 163/1000 | Loss: 0.00008597
Iteration 164/1000 | Loss: 0.00008625
Iteration 165/1000 | Loss: 0.00008620
Iteration 166/1000 | Loss: 0.00016058
Iteration 167/1000 | Loss: 0.00010080
Iteration 168/1000 | Loss: 0.00008813
Iteration 169/1000 | Loss: 0.00008993
Iteration 170/1000 | Loss: 0.00016619
Iteration 171/1000 | Loss: 0.00018034
Iteration 172/1000 | Loss: 0.00100177
Iteration 173/1000 | Loss: 0.00047225
Iteration 174/1000 | Loss: 0.00013953
Iteration 175/1000 | Loss: 0.00030599
Iteration 176/1000 | Loss: 0.00022041
Iteration 177/1000 | Loss: 0.00052971
Iteration 178/1000 | Loss: 0.00023891
Iteration 179/1000 | Loss: 0.00012463
Iteration 180/1000 | Loss: 0.00009193
Iteration 181/1000 | Loss: 0.00008744
Iteration 182/1000 | Loss: 0.00008498
Iteration 183/1000 | Loss: 0.00008355
Iteration 184/1000 | Loss: 0.00008208
Iteration 185/1000 | Loss: 0.00008143
Iteration 186/1000 | Loss: 0.00008081
Iteration 187/1000 | Loss: 0.00007986
Iteration 188/1000 | Loss: 0.00015322
Iteration 189/1000 | Loss: 0.00011005
Iteration 190/1000 | Loss: 0.00007892
Iteration 191/1000 | Loss: 0.00007860
Iteration 192/1000 | Loss: 0.00007835
Iteration 193/1000 | Loss: 0.00007823
Iteration 194/1000 | Loss: 0.00007823
Iteration 195/1000 | Loss: 0.00007823
Iteration 196/1000 | Loss: 0.00007823
Iteration 197/1000 | Loss: 0.00007823
Iteration 198/1000 | Loss: 0.00007823
Iteration 199/1000 | Loss: 0.00007823
Iteration 200/1000 | Loss: 0.00007822
Iteration 201/1000 | Loss: 0.00007822
Iteration 202/1000 | Loss: 0.00007822
Iteration 203/1000 | Loss: 0.00007821
Iteration 204/1000 | Loss: 0.00007821
Iteration 205/1000 | Loss: 0.00007821
Iteration 206/1000 | Loss: 0.00007821
Iteration 207/1000 | Loss: 0.00007821
Iteration 208/1000 | Loss: 0.00007821
Iteration 209/1000 | Loss: 0.00007821
Iteration 210/1000 | Loss: 0.00007821
Iteration 211/1000 | Loss: 0.00007821
Iteration 212/1000 | Loss: 0.00007821
Iteration 213/1000 | Loss: 0.00007821
Iteration 214/1000 | Loss: 0.00007820
Iteration 215/1000 | Loss: 0.00007820
Iteration 216/1000 | Loss: 0.00007820
Iteration 217/1000 | Loss: 0.00007820
Iteration 218/1000 | Loss: 0.00007820
Iteration 219/1000 | Loss: 0.00007820
Iteration 220/1000 | Loss: 0.00007820
Iteration 221/1000 | Loss: 0.00007820
Iteration 222/1000 | Loss: 0.00007819
Iteration 223/1000 | Loss: 0.00007819
Iteration 224/1000 | Loss: 0.00007819
Iteration 225/1000 | Loss: 0.00007819
Iteration 226/1000 | Loss: 0.00007819
Iteration 227/1000 | Loss: 0.00007819
Iteration 228/1000 | Loss: 0.00007818
Iteration 229/1000 | Loss: 0.00007818
Iteration 230/1000 | Loss: 0.00007818
Iteration 231/1000 | Loss: 0.00007818
Iteration 232/1000 | Loss: 0.00007818
Iteration 233/1000 | Loss: 0.00007818
Iteration 234/1000 | Loss: 0.00007818
Iteration 235/1000 | Loss: 0.00007818
Iteration 236/1000 | Loss: 0.00007818
Iteration 237/1000 | Loss: 0.00007818
Iteration 238/1000 | Loss: 0.00007818
Iteration 239/1000 | Loss: 0.00007818
Iteration 240/1000 | Loss: 0.00007818
Iteration 241/1000 | Loss: 0.00007818
Iteration 242/1000 | Loss: 0.00007818
Iteration 243/1000 | Loss: 0.00007818
Iteration 244/1000 | Loss: 0.00007818
Iteration 245/1000 | Loss: 0.00007818
Iteration 246/1000 | Loss: 0.00007818
Iteration 247/1000 | Loss: 0.00007818
Iteration 248/1000 | Loss: 0.00007818
Iteration 249/1000 | Loss: 0.00007818
Iteration 250/1000 | Loss: 0.00007818
Iteration 251/1000 | Loss: 0.00007818
Iteration 252/1000 | Loss: 0.00007818
Iteration 253/1000 | Loss: 0.00007818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [7.81769267632626e-05, 7.81769267632626e-05, 7.81769267632626e-05, 7.81769267632626e-05, 7.81769267632626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.81769267632626e-05

Optimization complete. Final v2v error: 6.720019340515137 mm

Highest mean error: 14.12520694732666 mm for frame 138

Lowest mean error: 4.86426305770874 mm for frame 12

Saving results

Total time: 363.27715587615967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984109
Iteration 2/25 | Loss: 0.00161417
Iteration 3/25 | Loss: 0.00109981
Iteration 4/25 | Loss: 0.00104643
Iteration 5/25 | Loss: 0.00102189
Iteration 6/25 | Loss: 0.00101640
Iteration 7/25 | Loss: 0.00101583
Iteration 8/25 | Loss: 0.00101583
Iteration 9/25 | Loss: 0.00101583
Iteration 10/25 | Loss: 0.00101583
Iteration 11/25 | Loss: 0.00101583
Iteration 12/25 | Loss: 0.00101583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010158311342820525, 0.0010158311342820525, 0.0010158311342820525, 0.0010158311342820525, 0.0010158311342820525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010158311342820525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12951672
Iteration 2/25 | Loss: 0.00063408
Iteration 3/25 | Loss: 0.00063406
Iteration 4/25 | Loss: 0.00063406
Iteration 5/25 | Loss: 0.00063406
Iteration 6/25 | Loss: 0.00063406
Iteration 7/25 | Loss: 0.00063406
Iteration 8/25 | Loss: 0.00063406
Iteration 9/25 | Loss: 0.00063406
Iteration 10/25 | Loss: 0.00063406
Iteration 11/25 | Loss: 0.00063406
Iteration 12/25 | Loss: 0.00063406
Iteration 13/25 | Loss: 0.00063406
Iteration 14/25 | Loss: 0.00063406
Iteration 15/25 | Loss: 0.00063406
Iteration 16/25 | Loss: 0.00063406
Iteration 17/25 | Loss: 0.00063406
Iteration 18/25 | Loss: 0.00063406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006340616382658482, 0.0006340616382658482, 0.0006340616382658482, 0.0006340616382658482, 0.0006340616382658482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006340616382658482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063406
Iteration 2/1000 | Loss: 0.00005717
Iteration 3/1000 | Loss: 0.00004605
Iteration 4/1000 | Loss: 0.00004217
Iteration 5/1000 | Loss: 0.00004002
Iteration 6/1000 | Loss: 0.00003845
Iteration 7/1000 | Loss: 0.00003759
Iteration 8/1000 | Loss: 0.00003677
Iteration 9/1000 | Loss: 0.00003629
Iteration 10/1000 | Loss: 0.00003600
Iteration 11/1000 | Loss: 0.00003576
Iteration 12/1000 | Loss: 0.00003561
Iteration 13/1000 | Loss: 0.00003554
Iteration 14/1000 | Loss: 0.00003553
Iteration 15/1000 | Loss: 0.00003553
Iteration 16/1000 | Loss: 0.00003550
Iteration 17/1000 | Loss: 0.00003549
Iteration 18/1000 | Loss: 0.00003549
Iteration 19/1000 | Loss: 0.00003546
Iteration 20/1000 | Loss: 0.00003545
Iteration 21/1000 | Loss: 0.00003544
Iteration 22/1000 | Loss: 0.00003543
Iteration 23/1000 | Loss: 0.00003543
Iteration 24/1000 | Loss: 0.00003542
Iteration 25/1000 | Loss: 0.00003540
Iteration 26/1000 | Loss: 0.00003540
Iteration 27/1000 | Loss: 0.00003540
Iteration 28/1000 | Loss: 0.00003540
Iteration 29/1000 | Loss: 0.00003540
Iteration 30/1000 | Loss: 0.00003540
Iteration 31/1000 | Loss: 0.00003540
Iteration 32/1000 | Loss: 0.00003540
Iteration 33/1000 | Loss: 0.00003539
Iteration 34/1000 | Loss: 0.00003538
Iteration 35/1000 | Loss: 0.00003536
Iteration 36/1000 | Loss: 0.00003536
Iteration 37/1000 | Loss: 0.00003536
Iteration 38/1000 | Loss: 0.00003536
Iteration 39/1000 | Loss: 0.00003536
Iteration 40/1000 | Loss: 0.00003536
Iteration 41/1000 | Loss: 0.00003536
Iteration 42/1000 | Loss: 0.00003536
Iteration 43/1000 | Loss: 0.00003536
Iteration 44/1000 | Loss: 0.00003536
Iteration 45/1000 | Loss: 0.00003536
Iteration 46/1000 | Loss: 0.00003535
Iteration 47/1000 | Loss: 0.00003533
Iteration 48/1000 | Loss: 0.00003532
Iteration 49/1000 | Loss: 0.00003529
Iteration 50/1000 | Loss: 0.00003526
Iteration 51/1000 | Loss: 0.00003523
Iteration 52/1000 | Loss: 0.00003522
Iteration 53/1000 | Loss: 0.00003515
Iteration 54/1000 | Loss: 0.00003513
Iteration 55/1000 | Loss: 0.00003512
Iteration 56/1000 | Loss: 0.00003512
Iteration 57/1000 | Loss: 0.00003512
Iteration 58/1000 | Loss: 0.00003511
Iteration 59/1000 | Loss: 0.00003511
Iteration 60/1000 | Loss: 0.00003510
Iteration 61/1000 | Loss: 0.00003510
Iteration 62/1000 | Loss: 0.00003509
Iteration 63/1000 | Loss: 0.00003509
Iteration 64/1000 | Loss: 0.00003507
Iteration 65/1000 | Loss: 0.00003504
Iteration 66/1000 | Loss: 0.00003501
Iteration 67/1000 | Loss: 0.00003499
Iteration 68/1000 | Loss: 0.00003497
Iteration 69/1000 | Loss: 0.00003497
Iteration 70/1000 | Loss: 0.00003497
Iteration 71/1000 | Loss: 0.00003496
Iteration 72/1000 | Loss: 0.00003496
Iteration 73/1000 | Loss: 0.00003494
Iteration 74/1000 | Loss: 0.00003494
Iteration 75/1000 | Loss: 0.00003494
Iteration 76/1000 | Loss: 0.00003493
Iteration 77/1000 | Loss: 0.00003493
Iteration 78/1000 | Loss: 0.00003490
Iteration 79/1000 | Loss: 0.00003490
Iteration 80/1000 | Loss: 0.00003490
Iteration 81/1000 | Loss: 0.00003490
Iteration 82/1000 | Loss: 0.00003489
Iteration 83/1000 | Loss: 0.00003489
Iteration 84/1000 | Loss: 0.00003488
Iteration 85/1000 | Loss: 0.00003487
Iteration 86/1000 | Loss: 0.00003487
Iteration 87/1000 | Loss: 0.00003487
Iteration 88/1000 | Loss: 0.00003487
Iteration 89/1000 | Loss: 0.00003486
Iteration 90/1000 | Loss: 0.00003486
Iteration 91/1000 | Loss: 0.00003486
Iteration 92/1000 | Loss: 0.00003486
Iteration 93/1000 | Loss: 0.00003486
Iteration 94/1000 | Loss: 0.00003484
Iteration 95/1000 | Loss: 0.00003484
Iteration 96/1000 | Loss: 0.00003484
Iteration 97/1000 | Loss: 0.00003484
Iteration 98/1000 | Loss: 0.00003484
Iteration 99/1000 | Loss: 0.00003484
Iteration 100/1000 | Loss: 0.00003484
Iteration 101/1000 | Loss: 0.00003484
Iteration 102/1000 | Loss: 0.00003484
Iteration 103/1000 | Loss: 0.00003484
Iteration 104/1000 | Loss: 0.00003484
Iteration 105/1000 | Loss: 0.00003483
Iteration 106/1000 | Loss: 0.00003483
Iteration 107/1000 | Loss: 0.00003483
Iteration 108/1000 | Loss: 0.00003483
Iteration 109/1000 | Loss: 0.00003483
Iteration 110/1000 | Loss: 0.00003482
Iteration 111/1000 | Loss: 0.00003482
Iteration 112/1000 | Loss: 0.00003482
Iteration 113/1000 | Loss: 0.00003482
Iteration 114/1000 | Loss: 0.00003482
Iteration 115/1000 | Loss: 0.00003481
Iteration 116/1000 | Loss: 0.00003481
Iteration 117/1000 | Loss: 0.00003481
Iteration 118/1000 | Loss: 0.00003481
Iteration 119/1000 | Loss: 0.00003481
Iteration 120/1000 | Loss: 0.00003481
Iteration 121/1000 | Loss: 0.00003481
Iteration 122/1000 | Loss: 0.00003481
Iteration 123/1000 | Loss: 0.00003481
Iteration 124/1000 | Loss: 0.00003481
Iteration 125/1000 | Loss: 0.00003480
Iteration 126/1000 | Loss: 0.00003480
Iteration 127/1000 | Loss: 0.00003480
Iteration 128/1000 | Loss: 0.00003480
Iteration 129/1000 | Loss: 0.00003480
Iteration 130/1000 | Loss: 0.00003479
Iteration 131/1000 | Loss: 0.00003479
Iteration 132/1000 | Loss: 0.00003479
Iteration 133/1000 | Loss: 0.00003479
Iteration 134/1000 | Loss: 0.00003479
Iteration 135/1000 | Loss: 0.00003479
Iteration 136/1000 | Loss: 0.00003479
Iteration 137/1000 | Loss: 0.00003478
Iteration 138/1000 | Loss: 0.00003478
Iteration 139/1000 | Loss: 0.00003478
Iteration 140/1000 | Loss: 0.00003478
Iteration 141/1000 | Loss: 0.00003478
Iteration 142/1000 | Loss: 0.00003478
Iteration 143/1000 | Loss: 0.00003478
Iteration 144/1000 | Loss: 0.00003478
Iteration 145/1000 | Loss: 0.00003478
Iteration 146/1000 | Loss: 0.00003478
Iteration 147/1000 | Loss: 0.00003478
Iteration 148/1000 | Loss: 0.00003478
Iteration 149/1000 | Loss: 0.00003478
Iteration 150/1000 | Loss: 0.00003477
Iteration 151/1000 | Loss: 0.00003477
Iteration 152/1000 | Loss: 0.00003477
Iteration 153/1000 | Loss: 0.00003477
Iteration 154/1000 | Loss: 0.00003477
Iteration 155/1000 | Loss: 0.00003476
Iteration 156/1000 | Loss: 0.00003476
Iteration 157/1000 | Loss: 0.00003476
Iteration 158/1000 | Loss: 0.00003476
Iteration 159/1000 | Loss: 0.00003476
Iteration 160/1000 | Loss: 0.00003476
Iteration 161/1000 | Loss: 0.00003476
Iteration 162/1000 | Loss: 0.00003476
Iteration 163/1000 | Loss: 0.00003476
Iteration 164/1000 | Loss: 0.00003476
Iteration 165/1000 | Loss: 0.00003476
Iteration 166/1000 | Loss: 0.00003476
Iteration 167/1000 | Loss: 0.00003475
Iteration 168/1000 | Loss: 0.00003475
Iteration 169/1000 | Loss: 0.00003475
Iteration 170/1000 | Loss: 0.00003475
Iteration 171/1000 | Loss: 0.00003475
Iteration 172/1000 | Loss: 0.00003475
Iteration 173/1000 | Loss: 0.00003475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.475471385172568e-05, 3.475471385172568e-05, 3.475471385172568e-05, 3.475471385172568e-05, 3.475471385172568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.475471385172568e-05

Optimization complete. Final v2v error: 4.849404811859131 mm

Highest mean error: 5.942759037017822 mm for frame 101

Lowest mean error: 3.876225233078003 mm for frame 120

Saving results

Total time: 44.65549302101135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421260
Iteration 2/25 | Loss: 0.00104695
Iteration 3/25 | Loss: 0.00092669
Iteration 4/25 | Loss: 0.00090790
Iteration 5/25 | Loss: 0.00090027
Iteration 6/25 | Loss: 0.00089851
Iteration 7/25 | Loss: 0.00089799
Iteration 8/25 | Loss: 0.00089799
Iteration 9/25 | Loss: 0.00089799
Iteration 10/25 | Loss: 0.00089799
Iteration 11/25 | Loss: 0.00089799
Iteration 12/25 | Loss: 0.00089799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008979944395832717, 0.0008979944395832717, 0.0008979944395832717, 0.0008979944395832717, 0.0008979944395832717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008979944395832717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18060803
Iteration 2/25 | Loss: 0.00060806
Iteration 3/25 | Loss: 0.00060805
Iteration 4/25 | Loss: 0.00060805
Iteration 5/25 | Loss: 0.00060805
Iteration 6/25 | Loss: 0.00060805
Iteration 7/25 | Loss: 0.00060804
Iteration 8/25 | Loss: 0.00060804
Iteration 9/25 | Loss: 0.00060804
Iteration 10/25 | Loss: 0.00060804
Iteration 11/25 | Loss: 0.00060804
Iteration 12/25 | Loss: 0.00060804
Iteration 13/25 | Loss: 0.00060804
Iteration 14/25 | Loss: 0.00060804
Iteration 15/25 | Loss: 0.00060804
Iteration 16/25 | Loss: 0.00060804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006080440944060683, 0.0006080440944060683, 0.0006080440944060683, 0.0006080440944060683, 0.0006080440944060683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006080440944060683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060804
Iteration 2/1000 | Loss: 0.00004646
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002561
Iteration 5/1000 | Loss: 0.00002374
Iteration 6/1000 | Loss: 0.00002249
Iteration 7/1000 | Loss: 0.00002176
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002091
Iteration 10/1000 | Loss: 0.00002078
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002062
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002046
Iteration 16/1000 | Loss: 0.00002045
Iteration 17/1000 | Loss: 0.00002045
Iteration 18/1000 | Loss: 0.00002044
Iteration 19/1000 | Loss: 0.00002043
Iteration 20/1000 | Loss: 0.00002043
Iteration 21/1000 | Loss: 0.00002042
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00002041
Iteration 24/1000 | Loss: 0.00002040
Iteration 25/1000 | Loss: 0.00002040
Iteration 26/1000 | Loss: 0.00002040
Iteration 27/1000 | Loss: 0.00002039
Iteration 28/1000 | Loss: 0.00002039
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002038
Iteration 37/1000 | Loss: 0.00002038
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002034
Iteration 50/1000 | Loss: 0.00002034
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002033
Iteration 54/1000 | Loss: 0.00002033
Iteration 55/1000 | Loss: 0.00002032
Iteration 56/1000 | Loss: 0.00002032
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002032
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002032
Iteration 61/1000 | Loss: 0.00002032
Iteration 62/1000 | Loss: 0.00002032
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002031
Iteration 65/1000 | Loss: 0.00002031
Iteration 66/1000 | Loss: 0.00002031
Iteration 67/1000 | Loss: 0.00002031
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002030
Iteration 77/1000 | Loss: 0.00002030
Iteration 78/1000 | Loss: 0.00002030
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002029
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002029
Iteration 84/1000 | Loss: 0.00002029
Iteration 85/1000 | Loss: 0.00002029
Iteration 86/1000 | Loss: 0.00002029
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002028
Iteration 100/1000 | Loss: 0.00002028
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002028
Iteration 107/1000 | Loss: 0.00002028
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002028
Iteration 114/1000 | Loss: 0.00002028
Iteration 115/1000 | Loss: 0.00002028
Iteration 116/1000 | Loss: 0.00002028
Iteration 117/1000 | Loss: 0.00002028
Iteration 118/1000 | Loss: 0.00002028
Iteration 119/1000 | Loss: 0.00002028
Iteration 120/1000 | Loss: 0.00002028
Iteration 121/1000 | Loss: 0.00002028
Iteration 122/1000 | Loss: 0.00002027
Iteration 123/1000 | Loss: 0.00002027
Iteration 124/1000 | Loss: 0.00002027
Iteration 125/1000 | Loss: 0.00002027
Iteration 126/1000 | Loss: 0.00002027
Iteration 127/1000 | Loss: 0.00002027
Iteration 128/1000 | Loss: 0.00002027
Iteration 129/1000 | Loss: 0.00002027
Iteration 130/1000 | Loss: 0.00002027
Iteration 131/1000 | Loss: 0.00002027
Iteration 132/1000 | Loss: 0.00002027
Iteration 133/1000 | Loss: 0.00002027
Iteration 134/1000 | Loss: 0.00002027
Iteration 135/1000 | Loss: 0.00002027
Iteration 136/1000 | Loss: 0.00002027
Iteration 137/1000 | Loss: 0.00002027
Iteration 138/1000 | Loss: 0.00002027
Iteration 139/1000 | Loss: 0.00002027
Iteration 140/1000 | Loss: 0.00002027
Iteration 141/1000 | Loss: 0.00002027
Iteration 142/1000 | Loss: 0.00002027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.0268083972041495e-05, 2.0268083972041495e-05, 2.0268083972041495e-05, 2.0268083972041495e-05, 2.0268083972041495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0268083972041495e-05

Optimization complete. Final v2v error: 3.8753509521484375 mm

Highest mean error: 4.24363374710083 mm for frame 67

Lowest mean error: 3.5657832622528076 mm for frame 129

Saving results

Total time: 33.78210091590881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149883
Iteration 2/25 | Loss: 0.01149883
Iteration 3/25 | Loss: 0.01149883
Iteration 4/25 | Loss: 0.01149883
Iteration 5/25 | Loss: 0.01149883
Iteration 6/25 | Loss: 0.01149883
Iteration 7/25 | Loss: 0.01149883
Iteration 8/25 | Loss: 0.01149883
Iteration 9/25 | Loss: 0.01149883
Iteration 10/25 | Loss: 0.01149883
Iteration 11/25 | Loss: 0.01149883
Iteration 12/25 | Loss: 0.01149882
Iteration 13/25 | Loss: 0.01149882
Iteration 14/25 | Loss: 0.01149882
Iteration 15/25 | Loss: 0.01149882
Iteration 16/25 | Loss: 0.01149882
Iteration 17/25 | Loss: 0.01149882
Iteration 18/25 | Loss: 0.01149882
Iteration 19/25 | Loss: 0.01149882
Iteration 20/25 | Loss: 0.01149882
Iteration 21/25 | Loss: 0.01149882
Iteration 22/25 | Loss: 0.01149881
Iteration 23/25 | Loss: 0.01149881
Iteration 24/25 | Loss: 0.01149881
Iteration 25/25 | Loss: 0.01149881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94426262
Iteration 2/25 | Loss: 0.05210719
Iteration 3/25 | Loss: 0.05210701
Iteration 4/25 | Loss: 0.05210701
Iteration 5/25 | Loss: 0.05210701
Iteration 6/25 | Loss: 0.05210701
Iteration 7/25 | Loss: 0.05210701
Iteration 8/25 | Loss: 0.05210701
Iteration 9/25 | Loss: 0.05210701
Iteration 10/25 | Loss: 0.05210701
Iteration 11/25 | Loss: 0.05210701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.052107006311416626, 0.052107006311416626, 0.052107006311416626, 0.052107006311416626, 0.052107006311416626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.052107006311416626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05210701
Iteration 2/1000 | Loss: 0.00830133
Iteration 3/1000 | Loss: 0.00059014
Iteration 4/1000 | Loss: 0.00093978
Iteration 5/1000 | Loss: 0.00370780
Iteration 6/1000 | Loss: 0.00015343
Iteration 7/1000 | Loss: 0.00057761
Iteration 8/1000 | Loss: 0.00308456
Iteration 9/1000 | Loss: 0.00096064
Iteration 10/1000 | Loss: 0.00114434
Iteration 11/1000 | Loss: 0.00025484
Iteration 12/1000 | Loss: 0.00007125
Iteration 13/1000 | Loss: 0.00005875
Iteration 14/1000 | Loss: 0.00061323
Iteration 15/1000 | Loss: 0.00005015
Iteration 16/1000 | Loss: 0.00003994
Iteration 17/1000 | Loss: 0.00003557
Iteration 18/1000 | Loss: 0.00012440
Iteration 19/1000 | Loss: 0.00079476
Iteration 20/1000 | Loss: 0.00006868
Iteration 21/1000 | Loss: 0.00012148
Iteration 22/1000 | Loss: 0.00003584
Iteration 23/1000 | Loss: 0.00004191
Iteration 24/1000 | Loss: 0.00003081
Iteration 25/1000 | Loss: 0.00043777
Iteration 26/1000 | Loss: 0.00051212
Iteration 27/1000 | Loss: 0.00009556
Iteration 28/1000 | Loss: 0.00024426
Iteration 29/1000 | Loss: 0.00017889
Iteration 30/1000 | Loss: 0.00002595
Iteration 31/1000 | Loss: 0.00016216
Iteration 32/1000 | Loss: 0.00047153
Iteration 33/1000 | Loss: 0.00018606
Iteration 34/1000 | Loss: 0.00029166
Iteration 35/1000 | Loss: 0.00011619
Iteration 36/1000 | Loss: 0.00008692
Iteration 37/1000 | Loss: 0.00017578
Iteration 38/1000 | Loss: 0.00006451
Iteration 39/1000 | Loss: 0.00002722
Iteration 40/1000 | Loss: 0.00004122
Iteration 41/1000 | Loss: 0.00002233
Iteration 42/1000 | Loss: 0.00003375
Iteration 43/1000 | Loss: 0.00004333
Iteration 44/1000 | Loss: 0.00002360
Iteration 45/1000 | Loss: 0.00003160
Iteration 46/1000 | Loss: 0.00026123
Iteration 47/1000 | Loss: 0.00002383
Iteration 48/1000 | Loss: 0.00002100
Iteration 49/1000 | Loss: 0.00003533
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00002704
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00002036
Iteration 56/1000 | Loss: 0.00001953
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001795
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00001765
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001764
Iteration 68/1000 | Loss: 0.00001764
Iteration 69/1000 | Loss: 0.00001764
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001757
Iteration 79/1000 | Loss: 0.00001757
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001752
Iteration 88/1000 | Loss: 0.00001752
Iteration 89/1000 | Loss: 0.00001752
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001751
Iteration 95/1000 | Loss: 0.00001751
Iteration 96/1000 | Loss: 0.00001751
Iteration 97/1000 | Loss: 0.00001750
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00002463
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001748
Iteration 105/1000 | Loss: 0.00001748
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001747
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001747
Iteration 110/1000 | Loss: 0.00001747
Iteration 111/1000 | Loss: 0.00001747
Iteration 112/1000 | Loss: 0.00001747
Iteration 113/1000 | Loss: 0.00001747
Iteration 114/1000 | Loss: 0.00001747
Iteration 115/1000 | Loss: 0.00001747
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001746
Iteration 118/1000 | Loss: 0.00002071
Iteration 119/1000 | Loss: 0.00023416
Iteration 120/1000 | Loss: 0.00004258
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001746
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001743
Iteration 131/1000 | Loss: 0.00001743
Iteration 132/1000 | Loss: 0.00001743
Iteration 133/1000 | Loss: 0.00001743
Iteration 134/1000 | Loss: 0.00002570
Iteration 135/1000 | Loss: 0.00052691
Iteration 136/1000 | Loss: 0.00002637
Iteration 137/1000 | Loss: 0.00002887
Iteration 138/1000 | Loss: 0.00011420
Iteration 139/1000 | Loss: 0.00010398
Iteration 140/1000 | Loss: 0.00010297
Iteration 141/1000 | Loss: 0.00002506
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001770
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00002791
Iteration 146/1000 | Loss: 0.00001846
Iteration 147/1000 | Loss: 0.00001979
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001754
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001738
Iteration 153/1000 | Loss: 0.00001738
Iteration 154/1000 | Loss: 0.00001738
Iteration 155/1000 | Loss: 0.00001738
Iteration 156/1000 | Loss: 0.00001737
Iteration 157/1000 | Loss: 0.00001737
Iteration 158/1000 | Loss: 0.00001737
Iteration 159/1000 | Loss: 0.00001737
Iteration 160/1000 | Loss: 0.00001737
Iteration 161/1000 | Loss: 0.00001737
Iteration 162/1000 | Loss: 0.00001737
Iteration 163/1000 | Loss: 0.00001737
Iteration 164/1000 | Loss: 0.00001737
Iteration 165/1000 | Loss: 0.00001737
Iteration 166/1000 | Loss: 0.00001736
Iteration 167/1000 | Loss: 0.00001736
Iteration 168/1000 | Loss: 0.00001736
Iteration 169/1000 | Loss: 0.00001736
Iteration 170/1000 | Loss: 0.00001736
Iteration 171/1000 | Loss: 0.00001736
Iteration 172/1000 | Loss: 0.00001736
Iteration 173/1000 | Loss: 0.00001736
Iteration 174/1000 | Loss: 0.00001736
Iteration 175/1000 | Loss: 0.00001736
Iteration 176/1000 | Loss: 0.00001736
Iteration 177/1000 | Loss: 0.00001736
Iteration 178/1000 | Loss: 0.00001736
Iteration 179/1000 | Loss: 0.00001736
Iteration 180/1000 | Loss: 0.00001735
Iteration 181/1000 | Loss: 0.00001735
Iteration 182/1000 | Loss: 0.00001735
Iteration 183/1000 | Loss: 0.00001735
Iteration 184/1000 | Loss: 0.00001735
Iteration 185/1000 | Loss: 0.00001735
Iteration 186/1000 | Loss: 0.00001735
Iteration 187/1000 | Loss: 0.00001735
Iteration 188/1000 | Loss: 0.00001735
Iteration 189/1000 | Loss: 0.00001735
Iteration 190/1000 | Loss: 0.00001735
Iteration 191/1000 | Loss: 0.00001735
Iteration 192/1000 | Loss: 0.00001734
Iteration 193/1000 | Loss: 0.00001734
Iteration 194/1000 | Loss: 0.00001734
Iteration 195/1000 | Loss: 0.00001734
Iteration 196/1000 | Loss: 0.00001734
Iteration 197/1000 | Loss: 0.00001734
Iteration 198/1000 | Loss: 0.00001734
Iteration 199/1000 | Loss: 0.00001734
Iteration 200/1000 | Loss: 0.00001734
Iteration 201/1000 | Loss: 0.00001734
Iteration 202/1000 | Loss: 0.00001734
Iteration 203/1000 | Loss: 0.00001734
Iteration 204/1000 | Loss: 0.00001734
Iteration 205/1000 | Loss: 0.00001734
Iteration 206/1000 | Loss: 0.00001734
Iteration 207/1000 | Loss: 0.00001734
Iteration 208/1000 | Loss: 0.00001734
Iteration 209/1000 | Loss: 0.00001734
Iteration 210/1000 | Loss: 0.00001734
Iteration 211/1000 | Loss: 0.00001734
Iteration 212/1000 | Loss: 0.00001734
Iteration 213/1000 | Loss: 0.00001734
Iteration 214/1000 | Loss: 0.00001734
Iteration 215/1000 | Loss: 0.00001734
Iteration 216/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.7338274119538255e-05, 1.7338274119538255e-05, 1.7338274119538255e-05, 1.7338274119538255e-05, 1.7338274119538255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7338274119538255e-05

Optimization complete. Final v2v error: 3.5516679286956787 mm

Highest mean error: 10.680618286132812 mm for frame 131

Lowest mean error: 3.2003414630889893 mm for frame 145

Saving results

Total time: 144.93229627609253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911776
Iteration 2/25 | Loss: 0.00205260
Iteration 3/25 | Loss: 0.00163010
Iteration 4/25 | Loss: 0.00180510
Iteration 5/25 | Loss: 0.00173083
Iteration 6/25 | Loss: 0.00130260
Iteration 7/25 | Loss: 0.00117102
Iteration 8/25 | Loss: 0.00115623
Iteration 9/25 | Loss: 0.00115505
Iteration 10/25 | Loss: 0.00115475
Iteration 11/25 | Loss: 0.00115471
Iteration 12/25 | Loss: 0.00115471
Iteration 13/25 | Loss: 0.00115471
Iteration 14/25 | Loss: 0.00115471
Iteration 15/25 | Loss: 0.00115471
Iteration 16/25 | Loss: 0.00115471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001154705765657127, 0.001154705765657127, 0.001154705765657127, 0.001154705765657127, 0.001154705765657127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001154705765657127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19755816
Iteration 2/25 | Loss: 0.00113733
Iteration 3/25 | Loss: 0.00113733
Iteration 4/25 | Loss: 0.00113733
Iteration 5/25 | Loss: 0.00113733
Iteration 6/25 | Loss: 0.00113733
Iteration 7/25 | Loss: 0.00113733
Iteration 8/25 | Loss: 0.00113733
Iteration 9/25 | Loss: 0.00113733
Iteration 10/25 | Loss: 0.00113733
Iteration 11/25 | Loss: 0.00113733
Iteration 12/25 | Loss: 0.00113733
Iteration 13/25 | Loss: 0.00113733
Iteration 14/25 | Loss: 0.00113733
Iteration 15/25 | Loss: 0.00113733
Iteration 16/25 | Loss: 0.00113733
Iteration 17/25 | Loss: 0.00113733
Iteration 18/25 | Loss: 0.00113733
Iteration 19/25 | Loss: 0.00113733
Iteration 20/25 | Loss: 0.00113733
Iteration 21/25 | Loss: 0.00113733
Iteration 22/25 | Loss: 0.00113733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011373283341526985, 0.0011373283341526985, 0.0011373283341526985, 0.0011373283341526985, 0.0011373283341526985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011373283341526985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113733
Iteration 2/1000 | Loss: 0.00007278
Iteration 3/1000 | Loss: 0.00005539
Iteration 4/1000 | Loss: 0.00004943
Iteration 5/1000 | Loss: 0.00004648
Iteration 6/1000 | Loss: 0.00004495
Iteration 7/1000 | Loss: 0.00004423
Iteration 8/1000 | Loss: 0.00004382
Iteration 9/1000 | Loss: 0.00004359
Iteration 10/1000 | Loss: 0.00004354
Iteration 11/1000 | Loss: 0.00004354
Iteration 12/1000 | Loss: 0.00004335
Iteration 13/1000 | Loss: 0.00004325
Iteration 14/1000 | Loss: 0.00004321
Iteration 15/1000 | Loss: 0.00004319
Iteration 16/1000 | Loss: 0.00004313
Iteration 17/1000 | Loss: 0.00004310
Iteration 18/1000 | Loss: 0.00004310
Iteration 19/1000 | Loss: 0.00004310
Iteration 20/1000 | Loss: 0.00004309
Iteration 21/1000 | Loss: 0.00004308
Iteration 22/1000 | Loss: 0.00004305
Iteration 23/1000 | Loss: 0.00004305
Iteration 24/1000 | Loss: 0.00004305
Iteration 25/1000 | Loss: 0.00004305
Iteration 26/1000 | Loss: 0.00004305
Iteration 27/1000 | Loss: 0.00004305
Iteration 28/1000 | Loss: 0.00004305
Iteration 29/1000 | Loss: 0.00004305
Iteration 30/1000 | Loss: 0.00004303
Iteration 31/1000 | Loss: 0.00004303
Iteration 32/1000 | Loss: 0.00004303
Iteration 33/1000 | Loss: 0.00004303
Iteration 34/1000 | Loss: 0.00004303
Iteration 35/1000 | Loss: 0.00004303
Iteration 36/1000 | Loss: 0.00004303
Iteration 37/1000 | Loss: 0.00004303
Iteration 38/1000 | Loss: 0.00004303
Iteration 39/1000 | Loss: 0.00004303
Iteration 40/1000 | Loss: 0.00004303
Iteration 41/1000 | Loss: 0.00004303
Iteration 42/1000 | Loss: 0.00004302
Iteration 43/1000 | Loss: 0.00004302
Iteration 44/1000 | Loss: 0.00004302
Iteration 45/1000 | Loss: 0.00004302
Iteration 46/1000 | Loss: 0.00004302
Iteration 47/1000 | Loss: 0.00004302
Iteration 48/1000 | Loss: 0.00004302
Iteration 49/1000 | Loss: 0.00004302
Iteration 50/1000 | Loss: 0.00004302
Iteration 51/1000 | Loss: 0.00004302
Iteration 52/1000 | Loss: 0.00004302
Iteration 53/1000 | Loss: 0.00004302
Iteration 54/1000 | Loss: 0.00004302
Iteration 55/1000 | Loss: 0.00004302
Iteration 56/1000 | Loss: 0.00004301
Iteration 57/1000 | Loss: 0.00004301
Iteration 58/1000 | Loss: 0.00004300
Iteration 59/1000 | Loss: 0.00004300
Iteration 60/1000 | Loss: 0.00004300
Iteration 61/1000 | Loss: 0.00004299
Iteration 62/1000 | Loss: 0.00004299
Iteration 63/1000 | Loss: 0.00004298
Iteration 64/1000 | Loss: 0.00004298
Iteration 65/1000 | Loss: 0.00004298
Iteration 66/1000 | Loss: 0.00004298
Iteration 67/1000 | Loss: 0.00004298
Iteration 68/1000 | Loss: 0.00004298
Iteration 69/1000 | Loss: 0.00004298
Iteration 70/1000 | Loss: 0.00004298
Iteration 71/1000 | Loss: 0.00004298
Iteration 72/1000 | Loss: 0.00004297
Iteration 73/1000 | Loss: 0.00004297
Iteration 74/1000 | Loss: 0.00004297
Iteration 75/1000 | Loss: 0.00004297
Iteration 76/1000 | Loss: 0.00004296
Iteration 77/1000 | Loss: 0.00004296
Iteration 78/1000 | Loss: 0.00004296
Iteration 79/1000 | Loss: 0.00004295
Iteration 80/1000 | Loss: 0.00004295
Iteration 81/1000 | Loss: 0.00004295
Iteration 82/1000 | Loss: 0.00004295
Iteration 83/1000 | Loss: 0.00004295
Iteration 84/1000 | Loss: 0.00004294
Iteration 85/1000 | Loss: 0.00004294
Iteration 86/1000 | Loss: 0.00004294
Iteration 87/1000 | Loss: 0.00004294
Iteration 88/1000 | Loss: 0.00004294
Iteration 89/1000 | Loss: 0.00004294
Iteration 90/1000 | Loss: 0.00004294
Iteration 91/1000 | Loss: 0.00004294
Iteration 92/1000 | Loss: 0.00004294
Iteration 93/1000 | Loss: 0.00004294
Iteration 94/1000 | Loss: 0.00004294
Iteration 95/1000 | Loss: 0.00004293
Iteration 96/1000 | Loss: 0.00004293
Iteration 97/1000 | Loss: 0.00004293
Iteration 98/1000 | Loss: 0.00004293
Iteration 99/1000 | Loss: 0.00004293
Iteration 100/1000 | Loss: 0.00004293
Iteration 101/1000 | Loss: 0.00004293
Iteration 102/1000 | Loss: 0.00004293
Iteration 103/1000 | Loss: 0.00004293
Iteration 104/1000 | Loss: 0.00004293
Iteration 105/1000 | Loss: 0.00004293
Iteration 106/1000 | Loss: 0.00004293
Iteration 107/1000 | Loss: 0.00004293
Iteration 108/1000 | Loss: 0.00004293
Iteration 109/1000 | Loss: 0.00004293
Iteration 110/1000 | Loss: 0.00004293
Iteration 111/1000 | Loss: 0.00004292
Iteration 112/1000 | Loss: 0.00004292
Iteration 113/1000 | Loss: 0.00004292
Iteration 114/1000 | Loss: 0.00004292
Iteration 115/1000 | Loss: 0.00004292
Iteration 116/1000 | Loss: 0.00004292
Iteration 117/1000 | Loss: 0.00004292
Iteration 118/1000 | Loss: 0.00004292
Iteration 119/1000 | Loss: 0.00004292
Iteration 120/1000 | Loss: 0.00004292
Iteration 121/1000 | Loss: 0.00004292
Iteration 122/1000 | Loss: 0.00004292
Iteration 123/1000 | Loss: 0.00004292
Iteration 124/1000 | Loss: 0.00004292
Iteration 125/1000 | Loss: 0.00004292
Iteration 126/1000 | Loss: 0.00004292
Iteration 127/1000 | Loss: 0.00004292
Iteration 128/1000 | Loss: 0.00004292
Iteration 129/1000 | Loss: 0.00004292
Iteration 130/1000 | Loss: 0.00004292
Iteration 131/1000 | Loss: 0.00004292
Iteration 132/1000 | Loss: 0.00004292
Iteration 133/1000 | Loss: 0.00004292
Iteration 134/1000 | Loss: 0.00004292
Iteration 135/1000 | Loss: 0.00004292
Iteration 136/1000 | Loss: 0.00004292
Iteration 137/1000 | Loss: 0.00004292
Iteration 138/1000 | Loss: 0.00004292
Iteration 139/1000 | Loss: 0.00004292
Iteration 140/1000 | Loss: 0.00004292
Iteration 141/1000 | Loss: 0.00004292
Iteration 142/1000 | Loss: 0.00004292
Iteration 143/1000 | Loss: 0.00004292
Iteration 144/1000 | Loss: 0.00004292
Iteration 145/1000 | Loss: 0.00004292
Iteration 146/1000 | Loss: 0.00004292
Iteration 147/1000 | Loss: 0.00004292
Iteration 148/1000 | Loss: 0.00004292
Iteration 149/1000 | Loss: 0.00004292
Iteration 150/1000 | Loss: 0.00004292
Iteration 151/1000 | Loss: 0.00004292
Iteration 152/1000 | Loss: 0.00004292
Iteration 153/1000 | Loss: 0.00004292
Iteration 154/1000 | Loss: 0.00004292
Iteration 155/1000 | Loss: 0.00004292
Iteration 156/1000 | Loss: 0.00004292
Iteration 157/1000 | Loss: 0.00004292
Iteration 158/1000 | Loss: 0.00004292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [4.29176798206754e-05, 4.29176798206754e-05, 4.29176798206754e-05, 4.29176798206754e-05, 4.29176798206754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.29176798206754e-05

Optimization complete. Final v2v error: 5.540127754211426 mm

Highest mean error: 5.747101783752441 mm for frame 26

Lowest mean error: 5.442021369934082 mm for frame 69

Saving results

Total time: 41.31073617935181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103536
Iteration 2/25 | Loss: 0.00298029
Iteration 3/25 | Loss: 0.00202906
Iteration 4/25 | Loss: 0.00167179
Iteration 5/25 | Loss: 0.00149581
Iteration 6/25 | Loss: 0.00134308
Iteration 7/25 | Loss: 0.00124816
Iteration 8/25 | Loss: 0.00120719
Iteration 9/25 | Loss: 0.00118038
Iteration 10/25 | Loss: 0.00113678
Iteration 11/25 | Loss: 0.00112451
Iteration 12/25 | Loss: 0.00109532
Iteration 13/25 | Loss: 0.00108050
Iteration 14/25 | Loss: 0.00107209
Iteration 15/25 | Loss: 0.00109668
Iteration 16/25 | Loss: 0.00107743
Iteration 17/25 | Loss: 0.00105458
Iteration 18/25 | Loss: 0.00104132
Iteration 19/25 | Loss: 0.00102094
Iteration 20/25 | Loss: 0.00100987
Iteration 21/25 | Loss: 0.00100428
Iteration 22/25 | Loss: 0.00100402
Iteration 23/25 | Loss: 0.00099656
Iteration 24/25 | Loss: 0.00099133
Iteration 25/25 | Loss: 0.00098652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48592103
Iteration 2/25 | Loss: 0.00467561
Iteration 3/25 | Loss: 0.00221586
Iteration 4/25 | Loss: 0.00221586
Iteration 5/25 | Loss: 0.00221586
Iteration 6/25 | Loss: 0.00221586
Iteration 7/25 | Loss: 0.00221586
Iteration 8/25 | Loss: 0.00221586
Iteration 9/25 | Loss: 0.00221586
Iteration 10/25 | Loss: 0.00221586
Iteration 11/25 | Loss: 0.00221586
Iteration 12/25 | Loss: 0.00221586
Iteration 13/25 | Loss: 0.00221586
Iteration 14/25 | Loss: 0.00221586
Iteration 15/25 | Loss: 0.00221586
Iteration 16/25 | Loss: 0.00221586
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022158620413392782, 0.0022158620413392782, 0.0022158620413392782, 0.0022158620413392782, 0.0022158620413392782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022158620413392782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221586
Iteration 2/1000 | Loss: 0.00059352
Iteration 3/1000 | Loss: 0.00049038
Iteration 4/1000 | Loss: 0.00411060
Iteration 5/1000 | Loss: 0.00079909
Iteration 6/1000 | Loss: 0.00146420
Iteration 7/1000 | Loss: 0.00099387
Iteration 8/1000 | Loss: 0.00328071
Iteration 9/1000 | Loss: 0.00055465
Iteration 10/1000 | Loss: 0.00109607
Iteration 11/1000 | Loss: 0.00476599
Iteration 12/1000 | Loss: 0.00108348
Iteration 13/1000 | Loss: 0.00129935
Iteration 14/1000 | Loss: 0.00162762
Iteration 15/1000 | Loss: 0.00467721
Iteration 16/1000 | Loss: 0.00126180
Iteration 17/1000 | Loss: 0.00080655
Iteration 18/1000 | Loss: 0.00219943
Iteration 19/1000 | Loss: 0.00147580
Iteration 20/1000 | Loss: 0.00173392
Iteration 21/1000 | Loss: 0.00049000
Iteration 22/1000 | Loss: 0.00019418
Iteration 23/1000 | Loss: 0.00009532
Iteration 24/1000 | Loss: 0.00008254
Iteration 25/1000 | Loss: 0.00091875
Iteration 26/1000 | Loss: 0.00027786
Iteration 27/1000 | Loss: 0.00133497
Iteration 28/1000 | Loss: 0.00023549
Iteration 29/1000 | Loss: 0.00018527
Iteration 30/1000 | Loss: 0.00017802
Iteration 31/1000 | Loss: 0.00016712
Iteration 32/1000 | Loss: 0.00014168
Iteration 33/1000 | Loss: 0.00053815
Iteration 34/1000 | Loss: 0.00037267
Iteration 35/1000 | Loss: 0.00075920
Iteration 36/1000 | Loss: 0.00052597
Iteration 37/1000 | Loss: 0.00084290
Iteration 38/1000 | Loss: 0.00119751
Iteration 39/1000 | Loss: 0.00168222
Iteration 40/1000 | Loss: 0.00097723
Iteration 41/1000 | Loss: 0.00116630
Iteration 42/1000 | Loss: 0.00147656
Iteration 43/1000 | Loss: 0.00032474
Iteration 44/1000 | Loss: 0.00013558
Iteration 45/1000 | Loss: 0.00007302
Iteration 46/1000 | Loss: 0.00006638
Iteration 47/1000 | Loss: 0.00022939
Iteration 48/1000 | Loss: 0.00007185
Iteration 49/1000 | Loss: 0.00006104
Iteration 50/1000 | Loss: 0.00005672
Iteration 51/1000 | Loss: 0.00023757
Iteration 52/1000 | Loss: 0.00005380
Iteration 53/1000 | Loss: 0.00005240
Iteration 54/1000 | Loss: 0.00005130
Iteration 55/1000 | Loss: 0.00005063
Iteration 56/1000 | Loss: 0.00102544
Iteration 57/1000 | Loss: 0.00006462
Iteration 58/1000 | Loss: 0.00005123
Iteration 59/1000 | Loss: 0.00004788
Iteration 60/1000 | Loss: 0.00004686
Iteration 61/1000 | Loss: 0.00041158
Iteration 62/1000 | Loss: 0.00004595
Iteration 63/1000 | Loss: 0.00040305
Iteration 64/1000 | Loss: 0.00005604
Iteration 65/1000 | Loss: 0.00004691
Iteration 66/1000 | Loss: 0.00004325
Iteration 67/1000 | Loss: 0.00004230
Iteration 68/1000 | Loss: 0.00004162
Iteration 69/1000 | Loss: 0.00004098
Iteration 70/1000 | Loss: 0.00004066
Iteration 71/1000 | Loss: 0.00004047
Iteration 72/1000 | Loss: 0.00004026
Iteration 73/1000 | Loss: 0.00004019
Iteration 74/1000 | Loss: 0.00004007
Iteration 75/1000 | Loss: 0.00004002
Iteration 76/1000 | Loss: 0.00004002
Iteration 77/1000 | Loss: 0.00004001
Iteration 78/1000 | Loss: 0.00004000
Iteration 79/1000 | Loss: 0.00003993
Iteration 80/1000 | Loss: 0.00003991
Iteration 81/1000 | Loss: 0.00003985
Iteration 82/1000 | Loss: 0.00003979
Iteration 83/1000 | Loss: 0.00003979
Iteration 84/1000 | Loss: 0.00003977
Iteration 85/1000 | Loss: 0.00003976
Iteration 86/1000 | Loss: 0.00003976
Iteration 87/1000 | Loss: 0.00003975
Iteration 88/1000 | Loss: 0.00003971
Iteration 89/1000 | Loss: 0.00003969
Iteration 90/1000 | Loss: 0.00003967
Iteration 91/1000 | Loss: 0.00003966
Iteration 92/1000 | Loss: 0.00003965
Iteration 93/1000 | Loss: 0.00003962
Iteration 94/1000 | Loss: 0.00003959
Iteration 95/1000 | Loss: 0.00003959
Iteration 96/1000 | Loss: 0.00003958
Iteration 97/1000 | Loss: 0.00003958
Iteration 98/1000 | Loss: 0.00003957
Iteration 99/1000 | Loss: 0.00003957
Iteration 100/1000 | Loss: 0.00003957
Iteration 101/1000 | Loss: 0.00003956
Iteration 102/1000 | Loss: 0.00003955
Iteration 103/1000 | Loss: 0.00003955
Iteration 104/1000 | Loss: 0.00003954
Iteration 105/1000 | Loss: 0.00003954
Iteration 106/1000 | Loss: 0.00003954
Iteration 107/1000 | Loss: 0.00003954
Iteration 108/1000 | Loss: 0.00003954
Iteration 109/1000 | Loss: 0.00003954
Iteration 110/1000 | Loss: 0.00003954
Iteration 111/1000 | Loss: 0.00003954
Iteration 112/1000 | Loss: 0.00003954
Iteration 113/1000 | Loss: 0.00003954
Iteration 114/1000 | Loss: 0.00003954
Iteration 115/1000 | Loss: 0.00003954
Iteration 116/1000 | Loss: 0.00003953
Iteration 117/1000 | Loss: 0.00003953
Iteration 118/1000 | Loss: 0.00003953
Iteration 119/1000 | Loss: 0.00003951
Iteration 120/1000 | Loss: 0.00003951
Iteration 121/1000 | Loss: 0.00003950
Iteration 122/1000 | Loss: 0.00003950
Iteration 123/1000 | Loss: 0.00003949
Iteration 124/1000 | Loss: 0.00003949
Iteration 125/1000 | Loss: 0.00003948
Iteration 126/1000 | Loss: 0.00003948
Iteration 127/1000 | Loss: 0.00003947
Iteration 128/1000 | Loss: 0.00003947
Iteration 129/1000 | Loss: 0.00003947
Iteration 130/1000 | Loss: 0.00003946
Iteration 131/1000 | Loss: 0.00003944
Iteration 132/1000 | Loss: 0.00003944
Iteration 133/1000 | Loss: 0.00003943
Iteration 134/1000 | Loss: 0.00028572
Iteration 135/1000 | Loss: 0.00028301
Iteration 136/1000 | Loss: 0.00056231
Iteration 137/1000 | Loss: 0.00007713
Iteration 138/1000 | Loss: 0.00005751
Iteration 139/1000 | Loss: 0.00009943
Iteration 140/1000 | Loss: 0.00008955
Iteration 141/1000 | Loss: 0.00009228
Iteration 142/1000 | Loss: 0.00007617
Iteration 143/1000 | Loss: 0.00006855
Iteration 144/1000 | Loss: 0.00009891
Iteration 145/1000 | Loss: 0.00009881
Iteration 146/1000 | Loss: 0.00016461
Iteration 147/1000 | Loss: 0.00005100
Iteration 148/1000 | Loss: 0.00004659
Iteration 149/1000 | Loss: 0.00004484
Iteration 150/1000 | Loss: 0.00004366
Iteration 151/1000 | Loss: 0.00012045
Iteration 152/1000 | Loss: 0.00005086
Iteration 153/1000 | Loss: 0.00004433
Iteration 154/1000 | Loss: 0.00004379
Iteration 155/1000 | Loss: 0.00004063
Iteration 156/1000 | Loss: 0.00004005
Iteration 157/1000 | Loss: 0.00105036
Iteration 158/1000 | Loss: 0.00004731
Iteration 159/1000 | Loss: 0.00003850
Iteration 160/1000 | Loss: 0.00003715
Iteration 161/1000 | Loss: 0.00003665
Iteration 162/1000 | Loss: 0.00003624
Iteration 163/1000 | Loss: 0.00003607
Iteration 164/1000 | Loss: 0.00003606
Iteration 165/1000 | Loss: 0.00003606
Iteration 166/1000 | Loss: 0.00003606
Iteration 167/1000 | Loss: 0.00003606
Iteration 168/1000 | Loss: 0.00003604
Iteration 169/1000 | Loss: 0.00003604
Iteration 170/1000 | Loss: 0.00003604
Iteration 171/1000 | Loss: 0.00003603
Iteration 172/1000 | Loss: 0.00003603
Iteration 173/1000 | Loss: 0.00003602
Iteration 174/1000 | Loss: 0.00003601
Iteration 175/1000 | Loss: 0.00003600
Iteration 176/1000 | Loss: 0.00003600
Iteration 177/1000 | Loss: 0.00003599
Iteration 178/1000 | Loss: 0.00003596
Iteration 179/1000 | Loss: 0.00003595
Iteration 180/1000 | Loss: 0.00003595
Iteration 181/1000 | Loss: 0.00003595
Iteration 182/1000 | Loss: 0.00003594
Iteration 183/1000 | Loss: 0.00003594
Iteration 184/1000 | Loss: 0.00003594
Iteration 185/1000 | Loss: 0.00003593
Iteration 186/1000 | Loss: 0.00003593
Iteration 187/1000 | Loss: 0.00003593
Iteration 188/1000 | Loss: 0.00003593
Iteration 189/1000 | Loss: 0.00003592
Iteration 190/1000 | Loss: 0.00003592
Iteration 191/1000 | Loss: 0.00003592
Iteration 192/1000 | Loss: 0.00003592
Iteration 193/1000 | Loss: 0.00003591
Iteration 194/1000 | Loss: 0.00003591
Iteration 195/1000 | Loss: 0.00003591
Iteration 196/1000 | Loss: 0.00003591
Iteration 197/1000 | Loss: 0.00003591
Iteration 198/1000 | Loss: 0.00003591
Iteration 199/1000 | Loss: 0.00003591
Iteration 200/1000 | Loss: 0.00003590
Iteration 201/1000 | Loss: 0.00003590
Iteration 202/1000 | Loss: 0.00003590
Iteration 203/1000 | Loss: 0.00003590
Iteration 204/1000 | Loss: 0.00003589
Iteration 205/1000 | Loss: 0.00003589
Iteration 206/1000 | Loss: 0.00003589
Iteration 207/1000 | Loss: 0.00003589
Iteration 208/1000 | Loss: 0.00003588
Iteration 209/1000 | Loss: 0.00003588
Iteration 210/1000 | Loss: 0.00003588
Iteration 211/1000 | Loss: 0.00003588
Iteration 212/1000 | Loss: 0.00003588
Iteration 213/1000 | Loss: 0.00003588
Iteration 214/1000 | Loss: 0.00003588
Iteration 215/1000 | Loss: 0.00003587
Iteration 216/1000 | Loss: 0.00003587
Iteration 217/1000 | Loss: 0.00003587
Iteration 218/1000 | Loss: 0.00003586
Iteration 219/1000 | Loss: 0.00003586
Iteration 220/1000 | Loss: 0.00003586
Iteration 221/1000 | Loss: 0.00003586
Iteration 222/1000 | Loss: 0.00003585
Iteration 223/1000 | Loss: 0.00003585
Iteration 224/1000 | Loss: 0.00003585
Iteration 225/1000 | Loss: 0.00003585
Iteration 226/1000 | Loss: 0.00003584
Iteration 227/1000 | Loss: 0.00003584
Iteration 228/1000 | Loss: 0.00003584
Iteration 229/1000 | Loss: 0.00003584
Iteration 230/1000 | Loss: 0.00003584
Iteration 231/1000 | Loss: 0.00003584
Iteration 232/1000 | Loss: 0.00003583
Iteration 233/1000 | Loss: 0.00003583
Iteration 234/1000 | Loss: 0.00003583
Iteration 235/1000 | Loss: 0.00003582
Iteration 236/1000 | Loss: 0.00003582
Iteration 237/1000 | Loss: 0.00003582
Iteration 238/1000 | Loss: 0.00003582
Iteration 239/1000 | Loss: 0.00003581
Iteration 240/1000 | Loss: 0.00003581
Iteration 241/1000 | Loss: 0.00003581
Iteration 242/1000 | Loss: 0.00003581
Iteration 243/1000 | Loss: 0.00003581
Iteration 244/1000 | Loss: 0.00003581
Iteration 245/1000 | Loss: 0.00003581
Iteration 246/1000 | Loss: 0.00003581
Iteration 247/1000 | Loss: 0.00003581
Iteration 248/1000 | Loss: 0.00003581
Iteration 249/1000 | Loss: 0.00003581
Iteration 250/1000 | Loss: 0.00003580
Iteration 251/1000 | Loss: 0.00003580
Iteration 252/1000 | Loss: 0.00003580
Iteration 253/1000 | Loss: 0.00003579
Iteration 254/1000 | Loss: 0.00003579
Iteration 255/1000 | Loss: 0.00003579
Iteration 256/1000 | Loss: 0.00003578
Iteration 257/1000 | Loss: 0.00003578
Iteration 258/1000 | Loss: 0.00003578
Iteration 259/1000 | Loss: 0.00003577
Iteration 260/1000 | Loss: 0.00003577
Iteration 261/1000 | Loss: 0.00003577
Iteration 262/1000 | Loss: 0.00003577
Iteration 263/1000 | Loss: 0.00003577
Iteration 264/1000 | Loss: 0.00003577
Iteration 265/1000 | Loss: 0.00003577
Iteration 266/1000 | Loss: 0.00003577
Iteration 267/1000 | Loss: 0.00003576
Iteration 268/1000 | Loss: 0.00003576
Iteration 269/1000 | Loss: 0.00003576
Iteration 270/1000 | Loss: 0.00003576
Iteration 271/1000 | Loss: 0.00003576
Iteration 272/1000 | Loss: 0.00003576
Iteration 273/1000 | Loss: 0.00003576
Iteration 274/1000 | Loss: 0.00003576
Iteration 275/1000 | Loss: 0.00003576
Iteration 276/1000 | Loss: 0.00003576
Iteration 277/1000 | Loss: 0.00003576
Iteration 278/1000 | Loss: 0.00003576
Iteration 279/1000 | Loss: 0.00003576
Iteration 280/1000 | Loss: 0.00003576
Iteration 281/1000 | Loss: 0.00003576
Iteration 282/1000 | Loss: 0.00003576
Iteration 283/1000 | Loss: 0.00003576
Iteration 284/1000 | Loss: 0.00003576
Iteration 285/1000 | Loss: 0.00003576
Iteration 286/1000 | Loss: 0.00003576
Iteration 287/1000 | Loss: 0.00003576
Iteration 288/1000 | Loss: 0.00003576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [3.576256858650595e-05, 3.576256858650595e-05, 3.576256858650595e-05, 3.576256858650595e-05, 3.576256858650595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.576256858650595e-05

Optimization complete. Final v2v error: 4.225545406341553 mm

Highest mean error: 14.870782852172852 mm for frame 140

Lowest mean error: 3.2602672576904297 mm for frame 217

Saving results

Total time: 237.47037172317505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997132
Iteration 2/25 | Loss: 0.00248558
Iteration 3/25 | Loss: 0.00261717
Iteration 4/25 | Loss: 0.00129540
Iteration 5/25 | Loss: 0.00111543
Iteration 6/25 | Loss: 0.00101334
Iteration 7/25 | Loss: 0.00100156
Iteration 8/25 | Loss: 0.00099865
Iteration 9/25 | Loss: 0.00100281
Iteration 10/25 | Loss: 0.00099241
Iteration 11/25 | Loss: 0.00098809
Iteration 12/25 | Loss: 0.00098652
Iteration 13/25 | Loss: 0.00098583
Iteration 14/25 | Loss: 0.00098467
Iteration 15/25 | Loss: 0.00098382
Iteration 16/25 | Loss: 0.00098363
Iteration 17/25 | Loss: 0.00098362
Iteration 18/25 | Loss: 0.00098362
Iteration 19/25 | Loss: 0.00098362
Iteration 20/25 | Loss: 0.00098362
Iteration 21/25 | Loss: 0.00098362
Iteration 22/25 | Loss: 0.00098362
Iteration 23/25 | Loss: 0.00098361
Iteration 24/25 | Loss: 0.00098361
Iteration 25/25 | Loss: 0.00098361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49106336
Iteration 2/25 | Loss: 0.00067461
Iteration 3/25 | Loss: 0.00067460
Iteration 4/25 | Loss: 0.00067460
Iteration 5/25 | Loss: 0.00067460
Iteration 6/25 | Loss: 0.00067460
Iteration 7/25 | Loss: 0.00067460
Iteration 8/25 | Loss: 0.00067460
Iteration 9/25 | Loss: 0.00067460
Iteration 10/25 | Loss: 0.00067460
Iteration 11/25 | Loss: 0.00067460
Iteration 12/25 | Loss: 0.00067460
Iteration 13/25 | Loss: 0.00067460
Iteration 14/25 | Loss: 0.00067460
Iteration 15/25 | Loss: 0.00067460
Iteration 16/25 | Loss: 0.00067460
Iteration 17/25 | Loss: 0.00067460
Iteration 18/25 | Loss: 0.00067460
Iteration 19/25 | Loss: 0.00067460
Iteration 20/25 | Loss: 0.00067460
Iteration 21/25 | Loss: 0.00067460
Iteration 22/25 | Loss: 0.00067460
Iteration 23/25 | Loss: 0.00067460
Iteration 24/25 | Loss: 0.00067460
Iteration 25/25 | Loss: 0.00067460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067460
Iteration 2/1000 | Loss: 0.00006316
Iteration 3/1000 | Loss: 0.00004449
Iteration 4/1000 | Loss: 0.00003887
Iteration 5/1000 | Loss: 0.00003621
Iteration 6/1000 | Loss: 0.00003447
Iteration 7/1000 | Loss: 0.00003357
Iteration 8/1000 | Loss: 0.00003276
Iteration 9/1000 | Loss: 0.00003215
Iteration 10/1000 | Loss: 0.00003182
Iteration 11/1000 | Loss: 0.00003161
Iteration 12/1000 | Loss: 0.00003156
Iteration 13/1000 | Loss: 0.00003152
Iteration 14/1000 | Loss: 0.00003148
Iteration 15/1000 | Loss: 0.00003146
Iteration 16/1000 | Loss: 0.00003137
Iteration 17/1000 | Loss: 0.00003137
Iteration 18/1000 | Loss: 0.00003136
Iteration 19/1000 | Loss: 0.00003136
Iteration 20/1000 | Loss: 0.00003136
Iteration 21/1000 | Loss: 0.00003135
Iteration 22/1000 | Loss: 0.00003135
Iteration 23/1000 | Loss: 0.00003135
Iteration 24/1000 | Loss: 0.00003134
Iteration 25/1000 | Loss: 0.00003134
Iteration 26/1000 | Loss: 0.00003134
Iteration 27/1000 | Loss: 0.00003134
Iteration 28/1000 | Loss: 0.00003134
Iteration 29/1000 | Loss: 0.00003134
Iteration 30/1000 | Loss: 0.00003133
Iteration 31/1000 | Loss: 0.00003133
Iteration 32/1000 | Loss: 0.00003133
Iteration 33/1000 | Loss: 0.00003133
Iteration 34/1000 | Loss: 0.00003133
Iteration 35/1000 | Loss: 0.00003133
Iteration 36/1000 | Loss: 0.00003133
Iteration 37/1000 | Loss: 0.00003133
Iteration 38/1000 | Loss: 0.00003133
Iteration 39/1000 | Loss: 0.00003133
Iteration 40/1000 | Loss: 0.00003133
Iteration 41/1000 | Loss: 0.00003133
Iteration 42/1000 | Loss: 0.00003133
Iteration 43/1000 | Loss: 0.00003133
Iteration 44/1000 | Loss: 0.00003132
Iteration 45/1000 | Loss: 0.00003132
Iteration 46/1000 | Loss: 0.00003132
Iteration 47/1000 | Loss: 0.00003131
Iteration 48/1000 | Loss: 0.00003131
Iteration 49/1000 | Loss: 0.00003131
Iteration 50/1000 | Loss: 0.00003131
Iteration 51/1000 | Loss: 0.00003130
Iteration 52/1000 | Loss: 0.00003130
Iteration 53/1000 | Loss: 0.00003130
Iteration 54/1000 | Loss: 0.00003130
Iteration 55/1000 | Loss: 0.00003130
Iteration 56/1000 | Loss: 0.00003130
Iteration 57/1000 | Loss: 0.00003130
Iteration 58/1000 | Loss: 0.00003130
Iteration 59/1000 | Loss: 0.00003130
Iteration 60/1000 | Loss: 0.00003130
Iteration 61/1000 | Loss: 0.00003130
Iteration 62/1000 | Loss: 0.00003129
Iteration 63/1000 | Loss: 0.00003129
Iteration 64/1000 | Loss: 0.00003129
Iteration 65/1000 | Loss: 0.00003129
Iteration 66/1000 | Loss: 0.00003129
Iteration 67/1000 | Loss: 0.00003129
Iteration 68/1000 | Loss: 0.00003129
Iteration 69/1000 | Loss: 0.00003129
Iteration 70/1000 | Loss: 0.00003129
Iteration 71/1000 | Loss: 0.00003129
Iteration 72/1000 | Loss: 0.00003128
Iteration 73/1000 | Loss: 0.00003128
Iteration 74/1000 | Loss: 0.00003128
Iteration 75/1000 | Loss: 0.00003128
Iteration 76/1000 | Loss: 0.00003128
Iteration 77/1000 | Loss: 0.00003128
Iteration 78/1000 | Loss: 0.00003128
Iteration 79/1000 | Loss: 0.00003128
Iteration 80/1000 | Loss: 0.00003128
Iteration 81/1000 | Loss: 0.00003128
Iteration 82/1000 | Loss: 0.00003128
Iteration 83/1000 | Loss: 0.00003128
Iteration 84/1000 | Loss: 0.00003128
Iteration 85/1000 | Loss: 0.00003128
Iteration 86/1000 | Loss: 0.00003128
Iteration 87/1000 | Loss: 0.00003128
Iteration 88/1000 | Loss: 0.00003128
Iteration 89/1000 | Loss: 0.00003127
Iteration 90/1000 | Loss: 0.00003127
Iteration 91/1000 | Loss: 0.00003127
Iteration 92/1000 | Loss: 0.00003127
Iteration 93/1000 | Loss: 0.00003127
Iteration 94/1000 | Loss: 0.00003127
Iteration 95/1000 | Loss: 0.00003127
Iteration 96/1000 | Loss: 0.00003127
Iteration 97/1000 | Loss: 0.00003127
Iteration 98/1000 | Loss: 0.00003127
Iteration 99/1000 | Loss: 0.00003127
Iteration 100/1000 | Loss: 0.00003127
Iteration 101/1000 | Loss: 0.00003127
Iteration 102/1000 | Loss: 0.00003127
Iteration 103/1000 | Loss: 0.00003127
Iteration 104/1000 | Loss: 0.00003127
Iteration 105/1000 | Loss: 0.00003127
Iteration 106/1000 | Loss: 0.00003127
Iteration 107/1000 | Loss: 0.00003127
Iteration 108/1000 | Loss: 0.00003127
Iteration 109/1000 | Loss: 0.00003127
Iteration 110/1000 | Loss: 0.00003127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.127317540929653e-05, 3.127317540929653e-05, 3.127317540929653e-05, 3.127317540929653e-05, 3.127317540929653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.127317540929653e-05

Optimization complete. Final v2v error: 4.662405490875244 mm

Highest mean error: 6.420942783355713 mm for frame 69

Lowest mean error: 3.963416337966919 mm for frame 95

Saving results

Total time: 49.82722449302673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536134
Iteration 2/25 | Loss: 0.00114170
Iteration 3/25 | Loss: 0.00097926
Iteration 4/25 | Loss: 0.00096701
Iteration 5/25 | Loss: 0.00096319
Iteration 6/25 | Loss: 0.00096276
Iteration 7/25 | Loss: 0.00096276
Iteration 8/25 | Loss: 0.00096276
Iteration 9/25 | Loss: 0.00096276
Iteration 10/25 | Loss: 0.00096276
Iteration 11/25 | Loss: 0.00096276
Iteration 12/25 | Loss: 0.00096276
Iteration 13/25 | Loss: 0.00096276
Iteration 14/25 | Loss: 0.00096276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009627643739804626, 0.0009627643739804626, 0.0009627643739804626, 0.0009627643739804626, 0.0009627643739804626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009627643739804626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87309974
Iteration 2/25 | Loss: 0.00067794
Iteration 3/25 | Loss: 0.00067793
Iteration 4/25 | Loss: 0.00067793
Iteration 5/25 | Loss: 0.00067793
Iteration 6/25 | Loss: 0.00067793
Iteration 7/25 | Loss: 0.00067793
Iteration 8/25 | Loss: 0.00067793
Iteration 9/25 | Loss: 0.00067793
Iteration 10/25 | Loss: 0.00067793
Iteration 11/25 | Loss: 0.00067793
Iteration 12/25 | Loss: 0.00067793
Iteration 13/25 | Loss: 0.00067793
Iteration 14/25 | Loss: 0.00067793
Iteration 15/25 | Loss: 0.00067793
Iteration 16/25 | Loss: 0.00067793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006779295508749783, 0.0006779295508749783, 0.0006779295508749783, 0.0006779295508749783, 0.0006779295508749783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006779295508749783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067793
Iteration 2/1000 | Loss: 0.00003884
Iteration 3/1000 | Loss: 0.00003243
Iteration 4/1000 | Loss: 0.00002921
Iteration 5/1000 | Loss: 0.00002773
Iteration 6/1000 | Loss: 0.00002721
Iteration 7/1000 | Loss: 0.00002681
Iteration 8/1000 | Loss: 0.00002667
Iteration 9/1000 | Loss: 0.00002639
Iteration 10/1000 | Loss: 0.00002610
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00002603
Iteration 13/1000 | Loss: 0.00002602
Iteration 14/1000 | Loss: 0.00002599
Iteration 15/1000 | Loss: 0.00002599
Iteration 16/1000 | Loss: 0.00002594
Iteration 17/1000 | Loss: 0.00002593
Iteration 18/1000 | Loss: 0.00002593
Iteration 19/1000 | Loss: 0.00002593
Iteration 20/1000 | Loss: 0.00002592
Iteration 21/1000 | Loss: 0.00002591
Iteration 22/1000 | Loss: 0.00002590
Iteration 23/1000 | Loss: 0.00002590
Iteration 24/1000 | Loss: 0.00002589
Iteration 25/1000 | Loss: 0.00002589
Iteration 26/1000 | Loss: 0.00002589
Iteration 27/1000 | Loss: 0.00002587
Iteration 28/1000 | Loss: 0.00002587
Iteration 29/1000 | Loss: 0.00002587
Iteration 30/1000 | Loss: 0.00002587
Iteration 31/1000 | Loss: 0.00002587
Iteration 32/1000 | Loss: 0.00002587
Iteration 33/1000 | Loss: 0.00002585
Iteration 34/1000 | Loss: 0.00002585
Iteration 35/1000 | Loss: 0.00002585
Iteration 36/1000 | Loss: 0.00002585
Iteration 37/1000 | Loss: 0.00002584
Iteration 38/1000 | Loss: 0.00002584
Iteration 39/1000 | Loss: 0.00002584
Iteration 40/1000 | Loss: 0.00002584
Iteration 41/1000 | Loss: 0.00002584
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002584
Iteration 45/1000 | Loss: 0.00002584
Iteration 46/1000 | Loss: 0.00002584
Iteration 47/1000 | Loss: 0.00002583
Iteration 48/1000 | Loss: 0.00002583
Iteration 49/1000 | Loss: 0.00002583
Iteration 50/1000 | Loss: 0.00002583
Iteration 51/1000 | Loss: 0.00002583
Iteration 52/1000 | Loss: 0.00002582
Iteration 53/1000 | Loss: 0.00002582
Iteration 54/1000 | Loss: 0.00002582
Iteration 55/1000 | Loss: 0.00002582
Iteration 56/1000 | Loss: 0.00002582
Iteration 57/1000 | Loss: 0.00002582
Iteration 58/1000 | Loss: 0.00002582
Iteration 59/1000 | Loss: 0.00002582
Iteration 60/1000 | Loss: 0.00002582
Iteration 61/1000 | Loss: 0.00002582
Iteration 62/1000 | Loss: 0.00002582
Iteration 63/1000 | Loss: 0.00002582
Iteration 64/1000 | Loss: 0.00002581
Iteration 65/1000 | Loss: 0.00002581
Iteration 66/1000 | Loss: 0.00002581
Iteration 67/1000 | Loss: 0.00002581
Iteration 68/1000 | Loss: 0.00002581
Iteration 69/1000 | Loss: 0.00002581
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002581
Iteration 72/1000 | Loss: 0.00002581
Iteration 73/1000 | Loss: 0.00002581
Iteration 74/1000 | Loss: 0.00002581
Iteration 75/1000 | Loss: 0.00002580
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002580
Iteration 78/1000 | Loss: 0.00002580
Iteration 79/1000 | Loss: 0.00002580
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002579
Iteration 82/1000 | Loss: 0.00002579
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002579
Iteration 85/1000 | Loss: 0.00002578
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002578
Iteration 89/1000 | Loss: 0.00002578
Iteration 90/1000 | Loss: 0.00002578
Iteration 91/1000 | Loss: 0.00002578
Iteration 92/1000 | Loss: 0.00002578
Iteration 93/1000 | Loss: 0.00002575
Iteration 94/1000 | Loss: 0.00002575
Iteration 95/1000 | Loss: 0.00002575
Iteration 96/1000 | Loss: 0.00002575
Iteration 97/1000 | Loss: 0.00002575
Iteration 98/1000 | Loss: 0.00002575
Iteration 99/1000 | Loss: 0.00002575
Iteration 100/1000 | Loss: 0.00002575
Iteration 101/1000 | Loss: 0.00002574
Iteration 102/1000 | Loss: 0.00002574
Iteration 103/1000 | Loss: 0.00002574
Iteration 104/1000 | Loss: 0.00002574
Iteration 105/1000 | Loss: 0.00002574
Iteration 106/1000 | Loss: 0.00002574
Iteration 107/1000 | Loss: 0.00002573
Iteration 108/1000 | Loss: 0.00002572
Iteration 109/1000 | Loss: 0.00002572
Iteration 110/1000 | Loss: 0.00002572
Iteration 111/1000 | Loss: 0.00002572
Iteration 112/1000 | Loss: 0.00002572
Iteration 113/1000 | Loss: 0.00002572
Iteration 114/1000 | Loss: 0.00002571
Iteration 115/1000 | Loss: 0.00002570
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002570
Iteration 119/1000 | Loss: 0.00002570
Iteration 120/1000 | Loss: 0.00002570
Iteration 121/1000 | Loss: 0.00002569
Iteration 122/1000 | Loss: 0.00002569
Iteration 123/1000 | Loss: 0.00002569
Iteration 124/1000 | Loss: 0.00002568
Iteration 125/1000 | Loss: 0.00002568
Iteration 126/1000 | Loss: 0.00002568
Iteration 127/1000 | Loss: 0.00002568
Iteration 128/1000 | Loss: 0.00002568
Iteration 129/1000 | Loss: 0.00002568
Iteration 130/1000 | Loss: 0.00002568
Iteration 131/1000 | Loss: 0.00002568
Iteration 132/1000 | Loss: 0.00002568
Iteration 133/1000 | Loss: 0.00002568
Iteration 134/1000 | Loss: 0.00002568
Iteration 135/1000 | Loss: 0.00002568
Iteration 136/1000 | Loss: 0.00002567
Iteration 137/1000 | Loss: 0.00002567
Iteration 138/1000 | Loss: 0.00002567
Iteration 139/1000 | Loss: 0.00002566
Iteration 140/1000 | Loss: 0.00002566
Iteration 141/1000 | Loss: 0.00002566
Iteration 142/1000 | Loss: 0.00002566
Iteration 143/1000 | Loss: 0.00002566
Iteration 144/1000 | Loss: 0.00002566
Iteration 145/1000 | Loss: 0.00002566
Iteration 146/1000 | Loss: 0.00002565
Iteration 147/1000 | Loss: 0.00002565
Iteration 148/1000 | Loss: 0.00002565
Iteration 149/1000 | Loss: 0.00002564
Iteration 150/1000 | Loss: 0.00002564
Iteration 151/1000 | Loss: 0.00002564
Iteration 152/1000 | Loss: 0.00002564
Iteration 153/1000 | Loss: 0.00002564
Iteration 154/1000 | Loss: 0.00002564
Iteration 155/1000 | Loss: 0.00002564
Iteration 156/1000 | Loss: 0.00002564
Iteration 157/1000 | Loss: 0.00002564
Iteration 158/1000 | Loss: 0.00002564
Iteration 159/1000 | Loss: 0.00002564
Iteration 160/1000 | Loss: 0.00002563
Iteration 161/1000 | Loss: 0.00002563
Iteration 162/1000 | Loss: 0.00002563
Iteration 163/1000 | Loss: 0.00002563
Iteration 164/1000 | Loss: 0.00002563
Iteration 165/1000 | Loss: 0.00002563
Iteration 166/1000 | Loss: 0.00002563
Iteration 167/1000 | Loss: 0.00002563
Iteration 168/1000 | Loss: 0.00002563
Iteration 169/1000 | Loss: 0.00002563
Iteration 170/1000 | Loss: 0.00002563
Iteration 171/1000 | Loss: 0.00002563
Iteration 172/1000 | Loss: 0.00002563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.5632883989601396e-05, 2.5632883989601396e-05, 2.5632883989601396e-05, 2.5632883989601396e-05, 2.5632883989601396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5632883989601396e-05

Optimization complete. Final v2v error: 4.381274700164795 mm

Highest mean error: 4.635963439941406 mm for frame 34

Lowest mean error: 4.159763336181641 mm for frame 119

Saving results

Total time: 33.67067313194275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838056
Iteration 2/25 | Loss: 0.00146662
Iteration 3/25 | Loss: 0.00100798
Iteration 4/25 | Loss: 0.00090544
Iteration 5/25 | Loss: 0.00088441
Iteration 6/25 | Loss: 0.00087857
Iteration 7/25 | Loss: 0.00087727
Iteration 8/25 | Loss: 0.00087704
Iteration 9/25 | Loss: 0.00087704
Iteration 10/25 | Loss: 0.00087704
Iteration 11/25 | Loss: 0.00087704
Iteration 12/25 | Loss: 0.00087704
Iteration 13/25 | Loss: 0.00087704
Iteration 14/25 | Loss: 0.00087704
Iteration 15/25 | Loss: 0.00087704
Iteration 16/25 | Loss: 0.00087704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008770421845838428, 0.0008770421845838428, 0.0008770421845838428, 0.0008770421845838428, 0.0008770421845838428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008770421845838428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56278610
Iteration 2/25 | Loss: 0.00066552
Iteration 3/25 | Loss: 0.00066551
Iteration 4/25 | Loss: 0.00066551
Iteration 5/25 | Loss: 0.00066551
Iteration 6/25 | Loss: 0.00066551
Iteration 7/25 | Loss: 0.00066551
Iteration 8/25 | Loss: 0.00066551
Iteration 9/25 | Loss: 0.00066551
Iteration 10/25 | Loss: 0.00066551
Iteration 11/25 | Loss: 0.00066551
Iteration 12/25 | Loss: 0.00066551
Iteration 13/25 | Loss: 0.00066551
Iteration 14/25 | Loss: 0.00066551
Iteration 15/25 | Loss: 0.00066551
Iteration 16/25 | Loss: 0.00066551
Iteration 17/25 | Loss: 0.00066551
Iteration 18/25 | Loss: 0.00066551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006655115284956992, 0.0006655115284956992, 0.0006655115284956992, 0.0006655115284956992, 0.0006655115284956992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006655115284956992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066551
Iteration 2/1000 | Loss: 0.00003789
Iteration 3/1000 | Loss: 0.00002473
Iteration 4/1000 | Loss: 0.00002041
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001749
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001615
Iteration 10/1000 | Loss: 0.00001613
Iteration 11/1000 | Loss: 0.00001597
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001579
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001564
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001562
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001561
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001556
Iteration 30/1000 | Loss: 0.00001556
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001551
Iteration 37/1000 | Loss: 0.00001550
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001549
Iteration 40/1000 | Loss: 0.00001549
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001543
Iteration 66/1000 | Loss: 0.00001543
Iteration 67/1000 | Loss: 0.00001543
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001541
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001539
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001539
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001537
Iteration 96/1000 | Loss: 0.00001537
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001534
Iteration 110/1000 | Loss: 0.00001534
Iteration 111/1000 | Loss: 0.00001534
Iteration 112/1000 | Loss: 0.00001534
Iteration 113/1000 | Loss: 0.00001534
Iteration 114/1000 | Loss: 0.00001534
Iteration 115/1000 | Loss: 0.00001534
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001533
Iteration 120/1000 | Loss: 0.00001533
Iteration 121/1000 | Loss: 0.00001532
Iteration 122/1000 | Loss: 0.00001532
Iteration 123/1000 | Loss: 0.00001532
Iteration 124/1000 | Loss: 0.00001532
Iteration 125/1000 | Loss: 0.00001532
Iteration 126/1000 | Loss: 0.00001532
Iteration 127/1000 | Loss: 0.00001532
Iteration 128/1000 | Loss: 0.00001531
Iteration 129/1000 | Loss: 0.00001531
Iteration 130/1000 | Loss: 0.00001531
Iteration 131/1000 | Loss: 0.00001531
Iteration 132/1000 | Loss: 0.00001531
Iteration 133/1000 | Loss: 0.00001531
Iteration 134/1000 | Loss: 0.00001531
Iteration 135/1000 | Loss: 0.00001531
Iteration 136/1000 | Loss: 0.00001531
Iteration 137/1000 | Loss: 0.00001531
Iteration 138/1000 | Loss: 0.00001531
Iteration 139/1000 | Loss: 0.00001531
Iteration 140/1000 | Loss: 0.00001531
Iteration 141/1000 | Loss: 0.00001531
Iteration 142/1000 | Loss: 0.00001531
Iteration 143/1000 | Loss: 0.00001530
Iteration 144/1000 | Loss: 0.00001530
Iteration 145/1000 | Loss: 0.00001530
Iteration 146/1000 | Loss: 0.00001530
Iteration 147/1000 | Loss: 0.00001530
Iteration 148/1000 | Loss: 0.00001530
Iteration 149/1000 | Loss: 0.00001530
Iteration 150/1000 | Loss: 0.00001530
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001530
Iteration 154/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.530397821625229e-05, 1.530397821625229e-05, 1.530397821625229e-05, 1.530397821625229e-05, 1.530397821625229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530397821625229e-05

Optimization complete. Final v2v error: 3.373522996902466 mm

Highest mean error: 4.062856674194336 mm for frame 59

Lowest mean error: 2.959883213043213 mm for frame 7

Saving results

Total time: 40.344083070755005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01150801
Iteration 2/25 | Loss: 0.01150801
Iteration 3/25 | Loss: 0.00274007
Iteration 4/25 | Loss: 0.00152472
Iteration 5/25 | Loss: 0.00126404
Iteration 6/25 | Loss: 0.00120750
Iteration 7/25 | Loss: 0.00113772
Iteration 8/25 | Loss: 0.00103971
Iteration 9/25 | Loss: 0.00098429
Iteration 10/25 | Loss: 0.00096729
Iteration 11/25 | Loss: 0.00096207
Iteration 12/25 | Loss: 0.00096468
Iteration 13/25 | Loss: 0.00096345
Iteration 14/25 | Loss: 0.00096137
Iteration 15/25 | Loss: 0.00095962
Iteration 16/25 | Loss: 0.00095583
Iteration 17/25 | Loss: 0.00095407
Iteration 18/25 | Loss: 0.00095693
Iteration 19/25 | Loss: 0.00095537
Iteration 20/25 | Loss: 0.00095411
Iteration 21/25 | Loss: 0.00095487
Iteration 22/25 | Loss: 0.00095443
Iteration 23/25 | Loss: 0.00095572
Iteration 24/25 | Loss: 0.00095348
Iteration 25/25 | Loss: 0.00095384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44963038
Iteration 2/25 | Loss: 0.00114119
Iteration 3/25 | Loss: 0.00068663
Iteration 4/25 | Loss: 0.00068662
Iteration 5/25 | Loss: 0.00068662
Iteration 6/25 | Loss: 0.00068662
Iteration 7/25 | Loss: 0.00068662
Iteration 8/25 | Loss: 0.00068662
Iteration 9/25 | Loss: 0.00068662
Iteration 10/25 | Loss: 0.00068662
Iteration 11/25 | Loss: 0.00068662
Iteration 12/25 | Loss: 0.00068662
Iteration 13/25 | Loss: 0.00068662
Iteration 14/25 | Loss: 0.00068662
Iteration 15/25 | Loss: 0.00068662
Iteration 16/25 | Loss: 0.00068662
Iteration 17/25 | Loss: 0.00068662
Iteration 18/25 | Loss: 0.00068662
Iteration 19/25 | Loss: 0.00068662
Iteration 20/25 | Loss: 0.00068662
Iteration 21/25 | Loss: 0.00068662
Iteration 22/25 | Loss: 0.00068662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006866200128570199, 0.0006866200128570199, 0.0006866200128570199, 0.0006866200128570199, 0.0006866200128570199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006866200128570199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068662
Iteration 2/1000 | Loss: 0.00044155
Iteration 3/1000 | Loss: 0.00021767
Iteration 4/1000 | Loss: 0.00004014
Iteration 5/1000 | Loss: 0.00014604
Iteration 6/1000 | Loss: 0.00015158
Iteration 7/1000 | Loss: 0.00022799
Iteration 8/1000 | Loss: 0.00026984
Iteration 9/1000 | Loss: 0.00019555
Iteration 10/1000 | Loss: 0.00012614
Iteration 11/1000 | Loss: 0.00012591
Iteration 12/1000 | Loss: 0.00016885
Iteration 13/1000 | Loss: 0.00015159
Iteration 14/1000 | Loss: 0.00012333
Iteration 15/1000 | Loss: 0.00014826
Iteration 16/1000 | Loss: 0.00017272
Iteration 17/1000 | Loss: 0.00021585
Iteration 18/1000 | Loss: 0.00016362
Iteration 19/1000 | Loss: 0.00020541
Iteration 20/1000 | Loss: 0.00022594
Iteration 21/1000 | Loss: 0.00039544
Iteration 22/1000 | Loss: 0.00008703
Iteration 23/1000 | Loss: 0.00019409
Iteration 24/1000 | Loss: 0.00024899
Iteration 25/1000 | Loss: 0.00021982
Iteration 26/1000 | Loss: 0.00021381
Iteration 27/1000 | Loss: 0.00021081
Iteration 28/1000 | Loss: 0.00016565
Iteration 29/1000 | Loss: 0.00014595
Iteration 30/1000 | Loss: 0.00018360
Iteration 31/1000 | Loss: 0.00017784
Iteration 32/1000 | Loss: 0.00014105
Iteration 33/1000 | Loss: 0.00014158
Iteration 34/1000 | Loss: 0.00029304
Iteration 35/1000 | Loss: 0.00020066
Iteration 36/1000 | Loss: 0.00022251
Iteration 37/1000 | Loss: 0.00021420
Iteration 38/1000 | Loss: 0.00020550
Iteration 39/1000 | Loss: 0.00015952
Iteration 40/1000 | Loss: 0.00006491
Iteration 41/1000 | Loss: 0.00008110
Iteration 42/1000 | Loss: 0.00017415
Iteration 43/1000 | Loss: 0.00024878
Iteration 44/1000 | Loss: 0.00016668
Iteration 45/1000 | Loss: 0.00009246
Iteration 46/1000 | Loss: 0.00025455
Iteration 47/1000 | Loss: 0.00020905
Iteration 48/1000 | Loss: 0.00007806
Iteration 49/1000 | Loss: 0.00051914
Iteration 50/1000 | Loss: 0.00004272
Iteration 51/1000 | Loss: 0.00002939
Iteration 52/1000 | Loss: 0.00002833
Iteration 53/1000 | Loss: 0.00002756
Iteration 54/1000 | Loss: 0.00002715
Iteration 55/1000 | Loss: 0.00002672
Iteration 56/1000 | Loss: 0.00002639
Iteration 57/1000 | Loss: 0.00002626
Iteration 58/1000 | Loss: 0.00002617
Iteration 59/1000 | Loss: 0.00002617
Iteration 60/1000 | Loss: 0.00002617
Iteration 61/1000 | Loss: 0.00002617
Iteration 62/1000 | Loss: 0.00002617
Iteration 63/1000 | Loss: 0.00002616
Iteration 64/1000 | Loss: 0.00002616
Iteration 65/1000 | Loss: 0.00002613
Iteration 66/1000 | Loss: 0.00002613
Iteration 67/1000 | Loss: 0.00002612
Iteration 68/1000 | Loss: 0.00002612
Iteration 69/1000 | Loss: 0.00002611
Iteration 70/1000 | Loss: 0.00002611
Iteration 71/1000 | Loss: 0.00002611
Iteration 72/1000 | Loss: 0.00002610
Iteration 73/1000 | Loss: 0.00002610
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002609
Iteration 76/1000 | Loss: 0.00002609
Iteration 77/1000 | Loss: 0.00002609
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002608
Iteration 80/1000 | Loss: 0.00002608
Iteration 81/1000 | Loss: 0.00002608
Iteration 82/1000 | Loss: 0.00002608
Iteration 83/1000 | Loss: 0.00002608
Iteration 84/1000 | Loss: 0.00002608
Iteration 85/1000 | Loss: 0.00002607
Iteration 86/1000 | Loss: 0.00002607
Iteration 87/1000 | Loss: 0.00002607
Iteration 88/1000 | Loss: 0.00002607
Iteration 89/1000 | Loss: 0.00002607
Iteration 90/1000 | Loss: 0.00002607
Iteration 91/1000 | Loss: 0.00002606
Iteration 92/1000 | Loss: 0.00002606
Iteration 93/1000 | Loss: 0.00002606
Iteration 94/1000 | Loss: 0.00002606
Iteration 95/1000 | Loss: 0.00002606
Iteration 96/1000 | Loss: 0.00002606
Iteration 97/1000 | Loss: 0.00002605
Iteration 98/1000 | Loss: 0.00002605
Iteration 99/1000 | Loss: 0.00002605
Iteration 100/1000 | Loss: 0.00002605
Iteration 101/1000 | Loss: 0.00002605
Iteration 102/1000 | Loss: 0.00002605
Iteration 103/1000 | Loss: 0.00002605
Iteration 104/1000 | Loss: 0.00002605
Iteration 105/1000 | Loss: 0.00002605
Iteration 106/1000 | Loss: 0.00002605
Iteration 107/1000 | Loss: 0.00002605
Iteration 108/1000 | Loss: 0.00002605
Iteration 109/1000 | Loss: 0.00002605
Iteration 110/1000 | Loss: 0.00002605
Iteration 111/1000 | Loss: 0.00002605
Iteration 112/1000 | Loss: 0.00002605
Iteration 113/1000 | Loss: 0.00002605
Iteration 114/1000 | Loss: 0.00002605
Iteration 115/1000 | Loss: 0.00002605
Iteration 116/1000 | Loss: 0.00002605
Iteration 117/1000 | Loss: 0.00002605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.6045585400424898e-05, 2.6045585400424898e-05, 2.6045585400424898e-05, 2.6045585400424898e-05, 2.6045585400424898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6045585400424898e-05

Optimization complete. Final v2v error: 4.334444046020508 mm

Highest mean error: 4.868830680847168 mm for frame 20

Lowest mean error: 3.991425037384033 mm for frame 30

Saving results

Total time: 128.08095574378967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839163
Iteration 2/25 | Loss: 0.00121968
Iteration 3/25 | Loss: 0.00093191
Iteration 4/25 | Loss: 0.00089321
Iteration 5/25 | Loss: 0.00087836
Iteration 6/25 | Loss: 0.00087340
Iteration 7/25 | Loss: 0.00087304
Iteration 8/25 | Loss: 0.00087206
Iteration 9/25 | Loss: 0.00087186
Iteration 10/25 | Loss: 0.00087161
Iteration 11/25 | Loss: 0.00087159
Iteration 12/25 | Loss: 0.00087159
Iteration 13/25 | Loss: 0.00087159
Iteration 14/25 | Loss: 0.00087159
Iteration 15/25 | Loss: 0.00087159
Iteration 16/25 | Loss: 0.00087159
Iteration 17/25 | Loss: 0.00087159
Iteration 18/25 | Loss: 0.00087159
Iteration 19/25 | Loss: 0.00087159
Iteration 20/25 | Loss: 0.00087159
Iteration 21/25 | Loss: 0.00087159
Iteration 22/25 | Loss: 0.00087159
Iteration 23/25 | Loss: 0.00087159
Iteration 24/25 | Loss: 0.00087159
Iteration 25/25 | Loss: 0.00087159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.55479598
Iteration 2/25 | Loss: 0.00059222
Iteration 3/25 | Loss: 0.00057622
Iteration 4/25 | Loss: 0.00057622
Iteration 5/25 | Loss: 0.00057621
Iteration 6/25 | Loss: 0.00057621
Iteration 7/25 | Loss: 0.00057621
Iteration 8/25 | Loss: 0.00057621
Iteration 9/25 | Loss: 0.00057621
Iteration 10/25 | Loss: 0.00057621
Iteration 11/25 | Loss: 0.00057621
Iteration 12/25 | Loss: 0.00057621
Iteration 13/25 | Loss: 0.00057621
Iteration 14/25 | Loss: 0.00057621
Iteration 15/25 | Loss: 0.00057621
Iteration 16/25 | Loss: 0.00057621
Iteration 17/25 | Loss: 0.00057621
Iteration 18/25 | Loss: 0.00057621
Iteration 19/25 | Loss: 0.00057621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005762132932431996, 0.0005762132932431996, 0.0005762132932431996, 0.0005762132932431996, 0.0005762132932431996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005762132932431996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057621
Iteration 2/1000 | Loss: 0.00005093
Iteration 3/1000 | Loss: 0.00003284
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002406
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001731
Iteration 10/1000 | Loss: 0.00003687
Iteration 11/1000 | Loss: 0.00002067
Iteration 12/1000 | Loss: 0.00002629
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00002537
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001698
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00001983
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001686
Iteration 21/1000 | Loss: 0.00001686
Iteration 22/1000 | Loss: 0.00001686
Iteration 23/1000 | Loss: 0.00001686
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00001685
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001685
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001684
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.6844618585309945e-05, 1.6844618585309945e-05, 1.6844618585309945e-05, 1.6844618585309945e-05, 1.6844618585309945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6844618585309945e-05

Optimization complete. Final v2v error: 3.527702808380127 mm

Highest mean error: 3.882157564163208 mm for frame 32

Lowest mean error: 2.9455313682556152 mm for frame 51

Saving results

Total time: 45.647536277770996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949850
Iteration 2/25 | Loss: 0.00153733
Iteration 3/25 | Loss: 0.00115946
Iteration 4/25 | Loss: 0.00110927
Iteration 5/25 | Loss: 0.00110100
Iteration 6/25 | Loss: 0.00109941
Iteration 7/25 | Loss: 0.00110129
Iteration 8/25 | Loss: 0.00109658
Iteration 9/25 | Loss: 0.00109195
Iteration 10/25 | Loss: 0.00108995
Iteration 11/25 | Loss: 0.00108937
Iteration 12/25 | Loss: 0.00108926
Iteration 13/25 | Loss: 0.00108926
Iteration 14/25 | Loss: 0.00108926
Iteration 15/25 | Loss: 0.00108926
Iteration 16/25 | Loss: 0.00108926
Iteration 17/25 | Loss: 0.00108926
Iteration 18/25 | Loss: 0.00108925
Iteration 19/25 | Loss: 0.00108925
Iteration 20/25 | Loss: 0.00108925
Iteration 21/25 | Loss: 0.00108925
Iteration 22/25 | Loss: 0.00108925
Iteration 23/25 | Loss: 0.00108925
Iteration 24/25 | Loss: 0.00108925
Iteration 25/25 | Loss: 0.00108925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00314391
Iteration 2/25 | Loss: 0.00084304
Iteration 3/25 | Loss: 0.00084304
Iteration 4/25 | Loss: 0.00084304
Iteration 5/25 | Loss: 0.00084304
Iteration 6/25 | Loss: 0.00084304
Iteration 7/25 | Loss: 0.00084303
Iteration 8/25 | Loss: 0.00084303
Iteration 9/25 | Loss: 0.00084303
Iteration 10/25 | Loss: 0.00084303
Iteration 11/25 | Loss: 0.00084303
Iteration 12/25 | Loss: 0.00084303
Iteration 13/25 | Loss: 0.00084303
Iteration 14/25 | Loss: 0.00084303
Iteration 15/25 | Loss: 0.00084303
Iteration 16/25 | Loss: 0.00084303
Iteration 17/25 | Loss: 0.00084303
Iteration 18/25 | Loss: 0.00084303
Iteration 19/25 | Loss: 0.00084303
Iteration 20/25 | Loss: 0.00084303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008430340676568449, 0.0008430340676568449, 0.0008430340676568449, 0.0008430340676568449, 0.0008430340676568449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008430340676568449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084303
Iteration 2/1000 | Loss: 0.00006580
Iteration 3/1000 | Loss: 0.00005574
Iteration 4/1000 | Loss: 0.00004893
Iteration 5/1000 | Loss: 0.00004608
Iteration 6/1000 | Loss: 0.00004401
Iteration 7/1000 | Loss: 0.00004309
Iteration 8/1000 | Loss: 0.00004242
Iteration 9/1000 | Loss: 0.00004183
Iteration 10/1000 | Loss: 0.00004153
Iteration 11/1000 | Loss: 0.00004143
Iteration 12/1000 | Loss: 0.00004142
Iteration 13/1000 | Loss: 0.00004128
Iteration 14/1000 | Loss: 0.00004123
Iteration 15/1000 | Loss: 0.00004123
Iteration 16/1000 | Loss: 0.00004122
Iteration 17/1000 | Loss: 0.00004122
Iteration 18/1000 | Loss: 0.00004121
Iteration 19/1000 | Loss: 0.00004119
Iteration 20/1000 | Loss: 0.00004114
Iteration 21/1000 | Loss: 0.00004114
Iteration 22/1000 | Loss: 0.00004114
Iteration 23/1000 | Loss: 0.00004114
Iteration 24/1000 | Loss: 0.00004114
Iteration 25/1000 | Loss: 0.00004114
Iteration 26/1000 | Loss: 0.00004114
Iteration 27/1000 | Loss: 0.00004113
Iteration 28/1000 | Loss: 0.00004113
Iteration 29/1000 | Loss: 0.00004113
Iteration 30/1000 | Loss: 0.00004113
Iteration 31/1000 | Loss: 0.00004113
Iteration 32/1000 | Loss: 0.00004113
Iteration 33/1000 | Loss: 0.00004113
Iteration 34/1000 | Loss: 0.00004113
Iteration 35/1000 | Loss: 0.00004113
Iteration 36/1000 | Loss: 0.00004112
Iteration 37/1000 | Loss: 0.00004112
Iteration 38/1000 | Loss: 0.00004112
Iteration 39/1000 | Loss: 0.00004112
Iteration 40/1000 | Loss: 0.00004112
Iteration 41/1000 | Loss: 0.00004112
Iteration 42/1000 | Loss: 0.00004112
Iteration 43/1000 | Loss: 0.00004112
Iteration 44/1000 | Loss: 0.00004112
Iteration 45/1000 | Loss: 0.00004112
Iteration 46/1000 | Loss: 0.00004111
Iteration 47/1000 | Loss: 0.00004111
Iteration 48/1000 | Loss: 0.00004111
Iteration 49/1000 | Loss: 0.00004111
Iteration 50/1000 | Loss: 0.00004111
Iteration 51/1000 | Loss: 0.00004110
Iteration 52/1000 | Loss: 0.00004110
Iteration 53/1000 | Loss: 0.00004110
Iteration 54/1000 | Loss: 0.00004110
Iteration 55/1000 | Loss: 0.00004110
Iteration 56/1000 | Loss: 0.00004110
Iteration 57/1000 | Loss: 0.00004110
Iteration 58/1000 | Loss: 0.00004108
Iteration 59/1000 | Loss: 0.00004107
Iteration 60/1000 | Loss: 0.00004107
Iteration 61/1000 | Loss: 0.00004107
Iteration 62/1000 | Loss: 0.00004107
Iteration 63/1000 | Loss: 0.00004107
Iteration 64/1000 | Loss: 0.00004106
Iteration 65/1000 | Loss: 0.00004106
Iteration 66/1000 | Loss: 0.00004106
Iteration 67/1000 | Loss: 0.00004106
Iteration 68/1000 | Loss: 0.00004106
Iteration 69/1000 | Loss: 0.00004106
Iteration 70/1000 | Loss: 0.00004106
Iteration 71/1000 | Loss: 0.00004105
Iteration 72/1000 | Loss: 0.00004105
Iteration 73/1000 | Loss: 0.00004105
Iteration 74/1000 | Loss: 0.00004105
Iteration 75/1000 | Loss: 0.00004105
Iteration 76/1000 | Loss: 0.00004105
Iteration 77/1000 | Loss: 0.00004105
Iteration 78/1000 | Loss: 0.00004105
Iteration 79/1000 | Loss: 0.00004105
Iteration 80/1000 | Loss: 0.00004104
Iteration 81/1000 | Loss: 0.00004104
Iteration 82/1000 | Loss: 0.00004104
Iteration 83/1000 | Loss: 0.00004103
Iteration 84/1000 | Loss: 0.00004103
Iteration 85/1000 | Loss: 0.00004103
Iteration 86/1000 | Loss: 0.00004103
Iteration 87/1000 | Loss: 0.00004102
Iteration 88/1000 | Loss: 0.00004102
Iteration 89/1000 | Loss: 0.00004102
Iteration 90/1000 | Loss: 0.00004102
Iteration 91/1000 | Loss: 0.00004102
Iteration 92/1000 | Loss: 0.00004102
Iteration 93/1000 | Loss: 0.00004102
Iteration 94/1000 | Loss: 0.00004102
Iteration 95/1000 | Loss: 0.00004102
Iteration 96/1000 | Loss: 0.00004102
Iteration 97/1000 | Loss: 0.00004102
Iteration 98/1000 | Loss: 0.00004102
Iteration 99/1000 | Loss: 0.00004101
Iteration 100/1000 | Loss: 0.00004101
Iteration 101/1000 | Loss: 0.00004101
Iteration 102/1000 | Loss: 0.00004101
Iteration 103/1000 | Loss: 0.00004101
Iteration 104/1000 | Loss: 0.00004101
Iteration 105/1000 | Loss: 0.00004101
Iteration 106/1000 | Loss: 0.00004101
Iteration 107/1000 | Loss: 0.00004101
Iteration 108/1000 | Loss: 0.00004101
Iteration 109/1000 | Loss: 0.00004101
Iteration 110/1000 | Loss: 0.00004101
Iteration 111/1000 | Loss: 0.00004100
Iteration 112/1000 | Loss: 0.00004100
Iteration 113/1000 | Loss: 0.00004100
Iteration 114/1000 | Loss: 0.00004100
Iteration 115/1000 | Loss: 0.00004100
Iteration 116/1000 | Loss: 0.00004100
Iteration 117/1000 | Loss: 0.00004100
Iteration 118/1000 | Loss: 0.00004100
Iteration 119/1000 | Loss: 0.00004100
Iteration 120/1000 | Loss: 0.00004099
Iteration 121/1000 | Loss: 0.00004099
Iteration 122/1000 | Loss: 0.00004099
Iteration 123/1000 | Loss: 0.00004099
Iteration 124/1000 | Loss: 0.00004099
Iteration 125/1000 | Loss: 0.00004099
Iteration 126/1000 | Loss: 0.00004099
Iteration 127/1000 | Loss: 0.00004099
Iteration 128/1000 | Loss: 0.00004098
Iteration 129/1000 | Loss: 0.00004098
Iteration 130/1000 | Loss: 0.00004098
Iteration 131/1000 | Loss: 0.00004098
Iteration 132/1000 | Loss: 0.00004098
Iteration 133/1000 | Loss: 0.00004097
Iteration 134/1000 | Loss: 0.00004097
Iteration 135/1000 | Loss: 0.00004097
Iteration 136/1000 | Loss: 0.00004097
Iteration 137/1000 | Loss: 0.00004097
Iteration 138/1000 | Loss: 0.00004097
Iteration 139/1000 | Loss: 0.00004097
Iteration 140/1000 | Loss: 0.00004097
Iteration 141/1000 | Loss: 0.00004097
Iteration 142/1000 | Loss: 0.00004097
Iteration 143/1000 | Loss: 0.00004097
Iteration 144/1000 | Loss: 0.00004097
Iteration 145/1000 | Loss: 0.00004097
Iteration 146/1000 | Loss: 0.00004097
Iteration 147/1000 | Loss: 0.00004097
Iteration 148/1000 | Loss: 0.00004097
Iteration 149/1000 | Loss: 0.00004097
Iteration 150/1000 | Loss: 0.00004097
Iteration 151/1000 | Loss: 0.00004097
Iteration 152/1000 | Loss: 0.00004097
Iteration 153/1000 | Loss: 0.00004097
Iteration 154/1000 | Loss: 0.00004097
Iteration 155/1000 | Loss: 0.00004097
Iteration 156/1000 | Loss: 0.00004097
Iteration 157/1000 | Loss: 0.00004097
Iteration 158/1000 | Loss: 0.00004097
Iteration 159/1000 | Loss: 0.00004097
Iteration 160/1000 | Loss: 0.00004097
Iteration 161/1000 | Loss: 0.00004097
Iteration 162/1000 | Loss: 0.00004097
Iteration 163/1000 | Loss: 0.00004097
Iteration 164/1000 | Loss: 0.00004097
Iteration 165/1000 | Loss: 0.00004097
Iteration 166/1000 | Loss: 0.00004097
Iteration 167/1000 | Loss: 0.00004097
Iteration 168/1000 | Loss: 0.00004097
Iteration 169/1000 | Loss: 0.00004097
Iteration 170/1000 | Loss: 0.00004097
Iteration 171/1000 | Loss: 0.00004097
Iteration 172/1000 | Loss: 0.00004097
Iteration 173/1000 | Loss: 0.00004097
Iteration 174/1000 | Loss: 0.00004097
Iteration 175/1000 | Loss: 0.00004097
Iteration 176/1000 | Loss: 0.00004097
Iteration 177/1000 | Loss: 0.00004097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [4.096784323337488e-05, 4.096784323337488e-05, 4.096784323337488e-05, 4.096784323337488e-05, 4.096784323337488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.096784323337488e-05

Optimization complete. Final v2v error: 5.311954021453857 mm

Highest mean error: 5.397289752960205 mm for frame 121

Lowest mean error: 5.213330268859863 mm for frame 128

Saving results

Total time: 46.6712007522583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837886
Iteration 2/25 | Loss: 0.00107196
Iteration 3/25 | Loss: 0.00093847
Iteration 4/25 | Loss: 0.00090992
Iteration 5/25 | Loss: 0.00090062
Iteration 6/25 | Loss: 0.00089836
Iteration 7/25 | Loss: 0.00089746
Iteration 8/25 | Loss: 0.00089739
Iteration 9/25 | Loss: 0.00089739
Iteration 10/25 | Loss: 0.00089739
Iteration 11/25 | Loss: 0.00089739
Iteration 12/25 | Loss: 0.00089739
Iteration 13/25 | Loss: 0.00089739
Iteration 14/25 | Loss: 0.00089739
Iteration 15/25 | Loss: 0.00089739
Iteration 16/25 | Loss: 0.00089739
Iteration 17/25 | Loss: 0.00089739
Iteration 18/25 | Loss: 0.00089739
Iteration 19/25 | Loss: 0.00089739
Iteration 20/25 | Loss: 0.00089739
Iteration 21/25 | Loss: 0.00089739
Iteration 22/25 | Loss: 0.00089739
Iteration 23/25 | Loss: 0.00089739
Iteration 24/25 | Loss: 0.00089739
Iteration 25/25 | Loss: 0.00089739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50368845
Iteration 2/25 | Loss: 0.00068097
Iteration 3/25 | Loss: 0.00068096
Iteration 4/25 | Loss: 0.00068096
Iteration 5/25 | Loss: 0.00068096
Iteration 6/25 | Loss: 0.00068096
Iteration 7/25 | Loss: 0.00068096
Iteration 8/25 | Loss: 0.00068096
Iteration 9/25 | Loss: 0.00068096
Iteration 10/25 | Loss: 0.00068096
Iteration 11/25 | Loss: 0.00068096
Iteration 12/25 | Loss: 0.00068096
Iteration 13/25 | Loss: 0.00068096
Iteration 14/25 | Loss: 0.00068096
Iteration 15/25 | Loss: 0.00068096
Iteration 16/25 | Loss: 0.00068096
Iteration 17/25 | Loss: 0.00068096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006809611804783344, 0.0006809611804783344, 0.0006809611804783344, 0.0006809611804783344, 0.0006809611804783344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006809611804783344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068096
Iteration 2/1000 | Loss: 0.00005299
Iteration 3/1000 | Loss: 0.00003359
Iteration 4/1000 | Loss: 0.00002748
Iteration 5/1000 | Loss: 0.00002551
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002326
Iteration 8/1000 | Loss: 0.00002270
Iteration 9/1000 | Loss: 0.00002230
Iteration 10/1000 | Loss: 0.00002214
Iteration 11/1000 | Loss: 0.00002214
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002183
Iteration 14/1000 | Loss: 0.00002178
Iteration 15/1000 | Loss: 0.00002172
Iteration 16/1000 | Loss: 0.00002170
Iteration 17/1000 | Loss: 0.00002169
Iteration 18/1000 | Loss: 0.00002169
Iteration 19/1000 | Loss: 0.00002168
Iteration 20/1000 | Loss: 0.00002167
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002166
Iteration 25/1000 | Loss: 0.00002166
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002165
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002164
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002160
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002160
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002159
Iteration 49/1000 | Loss: 0.00002159
Iteration 50/1000 | Loss: 0.00002159
Iteration 51/1000 | Loss: 0.00002159
Iteration 52/1000 | Loss: 0.00002159
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002158
Iteration 55/1000 | Loss: 0.00002158
Iteration 56/1000 | Loss: 0.00002158
Iteration 57/1000 | Loss: 0.00002158
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002158
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002157
Iteration 64/1000 | Loss: 0.00002157
Iteration 65/1000 | Loss: 0.00002157
Iteration 66/1000 | Loss: 0.00002157
Iteration 67/1000 | Loss: 0.00002157
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002156
Iteration 71/1000 | Loss: 0.00002156
Iteration 72/1000 | Loss: 0.00002156
Iteration 73/1000 | Loss: 0.00002156
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002156
Iteration 77/1000 | Loss: 0.00002155
Iteration 78/1000 | Loss: 0.00002155
Iteration 79/1000 | Loss: 0.00002155
Iteration 80/1000 | Loss: 0.00002155
Iteration 81/1000 | Loss: 0.00002155
Iteration 82/1000 | Loss: 0.00002155
Iteration 83/1000 | Loss: 0.00002155
Iteration 84/1000 | Loss: 0.00002155
Iteration 85/1000 | Loss: 0.00002155
Iteration 86/1000 | Loss: 0.00002155
Iteration 87/1000 | Loss: 0.00002155
Iteration 88/1000 | Loss: 0.00002155
Iteration 89/1000 | Loss: 0.00002155
Iteration 90/1000 | Loss: 0.00002155
Iteration 91/1000 | Loss: 0.00002155
Iteration 92/1000 | Loss: 0.00002155
Iteration 93/1000 | Loss: 0.00002155
Iteration 94/1000 | Loss: 0.00002155
Iteration 95/1000 | Loss: 0.00002155
Iteration 96/1000 | Loss: 0.00002155
Iteration 97/1000 | Loss: 0.00002154
Iteration 98/1000 | Loss: 0.00002154
Iteration 99/1000 | Loss: 0.00002154
Iteration 100/1000 | Loss: 0.00002154
Iteration 101/1000 | Loss: 0.00002154
Iteration 102/1000 | Loss: 0.00002154
Iteration 103/1000 | Loss: 0.00002154
Iteration 104/1000 | Loss: 0.00002154
Iteration 105/1000 | Loss: 0.00002154
Iteration 106/1000 | Loss: 0.00002154
Iteration 107/1000 | Loss: 0.00002154
Iteration 108/1000 | Loss: 0.00002154
Iteration 109/1000 | Loss: 0.00002154
Iteration 110/1000 | Loss: 0.00002154
Iteration 111/1000 | Loss: 0.00002154
Iteration 112/1000 | Loss: 0.00002154
Iteration 113/1000 | Loss: 0.00002154
Iteration 114/1000 | Loss: 0.00002154
Iteration 115/1000 | Loss: 0.00002154
Iteration 116/1000 | Loss: 0.00002154
Iteration 117/1000 | Loss: 0.00002154
Iteration 118/1000 | Loss: 0.00002154
Iteration 119/1000 | Loss: 0.00002154
Iteration 120/1000 | Loss: 0.00002154
Iteration 121/1000 | Loss: 0.00002154
Iteration 122/1000 | Loss: 0.00002154
Iteration 123/1000 | Loss: 0.00002154
Iteration 124/1000 | Loss: 0.00002154
Iteration 125/1000 | Loss: 0.00002154
Iteration 126/1000 | Loss: 0.00002154
Iteration 127/1000 | Loss: 0.00002154
Iteration 128/1000 | Loss: 0.00002154
Iteration 129/1000 | Loss: 0.00002154
Iteration 130/1000 | Loss: 0.00002154
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002154
Iteration 133/1000 | Loss: 0.00002154
Iteration 134/1000 | Loss: 0.00002154
Iteration 135/1000 | Loss: 0.00002154
Iteration 136/1000 | Loss: 0.00002154
Iteration 137/1000 | Loss: 0.00002154
Iteration 138/1000 | Loss: 0.00002154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.1537684006034397e-05, 2.1537684006034397e-05, 2.1537684006034397e-05, 2.1537684006034397e-05, 2.1537684006034397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1537684006034397e-05

Optimization complete. Final v2v error: 4.006051540374756 mm

Highest mean error: 4.50364351272583 mm for frame 115

Lowest mean error: 3.643301486968994 mm for frame 12

Saving results

Total time: 34.48271822929382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505925
Iteration 2/25 | Loss: 0.00116700
Iteration 3/25 | Loss: 0.00098344
Iteration 4/25 | Loss: 0.00093674
Iteration 5/25 | Loss: 0.00092602
Iteration 6/25 | Loss: 0.00092383
Iteration 7/25 | Loss: 0.00092349
Iteration 8/25 | Loss: 0.00092349
Iteration 9/25 | Loss: 0.00092349
Iteration 10/25 | Loss: 0.00092349
Iteration 11/25 | Loss: 0.00092349
Iteration 12/25 | Loss: 0.00092349
Iteration 13/25 | Loss: 0.00092349
Iteration 14/25 | Loss: 0.00092349
Iteration 15/25 | Loss: 0.00092349
Iteration 16/25 | Loss: 0.00092349
Iteration 17/25 | Loss: 0.00092349
Iteration 18/25 | Loss: 0.00092349
Iteration 19/25 | Loss: 0.00092349
Iteration 20/25 | Loss: 0.00092349
Iteration 21/25 | Loss: 0.00092349
Iteration 22/25 | Loss: 0.00092349
Iteration 23/25 | Loss: 0.00092349
Iteration 24/25 | Loss: 0.00092349
Iteration 25/25 | Loss: 0.00092349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49113870
Iteration 2/25 | Loss: 0.00064122
Iteration 3/25 | Loss: 0.00064118
Iteration 4/25 | Loss: 0.00064118
Iteration 5/25 | Loss: 0.00064118
Iteration 6/25 | Loss: 0.00064118
Iteration 7/25 | Loss: 0.00064118
Iteration 8/25 | Loss: 0.00064118
Iteration 9/25 | Loss: 0.00064118
Iteration 10/25 | Loss: 0.00064118
Iteration 11/25 | Loss: 0.00064118
Iteration 12/25 | Loss: 0.00064118
Iteration 13/25 | Loss: 0.00064118
Iteration 14/25 | Loss: 0.00064118
Iteration 15/25 | Loss: 0.00064118
Iteration 16/25 | Loss: 0.00064118
Iteration 17/25 | Loss: 0.00064118
Iteration 18/25 | Loss: 0.00064118
Iteration 19/25 | Loss: 0.00064118
Iteration 20/25 | Loss: 0.00064118
Iteration 21/25 | Loss: 0.00064118
Iteration 22/25 | Loss: 0.00064118
Iteration 23/25 | Loss: 0.00064118
Iteration 24/25 | Loss: 0.00064118
Iteration 25/25 | Loss: 0.00064118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064118
Iteration 2/1000 | Loss: 0.00003303
Iteration 3/1000 | Loss: 0.00002167
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001751
Iteration 6/1000 | Loss: 0.00001660
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001571
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001560
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001554
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001551
Iteration 31/1000 | Loss: 0.00001551
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001550
Iteration 34/1000 | Loss: 0.00001550
Iteration 35/1000 | Loss: 0.00001550
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001549
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001544
Iteration 86/1000 | Loss: 0.00001544
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.5421515854541212e-05, 1.5421515854541212e-05, 1.5421515854541212e-05, 1.5421515854541212e-05, 1.5421515854541212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5421515854541212e-05

Optimization complete. Final v2v error: 3.4057042598724365 mm

Highest mean error: 3.703951358795166 mm for frame 221

Lowest mean error: 3.2126212120056152 mm for frame 183

Saving results

Total time: 33.3261444568634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846433
Iteration 2/25 | Loss: 0.00118686
Iteration 3/25 | Loss: 0.00096488
Iteration 4/25 | Loss: 0.00090787
Iteration 5/25 | Loss: 0.00089301
Iteration 6/25 | Loss: 0.00088728
Iteration 7/25 | Loss: 0.00089172
Iteration 8/25 | Loss: 0.00089120
Iteration 9/25 | Loss: 0.00088699
Iteration 10/25 | Loss: 0.00088502
Iteration 11/25 | Loss: 0.00089113
Iteration 12/25 | Loss: 0.00088311
Iteration 13/25 | Loss: 0.00088203
Iteration 14/25 | Loss: 0.00088041
Iteration 15/25 | Loss: 0.00088137
Iteration 16/25 | Loss: 0.00088097
Iteration 17/25 | Loss: 0.00088133
Iteration 18/25 | Loss: 0.00088104
Iteration 19/25 | Loss: 0.00088118
Iteration 20/25 | Loss: 0.00088103
Iteration 21/25 | Loss: 0.00088127
Iteration 22/25 | Loss: 0.00088098
Iteration 23/25 | Loss: 0.00088112
Iteration 24/25 | Loss: 0.00088078
Iteration 25/25 | Loss: 0.00088062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.99896383
Iteration 2/25 | Loss: 0.00063491
Iteration 3/25 | Loss: 0.00063477
Iteration 4/25 | Loss: 0.00063477
Iteration 5/25 | Loss: 0.00063477
Iteration 6/25 | Loss: 0.00063477
Iteration 7/25 | Loss: 0.00063477
Iteration 8/25 | Loss: 0.00063477
Iteration 9/25 | Loss: 0.00063477
Iteration 10/25 | Loss: 0.00063477
Iteration 11/25 | Loss: 0.00063477
Iteration 12/25 | Loss: 0.00063477
Iteration 13/25 | Loss: 0.00063477
Iteration 14/25 | Loss: 0.00063477
Iteration 15/25 | Loss: 0.00063477
Iteration 16/25 | Loss: 0.00063477
Iteration 17/25 | Loss: 0.00063477
Iteration 18/25 | Loss: 0.00063477
Iteration 19/25 | Loss: 0.00063477
Iteration 20/25 | Loss: 0.00063477
Iteration 21/25 | Loss: 0.00063477
Iteration 22/25 | Loss: 0.00063477
Iteration 23/25 | Loss: 0.00063477
Iteration 24/25 | Loss: 0.00063477
Iteration 25/25 | Loss: 0.00063477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063477
Iteration 2/1000 | Loss: 0.00003686
Iteration 3/1000 | Loss: 0.00002816
Iteration 4/1000 | Loss: 0.00002514
Iteration 5/1000 | Loss: 0.00002342
Iteration 6/1000 | Loss: 0.00002263
Iteration 7/1000 | Loss: 0.00002215
Iteration 8/1000 | Loss: 0.00002177
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002129
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002112
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002108
Iteration 15/1000 | Loss: 0.00002092
Iteration 16/1000 | Loss: 0.00002087
Iteration 17/1000 | Loss: 0.00002086
Iteration 18/1000 | Loss: 0.00002084
Iteration 19/1000 | Loss: 0.00002083
Iteration 20/1000 | Loss: 0.00002082
Iteration 21/1000 | Loss: 0.00002082
Iteration 22/1000 | Loss: 0.00002081
Iteration 23/1000 | Loss: 0.00002080
Iteration 24/1000 | Loss: 0.00002080
Iteration 25/1000 | Loss: 0.00002078
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002078
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002078
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002077
Iteration 33/1000 | Loss: 0.00002077
Iteration 34/1000 | Loss: 0.00002077
Iteration 35/1000 | Loss: 0.00002075
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002075
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002074
Iteration 40/1000 | Loss: 0.00002074
Iteration 41/1000 | Loss: 0.00002073
Iteration 42/1000 | Loss: 0.00002073
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002071
Iteration 45/1000 | Loss: 0.00002071
Iteration 46/1000 | Loss: 0.00002071
Iteration 47/1000 | Loss: 0.00002070
Iteration 48/1000 | Loss: 0.00002163
Iteration 49/1000 | Loss: 0.00002162
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002083
Iteration 52/1000 | Loss: 0.00002066
Iteration 53/1000 | Loss: 0.00002066
Iteration 54/1000 | Loss: 0.00002066
Iteration 55/1000 | Loss: 0.00002066
Iteration 56/1000 | Loss: 0.00002066
Iteration 57/1000 | Loss: 0.00002066
Iteration 58/1000 | Loss: 0.00002066
Iteration 59/1000 | Loss: 0.00002066
Iteration 60/1000 | Loss: 0.00002066
Iteration 61/1000 | Loss: 0.00002066
Iteration 62/1000 | Loss: 0.00002066
Iteration 63/1000 | Loss: 0.00002066
Iteration 64/1000 | Loss: 0.00002066
Iteration 65/1000 | Loss: 0.00002066
Iteration 66/1000 | Loss: 0.00002065
Iteration 67/1000 | Loss: 0.00002064
Iteration 68/1000 | Loss: 0.00002064
Iteration 69/1000 | Loss: 0.00002063
Iteration 70/1000 | Loss: 0.00002063
Iteration 71/1000 | Loss: 0.00002062
Iteration 72/1000 | Loss: 0.00002062
Iteration 73/1000 | Loss: 0.00002062
Iteration 74/1000 | Loss: 0.00002062
Iteration 75/1000 | Loss: 0.00002062
Iteration 76/1000 | Loss: 0.00002062
Iteration 77/1000 | Loss: 0.00002062
Iteration 78/1000 | Loss: 0.00002062
Iteration 79/1000 | Loss: 0.00002062
Iteration 80/1000 | Loss: 0.00002062
Iteration 81/1000 | Loss: 0.00002062
Iteration 82/1000 | Loss: 0.00002062
Iteration 83/1000 | Loss: 0.00002062
Iteration 84/1000 | Loss: 0.00002062
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002062
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002061
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002061
Iteration 95/1000 | Loss: 0.00002061
Iteration 96/1000 | Loss: 0.00002061
Iteration 97/1000 | Loss: 0.00002061
Iteration 98/1000 | Loss: 0.00002061
Iteration 99/1000 | Loss: 0.00002061
Iteration 100/1000 | Loss: 0.00002061
Iteration 101/1000 | Loss: 0.00002061
Iteration 102/1000 | Loss: 0.00002061
Iteration 103/1000 | Loss: 0.00002061
Iteration 104/1000 | Loss: 0.00002061
Iteration 105/1000 | Loss: 0.00002061
Iteration 106/1000 | Loss: 0.00002061
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002061
Iteration 109/1000 | Loss: 0.00002061
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002061
Iteration 114/1000 | Loss: 0.00002061
Iteration 115/1000 | Loss: 0.00002061
Iteration 116/1000 | Loss: 0.00002061
Iteration 117/1000 | Loss: 0.00002061
Iteration 118/1000 | Loss: 0.00002061
Iteration 119/1000 | Loss: 0.00002061
Iteration 120/1000 | Loss: 0.00002061
Iteration 121/1000 | Loss: 0.00002061
Iteration 122/1000 | Loss: 0.00002061
Iteration 123/1000 | Loss: 0.00002061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.0607380065484904e-05, 2.0607380065484904e-05, 2.0607380065484904e-05, 2.0607380065484904e-05, 2.0607380065484904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0607380065484904e-05

Optimization complete. Final v2v error: 3.9007277488708496 mm

Highest mean error: 9.44775390625 mm for frame 52

Lowest mean error: 3.390284299850464 mm for frame 203

Saving results

Total time: 76.24757814407349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033797
Iteration 2/25 | Loss: 0.00155524
Iteration 3/25 | Loss: 0.00107026
Iteration 4/25 | Loss: 0.00099337
Iteration 5/25 | Loss: 0.00098337
Iteration 6/25 | Loss: 0.00096538
Iteration 7/25 | Loss: 0.00097203
Iteration 8/25 | Loss: 0.00097542
Iteration 9/25 | Loss: 0.00096209
Iteration 10/25 | Loss: 0.00093341
Iteration 11/25 | Loss: 0.00091035
Iteration 12/25 | Loss: 0.00090657
Iteration 13/25 | Loss: 0.00090514
Iteration 14/25 | Loss: 0.00090417
Iteration 15/25 | Loss: 0.00090395
Iteration 16/25 | Loss: 0.00091304
Iteration 17/25 | Loss: 0.00090341
Iteration 18/25 | Loss: 0.00090193
Iteration 19/25 | Loss: 0.00090164
Iteration 20/25 | Loss: 0.00090155
Iteration 21/25 | Loss: 0.00090155
Iteration 22/25 | Loss: 0.00090155
Iteration 23/25 | Loss: 0.00090155
Iteration 24/25 | Loss: 0.00090155
Iteration 25/25 | Loss: 0.00090155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.00473022
Iteration 2/25 | Loss: 0.00065375
Iteration 3/25 | Loss: 0.00065372
Iteration 4/25 | Loss: 0.00065372
Iteration 5/25 | Loss: 0.00065372
Iteration 6/25 | Loss: 0.00065372
Iteration 7/25 | Loss: 0.00065372
Iteration 8/25 | Loss: 0.00065372
Iteration 9/25 | Loss: 0.00065372
Iteration 10/25 | Loss: 0.00065372
Iteration 11/25 | Loss: 0.00065372
Iteration 12/25 | Loss: 0.00065372
Iteration 13/25 | Loss: 0.00065372
Iteration 14/25 | Loss: 0.00065372
Iteration 15/25 | Loss: 0.00065372
Iteration 16/25 | Loss: 0.00065372
Iteration 17/25 | Loss: 0.00065372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006537196459248662, 0.0006537196459248662, 0.0006537196459248662, 0.0006537196459248662, 0.0006537196459248662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006537196459248662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065372
Iteration 2/1000 | Loss: 0.00003815
Iteration 3/1000 | Loss: 0.00002714
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002179
Iteration 6/1000 | Loss: 0.00002078
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00001990
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001973
Iteration 11/1000 | Loss: 0.00001961
Iteration 12/1000 | Loss: 0.00001955
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001950
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00001946
Iteration 17/1000 | Loss: 0.00001945
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001941
Iteration 23/1000 | Loss: 0.00001941
Iteration 24/1000 | Loss: 0.00001941
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001938
Iteration 28/1000 | Loss: 0.00001938
Iteration 29/1000 | Loss: 0.00001938
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001936
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001935
Iteration 36/1000 | Loss: 0.00001935
Iteration 37/1000 | Loss: 0.00001935
Iteration 38/1000 | Loss: 0.00001934
Iteration 39/1000 | Loss: 0.00001934
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001934
Iteration 42/1000 | Loss: 0.00001934
Iteration 43/1000 | Loss: 0.00001933
Iteration 44/1000 | Loss: 0.00001933
Iteration 45/1000 | Loss: 0.00001933
Iteration 46/1000 | Loss: 0.00001933
Iteration 47/1000 | Loss: 0.00001933
Iteration 48/1000 | Loss: 0.00001933
Iteration 49/1000 | Loss: 0.00001933
Iteration 50/1000 | Loss: 0.00001932
Iteration 51/1000 | Loss: 0.00001932
Iteration 52/1000 | Loss: 0.00001932
Iteration 53/1000 | Loss: 0.00001932
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001932
Iteration 56/1000 | Loss: 0.00001932
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001932
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001931
Iteration 62/1000 | Loss: 0.00001931
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001930
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001929
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001928
Iteration 98/1000 | Loss: 0.00001928
Iteration 99/1000 | Loss: 0.00001928
Iteration 100/1000 | Loss: 0.00001928
Iteration 101/1000 | Loss: 0.00001928
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001928
Iteration 107/1000 | Loss: 0.00001928
Iteration 108/1000 | Loss: 0.00001928
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001928
Iteration 111/1000 | Loss: 0.00001928
Iteration 112/1000 | Loss: 0.00001928
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001928
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001928
Iteration 120/1000 | Loss: 0.00001928
Iteration 121/1000 | Loss: 0.00001928
Iteration 122/1000 | Loss: 0.00001928
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001927
Iteration 127/1000 | Loss: 0.00001927
Iteration 128/1000 | Loss: 0.00001927
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001927
Iteration 132/1000 | Loss: 0.00001927
Iteration 133/1000 | Loss: 0.00001927
Iteration 134/1000 | Loss: 0.00001927
Iteration 135/1000 | Loss: 0.00001927
Iteration 136/1000 | Loss: 0.00001927
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.9268542018835433e-05, 1.9268542018835433e-05, 1.9268542018835433e-05, 1.9268542018835433e-05, 1.9268542018835433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9268542018835433e-05

Optimization complete. Final v2v error: 3.78743314743042 mm

Highest mean error: 4.162116050720215 mm for frame 81

Lowest mean error: 3.0572941303253174 mm for frame 33

Saving results

Total time: 55.72517466545105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853878
Iteration 2/25 | Loss: 0.00140014
Iteration 3/25 | Loss: 0.00103538
Iteration 4/25 | Loss: 0.00098777
Iteration 5/25 | Loss: 0.00097035
Iteration 6/25 | Loss: 0.00096750
Iteration 7/25 | Loss: 0.00096715
Iteration 8/25 | Loss: 0.00096715
Iteration 9/25 | Loss: 0.00096715
Iteration 10/25 | Loss: 0.00096715
Iteration 11/25 | Loss: 0.00096715
Iteration 12/25 | Loss: 0.00096715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009671450825408101, 0.0009671450825408101, 0.0009671450825408101, 0.0009671450825408101, 0.0009671450825408101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009671450825408101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49701583
Iteration 2/25 | Loss: 0.00079506
Iteration 3/25 | Loss: 0.00079505
Iteration 4/25 | Loss: 0.00079505
Iteration 5/25 | Loss: 0.00079505
Iteration 6/25 | Loss: 0.00079505
Iteration 7/25 | Loss: 0.00079505
Iteration 8/25 | Loss: 0.00079505
Iteration 9/25 | Loss: 0.00079505
Iteration 10/25 | Loss: 0.00079505
Iteration 11/25 | Loss: 0.00079505
Iteration 12/25 | Loss: 0.00079505
Iteration 13/25 | Loss: 0.00079505
Iteration 14/25 | Loss: 0.00079505
Iteration 15/25 | Loss: 0.00079505
Iteration 16/25 | Loss: 0.00079505
Iteration 17/25 | Loss: 0.00079505
Iteration 18/25 | Loss: 0.00079505
Iteration 19/25 | Loss: 0.00079505
Iteration 20/25 | Loss: 0.00079505
Iteration 21/25 | Loss: 0.00079505
Iteration 22/25 | Loss: 0.00079505
Iteration 23/25 | Loss: 0.00079505
Iteration 24/25 | Loss: 0.00079505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007950525614432991, 0.0007950525614432991, 0.0007950525614432991, 0.0007950525614432991, 0.0007950525614432991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007950525614432991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079505
Iteration 2/1000 | Loss: 0.00004245
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00002890
Iteration 5/1000 | Loss: 0.00002643
Iteration 6/1000 | Loss: 0.00002536
Iteration 7/1000 | Loss: 0.00002487
Iteration 8/1000 | Loss: 0.00002446
Iteration 9/1000 | Loss: 0.00002419
Iteration 10/1000 | Loss: 0.00002411
Iteration 11/1000 | Loss: 0.00002392
Iteration 12/1000 | Loss: 0.00002380
Iteration 13/1000 | Loss: 0.00002380
Iteration 14/1000 | Loss: 0.00002380
Iteration 15/1000 | Loss: 0.00002379
Iteration 16/1000 | Loss: 0.00002379
Iteration 17/1000 | Loss: 0.00002379
Iteration 18/1000 | Loss: 0.00002379
Iteration 19/1000 | Loss: 0.00002378
Iteration 20/1000 | Loss: 0.00002378
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002376
Iteration 23/1000 | Loss: 0.00002374
Iteration 24/1000 | Loss: 0.00002373
Iteration 25/1000 | Loss: 0.00002372
Iteration 26/1000 | Loss: 0.00002372
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002371
Iteration 30/1000 | Loss: 0.00002371
Iteration 31/1000 | Loss: 0.00002371
Iteration 32/1000 | Loss: 0.00002371
Iteration 33/1000 | Loss: 0.00002371
Iteration 34/1000 | Loss: 0.00002371
Iteration 35/1000 | Loss: 0.00002370
Iteration 36/1000 | Loss: 0.00002370
Iteration 37/1000 | Loss: 0.00002370
Iteration 38/1000 | Loss: 0.00002370
Iteration 39/1000 | Loss: 0.00002370
Iteration 40/1000 | Loss: 0.00002370
Iteration 41/1000 | Loss: 0.00002370
Iteration 42/1000 | Loss: 0.00002370
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002369
Iteration 45/1000 | Loss: 0.00002369
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00002367
Iteration 48/1000 | Loss: 0.00002367
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00002367
Iteration 52/1000 | Loss: 0.00002367
Iteration 53/1000 | Loss: 0.00002367
Iteration 54/1000 | Loss: 0.00002366
Iteration 55/1000 | Loss: 0.00002366
Iteration 56/1000 | Loss: 0.00002366
Iteration 57/1000 | Loss: 0.00002366
Iteration 58/1000 | Loss: 0.00002365
Iteration 59/1000 | Loss: 0.00002365
Iteration 60/1000 | Loss: 0.00002365
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002364
Iteration 64/1000 | Loss: 0.00002364
Iteration 65/1000 | Loss: 0.00002364
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002363
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002362
Iteration 72/1000 | Loss: 0.00002362
Iteration 73/1000 | Loss: 0.00002362
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002362
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002361
Iteration 84/1000 | Loss: 0.00002361
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002360
Iteration 91/1000 | Loss: 0.00002360
Iteration 92/1000 | Loss: 0.00002360
Iteration 93/1000 | Loss: 0.00002360
Iteration 94/1000 | Loss: 0.00002360
Iteration 95/1000 | Loss: 0.00002360
Iteration 96/1000 | Loss: 0.00002360
Iteration 97/1000 | Loss: 0.00002360
Iteration 98/1000 | Loss: 0.00002360
Iteration 99/1000 | Loss: 0.00002360
Iteration 100/1000 | Loss: 0.00002360
Iteration 101/1000 | Loss: 0.00002360
Iteration 102/1000 | Loss: 0.00002360
Iteration 103/1000 | Loss: 0.00002360
Iteration 104/1000 | Loss: 0.00002360
Iteration 105/1000 | Loss: 0.00002360
Iteration 106/1000 | Loss: 0.00002360
Iteration 107/1000 | Loss: 0.00002360
Iteration 108/1000 | Loss: 0.00002359
Iteration 109/1000 | Loss: 0.00002359
Iteration 110/1000 | Loss: 0.00002359
Iteration 111/1000 | Loss: 0.00002359
Iteration 112/1000 | Loss: 0.00002359
Iteration 113/1000 | Loss: 0.00002359
Iteration 114/1000 | Loss: 0.00002359
Iteration 115/1000 | Loss: 0.00002359
Iteration 116/1000 | Loss: 0.00002359
Iteration 117/1000 | Loss: 0.00002359
Iteration 118/1000 | Loss: 0.00002359
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002359
Iteration 121/1000 | Loss: 0.00002359
Iteration 122/1000 | Loss: 0.00002359
Iteration 123/1000 | Loss: 0.00002359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.359388417971786e-05, 2.359388417971786e-05, 2.359388417971786e-05, 2.359388417971786e-05, 2.359388417971786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.359388417971786e-05

Optimization complete. Final v2v error: 4.1933770179748535 mm

Highest mean error: 4.588566780090332 mm for frame 68

Lowest mean error: 3.8372836112976074 mm for frame 8

Saving results

Total time: 37.220524311065674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998794
Iteration 2/25 | Loss: 0.00123411
Iteration 3/25 | Loss: 0.00104710
Iteration 4/25 | Loss: 0.00100956
Iteration 5/25 | Loss: 0.00099021
Iteration 6/25 | Loss: 0.00098488
Iteration 7/25 | Loss: 0.00098361
Iteration 8/25 | Loss: 0.00098353
Iteration 9/25 | Loss: 0.00098353
Iteration 10/25 | Loss: 0.00098353
Iteration 11/25 | Loss: 0.00098353
Iteration 12/25 | Loss: 0.00098353
Iteration 13/25 | Loss: 0.00098353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009835310047492385, 0.0009835310047492385, 0.0009835310047492385, 0.0009835310047492385, 0.0009835310047492385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009835310047492385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48672330
Iteration 2/25 | Loss: 0.00067536
Iteration 3/25 | Loss: 0.00067534
Iteration 4/25 | Loss: 0.00067534
Iteration 5/25 | Loss: 0.00067534
Iteration 6/25 | Loss: 0.00067534
Iteration 7/25 | Loss: 0.00067534
Iteration 8/25 | Loss: 0.00067534
Iteration 9/25 | Loss: 0.00067534
Iteration 10/25 | Loss: 0.00067534
Iteration 11/25 | Loss: 0.00067534
Iteration 12/25 | Loss: 0.00067534
Iteration 13/25 | Loss: 0.00067534
Iteration 14/25 | Loss: 0.00067534
Iteration 15/25 | Loss: 0.00067534
Iteration 16/25 | Loss: 0.00067534
Iteration 17/25 | Loss: 0.00067534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006753387278877199, 0.0006753387278877199, 0.0006753387278877199, 0.0006753387278877199, 0.0006753387278877199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006753387278877199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067534
Iteration 2/1000 | Loss: 0.00006709
Iteration 3/1000 | Loss: 0.00004510
Iteration 4/1000 | Loss: 0.00003923
Iteration 5/1000 | Loss: 0.00003584
Iteration 6/1000 | Loss: 0.00003383
Iteration 7/1000 | Loss: 0.00003260
Iteration 8/1000 | Loss: 0.00003176
Iteration 9/1000 | Loss: 0.00003126
Iteration 10/1000 | Loss: 0.00003074
Iteration 11/1000 | Loss: 0.00003047
Iteration 12/1000 | Loss: 0.00003027
Iteration 13/1000 | Loss: 0.00003018
Iteration 14/1000 | Loss: 0.00003011
Iteration 15/1000 | Loss: 0.00003011
Iteration 16/1000 | Loss: 0.00003008
Iteration 17/1000 | Loss: 0.00003008
Iteration 18/1000 | Loss: 0.00003008
Iteration 19/1000 | Loss: 0.00003008
Iteration 20/1000 | Loss: 0.00003007
Iteration 21/1000 | Loss: 0.00003007
Iteration 22/1000 | Loss: 0.00003007
Iteration 23/1000 | Loss: 0.00003007
Iteration 24/1000 | Loss: 0.00003007
Iteration 25/1000 | Loss: 0.00003007
Iteration 26/1000 | Loss: 0.00003007
Iteration 27/1000 | Loss: 0.00003007
Iteration 28/1000 | Loss: 0.00003007
Iteration 29/1000 | Loss: 0.00003007
Iteration 30/1000 | Loss: 0.00003007
Iteration 31/1000 | Loss: 0.00003007
Iteration 32/1000 | Loss: 0.00003006
Iteration 33/1000 | Loss: 0.00003006
Iteration 34/1000 | Loss: 0.00003006
Iteration 35/1000 | Loss: 0.00003006
Iteration 36/1000 | Loss: 0.00003006
Iteration 37/1000 | Loss: 0.00003004
Iteration 38/1000 | Loss: 0.00003004
Iteration 39/1000 | Loss: 0.00003003
Iteration 40/1000 | Loss: 0.00003003
Iteration 41/1000 | Loss: 0.00003003
Iteration 42/1000 | Loss: 0.00003003
Iteration 43/1000 | Loss: 0.00003002
Iteration 44/1000 | Loss: 0.00003002
Iteration 45/1000 | Loss: 0.00003002
Iteration 46/1000 | Loss: 0.00003002
Iteration 47/1000 | Loss: 0.00003002
Iteration 48/1000 | Loss: 0.00003002
Iteration 49/1000 | Loss: 0.00003002
Iteration 50/1000 | Loss: 0.00003002
Iteration 51/1000 | Loss: 0.00003001
Iteration 52/1000 | Loss: 0.00003001
Iteration 53/1000 | Loss: 0.00003001
Iteration 54/1000 | Loss: 0.00003001
Iteration 55/1000 | Loss: 0.00003000
Iteration 56/1000 | Loss: 0.00003000
Iteration 57/1000 | Loss: 0.00002999
Iteration 58/1000 | Loss: 0.00002999
Iteration 59/1000 | Loss: 0.00002999
Iteration 60/1000 | Loss: 0.00002999
Iteration 61/1000 | Loss: 0.00002998
Iteration 62/1000 | Loss: 0.00002998
Iteration 63/1000 | Loss: 0.00002998
Iteration 64/1000 | Loss: 0.00002997
Iteration 65/1000 | Loss: 0.00002997
Iteration 66/1000 | Loss: 0.00002997
Iteration 67/1000 | Loss: 0.00002997
Iteration 68/1000 | Loss: 0.00002997
Iteration 69/1000 | Loss: 0.00002997
Iteration 70/1000 | Loss: 0.00002997
Iteration 71/1000 | Loss: 0.00002997
Iteration 72/1000 | Loss: 0.00002997
Iteration 73/1000 | Loss: 0.00002997
Iteration 74/1000 | Loss: 0.00002997
Iteration 75/1000 | Loss: 0.00002997
Iteration 76/1000 | Loss: 0.00002997
Iteration 77/1000 | Loss: 0.00002997
Iteration 78/1000 | Loss: 0.00002997
Iteration 79/1000 | Loss: 0.00002997
Iteration 80/1000 | Loss: 0.00002997
Iteration 81/1000 | Loss: 0.00002997
Iteration 82/1000 | Loss: 0.00002997
Iteration 83/1000 | Loss: 0.00002997
Iteration 84/1000 | Loss: 0.00002996
Iteration 85/1000 | Loss: 0.00002996
Iteration 86/1000 | Loss: 0.00002996
Iteration 87/1000 | Loss: 0.00002996
Iteration 88/1000 | Loss: 0.00002996
Iteration 89/1000 | Loss: 0.00002996
Iteration 90/1000 | Loss: 0.00002996
Iteration 91/1000 | Loss: 0.00002996
Iteration 92/1000 | Loss: 0.00002996
Iteration 93/1000 | Loss: 0.00002996
Iteration 94/1000 | Loss: 0.00002996
Iteration 95/1000 | Loss: 0.00002996
Iteration 96/1000 | Loss: 0.00002996
Iteration 97/1000 | Loss: 0.00002995
Iteration 98/1000 | Loss: 0.00002995
Iteration 99/1000 | Loss: 0.00002995
Iteration 100/1000 | Loss: 0.00002995
Iteration 101/1000 | Loss: 0.00002995
Iteration 102/1000 | Loss: 0.00002995
Iteration 103/1000 | Loss: 0.00002995
Iteration 104/1000 | Loss: 0.00002995
Iteration 105/1000 | Loss: 0.00002995
Iteration 106/1000 | Loss: 0.00002994
Iteration 107/1000 | Loss: 0.00002994
Iteration 108/1000 | Loss: 0.00002994
Iteration 109/1000 | Loss: 0.00002994
Iteration 110/1000 | Loss: 0.00002993
Iteration 111/1000 | Loss: 0.00002993
Iteration 112/1000 | Loss: 0.00002993
Iteration 113/1000 | Loss: 0.00002993
Iteration 114/1000 | Loss: 0.00002993
Iteration 115/1000 | Loss: 0.00002993
Iteration 116/1000 | Loss: 0.00002993
Iteration 117/1000 | Loss: 0.00002993
Iteration 118/1000 | Loss: 0.00002993
Iteration 119/1000 | Loss: 0.00002993
Iteration 120/1000 | Loss: 0.00002992
Iteration 121/1000 | Loss: 0.00002992
Iteration 122/1000 | Loss: 0.00002992
Iteration 123/1000 | Loss: 0.00002992
Iteration 124/1000 | Loss: 0.00002992
Iteration 125/1000 | Loss: 0.00002992
Iteration 126/1000 | Loss: 0.00002992
Iteration 127/1000 | Loss: 0.00002992
Iteration 128/1000 | Loss: 0.00002992
Iteration 129/1000 | Loss: 0.00002991
Iteration 130/1000 | Loss: 0.00002991
Iteration 131/1000 | Loss: 0.00002991
Iteration 132/1000 | Loss: 0.00002991
Iteration 133/1000 | Loss: 0.00002991
Iteration 134/1000 | Loss: 0.00002991
Iteration 135/1000 | Loss: 0.00002991
Iteration 136/1000 | Loss: 0.00002991
Iteration 137/1000 | Loss: 0.00002991
Iteration 138/1000 | Loss: 0.00002990
Iteration 139/1000 | Loss: 0.00002990
Iteration 140/1000 | Loss: 0.00002990
Iteration 141/1000 | Loss: 0.00002990
Iteration 142/1000 | Loss: 0.00002990
Iteration 143/1000 | Loss: 0.00002990
Iteration 144/1000 | Loss: 0.00002989
Iteration 145/1000 | Loss: 0.00002989
Iteration 146/1000 | Loss: 0.00002989
Iteration 147/1000 | Loss: 0.00002989
Iteration 148/1000 | Loss: 0.00002989
Iteration 149/1000 | Loss: 0.00002989
Iteration 150/1000 | Loss: 0.00002989
Iteration 151/1000 | Loss: 0.00002989
Iteration 152/1000 | Loss: 0.00002989
Iteration 153/1000 | Loss: 0.00002988
Iteration 154/1000 | Loss: 0.00002988
Iteration 155/1000 | Loss: 0.00002988
Iteration 156/1000 | Loss: 0.00002988
Iteration 157/1000 | Loss: 0.00002988
Iteration 158/1000 | Loss: 0.00002988
Iteration 159/1000 | Loss: 0.00002987
Iteration 160/1000 | Loss: 0.00002987
Iteration 161/1000 | Loss: 0.00002987
Iteration 162/1000 | Loss: 0.00002987
Iteration 163/1000 | Loss: 0.00002987
Iteration 164/1000 | Loss: 0.00002987
Iteration 165/1000 | Loss: 0.00002987
Iteration 166/1000 | Loss: 0.00002987
Iteration 167/1000 | Loss: 0.00002987
Iteration 168/1000 | Loss: 0.00002986
Iteration 169/1000 | Loss: 0.00002986
Iteration 170/1000 | Loss: 0.00002986
Iteration 171/1000 | Loss: 0.00002986
Iteration 172/1000 | Loss: 0.00002986
Iteration 173/1000 | Loss: 0.00002986
Iteration 174/1000 | Loss: 0.00002986
Iteration 175/1000 | Loss: 0.00002986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.9864509997423738e-05, 2.9864509997423738e-05, 2.9864509997423738e-05, 2.9864509997423738e-05, 2.9864509997423738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9864509997423738e-05

Optimization complete. Final v2v error: 4.472512245178223 mm

Highest mean error: 6.40315580368042 mm for frame 73

Lowest mean error: 3.8713881969451904 mm for frame 15

Saving results

Total time: 38.36799740791321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404734
Iteration 2/25 | Loss: 0.00116898
Iteration 3/25 | Loss: 0.00103575
Iteration 4/25 | Loss: 0.00100786
Iteration 5/25 | Loss: 0.00099705
Iteration 6/25 | Loss: 0.00099403
Iteration 7/25 | Loss: 0.00099289
Iteration 8/25 | Loss: 0.00099241
Iteration 9/25 | Loss: 0.00099224
Iteration 10/25 | Loss: 0.00099224
Iteration 11/25 | Loss: 0.00099224
Iteration 12/25 | Loss: 0.00099224
Iteration 13/25 | Loss: 0.00099224
Iteration 14/25 | Loss: 0.00099224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009922394528985023, 0.0009922394528985023, 0.0009922394528985023, 0.0009922394528985023, 0.0009922394528985023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009922394528985023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62296665
Iteration 2/25 | Loss: 0.00108394
Iteration 3/25 | Loss: 0.00108393
Iteration 4/25 | Loss: 0.00108393
Iteration 5/25 | Loss: 0.00108393
Iteration 6/25 | Loss: 0.00108393
Iteration 7/25 | Loss: 0.00108393
Iteration 8/25 | Loss: 0.00108393
Iteration 9/25 | Loss: 0.00108393
Iteration 10/25 | Loss: 0.00108393
Iteration 11/25 | Loss: 0.00108393
Iteration 12/25 | Loss: 0.00108393
Iteration 13/25 | Loss: 0.00108393
Iteration 14/25 | Loss: 0.00108393
Iteration 15/25 | Loss: 0.00108393
Iteration 16/25 | Loss: 0.00108393
Iteration 17/25 | Loss: 0.00108393
Iteration 18/25 | Loss: 0.00108393
Iteration 19/25 | Loss: 0.00108393
Iteration 20/25 | Loss: 0.00108393
Iteration 21/25 | Loss: 0.00108393
Iteration 22/25 | Loss: 0.00108393
Iteration 23/25 | Loss: 0.00108393
Iteration 24/25 | Loss: 0.00108393
Iteration 25/25 | Loss: 0.00108393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108393
Iteration 2/1000 | Loss: 0.00005485
Iteration 3/1000 | Loss: 0.00003815
Iteration 4/1000 | Loss: 0.00003268
Iteration 5/1000 | Loss: 0.00002881
Iteration 6/1000 | Loss: 0.00002723
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00002554
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002461
Iteration 11/1000 | Loss: 0.00002436
Iteration 12/1000 | Loss: 0.00002414
Iteration 13/1000 | Loss: 0.00002395
Iteration 14/1000 | Loss: 0.00002387
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002385
Iteration 17/1000 | Loss: 0.00002385
Iteration 18/1000 | Loss: 0.00002385
Iteration 19/1000 | Loss: 0.00002384
Iteration 20/1000 | Loss: 0.00002383
Iteration 21/1000 | Loss: 0.00002382
Iteration 22/1000 | Loss: 0.00002382
Iteration 23/1000 | Loss: 0.00002381
Iteration 24/1000 | Loss: 0.00002380
Iteration 25/1000 | Loss: 0.00002379
Iteration 26/1000 | Loss: 0.00002379
Iteration 27/1000 | Loss: 0.00002378
Iteration 28/1000 | Loss: 0.00002378
Iteration 29/1000 | Loss: 0.00002375
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002368
Iteration 32/1000 | Loss: 0.00002364
Iteration 33/1000 | Loss: 0.00002363
Iteration 34/1000 | Loss: 0.00002363
Iteration 35/1000 | Loss: 0.00002362
Iteration 36/1000 | Loss: 0.00002361
Iteration 37/1000 | Loss: 0.00002361
Iteration 38/1000 | Loss: 0.00002361
Iteration 39/1000 | Loss: 0.00002361
Iteration 40/1000 | Loss: 0.00002361
Iteration 41/1000 | Loss: 0.00002360
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002360
Iteration 44/1000 | Loss: 0.00002360
Iteration 45/1000 | Loss: 0.00002359
Iteration 46/1000 | Loss: 0.00002359
Iteration 47/1000 | Loss: 0.00002359
Iteration 48/1000 | Loss: 0.00002358
Iteration 49/1000 | Loss: 0.00002358
Iteration 50/1000 | Loss: 0.00002358
Iteration 51/1000 | Loss: 0.00002357
Iteration 52/1000 | Loss: 0.00002357
Iteration 53/1000 | Loss: 0.00002356
Iteration 54/1000 | Loss: 0.00002356
Iteration 55/1000 | Loss: 0.00002355
Iteration 56/1000 | Loss: 0.00002355
Iteration 57/1000 | Loss: 0.00002355
Iteration 58/1000 | Loss: 0.00002354
Iteration 59/1000 | Loss: 0.00002354
Iteration 60/1000 | Loss: 0.00002354
Iteration 61/1000 | Loss: 0.00002354
Iteration 62/1000 | Loss: 0.00002353
Iteration 63/1000 | Loss: 0.00002353
Iteration 64/1000 | Loss: 0.00002352
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002351
Iteration 70/1000 | Loss: 0.00002351
Iteration 71/1000 | Loss: 0.00002351
Iteration 72/1000 | Loss: 0.00002351
Iteration 73/1000 | Loss: 0.00002351
Iteration 74/1000 | Loss: 0.00002350
Iteration 75/1000 | Loss: 0.00002350
Iteration 76/1000 | Loss: 0.00002350
Iteration 77/1000 | Loss: 0.00002350
Iteration 78/1000 | Loss: 0.00002350
Iteration 79/1000 | Loss: 0.00002350
Iteration 80/1000 | Loss: 0.00002350
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002349
Iteration 84/1000 | Loss: 0.00002349
Iteration 85/1000 | Loss: 0.00002349
Iteration 86/1000 | Loss: 0.00002349
Iteration 87/1000 | Loss: 0.00002349
Iteration 88/1000 | Loss: 0.00002349
Iteration 89/1000 | Loss: 0.00002349
Iteration 90/1000 | Loss: 0.00002348
Iteration 91/1000 | Loss: 0.00002348
Iteration 92/1000 | Loss: 0.00002348
Iteration 93/1000 | Loss: 0.00002347
Iteration 94/1000 | Loss: 0.00002347
Iteration 95/1000 | Loss: 0.00002347
Iteration 96/1000 | Loss: 0.00002346
Iteration 97/1000 | Loss: 0.00002346
Iteration 98/1000 | Loss: 0.00002346
Iteration 99/1000 | Loss: 0.00002345
Iteration 100/1000 | Loss: 0.00002345
Iteration 101/1000 | Loss: 0.00002345
Iteration 102/1000 | Loss: 0.00002345
Iteration 103/1000 | Loss: 0.00002345
Iteration 104/1000 | Loss: 0.00002344
Iteration 105/1000 | Loss: 0.00002344
Iteration 106/1000 | Loss: 0.00002344
Iteration 107/1000 | Loss: 0.00002344
Iteration 108/1000 | Loss: 0.00002343
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002342
Iteration 114/1000 | Loss: 0.00002341
Iteration 115/1000 | Loss: 0.00002341
Iteration 116/1000 | Loss: 0.00002341
Iteration 117/1000 | Loss: 0.00002341
Iteration 118/1000 | Loss: 0.00002340
Iteration 119/1000 | Loss: 0.00002340
Iteration 120/1000 | Loss: 0.00002340
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00002339
Iteration 123/1000 | Loss: 0.00002339
Iteration 124/1000 | Loss: 0.00002339
Iteration 125/1000 | Loss: 0.00002339
Iteration 126/1000 | Loss: 0.00002339
Iteration 127/1000 | Loss: 0.00002338
Iteration 128/1000 | Loss: 0.00002338
Iteration 129/1000 | Loss: 0.00002338
Iteration 130/1000 | Loss: 0.00002338
Iteration 131/1000 | Loss: 0.00002338
Iteration 132/1000 | Loss: 0.00002338
Iteration 133/1000 | Loss: 0.00002338
Iteration 134/1000 | Loss: 0.00002338
Iteration 135/1000 | Loss: 0.00002337
Iteration 136/1000 | Loss: 0.00002337
Iteration 137/1000 | Loss: 0.00002337
Iteration 138/1000 | Loss: 0.00002337
Iteration 139/1000 | Loss: 0.00002337
Iteration 140/1000 | Loss: 0.00002337
Iteration 141/1000 | Loss: 0.00002337
Iteration 142/1000 | Loss: 0.00002337
Iteration 143/1000 | Loss: 0.00002337
Iteration 144/1000 | Loss: 0.00002337
Iteration 145/1000 | Loss: 0.00002337
Iteration 146/1000 | Loss: 0.00002337
Iteration 147/1000 | Loss: 0.00002337
Iteration 148/1000 | Loss: 0.00002336
Iteration 149/1000 | Loss: 0.00002336
Iteration 150/1000 | Loss: 0.00002336
Iteration 151/1000 | Loss: 0.00002336
Iteration 152/1000 | Loss: 0.00002336
Iteration 153/1000 | Loss: 0.00002336
Iteration 154/1000 | Loss: 0.00002336
Iteration 155/1000 | Loss: 0.00002336
Iteration 156/1000 | Loss: 0.00002336
Iteration 157/1000 | Loss: 0.00002336
Iteration 158/1000 | Loss: 0.00002336
Iteration 159/1000 | Loss: 0.00002336
Iteration 160/1000 | Loss: 0.00002336
Iteration 161/1000 | Loss: 0.00002336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.3360598788713105e-05, 2.3360598788713105e-05, 2.3360598788713105e-05, 2.3360598788713105e-05, 2.3360598788713105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3360598788713105e-05

Optimization complete. Final v2v error: 4.027363300323486 mm

Highest mean error: 5.474215030670166 mm for frame 76

Lowest mean error: 3.1953654289245605 mm for frame 122

Saving results

Total time: 44.407827615737915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00543204
Iteration 2/25 | Loss: 0.00120483
Iteration 3/25 | Loss: 0.00103913
Iteration 4/25 | Loss: 0.00099712
Iteration 5/25 | Loss: 0.00099143
Iteration 6/25 | Loss: 0.00099084
Iteration 7/25 | Loss: 0.00099078
Iteration 8/25 | Loss: 0.00099078
Iteration 9/25 | Loss: 0.00099078
Iteration 10/25 | Loss: 0.00099078
Iteration 11/25 | Loss: 0.00099078
Iteration 12/25 | Loss: 0.00099078
Iteration 13/25 | Loss: 0.00099078
Iteration 14/25 | Loss: 0.00099078
Iteration 15/25 | Loss: 0.00099078
Iteration 16/25 | Loss: 0.00099078
Iteration 17/25 | Loss: 0.00099078
Iteration 18/25 | Loss: 0.00099078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009907777421176434, 0.0009907777421176434, 0.0009907777421176434, 0.0009907777421176434, 0.0009907777421176434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009907777421176434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44111192
Iteration 2/25 | Loss: 0.00083216
Iteration 3/25 | Loss: 0.00083215
Iteration 4/25 | Loss: 0.00083214
Iteration 5/25 | Loss: 0.00083214
Iteration 6/25 | Loss: 0.00083214
Iteration 7/25 | Loss: 0.00083214
Iteration 8/25 | Loss: 0.00083214
Iteration 9/25 | Loss: 0.00083214
Iteration 10/25 | Loss: 0.00083214
Iteration 11/25 | Loss: 0.00083214
Iteration 12/25 | Loss: 0.00083214
Iteration 13/25 | Loss: 0.00083214
Iteration 14/25 | Loss: 0.00083214
Iteration 15/25 | Loss: 0.00083214
Iteration 16/25 | Loss: 0.00083214
Iteration 17/25 | Loss: 0.00083214
Iteration 18/25 | Loss: 0.00083214
Iteration 19/25 | Loss: 0.00083214
Iteration 20/25 | Loss: 0.00083214
Iteration 21/25 | Loss: 0.00083214
Iteration 22/25 | Loss: 0.00083214
Iteration 23/25 | Loss: 0.00083214
Iteration 24/25 | Loss: 0.00083214
Iteration 25/25 | Loss: 0.00083214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083214
Iteration 2/1000 | Loss: 0.00003635
Iteration 3/1000 | Loss: 0.00002933
Iteration 4/1000 | Loss: 0.00002574
Iteration 5/1000 | Loss: 0.00002413
Iteration 6/1000 | Loss: 0.00002305
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002198
Iteration 9/1000 | Loss: 0.00002171
Iteration 10/1000 | Loss: 0.00002150
Iteration 11/1000 | Loss: 0.00002150
Iteration 12/1000 | Loss: 0.00002149
Iteration 13/1000 | Loss: 0.00002147
Iteration 14/1000 | Loss: 0.00002145
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002143
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002137
Iteration 21/1000 | Loss: 0.00002136
Iteration 22/1000 | Loss: 0.00002134
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002133
Iteration 25/1000 | Loss: 0.00002132
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002130
Iteration 30/1000 | Loss: 0.00002130
Iteration 31/1000 | Loss: 0.00002130
Iteration 32/1000 | Loss: 0.00002129
Iteration 33/1000 | Loss: 0.00002129
Iteration 34/1000 | Loss: 0.00002129
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002127
Iteration 41/1000 | Loss: 0.00002127
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002126
Iteration 44/1000 | Loss: 0.00002126
Iteration 45/1000 | Loss: 0.00002126
Iteration 46/1000 | Loss: 0.00002126
Iteration 47/1000 | Loss: 0.00002126
Iteration 48/1000 | Loss: 0.00002126
Iteration 49/1000 | Loss: 0.00002126
Iteration 50/1000 | Loss: 0.00002126
Iteration 51/1000 | Loss: 0.00002126
Iteration 52/1000 | Loss: 0.00002125
Iteration 53/1000 | Loss: 0.00002125
Iteration 54/1000 | Loss: 0.00002125
Iteration 55/1000 | Loss: 0.00002125
Iteration 56/1000 | Loss: 0.00002124
Iteration 57/1000 | Loss: 0.00002124
Iteration 58/1000 | Loss: 0.00002124
Iteration 59/1000 | Loss: 0.00002124
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002123
Iteration 71/1000 | Loss: 0.00002123
Iteration 72/1000 | Loss: 0.00002123
Iteration 73/1000 | Loss: 0.00002123
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002122
Iteration 77/1000 | Loss: 0.00002122
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002122
Iteration 81/1000 | Loss: 0.00002122
Iteration 82/1000 | Loss: 0.00002122
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002121
Iteration 87/1000 | Loss: 0.00002121
Iteration 88/1000 | Loss: 0.00002121
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002121
Iteration 93/1000 | Loss: 0.00002121
Iteration 94/1000 | Loss: 0.00002121
Iteration 95/1000 | Loss: 0.00002121
Iteration 96/1000 | Loss: 0.00002121
Iteration 97/1000 | Loss: 0.00002121
Iteration 98/1000 | Loss: 0.00002121
Iteration 99/1000 | Loss: 0.00002120
Iteration 100/1000 | Loss: 0.00002120
Iteration 101/1000 | Loss: 0.00002120
Iteration 102/1000 | Loss: 0.00002120
Iteration 103/1000 | Loss: 0.00002120
Iteration 104/1000 | Loss: 0.00002120
Iteration 105/1000 | Loss: 0.00002120
Iteration 106/1000 | Loss: 0.00002120
Iteration 107/1000 | Loss: 0.00002120
Iteration 108/1000 | Loss: 0.00002120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.1204912627581507e-05, 2.1204912627581507e-05, 2.1204912627581507e-05, 2.1204912627581507e-05, 2.1204912627581507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1204912627581507e-05

Optimization complete. Final v2v error: 3.927672863006592 mm

Highest mean error: 4.166205406188965 mm for frame 136

Lowest mean error: 3.777104616165161 mm for frame 53

Saving results

Total time: 30.876802444458008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421351
Iteration 2/25 | Loss: 0.00099240
Iteration 3/25 | Loss: 0.00086277
Iteration 4/25 | Loss: 0.00084787
Iteration 5/25 | Loss: 0.00084426
Iteration 6/25 | Loss: 0.00084295
Iteration 7/25 | Loss: 0.00084263
Iteration 8/25 | Loss: 0.00084263
Iteration 9/25 | Loss: 0.00084263
Iteration 10/25 | Loss: 0.00084263
Iteration 11/25 | Loss: 0.00084263
Iteration 12/25 | Loss: 0.00084263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008426274289377034, 0.0008426274289377034, 0.0008426274289377034, 0.0008426274289377034, 0.0008426274289377034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008426274289377034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49804139
Iteration 2/25 | Loss: 0.00056326
Iteration 3/25 | Loss: 0.00056326
Iteration 4/25 | Loss: 0.00056326
Iteration 5/25 | Loss: 0.00056326
Iteration 6/25 | Loss: 0.00056326
Iteration 7/25 | Loss: 0.00056326
Iteration 8/25 | Loss: 0.00056326
Iteration 9/25 | Loss: 0.00056326
Iteration 10/25 | Loss: 0.00056326
Iteration 11/25 | Loss: 0.00056326
Iteration 12/25 | Loss: 0.00056326
Iteration 13/25 | Loss: 0.00056326
Iteration 14/25 | Loss: 0.00056326
Iteration 15/25 | Loss: 0.00056326
Iteration 16/25 | Loss: 0.00056326
Iteration 17/25 | Loss: 0.00056326
Iteration 18/25 | Loss: 0.00056326
Iteration 19/25 | Loss: 0.00056326
Iteration 20/25 | Loss: 0.00056326
Iteration 21/25 | Loss: 0.00056326
Iteration 22/25 | Loss: 0.00056326
Iteration 23/25 | Loss: 0.00056326
Iteration 24/25 | Loss: 0.00056326
Iteration 25/25 | Loss: 0.00056326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005632572574540973, 0.0005632572574540973, 0.0005632572574540973, 0.0005632572574540973, 0.0005632572574540973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005632572574540973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056326
Iteration 2/1000 | Loss: 0.00003182
Iteration 3/1000 | Loss: 0.00002017
Iteration 4/1000 | Loss: 0.00001688
Iteration 5/1000 | Loss: 0.00001576
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001433
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001430
Iteration 17/1000 | Loss: 0.00001430
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001429
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001428
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001427
Iteration 32/1000 | Loss: 0.00001426
Iteration 33/1000 | Loss: 0.00001425
Iteration 34/1000 | Loss: 0.00001425
Iteration 35/1000 | Loss: 0.00001425
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001421
Iteration 43/1000 | Loss: 0.00001421
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001421
Iteration 47/1000 | Loss: 0.00001420
Iteration 48/1000 | Loss: 0.00001420
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001420
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001417
Iteration 61/1000 | Loss: 0.00001417
Iteration 62/1000 | Loss: 0.00001416
Iteration 63/1000 | Loss: 0.00001416
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001416
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001415
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001413
Iteration 78/1000 | Loss: 0.00001413
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001411
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001411
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001411
Iteration 113/1000 | Loss: 0.00001411
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Iteration 144/1000 | Loss: 0.00001411
Iteration 145/1000 | Loss: 0.00001411
Iteration 146/1000 | Loss: 0.00001411
Iteration 147/1000 | Loss: 0.00001411
Iteration 148/1000 | Loss: 0.00001411
Iteration 149/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4108068171481136e-05, 1.4108068171481136e-05, 1.4108068171481136e-05, 1.4108068171481136e-05, 1.4108068171481136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4108068171481136e-05

Optimization complete. Final v2v error: 3.2483301162719727 mm

Highest mean error: 3.8967719078063965 mm for frame 55

Lowest mean error: 2.999389886856079 mm for frame 142

Saving results

Total time: 30.012482404708862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_33_nl_5886/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_33_nl_5886/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959379
Iteration 2/25 | Loss: 0.00303207
Iteration 3/25 | Loss: 0.00198780
Iteration 4/25 | Loss: 0.00159638
Iteration 5/25 | Loss: 0.00146496
Iteration 6/25 | Loss: 0.00150021
Iteration 7/25 | Loss: 0.00136951
Iteration 8/25 | Loss: 0.00131753
Iteration 9/25 | Loss: 0.00123569
Iteration 10/25 | Loss: 0.00116435
Iteration 11/25 | Loss: 0.00116674
Iteration 12/25 | Loss: 0.00116413
Iteration 13/25 | Loss: 0.00112261
Iteration 14/25 | Loss: 0.00112192
Iteration 15/25 | Loss: 0.00113061
Iteration 16/25 | Loss: 0.00111909
Iteration 17/25 | Loss: 0.00109625
Iteration 18/25 | Loss: 0.00107971
Iteration 19/25 | Loss: 0.00109288
Iteration 20/25 | Loss: 0.00109292
Iteration 21/25 | Loss: 0.00107943
Iteration 22/25 | Loss: 0.00108877
Iteration 23/25 | Loss: 0.00108137
Iteration 24/25 | Loss: 0.00108925
Iteration 25/25 | Loss: 0.00107767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53530025
Iteration 2/25 | Loss: 0.00272652
Iteration 3/25 | Loss: 0.00267363
Iteration 4/25 | Loss: 0.00267363
Iteration 5/25 | Loss: 0.00267363
Iteration 6/25 | Loss: 0.00267363
Iteration 7/25 | Loss: 0.00267363
Iteration 8/25 | Loss: 0.00267363
Iteration 9/25 | Loss: 0.00267363
Iteration 10/25 | Loss: 0.00267363
Iteration 11/25 | Loss: 0.00267362
Iteration 12/25 | Loss: 0.00267362
Iteration 13/25 | Loss: 0.00267362
Iteration 14/25 | Loss: 0.00267362
Iteration 15/25 | Loss: 0.00267362
Iteration 16/25 | Loss: 0.00267362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026736247818917036, 0.0026736247818917036, 0.0026736247818917036, 0.0026736247818917036, 0.0026736247818917036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026736247818917036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267363
Iteration 2/1000 | Loss: 0.00056484
Iteration 3/1000 | Loss: 0.00457560
Iteration 4/1000 | Loss: 0.00062628
Iteration 5/1000 | Loss: 0.00832120
Iteration 6/1000 | Loss: 0.00218702
Iteration 7/1000 | Loss: 0.00602365
Iteration 8/1000 | Loss: 0.00171267
Iteration 9/1000 | Loss: 0.00602470
Iteration 10/1000 | Loss: 0.00341003
Iteration 11/1000 | Loss: 0.00366721
Iteration 12/1000 | Loss: 0.00210454
Iteration 13/1000 | Loss: 0.00086814
Iteration 14/1000 | Loss: 0.00151629
Iteration 15/1000 | Loss: 0.00228659
Iteration 16/1000 | Loss: 0.00274139
Iteration 17/1000 | Loss: 0.00122414
Iteration 18/1000 | Loss: 0.00066100
Iteration 19/1000 | Loss: 0.00128263
Iteration 20/1000 | Loss: 0.00176599
Iteration 21/1000 | Loss: 0.00058795
Iteration 22/1000 | Loss: 0.00136171
Iteration 23/1000 | Loss: 0.00067650
Iteration 24/1000 | Loss: 0.00449789
Iteration 25/1000 | Loss: 0.00538252
Iteration 26/1000 | Loss: 0.00126806
Iteration 27/1000 | Loss: 0.00218318
Iteration 28/1000 | Loss: 0.00089961
Iteration 29/1000 | Loss: 0.00103053
Iteration 30/1000 | Loss: 0.00121885
Iteration 31/1000 | Loss: 0.00242173
Iteration 32/1000 | Loss: 0.00206054
Iteration 33/1000 | Loss: 0.00286304
Iteration 34/1000 | Loss: 0.00236730
Iteration 35/1000 | Loss: 0.00277108
Iteration 36/1000 | Loss: 0.00215116
Iteration 37/1000 | Loss: 0.00099299
Iteration 38/1000 | Loss: 0.00033646
Iteration 39/1000 | Loss: 0.00157105
Iteration 40/1000 | Loss: 0.00199760
Iteration 41/1000 | Loss: 0.00124200
Iteration 42/1000 | Loss: 0.00133768
Iteration 43/1000 | Loss: 0.00181739
Iteration 44/1000 | Loss: 0.00106429
Iteration 45/1000 | Loss: 0.00173093
Iteration 46/1000 | Loss: 0.00224286
Iteration 47/1000 | Loss: 0.00191083
Iteration 48/1000 | Loss: 0.00179462
Iteration 49/1000 | Loss: 0.00124057
Iteration 50/1000 | Loss: 0.00244784
Iteration 51/1000 | Loss: 0.00145483
Iteration 52/1000 | Loss: 0.00200813
Iteration 53/1000 | Loss: 0.00157430
Iteration 54/1000 | Loss: 0.00090910
Iteration 55/1000 | Loss: 0.00117922
Iteration 56/1000 | Loss: 0.00079916
Iteration 57/1000 | Loss: 0.00151232
Iteration 58/1000 | Loss: 0.00162201
Iteration 59/1000 | Loss: 0.00142235
Iteration 60/1000 | Loss: 0.00133320
Iteration 61/1000 | Loss: 0.00142943
Iteration 62/1000 | Loss: 0.00175637
Iteration 63/1000 | Loss: 0.00091726
Iteration 64/1000 | Loss: 0.00168074
Iteration 65/1000 | Loss: 0.00186386
Iteration 66/1000 | Loss: 0.00189679
Iteration 67/1000 | Loss: 0.00202334
Iteration 68/1000 | Loss: 0.00320230
Iteration 69/1000 | Loss: 0.00096972
Iteration 70/1000 | Loss: 0.00054341
Iteration 71/1000 | Loss: 0.00035225
Iteration 72/1000 | Loss: 0.00089641
Iteration 73/1000 | Loss: 0.00140421
Iteration 74/1000 | Loss: 0.00045064
Iteration 75/1000 | Loss: 0.00024613
Iteration 76/1000 | Loss: 0.00025019
Iteration 77/1000 | Loss: 0.00032610
Iteration 78/1000 | Loss: 0.00021605
Iteration 79/1000 | Loss: 0.00108025
Iteration 80/1000 | Loss: 0.00044023
Iteration 81/1000 | Loss: 0.00026395
Iteration 82/1000 | Loss: 0.00033936
Iteration 83/1000 | Loss: 0.00080932
Iteration 84/1000 | Loss: 0.00022468
Iteration 85/1000 | Loss: 0.00015690
Iteration 86/1000 | Loss: 0.00018896
Iteration 87/1000 | Loss: 0.00006058
Iteration 88/1000 | Loss: 0.00030274
Iteration 89/1000 | Loss: 0.00009712
Iteration 90/1000 | Loss: 0.00010946
Iteration 91/1000 | Loss: 0.00006375
Iteration 92/1000 | Loss: 0.00005906
Iteration 93/1000 | Loss: 0.00012424
Iteration 94/1000 | Loss: 0.00004600
Iteration 95/1000 | Loss: 0.00004275
Iteration 96/1000 | Loss: 0.00073627
Iteration 97/1000 | Loss: 0.00032599
Iteration 98/1000 | Loss: 0.00081936
Iteration 99/1000 | Loss: 0.00036722
Iteration 100/1000 | Loss: 0.00026180
Iteration 101/1000 | Loss: 0.00048331
Iteration 102/1000 | Loss: 0.00052258
Iteration 103/1000 | Loss: 0.00027698
Iteration 104/1000 | Loss: 0.00013899
Iteration 105/1000 | Loss: 0.00071598
Iteration 106/1000 | Loss: 0.00063003
Iteration 107/1000 | Loss: 0.00074402
Iteration 108/1000 | Loss: 0.00062099
Iteration 109/1000 | Loss: 0.00040033
Iteration 110/1000 | Loss: 0.00142805
Iteration 111/1000 | Loss: 0.00099093
Iteration 112/1000 | Loss: 0.00034782
Iteration 113/1000 | Loss: 0.00005952
Iteration 114/1000 | Loss: 0.00103687
Iteration 115/1000 | Loss: 0.00019606
Iteration 116/1000 | Loss: 0.00082824
Iteration 117/1000 | Loss: 0.00005153
Iteration 118/1000 | Loss: 0.00044252
Iteration 119/1000 | Loss: 0.00004183
Iteration 120/1000 | Loss: 0.00003622
Iteration 121/1000 | Loss: 0.00003337
Iteration 122/1000 | Loss: 0.00003164
Iteration 123/1000 | Loss: 0.00045480
Iteration 124/1000 | Loss: 0.00003016
Iteration 125/1000 | Loss: 0.00002925
Iteration 126/1000 | Loss: 0.00002879
Iteration 127/1000 | Loss: 0.00002821
Iteration 128/1000 | Loss: 0.00002775
Iteration 129/1000 | Loss: 0.00002743
Iteration 130/1000 | Loss: 0.00002718
Iteration 131/1000 | Loss: 0.00002689
Iteration 132/1000 | Loss: 0.00002687
Iteration 133/1000 | Loss: 0.00002670
Iteration 134/1000 | Loss: 0.00002669
Iteration 135/1000 | Loss: 0.00002666
Iteration 136/1000 | Loss: 0.00002665
Iteration 137/1000 | Loss: 0.00002651
Iteration 138/1000 | Loss: 0.00002648
Iteration 139/1000 | Loss: 0.00002640
Iteration 140/1000 | Loss: 0.00002639
Iteration 141/1000 | Loss: 0.00002636
Iteration 142/1000 | Loss: 0.00002635
Iteration 143/1000 | Loss: 0.00002634
Iteration 144/1000 | Loss: 0.00002633
Iteration 145/1000 | Loss: 0.00002633
Iteration 146/1000 | Loss: 0.00002632
Iteration 147/1000 | Loss: 0.00002629
Iteration 148/1000 | Loss: 0.00002629
Iteration 149/1000 | Loss: 0.00002629
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002627
Iteration 154/1000 | Loss: 0.00002627
Iteration 155/1000 | Loss: 0.00002627
Iteration 156/1000 | Loss: 0.00002627
Iteration 157/1000 | Loss: 0.00002626
Iteration 158/1000 | Loss: 0.00002626
Iteration 159/1000 | Loss: 0.00002626
Iteration 160/1000 | Loss: 0.00002626
Iteration 161/1000 | Loss: 0.00002626
Iteration 162/1000 | Loss: 0.00002626
Iteration 163/1000 | Loss: 0.00002625
Iteration 164/1000 | Loss: 0.00002625
Iteration 165/1000 | Loss: 0.00002625
Iteration 166/1000 | Loss: 0.00002625
Iteration 167/1000 | Loss: 0.00002625
Iteration 168/1000 | Loss: 0.00002625
Iteration 169/1000 | Loss: 0.00002624
Iteration 170/1000 | Loss: 0.00002624
Iteration 171/1000 | Loss: 0.00002624
Iteration 172/1000 | Loss: 0.00002623
Iteration 173/1000 | Loss: 0.00002623
Iteration 174/1000 | Loss: 0.00002622
Iteration 175/1000 | Loss: 0.00002621
Iteration 176/1000 | Loss: 0.00002621
Iteration 177/1000 | Loss: 0.00002620
Iteration 178/1000 | Loss: 0.00002619
Iteration 179/1000 | Loss: 0.00002619
Iteration 180/1000 | Loss: 0.00002619
Iteration 181/1000 | Loss: 0.00002619
Iteration 182/1000 | Loss: 0.00002619
Iteration 183/1000 | Loss: 0.00002619
Iteration 184/1000 | Loss: 0.00002618
Iteration 185/1000 | Loss: 0.00002618
Iteration 186/1000 | Loss: 0.00002618
Iteration 187/1000 | Loss: 0.00002618
Iteration 188/1000 | Loss: 0.00002618
Iteration 189/1000 | Loss: 0.00002618
Iteration 190/1000 | Loss: 0.00002618
Iteration 191/1000 | Loss: 0.00002618
Iteration 192/1000 | Loss: 0.00002617
Iteration 193/1000 | Loss: 0.00002617
Iteration 194/1000 | Loss: 0.00002617
Iteration 195/1000 | Loss: 0.00002617
Iteration 196/1000 | Loss: 0.00002617
Iteration 197/1000 | Loss: 0.00002616
Iteration 198/1000 | Loss: 0.00002616
Iteration 199/1000 | Loss: 0.00002616
Iteration 200/1000 | Loss: 0.00002616
Iteration 201/1000 | Loss: 0.00002615
Iteration 202/1000 | Loss: 0.00002615
Iteration 203/1000 | Loss: 0.00002615
Iteration 204/1000 | Loss: 0.00002615
Iteration 205/1000 | Loss: 0.00002615
Iteration 206/1000 | Loss: 0.00002615
Iteration 207/1000 | Loss: 0.00002615
Iteration 208/1000 | Loss: 0.00002615
Iteration 209/1000 | Loss: 0.00002615
Iteration 210/1000 | Loss: 0.00002614
Iteration 211/1000 | Loss: 0.00002614
Iteration 212/1000 | Loss: 0.00002614
Iteration 213/1000 | Loss: 0.00002614
Iteration 214/1000 | Loss: 0.00002614
Iteration 215/1000 | Loss: 0.00002613
Iteration 216/1000 | Loss: 0.00002613
Iteration 217/1000 | Loss: 0.00002613
Iteration 218/1000 | Loss: 0.00002613
Iteration 219/1000 | Loss: 0.00002613
Iteration 220/1000 | Loss: 0.00002613
Iteration 221/1000 | Loss: 0.00002613
Iteration 222/1000 | Loss: 0.00002613
Iteration 223/1000 | Loss: 0.00002613
Iteration 224/1000 | Loss: 0.00002613
Iteration 225/1000 | Loss: 0.00002612
Iteration 226/1000 | Loss: 0.00002612
Iteration 227/1000 | Loss: 0.00002611
Iteration 228/1000 | Loss: 0.00002611
Iteration 229/1000 | Loss: 0.00002611
Iteration 230/1000 | Loss: 0.00002611
Iteration 231/1000 | Loss: 0.00002611
Iteration 232/1000 | Loss: 0.00002611
Iteration 233/1000 | Loss: 0.00002610
Iteration 234/1000 | Loss: 0.00002610
Iteration 235/1000 | Loss: 0.00002610
Iteration 236/1000 | Loss: 0.00002610
Iteration 237/1000 | Loss: 0.00002609
Iteration 238/1000 | Loss: 0.00002609
Iteration 239/1000 | Loss: 0.00002609
Iteration 240/1000 | Loss: 0.00002609
Iteration 241/1000 | Loss: 0.00002609
Iteration 242/1000 | Loss: 0.00002609
Iteration 243/1000 | Loss: 0.00002608
Iteration 244/1000 | Loss: 0.00002608
Iteration 245/1000 | Loss: 0.00002608
Iteration 246/1000 | Loss: 0.00002608
Iteration 247/1000 | Loss: 0.00002607
Iteration 248/1000 | Loss: 0.00002607
Iteration 249/1000 | Loss: 0.00002607
Iteration 250/1000 | Loss: 0.00002607
Iteration 251/1000 | Loss: 0.00002607
Iteration 252/1000 | Loss: 0.00002607
Iteration 253/1000 | Loss: 0.00002607
Iteration 254/1000 | Loss: 0.00002607
Iteration 255/1000 | Loss: 0.00002607
Iteration 256/1000 | Loss: 0.00002606
Iteration 257/1000 | Loss: 0.00002606
Iteration 258/1000 | Loss: 0.00002606
Iteration 259/1000 | Loss: 0.00002606
Iteration 260/1000 | Loss: 0.00002606
Iteration 261/1000 | Loss: 0.00002606
Iteration 262/1000 | Loss: 0.00002606
Iteration 263/1000 | Loss: 0.00002606
Iteration 264/1000 | Loss: 0.00002605
Iteration 265/1000 | Loss: 0.00002605
Iteration 266/1000 | Loss: 0.00002605
Iteration 267/1000 | Loss: 0.00002605
Iteration 268/1000 | Loss: 0.00002604
Iteration 269/1000 | Loss: 0.00002604
Iteration 270/1000 | Loss: 0.00002603
Iteration 271/1000 | Loss: 0.00002603
Iteration 272/1000 | Loss: 0.00002603
Iteration 273/1000 | Loss: 0.00002602
Iteration 274/1000 | Loss: 0.00002602
Iteration 275/1000 | Loss: 0.00002602
Iteration 276/1000 | Loss: 0.00002602
Iteration 277/1000 | Loss: 0.00002601
Iteration 278/1000 | Loss: 0.00002601
Iteration 279/1000 | Loss: 0.00002601
Iteration 280/1000 | Loss: 0.00002601
Iteration 281/1000 | Loss: 0.00002601
Iteration 282/1000 | Loss: 0.00002600
Iteration 283/1000 | Loss: 0.00002600
Iteration 284/1000 | Loss: 0.00002598
Iteration 285/1000 | Loss: 0.00002597
Iteration 286/1000 | Loss: 0.00002595
Iteration 287/1000 | Loss: 0.00002581
Iteration 288/1000 | Loss: 0.00002580
Iteration 289/1000 | Loss: 0.00044529
Iteration 290/1000 | Loss: 0.00039518
Iteration 291/1000 | Loss: 0.00003435
Iteration 292/1000 | Loss: 0.00002979
Iteration 293/1000 | Loss: 0.00002765
Iteration 294/1000 | Loss: 0.00002653
Iteration 295/1000 | Loss: 0.00002576
Iteration 296/1000 | Loss: 0.00002518
Iteration 297/1000 | Loss: 0.00002488
Iteration 298/1000 | Loss: 0.00002470
Iteration 299/1000 | Loss: 0.00002459
Iteration 300/1000 | Loss: 0.00002454
Iteration 301/1000 | Loss: 0.00002451
Iteration 302/1000 | Loss: 0.00002451
Iteration 303/1000 | Loss: 0.00002450
Iteration 304/1000 | Loss: 0.00002450
Iteration 305/1000 | Loss: 0.00002449
Iteration 306/1000 | Loss: 0.00002449
Iteration 307/1000 | Loss: 0.00002448
Iteration 308/1000 | Loss: 0.00002448
Iteration 309/1000 | Loss: 0.00002447
Iteration 310/1000 | Loss: 0.00002447
Iteration 311/1000 | Loss: 0.00002447
Iteration 312/1000 | Loss: 0.00002447
Iteration 313/1000 | Loss: 0.00002446
Iteration 314/1000 | Loss: 0.00002446
Iteration 315/1000 | Loss: 0.00002446
Iteration 316/1000 | Loss: 0.00002446
Iteration 317/1000 | Loss: 0.00002445
Iteration 318/1000 | Loss: 0.00002445
Iteration 319/1000 | Loss: 0.00002445
Iteration 320/1000 | Loss: 0.00002445
Iteration 321/1000 | Loss: 0.00002445
Iteration 322/1000 | Loss: 0.00002445
Iteration 323/1000 | Loss: 0.00002445
Iteration 324/1000 | Loss: 0.00002445
Iteration 325/1000 | Loss: 0.00002445
Iteration 326/1000 | Loss: 0.00002445
Iteration 327/1000 | Loss: 0.00002445
Iteration 328/1000 | Loss: 0.00002445
Iteration 329/1000 | Loss: 0.00002444
Iteration 330/1000 | Loss: 0.00002444
Iteration 331/1000 | Loss: 0.00002444
Iteration 332/1000 | Loss: 0.00002443
Iteration 333/1000 | Loss: 0.00002443
Iteration 334/1000 | Loss: 0.00002443
Iteration 335/1000 | Loss: 0.00002443
Iteration 336/1000 | Loss: 0.00002442
Iteration 337/1000 | Loss: 0.00002442
Iteration 338/1000 | Loss: 0.00002442
Iteration 339/1000 | Loss: 0.00002442
Iteration 340/1000 | Loss: 0.00002442
Iteration 341/1000 | Loss: 0.00002441
Iteration 342/1000 | Loss: 0.00002441
Iteration 343/1000 | Loss: 0.00002441
Iteration 344/1000 | Loss: 0.00002441
Iteration 345/1000 | Loss: 0.00002441
Iteration 346/1000 | Loss: 0.00002441
Iteration 347/1000 | Loss: 0.00002441
Iteration 348/1000 | Loss: 0.00002441
Iteration 349/1000 | Loss: 0.00002441
Iteration 350/1000 | Loss: 0.00002441
Iteration 351/1000 | Loss: 0.00002441
Iteration 352/1000 | Loss: 0.00002441
Iteration 353/1000 | Loss: 0.00002440
Iteration 354/1000 | Loss: 0.00002440
Iteration 355/1000 | Loss: 0.00002440
Iteration 356/1000 | Loss: 0.00002440
Iteration 357/1000 | Loss: 0.00002440
Iteration 358/1000 | Loss: 0.00002440
Iteration 359/1000 | Loss: 0.00002440
Iteration 360/1000 | Loss: 0.00002440
Iteration 361/1000 | Loss: 0.00002440
Iteration 362/1000 | Loss: 0.00002440
Iteration 363/1000 | Loss: 0.00002440
Iteration 364/1000 | Loss: 0.00002439
Iteration 365/1000 | Loss: 0.00002439
Iteration 366/1000 | Loss: 0.00002439
Iteration 367/1000 | Loss: 0.00002439
Iteration 368/1000 | Loss: 0.00002439
Iteration 369/1000 | Loss: 0.00002439
Iteration 370/1000 | Loss: 0.00002439
Iteration 371/1000 | Loss: 0.00002439
Iteration 372/1000 | Loss: 0.00002439
Iteration 373/1000 | Loss: 0.00002439
Iteration 374/1000 | Loss: 0.00002439
Iteration 375/1000 | Loss: 0.00002439
Iteration 376/1000 | Loss: 0.00002439
Iteration 377/1000 | Loss: 0.00002439
Iteration 378/1000 | Loss: 0.00002439
Iteration 379/1000 | Loss: 0.00002439
Iteration 380/1000 | Loss: 0.00002439
Iteration 381/1000 | Loss: 0.00002439
Iteration 382/1000 | Loss: 0.00002439
Iteration 383/1000 | Loss: 0.00002439
Iteration 384/1000 | Loss: 0.00002439
Iteration 385/1000 | Loss: 0.00002439
Iteration 386/1000 | Loss: 0.00002439
Iteration 387/1000 | Loss: 0.00002439
Iteration 388/1000 | Loss: 0.00002439
Iteration 389/1000 | Loss: 0.00002439
Iteration 390/1000 | Loss: 0.00002439
Iteration 391/1000 | Loss: 0.00002439
Iteration 392/1000 | Loss: 0.00002439
Iteration 393/1000 | Loss: 0.00002439
Iteration 394/1000 | Loss: 0.00002439
Iteration 395/1000 | Loss: 0.00002439
Iteration 396/1000 | Loss: 0.00002439
Iteration 397/1000 | Loss: 0.00002439
Iteration 398/1000 | Loss: 0.00002439
Iteration 399/1000 | Loss: 0.00002439
Iteration 400/1000 | Loss: 0.00002439
Iteration 401/1000 | Loss: 0.00002439
Iteration 402/1000 | Loss: 0.00002439
Iteration 403/1000 | Loss: 0.00002439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 403. Stopping optimization.
Last 5 losses: [2.4394956199103035e-05, 2.4394956199103035e-05, 2.4394956199103035e-05, 2.4394956199103035e-05, 2.4394956199103035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4394956199103035e-05

Optimization complete. Final v2v error: 3.942641496658325 mm

Highest mean error: 8.607608795166016 mm for frame 26

Lowest mean error: 2.871659755706787 mm for frame 11

Saving results

Total time: 267.0114960670471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054084
Iteration 2/25 | Loss: 0.00311302
Iteration 3/25 | Loss: 0.00224039
Iteration 4/25 | Loss: 0.00191284
Iteration 5/25 | Loss: 0.00176251
Iteration 6/25 | Loss: 0.00159198
Iteration 7/25 | Loss: 0.00139384
Iteration 8/25 | Loss: 0.00131204
Iteration 9/25 | Loss: 0.00129141
Iteration 10/25 | Loss: 0.00129536
Iteration 11/25 | Loss: 0.00128078
Iteration 12/25 | Loss: 0.00127612
Iteration 13/25 | Loss: 0.00127373
Iteration 14/25 | Loss: 0.00127921
Iteration 15/25 | Loss: 0.00127592
Iteration 16/25 | Loss: 0.00126943
Iteration 17/25 | Loss: 0.00126718
Iteration 18/25 | Loss: 0.00126764
Iteration 19/25 | Loss: 0.00126574
Iteration 20/25 | Loss: 0.00126560
Iteration 21/25 | Loss: 0.00126552
Iteration 22/25 | Loss: 0.00126551
Iteration 23/25 | Loss: 0.00126551
Iteration 24/25 | Loss: 0.00126551
Iteration 25/25 | Loss: 0.00126550

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45000410
Iteration 2/25 | Loss: 0.00077695
Iteration 3/25 | Loss: 0.00075900
Iteration 4/25 | Loss: 0.00075900
Iteration 5/25 | Loss: 0.00075900
Iteration 6/25 | Loss: 0.00075900
Iteration 7/25 | Loss: 0.00075900
Iteration 8/25 | Loss: 0.00075900
Iteration 9/25 | Loss: 0.00075900
Iteration 10/25 | Loss: 0.00075900
Iteration 11/25 | Loss: 0.00075900
Iteration 12/25 | Loss: 0.00075900
Iteration 13/25 | Loss: 0.00075900
Iteration 14/25 | Loss: 0.00075900
Iteration 15/25 | Loss: 0.00075900
Iteration 16/25 | Loss: 0.00075900
Iteration 17/25 | Loss: 0.00075900
Iteration 18/25 | Loss: 0.00075900
Iteration 19/25 | Loss: 0.00075900
Iteration 20/25 | Loss: 0.00075900
Iteration 21/25 | Loss: 0.00075900
Iteration 22/25 | Loss: 0.00075900
Iteration 23/25 | Loss: 0.00075900
Iteration 24/25 | Loss: 0.00075900
Iteration 25/25 | Loss: 0.00075900

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075900
Iteration 2/1000 | Loss: 0.00025338
Iteration 3/1000 | Loss: 0.00008460
Iteration 4/1000 | Loss: 0.00016191
Iteration 5/1000 | Loss: 0.00007137
Iteration 6/1000 | Loss: 0.00005458
Iteration 7/1000 | Loss: 0.00004982
Iteration 8/1000 | Loss: 0.00004638
Iteration 9/1000 | Loss: 0.00004478
Iteration 10/1000 | Loss: 0.00004378
Iteration 11/1000 | Loss: 0.00004312
Iteration 12/1000 | Loss: 0.00005147
Iteration 13/1000 | Loss: 0.00004227
Iteration 14/1000 | Loss: 0.00004207
Iteration 15/1000 | Loss: 0.00004177
Iteration 16/1000 | Loss: 0.00005867
Iteration 17/1000 | Loss: 0.00004542
Iteration 18/1000 | Loss: 0.00004322
Iteration 19/1000 | Loss: 0.00004152
Iteration 20/1000 | Loss: 0.00004152
Iteration 21/1000 | Loss: 0.00004152
Iteration 22/1000 | Loss: 0.00004152
Iteration 23/1000 | Loss: 0.00004152
Iteration 24/1000 | Loss: 0.00004152
Iteration 25/1000 | Loss: 0.00004152
Iteration 26/1000 | Loss: 0.00004152
Iteration 27/1000 | Loss: 0.00004152
Iteration 28/1000 | Loss: 0.00004152
Iteration 29/1000 | Loss: 0.00004151
Iteration 30/1000 | Loss: 0.00004148
Iteration 31/1000 | Loss: 0.00004145
Iteration 32/1000 | Loss: 0.00004145
Iteration 33/1000 | Loss: 0.00004145
Iteration 34/1000 | Loss: 0.00004145
Iteration 35/1000 | Loss: 0.00004145
Iteration 36/1000 | Loss: 0.00004145
Iteration 37/1000 | Loss: 0.00004145
Iteration 38/1000 | Loss: 0.00004144
Iteration 39/1000 | Loss: 0.00004143
Iteration 40/1000 | Loss: 0.00004142
Iteration 41/1000 | Loss: 0.00004141
Iteration 42/1000 | Loss: 0.00004141
Iteration 43/1000 | Loss: 0.00004141
Iteration 44/1000 | Loss: 0.00004140
Iteration 45/1000 | Loss: 0.00004140
Iteration 46/1000 | Loss: 0.00004140
Iteration 47/1000 | Loss: 0.00004140
Iteration 48/1000 | Loss: 0.00004140
Iteration 49/1000 | Loss: 0.00004140
Iteration 50/1000 | Loss: 0.00004140
Iteration 51/1000 | Loss: 0.00004140
Iteration 52/1000 | Loss: 0.00004139
Iteration 53/1000 | Loss: 0.00004138
Iteration 54/1000 | Loss: 0.00004137
Iteration 55/1000 | Loss: 0.00004137
Iteration 56/1000 | Loss: 0.00004137
Iteration 57/1000 | Loss: 0.00004137
Iteration 58/1000 | Loss: 0.00004137
Iteration 59/1000 | Loss: 0.00004137
Iteration 60/1000 | Loss: 0.00004137
Iteration 61/1000 | Loss: 0.00004137
Iteration 62/1000 | Loss: 0.00004137
Iteration 63/1000 | Loss: 0.00004137
Iteration 64/1000 | Loss: 0.00004137
Iteration 65/1000 | Loss: 0.00004137
Iteration 66/1000 | Loss: 0.00004137
Iteration 67/1000 | Loss: 0.00004137
Iteration 68/1000 | Loss: 0.00004137
Iteration 69/1000 | Loss: 0.00004137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [4.136754068895243e-05, 4.136754068895243e-05, 4.136754068895243e-05, 4.136754068895243e-05, 4.136754068895243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.136754068895243e-05

Optimization complete. Final v2v error: 5.450006008148193 mm

Highest mean error: 10.792781829833984 mm for frame 112

Lowest mean error: 4.6231818199157715 mm for frame 156

Saving results

Total time: 75.26456093788147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01177967
Iteration 2/25 | Loss: 0.00420066
Iteration 3/25 | Loss: 0.00317110
Iteration 4/25 | Loss: 0.00232843
Iteration 5/25 | Loss: 0.00176498
Iteration 6/25 | Loss: 0.00186215
Iteration 7/25 | Loss: 0.00165643
Iteration 8/25 | Loss: 0.00155780
Iteration 9/25 | Loss: 0.00142771
Iteration 10/25 | Loss: 0.00136912
Iteration 11/25 | Loss: 0.00130993
Iteration 12/25 | Loss: 0.00127093
Iteration 13/25 | Loss: 0.00124088
Iteration 14/25 | Loss: 0.00122217
Iteration 15/25 | Loss: 0.00121947
Iteration 16/25 | Loss: 0.00122305
Iteration 17/25 | Loss: 0.00120514
Iteration 18/25 | Loss: 0.00119820
Iteration 19/25 | Loss: 0.00120111
Iteration 20/25 | Loss: 0.00119199
Iteration 21/25 | Loss: 0.00119160
Iteration 22/25 | Loss: 0.00120302
Iteration 23/25 | Loss: 0.00119234
Iteration 24/25 | Loss: 0.00119196
Iteration 25/25 | Loss: 0.00118565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81335682
Iteration 2/25 | Loss: 0.00276152
Iteration 3/25 | Loss: 0.00103384
Iteration 4/25 | Loss: 0.00103384
Iteration 5/25 | Loss: 0.00103383
Iteration 6/25 | Loss: 0.00103383
Iteration 7/25 | Loss: 0.00103383
Iteration 8/25 | Loss: 0.00103383
Iteration 9/25 | Loss: 0.00103383
Iteration 10/25 | Loss: 0.00103383
Iteration 11/25 | Loss: 0.00103383
Iteration 12/25 | Loss: 0.00103383
Iteration 13/25 | Loss: 0.00103383
Iteration 14/25 | Loss: 0.00103383
Iteration 15/25 | Loss: 0.00103383
Iteration 16/25 | Loss: 0.00103383
Iteration 17/25 | Loss: 0.00103383
Iteration 18/25 | Loss: 0.00103383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010338322026655078, 0.0010338322026655078, 0.0010338322026655078, 0.0010338322026655078, 0.0010338322026655078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010338322026655078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103383
Iteration 2/1000 | Loss: 0.00059786
Iteration 3/1000 | Loss: 0.00041923
Iteration 4/1000 | Loss: 0.00059866
Iteration 5/1000 | Loss: 0.00060470
Iteration 6/1000 | Loss: 0.00061149
Iteration 7/1000 | Loss: 0.00155422
Iteration 8/1000 | Loss: 0.00092038
Iteration 9/1000 | Loss: 0.00115314
Iteration 10/1000 | Loss: 0.00088630
Iteration 11/1000 | Loss: 0.00049597
Iteration 12/1000 | Loss: 0.00158823
Iteration 13/1000 | Loss: 0.00087930
Iteration 14/1000 | Loss: 0.00091387
Iteration 15/1000 | Loss: 0.00092799
Iteration 16/1000 | Loss: 0.00077468
Iteration 17/1000 | Loss: 0.00069495
Iteration 18/1000 | Loss: 0.00101861
Iteration 19/1000 | Loss: 0.00072334
Iteration 20/1000 | Loss: 0.00089961
Iteration 21/1000 | Loss: 0.00118681
Iteration 22/1000 | Loss: 0.00073508
Iteration 23/1000 | Loss: 0.00065718
Iteration 24/1000 | Loss: 0.00065610
Iteration 25/1000 | Loss: 0.00026269
Iteration 26/1000 | Loss: 0.00032895
Iteration 27/1000 | Loss: 0.00049964
Iteration 28/1000 | Loss: 0.00035836
Iteration 29/1000 | Loss: 0.00070995
Iteration 30/1000 | Loss: 0.00047026
Iteration 31/1000 | Loss: 0.00057523
Iteration 32/1000 | Loss: 0.00046245
Iteration 33/1000 | Loss: 0.00063957
Iteration 34/1000 | Loss: 0.00047343
Iteration 35/1000 | Loss: 0.00054042
Iteration 36/1000 | Loss: 0.00044857
Iteration 37/1000 | Loss: 0.00028300
Iteration 38/1000 | Loss: 0.00016610
Iteration 39/1000 | Loss: 0.00033287
Iteration 40/1000 | Loss: 0.00060129
Iteration 41/1000 | Loss: 0.00070215
Iteration 42/1000 | Loss: 0.00038338
Iteration 43/1000 | Loss: 0.00079831
Iteration 44/1000 | Loss: 0.00071939
Iteration 45/1000 | Loss: 0.00038210
Iteration 46/1000 | Loss: 0.00021626
Iteration 47/1000 | Loss: 0.00016167
Iteration 48/1000 | Loss: 0.00030541
Iteration 49/1000 | Loss: 0.00029885
Iteration 50/1000 | Loss: 0.00031389
Iteration 51/1000 | Loss: 0.00063186
Iteration 52/1000 | Loss: 0.00024338
Iteration 53/1000 | Loss: 0.00040687
Iteration 54/1000 | Loss: 0.00046650
Iteration 55/1000 | Loss: 0.00022189
Iteration 56/1000 | Loss: 0.00019229
Iteration 57/1000 | Loss: 0.00015356
Iteration 58/1000 | Loss: 0.00019780
Iteration 59/1000 | Loss: 0.00008062
Iteration 60/1000 | Loss: 0.00055683
Iteration 61/1000 | Loss: 0.00130599
Iteration 62/1000 | Loss: 0.00088384
Iteration 63/1000 | Loss: 0.00232022
Iteration 64/1000 | Loss: 0.00052093
Iteration 65/1000 | Loss: 0.00018015
Iteration 66/1000 | Loss: 0.00045293
Iteration 67/1000 | Loss: 0.00028574
Iteration 68/1000 | Loss: 0.00023155
Iteration 69/1000 | Loss: 0.00063730
Iteration 70/1000 | Loss: 0.00046239
Iteration 71/1000 | Loss: 0.00071718
Iteration 72/1000 | Loss: 0.00078449
Iteration 73/1000 | Loss: 0.00033298
Iteration 74/1000 | Loss: 0.00029669
Iteration 75/1000 | Loss: 0.00018701
Iteration 76/1000 | Loss: 0.00015805
Iteration 77/1000 | Loss: 0.00017665
Iteration 78/1000 | Loss: 0.00030710
Iteration 79/1000 | Loss: 0.00057055
Iteration 80/1000 | Loss: 0.00136846
Iteration 81/1000 | Loss: 0.00044767
Iteration 82/1000 | Loss: 0.00017578
Iteration 83/1000 | Loss: 0.00007054
Iteration 84/1000 | Loss: 0.00028972
Iteration 85/1000 | Loss: 0.00022891
Iteration 86/1000 | Loss: 0.00029463
Iteration 87/1000 | Loss: 0.00033113
Iteration 88/1000 | Loss: 0.00019894
Iteration 89/1000 | Loss: 0.00089815
Iteration 90/1000 | Loss: 0.00031758
Iteration 91/1000 | Loss: 0.00018972
Iteration 92/1000 | Loss: 0.00020780
Iteration 93/1000 | Loss: 0.00025300
Iteration 94/1000 | Loss: 0.00083967
Iteration 95/1000 | Loss: 0.00026300
Iteration 96/1000 | Loss: 0.00018543
Iteration 97/1000 | Loss: 0.00006258
Iteration 98/1000 | Loss: 0.00085801
Iteration 99/1000 | Loss: 0.00021219
Iteration 100/1000 | Loss: 0.00006299
Iteration 101/1000 | Loss: 0.00013846
Iteration 102/1000 | Loss: 0.00031217
Iteration 103/1000 | Loss: 0.00006265
Iteration 104/1000 | Loss: 0.00005372
Iteration 105/1000 | Loss: 0.00011311
Iteration 106/1000 | Loss: 0.00005180
Iteration 107/1000 | Loss: 0.00015195
Iteration 108/1000 | Loss: 0.00013416
Iteration 109/1000 | Loss: 0.00016093
Iteration 110/1000 | Loss: 0.00039620
Iteration 111/1000 | Loss: 0.00023052
Iteration 112/1000 | Loss: 0.00008271
Iteration 113/1000 | Loss: 0.00047756
Iteration 114/1000 | Loss: 0.00041593
Iteration 115/1000 | Loss: 0.00018590
Iteration 116/1000 | Loss: 0.00022916
Iteration 117/1000 | Loss: 0.00019826
Iteration 118/1000 | Loss: 0.00014522
Iteration 119/1000 | Loss: 0.00015034
Iteration 120/1000 | Loss: 0.00012484
Iteration 121/1000 | Loss: 0.00021811
Iteration 122/1000 | Loss: 0.00017482
Iteration 123/1000 | Loss: 0.00035260
Iteration 124/1000 | Loss: 0.00052979
Iteration 125/1000 | Loss: 0.00005625
Iteration 126/1000 | Loss: 0.00005172
Iteration 127/1000 | Loss: 0.00035551
Iteration 128/1000 | Loss: 0.00005061
Iteration 129/1000 | Loss: 0.00004424
Iteration 130/1000 | Loss: 0.00003952
Iteration 131/1000 | Loss: 0.00048286
Iteration 132/1000 | Loss: 0.00004914
Iteration 133/1000 | Loss: 0.00004195
Iteration 134/1000 | Loss: 0.00003588
Iteration 135/1000 | Loss: 0.00003513
Iteration 136/1000 | Loss: 0.00038627
Iteration 137/1000 | Loss: 0.00005558
Iteration 138/1000 | Loss: 0.00005124
Iteration 139/1000 | Loss: 0.00039607
Iteration 140/1000 | Loss: 0.00004996
Iteration 141/1000 | Loss: 0.00014009
Iteration 142/1000 | Loss: 0.00004283
Iteration 143/1000 | Loss: 0.00004158
Iteration 144/1000 | Loss: 0.00003554
Iteration 145/1000 | Loss: 0.00003481
Iteration 146/1000 | Loss: 0.00003440
Iteration 147/1000 | Loss: 0.00003412
Iteration 148/1000 | Loss: 0.00003411
Iteration 149/1000 | Loss: 0.00003394
Iteration 150/1000 | Loss: 0.00003385
Iteration 151/1000 | Loss: 0.00003384
Iteration 152/1000 | Loss: 0.00003384
Iteration 153/1000 | Loss: 0.00003382
Iteration 154/1000 | Loss: 0.00003375
Iteration 155/1000 | Loss: 0.00003373
Iteration 156/1000 | Loss: 0.00003373
Iteration 157/1000 | Loss: 0.00003372
Iteration 158/1000 | Loss: 0.00003372
Iteration 159/1000 | Loss: 0.00003372
Iteration 160/1000 | Loss: 0.00003372
Iteration 161/1000 | Loss: 0.00003372
Iteration 162/1000 | Loss: 0.00003372
Iteration 163/1000 | Loss: 0.00003372
Iteration 164/1000 | Loss: 0.00003372
Iteration 165/1000 | Loss: 0.00003372
Iteration 166/1000 | Loss: 0.00003371
Iteration 167/1000 | Loss: 0.00003371
Iteration 168/1000 | Loss: 0.00003371
Iteration 169/1000 | Loss: 0.00003371
Iteration 170/1000 | Loss: 0.00003371
Iteration 171/1000 | Loss: 0.00003371
Iteration 172/1000 | Loss: 0.00003369
Iteration 173/1000 | Loss: 0.00003369
Iteration 174/1000 | Loss: 0.00003369
Iteration 175/1000 | Loss: 0.00003368
Iteration 176/1000 | Loss: 0.00003368
Iteration 177/1000 | Loss: 0.00003368
Iteration 178/1000 | Loss: 0.00003368
Iteration 179/1000 | Loss: 0.00003367
Iteration 180/1000 | Loss: 0.00003367
Iteration 181/1000 | Loss: 0.00003367
Iteration 182/1000 | Loss: 0.00003367
Iteration 183/1000 | Loss: 0.00003366
Iteration 184/1000 | Loss: 0.00003366
Iteration 185/1000 | Loss: 0.00003366
Iteration 186/1000 | Loss: 0.00003366
Iteration 187/1000 | Loss: 0.00003366
Iteration 188/1000 | Loss: 0.00003366
Iteration 189/1000 | Loss: 0.00003366
Iteration 190/1000 | Loss: 0.00003366
Iteration 191/1000 | Loss: 0.00003366
Iteration 192/1000 | Loss: 0.00003366
Iteration 193/1000 | Loss: 0.00003366
Iteration 194/1000 | Loss: 0.00003366
Iteration 195/1000 | Loss: 0.00003366
Iteration 196/1000 | Loss: 0.00003366
Iteration 197/1000 | Loss: 0.00003366
Iteration 198/1000 | Loss: 0.00003365
Iteration 199/1000 | Loss: 0.00003365
Iteration 200/1000 | Loss: 0.00003365
Iteration 201/1000 | Loss: 0.00003365
Iteration 202/1000 | Loss: 0.00003365
Iteration 203/1000 | Loss: 0.00003365
Iteration 204/1000 | Loss: 0.00003365
Iteration 205/1000 | Loss: 0.00003365
Iteration 206/1000 | Loss: 0.00003365
Iteration 207/1000 | Loss: 0.00003365
Iteration 208/1000 | Loss: 0.00003365
Iteration 209/1000 | Loss: 0.00003365
Iteration 210/1000 | Loss: 0.00003365
Iteration 211/1000 | Loss: 0.00003365
Iteration 212/1000 | Loss: 0.00003365
Iteration 213/1000 | Loss: 0.00003365
Iteration 214/1000 | Loss: 0.00003365
Iteration 215/1000 | Loss: 0.00003365
Iteration 216/1000 | Loss: 0.00003365
Iteration 217/1000 | Loss: 0.00003365
Iteration 218/1000 | Loss: 0.00003365
Iteration 219/1000 | Loss: 0.00003365
Iteration 220/1000 | Loss: 0.00003365
Iteration 221/1000 | Loss: 0.00003365
Iteration 222/1000 | Loss: 0.00003365
Iteration 223/1000 | Loss: 0.00003365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [3.364591611898504e-05, 3.364591611898504e-05, 3.364591611898504e-05, 3.364591611898504e-05, 3.364591611898504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.364591611898504e-05

Optimization complete. Final v2v error: 4.709286212921143 mm

Highest mean error: 11.042574882507324 mm for frame 14

Lowest mean error: 4.110378265380859 mm for frame 97

Saving results

Total time: 295.0555987358093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547064
Iteration 2/25 | Loss: 0.00145757
Iteration 3/25 | Loss: 0.00113044
Iteration 4/25 | Loss: 0.00108907
Iteration 5/25 | Loss: 0.00107979
Iteration 6/25 | Loss: 0.00107481
Iteration 7/25 | Loss: 0.00107363
Iteration 8/25 | Loss: 0.00107355
Iteration 9/25 | Loss: 0.00107355
Iteration 10/25 | Loss: 0.00107355
Iteration 11/25 | Loss: 0.00107355
Iteration 12/25 | Loss: 0.00107355
Iteration 13/25 | Loss: 0.00107355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010735464747995138, 0.0010735464747995138, 0.0010735464747995138, 0.0010735464747995138, 0.0010735464747995138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010735464747995138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21007764
Iteration 2/25 | Loss: 0.00043944
Iteration 3/25 | Loss: 0.00043942
Iteration 4/25 | Loss: 0.00043942
Iteration 5/25 | Loss: 0.00043942
Iteration 6/25 | Loss: 0.00043942
Iteration 7/25 | Loss: 0.00043942
Iteration 8/25 | Loss: 0.00043942
Iteration 9/25 | Loss: 0.00043942
Iteration 10/25 | Loss: 0.00043941
Iteration 11/25 | Loss: 0.00043941
Iteration 12/25 | Loss: 0.00043941
Iteration 13/25 | Loss: 0.00043941
Iteration 14/25 | Loss: 0.00043941
Iteration 15/25 | Loss: 0.00043941
Iteration 16/25 | Loss: 0.00043941
Iteration 17/25 | Loss: 0.00043941
Iteration 18/25 | Loss: 0.00043941
Iteration 19/25 | Loss: 0.00043941
Iteration 20/25 | Loss: 0.00043941
Iteration 21/25 | Loss: 0.00043941
Iteration 22/25 | Loss: 0.00043941
Iteration 23/25 | Loss: 0.00043941
Iteration 24/25 | Loss: 0.00043941
Iteration 25/25 | Loss: 0.00043941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043941
Iteration 2/1000 | Loss: 0.00006762
Iteration 3/1000 | Loss: 0.00004675
Iteration 4/1000 | Loss: 0.00004060
Iteration 5/1000 | Loss: 0.00003723
Iteration 6/1000 | Loss: 0.00003501
Iteration 7/1000 | Loss: 0.00003368
Iteration 8/1000 | Loss: 0.00003300
Iteration 9/1000 | Loss: 0.00003264
Iteration 10/1000 | Loss: 0.00003238
Iteration 11/1000 | Loss: 0.00003208
Iteration 12/1000 | Loss: 0.00003182
Iteration 13/1000 | Loss: 0.00003166
Iteration 14/1000 | Loss: 0.00003165
Iteration 15/1000 | Loss: 0.00003161
Iteration 16/1000 | Loss: 0.00003156
Iteration 17/1000 | Loss: 0.00003144
Iteration 18/1000 | Loss: 0.00003143
Iteration 19/1000 | Loss: 0.00003143
Iteration 20/1000 | Loss: 0.00003142
Iteration 21/1000 | Loss: 0.00003139
Iteration 22/1000 | Loss: 0.00003139
Iteration 23/1000 | Loss: 0.00003139
Iteration 24/1000 | Loss: 0.00003139
Iteration 25/1000 | Loss: 0.00003139
Iteration 26/1000 | Loss: 0.00003139
Iteration 27/1000 | Loss: 0.00003138
Iteration 28/1000 | Loss: 0.00003137
Iteration 29/1000 | Loss: 0.00003136
Iteration 30/1000 | Loss: 0.00003136
Iteration 31/1000 | Loss: 0.00003135
Iteration 32/1000 | Loss: 0.00003133
Iteration 33/1000 | Loss: 0.00003132
Iteration 34/1000 | Loss: 0.00003129
Iteration 35/1000 | Loss: 0.00003129
Iteration 36/1000 | Loss: 0.00003126
Iteration 37/1000 | Loss: 0.00003124
Iteration 38/1000 | Loss: 0.00003123
Iteration 39/1000 | Loss: 0.00003122
Iteration 40/1000 | Loss: 0.00003122
Iteration 41/1000 | Loss: 0.00003120
Iteration 42/1000 | Loss: 0.00003119
Iteration 43/1000 | Loss: 0.00003118
Iteration 44/1000 | Loss: 0.00003118
Iteration 45/1000 | Loss: 0.00003117
Iteration 46/1000 | Loss: 0.00003116
Iteration 47/1000 | Loss: 0.00003116
Iteration 48/1000 | Loss: 0.00003116
Iteration 49/1000 | Loss: 0.00003116
Iteration 50/1000 | Loss: 0.00003115
Iteration 51/1000 | Loss: 0.00003110
Iteration 52/1000 | Loss: 0.00003110
Iteration 53/1000 | Loss: 0.00003109
Iteration 54/1000 | Loss: 0.00003109
Iteration 55/1000 | Loss: 0.00003109
Iteration 56/1000 | Loss: 0.00003109
Iteration 57/1000 | Loss: 0.00003109
Iteration 58/1000 | Loss: 0.00003109
Iteration 59/1000 | Loss: 0.00003109
Iteration 60/1000 | Loss: 0.00003109
Iteration 61/1000 | Loss: 0.00003109
Iteration 62/1000 | Loss: 0.00003109
Iteration 63/1000 | Loss: 0.00003108
Iteration 64/1000 | Loss: 0.00003108
Iteration 65/1000 | Loss: 0.00003108
Iteration 66/1000 | Loss: 0.00003107
Iteration 67/1000 | Loss: 0.00003107
Iteration 68/1000 | Loss: 0.00003107
Iteration 69/1000 | Loss: 0.00003107
Iteration 70/1000 | Loss: 0.00003107
Iteration 71/1000 | Loss: 0.00003107
Iteration 72/1000 | Loss: 0.00003106
Iteration 73/1000 | Loss: 0.00003106
Iteration 74/1000 | Loss: 0.00003106
Iteration 75/1000 | Loss: 0.00003105
Iteration 76/1000 | Loss: 0.00003105
Iteration 77/1000 | Loss: 0.00003105
Iteration 78/1000 | Loss: 0.00003105
Iteration 79/1000 | Loss: 0.00003104
Iteration 80/1000 | Loss: 0.00003104
Iteration 81/1000 | Loss: 0.00003104
Iteration 82/1000 | Loss: 0.00003104
Iteration 83/1000 | Loss: 0.00003104
Iteration 84/1000 | Loss: 0.00003104
Iteration 85/1000 | Loss: 0.00003103
Iteration 86/1000 | Loss: 0.00003103
Iteration 87/1000 | Loss: 0.00003102
Iteration 88/1000 | Loss: 0.00003102
Iteration 89/1000 | Loss: 0.00003102
Iteration 90/1000 | Loss: 0.00003101
Iteration 91/1000 | Loss: 0.00003101
Iteration 92/1000 | Loss: 0.00003101
Iteration 93/1000 | Loss: 0.00003101
Iteration 94/1000 | Loss: 0.00003100
Iteration 95/1000 | Loss: 0.00003100
Iteration 96/1000 | Loss: 0.00003099
Iteration 97/1000 | Loss: 0.00003099
Iteration 98/1000 | Loss: 0.00003099
Iteration 99/1000 | Loss: 0.00003099
Iteration 100/1000 | Loss: 0.00003099
Iteration 101/1000 | Loss: 0.00003099
Iteration 102/1000 | Loss: 0.00003099
Iteration 103/1000 | Loss: 0.00003098
Iteration 104/1000 | Loss: 0.00003098
Iteration 105/1000 | Loss: 0.00003098
Iteration 106/1000 | Loss: 0.00003098
Iteration 107/1000 | Loss: 0.00003098
Iteration 108/1000 | Loss: 0.00003097
Iteration 109/1000 | Loss: 0.00003097
Iteration 110/1000 | Loss: 0.00003097
Iteration 111/1000 | Loss: 0.00003097
Iteration 112/1000 | Loss: 0.00003097
Iteration 113/1000 | Loss: 0.00003097
Iteration 114/1000 | Loss: 0.00003097
Iteration 115/1000 | Loss: 0.00003097
Iteration 116/1000 | Loss: 0.00003097
Iteration 117/1000 | Loss: 0.00003096
Iteration 118/1000 | Loss: 0.00003096
Iteration 119/1000 | Loss: 0.00003096
Iteration 120/1000 | Loss: 0.00003096
Iteration 121/1000 | Loss: 0.00003096
Iteration 122/1000 | Loss: 0.00003096
Iteration 123/1000 | Loss: 0.00003096
Iteration 124/1000 | Loss: 0.00003096
Iteration 125/1000 | Loss: 0.00003096
Iteration 126/1000 | Loss: 0.00003095
Iteration 127/1000 | Loss: 0.00003095
Iteration 128/1000 | Loss: 0.00003095
Iteration 129/1000 | Loss: 0.00003095
Iteration 130/1000 | Loss: 0.00003095
Iteration 131/1000 | Loss: 0.00003095
Iteration 132/1000 | Loss: 0.00003095
Iteration 133/1000 | Loss: 0.00003095
Iteration 134/1000 | Loss: 0.00003094
Iteration 135/1000 | Loss: 0.00003094
Iteration 136/1000 | Loss: 0.00003094
Iteration 137/1000 | Loss: 0.00003094
Iteration 138/1000 | Loss: 0.00003094
Iteration 139/1000 | Loss: 0.00003094
Iteration 140/1000 | Loss: 0.00003094
Iteration 141/1000 | Loss: 0.00003093
Iteration 142/1000 | Loss: 0.00003093
Iteration 143/1000 | Loss: 0.00003093
Iteration 144/1000 | Loss: 0.00003093
Iteration 145/1000 | Loss: 0.00003093
Iteration 146/1000 | Loss: 0.00003093
Iteration 147/1000 | Loss: 0.00003093
Iteration 148/1000 | Loss: 0.00003093
Iteration 149/1000 | Loss: 0.00003092
Iteration 150/1000 | Loss: 0.00003092
Iteration 151/1000 | Loss: 0.00003092
Iteration 152/1000 | Loss: 0.00003092
Iteration 153/1000 | Loss: 0.00003092
Iteration 154/1000 | Loss: 0.00003091
Iteration 155/1000 | Loss: 0.00003091
Iteration 156/1000 | Loss: 0.00003091
Iteration 157/1000 | Loss: 0.00003091
Iteration 158/1000 | Loss: 0.00003090
Iteration 159/1000 | Loss: 0.00003090
Iteration 160/1000 | Loss: 0.00003090
Iteration 161/1000 | Loss: 0.00003090
Iteration 162/1000 | Loss: 0.00003090
Iteration 163/1000 | Loss: 0.00003090
Iteration 164/1000 | Loss: 0.00003090
Iteration 165/1000 | Loss: 0.00003089
Iteration 166/1000 | Loss: 0.00003089
Iteration 167/1000 | Loss: 0.00003089
Iteration 168/1000 | Loss: 0.00003089
Iteration 169/1000 | Loss: 0.00003089
Iteration 170/1000 | Loss: 0.00003089
Iteration 171/1000 | Loss: 0.00003089
Iteration 172/1000 | Loss: 0.00003089
Iteration 173/1000 | Loss: 0.00003089
Iteration 174/1000 | Loss: 0.00003089
Iteration 175/1000 | Loss: 0.00003089
Iteration 176/1000 | Loss: 0.00003089
Iteration 177/1000 | Loss: 0.00003089
Iteration 178/1000 | Loss: 0.00003089
Iteration 179/1000 | Loss: 0.00003089
Iteration 180/1000 | Loss: 0.00003089
Iteration 181/1000 | Loss: 0.00003089
Iteration 182/1000 | Loss: 0.00003088
Iteration 183/1000 | Loss: 0.00003088
Iteration 184/1000 | Loss: 0.00003088
Iteration 185/1000 | Loss: 0.00003088
Iteration 186/1000 | Loss: 0.00003088
Iteration 187/1000 | Loss: 0.00003088
Iteration 188/1000 | Loss: 0.00003088
Iteration 189/1000 | Loss: 0.00003088
Iteration 190/1000 | Loss: 0.00003088
Iteration 191/1000 | Loss: 0.00003088
Iteration 192/1000 | Loss: 0.00003088
Iteration 193/1000 | Loss: 0.00003088
Iteration 194/1000 | Loss: 0.00003088
Iteration 195/1000 | Loss: 0.00003088
Iteration 196/1000 | Loss: 0.00003088
Iteration 197/1000 | Loss: 0.00003088
Iteration 198/1000 | Loss: 0.00003088
Iteration 199/1000 | Loss: 0.00003087
Iteration 200/1000 | Loss: 0.00003087
Iteration 201/1000 | Loss: 0.00003087
Iteration 202/1000 | Loss: 0.00003087
Iteration 203/1000 | Loss: 0.00003087
Iteration 204/1000 | Loss: 0.00003087
Iteration 205/1000 | Loss: 0.00003087
Iteration 206/1000 | Loss: 0.00003087
Iteration 207/1000 | Loss: 0.00003087
Iteration 208/1000 | Loss: 0.00003087
Iteration 209/1000 | Loss: 0.00003087
Iteration 210/1000 | Loss: 0.00003087
Iteration 211/1000 | Loss: 0.00003087
Iteration 212/1000 | Loss: 0.00003087
Iteration 213/1000 | Loss: 0.00003086
Iteration 214/1000 | Loss: 0.00003086
Iteration 215/1000 | Loss: 0.00003086
Iteration 216/1000 | Loss: 0.00003086
Iteration 217/1000 | Loss: 0.00003086
Iteration 218/1000 | Loss: 0.00003086
Iteration 219/1000 | Loss: 0.00003086
Iteration 220/1000 | Loss: 0.00003086
Iteration 221/1000 | Loss: 0.00003086
Iteration 222/1000 | Loss: 0.00003086
Iteration 223/1000 | Loss: 0.00003086
Iteration 224/1000 | Loss: 0.00003086
Iteration 225/1000 | Loss: 0.00003086
Iteration 226/1000 | Loss: 0.00003086
Iteration 227/1000 | Loss: 0.00003086
Iteration 228/1000 | Loss: 0.00003086
Iteration 229/1000 | Loss: 0.00003086
Iteration 230/1000 | Loss: 0.00003085
Iteration 231/1000 | Loss: 0.00003085
Iteration 232/1000 | Loss: 0.00003085
Iteration 233/1000 | Loss: 0.00003085
Iteration 234/1000 | Loss: 0.00003085
Iteration 235/1000 | Loss: 0.00003085
Iteration 236/1000 | Loss: 0.00003085
Iteration 237/1000 | Loss: 0.00003085
Iteration 238/1000 | Loss: 0.00003085
Iteration 239/1000 | Loss: 0.00003085
Iteration 240/1000 | Loss: 0.00003085
Iteration 241/1000 | Loss: 0.00003085
Iteration 242/1000 | Loss: 0.00003085
Iteration 243/1000 | Loss: 0.00003085
Iteration 244/1000 | Loss: 0.00003085
Iteration 245/1000 | Loss: 0.00003085
Iteration 246/1000 | Loss: 0.00003085
Iteration 247/1000 | Loss: 0.00003085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [3.084917989326641e-05, 3.084917989326641e-05, 3.084917989326641e-05, 3.084917989326641e-05, 3.084917989326641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.084917989326641e-05

Optimization complete. Final v2v error: 4.506255149841309 mm

Highest mean error: 7.084427356719971 mm for frame 82

Lowest mean error: 3.6209161281585693 mm for frame 119

Saving results

Total time: 51.437668800354004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926675
Iteration 2/25 | Loss: 0.00144647
Iteration 3/25 | Loss: 0.00117457
Iteration 4/25 | Loss: 0.00114314
Iteration 5/25 | Loss: 0.00113293
Iteration 6/25 | Loss: 0.00113064
Iteration 7/25 | Loss: 0.00113041
Iteration 8/25 | Loss: 0.00113041
Iteration 9/25 | Loss: 0.00113041
Iteration 10/25 | Loss: 0.00113041
Iteration 11/25 | Loss: 0.00113041
Iteration 12/25 | Loss: 0.00113041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011304108193144202, 0.0011304108193144202, 0.0011304108193144202, 0.0011304108193144202, 0.0011304108193144202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011304108193144202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98052835
Iteration 2/25 | Loss: 0.00064443
Iteration 3/25 | Loss: 0.00064443
Iteration 4/25 | Loss: 0.00064443
Iteration 5/25 | Loss: 0.00064443
Iteration 6/25 | Loss: 0.00064443
Iteration 7/25 | Loss: 0.00064443
Iteration 8/25 | Loss: 0.00064443
Iteration 9/25 | Loss: 0.00064443
Iteration 10/25 | Loss: 0.00064443
Iteration 11/25 | Loss: 0.00064443
Iteration 12/25 | Loss: 0.00064443
Iteration 13/25 | Loss: 0.00064443
Iteration 14/25 | Loss: 0.00064443
Iteration 15/25 | Loss: 0.00064443
Iteration 16/25 | Loss: 0.00064443
Iteration 17/25 | Loss: 0.00064443
Iteration 18/25 | Loss: 0.00064443
Iteration 19/25 | Loss: 0.00064443
Iteration 20/25 | Loss: 0.00064443
Iteration 21/25 | Loss: 0.00064443
Iteration 22/25 | Loss: 0.00064443
Iteration 23/25 | Loss: 0.00064443
Iteration 24/25 | Loss: 0.00064443
Iteration 25/25 | Loss: 0.00064443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064443
Iteration 2/1000 | Loss: 0.00007142
Iteration 3/1000 | Loss: 0.00004969
Iteration 4/1000 | Loss: 0.00004220
Iteration 5/1000 | Loss: 0.00003863
Iteration 6/1000 | Loss: 0.00003651
Iteration 7/1000 | Loss: 0.00003567
Iteration 8/1000 | Loss: 0.00003531
Iteration 9/1000 | Loss: 0.00003502
Iteration 10/1000 | Loss: 0.00003471
Iteration 11/1000 | Loss: 0.00003467
Iteration 12/1000 | Loss: 0.00003463
Iteration 13/1000 | Loss: 0.00003452
Iteration 14/1000 | Loss: 0.00003444
Iteration 15/1000 | Loss: 0.00003444
Iteration 16/1000 | Loss: 0.00003444
Iteration 17/1000 | Loss: 0.00003444
Iteration 18/1000 | Loss: 0.00003443
Iteration 19/1000 | Loss: 0.00003443
Iteration 20/1000 | Loss: 0.00003443
Iteration 21/1000 | Loss: 0.00003443
Iteration 22/1000 | Loss: 0.00003443
Iteration 23/1000 | Loss: 0.00003443
Iteration 24/1000 | Loss: 0.00003443
Iteration 25/1000 | Loss: 0.00003443
Iteration 26/1000 | Loss: 0.00003443
Iteration 27/1000 | Loss: 0.00003443
Iteration 28/1000 | Loss: 0.00003443
Iteration 29/1000 | Loss: 0.00003443
Iteration 30/1000 | Loss: 0.00003443
Iteration 31/1000 | Loss: 0.00003443
Iteration 32/1000 | Loss: 0.00003443
Iteration 33/1000 | Loss: 0.00003443
Iteration 34/1000 | Loss: 0.00003443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 34. Stopping optimization.
Last 5 losses: [3.4430526284268126e-05, 3.4430526284268126e-05, 3.4430526284268126e-05, 3.4430526284268126e-05, 3.4430526284268126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4430526284268126e-05

Optimization complete. Final v2v error: 5.012590408325195 mm

Highest mean error: 5.187191009521484 mm for frame 121

Lowest mean error: 4.8634562492370605 mm for frame 43

Saving results

Total time: 26.06758713722229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591067
Iteration 2/25 | Loss: 0.00126683
Iteration 3/25 | Loss: 0.00113412
Iteration 4/25 | Loss: 0.00111956
Iteration 5/25 | Loss: 0.00111353
Iteration 6/25 | Loss: 0.00111105
Iteration 7/25 | Loss: 0.00111072
Iteration 8/25 | Loss: 0.00111072
Iteration 9/25 | Loss: 0.00111072
Iteration 10/25 | Loss: 0.00111072
Iteration 11/25 | Loss: 0.00111072
Iteration 12/25 | Loss: 0.00111072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011107175378128886, 0.0011107175378128886, 0.0011107175378128886, 0.0011107175378128886, 0.0011107175378128886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011107175378128886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.85060406
Iteration 2/25 | Loss: 0.00064922
Iteration 3/25 | Loss: 0.00064911
Iteration 4/25 | Loss: 0.00064911
Iteration 5/25 | Loss: 0.00064910
Iteration 6/25 | Loss: 0.00064910
Iteration 7/25 | Loss: 0.00064910
Iteration 8/25 | Loss: 0.00064910
Iteration 9/25 | Loss: 0.00064910
Iteration 10/25 | Loss: 0.00064910
Iteration 11/25 | Loss: 0.00064910
Iteration 12/25 | Loss: 0.00064910
Iteration 13/25 | Loss: 0.00064910
Iteration 14/25 | Loss: 0.00064910
Iteration 15/25 | Loss: 0.00064910
Iteration 16/25 | Loss: 0.00064910
Iteration 17/25 | Loss: 0.00064910
Iteration 18/25 | Loss: 0.00064910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00064910325454548, 0.00064910325454548, 0.00064910325454548, 0.00064910325454548, 0.00064910325454548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00064910325454548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064910
Iteration 2/1000 | Loss: 0.00005873
Iteration 3/1000 | Loss: 0.00004370
Iteration 4/1000 | Loss: 0.00003554
Iteration 5/1000 | Loss: 0.00003244
Iteration 6/1000 | Loss: 0.00003137
Iteration 7/1000 | Loss: 0.00003097
Iteration 8/1000 | Loss: 0.00003051
Iteration 9/1000 | Loss: 0.00003024
Iteration 10/1000 | Loss: 0.00002999
Iteration 11/1000 | Loss: 0.00002978
Iteration 12/1000 | Loss: 0.00002978
Iteration 13/1000 | Loss: 0.00002977
Iteration 14/1000 | Loss: 0.00002976
Iteration 15/1000 | Loss: 0.00002976
Iteration 16/1000 | Loss: 0.00002974
Iteration 17/1000 | Loss: 0.00002974
Iteration 18/1000 | Loss: 0.00002973
Iteration 19/1000 | Loss: 0.00002973
Iteration 20/1000 | Loss: 0.00002973
Iteration 21/1000 | Loss: 0.00002973
Iteration 22/1000 | Loss: 0.00002973
Iteration 23/1000 | Loss: 0.00002973
Iteration 24/1000 | Loss: 0.00002972
Iteration 25/1000 | Loss: 0.00002972
Iteration 26/1000 | Loss: 0.00002971
Iteration 27/1000 | Loss: 0.00002971
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002971
Iteration 30/1000 | Loss: 0.00002971
Iteration 31/1000 | Loss: 0.00002971
Iteration 32/1000 | Loss: 0.00002970
Iteration 33/1000 | Loss: 0.00002970
Iteration 34/1000 | Loss: 0.00002970
Iteration 35/1000 | Loss: 0.00002970
Iteration 36/1000 | Loss: 0.00002970
Iteration 37/1000 | Loss: 0.00002970
Iteration 38/1000 | Loss: 0.00002970
Iteration 39/1000 | Loss: 0.00002970
Iteration 40/1000 | Loss: 0.00002970
Iteration 41/1000 | Loss: 0.00002970
Iteration 42/1000 | Loss: 0.00002970
Iteration 43/1000 | Loss: 0.00002969
Iteration 44/1000 | Loss: 0.00002969
Iteration 45/1000 | Loss: 0.00002969
Iteration 46/1000 | Loss: 0.00002969
Iteration 47/1000 | Loss: 0.00002969
Iteration 48/1000 | Loss: 0.00002968
Iteration 49/1000 | Loss: 0.00002968
Iteration 50/1000 | Loss: 0.00002968
Iteration 51/1000 | Loss: 0.00002968
Iteration 52/1000 | Loss: 0.00002968
Iteration 53/1000 | Loss: 0.00002967
Iteration 54/1000 | Loss: 0.00002967
Iteration 55/1000 | Loss: 0.00002967
Iteration 56/1000 | Loss: 0.00002967
Iteration 57/1000 | Loss: 0.00002967
Iteration 58/1000 | Loss: 0.00002967
Iteration 59/1000 | Loss: 0.00002967
Iteration 60/1000 | Loss: 0.00002967
Iteration 61/1000 | Loss: 0.00002967
Iteration 62/1000 | Loss: 0.00002967
Iteration 63/1000 | Loss: 0.00002967
Iteration 64/1000 | Loss: 0.00002967
Iteration 65/1000 | Loss: 0.00002967
Iteration 66/1000 | Loss: 0.00002967
Iteration 67/1000 | Loss: 0.00002967
Iteration 68/1000 | Loss: 0.00002967
Iteration 69/1000 | Loss: 0.00002967
Iteration 70/1000 | Loss: 0.00002967
Iteration 71/1000 | Loss: 0.00002967
Iteration 72/1000 | Loss: 0.00002967
Iteration 73/1000 | Loss: 0.00002967
Iteration 74/1000 | Loss: 0.00002967
Iteration 75/1000 | Loss: 0.00002967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.966679494420532e-05, 2.966679494420532e-05, 2.966679494420532e-05, 2.966679494420532e-05, 2.966679494420532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.966679494420532e-05

Optimization complete. Final v2v error: 4.682804584503174 mm

Highest mean error: 5.271988868713379 mm for frame 151

Lowest mean error: 4.213993072509766 mm for frame 0

Saving results

Total time: 32.44983458518982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422617
Iteration 2/25 | Loss: 0.00122345
Iteration 3/25 | Loss: 0.00107581
Iteration 4/25 | Loss: 0.00105636
Iteration 5/25 | Loss: 0.00105227
Iteration 6/25 | Loss: 0.00105095
Iteration 7/25 | Loss: 0.00105060
Iteration 8/25 | Loss: 0.00105060
Iteration 9/25 | Loss: 0.00105060
Iteration 10/25 | Loss: 0.00105060
Iteration 11/25 | Loss: 0.00105060
Iteration 12/25 | Loss: 0.00105060
Iteration 13/25 | Loss: 0.00105060
Iteration 14/25 | Loss: 0.00105060
Iteration 15/25 | Loss: 0.00105060
Iteration 16/25 | Loss: 0.00105060
Iteration 17/25 | Loss: 0.00105060
Iteration 18/25 | Loss: 0.00105060
Iteration 19/25 | Loss: 0.00105060
Iteration 20/25 | Loss: 0.00105060
Iteration 21/25 | Loss: 0.00105060
Iteration 22/25 | Loss: 0.00105060
Iteration 23/25 | Loss: 0.00105060
Iteration 24/25 | Loss: 0.00105060
Iteration 25/25 | Loss: 0.00105060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42904210
Iteration 2/25 | Loss: 0.00053327
Iteration 3/25 | Loss: 0.00053327
Iteration 4/25 | Loss: 0.00053327
Iteration 5/25 | Loss: 0.00053327
Iteration 6/25 | Loss: 0.00053327
Iteration 7/25 | Loss: 0.00053327
Iteration 8/25 | Loss: 0.00053326
Iteration 9/25 | Loss: 0.00053326
Iteration 10/25 | Loss: 0.00053326
Iteration 11/25 | Loss: 0.00053326
Iteration 12/25 | Loss: 0.00053326
Iteration 13/25 | Loss: 0.00053326
Iteration 14/25 | Loss: 0.00053326
Iteration 15/25 | Loss: 0.00053326
Iteration 16/25 | Loss: 0.00053326
Iteration 17/25 | Loss: 0.00053326
Iteration 18/25 | Loss: 0.00053326
Iteration 19/25 | Loss: 0.00053326
Iteration 20/25 | Loss: 0.00053326
Iteration 21/25 | Loss: 0.00053326
Iteration 22/25 | Loss: 0.00053326
Iteration 23/25 | Loss: 0.00053326
Iteration 24/25 | Loss: 0.00053326
Iteration 25/25 | Loss: 0.00053326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053326
Iteration 2/1000 | Loss: 0.00007947
Iteration 3/1000 | Loss: 0.00004747
Iteration 4/1000 | Loss: 0.00003613
Iteration 5/1000 | Loss: 0.00003181
Iteration 6/1000 | Loss: 0.00002882
Iteration 7/1000 | Loss: 0.00002748
Iteration 8/1000 | Loss: 0.00002677
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00002615
Iteration 11/1000 | Loss: 0.00002593
Iteration 12/1000 | Loss: 0.00002593
Iteration 13/1000 | Loss: 0.00002583
Iteration 14/1000 | Loss: 0.00002580
Iteration 15/1000 | Loss: 0.00002570
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002569
Iteration 18/1000 | Loss: 0.00002569
Iteration 19/1000 | Loss: 0.00002568
Iteration 20/1000 | Loss: 0.00002567
Iteration 21/1000 | Loss: 0.00002563
Iteration 22/1000 | Loss: 0.00002561
Iteration 23/1000 | Loss: 0.00002561
Iteration 24/1000 | Loss: 0.00002561
Iteration 25/1000 | Loss: 0.00002560
Iteration 26/1000 | Loss: 0.00002560
Iteration 27/1000 | Loss: 0.00002560
Iteration 28/1000 | Loss: 0.00002560
Iteration 29/1000 | Loss: 0.00002560
Iteration 30/1000 | Loss: 0.00002559
Iteration 31/1000 | Loss: 0.00002559
Iteration 32/1000 | Loss: 0.00002559
Iteration 33/1000 | Loss: 0.00002558
Iteration 34/1000 | Loss: 0.00002558
Iteration 35/1000 | Loss: 0.00002558
Iteration 36/1000 | Loss: 0.00002558
Iteration 37/1000 | Loss: 0.00002558
Iteration 38/1000 | Loss: 0.00002558
Iteration 39/1000 | Loss: 0.00002558
Iteration 40/1000 | Loss: 0.00002558
Iteration 41/1000 | Loss: 0.00002558
Iteration 42/1000 | Loss: 0.00002557
Iteration 43/1000 | Loss: 0.00002557
Iteration 44/1000 | Loss: 0.00002557
Iteration 45/1000 | Loss: 0.00002557
Iteration 46/1000 | Loss: 0.00002557
Iteration 47/1000 | Loss: 0.00002557
Iteration 48/1000 | Loss: 0.00002557
Iteration 49/1000 | Loss: 0.00002557
Iteration 50/1000 | Loss: 0.00002557
Iteration 51/1000 | Loss: 0.00002557
Iteration 52/1000 | Loss: 0.00002557
Iteration 53/1000 | Loss: 0.00002556
Iteration 54/1000 | Loss: 0.00002556
Iteration 55/1000 | Loss: 0.00002556
Iteration 56/1000 | Loss: 0.00002556
Iteration 57/1000 | Loss: 0.00002556
Iteration 58/1000 | Loss: 0.00002556
Iteration 59/1000 | Loss: 0.00002556
Iteration 60/1000 | Loss: 0.00002556
Iteration 61/1000 | Loss: 0.00002556
Iteration 62/1000 | Loss: 0.00002556
Iteration 63/1000 | Loss: 0.00002556
Iteration 64/1000 | Loss: 0.00002555
Iteration 65/1000 | Loss: 0.00002555
Iteration 66/1000 | Loss: 0.00002555
Iteration 67/1000 | Loss: 0.00002555
Iteration 68/1000 | Loss: 0.00002555
Iteration 69/1000 | Loss: 0.00002555
Iteration 70/1000 | Loss: 0.00002555
Iteration 71/1000 | Loss: 0.00002555
Iteration 72/1000 | Loss: 0.00002555
Iteration 73/1000 | Loss: 0.00002555
Iteration 74/1000 | Loss: 0.00002555
Iteration 75/1000 | Loss: 0.00002555
Iteration 76/1000 | Loss: 0.00002554
Iteration 77/1000 | Loss: 0.00002554
Iteration 78/1000 | Loss: 0.00002554
Iteration 79/1000 | Loss: 0.00002554
Iteration 80/1000 | Loss: 0.00002554
Iteration 81/1000 | Loss: 0.00002554
Iteration 82/1000 | Loss: 0.00002554
Iteration 83/1000 | Loss: 0.00002554
Iteration 84/1000 | Loss: 0.00002554
Iteration 85/1000 | Loss: 0.00002554
Iteration 86/1000 | Loss: 0.00002554
Iteration 87/1000 | Loss: 0.00002553
Iteration 88/1000 | Loss: 0.00002553
Iteration 89/1000 | Loss: 0.00002553
Iteration 90/1000 | Loss: 0.00002553
Iteration 91/1000 | Loss: 0.00002553
Iteration 92/1000 | Loss: 0.00002553
Iteration 93/1000 | Loss: 0.00002553
Iteration 94/1000 | Loss: 0.00002553
Iteration 95/1000 | Loss: 0.00002553
Iteration 96/1000 | Loss: 0.00002553
Iteration 97/1000 | Loss: 0.00002553
Iteration 98/1000 | Loss: 0.00002553
Iteration 99/1000 | Loss: 0.00002553
Iteration 100/1000 | Loss: 0.00002553
Iteration 101/1000 | Loss: 0.00002553
Iteration 102/1000 | Loss: 0.00002553
Iteration 103/1000 | Loss: 0.00002553
Iteration 104/1000 | Loss: 0.00002553
Iteration 105/1000 | Loss: 0.00002553
Iteration 106/1000 | Loss: 0.00002553
Iteration 107/1000 | Loss: 0.00002553
Iteration 108/1000 | Loss: 0.00002553
Iteration 109/1000 | Loss: 0.00002553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.5527249817969278e-05, 2.5527249817969278e-05, 2.5527249817969278e-05, 2.5527249817969278e-05, 2.5527249817969278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5527249817969278e-05

Optimization complete. Final v2v error: 4.3244500160217285 mm

Highest mean error: 4.990017890930176 mm for frame 21

Lowest mean error: 3.815775156021118 mm for frame 37

Saving results

Total time: 32.75220489501953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00644917
Iteration 2/25 | Loss: 0.00114137
Iteration 3/25 | Loss: 0.00104364
Iteration 4/25 | Loss: 0.00102668
Iteration 5/25 | Loss: 0.00102175
Iteration 6/25 | Loss: 0.00102023
Iteration 7/25 | Loss: 0.00101994
Iteration 8/25 | Loss: 0.00101994
Iteration 9/25 | Loss: 0.00101994
Iteration 10/25 | Loss: 0.00101994
Iteration 11/25 | Loss: 0.00101994
Iteration 12/25 | Loss: 0.00101994
Iteration 13/25 | Loss: 0.00101994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001019938848912716, 0.001019938848912716, 0.001019938848912716, 0.001019938848912716, 0.001019938848912716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001019938848912716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94746709
Iteration 2/25 | Loss: 0.00058068
Iteration 3/25 | Loss: 0.00058068
Iteration 4/25 | Loss: 0.00058068
Iteration 5/25 | Loss: 0.00058067
Iteration 6/25 | Loss: 0.00058067
Iteration 7/25 | Loss: 0.00058067
Iteration 8/25 | Loss: 0.00058067
Iteration 9/25 | Loss: 0.00058067
Iteration 10/25 | Loss: 0.00058067
Iteration 11/25 | Loss: 0.00058067
Iteration 12/25 | Loss: 0.00058067
Iteration 13/25 | Loss: 0.00058067
Iteration 14/25 | Loss: 0.00058067
Iteration 15/25 | Loss: 0.00058067
Iteration 16/25 | Loss: 0.00058067
Iteration 17/25 | Loss: 0.00058067
Iteration 18/25 | Loss: 0.00058067
Iteration 19/25 | Loss: 0.00058067
Iteration 20/25 | Loss: 0.00058067
Iteration 21/25 | Loss: 0.00058067
Iteration 22/25 | Loss: 0.00058067
Iteration 23/25 | Loss: 0.00058067
Iteration 24/25 | Loss: 0.00058067
Iteration 25/25 | Loss: 0.00058067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058067
Iteration 2/1000 | Loss: 0.00006246
Iteration 3/1000 | Loss: 0.00003613
Iteration 4/1000 | Loss: 0.00002848
Iteration 5/1000 | Loss: 0.00002429
Iteration 6/1000 | Loss: 0.00002234
Iteration 7/1000 | Loss: 0.00002162
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002123
Iteration 10/1000 | Loss: 0.00002114
Iteration 11/1000 | Loss: 0.00002112
Iteration 12/1000 | Loss: 0.00002111
Iteration 13/1000 | Loss: 0.00002102
Iteration 14/1000 | Loss: 0.00002102
Iteration 15/1000 | Loss: 0.00002099
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002094
Iteration 18/1000 | Loss: 0.00002093
Iteration 19/1000 | Loss: 0.00002093
Iteration 20/1000 | Loss: 0.00002093
Iteration 21/1000 | Loss: 0.00002093
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002092
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002092
Iteration 30/1000 | Loss: 0.00002092
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002091
Iteration 36/1000 | Loss: 0.00002091
Iteration 37/1000 | Loss: 0.00002090
Iteration 38/1000 | Loss: 0.00002090
Iteration 39/1000 | Loss: 0.00002090
Iteration 40/1000 | Loss: 0.00002090
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002090
Iteration 43/1000 | Loss: 0.00002089
Iteration 44/1000 | Loss: 0.00002089
Iteration 45/1000 | Loss: 0.00002089
Iteration 46/1000 | Loss: 0.00002089
Iteration 47/1000 | Loss: 0.00002089
Iteration 48/1000 | Loss: 0.00002088
Iteration 49/1000 | Loss: 0.00002088
Iteration 50/1000 | Loss: 0.00002088
Iteration 51/1000 | Loss: 0.00002088
Iteration 52/1000 | Loss: 0.00002087
Iteration 53/1000 | Loss: 0.00002087
Iteration 54/1000 | Loss: 0.00002087
Iteration 55/1000 | Loss: 0.00002087
Iteration 56/1000 | Loss: 0.00002087
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002086
Iteration 70/1000 | Loss: 0.00002086
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002086
Iteration 77/1000 | Loss: 0.00002086
Iteration 78/1000 | Loss: 0.00002086
Iteration 79/1000 | Loss: 0.00002086
Iteration 80/1000 | Loss: 0.00002086
Iteration 81/1000 | Loss: 0.00002086
Iteration 82/1000 | Loss: 0.00002086
Iteration 83/1000 | Loss: 0.00002086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.086142194457352e-05, 2.086142194457352e-05, 2.086142194457352e-05, 2.086142194457352e-05, 2.086142194457352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.086142194457352e-05

Optimization complete. Final v2v error: 3.9320685863494873 mm

Highest mean error: 4.175249099731445 mm for frame 143

Lowest mean error: 3.684880495071411 mm for frame 78

Saving results

Total time: 28.548184871673584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829401
Iteration 2/25 | Loss: 0.00150189
Iteration 3/25 | Loss: 0.00109407
Iteration 4/25 | Loss: 0.00106843
Iteration 5/25 | Loss: 0.00106375
Iteration 6/25 | Loss: 0.00106284
Iteration 7/25 | Loss: 0.00106284
Iteration 8/25 | Loss: 0.00106284
Iteration 9/25 | Loss: 0.00106284
Iteration 10/25 | Loss: 0.00106284
Iteration 11/25 | Loss: 0.00106284
Iteration 12/25 | Loss: 0.00106284
Iteration 13/25 | Loss: 0.00106284
Iteration 14/25 | Loss: 0.00106284
Iteration 15/25 | Loss: 0.00106284
Iteration 16/25 | Loss: 0.00106284
Iteration 17/25 | Loss: 0.00106284
Iteration 18/25 | Loss: 0.00106284
Iteration 19/25 | Loss: 0.00106284
Iteration 20/25 | Loss: 0.00106284
Iteration 21/25 | Loss: 0.00106284
Iteration 22/25 | Loss: 0.00106284
Iteration 23/25 | Loss: 0.00106284
Iteration 24/25 | Loss: 0.00106284
Iteration 25/25 | Loss: 0.00106284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42026329
Iteration 2/25 | Loss: 0.00045318
Iteration 3/25 | Loss: 0.00045318
Iteration 4/25 | Loss: 0.00045318
Iteration 5/25 | Loss: 0.00045318
Iteration 6/25 | Loss: 0.00045318
Iteration 7/25 | Loss: 0.00045318
Iteration 8/25 | Loss: 0.00045318
Iteration 9/25 | Loss: 0.00045318
Iteration 10/25 | Loss: 0.00045318
Iteration 11/25 | Loss: 0.00045318
Iteration 12/25 | Loss: 0.00045318
Iteration 13/25 | Loss: 0.00045318
Iteration 14/25 | Loss: 0.00045318
Iteration 15/25 | Loss: 0.00045318
Iteration 16/25 | Loss: 0.00045318
Iteration 17/25 | Loss: 0.00045318
Iteration 18/25 | Loss: 0.00045318
Iteration 19/25 | Loss: 0.00045318
Iteration 20/25 | Loss: 0.00045318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00045317839249037206, 0.00045317839249037206, 0.00045317839249037206, 0.00045317839249037206, 0.00045317839249037206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045317839249037206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045318
Iteration 2/1000 | Loss: 0.00005540
Iteration 3/1000 | Loss: 0.00003931
Iteration 4/1000 | Loss: 0.00003239
Iteration 5/1000 | Loss: 0.00002928
Iteration 6/1000 | Loss: 0.00002812
Iteration 7/1000 | Loss: 0.00002762
Iteration 8/1000 | Loss: 0.00002744
Iteration 9/1000 | Loss: 0.00002743
Iteration 10/1000 | Loss: 0.00002720
Iteration 11/1000 | Loss: 0.00002695
Iteration 12/1000 | Loss: 0.00002694
Iteration 13/1000 | Loss: 0.00002688
Iteration 14/1000 | Loss: 0.00002674
Iteration 15/1000 | Loss: 0.00002674
Iteration 16/1000 | Loss: 0.00002674
Iteration 17/1000 | Loss: 0.00002674
Iteration 18/1000 | Loss: 0.00002673
Iteration 19/1000 | Loss: 0.00002673
Iteration 20/1000 | Loss: 0.00002673
Iteration 21/1000 | Loss: 0.00002673
Iteration 22/1000 | Loss: 0.00002673
Iteration 23/1000 | Loss: 0.00002673
Iteration 24/1000 | Loss: 0.00002673
Iteration 25/1000 | Loss: 0.00002673
Iteration 26/1000 | Loss: 0.00002672
Iteration 27/1000 | Loss: 0.00002672
Iteration 28/1000 | Loss: 0.00002670
Iteration 29/1000 | Loss: 0.00002670
Iteration 30/1000 | Loss: 0.00002669
Iteration 31/1000 | Loss: 0.00002669
Iteration 32/1000 | Loss: 0.00002668
Iteration 33/1000 | Loss: 0.00002667
Iteration 34/1000 | Loss: 0.00002667
Iteration 35/1000 | Loss: 0.00002666
Iteration 36/1000 | Loss: 0.00002666
Iteration 37/1000 | Loss: 0.00002666
Iteration 38/1000 | Loss: 0.00002666
Iteration 39/1000 | Loss: 0.00002665
Iteration 40/1000 | Loss: 0.00002664
Iteration 41/1000 | Loss: 0.00002664
Iteration 42/1000 | Loss: 0.00002663
Iteration 43/1000 | Loss: 0.00002659
Iteration 44/1000 | Loss: 0.00002658
Iteration 45/1000 | Loss: 0.00002658
Iteration 46/1000 | Loss: 0.00002658
Iteration 47/1000 | Loss: 0.00002658
Iteration 48/1000 | Loss: 0.00002657
Iteration 49/1000 | Loss: 0.00002657
Iteration 50/1000 | Loss: 0.00002657
Iteration 51/1000 | Loss: 0.00002656
Iteration 52/1000 | Loss: 0.00002655
Iteration 53/1000 | Loss: 0.00002655
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002655
Iteration 56/1000 | Loss: 0.00002655
Iteration 57/1000 | Loss: 0.00002655
Iteration 58/1000 | Loss: 0.00002655
Iteration 59/1000 | Loss: 0.00002654
Iteration 60/1000 | Loss: 0.00002654
Iteration 61/1000 | Loss: 0.00002654
Iteration 62/1000 | Loss: 0.00002654
Iteration 63/1000 | Loss: 0.00002654
Iteration 64/1000 | Loss: 0.00002654
Iteration 65/1000 | Loss: 0.00002654
Iteration 66/1000 | Loss: 0.00002653
Iteration 67/1000 | Loss: 0.00002653
Iteration 68/1000 | Loss: 0.00002653
Iteration 69/1000 | Loss: 0.00002653
Iteration 70/1000 | Loss: 0.00002653
Iteration 71/1000 | Loss: 0.00002652
Iteration 72/1000 | Loss: 0.00002652
Iteration 73/1000 | Loss: 0.00002652
Iteration 74/1000 | Loss: 0.00002652
Iteration 75/1000 | Loss: 0.00002652
Iteration 76/1000 | Loss: 0.00002652
Iteration 77/1000 | Loss: 0.00002652
Iteration 78/1000 | Loss: 0.00002652
Iteration 79/1000 | Loss: 0.00002652
Iteration 80/1000 | Loss: 0.00002652
Iteration 81/1000 | Loss: 0.00002652
Iteration 82/1000 | Loss: 0.00002652
Iteration 83/1000 | Loss: 0.00002652
Iteration 84/1000 | Loss: 0.00002652
Iteration 85/1000 | Loss: 0.00002652
Iteration 86/1000 | Loss: 0.00002651
Iteration 87/1000 | Loss: 0.00002651
Iteration 88/1000 | Loss: 0.00002651
Iteration 89/1000 | Loss: 0.00002651
Iteration 90/1000 | Loss: 0.00002651
Iteration 91/1000 | Loss: 0.00002651
Iteration 92/1000 | Loss: 0.00002651
Iteration 93/1000 | Loss: 0.00002651
Iteration 94/1000 | Loss: 0.00002651
Iteration 95/1000 | Loss: 0.00002650
Iteration 96/1000 | Loss: 0.00002650
Iteration 97/1000 | Loss: 0.00002650
Iteration 98/1000 | Loss: 0.00002650
Iteration 99/1000 | Loss: 0.00002650
Iteration 100/1000 | Loss: 0.00002650
Iteration 101/1000 | Loss: 0.00002650
Iteration 102/1000 | Loss: 0.00002650
Iteration 103/1000 | Loss: 0.00002650
Iteration 104/1000 | Loss: 0.00002650
Iteration 105/1000 | Loss: 0.00002650
Iteration 106/1000 | Loss: 0.00002650
Iteration 107/1000 | Loss: 0.00002649
Iteration 108/1000 | Loss: 0.00002649
Iteration 109/1000 | Loss: 0.00002649
Iteration 110/1000 | Loss: 0.00002649
Iteration 111/1000 | Loss: 0.00002649
Iteration 112/1000 | Loss: 0.00002649
Iteration 113/1000 | Loss: 0.00002649
Iteration 114/1000 | Loss: 0.00002649
Iteration 115/1000 | Loss: 0.00002649
Iteration 116/1000 | Loss: 0.00002649
Iteration 117/1000 | Loss: 0.00002649
Iteration 118/1000 | Loss: 0.00002649
Iteration 119/1000 | Loss: 0.00002649
Iteration 120/1000 | Loss: 0.00002649
Iteration 121/1000 | Loss: 0.00002649
Iteration 122/1000 | Loss: 0.00002648
Iteration 123/1000 | Loss: 0.00002648
Iteration 124/1000 | Loss: 0.00002648
Iteration 125/1000 | Loss: 0.00002648
Iteration 126/1000 | Loss: 0.00002648
Iteration 127/1000 | Loss: 0.00002648
Iteration 128/1000 | Loss: 0.00002648
Iteration 129/1000 | Loss: 0.00002648
Iteration 130/1000 | Loss: 0.00002648
Iteration 131/1000 | Loss: 0.00002648
Iteration 132/1000 | Loss: 0.00002648
Iteration 133/1000 | Loss: 0.00002648
Iteration 134/1000 | Loss: 0.00002648
Iteration 135/1000 | Loss: 0.00002648
Iteration 136/1000 | Loss: 0.00002648
Iteration 137/1000 | Loss: 0.00002648
Iteration 138/1000 | Loss: 0.00002648
Iteration 139/1000 | Loss: 0.00002648
Iteration 140/1000 | Loss: 0.00002648
Iteration 141/1000 | Loss: 0.00002648
Iteration 142/1000 | Loss: 0.00002648
Iteration 143/1000 | Loss: 0.00002648
Iteration 144/1000 | Loss: 0.00002648
Iteration 145/1000 | Loss: 0.00002648
Iteration 146/1000 | Loss: 0.00002648
Iteration 147/1000 | Loss: 0.00002648
Iteration 148/1000 | Loss: 0.00002648
Iteration 149/1000 | Loss: 0.00002648
Iteration 150/1000 | Loss: 0.00002648
Iteration 151/1000 | Loss: 0.00002648
Iteration 152/1000 | Loss: 0.00002648
Iteration 153/1000 | Loss: 0.00002648
Iteration 154/1000 | Loss: 0.00002648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.6476751372683793e-05, 2.6476751372683793e-05, 2.6476751372683793e-05, 2.6476751372683793e-05, 2.6476751372683793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6476751372683793e-05

Optimization complete. Final v2v error: 4.514148712158203 mm

Highest mean error: 4.685356140136719 mm for frame 36

Lowest mean error: 4.336758613586426 mm for frame 194

Saving results

Total time: 37.648213386535645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560849
Iteration 2/25 | Loss: 0.00139884
Iteration 3/25 | Loss: 0.00112648
Iteration 4/25 | Loss: 0.00109242
Iteration 5/25 | Loss: 0.00108338
Iteration 6/25 | Loss: 0.00108133
Iteration 7/25 | Loss: 0.00108091
Iteration 8/25 | Loss: 0.00108091
Iteration 9/25 | Loss: 0.00108091
Iteration 10/25 | Loss: 0.00108091
Iteration 11/25 | Loss: 0.00108091
Iteration 12/25 | Loss: 0.00108091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010809054365381598, 0.0010809054365381598, 0.0010809054365381598, 0.0010809054365381598, 0.0010809054365381598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010809054365381598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41765523
Iteration 2/25 | Loss: 0.00060398
Iteration 3/25 | Loss: 0.00060397
Iteration 4/25 | Loss: 0.00060397
Iteration 5/25 | Loss: 0.00060396
Iteration 6/25 | Loss: 0.00060396
Iteration 7/25 | Loss: 0.00060396
Iteration 8/25 | Loss: 0.00060396
Iteration 9/25 | Loss: 0.00060396
Iteration 10/25 | Loss: 0.00060396
Iteration 11/25 | Loss: 0.00060396
Iteration 12/25 | Loss: 0.00060396
Iteration 13/25 | Loss: 0.00060396
Iteration 14/25 | Loss: 0.00060396
Iteration 15/25 | Loss: 0.00060396
Iteration 16/25 | Loss: 0.00060396
Iteration 17/25 | Loss: 0.00060396
Iteration 18/25 | Loss: 0.00060396
Iteration 19/25 | Loss: 0.00060396
Iteration 20/25 | Loss: 0.00060396
Iteration 21/25 | Loss: 0.00060396
Iteration 22/25 | Loss: 0.00060396
Iteration 23/25 | Loss: 0.00060396
Iteration 24/25 | Loss: 0.00060396
Iteration 25/25 | Loss: 0.00060396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060396
Iteration 2/1000 | Loss: 0.00008331
Iteration 3/1000 | Loss: 0.00005283
Iteration 4/1000 | Loss: 0.00004236
Iteration 5/1000 | Loss: 0.00003840
Iteration 6/1000 | Loss: 0.00003616
Iteration 7/1000 | Loss: 0.00003495
Iteration 8/1000 | Loss: 0.00003431
Iteration 9/1000 | Loss: 0.00003397
Iteration 10/1000 | Loss: 0.00003360
Iteration 11/1000 | Loss: 0.00003323
Iteration 12/1000 | Loss: 0.00003301
Iteration 13/1000 | Loss: 0.00003288
Iteration 14/1000 | Loss: 0.00003280
Iteration 15/1000 | Loss: 0.00003279
Iteration 16/1000 | Loss: 0.00003279
Iteration 17/1000 | Loss: 0.00003277
Iteration 18/1000 | Loss: 0.00003276
Iteration 19/1000 | Loss: 0.00003274
Iteration 20/1000 | Loss: 0.00003273
Iteration 21/1000 | Loss: 0.00003273
Iteration 22/1000 | Loss: 0.00003273
Iteration 23/1000 | Loss: 0.00003272
Iteration 24/1000 | Loss: 0.00003272
Iteration 25/1000 | Loss: 0.00003271
Iteration 26/1000 | Loss: 0.00003271
Iteration 27/1000 | Loss: 0.00003271
Iteration 28/1000 | Loss: 0.00003270
Iteration 29/1000 | Loss: 0.00003270
Iteration 30/1000 | Loss: 0.00003270
Iteration 31/1000 | Loss: 0.00003269
Iteration 32/1000 | Loss: 0.00003269
Iteration 33/1000 | Loss: 0.00003269
Iteration 34/1000 | Loss: 0.00003268
Iteration 35/1000 | Loss: 0.00003268
Iteration 36/1000 | Loss: 0.00003268
Iteration 37/1000 | Loss: 0.00003268
Iteration 38/1000 | Loss: 0.00003268
Iteration 39/1000 | Loss: 0.00003268
Iteration 40/1000 | Loss: 0.00003268
Iteration 41/1000 | Loss: 0.00003268
Iteration 42/1000 | Loss: 0.00003267
Iteration 43/1000 | Loss: 0.00003267
Iteration 44/1000 | Loss: 0.00003267
Iteration 45/1000 | Loss: 0.00003266
Iteration 46/1000 | Loss: 0.00003266
Iteration 47/1000 | Loss: 0.00003266
Iteration 48/1000 | Loss: 0.00003266
Iteration 49/1000 | Loss: 0.00003265
Iteration 50/1000 | Loss: 0.00003265
Iteration 51/1000 | Loss: 0.00003265
Iteration 52/1000 | Loss: 0.00003265
Iteration 53/1000 | Loss: 0.00003265
Iteration 54/1000 | Loss: 0.00003265
Iteration 55/1000 | Loss: 0.00003265
Iteration 56/1000 | Loss: 0.00003264
Iteration 57/1000 | Loss: 0.00003264
Iteration 58/1000 | Loss: 0.00003264
Iteration 59/1000 | Loss: 0.00003264
Iteration 60/1000 | Loss: 0.00003264
Iteration 61/1000 | Loss: 0.00003264
Iteration 62/1000 | Loss: 0.00003264
Iteration 63/1000 | Loss: 0.00003264
Iteration 64/1000 | Loss: 0.00003264
Iteration 65/1000 | Loss: 0.00003263
Iteration 66/1000 | Loss: 0.00003263
Iteration 67/1000 | Loss: 0.00003263
Iteration 68/1000 | Loss: 0.00003263
Iteration 69/1000 | Loss: 0.00003263
Iteration 70/1000 | Loss: 0.00003263
Iteration 71/1000 | Loss: 0.00003263
Iteration 72/1000 | Loss: 0.00003263
Iteration 73/1000 | Loss: 0.00003263
Iteration 74/1000 | Loss: 0.00003263
Iteration 75/1000 | Loss: 0.00003263
Iteration 76/1000 | Loss: 0.00003263
Iteration 77/1000 | Loss: 0.00003263
Iteration 78/1000 | Loss: 0.00003263
Iteration 79/1000 | Loss: 0.00003263
Iteration 80/1000 | Loss: 0.00003262
Iteration 81/1000 | Loss: 0.00003262
Iteration 82/1000 | Loss: 0.00003262
Iteration 83/1000 | Loss: 0.00003262
Iteration 84/1000 | Loss: 0.00003262
Iteration 85/1000 | Loss: 0.00003262
Iteration 86/1000 | Loss: 0.00003262
Iteration 87/1000 | Loss: 0.00003262
Iteration 88/1000 | Loss: 0.00003262
Iteration 89/1000 | Loss: 0.00003262
Iteration 90/1000 | Loss: 0.00003262
Iteration 91/1000 | Loss: 0.00003262
Iteration 92/1000 | Loss: 0.00003261
Iteration 93/1000 | Loss: 0.00003261
Iteration 94/1000 | Loss: 0.00003261
Iteration 95/1000 | Loss: 0.00003261
Iteration 96/1000 | Loss: 0.00003261
Iteration 97/1000 | Loss: 0.00003261
Iteration 98/1000 | Loss: 0.00003261
Iteration 99/1000 | Loss: 0.00003261
Iteration 100/1000 | Loss: 0.00003261
Iteration 101/1000 | Loss: 0.00003261
Iteration 102/1000 | Loss: 0.00003261
Iteration 103/1000 | Loss: 0.00003260
Iteration 104/1000 | Loss: 0.00003260
Iteration 105/1000 | Loss: 0.00003260
Iteration 106/1000 | Loss: 0.00003260
Iteration 107/1000 | Loss: 0.00003260
Iteration 108/1000 | Loss: 0.00003260
Iteration 109/1000 | Loss: 0.00003260
Iteration 110/1000 | Loss: 0.00003260
Iteration 111/1000 | Loss: 0.00003260
Iteration 112/1000 | Loss: 0.00003260
Iteration 113/1000 | Loss: 0.00003260
Iteration 114/1000 | Loss: 0.00003259
Iteration 115/1000 | Loss: 0.00003259
Iteration 116/1000 | Loss: 0.00003259
Iteration 117/1000 | Loss: 0.00003259
Iteration 118/1000 | Loss: 0.00003259
Iteration 119/1000 | Loss: 0.00003259
Iteration 120/1000 | Loss: 0.00003259
Iteration 121/1000 | Loss: 0.00003259
Iteration 122/1000 | Loss: 0.00003259
Iteration 123/1000 | Loss: 0.00003259
Iteration 124/1000 | Loss: 0.00003259
Iteration 125/1000 | Loss: 0.00003259
Iteration 126/1000 | Loss: 0.00003259
Iteration 127/1000 | Loss: 0.00003259
Iteration 128/1000 | Loss: 0.00003259
Iteration 129/1000 | Loss: 0.00003259
Iteration 130/1000 | Loss: 0.00003259
Iteration 131/1000 | Loss: 0.00003259
Iteration 132/1000 | Loss: 0.00003259
Iteration 133/1000 | Loss: 0.00003259
Iteration 134/1000 | Loss: 0.00003259
Iteration 135/1000 | Loss: 0.00003259
Iteration 136/1000 | Loss: 0.00003259
Iteration 137/1000 | Loss: 0.00003259
Iteration 138/1000 | Loss: 0.00003259
Iteration 139/1000 | Loss: 0.00003259
Iteration 140/1000 | Loss: 0.00003259
Iteration 141/1000 | Loss: 0.00003259
Iteration 142/1000 | Loss: 0.00003259
Iteration 143/1000 | Loss: 0.00003259
Iteration 144/1000 | Loss: 0.00003259
Iteration 145/1000 | Loss: 0.00003259
Iteration 146/1000 | Loss: 0.00003259
Iteration 147/1000 | Loss: 0.00003259
Iteration 148/1000 | Loss: 0.00003259
Iteration 149/1000 | Loss: 0.00003259
Iteration 150/1000 | Loss: 0.00003259
Iteration 151/1000 | Loss: 0.00003259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [3.2585747248958796e-05, 3.2585747248958796e-05, 3.2585747248958796e-05, 3.2585747248958796e-05, 3.2585747248958796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2585747248958796e-05

Optimization complete. Final v2v error: 4.916316509246826 mm

Highest mean error: 5.586178302764893 mm for frame 40

Lowest mean error: 4.324134349822998 mm for frame 101

Saving results

Total time: 36.15896439552307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095815
Iteration 2/25 | Loss: 0.00202227
Iteration 3/25 | Loss: 0.00149787
Iteration 4/25 | Loss: 0.00140995
Iteration 5/25 | Loss: 0.00149428
Iteration 6/25 | Loss: 0.00150782
Iteration 7/25 | Loss: 0.00142155
Iteration 8/25 | Loss: 0.00135933
Iteration 9/25 | Loss: 0.00123130
Iteration 10/25 | Loss: 0.00117260
Iteration 11/25 | Loss: 0.00113900
Iteration 12/25 | Loss: 0.00111773
Iteration 13/25 | Loss: 0.00111483
Iteration 14/25 | Loss: 0.00110423
Iteration 15/25 | Loss: 0.00110645
Iteration 16/25 | Loss: 0.00110181
Iteration 17/25 | Loss: 0.00109545
Iteration 18/25 | Loss: 0.00108381
Iteration 19/25 | Loss: 0.00108755
Iteration 20/25 | Loss: 0.00108578
Iteration 21/25 | Loss: 0.00107811
Iteration 22/25 | Loss: 0.00108675
Iteration 23/25 | Loss: 0.00108557
Iteration 24/25 | Loss: 0.00107917
Iteration 25/25 | Loss: 0.00107564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48127818
Iteration 2/25 | Loss: 0.00098235
Iteration 3/25 | Loss: 0.00098235
Iteration 4/25 | Loss: 0.00098235
Iteration 5/25 | Loss: 0.00098235
Iteration 6/25 | Loss: 0.00098235
Iteration 7/25 | Loss: 0.00098235
Iteration 8/25 | Loss: 0.00098235
Iteration 9/25 | Loss: 0.00098235
Iteration 10/25 | Loss: 0.00098235
Iteration 11/25 | Loss: 0.00098235
Iteration 12/25 | Loss: 0.00098235
Iteration 13/25 | Loss: 0.00098235
Iteration 14/25 | Loss: 0.00098235
Iteration 15/25 | Loss: 0.00098235
Iteration 16/25 | Loss: 0.00098235
Iteration 17/25 | Loss: 0.00098235
Iteration 18/25 | Loss: 0.00098235
Iteration 19/25 | Loss: 0.00098235
Iteration 20/25 | Loss: 0.00098235
Iteration 21/25 | Loss: 0.00098235
Iteration 22/25 | Loss: 0.00098235
Iteration 23/25 | Loss: 0.00098235
Iteration 24/25 | Loss: 0.00098235
Iteration 25/25 | Loss: 0.00098235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098235
Iteration 2/1000 | Loss: 0.00049303
Iteration 3/1000 | Loss: 0.00064763
Iteration 4/1000 | Loss: 0.00041319
Iteration 5/1000 | Loss: 0.00072763
Iteration 6/1000 | Loss: 0.00028420
Iteration 7/1000 | Loss: 0.00073016
Iteration 8/1000 | Loss: 0.00060008
Iteration 9/1000 | Loss: 0.00041547
Iteration 10/1000 | Loss: 0.00076219
Iteration 11/1000 | Loss: 0.00058341
Iteration 12/1000 | Loss: 0.00044022
Iteration 13/1000 | Loss: 0.00046719
Iteration 14/1000 | Loss: 0.00037688
Iteration 15/1000 | Loss: 0.00057814
Iteration 16/1000 | Loss: 0.00030452
Iteration 17/1000 | Loss: 0.00017685
Iteration 18/1000 | Loss: 0.00035629
Iteration 19/1000 | Loss: 0.00045747
Iteration 20/1000 | Loss: 0.00009213
Iteration 21/1000 | Loss: 0.00018951
Iteration 22/1000 | Loss: 0.00034772
Iteration 23/1000 | Loss: 0.00033071
Iteration 24/1000 | Loss: 0.00009889
Iteration 25/1000 | Loss: 0.00007156
Iteration 26/1000 | Loss: 0.00039051
Iteration 27/1000 | Loss: 0.00032717
Iteration 28/1000 | Loss: 0.00055430
Iteration 29/1000 | Loss: 0.00029897
Iteration 30/1000 | Loss: 0.00026929
Iteration 31/1000 | Loss: 0.00019343
Iteration 32/1000 | Loss: 0.00011009
Iteration 33/1000 | Loss: 0.00007341
Iteration 34/1000 | Loss: 0.00017859
Iteration 35/1000 | Loss: 0.00022144
Iteration 36/1000 | Loss: 0.00008834
Iteration 37/1000 | Loss: 0.00031038
Iteration 38/1000 | Loss: 0.00117724
Iteration 39/1000 | Loss: 0.00042293
Iteration 40/1000 | Loss: 0.00030978
Iteration 41/1000 | Loss: 0.00015939
Iteration 42/1000 | Loss: 0.00037113
Iteration 43/1000 | Loss: 0.00028488
Iteration 44/1000 | Loss: 0.00015313
Iteration 45/1000 | Loss: 0.00033608
Iteration 46/1000 | Loss: 0.00029521
Iteration 47/1000 | Loss: 0.00020195
Iteration 48/1000 | Loss: 0.00039544
Iteration 49/1000 | Loss: 0.00035134
Iteration 50/1000 | Loss: 0.00022654
Iteration 51/1000 | Loss: 0.00019761
Iteration 52/1000 | Loss: 0.00011702
Iteration 53/1000 | Loss: 0.00013070
Iteration 54/1000 | Loss: 0.00011769
Iteration 55/1000 | Loss: 0.00010206
Iteration 56/1000 | Loss: 0.00009010
Iteration 57/1000 | Loss: 0.00009263
Iteration 58/1000 | Loss: 0.00068836
Iteration 59/1000 | Loss: 0.00006043
Iteration 60/1000 | Loss: 0.00004787
Iteration 61/1000 | Loss: 0.00006001
Iteration 62/1000 | Loss: 0.00006042
Iteration 63/1000 | Loss: 0.00005628
Iteration 64/1000 | Loss: 0.00005039
Iteration 65/1000 | Loss: 0.00005433
Iteration 66/1000 | Loss: 0.00037805
Iteration 67/1000 | Loss: 0.00033292
Iteration 68/1000 | Loss: 0.00033004
Iteration 69/1000 | Loss: 0.00006061
Iteration 70/1000 | Loss: 0.00031505
Iteration 71/1000 | Loss: 0.00006757
Iteration 72/1000 | Loss: 0.00046178
Iteration 73/1000 | Loss: 0.00029221
Iteration 74/1000 | Loss: 0.00017041
Iteration 75/1000 | Loss: 0.00026261
Iteration 76/1000 | Loss: 0.00006419
Iteration 77/1000 | Loss: 0.00028433
Iteration 78/1000 | Loss: 0.00033441
Iteration 79/1000 | Loss: 0.00017382
Iteration 80/1000 | Loss: 0.00037150
Iteration 81/1000 | Loss: 0.00030733
Iteration 82/1000 | Loss: 0.00026086
Iteration 83/1000 | Loss: 0.00038301
Iteration 84/1000 | Loss: 0.00046632
Iteration 85/1000 | Loss: 0.00058726
Iteration 86/1000 | Loss: 0.00056189
Iteration 87/1000 | Loss: 0.00063343
Iteration 88/1000 | Loss: 0.00067193
Iteration 89/1000 | Loss: 0.00044780
Iteration 90/1000 | Loss: 0.00019338
Iteration 91/1000 | Loss: 0.00040632
Iteration 92/1000 | Loss: 0.00023822
Iteration 93/1000 | Loss: 0.00012592
Iteration 94/1000 | Loss: 0.00047379
Iteration 95/1000 | Loss: 0.00055886
Iteration 96/1000 | Loss: 0.00058819
Iteration 97/1000 | Loss: 0.00024251
Iteration 98/1000 | Loss: 0.00006667
Iteration 99/1000 | Loss: 0.00006707
Iteration 100/1000 | Loss: 0.00006731
Iteration 101/1000 | Loss: 0.00031681
Iteration 102/1000 | Loss: 0.00011016
Iteration 103/1000 | Loss: 0.00005924
Iteration 104/1000 | Loss: 0.00054480
Iteration 105/1000 | Loss: 0.00024786
Iteration 106/1000 | Loss: 0.00037192
Iteration 107/1000 | Loss: 0.00050267
Iteration 108/1000 | Loss: 0.00022872
Iteration 109/1000 | Loss: 0.00031461
Iteration 110/1000 | Loss: 0.00007601
Iteration 111/1000 | Loss: 0.00029796
Iteration 112/1000 | Loss: 0.00045417
Iteration 113/1000 | Loss: 0.00039950
Iteration 114/1000 | Loss: 0.00068846
Iteration 115/1000 | Loss: 0.00038032
Iteration 116/1000 | Loss: 0.00019199
Iteration 117/1000 | Loss: 0.00009342
Iteration 118/1000 | Loss: 0.00004427
Iteration 119/1000 | Loss: 0.00004044
Iteration 120/1000 | Loss: 0.00024426
Iteration 121/1000 | Loss: 0.00020823
Iteration 122/1000 | Loss: 0.00053077
Iteration 123/1000 | Loss: 0.00019454
Iteration 124/1000 | Loss: 0.00034110
Iteration 125/1000 | Loss: 0.00046667
Iteration 126/1000 | Loss: 0.00014223
Iteration 127/1000 | Loss: 0.00053662
Iteration 128/1000 | Loss: 0.00021054
Iteration 129/1000 | Loss: 0.00021652
Iteration 130/1000 | Loss: 0.00004996
Iteration 131/1000 | Loss: 0.00025993
Iteration 132/1000 | Loss: 0.00021686
Iteration 133/1000 | Loss: 0.00030857
Iteration 134/1000 | Loss: 0.00064995
Iteration 135/1000 | Loss: 0.00005460
Iteration 136/1000 | Loss: 0.00008645
Iteration 137/1000 | Loss: 0.00003653
Iteration 138/1000 | Loss: 0.00003387
Iteration 139/1000 | Loss: 0.00003256
Iteration 140/1000 | Loss: 0.00003179
Iteration 141/1000 | Loss: 0.00003083
Iteration 142/1000 | Loss: 0.00003025
Iteration 143/1000 | Loss: 0.00002994
Iteration 144/1000 | Loss: 0.00002967
Iteration 145/1000 | Loss: 0.00002954
Iteration 146/1000 | Loss: 0.00002936
Iteration 147/1000 | Loss: 0.00002925
Iteration 148/1000 | Loss: 0.00002925
Iteration 149/1000 | Loss: 0.00002924
Iteration 150/1000 | Loss: 0.00002924
Iteration 151/1000 | Loss: 0.00002923
Iteration 152/1000 | Loss: 0.00002923
Iteration 153/1000 | Loss: 0.00002922
Iteration 154/1000 | Loss: 0.00002922
Iteration 155/1000 | Loss: 0.00002922
Iteration 156/1000 | Loss: 0.00002922
Iteration 157/1000 | Loss: 0.00002922
Iteration 158/1000 | Loss: 0.00002921
Iteration 159/1000 | Loss: 0.00002921
Iteration 160/1000 | Loss: 0.00002921
Iteration 161/1000 | Loss: 0.00002921
Iteration 162/1000 | Loss: 0.00002920
Iteration 163/1000 | Loss: 0.00002920
Iteration 164/1000 | Loss: 0.00002920
Iteration 165/1000 | Loss: 0.00002919
Iteration 166/1000 | Loss: 0.00002919
Iteration 167/1000 | Loss: 0.00002919
Iteration 168/1000 | Loss: 0.00002919
Iteration 169/1000 | Loss: 0.00002919
Iteration 170/1000 | Loss: 0.00002919
Iteration 171/1000 | Loss: 0.00002919
Iteration 172/1000 | Loss: 0.00002919
Iteration 173/1000 | Loss: 0.00002919
Iteration 174/1000 | Loss: 0.00002919
Iteration 175/1000 | Loss: 0.00002918
Iteration 176/1000 | Loss: 0.00002918
Iteration 177/1000 | Loss: 0.00002918
Iteration 178/1000 | Loss: 0.00002918
Iteration 179/1000 | Loss: 0.00002917
Iteration 180/1000 | Loss: 0.00002917
Iteration 181/1000 | Loss: 0.00002917
Iteration 182/1000 | Loss: 0.00002917
Iteration 183/1000 | Loss: 0.00002916
Iteration 184/1000 | Loss: 0.00002916
Iteration 185/1000 | Loss: 0.00002916
Iteration 186/1000 | Loss: 0.00002916
Iteration 187/1000 | Loss: 0.00002915
Iteration 188/1000 | Loss: 0.00002915
Iteration 189/1000 | Loss: 0.00002915
Iteration 190/1000 | Loss: 0.00002915
Iteration 191/1000 | Loss: 0.00002915
Iteration 192/1000 | Loss: 0.00002915
Iteration 193/1000 | Loss: 0.00002915
Iteration 194/1000 | Loss: 0.00002915
Iteration 195/1000 | Loss: 0.00002915
Iteration 196/1000 | Loss: 0.00002915
Iteration 197/1000 | Loss: 0.00002915
Iteration 198/1000 | Loss: 0.00002915
Iteration 199/1000 | Loss: 0.00002915
Iteration 200/1000 | Loss: 0.00002915
Iteration 201/1000 | Loss: 0.00002915
Iteration 202/1000 | Loss: 0.00002915
Iteration 203/1000 | Loss: 0.00002914
Iteration 204/1000 | Loss: 0.00002914
Iteration 205/1000 | Loss: 0.00002914
Iteration 206/1000 | Loss: 0.00002914
Iteration 207/1000 | Loss: 0.00002914
Iteration 208/1000 | Loss: 0.00002914
Iteration 209/1000 | Loss: 0.00002914
Iteration 210/1000 | Loss: 0.00002914
Iteration 211/1000 | Loss: 0.00002914
Iteration 212/1000 | Loss: 0.00002914
Iteration 213/1000 | Loss: 0.00002914
Iteration 214/1000 | Loss: 0.00002914
Iteration 215/1000 | Loss: 0.00002914
Iteration 216/1000 | Loss: 0.00002914
Iteration 217/1000 | Loss: 0.00002914
Iteration 218/1000 | Loss: 0.00002914
Iteration 219/1000 | Loss: 0.00002914
Iteration 220/1000 | Loss: 0.00002913
Iteration 221/1000 | Loss: 0.00002913
Iteration 222/1000 | Loss: 0.00002913
Iteration 223/1000 | Loss: 0.00002913
Iteration 224/1000 | Loss: 0.00002913
Iteration 225/1000 | Loss: 0.00002913
Iteration 226/1000 | Loss: 0.00002913
Iteration 227/1000 | Loss: 0.00002913
Iteration 228/1000 | Loss: 0.00002913
Iteration 229/1000 | Loss: 0.00002913
Iteration 230/1000 | Loss: 0.00002913
Iteration 231/1000 | Loss: 0.00002913
Iteration 232/1000 | Loss: 0.00002913
Iteration 233/1000 | Loss: 0.00002913
Iteration 234/1000 | Loss: 0.00002913
Iteration 235/1000 | Loss: 0.00002913
Iteration 236/1000 | Loss: 0.00002913
Iteration 237/1000 | Loss: 0.00002913
Iteration 238/1000 | Loss: 0.00002913
Iteration 239/1000 | Loss: 0.00002913
Iteration 240/1000 | Loss: 0.00002913
Iteration 241/1000 | Loss: 0.00002912
Iteration 242/1000 | Loss: 0.00002912
Iteration 243/1000 | Loss: 0.00002912
Iteration 244/1000 | Loss: 0.00002912
Iteration 245/1000 | Loss: 0.00002912
Iteration 246/1000 | Loss: 0.00002912
Iteration 247/1000 | Loss: 0.00002912
Iteration 248/1000 | Loss: 0.00002912
Iteration 249/1000 | Loss: 0.00002912
Iteration 250/1000 | Loss: 0.00002912
Iteration 251/1000 | Loss: 0.00002912
Iteration 252/1000 | Loss: 0.00002912
Iteration 253/1000 | Loss: 0.00002912
Iteration 254/1000 | Loss: 0.00002912
Iteration 255/1000 | Loss: 0.00002912
Iteration 256/1000 | Loss: 0.00002912
Iteration 257/1000 | Loss: 0.00002912
Iteration 258/1000 | Loss: 0.00002912
Iteration 259/1000 | Loss: 0.00002912
Iteration 260/1000 | Loss: 0.00002912
Iteration 261/1000 | Loss: 0.00002912
Iteration 262/1000 | Loss: 0.00002912
Iteration 263/1000 | Loss: 0.00002912
Iteration 264/1000 | Loss: 0.00002912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.9118473321432248e-05, 2.9118473321432248e-05, 2.9118473321432248e-05, 2.9118473321432248e-05, 2.9118473321432248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9118473321432248e-05

Optimization complete. Final v2v error: 4.60469913482666 mm

Highest mean error: 11.468899726867676 mm for frame 115

Lowest mean error: 4.129861354827881 mm for frame 21

Saving results

Total time: 252.1875102519989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620478
Iteration 2/25 | Loss: 0.00133892
Iteration 3/25 | Loss: 0.00116110
Iteration 4/25 | Loss: 0.00113450
Iteration 5/25 | Loss: 0.00112822
Iteration 6/25 | Loss: 0.00112498
Iteration 7/25 | Loss: 0.00112379
Iteration 8/25 | Loss: 0.00112365
Iteration 9/25 | Loss: 0.00112365
Iteration 10/25 | Loss: 0.00112365
Iteration 11/25 | Loss: 0.00112365
Iteration 12/25 | Loss: 0.00112365
Iteration 13/25 | Loss: 0.00112365
Iteration 14/25 | Loss: 0.00112365
Iteration 15/25 | Loss: 0.00112365
Iteration 16/25 | Loss: 0.00112365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011236497666686773, 0.0011236497666686773, 0.0011236497666686773, 0.0011236497666686773, 0.0011236497666686773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011236497666686773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09963953
Iteration 2/25 | Loss: 0.00061365
Iteration 3/25 | Loss: 0.00061357
Iteration 4/25 | Loss: 0.00061357
Iteration 5/25 | Loss: 0.00061357
Iteration 6/25 | Loss: 0.00061357
Iteration 7/25 | Loss: 0.00061357
Iteration 8/25 | Loss: 0.00061357
Iteration 9/25 | Loss: 0.00061357
Iteration 10/25 | Loss: 0.00061357
Iteration 11/25 | Loss: 0.00061357
Iteration 12/25 | Loss: 0.00061357
Iteration 13/25 | Loss: 0.00061357
Iteration 14/25 | Loss: 0.00061357
Iteration 15/25 | Loss: 0.00061357
Iteration 16/25 | Loss: 0.00061357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006135670701041818, 0.0006135670701041818, 0.0006135670701041818, 0.0006135670701041818, 0.0006135670701041818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006135670701041818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061357
Iteration 2/1000 | Loss: 0.00009966
Iteration 3/1000 | Loss: 0.00005552
Iteration 4/1000 | Loss: 0.00004318
Iteration 5/1000 | Loss: 0.00003683
Iteration 6/1000 | Loss: 0.00003361
Iteration 7/1000 | Loss: 0.00003060
Iteration 8/1000 | Loss: 0.00002949
Iteration 9/1000 | Loss: 0.00002887
Iteration 10/1000 | Loss: 0.00002844
Iteration 11/1000 | Loss: 0.00002816
Iteration 12/1000 | Loss: 0.00002783
Iteration 13/1000 | Loss: 0.00002760
Iteration 14/1000 | Loss: 0.00002738
Iteration 15/1000 | Loss: 0.00002718
Iteration 16/1000 | Loss: 0.00002707
Iteration 17/1000 | Loss: 0.00002700
Iteration 18/1000 | Loss: 0.00002692
Iteration 19/1000 | Loss: 0.00002689
Iteration 20/1000 | Loss: 0.00002687
Iteration 21/1000 | Loss: 0.00002687
Iteration 22/1000 | Loss: 0.00002686
Iteration 23/1000 | Loss: 0.00002675
Iteration 24/1000 | Loss: 0.00002658
Iteration 25/1000 | Loss: 0.00002641
Iteration 26/1000 | Loss: 0.00002640
Iteration 27/1000 | Loss: 0.00002639
Iteration 28/1000 | Loss: 0.00002639
Iteration 29/1000 | Loss: 0.00002639
Iteration 30/1000 | Loss: 0.00002639
Iteration 31/1000 | Loss: 0.00002638
Iteration 32/1000 | Loss: 0.00002638
Iteration 33/1000 | Loss: 0.00002632
Iteration 34/1000 | Loss: 0.00002630
Iteration 35/1000 | Loss: 0.00002630
Iteration 36/1000 | Loss: 0.00002627
Iteration 37/1000 | Loss: 0.00002627
Iteration 38/1000 | Loss: 0.00002626
Iteration 39/1000 | Loss: 0.00002621
Iteration 40/1000 | Loss: 0.00002621
Iteration 41/1000 | Loss: 0.00002621
Iteration 42/1000 | Loss: 0.00002621
Iteration 43/1000 | Loss: 0.00002621
Iteration 44/1000 | Loss: 0.00002621
Iteration 45/1000 | Loss: 0.00002618
Iteration 46/1000 | Loss: 0.00002617
Iteration 47/1000 | Loss: 0.00002616
Iteration 48/1000 | Loss: 0.00002616
Iteration 49/1000 | Loss: 0.00002613
Iteration 50/1000 | Loss: 0.00002613
Iteration 51/1000 | Loss: 0.00002612
Iteration 52/1000 | Loss: 0.00002612
Iteration 53/1000 | Loss: 0.00002612
Iteration 54/1000 | Loss: 0.00002612
Iteration 55/1000 | Loss: 0.00002612
Iteration 56/1000 | Loss: 0.00002611
Iteration 57/1000 | Loss: 0.00002611
Iteration 58/1000 | Loss: 0.00002608
Iteration 59/1000 | Loss: 0.00002608
Iteration 60/1000 | Loss: 0.00002608
Iteration 61/1000 | Loss: 0.00002607
Iteration 62/1000 | Loss: 0.00002607
Iteration 63/1000 | Loss: 0.00002607
Iteration 64/1000 | Loss: 0.00002607
Iteration 65/1000 | Loss: 0.00002607
Iteration 66/1000 | Loss: 0.00002606
Iteration 67/1000 | Loss: 0.00002606
Iteration 68/1000 | Loss: 0.00002603
Iteration 69/1000 | Loss: 0.00002603
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002603
Iteration 72/1000 | Loss: 0.00002602
Iteration 73/1000 | Loss: 0.00002602
Iteration 74/1000 | Loss: 0.00002602
Iteration 75/1000 | Loss: 0.00002602
Iteration 76/1000 | Loss: 0.00002602
Iteration 77/1000 | Loss: 0.00002601
Iteration 78/1000 | Loss: 0.00002600
Iteration 79/1000 | Loss: 0.00002600
Iteration 80/1000 | Loss: 0.00002600
Iteration 81/1000 | Loss: 0.00002600
Iteration 82/1000 | Loss: 0.00002600
Iteration 83/1000 | Loss: 0.00002600
Iteration 84/1000 | Loss: 0.00002599
Iteration 85/1000 | Loss: 0.00002598
Iteration 86/1000 | Loss: 0.00002597
Iteration 87/1000 | Loss: 0.00002596
Iteration 88/1000 | Loss: 0.00002596
Iteration 89/1000 | Loss: 0.00002596
Iteration 90/1000 | Loss: 0.00002596
Iteration 91/1000 | Loss: 0.00002595
Iteration 92/1000 | Loss: 0.00002595
Iteration 93/1000 | Loss: 0.00002595
Iteration 94/1000 | Loss: 0.00002595
Iteration 95/1000 | Loss: 0.00002595
Iteration 96/1000 | Loss: 0.00002595
Iteration 97/1000 | Loss: 0.00002594
Iteration 98/1000 | Loss: 0.00002594
Iteration 99/1000 | Loss: 0.00002594
Iteration 100/1000 | Loss: 0.00002594
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002593
Iteration 103/1000 | Loss: 0.00002593
Iteration 104/1000 | Loss: 0.00002593
Iteration 105/1000 | Loss: 0.00002593
Iteration 106/1000 | Loss: 0.00002593
Iteration 107/1000 | Loss: 0.00002593
Iteration 108/1000 | Loss: 0.00002592
Iteration 109/1000 | Loss: 0.00002592
Iteration 110/1000 | Loss: 0.00002592
Iteration 111/1000 | Loss: 0.00002592
Iteration 112/1000 | Loss: 0.00002591
Iteration 113/1000 | Loss: 0.00002591
Iteration 114/1000 | Loss: 0.00002591
Iteration 115/1000 | Loss: 0.00002591
Iteration 116/1000 | Loss: 0.00002591
Iteration 117/1000 | Loss: 0.00002591
Iteration 118/1000 | Loss: 0.00002591
Iteration 119/1000 | Loss: 0.00002591
Iteration 120/1000 | Loss: 0.00002591
Iteration 121/1000 | Loss: 0.00002590
Iteration 122/1000 | Loss: 0.00002590
Iteration 123/1000 | Loss: 0.00002590
Iteration 124/1000 | Loss: 0.00002590
Iteration 125/1000 | Loss: 0.00002590
Iteration 126/1000 | Loss: 0.00002590
Iteration 127/1000 | Loss: 0.00002589
Iteration 128/1000 | Loss: 0.00002589
Iteration 129/1000 | Loss: 0.00002589
Iteration 130/1000 | Loss: 0.00002589
Iteration 131/1000 | Loss: 0.00002589
Iteration 132/1000 | Loss: 0.00002589
Iteration 133/1000 | Loss: 0.00002589
Iteration 134/1000 | Loss: 0.00002589
Iteration 135/1000 | Loss: 0.00002589
Iteration 136/1000 | Loss: 0.00002589
Iteration 137/1000 | Loss: 0.00002589
Iteration 138/1000 | Loss: 0.00002589
Iteration 139/1000 | Loss: 0.00002589
Iteration 140/1000 | Loss: 0.00002589
Iteration 141/1000 | Loss: 0.00002589
Iteration 142/1000 | Loss: 0.00002589
Iteration 143/1000 | Loss: 0.00002588
Iteration 144/1000 | Loss: 0.00002588
Iteration 145/1000 | Loss: 0.00002588
Iteration 146/1000 | Loss: 0.00002588
Iteration 147/1000 | Loss: 0.00002588
Iteration 148/1000 | Loss: 0.00002588
Iteration 149/1000 | Loss: 0.00002588
Iteration 150/1000 | Loss: 0.00002588
Iteration 151/1000 | Loss: 0.00002588
Iteration 152/1000 | Loss: 0.00002588
Iteration 153/1000 | Loss: 0.00002588
Iteration 154/1000 | Loss: 0.00002588
Iteration 155/1000 | Loss: 0.00002588
Iteration 156/1000 | Loss: 0.00002588
Iteration 157/1000 | Loss: 0.00002588
Iteration 158/1000 | Loss: 0.00002588
Iteration 159/1000 | Loss: 0.00002588
Iteration 160/1000 | Loss: 0.00002588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.5882687623379752e-05, 2.5882687623379752e-05, 2.5882687623379752e-05, 2.5882687623379752e-05, 2.5882687623379752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5882687623379752e-05

Optimization complete. Final v2v error: 4.346009254455566 mm

Highest mean error: 4.7984418869018555 mm for frame 48

Lowest mean error: 3.8561806678771973 mm for frame 117

Saving results

Total time: 49.77049469947815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457762
Iteration 2/25 | Loss: 0.00134353
Iteration 3/25 | Loss: 0.00108227
Iteration 4/25 | Loss: 0.00103345
Iteration 5/25 | Loss: 0.00102321
Iteration 6/25 | Loss: 0.00102023
Iteration 7/25 | Loss: 0.00101916
Iteration 8/25 | Loss: 0.00101901
Iteration 9/25 | Loss: 0.00101901
Iteration 10/25 | Loss: 0.00101901
Iteration 11/25 | Loss: 0.00101901
Iteration 12/25 | Loss: 0.00101901
Iteration 13/25 | Loss: 0.00101901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010190117172896862, 0.0010190117172896862, 0.0010190117172896862, 0.0010190117172896862, 0.0010190117172896862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010190117172896862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40828419
Iteration 2/25 | Loss: 0.00047689
Iteration 3/25 | Loss: 0.00047688
Iteration 4/25 | Loss: 0.00047688
Iteration 5/25 | Loss: 0.00047688
Iteration 6/25 | Loss: 0.00047688
Iteration 7/25 | Loss: 0.00047688
Iteration 8/25 | Loss: 0.00047688
Iteration 9/25 | Loss: 0.00047688
Iteration 10/25 | Loss: 0.00047688
Iteration 11/25 | Loss: 0.00047688
Iteration 12/25 | Loss: 0.00047688
Iteration 13/25 | Loss: 0.00047688
Iteration 14/25 | Loss: 0.00047688
Iteration 15/25 | Loss: 0.00047688
Iteration 16/25 | Loss: 0.00047688
Iteration 17/25 | Loss: 0.00047688
Iteration 18/25 | Loss: 0.00047688
Iteration 19/25 | Loss: 0.00047688
Iteration 20/25 | Loss: 0.00047688
Iteration 21/25 | Loss: 0.00047688
Iteration 22/25 | Loss: 0.00047688
Iteration 23/25 | Loss: 0.00047688
Iteration 24/25 | Loss: 0.00047688
Iteration 25/25 | Loss: 0.00047688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047688
Iteration 2/1000 | Loss: 0.00005483
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002336
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00002008
Iteration 9/1000 | Loss: 0.00001986
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001952
Iteration 12/1000 | Loss: 0.00001947
Iteration 13/1000 | Loss: 0.00001945
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001937
Iteration 17/1000 | Loss: 0.00001935
Iteration 18/1000 | Loss: 0.00001935
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001934
Iteration 21/1000 | Loss: 0.00001934
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001933
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001932
Iteration 33/1000 | Loss: 0.00001932
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001928
Iteration 49/1000 | Loss: 0.00001928
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001927
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001926
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001926
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001924
Iteration 71/1000 | Loss: 0.00001924
Iteration 72/1000 | Loss: 0.00001924
Iteration 73/1000 | Loss: 0.00001924
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001924
Iteration 76/1000 | Loss: 0.00001924
Iteration 77/1000 | Loss: 0.00001923
Iteration 78/1000 | Loss: 0.00001923
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001923
Iteration 81/1000 | Loss: 0.00001923
Iteration 82/1000 | Loss: 0.00001923
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001922
Iteration 88/1000 | Loss: 0.00001922
Iteration 89/1000 | Loss: 0.00001922
Iteration 90/1000 | Loss: 0.00001922
Iteration 91/1000 | Loss: 0.00001922
Iteration 92/1000 | Loss: 0.00001922
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001921
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001921
Iteration 97/1000 | Loss: 0.00001921
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001921
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001921
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001920
Iteration 106/1000 | Loss: 0.00001920
Iteration 107/1000 | Loss: 0.00001920
Iteration 108/1000 | Loss: 0.00001920
Iteration 109/1000 | Loss: 0.00001920
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001920
Iteration 114/1000 | Loss: 0.00001920
Iteration 115/1000 | Loss: 0.00001920
Iteration 116/1000 | Loss: 0.00001920
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001919
Iteration 120/1000 | Loss: 0.00001919
Iteration 121/1000 | Loss: 0.00001919
Iteration 122/1000 | Loss: 0.00001919
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001918
Iteration 125/1000 | Loss: 0.00001918
Iteration 126/1000 | Loss: 0.00001918
Iteration 127/1000 | Loss: 0.00001918
Iteration 128/1000 | Loss: 0.00001918
Iteration 129/1000 | Loss: 0.00001918
Iteration 130/1000 | Loss: 0.00001918
Iteration 131/1000 | Loss: 0.00001918
Iteration 132/1000 | Loss: 0.00001918
Iteration 133/1000 | Loss: 0.00001918
Iteration 134/1000 | Loss: 0.00001918
Iteration 135/1000 | Loss: 0.00001918
Iteration 136/1000 | Loss: 0.00001918
Iteration 137/1000 | Loss: 0.00001918
Iteration 138/1000 | Loss: 0.00001918
Iteration 139/1000 | Loss: 0.00001918
Iteration 140/1000 | Loss: 0.00001918
Iteration 141/1000 | Loss: 0.00001918
Iteration 142/1000 | Loss: 0.00001918
Iteration 143/1000 | Loss: 0.00001917
Iteration 144/1000 | Loss: 0.00001917
Iteration 145/1000 | Loss: 0.00001917
Iteration 146/1000 | Loss: 0.00001917
Iteration 147/1000 | Loss: 0.00001917
Iteration 148/1000 | Loss: 0.00001917
Iteration 149/1000 | Loss: 0.00001917
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001917
Iteration 152/1000 | Loss: 0.00001917
Iteration 153/1000 | Loss: 0.00001917
Iteration 154/1000 | Loss: 0.00001917
Iteration 155/1000 | Loss: 0.00001917
Iteration 156/1000 | Loss: 0.00001917
Iteration 157/1000 | Loss: 0.00001917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.917247755045537e-05, 1.917247755045537e-05, 1.917247755045537e-05, 1.917247755045537e-05, 1.917247755045537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.917247755045537e-05

Optimization complete. Final v2v error: 3.7458508014678955 mm

Highest mean error: 4.477685451507568 mm for frame 92

Lowest mean error: 3.4471240043640137 mm for frame 187

Saving results

Total time: 40.04537653923035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873777
Iteration 2/25 | Loss: 0.00146883
Iteration 3/25 | Loss: 0.00112435
Iteration 4/25 | Loss: 0.00109287
Iteration 5/25 | Loss: 0.00108460
Iteration 6/25 | Loss: 0.00108154
Iteration 7/25 | Loss: 0.00108105
Iteration 8/25 | Loss: 0.00108105
Iteration 9/25 | Loss: 0.00108105
Iteration 10/25 | Loss: 0.00108105
Iteration 11/25 | Loss: 0.00108105
Iteration 12/25 | Loss: 0.00108105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010810515377670527, 0.0010810515377670527, 0.0010810515377670527, 0.0010810515377670527, 0.0010810515377670527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010810515377670527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16489959
Iteration 2/25 | Loss: 0.00050993
Iteration 3/25 | Loss: 0.00050993
Iteration 4/25 | Loss: 0.00050993
Iteration 5/25 | Loss: 0.00050993
Iteration 6/25 | Loss: 0.00050993
Iteration 7/25 | Loss: 0.00050993
Iteration 8/25 | Loss: 0.00050993
Iteration 9/25 | Loss: 0.00050993
Iteration 10/25 | Loss: 0.00050993
Iteration 11/25 | Loss: 0.00050993
Iteration 12/25 | Loss: 0.00050993
Iteration 13/25 | Loss: 0.00050993
Iteration 14/25 | Loss: 0.00050993
Iteration 15/25 | Loss: 0.00050993
Iteration 16/25 | Loss: 0.00050993
Iteration 17/25 | Loss: 0.00050993
Iteration 18/25 | Loss: 0.00050993
Iteration 19/25 | Loss: 0.00050993
Iteration 20/25 | Loss: 0.00050993
Iteration 21/25 | Loss: 0.00050993
Iteration 22/25 | Loss: 0.00050993
Iteration 23/25 | Loss: 0.00050993
Iteration 24/25 | Loss: 0.00050993
Iteration 25/25 | Loss: 0.00050993
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005099251866340637, 0.0005099251866340637, 0.0005099251866340637, 0.0005099251866340637, 0.0005099251866340637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005099251866340637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050993
Iteration 2/1000 | Loss: 0.00005564
Iteration 3/1000 | Loss: 0.00003961
Iteration 4/1000 | Loss: 0.00003411
Iteration 5/1000 | Loss: 0.00003100
Iteration 6/1000 | Loss: 0.00002946
Iteration 7/1000 | Loss: 0.00002821
Iteration 8/1000 | Loss: 0.00002764
Iteration 9/1000 | Loss: 0.00002724
Iteration 10/1000 | Loss: 0.00002696
Iteration 11/1000 | Loss: 0.00002667
Iteration 12/1000 | Loss: 0.00002651
Iteration 13/1000 | Loss: 0.00002642
Iteration 14/1000 | Loss: 0.00002640
Iteration 15/1000 | Loss: 0.00002636
Iteration 16/1000 | Loss: 0.00002636
Iteration 17/1000 | Loss: 0.00002636
Iteration 18/1000 | Loss: 0.00002635
Iteration 19/1000 | Loss: 0.00002634
Iteration 20/1000 | Loss: 0.00002633
Iteration 21/1000 | Loss: 0.00002630
Iteration 22/1000 | Loss: 0.00002629
Iteration 23/1000 | Loss: 0.00002629
Iteration 24/1000 | Loss: 0.00002629
Iteration 25/1000 | Loss: 0.00002628
Iteration 26/1000 | Loss: 0.00002628
Iteration 27/1000 | Loss: 0.00002628
Iteration 28/1000 | Loss: 0.00002627
Iteration 29/1000 | Loss: 0.00002627
Iteration 30/1000 | Loss: 0.00002625
Iteration 31/1000 | Loss: 0.00002625
Iteration 32/1000 | Loss: 0.00002625
Iteration 33/1000 | Loss: 0.00002625
Iteration 34/1000 | Loss: 0.00002625
Iteration 35/1000 | Loss: 0.00002625
Iteration 36/1000 | Loss: 0.00002625
Iteration 37/1000 | Loss: 0.00002625
Iteration 38/1000 | Loss: 0.00002625
Iteration 39/1000 | Loss: 0.00002624
Iteration 40/1000 | Loss: 0.00002624
Iteration 41/1000 | Loss: 0.00002624
Iteration 42/1000 | Loss: 0.00002624
Iteration 43/1000 | Loss: 0.00002623
Iteration 44/1000 | Loss: 0.00002623
Iteration 45/1000 | Loss: 0.00002623
Iteration 46/1000 | Loss: 0.00002623
Iteration 47/1000 | Loss: 0.00002623
Iteration 48/1000 | Loss: 0.00002622
Iteration 49/1000 | Loss: 0.00002622
Iteration 50/1000 | Loss: 0.00002622
Iteration 51/1000 | Loss: 0.00002621
Iteration 52/1000 | Loss: 0.00002621
Iteration 53/1000 | Loss: 0.00002621
Iteration 54/1000 | Loss: 0.00002620
Iteration 55/1000 | Loss: 0.00002620
Iteration 56/1000 | Loss: 0.00002620
Iteration 57/1000 | Loss: 0.00002619
Iteration 58/1000 | Loss: 0.00002619
Iteration 59/1000 | Loss: 0.00002619
Iteration 60/1000 | Loss: 0.00002618
Iteration 61/1000 | Loss: 0.00002618
Iteration 62/1000 | Loss: 0.00002618
Iteration 63/1000 | Loss: 0.00002618
Iteration 64/1000 | Loss: 0.00002618
Iteration 65/1000 | Loss: 0.00002618
Iteration 66/1000 | Loss: 0.00002617
Iteration 67/1000 | Loss: 0.00002617
Iteration 68/1000 | Loss: 0.00002617
Iteration 69/1000 | Loss: 0.00002617
Iteration 70/1000 | Loss: 0.00002617
Iteration 71/1000 | Loss: 0.00002617
Iteration 72/1000 | Loss: 0.00002616
Iteration 73/1000 | Loss: 0.00002616
Iteration 74/1000 | Loss: 0.00002616
Iteration 75/1000 | Loss: 0.00002616
Iteration 76/1000 | Loss: 0.00002615
Iteration 77/1000 | Loss: 0.00002615
Iteration 78/1000 | Loss: 0.00002614
Iteration 79/1000 | Loss: 0.00002614
Iteration 80/1000 | Loss: 0.00002614
Iteration 81/1000 | Loss: 0.00002614
Iteration 82/1000 | Loss: 0.00002613
Iteration 83/1000 | Loss: 0.00002613
Iteration 84/1000 | Loss: 0.00002613
Iteration 85/1000 | Loss: 0.00002613
Iteration 86/1000 | Loss: 0.00002613
Iteration 87/1000 | Loss: 0.00002613
Iteration 88/1000 | Loss: 0.00002613
Iteration 89/1000 | Loss: 0.00002613
Iteration 90/1000 | Loss: 0.00002613
Iteration 91/1000 | Loss: 0.00002612
Iteration 92/1000 | Loss: 0.00002612
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002612
Iteration 95/1000 | Loss: 0.00002612
Iteration 96/1000 | Loss: 0.00002612
Iteration 97/1000 | Loss: 0.00002612
Iteration 98/1000 | Loss: 0.00002612
Iteration 99/1000 | Loss: 0.00002612
Iteration 100/1000 | Loss: 0.00002612
Iteration 101/1000 | Loss: 0.00002611
Iteration 102/1000 | Loss: 0.00002611
Iteration 103/1000 | Loss: 0.00002610
Iteration 104/1000 | Loss: 0.00002610
Iteration 105/1000 | Loss: 0.00002610
Iteration 106/1000 | Loss: 0.00002610
Iteration 107/1000 | Loss: 0.00002610
Iteration 108/1000 | Loss: 0.00002610
Iteration 109/1000 | Loss: 0.00002610
Iteration 110/1000 | Loss: 0.00002610
Iteration 111/1000 | Loss: 0.00002610
Iteration 112/1000 | Loss: 0.00002610
Iteration 113/1000 | Loss: 0.00002610
Iteration 114/1000 | Loss: 0.00002610
Iteration 115/1000 | Loss: 0.00002610
Iteration 116/1000 | Loss: 0.00002610
Iteration 117/1000 | Loss: 0.00002610
Iteration 118/1000 | Loss: 0.00002610
Iteration 119/1000 | Loss: 0.00002610
Iteration 120/1000 | Loss: 0.00002610
Iteration 121/1000 | Loss: 0.00002609
Iteration 122/1000 | Loss: 0.00002609
Iteration 123/1000 | Loss: 0.00002609
Iteration 124/1000 | Loss: 0.00002609
Iteration 125/1000 | Loss: 0.00002609
Iteration 126/1000 | Loss: 0.00002609
Iteration 127/1000 | Loss: 0.00002609
Iteration 128/1000 | Loss: 0.00002609
Iteration 129/1000 | Loss: 0.00002608
Iteration 130/1000 | Loss: 0.00002608
Iteration 131/1000 | Loss: 0.00002608
Iteration 132/1000 | Loss: 0.00002608
Iteration 133/1000 | Loss: 0.00002608
Iteration 134/1000 | Loss: 0.00002608
Iteration 135/1000 | Loss: 0.00002608
Iteration 136/1000 | Loss: 0.00002608
Iteration 137/1000 | Loss: 0.00002608
Iteration 138/1000 | Loss: 0.00002608
Iteration 139/1000 | Loss: 0.00002608
Iteration 140/1000 | Loss: 0.00002608
Iteration 141/1000 | Loss: 0.00002608
Iteration 142/1000 | Loss: 0.00002608
Iteration 143/1000 | Loss: 0.00002608
Iteration 144/1000 | Loss: 0.00002608
Iteration 145/1000 | Loss: 0.00002607
Iteration 146/1000 | Loss: 0.00002607
Iteration 147/1000 | Loss: 0.00002607
Iteration 148/1000 | Loss: 0.00002607
Iteration 149/1000 | Loss: 0.00002607
Iteration 150/1000 | Loss: 0.00002607
Iteration 151/1000 | Loss: 0.00002607
Iteration 152/1000 | Loss: 0.00002607
Iteration 153/1000 | Loss: 0.00002607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.6072479158756323e-05, 2.6072479158756323e-05, 2.6072479158756323e-05, 2.6072479158756323e-05, 2.6072479158756323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6072479158756323e-05

Optimization complete. Final v2v error: 4.333863735198975 mm

Highest mean error: 5.6850080490112305 mm for frame 82

Lowest mean error: 3.724926233291626 mm for frame 195

Saving results

Total time: 43.751891136169434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765625
Iteration 2/25 | Loss: 0.00161820
Iteration 3/25 | Loss: 0.00130361
Iteration 4/25 | Loss: 0.00124524
Iteration 5/25 | Loss: 0.00122189
Iteration 6/25 | Loss: 0.00121956
Iteration 7/25 | Loss: 0.00122549
Iteration 8/25 | Loss: 0.00120689
Iteration 9/25 | Loss: 0.00119348
Iteration 10/25 | Loss: 0.00119096
Iteration 11/25 | Loss: 0.00119790
Iteration 12/25 | Loss: 0.00120391
Iteration 13/25 | Loss: 0.00121184
Iteration 14/25 | Loss: 0.00119178
Iteration 15/25 | Loss: 0.00119045
Iteration 16/25 | Loss: 0.00117989
Iteration 17/25 | Loss: 0.00117828
Iteration 18/25 | Loss: 0.00118204
Iteration 19/25 | Loss: 0.00117852
Iteration 20/25 | Loss: 0.00117494
Iteration 21/25 | Loss: 0.00117392
Iteration 22/25 | Loss: 0.00116873
Iteration 23/25 | Loss: 0.00115956
Iteration 24/25 | Loss: 0.00115524
Iteration 25/25 | Loss: 0.00115420

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.15068579
Iteration 2/25 | Loss: 0.00067167
Iteration 3/25 | Loss: 0.00067143
Iteration 4/25 | Loss: 0.00067143
Iteration 5/25 | Loss: 0.00067143
Iteration 6/25 | Loss: 0.00067143
Iteration 7/25 | Loss: 0.00067143
Iteration 8/25 | Loss: 0.00067143
Iteration 9/25 | Loss: 0.00067143
Iteration 10/25 | Loss: 0.00067143
Iteration 11/25 | Loss: 0.00067143
Iteration 12/25 | Loss: 0.00067143
Iteration 13/25 | Loss: 0.00067143
Iteration 14/25 | Loss: 0.00067143
Iteration 15/25 | Loss: 0.00067143
Iteration 16/25 | Loss: 0.00067143
Iteration 17/25 | Loss: 0.00067143
Iteration 18/25 | Loss: 0.00067143
Iteration 19/25 | Loss: 0.00067143
Iteration 20/25 | Loss: 0.00067143
Iteration 21/25 | Loss: 0.00067143
Iteration 22/25 | Loss: 0.00067143
Iteration 23/25 | Loss: 0.00067143
Iteration 24/25 | Loss: 0.00067143
Iteration 25/25 | Loss: 0.00067143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067143
Iteration 2/1000 | Loss: 0.00785981
Iteration 3/1000 | Loss: 0.01132183
Iteration 4/1000 | Loss: 0.00848672
Iteration 5/1000 | Loss: 0.00157644
Iteration 6/1000 | Loss: 0.00019394
Iteration 7/1000 | Loss: 0.00011871
Iteration 8/1000 | Loss: 0.00007487
Iteration 9/1000 | Loss: 0.00006005
Iteration 10/1000 | Loss: 0.00005099
Iteration 11/1000 | Loss: 0.00004737
Iteration 12/1000 | Loss: 0.00022399
Iteration 13/1000 | Loss: 0.00004677
Iteration 14/1000 | Loss: 0.00031687
Iteration 15/1000 | Loss: 0.00021196
Iteration 16/1000 | Loss: 0.00034665
Iteration 17/1000 | Loss: 0.00005252
Iteration 18/1000 | Loss: 0.00005869
Iteration 19/1000 | Loss: 0.00010026
Iteration 20/1000 | Loss: 0.00004341
Iteration 21/1000 | Loss: 0.00003917
Iteration 22/1000 | Loss: 0.00003683
Iteration 23/1000 | Loss: 0.00003574
Iteration 24/1000 | Loss: 0.00003480
Iteration 25/1000 | Loss: 0.00003428
Iteration 26/1000 | Loss: 0.00003382
Iteration 27/1000 | Loss: 0.00003355
Iteration 28/1000 | Loss: 0.00003349
Iteration 29/1000 | Loss: 0.00003341
Iteration 30/1000 | Loss: 0.00003325
Iteration 31/1000 | Loss: 0.00003311
Iteration 32/1000 | Loss: 0.00003308
Iteration 33/1000 | Loss: 0.00003302
Iteration 34/1000 | Loss: 0.00003300
Iteration 35/1000 | Loss: 0.00003299
Iteration 36/1000 | Loss: 0.00003299
Iteration 37/1000 | Loss: 0.00003299
Iteration 38/1000 | Loss: 0.00003298
Iteration 39/1000 | Loss: 0.00003298
Iteration 40/1000 | Loss: 0.00003298
Iteration 41/1000 | Loss: 0.00003298
Iteration 42/1000 | Loss: 0.00003297
Iteration 43/1000 | Loss: 0.00003297
Iteration 44/1000 | Loss: 0.00003297
Iteration 45/1000 | Loss: 0.00003297
Iteration 46/1000 | Loss: 0.00003297
Iteration 47/1000 | Loss: 0.00003296
Iteration 48/1000 | Loss: 0.00003295
Iteration 49/1000 | Loss: 0.00003295
Iteration 50/1000 | Loss: 0.00003295
Iteration 51/1000 | Loss: 0.00003295
Iteration 52/1000 | Loss: 0.00003295
Iteration 53/1000 | Loss: 0.00003295
Iteration 54/1000 | Loss: 0.00003294
Iteration 55/1000 | Loss: 0.00003294
Iteration 56/1000 | Loss: 0.00003294
Iteration 57/1000 | Loss: 0.00003294
Iteration 58/1000 | Loss: 0.00003294
Iteration 59/1000 | Loss: 0.00003293
Iteration 60/1000 | Loss: 0.00003293
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003293
Iteration 63/1000 | Loss: 0.00003292
Iteration 64/1000 | Loss: 0.00003292
Iteration 65/1000 | Loss: 0.00003292
Iteration 66/1000 | Loss: 0.00003292
Iteration 67/1000 | Loss: 0.00003292
Iteration 68/1000 | Loss: 0.00003292
Iteration 69/1000 | Loss: 0.00003292
Iteration 70/1000 | Loss: 0.00003289
Iteration 71/1000 | Loss: 0.00003288
Iteration 72/1000 | Loss: 0.00003288
Iteration 73/1000 | Loss: 0.00003287
Iteration 74/1000 | Loss: 0.00003287
Iteration 75/1000 | Loss: 0.00003287
Iteration 76/1000 | Loss: 0.00003287
Iteration 77/1000 | Loss: 0.00003287
Iteration 78/1000 | Loss: 0.00003287
Iteration 79/1000 | Loss: 0.00003286
Iteration 80/1000 | Loss: 0.00003286
Iteration 81/1000 | Loss: 0.00003285
Iteration 82/1000 | Loss: 0.00003285
Iteration 83/1000 | Loss: 0.00003285
Iteration 84/1000 | Loss: 0.00003285
Iteration 85/1000 | Loss: 0.00003285
Iteration 86/1000 | Loss: 0.00003285
Iteration 87/1000 | Loss: 0.00003285
Iteration 88/1000 | Loss: 0.00003285
Iteration 89/1000 | Loss: 0.00003285
Iteration 90/1000 | Loss: 0.00003285
Iteration 91/1000 | Loss: 0.00003284
Iteration 92/1000 | Loss: 0.00003284
Iteration 93/1000 | Loss: 0.00003284
Iteration 94/1000 | Loss: 0.00003284
Iteration 95/1000 | Loss: 0.00003284
Iteration 96/1000 | Loss: 0.00003283
Iteration 97/1000 | Loss: 0.00003283
Iteration 98/1000 | Loss: 0.00003283
Iteration 99/1000 | Loss: 0.00003283
Iteration 100/1000 | Loss: 0.00003283
Iteration 101/1000 | Loss: 0.00003282
Iteration 102/1000 | Loss: 0.00003282
Iteration 103/1000 | Loss: 0.00003282
Iteration 104/1000 | Loss: 0.00003282
Iteration 105/1000 | Loss: 0.00003282
Iteration 106/1000 | Loss: 0.00003282
Iteration 107/1000 | Loss: 0.00003282
Iteration 108/1000 | Loss: 0.00003281
Iteration 109/1000 | Loss: 0.00003281
Iteration 110/1000 | Loss: 0.00003281
Iteration 111/1000 | Loss: 0.00003281
Iteration 112/1000 | Loss: 0.00003281
Iteration 113/1000 | Loss: 0.00003281
Iteration 114/1000 | Loss: 0.00003281
Iteration 115/1000 | Loss: 0.00003281
Iteration 116/1000 | Loss: 0.00003281
Iteration 117/1000 | Loss: 0.00003281
Iteration 118/1000 | Loss: 0.00003281
Iteration 119/1000 | Loss: 0.00003280
Iteration 120/1000 | Loss: 0.00003280
Iteration 121/1000 | Loss: 0.00003280
Iteration 122/1000 | Loss: 0.00003280
Iteration 123/1000 | Loss: 0.00003280
Iteration 124/1000 | Loss: 0.00003280
Iteration 125/1000 | Loss: 0.00003280
Iteration 126/1000 | Loss: 0.00003280
Iteration 127/1000 | Loss: 0.00003280
Iteration 128/1000 | Loss: 0.00003280
Iteration 129/1000 | Loss: 0.00003280
Iteration 130/1000 | Loss: 0.00003279
Iteration 131/1000 | Loss: 0.00003279
Iteration 132/1000 | Loss: 0.00003279
Iteration 133/1000 | Loss: 0.00003279
Iteration 134/1000 | Loss: 0.00003279
Iteration 135/1000 | Loss: 0.00003279
Iteration 136/1000 | Loss: 0.00003279
Iteration 137/1000 | Loss: 0.00003279
Iteration 138/1000 | Loss: 0.00003279
Iteration 139/1000 | Loss: 0.00003279
Iteration 140/1000 | Loss: 0.00003279
Iteration 141/1000 | Loss: 0.00003279
Iteration 142/1000 | Loss: 0.00003279
Iteration 143/1000 | Loss: 0.00003279
Iteration 144/1000 | Loss: 0.00003279
Iteration 145/1000 | Loss: 0.00003279
Iteration 146/1000 | Loss: 0.00003279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.279044904047623e-05, 3.279044904047623e-05, 3.279044904047623e-05, 3.279044904047623e-05, 3.279044904047623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.279044904047623e-05

Optimization complete. Final v2v error: 4.7087836265563965 mm

Highest mean error: 6.681489944458008 mm for frame 72

Lowest mean error: 3.721489191055298 mm for frame 95

Saving results

Total time: 95.60897564888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813414
Iteration 2/25 | Loss: 0.00212574
Iteration 3/25 | Loss: 0.00160606
Iteration 4/25 | Loss: 0.00143176
Iteration 5/25 | Loss: 0.00130640
Iteration 6/25 | Loss: 0.00118495
Iteration 7/25 | Loss: 0.00116635
Iteration 8/25 | Loss: 0.00116415
Iteration 9/25 | Loss: 0.00116502
Iteration 10/25 | Loss: 0.00116155
Iteration 11/25 | Loss: 0.00116020
Iteration 12/25 | Loss: 0.00115667
Iteration 13/25 | Loss: 0.00115370
Iteration 14/25 | Loss: 0.00115157
Iteration 15/25 | Loss: 0.00114870
Iteration 16/25 | Loss: 0.00114732
Iteration 17/25 | Loss: 0.00114637
Iteration 18/25 | Loss: 0.00114593
Iteration 19/25 | Loss: 0.00114591
Iteration 20/25 | Loss: 0.00114590
Iteration 21/25 | Loss: 0.00114590
Iteration 22/25 | Loss: 0.00114590
Iteration 23/25 | Loss: 0.00114590
Iteration 24/25 | Loss: 0.00114590
Iteration 25/25 | Loss: 0.00114590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43138731
Iteration 2/25 | Loss: 0.00059829
Iteration 3/25 | Loss: 0.00059829
Iteration 4/25 | Loss: 0.00059829
Iteration 5/25 | Loss: 0.00059829
Iteration 6/25 | Loss: 0.00059829
Iteration 7/25 | Loss: 0.00059829
Iteration 8/25 | Loss: 0.00059829
Iteration 9/25 | Loss: 0.00059829
Iteration 10/25 | Loss: 0.00059829
Iteration 11/25 | Loss: 0.00059829
Iteration 12/25 | Loss: 0.00059829
Iteration 13/25 | Loss: 0.00059829
Iteration 14/25 | Loss: 0.00059829
Iteration 15/25 | Loss: 0.00059829
Iteration 16/25 | Loss: 0.00059829
Iteration 17/25 | Loss: 0.00059829
Iteration 18/25 | Loss: 0.00059829
Iteration 19/25 | Loss: 0.00059829
Iteration 20/25 | Loss: 0.00059829
Iteration 21/25 | Loss: 0.00059829
Iteration 22/25 | Loss: 0.00059829
Iteration 23/25 | Loss: 0.00059829
Iteration 24/25 | Loss: 0.00059829
Iteration 25/25 | Loss: 0.00059829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059829
Iteration 2/1000 | Loss: 0.00009548
Iteration 3/1000 | Loss: 0.00005624
Iteration 4/1000 | Loss: 0.00003913
Iteration 5/1000 | Loss: 0.00003417
Iteration 6/1000 | Loss: 0.00003229
Iteration 7/1000 | Loss: 0.00003055
Iteration 8/1000 | Loss: 0.00002985
Iteration 9/1000 | Loss: 0.00002945
Iteration 10/1000 | Loss: 0.00002913
Iteration 11/1000 | Loss: 0.00002885
Iteration 12/1000 | Loss: 0.00002867
Iteration 13/1000 | Loss: 0.00002862
Iteration 14/1000 | Loss: 0.00002851
Iteration 15/1000 | Loss: 0.00002848
Iteration 16/1000 | Loss: 0.00002848
Iteration 17/1000 | Loss: 0.00002848
Iteration 18/1000 | Loss: 0.00002847
Iteration 19/1000 | Loss: 0.00002847
Iteration 20/1000 | Loss: 0.00002847
Iteration 21/1000 | Loss: 0.00002846
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002845
Iteration 24/1000 | Loss: 0.00002844
Iteration 25/1000 | Loss: 0.00002844
Iteration 26/1000 | Loss: 0.00002840
Iteration 27/1000 | Loss: 0.00002831
Iteration 28/1000 | Loss: 0.00002826
Iteration 29/1000 | Loss: 0.00002826
Iteration 30/1000 | Loss: 0.00002826
Iteration 31/1000 | Loss: 0.00002826
Iteration 32/1000 | Loss: 0.00002825
Iteration 33/1000 | Loss: 0.00002825
Iteration 34/1000 | Loss: 0.00002825
Iteration 35/1000 | Loss: 0.00002824
Iteration 36/1000 | Loss: 0.00002824
Iteration 37/1000 | Loss: 0.00002823
Iteration 38/1000 | Loss: 0.00002823
Iteration 39/1000 | Loss: 0.00002823
Iteration 40/1000 | Loss: 0.00002822
Iteration 41/1000 | Loss: 0.00002822
Iteration 42/1000 | Loss: 0.00002822
Iteration 43/1000 | Loss: 0.00002821
Iteration 44/1000 | Loss: 0.00002821
Iteration 45/1000 | Loss: 0.00002821
Iteration 46/1000 | Loss: 0.00002821
Iteration 47/1000 | Loss: 0.00002821
Iteration 48/1000 | Loss: 0.00002821
Iteration 49/1000 | Loss: 0.00002821
Iteration 50/1000 | Loss: 0.00002820
Iteration 51/1000 | Loss: 0.00002820
Iteration 52/1000 | Loss: 0.00002820
Iteration 53/1000 | Loss: 0.00002819
Iteration 54/1000 | Loss: 0.00002819
Iteration 55/1000 | Loss: 0.00002819
Iteration 56/1000 | Loss: 0.00002819
Iteration 57/1000 | Loss: 0.00002819
Iteration 58/1000 | Loss: 0.00002819
Iteration 59/1000 | Loss: 0.00002818
Iteration 60/1000 | Loss: 0.00002818
Iteration 61/1000 | Loss: 0.00002818
Iteration 62/1000 | Loss: 0.00002818
Iteration 63/1000 | Loss: 0.00002818
Iteration 64/1000 | Loss: 0.00002818
Iteration 65/1000 | Loss: 0.00002818
Iteration 66/1000 | Loss: 0.00002818
Iteration 67/1000 | Loss: 0.00002818
Iteration 68/1000 | Loss: 0.00002817
Iteration 69/1000 | Loss: 0.00002817
Iteration 70/1000 | Loss: 0.00002817
Iteration 71/1000 | Loss: 0.00002817
Iteration 72/1000 | Loss: 0.00002817
Iteration 73/1000 | Loss: 0.00002816
Iteration 74/1000 | Loss: 0.00002816
Iteration 75/1000 | Loss: 0.00002816
Iteration 76/1000 | Loss: 0.00002816
Iteration 77/1000 | Loss: 0.00002816
Iteration 78/1000 | Loss: 0.00002816
Iteration 79/1000 | Loss: 0.00002816
Iteration 80/1000 | Loss: 0.00002816
Iteration 81/1000 | Loss: 0.00002816
Iteration 82/1000 | Loss: 0.00002816
Iteration 83/1000 | Loss: 0.00002815
Iteration 84/1000 | Loss: 0.00002815
Iteration 85/1000 | Loss: 0.00002815
Iteration 86/1000 | Loss: 0.00002815
Iteration 87/1000 | Loss: 0.00002815
Iteration 88/1000 | Loss: 0.00002815
Iteration 89/1000 | Loss: 0.00002815
Iteration 90/1000 | Loss: 0.00002815
Iteration 91/1000 | Loss: 0.00002815
Iteration 92/1000 | Loss: 0.00002815
Iteration 93/1000 | Loss: 0.00002815
Iteration 94/1000 | Loss: 0.00002815
Iteration 95/1000 | Loss: 0.00002815
Iteration 96/1000 | Loss: 0.00002815
Iteration 97/1000 | Loss: 0.00002815
Iteration 98/1000 | Loss: 0.00002815
Iteration 99/1000 | Loss: 0.00002815
Iteration 100/1000 | Loss: 0.00002815
Iteration 101/1000 | Loss: 0.00002815
Iteration 102/1000 | Loss: 0.00002815
Iteration 103/1000 | Loss: 0.00002815
Iteration 104/1000 | Loss: 0.00002815
Iteration 105/1000 | Loss: 0.00002815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.814588151522912e-05, 2.814588151522912e-05, 2.814588151522912e-05, 2.814588151522912e-05, 2.814588151522912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.814588151522912e-05

Optimization complete. Final v2v error: 4.619877815246582 mm

Highest mean error: 5.034265041351318 mm for frame 144

Lowest mean error: 4.346508502960205 mm for frame 171

Saving results

Total time: 64.94135451316833
