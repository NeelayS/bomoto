Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=248, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13888-13943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426933
Iteration 2/25 | Loss: 0.00123555
Iteration 3/25 | Loss: 0.00118292
Iteration 4/25 | Loss: 0.00117507
Iteration 5/25 | Loss: 0.00117279
Iteration 6/25 | Loss: 0.00117278
Iteration 7/25 | Loss: 0.00117278
Iteration 8/25 | Loss: 0.00117278
Iteration 9/25 | Loss: 0.00117278
Iteration 10/25 | Loss: 0.00117278
Iteration 11/25 | Loss: 0.00117278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011727770324796438, 0.0011727770324796438, 0.0011727770324796438, 0.0011727770324796438, 0.0011727770324796438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011727770324796438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48090029
Iteration 2/25 | Loss: 0.00061604
Iteration 3/25 | Loss: 0.00061603
Iteration 4/25 | Loss: 0.00061603
Iteration 5/25 | Loss: 0.00061603
Iteration 6/25 | Loss: 0.00061603
Iteration 7/25 | Loss: 0.00061603
Iteration 8/25 | Loss: 0.00061603
Iteration 9/25 | Loss: 0.00061603
Iteration 10/25 | Loss: 0.00061603
Iteration 11/25 | Loss: 0.00061603
Iteration 12/25 | Loss: 0.00061603
Iteration 13/25 | Loss: 0.00061603
Iteration 14/25 | Loss: 0.00061603
Iteration 15/25 | Loss: 0.00061603
Iteration 16/25 | Loss: 0.00061603
Iteration 17/25 | Loss: 0.00061603
Iteration 18/25 | Loss: 0.00061603
Iteration 19/25 | Loss: 0.00061603
Iteration 20/25 | Loss: 0.00061603
Iteration 21/25 | Loss: 0.00061603
Iteration 22/25 | Loss: 0.00061603
Iteration 23/25 | Loss: 0.00061603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006160305929370224, 0.0006160305929370224, 0.0006160305929370224, 0.0006160305929370224, 0.0006160305929370224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006160305929370224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061603
Iteration 2/1000 | Loss: 0.00002547
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001806
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001644
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001567
Iteration 10/1000 | Loss: 0.00001552
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001511
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001495
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001473
Iteration 32/1000 | Loss: 0.00001473
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001457
Iteration 81/1000 | Loss: 0.00001457
Iteration 82/1000 | Loss: 0.00001457
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001456
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001456
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001456
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001455
Iteration 93/1000 | Loss: 0.00001455
Iteration 94/1000 | Loss: 0.00001455
Iteration 95/1000 | Loss: 0.00001455
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001455
Iteration 100/1000 | Loss: 0.00001455
Iteration 101/1000 | Loss: 0.00001455
Iteration 102/1000 | Loss: 0.00001455
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001455
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001454
Iteration 113/1000 | Loss: 0.00001454
Iteration 114/1000 | Loss: 0.00001454
Iteration 115/1000 | Loss: 0.00001454
Iteration 116/1000 | Loss: 0.00001454
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.4538010873366147e-05, 1.4538010873366147e-05, 1.4538010873366147e-05, 1.4538010873366147e-05, 1.4538010873366147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4538010873366147e-05

Optimization complete. Final v2v error: 3.2422492504119873 mm

Highest mean error: 3.420663356781006 mm for frame 4

Lowest mean error: 3.0327844619750977 mm for frame 59

Saving results

Total time: 38.04469108581543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964443
Iteration 2/25 | Loss: 0.00263219
Iteration 3/25 | Loss: 0.00217333
Iteration 4/25 | Loss: 0.00208297
Iteration 5/25 | Loss: 0.00204118
Iteration 6/25 | Loss: 0.00203553
Iteration 7/25 | Loss: 0.00205158
Iteration 8/25 | Loss: 0.00199970
Iteration 9/25 | Loss: 0.00188548
Iteration 10/25 | Loss: 0.00180638
Iteration 11/25 | Loss: 0.00178212
Iteration 12/25 | Loss: 0.00173897
Iteration 13/25 | Loss: 0.00171243
Iteration 14/25 | Loss: 0.00170477
Iteration 15/25 | Loss: 0.00169062
Iteration 16/25 | Loss: 0.00167227
Iteration 17/25 | Loss: 0.00167045
Iteration 18/25 | Loss: 0.00167130
Iteration 19/25 | Loss: 0.00165383
Iteration 20/25 | Loss: 0.00165578
Iteration 21/25 | Loss: 0.00166205
Iteration 22/25 | Loss: 0.00165148
Iteration 23/25 | Loss: 0.00164137
Iteration 24/25 | Loss: 0.00163598
Iteration 25/25 | Loss: 0.00163524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44887400
Iteration 2/25 | Loss: 0.00341906
Iteration 3/25 | Loss: 0.00341905
Iteration 4/25 | Loss: 0.00341905
Iteration 5/25 | Loss: 0.00341905
Iteration 6/25 | Loss: 0.00341905
Iteration 7/25 | Loss: 0.00341905
Iteration 8/25 | Loss: 0.00341905
Iteration 9/25 | Loss: 0.00341905
Iteration 10/25 | Loss: 0.00341905
Iteration 11/25 | Loss: 0.00341905
Iteration 12/25 | Loss: 0.00341905
Iteration 13/25 | Loss: 0.00341905
Iteration 14/25 | Loss: 0.00341905
Iteration 15/25 | Loss: 0.00341905
Iteration 16/25 | Loss: 0.00341905
Iteration 17/25 | Loss: 0.00341905
Iteration 18/25 | Loss: 0.00341905
Iteration 19/25 | Loss: 0.00341905
Iteration 20/25 | Loss: 0.00341905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003419045824557543, 0.003419045824557543, 0.003419045824557543, 0.003419045824557543, 0.003419045824557543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003419045824557543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00341905
Iteration 2/1000 | Loss: 0.00037152
Iteration 3/1000 | Loss: 0.00029220
Iteration 4/1000 | Loss: 0.00026111
Iteration 5/1000 | Loss: 0.00024185
Iteration 6/1000 | Loss: 0.00022520
Iteration 7/1000 | Loss: 0.00021260
Iteration 8/1000 | Loss: 0.00020565
Iteration 9/1000 | Loss: 0.00020111
Iteration 10/1000 | Loss: 0.00019697
Iteration 11/1000 | Loss: 0.00019338
Iteration 12/1000 | Loss: 0.00019048
Iteration 13/1000 | Loss: 0.00063612
Iteration 14/1000 | Loss: 0.00399191
Iteration 15/1000 | Loss: 0.02092349
Iteration 16/1000 | Loss: 0.00238242
Iteration 17/1000 | Loss: 0.00083427
Iteration 18/1000 | Loss: 0.00034461
Iteration 19/1000 | Loss: 0.00022312
Iteration 20/1000 | Loss: 0.00017989
Iteration 21/1000 | Loss: 0.00013166
Iteration 22/1000 | Loss: 0.00009383
Iteration 23/1000 | Loss: 0.00006850
Iteration 24/1000 | Loss: 0.00005833
Iteration 25/1000 | Loss: 0.00004831
Iteration 26/1000 | Loss: 0.00004187
Iteration 27/1000 | Loss: 0.00003614
Iteration 28/1000 | Loss: 0.00003249
Iteration 29/1000 | Loss: 0.00002958
Iteration 30/1000 | Loss: 0.00002673
Iteration 31/1000 | Loss: 0.00002448
Iteration 32/1000 | Loss: 0.00002279
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002001
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001935
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001911
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00001910
Iteration 52/1000 | Loss: 0.00001910
Iteration 53/1000 | Loss: 0.00001910
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001909
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001905
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001904
Iteration 80/1000 | Loss: 0.00001904
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001904
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001903
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001902
Iteration 91/1000 | Loss: 0.00001902
Iteration 92/1000 | Loss: 0.00001902
Iteration 93/1000 | Loss: 0.00001902
Iteration 94/1000 | Loss: 0.00001902
Iteration 95/1000 | Loss: 0.00001902
Iteration 96/1000 | Loss: 0.00001902
Iteration 97/1000 | Loss: 0.00001902
Iteration 98/1000 | Loss: 0.00001902
Iteration 99/1000 | Loss: 0.00001902
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001902
Iteration 102/1000 | Loss: 0.00001902
Iteration 103/1000 | Loss: 0.00001901
Iteration 104/1000 | Loss: 0.00001901
Iteration 105/1000 | Loss: 0.00001901
Iteration 106/1000 | Loss: 0.00001901
Iteration 107/1000 | Loss: 0.00001901
Iteration 108/1000 | Loss: 0.00001901
Iteration 109/1000 | Loss: 0.00001901
Iteration 110/1000 | Loss: 0.00001901
Iteration 111/1000 | Loss: 0.00001901
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001901
Iteration 115/1000 | Loss: 0.00001901
Iteration 116/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.901055838970933e-05, 1.901055838970933e-05, 1.901055838970933e-05, 1.901055838970933e-05, 1.901055838970933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.901055838970933e-05

Optimization complete. Final v2v error: 3.6796228885650635 mm

Highest mean error: 3.766446590423584 mm for frame 19

Lowest mean error: 3.60701322555542 mm for frame 142

Saving results

Total time: 110.40143299102783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804364
Iteration 2/25 | Loss: 0.00142009
Iteration 3/25 | Loss: 0.00120757
Iteration 4/25 | Loss: 0.00119197
Iteration 5/25 | Loss: 0.00118581
Iteration 6/25 | Loss: 0.00118439
Iteration 7/25 | Loss: 0.00118439
Iteration 8/25 | Loss: 0.00118439
Iteration 9/25 | Loss: 0.00118439
Iteration 10/25 | Loss: 0.00118439
Iteration 11/25 | Loss: 0.00118439
Iteration 12/25 | Loss: 0.00118439
Iteration 13/25 | Loss: 0.00118439
Iteration 14/25 | Loss: 0.00118439
Iteration 15/25 | Loss: 0.00118439
Iteration 16/25 | Loss: 0.00118439
Iteration 17/25 | Loss: 0.00118439
Iteration 18/25 | Loss: 0.00118439
Iteration 19/25 | Loss: 0.00118439
Iteration 20/25 | Loss: 0.00118439
Iteration 21/25 | Loss: 0.00118439
Iteration 22/25 | Loss: 0.00118439
Iteration 23/25 | Loss: 0.00118439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011843930697068572, 0.0011843930697068572, 0.0011843930697068572, 0.0011843930697068572, 0.0011843930697068572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011843930697068572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25919938
Iteration 2/25 | Loss: 0.00070676
Iteration 3/25 | Loss: 0.00070675
Iteration 4/25 | Loss: 0.00070675
Iteration 5/25 | Loss: 0.00070675
Iteration 6/25 | Loss: 0.00070675
Iteration 7/25 | Loss: 0.00070675
Iteration 8/25 | Loss: 0.00070675
Iteration 9/25 | Loss: 0.00070675
Iteration 10/25 | Loss: 0.00070675
Iteration 11/25 | Loss: 0.00070675
Iteration 12/25 | Loss: 0.00070675
Iteration 13/25 | Loss: 0.00070675
Iteration 14/25 | Loss: 0.00070675
Iteration 15/25 | Loss: 0.00070675
Iteration 16/25 | Loss: 0.00070675
Iteration 17/25 | Loss: 0.00070675
Iteration 18/25 | Loss: 0.00070675
Iteration 19/25 | Loss: 0.00070675
Iteration 20/25 | Loss: 0.00070675
Iteration 21/25 | Loss: 0.00070675
Iteration 22/25 | Loss: 0.00070675
Iteration 23/25 | Loss: 0.00070675
Iteration 24/25 | Loss: 0.00070675
Iteration 25/25 | Loss: 0.00070675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070675
Iteration 2/1000 | Loss: 0.00005436
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001904
Iteration 7/1000 | Loss: 0.00001814
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001696
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001631
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00001583
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001575
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001565
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001542
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001529
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001528
Iteration 45/1000 | Loss: 0.00001528
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001519
Iteration 53/1000 | Loss: 0.00001519
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001519
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001518
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001518
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.5177375644270796e-05, 1.5177375644270796e-05, 1.5177375644270796e-05, 1.5177375644270796e-05, 1.5177375644270796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5177375644270796e-05

Optimization complete. Final v2v error: 3.1950736045837402 mm

Highest mean error: 4.493704795837402 mm for frame 62

Lowest mean error: 2.7302160263061523 mm for frame 91

Saving results

Total time: 39.7005352973938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015242
Iteration 2/25 | Loss: 0.00187918
Iteration 3/25 | Loss: 0.00141838
Iteration 4/25 | Loss: 0.00136507
Iteration 5/25 | Loss: 0.00130122
Iteration 6/25 | Loss: 0.00128556
Iteration 7/25 | Loss: 0.00128313
Iteration 8/25 | Loss: 0.00128257
Iteration 9/25 | Loss: 0.00128248
Iteration 10/25 | Loss: 0.00128248
Iteration 11/25 | Loss: 0.00128248
Iteration 12/25 | Loss: 0.00128248
Iteration 13/25 | Loss: 0.00128248
Iteration 14/25 | Loss: 0.00128248
Iteration 15/25 | Loss: 0.00128248
Iteration 16/25 | Loss: 0.00128248
Iteration 17/25 | Loss: 0.00128248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012824767036363482, 0.0012824767036363482, 0.0012824767036363482, 0.0012824767036363482, 0.0012824767036363482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012824767036363482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44848454
Iteration 2/25 | Loss: 0.00067417
Iteration 3/25 | Loss: 0.00067417
Iteration 4/25 | Loss: 0.00067417
Iteration 5/25 | Loss: 0.00067417
Iteration 6/25 | Loss: 0.00067417
Iteration 7/25 | Loss: 0.00067417
Iteration 8/25 | Loss: 0.00067417
Iteration 9/25 | Loss: 0.00067417
Iteration 10/25 | Loss: 0.00067417
Iteration 11/25 | Loss: 0.00067417
Iteration 12/25 | Loss: 0.00067417
Iteration 13/25 | Loss: 0.00067417
Iteration 14/25 | Loss: 0.00067416
Iteration 15/25 | Loss: 0.00067416
Iteration 16/25 | Loss: 0.00067416
Iteration 17/25 | Loss: 0.00067416
Iteration 18/25 | Loss: 0.00067416
Iteration 19/25 | Loss: 0.00067416
Iteration 20/25 | Loss: 0.00067416
Iteration 21/25 | Loss: 0.00067416
Iteration 22/25 | Loss: 0.00067416
Iteration 23/25 | Loss: 0.00067416
Iteration 24/25 | Loss: 0.00067416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006741649121977389, 0.0006741649121977389, 0.0006741649121977389, 0.0006741649121977389, 0.0006741649121977389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006741649121977389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067416
Iteration 2/1000 | Loss: 0.00003790
Iteration 3/1000 | Loss: 0.00002777
Iteration 4/1000 | Loss: 0.00002641
Iteration 5/1000 | Loss: 0.00002562
Iteration 6/1000 | Loss: 0.00002524
Iteration 7/1000 | Loss: 0.00002486
Iteration 8/1000 | Loss: 0.00002482
Iteration 9/1000 | Loss: 0.00002472
Iteration 10/1000 | Loss: 0.00002472
Iteration 11/1000 | Loss: 0.00002471
Iteration 12/1000 | Loss: 0.00002454
Iteration 13/1000 | Loss: 0.00002440
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00002430
Iteration 16/1000 | Loss: 0.00002428
Iteration 17/1000 | Loss: 0.00002424
Iteration 18/1000 | Loss: 0.00002419
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00002417
Iteration 21/1000 | Loss: 0.00002415
Iteration 22/1000 | Loss: 0.00002414
Iteration 23/1000 | Loss: 0.00002414
Iteration 24/1000 | Loss: 0.00002414
Iteration 25/1000 | Loss: 0.00002414
Iteration 26/1000 | Loss: 0.00002414
Iteration 27/1000 | Loss: 0.00002413
Iteration 28/1000 | Loss: 0.00002412
Iteration 29/1000 | Loss: 0.00002412
Iteration 30/1000 | Loss: 0.00002411
Iteration 31/1000 | Loss: 0.00002411
Iteration 32/1000 | Loss: 0.00002411
Iteration 33/1000 | Loss: 0.00002411
Iteration 34/1000 | Loss: 0.00002410
Iteration 35/1000 | Loss: 0.00002410
Iteration 36/1000 | Loss: 0.00002410
Iteration 37/1000 | Loss: 0.00002410
Iteration 38/1000 | Loss: 0.00002409
Iteration 39/1000 | Loss: 0.00002409
Iteration 40/1000 | Loss: 0.00002409
Iteration 41/1000 | Loss: 0.00002409
Iteration 42/1000 | Loss: 0.00002409
Iteration 43/1000 | Loss: 0.00002408
Iteration 44/1000 | Loss: 0.00002408
Iteration 45/1000 | Loss: 0.00002408
Iteration 46/1000 | Loss: 0.00002408
Iteration 47/1000 | Loss: 0.00002408
Iteration 48/1000 | Loss: 0.00002407
Iteration 49/1000 | Loss: 0.00002407
Iteration 50/1000 | Loss: 0.00002407
Iteration 51/1000 | Loss: 0.00002406
Iteration 52/1000 | Loss: 0.00002406
Iteration 53/1000 | Loss: 0.00002406
Iteration 54/1000 | Loss: 0.00002406
Iteration 55/1000 | Loss: 0.00002406
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00002406
Iteration 58/1000 | Loss: 0.00002406
Iteration 59/1000 | Loss: 0.00002406
Iteration 60/1000 | Loss: 0.00002406
Iteration 61/1000 | Loss: 0.00002406
Iteration 62/1000 | Loss: 0.00002406
Iteration 63/1000 | Loss: 0.00002406
Iteration 64/1000 | Loss: 0.00002406
Iteration 65/1000 | Loss: 0.00002406
Iteration 66/1000 | Loss: 0.00002406
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002406
Iteration 69/1000 | Loss: 0.00002406
Iteration 70/1000 | Loss: 0.00002406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.4059918359853327e-05, 2.4059918359853327e-05, 2.4059918359853327e-05, 2.4059918359853327e-05, 2.4059918359853327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4059918359853327e-05

Optimization complete. Final v2v error: 4.095813751220703 mm

Highest mean error: 4.283605575561523 mm for frame 24

Lowest mean error: 3.9538443088531494 mm for frame 105

Saving results

Total time: 36.20391631126404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812650
Iteration 2/25 | Loss: 0.00133157
Iteration 3/25 | Loss: 0.00123946
Iteration 4/25 | Loss: 0.00121000
Iteration 5/25 | Loss: 0.00120111
Iteration 6/25 | Loss: 0.00119872
Iteration 7/25 | Loss: 0.00119775
Iteration 8/25 | Loss: 0.00119775
Iteration 9/25 | Loss: 0.00119775
Iteration 10/25 | Loss: 0.00119775
Iteration 11/25 | Loss: 0.00119775
Iteration 12/25 | Loss: 0.00119775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00119775312487036, 0.00119775312487036, 0.00119775312487036, 0.00119775312487036, 0.00119775312487036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00119775312487036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70370531
Iteration 2/25 | Loss: 0.00053712
Iteration 3/25 | Loss: 0.00053712
Iteration 4/25 | Loss: 0.00053712
Iteration 5/25 | Loss: 0.00053712
Iteration 6/25 | Loss: 0.00053712
Iteration 7/25 | Loss: 0.00053712
Iteration 8/25 | Loss: 0.00053712
Iteration 9/25 | Loss: 0.00053712
Iteration 10/25 | Loss: 0.00053712
Iteration 11/25 | Loss: 0.00053712
Iteration 12/25 | Loss: 0.00053712
Iteration 13/25 | Loss: 0.00053712
Iteration 14/25 | Loss: 0.00053712
Iteration 15/25 | Loss: 0.00053712
Iteration 16/25 | Loss: 0.00053712
Iteration 17/25 | Loss: 0.00053712
Iteration 18/25 | Loss: 0.00053712
Iteration 19/25 | Loss: 0.00053712
Iteration 20/25 | Loss: 0.00053712
Iteration 21/25 | Loss: 0.00053712
Iteration 22/25 | Loss: 0.00053712
Iteration 23/25 | Loss: 0.00053712
Iteration 24/25 | Loss: 0.00053712
Iteration 25/25 | Loss: 0.00053712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053712
Iteration 2/1000 | Loss: 0.00004645
Iteration 3/1000 | Loss: 0.00003481
Iteration 4/1000 | Loss: 0.00003050
Iteration 5/1000 | Loss: 0.00002860
Iteration 6/1000 | Loss: 0.00002733
Iteration 7/1000 | Loss: 0.00002636
Iteration 8/1000 | Loss: 0.00002584
Iteration 9/1000 | Loss: 0.00002542
Iteration 10/1000 | Loss: 0.00002508
Iteration 11/1000 | Loss: 0.00002506
Iteration 12/1000 | Loss: 0.00002486
Iteration 13/1000 | Loss: 0.00002480
Iteration 14/1000 | Loss: 0.00002473
Iteration 15/1000 | Loss: 0.00002456
Iteration 16/1000 | Loss: 0.00002442
Iteration 17/1000 | Loss: 0.00002439
Iteration 18/1000 | Loss: 0.00002436
Iteration 19/1000 | Loss: 0.00002430
Iteration 20/1000 | Loss: 0.00002430
Iteration 21/1000 | Loss: 0.00002424
Iteration 22/1000 | Loss: 0.00002424
Iteration 23/1000 | Loss: 0.00002420
Iteration 24/1000 | Loss: 0.00002417
Iteration 25/1000 | Loss: 0.00002416
Iteration 26/1000 | Loss: 0.00002416
Iteration 27/1000 | Loss: 0.00002415
Iteration 28/1000 | Loss: 0.00002414
Iteration 29/1000 | Loss: 0.00002413
Iteration 30/1000 | Loss: 0.00002412
Iteration 31/1000 | Loss: 0.00002412
Iteration 32/1000 | Loss: 0.00002411
Iteration 33/1000 | Loss: 0.00002410
Iteration 34/1000 | Loss: 0.00002408
Iteration 35/1000 | Loss: 0.00002408
Iteration 36/1000 | Loss: 0.00002407
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002404
Iteration 41/1000 | Loss: 0.00002403
Iteration 42/1000 | Loss: 0.00002403
Iteration 43/1000 | Loss: 0.00002403
Iteration 44/1000 | Loss: 0.00002402
Iteration 45/1000 | Loss: 0.00002402
Iteration 46/1000 | Loss: 0.00002402
Iteration 47/1000 | Loss: 0.00002401
Iteration 48/1000 | Loss: 0.00002401
Iteration 49/1000 | Loss: 0.00002401
Iteration 50/1000 | Loss: 0.00002401
Iteration 51/1000 | Loss: 0.00002401
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002400
Iteration 54/1000 | Loss: 0.00002400
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002400
Iteration 57/1000 | Loss: 0.00002399
Iteration 58/1000 | Loss: 0.00002399
Iteration 59/1000 | Loss: 0.00002399
Iteration 60/1000 | Loss: 0.00002398
Iteration 61/1000 | Loss: 0.00002398
Iteration 62/1000 | Loss: 0.00002398
Iteration 63/1000 | Loss: 0.00002397
Iteration 64/1000 | Loss: 0.00002397
Iteration 65/1000 | Loss: 0.00002397
Iteration 66/1000 | Loss: 0.00002396
Iteration 67/1000 | Loss: 0.00002396
Iteration 68/1000 | Loss: 0.00002396
Iteration 69/1000 | Loss: 0.00002396
Iteration 70/1000 | Loss: 0.00002396
Iteration 71/1000 | Loss: 0.00002395
Iteration 72/1000 | Loss: 0.00002395
Iteration 73/1000 | Loss: 0.00002395
Iteration 74/1000 | Loss: 0.00002395
Iteration 75/1000 | Loss: 0.00002395
Iteration 76/1000 | Loss: 0.00002394
Iteration 77/1000 | Loss: 0.00002394
Iteration 78/1000 | Loss: 0.00002394
Iteration 79/1000 | Loss: 0.00002393
Iteration 80/1000 | Loss: 0.00002393
Iteration 81/1000 | Loss: 0.00002393
Iteration 82/1000 | Loss: 0.00002393
Iteration 83/1000 | Loss: 0.00002392
Iteration 84/1000 | Loss: 0.00002392
Iteration 85/1000 | Loss: 0.00002392
Iteration 86/1000 | Loss: 0.00002391
Iteration 87/1000 | Loss: 0.00002391
Iteration 88/1000 | Loss: 0.00002391
Iteration 89/1000 | Loss: 0.00002391
Iteration 90/1000 | Loss: 0.00002390
Iteration 91/1000 | Loss: 0.00002390
Iteration 92/1000 | Loss: 0.00002390
Iteration 93/1000 | Loss: 0.00002390
Iteration 94/1000 | Loss: 0.00002390
Iteration 95/1000 | Loss: 0.00002390
Iteration 96/1000 | Loss: 0.00002390
Iteration 97/1000 | Loss: 0.00002390
Iteration 98/1000 | Loss: 0.00002390
Iteration 99/1000 | Loss: 0.00002389
Iteration 100/1000 | Loss: 0.00002389
Iteration 101/1000 | Loss: 0.00002389
Iteration 102/1000 | Loss: 0.00002389
Iteration 103/1000 | Loss: 0.00002389
Iteration 104/1000 | Loss: 0.00002389
Iteration 105/1000 | Loss: 0.00002389
Iteration 106/1000 | Loss: 0.00002389
Iteration 107/1000 | Loss: 0.00002389
Iteration 108/1000 | Loss: 0.00002389
Iteration 109/1000 | Loss: 0.00002389
Iteration 110/1000 | Loss: 0.00002389
Iteration 111/1000 | Loss: 0.00002389
Iteration 112/1000 | Loss: 0.00002388
Iteration 113/1000 | Loss: 0.00002388
Iteration 114/1000 | Loss: 0.00002388
Iteration 115/1000 | Loss: 0.00002388
Iteration 116/1000 | Loss: 0.00002388
Iteration 117/1000 | Loss: 0.00002388
Iteration 118/1000 | Loss: 0.00002388
Iteration 119/1000 | Loss: 0.00002387
Iteration 120/1000 | Loss: 0.00002387
Iteration 121/1000 | Loss: 0.00002387
Iteration 122/1000 | Loss: 0.00002387
Iteration 123/1000 | Loss: 0.00002387
Iteration 124/1000 | Loss: 0.00002387
Iteration 125/1000 | Loss: 0.00002387
Iteration 126/1000 | Loss: 0.00002387
Iteration 127/1000 | Loss: 0.00002387
Iteration 128/1000 | Loss: 0.00002387
Iteration 129/1000 | Loss: 0.00002387
Iteration 130/1000 | Loss: 0.00002387
Iteration 131/1000 | Loss: 0.00002387
Iteration 132/1000 | Loss: 0.00002387
Iteration 133/1000 | Loss: 0.00002386
Iteration 134/1000 | Loss: 0.00002386
Iteration 135/1000 | Loss: 0.00002386
Iteration 136/1000 | Loss: 0.00002386
Iteration 137/1000 | Loss: 0.00002386
Iteration 138/1000 | Loss: 0.00002386
Iteration 139/1000 | Loss: 0.00002385
Iteration 140/1000 | Loss: 0.00002385
Iteration 141/1000 | Loss: 0.00002385
Iteration 142/1000 | Loss: 0.00002385
Iteration 143/1000 | Loss: 0.00002385
Iteration 144/1000 | Loss: 0.00002385
Iteration 145/1000 | Loss: 0.00002385
Iteration 146/1000 | Loss: 0.00002385
Iteration 147/1000 | Loss: 0.00002385
Iteration 148/1000 | Loss: 0.00002385
Iteration 149/1000 | Loss: 0.00002385
Iteration 150/1000 | Loss: 0.00002385
Iteration 151/1000 | Loss: 0.00002385
Iteration 152/1000 | Loss: 0.00002385
Iteration 153/1000 | Loss: 0.00002385
Iteration 154/1000 | Loss: 0.00002385
Iteration 155/1000 | Loss: 0.00002385
Iteration 156/1000 | Loss: 0.00002385
Iteration 157/1000 | Loss: 0.00002385
Iteration 158/1000 | Loss: 0.00002385
Iteration 159/1000 | Loss: 0.00002385
Iteration 160/1000 | Loss: 0.00002385
Iteration 161/1000 | Loss: 0.00002385
Iteration 162/1000 | Loss: 0.00002385
Iteration 163/1000 | Loss: 0.00002385
Iteration 164/1000 | Loss: 0.00002385
Iteration 165/1000 | Loss: 0.00002385
Iteration 166/1000 | Loss: 0.00002385
Iteration 167/1000 | Loss: 0.00002385
Iteration 168/1000 | Loss: 0.00002385
Iteration 169/1000 | Loss: 0.00002385
Iteration 170/1000 | Loss: 0.00002385
Iteration 171/1000 | Loss: 0.00002385
Iteration 172/1000 | Loss: 0.00002385
Iteration 173/1000 | Loss: 0.00002385
Iteration 174/1000 | Loss: 0.00002385
Iteration 175/1000 | Loss: 0.00002385
Iteration 176/1000 | Loss: 0.00002385
Iteration 177/1000 | Loss: 0.00002385
Iteration 178/1000 | Loss: 0.00002385
Iteration 179/1000 | Loss: 0.00002385
Iteration 180/1000 | Loss: 0.00002385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.385377956670709e-05, 2.385377956670709e-05, 2.385377956670709e-05, 2.385377956670709e-05, 2.385377956670709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.385377956670709e-05

Optimization complete. Final v2v error: 4.007236957550049 mm

Highest mean error: 5.88521146774292 mm for frame 136

Lowest mean error: 3.050200939178467 mm for frame 151

Saving results

Total time: 44.96070408821106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00694618
Iteration 2/25 | Loss: 0.00165906
Iteration 3/25 | Loss: 0.00132892
Iteration 4/25 | Loss: 0.00128078
Iteration 5/25 | Loss: 0.00126604
Iteration 6/25 | Loss: 0.00125627
Iteration 7/25 | Loss: 0.00124517
Iteration 8/25 | Loss: 0.00124258
Iteration 9/25 | Loss: 0.00123509
Iteration 10/25 | Loss: 0.00123345
Iteration 11/25 | Loss: 0.00123338
Iteration 12/25 | Loss: 0.00123338
Iteration 13/25 | Loss: 0.00123338
Iteration 14/25 | Loss: 0.00123338
Iteration 15/25 | Loss: 0.00123337
Iteration 16/25 | Loss: 0.00123337
Iteration 17/25 | Loss: 0.00123337
Iteration 18/25 | Loss: 0.00123337
Iteration 19/25 | Loss: 0.00123337
Iteration 20/25 | Loss: 0.00123337
Iteration 21/25 | Loss: 0.00123337
Iteration 22/25 | Loss: 0.00123337
Iteration 23/25 | Loss: 0.00123337
Iteration 24/25 | Loss: 0.00123337
Iteration 25/25 | Loss: 0.00123337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14253998
Iteration 2/25 | Loss: 0.00075767
Iteration 3/25 | Loss: 0.00075736
Iteration 4/25 | Loss: 0.00075735
Iteration 5/25 | Loss: 0.00075735
Iteration 6/25 | Loss: 0.00075735
Iteration 7/25 | Loss: 0.00075735
Iteration 8/25 | Loss: 0.00075735
Iteration 9/25 | Loss: 0.00075735
Iteration 10/25 | Loss: 0.00075735
Iteration 11/25 | Loss: 0.00075735
Iteration 12/25 | Loss: 0.00075735
Iteration 13/25 | Loss: 0.00075735
Iteration 14/25 | Loss: 0.00075735
Iteration 15/25 | Loss: 0.00075735
Iteration 16/25 | Loss: 0.00075735
Iteration 17/25 | Loss: 0.00075735
Iteration 18/25 | Loss: 0.00075735
Iteration 19/25 | Loss: 0.00075735
Iteration 20/25 | Loss: 0.00075735
Iteration 21/25 | Loss: 0.00075735
Iteration 22/25 | Loss: 0.00075735
Iteration 23/25 | Loss: 0.00075735
Iteration 24/25 | Loss: 0.00075735
Iteration 25/25 | Loss: 0.00075735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075735
Iteration 2/1000 | Loss: 0.00006002
Iteration 3/1000 | Loss: 0.00003804
Iteration 4/1000 | Loss: 0.00003197
Iteration 5/1000 | Loss: 0.00002967
Iteration 6/1000 | Loss: 0.00002815
Iteration 7/1000 | Loss: 0.00002705
Iteration 8/1000 | Loss: 0.00002632
Iteration 9/1000 | Loss: 0.00002577
Iteration 10/1000 | Loss: 0.00002541
Iteration 11/1000 | Loss: 0.00002506
Iteration 12/1000 | Loss: 0.00002479
Iteration 13/1000 | Loss: 0.00002457
Iteration 14/1000 | Loss: 0.00002451
Iteration 15/1000 | Loss: 0.00002441
Iteration 16/1000 | Loss: 0.00002440
Iteration 17/1000 | Loss: 0.00002439
Iteration 18/1000 | Loss: 0.00002425
Iteration 19/1000 | Loss: 0.00002414
Iteration 20/1000 | Loss: 0.00002409
Iteration 21/1000 | Loss: 0.00002409
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002405
Iteration 24/1000 | Loss: 0.00002405
Iteration 25/1000 | Loss: 0.00002404
Iteration 26/1000 | Loss: 0.00002404
Iteration 27/1000 | Loss: 0.00002404
Iteration 28/1000 | Loss: 0.00002402
Iteration 29/1000 | Loss: 0.00002399
Iteration 30/1000 | Loss: 0.00002397
Iteration 31/1000 | Loss: 0.00002396
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002396
Iteration 34/1000 | Loss: 0.00002395
Iteration 35/1000 | Loss: 0.00002395
Iteration 36/1000 | Loss: 0.00002395
Iteration 37/1000 | Loss: 0.00002395
Iteration 38/1000 | Loss: 0.00002395
Iteration 39/1000 | Loss: 0.00002395
Iteration 40/1000 | Loss: 0.00002395
Iteration 41/1000 | Loss: 0.00002395
Iteration 42/1000 | Loss: 0.00002395
Iteration 43/1000 | Loss: 0.00002395
Iteration 44/1000 | Loss: 0.00002395
Iteration 45/1000 | Loss: 0.00002394
Iteration 46/1000 | Loss: 0.00002394
Iteration 47/1000 | Loss: 0.00002394
Iteration 48/1000 | Loss: 0.00002394
Iteration 49/1000 | Loss: 0.00002392
Iteration 50/1000 | Loss: 0.00002390
Iteration 51/1000 | Loss: 0.00002390
Iteration 52/1000 | Loss: 0.00002389
Iteration 53/1000 | Loss: 0.00002389
Iteration 54/1000 | Loss: 0.00002389
Iteration 55/1000 | Loss: 0.00002389
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00002389
Iteration 58/1000 | Loss: 0.00002389
Iteration 59/1000 | Loss: 0.00002389
Iteration 60/1000 | Loss: 0.00002388
Iteration 61/1000 | Loss: 0.00002388
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002384
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002383
Iteration 66/1000 | Loss: 0.00002383
Iteration 67/1000 | Loss: 0.00002382
Iteration 68/1000 | Loss: 0.00002382
Iteration 69/1000 | Loss: 0.00002381
Iteration 70/1000 | Loss: 0.00002380
Iteration 71/1000 | Loss: 0.00002379
Iteration 72/1000 | Loss: 0.00002379
Iteration 73/1000 | Loss: 0.00002378
Iteration 74/1000 | Loss: 0.00002377
Iteration 75/1000 | Loss: 0.00002377
Iteration 76/1000 | Loss: 0.00002377
Iteration 77/1000 | Loss: 0.00002377
Iteration 78/1000 | Loss: 0.00002376
Iteration 79/1000 | Loss: 0.00002376
Iteration 80/1000 | Loss: 0.00002376
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00002375
Iteration 84/1000 | Loss: 0.00002374
Iteration 85/1000 | Loss: 0.00002374
Iteration 86/1000 | Loss: 0.00002373
Iteration 87/1000 | Loss: 0.00002373
Iteration 88/1000 | Loss: 0.00002372
Iteration 89/1000 | Loss: 0.00002372
Iteration 90/1000 | Loss: 0.00002372
Iteration 91/1000 | Loss: 0.00002371
Iteration 92/1000 | Loss: 0.00002371
Iteration 93/1000 | Loss: 0.00002371
Iteration 94/1000 | Loss: 0.00002371
Iteration 95/1000 | Loss: 0.00002370
Iteration 96/1000 | Loss: 0.00002370
Iteration 97/1000 | Loss: 0.00002370
Iteration 98/1000 | Loss: 0.00002369
Iteration 99/1000 | Loss: 0.00002369
Iteration 100/1000 | Loss: 0.00002369
Iteration 101/1000 | Loss: 0.00002368
Iteration 102/1000 | Loss: 0.00002368
Iteration 103/1000 | Loss: 0.00002368
Iteration 104/1000 | Loss: 0.00002368
Iteration 105/1000 | Loss: 0.00002368
Iteration 106/1000 | Loss: 0.00002368
Iteration 107/1000 | Loss: 0.00002368
Iteration 108/1000 | Loss: 0.00002368
Iteration 109/1000 | Loss: 0.00002368
Iteration 110/1000 | Loss: 0.00002367
Iteration 111/1000 | Loss: 0.00002367
Iteration 112/1000 | Loss: 0.00002366
Iteration 113/1000 | Loss: 0.00002366
Iteration 114/1000 | Loss: 0.00002366
Iteration 115/1000 | Loss: 0.00002365
Iteration 116/1000 | Loss: 0.00002365
Iteration 117/1000 | Loss: 0.00002365
Iteration 118/1000 | Loss: 0.00002365
Iteration 119/1000 | Loss: 0.00002364
Iteration 120/1000 | Loss: 0.00002364
Iteration 121/1000 | Loss: 0.00002364
Iteration 122/1000 | Loss: 0.00002363
Iteration 123/1000 | Loss: 0.00002363
Iteration 124/1000 | Loss: 0.00002363
Iteration 125/1000 | Loss: 0.00002363
Iteration 126/1000 | Loss: 0.00002363
Iteration 127/1000 | Loss: 0.00002363
Iteration 128/1000 | Loss: 0.00002363
Iteration 129/1000 | Loss: 0.00002363
Iteration 130/1000 | Loss: 0.00002363
Iteration 131/1000 | Loss: 0.00002363
Iteration 132/1000 | Loss: 0.00002363
Iteration 133/1000 | Loss: 0.00002362
Iteration 134/1000 | Loss: 0.00002362
Iteration 135/1000 | Loss: 0.00002362
Iteration 136/1000 | Loss: 0.00002362
Iteration 137/1000 | Loss: 0.00002361
Iteration 138/1000 | Loss: 0.00002361
Iteration 139/1000 | Loss: 0.00002361
Iteration 140/1000 | Loss: 0.00002361
Iteration 141/1000 | Loss: 0.00002360
Iteration 142/1000 | Loss: 0.00002360
Iteration 143/1000 | Loss: 0.00002360
Iteration 144/1000 | Loss: 0.00002360
Iteration 145/1000 | Loss: 0.00002359
Iteration 146/1000 | Loss: 0.00002359
Iteration 147/1000 | Loss: 0.00002359
Iteration 148/1000 | Loss: 0.00002358
Iteration 149/1000 | Loss: 0.00002358
Iteration 150/1000 | Loss: 0.00002358
Iteration 151/1000 | Loss: 0.00002358
Iteration 152/1000 | Loss: 0.00002358
Iteration 153/1000 | Loss: 0.00002358
Iteration 154/1000 | Loss: 0.00002358
Iteration 155/1000 | Loss: 0.00002358
Iteration 156/1000 | Loss: 0.00002358
Iteration 157/1000 | Loss: 0.00002357
Iteration 158/1000 | Loss: 0.00002357
Iteration 159/1000 | Loss: 0.00002357
Iteration 160/1000 | Loss: 0.00002357
Iteration 161/1000 | Loss: 0.00002357
Iteration 162/1000 | Loss: 0.00002356
Iteration 163/1000 | Loss: 0.00002356
Iteration 164/1000 | Loss: 0.00002356
Iteration 165/1000 | Loss: 0.00002355
Iteration 166/1000 | Loss: 0.00002355
Iteration 167/1000 | Loss: 0.00002355
Iteration 168/1000 | Loss: 0.00002355
Iteration 169/1000 | Loss: 0.00002355
Iteration 170/1000 | Loss: 0.00002355
Iteration 171/1000 | Loss: 0.00002354
Iteration 172/1000 | Loss: 0.00002354
Iteration 173/1000 | Loss: 0.00002354
Iteration 174/1000 | Loss: 0.00002354
Iteration 175/1000 | Loss: 0.00002354
Iteration 176/1000 | Loss: 0.00002354
Iteration 177/1000 | Loss: 0.00002354
Iteration 178/1000 | Loss: 0.00002354
Iteration 179/1000 | Loss: 0.00002354
Iteration 180/1000 | Loss: 0.00002353
Iteration 181/1000 | Loss: 0.00002353
Iteration 182/1000 | Loss: 0.00002353
Iteration 183/1000 | Loss: 0.00002353
Iteration 184/1000 | Loss: 0.00002352
Iteration 185/1000 | Loss: 0.00002352
Iteration 186/1000 | Loss: 0.00002352
Iteration 187/1000 | Loss: 0.00002352
Iteration 188/1000 | Loss: 0.00002352
Iteration 189/1000 | Loss: 0.00002352
Iteration 190/1000 | Loss: 0.00002351
Iteration 191/1000 | Loss: 0.00002351
Iteration 192/1000 | Loss: 0.00002351
Iteration 193/1000 | Loss: 0.00002351
Iteration 194/1000 | Loss: 0.00002351
Iteration 195/1000 | Loss: 0.00002351
Iteration 196/1000 | Loss: 0.00002351
Iteration 197/1000 | Loss: 0.00002351
Iteration 198/1000 | Loss: 0.00002351
Iteration 199/1000 | Loss: 0.00002350
Iteration 200/1000 | Loss: 0.00002350
Iteration 201/1000 | Loss: 0.00002350
Iteration 202/1000 | Loss: 0.00002350
Iteration 203/1000 | Loss: 0.00002350
Iteration 204/1000 | Loss: 0.00002349
Iteration 205/1000 | Loss: 0.00002349
Iteration 206/1000 | Loss: 0.00002349
Iteration 207/1000 | Loss: 0.00002349
Iteration 208/1000 | Loss: 0.00002349
Iteration 209/1000 | Loss: 0.00002349
Iteration 210/1000 | Loss: 0.00002349
Iteration 211/1000 | Loss: 0.00002349
Iteration 212/1000 | Loss: 0.00002349
Iteration 213/1000 | Loss: 0.00002349
Iteration 214/1000 | Loss: 0.00002349
Iteration 215/1000 | Loss: 0.00002349
Iteration 216/1000 | Loss: 0.00002349
Iteration 217/1000 | Loss: 0.00002349
Iteration 218/1000 | Loss: 0.00002349
Iteration 219/1000 | Loss: 0.00002349
Iteration 220/1000 | Loss: 0.00002349
Iteration 221/1000 | Loss: 0.00002349
Iteration 222/1000 | Loss: 0.00002349
Iteration 223/1000 | Loss: 0.00002349
Iteration 224/1000 | Loss: 0.00002349
Iteration 225/1000 | Loss: 0.00002349
Iteration 226/1000 | Loss: 0.00002349
Iteration 227/1000 | Loss: 0.00002349
Iteration 228/1000 | Loss: 0.00002349
Iteration 229/1000 | Loss: 0.00002349
Iteration 230/1000 | Loss: 0.00002349
Iteration 231/1000 | Loss: 0.00002349
Iteration 232/1000 | Loss: 0.00002349
Iteration 233/1000 | Loss: 0.00002349
Iteration 234/1000 | Loss: 0.00002349
Iteration 235/1000 | Loss: 0.00002349
Iteration 236/1000 | Loss: 0.00002349
Iteration 237/1000 | Loss: 0.00002349
Iteration 238/1000 | Loss: 0.00002349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.3490842067985795e-05, 2.3490842067985795e-05, 2.3490842067985795e-05, 2.3490842067985795e-05, 2.3490842067985795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3490842067985795e-05

Optimization complete. Final v2v error: 3.8958797454833984 mm

Highest mean error: 6.123230457305908 mm for frame 132

Lowest mean error: 3.0292465686798096 mm for frame 206

Saving results

Total time: 68.50484800338745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051232
Iteration 2/25 | Loss: 0.00192379
Iteration 3/25 | Loss: 0.00159486
Iteration 4/25 | Loss: 0.00125390
Iteration 5/25 | Loss: 0.00120957
Iteration 6/25 | Loss: 0.00121031
Iteration 7/25 | Loss: 0.00119546
Iteration 8/25 | Loss: 0.00119535
Iteration 9/25 | Loss: 0.00119311
Iteration 10/25 | Loss: 0.00118808
Iteration 11/25 | Loss: 0.00118838
Iteration 12/25 | Loss: 0.00118618
Iteration 13/25 | Loss: 0.00118605
Iteration 14/25 | Loss: 0.00118598
Iteration 15/25 | Loss: 0.00118598
Iteration 16/25 | Loss: 0.00118597
Iteration 17/25 | Loss: 0.00118597
Iteration 18/25 | Loss: 0.00118596
Iteration 19/25 | Loss: 0.00118596
Iteration 20/25 | Loss: 0.00118596
Iteration 21/25 | Loss: 0.00118596
Iteration 22/25 | Loss: 0.00118596
Iteration 23/25 | Loss: 0.00118596
Iteration 24/25 | Loss: 0.00118596
Iteration 25/25 | Loss: 0.00118595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.01474380
Iteration 2/25 | Loss: 0.00098530
Iteration 3/25 | Loss: 0.00098530
Iteration 4/25 | Loss: 0.00064232
Iteration 5/25 | Loss: 0.00064232
Iteration 6/25 | Loss: 0.00064231
Iteration 7/25 | Loss: 0.00064231
Iteration 8/25 | Loss: 0.00064231
Iteration 9/25 | Loss: 0.00064231
Iteration 10/25 | Loss: 0.00064231
Iteration 11/25 | Loss: 0.00064231
Iteration 12/25 | Loss: 0.00064231
Iteration 13/25 | Loss: 0.00064231
Iteration 14/25 | Loss: 0.00064231
Iteration 15/25 | Loss: 0.00064231
Iteration 16/25 | Loss: 0.00064231
Iteration 17/25 | Loss: 0.00064231
Iteration 18/25 | Loss: 0.00064231
Iteration 19/25 | Loss: 0.00064231
Iteration 20/25 | Loss: 0.00064231
Iteration 21/25 | Loss: 0.00064231
Iteration 22/25 | Loss: 0.00064231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006423121085390449, 0.0006423121085390449, 0.0006423121085390449, 0.0006423121085390449, 0.0006423121085390449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006423121085390449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064231
Iteration 2/1000 | Loss: 0.00048223
Iteration 3/1000 | Loss: 0.00006355
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001710
Iteration 6/1000 | Loss: 0.00008105
Iteration 7/1000 | Loss: 0.00028329
Iteration 8/1000 | Loss: 0.00003083
Iteration 9/1000 | Loss: 0.00002548
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00003293
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00006794
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001569
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00009850
Iteration 19/1000 | Loss: 0.00007922
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001502
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001488
Iteration 28/1000 | Loss: 0.00010388
Iteration 29/1000 | Loss: 0.00010388
Iteration 30/1000 | Loss: 0.00030870
Iteration 31/1000 | Loss: 0.00001918
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00005861
Iteration 35/1000 | Loss: 0.00001480
Iteration 36/1000 | Loss: 0.00005827
Iteration 37/1000 | Loss: 0.00007380
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001463
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00003038
Iteration 55/1000 | Loss: 0.00001947
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00002404
Iteration 75/1000 | Loss: 0.00007051
Iteration 76/1000 | Loss: 0.00001465
Iteration 77/1000 | Loss: 0.00002517
Iteration 78/1000 | Loss: 0.00012892
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00003557
Iteration 81/1000 | Loss: 0.00003464
Iteration 82/1000 | Loss: 0.00002144
Iteration 83/1000 | Loss: 0.00001462
Iteration 84/1000 | Loss: 0.00001461
Iteration 85/1000 | Loss: 0.00001461
Iteration 86/1000 | Loss: 0.00001461
Iteration 87/1000 | Loss: 0.00001461
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00002591
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001455
Iteration 93/1000 | Loss: 0.00001455
Iteration 94/1000 | Loss: 0.00001455
Iteration 95/1000 | Loss: 0.00001455
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00002218
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00005459
Iteration 113/1000 | Loss: 0.00002758
Iteration 114/1000 | Loss: 0.00001513
Iteration 115/1000 | Loss: 0.00001461
Iteration 116/1000 | Loss: 0.00006679
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00003221
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001451
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001450
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001449
Iteration 139/1000 | Loss: 0.00001449
Iteration 140/1000 | Loss: 0.00001449
Iteration 141/1000 | Loss: 0.00001449
Iteration 142/1000 | Loss: 0.00001449
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001449
Iteration 147/1000 | Loss: 0.00001448
Iteration 148/1000 | Loss: 0.00001448
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001447
Iteration 152/1000 | Loss: 0.00001447
Iteration 153/1000 | Loss: 0.00001447
Iteration 154/1000 | Loss: 0.00001447
Iteration 155/1000 | Loss: 0.00001447
Iteration 156/1000 | Loss: 0.00001447
Iteration 157/1000 | Loss: 0.00001447
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001447
Iteration 167/1000 | Loss: 0.00001447
Iteration 168/1000 | Loss: 0.00001447
Iteration 169/1000 | Loss: 0.00001446
Iteration 170/1000 | Loss: 0.00001446
Iteration 171/1000 | Loss: 0.00001446
Iteration 172/1000 | Loss: 0.00001446
Iteration 173/1000 | Loss: 0.00001446
Iteration 174/1000 | Loss: 0.00001446
Iteration 175/1000 | Loss: 0.00001446
Iteration 176/1000 | Loss: 0.00001446
Iteration 177/1000 | Loss: 0.00001446
Iteration 178/1000 | Loss: 0.00001446
Iteration 179/1000 | Loss: 0.00001446
Iteration 180/1000 | Loss: 0.00001446
Iteration 181/1000 | Loss: 0.00001446
Iteration 182/1000 | Loss: 0.00001446
Iteration 183/1000 | Loss: 0.00001446
Iteration 184/1000 | Loss: 0.00001446
Iteration 185/1000 | Loss: 0.00001446
Iteration 186/1000 | Loss: 0.00001446
Iteration 187/1000 | Loss: 0.00001446
Iteration 188/1000 | Loss: 0.00001446
Iteration 189/1000 | Loss: 0.00001446
Iteration 190/1000 | Loss: 0.00001446
Iteration 191/1000 | Loss: 0.00001446
Iteration 192/1000 | Loss: 0.00001446
Iteration 193/1000 | Loss: 0.00001446
Iteration 194/1000 | Loss: 0.00001446
Iteration 195/1000 | Loss: 0.00001446
Iteration 196/1000 | Loss: 0.00001446
Iteration 197/1000 | Loss: 0.00001446
Iteration 198/1000 | Loss: 0.00001446
Iteration 199/1000 | Loss: 0.00001446
Iteration 200/1000 | Loss: 0.00001446
Iteration 201/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.4456156350206584e-05, 1.4456156350206584e-05, 1.4456156350206584e-05, 1.4456156350206584e-05, 1.4456156350206584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4456156350206584e-05

Optimization complete. Final v2v error: 3.213732957839966 mm

Highest mean error: 3.5831267833709717 mm for frame 46

Lowest mean error: 2.9072189331054688 mm for frame 30

Saving results

Total time: 108.29673266410828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433266
Iteration 2/25 | Loss: 0.00126095
Iteration 3/25 | Loss: 0.00119966
Iteration 4/25 | Loss: 0.00118451
Iteration 5/25 | Loss: 0.00118061
Iteration 6/25 | Loss: 0.00117981
Iteration 7/25 | Loss: 0.00117964
Iteration 8/25 | Loss: 0.00117964
Iteration 9/25 | Loss: 0.00117964
Iteration 10/25 | Loss: 0.00117964
Iteration 11/25 | Loss: 0.00117964
Iteration 12/25 | Loss: 0.00117964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011796412291005254, 0.0011796412291005254, 0.0011796412291005254, 0.0011796412291005254, 0.0011796412291005254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011796412291005254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72016573
Iteration 2/25 | Loss: 0.00061860
Iteration 3/25 | Loss: 0.00061860
Iteration 4/25 | Loss: 0.00061860
Iteration 5/25 | Loss: 0.00061860
Iteration 6/25 | Loss: 0.00061860
Iteration 7/25 | Loss: 0.00061860
Iteration 8/25 | Loss: 0.00061860
Iteration 9/25 | Loss: 0.00061860
Iteration 10/25 | Loss: 0.00061860
Iteration 11/25 | Loss: 0.00061860
Iteration 12/25 | Loss: 0.00061860
Iteration 13/25 | Loss: 0.00061860
Iteration 14/25 | Loss: 0.00061860
Iteration 15/25 | Loss: 0.00061860
Iteration 16/25 | Loss: 0.00061860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006185972597450018, 0.0006185972597450018, 0.0006185972597450018, 0.0006185972597450018, 0.0006185972597450018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006185972597450018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061860
Iteration 2/1000 | Loss: 0.00003065
Iteration 3/1000 | Loss: 0.00001995
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001579
Iteration 7/1000 | Loss: 0.00001572
Iteration 8/1000 | Loss: 0.00001557
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001518
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001485
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001429
Iteration 84/1000 | Loss: 0.00001429
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001421
Iteration 102/1000 | Loss: 0.00001421
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001420
Iteration 105/1000 | Loss: 0.00001420
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001418
Iteration 110/1000 | Loss: 0.00001418
Iteration 111/1000 | Loss: 0.00001418
Iteration 112/1000 | Loss: 0.00001418
Iteration 113/1000 | Loss: 0.00001418
Iteration 114/1000 | Loss: 0.00001418
Iteration 115/1000 | Loss: 0.00001418
Iteration 116/1000 | Loss: 0.00001418
Iteration 117/1000 | Loss: 0.00001417
Iteration 118/1000 | Loss: 0.00001417
Iteration 119/1000 | Loss: 0.00001417
Iteration 120/1000 | Loss: 0.00001417
Iteration 121/1000 | Loss: 0.00001417
Iteration 122/1000 | Loss: 0.00001417
Iteration 123/1000 | Loss: 0.00001417
Iteration 124/1000 | Loss: 0.00001417
Iteration 125/1000 | Loss: 0.00001417
Iteration 126/1000 | Loss: 0.00001417
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001416
Iteration 129/1000 | Loss: 0.00001416
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001415
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001415
Iteration 141/1000 | Loss: 0.00001415
Iteration 142/1000 | Loss: 0.00001415
Iteration 143/1000 | Loss: 0.00001415
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001415
Iteration 148/1000 | Loss: 0.00001415
Iteration 149/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4152201401884668e-05, 1.4152201401884668e-05, 1.4152201401884668e-05, 1.4152201401884668e-05, 1.4152201401884668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4152201401884668e-05

Optimization complete. Final v2v error: 3.190495252609253 mm

Highest mean error: 3.708322525024414 mm for frame 61

Lowest mean error: 3.0329606533050537 mm for frame 86

Saving results

Total time: 38.178569316864014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784262
Iteration 2/25 | Loss: 0.00149120
Iteration 3/25 | Loss: 0.00126803
Iteration 4/25 | Loss: 0.00123683
Iteration 5/25 | Loss: 0.00123012
Iteration 6/25 | Loss: 0.00122870
Iteration 7/25 | Loss: 0.00122870
Iteration 8/25 | Loss: 0.00122870
Iteration 9/25 | Loss: 0.00122870
Iteration 10/25 | Loss: 0.00122870
Iteration 11/25 | Loss: 0.00122870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012287041172385216, 0.0012287041172385216, 0.0012287041172385216, 0.0012287041172385216, 0.0012287041172385216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012287041172385216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44779265
Iteration 2/25 | Loss: 0.00057000
Iteration 3/25 | Loss: 0.00057000
Iteration 4/25 | Loss: 0.00057000
Iteration 5/25 | Loss: 0.00057000
Iteration 6/25 | Loss: 0.00057000
Iteration 7/25 | Loss: 0.00057000
Iteration 8/25 | Loss: 0.00057000
Iteration 9/25 | Loss: 0.00057000
Iteration 10/25 | Loss: 0.00057000
Iteration 11/25 | Loss: 0.00057000
Iteration 12/25 | Loss: 0.00057000
Iteration 13/25 | Loss: 0.00057000
Iteration 14/25 | Loss: 0.00057000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005699971225112677, 0.0005699971225112677, 0.0005699971225112677, 0.0005699971225112677, 0.0005699971225112677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005699971225112677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057000
Iteration 2/1000 | Loss: 0.00003347
Iteration 3/1000 | Loss: 0.00002568
Iteration 4/1000 | Loss: 0.00002370
Iteration 5/1000 | Loss: 0.00002268
Iteration 6/1000 | Loss: 0.00002179
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002105
Iteration 9/1000 | Loss: 0.00002065
Iteration 10/1000 | Loss: 0.00002045
Iteration 11/1000 | Loss: 0.00002033
Iteration 12/1000 | Loss: 0.00002033
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002014
Iteration 16/1000 | Loss: 0.00002008
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002003
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002001
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001998
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001997
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001995
Iteration 36/1000 | Loss: 0.00001994
Iteration 37/1000 | Loss: 0.00001994
Iteration 38/1000 | Loss: 0.00001994
Iteration 39/1000 | Loss: 0.00001993
Iteration 40/1000 | Loss: 0.00001993
Iteration 41/1000 | Loss: 0.00001993
Iteration 42/1000 | Loss: 0.00001993
Iteration 43/1000 | Loss: 0.00001992
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001990
Iteration 50/1000 | Loss: 0.00001990
Iteration 51/1000 | Loss: 0.00001990
Iteration 52/1000 | Loss: 0.00001989
Iteration 53/1000 | Loss: 0.00001989
Iteration 54/1000 | Loss: 0.00001989
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001986
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001986
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001985
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001985
Iteration 88/1000 | Loss: 0.00001984
Iteration 89/1000 | Loss: 0.00001984
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001984
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001983
Iteration 102/1000 | Loss: 0.00001983
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001982
Iteration 113/1000 | Loss: 0.00001982
Iteration 114/1000 | Loss: 0.00001982
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001981
Iteration 119/1000 | Loss: 0.00001981
Iteration 120/1000 | Loss: 0.00001981
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001980
Iteration 130/1000 | Loss: 0.00001980
Iteration 131/1000 | Loss: 0.00001980
Iteration 132/1000 | Loss: 0.00001980
Iteration 133/1000 | Loss: 0.00001980
Iteration 134/1000 | Loss: 0.00001979
Iteration 135/1000 | Loss: 0.00001979
Iteration 136/1000 | Loss: 0.00001979
Iteration 137/1000 | Loss: 0.00001979
Iteration 138/1000 | Loss: 0.00001978
Iteration 139/1000 | Loss: 0.00001978
Iteration 140/1000 | Loss: 0.00001978
Iteration 141/1000 | Loss: 0.00001978
Iteration 142/1000 | Loss: 0.00001978
Iteration 143/1000 | Loss: 0.00001978
Iteration 144/1000 | Loss: 0.00001978
Iteration 145/1000 | Loss: 0.00001977
Iteration 146/1000 | Loss: 0.00001977
Iteration 147/1000 | Loss: 0.00001977
Iteration 148/1000 | Loss: 0.00001977
Iteration 149/1000 | Loss: 0.00001977
Iteration 150/1000 | Loss: 0.00001976
Iteration 151/1000 | Loss: 0.00001976
Iteration 152/1000 | Loss: 0.00001976
Iteration 153/1000 | Loss: 0.00001976
Iteration 154/1000 | Loss: 0.00001976
Iteration 155/1000 | Loss: 0.00001976
Iteration 156/1000 | Loss: 0.00001976
Iteration 157/1000 | Loss: 0.00001976
Iteration 158/1000 | Loss: 0.00001976
Iteration 159/1000 | Loss: 0.00001976
Iteration 160/1000 | Loss: 0.00001976
Iteration 161/1000 | Loss: 0.00001976
Iteration 162/1000 | Loss: 0.00001975
Iteration 163/1000 | Loss: 0.00001975
Iteration 164/1000 | Loss: 0.00001975
Iteration 165/1000 | Loss: 0.00001975
Iteration 166/1000 | Loss: 0.00001975
Iteration 167/1000 | Loss: 0.00001975
Iteration 168/1000 | Loss: 0.00001975
Iteration 169/1000 | Loss: 0.00001975
Iteration 170/1000 | Loss: 0.00001975
Iteration 171/1000 | Loss: 0.00001975
Iteration 172/1000 | Loss: 0.00001975
Iteration 173/1000 | Loss: 0.00001975
Iteration 174/1000 | Loss: 0.00001975
Iteration 175/1000 | Loss: 0.00001975
Iteration 176/1000 | Loss: 0.00001975
Iteration 177/1000 | Loss: 0.00001975
Iteration 178/1000 | Loss: 0.00001975
Iteration 179/1000 | Loss: 0.00001975
Iteration 180/1000 | Loss: 0.00001975
Iteration 181/1000 | Loss: 0.00001975
Iteration 182/1000 | Loss: 0.00001975
Iteration 183/1000 | Loss: 0.00001975
Iteration 184/1000 | Loss: 0.00001975
Iteration 185/1000 | Loss: 0.00001975
Iteration 186/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.975025406864006e-05, 1.975025406864006e-05, 1.975025406864006e-05, 1.975025406864006e-05, 1.975025406864006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.975025406864006e-05

Optimization complete. Final v2v error: 3.7190001010894775 mm

Highest mean error: 4.1551194190979 mm for frame 151

Lowest mean error: 3.461012125015259 mm for frame 58

Saving results

Total time: 43.22266125679016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816644
Iteration 2/25 | Loss: 0.00154737
Iteration 3/25 | Loss: 0.00124760
Iteration 4/25 | Loss: 0.00123256
Iteration 5/25 | Loss: 0.00122997
Iteration 6/25 | Loss: 0.00122997
Iteration 7/25 | Loss: 0.00122997
Iteration 8/25 | Loss: 0.00122997
Iteration 9/25 | Loss: 0.00122997
Iteration 10/25 | Loss: 0.00122997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001229969086125493, 0.001229969086125493, 0.001229969086125493, 0.001229969086125493, 0.001229969086125493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001229969086125493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44169557
Iteration 2/25 | Loss: 0.00055803
Iteration 3/25 | Loss: 0.00055803
Iteration 4/25 | Loss: 0.00055802
Iteration 5/25 | Loss: 0.00055802
Iteration 6/25 | Loss: 0.00055802
Iteration 7/25 | Loss: 0.00055802
Iteration 8/25 | Loss: 0.00055802
Iteration 9/25 | Loss: 0.00055802
Iteration 10/25 | Loss: 0.00055802
Iteration 11/25 | Loss: 0.00055802
Iteration 12/25 | Loss: 0.00055802
Iteration 13/25 | Loss: 0.00055802
Iteration 14/25 | Loss: 0.00055802
Iteration 15/25 | Loss: 0.00055802
Iteration 16/25 | Loss: 0.00055802
Iteration 17/25 | Loss: 0.00055802
Iteration 18/25 | Loss: 0.00055802
Iteration 19/25 | Loss: 0.00055802
Iteration 20/25 | Loss: 0.00055802
Iteration 21/25 | Loss: 0.00055802
Iteration 22/25 | Loss: 0.00055802
Iteration 23/25 | Loss: 0.00055802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005580221186392009, 0.0005580221186392009, 0.0005580221186392009, 0.0005580221186392009, 0.0005580221186392009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005580221186392009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055802
Iteration 2/1000 | Loss: 0.00002691
Iteration 3/1000 | Loss: 0.00002218
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001971
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001854
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00001797
Iteration 10/1000 | Loss: 0.00001777
Iteration 11/1000 | Loss: 0.00001749
Iteration 12/1000 | Loss: 0.00001740
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001708
Iteration 26/1000 | Loss: 0.00001707
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001697
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001691
Iteration 33/1000 | Loss: 0.00001690
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001690
Iteration 36/1000 | Loss: 0.00001689
Iteration 37/1000 | Loss: 0.00001688
Iteration 38/1000 | Loss: 0.00001688
Iteration 39/1000 | Loss: 0.00001687
Iteration 40/1000 | Loss: 0.00001687
Iteration 41/1000 | Loss: 0.00001687
Iteration 42/1000 | Loss: 0.00001687
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001686
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001686
Iteration 51/1000 | Loss: 0.00001686
Iteration 52/1000 | Loss: 0.00001686
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001681
Iteration 60/1000 | Loss: 0.00001681
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001681
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001680
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001680
Iteration 87/1000 | Loss: 0.00001680
Iteration 88/1000 | Loss: 0.00001680
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.6795998817542568e-05, 1.6795998817542568e-05, 1.6795998817542568e-05, 1.6795998817542568e-05, 1.6795998817542568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6795998817542568e-05

Optimization complete. Final v2v error: 3.398155689239502 mm

Highest mean error: 3.6043200492858887 mm for frame 176

Lowest mean error: 3.2965173721313477 mm for frame 141

Saving results

Total time: 37.674230337142944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566078
Iteration 2/25 | Loss: 0.00131875
Iteration 3/25 | Loss: 0.00126032
Iteration 4/25 | Loss: 0.00125038
Iteration 5/25 | Loss: 0.00124681
Iteration 6/25 | Loss: 0.00124630
Iteration 7/25 | Loss: 0.00124630
Iteration 8/25 | Loss: 0.00124630
Iteration 9/25 | Loss: 0.00124624
Iteration 10/25 | Loss: 0.00124624
Iteration 11/25 | Loss: 0.00124624
Iteration 12/25 | Loss: 0.00124624
Iteration 13/25 | Loss: 0.00124624
Iteration 14/25 | Loss: 0.00124624
Iteration 15/25 | Loss: 0.00124624
Iteration 16/25 | Loss: 0.00124624
Iteration 17/25 | Loss: 0.00124624
Iteration 18/25 | Loss: 0.00124624
Iteration 19/25 | Loss: 0.00124624
Iteration 20/25 | Loss: 0.00124624
Iteration 21/25 | Loss: 0.00124624
Iteration 22/25 | Loss: 0.00124624
Iteration 23/25 | Loss: 0.00124624
Iteration 24/25 | Loss: 0.00124624
Iteration 25/25 | Loss: 0.00124624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43798316
Iteration 2/25 | Loss: 0.00065846
Iteration 3/25 | Loss: 0.00065843
Iteration 4/25 | Loss: 0.00065843
Iteration 5/25 | Loss: 0.00065843
Iteration 6/25 | Loss: 0.00065843
Iteration 7/25 | Loss: 0.00065843
Iteration 8/25 | Loss: 0.00065842
Iteration 9/25 | Loss: 0.00065842
Iteration 10/25 | Loss: 0.00065842
Iteration 11/25 | Loss: 0.00065842
Iteration 12/25 | Loss: 0.00065842
Iteration 13/25 | Loss: 0.00065842
Iteration 14/25 | Loss: 0.00065842
Iteration 15/25 | Loss: 0.00065842
Iteration 16/25 | Loss: 0.00065842
Iteration 17/25 | Loss: 0.00065842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006584243383258581, 0.0006584243383258581, 0.0006584243383258581, 0.0006584243383258581, 0.0006584243383258581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006584243383258581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065842
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00002895
Iteration 4/1000 | Loss: 0.00002357
Iteration 5/1000 | Loss: 0.00002229
Iteration 6/1000 | Loss: 0.00002156
Iteration 7/1000 | Loss: 0.00002114
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002013
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001970
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001929
Iteration 16/1000 | Loss: 0.00001928
Iteration 17/1000 | Loss: 0.00001928
Iteration 18/1000 | Loss: 0.00001927
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001925
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001905
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001890
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001886
Iteration 35/1000 | Loss: 0.00001886
Iteration 36/1000 | Loss: 0.00001885
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001883
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001882
Iteration 43/1000 | Loss: 0.00001882
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001880
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001879
Iteration 51/1000 | Loss: 0.00001879
Iteration 52/1000 | Loss: 0.00001879
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001878
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001875
Iteration 66/1000 | Loss: 0.00001875
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001874
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001874
Iteration 72/1000 | Loss: 0.00001874
Iteration 73/1000 | Loss: 0.00001874
Iteration 74/1000 | Loss: 0.00001874
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001872
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001872
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001872
Iteration 93/1000 | Loss: 0.00001872
Iteration 94/1000 | Loss: 0.00001872
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8715520127443597e-05, 1.8715520127443597e-05, 1.8715520127443597e-05, 1.8715520127443597e-05, 1.8715520127443597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8715520127443597e-05

Optimization complete. Final v2v error: 3.5969722270965576 mm

Highest mean error: 3.9719390869140625 mm for frame 52

Lowest mean error: 3.18389892578125 mm for frame 136

Saving results

Total time: 39.59200692176819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00543672
Iteration 2/25 | Loss: 0.00156712
Iteration 3/25 | Loss: 0.00132865
Iteration 4/25 | Loss: 0.00130283
Iteration 5/25 | Loss: 0.00129458
Iteration 6/25 | Loss: 0.00129179
Iteration 7/25 | Loss: 0.00129141
Iteration 8/25 | Loss: 0.00129141
Iteration 9/25 | Loss: 0.00129141
Iteration 10/25 | Loss: 0.00129141
Iteration 11/25 | Loss: 0.00129141
Iteration 12/25 | Loss: 0.00129141
Iteration 13/25 | Loss: 0.00129141
Iteration 14/25 | Loss: 0.00129141
Iteration 15/25 | Loss: 0.00129141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012914086692035198, 0.0012914086692035198, 0.0012914086692035198, 0.0012914086692035198, 0.0012914086692035198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012914086692035198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99828577
Iteration 2/25 | Loss: 0.00086616
Iteration 3/25 | Loss: 0.00086616
Iteration 4/25 | Loss: 0.00086616
Iteration 5/25 | Loss: 0.00086616
Iteration 6/25 | Loss: 0.00086616
Iteration 7/25 | Loss: 0.00086616
Iteration 8/25 | Loss: 0.00086616
Iteration 9/25 | Loss: 0.00086616
Iteration 10/25 | Loss: 0.00086616
Iteration 11/25 | Loss: 0.00086616
Iteration 12/25 | Loss: 0.00086616
Iteration 13/25 | Loss: 0.00086616
Iteration 14/25 | Loss: 0.00086616
Iteration 15/25 | Loss: 0.00086616
Iteration 16/25 | Loss: 0.00086616
Iteration 17/25 | Loss: 0.00086616
Iteration 18/25 | Loss: 0.00086616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008661603205837309, 0.0008661603205837309, 0.0008661603205837309, 0.0008661603205837309, 0.0008661603205837309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008661603205837309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086616
Iteration 2/1000 | Loss: 0.00008398
Iteration 3/1000 | Loss: 0.00004692
Iteration 4/1000 | Loss: 0.00003844
Iteration 5/1000 | Loss: 0.00003562
Iteration 6/1000 | Loss: 0.00003389
Iteration 7/1000 | Loss: 0.00003283
Iteration 8/1000 | Loss: 0.00003153
Iteration 9/1000 | Loss: 0.00003073
Iteration 10/1000 | Loss: 0.00003027
Iteration 11/1000 | Loss: 0.00002982
Iteration 12/1000 | Loss: 0.00002947
Iteration 13/1000 | Loss: 0.00002923
Iteration 14/1000 | Loss: 0.00002902
Iteration 15/1000 | Loss: 0.00002880
Iteration 16/1000 | Loss: 0.00002863
Iteration 17/1000 | Loss: 0.00002848
Iteration 18/1000 | Loss: 0.00002843
Iteration 19/1000 | Loss: 0.00002827
Iteration 20/1000 | Loss: 0.00002817
Iteration 21/1000 | Loss: 0.00002816
Iteration 22/1000 | Loss: 0.00002815
Iteration 23/1000 | Loss: 0.00002815
Iteration 24/1000 | Loss: 0.00002814
Iteration 25/1000 | Loss: 0.00002814
Iteration 26/1000 | Loss: 0.00002813
Iteration 27/1000 | Loss: 0.00002813
Iteration 28/1000 | Loss: 0.00002813
Iteration 29/1000 | Loss: 0.00002813
Iteration 30/1000 | Loss: 0.00002812
Iteration 31/1000 | Loss: 0.00002812
Iteration 32/1000 | Loss: 0.00002812
Iteration 33/1000 | Loss: 0.00002811
Iteration 34/1000 | Loss: 0.00002811
Iteration 35/1000 | Loss: 0.00002811
Iteration 36/1000 | Loss: 0.00002810
Iteration 37/1000 | Loss: 0.00002810
Iteration 38/1000 | Loss: 0.00002810
Iteration 39/1000 | Loss: 0.00002810
Iteration 40/1000 | Loss: 0.00002810
Iteration 41/1000 | Loss: 0.00002810
Iteration 42/1000 | Loss: 0.00002809
Iteration 43/1000 | Loss: 0.00002809
Iteration 44/1000 | Loss: 0.00002809
Iteration 45/1000 | Loss: 0.00002809
Iteration 46/1000 | Loss: 0.00002806
Iteration 47/1000 | Loss: 0.00002806
Iteration 48/1000 | Loss: 0.00002806
Iteration 49/1000 | Loss: 0.00002806
Iteration 50/1000 | Loss: 0.00002805
Iteration 51/1000 | Loss: 0.00002805
Iteration 52/1000 | Loss: 0.00002805
Iteration 53/1000 | Loss: 0.00002804
Iteration 54/1000 | Loss: 0.00002802
Iteration 55/1000 | Loss: 0.00002801
Iteration 56/1000 | Loss: 0.00002800
Iteration 57/1000 | Loss: 0.00002799
Iteration 58/1000 | Loss: 0.00002798
Iteration 59/1000 | Loss: 0.00002798
Iteration 60/1000 | Loss: 0.00002797
Iteration 61/1000 | Loss: 0.00002797
Iteration 62/1000 | Loss: 0.00002795
Iteration 63/1000 | Loss: 0.00002795
Iteration 64/1000 | Loss: 0.00002795
Iteration 65/1000 | Loss: 0.00002795
Iteration 66/1000 | Loss: 0.00002795
Iteration 67/1000 | Loss: 0.00002795
Iteration 68/1000 | Loss: 0.00002795
Iteration 69/1000 | Loss: 0.00002795
Iteration 70/1000 | Loss: 0.00002795
Iteration 71/1000 | Loss: 0.00002795
Iteration 72/1000 | Loss: 0.00002795
Iteration 73/1000 | Loss: 0.00002795
Iteration 74/1000 | Loss: 0.00002794
Iteration 75/1000 | Loss: 0.00002794
Iteration 76/1000 | Loss: 0.00002793
Iteration 77/1000 | Loss: 0.00002793
Iteration 78/1000 | Loss: 0.00002792
Iteration 79/1000 | Loss: 0.00002792
Iteration 80/1000 | Loss: 0.00002792
Iteration 81/1000 | Loss: 0.00002791
Iteration 82/1000 | Loss: 0.00002791
Iteration 83/1000 | Loss: 0.00002790
Iteration 84/1000 | Loss: 0.00002790
Iteration 85/1000 | Loss: 0.00002790
Iteration 86/1000 | Loss: 0.00002789
Iteration 87/1000 | Loss: 0.00002789
Iteration 88/1000 | Loss: 0.00002789
Iteration 89/1000 | Loss: 0.00002789
Iteration 90/1000 | Loss: 0.00002789
Iteration 91/1000 | Loss: 0.00002789
Iteration 92/1000 | Loss: 0.00002789
Iteration 93/1000 | Loss: 0.00002789
Iteration 94/1000 | Loss: 0.00002789
Iteration 95/1000 | Loss: 0.00002789
Iteration 96/1000 | Loss: 0.00002788
Iteration 97/1000 | Loss: 0.00002788
Iteration 98/1000 | Loss: 0.00002788
Iteration 99/1000 | Loss: 0.00002788
Iteration 100/1000 | Loss: 0.00002788
Iteration 101/1000 | Loss: 0.00002788
Iteration 102/1000 | Loss: 0.00002788
Iteration 103/1000 | Loss: 0.00002787
Iteration 104/1000 | Loss: 0.00002787
Iteration 105/1000 | Loss: 0.00002787
Iteration 106/1000 | Loss: 0.00002787
Iteration 107/1000 | Loss: 0.00002787
Iteration 108/1000 | Loss: 0.00002787
Iteration 109/1000 | Loss: 0.00002787
Iteration 110/1000 | Loss: 0.00002787
Iteration 111/1000 | Loss: 0.00002787
Iteration 112/1000 | Loss: 0.00002787
Iteration 113/1000 | Loss: 0.00002786
Iteration 114/1000 | Loss: 0.00002786
Iteration 115/1000 | Loss: 0.00002786
Iteration 116/1000 | Loss: 0.00002786
Iteration 117/1000 | Loss: 0.00002786
Iteration 118/1000 | Loss: 0.00002786
Iteration 119/1000 | Loss: 0.00002785
Iteration 120/1000 | Loss: 0.00002785
Iteration 121/1000 | Loss: 0.00002785
Iteration 122/1000 | Loss: 0.00002785
Iteration 123/1000 | Loss: 0.00002785
Iteration 124/1000 | Loss: 0.00002784
Iteration 125/1000 | Loss: 0.00002784
Iteration 126/1000 | Loss: 0.00002784
Iteration 127/1000 | Loss: 0.00002784
Iteration 128/1000 | Loss: 0.00002783
Iteration 129/1000 | Loss: 0.00002783
Iteration 130/1000 | Loss: 0.00002783
Iteration 131/1000 | Loss: 0.00002782
Iteration 132/1000 | Loss: 0.00002782
Iteration 133/1000 | Loss: 0.00002782
Iteration 134/1000 | Loss: 0.00002782
Iteration 135/1000 | Loss: 0.00002782
Iteration 136/1000 | Loss: 0.00002782
Iteration 137/1000 | Loss: 0.00002782
Iteration 138/1000 | Loss: 0.00002781
Iteration 139/1000 | Loss: 0.00002781
Iteration 140/1000 | Loss: 0.00002781
Iteration 141/1000 | Loss: 0.00002780
Iteration 142/1000 | Loss: 0.00002780
Iteration 143/1000 | Loss: 0.00002780
Iteration 144/1000 | Loss: 0.00002780
Iteration 145/1000 | Loss: 0.00002780
Iteration 146/1000 | Loss: 0.00002780
Iteration 147/1000 | Loss: 0.00002779
Iteration 148/1000 | Loss: 0.00002779
Iteration 149/1000 | Loss: 0.00002779
Iteration 150/1000 | Loss: 0.00002779
Iteration 151/1000 | Loss: 0.00002779
Iteration 152/1000 | Loss: 0.00002779
Iteration 153/1000 | Loss: 0.00002779
Iteration 154/1000 | Loss: 0.00002779
Iteration 155/1000 | Loss: 0.00002779
Iteration 156/1000 | Loss: 0.00002778
Iteration 157/1000 | Loss: 0.00002778
Iteration 158/1000 | Loss: 0.00002778
Iteration 159/1000 | Loss: 0.00002778
Iteration 160/1000 | Loss: 0.00002778
Iteration 161/1000 | Loss: 0.00002778
Iteration 162/1000 | Loss: 0.00002778
Iteration 163/1000 | Loss: 0.00002778
Iteration 164/1000 | Loss: 0.00002778
Iteration 165/1000 | Loss: 0.00002778
Iteration 166/1000 | Loss: 0.00002778
Iteration 167/1000 | Loss: 0.00002778
Iteration 168/1000 | Loss: 0.00002778
Iteration 169/1000 | Loss: 0.00002778
Iteration 170/1000 | Loss: 0.00002778
Iteration 171/1000 | Loss: 0.00002778
Iteration 172/1000 | Loss: 0.00002778
Iteration 173/1000 | Loss: 0.00002777
Iteration 174/1000 | Loss: 0.00002777
Iteration 175/1000 | Loss: 0.00002777
Iteration 176/1000 | Loss: 0.00002777
Iteration 177/1000 | Loss: 0.00002777
Iteration 178/1000 | Loss: 0.00002777
Iteration 179/1000 | Loss: 0.00002777
Iteration 180/1000 | Loss: 0.00002777
Iteration 181/1000 | Loss: 0.00002777
Iteration 182/1000 | Loss: 0.00002777
Iteration 183/1000 | Loss: 0.00002777
Iteration 184/1000 | Loss: 0.00002777
Iteration 185/1000 | Loss: 0.00002777
Iteration 186/1000 | Loss: 0.00002777
Iteration 187/1000 | Loss: 0.00002777
Iteration 188/1000 | Loss: 0.00002777
Iteration 189/1000 | Loss: 0.00002777
Iteration 190/1000 | Loss: 0.00002777
Iteration 191/1000 | Loss: 0.00002777
Iteration 192/1000 | Loss: 0.00002777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.777444024104625e-05, 2.777444024104625e-05, 2.777444024104625e-05, 2.777444024104625e-05, 2.777444024104625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.777444024104625e-05

Optimization complete. Final v2v error: 4.324347019195557 mm

Highest mean error: 5.369256496429443 mm for frame 19

Lowest mean error: 3.726330041885376 mm for frame 91

Saving results

Total time: 51.88138771057129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020136
Iteration 2/25 | Loss: 0.01020136
Iteration 3/25 | Loss: 0.01020136
Iteration 4/25 | Loss: 0.01020136
Iteration 5/25 | Loss: 0.01020135
Iteration 6/25 | Loss: 0.01020135
Iteration 7/25 | Loss: 0.01020135
Iteration 8/25 | Loss: 0.01020135
Iteration 9/25 | Loss: 0.01020135
Iteration 10/25 | Loss: 0.01020135
Iteration 11/25 | Loss: 0.01020135
Iteration 12/25 | Loss: 0.01020135
Iteration 13/25 | Loss: 0.01020135
Iteration 14/25 | Loss: 0.01020135
Iteration 15/25 | Loss: 0.01020135
Iteration 16/25 | Loss: 0.01020135
Iteration 17/25 | Loss: 0.01020134
Iteration 18/25 | Loss: 0.01020134
Iteration 19/25 | Loss: 0.01020134
Iteration 20/25 | Loss: 0.01020134
Iteration 21/25 | Loss: 0.01020134
Iteration 22/25 | Loss: 0.01020134
Iteration 23/25 | Loss: 0.01020134
Iteration 24/25 | Loss: 0.01020134
Iteration 25/25 | Loss: 0.01020134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95610309
Iteration 2/25 | Loss: 0.08789964
Iteration 3/25 | Loss: 0.08777893
Iteration 4/25 | Loss: 0.08777890
Iteration 5/25 | Loss: 0.08777890
Iteration 6/25 | Loss: 0.08777890
Iteration 7/25 | Loss: 0.08777890
Iteration 8/25 | Loss: 0.08777890
Iteration 9/25 | Loss: 0.08777890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.08777889609336853, 0.08777889609336853, 0.08777889609336853, 0.08777889609336853, 0.08777889609336853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08777889609336853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08777890
Iteration 2/1000 | Loss: 0.00478729
Iteration 3/1000 | Loss: 0.00275667
Iteration 4/1000 | Loss: 0.00223742
Iteration 5/1000 | Loss: 0.00108751
Iteration 6/1000 | Loss: 0.00143459
Iteration 7/1000 | Loss: 0.00093620
Iteration 8/1000 | Loss: 0.00108469
Iteration 9/1000 | Loss: 0.00084158
Iteration 10/1000 | Loss: 0.00060275
Iteration 11/1000 | Loss: 0.00226792
Iteration 12/1000 | Loss: 0.00023056
Iteration 13/1000 | Loss: 0.00044637
Iteration 14/1000 | Loss: 0.00043031
Iteration 15/1000 | Loss: 0.00053151
Iteration 16/1000 | Loss: 0.00042417
Iteration 17/1000 | Loss: 0.00032739
Iteration 18/1000 | Loss: 0.00018735
Iteration 19/1000 | Loss: 0.00004515
Iteration 20/1000 | Loss: 0.00061433
Iteration 21/1000 | Loss: 0.00004324
Iteration 22/1000 | Loss: 0.00003765
Iteration 23/1000 | Loss: 0.00003466
Iteration 24/1000 | Loss: 0.00003259
Iteration 25/1000 | Loss: 0.00056142
Iteration 26/1000 | Loss: 0.00026906
Iteration 27/1000 | Loss: 0.00006625
Iteration 28/1000 | Loss: 0.00002965
Iteration 29/1000 | Loss: 0.00003779
Iteration 30/1000 | Loss: 0.00088113
Iteration 31/1000 | Loss: 0.00004663
Iteration 32/1000 | Loss: 0.00002915
Iteration 33/1000 | Loss: 0.00009202
Iteration 34/1000 | Loss: 0.00003202
Iteration 35/1000 | Loss: 0.00002572
Iteration 36/1000 | Loss: 0.00039274
Iteration 37/1000 | Loss: 0.00003262
Iteration 38/1000 | Loss: 0.00002389
Iteration 39/1000 | Loss: 0.00002969
Iteration 40/1000 | Loss: 0.00002838
Iteration 41/1000 | Loss: 0.00002260
Iteration 42/1000 | Loss: 0.00002227
Iteration 43/1000 | Loss: 0.00002201
Iteration 44/1000 | Loss: 0.00012423
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002937
Iteration 47/1000 | Loss: 0.00049636
Iteration 48/1000 | Loss: 0.00020920
Iteration 49/1000 | Loss: 0.00013007
Iteration 50/1000 | Loss: 0.00002582
Iteration 51/1000 | Loss: 0.00003150
Iteration 52/1000 | Loss: 0.00002903
Iteration 53/1000 | Loss: 0.00002454
Iteration 54/1000 | Loss: 0.00002123
Iteration 55/1000 | Loss: 0.00002090
Iteration 56/1000 | Loss: 0.00002088
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002072
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002070
Iteration 74/1000 | Loss: 0.00002069
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002069
Iteration 77/1000 | Loss: 0.00002069
Iteration 78/1000 | Loss: 0.00002069
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002072
Iteration 81/1000 | Loss: 0.00002571
Iteration 82/1000 | Loss: 0.00002061
Iteration 83/1000 | Loss: 0.00002061
Iteration 84/1000 | Loss: 0.00002061
Iteration 85/1000 | Loss: 0.00002061
Iteration 86/1000 | Loss: 0.00002061
Iteration 87/1000 | Loss: 0.00002061
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002061
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002059
Iteration 96/1000 | Loss: 0.00002059
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002058
Iteration 101/1000 | Loss: 0.00002058
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002056
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002053
Iteration 115/1000 | Loss: 0.00002049
Iteration 116/1000 | Loss: 0.00002049
Iteration 117/1000 | Loss: 0.00003175
Iteration 118/1000 | Loss: 0.00002119
Iteration 119/1000 | Loss: 0.00002115
Iteration 120/1000 | Loss: 0.00003695
Iteration 121/1000 | Loss: 0.00002171
Iteration 122/1000 | Loss: 0.00002067
Iteration 123/1000 | Loss: 0.00002185
Iteration 124/1000 | Loss: 0.00002043
Iteration 125/1000 | Loss: 0.00002043
Iteration 126/1000 | Loss: 0.00002043
Iteration 127/1000 | Loss: 0.00002043
Iteration 128/1000 | Loss: 0.00002043
Iteration 129/1000 | Loss: 0.00002043
Iteration 130/1000 | Loss: 0.00002043
Iteration 131/1000 | Loss: 0.00002043
Iteration 132/1000 | Loss: 0.00002043
Iteration 133/1000 | Loss: 0.00002043
Iteration 134/1000 | Loss: 0.00002043
Iteration 135/1000 | Loss: 0.00002043
Iteration 136/1000 | Loss: 0.00002043
Iteration 137/1000 | Loss: 0.00002043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.0425624825293198e-05, 2.0425624825293198e-05, 2.0425624825293198e-05, 2.0425624825293198e-05, 2.0425624825293198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0425624825293198e-05

Optimization complete. Final v2v error: 3.7875144481658936 mm

Highest mean error: 4.581463813781738 mm for frame 91

Lowest mean error: 3.407233953475952 mm for frame 224

Saving results

Total time: 116.1871030330658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407343
Iteration 2/25 | Loss: 0.00127843
Iteration 3/25 | Loss: 0.00120478
Iteration 4/25 | Loss: 0.00119073
Iteration 5/25 | Loss: 0.00118547
Iteration 6/25 | Loss: 0.00118520
Iteration 7/25 | Loss: 0.00118520
Iteration 8/25 | Loss: 0.00118520
Iteration 9/25 | Loss: 0.00118520
Iteration 10/25 | Loss: 0.00118520
Iteration 11/25 | Loss: 0.00118520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011852027382701635, 0.0011852027382701635, 0.0011852027382701635, 0.0011852027382701635, 0.0011852027382701635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011852027382701635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46929753
Iteration 2/25 | Loss: 0.00065814
Iteration 3/25 | Loss: 0.00065814
Iteration 4/25 | Loss: 0.00065814
Iteration 5/25 | Loss: 0.00065814
Iteration 6/25 | Loss: 0.00065814
Iteration 7/25 | Loss: 0.00065814
Iteration 8/25 | Loss: 0.00065814
Iteration 9/25 | Loss: 0.00065814
Iteration 10/25 | Loss: 0.00065814
Iteration 11/25 | Loss: 0.00065814
Iteration 12/25 | Loss: 0.00065814
Iteration 13/25 | Loss: 0.00065814
Iteration 14/25 | Loss: 0.00065814
Iteration 15/25 | Loss: 0.00065814
Iteration 16/25 | Loss: 0.00065814
Iteration 17/25 | Loss: 0.00065814
Iteration 18/25 | Loss: 0.00065814
Iteration 19/25 | Loss: 0.00065814
Iteration 20/25 | Loss: 0.00065814
Iteration 21/25 | Loss: 0.00065814
Iteration 22/25 | Loss: 0.00065814
Iteration 23/25 | Loss: 0.00065814
Iteration 24/25 | Loss: 0.00065814
Iteration 25/25 | Loss: 0.00065814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065814
Iteration 2/1000 | Loss: 0.00002309
Iteration 3/1000 | Loss: 0.00001698
Iteration 4/1000 | Loss: 0.00001522
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001342
Iteration 12/1000 | Loss: 0.00001341
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001320
Iteration 19/1000 | Loss: 0.00001320
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001320
Iteration 22/1000 | Loss: 0.00001319
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001306
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001300
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001298
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001295
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001288
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001287
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.2811624401365407e-05, 1.2811624401365407e-05, 1.2811624401365407e-05, 1.2811624401365407e-05, 1.2811624401365407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2811624401365407e-05

Optimization complete. Final v2v error: 3.0776922702789307 mm

Highest mean error: 3.176462411880493 mm for frame 46

Lowest mean error: 2.9947783946990967 mm for frame 156

Saving results

Total time: 33.018930435180664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763693
Iteration 2/25 | Loss: 0.00139720
Iteration 3/25 | Loss: 0.00119925
Iteration 4/25 | Loss: 0.00117991
Iteration 5/25 | Loss: 0.00117452
Iteration 6/25 | Loss: 0.00117395
Iteration 7/25 | Loss: 0.00117395
Iteration 8/25 | Loss: 0.00117395
Iteration 9/25 | Loss: 0.00117395
Iteration 10/25 | Loss: 0.00117395
Iteration 11/25 | Loss: 0.00117395
Iteration 12/25 | Loss: 0.00117395
Iteration 13/25 | Loss: 0.00117395
Iteration 14/25 | Loss: 0.00117395
Iteration 15/25 | Loss: 0.00117395
Iteration 16/25 | Loss: 0.00117395
Iteration 17/25 | Loss: 0.00117395
Iteration 18/25 | Loss: 0.00117395
Iteration 19/25 | Loss: 0.00117395
Iteration 20/25 | Loss: 0.00117395
Iteration 21/25 | Loss: 0.00117395
Iteration 22/25 | Loss: 0.00117395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011739484034478664, 0.0011739484034478664, 0.0011739484034478664, 0.0011739484034478664, 0.0011739484034478664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011739484034478664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44062877
Iteration 2/25 | Loss: 0.00050171
Iteration 3/25 | Loss: 0.00050166
Iteration 4/25 | Loss: 0.00050166
Iteration 5/25 | Loss: 0.00050166
Iteration 6/25 | Loss: 0.00050166
Iteration 7/25 | Loss: 0.00050166
Iteration 8/25 | Loss: 0.00050166
Iteration 9/25 | Loss: 0.00050166
Iteration 10/25 | Loss: 0.00050166
Iteration 11/25 | Loss: 0.00050166
Iteration 12/25 | Loss: 0.00050166
Iteration 13/25 | Loss: 0.00050166
Iteration 14/25 | Loss: 0.00050166
Iteration 15/25 | Loss: 0.00050166
Iteration 16/25 | Loss: 0.00050166
Iteration 17/25 | Loss: 0.00050166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005016581271775067, 0.0005016581271775067, 0.0005016581271775067, 0.0005016581271775067, 0.0005016581271775067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005016581271775067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050166
Iteration 2/1000 | Loss: 0.00003856
Iteration 3/1000 | Loss: 0.00002882
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002294
Iteration 6/1000 | Loss: 0.00002183
Iteration 7/1000 | Loss: 0.00002093
Iteration 8/1000 | Loss: 0.00002036
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001963
Iteration 11/1000 | Loss: 0.00001938
Iteration 12/1000 | Loss: 0.00001934
Iteration 13/1000 | Loss: 0.00001930
Iteration 14/1000 | Loss: 0.00001929
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001850
Iteration 32/1000 | Loss: 0.00001850
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001844
Iteration 40/1000 | Loss: 0.00001843
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001842
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001841
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001840
Iteration 50/1000 | Loss: 0.00001840
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001839
Iteration 53/1000 | Loss: 0.00001839
Iteration 54/1000 | Loss: 0.00001839
Iteration 55/1000 | Loss: 0.00001839
Iteration 56/1000 | Loss: 0.00001839
Iteration 57/1000 | Loss: 0.00001839
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001838
Iteration 60/1000 | Loss: 0.00001838
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00001836
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00001833
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001832
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001831
Iteration 89/1000 | Loss: 0.00001831
Iteration 90/1000 | Loss: 0.00001831
Iteration 91/1000 | Loss: 0.00001831
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001830
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001829
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001829
Iteration 109/1000 | Loss: 0.00001829
Iteration 110/1000 | Loss: 0.00001829
Iteration 111/1000 | Loss: 0.00001829
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001828
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001828
Iteration 118/1000 | Loss: 0.00001828
Iteration 119/1000 | Loss: 0.00001828
Iteration 120/1000 | Loss: 0.00001828
Iteration 121/1000 | Loss: 0.00001828
Iteration 122/1000 | Loss: 0.00001828
Iteration 123/1000 | Loss: 0.00001828
Iteration 124/1000 | Loss: 0.00001828
Iteration 125/1000 | Loss: 0.00001828
Iteration 126/1000 | Loss: 0.00001828
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Iteration 130/1000 | Loss: 0.00001828
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001828
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001828
Iteration 135/1000 | Loss: 0.00001828
Iteration 136/1000 | Loss: 0.00001828
Iteration 137/1000 | Loss: 0.00001828
Iteration 138/1000 | Loss: 0.00001828
Iteration 139/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.828492349886801e-05, 1.828492349886801e-05, 1.828492349886801e-05, 1.828492349886801e-05, 1.828492349886801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.828492349886801e-05

Optimization complete. Final v2v error: 3.6235032081604004 mm

Highest mean error: 3.9358632564544678 mm for frame 141

Lowest mean error: 3.3551602363586426 mm for frame 102

Saving results

Total time: 45.98261618614197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994353
Iteration 2/25 | Loss: 0.00272016
Iteration 3/25 | Loss: 0.00208500
Iteration 4/25 | Loss: 0.00187987
Iteration 5/25 | Loss: 0.00182690
Iteration 6/25 | Loss: 0.00176213
Iteration 7/25 | Loss: 0.00170512
Iteration 8/25 | Loss: 0.00169976
Iteration 9/25 | Loss: 0.00160860
Iteration 10/25 | Loss: 0.00154295
Iteration 11/25 | Loss: 0.00151591
Iteration 12/25 | Loss: 0.00152662
Iteration 13/25 | Loss: 0.00144609
Iteration 14/25 | Loss: 0.00141910
Iteration 15/25 | Loss: 0.00140764
Iteration 16/25 | Loss: 0.00140323
Iteration 17/25 | Loss: 0.00140049
Iteration 18/25 | Loss: 0.00140854
Iteration 19/25 | Loss: 0.00140469
Iteration 20/25 | Loss: 0.00140694
Iteration 21/25 | Loss: 0.00139607
Iteration 22/25 | Loss: 0.00140338
Iteration 23/25 | Loss: 0.00140631
Iteration 24/25 | Loss: 0.00139946
Iteration 25/25 | Loss: 0.00139628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46260548
Iteration 2/25 | Loss: 0.00151718
Iteration 3/25 | Loss: 0.00136349
Iteration 4/25 | Loss: 0.00136349
Iteration 5/25 | Loss: 0.00136349
Iteration 6/25 | Loss: 0.00136349
Iteration 7/25 | Loss: 0.00136349
Iteration 8/25 | Loss: 0.00136349
Iteration 9/25 | Loss: 0.00136349
Iteration 10/25 | Loss: 0.00136349
Iteration 11/25 | Loss: 0.00136349
Iteration 12/25 | Loss: 0.00136349
Iteration 13/25 | Loss: 0.00136349
Iteration 14/25 | Loss: 0.00136349
Iteration 15/25 | Loss: 0.00136349
Iteration 16/25 | Loss: 0.00136349
Iteration 17/25 | Loss: 0.00136349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013634877977892756, 0.0013634877977892756, 0.0013634877977892756, 0.0013634877977892756, 0.0013634877977892756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013634877977892756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136349
Iteration 2/1000 | Loss: 0.00193607
Iteration 3/1000 | Loss: 0.00057607
Iteration 4/1000 | Loss: 0.00049784
Iteration 5/1000 | Loss: 0.00045174
Iteration 6/1000 | Loss: 0.00011450
Iteration 7/1000 | Loss: 0.00009569
Iteration 8/1000 | Loss: 0.00043856
Iteration 9/1000 | Loss: 0.00037343
Iteration 10/1000 | Loss: 0.00040669
Iteration 11/1000 | Loss: 0.00060633
Iteration 12/1000 | Loss: 0.00035205
Iteration 13/1000 | Loss: 0.00048176
Iteration 14/1000 | Loss: 0.00024370
Iteration 15/1000 | Loss: 0.00037864
Iteration 16/1000 | Loss: 0.00027521
Iteration 17/1000 | Loss: 0.00008744
Iteration 18/1000 | Loss: 0.00027896
Iteration 19/1000 | Loss: 0.00024917
Iteration 20/1000 | Loss: 0.00020143
Iteration 21/1000 | Loss: 0.00007947
Iteration 22/1000 | Loss: 0.00088760
Iteration 23/1000 | Loss: 0.00227157
Iteration 24/1000 | Loss: 0.00151016
Iteration 25/1000 | Loss: 0.00184809
Iteration 26/1000 | Loss: 0.00319372
Iteration 27/1000 | Loss: 0.00088332
Iteration 28/1000 | Loss: 0.00029176
Iteration 29/1000 | Loss: 0.00057220
Iteration 30/1000 | Loss: 0.00016677
Iteration 31/1000 | Loss: 0.00024608
Iteration 32/1000 | Loss: 0.00040867
Iteration 33/1000 | Loss: 0.00042877
Iteration 34/1000 | Loss: 0.00011856
Iteration 35/1000 | Loss: 0.00010915
Iteration 36/1000 | Loss: 0.00044477
Iteration 37/1000 | Loss: 0.00012723
Iteration 38/1000 | Loss: 0.00012523
Iteration 39/1000 | Loss: 0.00008920
Iteration 40/1000 | Loss: 0.00010250
Iteration 41/1000 | Loss: 0.00009462
Iteration 42/1000 | Loss: 0.00023898
Iteration 43/1000 | Loss: 0.00009303
Iteration 44/1000 | Loss: 0.00008067
Iteration 45/1000 | Loss: 0.00009769
Iteration 46/1000 | Loss: 0.00008735
Iteration 47/1000 | Loss: 0.00006765
Iteration 48/1000 | Loss: 0.00007258
Iteration 49/1000 | Loss: 0.00006219
Iteration 50/1000 | Loss: 0.00008684
Iteration 51/1000 | Loss: 0.00006548
Iteration 52/1000 | Loss: 0.00008897
Iteration 53/1000 | Loss: 0.00006869
Iteration 54/1000 | Loss: 0.00008031
Iteration 55/1000 | Loss: 0.00007514
Iteration 56/1000 | Loss: 0.00006167
Iteration 57/1000 | Loss: 0.00005850
Iteration 58/1000 | Loss: 0.00005735
Iteration 59/1000 | Loss: 0.00005668
Iteration 60/1000 | Loss: 0.00005618
Iteration 61/1000 | Loss: 0.00005566
Iteration 62/1000 | Loss: 0.00005500
Iteration 63/1000 | Loss: 0.00005448
Iteration 64/1000 | Loss: 0.00005401
Iteration 65/1000 | Loss: 0.00005346
Iteration 66/1000 | Loss: 0.00005292
Iteration 67/1000 | Loss: 0.00005242
Iteration 68/1000 | Loss: 0.00005207
Iteration 69/1000 | Loss: 0.00005174
Iteration 70/1000 | Loss: 0.00005136
Iteration 71/1000 | Loss: 0.00005098
Iteration 72/1000 | Loss: 0.00005061
Iteration 73/1000 | Loss: 0.00005056
Iteration 74/1000 | Loss: 0.00005020
Iteration 75/1000 | Loss: 0.00004999
Iteration 76/1000 | Loss: 0.00026346
Iteration 77/1000 | Loss: 0.00028742
Iteration 78/1000 | Loss: 0.00006597
Iteration 79/1000 | Loss: 0.00005877
Iteration 80/1000 | Loss: 0.00034467
Iteration 81/1000 | Loss: 0.00021127
Iteration 82/1000 | Loss: 0.00016928
Iteration 83/1000 | Loss: 0.00012300
Iteration 84/1000 | Loss: 0.00005617
Iteration 85/1000 | Loss: 0.00005188
Iteration 86/1000 | Loss: 0.00005086
Iteration 87/1000 | Loss: 0.00004997
Iteration 88/1000 | Loss: 0.00004976
Iteration 89/1000 | Loss: 0.00004970
Iteration 90/1000 | Loss: 0.00004969
Iteration 91/1000 | Loss: 0.00004968
Iteration 92/1000 | Loss: 0.00004968
Iteration 93/1000 | Loss: 0.00004968
Iteration 94/1000 | Loss: 0.00004967
Iteration 95/1000 | Loss: 0.00004967
Iteration 96/1000 | Loss: 0.00004967
Iteration 97/1000 | Loss: 0.00004967
Iteration 98/1000 | Loss: 0.00004967
Iteration 99/1000 | Loss: 0.00004966
Iteration 100/1000 | Loss: 0.00004966
Iteration 101/1000 | Loss: 0.00027426
Iteration 102/1000 | Loss: 0.00012299
Iteration 103/1000 | Loss: 0.00005405
Iteration 104/1000 | Loss: 0.00005280
Iteration 105/1000 | Loss: 0.00005070
Iteration 106/1000 | Loss: 0.00004984
Iteration 107/1000 | Loss: 0.00004977
Iteration 108/1000 | Loss: 0.00004976
Iteration 109/1000 | Loss: 0.00004967
Iteration 110/1000 | Loss: 0.00004967
Iteration 111/1000 | Loss: 0.00004967
Iteration 112/1000 | Loss: 0.00004966
Iteration 113/1000 | Loss: 0.00004966
Iteration 114/1000 | Loss: 0.00004966
Iteration 115/1000 | Loss: 0.00004966
Iteration 116/1000 | Loss: 0.00004965
Iteration 117/1000 | Loss: 0.00004965
Iteration 118/1000 | Loss: 0.00004965
Iteration 119/1000 | Loss: 0.00004965
Iteration 120/1000 | Loss: 0.00004965
Iteration 121/1000 | Loss: 0.00004964
Iteration 122/1000 | Loss: 0.00004964
Iteration 123/1000 | Loss: 0.00026364
Iteration 124/1000 | Loss: 0.00026372
Iteration 125/1000 | Loss: 0.00032530
Iteration 126/1000 | Loss: 0.00034133
Iteration 127/1000 | Loss: 0.00022987
Iteration 128/1000 | Loss: 0.00013807
Iteration 129/1000 | Loss: 0.00042402
Iteration 130/1000 | Loss: 0.00013851
Iteration 131/1000 | Loss: 0.00005432
Iteration 132/1000 | Loss: 0.00005362
Iteration 133/1000 | Loss: 0.00005295
Iteration 134/1000 | Loss: 0.00005187
Iteration 135/1000 | Loss: 0.00005128
Iteration 136/1000 | Loss: 0.00005097
Iteration 137/1000 | Loss: 0.00005077
Iteration 138/1000 | Loss: 0.00005063
Iteration 139/1000 | Loss: 0.00006658
Iteration 140/1000 | Loss: 0.00005467
Iteration 141/1000 | Loss: 0.00005213
Iteration 142/1000 | Loss: 0.00005087
Iteration 143/1000 | Loss: 0.00005024
Iteration 144/1000 | Loss: 0.00004997
Iteration 145/1000 | Loss: 0.00004987
Iteration 146/1000 | Loss: 0.00004975
Iteration 147/1000 | Loss: 0.00004974
Iteration 148/1000 | Loss: 0.00004973
Iteration 149/1000 | Loss: 0.00004971
Iteration 150/1000 | Loss: 0.00004970
Iteration 151/1000 | Loss: 0.00004958
Iteration 152/1000 | Loss: 0.00004942
Iteration 153/1000 | Loss: 0.00004933
Iteration 154/1000 | Loss: 0.00004925
Iteration 155/1000 | Loss: 0.00004919
Iteration 156/1000 | Loss: 0.00004917
Iteration 157/1000 | Loss: 0.00004916
Iteration 158/1000 | Loss: 0.00004916
Iteration 159/1000 | Loss: 0.00004915
Iteration 160/1000 | Loss: 0.00004915
Iteration 161/1000 | Loss: 0.00004915
Iteration 162/1000 | Loss: 0.00004914
Iteration 163/1000 | Loss: 0.00004914
Iteration 164/1000 | Loss: 0.00004914
Iteration 165/1000 | Loss: 0.00004913
Iteration 166/1000 | Loss: 0.00004913
Iteration 167/1000 | Loss: 0.00004913
Iteration 168/1000 | Loss: 0.00004912
Iteration 169/1000 | Loss: 0.00004912
Iteration 170/1000 | Loss: 0.00004912
Iteration 171/1000 | Loss: 0.00004912
Iteration 172/1000 | Loss: 0.00004911
Iteration 173/1000 | Loss: 0.00004911
Iteration 174/1000 | Loss: 0.00004911
Iteration 175/1000 | Loss: 0.00004911
Iteration 176/1000 | Loss: 0.00004910
Iteration 177/1000 | Loss: 0.00004910
Iteration 178/1000 | Loss: 0.00004910
Iteration 179/1000 | Loss: 0.00004909
Iteration 180/1000 | Loss: 0.00004909
Iteration 181/1000 | Loss: 0.00004909
Iteration 182/1000 | Loss: 0.00004909
Iteration 183/1000 | Loss: 0.00004909
Iteration 184/1000 | Loss: 0.00004908
Iteration 185/1000 | Loss: 0.00004908
Iteration 186/1000 | Loss: 0.00004908
Iteration 187/1000 | Loss: 0.00004908
Iteration 188/1000 | Loss: 0.00004907
Iteration 189/1000 | Loss: 0.00004907
Iteration 190/1000 | Loss: 0.00004907
Iteration 191/1000 | Loss: 0.00004907
Iteration 192/1000 | Loss: 0.00004906
Iteration 193/1000 | Loss: 0.00004906
Iteration 194/1000 | Loss: 0.00004906
Iteration 195/1000 | Loss: 0.00004905
Iteration 196/1000 | Loss: 0.00004905
Iteration 197/1000 | Loss: 0.00004905
Iteration 198/1000 | Loss: 0.00004905
Iteration 199/1000 | Loss: 0.00004904
Iteration 200/1000 | Loss: 0.00004904
Iteration 201/1000 | Loss: 0.00004904
Iteration 202/1000 | Loss: 0.00004904
Iteration 203/1000 | Loss: 0.00004904
Iteration 204/1000 | Loss: 0.00004903
Iteration 205/1000 | Loss: 0.00004903
Iteration 206/1000 | Loss: 0.00004903
Iteration 207/1000 | Loss: 0.00004903
Iteration 208/1000 | Loss: 0.00004903
Iteration 209/1000 | Loss: 0.00004903
Iteration 210/1000 | Loss: 0.00004902
Iteration 211/1000 | Loss: 0.00004902
Iteration 212/1000 | Loss: 0.00004902
Iteration 213/1000 | Loss: 0.00004902
Iteration 214/1000 | Loss: 0.00004902
Iteration 215/1000 | Loss: 0.00004902
Iteration 216/1000 | Loss: 0.00004901
Iteration 217/1000 | Loss: 0.00004901
Iteration 218/1000 | Loss: 0.00004901
Iteration 219/1000 | Loss: 0.00004901
Iteration 220/1000 | Loss: 0.00004901
Iteration 221/1000 | Loss: 0.00004901
Iteration 222/1000 | Loss: 0.00004900
Iteration 223/1000 | Loss: 0.00004900
Iteration 224/1000 | Loss: 0.00004900
Iteration 225/1000 | Loss: 0.00004900
Iteration 226/1000 | Loss: 0.00004900
Iteration 227/1000 | Loss: 0.00004900
Iteration 228/1000 | Loss: 0.00004899
Iteration 229/1000 | Loss: 0.00004899
Iteration 230/1000 | Loss: 0.00004899
Iteration 231/1000 | Loss: 0.00004899
Iteration 232/1000 | Loss: 0.00004898
Iteration 233/1000 | Loss: 0.00004898
Iteration 234/1000 | Loss: 0.00004898
Iteration 235/1000 | Loss: 0.00004898
Iteration 236/1000 | Loss: 0.00004897
Iteration 237/1000 | Loss: 0.00004897
Iteration 238/1000 | Loss: 0.00004897
Iteration 239/1000 | Loss: 0.00004897
Iteration 240/1000 | Loss: 0.00004897
Iteration 241/1000 | Loss: 0.00004897
Iteration 242/1000 | Loss: 0.00004897
Iteration 243/1000 | Loss: 0.00004896
Iteration 244/1000 | Loss: 0.00004896
Iteration 245/1000 | Loss: 0.00004896
Iteration 246/1000 | Loss: 0.00004896
Iteration 247/1000 | Loss: 0.00004896
Iteration 248/1000 | Loss: 0.00004896
Iteration 249/1000 | Loss: 0.00004896
Iteration 250/1000 | Loss: 0.00004896
Iteration 251/1000 | Loss: 0.00004896
Iteration 252/1000 | Loss: 0.00004896
Iteration 253/1000 | Loss: 0.00004896
Iteration 254/1000 | Loss: 0.00004896
Iteration 255/1000 | Loss: 0.00004896
Iteration 256/1000 | Loss: 0.00004896
Iteration 257/1000 | Loss: 0.00004896
Iteration 258/1000 | Loss: 0.00004895
Iteration 259/1000 | Loss: 0.00004895
Iteration 260/1000 | Loss: 0.00004895
Iteration 261/1000 | Loss: 0.00004895
Iteration 262/1000 | Loss: 0.00004895
Iteration 263/1000 | Loss: 0.00004895
Iteration 264/1000 | Loss: 0.00004895
Iteration 265/1000 | Loss: 0.00004895
Iteration 266/1000 | Loss: 0.00004895
Iteration 267/1000 | Loss: 0.00004895
Iteration 268/1000 | Loss: 0.00004895
Iteration 269/1000 | Loss: 0.00004895
Iteration 270/1000 | Loss: 0.00004895
Iteration 271/1000 | Loss: 0.00004895
Iteration 272/1000 | Loss: 0.00004895
Iteration 273/1000 | Loss: 0.00004895
Iteration 274/1000 | Loss: 0.00004895
Iteration 275/1000 | Loss: 0.00004895
Iteration 276/1000 | Loss: 0.00004895
Iteration 277/1000 | Loss: 0.00004895
Iteration 278/1000 | Loss: 0.00004895
Iteration 279/1000 | Loss: 0.00004895
Iteration 280/1000 | Loss: 0.00004895
Iteration 281/1000 | Loss: 0.00004895
Iteration 282/1000 | Loss: 0.00004895
Iteration 283/1000 | Loss: 0.00004895
Iteration 284/1000 | Loss: 0.00004895
Iteration 285/1000 | Loss: 0.00004895
Iteration 286/1000 | Loss: 0.00004895
Iteration 287/1000 | Loss: 0.00004895
Iteration 288/1000 | Loss: 0.00004895
Iteration 289/1000 | Loss: 0.00004895
Iteration 290/1000 | Loss: 0.00004895
Iteration 291/1000 | Loss: 0.00004895
Iteration 292/1000 | Loss: 0.00004895
Iteration 293/1000 | Loss: 0.00004895
Iteration 294/1000 | Loss: 0.00004895
Iteration 295/1000 | Loss: 0.00004895
Iteration 296/1000 | Loss: 0.00004895
Iteration 297/1000 | Loss: 0.00004895
Iteration 298/1000 | Loss: 0.00004895
Iteration 299/1000 | Loss: 0.00004895
Iteration 300/1000 | Loss: 0.00004895
Iteration 301/1000 | Loss: 0.00004895
Iteration 302/1000 | Loss: 0.00004895
Iteration 303/1000 | Loss: 0.00004895
Iteration 304/1000 | Loss: 0.00004895
Iteration 305/1000 | Loss: 0.00004895
Iteration 306/1000 | Loss: 0.00004895
Iteration 307/1000 | Loss: 0.00004895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [4.895364327239804e-05, 4.895364327239804e-05, 4.895364327239804e-05, 4.895364327239804e-05, 4.895364327239804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.895364327239804e-05

Optimization complete. Final v2v error: 4.373134136199951 mm

Highest mean error: 10.14422607421875 mm for frame 101

Lowest mean error: 3.166613817214966 mm for frame 121

Saving results

Total time: 227.08207511901855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775712
Iteration 2/25 | Loss: 0.00141365
Iteration 3/25 | Loss: 0.00124363
Iteration 4/25 | Loss: 0.00122958
Iteration 5/25 | Loss: 0.00122695
Iteration 6/25 | Loss: 0.00122695
Iteration 7/25 | Loss: 0.00122695
Iteration 8/25 | Loss: 0.00122695
Iteration 9/25 | Loss: 0.00122695
Iteration 10/25 | Loss: 0.00122695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012269512517377734, 0.0012269512517377734, 0.0012269512517377734, 0.0012269512517377734, 0.0012269512517377734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012269512517377734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43755078
Iteration 2/25 | Loss: 0.00057756
Iteration 3/25 | Loss: 0.00057755
Iteration 4/25 | Loss: 0.00057755
Iteration 5/25 | Loss: 0.00057755
Iteration 6/25 | Loss: 0.00057755
Iteration 7/25 | Loss: 0.00057755
Iteration 8/25 | Loss: 0.00057755
Iteration 9/25 | Loss: 0.00057755
Iteration 10/25 | Loss: 0.00057755
Iteration 11/25 | Loss: 0.00057755
Iteration 12/25 | Loss: 0.00057755
Iteration 13/25 | Loss: 0.00057755
Iteration 14/25 | Loss: 0.00057755
Iteration 15/25 | Loss: 0.00057755
Iteration 16/25 | Loss: 0.00057755
Iteration 17/25 | Loss: 0.00057755
Iteration 18/25 | Loss: 0.00057755
Iteration 19/25 | Loss: 0.00057755
Iteration 20/25 | Loss: 0.00057755
Iteration 21/25 | Loss: 0.00057755
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005775450263172388, 0.0005775450263172388, 0.0005775450263172388, 0.0005775450263172388, 0.0005775450263172388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005775450263172388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057755
Iteration 2/1000 | Loss: 0.00003670
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002215
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00002035
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001922
Iteration 9/1000 | Loss: 0.00001872
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001803
Iteration 13/1000 | Loss: 0.00001794
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00001777
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001775
Iteration 19/1000 | Loss: 0.00001774
Iteration 20/1000 | Loss: 0.00001774
Iteration 21/1000 | Loss: 0.00001773
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001771
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001748
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001740
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001735
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001733
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001725
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001715
Iteration 101/1000 | Loss: 0.00001715
Iteration 102/1000 | Loss: 0.00001715
Iteration 103/1000 | Loss: 0.00001715
Iteration 104/1000 | Loss: 0.00001714
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001712
Iteration 119/1000 | Loss: 0.00001711
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001711
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00001711
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001711
Iteration 127/1000 | Loss: 0.00001711
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00001710
Iteration 130/1000 | Loss: 0.00001710
Iteration 131/1000 | Loss: 0.00001710
Iteration 132/1000 | Loss: 0.00001710
Iteration 133/1000 | Loss: 0.00001710
Iteration 134/1000 | Loss: 0.00001710
Iteration 135/1000 | Loss: 0.00001710
Iteration 136/1000 | Loss: 0.00001710
Iteration 137/1000 | Loss: 0.00001710
Iteration 138/1000 | Loss: 0.00001710
Iteration 139/1000 | Loss: 0.00001710
Iteration 140/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.709910611680243e-05, 1.709910611680243e-05, 1.709910611680243e-05, 1.709910611680243e-05, 1.709910611680243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.709910611680243e-05

Optimization complete. Final v2v error: 3.469569683074951 mm

Highest mean error: 3.9848577976226807 mm for frame 159

Lowest mean error: 2.9592549800872803 mm for frame 6

Saving results

Total time: 44.56638216972351
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433217
Iteration 2/25 | Loss: 0.00134504
Iteration 3/25 | Loss: 0.00125626
Iteration 4/25 | Loss: 0.00124668
Iteration 5/25 | Loss: 0.00124447
Iteration 6/25 | Loss: 0.00124395
Iteration 7/25 | Loss: 0.00124395
Iteration 8/25 | Loss: 0.00124395
Iteration 9/25 | Loss: 0.00124395
Iteration 10/25 | Loss: 0.00124395
Iteration 11/25 | Loss: 0.00124395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012439540587365627, 0.0012439540587365627, 0.0012439540587365627, 0.0012439540587365627, 0.0012439540587365627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012439540587365627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48551095
Iteration 2/25 | Loss: 0.00082917
Iteration 3/25 | Loss: 0.00082916
Iteration 4/25 | Loss: 0.00082916
Iteration 5/25 | Loss: 0.00082916
Iteration 6/25 | Loss: 0.00082916
Iteration 7/25 | Loss: 0.00082916
Iteration 8/25 | Loss: 0.00082916
Iteration 9/25 | Loss: 0.00082916
Iteration 10/25 | Loss: 0.00082916
Iteration 11/25 | Loss: 0.00082916
Iteration 12/25 | Loss: 0.00082916
Iteration 13/25 | Loss: 0.00082916
Iteration 14/25 | Loss: 0.00082916
Iteration 15/25 | Loss: 0.00082916
Iteration 16/25 | Loss: 0.00082916
Iteration 17/25 | Loss: 0.00082916
Iteration 18/25 | Loss: 0.00082916
Iteration 19/25 | Loss: 0.00082916
Iteration 20/25 | Loss: 0.00082916
Iteration 21/25 | Loss: 0.00082916
Iteration 22/25 | Loss: 0.00082916
Iteration 23/25 | Loss: 0.00082916
Iteration 24/25 | Loss: 0.00082916
Iteration 25/25 | Loss: 0.00082916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082916
Iteration 2/1000 | Loss: 0.00004303
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00002231
Iteration 5/1000 | Loss: 0.00002136
Iteration 6/1000 | Loss: 0.00002053
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001963
Iteration 9/1000 | Loss: 0.00001948
Iteration 10/1000 | Loss: 0.00001941
Iteration 11/1000 | Loss: 0.00001937
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001922
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001919
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001908
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001886
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00001885
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001872
Iteration 41/1000 | Loss: 0.00001872
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001871
Iteration 46/1000 | Loss: 0.00001871
Iteration 47/1000 | Loss: 0.00001870
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001868
Iteration 51/1000 | Loss: 0.00001868
Iteration 52/1000 | Loss: 0.00001868
Iteration 53/1000 | Loss: 0.00001868
Iteration 54/1000 | Loss: 0.00001868
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001867
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001864
Iteration 81/1000 | Loss: 0.00001864
Iteration 82/1000 | Loss: 0.00001864
Iteration 83/1000 | Loss: 0.00001864
Iteration 84/1000 | Loss: 0.00001863
Iteration 85/1000 | Loss: 0.00001863
Iteration 86/1000 | Loss: 0.00001863
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001862
Iteration 91/1000 | Loss: 0.00001862
Iteration 92/1000 | Loss: 0.00001861
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001860
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001859
Iteration 106/1000 | Loss: 0.00001859
Iteration 107/1000 | Loss: 0.00001859
Iteration 108/1000 | Loss: 0.00001859
Iteration 109/1000 | Loss: 0.00001859
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001858
Iteration 112/1000 | Loss: 0.00001858
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001857
Iteration 121/1000 | Loss: 0.00001857
Iteration 122/1000 | Loss: 0.00001857
Iteration 123/1000 | Loss: 0.00001857
Iteration 124/1000 | Loss: 0.00001857
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001856
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001855
Iteration 133/1000 | Loss: 0.00001855
Iteration 134/1000 | Loss: 0.00001855
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001854
Iteration 143/1000 | Loss: 0.00001854
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001854
Iteration 148/1000 | Loss: 0.00001853
Iteration 149/1000 | Loss: 0.00001853
Iteration 150/1000 | Loss: 0.00001853
Iteration 151/1000 | Loss: 0.00001853
Iteration 152/1000 | Loss: 0.00001852
Iteration 153/1000 | Loss: 0.00001852
Iteration 154/1000 | Loss: 0.00001852
Iteration 155/1000 | Loss: 0.00001852
Iteration 156/1000 | Loss: 0.00001852
Iteration 157/1000 | Loss: 0.00001852
Iteration 158/1000 | Loss: 0.00001852
Iteration 159/1000 | Loss: 0.00001852
Iteration 160/1000 | Loss: 0.00001852
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Iteration 172/1000 | Loss: 0.00001850
Iteration 173/1000 | Loss: 0.00001850
Iteration 174/1000 | Loss: 0.00001850
Iteration 175/1000 | Loss: 0.00001850
Iteration 176/1000 | Loss: 0.00001850
Iteration 177/1000 | Loss: 0.00001850
Iteration 178/1000 | Loss: 0.00001850
Iteration 179/1000 | Loss: 0.00001850
Iteration 180/1000 | Loss: 0.00001850
Iteration 181/1000 | Loss: 0.00001850
Iteration 182/1000 | Loss: 0.00001850
Iteration 183/1000 | Loss: 0.00001850
Iteration 184/1000 | Loss: 0.00001850
Iteration 185/1000 | Loss: 0.00001850
Iteration 186/1000 | Loss: 0.00001849
Iteration 187/1000 | Loss: 0.00001849
Iteration 188/1000 | Loss: 0.00001849
Iteration 189/1000 | Loss: 0.00001849
Iteration 190/1000 | Loss: 0.00001849
Iteration 191/1000 | Loss: 0.00001849
Iteration 192/1000 | Loss: 0.00001849
Iteration 193/1000 | Loss: 0.00001849
Iteration 194/1000 | Loss: 0.00001849
Iteration 195/1000 | Loss: 0.00001849
Iteration 196/1000 | Loss: 0.00001849
Iteration 197/1000 | Loss: 0.00001849
Iteration 198/1000 | Loss: 0.00001849
Iteration 199/1000 | Loss: 0.00001849
Iteration 200/1000 | Loss: 0.00001849
Iteration 201/1000 | Loss: 0.00001849
Iteration 202/1000 | Loss: 0.00001849
Iteration 203/1000 | Loss: 0.00001849
Iteration 204/1000 | Loss: 0.00001848
Iteration 205/1000 | Loss: 0.00001848
Iteration 206/1000 | Loss: 0.00001848
Iteration 207/1000 | Loss: 0.00001848
Iteration 208/1000 | Loss: 0.00001848
Iteration 209/1000 | Loss: 0.00001848
Iteration 210/1000 | Loss: 0.00001848
Iteration 211/1000 | Loss: 0.00001848
Iteration 212/1000 | Loss: 0.00001848
Iteration 213/1000 | Loss: 0.00001848
Iteration 214/1000 | Loss: 0.00001848
Iteration 215/1000 | Loss: 0.00001848
Iteration 216/1000 | Loss: 0.00001848
Iteration 217/1000 | Loss: 0.00001848
Iteration 218/1000 | Loss: 0.00001848
Iteration 219/1000 | Loss: 0.00001848
Iteration 220/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.848414285632316e-05, 1.848414285632316e-05, 1.848414285632316e-05, 1.848414285632316e-05, 1.848414285632316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.848414285632316e-05

Optimization complete. Final v2v error: 3.4738166332244873 mm

Highest mean error: 4.7767205238342285 mm for frame 38

Lowest mean error: 3.0544936656951904 mm for frame 92

Saving results

Total time: 42.14583659172058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755012
Iteration 2/25 | Loss: 0.00152821
Iteration 3/25 | Loss: 0.00125819
Iteration 4/25 | Loss: 0.00124559
Iteration 5/25 | Loss: 0.00124353
Iteration 6/25 | Loss: 0.00124282
Iteration 7/25 | Loss: 0.00124280
Iteration 8/25 | Loss: 0.00124280
Iteration 9/25 | Loss: 0.00124280
Iteration 10/25 | Loss: 0.00124280
Iteration 11/25 | Loss: 0.00124280
Iteration 12/25 | Loss: 0.00124280
Iteration 13/25 | Loss: 0.00124280
Iteration 14/25 | Loss: 0.00124280
Iteration 15/25 | Loss: 0.00124280
Iteration 16/25 | Loss: 0.00124280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001242798287421465, 0.001242798287421465, 0.001242798287421465, 0.001242798287421465, 0.001242798287421465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242798287421465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45180857
Iteration 2/25 | Loss: 0.00076266
Iteration 3/25 | Loss: 0.00076265
Iteration 4/25 | Loss: 0.00076264
Iteration 5/25 | Loss: 0.00076264
Iteration 6/25 | Loss: 0.00076264
Iteration 7/25 | Loss: 0.00076264
Iteration 8/25 | Loss: 0.00076264
Iteration 9/25 | Loss: 0.00076264
Iteration 10/25 | Loss: 0.00076264
Iteration 11/25 | Loss: 0.00076264
Iteration 12/25 | Loss: 0.00076264
Iteration 13/25 | Loss: 0.00076264
Iteration 14/25 | Loss: 0.00076264
Iteration 15/25 | Loss: 0.00076264
Iteration 16/25 | Loss: 0.00076264
Iteration 17/25 | Loss: 0.00076264
Iteration 18/25 | Loss: 0.00076264
Iteration 19/25 | Loss: 0.00076264
Iteration 20/25 | Loss: 0.00076264
Iteration 21/25 | Loss: 0.00076264
Iteration 22/25 | Loss: 0.00076264
Iteration 23/25 | Loss: 0.00076264
Iteration 24/25 | Loss: 0.00076264
Iteration 25/25 | Loss: 0.00076264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076264
Iteration 2/1000 | Loss: 0.00003899
Iteration 3/1000 | Loss: 0.00002441
Iteration 4/1000 | Loss: 0.00001952
Iteration 5/1000 | Loss: 0.00001829
Iteration 6/1000 | Loss: 0.00001751
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001690
Iteration 9/1000 | Loss: 0.00001653
Iteration 10/1000 | Loss: 0.00001637
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001611
Iteration 22/1000 | Loss: 0.00001609
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001606
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001597
Iteration 40/1000 | Loss: 0.00001597
Iteration 41/1000 | Loss: 0.00001596
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001595
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001592
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001587
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001578
Iteration 68/1000 | Loss: 0.00001578
Iteration 69/1000 | Loss: 0.00001578
Iteration 70/1000 | Loss: 0.00001578
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001576
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001567
Iteration 120/1000 | Loss: 0.00001567
Iteration 121/1000 | Loss: 0.00001567
Iteration 122/1000 | Loss: 0.00001567
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001566
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001566
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001565
Iteration 157/1000 | Loss: 0.00001565
Iteration 158/1000 | Loss: 0.00001565
Iteration 159/1000 | Loss: 0.00001565
Iteration 160/1000 | Loss: 0.00001565
Iteration 161/1000 | Loss: 0.00001565
Iteration 162/1000 | Loss: 0.00001565
Iteration 163/1000 | Loss: 0.00001565
Iteration 164/1000 | Loss: 0.00001565
Iteration 165/1000 | Loss: 0.00001565
Iteration 166/1000 | Loss: 0.00001565
Iteration 167/1000 | Loss: 0.00001565
Iteration 168/1000 | Loss: 0.00001565
Iteration 169/1000 | Loss: 0.00001565
Iteration 170/1000 | Loss: 0.00001565
Iteration 171/1000 | Loss: 0.00001564
Iteration 172/1000 | Loss: 0.00001564
Iteration 173/1000 | Loss: 0.00001564
Iteration 174/1000 | Loss: 0.00001564
Iteration 175/1000 | Loss: 0.00001564
Iteration 176/1000 | Loss: 0.00001564
Iteration 177/1000 | Loss: 0.00001564
Iteration 178/1000 | Loss: 0.00001564
Iteration 179/1000 | Loss: 0.00001564
Iteration 180/1000 | Loss: 0.00001564
Iteration 181/1000 | Loss: 0.00001564
Iteration 182/1000 | Loss: 0.00001564
Iteration 183/1000 | Loss: 0.00001564
Iteration 184/1000 | Loss: 0.00001564
Iteration 185/1000 | Loss: 0.00001564
Iteration 186/1000 | Loss: 0.00001564
Iteration 187/1000 | Loss: 0.00001564
Iteration 188/1000 | Loss: 0.00001564
Iteration 189/1000 | Loss: 0.00001564
Iteration 190/1000 | Loss: 0.00001564
Iteration 191/1000 | Loss: 0.00001564
Iteration 192/1000 | Loss: 0.00001564
Iteration 193/1000 | Loss: 0.00001564
Iteration 194/1000 | Loss: 0.00001564
Iteration 195/1000 | Loss: 0.00001564
Iteration 196/1000 | Loss: 0.00001564
Iteration 197/1000 | Loss: 0.00001564
Iteration 198/1000 | Loss: 0.00001564
Iteration 199/1000 | Loss: 0.00001564
Iteration 200/1000 | Loss: 0.00001564
Iteration 201/1000 | Loss: 0.00001564
Iteration 202/1000 | Loss: 0.00001564
Iteration 203/1000 | Loss: 0.00001564
Iteration 204/1000 | Loss: 0.00001564
Iteration 205/1000 | Loss: 0.00001564
Iteration 206/1000 | Loss: 0.00001564
Iteration 207/1000 | Loss: 0.00001564
Iteration 208/1000 | Loss: 0.00001564
Iteration 209/1000 | Loss: 0.00001564
Iteration 210/1000 | Loss: 0.00001564
Iteration 211/1000 | Loss: 0.00001564
Iteration 212/1000 | Loss: 0.00001564
Iteration 213/1000 | Loss: 0.00001564
Iteration 214/1000 | Loss: 0.00001564
Iteration 215/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.563672412885353e-05, 1.563672412885353e-05, 1.563672412885353e-05, 1.563672412885353e-05, 1.563672412885353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.563672412885353e-05

Optimization complete. Final v2v error: 3.3206946849823 mm

Highest mean error: 3.7241804599761963 mm for frame 109

Lowest mean error: 3.079385280609131 mm for frame 17

Saving results

Total time: 39.45005774497986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884573
Iteration 2/25 | Loss: 0.00162068
Iteration 3/25 | Loss: 0.00144944
Iteration 4/25 | Loss: 0.00124996
Iteration 5/25 | Loss: 0.00122468
Iteration 6/25 | Loss: 0.00122187
Iteration 7/25 | Loss: 0.00120818
Iteration 8/25 | Loss: 0.00120517
Iteration 9/25 | Loss: 0.00120410
Iteration 10/25 | Loss: 0.00120367
Iteration 11/25 | Loss: 0.00120354
Iteration 12/25 | Loss: 0.00120352
Iteration 13/25 | Loss: 0.00120352
Iteration 14/25 | Loss: 0.00120352
Iteration 15/25 | Loss: 0.00120351
Iteration 16/25 | Loss: 0.00120351
Iteration 17/25 | Loss: 0.00120351
Iteration 18/25 | Loss: 0.00120351
Iteration 19/25 | Loss: 0.00120351
Iteration 20/25 | Loss: 0.00120351
Iteration 21/25 | Loss: 0.00120351
Iteration 22/25 | Loss: 0.00120351
Iteration 23/25 | Loss: 0.00120351
Iteration 24/25 | Loss: 0.00120351
Iteration 25/25 | Loss: 0.00120351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55883622
Iteration 2/25 | Loss: 0.00067734
Iteration 3/25 | Loss: 0.00067734
Iteration 4/25 | Loss: 0.00067734
Iteration 5/25 | Loss: 0.00067734
Iteration 6/25 | Loss: 0.00067734
Iteration 7/25 | Loss: 0.00067733
Iteration 8/25 | Loss: 0.00067733
Iteration 9/25 | Loss: 0.00067733
Iteration 10/25 | Loss: 0.00067733
Iteration 11/25 | Loss: 0.00067733
Iteration 12/25 | Loss: 0.00067733
Iteration 13/25 | Loss: 0.00067733
Iteration 14/25 | Loss: 0.00067733
Iteration 15/25 | Loss: 0.00067733
Iteration 16/25 | Loss: 0.00067733
Iteration 17/25 | Loss: 0.00067733
Iteration 18/25 | Loss: 0.00067733
Iteration 19/25 | Loss: 0.00067733
Iteration 20/25 | Loss: 0.00067733
Iteration 21/25 | Loss: 0.00067733
Iteration 22/25 | Loss: 0.00067733
Iteration 23/25 | Loss: 0.00067733
Iteration 24/25 | Loss: 0.00067733
Iteration 25/25 | Loss: 0.00067733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067733
Iteration 2/1000 | Loss: 0.00002876
Iteration 3/1000 | Loss: 0.00002060
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001813
Iteration 6/1000 | Loss: 0.00001777
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001721
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001654
Iteration 25/1000 | Loss: 0.00001654
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001650
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001640
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001639
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001637
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001633
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001629
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001619
Iteration 90/1000 | Loss: 0.00001619
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001618
Iteration 93/1000 | Loss: 0.00001618
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001617
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001616
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Iteration 113/1000 | Loss: 0.00001616
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001615
Iteration 121/1000 | Loss: 0.00001615
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001614
Iteration 131/1000 | Loss: 0.00001614
Iteration 132/1000 | Loss: 0.00001614
Iteration 133/1000 | Loss: 0.00001614
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001614
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001613
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001613
Iteration 141/1000 | Loss: 0.00001613
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001613
Iteration 144/1000 | Loss: 0.00001613
Iteration 145/1000 | Loss: 0.00001613
Iteration 146/1000 | Loss: 0.00001613
Iteration 147/1000 | Loss: 0.00001613
Iteration 148/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.61265124916099e-05, 1.61265124916099e-05, 1.61265124916099e-05, 1.61265124916099e-05, 1.61265124916099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.61265124916099e-05

Optimization complete. Final v2v error: 3.3347885608673096 mm

Highest mean error: 3.9631757736206055 mm for frame 200

Lowest mean error: 2.7775189876556396 mm for frame 31

Saving results

Total time: 54.719985246658325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975087
Iteration 2/25 | Loss: 0.00239789
Iteration 3/25 | Loss: 0.00187683
Iteration 4/25 | Loss: 0.00182232
Iteration 5/25 | Loss: 0.00180327
Iteration 6/25 | Loss: 0.00179536
Iteration 7/25 | Loss: 0.00179512
Iteration 8/25 | Loss: 0.00179317
Iteration 9/25 | Loss: 0.00179210
Iteration 10/25 | Loss: 0.00178839
Iteration 11/25 | Loss: 0.00178420
Iteration 12/25 | Loss: 0.00178343
Iteration 13/25 | Loss: 0.00178332
Iteration 14/25 | Loss: 0.00178331
Iteration 15/25 | Loss: 0.00178331
Iteration 16/25 | Loss: 0.00178331
Iteration 17/25 | Loss: 0.00178330
Iteration 18/25 | Loss: 0.00178330
Iteration 19/25 | Loss: 0.00178330
Iteration 20/25 | Loss: 0.00178330
Iteration 21/25 | Loss: 0.00178330
Iteration 22/25 | Loss: 0.00178330
Iteration 23/25 | Loss: 0.00178329
Iteration 24/25 | Loss: 0.00178329
Iteration 25/25 | Loss: 0.00178327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42978609
Iteration 2/25 | Loss: 0.00404370
Iteration 3/25 | Loss: 0.00404369
Iteration 4/25 | Loss: 0.00402646
Iteration 5/25 | Loss: 0.00402643
Iteration 6/25 | Loss: 0.00402643
Iteration 7/25 | Loss: 0.00402643
Iteration 8/25 | Loss: 0.00402643
Iteration 9/25 | Loss: 0.00402643
Iteration 10/25 | Loss: 0.00402643
Iteration 11/25 | Loss: 0.00402643
Iteration 12/25 | Loss: 0.00402643
Iteration 13/25 | Loss: 0.00402643
Iteration 14/25 | Loss: 0.00402643
Iteration 15/25 | Loss: 0.00402643
Iteration 16/25 | Loss: 0.00402643
Iteration 17/25 | Loss: 0.00402643
Iteration 18/25 | Loss: 0.00402643
Iteration 19/25 | Loss: 0.00402643
Iteration 20/25 | Loss: 0.00402643
Iteration 21/25 | Loss: 0.00402643
Iteration 22/25 | Loss: 0.00402643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004026427865028381, 0.004026427865028381, 0.004026427865028381, 0.004026427865028381, 0.004026427865028381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004026427865028381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00402643
Iteration 2/1000 | Loss: 0.00056740
Iteration 3/1000 | Loss: 0.00041204
Iteration 4/1000 | Loss: 0.00035932
Iteration 5/1000 | Loss: 0.00032863
Iteration 6/1000 | Loss: 0.00031325
Iteration 7/1000 | Loss: 0.00029518
Iteration 8/1000 | Loss: 0.00028293
Iteration 9/1000 | Loss: 0.00027446
Iteration 10/1000 | Loss: 0.00026536
Iteration 11/1000 | Loss: 0.00056767
Iteration 12/1000 | Loss: 0.00132933
Iteration 13/1000 | Loss: 0.01581459
Iteration 14/1000 | Loss: 0.00053761
Iteration 15/1000 | Loss: 0.00036890
Iteration 16/1000 | Loss: 0.00037299
Iteration 17/1000 | Loss: 0.00019797
Iteration 18/1000 | Loss: 0.00014338
Iteration 19/1000 | Loss: 0.00009652
Iteration 20/1000 | Loss: 0.00010039
Iteration 21/1000 | Loss: 0.00006956
Iteration 22/1000 | Loss: 0.00005832
Iteration 23/1000 | Loss: 0.00006854
Iteration 24/1000 | Loss: 0.00004140
Iteration 25/1000 | Loss: 0.00003638
Iteration 26/1000 | Loss: 0.00003136
Iteration 27/1000 | Loss: 0.00002755
Iteration 28/1000 | Loss: 0.00002495
Iteration 29/1000 | Loss: 0.00002298
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001776
Iteration 41/1000 | Loss: 0.00001768
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001759
Iteration 51/1000 | Loss: 0.00001759
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001759
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001759
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001754
Iteration 65/1000 | Loss: 0.00001754
Iteration 66/1000 | Loss: 0.00001754
Iteration 67/1000 | Loss: 0.00001754
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001753
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001750
Iteration 93/1000 | Loss: 0.00001750
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001749
Iteration 100/1000 | Loss: 0.00001749
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001748
Iteration 105/1000 | Loss: 0.00001748
Iteration 106/1000 | Loss: 0.00001748
Iteration 107/1000 | Loss: 0.00001748
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001747
Iteration 110/1000 | Loss: 0.00001747
Iteration 111/1000 | Loss: 0.00001747
Iteration 112/1000 | Loss: 0.00001747
Iteration 113/1000 | Loss: 0.00001747
Iteration 114/1000 | Loss: 0.00001747
Iteration 115/1000 | Loss: 0.00001747
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001747
Iteration 119/1000 | Loss: 0.00001747
Iteration 120/1000 | Loss: 0.00001747
Iteration 121/1000 | Loss: 0.00001747
Iteration 122/1000 | Loss: 0.00001746
Iteration 123/1000 | Loss: 0.00001746
Iteration 124/1000 | Loss: 0.00001746
Iteration 125/1000 | Loss: 0.00001746
Iteration 126/1000 | Loss: 0.00001746
Iteration 127/1000 | Loss: 0.00001746
Iteration 128/1000 | Loss: 0.00001746
Iteration 129/1000 | Loss: 0.00001746
Iteration 130/1000 | Loss: 0.00001746
Iteration 131/1000 | Loss: 0.00001746
Iteration 132/1000 | Loss: 0.00001746
Iteration 133/1000 | Loss: 0.00001745
Iteration 134/1000 | Loss: 0.00001745
Iteration 135/1000 | Loss: 0.00001745
Iteration 136/1000 | Loss: 0.00001745
Iteration 137/1000 | Loss: 0.00001745
Iteration 138/1000 | Loss: 0.00001745
Iteration 139/1000 | Loss: 0.00001745
Iteration 140/1000 | Loss: 0.00001745
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001745
Iteration 148/1000 | Loss: 0.00001745
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.744685687299352e-05, 1.744685687299352e-05, 1.744685687299352e-05, 1.744685687299352e-05, 1.744685687299352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.744685687299352e-05

Optimization complete. Final v2v error: 3.499980926513672 mm

Highest mean error: 3.6757936477661133 mm for frame 186

Lowest mean error: 3.3671762943267822 mm for frame 6

Saving results

Total time: 99.10441064834595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971352
Iteration 2/25 | Loss: 0.00269233
Iteration 3/25 | Loss: 0.00192114
Iteration 4/25 | Loss: 0.00172026
Iteration 5/25 | Loss: 0.00156177
Iteration 6/25 | Loss: 0.00141299
Iteration 7/25 | Loss: 0.00135656
Iteration 8/25 | Loss: 0.00133006
Iteration 9/25 | Loss: 0.00131758
Iteration 10/25 | Loss: 0.00128804
Iteration 11/25 | Loss: 0.00129032
Iteration 12/25 | Loss: 0.00125690
Iteration 13/25 | Loss: 0.00124597
Iteration 14/25 | Loss: 0.00124465
Iteration 15/25 | Loss: 0.00124438
Iteration 16/25 | Loss: 0.00123081
Iteration 17/25 | Loss: 0.00122636
Iteration 18/25 | Loss: 0.00122533
Iteration 19/25 | Loss: 0.00122524
Iteration 20/25 | Loss: 0.00122520
Iteration 21/25 | Loss: 0.00122519
Iteration 22/25 | Loss: 0.00122519
Iteration 23/25 | Loss: 0.00122509
Iteration 24/25 | Loss: 0.00122586
Iteration 25/25 | Loss: 0.00122476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44276559
Iteration 2/25 | Loss: 0.00050226
Iteration 3/25 | Loss: 0.00050226
Iteration 4/25 | Loss: 0.00050226
Iteration 5/25 | Loss: 0.00050226
Iteration 6/25 | Loss: 0.00050226
Iteration 7/25 | Loss: 0.00050226
Iteration 8/25 | Loss: 0.00050225
Iteration 9/25 | Loss: 0.00050225
Iteration 10/25 | Loss: 0.00050225
Iteration 11/25 | Loss: 0.00050225
Iteration 12/25 | Loss: 0.00050225
Iteration 13/25 | Loss: 0.00050225
Iteration 14/25 | Loss: 0.00050225
Iteration 15/25 | Loss: 0.00050225
Iteration 16/25 | Loss: 0.00050225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005022543482482433, 0.0005022543482482433, 0.0005022543482482433, 0.0005022543482482433, 0.0005022543482482433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005022543482482433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050225
Iteration 2/1000 | Loss: 0.00016082
Iteration 3/1000 | Loss: 0.00006255
Iteration 4/1000 | Loss: 0.00005229
Iteration 5/1000 | Loss: 0.00003746
Iteration 6/1000 | Loss: 0.00021058
Iteration 7/1000 | Loss: 0.00039965
Iteration 8/1000 | Loss: 0.00032863
Iteration 9/1000 | Loss: 0.00008193
Iteration 10/1000 | Loss: 0.00003268
Iteration 11/1000 | Loss: 0.00002949
Iteration 12/1000 | Loss: 0.00002751
Iteration 13/1000 | Loss: 0.00002603
Iteration 14/1000 | Loss: 0.00002491
Iteration 15/1000 | Loss: 0.00003156
Iteration 16/1000 | Loss: 0.00003618
Iteration 17/1000 | Loss: 0.00002604
Iteration 18/1000 | Loss: 0.00002897
Iteration 19/1000 | Loss: 0.00002320
Iteration 20/1000 | Loss: 0.00002638
Iteration 21/1000 | Loss: 0.00002284
Iteration 22/1000 | Loss: 0.00002916
Iteration 23/1000 | Loss: 0.00003347
Iteration 24/1000 | Loss: 0.00002239
Iteration 25/1000 | Loss: 0.00002438
Iteration 26/1000 | Loss: 0.00002235
Iteration 27/1000 | Loss: 0.00003172
Iteration 28/1000 | Loss: 0.00002226
Iteration 29/1000 | Loss: 0.00002226
Iteration 30/1000 | Loss: 0.00002224
Iteration 31/1000 | Loss: 0.00002223
Iteration 32/1000 | Loss: 0.00002223
Iteration 33/1000 | Loss: 0.00002222
Iteration 34/1000 | Loss: 0.00002222
Iteration 35/1000 | Loss: 0.00002221
Iteration 36/1000 | Loss: 0.00002817
Iteration 37/1000 | Loss: 0.00002221
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002215
Iteration 42/1000 | Loss: 0.00002215
Iteration 43/1000 | Loss: 0.00002215
Iteration 44/1000 | Loss: 0.00002214
Iteration 45/1000 | Loss: 0.00002214
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002214
Iteration 49/1000 | Loss: 0.00002214
Iteration 50/1000 | Loss: 0.00002214
Iteration 51/1000 | Loss: 0.00002214
Iteration 52/1000 | Loss: 0.00002213
Iteration 53/1000 | Loss: 0.00002208
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002867
Iteration 56/1000 | Loss: 0.00002720
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00002197
Iteration 59/1000 | Loss: 0.00002197
Iteration 60/1000 | Loss: 0.00002197
Iteration 61/1000 | Loss: 0.00002197
Iteration 62/1000 | Loss: 0.00002197
Iteration 63/1000 | Loss: 0.00002197
Iteration 64/1000 | Loss: 0.00002197
Iteration 65/1000 | Loss: 0.00002197
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002196
Iteration 68/1000 | Loss: 0.00002196
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002196
Iteration 71/1000 | Loss: 0.00002196
Iteration 72/1000 | Loss: 0.00002196
Iteration 73/1000 | Loss: 0.00002196
Iteration 74/1000 | Loss: 0.00002196
Iteration 75/1000 | Loss: 0.00002196
Iteration 76/1000 | Loss: 0.00002196
Iteration 77/1000 | Loss: 0.00002196
Iteration 78/1000 | Loss: 0.00002196
Iteration 79/1000 | Loss: 0.00002196
Iteration 80/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.1958212528261356e-05, 2.1958212528261356e-05, 2.1958212528261356e-05, 2.1958212528261356e-05, 2.1958212528261356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1958212528261356e-05

Optimization complete. Final v2v error: 3.8906941413879395 mm

Highest mean error: 4.693706035614014 mm for frame 156

Lowest mean error: 3.4678571224212646 mm for frame 65

Saving results

Total time: 98.120689868927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742101
Iteration 2/25 | Loss: 0.00154025
Iteration 3/25 | Loss: 0.00139730
Iteration 4/25 | Loss: 0.00138988
Iteration 5/25 | Loss: 0.00138838
Iteration 6/25 | Loss: 0.00138838
Iteration 7/25 | Loss: 0.00138838
Iteration 8/25 | Loss: 0.00138838
Iteration 9/25 | Loss: 0.00138838
Iteration 10/25 | Loss: 0.00138838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013883787905797362, 0.0013883787905797362, 0.0013883787905797362, 0.0013883787905797362, 0.0013883787905797362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013883787905797362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.83482265
Iteration 2/25 | Loss: 0.00079562
Iteration 3/25 | Loss: 0.00079561
Iteration 4/25 | Loss: 0.00079561
Iteration 5/25 | Loss: 0.00079561
Iteration 6/25 | Loss: 0.00079561
Iteration 7/25 | Loss: 0.00079561
Iteration 8/25 | Loss: 0.00079561
Iteration 9/25 | Loss: 0.00079561
Iteration 10/25 | Loss: 0.00079561
Iteration 11/25 | Loss: 0.00079561
Iteration 12/25 | Loss: 0.00079561
Iteration 13/25 | Loss: 0.00079561
Iteration 14/25 | Loss: 0.00079561
Iteration 15/25 | Loss: 0.00079561
Iteration 16/25 | Loss: 0.00079561
Iteration 17/25 | Loss: 0.00079561
Iteration 18/25 | Loss: 0.00079561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007956127519719303, 0.0007956127519719303, 0.0007956127519719303, 0.0007956127519719303, 0.0007956127519719303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007956127519719303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079561
Iteration 2/1000 | Loss: 0.00005152
Iteration 3/1000 | Loss: 0.00003307
Iteration 4/1000 | Loss: 0.00002921
Iteration 5/1000 | Loss: 0.00002799
Iteration 6/1000 | Loss: 0.00002732
Iteration 7/1000 | Loss: 0.00002675
Iteration 8/1000 | Loss: 0.00002642
Iteration 9/1000 | Loss: 0.00002607
Iteration 10/1000 | Loss: 0.00002582
Iteration 11/1000 | Loss: 0.00002577
Iteration 12/1000 | Loss: 0.00002557
Iteration 13/1000 | Loss: 0.00002547
Iteration 14/1000 | Loss: 0.00002532
Iteration 15/1000 | Loss: 0.00002518
Iteration 16/1000 | Loss: 0.00002516
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00002512
Iteration 19/1000 | Loss: 0.00002511
Iteration 20/1000 | Loss: 0.00002505
Iteration 21/1000 | Loss: 0.00002503
Iteration 22/1000 | Loss: 0.00002502
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002501
Iteration 25/1000 | Loss: 0.00002501
Iteration 26/1000 | Loss: 0.00002500
Iteration 27/1000 | Loss: 0.00002499
Iteration 28/1000 | Loss: 0.00002498
Iteration 29/1000 | Loss: 0.00002498
Iteration 30/1000 | Loss: 0.00002497
Iteration 31/1000 | Loss: 0.00002496
Iteration 32/1000 | Loss: 0.00002495
Iteration 33/1000 | Loss: 0.00002494
Iteration 34/1000 | Loss: 0.00002493
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002489
Iteration 37/1000 | Loss: 0.00002487
Iteration 38/1000 | Loss: 0.00002487
Iteration 39/1000 | Loss: 0.00002487
Iteration 40/1000 | Loss: 0.00002486
Iteration 41/1000 | Loss: 0.00002486
Iteration 42/1000 | Loss: 0.00002486
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00002484
Iteration 45/1000 | Loss: 0.00002483
Iteration 46/1000 | Loss: 0.00002482
Iteration 47/1000 | Loss: 0.00002482
Iteration 48/1000 | Loss: 0.00002482
Iteration 49/1000 | Loss: 0.00002481
Iteration 50/1000 | Loss: 0.00002481
Iteration 51/1000 | Loss: 0.00002481
Iteration 52/1000 | Loss: 0.00002479
Iteration 53/1000 | Loss: 0.00002477
Iteration 54/1000 | Loss: 0.00002477
Iteration 55/1000 | Loss: 0.00002477
Iteration 56/1000 | Loss: 0.00002477
Iteration 57/1000 | Loss: 0.00002477
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00002476
Iteration 61/1000 | Loss: 0.00002476
Iteration 62/1000 | Loss: 0.00002476
Iteration 63/1000 | Loss: 0.00002476
Iteration 64/1000 | Loss: 0.00002475
Iteration 65/1000 | Loss: 0.00002475
Iteration 66/1000 | Loss: 0.00002475
Iteration 67/1000 | Loss: 0.00002474
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002474
Iteration 74/1000 | Loss: 0.00002474
Iteration 75/1000 | Loss: 0.00002473
Iteration 76/1000 | Loss: 0.00002473
Iteration 77/1000 | Loss: 0.00002473
Iteration 78/1000 | Loss: 0.00002473
Iteration 79/1000 | Loss: 0.00002473
Iteration 80/1000 | Loss: 0.00002472
Iteration 81/1000 | Loss: 0.00002472
Iteration 82/1000 | Loss: 0.00002472
Iteration 83/1000 | Loss: 0.00002472
Iteration 84/1000 | Loss: 0.00002472
Iteration 85/1000 | Loss: 0.00002472
Iteration 86/1000 | Loss: 0.00002472
Iteration 87/1000 | Loss: 0.00002471
Iteration 88/1000 | Loss: 0.00002471
Iteration 89/1000 | Loss: 0.00002471
Iteration 90/1000 | Loss: 0.00002471
Iteration 91/1000 | Loss: 0.00002471
Iteration 92/1000 | Loss: 0.00002471
Iteration 93/1000 | Loss: 0.00002471
Iteration 94/1000 | Loss: 0.00002471
Iteration 95/1000 | Loss: 0.00002471
Iteration 96/1000 | Loss: 0.00002471
Iteration 97/1000 | Loss: 0.00002471
Iteration 98/1000 | Loss: 0.00002471
Iteration 99/1000 | Loss: 0.00002470
Iteration 100/1000 | Loss: 0.00002470
Iteration 101/1000 | Loss: 0.00002470
Iteration 102/1000 | Loss: 0.00002470
Iteration 103/1000 | Loss: 0.00002470
Iteration 104/1000 | Loss: 0.00002470
Iteration 105/1000 | Loss: 0.00002470
Iteration 106/1000 | Loss: 0.00002470
Iteration 107/1000 | Loss: 0.00002470
Iteration 108/1000 | Loss: 0.00002470
Iteration 109/1000 | Loss: 0.00002470
Iteration 110/1000 | Loss: 0.00002470
Iteration 111/1000 | Loss: 0.00002469
Iteration 112/1000 | Loss: 0.00002469
Iteration 113/1000 | Loss: 0.00002469
Iteration 114/1000 | Loss: 0.00002469
Iteration 115/1000 | Loss: 0.00002469
Iteration 116/1000 | Loss: 0.00002469
Iteration 117/1000 | Loss: 0.00002469
Iteration 118/1000 | Loss: 0.00002469
Iteration 119/1000 | Loss: 0.00002469
Iteration 120/1000 | Loss: 0.00002469
Iteration 121/1000 | Loss: 0.00002469
Iteration 122/1000 | Loss: 0.00002469
Iteration 123/1000 | Loss: 0.00002469
Iteration 124/1000 | Loss: 0.00002469
Iteration 125/1000 | Loss: 0.00002469
Iteration 126/1000 | Loss: 0.00002468
Iteration 127/1000 | Loss: 0.00002468
Iteration 128/1000 | Loss: 0.00002468
Iteration 129/1000 | Loss: 0.00002468
Iteration 130/1000 | Loss: 0.00002468
Iteration 131/1000 | Loss: 0.00002468
Iteration 132/1000 | Loss: 0.00002468
Iteration 133/1000 | Loss: 0.00002468
Iteration 134/1000 | Loss: 0.00002468
Iteration 135/1000 | Loss: 0.00002468
Iteration 136/1000 | Loss: 0.00002468
Iteration 137/1000 | Loss: 0.00002468
Iteration 138/1000 | Loss: 0.00002468
Iteration 139/1000 | Loss: 0.00002468
Iteration 140/1000 | Loss: 0.00002468
Iteration 141/1000 | Loss: 0.00002468
Iteration 142/1000 | Loss: 0.00002468
Iteration 143/1000 | Loss: 0.00002468
Iteration 144/1000 | Loss: 0.00002468
Iteration 145/1000 | Loss: 0.00002468
Iteration 146/1000 | Loss: 0.00002467
Iteration 147/1000 | Loss: 0.00002467
Iteration 148/1000 | Loss: 0.00002467
Iteration 149/1000 | Loss: 0.00002467
Iteration 150/1000 | Loss: 0.00002467
Iteration 151/1000 | Loss: 0.00002467
Iteration 152/1000 | Loss: 0.00002467
Iteration 153/1000 | Loss: 0.00002467
Iteration 154/1000 | Loss: 0.00002467
Iteration 155/1000 | Loss: 0.00002467
Iteration 156/1000 | Loss: 0.00002467
Iteration 157/1000 | Loss: 0.00002467
Iteration 158/1000 | Loss: 0.00002467
Iteration 159/1000 | Loss: 0.00002467
Iteration 160/1000 | Loss: 0.00002467
Iteration 161/1000 | Loss: 0.00002466
Iteration 162/1000 | Loss: 0.00002466
Iteration 163/1000 | Loss: 0.00002466
Iteration 164/1000 | Loss: 0.00002466
Iteration 165/1000 | Loss: 0.00002466
Iteration 166/1000 | Loss: 0.00002466
Iteration 167/1000 | Loss: 0.00002466
Iteration 168/1000 | Loss: 0.00002466
Iteration 169/1000 | Loss: 0.00002466
Iteration 170/1000 | Loss: 0.00002466
Iteration 171/1000 | Loss: 0.00002466
Iteration 172/1000 | Loss: 0.00002466
Iteration 173/1000 | Loss: 0.00002466
Iteration 174/1000 | Loss: 0.00002466
Iteration 175/1000 | Loss: 0.00002466
Iteration 176/1000 | Loss: 0.00002466
Iteration 177/1000 | Loss: 0.00002466
Iteration 178/1000 | Loss: 0.00002466
Iteration 179/1000 | Loss: 0.00002466
Iteration 180/1000 | Loss: 0.00002466
Iteration 181/1000 | Loss: 0.00002466
Iteration 182/1000 | Loss: 0.00002466
Iteration 183/1000 | Loss: 0.00002466
Iteration 184/1000 | Loss: 0.00002466
Iteration 185/1000 | Loss: 0.00002466
Iteration 186/1000 | Loss: 0.00002466
Iteration 187/1000 | Loss: 0.00002466
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.4663228032295592e-05, 2.4663228032295592e-05, 2.4663228032295592e-05, 2.4663228032295592e-05, 2.4663228032295592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4663228032295592e-05

Optimization complete. Final v2v error: 4.012808322906494 mm

Highest mean error: 4.57790994644165 mm for frame 166

Lowest mean error: 3.3919856548309326 mm for frame 239

Saving results

Total time: 45.21211767196655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467035
Iteration 2/25 | Loss: 0.00130781
Iteration 3/25 | Loss: 0.00121133
Iteration 4/25 | Loss: 0.00119752
Iteration 5/25 | Loss: 0.00119396
Iteration 6/25 | Loss: 0.00119392
Iteration 7/25 | Loss: 0.00119392
Iteration 8/25 | Loss: 0.00119392
Iteration 9/25 | Loss: 0.00119392
Iteration 10/25 | Loss: 0.00119392
Iteration 11/25 | Loss: 0.00119392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011939192190766335, 0.0011939192190766335, 0.0011939192190766335, 0.0011939192190766335, 0.0011939192190766335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011939192190766335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.82179308
Iteration 2/25 | Loss: 0.00067281
Iteration 3/25 | Loss: 0.00067280
Iteration 4/25 | Loss: 0.00067280
Iteration 5/25 | Loss: 0.00067280
Iteration 6/25 | Loss: 0.00067280
Iteration 7/25 | Loss: 0.00067280
Iteration 8/25 | Loss: 0.00067280
Iteration 9/25 | Loss: 0.00067280
Iteration 10/25 | Loss: 0.00067280
Iteration 11/25 | Loss: 0.00067280
Iteration 12/25 | Loss: 0.00067280
Iteration 13/25 | Loss: 0.00067280
Iteration 14/25 | Loss: 0.00067280
Iteration 15/25 | Loss: 0.00067280
Iteration 16/25 | Loss: 0.00067280
Iteration 17/25 | Loss: 0.00067280
Iteration 18/25 | Loss: 0.00067280
Iteration 19/25 | Loss: 0.00067280
Iteration 20/25 | Loss: 0.00067280
Iteration 21/25 | Loss: 0.00067280
Iteration 22/25 | Loss: 0.00067280
Iteration 23/25 | Loss: 0.00067280
Iteration 24/25 | Loss: 0.00067280
Iteration 25/25 | Loss: 0.00067280

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067280
Iteration 2/1000 | Loss: 0.00002802
Iteration 3/1000 | Loss: 0.00001872
Iteration 4/1000 | Loss: 0.00001664
Iteration 5/1000 | Loss: 0.00001569
Iteration 6/1000 | Loss: 0.00001512
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001446
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001393
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001372
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001369
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001367
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001367
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.3668758583662566e-05, 1.3668758583662566e-05, 1.3668758583662566e-05, 1.3668758583662566e-05, 1.3668758583662566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3668758583662566e-05

Optimization complete. Final v2v error: 3.1558616161346436 mm

Highest mean error: 3.563852548599243 mm for frame 134

Lowest mean error: 2.878573179244995 mm for frame 156

Saving results

Total time: 29.86523461341858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418804
Iteration 2/25 | Loss: 0.00129093
Iteration 3/25 | Loss: 0.00120515
Iteration 4/25 | Loss: 0.00118476
Iteration 5/25 | Loss: 0.00117925
Iteration 6/25 | Loss: 0.00117890
Iteration 7/25 | Loss: 0.00117890
Iteration 8/25 | Loss: 0.00117890
Iteration 9/25 | Loss: 0.00117890
Iteration 10/25 | Loss: 0.00117890
Iteration 11/25 | Loss: 0.00117890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011788955889642239, 0.0011788955889642239, 0.0011788955889642239, 0.0011788955889642239, 0.0011788955889642239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011788955889642239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46666861
Iteration 2/25 | Loss: 0.00060270
Iteration 3/25 | Loss: 0.00060270
Iteration 4/25 | Loss: 0.00060270
Iteration 5/25 | Loss: 0.00060270
Iteration 6/25 | Loss: 0.00060270
Iteration 7/25 | Loss: 0.00060270
Iteration 8/25 | Loss: 0.00060270
Iteration 9/25 | Loss: 0.00060270
Iteration 10/25 | Loss: 0.00060270
Iteration 11/25 | Loss: 0.00060270
Iteration 12/25 | Loss: 0.00060270
Iteration 13/25 | Loss: 0.00060270
Iteration 14/25 | Loss: 0.00060270
Iteration 15/25 | Loss: 0.00060270
Iteration 16/25 | Loss: 0.00060270
Iteration 17/25 | Loss: 0.00060270
Iteration 18/25 | Loss: 0.00060270
Iteration 19/25 | Loss: 0.00060270
Iteration 20/25 | Loss: 0.00060270
Iteration 21/25 | Loss: 0.00060270
Iteration 22/25 | Loss: 0.00060270
Iteration 23/25 | Loss: 0.00060270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006026953342370689, 0.0006026953342370689, 0.0006026953342370689, 0.0006026953342370689, 0.0006026953342370689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006026953342370689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060270
Iteration 2/1000 | Loss: 0.00002760
Iteration 3/1000 | Loss: 0.00001904
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001571
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001531
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001526
Iteration 22/1000 | Loss: 0.00001526
Iteration 23/1000 | Loss: 0.00001526
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001518
Iteration 31/1000 | Loss: 0.00001518
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001510
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001502
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001501
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001499
Iteration 72/1000 | Loss: 0.00001499
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001498
Iteration 75/1000 | Loss: 0.00001498
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001498
Iteration 80/1000 | Loss: 0.00001498
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001497
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001497
Iteration 88/1000 | Loss: 0.00001497
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001495
Iteration 100/1000 | Loss: 0.00001495
Iteration 101/1000 | Loss: 0.00001495
Iteration 102/1000 | Loss: 0.00001495
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001493
Iteration 107/1000 | Loss: 0.00001493
Iteration 108/1000 | Loss: 0.00001493
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001492
Iteration 112/1000 | Loss: 0.00001492
Iteration 113/1000 | Loss: 0.00001491
Iteration 114/1000 | Loss: 0.00001491
Iteration 115/1000 | Loss: 0.00001491
Iteration 116/1000 | Loss: 0.00001491
Iteration 117/1000 | Loss: 0.00001491
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001489
Iteration 128/1000 | Loss: 0.00001489
Iteration 129/1000 | Loss: 0.00001489
Iteration 130/1000 | Loss: 0.00001489
Iteration 131/1000 | Loss: 0.00001489
Iteration 132/1000 | Loss: 0.00001489
Iteration 133/1000 | Loss: 0.00001489
Iteration 134/1000 | Loss: 0.00001489
Iteration 135/1000 | Loss: 0.00001489
Iteration 136/1000 | Loss: 0.00001489
Iteration 137/1000 | Loss: 0.00001488
Iteration 138/1000 | Loss: 0.00001488
Iteration 139/1000 | Loss: 0.00001488
Iteration 140/1000 | Loss: 0.00001488
Iteration 141/1000 | Loss: 0.00001488
Iteration 142/1000 | Loss: 0.00001488
Iteration 143/1000 | Loss: 0.00001488
Iteration 144/1000 | Loss: 0.00001488
Iteration 145/1000 | Loss: 0.00001488
Iteration 146/1000 | Loss: 0.00001488
Iteration 147/1000 | Loss: 0.00001488
Iteration 148/1000 | Loss: 0.00001488
Iteration 149/1000 | Loss: 0.00001488
Iteration 150/1000 | Loss: 0.00001488
Iteration 151/1000 | Loss: 0.00001488
Iteration 152/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.4881049537507351e-05, 1.4881049537507351e-05, 1.4881049537507351e-05, 1.4881049537507351e-05, 1.4881049537507351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4881049537507351e-05

Optimization complete. Final v2v error: 3.293968915939331 mm

Highest mean error: 3.419212818145752 mm for frame 120

Lowest mean error: 3.2067148685455322 mm for frame 109

Saving results

Total time: 37.97815442085266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536628
Iteration 2/25 | Loss: 0.00145584
Iteration 3/25 | Loss: 0.00128783
Iteration 4/25 | Loss: 0.00127637
Iteration 5/25 | Loss: 0.00127265
Iteration 6/25 | Loss: 0.00127265
Iteration 7/25 | Loss: 0.00127265
Iteration 8/25 | Loss: 0.00127265
Iteration 9/25 | Loss: 0.00127265
Iteration 10/25 | Loss: 0.00127265
Iteration 11/25 | Loss: 0.00127260
Iteration 12/25 | Loss: 0.00127260
Iteration 13/25 | Loss: 0.00127260
Iteration 14/25 | Loss: 0.00127260
Iteration 15/25 | Loss: 0.00127260
Iteration 16/25 | Loss: 0.00127260
Iteration 17/25 | Loss: 0.00127260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012726025888696313, 0.0012726025888696313, 0.0012726025888696313, 0.0012726025888696313, 0.0012726025888696313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012726025888696313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83399791
Iteration 2/25 | Loss: 0.00072937
Iteration 3/25 | Loss: 0.00072937
Iteration 4/25 | Loss: 0.00072936
Iteration 5/25 | Loss: 0.00072936
Iteration 6/25 | Loss: 0.00072936
Iteration 7/25 | Loss: 0.00072936
Iteration 8/25 | Loss: 0.00072936
Iteration 9/25 | Loss: 0.00072936
Iteration 10/25 | Loss: 0.00072936
Iteration 11/25 | Loss: 0.00072936
Iteration 12/25 | Loss: 0.00072936
Iteration 13/25 | Loss: 0.00072936
Iteration 14/25 | Loss: 0.00072936
Iteration 15/25 | Loss: 0.00072936
Iteration 16/25 | Loss: 0.00072936
Iteration 17/25 | Loss: 0.00072936
Iteration 18/25 | Loss: 0.00072936
Iteration 19/25 | Loss: 0.00072936
Iteration 20/25 | Loss: 0.00072936
Iteration 21/25 | Loss: 0.00072936
Iteration 22/25 | Loss: 0.00072936
Iteration 23/25 | Loss: 0.00072936
Iteration 24/25 | Loss: 0.00072936
Iteration 25/25 | Loss: 0.00072936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072936
Iteration 2/1000 | Loss: 0.00005035
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002029
Iteration 6/1000 | Loss: 0.00001935
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001779
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00001732
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001692
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001687
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001647
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001633
Iteration 24/1000 | Loss: 0.00001630
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001621
Iteration 31/1000 | Loss: 0.00001621
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001617
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001611
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001610
Iteration 56/1000 | Loss: 0.00001610
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001604
Iteration 92/1000 | Loss: 0.00001604
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001601
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001599
Iteration 104/1000 | Loss: 0.00001599
Iteration 105/1000 | Loss: 0.00001599
Iteration 106/1000 | Loss: 0.00001599
Iteration 107/1000 | Loss: 0.00001598
Iteration 108/1000 | Loss: 0.00001598
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001597
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001597
Iteration 113/1000 | Loss: 0.00001597
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001596
Iteration 116/1000 | Loss: 0.00001596
Iteration 117/1000 | Loss: 0.00001596
Iteration 118/1000 | Loss: 0.00001596
Iteration 119/1000 | Loss: 0.00001596
Iteration 120/1000 | Loss: 0.00001596
Iteration 121/1000 | Loss: 0.00001596
Iteration 122/1000 | Loss: 0.00001595
Iteration 123/1000 | Loss: 0.00001595
Iteration 124/1000 | Loss: 0.00001595
Iteration 125/1000 | Loss: 0.00001595
Iteration 126/1000 | Loss: 0.00001595
Iteration 127/1000 | Loss: 0.00001595
Iteration 128/1000 | Loss: 0.00001595
Iteration 129/1000 | Loss: 0.00001595
Iteration 130/1000 | Loss: 0.00001595
Iteration 131/1000 | Loss: 0.00001594
Iteration 132/1000 | Loss: 0.00001594
Iteration 133/1000 | Loss: 0.00001594
Iteration 134/1000 | Loss: 0.00001594
Iteration 135/1000 | Loss: 0.00001594
Iteration 136/1000 | Loss: 0.00001594
Iteration 137/1000 | Loss: 0.00001594
Iteration 138/1000 | Loss: 0.00001593
Iteration 139/1000 | Loss: 0.00001593
Iteration 140/1000 | Loss: 0.00001593
Iteration 141/1000 | Loss: 0.00001593
Iteration 142/1000 | Loss: 0.00001593
Iteration 143/1000 | Loss: 0.00001593
Iteration 144/1000 | Loss: 0.00001593
Iteration 145/1000 | Loss: 0.00001593
Iteration 146/1000 | Loss: 0.00001593
Iteration 147/1000 | Loss: 0.00001593
Iteration 148/1000 | Loss: 0.00001593
Iteration 149/1000 | Loss: 0.00001593
Iteration 150/1000 | Loss: 0.00001593
Iteration 151/1000 | Loss: 0.00001593
Iteration 152/1000 | Loss: 0.00001593
Iteration 153/1000 | Loss: 0.00001592
Iteration 154/1000 | Loss: 0.00001592
Iteration 155/1000 | Loss: 0.00001592
Iteration 156/1000 | Loss: 0.00001592
Iteration 157/1000 | Loss: 0.00001592
Iteration 158/1000 | Loss: 0.00001592
Iteration 159/1000 | Loss: 0.00001591
Iteration 160/1000 | Loss: 0.00001591
Iteration 161/1000 | Loss: 0.00001591
Iteration 162/1000 | Loss: 0.00001591
Iteration 163/1000 | Loss: 0.00001591
Iteration 164/1000 | Loss: 0.00001591
Iteration 165/1000 | Loss: 0.00001591
Iteration 166/1000 | Loss: 0.00001590
Iteration 167/1000 | Loss: 0.00001590
Iteration 168/1000 | Loss: 0.00001589
Iteration 169/1000 | Loss: 0.00001588
Iteration 170/1000 | Loss: 0.00001588
Iteration 171/1000 | Loss: 0.00001588
Iteration 172/1000 | Loss: 0.00001587
Iteration 173/1000 | Loss: 0.00001587
Iteration 174/1000 | Loss: 0.00001586
Iteration 175/1000 | Loss: 0.00001586
Iteration 176/1000 | Loss: 0.00001586
Iteration 177/1000 | Loss: 0.00001586
Iteration 178/1000 | Loss: 0.00001585
Iteration 179/1000 | Loss: 0.00001585
Iteration 180/1000 | Loss: 0.00001585
Iteration 181/1000 | Loss: 0.00001585
Iteration 182/1000 | Loss: 0.00001584
Iteration 183/1000 | Loss: 0.00001584
Iteration 184/1000 | Loss: 0.00001584
Iteration 185/1000 | Loss: 0.00001584
Iteration 186/1000 | Loss: 0.00001583
Iteration 187/1000 | Loss: 0.00001583
Iteration 188/1000 | Loss: 0.00001583
Iteration 189/1000 | Loss: 0.00001583
Iteration 190/1000 | Loss: 0.00001583
Iteration 191/1000 | Loss: 0.00001583
Iteration 192/1000 | Loss: 0.00001583
Iteration 193/1000 | Loss: 0.00001582
Iteration 194/1000 | Loss: 0.00001582
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001582
Iteration 198/1000 | Loss: 0.00001582
Iteration 199/1000 | Loss: 0.00001582
Iteration 200/1000 | Loss: 0.00001582
Iteration 201/1000 | Loss: 0.00001582
Iteration 202/1000 | Loss: 0.00001582
Iteration 203/1000 | Loss: 0.00001581
Iteration 204/1000 | Loss: 0.00001581
Iteration 205/1000 | Loss: 0.00001581
Iteration 206/1000 | Loss: 0.00001581
Iteration 207/1000 | Loss: 0.00001581
Iteration 208/1000 | Loss: 0.00001581
Iteration 209/1000 | Loss: 0.00001581
Iteration 210/1000 | Loss: 0.00001581
Iteration 211/1000 | Loss: 0.00001581
Iteration 212/1000 | Loss: 0.00001581
Iteration 213/1000 | Loss: 0.00001581
Iteration 214/1000 | Loss: 0.00001581
Iteration 215/1000 | Loss: 0.00001581
Iteration 216/1000 | Loss: 0.00001581
Iteration 217/1000 | Loss: 0.00001581
Iteration 218/1000 | Loss: 0.00001580
Iteration 219/1000 | Loss: 0.00001580
Iteration 220/1000 | Loss: 0.00001580
Iteration 221/1000 | Loss: 0.00001580
Iteration 222/1000 | Loss: 0.00001580
Iteration 223/1000 | Loss: 0.00001580
Iteration 224/1000 | Loss: 0.00001580
Iteration 225/1000 | Loss: 0.00001580
Iteration 226/1000 | Loss: 0.00001580
Iteration 227/1000 | Loss: 0.00001580
Iteration 228/1000 | Loss: 0.00001580
Iteration 229/1000 | Loss: 0.00001580
Iteration 230/1000 | Loss: 0.00001580
Iteration 231/1000 | Loss: 0.00001580
Iteration 232/1000 | Loss: 0.00001580
Iteration 233/1000 | Loss: 0.00001580
Iteration 234/1000 | Loss: 0.00001580
Iteration 235/1000 | Loss: 0.00001579
Iteration 236/1000 | Loss: 0.00001579
Iteration 237/1000 | Loss: 0.00001579
Iteration 238/1000 | Loss: 0.00001579
Iteration 239/1000 | Loss: 0.00001579
Iteration 240/1000 | Loss: 0.00001579
Iteration 241/1000 | Loss: 0.00001579
Iteration 242/1000 | Loss: 0.00001579
Iteration 243/1000 | Loss: 0.00001579
Iteration 244/1000 | Loss: 0.00001579
Iteration 245/1000 | Loss: 0.00001578
Iteration 246/1000 | Loss: 0.00001578
Iteration 247/1000 | Loss: 0.00001578
Iteration 248/1000 | Loss: 0.00001578
Iteration 249/1000 | Loss: 0.00001578
Iteration 250/1000 | Loss: 0.00001578
Iteration 251/1000 | Loss: 0.00001578
Iteration 252/1000 | Loss: 0.00001578
Iteration 253/1000 | Loss: 0.00001578
Iteration 254/1000 | Loss: 0.00001578
Iteration 255/1000 | Loss: 0.00001578
Iteration 256/1000 | Loss: 0.00001578
Iteration 257/1000 | Loss: 0.00001578
Iteration 258/1000 | Loss: 0.00001578
Iteration 259/1000 | Loss: 0.00001578
Iteration 260/1000 | Loss: 0.00001577
Iteration 261/1000 | Loss: 0.00001577
Iteration 262/1000 | Loss: 0.00001577
Iteration 263/1000 | Loss: 0.00001577
Iteration 264/1000 | Loss: 0.00001577
Iteration 265/1000 | Loss: 0.00001577
Iteration 266/1000 | Loss: 0.00001577
Iteration 267/1000 | Loss: 0.00001577
Iteration 268/1000 | Loss: 0.00001577
Iteration 269/1000 | Loss: 0.00001577
Iteration 270/1000 | Loss: 0.00001577
Iteration 271/1000 | Loss: 0.00001577
Iteration 272/1000 | Loss: 0.00001577
Iteration 273/1000 | Loss: 0.00001577
Iteration 274/1000 | Loss: 0.00001577
Iteration 275/1000 | Loss: 0.00001577
Iteration 276/1000 | Loss: 0.00001577
Iteration 277/1000 | Loss: 0.00001577
Iteration 278/1000 | Loss: 0.00001576
Iteration 279/1000 | Loss: 0.00001576
Iteration 280/1000 | Loss: 0.00001576
Iteration 281/1000 | Loss: 0.00001576
Iteration 282/1000 | Loss: 0.00001576
Iteration 283/1000 | Loss: 0.00001576
Iteration 284/1000 | Loss: 0.00001576
Iteration 285/1000 | Loss: 0.00001576
Iteration 286/1000 | Loss: 0.00001576
Iteration 287/1000 | Loss: 0.00001576
Iteration 288/1000 | Loss: 0.00001576
Iteration 289/1000 | Loss: 0.00001576
Iteration 290/1000 | Loss: 0.00001576
Iteration 291/1000 | Loss: 0.00001576
Iteration 292/1000 | Loss: 0.00001576
Iteration 293/1000 | Loss: 0.00001576
Iteration 294/1000 | Loss: 0.00001576
Iteration 295/1000 | Loss: 0.00001576
Iteration 296/1000 | Loss: 0.00001575
Iteration 297/1000 | Loss: 0.00001575
Iteration 298/1000 | Loss: 0.00001575
Iteration 299/1000 | Loss: 0.00001575
Iteration 300/1000 | Loss: 0.00001575
Iteration 301/1000 | Loss: 0.00001575
Iteration 302/1000 | Loss: 0.00001575
Iteration 303/1000 | Loss: 0.00001575
Iteration 304/1000 | Loss: 0.00001575
Iteration 305/1000 | Loss: 0.00001575
Iteration 306/1000 | Loss: 0.00001575
Iteration 307/1000 | Loss: 0.00001575
Iteration 308/1000 | Loss: 0.00001575
Iteration 309/1000 | Loss: 0.00001574
Iteration 310/1000 | Loss: 0.00001574
Iteration 311/1000 | Loss: 0.00001574
Iteration 312/1000 | Loss: 0.00001574
Iteration 313/1000 | Loss: 0.00001574
Iteration 314/1000 | Loss: 0.00001574
Iteration 315/1000 | Loss: 0.00001574
Iteration 316/1000 | Loss: 0.00001574
Iteration 317/1000 | Loss: 0.00001574
Iteration 318/1000 | Loss: 0.00001574
Iteration 319/1000 | Loss: 0.00001574
Iteration 320/1000 | Loss: 0.00001574
Iteration 321/1000 | Loss: 0.00001574
Iteration 322/1000 | Loss: 0.00001574
Iteration 323/1000 | Loss: 0.00001574
Iteration 324/1000 | Loss: 0.00001574
Iteration 325/1000 | Loss: 0.00001574
Iteration 326/1000 | Loss: 0.00001573
Iteration 327/1000 | Loss: 0.00001573
Iteration 328/1000 | Loss: 0.00001573
Iteration 329/1000 | Loss: 0.00001573
Iteration 330/1000 | Loss: 0.00001573
Iteration 331/1000 | Loss: 0.00001573
Iteration 332/1000 | Loss: 0.00001573
Iteration 333/1000 | Loss: 0.00001573
Iteration 334/1000 | Loss: 0.00001573
Iteration 335/1000 | Loss: 0.00001573
Iteration 336/1000 | Loss: 0.00001573
Iteration 337/1000 | Loss: 0.00001573
Iteration 338/1000 | Loss: 0.00001573
Iteration 339/1000 | Loss: 0.00001573
Iteration 340/1000 | Loss: 0.00001573
Iteration 341/1000 | Loss: 0.00001573
Iteration 342/1000 | Loss: 0.00001573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [1.573466943227686e-05, 1.573466943227686e-05, 1.573466943227686e-05, 1.573466943227686e-05, 1.573466943227686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.573466943227686e-05

Optimization complete. Final v2v error: 3.333620309829712 mm

Highest mean error: 3.7342562675476074 mm for frame 239

Lowest mean error: 3.2804596424102783 mm for frame 134

Saving results

Total time: 63.938360929489136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604133
Iteration 2/25 | Loss: 0.00142918
Iteration 3/25 | Loss: 0.00129725
Iteration 4/25 | Loss: 0.00127002
Iteration 5/25 | Loss: 0.00126587
Iteration 6/25 | Loss: 0.00126413
Iteration 7/25 | Loss: 0.00126259
Iteration 8/25 | Loss: 0.00126446
Iteration 9/25 | Loss: 0.00126260
Iteration 10/25 | Loss: 0.00126510
Iteration 11/25 | Loss: 0.00126433
Iteration 12/25 | Loss: 0.00126404
Iteration 13/25 | Loss: 0.00126337
Iteration 14/25 | Loss: 0.00126518
Iteration 15/25 | Loss: 0.00126433
Iteration 16/25 | Loss: 0.00126485
Iteration 17/25 | Loss: 0.00126391
Iteration 18/25 | Loss: 0.00125960
Iteration 19/25 | Loss: 0.00126392
Iteration 20/25 | Loss: 0.00126359
Iteration 21/25 | Loss: 0.00126310
Iteration 22/25 | Loss: 0.00126405
Iteration 23/25 | Loss: 0.00126304
Iteration 24/25 | Loss: 0.00126554
Iteration 25/25 | Loss: 0.00126427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57004976
Iteration 2/25 | Loss: 0.00089354
Iteration 3/25 | Loss: 0.00089353
Iteration 4/25 | Loss: 0.00089352
Iteration 5/25 | Loss: 0.00089352
Iteration 6/25 | Loss: 0.00089352
Iteration 7/25 | Loss: 0.00089352
Iteration 8/25 | Loss: 0.00089352
Iteration 9/25 | Loss: 0.00089352
Iteration 10/25 | Loss: 0.00089352
Iteration 11/25 | Loss: 0.00089352
Iteration 12/25 | Loss: 0.00089352
Iteration 13/25 | Loss: 0.00089352
Iteration 14/25 | Loss: 0.00089352
Iteration 15/25 | Loss: 0.00089352
Iteration 16/25 | Loss: 0.00089352
Iteration 17/25 | Loss: 0.00089352
Iteration 18/25 | Loss: 0.00089352
Iteration 19/25 | Loss: 0.00089352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008935218793340027, 0.0008935218793340027, 0.0008935218793340027, 0.0008935218793340027, 0.0008935218793340027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008935218793340027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089352
Iteration 2/1000 | Loss: 0.00021366
Iteration 3/1000 | Loss: 0.00013714
Iteration 4/1000 | Loss: 0.00018488
Iteration 5/1000 | Loss: 0.00010373
Iteration 6/1000 | Loss: 0.00016308
Iteration 7/1000 | Loss: 0.00012392
Iteration 8/1000 | Loss: 0.00011552
Iteration 9/1000 | Loss: 0.00014442
Iteration 10/1000 | Loss: 0.00006214
Iteration 11/1000 | Loss: 0.00008537
Iteration 12/1000 | Loss: 0.00013157
Iteration 13/1000 | Loss: 0.00011863
Iteration 14/1000 | Loss: 0.00003541
Iteration 15/1000 | Loss: 0.00009432
Iteration 16/1000 | Loss: 0.00016211
Iteration 17/1000 | Loss: 0.00018772
Iteration 18/1000 | Loss: 0.00038325
Iteration 19/1000 | Loss: 0.00020248
Iteration 20/1000 | Loss: 0.00020490
Iteration 21/1000 | Loss: 0.00016700
Iteration 22/1000 | Loss: 0.00011171
Iteration 23/1000 | Loss: 0.00016036
Iteration 24/1000 | Loss: 0.00020542
Iteration 25/1000 | Loss: 0.00005622
Iteration 26/1000 | Loss: 0.00021298
Iteration 27/1000 | Loss: 0.00017511
Iteration 28/1000 | Loss: 0.00013919
Iteration 29/1000 | Loss: 0.00026685
Iteration 30/1000 | Loss: 0.00017817
Iteration 31/1000 | Loss: 0.00023904
Iteration 32/1000 | Loss: 0.00022878
Iteration 33/1000 | Loss: 0.00048003
Iteration 34/1000 | Loss: 0.00028991
Iteration 35/1000 | Loss: 0.00015016
Iteration 36/1000 | Loss: 0.00018719
Iteration 37/1000 | Loss: 0.00019557
Iteration 38/1000 | Loss: 0.00039178
Iteration 39/1000 | Loss: 0.00024393
Iteration 40/1000 | Loss: 0.00016278
Iteration 41/1000 | Loss: 0.00034370
Iteration 42/1000 | Loss: 0.00018782
Iteration 43/1000 | Loss: 0.00009187
Iteration 44/1000 | Loss: 0.00006247
Iteration 45/1000 | Loss: 0.00010065
Iteration 46/1000 | Loss: 0.00014823
Iteration 47/1000 | Loss: 0.00021701
Iteration 48/1000 | Loss: 0.00020245
Iteration 49/1000 | Loss: 0.00031064
Iteration 50/1000 | Loss: 0.00019750
Iteration 51/1000 | Loss: 0.00063227
Iteration 52/1000 | Loss: 0.00024658
Iteration 53/1000 | Loss: 0.00024750
Iteration 54/1000 | Loss: 0.00035225
Iteration 55/1000 | Loss: 0.00021724
Iteration 56/1000 | Loss: 0.00006459
Iteration 57/1000 | Loss: 0.00013597
Iteration 58/1000 | Loss: 0.00024319
Iteration 59/1000 | Loss: 0.00014391
Iteration 60/1000 | Loss: 0.00014261
Iteration 61/1000 | Loss: 0.00024770
Iteration 62/1000 | Loss: 0.00022571
Iteration 63/1000 | Loss: 0.00023838
Iteration 64/1000 | Loss: 0.00022793
Iteration 65/1000 | Loss: 0.00022332
Iteration 66/1000 | Loss: 0.00021926
Iteration 67/1000 | Loss: 0.00021324
Iteration 68/1000 | Loss: 0.00020843
Iteration 69/1000 | Loss: 0.00024072
Iteration 70/1000 | Loss: 0.00018699
Iteration 71/1000 | Loss: 0.00022299
Iteration 72/1000 | Loss: 0.00016274
Iteration 73/1000 | Loss: 0.00018779
Iteration 74/1000 | Loss: 0.00022041
Iteration 75/1000 | Loss: 0.00020474
Iteration 76/1000 | Loss: 0.00007386
Iteration 77/1000 | Loss: 0.00019697
Iteration 78/1000 | Loss: 0.00016622
Iteration 79/1000 | Loss: 0.00014562
Iteration 80/1000 | Loss: 0.00027527
Iteration 81/1000 | Loss: 0.00019915
Iteration 82/1000 | Loss: 0.00018306
Iteration 83/1000 | Loss: 0.00014635
Iteration 84/1000 | Loss: 0.00016222
Iteration 85/1000 | Loss: 0.00014579
Iteration 86/1000 | Loss: 0.00033488
Iteration 87/1000 | Loss: 0.00017608
Iteration 88/1000 | Loss: 0.00015124
Iteration 89/1000 | Loss: 0.00011141
Iteration 90/1000 | Loss: 0.00019869
Iteration 91/1000 | Loss: 0.00032175
Iteration 92/1000 | Loss: 0.00024220
Iteration 93/1000 | Loss: 0.00017277
Iteration 94/1000 | Loss: 0.00014134
Iteration 95/1000 | Loss: 0.00014614
Iteration 96/1000 | Loss: 0.00017217
Iteration 97/1000 | Loss: 0.00005573
Iteration 98/1000 | Loss: 0.00009737
Iteration 99/1000 | Loss: 0.00002904
Iteration 100/1000 | Loss: 0.00002714
Iteration 101/1000 | Loss: 0.00002576
Iteration 102/1000 | Loss: 0.00002514
Iteration 103/1000 | Loss: 0.00002491
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002455
Iteration 106/1000 | Loss: 0.00002449
Iteration 107/1000 | Loss: 0.00002430
Iteration 108/1000 | Loss: 0.00002425
Iteration 109/1000 | Loss: 0.00002414
Iteration 110/1000 | Loss: 0.00002409
Iteration 111/1000 | Loss: 0.00002395
Iteration 112/1000 | Loss: 0.00002394
Iteration 113/1000 | Loss: 0.00002388
Iteration 114/1000 | Loss: 0.00002383
Iteration 115/1000 | Loss: 0.00002383
Iteration 116/1000 | Loss: 0.00002382
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002380
Iteration 119/1000 | Loss: 0.00002379
Iteration 120/1000 | Loss: 0.00002377
Iteration 121/1000 | Loss: 0.00002377
Iteration 122/1000 | Loss: 0.00002376
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002374
Iteration 126/1000 | Loss: 0.00002374
Iteration 127/1000 | Loss: 0.00002373
Iteration 128/1000 | Loss: 0.00002370
Iteration 129/1000 | Loss: 0.00002804
Iteration 130/1000 | Loss: 0.00002460
Iteration 131/1000 | Loss: 0.00002399
Iteration 132/1000 | Loss: 0.00002372
Iteration 133/1000 | Loss: 0.00002347
Iteration 134/1000 | Loss: 0.00002344
Iteration 135/1000 | Loss: 0.00002343
Iteration 136/1000 | Loss: 0.00002342
Iteration 137/1000 | Loss: 0.00002342
Iteration 138/1000 | Loss: 0.00002341
Iteration 139/1000 | Loss: 0.00002341
Iteration 140/1000 | Loss: 0.00002341
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002340
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002338
Iteration 148/1000 | Loss: 0.00002338
Iteration 149/1000 | Loss: 0.00002337
Iteration 150/1000 | Loss: 0.00002337
Iteration 151/1000 | Loss: 0.00002337
Iteration 152/1000 | Loss: 0.00002336
Iteration 153/1000 | Loss: 0.00002336
Iteration 154/1000 | Loss: 0.00002335
Iteration 155/1000 | Loss: 0.00002335
Iteration 156/1000 | Loss: 0.00002334
Iteration 157/1000 | Loss: 0.00002334
Iteration 158/1000 | Loss: 0.00002334
Iteration 159/1000 | Loss: 0.00002333
Iteration 160/1000 | Loss: 0.00002333
Iteration 161/1000 | Loss: 0.00002332
Iteration 162/1000 | Loss: 0.00002332
Iteration 163/1000 | Loss: 0.00002332
Iteration 164/1000 | Loss: 0.00002331
Iteration 165/1000 | Loss: 0.00002331
Iteration 166/1000 | Loss: 0.00002331
Iteration 167/1000 | Loss: 0.00002331
Iteration 168/1000 | Loss: 0.00002331
Iteration 169/1000 | Loss: 0.00002331
Iteration 170/1000 | Loss: 0.00002331
Iteration 171/1000 | Loss: 0.00002330
Iteration 172/1000 | Loss: 0.00002330
Iteration 173/1000 | Loss: 0.00002330
Iteration 174/1000 | Loss: 0.00002330
Iteration 175/1000 | Loss: 0.00002330
Iteration 176/1000 | Loss: 0.00002330
Iteration 177/1000 | Loss: 0.00002329
Iteration 178/1000 | Loss: 0.00002329
Iteration 179/1000 | Loss: 0.00002329
Iteration 180/1000 | Loss: 0.00002329
Iteration 181/1000 | Loss: 0.00002328
Iteration 182/1000 | Loss: 0.00002328
Iteration 183/1000 | Loss: 0.00002328
Iteration 184/1000 | Loss: 0.00002328
Iteration 185/1000 | Loss: 0.00002328
Iteration 186/1000 | Loss: 0.00002328
Iteration 187/1000 | Loss: 0.00002328
Iteration 188/1000 | Loss: 0.00002328
Iteration 189/1000 | Loss: 0.00002328
Iteration 190/1000 | Loss: 0.00002327
Iteration 191/1000 | Loss: 0.00002327
Iteration 192/1000 | Loss: 0.00002327
Iteration 193/1000 | Loss: 0.00002327
Iteration 194/1000 | Loss: 0.00002327
Iteration 195/1000 | Loss: 0.00002327
Iteration 196/1000 | Loss: 0.00002327
Iteration 197/1000 | Loss: 0.00002327
Iteration 198/1000 | Loss: 0.00002327
Iteration 199/1000 | Loss: 0.00002327
Iteration 200/1000 | Loss: 0.00002326
Iteration 201/1000 | Loss: 0.00002326
Iteration 202/1000 | Loss: 0.00002326
Iteration 203/1000 | Loss: 0.00002326
Iteration 204/1000 | Loss: 0.00002325
Iteration 205/1000 | Loss: 0.00002324
Iteration 206/1000 | Loss: 0.00002324
Iteration 207/1000 | Loss: 0.00002324
Iteration 208/1000 | Loss: 0.00002323
Iteration 209/1000 | Loss: 0.00002323
Iteration 210/1000 | Loss: 0.00002322
Iteration 211/1000 | Loss: 0.00002322
Iteration 212/1000 | Loss: 0.00002322
Iteration 213/1000 | Loss: 0.00002321
Iteration 214/1000 | Loss: 0.00002321
Iteration 215/1000 | Loss: 0.00002321
Iteration 216/1000 | Loss: 0.00002320
Iteration 217/1000 | Loss: 0.00002320
Iteration 218/1000 | Loss: 0.00002320
Iteration 219/1000 | Loss: 0.00002320
Iteration 220/1000 | Loss: 0.00002320
Iteration 221/1000 | Loss: 0.00002320
Iteration 222/1000 | Loss: 0.00002320
Iteration 223/1000 | Loss: 0.00002320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.319814302609302e-05, 2.319814302609302e-05, 2.319814302609302e-05, 2.319814302609302e-05, 2.319814302609302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.319814302609302e-05

Optimization complete. Final v2v error: 3.890028953552246 mm

Highest mean error: 5.551568984985352 mm for frame 157

Lowest mean error: 3.051302194595337 mm for frame 196

Saving results

Total time: 243.80399823188782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447152
Iteration 2/25 | Loss: 0.00151564
Iteration 3/25 | Loss: 0.00126615
Iteration 4/25 | Loss: 0.00122460
Iteration 5/25 | Loss: 0.00121667
Iteration 6/25 | Loss: 0.00121463
Iteration 7/25 | Loss: 0.00121463
Iteration 8/25 | Loss: 0.00121463
Iteration 9/25 | Loss: 0.00121463
Iteration 10/25 | Loss: 0.00121463
Iteration 11/25 | Loss: 0.00121463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012146326480433345, 0.0012146326480433345, 0.0012146326480433345, 0.0012146326480433345, 0.0012146326480433345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012146326480433345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46321297
Iteration 2/25 | Loss: 0.00067395
Iteration 3/25 | Loss: 0.00067395
Iteration 4/25 | Loss: 0.00067394
Iteration 5/25 | Loss: 0.00067394
Iteration 6/25 | Loss: 0.00067394
Iteration 7/25 | Loss: 0.00067394
Iteration 8/25 | Loss: 0.00067394
Iteration 9/25 | Loss: 0.00067394
Iteration 10/25 | Loss: 0.00067394
Iteration 11/25 | Loss: 0.00067394
Iteration 12/25 | Loss: 0.00067394
Iteration 13/25 | Loss: 0.00067394
Iteration 14/25 | Loss: 0.00067394
Iteration 15/25 | Loss: 0.00067394
Iteration 16/25 | Loss: 0.00067394
Iteration 17/25 | Loss: 0.00067394
Iteration 18/25 | Loss: 0.00067394
Iteration 19/25 | Loss: 0.00067394
Iteration 20/25 | Loss: 0.00067394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006739407544955611, 0.0006739407544955611, 0.0006739407544955611, 0.0006739407544955611, 0.0006739407544955611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006739407544955611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067394
Iteration 2/1000 | Loss: 0.00004491
Iteration 3/1000 | Loss: 0.00002750
Iteration 4/1000 | Loss: 0.00002363
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002054
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00002015
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001977
Iteration 13/1000 | Loss: 0.00001975
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001971
Iteration 16/1000 | Loss: 0.00001971
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001960
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001950
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001942
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001938
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001930
Iteration 46/1000 | Loss: 0.00001930
Iteration 47/1000 | Loss: 0.00001930
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001929
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001929
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001928
Iteration 63/1000 | Loss: 0.00001928
Iteration 64/1000 | Loss: 0.00001928
Iteration 65/1000 | Loss: 0.00001927
Iteration 66/1000 | Loss: 0.00001927
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001925
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001925
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001924
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001924
Iteration 76/1000 | Loss: 0.00001924
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001923
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001923
Iteration 81/1000 | Loss: 0.00001923
Iteration 82/1000 | Loss: 0.00001923
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001919
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001918
Iteration 104/1000 | Loss: 0.00001918
Iteration 105/1000 | Loss: 0.00001917
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001916
Iteration 110/1000 | Loss: 0.00001916
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001916
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001916
Iteration 119/1000 | Loss: 0.00001916
Iteration 120/1000 | Loss: 0.00001916
Iteration 121/1000 | Loss: 0.00001916
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001915
Iteration 124/1000 | Loss: 0.00001915
Iteration 125/1000 | Loss: 0.00001915
Iteration 126/1000 | Loss: 0.00001915
Iteration 127/1000 | Loss: 0.00001915
Iteration 128/1000 | Loss: 0.00001915
Iteration 129/1000 | Loss: 0.00001915
Iteration 130/1000 | Loss: 0.00001915
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001914
Iteration 134/1000 | Loss: 0.00001914
Iteration 135/1000 | Loss: 0.00001914
Iteration 136/1000 | Loss: 0.00001914
Iteration 137/1000 | Loss: 0.00001914
Iteration 138/1000 | Loss: 0.00001914
Iteration 139/1000 | Loss: 0.00001914
Iteration 140/1000 | Loss: 0.00001914
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001914
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001913
Iteration 145/1000 | Loss: 0.00001913
Iteration 146/1000 | Loss: 0.00001913
Iteration 147/1000 | Loss: 0.00001913
Iteration 148/1000 | Loss: 0.00001913
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001913
Iteration 152/1000 | Loss: 0.00001913
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001912
Iteration 155/1000 | Loss: 0.00001912
Iteration 156/1000 | Loss: 0.00001912
Iteration 157/1000 | Loss: 0.00001912
Iteration 158/1000 | Loss: 0.00001912
Iteration 159/1000 | Loss: 0.00001912
Iteration 160/1000 | Loss: 0.00001911
Iteration 161/1000 | Loss: 0.00001911
Iteration 162/1000 | Loss: 0.00001911
Iteration 163/1000 | Loss: 0.00001911
Iteration 164/1000 | Loss: 0.00001911
Iteration 165/1000 | Loss: 0.00001911
Iteration 166/1000 | Loss: 0.00001911
Iteration 167/1000 | Loss: 0.00001911
Iteration 168/1000 | Loss: 0.00001911
Iteration 169/1000 | Loss: 0.00001911
Iteration 170/1000 | Loss: 0.00001911
Iteration 171/1000 | Loss: 0.00001910
Iteration 172/1000 | Loss: 0.00001910
Iteration 173/1000 | Loss: 0.00001910
Iteration 174/1000 | Loss: 0.00001910
Iteration 175/1000 | Loss: 0.00001910
Iteration 176/1000 | Loss: 0.00001910
Iteration 177/1000 | Loss: 0.00001910
Iteration 178/1000 | Loss: 0.00001910
Iteration 179/1000 | Loss: 0.00001910
Iteration 180/1000 | Loss: 0.00001910
Iteration 181/1000 | Loss: 0.00001910
Iteration 182/1000 | Loss: 0.00001910
Iteration 183/1000 | Loss: 0.00001910
Iteration 184/1000 | Loss: 0.00001910
Iteration 185/1000 | Loss: 0.00001910
Iteration 186/1000 | Loss: 0.00001910
Iteration 187/1000 | Loss: 0.00001910
Iteration 188/1000 | Loss: 0.00001910
Iteration 189/1000 | Loss: 0.00001910
Iteration 190/1000 | Loss: 0.00001910
Iteration 191/1000 | Loss: 0.00001910
Iteration 192/1000 | Loss: 0.00001910
Iteration 193/1000 | Loss: 0.00001910
Iteration 194/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.9101724319625646e-05, 1.9101724319625646e-05, 1.9101724319625646e-05, 1.9101724319625646e-05, 1.9101724319625646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9101724319625646e-05

Optimization complete. Final v2v error: 3.6465930938720703 mm

Highest mean error: 4.805772304534912 mm for frame 203

Lowest mean error: 3.195011615753174 mm for frame 97

Saving results

Total time: 47.201759815216064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025386
Iteration 2/25 | Loss: 0.01025386
Iteration 3/25 | Loss: 0.00304626
Iteration 4/25 | Loss: 0.00267805
Iteration 5/25 | Loss: 0.00214716
Iteration 6/25 | Loss: 0.00211997
Iteration 7/25 | Loss: 0.00196600
Iteration 8/25 | Loss: 0.00199202
Iteration 9/25 | Loss: 0.00195497
Iteration 10/25 | Loss: 0.00220255
Iteration 11/25 | Loss: 0.00186713
Iteration 12/25 | Loss: 0.00150555
Iteration 13/25 | Loss: 0.00138050
Iteration 14/25 | Loss: 0.00135036
Iteration 15/25 | Loss: 0.00131477
Iteration 16/25 | Loss: 0.00131763
Iteration 17/25 | Loss: 0.00128815
Iteration 18/25 | Loss: 0.00126340
Iteration 19/25 | Loss: 0.00124915
Iteration 20/25 | Loss: 0.00125032
Iteration 21/25 | Loss: 0.00124486
Iteration 22/25 | Loss: 0.00125298
Iteration 23/25 | Loss: 0.00121954
Iteration 24/25 | Loss: 0.00121463
Iteration 25/25 | Loss: 0.00121235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66397858
Iteration 2/25 | Loss: 0.00060899
Iteration 3/25 | Loss: 0.00060898
Iteration 4/25 | Loss: 0.00060898
Iteration 5/25 | Loss: 0.00060898
Iteration 6/25 | Loss: 0.00060898
Iteration 7/25 | Loss: 0.00060898
Iteration 8/25 | Loss: 0.00060898
Iteration 9/25 | Loss: 0.00060898
Iteration 10/25 | Loss: 0.00060898
Iteration 11/25 | Loss: 0.00060898
Iteration 12/25 | Loss: 0.00060898
Iteration 13/25 | Loss: 0.00060898
Iteration 14/25 | Loss: 0.00060898
Iteration 15/25 | Loss: 0.00060898
Iteration 16/25 | Loss: 0.00060898
Iteration 17/25 | Loss: 0.00060898
Iteration 18/25 | Loss: 0.00060898
Iteration 19/25 | Loss: 0.00060898
Iteration 20/25 | Loss: 0.00060898
Iteration 21/25 | Loss: 0.00060898
Iteration 22/25 | Loss: 0.00060898
Iteration 23/25 | Loss: 0.00060898
Iteration 24/25 | Loss: 0.00060898
Iteration 25/25 | Loss: 0.00060898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006089790840633214, 0.0006089790840633214, 0.0006089790840633214, 0.0006089790840633214, 0.0006089790840633214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006089790840633214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060898
Iteration 2/1000 | Loss: 0.00005063
Iteration 3/1000 | Loss: 0.00004061
Iteration 4/1000 | Loss: 0.00072161
Iteration 5/1000 | Loss: 0.00003353
Iteration 6/1000 | Loss: 0.00002845
Iteration 7/1000 | Loss: 0.00002558
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00043686
Iteration 10/1000 | Loss: 0.00024277
Iteration 11/1000 | Loss: 0.00026837
Iteration 12/1000 | Loss: 0.00002957
Iteration 13/1000 | Loss: 0.00002694
Iteration 14/1000 | Loss: 0.00002446
Iteration 15/1000 | Loss: 0.00002287
Iteration 16/1000 | Loss: 0.00002837
Iteration 17/1000 | Loss: 0.00002325
Iteration 18/1000 | Loss: 0.00002198
Iteration 19/1000 | Loss: 0.00002102
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002046
Iteration 22/1000 | Loss: 0.00015517
Iteration 23/1000 | Loss: 0.00016961
Iteration 24/1000 | Loss: 0.00043173
Iteration 25/1000 | Loss: 0.00002841
Iteration 26/1000 | Loss: 0.00002856
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00002391
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001689
Iteration 36/1000 | Loss: 0.00001677
Iteration 37/1000 | Loss: 0.00001673
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001668
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001666
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001651
Iteration 52/1000 | Loss: 0.00001651
Iteration 53/1000 | Loss: 0.00001650
Iteration 54/1000 | Loss: 0.00001650
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001643
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001642
Iteration 98/1000 | Loss: 0.00001642
Iteration 99/1000 | Loss: 0.00001642
Iteration 100/1000 | Loss: 0.00001642
Iteration 101/1000 | Loss: 0.00001642
Iteration 102/1000 | Loss: 0.00001642
Iteration 103/1000 | Loss: 0.00001641
Iteration 104/1000 | Loss: 0.00001641
Iteration 105/1000 | Loss: 0.00001641
Iteration 106/1000 | Loss: 0.00001641
Iteration 107/1000 | Loss: 0.00001641
Iteration 108/1000 | Loss: 0.00001641
Iteration 109/1000 | Loss: 0.00001641
Iteration 110/1000 | Loss: 0.00001641
Iteration 111/1000 | Loss: 0.00001641
Iteration 112/1000 | Loss: 0.00001640
Iteration 113/1000 | Loss: 0.00001640
Iteration 114/1000 | Loss: 0.00001640
Iteration 115/1000 | Loss: 0.00001640
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Iteration 125/1000 | Loss: 0.00001639
Iteration 126/1000 | Loss: 0.00001639
Iteration 127/1000 | Loss: 0.00001639
Iteration 128/1000 | Loss: 0.00001639
Iteration 129/1000 | Loss: 0.00001639
Iteration 130/1000 | Loss: 0.00001638
Iteration 131/1000 | Loss: 0.00001638
Iteration 132/1000 | Loss: 0.00001638
Iteration 133/1000 | Loss: 0.00001638
Iteration 134/1000 | Loss: 0.00001638
Iteration 135/1000 | Loss: 0.00001638
Iteration 136/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.6384827176807448e-05, 1.6384827176807448e-05, 1.6384827176807448e-05, 1.6384827176807448e-05, 1.6384827176807448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6384827176807448e-05

Optimization complete. Final v2v error: 3.4416396617889404 mm

Highest mean error: 4.357113838195801 mm for frame 23

Lowest mean error: 3.2812492847442627 mm for frame 239

Saving results

Total time: 115.98109459877014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001430
Iteration 2/25 | Loss: 0.00288217
Iteration 3/25 | Loss: 0.00197651
Iteration 4/25 | Loss: 0.00189250
Iteration 5/25 | Loss: 0.00184640
Iteration 6/25 | Loss: 0.00180663
Iteration 7/25 | Loss: 0.00154002
Iteration 8/25 | Loss: 0.00148946
Iteration 9/25 | Loss: 0.00146547
Iteration 10/25 | Loss: 0.00144125
Iteration 11/25 | Loss: 0.00143145
Iteration 12/25 | Loss: 0.00139202
Iteration 13/25 | Loss: 0.00140812
Iteration 14/25 | Loss: 0.00137422
Iteration 15/25 | Loss: 0.00137044
Iteration 16/25 | Loss: 0.00136494
Iteration 17/25 | Loss: 0.00136209
Iteration 18/25 | Loss: 0.00136215
Iteration 19/25 | Loss: 0.00136700
Iteration 20/25 | Loss: 0.00135923
Iteration 21/25 | Loss: 0.00135773
Iteration 22/25 | Loss: 0.00135221
Iteration 23/25 | Loss: 0.00134931
Iteration 24/25 | Loss: 0.00134529
Iteration 25/25 | Loss: 0.00134356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45835209
Iteration 2/25 | Loss: 0.00240437
Iteration 3/25 | Loss: 0.00238434
Iteration 4/25 | Loss: 0.00238434
Iteration 5/25 | Loss: 0.00238434
Iteration 6/25 | Loss: 0.00238434
Iteration 7/25 | Loss: 0.00238434
Iteration 8/25 | Loss: 0.00238434
Iteration 9/25 | Loss: 0.00238434
Iteration 10/25 | Loss: 0.00238434
Iteration 11/25 | Loss: 0.00238434
Iteration 12/25 | Loss: 0.00238434
Iteration 13/25 | Loss: 0.00238434
Iteration 14/25 | Loss: 0.00238434
Iteration 15/25 | Loss: 0.00238434
Iteration 16/25 | Loss: 0.00238434
Iteration 17/25 | Loss: 0.00238434
Iteration 18/25 | Loss: 0.00238434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023843375965952873, 0.0023843375965952873, 0.0023843375965952873, 0.0023843375965952873, 0.0023843375965952873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023843375965952873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238434
Iteration 2/1000 | Loss: 0.00066668
Iteration 3/1000 | Loss: 0.00046875
Iteration 4/1000 | Loss: 0.00025974
Iteration 5/1000 | Loss: 0.00033499
Iteration 6/1000 | Loss: 0.00014881
Iteration 7/1000 | Loss: 0.00034751
Iteration 8/1000 | Loss: 0.00033339
Iteration 9/1000 | Loss: 0.00035283
Iteration 10/1000 | Loss: 0.00018029
Iteration 11/1000 | Loss: 0.00011653
Iteration 12/1000 | Loss: 0.00008194
Iteration 13/1000 | Loss: 0.00017647
Iteration 14/1000 | Loss: 0.00025245
Iteration 15/1000 | Loss: 0.00330947
Iteration 16/1000 | Loss: 0.00287493
Iteration 17/1000 | Loss: 0.00088694
Iteration 18/1000 | Loss: 0.00121511
Iteration 19/1000 | Loss: 0.00030352
Iteration 20/1000 | Loss: 0.00009215
Iteration 21/1000 | Loss: 0.00008248
Iteration 22/1000 | Loss: 0.00004983
Iteration 23/1000 | Loss: 0.00004024
Iteration 24/1000 | Loss: 0.00005197
Iteration 25/1000 | Loss: 0.00004312
Iteration 26/1000 | Loss: 0.00002613
Iteration 27/1000 | Loss: 0.00002587
Iteration 28/1000 | Loss: 0.00002085
Iteration 29/1000 | Loss: 0.00001916
Iteration 30/1000 | Loss: 0.00020822
Iteration 31/1000 | Loss: 0.00006192
Iteration 32/1000 | Loss: 0.00019196
Iteration 33/1000 | Loss: 0.00038052
Iteration 34/1000 | Loss: 0.00026108
Iteration 35/1000 | Loss: 0.00043660
Iteration 36/1000 | Loss: 0.00018759
Iteration 37/1000 | Loss: 0.00027659
Iteration 38/1000 | Loss: 0.00002442
Iteration 39/1000 | Loss: 0.00003143
Iteration 40/1000 | Loss: 0.00002032
Iteration 41/1000 | Loss: 0.00009534
Iteration 42/1000 | Loss: 0.00017736
Iteration 43/1000 | Loss: 0.00022237
Iteration 44/1000 | Loss: 0.00014580
Iteration 45/1000 | Loss: 0.00005875
Iteration 46/1000 | Loss: 0.00023902
Iteration 47/1000 | Loss: 0.00008489
Iteration 48/1000 | Loss: 0.00009802
Iteration 49/1000 | Loss: 0.00002258
Iteration 50/1000 | Loss: 0.00002067
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00003143
Iteration 53/1000 | Loss: 0.00002564
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00004033
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00002221
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00003922
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001380
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001366
Iteration 107/1000 | Loss: 0.00001366
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001365
Iteration 112/1000 | Loss: 0.00001365
Iteration 113/1000 | Loss: 0.00001365
Iteration 114/1000 | Loss: 0.00001365
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00003087
Iteration 118/1000 | Loss: 0.00003087
Iteration 119/1000 | Loss: 0.00009514
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001360
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001359
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001356
Iteration 133/1000 | Loss: 0.00001356
Iteration 134/1000 | Loss: 0.00001356
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001355
Iteration 137/1000 | Loss: 0.00001355
Iteration 138/1000 | Loss: 0.00001354
Iteration 139/1000 | Loss: 0.00001354
Iteration 140/1000 | Loss: 0.00001354
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001353
Iteration 143/1000 | Loss: 0.00001353
Iteration 144/1000 | Loss: 0.00001353
Iteration 145/1000 | Loss: 0.00001353
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001352
Iteration 150/1000 | Loss: 0.00001352
Iteration 151/1000 | Loss: 0.00001352
Iteration 152/1000 | Loss: 0.00001352
Iteration 153/1000 | Loss: 0.00001352
Iteration 154/1000 | Loss: 0.00001352
Iteration 155/1000 | Loss: 0.00001352
Iteration 156/1000 | Loss: 0.00001352
Iteration 157/1000 | Loss: 0.00001352
Iteration 158/1000 | Loss: 0.00001352
Iteration 159/1000 | Loss: 0.00001352
Iteration 160/1000 | Loss: 0.00001352
Iteration 161/1000 | Loss: 0.00001352
Iteration 162/1000 | Loss: 0.00001352
Iteration 163/1000 | Loss: 0.00001351
Iteration 164/1000 | Loss: 0.00001351
Iteration 165/1000 | Loss: 0.00001351
Iteration 166/1000 | Loss: 0.00001351
Iteration 167/1000 | Loss: 0.00001351
Iteration 168/1000 | Loss: 0.00001351
Iteration 169/1000 | Loss: 0.00001351
Iteration 170/1000 | Loss: 0.00001350
Iteration 171/1000 | Loss: 0.00001350
Iteration 172/1000 | Loss: 0.00001350
Iteration 173/1000 | Loss: 0.00001350
Iteration 174/1000 | Loss: 0.00001350
Iteration 175/1000 | Loss: 0.00001350
Iteration 176/1000 | Loss: 0.00002745
Iteration 177/1000 | Loss: 0.00001351
Iteration 178/1000 | Loss: 0.00001350
Iteration 179/1000 | Loss: 0.00001349
Iteration 180/1000 | Loss: 0.00001349
Iteration 181/1000 | Loss: 0.00001349
Iteration 182/1000 | Loss: 0.00001349
Iteration 183/1000 | Loss: 0.00001349
Iteration 184/1000 | Loss: 0.00001349
Iteration 185/1000 | Loss: 0.00001349
Iteration 186/1000 | Loss: 0.00001349
Iteration 187/1000 | Loss: 0.00001349
Iteration 188/1000 | Loss: 0.00001349
Iteration 189/1000 | Loss: 0.00001349
Iteration 190/1000 | Loss: 0.00001349
Iteration 191/1000 | Loss: 0.00001349
Iteration 192/1000 | Loss: 0.00001349
Iteration 193/1000 | Loss: 0.00001349
Iteration 194/1000 | Loss: 0.00001348
Iteration 195/1000 | Loss: 0.00001348
Iteration 196/1000 | Loss: 0.00001348
Iteration 197/1000 | Loss: 0.00001348
Iteration 198/1000 | Loss: 0.00001348
Iteration 199/1000 | Loss: 0.00001348
Iteration 200/1000 | Loss: 0.00001348
Iteration 201/1000 | Loss: 0.00001348
Iteration 202/1000 | Loss: 0.00001348
Iteration 203/1000 | Loss: 0.00001348
Iteration 204/1000 | Loss: 0.00001348
Iteration 205/1000 | Loss: 0.00001348
Iteration 206/1000 | Loss: 0.00001348
Iteration 207/1000 | Loss: 0.00001348
Iteration 208/1000 | Loss: 0.00001348
Iteration 209/1000 | Loss: 0.00001348
Iteration 210/1000 | Loss: 0.00001348
Iteration 211/1000 | Loss: 0.00001348
Iteration 212/1000 | Loss: 0.00001348
Iteration 213/1000 | Loss: 0.00001348
Iteration 214/1000 | Loss: 0.00001348
Iteration 215/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.3481838323059492e-05, 1.3481838323059492e-05, 1.3481838323059492e-05, 1.3481838323059492e-05, 1.3481838323059492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3481838323059492e-05

Optimization complete. Final v2v error: 2.9716408252716064 mm

Highest mean error: 10.24040699005127 mm for frame 72

Lowest mean error: 2.6085081100463867 mm for frame 110

Saving results

Total time: 171.79229259490967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393815
Iteration 2/25 | Loss: 0.00140080
Iteration 3/25 | Loss: 0.00122852
Iteration 4/25 | Loss: 0.00120192
Iteration 5/25 | Loss: 0.00119692
Iteration 6/25 | Loss: 0.00119640
Iteration 7/25 | Loss: 0.00119640
Iteration 8/25 | Loss: 0.00119640
Iteration 9/25 | Loss: 0.00119640
Iteration 10/25 | Loss: 0.00119640
Iteration 11/25 | Loss: 0.00119640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011963954893872142, 0.0011963954893872142, 0.0011963954893872142, 0.0011963954893872142, 0.0011963954893872142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011963954893872142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45424378
Iteration 2/25 | Loss: 0.00057146
Iteration 3/25 | Loss: 0.00057145
Iteration 4/25 | Loss: 0.00057145
Iteration 5/25 | Loss: 0.00057145
Iteration 6/25 | Loss: 0.00057145
Iteration 7/25 | Loss: 0.00057145
Iteration 8/25 | Loss: 0.00057145
Iteration 9/25 | Loss: 0.00057145
Iteration 10/25 | Loss: 0.00057145
Iteration 11/25 | Loss: 0.00057145
Iteration 12/25 | Loss: 0.00057145
Iteration 13/25 | Loss: 0.00057145
Iteration 14/25 | Loss: 0.00057145
Iteration 15/25 | Loss: 0.00057145
Iteration 16/25 | Loss: 0.00057145
Iteration 17/25 | Loss: 0.00057145
Iteration 18/25 | Loss: 0.00057145
Iteration 19/25 | Loss: 0.00057145
Iteration 20/25 | Loss: 0.00057145
Iteration 21/25 | Loss: 0.00057145
Iteration 22/25 | Loss: 0.00057145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005714498111046851, 0.0005714498111046851, 0.0005714498111046851, 0.0005714498111046851, 0.0005714498111046851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005714498111046851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057145
Iteration 2/1000 | Loss: 0.00003016
Iteration 3/1000 | Loss: 0.00002214
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001708
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001678
Iteration 12/1000 | Loss: 0.00001661
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001655
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001654
Iteration 18/1000 | Loss: 0.00001653
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001649
Iteration 21/1000 | Loss: 0.00001648
Iteration 22/1000 | Loss: 0.00001637
Iteration 23/1000 | Loss: 0.00001636
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001632
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001630
Iteration 31/1000 | Loss: 0.00001624
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001614
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001612
Iteration 50/1000 | Loss: 0.00001612
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001611
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001602
Iteration 87/1000 | Loss: 0.00001602
Iteration 88/1000 | Loss: 0.00001602
Iteration 89/1000 | Loss: 0.00001602
Iteration 90/1000 | Loss: 0.00001602
Iteration 91/1000 | Loss: 0.00001602
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001601
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001600
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001600
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001599
Iteration 109/1000 | Loss: 0.00001599
Iteration 110/1000 | Loss: 0.00001599
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001599
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001598
Iteration 135/1000 | Loss: 0.00001598
Iteration 136/1000 | Loss: 0.00001598
Iteration 137/1000 | Loss: 0.00001598
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001597
Iteration 144/1000 | Loss: 0.00001597
Iteration 145/1000 | Loss: 0.00001596
Iteration 146/1000 | Loss: 0.00001596
Iteration 147/1000 | Loss: 0.00001596
Iteration 148/1000 | Loss: 0.00001596
Iteration 149/1000 | Loss: 0.00001596
Iteration 150/1000 | Loss: 0.00001596
Iteration 151/1000 | Loss: 0.00001596
Iteration 152/1000 | Loss: 0.00001596
Iteration 153/1000 | Loss: 0.00001596
Iteration 154/1000 | Loss: 0.00001596
Iteration 155/1000 | Loss: 0.00001596
Iteration 156/1000 | Loss: 0.00001596
Iteration 157/1000 | Loss: 0.00001596
Iteration 158/1000 | Loss: 0.00001596
Iteration 159/1000 | Loss: 0.00001596
Iteration 160/1000 | Loss: 0.00001595
Iteration 161/1000 | Loss: 0.00001595
Iteration 162/1000 | Loss: 0.00001595
Iteration 163/1000 | Loss: 0.00001595
Iteration 164/1000 | Loss: 0.00001595
Iteration 165/1000 | Loss: 0.00001594
Iteration 166/1000 | Loss: 0.00001594
Iteration 167/1000 | Loss: 0.00001594
Iteration 168/1000 | Loss: 0.00001594
Iteration 169/1000 | Loss: 0.00001593
Iteration 170/1000 | Loss: 0.00001593
Iteration 171/1000 | Loss: 0.00001593
Iteration 172/1000 | Loss: 0.00001593
Iteration 173/1000 | Loss: 0.00001592
Iteration 174/1000 | Loss: 0.00001592
Iteration 175/1000 | Loss: 0.00001592
Iteration 176/1000 | Loss: 0.00001592
Iteration 177/1000 | Loss: 0.00001592
Iteration 178/1000 | Loss: 0.00001592
Iteration 179/1000 | Loss: 0.00001592
Iteration 180/1000 | Loss: 0.00001592
Iteration 181/1000 | Loss: 0.00001592
Iteration 182/1000 | Loss: 0.00001592
Iteration 183/1000 | Loss: 0.00001591
Iteration 184/1000 | Loss: 0.00001591
Iteration 185/1000 | Loss: 0.00001591
Iteration 186/1000 | Loss: 0.00001591
Iteration 187/1000 | Loss: 0.00001591
Iteration 188/1000 | Loss: 0.00001591
Iteration 189/1000 | Loss: 0.00001590
Iteration 190/1000 | Loss: 0.00001590
Iteration 191/1000 | Loss: 0.00001590
Iteration 192/1000 | Loss: 0.00001590
Iteration 193/1000 | Loss: 0.00001590
Iteration 194/1000 | Loss: 0.00001590
Iteration 195/1000 | Loss: 0.00001590
Iteration 196/1000 | Loss: 0.00001590
Iteration 197/1000 | Loss: 0.00001590
Iteration 198/1000 | Loss: 0.00001590
Iteration 199/1000 | Loss: 0.00001590
Iteration 200/1000 | Loss: 0.00001590
Iteration 201/1000 | Loss: 0.00001589
Iteration 202/1000 | Loss: 0.00001589
Iteration 203/1000 | Loss: 0.00001589
Iteration 204/1000 | Loss: 0.00001589
Iteration 205/1000 | Loss: 0.00001589
Iteration 206/1000 | Loss: 0.00001589
Iteration 207/1000 | Loss: 0.00001589
Iteration 208/1000 | Loss: 0.00001588
Iteration 209/1000 | Loss: 0.00001588
Iteration 210/1000 | Loss: 0.00001588
Iteration 211/1000 | Loss: 0.00001588
Iteration 212/1000 | Loss: 0.00001588
Iteration 213/1000 | Loss: 0.00001588
Iteration 214/1000 | Loss: 0.00001588
Iteration 215/1000 | Loss: 0.00001588
Iteration 216/1000 | Loss: 0.00001588
Iteration 217/1000 | Loss: 0.00001588
Iteration 218/1000 | Loss: 0.00001588
Iteration 219/1000 | Loss: 0.00001588
Iteration 220/1000 | Loss: 0.00001588
Iteration 221/1000 | Loss: 0.00001588
Iteration 222/1000 | Loss: 0.00001588
Iteration 223/1000 | Loss: 0.00001588
Iteration 224/1000 | Loss: 0.00001588
Iteration 225/1000 | Loss: 0.00001588
Iteration 226/1000 | Loss: 0.00001588
Iteration 227/1000 | Loss: 0.00001588
Iteration 228/1000 | Loss: 0.00001588
Iteration 229/1000 | Loss: 0.00001588
Iteration 230/1000 | Loss: 0.00001588
Iteration 231/1000 | Loss: 0.00001588
Iteration 232/1000 | Loss: 0.00001588
Iteration 233/1000 | Loss: 0.00001588
Iteration 234/1000 | Loss: 0.00001588
Iteration 235/1000 | Loss: 0.00001588
Iteration 236/1000 | Loss: 0.00001588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.587986116646789e-05, 1.587986116646789e-05, 1.587986116646789e-05, 1.587986116646789e-05, 1.587986116646789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.587986116646789e-05

Optimization complete. Final v2v error: 3.315105676651001 mm

Highest mean error: 3.5672385692596436 mm for frame 0

Lowest mean error: 3.156862735748291 mm for frame 150

Saving results

Total time: 40.97870397567749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031789
Iteration 2/25 | Loss: 0.00221391
Iteration 3/25 | Loss: 0.00176652
Iteration 4/25 | Loss: 0.00168433
Iteration 5/25 | Loss: 0.00155199
Iteration 6/25 | Loss: 0.00144432
Iteration 7/25 | Loss: 0.00140912
Iteration 8/25 | Loss: 0.00135050
Iteration 9/25 | Loss: 0.00132611
Iteration 10/25 | Loss: 0.00130302
Iteration 11/25 | Loss: 0.00127589
Iteration 12/25 | Loss: 0.00124470
Iteration 13/25 | Loss: 0.00123665
Iteration 14/25 | Loss: 0.00123288
Iteration 15/25 | Loss: 0.00123078
Iteration 16/25 | Loss: 0.00123195
Iteration 17/25 | Loss: 0.00123090
Iteration 18/25 | Loss: 0.00122964
Iteration 19/25 | Loss: 0.00123135
Iteration 20/25 | Loss: 0.00123004
Iteration 21/25 | Loss: 0.00122847
Iteration 22/25 | Loss: 0.00123007
Iteration 23/25 | Loss: 0.00122913
Iteration 24/25 | Loss: 0.00123008
Iteration 25/25 | Loss: 0.00123102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67077625
Iteration 2/25 | Loss: 0.00082209
Iteration 3/25 | Loss: 0.00082209
Iteration 4/25 | Loss: 0.00082209
Iteration 5/25 | Loss: 0.00082209
Iteration 6/25 | Loss: 0.00082209
Iteration 7/25 | Loss: 0.00082209
Iteration 8/25 | Loss: 0.00082209
Iteration 9/25 | Loss: 0.00082209
Iteration 10/25 | Loss: 0.00082209
Iteration 11/25 | Loss: 0.00082209
Iteration 12/25 | Loss: 0.00082209
Iteration 13/25 | Loss: 0.00082209
Iteration 14/25 | Loss: 0.00082209
Iteration 15/25 | Loss: 0.00082209
Iteration 16/25 | Loss: 0.00082209
Iteration 17/25 | Loss: 0.00082209
Iteration 18/25 | Loss: 0.00082209
Iteration 19/25 | Loss: 0.00082209
Iteration 20/25 | Loss: 0.00082209
Iteration 21/25 | Loss: 0.00082209
Iteration 22/25 | Loss: 0.00082209
Iteration 23/25 | Loss: 0.00082209
Iteration 24/25 | Loss: 0.00082209
Iteration 25/25 | Loss: 0.00082209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082209
Iteration 2/1000 | Loss: 0.00007113
Iteration 3/1000 | Loss: 0.00026392
Iteration 4/1000 | Loss: 0.00012246
Iteration 5/1000 | Loss: 0.00007093
Iteration 6/1000 | Loss: 0.00005318
Iteration 7/1000 | Loss: 0.00005223
Iteration 8/1000 | Loss: 0.00004927
Iteration 9/1000 | Loss: 0.00005919
Iteration 10/1000 | Loss: 0.00007321
Iteration 11/1000 | Loss: 0.00006801
Iteration 12/1000 | Loss: 0.00027735
Iteration 13/1000 | Loss: 0.00327269
Iteration 14/1000 | Loss: 0.00010307
Iteration 15/1000 | Loss: 0.00007090
Iteration 16/1000 | Loss: 0.00005087
Iteration 17/1000 | Loss: 0.00006168
Iteration 18/1000 | Loss: 0.00026327
Iteration 19/1000 | Loss: 0.00019804
Iteration 20/1000 | Loss: 0.00004642
Iteration 21/1000 | Loss: 0.00015141
Iteration 22/1000 | Loss: 0.00005507
Iteration 23/1000 | Loss: 0.00003278
Iteration 24/1000 | Loss: 0.00003087
Iteration 25/1000 | Loss: 0.00017228
Iteration 26/1000 | Loss: 0.00082826
Iteration 27/1000 | Loss: 0.00053353
Iteration 28/1000 | Loss: 0.00008443
Iteration 29/1000 | Loss: 0.00006140
Iteration 30/1000 | Loss: 0.00004891
Iteration 31/1000 | Loss: 0.00004580
Iteration 32/1000 | Loss: 0.00003892
Iteration 33/1000 | Loss: 0.00003284
Iteration 34/1000 | Loss: 0.00002716
Iteration 35/1000 | Loss: 0.00003214
Iteration 36/1000 | Loss: 0.00002653
Iteration 37/1000 | Loss: 0.00002482
Iteration 38/1000 | Loss: 0.00002790
Iteration 39/1000 | Loss: 0.00002418
Iteration 40/1000 | Loss: 0.00002310
Iteration 41/1000 | Loss: 0.00002262
Iteration 42/1000 | Loss: 0.00002835
Iteration 43/1000 | Loss: 0.00002805
Iteration 44/1000 | Loss: 0.00002837
Iteration 45/1000 | Loss: 0.00002788
Iteration 46/1000 | Loss: 0.00002853
Iteration 47/1000 | Loss: 0.00002726
Iteration 48/1000 | Loss: 0.00002897
Iteration 49/1000 | Loss: 0.00002770
Iteration 50/1000 | Loss: 0.00002247
Iteration 51/1000 | Loss: 0.00002941
Iteration 52/1000 | Loss: 0.00002729
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002980
Iteration 55/1000 | Loss: 0.00002834
Iteration 56/1000 | Loss: 0.00002903
Iteration 57/1000 | Loss: 0.00002869
Iteration 58/1000 | Loss: 0.00002848
Iteration 59/1000 | Loss: 0.00002853
Iteration 60/1000 | Loss: 0.00002713
Iteration 61/1000 | Loss: 0.00002403
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001894
Iteration 64/1000 | Loss: 0.00001853
Iteration 65/1000 | Loss: 0.00003141
Iteration 66/1000 | Loss: 0.00003297
Iteration 67/1000 | Loss: 0.00003587
Iteration 68/1000 | Loss: 0.00004930
Iteration 69/1000 | Loss: 0.00002003
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001604
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001508
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001507
Iteration 91/1000 | Loss: 0.00001507
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001504
Iteration 108/1000 | Loss: 0.00001504
Iteration 109/1000 | Loss: 0.00001504
Iteration 110/1000 | Loss: 0.00001504
Iteration 111/1000 | Loss: 0.00001504
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001501
Iteration 152/1000 | Loss: 0.00001501
Iteration 153/1000 | Loss: 0.00001501
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001501
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Iteration 170/1000 | Loss: 0.00001501
Iteration 171/1000 | Loss: 0.00001501
Iteration 172/1000 | Loss: 0.00001501
Iteration 173/1000 | Loss: 0.00001501
Iteration 174/1000 | Loss: 0.00001501
Iteration 175/1000 | Loss: 0.00001501
Iteration 176/1000 | Loss: 0.00001501
Iteration 177/1000 | Loss: 0.00001501
Iteration 178/1000 | Loss: 0.00001501
Iteration 179/1000 | Loss: 0.00001501
Iteration 180/1000 | Loss: 0.00001501
Iteration 181/1000 | Loss: 0.00001501
Iteration 182/1000 | Loss: 0.00001501
Iteration 183/1000 | Loss: 0.00001501
Iteration 184/1000 | Loss: 0.00001501
Iteration 185/1000 | Loss: 0.00001501
Iteration 186/1000 | Loss: 0.00001501
Iteration 187/1000 | Loss: 0.00001501
Iteration 188/1000 | Loss: 0.00001501
Iteration 189/1000 | Loss: 0.00001501
Iteration 190/1000 | Loss: 0.00001501
Iteration 191/1000 | Loss: 0.00001501
Iteration 192/1000 | Loss: 0.00001501
Iteration 193/1000 | Loss: 0.00001501
Iteration 194/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.5009964954515453e-05, 1.5009964954515453e-05, 1.5009964954515453e-05, 1.5009964954515453e-05, 1.5009964954515453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5009964954515453e-05

Optimization complete. Final v2v error: 3.2662699222564697 mm

Highest mean error: 4.036199569702148 mm for frame 74

Lowest mean error: 2.968082904815674 mm for frame 30

Saving results

Total time: 158.64265394210815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00717292
Iteration 2/25 | Loss: 0.00164919
Iteration 3/25 | Loss: 0.00138222
Iteration 4/25 | Loss: 0.00133789
Iteration 5/25 | Loss: 0.00133036
Iteration 6/25 | Loss: 0.00132854
Iteration 7/25 | Loss: 0.00133113
Iteration 8/25 | Loss: 0.00132754
Iteration 9/25 | Loss: 0.00132499
Iteration 10/25 | Loss: 0.00132667
Iteration 11/25 | Loss: 0.00132622
Iteration 12/25 | Loss: 0.00132154
Iteration 13/25 | Loss: 0.00132033
Iteration 14/25 | Loss: 0.00131978
Iteration 15/25 | Loss: 0.00131961
Iteration 16/25 | Loss: 0.00131952
Iteration 17/25 | Loss: 0.00131938
Iteration 18/25 | Loss: 0.00131938
Iteration 19/25 | Loss: 0.00131938
Iteration 20/25 | Loss: 0.00131938
Iteration 21/25 | Loss: 0.00131938
Iteration 22/25 | Loss: 0.00131938
Iteration 23/25 | Loss: 0.00131937
Iteration 24/25 | Loss: 0.00131937
Iteration 25/25 | Loss: 0.00131937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35906923
Iteration 2/25 | Loss: 0.00114194
Iteration 3/25 | Loss: 0.00114190
Iteration 4/25 | Loss: 0.00114190
Iteration 5/25 | Loss: 0.00114190
Iteration 6/25 | Loss: 0.00114190
Iteration 7/25 | Loss: 0.00114190
Iteration 8/25 | Loss: 0.00114190
Iteration 9/25 | Loss: 0.00114190
Iteration 10/25 | Loss: 0.00114190
Iteration 11/25 | Loss: 0.00114190
Iteration 12/25 | Loss: 0.00114190
Iteration 13/25 | Loss: 0.00114190
Iteration 14/25 | Loss: 0.00114190
Iteration 15/25 | Loss: 0.00114190
Iteration 16/25 | Loss: 0.00114190
Iteration 17/25 | Loss: 0.00114190
Iteration 18/25 | Loss: 0.00114190
Iteration 19/25 | Loss: 0.00114190
Iteration 20/25 | Loss: 0.00114190
Iteration 21/25 | Loss: 0.00114190
Iteration 22/25 | Loss: 0.00114190
Iteration 23/25 | Loss: 0.00114190
Iteration 24/25 | Loss: 0.00114190
Iteration 25/25 | Loss: 0.00114190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114190
Iteration 2/1000 | Loss: 0.00008618
Iteration 3/1000 | Loss: 0.00006485
Iteration 4/1000 | Loss: 0.00005732
Iteration 5/1000 | Loss: 0.00005400
Iteration 6/1000 | Loss: 0.00005215
Iteration 7/1000 | Loss: 0.00005084
Iteration 8/1000 | Loss: 0.00004990
Iteration 9/1000 | Loss: 0.00060230
Iteration 10/1000 | Loss: 0.00024041
Iteration 11/1000 | Loss: 0.00005205
Iteration 12/1000 | Loss: 0.00004958
Iteration 13/1000 | Loss: 0.00004880
Iteration 14/1000 | Loss: 0.00004838
Iteration 15/1000 | Loss: 0.00004797
Iteration 16/1000 | Loss: 0.00004762
Iteration 17/1000 | Loss: 0.00004741
Iteration 18/1000 | Loss: 0.00004716
Iteration 19/1000 | Loss: 0.00004690
Iteration 20/1000 | Loss: 0.00004666
Iteration 21/1000 | Loss: 0.00004665
Iteration 22/1000 | Loss: 0.00004649
Iteration 23/1000 | Loss: 0.00004640
Iteration 24/1000 | Loss: 0.00004626
Iteration 25/1000 | Loss: 0.00004625
Iteration 26/1000 | Loss: 0.00004622
Iteration 27/1000 | Loss: 0.00004618
Iteration 28/1000 | Loss: 0.00004617
Iteration 29/1000 | Loss: 0.00004617
Iteration 30/1000 | Loss: 0.00004612
Iteration 31/1000 | Loss: 0.00004608
Iteration 32/1000 | Loss: 0.00004604
Iteration 33/1000 | Loss: 0.00004604
Iteration 34/1000 | Loss: 0.00004604
Iteration 35/1000 | Loss: 0.00004603
Iteration 36/1000 | Loss: 0.00004602
Iteration 37/1000 | Loss: 0.00004602
Iteration 38/1000 | Loss: 0.00004602
Iteration 39/1000 | Loss: 0.00004601
Iteration 40/1000 | Loss: 0.00004601
Iteration 41/1000 | Loss: 0.00004601
Iteration 42/1000 | Loss: 0.00004601
Iteration 43/1000 | Loss: 0.00004601
Iteration 44/1000 | Loss: 0.00004600
Iteration 45/1000 | Loss: 0.00004600
Iteration 46/1000 | Loss: 0.00004600
Iteration 47/1000 | Loss: 0.00004599
Iteration 48/1000 | Loss: 0.00004599
Iteration 49/1000 | Loss: 0.00004599
Iteration 50/1000 | Loss: 0.00004598
Iteration 51/1000 | Loss: 0.00004597
Iteration 52/1000 | Loss: 0.00004597
Iteration 53/1000 | Loss: 0.00004597
Iteration 54/1000 | Loss: 0.00004596
Iteration 55/1000 | Loss: 0.00004596
Iteration 56/1000 | Loss: 0.00004596
Iteration 57/1000 | Loss: 0.00004596
Iteration 58/1000 | Loss: 0.00004596
Iteration 59/1000 | Loss: 0.00004596
Iteration 60/1000 | Loss: 0.00004596
Iteration 61/1000 | Loss: 0.00004596
Iteration 62/1000 | Loss: 0.00004595
Iteration 63/1000 | Loss: 0.00004595
Iteration 64/1000 | Loss: 0.00004595
Iteration 65/1000 | Loss: 0.00004595
Iteration 66/1000 | Loss: 0.00004595
Iteration 67/1000 | Loss: 0.00004594
Iteration 68/1000 | Loss: 0.00004594
Iteration 69/1000 | Loss: 0.00004593
Iteration 70/1000 | Loss: 0.00004592
Iteration 71/1000 | Loss: 0.00004592
Iteration 72/1000 | Loss: 0.00004591
Iteration 73/1000 | Loss: 0.00004590
Iteration 74/1000 | Loss: 0.00004590
Iteration 75/1000 | Loss: 0.00004589
Iteration 76/1000 | Loss: 0.00004589
Iteration 77/1000 | Loss: 0.00004588
Iteration 78/1000 | Loss: 0.00004588
Iteration 79/1000 | Loss: 0.00004588
Iteration 80/1000 | Loss: 0.00004586
Iteration 81/1000 | Loss: 0.00004585
Iteration 82/1000 | Loss: 0.00004584
Iteration 83/1000 | Loss: 0.00004584
Iteration 84/1000 | Loss: 0.00004584
Iteration 85/1000 | Loss: 0.00004583
Iteration 86/1000 | Loss: 0.00004583
Iteration 87/1000 | Loss: 0.00004583
Iteration 88/1000 | Loss: 0.00004583
Iteration 89/1000 | Loss: 0.00004582
Iteration 90/1000 | Loss: 0.00004582
Iteration 91/1000 | Loss: 0.00004581
Iteration 92/1000 | Loss: 0.00004581
Iteration 93/1000 | Loss: 0.00004580
Iteration 94/1000 | Loss: 0.00004580
Iteration 95/1000 | Loss: 0.00004579
Iteration 96/1000 | Loss: 0.00004579
Iteration 97/1000 | Loss: 0.00004578
Iteration 98/1000 | Loss: 0.00004578
Iteration 99/1000 | Loss: 0.00004577
Iteration 100/1000 | Loss: 0.00004577
Iteration 101/1000 | Loss: 0.00004577
Iteration 102/1000 | Loss: 0.00004576
Iteration 103/1000 | Loss: 0.00004576
Iteration 104/1000 | Loss: 0.00004575
Iteration 105/1000 | Loss: 0.00004575
Iteration 106/1000 | Loss: 0.00004574
Iteration 107/1000 | Loss: 0.00004574
Iteration 108/1000 | Loss: 0.00004574
Iteration 109/1000 | Loss: 0.00004573
Iteration 110/1000 | Loss: 0.00004573
Iteration 111/1000 | Loss: 0.00004573
Iteration 112/1000 | Loss: 0.00004572
Iteration 113/1000 | Loss: 0.00004572
Iteration 114/1000 | Loss: 0.00004572
Iteration 115/1000 | Loss: 0.00004571
Iteration 116/1000 | Loss: 0.00004571
Iteration 117/1000 | Loss: 0.00004571
Iteration 118/1000 | Loss: 0.00004570
Iteration 119/1000 | Loss: 0.00004569
Iteration 120/1000 | Loss: 0.00004569
Iteration 121/1000 | Loss: 0.00004569
Iteration 122/1000 | Loss: 0.00004569
Iteration 123/1000 | Loss: 0.00004568
Iteration 124/1000 | Loss: 0.00004568
Iteration 125/1000 | Loss: 0.00004568
Iteration 126/1000 | Loss: 0.00004568
Iteration 127/1000 | Loss: 0.00004568
Iteration 128/1000 | Loss: 0.00004568
Iteration 129/1000 | Loss: 0.00004568
Iteration 130/1000 | Loss: 0.00004567
Iteration 131/1000 | Loss: 0.00004567
Iteration 132/1000 | Loss: 0.00004566
Iteration 133/1000 | Loss: 0.00004566
Iteration 134/1000 | Loss: 0.00004566
Iteration 135/1000 | Loss: 0.00004565
Iteration 136/1000 | Loss: 0.00004565
Iteration 137/1000 | Loss: 0.00004563
Iteration 138/1000 | Loss: 0.00004563
Iteration 139/1000 | Loss: 0.00004563
Iteration 140/1000 | Loss: 0.00004563
Iteration 141/1000 | Loss: 0.00004562
Iteration 142/1000 | Loss: 0.00004562
Iteration 143/1000 | Loss: 0.00004562
Iteration 144/1000 | Loss: 0.00004561
Iteration 145/1000 | Loss: 0.00004561
Iteration 146/1000 | Loss: 0.00004560
Iteration 147/1000 | Loss: 0.00004560
Iteration 148/1000 | Loss: 0.00004560
Iteration 149/1000 | Loss: 0.00004560
Iteration 150/1000 | Loss: 0.00004560
Iteration 151/1000 | Loss: 0.00004560
Iteration 152/1000 | Loss: 0.00004560
Iteration 153/1000 | Loss: 0.00004560
Iteration 154/1000 | Loss: 0.00004560
Iteration 155/1000 | Loss: 0.00004560
Iteration 156/1000 | Loss: 0.00004560
Iteration 157/1000 | Loss: 0.00004560
Iteration 158/1000 | Loss: 0.00004560
Iteration 159/1000 | Loss: 0.00004560
Iteration 160/1000 | Loss: 0.00004560
Iteration 161/1000 | Loss: 0.00004560
Iteration 162/1000 | Loss: 0.00004559
Iteration 163/1000 | Loss: 0.00004559
Iteration 164/1000 | Loss: 0.00004559
Iteration 165/1000 | Loss: 0.00004559
Iteration 166/1000 | Loss: 0.00004559
Iteration 167/1000 | Loss: 0.00004558
Iteration 168/1000 | Loss: 0.00004558
Iteration 169/1000 | Loss: 0.00004557
Iteration 170/1000 | Loss: 0.00004557
Iteration 171/1000 | Loss: 0.00004557
Iteration 172/1000 | Loss: 0.00004557
Iteration 173/1000 | Loss: 0.00004557
Iteration 174/1000 | Loss: 0.00004557
Iteration 175/1000 | Loss: 0.00004557
Iteration 176/1000 | Loss: 0.00004556
Iteration 177/1000 | Loss: 0.00004556
Iteration 178/1000 | Loss: 0.00004555
Iteration 179/1000 | Loss: 0.00004555
Iteration 180/1000 | Loss: 0.00004555
Iteration 181/1000 | Loss: 0.00004555
Iteration 182/1000 | Loss: 0.00004555
Iteration 183/1000 | Loss: 0.00004555
Iteration 184/1000 | Loss: 0.00004554
Iteration 185/1000 | Loss: 0.00004553
Iteration 186/1000 | Loss: 0.00004553
Iteration 187/1000 | Loss: 0.00004553
Iteration 188/1000 | Loss: 0.00004553
Iteration 189/1000 | Loss: 0.00004553
Iteration 190/1000 | Loss: 0.00004553
Iteration 191/1000 | Loss: 0.00004553
Iteration 192/1000 | Loss: 0.00004553
Iteration 193/1000 | Loss: 0.00004552
Iteration 194/1000 | Loss: 0.00004552
Iteration 195/1000 | Loss: 0.00004552
Iteration 196/1000 | Loss: 0.00004552
Iteration 197/1000 | Loss: 0.00004551
Iteration 198/1000 | Loss: 0.00004551
Iteration 199/1000 | Loss: 0.00004551
Iteration 200/1000 | Loss: 0.00004551
Iteration 201/1000 | Loss: 0.00004551
Iteration 202/1000 | Loss: 0.00004551
Iteration 203/1000 | Loss: 0.00004551
Iteration 204/1000 | Loss: 0.00004551
Iteration 205/1000 | Loss: 0.00004550
Iteration 206/1000 | Loss: 0.00004550
Iteration 207/1000 | Loss: 0.00004550
Iteration 208/1000 | Loss: 0.00004550
Iteration 209/1000 | Loss: 0.00004549
Iteration 210/1000 | Loss: 0.00004549
Iteration 211/1000 | Loss: 0.00004549
Iteration 212/1000 | Loss: 0.00004549
Iteration 213/1000 | Loss: 0.00004549
Iteration 214/1000 | Loss: 0.00004549
Iteration 215/1000 | Loss: 0.00004549
Iteration 216/1000 | Loss: 0.00004548
Iteration 217/1000 | Loss: 0.00004548
Iteration 218/1000 | Loss: 0.00004548
Iteration 219/1000 | Loss: 0.00004548
Iteration 220/1000 | Loss: 0.00004548
Iteration 221/1000 | Loss: 0.00004547
Iteration 222/1000 | Loss: 0.00004547
Iteration 223/1000 | Loss: 0.00004547
Iteration 224/1000 | Loss: 0.00004547
Iteration 225/1000 | Loss: 0.00004547
Iteration 226/1000 | Loss: 0.00004547
Iteration 227/1000 | Loss: 0.00004547
Iteration 228/1000 | Loss: 0.00004547
Iteration 229/1000 | Loss: 0.00004547
Iteration 230/1000 | Loss: 0.00004547
Iteration 231/1000 | Loss: 0.00004546
Iteration 232/1000 | Loss: 0.00004546
Iteration 233/1000 | Loss: 0.00004546
Iteration 234/1000 | Loss: 0.00004545
Iteration 235/1000 | Loss: 0.00004545
Iteration 236/1000 | Loss: 0.00004545
Iteration 237/1000 | Loss: 0.00004544
Iteration 238/1000 | Loss: 0.00004544
Iteration 239/1000 | Loss: 0.00004544
Iteration 240/1000 | Loss: 0.00004544
Iteration 241/1000 | Loss: 0.00004544
Iteration 242/1000 | Loss: 0.00004544
Iteration 243/1000 | Loss: 0.00004544
Iteration 244/1000 | Loss: 0.00004544
Iteration 245/1000 | Loss: 0.00004544
Iteration 246/1000 | Loss: 0.00004544
Iteration 247/1000 | Loss: 0.00004543
Iteration 248/1000 | Loss: 0.00004543
Iteration 249/1000 | Loss: 0.00004543
Iteration 250/1000 | Loss: 0.00004543
Iteration 251/1000 | Loss: 0.00004543
Iteration 252/1000 | Loss: 0.00004543
Iteration 253/1000 | Loss: 0.00004543
Iteration 254/1000 | Loss: 0.00004543
Iteration 255/1000 | Loss: 0.00004543
Iteration 256/1000 | Loss: 0.00004543
Iteration 257/1000 | Loss: 0.00004543
Iteration 258/1000 | Loss: 0.00004543
Iteration 259/1000 | Loss: 0.00004543
Iteration 260/1000 | Loss: 0.00004543
Iteration 261/1000 | Loss: 0.00004543
Iteration 262/1000 | Loss: 0.00004543
Iteration 263/1000 | Loss: 0.00004543
Iteration 264/1000 | Loss: 0.00004543
Iteration 265/1000 | Loss: 0.00004543
Iteration 266/1000 | Loss: 0.00004542
Iteration 267/1000 | Loss: 0.00004542
Iteration 268/1000 | Loss: 0.00004542
Iteration 269/1000 | Loss: 0.00004542
Iteration 270/1000 | Loss: 0.00004542
Iteration 271/1000 | Loss: 0.00004542
Iteration 272/1000 | Loss: 0.00004542
Iteration 273/1000 | Loss: 0.00004542
Iteration 274/1000 | Loss: 0.00004542
Iteration 275/1000 | Loss: 0.00004542
Iteration 276/1000 | Loss: 0.00004541
Iteration 277/1000 | Loss: 0.00004541
Iteration 278/1000 | Loss: 0.00004541
Iteration 279/1000 | Loss: 0.00004541
Iteration 280/1000 | Loss: 0.00004541
Iteration 281/1000 | Loss: 0.00004541
Iteration 282/1000 | Loss: 0.00004541
Iteration 283/1000 | Loss: 0.00004540
Iteration 284/1000 | Loss: 0.00004540
Iteration 285/1000 | Loss: 0.00004540
Iteration 286/1000 | Loss: 0.00004540
Iteration 287/1000 | Loss: 0.00004540
Iteration 288/1000 | Loss: 0.00004540
Iteration 289/1000 | Loss: 0.00004539
Iteration 290/1000 | Loss: 0.00004539
Iteration 291/1000 | Loss: 0.00004539
Iteration 292/1000 | Loss: 0.00004539
Iteration 293/1000 | Loss: 0.00004539
Iteration 294/1000 | Loss: 0.00004539
Iteration 295/1000 | Loss: 0.00004539
Iteration 296/1000 | Loss: 0.00004539
Iteration 297/1000 | Loss: 0.00004539
Iteration 298/1000 | Loss: 0.00004539
Iteration 299/1000 | Loss: 0.00004539
Iteration 300/1000 | Loss: 0.00004539
Iteration 301/1000 | Loss: 0.00004539
Iteration 302/1000 | Loss: 0.00004539
Iteration 303/1000 | Loss: 0.00004539
Iteration 304/1000 | Loss: 0.00004539
Iteration 305/1000 | Loss: 0.00004539
Iteration 306/1000 | Loss: 0.00004539
Iteration 307/1000 | Loss: 0.00004539
Iteration 308/1000 | Loss: 0.00004539
Iteration 309/1000 | Loss: 0.00004539
Iteration 310/1000 | Loss: 0.00004539
Iteration 311/1000 | Loss: 0.00004539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [4.5392447646008804e-05, 4.5392447646008804e-05, 4.5392447646008804e-05, 4.5392447646008804e-05, 4.5392447646008804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5392447646008804e-05

Optimization complete. Final v2v error: 4.280069351196289 mm

Highest mean error: 11.716263771057129 mm for frame 140

Lowest mean error: 3.281843423843384 mm for frame 9

Saving results

Total time: 97.4997398853302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008731
Iteration 2/25 | Loss: 0.00203907
Iteration 3/25 | Loss: 0.00151668
Iteration 4/25 | Loss: 0.00145892
Iteration 5/25 | Loss: 0.00141090
Iteration 6/25 | Loss: 0.00133513
Iteration 7/25 | Loss: 0.00133098
Iteration 8/25 | Loss: 0.00136915
Iteration 9/25 | Loss: 0.00128958
Iteration 10/25 | Loss: 0.00127250
Iteration 11/25 | Loss: 0.00125412
Iteration 12/25 | Loss: 0.00125453
Iteration 13/25 | Loss: 0.00124333
Iteration 14/25 | Loss: 0.00124072
Iteration 15/25 | Loss: 0.00123368
Iteration 16/25 | Loss: 0.00123890
Iteration 17/25 | Loss: 0.00123749
Iteration 18/25 | Loss: 0.00123924
Iteration 19/25 | Loss: 0.00123794
Iteration 20/25 | Loss: 0.00123922
Iteration 21/25 | Loss: 0.00124357
Iteration 22/25 | Loss: 0.00123626
Iteration 23/25 | Loss: 0.00123352
Iteration 24/25 | Loss: 0.00123332
Iteration 25/25 | Loss: 0.00122804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50899243
Iteration 2/25 | Loss: 0.00109821
Iteration 3/25 | Loss: 0.00085671
Iteration 4/25 | Loss: 0.00085671
Iteration 5/25 | Loss: 0.00085671
Iteration 6/25 | Loss: 0.00085671
Iteration 7/25 | Loss: 0.00085671
Iteration 8/25 | Loss: 0.00085671
Iteration 9/25 | Loss: 0.00085671
Iteration 10/25 | Loss: 0.00085671
Iteration 11/25 | Loss: 0.00085671
Iteration 12/25 | Loss: 0.00085671
Iteration 13/25 | Loss: 0.00085671
Iteration 14/25 | Loss: 0.00085671
Iteration 15/25 | Loss: 0.00085671
Iteration 16/25 | Loss: 0.00085671
Iteration 17/25 | Loss: 0.00085671
Iteration 18/25 | Loss: 0.00085671
Iteration 19/25 | Loss: 0.00085671
Iteration 20/25 | Loss: 0.00085670
Iteration 21/25 | Loss: 0.00085670
Iteration 22/25 | Loss: 0.00085670
Iteration 23/25 | Loss: 0.00085670
Iteration 24/25 | Loss: 0.00085670
Iteration 25/25 | Loss: 0.00085670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085670
Iteration 2/1000 | Loss: 0.00020039
Iteration 3/1000 | Loss: 0.00099118
Iteration 4/1000 | Loss: 0.00076729
Iteration 5/1000 | Loss: 0.00030855
Iteration 6/1000 | Loss: 0.00042823
Iteration 7/1000 | Loss: 0.00030900
Iteration 8/1000 | Loss: 0.00039028
Iteration 9/1000 | Loss: 0.00034746
Iteration 10/1000 | Loss: 0.00044363
Iteration 11/1000 | Loss: 0.00022891
Iteration 12/1000 | Loss: 0.00021747
Iteration 13/1000 | Loss: 0.00031821
Iteration 14/1000 | Loss: 0.00023886
Iteration 15/1000 | Loss: 0.00014685
Iteration 16/1000 | Loss: 0.00003578
Iteration 17/1000 | Loss: 0.00012127
Iteration 18/1000 | Loss: 0.00047858
Iteration 19/1000 | Loss: 0.00016966
Iteration 20/1000 | Loss: 0.00020877
Iteration 21/1000 | Loss: 0.00015888
Iteration 22/1000 | Loss: 0.00007719
Iteration 23/1000 | Loss: 0.00021748
Iteration 24/1000 | Loss: 0.00020857
Iteration 25/1000 | Loss: 0.00005308
Iteration 26/1000 | Loss: 0.00015490
Iteration 27/1000 | Loss: 0.00023339
Iteration 28/1000 | Loss: 0.00024495
Iteration 29/1000 | Loss: 0.00022183
Iteration 30/1000 | Loss: 0.00054075
Iteration 31/1000 | Loss: 0.00045400
Iteration 32/1000 | Loss: 0.00032767
Iteration 33/1000 | Loss: 0.00049512
Iteration 34/1000 | Loss: 0.00030338
Iteration 35/1000 | Loss: 0.00020698
Iteration 36/1000 | Loss: 0.00017547
Iteration 37/1000 | Loss: 0.00008181
Iteration 38/1000 | Loss: 0.00008049
Iteration 39/1000 | Loss: 0.00006037
Iteration 40/1000 | Loss: 0.00009990
Iteration 41/1000 | Loss: 0.00026607
Iteration 42/1000 | Loss: 0.00009350
Iteration 43/1000 | Loss: 0.00021641
Iteration 44/1000 | Loss: 0.00016156
Iteration 45/1000 | Loss: 0.00014808
Iteration 46/1000 | Loss: 0.00020079
Iteration 47/1000 | Loss: 0.00015383
Iteration 48/1000 | Loss: 0.00024744
Iteration 49/1000 | Loss: 0.00033006
Iteration 50/1000 | Loss: 0.00015178
Iteration 51/1000 | Loss: 0.00014606
Iteration 52/1000 | Loss: 0.00042622
Iteration 53/1000 | Loss: 0.00015637
Iteration 54/1000 | Loss: 0.00041417
Iteration 55/1000 | Loss: 0.00036900
Iteration 56/1000 | Loss: 0.00046241
Iteration 57/1000 | Loss: 0.00038810
Iteration 58/1000 | Loss: 0.00010623
Iteration 59/1000 | Loss: 0.00003977
Iteration 60/1000 | Loss: 0.00004123
Iteration 61/1000 | Loss: 0.00002886
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002479
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002362
Iteration 66/1000 | Loss: 0.00002325
Iteration 67/1000 | Loss: 0.00014415
Iteration 68/1000 | Loss: 0.00032026
Iteration 69/1000 | Loss: 0.00013513
Iteration 70/1000 | Loss: 0.00108144
Iteration 71/1000 | Loss: 0.00009707
Iteration 72/1000 | Loss: 0.00003518
Iteration 73/1000 | Loss: 0.00015672
Iteration 74/1000 | Loss: 0.00003050
Iteration 75/1000 | Loss: 0.00002623
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002300
Iteration 78/1000 | Loss: 0.00011193
Iteration 79/1000 | Loss: 0.00002340
Iteration 80/1000 | Loss: 0.00002226
Iteration 81/1000 | Loss: 0.00006883
Iteration 82/1000 | Loss: 0.00002218
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002194
Iteration 86/1000 | Loss: 0.00002178
Iteration 87/1000 | Loss: 0.00013326
Iteration 88/1000 | Loss: 0.00003583
Iteration 89/1000 | Loss: 0.00003039
Iteration 90/1000 | Loss: 0.00002180
Iteration 91/1000 | Loss: 0.00002167
Iteration 92/1000 | Loss: 0.00004217
Iteration 93/1000 | Loss: 0.00027511
Iteration 94/1000 | Loss: 0.00030806
Iteration 95/1000 | Loss: 0.00004952
Iteration 96/1000 | Loss: 0.00003182
Iteration 97/1000 | Loss: 0.00004245
Iteration 98/1000 | Loss: 0.00002847
Iteration 99/1000 | Loss: 0.00038078
Iteration 100/1000 | Loss: 0.00006312
Iteration 101/1000 | Loss: 0.00002950
Iteration 102/1000 | Loss: 0.00005009
Iteration 103/1000 | Loss: 0.00002872
Iteration 104/1000 | Loss: 0.00002414
Iteration 105/1000 | Loss: 0.00002307
Iteration 106/1000 | Loss: 0.00002223
Iteration 107/1000 | Loss: 0.00006140
Iteration 108/1000 | Loss: 0.00003459
Iteration 109/1000 | Loss: 0.00014445
Iteration 110/1000 | Loss: 0.00003198
Iteration 111/1000 | Loss: 0.00003619
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00028221
Iteration 114/1000 | Loss: 0.00060661
Iteration 115/1000 | Loss: 0.00050229
Iteration 116/1000 | Loss: 0.00018680
Iteration 117/1000 | Loss: 0.00015568
Iteration 118/1000 | Loss: 0.00027414
Iteration 119/1000 | Loss: 0.00011721
Iteration 120/1000 | Loss: 0.00001945
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00034925
Iteration 123/1000 | Loss: 0.00058883
Iteration 124/1000 | Loss: 0.00040304
Iteration 125/1000 | Loss: 0.00003799
Iteration 126/1000 | Loss: 0.00006998
Iteration 127/1000 | Loss: 0.00002430
Iteration 128/1000 | Loss: 0.00002158
Iteration 129/1000 | Loss: 0.00006004
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00005961
Iteration 132/1000 | Loss: 0.00001778
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00014770
Iteration 135/1000 | Loss: 0.00036098
Iteration 136/1000 | Loss: 0.00014185
Iteration 137/1000 | Loss: 0.00012634
Iteration 138/1000 | Loss: 0.00029416
Iteration 139/1000 | Loss: 0.00005681
Iteration 140/1000 | Loss: 0.00010517
Iteration 141/1000 | Loss: 0.00036310
Iteration 142/1000 | Loss: 0.00031451
Iteration 143/1000 | Loss: 0.00027720
Iteration 144/1000 | Loss: 0.00034044
Iteration 145/1000 | Loss: 0.00025848
Iteration 146/1000 | Loss: 0.00001776
Iteration 147/1000 | Loss: 0.00030792
Iteration 148/1000 | Loss: 0.00009434
Iteration 149/1000 | Loss: 0.00068404
Iteration 150/1000 | Loss: 0.00007455
Iteration 151/1000 | Loss: 0.00004043
Iteration 152/1000 | Loss: 0.00004071
Iteration 153/1000 | Loss: 0.00005709
Iteration 154/1000 | Loss: 0.00006382
Iteration 155/1000 | Loss: 0.00001978
Iteration 156/1000 | Loss: 0.00023536
Iteration 157/1000 | Loss: 0.00001793
Iteration 158/1000 | Loss: 0.00003222
Iteration 159/1000 | Loss: 0.00015884
Iteration 160/1000 | Loss: 0.00004082
Iteration 161/1000 | Loss: 0.00003121
Iteration 162/1000 | Loss: 0.00002942
Iteration 163/1000 | Loss: 0.00004901
Iteration 164/1000 | Loss: 0.00007883
Iteration 165/1000 | Loss: 0.00006248
Iteration 166/1000 | Loss: 0.00013661
Iteration 167/1000 | Loss: 0.00002845
Iteration 168/1000 | Loss: 0.00005926
Iteration 169/1000 | Loss: 0.00002946
Iteration 170/1000 | Loss: 0.00004461
Iteration 171/1000 | Loss: 0.00007880
Iteration 172/1000 | Loss: 0.00005236
Iteration 173/1000 | Loss: 0.00002580
Iteration 174/1000 | Loss: 0.00004531
Iteration 175/1000 | Loss: 0.00004220
Iteration 176/1000 | Loss: 0.00002105
Iteration 177/1000 | Loss: 0.00003573
Iteration 178/1000 | Loss: 0.00004424
Iteration 179/1000 | Loss: 0.00003932
Iteration 180/1000 | Loss: 0.00006471
Iteration 181/1000 | Loss: 0.00003809
Iteration 182/1000 | Loss: 0.00002576
Iteration 183/1000 | Loss: 0.00004333
Iteration 184/1000 | Loss: 0.00004524
Iteration 185/1000 | Loss: 0.00003581
Iteration 186/1000 | Loss: 0.00001898
Iteration 187/1000 | Loss: 0.00003104
Iteration 188/1000 | Loss: 0.00002951
Iteration 189/1000 | Loss: 0.00007333
Iteration 190/1000 | Loss: 0.00003648
Iteration 191/1000 | Loss: 0.00002211
Iteration 192/1000 | Loss: 0.00004806
Iteration 193/1000 | Loss: 0.00003598
Iteration 194/1000 | Loss: 0.00003307
Iteration 195/1000 | Loss: 0.00006496
Iteration 196/1000 | Loss: 0.00007822
Iteration 197/1000 | Loss: 0.00002292
Iteration 198/1000 | Loss: 0.00003638
Iteration 199/1000 | Loss: 0.00004377
Iteration 200/1000 | Loss: 0.00003769
Iteration 201/1000 | Loss: 0.00001629
Iteration 202/1000 | Loss: 0.00001948
Iteration 203/1000 | Loss: 0.00004383
Iteration 204/1000 | Loss: 0.00007396
Iteration 205/1000 | Loss: 0.00003730
Iteration 206/1000 | Loss: 0.00003081
Iteration 207/1000 | Loss: 0.00003111
Iteration 208/1000 | Loss: 0.00004173
Iteration 209/1000 | Loss: 0.00003446
Iteration 210/1000 | Loss: 0.00003167
Iteration 211/1000 | Loss: 0.00002939
Iteration 212/1000 | Loss: 0.00003458
Iteration 213/1000 | Loss: 0.00004927
Iteration 214/1000 | Loss: 0.00003752
Iteration 215/1000 | Loss: 0.00006096
Iteration 216/1000 | Loss: 0.00004099
Iteration 217/1000 | Loss: 0.00003355
Iteration 218/1000 | Loss: 0.00002747
Iteration 219/1000 | Loss: 0.00003565
Iteration 220/1000 | Loss: 0.00004512
Iteration 221/1000 | Loss: 0.00002799
Iteration 222/1000 | Loss: 0.00003657
Iteration 223/1000 | Loss: 0.00009901
Iteration 224/1000 | Loss: 0.00012869
Iteration 225/1000 | Loss: 0.00008203
Iteration 226/1000 | Loss: 0.00007054
Iteration 227/1000 | Loss: 0.00003163
Iteration 228/1000 | Loss: 0.00002870
Iteration 229/1000 | Loss: 0.00003790
Iteration 230/1000 | Loss: 0.00045083
Iteration 231/1000 | Loss: 0.00008387
Iteration 232/1000 | Loss: 0.00015046
Iteration 233/1000 | Loss: 0.00023531
Iteration 234/1000 | Loss: 0.00002791
Iteration 235/1000 | Loss: 0.00002145
Iteration 236/1000 | Loss: 0.00001834
Iteration 237/1000 | Loss: 0.00009913
Iteration 238/1000 | Loss: 0.00002424
Iteration 239/1000 | Loss: 0.00004057
Iteration 240/1000 | Loss: 0.00004059
Iteration 241/1000 | Loss: 0.00007457
Iteration 242/1000 | Loss: 0.00004011
Iteration 243/1000 | Loss: 0.00003901
Iteration 244/1000 | Loss: 0.00004182
Iteration 245/1000 | Loss: 0.00003602
Iteration 246/1000 | Loss: 0.00004376
Iteration 247/1000 | Loss: 0.00002474
Iteration 248/1000 | Loss: 0.00004250
Iteration 249/1000 | Loss: 0.00003085
Iteration 250/1000 | Loss: 0.00004399
Iteration 251/1000 | Loss: 0.00003273
Iteration 252/1000 | Loss: 0.00004769
Iteration 253/1000 | Loss: 0.00003165
Iteration 254/1000 | Loss: 0.00005194
Iteration 255/1000 | Loss: 0.00003105
Iteration 256/1000 | Loss: 0.00004800
Iteration 257/1000 | Loss: 0.00003255
Iteration 258/1000 | Loss: 0.00009907
Iteration 259/1000 | Loss: 0.00004228
Iteration 260/1000 | Loss: 0.00004354
Iteration 261/1000 | Loss: 0.00012544
Iteration 262/1000 | Loss: 0.00013599
Iteration 263/1000 | Loss: 0.00003963
Iteration 264/1000 | Loss: 0.00004033
Iteration 265/1000 | Loss: 0.00013068
Iteration 266/1000 | Loss: 0.00004491
Iteration 267/1000 | Loss: 0.00004636
Iteration 268/1000 | Loss: 0.00003757
Iteration 269/1000 | Loss: 0.00004591
Iteration 270/1000 | Loss: 0.00003811
Iteration 271/1000 | Loss: 0.00011689
Iteration 272/1000 | Loss: 0.00004653
Iteration 273/1000 | Loss: 0.00006779
Iteration 274/1000 | Loss: 0.00004694
Iteration 275/1000 | Loss: 0.00003619
Iteration 276/1000 | Loss: 0.00004995
Iteration 277/1000 | Loss: 0.00003607
Iteration 278/1000 | Loss: 0.00004464
Iteration 279/1000 | Loss: 0.00003759
Iteration 280/1000 | Loss: 0.00003138
Iteration 281/1000 | Loss: 0.00003488
Iteration 282/1000 | Loss: 0.00003921
Iteration 283/1000 | Loss: 0.00003185
Iteration 284/1000 | Loss: 0.00004219
Iteration 285/1000 | Loss: 0.00003737
Iteration 286/1000 | Loss: 0.00003825
Iteration 287/1000 | Loss: 0.00003820
Iteration 288/1000 | Loss: 0.00003825
Iteration 289/1000 | Loss: 0.00003907
Iteration 290/1000 | Loss: 0.00004035
Iteration 291/1000 | Loss: 0.00003971
Iteration 292/1000 | Loss: 0.00003399
Iteration 293/1000 | Loss: 0.00003795
Iteration 294/1000 | Loss: 0.00003735
Iteration 295/1000 | Loss: 0.00002876
Iteration 296/1000 | Loss: 0.00003713
Iteration 297/1000 | Loss: 0.00007016
Iteration 298/1000 | Loss: 0.00026692
Iteration 299/1000 | Loss: 0.00007415
Iteration 300/1000 | Loss: 0.00004664
Iteration 301/1000 | Loss: 0.00004473
Iteration 302/1000 | Loss: 0.00009879
Iteration 303/1000 | Loss: 0.00002147
Iteration 304/1000 | Loss: 0.00001907
Iteration 305/1000 | Loss: 0.00001788
Iteration 306/1000 | Loss: 0.00024131
Iteration 307/1000 | Loss: 0.00011689
Iteration 308/1000 | Loss: 0.00006754
Iteration 309/1000 | Loss: 0.00003463
Iteration 310/1000 | Loss: 0.00004606
Iteration 311/1000 | Loss: 0.00004021
Iteration 312/1000 | Loss: 0.00006194
Iteration 313/1000 | Loss: 0.00002179
Iteration 314/1000 | Loss: 0.00001757
Iteration 315/1000 | Loss: 0.00014962
Iteration 316/1000 | Loss: 0.00002908
Iteration 317/1000 | Loss: 0.00003585
Iteration 318/1000 | Loss: 0.00001756
Iteration 319/1000 | Loss: 0.00001539
Iteration 320/1000 | Loss: 0.00001503
Iteration 321/1000 | Loss: 0.00001476
Iteration 322/1000 | Loss: 0.00011317
Iteration 323/1000 | Loss: 0.00001498
Iteration 324/1000 | Loss: 0.00001451
Iteration 325/1000 | Loss: 0.00001445
Iteration 326/1000 | Loss: 0.00001445
Iteration 327/1000 | Loss: 0.00001445
Iteration 328/1000 | Loss: 0.00001444
Iteration 329/1000 | Loss: 0.00001444
Iteration 330/1000 | Loss: 0.00001444
Iteration 331/1000 | Loss: 0.00001443
Iteration 332/1000 | Loss: 0.00001443
Iteration 333/1000 | Loss: 0.00001442
Iteration 334/1000 | Loss: 0.00001442
Iteration 335/1000 | Loss: 0.00001442
Iteration 336/1000 | Loss: 0.00001442
Iteration 337/1000 | Loss: 0.00001441
Iteration 338/1000 | Loss: 0.00001441
Iteration 339/1000 | Loss: 0.00001441
Iteration 340/1000 | Loss: 0.00001441
Iteration 341/1000 | Loss: 0.00001441
Iteration 342/1000 | Loss: 0.00001440
Iteration 343/1000 | Loss: 0.00001440
Iteration 344/1000 | Loss: 0.00001440
Iteration 345/1000 | Loss: 0.00001440
Iteration 346/1000 | Loss: 0.00001440
Iteration 347/1000 | Loss: 0.00001440
Iteration 348/1000 | Loss: 0.00001440
Iteration 349/1000 | Loss: 0.00001440
Iteration 350/1000 | Loss: 0.00001439
Iteration 351/1000 | Loss: 0.00001439
Iteration 352/1000 | Loss: 0.00001439
Iteration 353/1000 | Loss: 0.00001439
Iteration 354/1000 | Loss: 0.00001439
Iteration 355/1000 | Loss: 0.00001439
Iteration 356/1000 | Loss: 0.00001439
Iteration 357/1000 | Loss: 0.00001439
Iteration 358/1000 | Loss: 0.00001439
Iteration 359/1000 | Loss: 0.00001439
Iteration 360/1000 | Loss: 0.00001439
Iteration 361/1000 | Loss: 0.00001439
Iteration 362/1000 | Loss: 0.00001439
Iteration 363/1000 | Loss: 0.00001439
Iteration 364/1000 | Loss: 0.00001439
Iteration 365/1000 | Loss: 0.00001439
Iteration 366/1000 | Loss: 0.00001439
Iteration 367/1000 | Loss: 0.00001439
Iteration 368/1000 | Loss: 0.00001439
Iteration 369/1000 | Loss: 0.00001439
Iteration 370/1000 | Loss: 0.00001439
Iteration 371/1000 | Loss: 0.00001439
Iteration 372/1000 | Loss: 0.00001439
Iteration 373/1000 | Loss: 0.00001439
Iteration 374/1000 | Loss: 0.00001439
Iteration 375/1000 | Loss: 0.00001438
Iteration 376/1000 | Loss: 0.00001438
Iteration 377/1000 | Loss: 0.00001438
Iteration 378/1000 | Loss: 0.00001438
Iteration 379/1000 | Loss: 0.00001438
Iteration 380/1000 | Loss: 0.00001438
Iteration 381/1000 | Loss: 0.00001438
Iteration 382/1000 | Loss: 0.00001438
Iteration 383/1000 | Loss: 0.00001438
Iteration 384/1000 | Loss: 0.00001438
Iteration 385/1000 | Loss: 0.00001438
Iteration 386/1000 | Loss: 0.00001438
Iteration 387/1000 | Loss: 0.00001438
Iteration 388/1000 | Loss: 0.00001438
Iteration 389/1000 | Loss: 0.00001438
Iteration 390/1000 | Loss: 0.00001438
Iteration 391/1000 | Loss: 0.00001438
Iteration 392/1000 | Loss: 0.00001438
Iteration 393/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [1.4381964319909457e-05, 1.4381964319909457e-05, 1.4381964319909457e-05, 1.4381964319909457e-05, 1.4381964319909457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4381964319909457e-05

Optimization complete. Final v2v error: 3.160844564437866 mm

Highest mean error: 4.224123954772949 mm for frame 23

Lowest mean error: 2.7607433795928955 mm for frame 59

Saving results

Total time: 500.7160964012146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793429
Iteration 2/25 | Loss: 0.00126223
Iteration 3/25 | Loss: 0.00117744
Iteration 4/25 | Loss: 0.00116899
Iteration 5/25 | Loss: 0.00116642
Iteration 6/25 | Loss: 0.00116620
Iteration 7/25 | Loss: 0.00116620
Iteration 8/25 | Loss: 0.00116620
Iteration 9/25 | Loss: 0.00116620
Iteration 10/25 | Loss: 0.00116620
Iteration 11/25 | Loss: 0.00116620
Iteration 12/25 | Loss: 0.00116620
Iteration 13/25 | Loss: 0.00116620
Iteration 14/25 | Loss: 0.00116620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011662014294415712, 0.0011662014294415712, 0.0011662014294415712, 0.0011662014294415712, 0.0011662014294415712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011662014294415712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47189975
Iteration 2/25 | Loss: 0.00062176
Iteration 3/25 | Loss: 0.00062176
Iteration 4/25 | Loss: 0.00062176
Iteration 5/25 | Loss: 0.00062175
Iteration 6/25 | Loss: 0.00062175
Iteration 7/25 | Loss: 0.00062175
Iteration 8/25 | Loss: 0.00062175
Iteration 9/25 | Loss: 0.00062175
Iteration 10/25 | Loss: 0.00062175
Iteration 11/25 | Loss: 0.00062175
Iteration 12/25 | Loss: 0.00062175
Iteration 13/25 | Loss: 0.00062175
Iteration 14/25 | Loss: 0.00062175
Iteration 15/25 | Loss: 0.00062175
Iteration 16/25 | Loss: 0.00062175
Iteration 17/25 | Loss: 0.00062175
Iteration 18/25 | Loss: 0.00062175
Iteration 19/25 | Loss: 0.00062175
Iteration 20/25 | Loss: 0.00062175
Iteration 21/25 | Loss: 0.00062175
Iteration 22/25 | Loss: 0.00062175
Iteration 23/25 | Loss: 0.00062175
Iteration 24/25 | Loss: 0.00062175
Iteration 25/25 | Loss: 0.00062175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062175
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001259
Iteration 9/1000 | Loss: 0.00001257
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001247
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001192
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001187
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001181
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001167
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001166
Iteration 124/1000 | Loss: 0.00001166
Iteration 125/1000 | Loss: 0.00001166
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001166
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001165
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001163
Iteration 137/1000 | Loss: 0.00001163
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001162
Iteration 142/1000 | Loss: 0.00001162
Iteration 143/1000 | Loss: 0.00001162
Iteration 144/1000 | Loss: 0.00001162
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001161
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001161
Iteration 162/1000 | Loss: 0.00001161
Iteration 163/1000 | Loss: 0.00001161
Iteration 164/1000 | Loss: 0.00001161
Iteration 165/1000 | Loss: 0.00001161
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001160
Iteration 179/1000 | Loss: 0.00001160
Iteration 180/1000 | Loss: 0.00001160
Iteration 181/1000 | Loss: 0.00001160
Iteration 182/1000 | Loss: 0.00001160
Iteration 183/1000 | Loss: 0.00001160
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001159
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001157
Iteration 204/1000 | Loss: 0.00001157
Iteration 205/1000 | Loss: 0.00001157
Iteration 206/1000 | Loss: 0.00001157
Iteration 207/1000 | Loss: 0.00001157
Iteration 208/1000 | Loss: 0.00001157
Iteration 209/1000 | Loss: 0.00001157
Iteration 210/1000 | Loss: 0.00001157
Iteration 211/1000 | Loss: 0.00001157
Iteration 212/1000 | Loss: 0.00001157
Iteration 213/1000 | Loss: 0.00001156
Iteration 214/1000 | Loss: 0.00001156
Iteration 215/1000 | Loss: 0.00001156
Iteration 216/1000 | Loss: 0.00001156
Iteration 217/1000 | Loss: 0.00001156
Iteration 218/1000 | Loss: 0.00001156
Iteration 219/1000 | Loss: 0.00001156
Iteration 220/1000 | Loss: 0.00001156
Iteration 221/1000 | Loss: 0.00001156
Iteration 222/1000 | Loss: 0.00001156
Iteration 223/1000 | Loss: 0.00001156
Iteration 224/1000 | Loss: 0.00001156
Iteration 225/1000 | Loss: 0.00001156
Iteration 226/1000 | Loss: 0.00001156
Iteration 227/1000 | Loss: 0.00001156
Iteration 228/1000 | Loss: 0.00001156
Iteration 229/1000 | Loss: 0.00001156
Iteration 230/1000 | Loss: 0.00001156
Iteration 231/1000 | Loss: 0.00001156
Iteration 232/1000 | Loss: 0.00001156
Iteration 233/1000 | Loss: 0.00001155
Iteration 234/1000 | Loss: 0.00001155
Iteration 235/1000 | Loss: 0.00001155
Iteration 236/1000 | Loss: 0.00001155
Iteration 237/1000 | Loss: 0.00001155
Iteration 238/1000 | Loss: 0.00001155
Iteration 239/1000 | Loss: 0.00001155
Iteration 240/1000 | Loss: 0.00001155
Iteration 241/1000 | Loss: 0.00001155
Iteration 242/1000 | Loss: 0.00001155
Iteration 243/1000 | Loss: 0.00001155
Iteration 244/1000 | Loss: 0.00001155
Iteration 245/1000 | Loss: 0.00001155
Iteration 246/1000 | Loss: 0.00001155
Iteration 247/1000 | Loss: 0.00001155
Iteration 248/1000 | Loss: 0.00001155
Iteration 249/1000 | Loss: 0.00001155
Iteration 250/1000 | Loss: 0.00001155
Iteration 251/1000 | Loss: 0.00001155
Iteration 252/1000 | Loss: 0.00001154
Iteration 253/1000 | Loss: 0.00001154
Iteration 254/1000 | Loss: 0.00001154
Iteration 255/1000 | Loss: 0.00001154
Iteration 256/1000 | Loss: 0.00001154
Iteration 257/1000 | Loss: 0.00001154
Iteration 258/1000 | Loss: 0.00001154
Iteration 259/1000 | Loss: 0.00001154
Iteration 260/1000 | Loss: 0.00001154
Iteration 261/1000 | Loss: 0.00001154
Iteration 262/1000 | Loss: 0.00001154
Iteration 263/1000 | Loss: 0.00001154
Iteration 264/1000 | Loss: 0.00001154
Iteration 265/1000 | Loss: 0.00001154
Iteration 266/1000 | Loss: 0.00001154
Iteration 267/1000 | Loss: 0.00001154
Iteration 268/1000 | Loss: 0.00001154
Iteration 269/1000 | Loss: 0.00001154
Iteration 270/1000 | Loss: 0.00001154
Iteration 271/1000 | Loss: 0.00001154
Iteration 272/1000 | Loss: 0.00001154
Iteration 273/1000 | Loss: 0.00001154
Iteration 274/1000 | Loss: 0.00001154
Iteration 275/1000 | Loss: 0.00001154
Iteration 276/1000 | Loss: 0.00001154
Iteration 277/1000 | Loss: 0.00001154
Iteration 278/1000 | Loss: 0.00001154
Iteration 279/1000 | Loss: 0.00001154
Iteration 280/1000 | Loss: 0.00001154
Iteration 281/1000 | Loss: 0.00001154
Iteration 282/1000 | Loss: 0.00001154
Iteration 283/1000 | Loss: 0.00001154
Iteration 284/1000 | Loss: 0.00001154
Iteration 285/1000 | Loss: 0.00001154
Iteration 286/1000 | Loss: 0.00001154
Iteration 287/1000 | Loss: 0.00001154
Iteration 288/1000 | Loss: 0.00001154
Iteration 289/1000 | Loss: 0.00001154
Iteration 290/1000 | Loss: 0.00001154
Iteration 291/1000 | Loss: 0.00001154
Iteration 292/1000 | Loss: 0.00001154
Iteration 293/1000 | Loss: 0.00001154
Iteration 294/1000 | Loss: 0.00001154
Iteration 295/1000 | Loss: 0.00001154
Iteration 296/1000 | Loss: 0.00001154
Iteration 297/1000 | Loss: 0.00001154
Iteration 298/1000 | Loss: 0.00001154
Iteration 299/1000 | Loss: 0.00001154
Iteration 300/1000 | Loss: 0.00001154
Iteration 301/1000 | Loss: 0.00001154
Iteration 302/1000 | Loss: 0.00001154
Iteration 303/1000 | Loss: 0.00001154
Iteration 304/1000 | Loss: 0.00001154
Iteration 305/1000 | Loss: 0.00001154
Iteration 306/1000 | Loss: 0.00001154
Iteration 307/1000 | Loss: 0.00001154
Iteration 308/1000 | Loss: 0.00001154
Iteration 309/1000 | Loss: 0.00001154
Iteration 310/1000 | Loss: 0.00001154
Iteration 311/1000 | Loss: 0.00001154
Iteration 312/1000 | Loss: 0.00001154
Iteration 313/1000 | Loss: 0.00001154
Iteration 314/1000 | Loss: 0.00001154
Iteration 315/1000 | Loss: 0.00001154
Iteration 316/1000 | Loss: 0.00001154
Iteration 317/1000 | Loss: 0.00001154
Iteration 318/1000 | Loss: 0.00001154
Iteration 319/1000 | Loss: 0.00001154
Iteration 320/1000 | Loss: 0.00001154
Iteration 321/1000 | Loss: 0.00001154
Iteration 322/1000 | Loss: 0.00001154
Iteration 323/1000 | Loss: 0.00001154
Iteration 324/1000 | Loss: 0.00001154
Iteration 325/1000 | Loss: 0.00001154
Iteration 326/1000 | Loss: 0.00001154
Iteration 327/1000 | Loss: 0.00001154
Iteration 328/1000 | Loss: 0.00001154
Iteration 329/1000 | Loss: 0.00001154
Iteration 330/1000 | Loss: 0.00001154
Iteration 331/1000 | Loss: 0.00001154
Iteration 332/1000 | Loss: 0.00001154
Iteration 333/1000 | Loss: 0.00001154
Iteration 334/1000 | Loss: 0.00001154
Iteration 335/1000 | Loss: 0.00001154
Iteration 336/1000 | Loss: 0.00001154
Iteration 337/1000 | Loss: 0.00001154
Iteration 338/1000 | Loss: 0.00001154
Iteration 339/1000 | Loss: 0.00001154
Iteration 340/1000 | Loss: 0.00001154
Iteration 341/1000 | Loss: 0.00001154
Iteration 342/1000 | Loss: 0.00001154
Iteration 343/1000 | Loss: 0.00001154
Iteration 344/1000 | Loss: 0.00001154
Iteration 345/1000 | Loss: 0.00001154
Iteration 346/1000 | Loss: 0.00001154
Iteration 347/1000 | Loss: 0.00001154
Iteration 348/1000 | Loss: 0.00001154
Iteration 349/1000 | Loss: 0.00001154
Iteration 350/1000 | Loss: 0.00001154
Iteration 351/1000 | Loss: 0.00001154
Iteration 352/1000 | Loss: 0.00001154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [1.154157052951632e-05, 1.154157052951632e-05, 1.154157052951632e-05, 1.154157052951632e-05, 1.154157052951632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.154157052951632e-05

Optimization complete. Final v2v error: 2.8845374584198 mm

Highest mean error: 3.1008691787719727 mm for frame 44

Lowest mean error: 2.7302744388580322 mm for frame 149

Saving results

Total time: 44.730560541152954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408716
Iteration 2/25 | Loss: 0.00129033
Iteration 3/25 | Loss: 0.00119085
Iteration 4/25 | Loss: 0.00118357
Iteration 5/25 | Loss: 0.00118130
Iteration 6/25 | Loss: 0.00118089
Iteration 7/25 | Loss: 0.00118089
Iteration 8/25 | Loss: 0.00118090
Iteration 9/25 | Loss: 0.00118090
Iteration 10/25 | Loss: 0.00118089
Iteration 11/25 | Loss: 0.00118090
Iteration 12/25 | Loss: 0.00118090
Iteration 13/25 | Loss: 0.00118090
Iteration 14/25 | Loss: 0.00118090
Iteration 15/25 | Loss: 0.00118089
Iteration 16/25 | Loss: 0.00118090
Iteration 17/25 | Loss: 0.00118089
Iteration 18/25 | Loss: 0.00118089
Iteration 19/25 | Loss: 0.00118090
Iteration 20/25 | Loss: 0.00118089
Iteration 21/25 | Loss: 0.00118089
Iteration 22/25 | Loss: 0.00118089
Iteration 23/25 | Loss: 0.00118090
Iteration 24/25 | Loss: 0.00118089
Iteration 25/25 | Loss: 0.00118089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45822442
Iteration 2/25 | Loss: 0.00050311
Iteration 3/25 | Loss: 0.00050310
Iteration 4/25 | Loss: 0.00050310
Iteration 5/25 | Loss: 0.00050310
Iteration 6/25 | Loss: 0.00050310
Iteration 7/25 | Loss: 0.00050310
Iteration 8/25 | Loss: 0.00050310
Iteration 9/25 | Loss: 0.00050310
Iteration 10/25 | Loss: 0.00050310
Iteration 11/25 | Loss: 0.00050310
Iteration 12/25 | Loss: 0.00050310
Iteration 13/25 | Loss: 0.00050310
Iteration 14/25 | Loss: 0.00050310
Iteration 15/25 | Loss: 0.00050310
Iteration 16/25 | Loss: 0.00050310
Iteration 17/25 | Loss: 0.00050310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005030989414080977, 0.0005030989414080977, 0.0005030989414080977, 0.0005030989414080977, 0.0005030989414080977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005030989414080977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050310
Iteration 2/1000 | Loss: 0.00004155
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002180
Iteration 5/1000 | Loss: 0.00001988
Iteration 6/1000 | Loss: 0.00001891
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001696
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001674
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001661
Iteration 18/1000 | Loss: 0.00001660
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001652
Iteration 26/1000 | Loss: 0.00001651
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001650
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001641
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001639
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001638
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001637
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001628
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001620
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001614
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001613
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001612
Iteration 141/1000 | Loss: 0.00001612
Iteration 142/1000 | Loss: 0.00001612
Iteration 143/1000 | Loss: 0.00001612
Iteration 144/1000 | Loss: 0.00001612
Iteration 145/1000 | Loss: 0.00001611
Iteration 146/1000 | Loss: 0.00001611
Iteration 147/1000 | Loss: 0.00001611
Iteration 148/1000 | Loss: 0.00001611
Iteration 149/1000 | Loss: 0.00001611
Iteration 150/1000 | Loss: 0.00001611
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001610
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001610
Iteration 157/1000 | Loss: 0.00001610
Iteration 158/1000 | Loss: 0.00001609
Iteration 159/1000 | Loss: 0.00001609
Iteration 160/1000 | Loss: 0.00001609
Iteration 161/1000 | Loss: 0.00001609
Iteration 162/1000 | Loss: 0.00001609
Iteration 163/1000 | Loss: 0.00001609
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001609
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001609
Iteration 169/1000 | Loss: 0.00001609
Iteration 170/1000 | Loss: 0.00001609
Iteration 171/1000 | Loss: 0.00001609
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001608
Iteration 179/1000 | Loss: 0.00001608
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00001608
Iteration 182/1000 | Loss: 0.00001608
Iteration 183/1000 | Loss: 0.00001607
Iteration 184/1000 | Loss: 0.00001607
Iteration 185/1000 | Loss: 0.00001607
Iteration 186/1000 | Loss: 0.00001607
Iteration 187/1000 | Loss: 0.00001607
Iteration 188/1000 | Loss: 0.00001607
Iteration 189/1000 | Loss: 0.00001607
Iteration 190/1000 | Loss: 0.00001607
Iteration 191/1000 | Loss: 0.00001606
Iteration 192/1000 | Loss: 0.00001606
Iteration 193/1000 | Loss: 0.00001606
Iteration 194/1000 | Loss: 0.00001606
Iteration 195/1000 | Loss: 0.00001606
Iteration 196/1000 | Loss: 0.00001606
Iteration 197/1000 | Loss: 0.00001605
Iteration 198/1000 | Loss: 0.00001605
Iteration 199/1000 | Loss: 0.00001605
Iteration 200/1000 | Loss: 0.00001605
Iteration 201/1000 | Loss: 0.00001605
Iteration 202/1000 | Loss: 0.00001605
Iteration 203/1000 | Loss: 0.00001604
Iteration 204/1000 | Loss: 0.00001604
Iteration 205/1000 | Loss: 0.00001604
Iteration 206/1000 | Loss: 0.00001604
Iteration 207/1000 | Loss: 0.00001604
Iteration 208/1000 | Loss: 0.00001604
Iteration 209/1000 | Loss: 0.00001604
Iteration 210/1000 | Loss: 0.00001603
Iteration 211/1000 | Loss: 0.00001603
Iteration 212/1000 | Loss: 0.00001603
Iteration 213/1000 | Loss: 0.00001603
Iteration 214/1000 | Loss: 0.00001603
Iteration 215/1000 | Loss: 0.00001602
Iteration 216/1000 | Loss: 0.00001602
Iteration 217/1000 | Loss: 0.00001602
Iteration 218/1000 | Loss: 0.00001602
Iteration 219/1000 | Loss: 0.00001601
Iteration 220/1000 | Loss: 0.00001601
Iteration 221/1000 | Loss: 0.00001601
Iteration 222/1000 | Loss: 0.00001601
Iteration 223/1000 | Loss: 0.00001601
Iteration 224/1000 | Loss: 0.00001600
Iteration 225/1000 | Loss: 0.00001600
Iteration 226/1000 | Loss: 0.00001600
Iteration 227/1000 | Loss: 0.00001600
Iteration 228/1000 | Loss: 0.00001600
Iteration 229/1000 | Loss: 0.00001600
Iteration 230/1000 | Loss: 0.00001600
Iteration 231/1000 | Loss: 0.00001600
Iteration 232/1000 | Loss: 0.00001600
Iteration 233/1000 | Loss: 0.00001600
Iteration 234/1000 | Loss: 0.00001600
Iteration 235/1000 | Loss: 0.00001600
Iteration 236/1000 | Loss: 0.00001600
Iteration 237/1000 | Loss: 0.00001600
Iteration 238/1000 | Loss: 0.00001599
Iteration 239/1000 | Loss: 0.00001599
Iteration 240/1000 | Loss: 0.00001599
Iteration 241/1000 | Loss: 0.00001599
Iteration 242/1000 | Loss: 0.00001599
Iteration 243/1000 | Loss: 0.00001599
Iteration 244/1000 | Loss: 0.00001598
Iteration 245/1000 | Loss: 0.00001598
Iteration 246/1000 | Loss: 0.00001598
Iteration 247/1000 | Loss: 0.00001598
Iteration 248/1000 | Loss: 0.00001598
Iteration 249/1000 | Loss: 0.00001598
Iteration 250/1000 | Loss: 0.00001598
Iteration 251/1000 | Loss: 0.00001598
Iteration 252/1000 | Loss: 0.00001598
Iteration 253/1000 | Loss: 0.00001598
Iteration 254/1000 | Loss: 0.00001598
Iteration 255/1000 | Loss: 0.00001598
Iteration 256/1000 | Loss: 0.00001598
Iteration 257/1000 | Loss: 0.00001598
Iteration 258/1000 | Loss: 0.00001598
Iteration 259/1000 | Loss: 0.00001598
Iteration 260/1000 | Loss: 0.00001598
Iteration 261/1000 | Loss: 0.00001598
Iteration 262/1000 | Loss: 0.00001598
Iteration 263/1000 | Loss: 0.00001598
Iteration 264/1000 | Loss: 0.00001598
Iteration 265/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.59768733283272e-05, 1.59768733283272e-05, 1.59768733283272e-05, 1.59768733283272e-05, 1.59768733283272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.59768733283272e-05

Optimization complete. Final v2v error: 3.316042184829712 mm

Highest mean error: 3.881593942642212 mm for frame 67

Lowest mean error: 3.013356924057007 mm for frame 9

Saving results

Total time: 44.33841919898987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834176
Iteration 2/25 | Loss: 0.00162912
Iteration 3/25 | Loss: 0.00132137
Iteration 4/25 | Loss: 0.00126091
Iteration 5/25 | Loss: 0.00124588
Iteration 6/25 | Loss: 0.00124389
Iteration 7/25 | Loss: 0.00124062
Iteration 8/25 | Loss: 0.00123883
Iteration 9/25 | Loss: 0.00123837
Iteration 10/25 | Loss: 0.00123745
Iteration 11/25 | Loss: 0.00123476
Iteration 12/25 | Loss: 0.00123394
Iteration 13/25 | Loss: 0.00123458
Iteration 14/25 | Loss: 0.00123327
Iteration 15/25 | Loss: 0.00123275
Iteration 16/25 | Loss: 0.00123263
Iteration 17/25 | Loss: 0.00123258
Iteration 18/25 | Loss: 0.00123258
Iteration 19/25 | Loss: 0.00123258
Iteration 20/25 | Loss: 0.00123258
Iteration 21/25 | Loss: 0.00123258
Iteration 22/25 | Loss: 0.00123258
Iteration 23/25 | Loss: 0.00123258
Iteration 24/25 | Loss: 0.00123258
Iteration 25/25 | Loss: 0.00123258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94585228
Iteration 2/25 | Loss: 0.00073256
Iteration 3/25 | Loss: 0.00073255
Iteration 4/25 | Loss: 0.00073255
Iteration 5/25 | Loss: 0.00073255
Iteration 6/25 | Loss: 0.00073255
Iteration 7/25 | Loss: 0.00073255
Iteration 8/25 | Loss: 0.00073255
Iteration 9/25 | Loss: 0.00073255
Iteration 10/25 | Loss: 0.00073255
Iteration 11/25 | Loss: 0.00073255
Iteration 12/25 | Loss: 0.00073255
Iteration 13/25 | Loss: 0.00073255
Iteration 14/25 | Loss: 0.00073255
Iteration 15/25 | Loss: 0.00073255
Iteration 16/25 | Loss: 0.00073255
Iteration 17/25 | Loss: 0.00073255
Iteration 18/25 | Loss: 0.00073255
Iteration 19/25 | Loss: 0.00073255
Iteration 20/25 | Loss: 0.00073255
Iteration 21/25 | Loss: 0.00073255
Iteration 22/25 | Loss: 0.00073255
Iteration 23/25 | Loss: 0.00073255
Iteration 24/25 | Loss: 0.00073255
Iteration 25/25 | Loss: 0.00073255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073255
Iteration 2/1000 | Loss: 0.00002440
Iteration 3/1000 | Loss: 0.00002057
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001832
Iteration 6/1000 | Loss: 0.00001805
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00009903
Iteration 9/1000 | Loss: 0.00001779
Iteration 10/1000 | Loss: 0.00006674
Iteration 11/1000 | Loss: 0.00001759
Iteration 12/1000 | Loss: 0.00004739
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001716
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001688
Iteration 19/1000 | Loss: 0.00001686
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001660
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001659
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00001651
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001649
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001647
Iteration 40/1000 | Loss: 0.00001647
Iteration 41/1000 | Loss: 0.00001647
Iteration 42/1000 | Loss: 0.00001647
Iteration 43/1000 | Loss: 0.00001647
Iteration 44/1000 | Loss: 0.00001647
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001646
Iteration 48/1000 | Loss: 0.00009799
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001636
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001635
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001635
Iteration 60/1000 | Loss: 0.00001635
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001634
Iteration 64/1000 | Loss: 0.00001634
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.634270120121073e-05, 1.634270120121073e-05, 1.634270120121073e-05, 1.634270120121073e-05, 1.634270120121073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.634270120121073e-05

Optimization complete. Final v2v error: 3.3865132331848145 mm

Highest mean error: 3.783860921859741 mm for frame 55

Lowest mean error: 3.002077579498291 mm for frame 239

Saving results

Total time: 68.37219524383545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386830
Iteration 2/25 | Loss: 0.00139391
Iteration 3/25 | Loss: 0.00123033
Iteration 4/25 | Loss: 0.00121349
Iteration 5/25 | Loss: 0.00120713
Iteration 6/25 | Loss: 0.00120627
Iteration 7/25 | Loss: 0.00120627
Iteration 8/25 | Loss: 0.00120627
Iteration 9/25 | Loss: 0.00120627
Iteration 10/25 | Loss: 0.00120627
Iteration 11/25 | Loss: 0.00120627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012062706518918276, 0.0012062706518918276, 0.0012062706518918276, 0.0012062706518918276, 0.0012062706518918276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012062706518918276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57615352
Iteration 2/25 | Loss: 0.00063498
Iteration 3/25 | Loss: 0.00063497
Iteration 4/25 | Loss: 0.00063497
Iteration 5/25 | Loss: 0.00063497
Iteration 6/25 | Loss: 0.00063497
Iteration 7/25 | Loss: 0.00063497
Iteration 8/25 | Loss: 0.00063497
Iteration 9/25 | Loss: 0.00063497
Iteration 10/25 | Loss: 0.00063497
Iteration 11/25 | Loss: 0.00063497
Iteration 12/25 | Loss: 0.00063497
Iteration 13/25 | Loss: 0.00063497
Iteration 14/25 | Loss: 0.00063497
Iteration 15/25 | Loss: 0.00063497
Iteration 16/25 | Loss: 0.00063497
Iteration 17/25 | Loss: 0.00063497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006349678733386099, 0.0006349678733386099, 0.0006349678733386099, 0.0006349678733386099, 0.0006349678733386099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006349678733386099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063497
Iteration 2/1000 | Loss: 0.00003861
Iteration 3/1000 | Loss: 0.00002682
Iteration 4/1000 | Loss: 0.00002355
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002122
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001980
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001925
Iteration 14/1000 | Loss: 0.00001925
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001908
Iteration 17/1000 | Loss: 0.00001908
Iteration 18/1000 | Loss: 0.00001907
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001888
Iteration 25/1000 | Loss: 0.00001887
Iteration 26/1000 | Loss: 0.00001887
Iteration 27/1000 | Loss: 0.00001887
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001887
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001886
Iteration 33/1000 | Loss: 0.00001885
Iteration 34/1000 | Loss: 0.00001885
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001885
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001885
Iteration 39/1000 | Loss: 0.00001885
Iteration 40/1000 | Loss: 0.00001885
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001884
Iteration 44/1000 | Loss: 0.00001884
Iteration 45/1000 | Loss: 0.00001884
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001883
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001877
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001876
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001871
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001869
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001865
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Iteration 149/1000 | Loss: 0.00001852
Iteration 150/1000 | Loss: 0.00001852
Iteration 151/1000 | Loss: 0.00001852
Iteration 152/1000 | Loss: 0.00001852
Iteration 153/1000 | Loss: 0.00001852
Iteration 154/1000 | Loss: 0.00001852
Iteration 155/1000 | Loss: 0.00001852
Iteration 156/1000 | Loss: 0.00001851
Iteration 157/1000 | Loss: 0.00001851
Iteration 158/1000 | Loss: 0.00001851
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Iteration 172/1000 | Loss: 0.00001851
Iteration 173/1000 | Loss: 0.00001851
Iteration 174/1000 | Loss: 0.00001851
Iteration 175/1000 | Loss: 0.00001851
Iteration 176/1000 | Loss: 0.00001851
Iteration 177/1000 | Loss: 0.00001851
Iteration 178/1000 | Loss: 0.00001851
Iteration 179/1000 | Loss: 0.00001851
Iteration 180/1000 | Loss: 0.00001851
Iteration 181/1000 | Loss: 0.00001851
Iteration 182/1000 | Loss: 0.00001851
Iteration 183/1000 | Loss: 0.00001851
Iteration 184/1000 | Loss: 0.00001851
Iteration 185/1000 | Loss: 0.00001851
Iteration 186/1000 | Loss: 0.00001851
Iteration 187/1000 | Loss: 0.00001851
Iteration 188/1000 | Loss: 0.00001851
Iteration 189/1000 | Loss: 0.00001851
Iteration 190/1000 | Loss: 0.00001851
Iteration 191/1000 | Loss: 0.00001851
Iteration 192/1000 | Loss: 0.00001851
Iteration 193/1000 | Loss: 0.00001851
Iteration 194/1000 | Loss: 0.00001851
Iteration 195/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.851190609158948e-05, 1.851190609158948e-05, 1.851190609158948e-05, 1.851190609158948e-05, 1.851190609158948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.851190609158948e-05

Optimization complete. Final v2v error: 3.6123616695404053 mm

Highest mean error: 3.9154775142669678 mm for frame 39

Lowest mean error: 2.963230609893799 mm for frame 4

Saving results

Total time: 42.12792372703552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462215
Iteration 2/25 | Loss: 0.00143226
Iteration 3/25 | Loss: 0.00125539
Iteration 4/25 | Loss: 0.00124472
Iteration 5/25 | Loss: 0.00124276
Iteration 6/25 | Loss: 0.00124268
Iteration 7/25 | Loss: 0.00124268
Iteration 8/25 | Loss: 0.00124268
Iteration 9/25 | Loss: 0.00124268
Iteration 10/25 | Loss: 0.00124268
Iteration 11/25 | Loss: 0.00124268
Iteration 12/25 | Loss: 0.00124268
Iteration 13/25 | Loss: 0.00124268
Iteration 14/25 | Loss: 0.00124268
Iteration 15/25 | Loss: 0.00124268
Iteration 16/25 | Loss: 0.00124268
Iteration 17/25 | Loss: 0.00124268
Iteration 18/25 | Loss: 0.00124268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012426824541762471, 0.0012426824541762471, 0.0012426824541762471, 0.0012426824541762471, 0.0012426824541762471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012426824541762471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44546294
Iteration 2/25 | Loss: 0.00052744
Iteration 3/25 | Loss: 0.00052744
Iteration 4/25 | Loss: 0.00052744
Iteration 5/25 | Loss: 0.00052744
Iteration 6/25 | Loss: 0.00052744
Iteration 7/25 | Loss: 0.00052744
Iteration 8/25 | Loss: 0.00052744
Iteration 9/25 | Loss: 0.00052744
Iteration 10/25 | Loss: 0.00052743
Iteration 11/25 | Loss: 0.00052743
Iteration 12/25 | Loss: 0.00052743
Iteration 13/25 | Loss: 0.00052743
Iteration 14/25 | Loss: 0.00052743
Iteration 15/25 | Loss: 0.00052743
Iteration 16/25 | Loss: 0.00052743
Iteration 17/25 | Loss: 0.00052743
Iteration 18/25 | Loss: 0.00052743
Iteration 19/25 | Loss: 0.00052743
Iteration 20/25 | Loss: 0.00052743
Iteration 21/25 | Loss: 0.00052743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005274346331134439, 0.0005274346331134439, 0.0005274346331134439, 0.0005274346331134439, 0.0005274346331134439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005274346331134439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052743
Iteration 2/1000 | Loss: 0.00003670
Iteration 3/1000 | Loss: 0.00002866
Iteration 4/1000 | Loss: 0.00002621
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002391
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002216
Iteration 10/1000 | Loss: 0.00002189
Iteration 11/1000 | Loss: 0.00002188
Iteration 12/1000 | Loss: 0.00002176
Iteration 13/1000 | Loss: 0.00002176
Iteration 14/1000 | Loss: 0.00002175
Iteration 15/1000 | Loss: 0.00002173
Iteration 16/1000 | Loss: 0.00002170
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002167
Iteration 19/1000 | Loss: 0.00002166
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002165
Iteration 22/1000 | Loss: 0.00002165
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002164
Iteration 25/1000 | Loss: 0.00002164
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002161
Iteration 33/1000 | Loss: 0.00002161
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002160
Iteration 36/1000 | Loss: 0.00002160
Iteration 37/1000 | Loss: 0.00002160
Iteration 38/1000 | Loss: 0.00002160
Iteration 39/1000 | Loss: 0.00002159
Iteration 40/1000 | Loss: 0.00002159
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00002159
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002159
Iteration 45/1000 | Loss: 0.00002159
Iteration 46/1000 | Loss: 0.00002159
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002158
Iteration 50/1000 | Loss: 0.00002157
Iteration 51/1000 | Loss: 0.00002157
Iteration 52/1000 | Loss: 0.00002157
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002154
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002152
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00002151
Iteration 74/1000 | Loss: 0.00002151
Iteration 75/1000 | Loss: 0.00002151
Iteration 76/1000 | Loss: 0.00002151
Iteration 77/1000 | Loss: 0.00002151
Iteration 78/1000 | Loss: 0.00002151
Iteration 79/1000 | Loss: 0.00002151
Iteration 80/1000 | Loss: 0.00002151
Iteration 81/1000 | Loss: 0.00002151
Iteration 82/1000 | Loss: 0.00002151
Iteration 83/1000 | Loss: 0.00002151
Iteration 84/1000 | Loss: 0.00002151
Iteration 85/1000 | Loss: 0.00002151
Iteration 86/1000 | Loss: 0.00002150
Iteration 87/1000 | Loss: 0.00002150
Iteration 88/1000 | Loss: 0.00002150
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002149
Iteration 92/1000 | Loss: 0.00002149
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002148
Iteration 96/1000 | Loss: 0.00002147
Iteration 97/1000 | Loss: 0.00002147
Iteration 98/1000 | Loss: 0.00002147
Iteration 99/1000 | Loss: 0.00002147
Iteration 100/1000 | Loss: 0.00002146
Iteration 101/1000 | Loss: 0.00002146
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002144
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002142
Iteration 113/1000 | Loss: 0.00002142
Iteration 114/1000 | Loss: 0.00002142
Iteration 115/1000 | Loss: 0.00002142
Iteration 116/1000 | Loss: 0.00002142
Iteration 117/1000 | Loss: 0.00002142
Iteration 118/1000 | Loss: 0.00002142
Iteration 119/1000 | Loss: 0.00002142
Iteration 120/1000 | Loss: 0.00002142
Iteration 121/1000 | Loss: 0.00002141
Iteration 122/1000 | Loss: 0.00002141
Iteration 123/1000 | Loss: 0.00002141
Iteration 124/1000 | Loss: 0.00002141
Iteration 125/1000 | Loss: 0.00002141
Iteration 126/1000 | Loss: 0.00002141
Iteration 127/1000 | Loss: 0.00002141
Iteration 128/1000 | Loss: 0.00002141
Iteration 129/1000 | Loss: 0.00002141
Iteration 130/1000 | Loss: 0.00002141
Iteration 131/1000 | Loss: 0.00002140
Iteration 132/1000 | Loss: 0.00002140
Iteration 133/1000 | Loss: 0.00002140
Iteration 134/1000 | Loss: 0.00002140
Iteration 135/1000 | Loss: 0.00002140
Iteration 136/1000 | Loss: 0.00002140
Iteration 137/1000 | Loss: 0.00002139
Iteration 138/1000 | Loss: 0.00002139
Iteration 139/1000 | Loss: 0.00002139
Iteration 140/1000 | Loss: 0.00002139
Iteration 141/1000 | Loss: 0.00002139
Iteration 142/1000 | Loss: 0.00002139
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00002139
Iteration 146/1000 | Loss: 0.00002139
Iteration 147/1000 | Loss: 0.00002139
Iteration 148/1000 | Loss: 0.00002139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.139370553777553e-05, 2.139370553777553e-05, 2.139370553777553e-05, 2.139370553777553e-05, 2.139370553777553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.139370553777553e-05

Optimization complete. Final v2v error: 3.8840718269348145 mm

Highest mean error: 4.030287742614746 mm for frame 0

Lowest mean error: 3.7173824310302734 mm for frame 129

Saving results

Total time: 33.562904834747314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008599
Iteration 2/25 | Loss: 0.00242293
Iteration 3/25 | Loss: 0.00198005
Iteration 4/25 | Loss: 0.00188902
Iteration 5/25 | Loss: 0.00182442
Iteration 6/25 | Loss: 0.00172293
Iteration 7/25 | Loss: 0.00165913
Iteration 8/25 | Loss: 0.00161287
Iteration 9/25 | Loss: 0.00158883
Iteration 10/25 | Loss: 0.00157819
Iteration 11/25 | Loss: 0.00156344
Iteration 12/25 | Loss: 0.00153843
Iteration 13/25 | Loss: 0.00151888
Iteration 14/25 | Loss: 0.00150859
Iteration 15/25 | Loss: 0.00151820
Iteration 16/25 | Loss: 0.00150773
Iteration 17/25 | Loss: 0.00150387
Iteration 18/25 | Loss: 0.00150309
Iteration 19/25 | Loss: 0.00150291
Iteration 20/25 | Loss: 0.00150280
Iteration 21/25 | Loss: 0.00150279
Iteration 22/25 | Loss: 0.00150278
Iteration 23/25 | Loss: 0.00150278
Iteration 24/25 | Loss: 0.00150278
Iteration 25/25 | Loss: 0.00150278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44666791
Iteration 2/25 | Loss: 0.00262905
Iteration 3/25 | Loss: 0.00262905
Iteration 4/25 | Loss: 0.00262905
Iteration 5/25 | Loss: 0.00262905
Iteration 6/25 | Loss: 0.00262905
Iteration 7/25 | Loss: 0.00262905
Iteration 8/25 | Loss: 0.00262905
Iteration 9/25 | Loss: 0.00262905
Iteration 10/25 | Loss: 0.00262905
Iteration 11/25 | Loss: 0.00262905
Iteration 12/25 | Loss: 0.00262904
Iteration 13/25 | Loss: 0.00262904
Iteration 14/25 | Loss: 0.00262904
Iteration 15/25 | Loss: 0.00262904
Iteration 16/25 | Loss: 0.00262904
Iteration 17/25 | Loss: 0.00262904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002629044931381941, 0.002629044931381941, 0.002629044931381941, 0.002629044931381941, 0.002629044931381941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002629044931381941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262904
Iteration 2/1000 | Loss: 0.00035684
Iteration 3/1000 | Loss: 0.00028677
Iteration 4/1000 | Loss: 0.00038089
Iteration 5/1000 | Loss: 0.00018201
Iteration 6/1000 | Loss: 0.00015935
Iteration 7/1000 | Loss: 0.00014879
Iteration 8/1000 | Loss: 0.00013848
Iteration 9/1000 | Loss: 0.00012921
Iteration 10/1000 | Loss: 0.00012317
Iteration 11/1000 | Loss: 0.00011862
Iteration 12/1000 | Loss: 0.00011571
Iteration 13/1000 | Loss: 0.00011316
Iteration 14/1000 | Loss: 0.00011150
Iteration 15/1000 | Loss: 0.00011000
Iteration 16/1000 | Loss: 0.00010859
Iteration 17/1000 | Loss: 0.00010737
Iteration 18/1000 | Loss: 0.00010656
Iteration 19/1000 | Loss: 0.00010566
Iteration 20/1000 | Loss: 0.00016752
Iteration 21/1000 | Loss: 0.00131085
Iteration 22/1000 | Loss: 0.00322406
Iteration 23/1000 | Loss: 0.00089391
Iteration 24/1000 | Loss: 0.00059496
Iteration 25/1000 | Loss: 0.00074789
Iteration 26/1000 | Loss: 0.00026499
Iteration 27/1000 | Loss: 0.00017234
Iteration 28/1000 | Loss: 0.00013494
Iteration 29/1000 | Loss: 0.00010317
Iteration 30/1000 | Loss: 0.00006668
Iteration 31/1000 | Loss: 0.00005266
Iteration 32/1000 | Loss: 0.00004402
Iteration 33/1000 | Loss: 0.00003852
Iteration 34/1000 | Loss: 0.00003422
Iteration 35/1000 | Loss: 0.00003048
Iteration 36/1000 | Loss: 0.00002773
Iteration 37/1000 | Loss: 0.00004712
Iteration 38/1000 | Loss: 0.00003825
Iteration 39/1000 | Loss: 0.00002744
Iteration 40/1000 | Loss: 0.00002432
Iteration 41/1000 | Loss: 0.00003263
Iteration 42/1000 | Loss: 0.00003820
Iteration 43/1000 | Loss: 0.00002658
Iteration 44/1000 | Loss: 0.00002045
Iteration 45/1000 | Loss: 0.00004038
Iteration 46/1000 | Loss: 0.00003337
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001817
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001720
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001674
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001655
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001646
Iteration 60/1000 | Loss: 0.00001646
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001634
Iteration 63/1000 | Loss: 0.00001633
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001629
Iteration 73/1000 | Loss: 0.00001629
Iteration 74/1000 | Loss: 0.00001629
Iteration 75/1000 | Loss: 0.00001629
Iteration 76/1000 | Loss: 0.00001629
Iteration 77/1000 | Loss: 0.00001629
Iteration 78/1000 | Loss: 0.00001629
Iteration 79/1000 | Loss: 0.00001629
Iteration 80/1000 | Loss: 0.00001628
Iteration 81/1000 | Loss: 0.00001628
Iteration 82/1000 | Loss: 0.00001628
Iteration 83/1000 | Loss: 0.00001628
Iteration 84/1000 | Loss: 0.00001628
Iteration 85/1000 | Loss: 0.00001628
Iteration 86/1000 | Loss: 0.00001628
Iteration 87/1000 | Loss: 0.00001628
Iteration 88/1000 | Loss: 0.00001628
Iteration 89/1000 | Loss: 0.00001628
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.627868005016353e-05, 1.627868005016353e-05, 1.627868005016353e-05, 1.627868005016353e-05, 1.627868005016353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.627868005016353e-05

Optimization complete. Final v2v error: 3.3215417861938477 mm

Highest mean error: 10.869539260864258 mm for frame 197

Lowest mean error: 3.07846999168396 mm for frame 4

Saving results

Total time: 132.51040744781494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403930
Iteration 2/25 | Loss: 0.00125656
Iteration 3/25 | Loss: 0.00119068
Iteration 4/25 | Loss: 0.00118061
Iteration 5/25 | Loss: 0.00117636
Iteration 6/25 | Loss: 0.00117622
Iteration 7/25 | Loss: 0.00117622
Iteration 8/25 | Loss: 0.00117622
Iteration 9/25 | Loss: 0.00117622
Iteration 10/25 | Loss: 0.00117622
Iteration 11/25 | Loss: 0.00117622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011762197827920318, 0.0011762197827920318, 0.0011762197827920318, 0.0011762197827920318, 0.0011762197827920318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011762197827920318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47151601
Iteration 2/25 | Loss: 0.00066048
Iteration 3/25 | Loss: 0.00066048
Iteration 4/25 | Loss: 0.00066048
Iteration 5/25 | Loss: 0.00066048
Iteration 6/25 | Loss: 0.00066048
Iteration 7/25 | Loss: 0.00066048
Iteration 8/25 | Loss: 0.00066048
Iteration 9/25 | Loss: 0.00066048
Iteration 10/25 | Loss: 0.00066048
Iteration 11/25 | Loss: 0.00066048
Iteration 12/25 | Loss: 0.00066048
Iteration 13/25 | Loss: 0.00066048
Iteration 14/25 | Loss: 0.00066048
Iteration 15/25 | Loss: 0.00066048
Iteration 16/25 | Loss: 0.00066048
Iteration 17/25 | Loss: 0.00066048
Iteration 18/25 | Loss: 0.00066048
Iteration 19/25 | Loss: 0.00066048
Iteration 20/25 | Loss: 0.00066048
Iteration 21/25 | Loss: 0.00066048
Iteration 22/25 | Loss: 0.00066048
Iteration 23/25 | Loss: 0.00066048
Iteration 24/25 | Loss: 0.00066047
Iteration 25/25 | Loss: 0.00066047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066047
Iteration 2/1000 | Loss: 0.00002110
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001393
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001310
Iteration 7/1000 | Loss: 0.00001309
Iteration 8/1000 | Loss: 0.00001306
Iteration 9/1000 | Loss: 0.00001305
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001299
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001255
Iteration 17/1000 | Loss: 0.00001253
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001237
Iteration 24/1000 | Loss: 0.00001231
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001215
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001209
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001200
Iteration 83/1000 | Loss: 0.00001200
Iteration 84/1000 | Loss: 0.00001200
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001199
Iteration 88/1000 | Loss: 0.00001199
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001197
Iteration 94/1000 | Loss: 0.00001197
Iteration 95/1000 | Loss: 0.00001196
Iteration 96/1000 | Loss: 0.00001196
Iteration 97/1000 | Loss: 0.00001196
Iteration 98/1000 | Loss: 0.00001195
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001195
Iteration 101/1000 | Loss: 0.00001195
Iteration 102/1000 | Loss: 0.00001195
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001194
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001194
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001191
Iteration 116/1000 | Loss: 0.00001191
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001190
Iteration 122/1000 | Loss: 0.00001190
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001190
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001189
Iteration 129/1000 | Loss: 0.00001189
Iteration 130/1000 | Loss: 0.00001189
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001188
Iteration 138/1000 | Loss: 0.00001188
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001188
Iteration 150/1000 | Loss: 0.00001188
Iteration 151/1000 | Loss: 0.00001188
Iteration 152/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1881098544108681e-05, 1.1881098544108681e-05, 1.1881098544108681e-05, 1.1881098544108681e-05, 1.1881098544108681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1881098544108681e-05

Optimization complete. Final v2v error: 2.9623794555664062 mm

Highest mean error: 3.147289276123047 mm for frame 128

Lowest mean error: 2.8454430103302 mm for frame 226

Saving results

Total time: 38.3206102848053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497133
Iteration 2/25 | Loss: 0.00127584
Iteration 3/25 | Loss: 0.00119871
Iteration 4/25 | Loss: 0.00118802
Iteration 5/25 | Loss: 0.00118526
Iteration 6/25 | Loss: 0.00118456
Iteration 7/25 | Loss: 0.00118456
Iteration 8/25 | Loss: 0.00118456
Iteration 9/25 | Loss: 0.00118456
Iteration 10/25 | Loss: 0.00118456
Iteration 11/25 | Loss: 0.00118456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001184558030217886, 0.001184558030217886, 0.001184558030217886, 0.001184558030217886, 0.001184558030217886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184558030217886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46941841
Iteration 2/25 | Loss: 0.00060044
Iteration 3/25 | Loss: 0.00060040
Iteration 4/25 | Loss: 0.00060040
Iteration 5/25 | Loss: 0.00060039
Iteration 6/25 | Loss: 0.00060039
Iteration 7/25 | Loss: 0.00060039
Iteration 8/25 | Loss: 0.00060039
Iteration 9/25 | Loss: 0.00060039
Iteration 10/25 | Loss: 0.00060039
Iteration 11/25 | Loss: 0.00060039
Iteration 12/25 | Loss: 0.00060039
Iteration 13/25 | Loss: 0.00060039
Iteration 14/25 | Loss: 0.00060039
Iteration 15/25 | Loss: 0.00060039
Iteration 16/25 | Loss: 0.00060039
Iteration 17/25 | Loss: 0.00060039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006003932794556022, 0.0006003932794556022, 0.0006003932794556022, 0.0006003932794556022, 0.0006003932794556022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006003932794556022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060039
Iteration 2/1000 | Loss: 0.00002773
Iteration 3/1000 | Loss: 0.00001925
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001597
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001474
Iteration 8/1000 | Loss: 0.00001443
Iteration 9/1000 | Loss: 0.00001429
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001411
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001409
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001393
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001382
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001363
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001351
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001350
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001345
Iteration 64/1000 | Loss: 0.00001345
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001335
Iteration 80/1000 | Loss: 0.00001335
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001331
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001325
Iteration 139/1000 | Loss: 0.00001325
Iteration 140/1000 | Loss: 0.00001325
Iteration 141/1000 | Loss: 0.00001325
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001324
Iteration 148/1000 | Loss: 0.00001324
Iteration 149/1000 | Loss: 0.00001324
Iteration 150/1000 | Loss: 0.00001324
Iteration 151/1000 | Loss: 0.00001324
Iteration 152/1000 | Loss: 0.00001324
Iteration 153/1000 | Loss: 0.00001324
Iteration 154/1000 | Loss: 0.00001324
Iteration 155/1000 | Loss: 0.00001324
Iteration 156/1000 | Loss: 0.00001324
Iteration 157/1000 | Loss: 0.00001324
Iteration 158/1000 | Loss: 0.00001324
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001323
Iteration 165/1000 | Loss: 0.00001323
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001323
Iteration 170/1000 | Loss: 0.00001323
Iteration 171/1000 | Loss: 0.00001323
Iteration 172/1000 | Loss: 0.00001323
Iteration 173/1000 | Loss: 0.00001323
Iteration 174/1000 | Loss: 0.00001323
Iteration 175/1000 | Loss: 0.00001323
Iteration 176/1000 | Loss: 0.00001323
Iteration 177/1000 | Loss: 0.00001323
Iteration 178/1000 | Loss: 0.00001322
Iteration 179/1000 | Loss: 0.00001322
Iteration 180/1000 | Loss: 0.00001322
Iteration 181/1000 | Loss: 0.00001322
Iteration 182/1000 | Loss: 0.00001322
Iteration 183/1000 | Loss: 0.00001322
Iteration 184/1000 | Loss: 0.00001322
Iteration 185/1000 | Loss: 0.00001322
Iteration 186/1000 | Loss: 0.00001322
Iteration 187/1000 | Loss: 0.00001322
Iteration 188/1000 | Loss: 0.00001322
Iteration 189/1000 | Loss: 0.00001322
Iteration 190/1000 | Loss: 0.00001321
Iteration 191/1000 | Loss: 0.00001321
Iteration 192/1000 | Loss: 0.00001321
Iteration 193/1000 | Loss: 0.00001321
Iteration 194/1000 | Loss: 0.00001321
Iteration 195/1000 | Loss: 0.00001321
Iteration 196/1000 | Loss: 0.00001321
Iteration 197/1000 | Loss: 0.00001321
Iteration 198/1000 | Loss: 0.00001321
Iteration 199/1000 | Loss: 0.00001321
Iteration 200/1000 | Loss: 0.00001321
Iteration 201/1000 | Loss: 0.00001321
Iteration 202/1000 | Loss: 0.00001321
Iteration 203/1000 | Loss: 0.00001321
Iteration 204/1000 | Loss: 0.00001321
Iteration 205/1000 | Loss: 0.00001321
Iteration 206/1000 | Loss: 0.00001321
Iteration 207/1000 | Loss: 0.00001321
Iteration 208/1000 | Loss: 0.00001321
Iteration 209/1000 | Loss: 0.00001321
Iteration 210/1000 | Loss: 0.00001321
Iteration 211/1000 | Loss: 0.00001321
Iteration 212/1000 | Loss: 0.00001321
Iteration 213/1000 | Loss: 0.00001321
Iteration 214/1000 | Loss: 0.00001321
Iteration 215/1000 | Loss: 0.00001321
Iteration 216/1000 | Loss: 0.00001321
Iteration 217/1000 | Loss: 0.00001321
Iteration 218/1000 | Loss: 0.00001321
Iteration 219/1000 | Loss: 0.00001321
Iteration 220/1000 | Loss: 0.00001321
Iteration 221/1000 | Loss: 0.00001321
Iteration 222/1000 | Loss: 0.00001321
Iteration 223/1000 | Loss: 0.00001321
Iteration 224/1000 | Loss: 0.00001321
Iteration 225/1000 | Loss: 0.00001321
Iteration 226/1000 | Loss: 0.00001321
Iteration 227/1000 | Loss: 0.00001321
Iteration 228/1000 | Loss: 0.00001321
Iteration 229/1000 | Loss: 0.00001321
Iteration 230/1000 | Loss: 0.00001321
Iteration 231/1000 | Loss: 0.00001321
Iteration 232/1000 | Loss: 0.00001321
Iteration 233/1000 | Loss: 0.00001321
Iteration 234/1000 | Loss: 0.00001321
Iteration 235/1000 | Loss: 0.00001321
Iteration 236/1000 | Loss: 0.00001321
Iteration 237/1000 | Loss: 0.00001321
Iteration 238/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.321196396020241e-05, 1.321196396020241e-05, 1.321196396020241e-05, 1.321196396020241e-05, 1.321196396020241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.321196396020241e-05

Optimization complete. Final v2v error: 3.0386011600494385 mm

Highest mean error: 3.343522787094116 mm for frame 27

Lowest mean error: 2.737062931060791 mm for frame 13

Saving results

Total time: 42.34155011177063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046363
Iteration 2/25 | Loss: 0.00194654
Iteration 3/25 | Loss: 0.00159816
Iteration 4/25 | Loss: 0.00159738
Iteration 5/25 | Loss: 0.00153337
Iteration 6/25 | Loss: 0.00139371
Iteration 7/25 | Loss: 0.00141549
Iteration 8/25 | Loss: 0.00139542
Iteration 9/25 | Loss: 0.00132677
Iteration 10/25 | Loss: 0.00131040
Iteration 11/25 | Loss: 0.00131895
Iteration 12/25 | Loss: 0.00136351
Iteration 13/25 | Loss: 0.00130087
Iteration 14/25 | Loss: 0.00129030
Iteration 15/25 | Loss: 0.00130912
Iteration 16/25 | Loss: 0.00125968
Iteration 17/25 | Loss: 0.00125119
Iteration 18/25 | Loss: 0.00127318
Iteration 19/25 | Loss: 0.00126517
Iteration 20/25 | Loss: 0.00123482
Iteration 21/25 | Loss: 0.00123323
Iteration 22/25 | Loss: 0.00123401
Iteration 23/25 | Loss: 0.00123323
Iteration 24/25 | Loss: 0.00123246
Iteration 25/25 | Loss: 0.00123456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48513770
Iteration 2/25 | Loss: 0.00080106
Iteration 3/25 | Loss: 0.00080106
Iteration 4/25 | Loss: 0.00080106
Iteration 5/25 | Loss: 0.00080106
Iteration 6/25 | Loss: 0.00080106
Iteration 7/25 | Loss: 0.00080106
Iteration 8/25 | Loss: 0.00080106
Iteration 9/25 | Loss: 0.00080106
Iteration 10/25 | Loss: 0.00080106
Iteration 11/25 | Loss: 0.00080106
Iteration 12/25 | Loss: 0.00080105
Iteration 13/25 | Loss: 0.00080105
Iteration 14/25 | Loss: 0.00080105
Iteration 15/25 | Loss: 0.00080105
Iteration 16/25 | Loss: 0.00080105
Iteration 17/25 | Loss: 0.00080105
Iteration 18/25 | Loss: 0.00080105
Iteration 19/25 | Loss: 0.00080105
Iteration 20/25 | Loss: 0.00080105
Iteration 21/25 | Loss: 0.00080105
Iteration 22/25 | Loss: 0.00080105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000801054819021374, 0.000801054819021374, 0.000801054819021374, 0.000801054819021374, 0.000801054819021374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000801054819021374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080105
Iteration 2/1000 | Loss: 0.00006548
Iteration 3/1000 | Loss: 0.00006160
Iteration 4/1000 | Loss: 0.00003674
Iteration 5/1000 | Loss: 0.00003095
Iteration 6/1000 | Loss: 0.00002907
Iteration 7/1000 | Loss: 0.00004404
Iteration 8/1000 | Loss: 0.00004862
Iteration 9/1000 | Loss: 0.00003061
Iteration 10/1000 | Loss: 0.00002647
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00004051
Iteration 13/1000 | Loss: 0.00187216
Iteration 14/1000 | Loss: 0.00011290
Iteration 15/1000 | Loss: 0.00002656
Iteration 16/1000 | Loss: 0.00005091
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002016
Iteration 19/1000 | Loss: 0.00003302
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00022145
Iteration 22/1000 | Loss: 0.00017875
Iteration 23/1000 | Loss: 0.00025375
Iteration 24/1000 | Loss: 0.00020598
Iteration 25/1000 | Loss: 0.00022362
Iteration 26/1000 | Loss: 0.00009697
Iteration 27/1000 | Loss: 0.00002100
Iteration 28/1000 | Loss: 0.00021251
Iteration 29/1000 | Loss: 0.00005468
Iteration 30/1000 | Loss: 0.00015056
Iteration 31/1000 | Loss: 0.00026112
Iteration 32/1000 | Loss: 0.00002954
Iteration 33/1000 | Loss: 0.00018865
Iteration 34/1000 | Loss: 0.00028090
Iteration 35/1000 | Loss: 0.00018578
Iteration 36/1000 | Loss: 0.00026280
Iteration 37/1000 | Loss: 0.00026681
Iteration 38/1000 | Loss: 0.00022035
Iteration 39/1000 | Loss: 0.00017637
Iteration 40/1000 | Loss: 0.00016251
Iteration 41/1000 | Loss: 0.00008666
Iteration 42/1000 | Loss: 0.00009689
Iteration 43/1000 | Loss: 0.00007423
Iteration 44/1000 | Loss: 0.00020685
Iteration 45/1000 | Loss: 0.00003015
Iteration 46/1000 | Loss: 0.00015165
Iteration 47/1000 | Loss: 0.00021139
Iteration 48/1000 | Loss: 0.00003307
Iteration 49/1000 | Loss: 0.00034062
Iteration 50/1000 | Loss: 0.00002279
Iteration 51/1000 | Loss: 0.00002141
Iteration 52/1000 | Loss: 0.00071136
Iteration 53/1000 | Loss: 0.00080106
Iteration 54/1000 | Loss: 0.00018962
Iteration 55/1000 | Loss: 0.00030342
Iteration 56/1000 | Loss: 0.00035535
Iteration 57/1000 | Loss: 0.00045426
Iteration 58/1000 | Loss: 0.00005618
Iteration 59/1000 | Loss: 0.00004048
Iteration 60/1000 | Loss: 0.00031686
Iteration 61/1000 | Loss: 0.00053915
Iteration 62/1000 | Loss: 0.00029970
Iteration 63/1000 | Loss: 0.00003745
Iteration 64/1000 | Loss: 0.00002626
Iteration 65/1000 | Loss: 0.00002416
Iteration 66/1000 | Loss: 0.00002290
Iteration 67/1000 | Loss: 0.00003631
Iteration 68/1000 | Loss: 0.00002196
Iteration 69/1000 | Loss: 0.00003032
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00048606
Iteration 72/1000 | Loss: 0.00002260
Iteration 73/1000 | Loss: 0.00002012
Iteration 74/1000 | Loss: 0.00002493
Iteration 75/1000 | Loss: 0.00004411
Iteration 76/1000 | Loss: 0.00002144
Iteration 77/1000 | Loss: 0.00001791
Iteration 78/1000 | Loss: 0.00022444
Iteration 79/1000 | Loss: 0.00014285
Iteration 80/1000 | Loss: 0.00016755
Iteration 81/1000 | Loss: 0.00033729
Iteration 82/1000 | Loss: 0.00004259
Iteration 83/1000 | Loss: 0.00006065
Iteration 84/1000 | Loss: 0.00016056
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00002692
Iteration 88/1000 | Loss: 0.00001803
Iteration 89/1000 | Loss: 0.00003248
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00004334
Iteration 94/1000 | Loss: 0.00012582
Iteration 95/1000 | Loss: 0.00037456
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001659
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001658
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001656
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001655
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001655
Iteration 130/1000 | Loss: 0.00001655
Iteration 131/1000 | Loss: 0.00001655
Iteration 132/1000 | Loss: 0.00001654
Iteration 133/1000 | Loss: 0.00001654
Iteration 134/1000 | Loss: 0.00001654
Iteration 135/1000 | Loss: 0.00001654
Iteration 136/1000 | Loss: 0.00001654
Iteration 137/1000 | Loss: 0.00001654
Iteration 138/1000 | Loss: 0.00001654
Iteration 139/1000 | Loss: 0.00001654
Iteration 140/1000 | Loss: 0.00001654
Iteration 141/1000 | Loss: 0.00001653
Iteration 142/1000 | Loss: 0.00001653
Iteration 143/1000 | Loss: 0.00001653
Iteration 144/1000 | Loss: 0.00001653
Iteration 145/1000 | Loss: 0.00001653
Iteration 146/1000 | Loss: 0.00001653
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001652
Iteration 150/1000 | Loss: 0.00001652
Iteration 151/1000 | Loss: 0.00001652
Iteration 152/1000 | Loss: 0.00001652
Iteration 153/1000 | Loss: 0.00001652
Iteration 154/1000 | Loss: 0.00001652
Iteration 155/1000 | Loss: 0.00001652
Iteration 156/1000 | Loss: 0.00001652
Iteration 157/1000 | Loss: 0.00001652
Iteration 158/1000 | Loss: 0.00001652
Iteration 159/1000 | Loss: 0.00001652
Iteration 160/1000 | Loss: 0.00001652
Iteration 161/1000 | Loss: 0.00001651
Iteration 162/1000 | Loss: 0.00001651
Iteration 163/1000 | Loss: 0.00001651
Iteration 164/1000 | Loss: 0.00001651
Iteration 165/1000 | Loss: 0.00001651
Iteration 166/1000 | Loss: 0.00001651
Iteration 167/1000 | Loss: 0.00001651
Iteration 168/1000 | Loss: 0.00001651
Iteration 169/1000 | Loss: 0.00001651
Iteration 170/1000 | Loss: 0.00001651
Iteration 171/1000 | Loss: 0.00001651
Iteration 172/1000 | Loss: 0.00001651
Iteration 173/1000 | Loss: 0.00001651
Iteration 174/1000 | Loss: 0.00001651
Iteration 175/1000 | Loss: 0.00001651
Iteration 176/1000 | Loss: 0.00001651
Iteration 177/1000 | Loss: 0.00001651
Iteration 178/1000 | Loss: 0.00001651
Iteration 179/1000 | Loss: 0.00001651
Iteration 180/1000 | Loss: 0.00001651
Iteration 181/1000 | Loss: 0.00001651
Iteration 182/1000 | Loss: 0.00001651
Iteration 183/1000 | Loss: 0.00001651
Iteration 184/1000 | Loss: 0.00001651
Iteration 185/1000 | Loss: 0.00001651
Iteration 186/1000 | Loss: 0.00001651
Iteration 187/1000 | Loss: 0.00001651
Iteration 188/1000 | Loss: 0.00001651
Iteration 189/1000 | Loss: 0.00001651
Iteration 190/1000 | Loss: 0.00001651
Iteration 191/1000 | Loss: 0.00001651
Iteration 192/1000 | Loss: 0.00001651
Iteration 193/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.6511949070263654e-05, 1.6511949070263654e-05, 1.6511949070263654e-05, 1.6511949070263654e-05, 1.6511949070263654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6511949070263654e-05

Optimization complete. Final v2v error: 3.3645987510681152 mm

Highest mean error: 5.690054416656494 mm for frame 68

Lowest mean error: 2.949070453643799 mm for frame 139

Saving results

Total time: 188.20646691322327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00281518
Iteration 2/25 | Loss: 0.00135659
Iteration 3/25 | Loss: 0.00119823
Iteration 4/25 | Loss: 0.00117837
Iteration 5/25 | Loss: 0.00117258
Iteration 6/25 | Loss: 0.00117072
Iteration 7/25 | Loss: 0.00117023
Iteration 8/25 | Loss: 0.00117014
Iteration 9/25 | Loss: 0.00117014
Iteration 10/25 | Loss: 0.00117014
Iteration 11/25 | Loss: 0.00117014
Iteration 12/25 | Loss: 0.00117014
Iteration 13/25 | Loss: 0.00117014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011701409239321947, 0.0011701409239321947, 0.0011701409239321947, 0.0011701409239321947, 0.0011701409239321947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011701409239321947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44916809
Iteration 2/25 | Loss: 0.00050457
Iteration 3/25 | Loss: 0.00050457
Iteration 4/25 | Loss: 0.00050457
Iteration 5/25 | Loss: 0.00050457
Iteration 6/25 | Loss: 0.00050457
Iteration 7/25 | Loss: 0.00050457
Iteration 8/25 | Loss: 0.00050457
Iteration 9/25 | Loss: 0.00050457
Iteration 10/25 | Loss: 0.00050457
Iteration 11/25 | Loss: 0.00050457
Iteration 12/25 | Loss: 0.00050457
Iteration 13/25 | Loss: 0.00050457
Iteration 14/25 | Loss: 0.00050457
Iteration 15/25 | Loss: 0.00050457
Iteration 16/25 | Loss: 0.00050457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005045717698521912, 0.0005045717698521912, 0.0005045717698521912, 0.0005045717698521912, 0.0005045717698521912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005045717698521912

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050457
Iteration 2/1000 | Loss: 0.00004091
Iteration 3/1000 | Loss: 0.00002714
Iteration 4/1000 | Loss: 0.00002176
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001691
Iteration 12/1000 | Loss: 0.00001668
Iteration 13/1000 | Loss: 0.00001655
Iteration 14/1000 | Loss: 0.00001653
Iteration 15/1000 | Loss: 0.00001648
Iteration 16/1000 | Loss: 0.00001646
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001639
Iteration 23/1000 | Loss: 0.00001638
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001636
Iteration 27/1000 | Loss: 0.00001636
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001631
Iteration 37/1000 | Loss: 0.00001631
Iteration 38/1000 | Loss: 0.00001631
Iteration 39/1000 | Loss: 0.00001631
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001623
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001620
Iteration 121/1000 | Loss: 0.00001620
Iteration 122/1000 | Loss: 0.00001620
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Iteration 127/1000 | Loss: 0.00001620
Iteration 128/1000 | Loss: 0.00001620
Iteration 129/1000 | Loss: 0.00001620
Iteration 130/1000 | Loss: 0.00001620
Iteration 131/1000 | Loss: 0.00001620
Iteration 132/1000 | Loss: 0.00001620
Iteration 133/1000 | Loss: 0.00001620
Iteration 134/1000 | Loss: 0.00001620
Iteration 135/1000 | Loss: 0.00001620
Iteration 136/1000 | Loss: 0.00001620
Iteration 137/1000 | Loss: 0.00001620
Iteration 138/1000 | Loss: 0.00001620
Iteration 139/1000 | Loss: 0.00001620
Iteration 140/1000 | Loss: 0.00001620
Iteration 141/1000 | Loss: 0.00001620
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.6198948287637904e-05, 1.6198948287637904e-05, 1.6198948287637904e-05, 1.6198948287637904e-05, 1.6198948287637904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6198948287637904e-05

Optimization complete. Final v2v error: 3.4330010414123535 mm

Highest mean error: 3.87678861618042 mm for frame 83

Lowest mean error: 3.0385448932647705 mm for frame 3

Saving results

Total time: 38.68938064575195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441990
Iteration 2/25 | Loss: 0.00130059
Iteration 3/25 | Loss: 0.00121937
Iteration 4/25 | Loss: 0.00120399
Iteration 5/25 | Loss: 0.00119887
Iteration 6/25 | Loss: 0.00119843
Iteration 7/25 | Loss: 0.00119843
Iteration 8/25 | Loss: 0.00119843
Iteration 9/25 | Loss: 0.00119843
Iteration 10/25 | Loss: 0.00119843
Iteration 11/25 | Loss: 0.00119843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011984292650595307, 0.0011984292650595307, 0.0011984292650595307, 0.0011984292650595307, 0.0011984292650595307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011984292650595307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46413445
Iteration 2/25 | Loss: 0.00062208
Iteration 3/25 | Loss: 0.00062208
Iteration 4/25 | Loss: 0.00062208
Iteration 5/25 | Loss: 0.00062207
Iteration 6/25 | Loss: 0.00062207
Iteration 7/25 | Loss: 0.00062207
Iteration 8/25 | Loss: 0.00062207
Iteration 9/25 | Loss: 0.00062207
Iteration 10/25 | Loss: 0.00062207
Iteration 11/25 | Loss: 0.00062207
Iteration 12/25 | Loss: 0.00062207
Iteration 13/25 | Loss: 0.00062207
Iteration 14/25 | Loss: 0.00062207
Iteration 15/25 | Loss: 0.00062207
Iteration 16/25 | Loss: 0.00062207
Iteration 17/25 | Loss: 0.00062207
Iteration 18/25 | Loss: 0.00062207
Iteration 19/25 | Loss: 0.00062207
Iteration 20/25 | Loss: 0.00062207
Iteration 21/25 | Loss: 0.00062207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006220717332325876, 0.0006220717332325876, 0.0006220717332325876, 0.0006220717332325876, 0.0006220717332325876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006220717332325876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062207
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001799
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001706
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001674
Iteration 12/1000 | Loss: 0.00001659
Iteration 13/1000 | Loss: 0.00001645
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001632
Iteration 16/1000 | Loss: 0.00001632
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001632
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001628
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001626
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001625
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001624
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001618
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001618
Iteration 49/1000 | Loss: 0.00001618
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001615
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001614
Iteration 62/1000 | Loss: 0.00001613
Iteration 63/1000 | Loss: 0.00001613
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001612
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001609
Iteration 74/1000 | Loss: 0.00001609
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001607
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001607
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001606
Iteration 87/1000 | Loss: 0.00001606
Iteration 88/1000 | Loss: 0.00001606
Iteration 89/1000 | Loss: 0.00001606
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001604
Iteration 96/1000 | Loss: 0.00001603
Iteration 97/1000 | Loss: 0.00001603
Iteration 98/1000 | Loss: 0.00001603
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001603
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001601
Iteration 107/1000 | Loss: 0.00001601
Iteration 108/1000 | Loss: 0.00001601
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001600
Iteration 114/1000 | Loss: 0.00001600
Iteration 115/1000 | Loss: 0.00001600
Iteration 116/1000 | Loss: 0.00001600
Iteration 117/1000 | Loss: 0.00001600
Iteration 118/1000 | Loss: 0.00001600
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001600
Iteration 122/1000 | Loss: 0.00001600
Iteration 123/1000 | Loss: 0.00001600
Iteration 124/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.599529423401691e-05, 1.599529423401691e-05, 1.599529423401691e-05, 1.599529423401691e-05, 1.599529423401691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.599529423401691e-05

Optimization complete. Final v2v error: 3.3288705348968506 mm

Highest mean error: 3.615032434463501 mm for frame 97

Lowest mean error: 3.0237081050872803 mm for frame 3

Saving results

Total time: 34.401047229766846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057866
Iteration 2/25 | Loss: 0.00162724
Iteration 3/25 | Loss: 0.00132833
Iteration 4/25 | Loss: 0.00126216
Iteration 5/25 | Loss: 0.00126369
Iteration 6/25 | Loss: 0.00123741
Iteration 7/25 | Loss: 0.00123598
Iteration 8/25 | Loss: 0.00123562
Iteration 9/25 | Loss: 0.00123550
Iteration 10/25 | Loss: 0.00123549
Iteration 11/25 | Loss: 0.00123549
Iteration 12/25 | Loss: 0.00123549
Iteration 13/25 | Loss: 0.00123549
Iteration 14/25 | Loss: 0.00123549
Iteration 15/25 | Loss: 0.00123548
Iteration 16/25 | Loss: 0.00123548
Iteration 17/25 | Loss: 0.00123548
Iteration 18/25 | Loss: 0.00123548
Iteration 19/25 | Loss: 0.00123548
Iteration 20/25 | Loss: 0.00123548
Iteration 21/25 | Loss: 0.00123548
Iteration 22/25 | Loss: 0.00123548
Iteration 23/25 | Loss: 0.00123548
Iteration 24/25 | Loss: 0.00123548
Iteration 25/25 | Loss: 0.00123547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44904840
Iteration 2/25 | Loss: 0.00071897
Iteration 3/25 | Loss: 0.00071897
Iteration 4/25 | Loss: 0.00068713
Iteration 5/25 | Loss: 0.00068713
Iteration 6/25 | Loss: 0.00068712
Iteration 7/25 | Loss: 0.00068712
Iteration 8/25 | Loss: 0.00068712
Iteration 9/25 | Loss: 0.00068712
Iteration 10/25 | Loss: 0.00068712
Iteration 11/25 | Loss: 0.00068712
Iteration 12/25 | Loss: 0.00068712
Iteration 13/25 | Loss: 0.00068712
Iteration 14/25 | Loss: 0.00068712
Iteration 15/25 | Loss: 0.00068712
Iteration 16/25 | Loss: 0.00068712
Iteration 17/25 | Loss: 0.00068712
Iteration 18/25 | Loss: 0.00068712
Iteration 19/25 | Loss: 0.00068712
Iteration 20/25 | Loss: 0.00068712
Iteration 21/25 | Loss: 0.00068712
Iteration 22/25 | Loss: 0.00068712
Iteration 23/25 | Loss: 0.00068712
Iteration 24/25 | Loss: 0.00068712
Iteration 25/25 | Loss: 0.00068712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068712
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00006344
Iteration 4/1000 | Loss: 0.00003668
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00003713
Iteration 7/1000 | Loss: 0.00001851
Iteration 8/1000 | Loss: 0.00004882
Iteration 9/1000 | Loss: 0.00001828
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00007481
Iteration 14/1000 | Loss: 0.00001770
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001716
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00008508
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001693
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00009203
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00001669
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001669
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00001663
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001662
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001659
Iteration 54/1000 | Loss: 0.00001659
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001659
Iteration 62/1000 | Loss: 0.00001658
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001653
Iteration 68/1000 | Loss: 0.00001653
Iteration 69/1000 | Loss: 0.00010389
Iteration 70/1000 | Loss: 0.00003166
Iteration 71/1000 | Loss: 0.00002904
Iteration 72/1000 | Loss: 0.00001656
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001650
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001649
Iteration 83/1000 | Loss: 0.00001648
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00001648
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001646
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001645
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001644
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001642
Iteration 108/1000 | Loss: 0.00001642
Iteration 109/1000 | Loss: 0.00001641
Iteration 110/1000 | Loss: 0.00001641
Iteration 111/1000 | Loss: 0.00001641
Iteration 112/1000 | Loss: 0.00001641
Iteration 113/1000 | Loss: 0.00001640
Iteration 114/1000 | Loss: 0.00007129
Iteration 115/1000 | Loss: 0.00002362
Iteration 116/1000 | Loss: 0.00003753
Iteration 117/1000 | Loss: 0.00001652
Iteration 118/1000 | Loss: 0.00001649
Iteration 119/1000 | Loss: 0.00001648
Iteration 120/1000 | Loss: 0.00001648
Iteration 121/1000 | Loss: 0.00001647
Iteration 122/1000 | Loss: 0.00001647
Iteration 123/1000 | Loss: 0.00001647
Iteration 124/1000 | Loss: 0.00001647
Iteration 125/1000 | Loss: 0.00001647
Iteration 126/1000 | Loss: 0.00001647
Iteration 127/1000 | Loss: 0.00001647
Iteration 128/1000 | Loss: 0.00001646
Iteration 129/1000 | Loss: 0.00001646
Iteration 130/1000 | Loss: 0.00001646
Iteration 131/1000 | Loss: 0.00001646
Iteration 132/1000 | Loss: 0.00001645
Iteration 133/1000 | Loss: 0.00001645
Iteration 134/1000 | Loss: 0.00001645
Iteration 135/1000 | Loss: 0.00001645
Iteration 136/1000 | Loss: 0.00001644
Iteration 137/1000 | Loss: 0.00001644
Iteration 138/1000 | Loss: 0.00001643
Iteration 139/1000 | Loss: 0.00001643
Iteration 140/1000 | Loss: 0.00001642
Iteration 141/1000 | Loss: 0.00001642
Iteration 142/1000 | Loss: 0.00001642
Iteration 143/1000 | Loss: 0.00001641
Iteration 144/1000 | Loss: 0.00001641
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001638
Iteration 150/1000 | Loss: 0.00001638
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001637
Iteration 155/1000 | Loss: 0.00001637
Iteration 156/1000 | Loss: 0.00001637
Iteration 157/1000 | Loss: 0.00001637
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001636
Iteration 162/1000 | Loss: 0.00001636
Iteration 163/1000 | Loss: 0.00001636
Iteration 164/1000 | Loss: 0.00001636
Iteration 165/1000 | Loss: 0.00001636
Iteration 166/1000 | Loss: 0.00001636
Iteration 167/1000 | Loss: 0.00001636
Iteration 168/1000 | Loss: 0.00001636
Iteration 169/1000 | Loss: 0.00001636
Iteration 170/1000 | Loss: 0.00001636
Iteration 171/1000 | Loss: 0.00001636
Iteration 172/1000 | Loss: 0.00001635
Iteration 173/1000 | Loss: 0.00001635
Iteration 174/1000 | Loss: 0.00001635
Iteration 175/1000 | Loss: 0.00001635
Iteration 176/1000 | Loss: 0.00001635
Iteration 177/1000 | Loss: 0.00001635
Iteration 178/1000 | Loss: 0.00001635
Iteration 179/1000 | Loss: 0.00001635
Iteration 180/1000 | Loss: 0.00001635
Iteration 181/1000 | Loss: 0.00001635
Iteration 182/1000 | Loss: 0.00001635
Iteration 183/1000 | Loss: 0.00001635
Iteration 184/1000 | Loss: 0.00001635
Iteration 185/1000 | Loss: 0.00001635
Iteration 186/1000 | Loss: 0.00001635
Iteration 187/1000 | Loss: 0.00001635
Iteration 188/1000 | Loss: 0.00001635
Iteration 189/1000 | Loss: 0.00001635
Iteration 190/1000 | Loss: 0.00001635
Iteration 191/1000 | Loss: 0.00001635
Iteration 192/1000 | Loss: 0.00001635
Iteration 193/1000 | Loss: 0.00001635
Iteration 194/1000 | Loss: 0.00001635
Iteration 195/1000 | Loss: 0.00001635
Iteration 196/1000 | Loss: 0.00001634
Iteration 197/1000 | Loss: 0.00001634
Iteration 198/1000 | Loss: 0.00001634
Iteration 199/1000 | Loss: 0.00001634
Iteration 200/1000 | Loss: 0.00001634
Iteration 201/1000 | Loss: 0.00001634
Iteration 202/1000 | Loss: 0.00001634
Iteration 203/1000 | Loss: 0.00001634
Iteration 204/1000 | Loss: 0.00001634
Iteration 205/1000 | Loss: 0.00001634
Iteration 206/1000 | Loss: 0.00001634
Iteration 207/1000 | Loss: 0.00001634
Iteration 208/1000 | Loss: 0.00001634
Iteration 209/1000 | Loss: 0.00001634
Iteration 210/1000 | Loss: 0.00001634
Iteration 211/1000 | Loss: 0.00001634
Iteration 212/1000 | Loss: 0.00001634
Iteration 213/1000 | Loss: 0.00001634
Iteration 214/1000 | Loss: 0.00001634
Iteration 215/1000 | Loss: 0.00001634
Iteration 216/1000 | Loss: 0.00001634
Iteration 217/1000 | Loss: 0.00001634
Iteration 218/1000 | Loss: 0.00001634
Iteration 219/1000 | Loss: 0.00001634
Iteration 220/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.6339196008630097e-05, 1.6339196008630097e-05, 1.6339196008630097e-05, 1.6339196008630097e-05, 1.6339196008630097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6339196008630097e-05

Optimization complete. Final v2v error: 3.4271233081817627 mm

Highest mean error: 4.087597846984863 mm for frame 20

Lowest mean error: 3.2306606769561768 mm for frame 3

Saving results

Total time: 70.223397731781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036269
Iteration 2/25 | Loss: 0.00165865
Iteration 3/25 | Loss: 0.00129165
Iteration 4/25 | Loss: 0.00125394
Iteration 5/25 | Loss: 0.00122531
Iteration 6/25 | Loss: 0.00122452
Iteration 7/25 | Loss: 0.00120496
Iteration 8/25 | Loss: 0.00120405
Iteration 9/25 | Loss: 0.00120005
Iteration 10/25 | Loss: 0.00119974
Iteration 11/25 | Loss: 0.00119960
Iteration 12/25 | Loss: 0.00119957
Iteration 13/25 | Loss: 0.00119957
Iteration 14/25 | Loss: 0.00119952
Iteration 15/25 | Loss: 0.00119951
Iteration 16/25 | Loss: 0.00119950
Iteration 17/25 | Loss: 0.00119950
Iteration 18/25 | Loss: 0.00119950
Iteration 19/25 | Loss: 0.00119949
Iteration 20/25 | Loss: 0.00119949
Iteration 21/25 | Loss: 0.00119949
Iteration 22/25 | Loss: 0.00119949
Iteration 23/25 | Loss: 0.00119949
Iteration 24/25 | Loss: 0.00119949
Iteration 25/25 | Loss: 0.00119948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.47326088
Iteration 2/25 | Loss: 0.00061666
Iteration 3/25 | Loss: 0.00061666
Iteration 4/25 | Loss: 0.00061666
Iteration 5/25 | Loss: 0.00061666
Iteration 6/25 | Loss: 0.00061665
Iteration 7/25 | Loss: 0.00061665
Iteration 8/25 | Loss: 0.00061665
Iteration 9/25 | Loss: 0.00061665
Iteration 10/25 | Loss: 0.00061665
Iteration 11/25 | Loss: 0.00061665
Iteration 12/25 | Loss: 0.00061665
Iteration 13/25 | Loss: 0.00061665
Iteration 14/25 | Loss: 0.00061665
Iteration 15/25 | Loss: 0.00061665
Iteration 16/25 | Loss: 0.00061665
Iteration 17/25 | Loss: 0.00061665
Iteration 18/25 | Loss: 0.00061665
Iteration 19/25 | Loss: 0.00061665
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006166528328321874, 0.0006166528328321874, 0.0006166528328321874, 0.0006166528328321874, 0.0006166528328321874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006166528328321874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061665
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001640
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001513
Iteration 12/1000 | Loss: 0.00001497
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001487
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001478
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001463
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001457
Iteration 81/1000 | Loss: 0.00001457
Iteration 82/1000 | Loss: 0.00001457
Iteration 83/1000 | Loss: 0.00001457
Iteration 84/1000 | Loss: 0.00001456
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001456
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001456
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001455
Iteration 95/1000 | Loss: 0.00001455
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001455
Iteration 100/1000 | Loss: 0.00001455
Iteration 101/1000 | Loss: 0.00001455
Iteration 102/1000 | Loss: 0.00001455
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001454
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001452
Iteration 116/1000 | Loss: 0.00001452
Iteration 117/1000 | Loss: 0.00001452
Iteration 118/1000 | Loss: 0.00001452
Iteration 119/1000 | Loss: 0.00001452
Iteration 120/1000 | Loss: 0.00001452
Iteration 121/1000 | Loss: 0.00001452
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00001451
Iteration 125/1000 | Loss: 0.00001451
Iteration 126/1000 | Loss: 0.00001451
Iteration 127/1000 | Loss: 0.00001451
Iteration 128/1000 | Loss: 0.00001451
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001450
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001450
Iteration 135/1000 | Loss: 0.00001450
Iteration 136/1000 | Loss: 0.00001450
Iteration 137/1000 | Loss: 0.00001450
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001449
Iteration 140/1000 | Loss: 0.00001449
Iteration 141/1000 | Loss: 0.00001449
Iteration 142/1000 | Loss: 0.00001449
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001449
Iteration 147/1000 | Loss: 0.00001449
Iteration 148/1000 | Loss: 0.00001449
Iteration 149/1000 | Loss: 0.00001449
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001448
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001447
Iteration 167/1000 | Loss: 0.00001447
Iteration 168/1000 | Loss: 0.00001447
Iteration 169/1000 | Loss: 0.00001447
Iteration 170/1000 | Loss: 0.00001447
Iteration 171/1000 | Loss: 0.00001447
Iteration 172/1000 | Loss: 0.00001447
Iteration 173/1000 | Loss: 0.00001446
Iteration 174/1000 | Loss: 0.00001446
Iteration 175/1000 | Loss: 0.00001446
Iteration 176/1000 | Loss: 0.00001446
Iteration 177/1000 | Loss: 0.00001446
Iteration 178/1000 | Loss: 0.00001446
Iteration 179/1000 | Loss: 0.00001446
Iteration 180/1000 | Loss: 0.00001446
Iteration 181/1000 | Loss: 0.00001446
Iteration 182/1000 | Loss: 0.00001446
Iteration 183/1000 | Loss: 0.00001446
Iteration 184/1000 | Loss: 0.00001446
Iteration 185/1000 | Loss: 0.00001446
Iteration 186/1000 | Loss: 0.00001446
Iteration 187/1000 | Loss: 0.00001446
Iteration 188/1000 | Loss: 0.00001446
Iteration 189/1000 | Loss: 0.00001446
Iteration 190/1000 | Loss: 0.00001446
Iteration 191/1000 | Loss: 0.00001446
Iteration 192/1000 | Loss: 0.00001446
Iteration 193/1000 | Loss: 0.00001446
Iteration 194/1000 | Loss: 0.00001446
Iteration 195/1000 | Loss: 0.00001446
Iteration 196/1000 | Loss: 0.00001446
Iteration 197/1000 | Loss: 0.00001446
Iteration 198/1000 | Loss: 0.00001446
Iteration 199/1000 | Loss: 0.00001446
Iteration 200/1000 | Loss: 0.00001446
Iteration 201/1000 | Loss: 0.00001446
Iteration 202/1000 | Loss: 0.00001446
Iteration 203/1000 | Loss: 0.00001446
Iteration 204/1000 | Loss: 0.00001446
Iteration 205/1000 | Loss: 0.00001446
Iteration 206/1000 | Loss: 0.00001446
Iteration 207/1000 | Loss: 0.00001446
Iteration 208/1000 | Loss: 0.00001446
Iteration 209/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4458089026447851e-05, 1.4458089026447851e-05, 1.4458089026447851e-05, 1.4458089026447851e-05, 1.4458089026447851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4458089026447851e-05

Optimization complete. Final v2v error: 3.2158589363098145 mm

Highest mean error: 3.6585495471954346 mm for frame 182

Lowest mean error: 2.938899278640747 mm for frame 37

Saving results

Total time: 56.082358598709106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775151
Iteration 2/25 | Loss: 0.00175413
Iteration 3/25 | Loss: 0.00140073
Iteration 4/25 | Loss: 0.00131319
Iteration 5/25 | Loss: 0.00130383
Iteration 6/25 | Loss: 0.00129129
Iteration 7/25 | Loss: 0.00129329
Iteration 8/25 | Loss: 0.00128844
Iteration 9/25 | Loss: 0.00128875
Iteration 10/25 | Loss: 0.00127955
Iteration 11/25 | Loss: 0.00127683
Iteration 12/25 | Loss: 0.00127356
Iteration 13/25 | Loss: 0.00127301
Iteration 14/25 | Loss: 0.00127288
Iteration 15/25 | Loss: 0.00127285
Iteration 16/25 | Loss: 0.00127285
Iteration 17/25 | Loss: 0.00127285
Iteration 18/25 | Loss: 0.00127285
Iteration 19/25 | Loss: 0.00127284
Iteration 20/25 | Loss: 0.00127284
Iteration 21/25 | Loss: 0.00127284
Iteration 22/25 | Loss: 0.00127284
Iteration 23/25 | Loss: 0.00127284
Iteration 24/25 | Loss: 0.00127284
Iteration 25/25 | Loss: 0.00127284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18122482
Iteration 2/25 | Loss: 0.00078150
Iteration 3/25 | Loss: 0.00078148
Iteration 4/25 | Loss: 0.00078148
Iteration 5/25 | Loss: 0.00078147
Iteration 6/25 | Loss: 0.00078147
Iteration 7/25 | Loss: 0.00078147
Iteration 8/25 | Loss: 0.00078147
Iteration 9/25 | Loss: 0.00078147
Iteration 10/25 | Loss: 0.00078147
Iteration 11/25 | Loss: 0.00078147
Iteration 12/25 | Loss: 0.00078147
Iteration 13/25 | Loss: 0.00078147
Iteration 14/25 | Loss: 0.00078147
Iteration 15/25 | Loss: 0.00078147
Iteration 16/25 | Loss: 0.00078147
Iteration 17/25 | Loss: 0.00078147
Iteration 18/25 | Loss: 0.00078147
Iteration 19/25 | Loss: 0.00078147
Iteration 20/25 | Loss: 0.00078147
Iteration 21/25 | Loss: 0.00078147
Iteration 22/25 | Loss: 0.00078147
Iteration 23/25 | Loss: 0.00078147
Iteration 24/25 | Loss: 0.00078147
Iteration 25/25 | Loss: 0.00078147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078147
Iteration 2/1000 | Loss: 0.00004103
Iteration 3/1000 | Loss: 0.00002355
Iteration 4/1000 | Loss: 0.00002144
Iteration 5/1000 | Loss: 0.00002036
Iteration 6/1000 | Loss: 0.00001950
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001744
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001743
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00003618
Iteration 30/1000 | Loss: 0.00019864
Iteration 31/1000 | Loss: 0.00002794
Iteration 32/1000 | Loss: 0.00002394
Iteration 33/1000 | Loss: 0.00002629
Iteration 34/1000 | Loss: 0.00001992
Iteration 35/1000 | Loss: 0.00001899
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001822
Iteration 38/1000 | Loss: 0.00003209
Iteration 39/1000 | Loss: 0.00002493
Iteration 40/1000 | Loss: 0.00003128
Iteration 41/1000 | Loss: 0.00002414
Iteration 42/1000 | Loss: 0.00002610
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00001936
Iteration 45/1000 | Loss: 0.00002219
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002983
Iteration 48/1000 | Loss: 0.00003429
Iteration 49/1000 | Loss: 0.00002922
Iteration 50/1000 | Loss: 0.00002922
Iteration 51/1000 | Loss: 0.00002980
Iteration 52/1000 | Loss: 0.00002918
Iteration 53/1000 | Loss: 0.00003300
Iteration 54/1000 | Loss: 0.00002218
Iteration 55/1000 | Loss: 0.00002745
Iteration 56/1000 | Loss: 0.00002833
Iteration 57/1000 | Loss: 0.00002163
Iteration 58/1000 | Loss: 0.00001867
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001746
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001742
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00001727
Iteration 83/1000 | Loss: 0.00001727
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001725
Iteration 89/1000 | Loss: 0.00001725
Iteration 90/1000 | Loss: 0.00001725
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001725
Iteration 93/1000 | Loss: 0.00001725
Iteration 94/1000 | Loss: 0.00001725
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001724
Iteration 97/1000 | Loss: 0.00001724
Iteration 98/1000 | Loss: 0.00001724
Iteration 99/1000 | Loss: 0.00001724
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001722
Iteration 118/1000 | Loss: 0.00001722
Iteration 119/1000 | Loss: 0.00001722
Iteration 120/1000 | Loss: 0.00001721
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001720
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001718
Iteration 130/1000 | Loss: 0.00001718
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001717
Iteration 137/1000 | Loss: 0.00001717
Iteration 138/1000 | Loss: 0.00001717
Iteration 139/1000 | Loss: 0.00001717
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001717
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001717
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001715
Iteration 157/1000 | Loss: 0.00001715
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00001714
Iteration 167/1000 | Loss: 0.00001714
Iteration 168/1000 | Loss: 0.00001714
Iteration 169/1000 | Loss: 0.00001714
Iteration 170/1000 | Loss: 0.00001714
Iteration 171/1000 | Loss: 0.00001714
Iteration 172/1000 | Loss: 0.00001714
Iteration 173/1000 | Loss: 0.00001714
Iteration 174/1000 | Loss: 0.00001713
Iteration 175/1000 | Loss: 0.00001713
Iteration 176/1000 | Loss: 0.00001713
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001713
Iteration 181/1000 | Loss: 0.00001713
Iteration 182/1000 | Loss: 0.00001712
Iteration 183/1000 | Loss: 0.00001712
Iteration 184/1000 | Loss: 0.00001712
Iteration 185/1000 | Loss: 0.00001712
Iteration 186/1000 | Loss: 0.00001712
Iteration 187/1000 | Loss: 0.00001712
Iteration 188/1000 | Loss: 0.00001712
Iteration 189/1000 | Loss: 0.00001711
Iteration 190/1000 | Loss: 0.00001711
Iteration 191/1000 | Loss: 0.00001711
Iteration 192/1000 | Loss: 0.00001710
Iteration 193/1000 | Loss: 0.00001710
Iteration 194/1000 | Loss: 0.00001710
Iteration 195/1000 | Loss: 0.00001709
Iteration 196/1000 | Loss: 0.00001709
Iteration 197/1000 | Loss: 0.00001709
Iteration 198/1000 | Loss: 0.00001708
Iteration 199/1000 | Loss: 0.00001708
Iteration 200/1000 | Loss: 0.00001708
Iteration 201/1000 | Loss: 0.00001707
Iteration 202/1000 | Loss: 0.00001707
Iteration 203/1000 | Loss: 0.00001707
Iteration 204/1000 | Loss: 0.00001706
Iteration 205/1000 | Loss: 0.00001706
Iteration 206/1000 | Loss: 0.00001706
Iteration 207/1000 | Loss: 0.00001706
Iteration 208/1000 | Loss: 0.00001705
Iteration 209/1000 | Loss: 0.00001705
Iteration 210/1000 | Loss: 0.00001704
Iteration 211/1000 | Loss: 0.00001704
Iteration 212/1000 | Loss: 0.00001704
Iteration 213/1000 | Loss: 0.00001704
Iteration 214/1000 | Loss: 0.00001703
Iteration 215/1000 | Loss: 0.00001703
Iteration 216/1000 | Loss: 0.00001702
Iteration 217/1000 | Loss: 0.00001702
Iteration 218/1000 | Loss: 0.00001702
Iteration 219/1000 | Loss: 0.00001702
Iteration 220/1000 | Loss: 0.00001702
Iteration 221/1000 | Loss: 0.00001701
Iteration 222/1000 | Loss: 0.00001701
Iteration 223/1000 | Loss: 0.00001701
Iteration 224/1000 | Loss: 0.00001701
Iteration 225/1000 | Loss: 0.00001701
Iteration 226/1000 | Loss: 0.00001700
Iteration 227/1000 | Loss: 0.00001700
Iteration 228/1000 | Loss: 0.00001700
Iteration 229/1000 | Loss: 0.00001700
Iteration 230/1000 | Loss: 0.00001700
Iteration 231/1000 | Loss: 0.00001700
Iteration 232/1000 | Loss: 0.00001700
Iteration 233/1000 | Loss: 0.00001699
Iteration 234/1000 | Loss: 0.00001699
Iteration 235/1000 | Loss: 0.00001699
Iteration 236/1000 | Loss: 0.00001699
Iteration 237/1000 | Loss: 0.00001699
Iteration 238/1000 | Loss: 0.00001699
Iteration 239/1000 | Loss: 0.00001699
Iteration 240/1000 | Loss: 0.00001698
Iteration 241/1000 | Loss: 0.00001698
Iteration 242/1000 | Loss: 0.00001697
Iteration 243/1000 | Loss: 0.00001697
Iteration 244/1000 | Loss: 0.00001697
Iteration 245/1000 | Loss: 0.00001697
Iteration 246/1000 | Loss: 0.00001697
Iteration 247/1000 | Loss: 0.00001697
Iteration 248/1000 | Loss: 0.00001697
Iteration 249/1000 | Loss: 0.00001696
Iteration 250/1000 | Loss: 0.00001696
Iteration 251/1000 | Loss: 0.00001696
Iteration 252/1000 | Loss: 0.00001696
Iteration 253/1000 | Loss: 0.00001695
Iteration 254/1000 | Loss: 0.00001695
Iteration 255/1000 | Loss: 0.00001695
Iteration 256/1000 | Loss: 0.00001694
Iteration 257/1000 | Loss: 0.00001694
Iteration 258/1000 | Loss: 0.00003103
Iteration 259/1000 | Loss: 0.00003103
Iteration 260/1000 | Loss: 0.00002652
Iteration 261/1000 | Loss: 0.00001695
Iteration 262/1000 | Loss: 0.00003061
Iteration 263/1000 | Loss: 0.00002620
Iteration 264/1000 | Loss: 0.00001695
Iteration 265/1000 | Loss: 0.00003014
Iteration 266/1000 | Loss: 0.00002628
Iteration 267/1000 | Loss: 0.00001695
Iteration 268/1000 | Loss: 0.00003011
Iteration 269/1000 | Loss: 0.00002488
Iteration 270/1000 | Loss: 0.00002980
Iteration 271/1000 | Loss: 0.00002368
Iteration 272/1000 | Loss: 0.00002991
Iteration 273/1000 | Loss: 0.00002208
Iteration 274/1000 | Loss: 0.00002969
Iteration 275/1000 | Loss: 0.00001845
Iteration 276/1000 | Loss: 0.00002001
Iteration 277/1000 | Loss: 0.00002777
Iteration 278/1000 | Loss: 0.00001949
Iteration 279/1000 | Loss: 0.00002494
Iteration 280/1000 | Loss: 0.00001862
Iteration 281/1000 | Loss: 0.00002649
Iteration 282/1000 | Loss: 0.00001816
Iteration 283/1000 | Loss: 0.00002703
Iteration 284/1000 | Loss: 0.00001829
Iteration 285/1000 | Loss: 0.00002155
Iteration 286/1000 | Loss: 0.00001843
Iteration 287/1000 | Loss: 0.00002372
Iteration 288/1000 | Loss: 0.00001766
Iteration 289/1000 | Loss: 0.00002577
Iteration 290/1000 | Loss: 0.00001815
Iteration 291/1000 | Loss: 0.00001761
Iteration 292/1000 | Loss: 0.00001735
Iteration 293/1000 | Loss: 0.00002179
Iteration 294/1000 | Loss: 0.00001749
Iteration 295/1000 | Loss: 0.00002064
Iteration 296/1000 | Loss: 0.00001741
Iteration 297/1000 | Loss: 0.00001740
Iteration 298/1000 | Loss: 0.00001782
Iteration 299/1000 | Loss: 0.00001729
Iteration 300/1000 | Loss: 0.00002090
Iteration 301/1000 | Loss: 0.00001702
Iteration 302/1000 | Loss: 0.00001687
Iteration 303/1000 | Loss: 0.00001687
Iteration 304/1000 | Loss: 0.00001686
Iteration 305/1000 | Loss: 0.00001686
Iteration 306/1000 | Loss: 0.00001685
Iteration 307/1000 | Loss: 0.00001684
Iteration 308/1000 | Loss: 0.00001684
Iteration 309/1000 | Loss: 0.00001683
Iteration 310/1000 | Loss: 0.00001679
Iteration 311/1000 | Loss: 0.00001676
Iteration 312/1000 | Loss: 0.00001676
Iteration 313/1000 | Loss: 0.00001675
Iteration 314/1000 | Loss: 0.00001675
Iteration 315/1000 | Loss: 0.00001675
Iteration 316/1000 | Loss: 0.00001674
Iteration 317/1000 | Loss: 0.00001674
Iteration 318/1000 | Loss: 0.00001674
Iteration 319/1000 | Loss: 0.00001673
Iteration 320/1000 | Loss: 0.00001673
Iteration 321/1000 | Loss: 0.00001673
Iteration 322/1000 | Loss: 0.00001673
Iteration 323/1000 | Loss: 0.00001672
Iteration 324/1000 | Loss: 0.00001672
Iteration 325/1000 | Loss: 0.00001672
Iteration 326/1000 | Loss: 0.00001672
Iteration 327/1000 | Loss: 0.00001671
Iteration 328/1000 | Loss: 0.00001671
Iteration 329/1000 | Loss: 0.00001671
Iteration 330/1000 | Loss: 0.00001671
Iteration 331/1000 | Loss: 0.00001670
Iteration 332/1000 | Loss: 0.00001670
Iteration 333/1000 | Loss: 0.00001670
Iteration 334/1000 | Loss: 0.00001670
Iteration 335/1000 | Loss: 0.00001669
Iteration 336/1000 | Loss: 0.00001669
Iteration 337/1000 | Loss: 0.00001669
Iteration 338/1000 | Loss: 0.00001668
Iteration 339/1000 | Loss: 0.00001668
Iteration 340/1000 | Loss: 0.00001668
Iteration 341/1000 | Loss: 0.00001668
Iteration 342/1000 | Loss: 0.00001668
Iteration 343/1000 | Loss: 0.00001668
Iteration 344/1000 | Loss: 0.00001668
Iteration 345/1000 | Loss: 0.00001667
Iteration 346/1000 | Loss: 0.00001667
Iteration 347/1000 | Loss: 0.00001667
Iteration 348/1000 | Loss: 0.00001667
Iteration 349/1000 | Loss: 0.00001667
Iteration 350/1000 | Loss: 0.00001667
Iteration 351/1000 | Loss: 0.00001667
Iteration 352/1000 | Loss: 0.00001667
Iteration 353/1000 | Loss: 0.00001667
Iteration 354/1000 | Loss: 0.00001667
Iteration 355/1000 | Loss: 0.00001667
Iteration 356/1000 | Loss: 0.00001667
Iteration 357/1000 | Loss: 0.00001667
Iteration 358/1000 | Loss: 0.00001667
Iteration 359/1000 | Loss: 0.00001667
Iteration 360/1000 | Loss: 0.00001667
Iteration 361/1000 | Loss: 0.00001667
Iteration 362/1000 | Loss: 0.00001667
Iteration 363/1000 | Loss: 0.00001667
Iteration 364/1000 | Loss: 0.00001667
Iteration 365/1000 | Loss: 0.00001667
Iteration 366/1000 | Loss: 0.00001667
Iteration 367/1000 | Loss: 0.00001667
Iteration 368/1000 | Loss: 0.00001667
Iteration 369/1000 | Loss: 0.00001667
Iteration 370/1000 | Loss: 0.00001667
Iteration 371/1000 | Loss: 0.00001667
Iteration 372/1000 | Loss: 0.00001667
Iteration 373/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 373. Stopping optimization.
Last 5 losses: [1.6665873772581108e-05, 1.6665873772581108e-05, 1.6665873772581108e-05, 1.6665873772581108e-05, 1.6665873772581108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6665873772581108e-05

Optimization complete. Final v2v error: 3.402794361114502 mm

Highest mean error: 4.298603534698486 mm for frame 72

Lowest mean error: 3.105039119720459 mm for frame 16

Saving results

Total time: 187.46665120124817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00633731
Iteration 2/25 | Loss: 0.00125670
Iteration 3/25 | Loss: 0.00117998
Iteration 4/25 | Loss: 0.00116878
Iteration 5/25 | Loss: 0.00116454
Iteration 6/25 | Loss: 0.00116383
Iteration 7/25 | Loss: 0.00116383
Iteration 8/25 | Loss: 0.00116383
Iteration 9/25 | Loss: 0.00116383
Iteration 10/25 | Loss: 0.00116383
Iteration 11/25 | Loss: 0.00116383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011638280702754855, 0.0011638280702754855, 0.0011638280702754855, 0.0011638280702754855, 0.0011638280702754855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011638280702754855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51095307
Iteration 2/25 | Loss: 0.00065188
Iteration 3/25 | Loss: 0.00065188
Iteration 4/25 | Loss: 0.00065188
Iteration 5/25 | Loss: 0.00065188
Iteration 6/25 | Loss: 0.00065188
Iteration 7/25 | Loss: 0.00065188
Iteration 8/25 | Loss: 0.00065188
Iteration 9/25 | Loss: 0.00065188
Iteration 10/25 | Loss: 0.00065188
Iteration 11/25 | Loss: 0.00065188
Iteration 12/25 | Loss: 0.00065188
Iteration 13/25 | Loss: 0.00065188
Iteration 14/25 | Loss: 0.00065188
Iteration 15/25 | Loss: 0.00065188
Iteration 16/25 | Loss: 0.00065188
Iteration 17/25 | Loss: 0.00065188
Iteration 18/25 | Loss: 0.00065188
Iteration 19/25 | Loss: 0.00065188
Iteration 20/25 | Loss: 0.00065188
Iteration 21/25 | Loss: 0.00065188
Iteration 22/25 | Loss: 0.00065188
Iteration 23/25 | Loss: 0.00065188
Iteration 24/25 | Loss: 0.00065188
Iteration 25/25 | Loss: 0.00065188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065188
Iteration 2/1000 | Loss: 0.00002252
Iteration 3/1000 | Loss: 0.00001498
Iteration 4/1000 | Loss: 0.00001344
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001247
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001198
Iteration 10/1000 | Loss: 0.00001192
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001151
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001146
Iteration 26/1000 | Loss: 0.00001146
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001143
Iteration 30/1000 | Loss: 0.00001142
Iteration 31/1000 | Loss: 0.00001142
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001130
Iteration 72/1000 | Loss: 0.00001130
Iteration 73/1000 | Loss: 0.00001130
Iteration 74/1000 | Loss: 0.00001130
Iteration 75/1000 | Loss: 0.00001130
Iteration 76/1000 | Loss: 0.00001130
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001129
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001124
Iteration 97/1000 | Loss: 0.00001124
Iteration 98/1000 | Loss: 0.00001124
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00001123
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001120
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001120
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001119
Iteration 134/1000 | Loss: 0.00001118
Iteration 135/1000 | Loss: 0.00001118
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001117
Iteration 139/1000 | Loss: 0.00001117
Iteration 140/1000 | Loss: 0.00001117
Iteration 141/1000 | Loss: 0.00001117
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.1174549399584066e-05, 1.1174549399584066e-05, 1.1174549399584066e-05, 1.1174549399584066e-05, 1.1174549399584066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1174549399584066e-05

Optimization complete. Final v2v error: 2.8342936038970947 mm

Highest mean error: 3.4450573921203613 mm for frame 80

Lowest mean error: 2.6468045711517334 mm for frame 145

Saving results

Total time: 35.18286895751953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592742
Iteration 2/25 | Loss: 0.00123740
Iteration 3/25 | Loss: 0.00117305
Iteration 4/25 | Loss: 0.00116321
Iteration 5/25 | Loss: 0.00116074
Iteration 6/25 | Loss: 0.00116064
Iteration 7/25 | Loss: 0.00116064
Iteration 8/25 | Loss: 0.00116064
Iteration 9/25 | Loss: 0.00116064
Iteration 10/25 | Loss: 0.00116064
Iteration 11/25 | Loss: 0.00116064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011606443440541625, 0.0011606443440541625, 0.0011606443440541625, 0.0011606443440541625, 0.0011606443440541625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011606443440541625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81336784
Iteration 2/25 | Loss: 0.00063577
Iteration 3/25 | Loss: 0.00063577
Iteration 4/25 | Loss: 0.00063577
Iteration 5/25 | Loss: 0.00063577
Iteration 6/25 | Loss: 0.00063577
Iteration 7/25 | Loss: 0.00063577
Iteration 8/25 | Loss: 0.00063577
Iteration 9/25 | Loss: 0.00063577
Iteration 10/25 | Loss: 0.00063577
Iteration 11/25 | Loss: 0.00063577
Iteration 12/25 | Loss: 0.00063577
Iteration 13/25 | Loss: 0.00063577
Iteration 14/25 | Loss: 0.00063577
Iteration 15/25 | Loss: 0.00063577
Iteration 16/25 | Loss: 0.00063577
Iteration 17/25 | Loss: 0.00063577
Iteration 18/25 | Loss: 0.00063577
Iteration 19/25 | Loss: 0.00063577
Iteration 20/25 | Loss: 0.00063577
Iteration 21/25 | Loss: 0.00063577
Iteration 22/25 | Loss: 0.00063577
Iteration 23/25 | Loss: 0.00063577
Iteration 24/25 | Loss: 0.00063577
Iteration 25/25 | Loss: 0.00063577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006357660167850554, 0.0006357660167850554, 0.0006357660167850554, 0.0006357660167850554, 0.0006357660167850554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006357660167850554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063577
Iteration 2/1000 | Loss: 0.00002370
Iteration 3/1000 | Loss: 0.00001664
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001461
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001366
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001317
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001307
Iteration 18/1000 | Loss: 0.00001307
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001288
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001277
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001276
Iteration 70/1000 | Loss: 0.00001276
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001276
Iteration 73/1000 | Loss: 0.00001276
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001276
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001276
Iteration 96/1000 | Loss: 0.00001276
Iteration 97/1000 | Loss: 0.00001276
Iteration 98/1000 | Loss: 0.00001276
Iteration 99/1000 | Loss: 0.00001276
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001276
Iteration 103/1000 | Loss: 0.00001276
Iteration 104/1000 | Loss: 0.00001276
Iteration 105/1000 | Loss: 0.00001276
Iteration 106/1000 | Loss: 0.00001276
Iteration 107/1000 | Loss: 0.00001276
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001276
Iteration 110/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.2764014172716998e-05, 1.2764014172716998e-05, 1.2764014172716998e-05, 1.2764014172716998e-05, 1.2764014172716998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2764014172716998e-05

Optimization complete. Final v2v error: 3.0360257625579834 mm

Highest mean error: 3.526519298553467 mm for frame 116

Lowest mean error: 2.7753326892852783 mm for frame 165

Saving results

Total time: 31.95398783683777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459383
Iteration 2/25 | Loss: 0.00130553
Iteration 3/25 | Loss: 0.00122171
Iteration 4/25 | Loss: 0.00120091
Iteration 5/25 | Loss: 0.00119662
Iteration 6/25 | Loss: 0.00119648
Iteration 7/25 | Loss: 0.00119648
Iteration 8/25 | Loss: 0.00119648
Iteration 9/25 | Loss: 0.00119648
Iteration 10/25 | Loss: 0.00119648
Iteration 11/25 | Loss: 0.00119648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011964829172939062, 0.0011964829172939062, 0.0011964829172939062, 0.0011964829172939062, 0.0011964829172939062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011964829172939062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46403217
Iteration 2/25 | Loss: 0.00061468
Iteration 3/25 | Loss: 0.00061468
Iteration 4/25 | Loss: 0.00061468
Iteration 5/25 | Loss: 0.00061468
Iteration 6/25 | Loss: 0.00061468
Iteration 7/25 | Loss: 0.00061468
Iteration 8/25 | Loss: 0.00061468
Iteration 9/25 | Loss: 0.00061468
Iteration 10/25 | Loss: 0.00061468
Iteration 11/25 | Loss: 0.00061468
Iteration 12/25 | Loss: 0.00061468
Iteration 13/25 | Loss: 0.00061468
Iteration 14/25 | Loss: 0.00061468
Iteration 15/25 | Loss: 0.00061468
Iteration 16/25 | Loss: 0.00061468
Iteration 17/25 | Loss: 0.00061468
Iteration 18/25 | Loss: 0.00061468
Iteration 19/25 | Loss: 0.00061468
Iteration 20/25 | Loss: 0.00061468
Iteration 21/25 | Loss: 0.00061468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006146750529296696, 0.0006146750529296696, 0.0006146750529296696, 0.0006146750529296696, 0.0006146750529296696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006146750529296696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061468
Iteration 2/1000 | Loss: 0.00003309
Iteration 3/1000 | Loss: 0.00002144
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001812
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001766
Iteration 10/1000 | Loss: 0.00001765
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00001739
Iteration 13/1000 | Loss: 0.00001725
Iteration 14/1000 | Loss: 0.00001724
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001705
Iteration 17/1000 | Loss: 0.00001702
Iteration 18/1000 | Loss: 0.00001702
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001698
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001694
Iteration 25/1000 | Loss: 0.00001693
Iteration 26/1000 | Loss: 0.00001693
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001690
Iteration 33/1000 | Loss: 0.00001688
Iteration 34/1000 | Loss: 0.00001687
Iteration 35/1000 | Loss: 0.00001687
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001687
Iteration 40/1000 | Loss: 0.00001687
Iteration 41/1000 | Loss: 0.00001687
Iteration 42/1000 | Loss: 0.00001687
Iteration 43/1000 | Loss: 0.00001686
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001668
Iteration 81/1000 | Loss: 0.00001666
Iteration 82/1000 | Loss: 0.00001666
Iteration 83/1000 | Loss: 0.00001666
Iteration 84/1000 | Loss: 0.00001666
Iteration 85/1000 | Loss: 0.00001665
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001662
Iteration 101/1000 | Loss: 0.00001662
Iteration 102/1000 | Loss: 0.00001661
Iteration 103/1000 | Loss: 0.00001661
Iteration 104/1000 | Loss: 0.00001661
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001661
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001659
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001658
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001658
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001658
Iteration 127/1000 | Loss: 0.00001658
Iteration 128/1000 | Loss: 0.00001658
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Iteration 151/1000 | Loss: 0.00001655
Iteration 152/1000 | Loss: 0.00001655
Iteration 153/1000 | Loss: 0.00001655
Iteration 154/1000 | Loss: 0.00001655
Iteration 155/1000 | Loss: 0.00001655
Iteration 156/1000 | Loss: 0.00001655
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001655
Iteration 162/1000 | Loss: 0.00001655
Iteration 163/1000 | Loss: 0.00001655
Iteration 164/1000 | Loss: 0.00001655
Iteration 165/1000 | Loss: 0.00001655
Iteration 166/1000 | Loss: 0.00001655
Iteration 167/1000 | Loss: 0.00001655
Iteration 168/1000 | Loss: 0.00001655
Iteration 169/1000 | Loss: 0.00001655
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001655
Iteration 186/1000 | Loss: 0.00001655
Iteration 187/1000 | Loss: 0.00001655
Iteration 188/1000 | Loss: 0.00001655
Iteration 189/1000 | Loss: 0.00001655
Iteration 190/1000 | Loss: 0.00001655
Iteration 191/1000 | Loss: 0.00001655
Iteration 192/1000 | Loss: 0.00001655
Iteration 193/1000 | Loss: 0.00001655
Iteration 194/1000 | Loss: 0.00001655
Iteration 195/1000 | Loss: 0.00001655
Iteration 196/1000 | Loss: 0.00001655
Iteration 197/1000 | Loss: 0.00001655
Iteration 198/1000 | Loss: 0.00001655
Iteration 199/1000 | Loss: 0.00001655
Iteration 200/1000 | Loss: 0.00001655
Iteration 201/1000 | Loss: 0.00001655
Iteration 202/1000 | Loss: 0.00001655
Iteration 203/1000 | Loss: 0.00001655
Iteration 204/1000 | Loss: 0.00001655
Iteration 205/1000 | Loss: 0.00001655
Iteration 206/1000 | Loss: 0.00001655
Iteration 207/1000 | Loss: 0.00001655
Iteration 208/1000 | Loss: 0.00001655
Iteration 209/1000 | Loss: 0.00001655
Iteration 210/1000 | Loss: 0.00001655
Iteration 211/1000 | Loss: 0.00001655
Iteration 212/1000 | Loss: 0.00001655
Iteration 213/1000 | Loss: 0.00001655
Iteration 214/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.655170308367815e-05, 1.655170308367815e-05, 1.655170308367815e-05, 1.655170308367815e-05, 1.655170308367815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.655170308367815e-05

Optimization complete. Final v2v error: 3.396850109100342 mm

Highest mean error: 3.7274906635284424 mm for frame 133

Lowest mean error: 3.196593999862671 mm for frame 15

Saving results

Total time: 38.612354040145874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00682378
Iteration 2/25 | Loss: 0.00128931
Iteration 3/25 | Loss: 0.00118961
Iteration 4/25 | Loss: 0.00117716
Iteration 5/25 | Loss: 0.00117247
Iteration 6/25 | Loss: 0.00117168
Iteration 7/25 | Loss: 0.00117154
Iteration 8/25 | Loss: 0.00117154
Iteration 9/25 | Loss: 0.00117154
Iteration 10/25 | Loss: 0.00117154
Iteration 11/25 | Loss: 0.00117154
Iteration 12/25 | Loss: 0.00117154
Iteration 13/25 | Loss: 0.00117154
Iteration 14/25 | Loss: 0.00117154
Iteration 15/25 | Loss: 0.00117154
Iteration 16/25 | Loss: 0.00117154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001171542564406991, 0.001171542564406991, 0.001171542564406991, 0.001171542564406991, 0.001171542564406991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001171542564406991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53903925
Iteration 2/25 | Loss: 0.00069709
Iteration 3/25 | Loss: 0.00069709
Iteration 4/25 | Loss: 0.00069709
Iteration 5/25 | Loss: 0.00069709
Iteration 6/25 | Loss: 0.00069709
Iteration 7/25 | Loss: 0.00069709
Iteration 8/25 | Loss: 0.00069709
Iteration 9/25 | Loss: 0.00069708
Iteration 10/25 | Loss: 0.00069708
Iteration 11/25 | Loss: 0.00069708
Iteration 12/25 | Loss: 0.00069708
Iteration 13/25 | Loss: 0.00069708
Iteration 14/25 | Loss: 0.00069708
Iteration 15/25 | Loss: 0.00069708
Iteration 16/25 | Loss: 0.00069708
Iteration 17/25 | Loss: 0.00069708
Iteration 18/25 | Loss: 0.00069708
Iteration 19/25 | Loss: 0.00069708
Iteration 20/25 | Loss: 0.00069708
Iteration 21/25 | Loss: 0.00069708
Iteration 22/25 | Loss: 0.00069708
Iteration 23/25 | Loss: 0.00069708
Iteration 24/25 | Loss: 0.00069708
Iteration 25/25 | Loss: 0.00069708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069708
Iteration 2/1000 | Loss: 0.00002632
Iteration 3/1000 | Loss: 0.00001641
Iteration 4/1000 | Loss: 0.00001462
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001327
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001237
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001213
Iteration 27/1000 | Loss: 0.00001209
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.1949166037084069e-05, 1.1949166037084069e-05, 1.1949166037084069e-05, 1.1949166037084069e-05, 1.1949166037084069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1949166037084069e-05

Optimization complete. Final v2v error: 2.9188413619995117 mm

Highest mean error: 3.79951810836792 mm for frame 73

Lowest mean error: 2.658217668533325 mm for frame 14

Saving results

Total time: 31.46695327758789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575523
Iteration 2/25 | Loss: 0.00144407
Iteration 3/25 | Loss: 0.00127886
Iteration 4/25 | Loss: 0.00120413
Iteration 5/25 | Loss: 0.00119738
Iteration 6/25 | Loss: 0.00119063
Iteration 7/25 | Loss: 0.00118835
Iteration 8/25 | Loss: 0.00118821
Iteration 9/25 | Loss: 0.00118499
Iteration 10/25 | Loss: 0.00118635
Iteration 11/25 | Loss: 0.00118256
Iteration 12/25 | Loss: 0.00118234
Iteration 13/25 | Loss: 0.00118223
Iteration 14/25 | Loss: 0.00118215
Iteration 15/25 | Loss: 0.00118215
Iteration 16/25 | Loss: 0.00118215
Iteration 17/25 | Loss: 0.00118215
Iteration 18/25 | Loss: 0.00118215
Iteration 19/25 | Loss: 0.00118215
Iteration 20/25 | Loss: 0.00118215
Iteration 21/25 | Loss: 0.00118215
Iteration 22/25 | Loss: 0.00118214
Iteration 23/25 | Loss: 0.00118214
Iteration 24/25 | Loss: 0.00118214
Iteration 25/25 | Loss: 0.00118214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.27180243
Iteration 2/25 | Loss: 0.00063012
Iteration 3/25 | Loss: 0.00063011
Iteration 4/25 | Loss: 0.00063011
Iteration 5/25 | Loss: 0.00063011
Iteration 6/25 | Loss: 0.00063011
Iteration 7/25 | Loss: 0.00063011
Iteration 8/25 | Loss: 0.00063011
Iteration 9/25 | Loss: 0.00063011
Iteration 10/25 | Loss: 0.00063011
Iteration 11/25 | Loss: 0.00063011
Iteration 12/25 | Loss: 0.00063011
Iteration 13/25 | Loss: 0.00063011
Iteration 14/25 | Loss: 0.00063011
Iteration 15/25 | Loss: 0.00063011
Iteration 16/25 | Loss: 0.00063011
Iteration 17/25 | Loss: 0.00063011
Iteration 18/25 | Loss: 0.00063011
Iteration 19/25 | Loss: 0.00063011
Iteration 20/25 | Loss: 0.00063011
Iteration 21/25 | Loss: 0.00063011
Iteration 22/25 | Loss: 0.00063011
Iteration 23/25 | Loss: 0.00063011
Iteration 24/25 | Loss: 0.00063011
Iteration 25/25 | Loss: 0.00063011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063011
Iteration 2/1000 | Loss: 0.00002765
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001736
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00005346
Iteration 7/1000 | Loss: 0.00002426
Iteration 8/1000 | Loss: 0.00001561
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00001533
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001481
Iteration 14/1000 | Loss: 0.00001477
Iteration 15/1000 | Loss: 0.00001464
Iteration 16/1000 | Loss: 0.00001461
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001458
Iteration 19/1000 | Loss: 0.00006580
Iteration 20/1000 | Loss: 0.00001457
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001431
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001422
Iteration 54/1000 | Loss: 0.00001422
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001419
Iteration 63/1000 | Loss: 0.00001419
Iteration 64/1000 | Loss: 0.00001419
Iteration 65/1000 | Loss: 0.00001419
Iteration 66/1000 | Loss: 0.00006711
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001417
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001416
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001411
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001410
Iteration 96/1000 | Loss: 0.00001410
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001410
Iteration 102/1000 | Loss: 0.00001410
Iteration 103/1000 | Loss: 0.00001410
Iteration 104/1000 | Loss: 0.00001410
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001410
Iteration 111/1000 | Loss: 0.00001410
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001410
Iteration 119/1000 | Loss: 0.00001410
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001410
Iteration 130/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4099060535954777e-05, 1.4099060535954777e-05, 1.4099060535954777e-05, 1.4099060535954777e-05, 1.4099060535954777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4099060535954777e-05

Optimization complete. Final v2v error: 3.14872145652771 mm

Highest mean error: 3.8501999378204346 mm for frame 171

Lowest mean error: 2.8409039974212646 mm for frame 188

Saving results

Total time: 66.35578465461731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014431
Iteration 2/25 | Loss: 0.00282509
Iteration 3/25 | Loss: 0.00168827
Iteration 4/25 | Loss: 0.00147732
Iteration 5/25 | Loss: 0.00145994
Iteration 6/25 | Loss: 0.00143896
Iteration 7/25 | Loss: 0.00134667
Iteration 8/25 | Loss: 0.00128314
Iteration 9/25 | Loss: 0.00126486
Iteration 10/25 | Loss: 0.00125987
Iteration 11/25 | Loss: 0.00125861
Iteration 12/25 | Loss: 0.00125810
Iteration 13/25 | Loss: 0.00125791
Iteration 14/25 | Loss: 0.00125789
Iteration 15/25 | Loss: 0.00125789
Iteration 16/25 | Loss: 0.00125789
Iteration 17/25 | Loss: 0.00125789
Iteration 18/25 | Loss: 0.00125788
Iteration 19/25 | Loss: 0.00125788
Iteration 20/25 | Loss: 0.00125788
Iteration 21/25 | Loss: 0.00125788
Iteration 22/25 | Loss: 0.00125788
Iteration 23/25 | Loss: 0.00125788
Iteration 24/25 | Loss: 0.00125788
Iteration 25/25 | Loss: 0.00125788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42695546
Iteration 2/25 | Loss: 0.00061140
Iteration 3/25 | Loss: 0.00061140
Iteration 4/25 | Loss: 0.00061140
Iteration 5/25 | Loss: 0.00061140
Iteration 6/25 | Loss: 0.00061140
Iteration 7/25 | Loss: 0.00061140
Iteration 8/25 | Loss: 0.00061140
Iteration 9/25 | Loss: 0.00061140
Iteration 10/25 | Loss: 0.00061140
Iteration 11/25 | Loss: 0.00061140
Iteration 12/25 | Loss: 0.00061140
Iteration 13/25 | Loss: 0.00061140
Iteration 14/25 | Loss: 0.00061140
Iteration 15/25 | Loss: 0.00061140
Iteration 16/25 | Loss: 0.00061140
Iteration 17/25 | Loss: 0.00061140
Iteration 18/25 | Loss: 0.00061140
Iteration 19/25 | Loss: 0.00061140
Iteration 20/25 | Loss: 0.00061140
Iteration 21/25 | Loss: 0.00061140
Iteration 22/25 | Loss: 0.00061140
Iteration 23/25 | Loss: 0.00061140
Iteration 24/25 | Loss: 0.00061140
Iteration 25/25 | Loss: 0.00061140

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061140
Iteration 2/1000 | Loss: 0.00004204
Iteration 3/1000 | Loss: 0.00002585
Iteration 4/1000 | Loss: 0.00002360
Iteration 5/1000 | Loss: 0.00002274
Iteration 6/1000 | Loss: 0.00002218
Iteration 7/1000 | Loss: 0.00002176
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002100
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002076
Iteration 13/1000 | Loss: 0.00002069
Iteration 14/1000 | Loss: 0.00002064
Iteration 15/1000 | Loss: 0.00002063
Iteration 16/1000 | Loss: 0.00002062
Iteration 17/1000 | Loss: 0.00002055
Iteration 18/1000 | Loss: 0.00002051
Iteration 19/1000 | Loss: 0.00002046
Iteration 20/1000 | Loss: 0.00002046
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002044
Iteration 23/1000 | Loss: 0.00002043
Iteration 24/1000 | Loss: 0.00002041
Iteration 25/1000 | Loss: 0.00002040
Iteration 26/1000 | Loss: 0.00002040
Iteration 27/1000 | Loss: 0.00002039
Iteration 28/1000 | Loss: 0.00002039
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002038
Iteration 31/1000 | Loss: 0.00002038
Iteration 32/1000 | Loss: 0.00002038
Iteration 33/1000 | Loss: 0.00002037
Iteration 34/1000 | Loss: 0.00002037
Iteration 35/1000 | Loss: 0.00002036
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002036
Iteration 38/1000 | Loss: 0.00002034
Iteration 39/1000 | Loss: 0.00002034
Iteration 40/1000 | Loss: 0.00002034
Iteration 41/1000 | Loss: 0.00002034
Iteration 42/1000 | Loss: 0.00002034
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002033
Iteration 45/1000 | Loss: 0.00002033
Iteration 46/1000 | Loss: 0.00002033
Iteration 47/1000 | Loss: 0.00002033
Iteration 48/1000 | Loss: 0.00002033
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002033
Iteration 54/1000 | Loss: 0.00002033
Iteration 55/1000 | Loss: 0.00002033
Iteration 56/1000 | Loss: 0.00002033
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002032
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002032
Iteration 61/1000 | Loss: 0.00002032
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002031
Iteration 65/1000 | Loss: 0.00002031
Iteration 66/1000 | Loss: 0.00002030
Iteration 67/1000 | Loss: 0.00002030
Iteration 68/1000 | Loss: 0.00002030
Iteration 69/1000 | Loss: 0.00002029
Iteration 70/1000 | Loss: 0.00002029
Iteration 71/1000 | Loss: 0.00002029
Iteration 72/1000 | Loss: 0.00002029
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00002028
Iteration 75/1000 | Loss: 0.00002028
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00002028
Iteration 78/1000 | Loss: 0.00002028
Iteration 79/1000 | Loss: 0.00002028
Iteration 80/1000 | Loss: 0.00002028
Iteration 81/1000 | Loss: 0.00002028
Iteration 82/1000 | Loss: 0.00002028
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002027
Iteration 85/1000 | Loss: 0.00002027
Iteration 86/1000 | Loss: 0.00002027
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002026
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002026
Iteration 92/1000 | Loss: 0.00002026
Iteration 93/1000 | Loss: 0.00002026
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002025
Iteration 96/1000 | Loss: 0.00002025
Iteration 97/1000 | Loss: 0.00002025
Iteration 98/1000 | Loss: 0.00002025
Iteration 99/1000 | Loss: 0.00002025
Iteration 100/1000 | Loss: 0.00002025
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.025135290750768e-05, 2.025135290750768e-05, 2.025135290750768e-05, 2.025135290750768e-05, 2.025135290750768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.025135290750768e-05

Optimization complete. Final v2v error: 3.752631902694702 mm

Highest mean error: 3.8043079376220703 mm for frame 86

Lowest mean error: 3.651717185974121 mm for frame 9

Saving results

Total time: 45.33903408050537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846505
Iteration 2/25 | Loss: 0.00124542
Iteration 3/25 | Loss: 0.00119000
Iteration 4/25 | Loss: 0.00117931
Iteration 5/25 | Loss: 0.00117641
Iteration 6/25 | Loss: 0.00117628
Iteration 7/25 | Loss: 0.00117628
Iteration 8/25 | Loss: 0.00117628
Iteration 9/25 | Loss: 0.00117628
Iteration 10/25 | Loss: 0.00117628
Iteration 11/25 | Loss: 0.00117628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011762799695134163, 0.0011762799695134163, 0.0011762799695134163, 0.0011762799695134163, 0.0011762799695134163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011762799695134163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46483946
Iteration 2/25 | Loss: 0.00063170
Iteration 3/25 | Loss: 0.00063170
Iteration 4/25 | Loss: 0.00063170
Iteration 5/25 | Loss: 0.00063170
Iteration 6/25 | Loss: 0.00063170
Iteration 7/25 | Loss: 0.00063170
Iteration 8/25 | Loss: 0.00063170
Iteration 9/25 | Loss: 0.00063170
Iteration 10/25 | Loss: 0.00063170
Iteration 11/25 | Loss: 0.00063170
Iteration 12/25 | Loss: 0.00063170
Iteration 13/25 | Loss: 0.00063170
Iteration 14/25 | Loss: 0.00063170
Iteration 15/25 | Loss: 0.00063170
Iteration 16/25 | Loss: 0.00063170
Iteration 17/25 | Loss: 0.00063170
Iteration 18/25 | Loss: 0.00063170
Iteration 19/25 | Loss: 0.00063170
Iteration 20/25 | Loss: 0.00063170
Iteration 21/25 | Loss: 0.00063170
Iteration 22/25 | Loss: 0.00063170
Iteration 23/25 | Loss: 0.00063170
Iteration 24/25 | Loss: 0.00063170
Iteration 25/25 | Loss: 0.00063170

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063170
Iteration 2/1000 | Loss: 0.00002290
Iteration 3/1000 | Loss: 0.00001756
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001498
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001476
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001454
Iteration 12/1000 | Loss: 0.00001453
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001448
Iteration 16/1000 | Loss: 0.00001447
Iteration 17/1000 | Loss: 0.00001446
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001444
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001433
Iteration 25/1000 | Loss: 0.00001432
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001419
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001418
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001418
Iteration 40/1000 | Loss: 0.00001417
Iteration 41/1000 | Loss: 0.00001417
Iteration 42/1000 | Loss: 0.00001417
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001416
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001415
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001414
Iteration 56/1000 | Loss: 0.00001411
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001407
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001403
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001401
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001396
Iteration 114/1000 | Loss: 0.00001396
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001394
Iteration 122/1000 | Loss: 0.00001394
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001393
Iteration 129/1000 | Loss: 0.00001393
Iteration 130/1000 | Loss: 0.00001393
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001393
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001392
Iteration 139/1000 | Loss: 0.00001392
Iteration 140/1000 | Loss: 0.00001392
Iteration 141/1000 | Loss: 0.00001392
Iteration 142/1000 | Loss: 0.00001392
Iteration 143/1000 | Loss: 0.00001392
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001392
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001391
Iteration 149/1000 | Loss: 0.00001391
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001391
Iteration 154/1000 | Loss: 0.00001391
Iteration 155/1000 | Loss: 0.00001391
Iteration 156/1000 | Loss: 0.00001391
Iteration 157/1000 | Loss: 0.00001391
Iteration 158/1000 | Loss: 0.00001391
Iteration 159/1000 | Loss: 0.00001391
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001391
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001390
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001390
Iteration 168/1000 | Loss: 0.00001390
Iteration 169/1000 | Loss: 0.00001390
Iteration 170/1000 | Loss: 0.00001390
Iteration 171/1000 | Loss: 0.00001390
Iteration 172/1000 | Loss: 0.00001390
Iteration 173/1000 | Loss: 0.00001390
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001389
Iteration 178/1000 | Loss: 0.00001389
Iteration 179/1000 | Loss: 0.00001389
Iteration 180/1000 | Loss: 0.00001389
Iteration 181/1000 | Loss: 0.00001389
Iteration 182/1000 | Loss: 0.00001389
Iteration 183/1000 | Loss: 0.00001389
Iteration 184/1000 | Loss: 0.00001389
Iteration 185/1000 | Loss: 0.00001389
Iteration 186/1000 | Loss: 0.00001389
Iteration 187/1000 | Loss: 0.00001389
Iteration 188/1000 | Loss: 0.00001389
Iteration 189/1000 | Loss: 0.00001389
Iteration 190/1000 | Loss: 0.00001389
Iteration 191/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.3886867236578837e-05, 1.3886867236578837e-05, 1.3886867236578837e-05, 1.3886867236578837e-05, 1.3886867236578837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3886867236578837e-05

Optimization complete. Final v2v error: 3.1393775939941406 mm

Highest mean error: 3.280806064605713 mm for frame 106

Lowest mean error: 3.033050298690796 mm for frame 138

Saving results

Total time: 36.22796273231506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021257
Iteration 2/25 | Loss: 0.00205267
Iteration 3/25 | Loss: 0.00152918
Iteration 4/25 | Loss: 0.00153179
Iteration 5/25 | Loss: 0.00145966
Iteration 6/25 | Loss: 0.00143749
Iteration 7/25 | Loss: 0.00143374
Iteration 8/25 | Loss: 0.00141731
Iteration 9/25 | Loss: 0.00134778
Iteration 10/25 | Loss: 0.00132797
Iteration 11/25 | Loss: 0.00132865
Iteration 12/25 | Loss: 0.00129881
Iteration 13/25 | Loss: 0.00129714
Iteration 14/25 | Loss: 0.00128704
Iteration 15/25 | Loss: 0.00127033
Iteration 16/25 | Loss: 0.00125258
Iteration 17/25 | Loss: 0.00125859
Iteration 18/25 | Loss: 0.00125914
Iteration 19/25 | Loss: 0.00125308
Iteration 20/25 | Loss: 0.00125114
Iteration 21/25 | Loss: 0.00124874
Iteration 22/25 | Loss: 0.00125428
Iteration 23/25 | Loss: 0.00125420
Iteration 24/25 | Loss: 0.00125035
Iteration 25/25 | Loss: 0.00125569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51450980
Iteration 2/25 | Loss: 0.00101506
Iteration 3/25 | Loss: 0.00098037
Iteration 4/25 | Loss: 0.00098037
Iteration 5/25 | Loss: 0.00098036
Iteration 6/25 | Loss: 0.00098036
Iteration 7/25 | Loss: 0.00098036
Iteration 8/25 | Loss: 0.00098036
Iteration 9/25 | Loss: 0.00098036
Iteration 10/25 | Loss: 0.00098036
Iteration 11/25 | Loss: 0.00098036
Iteration 12/25 | Loss: 0.00098036
Iteration 13/25 | Loss: 0.00098036
Iteration 14/25 | Loss: 0.00098036
Iteration 15/25 | Loss: 0.00098036
Iteration 16/25 | Loss: 0.00098036
Iteration 17/25 | Loss: 0.00098036
Iteration 18/25 | Loss: 0.00098036
Iteration 19/25 | Loss: 0.00098036
Iteration 20/25 | Loss: 0.00098036
Iteration 21/25 | Loss: 0.00098036
Iteration 22/25 | Loss: 0.00098036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009803619468584657, 0.0009803619468584657, 0.0009803619468584657, 0.0009803619468584657, 0.0009803619468584657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009803619468584657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098036
Iteration 2/1000 | Loss: 0.00060115
Iteration 3/1000 | Loss: 0.00035869
Iteration 4/1000 | Loss: 0.00038152
Iteration 5/1000 | Loss: 0.00038169
Iteration 6/1000 | Loss: 0.00054425
Iteration 7/1000 | Loss: 0.00068917
Iteration 8/1000 | Loss: 0.00047698
Iteration 9/1000 | Loss: 0.00037417
Iteration 10/1000 | Loss: 0.00030529
Iteration 11/1000 | Loss: 0.00052692
Iteration 12/1000 | Loss: 0.00049415
Iteration 13/1000 | Loss: 0.00055252
Iteration 14/1000 | Loss: 0.00058808
Iteration 15/1000 | Loss: 0.00056054
Iteration 16/1000 | Loss: 0.00033285
Iteration 17/1000 | Loss: 0.00015086
Iteration 18/1000 | Loss: 0.00035045
Iteration 19/1000 | Loss: 0.00005780
Iteration 20/1000 | Loss: 0.00025020
Iteration 21/1000 | Loss: 0.00013157
Iteration 22/1000 | Loss: 0.00012473
Iteration 23/1000 | Loss: 0.00009318
Iteration 24/1000 | Loss: 0.00026650
Iteration 25/1000 | Loss: 0.00057840
Iteration 26/1000 | Loss: 0.00056363
Iteration 27/1000 | Loss: 0.00051801
Iteration 28/1000 | Loss: 0.00040100
Iteration 29/1000 | Loss: 0.00096604
Iteration 30/1000 | Loss: 0.00048536
Iteration 31/1000 | Loss: 0.00095033
Iteration 32/1000 | Loss: 0.00056617
Iteration 33/1000 | Loss: 0.00063125
Iteration 34/1000 | Loss: 0.00019134
Iteration 35/1000 | Loss: 0.00017035
Iteration 36/1000 | Loss: 0.00018703
Iteration 37/1000 | Loss: 0.00037176
Iteration 38/1000 | Loss: 0.00051373
Iteration 39/1000 | Loss: 0.00196602
Iteration 40/1000 | Loss: 0.00057975
Iteration 41/1000 | Loss: 0.00026464
Iteration 42/1000 | Loss: 0.00030429
Iteration 43/1000 | Loss: 0.00021887
Iteration 44/1000 | Loss: 0.00004801
Iteration 45/1000 | Loss: 0.00004025
Iteration 46/1000 | Loss: 0.00005700
Iteration 47/1000 | Loss: 0.00003354
Iteration 48/1000 | Loss: 0.00004324
Iteration 49/1000 | Loss: 0.00002746
Iteration 50/1000 | Loss: 0.00002425
Iteration 51/1000 | Loss: 0.00003774
Iteration 52/1000 | Loss: 0.00002251
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001707
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001533
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001457
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001447
Iteration 68/1000 | Loss: 0.00001443
Iteration 69/1000 | Loss: 0.00001443
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001427
Iteration 146/1000 | Loss: 0.00001427
Iteration 147/1000 | Loss: 0.00001427
Iteration 148/1000 | Loss: 0.00001427
Iteration 149/1000 | Loss: 0.00001427
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001426
Iteration 156/1000 | Loss: 0.00001426
Iteration 157/1000 | Loss: 0.00001426
Iteration 158/1000 | Loss: 0.00001426
Iteration 159/1000 | Loss: 0.00001426
Iteration 160/1000 | Loss: 0.00001426
Iteration 161/1000 | Loss: 0.00001426
Iteration 162/1000 | Loss: 0.00001426
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001426
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001425
Iteration 174/1000 | Loss: 0.00001425
Iteration 175/1000 | Loss: 0.00001425
Iteration 176/1000 | Loss: 0.00001425
Iteration 177/1000 | Loss: 0.00001425
Iteration 178/1000 | Loss: 0.00001425
Iteration 179/1000 | Loss: 0.00001425
Iteration 180/1000 | Loss: 0.00001425
Iteration 181/1000 | Loss: 0.00001425
Iteration 182/1000 | Loss: 0.00001425
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001425
Iteration 189/1000 | Loss: 0.00001425
Iteration 190/1000 | Loss: 0.00001425
Iteration 191/1000 | Loss: 0.00001425
Iteration 192/1000 | Loss: 0.00001425
Iteration 193/1000 | Loss: 0.00001425
Iteration 194/1000 | Loss: 0.00001425
Iteration 195/1000 | Loss: 0.00001425
Iteration 196/1000 | Loss: 0.00001424
Iteration 197/1000 | Loss: 0.00001424
Iteration 198/1000 | Loss: 0.00001424
Iteration 199/1000 | Loss: 0.00001424
Iteration 200/1000 | Loss: 0.00001424
Iteration 201/1000 | Loss: 0.00001424
Iteration 202/1000 | Loss: 0.00001424
Iteration 203/1000 | Loss: 0.00001424
Iteration 204/1000 | Loss: 0.00001424
Iteration 205/1000 | Loss: 0.00001424
Iteration 206/1000 | Loss: 0.00001424
Iteration 207/1000 | Loss: 0.00001424
Iteration 208/1000 | Loss: 0.00001424
Iteration 209/1000 | Loss: 0.00001424
Iteration 210/1000 | Loss: 0.00001424
Iteration 211/1000 | Loss: 0.00001423
Iteration 212/1000 | Loss: 0.00001423
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Iteration 216/1000 | Loss: 0.00001423
Iteration 217/1000 | Loss: 0.00001423
Iteration 218/1000 | Loss: 0.00001423
Iteration 219/1000 | Loss: 0.00001423
Iteration 220/1000 | Loss: 0.00001423
Iteration 221/1000 | Loss: 0.00001423
Iteration 222/1000 | Loss: 0.00001423
Iteration 223/1000 | Loss: 0.00001423
Iteration 224/1000 | Loss: 0.00001423
Iteration 225/1000 | Loss: 0.00001423
Iteration 226/1000 | Loss: 0.00001423
Iteration 227/1000 | Loss: 0.00001423
Iteration 228/1000 | Loss: 0.00001423
Iteration 229/1000 | Loss: 0.00001423
Iteration 230/1000 | Loss: 0.00001423
Iteration 231/1000 | Loss: 0.00001423
Iteration 232/1000 | Loss: 0.00001423
Iteration 233/1000 | Loss: 0.00001423
Iteration 234/1000 | Loss: 0.00001423
Iteration 235/1000 | Loss: 0.00001422
Iteration 236/1000 | Loss: 0.00001422
Iteration 237/1000 | Loss: 0.00001422
Iteration 238/1000 | Loss: 0.00001422
Iteration 239/1000 | Loss: 0.00001422
Iteration 240/1000 | Loss: 0.00001422
Iteration 241/1000 | Loss: 0.00001422
Iteration 242/1000 | Loss: 0.00001422
Iteration 243/1000 | Loss: 0.00001422
Iteration 244/1000 | Loss: 0.00001422
Iteration 245/1000 | Loss: 0.00001422
Iteration 246/1000 | Loss: 0.00001422
Iteration 247/1000 | Loss: 0.00001422
Iteration 248/1000 | Loss: 0.00001422
Iteration 249/1000 | Loss: 0.00001422
Iteration 250/1000 | Loss: 0.00001422
Iteration 251/1000 | Loss: 0.00001422
Iteration 252/1000 | Loss: 0.00001422
Iteration 253/1000 | Loss: 0.00001422
Iteration 254/1000 | Loss: 0.00001422
Iteration 255/1000 | Loss: 0.00001422
Iteration 256/1000 | Loss: 0.00001422
Iteration 257/1000 | Loss: 0.00001422
Iteration 258/1000 | Loss: 0.00001422
Iteration 259/1000 | Loss: 0.00001422
Iteration 260/1000 | Loss: 0.00001422
Iteration 261/1000 | Loss: 0.00001422
Iteration 262/1000 | Loss: 0.00001422
Iteration 263/1000 | Loss: 0.00001422
Iteration 264/1000 | Loss: 0.00001422
Iteration 265/1000 | Loss: 0.00001422
Iteration 266/1000 | Loss: 0.00001422
Iteration 267/1000 | Loss: 0.00001422
Iteration 268/1000 | Loss: 0.00001422
Iteration 269/1000 | Loss: 0.00001422
Iteration 270/1000 | Loss: 0.00001422
Iteration 271/1000 | Loss: 0.00001422
Iteration 272/1000 | Loss: 0.00001422
Iteration 273/1000 | Loss: 0.00001422
Iteration 274/1000 | Loss: 0.00001422
Iteration 275/1000 | Loss: 0.00001422
Iteration 276/1000 | Loss: 0.00001422
Iteration 277/1000 | Loss: 0.00001422
Iteration 278/1000 | Loss: 0.00001422
Iteration 279/1000 | Loss: 0.00001422
Iteration 280/1000 | Loss: 0.00001422
Iteration 281/1000 | Loss: 0.00001422
Iteration 282/1000 | Loss: 0.00001422
Iteration 283/1000 | Loss: 0.00001422
Iteration 284/1000 | Loss: 0.00001422
Iteration 285/1000 | Loss: 0.00001422
Iteration 286/1000 | Loss: 0.00001422
Iteration 287/1000 | Loss: 0.00001422
Iteration 288/1000 | Loss: 0.00001422
Iteration 289/1000 | Loss: 0.00001422
Iteration 290/1000 | Loss: 0.00001422
Iteration 291/1000 | Loss: 0.00001422
Iteration 292/1000 | Loss: 0.00001422
Iteration 293/1000 | Loss: 0.00001422
Iteration 294/1000 | Loss: 0.00001422
Iteration 295/1000 | Loss: 0.00001422
Iteration 296/1000 | Loss: 0.00001422
Iteration 297/1000 | Loss: 0.00001422
Iteration 298/1000 | Loss: 0.00001422
Iteration 299/1000 | Loss: 0.00001422
Iteration 300/1000 | Loss: 0.00001422
Iteration 301/1000 | Loss: 0.00001422
Iteration 302/1000 | Loss: 0.00001422
Iteration 303/1000 | Loss: 0.00001422
Iteration 304/1000 | Loss: 0.00001422
Iteration 305/1000 | Loss: 0.00001422
Iteration 306/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [1.4216810996003915e-05, 1.4216810996003915e-05, 1.4216810996003915e-05, 1.4216810996003915e-05, 1.4216810996003915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4216810996003915e-05

Optimization complete. Final v2v error: 3.1502084732055664 mm

Highest mean error: 4.284994602203369 mm for frame 64

Lowest mean error: 2.783843517303467 mm for frame 17

Saving results

Total time: 149.4034242630005
