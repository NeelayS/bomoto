Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=278, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15568-15623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977756
Iteration 2/25 | Loss: 0.00195519
Iteration 3/25 | Loss: 0.00164111
Iteration 4/25 | Loss: 0.00158688
Iteration 5/25 | Loss: 0.00157298
Iteration 6/25 | Loss: 0.00157055
Iteration 7/25 | Loss: 0.00156956
Iteration 8/25 | Loss: 0.00156951
Iteration 9/25 | Loss: 0.00156951
Iteration 10/25 | Loss: 0.00156951
Iteration 11/25 | Loss: 0.00156951
Iteration 12/25 | Loss: 0.00156951
Iteration 13/25 | Loss: 0.00156951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015695077599957585, 0.0015695077599957585, 0.0015695077599957585, 0.0015695077599957585, 0.0015695077599957585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015695077599957585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40780139
Iteration 2/25 | Loss: 0.00245522
Iteration 3/25 | Loss: 0.00245522
Iteration 4/25 | Loss: 0.00245521
Iteration 5/25 | Loss: 0.00245521
Iteration 6/25 | Loss: 0.00245521
Iteration 7/25 | Loss: 0.00245521
Iteration 8/25 | Loss: 0.00245521
Iteration 9/25 | Loss: 0.00245521
Iteration 10/25 | Loss: 0.00245521
Iteration 11/25 | Loss: 0.00245521
Iteration 12/25 | Loss: 0.00245521
Iteration 13/25 | Loss: 0.00245521
Iteration 14/25 | Loss: 0.00245521
Iteration 15/25 | Loss: 0.00245521
Iteration 16/25 | Loss: 0.00245521
Iteration 17/25 | Loss: 0.00245521
Iteration 18/25 | Loss: 0.00245521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002455213339999318, 0.002455213339999318, 0.002455213339999318, 0.002455213339999318, 0.002455213339999318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002455213339999318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245521
Iteration 2/1000 | Loss: 0.00015125
Iteration 3/1000 | Loss: 0.00009057
Iteration 4/1000 | Loss: 0.00006873
Iteration 5/1000 | Loss: 0.00006169
Iteration 6/1000 | Loss: 0.00005668
Iteration 7/1000 | Loss: 0.00005243
Iteration 8/1000 | Loss: 0.00005002
Iteration 9/1000 | Loss: 0.00004793
Iteration 10/1000 | Loss: 0.00004675
Iteration 11/1000 | Loss: 0.00004559
Iteration 12/1000 | Loss: 0.00004484
Iteration 13/1000 | Loss: 0.00004424
Iteration 14/1000 | Loss: 0.00004384
Iteration 15/1000 | Loss: 0.00004343
Iteration 16/1000 | Loss: 0.00004318
Iteration 17/1000 | Loss: 0.00004296
Iteration 18/1000 | Loss: 0.00004290
Iteration 19/1000 | Loss: 0.00004289
Iteration 20/1000 | Loss: 0.00004278
Iteration 21/1000 | Loss: 0.00004277
Iteration 22/1000 | Loss: 0.00004274
Iteration 23/1000 | Loss: 0.00004271
Iteration 24/1000 | Loss: 0.00004268
Iteration 25/1000 | Loss: 0.00004267
Iteration 26/1000 | Loss: 0.00004267
Iteration 27/1000 | Loss: 0.00004267
Iteration 28/1000 | Loss: 0.00004267
Iteration 29/1000 | Loss: 0.00004267
Iteration 30/1000 | Loss: 0.00004266
Iteration 31/1000 | Loss: 0.00004264
Iteration 32/1000 | Loss: 0.00004264
Iteration 33/1000 | Loss: 0.00004263
Iteration 34/1000 | Loss: 0.00004263
Iteration 35/1000 | Loss: 0.00004259
Iteration 36/1000 | Loss: 0.00004259
Iteration 37/1000 | Loss: 0.00004258
Iteration 38/1000 | Loss: 0.00004258
Iteration 39/1000 | Loss: 0.00004258
Iteration 40/1000 | Loss: 0.00004258
Iteration 41/1000 | Loss: 0.00004257
Iteration 42/1000 | Loss: 0.00004257
Iteration 43/1000 | Loss: 0.00004257
Iteration 44/1000 | Loss: 0.00004255
Iteration 45/1000 | Loss: 0.00004255
Iteration 46/1000 | Loss: 0.00004254
Iteration 47/1000 | Loss: 0.00004254
Iteration 48/1000 | Loss: 0.00004254
Iteration 49/1000 | Loss: 0.00004254
Iteration 50/1000 | Loss: 0.00004253
Iteration 51/1000 | Loss: 0.00004253
Iteration 52/1000 | Loss: 0.00004253
Iteration 53/1000 | Loss: 0.00004253
Iteration 54/1000 | Loss: 0.00004253
Iteration 55/1000 | Loss: 0.00004253
Iteration 56/1000 | Loss: 0.00004253
Iteration 57/1000 | Loss: 0.00004253
Iteration 58/1000 | Loss: 0.00004253
Iteration 59/1000 | Loss: 0.00004252
Iteration 60/1000 | Loss: 0.00004252
Iteration 61/1000 | Loss: 0.00004251
Iteration 62/1000 | Loss: 0.00004251
Iteration 63/1000 | Loss: 0.00004251
Iteration 64/1000 | Loss: 0.00004250
Iteration 65/1000 | Loss: 0.00004250
Iteration 66/1000 | Loss: 0.00004250
Iteration 67/1000 | Loss: 0.00004250
Iteration 68/1000 | Loss: 0.00004249
Iteration 69/1000 | Loss: 0.00004249
Iteration 70/1000 | Loss: 0.00004249
Iteration 71/1000 | Loss: 0.00004249
Iteration 72/1000 | Loss: 0.00004249
Iteration 73/1000 | Loss: 0.00004248
Iteration 74/1000 | Loss: 0.00004248
Iteration 75/1000 | Loss: 0.00004247
Iteration 76/1000 | Loss: 0.00004247
Iteration 77/1000 | Loss: 0.00004247
Iteration 78/1000 | Loss: 0.00004247
Iteration 79/1000 | Loss: 0.00004247
Iteration 80/1000 | Loss: 0.00004247
Iteration 81/1000 | Loss: 0.00004247
Iteration 82/1000 | Loss: 0.00004246
Iteration 83/1000 | Loss: 0.00004246
Iteration 84/1000 | Loss: 0.00004246
Iteration 85/1000 | Loss: 0.00004246
Iteration 86/1000 | Loss: 0.00004246
Iteration 87/1000 | Loss: 0.00004246
Iteration 88/1000 | Loss: 0.00004246
Iteration 89/1000 | Loss: 0.00004246
Iteration 90/1000 | Loss: 0.00004245
Iteration 91/1000 | Loss: 0.00004245
Iteration 92/1000 | Loss: 0.00004245
Iteration 93/1000 | Loss: 0.00004244
Iteration 94/1000 | Loss: 0.00004244
Iteration 95/1000 | Loss: 0.00004244
Iteration 96/1000 | Loss: 0.00004244
Iteration 97/1000 | Loss: 0.00004243
Iteration 98/1000 | Loss: 0.00004243
Iteration 99/1000 | Loss: 0.00004243
Iteration 100/1000 | Loss: 0.00004243
Iteration 101/1000 | Loss: 0.00004242
Iteration 102/1000 | Loss: 0.00004242
Iteration 103/1000 | Loss: 0.00004242
Iteration 104/1000 | Loss: 0.00004242
Iteration 105/1000 | Loss: 0.00004241
Iteration 106/1000 | Loss: 0.00004241
Iteration 107/1000 | Loss: 0.00004241
Iteration 108/1000 | Loss: 0.00004241
Iteration 109/1000 | Loss: 0.00004241
Iteration 110/1000 | Loss: 0.00004241
Iteration 111/1000 | Loss: 0.00004241
Iteration 112/1000 | Loss: 0.00004240
Iteration 113/1000 | Loss: 0.00004240
Iteration 114/1000 | Loss: 0.00004240
Iteration 115/1000 | Loss: 0.00004240
Iteration 116/1000 | Loss: 0.00004240
Iteration 117/1000 | Loss: 0.00004240
Iteration 118/1000 | Loss: 0.00004239
Iteration 119/1000 | Loss: 0.00004239
Iteration 120/1000 | Loss: 0.00004239
Iteration 121/1000 | Loss: 0.00004239
Iteration 122/1000 | Loss: 0.00004239
Iteration 123/1000 | Loss: 0.00004239
Iteration 124/1000 | Loss: 0.00004239
Iteration 125/1000 | Loss: 0.00004238
Iteration 126/1000 | Loss: 0.00004238
Iteration 127/1000 | Loss: 0.00004238
Iteration 128/1000 | Loss: 0.00004238
Iteration 129/1000 | Loss: 0.00004238
Iteration 130/1000 | Loss: 0.00004238
Iteration 131/1000 | Loss: 0.00004238
Iteration 132/1000 | Loss: 0.00004238
Iteration 133/1000 | Loss: 0.00004237
Iteration 134/1000 | Loss: 0.00004237
Iteration 135/1000 | Loss: 0.00004237
Iteration 136/1000 | Loss: 0.00004237
Iteration 137/1000 | Loss: 0.00004237
Iteration 138/1000 | Loss: 0.00004237
Iteration 139/1000 | Loss: 0.00004237
Iteration 140/1000 | Loss: 0.00004237
Iteration 141/1000 | Loss: 0.00004237
Iteration 142/1000 | Loss: 0.00004237
Iteration 143/1000 | Loss: 0.00004237
Iteration 144/1000 | Loss: 0.00004236
Iteration 145/1000 | Loss: 0.00004236
Iteration 146/1000 | Loss: 0.00004236
Iteration 147/1000 | Loss: 0.00004236
Iteration 148/1000 | Loss: 0.00004236
Iteration 149/1000 | Loss: 0.00004236
Iteration 150/1000 | Loss: 0.00004236
Iteration 151/1000 | Loss: 0.00004236
Iteration 152/1000 | Loss: 0.00004236
Iteration 153/1000 | Loss: 0.00004236
Iteration 154/1000 | Loss: 0.00004236
Iteration 155/1000 | Loss: 0.00004236
Iteration 156/1000 | Loss: 0.00004236
Iteration 157/1000 | Loss: 0.00004236
Iteration 158/1000 | Loss: 0.00004236
Iteration 159/1000 | Loss: 0.00004236
Iteration 160/1000 | Loss: 0.00004235
Iteration 161/1000 | Loss: 0.00004235
Iteration 162/1000 | Loss: 0.00004235
Iteration 163/1000 | Loss: 0.00004235
Iteration 164/1000 | Loss: 0.00004235
Iteration 165/1000 | Loss: 0.00004235
Iteration 166/1000 | Loss: 0.00004235
Iteration 167/1000 | Loss: 0.00004235
Iteration 168/1000 | Loss: 0.00004235
Iteration 169/1000 | Loss: 0.00004235
Iteration 170/1000 | Loss: 0.00004235
Iteration 171/1000 | Loss: 0.00004235
Iteration 172/1000 | Loss: 0.00004235
Iteration 173/1000 | Loss: 0.00004235
Iteration 174/1000 | Loss: 0.00004235
Iteration 175/1000 | Loss: 0.00004235
Iteration 176/1000 | Loss: 0.00004234
Iteration 177/1000 | Loss: 0.00004234
Iteration 178/1000 | Loss: 0.00004234
Iteration 179/1000 | Loss: 0.00004234
Iteration 180/1000 | Loss: 0.00004234
Iteration 181/1000 | Loss: 0.00004234
Iteration 182/1000 | Loss: 0.00004234
Iteration 183/1000 | Loss: 0.00004234
Iteration 184/1000 | Loss: 0.00004234
Iteration 185/1000 | Loss: 0.00004233
Iteration 186/1000 | Loss: 0.00004233
Iteration 187/1000 | Loss: 0.00004233
Iteration 188/1000 | Loss: 0.00004233
Iteration 189/1000 | Loss: 0.00004233
Iteration 190/1000 | Loss: 0.00004233
Iteration 191/1000 | Loss: 0.00004233
Iteration 192/1000 | Loss: 0.00004233
Iteration 193/1000 | Loss: 0.00004233
Iteration 194/1000 | Loss: 0.00004233
Iteration 195/1000 | Loss: 0.00004233
Iteration 196/1000 | Loss: 0.00004233
Iteration 197/1000 | Loss: 0.00004233
Iteration 198/1000 | Loss: 0.00004233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [4.233461731928401e-05, 4.233461731928401e-05, 4.233461731928401e-05, 4.233461731928401e-05, 4.233461731928401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.233461731928401e-05

Optimization complete. Final v2v error: 5.32698392868042 mm

Highest mean error: 6.8596649169921875 mm for frame 82

Lowest mean error: 4.583320617675781 mm for frame 46

Saving results

Total time: 49.65472936630249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426057
Iteration 2/25 | Loss: 0.00160148
Iteration 3/25 | Loss: 0.00151378
Iteration 4/25 | Loss: 0.00150017
Iteration 5/25 | Loss: 0.00149669
Iteration 6/25 | Loss: 0.00149550
Iteration 7/25 | Loss: 0.00149549
Iteration 8/25 | Loss: 0.00149549
Iteration 9/25 | Loss: 0.00149549
Iteration 10/25 | Loss: 0.00149549
Iteration 11/25 | Loss: 0.00149549
Iteration 12/25 | Loss: 0.00149549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001495488453656435, 0.001495488453656435, 0.001495488453656435, 0.001495488453656435, 0.001495488453656435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001495488453656435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14270568
Iteration 2/25 | Loss: 0.00315302
Iteration 3/25 | Loss: 0.00315301
Iteration 4/25 | Loss: 0.00315301
Iteration 5/25 | Loss: 0.00315301
Iteration 6/25 | Loss: 0.00315301
Iteration 7/25 | Loss: 0.00315301
Iteration 8/25 | Loss: 0.00315301
Iteration 9/25 | Loss: 0.00315301
Iteration 10/25 | Loss: 0.00315301
Iteration 11/25 | Loss: 0.00315301
Iteration 12/25 | Loss: 0.00315301
Iteration 13/25 | Loss: 0.00315301
Iteration 14/25 | Loss: 0.00315301
Iteration 15/25 | Loss: 0.00315301
Iteration 16/25 | Loss: 0.00315301
Iteration 17/25 | Loss: 0.00315301
Iteration 18/25 | Loss: 0.00315301
Iteration 19/25 | Loss: 0.00315301
Iteration 20/25 | Loss: 0.00315301
Iteration 21/25 | Loss: 0.00315301
Iteration 22/25 | Loss: 0.00315301
Iteration 23/25 | Loss: 0.00315301
Iteration 24/25 | Loss: 0.00315301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003153009805828333, 0.003153009805828333, 0.003153009805828333, 0.003153009805828333, 0.003153009805828333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003153009805828333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315301
Iteration 2/1000 | Loss: 0.00009504
Iteration 3/1000 | Loss: 0.00006599
Iteration 4/1000 | Loss: 0.00004921
Iteration 5/1000 | Loss: 0.00004401
Iteration 6/1000 | Loss: 0.00003981
Iteration 7/1000 | Loss: 0.00003647
Iteration 8/1000 | Loss: 0.00003418
Iteration 9/1000 | Loss: 0.00003251
Iteration 10/1000 | Loss: 0.00003138
Iteration 11/1000 | Loss: 0.00003061
Iteration 12/1000 | Loss: 0.00002998
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002929
Iteration 15/1000 | Loss: 0.00002915
Iteration 16/1000 | Loss: 0.00002901
Iteration 17/1000 | Loss: 0.00002898
Iteration 18/1000 | Loss: 0.00002891
Iteration 19/1000 | Loss: 0.00002891
Iteration 20/1000 | Loss: 0.00002890
Iteration 21/1000 | Loss: 0.00002888
Iteration 22/1000 | Loss: 0.00002888
Iteration 23/1000 | Loss: 0.00002887
Iteration 24/1000 | Loss: 0.00002886
Iteration 25/1000 | Loss: 0.00002885
Iteration 26/1000 | Loss: 0.00002884
Iteration 27/1000 | Loss: 0.00002884
Iteration 28/1000 | Loss: 0.00002884
Iteration 29/1000 | Loss: 0.00002883
Iteration 30/1000 | Loss: 0.00002882
Iteration 31/1000 | Loss: 0.00002882
Iteration 32/1000 | Loss: 0.00002881
Iteration 33/1000 | Loss: 0.00002881
Iteration 34/1000 | Loss: 0.00002881
Iteration 35/1000 | Loss: 0.00002880
Iteration 36/1000 | Loss: 0.00002880
Iteration 37/1000 | Loss: 0.00002879
Iteration 38/1000 | Loss: 0.00002879
Iteration 39/1000 | Loss: 0.00002879
Iteration 40/1000 | Loss: 0.00002879
Iteration 41/1000 | Loss: 0.00002879
Iteration 42/1000 | Loss: 0.00002879
Iteration 43/1000 | Loss: 0.00002879
Iteration 44/1000 | Loss: 0.00002879
Iteration 45/1000 | Loss: 0.00002878
Iteration 46/1000 | Loss: 0.00002878
Iteration 47/1000 | Loss: 0.00002878
Iteration 48/1000 | Loss: 0.00002877
Iteration 49/1000 | Loss: 0.00002877
Iteration 50/1000 | Loss: 0.00002877
Iteration 51/1000 | Loss: 0.00002877
Iteration 52/1000 | Loss: 0.00002877
Iteration 53/1000 | Loss: 0.00002876
Iteration 54/1000 | Loss: 0.00002876
Iteration 55/1000 | Loss: 0.00002876
Iteration 56/1000 | Loss: 0.00002875
Iteration 57/1000 | Loss: 0.00002875
Iteration 58/1000 | Loss: 0.00002875
Iteration 59/1000 | Loss: 0.00002874
Iteration 60/1000 | Loss: 0.00002874
Iteration 61/1000 | Loss: 0.00002874
Iteration 62/1000 | Loss: 0.00002874
Iteration 63/1000 | Loss: 0.00002874
Iteration 64/1000 | Loss: 0.00002874
Iteration 65/1000 | Loss: 0.00002874
Iteration 66/1000 | Loss: 0.00002874
Iteration 67/1000 | Loss: 0.00002873
Iteration 68/1000 | Loss: 0.00002873
Iteration 69/1000 | Loss: 0.00002873
Iteration 70/1000 | Loss: 0.00002873
Iteration 71/1000 | Loss: 0.00002873
Iteration 72/1000 | Loss: 0.00002873
Iteration 73/1000 | Loss: 0.00002873
Iteration 74/1000 | Loss: 0.00002873
Iteration 75/1000 | Loss: 0.00002873
Iteration 76/1000 | Loss: 0.00002873
Iteration 77/1000 | Loss: 0.00002873
Iteration 78/1000 | Loss: 0.00002872
Iteration 79/1000 | Loss: 0.00002872
Iteration 80/1000 | Loss: 0.00002872
Iteration 81/1000 | Loss: 0.00002872
Iteration 82/1000 | Loss: 0.00002872
Iteration 83/1000 | Loss: 0.00002872
Iteration 84/1000 | Loss: 0.00002871
Iteration 85/1000 | Loss: 0.00002871
Iteration 86/1000 | Loss: 0.00002871
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002871
Iteration 89/1000 | Loss: 0.00002871
Iteration 90/1000 | Loss: 0.00002871
Iteration 91/1000 | Loss: 0.00002871
Iteration 92/1000 | Loss: 0.00002871
Iteration 93/1000 | Loss: 0.00002871
Iteration 94/1000 | Loss: 0.00002871
Iteration 95/1000 | Loss: 0.00002871
Iteration 96/1000 | Loss: 0.00002870
Iteration 97/1000 | Loss: 0.00002870
Iteration 98/1000 | Loss: 0.00002870
Iteration 99/1000 | Loss: 0.00002870
Iteration 100/1000 | Loss: 0.00002870
Iteration 101/1000 | Loss: 0.00002870
Iteration 102/1000 | Loss: 0.00002870
Iteration 103/1000 | Loss: 0.00002870
Iteration 104/1000 | Loss: 0.00002869
Iteration 105/1000 | Loss: 0.00002869
Iteration 106/1000 | Loss: 0.00002869
Iteration 107/1000 | Loss: 0.00002869
Iteration 108/1000 | Loss: 0.00002869
Iteration 109/1000 | Loss: 0.00002868
Iteration 110/1000 | Loss: 0.00002868
Iteration 111/1000 | Loss: 0.00002868
Iteration 112/1000 | Loss: 0.00002868
Iteration 113/1000 | Loss: 0.00002868
Iteration 114/1000 | Loss: 0.00002867
Iteration 115/1000 | Loss: 0.00002867
Iteration 116/1000 | Loss: 0.00002867
Iteration 117/1000 | Loss: 0.00002867
Iteration 118/1000 | Loss: 0.00002867
Iteration 119/1000 | Loss: 0.00002867
Iteration 120/1000 | Loss: 0.00002867
Iteration 121/1000 | Loss: 0.00002867
Iteration 122/1000 | Loss: 0.00002867
Iteration 123/1000 | Loss: 0.00002867
Iteration 124/1000 | Loss: 0.00002867
Iteration 125/1000 | Loss: 0.00002866
Iteration 126/1000 | Loss: 0.00002866
Iteration 127/1000 | Loss: 0.00002866
Iteration 128/1000 | Loss: 0.00002866
Iteration 129/1000 | Loss: 0.00002866
Iteration 130/1000 | Loss: 0.00002866
Iteration 131/1000 | Loss: 0.00002866
Iteration 132/1000 | Loss: 0.00002866
Iteration 133/1000 | Loss: 0.00002866
Iteration 134/1000 | Loss: 0.00002866
Iteration 135/1000 | Loss: 0.00002866
Iteration 136/1000 | Loss: 0.00002866
Iteration 137/1000 | Loss: 0.00002866
Iteration 138/1000 | Loss: 0.00002866
Iteration 139/1000 | Loss: 0.00002866
Iteration 140/1000 | Loss: 0.00002866
Iteration 141/1000 | Loss: 0.00002866
Iteration 142/1000 | Loss: 0.00002866
Iteration 143/1000 | Loss: 0.00002866
Iteration 144/1000 | Loss: 0.00002865
Iteration 145/1000 | Loss: 0.00002865
Iteration 146/1000 | Loss: 0.00002865
Iteration 147/1000 | Loss: 0.00002865
Iteration 148/1000 | Loss: 0.00002865
Iteration 149/1000 | Loss: 0.00002865
Iteration 150/1000 | Loss: 0.00002865
Iteration 151/1000 | Loss: 0.00002865
Iteration 152/1000 | Loss: 0.00002865
Iteration 153/1000 | Loss: 0.00002865
Iteration 154/1000 | Loss: 0.00002865
Iteration 155/1000 | Loss: 0.00002865
Iteration 156/1000 | Loss: 0.00002865
Iteration 157/1000 | Loss: 0.00002865
Iteration 158/1000 | Loss: 0.00002864
Iteration 159/1000 | Loss: 0.00002864
Iteration 160/1000 | Loss: 0.00002864
Iteration 161/1000 | Loss: 0.00002864
Iteration 162/1000 | Loss: 0.00002864
Iteration 163/1000 | Loss: 0.00002864
Iteration 164/1000 | Loss: 0.00002864
Iteration 165/1000 | Loss: 0.00002864
Iteration 166/1000 | Loss: 0.00002864
Iteration 167/1000 | Loss: 0.00002864
Iteration 168/1000 | Loss: 0.00002864
Iteration 169/1000 | Loss: 0.00002864
Iteration 170/1000 | Loss: 0.00002864
Iteration 171/1000 | Loss: 0.00002864
Iteration 172/1000 | Loss: 0.00002864
Iteration 173/1000 | Loss: 0.00002864
Iteration 174/1000 | Loss: 0.00002864
Iteration 175/1000 | Loss: 0.00002864
Iteration 176/1000 | Loss: 0.00002864
Iteration 177/1000 | Loss: 0.00002864
Iteration 178/1000 | Loss: 0.00002864
Iteration 179/1000 | Loss: 0.00002864
Iteration 180/1000 | Loss: 0.00002864
Iteration 181/1000 | Loss: 0.00002864
Iteration 182/1000 | Loss: 0.00002864
Iteration 183/1000 | Loss: 0.00002864
Iteration 184/1000 | Loss: 0.00002864
Iteration 185/1000 | Loss: 0.00002864
Iteration 186/1000 | Loss: 0.00002864
Iteration 187/1000 | Loss: 0.00002864
Iteration 188/1000 | Loss: 0.00002864
Iteration 189/1000 | Loss: 0.00002864
Iteration 190/1000 | Loss: 0.00002864
Iteration 191/1000 | Loss: 0.00002864
Iteration 192/1000 | Loss: 0.00002864
Iteration 193/1000 | Loss: 0.00002864
Iteration 194/1000 | Loss: 0.00002864
Iteration 195/1000 | Loss: 0.00002864
Iteration 196/1000 | Loss: 0.00002864
Iteration 197/1000 | Loss: 0.00002864
Iteration 198/1000 | Loss: 0.00002864
Iteration 199/1000 | Loss: 0.00002864
Iteration 200/1000 | Loss: 0.00002864
Iteration 201/1000 | Loss: 0.00002864
Iteration 202/1000 | Loss: 0.00002864
Iteration 203/1000 | Loss: 0.00002864
Iteration 204/1000 | Loss: 0.00002864
Iteration 205/1000 | Loss: 0.00002864
Iteration 206/1000 | Loss: 0.00002864
Iteration 207/1000 | Loss: 0.00002864
Iteration 208/1000 | Loss: 0.00002864
Iteration 209/1000 | Loss: 0.00002864
Iteration 210/1000 | Loss: 0.00002864
Iteration 211/1000 | Loss: 0.00002864
Iteration 212/1000 | Loss: 0.00002864
Iteration 213/1000 | Loss: 0.00002864
Iteration 214/1000 | Loss: 0.00002864
Iteration 215/1000 | Loss: 0.00002864
Iteration 216/1000 | Loss: 0.00002864
Iteration 217/1000 | Loss: 0.00002864
Iteration 218/1000 | Loss: 0.00002864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.8636144634219818e-05, 2.8636144634219818e-05, 2.8636144634219818e-05, 2.8636144634219818e-05, 2.8636144634219818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8636144634219818e-05

Optimization complete. Final v2v error: 4.589465618133545 mm

Highest mean error: 4.813690662384033 mm for frame 18

Lowest mean error: 4.19413423538208 mm for frame 138

Saving results

Total time: 42.7162823677063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072156
Iteration 2/25 | Loss: 0.00224007
Iteration 3/25 | Loss: 0.00167113
Iteration 4/25 | Loss: 0.00155863
Iteration 5/25 | Loss: 0.00152217
Iteration 6/25 | Loss: 0.00152928
Iteration 7/25 | Loss: 0.00150658
Iteration 8/25 | Loss: 0.00149946
Iteration 9/25 | Loss: 0.00148845
Iteration 10/25 | Loss: 0.00148846
Iteration 11/25 | Loss: 0.00148258
Iteration 12/25 | Loss: 0.00147786
Iteration 13/25 | Loss: 0.00147594
Iteration 14/25 | Loss: 0.00148013
Iteration 15/25 | Loss: 0.00147775
Iteration 16/25 | Loss: 0.00147569
Iteration 17/25 | Loss: 0.00147506
Iteration 18/25 | Loss: 0.00147484
Iteration 19/25 | Loss: 0.00147410
Iteration 20/25 | Loss: 0.00147372
Iteration 21/25 | Loss: 0.00147366
Iteration 22/25 | Loss: 0.00147366
Iteration 23/25 | Loss: 0.00147366
Iteration 24/25 | Loss: 0.00147366
Iteration 25/25 | Loss: 0.00147366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18399632
Iteration 2/25 | Loss: 0.00264579
Iteration 3/25 | Loss: 0.00258424
Iteration 4/25 | Loss: 0.00258424
Iteration 5/25 | Loss: 0.00258424
Iteration 6/25 | Loss: 0.00258424
Iteration 7/25 | Loss: 0.00258423
Iteration 8/25 | Loss: 0.00258423
Iteration 9/25 | Loss: 0.00258423
Iteration 10/25 | Loss: 0.00258423
Iteration 11/25 | Loss: 0.00258423
Iteration 12/25 | Loss: 0.00258423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002584234345704317, 0.002584234345704317, 0.002584234345704317, 0.002584234345704317, 0.002584234345704317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002584234345704317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258423
Iteration 2/1000 | Loss: 0.00014132
Iteration 3/1000 | Loss: 0.00016045
Iteration 4/1000 | Loss: 0.00018203
Iteration 5/1000 | Loss: 0.00005191
Iteration 6/1000 | Loss: 0.00004560
Iteration 7/1000 | Loss: 0.00005221
Iteration 8/1000 | Loss: 0.00003927
Iteration 9/1000 | Loss: 0.00003754
Iteration 10/1000 | Loss: 0.00003622
Iteration 11/1000 | Loss: 0.00003478
Iteration 12/1000 | Loss: 0.00003403
Iteration 13/1000 | Loss: 0.00003338
Iteration 14/1000 | Loss: 0.00003304
Iteration 15/1000 | Loss: 0.00003293
Iteration 16/1000 | Loss: 0.00007562
Iteration 17/1000 | Loss: 0.00003826
Iteration 18/1000 | Loss: 0.00003480
Iteration 19/1000 | Loss: 0.00003315
Iteration 20/1000 | Loss: 0.00003219
Iteration 21/1000 | Loss: 0.00003163
Iteration 22/1000 | Loss: 0.00003127
Iteration 23/1000 | Loss: 0.00003107
Iteration 24/1000 | Loss: 0.00003103
Iteration 25/1000 | Loss: 0.00003100
Iteration 26/1000 | Loss: 0.00003100
Iteration 27/1000 | Loss: 0.00003099
Iteration 28/1000 | Loss: 0.00003097
Iteration 29/1000 | Loss: 0.00003097
Iteration 30/1000 | Loss: 0.00003097
Iteration 31/1000 | Loss: 0.00003097
Iteration 32/1000 | Loss: 0.00003097
Iteration 33/1000 | Loss: 0.00003096
Iteration 34/1000 | Loss: 0.00003096
Iteration 35/1000 | Loss: 0.00003096
Iteration 36/1000 | Loss: 0.00003096
Iteration 37/1000 | Loss: 0.00003096
Iteration 38/1000 | Loss: 0.00003096
Iteration 39/1000 | Loss: 0.00003096
Iteration 40/1000 | Loss: 0.00003096
Iteration 41/1000 | Loss: 0.00003096
Iteration 42/1000 | Loss: 0.00003095
Iteration 43/1000 | Loss: 0.00003095
Iteration 44/1000 | Loss: 0.00003094
Iteration 45/1000 | Loss: 0.00003094
Iteration 46/1000 | Loss: 0.00003094
Iteration 47/1000 | Loss: 0.00003093
Iteration 48/1000 | Loss: 0.00003092
Iteration 49/1000 | Loss: 0.00003090
Iteration 50/1000 | Loss: 0.00003090
Iteration 51/1000 | Loss: 0.00003089
Iteration 52/1000 | Loss: 0.00003089
Iteration 53/1000 | Loss: 0.00003088
Iteration 54/1000 | Loss: 0.00003087
Iteration 55/1000 | Loss: 0.00003087
Iteration 56/1000 | Loss: 0.00003087
Iteration 57/1000 | Loss: 0.00003087
Iteration 58/1000 | Loss: 0.00003087
Iteration 59/1000 | Loss: 0.00003087
Iteration 60/1000 | Loss: 0.00003087
Iteration 61/1000 | Loss: 0.00003087
Iteration 62/1000 | Loss: 0.00003087
Iteration 63/1000 | Loss: 0.00003086
Iteration 64/1000 | Loss: 0.00003086
Iteration 65/1000 | Loss: 0.00003086
Iteration 66/1000 | Loss: 0.00003085
Iteration 67/1000 | Loss: 0.00003085
Iteration 68/1000 | Loss: 0.00003084
Iteration 69/1000 | Loss: 0.00003084
Iteration 70/1000 | Loss: 0.00003084
Iteration 71/1000 | Loss: 0.00003084
Iteration 72/1000 | Loss: 0.00003084
Iteration 73/1000 | Loss: 0.00003084
Iteration 74/1000 | Loss: 0.00003083
Iteration 75/1000 | Loss: 0.00003083
Iteration 76/1000 | Loss: 0.00003082
Iteration 77/1000 | Loss: 0.00003082
Iteration 78/1000 | Loss: 0.00003080
Iteration 79/1000 | Loss: 0.00003080
Iteration 80/1000 | Loss: 0.00003077
Iteration 81/1000 | Loss: 0.00003076
Iteration 82/1000 | Loss: 0.00003076
Iteration 83/1000 | Loss: 0.00003075
Iteration 84/1000 | Loss: 0.00003073
Iteration 85/1000 | Loss: 0.00003073
Iteration 86/1000 | Loss: 0.00003073
Iteration 87/1000 | Loss: 0.00003073
Iteration 88/1000 | Loss: 0.00003073
Iteration 89/1000 | Loss: 0.00003073
Iteration 90/1000 | Loss: 0.00003073
Iteration 91/1000 | Loss: 0.00003072
Iteration 92/1000 | Loss: 0.00003071
Iteration 93/1000 | Loss: 0.00003071
Iteration 94/1000 | Loss: 0.00003071
Iteration 95/1000 | Loss: 0.00003071
Iteration 96/1000 | Loss: 0.00003071
Iteration 97/1000 | Loss: 0.00003071
Iteration 98/1000 | Loss: 0.00003071
Iteration 99/1000 | Loss: 0.00003071
Iteration 100/1000 | Loss: 0.00003070
Iteration 101/1000 | Loss: 0.00003070
Iteration 102/1000 | Loss: 0.00003070
Iteration 103/1000 | Loss: 0.00003070
Iteration 104/1000 | Loss: 0.00003069
Iteration 105/1000 | Loss: 0.00003069
Iteration 106/1000 | Loss: 0.00003069
Iteration 107/1000 | Loss: 0.00003069
Iteration 108/1000 | Loss: 0.00003069
Iteration 109/1000 | Loss: 0.00003069
Iteration 110/1000 | Loss: 0.00003068
Iteration 111/1000 | Loss: 0.00003068
Iteration 112/1000 | Loss: 0.00003068
Iteration 113/1000 | Loss: 0.00003068
Iteration 114/1000 | Loss: 0.00003068
Iteration 115/1000 | Loss: 0.00003067
Iteration 116/1000 | Loss: 0.00003067
Iteration 117/1000 | Loss: 0.00003067
Iteration 118/1000 | Loss: 0.00003067
Iteration 119/1000 | Loss: 0.00003067
Iteration 120/1000 | Loss: 0.00003067
Iteration 121/1000 | Loss: 0.00003066
Iteration 122/1000 | Loss: 0.00003066
Iteration 123/1000 | Loss: 0.00003066
Iteration 124/1000 | Loss: 0.00003066
Iteration 125/1000 | Loss: 0.00003066
Iteration 126/1000 | Loss: 0.00003066
Iteration 127/1000 | Loss: 0.00003066
Iteration 128/1000 | Loss: 0.00003066
Iteration 129/1000 | Loss: 0.00003066
Iteration 130/1000 | Loss: 0.00003066
Iteration 131/1000 | Loss: 0.00003065
Iteration 132/1000 | Loss: 0.00003065
Iteration 133/1000 | Loss: 0.00003065
Iteration 134/1000 | Loss: 0.00003065
Iteration 135/1000 | Loss: 0.00003065
Iteration 136/1000 | Loss: 0.00003065
Iteration 137/1000 | Loss: 0.00003064
Iteration 138/1000 | Loss: 0.00003064
Iteration 139/1000 | Loss: 0.00003064
Iteration 140/1000 | Loss: 0.00003064
Iteration 141/1000 | Loss: 0.00003064
Iteration 142/1000 | Loss: 0.00003064
Iteration 143/1000 | Loss: 0.00003064
Iteration 144/1000 | Loss: 0.00003064
Iteration 145/1000 | Loss: 0.00003063
Iteration 146/1000 | Loss: 0.00003063
Iteration 147/1000 | Loss: 0.00003063
Iteration 148/1000 | Loss: 0.00003063
Iteration 149/1000 | Loss: 0.00003062
Iteration 150/1000 | Loss: 0.00003062
Iteration 151/1000 | Loss: 0.00003062
Iteration 152/1000 | Loss: 0.00003062
Iteration 153/1000 | Loss: 0.00003062
Iteration 154/1000 | Loss: 0.00003062
Iteration 155/1000 | Loss: 0.00003062
Iteration 156/1000 | Loss: 0.00003062
Iteration 157/1000 | Loss: 0.00003062
Iteration 158/1000 | Loss: 0.00003062
Iteration 159/1000 | Loss: 0.00003061
Iteration 160/1000 | Loss: 0.00003061
Iteration 161/1000 | Loss: 0.00003061
Iteration 162/1000 | Loss: 0.00003061
Iteration 163/1000 | Loss: 0.00003061
Iteration 164/1000 | Loss: 0.00003061
Iteration 165/1000 | Loss: 0.00003061
Iteration 166/1000 | Loss: 0.00003061
Iteration 167/1000 | Loss: 0.00003061
Iteration 168/1000 | Loss: 0.00003061
Iteration 169/1000 | Loss: 0.00003061
Iteration 170/1000 | Loss: 0.00003061
Iteration 171/1000 | Loss: 0.00003061
Iteration 172/1000 | Loss: 0.00003061
Iteration 173/1000 | Loss: 0.00003061
Iteration 174/1000 | Loss: 0.00003061
Iteration 175/1000 | Loss: 0.00003060
Iteration 176/1000 | Loss: 0.00003060
Iteration 177/1000 | Loss: 0.00003060
Iteration 178/1000 | Loss: 0.00003060
Iteration 179/1000 | Loss: 0.00003060
Iteration 180/1000 | Loss: 0.00003060
Iteration 181/1000 | Loss: 0.00003060
Iteration 182/1000 | Loss: 0.00003060
Iteration 183/1000 | Loss: 0.00003060
Iteration 184/1000 | Loss: 0.00003060
Iteration 185/1000 | Loss: 0.00003060
Iteration 186/1000 | Loss: 0.00003060
Iteration 187/1000 | Loss: 0.00003060
Iteration 188/1000 | Loss: 0.00003060
Iteration 189/1000 | Loss: 0.00003060
Iteration 190/1000 | Loss: 0.00003060
Iteration 191/1000 | Loss: 0.00003060
Iteration 192/1000 | Loss: 0.00003060
Iteration 193/1000 | Loss: 0.00003060
Iteration 194/1000 | Loss: 0.00003060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [3.060486778849736e-05, 3.060486778849736e-05, 3.060486778849736e-05, 3.060486778849736e-05, 3.060486778849736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060486778849736e-05

Optimization complete. Final v2v error: 4.703890800476074 mm

Highest mean error: 11.527120590209961 mm for frame 76

Lowest mean error: 4.170310974121094 mm for frame 29

Saving results

Total time: 90.37088632583618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448793
Iteration 2/25 | Loss: 0.00159352
Iteration 3/25 | Loss: 0.00153007
Iteration 4/25 | Loss: 0.00152050
Iteration 5/25 | Loss: 0.00151623
Iteration 6/25 | Loss: 0.00151582
Iteration 7/25 | Loss: 0.00151582
Iteration 8/25 | Loss: 0.00151582
Iteration 9/25 | Loss: 0.00151582
Iteration 10/25 | Loss: 0.00151582
Iteration 11/25 | Loss: 0.00151582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015158153837546706, 0.0015158153837546706, 0.0015158153837546706, 0.0015158153837546706, 0.0015158153837546706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015158153837546706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12798166
Iteration 2/25 | Loss: 0.00259464
Iteration 3/25 | Loss: 0.00259463
Iteration 4/25 | Loss: 0.00259463
Iteration 5/25 | Loss: 0.00259463
Iteration 6/25 | Loss: 0.00259463
Iteration 7/25 | Loss: 0.00259463
Iteration 8/25 | Loss: 0.00259463
Iteration 9/25 | Loss: 0.00259463
Iteration 10/25 | Loss: 0.00259463
Iteration 11/25 | Loss: 0.00259463
Iteration 12/25 | Loss: 0.00259463
Iteration 13/25 | Loss: 0.00259463
Iteration 14/25 | Loss: 0.00259463
Iteration 15/25 | Loss: 0.00259463
Iteration 16/25 | Loss: 0.00259463
Iteration 17/25 | Loss: 0.00259463
Iteration 18/25 | Loss: 0.00259463
Iteration 19/25 | Loss: 0.00259463
Iteration 20/25 | Loss: 0.00259463
Iteration 21/25 | Loss: 0.00259463
Iteration 22/25 | Loss: 0.00259463
Iteration 23/25 | Loss: 0.00259463
Iteration 24/25 | Loss: 0.00259463
Iteration 25/25 | Loss: 0.00259463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259463
Iteration 2/1000 | Loss: 0.00008301
Iteration 3/1000 | Loss: 0.00005450
Iteration 4/1000 | Loss: 0.00004338
Iteration 5/1000 | Loss: 0.00003955
Iteration 6/1000 | Loss: 0.00003742
Iteration 7/1000 | Loss: 0.00003637
Iteration 8/1000 | Loss: 0.00003562
Iteration 9/1000 | Loss: 0.00003520
Iteration 10/1000 | Loss: 0.00003486
Iteration 11/1000 | Loss: 0.00003479
Iteration 12/1000 | Loss: 0.00003477
Iteration 13/1000 | Loss: 0.00003472
Iteration 14/1000 | Loss: 0.00003460
Iteration 15/1000 | Loss: 0.00003455
Iteration 16/1000 | Loss: 0.00003454
Iteration 17/1000 | Loss: 0.00003450
Iteration 18/1000 | Loss: 0.00003450
Iteration 19/1000 | Loss: 0.00003449
Iteration 20/1000 | Loss: 0.00003449
Iteration 21/1000 | Loss: 0.00003449
Iteration 22/1000 | Loss: 0.00003449
Iteration 23/1000 | Loss: 0.00003449
Iteration 24/1000 | Loss: 0.00003449
Iteration 25/1000 | Loss: 0.00003449
Iteration 26/1000 | Loss: 0.00003449
Iteration 27/1000 | Loss: 0.00003449
Iteration 28/1000 | Loss: 0.00003448
Iteration 29/1000 | Loss: 0.00003448
Iteration 30/1000 | Loss: 0.00003448
Iteration 31/1000 | Loss: 0.00003448
Iteration 32/1000 | Loss: 0.00003448
Iteration 33/1000 | Loss: 0.00003448
Iteration 34/1000 | Loss: 0.00003448
Iteration 35/1000 | Loss: 0.00003447
Iteration 36/1000 | Loss: 0.00003447
Iteration 37/1000 | Loss: 0.00003445
Iteration 38/1000 | Loss: 0.00003444
Iteration 39/1000 | Loss: 0.00003444
Iteration 40/1000 | Loss: 0.00003443
Iteration 41/1000 | Loss: 0.00003442
Iteration 42/1000 | Loss: 0.00003442
Iteration 43/1000 | Loss: 0.00003442
Iteration 44/1000 | Loss: 0.00003442
Iteration 45/1000 | Loss: 0.00003442
Iteration 46/1000 | Loss: 0.00003442
Iteration 47/1000 | Loss: 0.00003442
Iteration 48/1000 | Loss: 0.00003442
Iteration 49/1000 | Loss: 0.00003441
Iteration 50/1000 | Loss: 0.00003441
Iteration 51/1000 | Loss: 0.00003440
Iteration 52/1000 | Loss: 0.00003440
Iteration 53/1000 | Loss: 0.00003440
Iteration 54/1000 | Loss: 0.00003440
Iteration 55/1000 | Loss: 0.00003439
Iteration 56/1000 | Loss: 0.00003439
Iteration 57/1000 | Loss: 0.00003438
Iteration 58/1000 | Loss: 0.00003438
Iteration 59/1000 | Loss: 0.00003438
Iteration 60/1000 | Loss: 0.00003438
Iteration 61/1000 | Loss: 0.00003438
Iteration 62/1000 | Loss: 0.00003437
Iteration 63/1000 | Loss: 0.00003437
Iteration 64/1000 | Loss: 0.00003437
Iteration 65/1000 | Loss: 0.00003437
Iteration 66/1000 | Loss: 0.00003437
Iteration 67/1000 | Loss: 0.00003437
Iteration 68/1000 | Loss: 0.00003436
Iteration 69/1000 | Loss: 0.00003436
Iteration 70/1000 | Loss: 0.00003436
Iteration 71/1000 | Loss: 0.00003436
Iteration 72/1000 | Loss: 0.00003435
Iteration 73/1000 | Loss: 0.00003435
Iteration 74/1000 | Loss: 0.00003435
Iteration 75/1000 | Loss: 0.00003435
Iteration 76/1000 | Loss: 0.00003435
Iteration 77/1000 | Loss: 0.00003435
Iteration 78/1000 | Loss: 0.00003435
Iteration 79/1000 | Loss: 0.00003435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.435295366216451e-05, 3.435295366216451e-05, 3.435295366216451e-05, 3.435295366216451e-05, 3.435295366216451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.435295366216451e-05

Optimization complete. Final v2v error: 4.891546726226807 mm

Highest mean error: 6.058177471160889 mm for frame 88

Lowest mean error: 4.376427173614502 mm for frame 2

Saving results

Total time: 30.978378534317017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105321
Iteration 2/25 | Loss: 0.00248351
Iteration 3/25 | Loss: 0.00362951
Iteration 4/25 | Loss: 0.00241526
Iteration 5/25 | Loss: 0.00201081
Iteration 6/25 | Loss: 0.00169552
Iteration 7/25 | Loss: 0.00175555
Iteration 8/25 | Loss: 0.00162204
Iteration 9/25 | Loss: 0.00158500
Iteration 10/25 | Loss: 0.00152506
Iteration 11/25 | Loss: 0.00151224
Iteration 12/25 | Loss: 0.00150338
Iteration 13/25 | Loss: 0.00149044
Iteration 14/25 | Loss: 0.00148771
Iteration 15/25 | Loss: 0.00148057
Iteration 16/25 | Loss: 0.00147887
Iteration 17/25 | Loss: 0.00147879
Iteration 18/25 | Loss: 0.00147683
Iteration 19/25 | Loss: 0.00147856
Iteration 20/25 | Loss: 0.00148190
Iteration 21/25 | Loss: 0.00147395
Iteration 22/25 | Loss: 0.00146950
Iteration 23/25 | Loss: 0.00146733
Iteration 24/25 | Loss: 0.00147005
Iteration 25/25 | Loss: 0.00146924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23211503
Iteration 2/25 | Loss: 0.00341193
Iteration 3/25 | Loss: 0.00242256
Iteration 4/25 | Loss: 0.00242256
Iteration 5/25 | Loss: 0.00242256
Iteration 6/25 | Loss: 0.00242256
Iteration 7/25 | Loss: 0.00242256
Iteration 8/25 | Loss: 0.00242255
Iteration 9/25 | Loss: 0.00242256
Iteration 10/25 | Loss: 0.00242255
Iteration 11/25 | Loss: 0.00242255
Iteration 12/25 | Loss: 0.00242255
Iteration 13/25 | Loss: 0.00242255
Iteration 14/25 | Loss: 0.00242255
Iteration 15/25 | Loss: 0.00242255
Iteration 16/25 | Loss: 0.00242255
Iteration 17/25 | Loss: 0.00242255
Iteration 18/25 | Loss: 0.00242255
Iteration 19/25 | Loss: 0.00242255
Iteration 20/25 | Loss: 0.00242255
Iteration 21/25 | Loss: 0.00242255
Iteration 22/25 | Loss: 0.00242255
Iteration 23/25 | Loss: 0.00242255
Iteration 24/25 | Loss: 0.00242255
Iteration 25/25 | Loss: 0.00242255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242255
Iteration 2/1000 | Loss: 0.00103179
Iteration 3/1000 | Loss: 0.00171213
Iteration 4/1000 | Loss: 0.00230138
Iteration 5/1000 | Loss: 0.00180947
Iteration 6/1000 | Loss: 0.00099159
Iteration 7/1000 | Loss: 0.00049675
Iteration 8/1000 | Loss: 0.00016863
Iteration 9/1000 | Loss: 0.00023152
Iteration 10/1000 | Loss: 0.00028774
Iteration 11/1000 | Loss: 0.00024502
Iteration 12/1000 | Loss: 0.00027952
Iteration 13/1000 | Loss: 0.00023546
Iteration 14/1000 | Loss: 0.00010231
Iteration 15/1000 | Loss: 0.00024477
Iteration 16/1000 | Loss: 0.00024317
Iteration 17/1000 | Loss: 0.00034594
Iteration 18/1000 | Loss: 0.00028026
Iteration 19/1000 | Loss: 0.00054914
Iteration 20/1000 | Loss: 0.00056173
Iteration 21/1000 | Loss: 0.00091588
Iteration 22/1000 | Loss: 0.00168890
Iteration 23/1000 | Loss: 0.00334298
Iteration 24/1000 | Loss: 0.00176759
Iteration 25/1000 | Loss: 0.00034306
Iteration 26/1000 | Loss: 0.00146091
Iteration 27/1000 | Loss: 0.00012968
Iteration 28/1000 | Loss: 0.00038107
Iteration 29/1000 | Loss: 0.00037905
Iteration 30/1000 | Loss: 0.00012443
Iteration 31/1000 | Loss: 0.00029629
Iteration 32/1000 | Loss: 0.00064676
Iteration 33/1000 | Loss: 0.00075644
Iteration 34/1000 | Loss: 0.00070659
Iteration 35/1000 | Loss: 0.00041279
Iteration 36/1000 | Loss: 0.00080018
Iteration 37/1000 | Loss: 0.00052720
Iteration 38/1000 | Loss: 0.00025130
Iteration 39/1000 | Loss: 0.00019044
Iteration 40/1000 | Loss: 0.00007175
Iteration 41/1000 | Loss: 0.00026234
Iteration 42/1000 | Loss: 0.00039237
Iteration 43/1000 | Loss: 0.00050305
Iteration 44/1000 | Loss: 0.00019282
Iteration 45/1000 | Loss: 0.00027311
Iteration 46/1000 | Loss: 0.00023066
Iteration 47/1000 | Loss: 0.00032397
Iteration 48/1000 | Loss: 0.00025296
Iteration 49/1000 | Loss: 0.00012466
Iteration 50/1000 | Loss: 0.00055139
Iteration 51/1000 | Loss: 0.00030621
Iteration 52/1000 | Loss: 0.00030995
Iteration 53/1000 | Loss: 0.00017631
Iteration 54/1000 | Loss: 0.00012899
Iteration 55/1000 | Loss: 0.00013375
Iteration 56/1000 | Loss: 0.00011281
Iteration 57/1000 | Loss: 0.00014049
Iteration 58/1000 | Loss: 0.00013621
Iteration 59/1000 | Loss: 0.00010969
Iteration 60/1000 | Loss: 0.00012444
Iteration 61/1000 | Loss: 0.00018758
Iteration 62/1000 | Loss: 0.00016424
Iteration 63/1000 | Loss: 0.00015169
Iteration 64/1000 | Loss: 0.00010921
Iteration 65/1000 | Loss: 0.00029529
Iteration 66/1000 | Loss: 0.00021370
Iteration 67/1000 | Loss: 0.00026159
Iteration 68/1000 | Loss: 0.00020756
Iteration 69/1000 | Loss: 0.00018715
Iteration 70/1000 | Loss: 0.00049347
Iteration 71/1000 | Loss: 0.00034756
Iteration 72/1000 | Loss: 0.00029233
Iteration 73/1000 | Loss: 0.00005849
Iteration 74/1000 | Loss: 0.00005285
Iteration 75/1000 | Loss: 0.00021911
Iteration 76/1000 | Loss: 0.00021935
Iteration 77/1000 | Loss: 0.00022130
Iteration 78/1000 | Loss: 0.00005536
Iteration 79/1000 | Loss: 0.00004974
Iteration 80/1000 | Loss: 0.00004775
Iteration 81/1000 | Loss: 0.00021711
Iteration 82/1000 | Loss: 0.00012211
Iteration 83/1000 | Loss: 0.00018783
Iteration 84/1000 | Loss: 0.00005411
Iteration 85/1000 | Loss: 0.00053064
Iteration 86/1000 | Loss: 0.00004958
Iteration 87/1000 | Loss: 0.00004669
Iteration 88/1000 | Loss: 0.00004527
Iteration 89/1000 | Loss: 0.00004432
Iteration 90/1000 | Loss: 0.00004362
Iteration 91/1000 | Loss: 0.00004319
Iteration 92/1000 | Loss: 0.00004292
Iteration 93/1000 | Loss: 0.00023789
Iteration 94/1000 | Loss: 0.00041765
Iteration 95/1000 | Loss: 0.00014757
Iteration 96/1000 | Loss: 0.00005370
Iteration 97/1000 | Loss: 0.00004508
Iteration 98/1000 | Loss: 0.00004297
Iteration 99/1000 | Loss: 0.00004176
Iteration 100/1000 | Loss: 0.00004086
Iteration 101/1000 | Loss: 0.00027777
Iteration 102/1000 | Loss: 0.00029205
Iteration 103/1000 | Loss: 0.00004311
Iteration 104/1000 | Loss: 0.00027787
Iteration 105/1000 | Loss: 0.00022667
Iteration 106/1000 | Loss: 0.00014935
Iteration 107/1000 | Loss: 0.00005021
Iteration 108/1000 | Loss: 0.00004512
Iteration 109/1000 | Loss: 0.00004226
Iteration 110/1000 | Loss: 0.00004114
Iteration 111/1000 | Loss: 0.00004043
Iteration 112/1000 | Loss: 0.00003963
Iteration 113/1000 | Loss: 0.00011386
Iteration 114/1000 | Loss: 0.00004377
Iteration 115/1000 | Loss: 0.00004182
Iteration 116/1000 | Loss: 0.00004071
Iteration 117/1000 | Loss: 0.00003965
Iteration 118/1000 | Loss: 0.00013391
Iteration 119/1000 | Loss: 0.00012882
Iteration 120/1000 | Loss: 0.00004916
Iteration 121/1000 | Loss: 0.00051087
Iteration 122/1000 | Loss: 0.00017017
Iteration 123/1000 | Loss: 0.00013904
Iteration 124/1000 | Loss: 0.00010410
Iteration 125/1000 | Loss: 0.00004593
Iteration 126/1000 | Loss: 0.00003920
Iteration 127/1000 | Loss: 0.00003825
Iteration 128/1000 | Loss: 0.00003790
Iteration 129/1000 | Loss: 0.00009922
Iteration 130/1000 | Loss: 0.00004085
Iteration 131/1000 | Loss: 0.00003859
Iteration 132/1000 | Loss: 0.00003786
Iteration 133/1000 | Loss: 0.00025468
Iteration 134/1000 | Loss: 0.00003957
Iteration 135/1000 | Loss: 0.00003687
Iteration 136/1000 | Loss: 0.00003587
Iteration 137/1000 | Loss: 0.00003533
Iteration 138/1000 | Loss: 0.00003498
Iteration 139/1000 | Loss: 0.00003490
Iteration 140/1000 | Loss: 0.00003486
Iteration 141/1000 | Loss: 0.00003476
Iteration 142/1000 | Loss: 0.00003474
Iteration 143/1000 | Loss: 0.00003471
Iteration 144/1000 | Loss: 0.00003471
Iteration 145/1000 | Loss: 0.00003470
Iteration 146/1000 | Loss: 0.00003470
Iteration 147/1000 | Loss: 0.00003468
Iteration 148/1000 | Loss: 0.00003466
Iteration 149/1000 | Loss: 0.00003466
Iteration 150/1000 | Loss: 0.00003458
Iteration 151/1000 | Loss: 0.00003451
Iteration 152/1000 | Loss: 0.00003451
Iteration 153/1000 | Loss: 0.00003451
Iteration 154/1000 | Loss: 0.00003450
Iteration 155/1000 | Loss: 0.00003450
Iteration 156/1000 | Loss: 0.00003449
Iteration 157/1000 | Loss: 0.00003449
Iteration 158/1000 | Loss: 0.00003449
Iteration 159/1000 | Loss: 0.00003449
Iteration 160/1000 | Loss: 0.00003449
Iteration 161/1000 | Loss: 0.00003448
Iteration 162/1000 | Loss: 0.00003448
Iteration 163/1000 | Loss: 0.00003448
Iteration 164/1000 | Loss: 0.00003448
Iteration 165/1000 | Loss: 0.00003448
Iteration 166/1000 | Loss: 0.00003448
Iteration 167/1000 | Loss: 0.00003447
Iteration 168/1000 | Loss: 0.00003447
Iteration 169/1000 | Loss: 0.00003447
Iteration 170/1000 | Loss: 0.00003446
Iteration 171/1000 | Loss: 0.00003446
Iteration 172/1000 | Loss: 0.00003446
Iteration 173/1000 | Loss: 0.00003446
Iteration 174/1000 | Loss: 0.00003445
Iteration 175/1000 | Loss: 0.00003445
Iteration 176/1000 | Loss: 0.00003445
Iteration 177/1000 | Loss: 0.00003445
Iteration 178/1000 | Loss: 0.00003445
Iteration 179/1000 | Loss: 0.00003444
Iteration 180/1000 | Loss: 0.00003444
Iteration 181/1000 | Loss: 0.00003444
Iteration 182/1000 | Loss: 0.00003444
Iteration 183/1000 | Loss: 0.00003444
Iteration 184/1000 | Loss: 0.00003444
Iteration 185/1000 | Loss: 0.00003444
Iteration 186/1000 | Loss: 0.00003444
Iteration 187/1000 | Loss: 0.00003444
Iteration 188/1000 | Loss: 0.00003444
Iteration 189/1000 | Loss: 0.00003444
Iteration 190/1000 | Loss: 0.00003444
Iteration 191/1000 | Loss: 0.00003444
Iteration 192/1000 | Loss: 0.00003444
Iteration 193/1000 | Loss: 0.00003444
Iteration 194/1000 | Loss: 0.00003444
Iteration 195/1000 | Loss: 0.00003444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [3.443803871050477e-05, 3.443803871050477e-05, 3.443803871050477e-05, 3.443803871050477e-05, 3.443803871050477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.443803871050477e-05

Optimization complete. Final v2v error: 4.767281532287598 mm

Highest mean error: 10.75876235961914 mm for frame 146

Lowest mean error: 4.330175876617432 mm for frame 168

Saving results

Total time: 283.9431335926056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049189
Iteration 2/25 | Loss: 0.00493002
Iteration 3/25 | Loss: 0.00274215
Iteration 4/25 | Loss: 0.00228678
Iteration 5/25 | Loss: 0.00214974
Iteration 6/25 | Loss: 0.00204798
Iteration 7/25 | Loss: 0.00202294
Iteration 8/25 | Loss: 0.00195550
Iteration 9/25 | Loss: 0.00192786
Iteration 10/25 | Loss: 0.00192439
Iteration 11/25 | Loss: 0.00191234
Iteration 12/25 | Loss: 0.00190134
Iteration 13/25 | Loss: 0.00189862
Iteration 14/25 | Loss: 0.00189765
Iteration 15/25 | Loss: 0.00189562
Iteration 16/25 | Loss: 0.00189089
Iteration 17/25 | Loss: 0.00188937
Iteration 18/25 | Loss: 0.00188508
Iteration 19/25 | Loss: 0.00187803
Iteration 20/25 | Loss: 0.00187530
Iteration 21/25 | Loss: 0.00187185
Iteration 22/25 | Loss: 0.00187026
Iteration 23/25 | Loss: 0.00186951
Iteration 24/25 | Loss: 0.00187320
Iteration 25/25 | Loss: 0.00187096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14731407
Iteration 2/25 | Loss: 0.00983432
Iteration 3/25 | Loss: 0.00954496
Iteration 4/25 | Loss: 0.00954496
Iteration 5/25 | Loss: 0.00954496
Iteration 6/25 | Loss: 0.00954496
Iteration 7/25 | Loss: 0.00954496
Iteration 8/25 | Loss: 0.00954496
Iteration 9/25 | Loss: 0.00954496
Iteration 10/25 | Loss: 0.00954496
Iteration 11/25 | Loss: 0.00954496
Iteration 12/25 | Loss: 0.00954496
Iteration 13/25 | Loss: 0.00954496
Iteration 14/25 | Loss: 0.00954496
Iteration 15/25 | Loss: 0.00954496
Iteration 16/25 | Loss: 0.00954496
Iteration 17/25 | Loss: 0.00954496
Iteration 18/25 | Loss: 0.00954496
Iteration 19/25 | Loss: 0.00954496
Iteration 20/25 | Loss: 0.00954496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.009544958360493183, 0.009544958360493183, 0.009544958360493183, 0.009544958360493183, 0.009544958360493183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009544958360493183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00954496
Iteration 2/1000 | Loss: 0.00169774
Iteration 3/1000 | Loss: 0.00141378
Iteration 4/1000 | Loss: 0.00128188
Iteration 5/1000 | Loss: 0.00138699
Iteration 6/1000 | Loss: 0.00228549
Iteration 7/1000 | Loss: 0.00258935
Iteration 8/1000 | Loss: 0.00484805
Iteration 9/1000 | Loss: 0.00221750
Iteration 10/1000 | Loss: 0.00261608
Iteration 11/1000 | Loss: 0.00138359
Iteration 12/1000 | Loss: 0.00083318
Iteration 13/1000 | Loss: 0.00131023
Iteration 14/1000 | Loss: 0.00130876
Iteration 15/1000 | Loss: 0.00186685
Iteration 16/1000 | Loss: 0.00230253
Iteration 17/1000 | Loss: 0.00045592
Iteration 18/1000 | Loss: 0.00118405
Iteration 19/1000 | Loss: 0.00037020
Iteration 20/1000 | Loss: 0.00030193
Iteration 21/1000 | Loss: 0.00083137
Iteration 22/1000 | Loss: 0.00410406
Iteration 23/1000 | Loss: 0.00410161
Iteration 24/1000 | Loss: 0.00383807
Iteration 25/1000 | Loss: 0.00151029
Iteration 26/1000 | Loss: 0.00124596
Iteration 27/1000 | Loss: 0.00048634
Iteration 28/1000 | Loss: 0.00082319
Iteration 29/1000 | Loss: 0.00109498
Iteration 30/1000 | Loss: 0.00107867
Iteration 31/1000 | Loss: 0.00063260
Iteration 32/1000 | Loss: 0.00048665
Iteration 33/1000 | Loss: 0.00068207
Iteration 34/1000 | Loss: 0.00045923
Iteration 35/1000 | Loss: 0.00051397
Iteration 36/1000 | Loss: 0.00039766
Iteration 37/1000 | Loss: 0.00115329
Iteration 38/1000 | Loss: 0.00043214
Iteration 39/1000 | Loss: 0.00041093
Iteration 40/1000 | Loss: 0.00040722
Iteration 41/1000 | Loss: 0.00061025
Iteration 42/1000 | Loss: 0.00046211
Iteration 43/1000 | Loss: 0.00039363
Iteration 44/1000 | Loss: 0.00075247
Iteration 45/1000 | Loss: 0.00040756
Iteration 46/1000 | Loss: 0.00136801
Iteration 47/1000 | Loss: 0.00041821
Iteration 48/1000 | Loss: 0.00029223
Iteration 49/1000 | Loss: 0.00038814
Iteration 50/1000 | Loss: 0.00023206
Iteration 51/1000 | Loss: 0.00037023
Iteration 52/1000 | Loss: 0.00019413
Iteration 53/1000 | Loss: 0.00036305
Iteration 54/1000 | Loss: 0.00039224
Iteration 55/1000 | Loss: 0.00025541
Iteration 56/1000 | Loss: 0.00052057
Iteration 57/1000 | Loss: 0.00064742
Iteration 58/1000 | Loss: 0.00020653
Iteration 59/1000 | Loss: 0.00018736
Iteration 60/1000 | Loss: 0.00039918
Iteration 61/1000 | Loss: 0.00038635
Iteration 62/1000 | Loss: 0.00114551
Iteration 63/1000 | Loss: 0.00085583
Iteration 64/1000 | Loss: 0.00052987
Iteration 65/1000 | Loss: 0.00060183
Iteration 66/1000 | Loss: 0.00053678
Iteration 67/1000 | Loss: 0.00029578
Iteration 68/1000 | Loss: 0.00036118
Iteration 69/1000 | Loss: 0.00055020
Iteration 70/1000 | Loss: 0.00052489
Iteration 71/1000 | Loss: 0.00030048
Iteration 72/1000 | Loss: 0.00022473
Iteration 73/1000 | Loss: 0.00025196
Iteration 74/1000 | Loss: 0.00026059
Iteration 75/1000 | Loss: 0.00017936
Iteration 76/1000 | Loss: 0.00018258
Iteration 77/1000 | Loss: 0.00017596
Iteration 78/1000 | Loss: 0.00017073
Iteration 79/1000 | Loss: 0.00016197
Iteration 80/1000 | Loss: 0.00017824
Iteration 81/1000 | Loss: 0.00023301
Iteration 82/1000 | Loss: 0.00016091
Iteration 83/1000 | Loss: 0.00016232
Iteration 84/1000 | Loss: 0.00016540
Iteration 85/1000 | Loss: 0.00017605
Iteration 86/1000 | Loss: 0.00072049
Iteration 87/1000 | Loss: 0.00129458
Iteration 88/1000 | Loss: 0.00311321
Iteration 89/1000 | Loss: 0.00300571
Iteration 90/1000 | Loss: 0.00295647
Iteration 91/1000 | Loss: 0.00043763
Iteration 92/1000 | Loss: 0.00091422
Iteration 93/1000 | Loss: 0.00085853
Iteration 94/1000 | Loss: 0.00092645
Iteration 95/1000 | Loss: 0.00139843
Iteration 96/1000 | Loss: 0.00065132
Iteration 97/1000 | Loss: 0.00079736
Iteration 98/1000 | Loss: 0.00068737
Iteration 99/1000 | Loss: 0.00055856
Iteration 100/1000 | Loss: 0.00048166
Iteration 101/1000 | Loss: 0.00019142
Iteration 102/1000 | Loss: 0.00017639
Iteration 103/1000 | Loss: 0.00052753
Iteration 104/1000 | Loss: 0.00063830
Iteration 105/1000 | Loss: 0.00069399
Iteration 106/1000 | Loss: 0.00031743
Iteration 107/1000 | Loss: 0.00049903
Iteration 108/1000 | Loss: 0.00063906
Iteration 109/1000 | Loss: 0.00041843
Iteration 110/1000 | Loss: 0.00022303
Iteration 111/1000 | Loss: 0.00052881
Iteration 112/1000 | Loss: 0.00150235
Iteration 113/1000 | Loss: 0.00037843
Iteration 114/1000 | Loss: 0.00017330
Iteration 115/1000 | Loss: 0.00040653
Iteration 116/1000 | Loss: 0.00018011
Iteration 117/1000 | Loss: 0.00015431
Iteration 118/1000 | Loss: 0.00040901
Iteration 119/1000 | Loss: 0.00066528
Iteration 120/1000 | Loss: 0.00041022
Iteration 121/1000 | Loss: 0.00055267
Iteration 122/1000 | Loss: 0.00039343
Iteration 123/1000 | Loss: 0.00013989
Iteration 124/1000 | Loss: 0.00015423
Iteration 125/1000 | Loss: 0.00083512
Iteration 126/1000 | Loss: 0.00068520
Iteration 127/1000 | Loss: 0.00026940
Iteration 128/1000 | Loss: 0.00059753
Iteration 129/1000 | Loss: 0.00028587
Iteration 130/1000 | Loss: 0.00034371
Iteration 131/1000 | Loss: 0.00025699
Iteration 132/1000 | Loss: 0.00017686
Iteration 133/1000 | Loss: 0.00021477
Iteration 134/1000 | Loss: 0.00013982
Iteration 135/1000 | Loss: 0.00017845
Iteration 136/1000 | Loss: 0.00044431
Iteration 137/1000 | Loss: 0.00021270
Iteration 138/1000 | Loss: 0.00042151
Iteration 139/1000 | Loss: 0.00025140
Iteration 140/1000 | Loss: 0.00043589
Iteration 141/1000 | Loss: 0.00018534
Iteration 142/1000 | Loss: 0.00025967
Iteration 143/1000 | Loss: 0.00034727
Iteration 144/1000 | Loss: 0.00019820
Iteration 145/1000 | Loss: 0.00035489
Iteration 146/1000 | Loss: 0.00089106
Iteration 147/1000 | Loss: 0.00055109
Iteration 148/1000 | Loss: 0.00038825
Iteration 149/1000 | Loss: 0.00028539
Iteration 150/1000 | Loss: 0.00017529
Iteration 151/1000 | Loss: 0.00036525
Iteration 152/1000 | Loss: 0.00014693
Iteration 153/1000 | Loss: 0.00018012
Iteration 154/1000 | Loss: 0.00038208
Iteration 155/1000 | Loss: 0.00052293
Iteration 156/1000 | Loss: 0.00019551
Iteration 157/1000 | Loss: 0.00018853
Iteration 158/1000 | Loss: 0.00013021
Iteration 159/1000 | Loss: 0.00058303
Iteration 160/1000 | Loss: 0.00019455
Iteration 161/1000 | Loss: 0.00022985
Iteration 162/1000 | Loss: 0.00034958
Iteration 163/1000 | Loss: 0.00042504
Iteration 164/1000 | Loss: 0.00063788
Iteration 165/1000 | Loss: 0.00134897
Iteration 166/1000 | Loss: 0.00095677
Iteration 167/1000 | Loss: 0.00044157
Iteration 168/1000 | Loss: 0.00020008
Iteration 169/1000 | Loss: 0.00013160
Iteration 170/1000 | Loss: 0.00025665
Iteration 171/1000 | Loss: 0.00012742
Iteration 172/1000 | Loss: 0.00034076
Iteration 173/1000 | Loss: 0.00017209
Iteration 174/1000 | Loss: 0.00015858
Iteration 175/1000 | Loss: 0.00017809
Iteration 176/1000 | Loss: 0.00012454
Iteration 177/1000 | Loss: 0.00013820
Iteration 178/1000 | Loss: 0.00015222
Iteration 179/1000 | Loss: 0.00012161
Iteration 180/1000 | Loss: 0.00043452
Iteration 181/1000 | Loss: 0.00013187
Iteration 182/1000 | Loss: 0.00012312
Iteration 183/1000 | Loss: 0.00012689
Iteration 184/1000 | Loss: 0.00014464
Iteration 185/1000 | Loss: 0.00011344
Iteration 186/1000 | Loss: 0.00012605
Iteration 187/1000 | Loss: 0.00011760
Iteration 188/1000 | Loss: 0.00011480
Iteration 189/1000 | Loss: 0.00011266
Iteration 190/1000 | Loss: 0.00011251
Iteration 191/1000 | Loss: 0.00012208
Iteration 192/1000 | Loss: 0.00011348
Iteration 193/1000 | Loss: 0.00018362
Iteration 194/1000 | Loss: 0.00012984
Iteration 195/1000 | Loss: 0.00013714
Iteration 196/1000 | Loss: 0.00011574
Iteration 197/1000 | Loss: 0.00014026
Iteration 198/1000 | Loss: 0.00011406
Iteration 199/1000 | Loss: 0.00015134
Iteration 200/1000 | Loss: 0.00011454
Iteration 201/1000 | Loss: 0.00012525
Iteration 202/1000 | Loss: 0.00011252
Iteration 203/1000 | Loss: 0.00011689
Iteration 204/1000 | Loss: 0.00011235
Iteration 205/1000 | Loss: 0.00011245
Iteration 206/1000 | Loss: 0.00011203
Iteration 207/1000 | Loss: 0.00011260
Iteration 208/1000 | Loss: 0.00011209
Iteration 209/1000 | Loss: 0.00014049
Iteration 210/1000 | Loss: 0.00011229
Iteration 211/1000 | Loss: 0.00011934
Iteration 212/1000 | Loss: 0.00011211
Iteration 213/1000 | Loss: 0.00012412
Iteration 214/1000 | Loss: 0.00011194
Iteration 215/1000 | Loss: 0.00011259
Iteration 216/1000 | Loss: 0.00065765
Iteration 217/1000 | Loss: 0.00024221
Iteration 218/1000 | Loss: 0.00012351
Iteration 219/1000 | Loss: 0.00011802
Iteration 220/1000 | Loss: 0.00012075
Iteration 221/1000 | Loss: 0.00013927
Iteration 222/1000 | Loss: 0.00013779
Iteration 223/1000 | Loss: 0.00011125
Iteration 224/1000 | Loss: 0.00014557
Iteration 225/1000 | Loss: 0.00010826
Iteration 226/1000 | Loss: 0.00012795
Iteration 227/1000 | Loss: 0.00011597
Iteration 228/1000 | Loss: 0.00010661
Iteration 229/1000 | Loss: 0.00010624
Iteration 230/1000 | Loss: 0.00011960
Iteration 231/1000 | Loss: 0.00011665
Iteration 232/1000 | Loss: 0.00012152
Iteration 233/1000 | Loss: 0.00010578
Iteration 234/1000 | Loss: 0.00010570
Iteration 235/1000 | Loss: 0.00010565
Iteration 236/1000 | Loss: 0.00010563
Iteration 237/1000 | Loss: 0.00010563
Iteration 238/1000 | Loss: 0.00010562
Iteration 239/1000 | Loss: 0.00010562
Iteration 240/1000 | Loss: 0.00010562
Iteration 241/1000 | Loss: 0.00010562
Iteration 242/1000 | Loss: 0.00010562
Iteration 243/1000 | Loss: 0.00010562
Iteration 244/1000 | Loss: 0.00010562
Iteration 245/1000 | Loss: 0.00010562
Iteration 246/1000 | Loss: 0.00010562
Iteration 247/1000 | Loss: 0.00010562
Iteration 248/1000 | Loss: 0.00010562
Iteration 249/1000 | Loss: 0.00010562
Iteration 250/1000 | Loss: 0.00010561
Iteration 251/1000 | Loss: 0.00010561
Iteration 252/1000 | Loss: 0.00010560
Iteration 253/1000 | Loss: 0.00010560
Iteration 254/1000 | Loss: 0.00010559
Iteration 255/1000 | Loss: 0.00010559
Iteration 256/1000 | Loss: 0.00010557
Iteration 257/1000 | Loss: 0.00010556
Iteration 258/1000 | Loss: 0.00011924
Iteration 259/1000 | Loss: 0.00011473
Iteration 260/1000 | Loss: 0.00010792
Iteration 261/1000 | Loss: 0.00010546
Iteration 262/1000 | Loss: 0.00010546
Iteration 263/1000 | Loss: 0.00010546
Iteration 264/1000 | Loss: 0.00010545
Iteration 265/1000 | Loss: 0.00010545
Iteration 266/1000 | Loss: 0.00010545
Iteration 267/1000 | Loss: 0.00010545
Iteration 268/1000 | Loss: 0.00010545
Iteration 269/1000 | Loss: 0.00010545
Iteration 270/1000 | Loss: 0.00010545
Iteration 271/1000 | Loss: 0.00010545
Iteration 272/1000 | Loss: 0.00010545
Iteration 273/1000 | Loss: 0.00010545
Iteration 274/1000 | Loss: 0.00010545
Iteration 275/1000 | Loss: 0.00010544
Iteration 276/1000 | Loss: 0.00010544
Iteration 277/1000 | Loss: 0.00010544
Iteration 278/1000 | Loss: 0.00010544
Iteration 279/1000 | Loss: 0.00010544
Iteration 280/1000 | Loss: 0.00010544
Iteration 281/1000 | Loss: 0.00010544
Iteration 282/1000 | Loss: 0.00010544
Iteration 283/1000 | Loss: 0.00010544
Iteration 284/1000 | Loss: 0.00010544
Iteration 285/1000 | Loss: 0.00010543
Iteration 286/1000 | Loss: 0.00010543
Iteration 287/1000 | Loss: 0.00010543
Iteration 288/1000 | Loss: 0.00010543
Iteration 289/1000 | Loss: 0.00010543
Iteration 290/1000 | Loss: 0.00010543
Iteration 291/1000 | Loss: 0.00010543
Iteration 292/1000 | Loss: 0.00010543
Iteration 293/1000 | Loss: 0.00010543
Iteration 294/1000 | Loss: 0.00010543
Iteration 295/1000 | Loss: 0.00010543
Iteration 296/1000 | Loss: 0.00010543
Iteration 297/1000 | Loss: 0.00010543
Iteration 298/1000 | Loss: 0.00010543
Iteration 299/1000 | Loss: 0.00010543
Iteration 300/1000 | Loss: 0.00010543
Iteration 301/1000 | Loss: 0.00010543
Iteration 302/1000 | Loss: 0.00010543
Iteration 303/1000 | Loss: 0.00010543
Iteration 304/1000 | Loss: 0.00010542
Iteration 305/1000 | Loss: 0.00010542
Iteration 306/1000 | Loss: 0.00010542
Iteration 307/1000 | Loss: 0.00010542
Iteration 308/1000 | Loss: 0.00010542
Iteration 309/1000 | Loss: 0.00010542
Iteration 310/1000 | Loss: 0.00010542
Iteration 311/1000 | Loss: 0.00010542
Iteration 312/1000 | Loss: 0.00010542
Iteration 313/1000 | Loss: 0.00010542
Iteration 314/1000 | Loss: 0.00010541
Iteration 315/1000 | Loss: 0.00010541
Iteration 316/1000 | Loss: 0.00010541
Iteration 317/1000 | Loss: 0.00010541
Iteration 318/1000 | Loss: 0.00010541
Iteration 319/1000 | Loss: 0.00010541
Iteration 320/1000 | Loss: 0.00010541
Iteration 321/1000 | Loss: 0.00010541
Iteration 322/1000 | Loss: 0.00010541
Iteration 323/1000 | Loss: 0.00010540
Iteration 324/1000 | Loss: 0.00010540
Iteration 325/1000 | Loss: 0.00010540
Iteration 326/1000 | Loss: 0.00010540
Iteration 327/1000 | Loss: 0.00010540
Iteration 328/1000 | Loss: 0.00010540
Iteration 329/1000 | Loss: 0.00010540
Iteration 330/1000 | Loss: 0.00010540
Iteration 331/1000 | Loss: 0.00010540
Iteration 332/1000 | Loss: 0.00010540
Iteration 333/1000 | Loss: 0.00010540
Iteration 334/1000 | Loss: 0.00010540
Iteration 335/1000 | Loss: 0.00010540
Iteration 336/1000 | Loss: 0.00010540
Iteration 337/1000 | Loss: 0.00010540
Iteration 338/1000 | Loss: 0.00010540
Iteration 339/1000 | Loss: 0.00010540
Iteration 340/1000 | Loss: 0.00010540
Iteration 341/1000 | Loss: 0.00010540
Iteration 342/1000 | Loss: 0.00010540
Iteration 343/1000 | Loss: 0.00010540
Iteration 344/1000 | Loss: 0.00010539
Iteration 345/1000 | Loss: 0.00010539
Iteration 346/1000 | Loss: 0.00010539
Iteration 347/1000 | Loss: 0.00010539
Iteration 348/1000 | Loss: 0.00010539
Iteration 349/1000 | Loss: 0.00010539
Iteration 350/1000 | Loss: 0.00010539
Iteration 351/1000 | Loss: 0.00011635
Iteration 352/1000 | Loss: 0.00014532
Iteration 353/1000 | Loss: 0.00010766
Iteration 354/1000 | Loss: 0.00010536
Iteration 355/1000 | Loss: 0.00010536
Iteration 356/1000 | Loss: 0.00010536
Iteration 357/1000 | Loss: 0.00010536
Iteration 358/1000 | Loss: 0.00010536
Iteration 359/1000 | Loss: 0.00010536
Iteration 360/1000 | Loss: 0.00010536
Iteration 361/1000 | Loss: 0.00010536
Iteration 362/1000 | Loss: 0.00010536
Iteration 363/1000 | Loss: 0.00010536
Iteration 364/1000 | Loss: 0.00010536
Iteration 365/1000 | Loss: 0.00010536
Iteration 366/1000 | Loss: 0.00010536
Iteration 367/1000 | Loss: 0.00010536
Iteration 368/1000 | Loss: 0.00010536
Iteration 369/1000 | Loss: 0.00010536
Iteration 370/1000 | Loss: 0.00010536
Iteration 371/1000 | Loss: 0.00010536
Iteration 372/1000 | Loss: 0.00010536
Iteration 373/1000 | Loss: 0.00010536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 373. Stopping optimization.
Last 5 losses: [0.0001053558080457151, 0.0001053558080457151, 0.0001053558080457151, 0.0001053558080457151, 0.0001053558080457151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001053558080457151

Optimization complete. Final v2v error: 5.536215305328369 mm

Highest mean error: 13.410045623779297 mm for frame 98

Lowest mean error: 3.9522106647491455 mm for frame 237

Saving results

Total time: 440.4647173881531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465109
Iteration 2/25 | Loss: 0.00160877
Iteration 3/25 | Loss: 0.00150550
Iteration 4/25 | Loss: 0.00148138
Iteration 5/25 | Loss: 0.00147237
Iteration 6/25 | Loss: 0.00147061
Iteration 7/25 | Loss: 0.00147015
Iteration 8/25 | Loss: 0.00147015
Iteration 9/25 | Loss: 0.00147015
Iteration 10/25 | Loss: 0.00147015
Iteration 11/25 | Loss: 0.00147015
Iteration 12/25 | Loss: 0.00147015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014701536856591702, 0.0014701536856591702, 0.0014701536856591702, 0.0014701536856591702, 0.0014701536856591702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014701536856591702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16079032
Iteration 2/25 | Loss: 0.00260890
Iteration 3/25 | Loss: 0.00260889
Iteration 4/25 | Loss: 0.00260889
Iteration 5/25 | Loss: 0.00260889
Iteration 6/25 | Loss: 0.00260889
Iteration 7/25 | Loss: 0.00260889
Iteration 8/25 | Loss: 0.00260889
Iteration 9/25 | Loss: 0.00260889
Iteration 10/25 | Loss: 0.00260889
Iteration 11/25 | Loss: 0.00260889
Iteration 12/25 | Loss: 0.00260889
Iteration 13/25 | Loss: 0.00260889
Iteration 14/25 | Loss: 0.00260889
Iteration 15/25 | Loss: 0.00260889
Iteration 16/25 | Loss: 0.00260889
Iteration 17/25 | Loss: 0.00260889
Iteration 18/25 | Loss: 0.00260889
Iteration 19/25 | Loss: 0.00260889
Iteration 20/25 | Loss: 0.00260889
Iteration 21/25 | Loss: 0.00260889
Iteration 22/25 | Loss: 0.00260889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002608891576528549, 0.002608891576528549, 0.002608891576528549, 0.002608891576528549, 0.002608891576528549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002608891576528549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260889
Iteration 2/1000 | Loss: 0.00008707
Iteration 3/1000 | Loss: 0.00005443
Iteration 4/1000 | Loss: 0.00004716
Iteration 5/1000 | Loss: 0.00004060
Iteration 6/1000 | Loss: 0.00003717
Iteration 7/1000 | Loss: 0.00003517
Iteration 8/1000 | Loss: 0.00003388
Iteration 9/1000 | Loss: 0.00003314
Iteration 10/1000 | Loss: 0.00003253
Iteration 11/1000 | Loss: 0.00003219
Iteration 12/1000 | Loss: 0.00003191
Iteration 13/1000 | Loss: 0.00003178
Iteration 14/1000 | Loss: 0.00003178
Iteration 15/1000 | Loss: 0.00003178
Iteration 16/1000 | Loss: 0.00003178
Iteration 17/1000 | Loss: 0.00003178
Iteration 18/1000 | Loss: 0.00003178
Iteration 19/1000 | Loss: 0.00003178
Iteration 20/1000 | Loss: 0.00003178
Iteration 21/1000 | Loss: 0.00003175
Iteration 22/1000 | Loss: 0.00003175
Iteration 23/1000 | Loss: 0.00003174
Iteration 24/1000 | Loss: 0.00003170
Iteration 25/1000 | Loss: 0.00003169
Iteration 26/1000 | Loss: 0.00003168
Iteration 27/1000 | Loss: 0.00003168
Iteration 28/1000 | Loss: 0.00003167
Iteration 29/1000 | Loss: 0.00003163
Iteration 30/1000 | Loss: 0.00003161
Iteration 31/1000 | Loss: 0.00003161
Iteration 32/1000 | Loss: 0.00003161
Iteration 33/1000 | Loss: 0.00003161
Iteration 34/1000 | Loss: 0.00003161
Iteration 35/1000 | Loss: 0.00003160
Iteration 36/1000 | Loss: 0.00003159
Iteration 37/1000 | Loss: 0.00003159
Iteration 38/1000 | Loss: 0.00003158
Iteration 39/1000 | Loss: 0.00003158
Iteration 40/1000 | Loss: 0.00003158
Iteration 41/1000 | Loss: 0.00003158
Iteration 42/1000 | Loss: 0.00003158
Iteration 43/1000 | Loss: 0.00003157
Iteration 44/1000 | Loss: 0.00003157
Iteration 45/1000 | Loss: 0.00003156
Iteration 46/1000 | Loss: 0.00003156
Iteration 47/1000 | Loss: 0.00003156
Iteration 48/1000 | Loss: 0.00003156
Iteration 49/1000 | Loss: 0.00003156
Iteration 50/1000 | Loss: 0.00003156
Iteration 51/1000 | Loss: 0.00003156
Iteration 52/1000 | Loss: 0.00003156
Iteration 53/1000 | Loss: 0.00003156
Iteration 54/1000 | Loss: 0.00003156
Iteration 55/1000 | Loss: 0.00003156
Iteration 56/1000 | Loss: 0.00003155
Iteration 57/1000 | Loss: 0.00003155
Iteration 58/1000 | Loss: 0.00003155
Iteration 59/1000 | Loss: 0.00003155
Iteration 60/1000 | Loss: 0.00003155
Iteration 61/1000 | Loss: 0.00003155
Iteration 62/1000 | Loss: 0.00003155
Iteration 63/1000 | Loss: 0.00003155
Iteration 64/1000 | Loss: 0.00003155
Iteration 65/1000 | Loss: 0.00003154
Iteration 66/1000 | Loss: 0.00003154
Iteration 67/1000 | Loss: 0.00003153
Iteration 68/1000 | Loss: 0.00003153
Iteration 69/1000 | Loss: 0.00003153
Iteration 70/1000 | Loss: 0.00003153
Iteration 71/1000 | Loss: 0.00003152
Iteration 72/1000 | Loss: 0.00003152
Iteration 73/1000 | Loss: 0.00003152
Iteration 74/1000 | Loss: 0.00003151
Iteration 75/1000 | Loss: 0.00003151
Iteration 76/1000 | Loss: 0.00003151
Iteration 77/1000 | Loss: 0.00003151
Iteration 78/1000 | Loss: 0.00003151
Iteration 79/1000 | Loss: 0.00003151
Iteration 80/1000 | Loss: 0.00003151
Iteration 81/1000 | Loss: 0.00003151
Iteration 82/1000 | Loss: 0.00003150
Iteration 83/1000 | Loss: 0.00003150
Iteration 84/1000 | Loss: 0.00003150
Iteration 85/1000 | Loss: 0.00003149
Iteration 86/1000 | Loss: 0.00003149
Iteration 87/1000 | Loss: 0.00003149
Iteration 88/1000 | Loss: 0.00003149
Iteration 89/1000 | Loss: 0.00003149
Iteration 90/1000 | Loss: 0.00003149
Iteration 91/1000 | Loss: 0.00003149
Iteration 92/1000 | Loss: 0.00003149
Iteration 93/1000 | Loss: 0.00003148
Iteration 94/1000 | Loss: 0.00003148
Iteration 95/1000 | Loss: 0.00003148
Iteration 96/1000 | Loss: 0.00003148
Iteration 97/1000 | Loss: 0.00003148
Iteration 98/1000 | Loss: 0.00003147
Iteration 99/1000 | Loss: 0.00003147
Iteration 100/1000 | Loss: 0.00003147
Iteration 101/1000 | Loss: 0.00003147
Iteration 102/1000 | Loss: 0.00003147
Iteration 103/1000 | Loss: 0.00003147
Iteration 104/1000 | Loss: 0.00003147
Iteration 105/1000 | Loss: 0.00003147
Iteration 106/1000 | Loss: 0.00003147
Iteration 107/1000 | Loss: 0.00003147
Iteration 108/1000 | Loss: 0.00003147
Iteration 109/1000 | Loss: 0.00003147
Iteration 110/1000 | Loss: 0.00003147
Iteration 111/1000 | Loss: 0.00003147
Iteration 112/1000 | Loss: 0.00003147
Iteration 113/1000 | Loss: 0.00003147
Iteration 114/1000 | Loss: 0.00003147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.1469608074985445e-05, 3.1469608074985445e-05, 3.1469608074985445e-05, 3.1469608074985445e-05, 3.1469608074985445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1469608074985445e-05

Optimization complete. Final v2v error: 4.659970283508301 mm

Highest mean error: 5.0998711585998535 mm for frame 85

Lowest mean error: 4.394229888916016 mm for frame 2

Saving results

Total time: 35.560428857803345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452523
Iteration 2/25 | Loss: 0.00119279
Iteration 3/25 | Loss: 0.00112321
Iteration 4/25 | Loss: 0.00111668
Iteration 5/25 | Loss: 0.00111618
Iteration 6/25 | Loss: 0.00111618
Iteration 7/25 | Loss: 0.00111618
Iteration 8/25 | Loss: 0.00111618
Iteration 9/25 | Loss: 0.00111618
Iteration 10/25 | Loss: 0.00111618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011161777656525373, 0.0011161777656525373, 0.0011161777656525373, 0.0011161777656525373, 0.0011161777656525373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011161777656525373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75940824
Iteration 2/25 | Loss: 0.00298618
Iteration 3/25 | Loss: 0.00298614
Iteration 4/25 | Loss: 0.00298614
Iteration 5/25 | Loss: 0.00298614
Iteration 6/25 | Loss: 0.00298614
Iteration 7/25 | Loss: 0.00298614
Iteration 8/25 | Loss: 0.00298614
Iteration 9/25 | Loss: 0.00298614
Iteration 10/25 | Loss: 0.00298614
Iteration 11/25 | Loss: 0.00298614
Iteration 12/25 | Loss: 0.00298614
Iteration 13/25 | Loss: 0.00298614
Iteration 14/25 | Loss: 0.00298614
Iteration 15/25 | Loss: 0.00298614
Iteration 16/25 | Loss: 0.00298614
Iteration 17/25 | Loss: 0.00298614
Iteration 18/25 | Loss: 0.00298614
Iteration 19/25 | Loss: 0.00298614
Iteration 20/25 | Loss: 0.00298614
Iteration 21/25 | Loss: 0.00298614
Iteration 22/25 | Loss: 0.00298614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002986140316352248, 0.002986140316352248, 0.002986140316352248, 0.002986140316352248, 0.002986140316352248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002986140316352248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298614
Iteration 2/1000 | Loss: 0.00003038
Iteration 3/1000 | Loss: 0.00002175
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001372
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001204
Iteration 17/1000 | Loss: 0.00001204
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001201
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001199
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001192
Iteration 52/1000 | Loss: 0.00001192
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001177
Iteration 116/1000 | Loss: 0.00001177
Iteration 117/1000 | Loss: 0.00001177
Iteration 118/1000 | Loss: 0.00001177
Iteration 119/1000 | Loss: 0.00001176
Iteration 120/1000 | Loss: 0.00001176
Iteration 121/1000 | Loss: 0.00001176
Iteration 122/1000 | Loss: 0.00001176
Iteration 123/1000 | Loss: 0.00001176
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001176
Iteration 126/1000 | Loss: 0.00001175
Iteration 127/1000 | Loss: 0.00001175
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001175
Iteration 130/1000 | Loss: 0.00001175
Iteration 131/1000 | Loss: 0.00001174
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001174
Iteration 135/1000 | Loss: 0.00001174
Iteration 136/1000 | Loss: 0.00001174
Iteration 137/1000 | Loss: 0.00001174
Iteration 138/1000 | Loss: 0.00001173
Iteration 139/1000 | Loss: 0.00001173
Iteration 140/1000 | Loss: 0.00001173
Iteration 141/1000 | Loss: 0.00001173
Iteration 142/1000 | Loss: 0.00001173
Iteration 143/1000 | Loss: 0.00001173
Iteration 144/1000 | Loss: 0.00001173
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001172
Iteration 147/1000 | Loss: 0.00001172
Iteration 148/1000 | Loss: 0.00001172
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001170
Iteration 164/1000 | Loss: 0.00001170
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001170
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.17038698590477e-05, 1.17038698590477e-05, 1.17038698590477e-05, 1.17038698590477e-05, 1.17038698590477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.17038698590477e-05

Optimization complete. Final v2v error: 2.9635095596313477 mm

Highest mean error: 3.3957371711730957 mm for frame 142

Lowest mean error: 2.660274028778076 mm for frame 204

Saving results

Total time: 42.28405404090881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865363
Iteration 2/25 | Loss: 0.00139455
Iteration 3/25 | Loss: 0.00118756
Iteration 4/25 | Loss: 0.00115995
Iteration 5/25 | Loss: 0.00115680
Iteration 6/25 | Loss: 0.00115680
Iteration 7/25 | Loss: 0.00115680
Iteration 8/25 | Loss: 0.00115680
Iteration 9/25 | Loss: 0.00115680
Iteration 10/25 | Loss: 0.00115680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011568034533411264, 0.0011568034533411264, 0.0011568034533411264, 0.0011568034533411264, 0.0011568034533411264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011568034533411264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13708305
Iteration 2/25 | Loss: 0.00333566
Iteration 3/25 | Loss: 0.00333566
Iteration 4/25 | Loss: 0.00333566
Iteration 5/25 | Loss: 0.00333565
Iteration 6/25 | Loss: 0.00333565
Iteration 7/25 | Loss: 0.00333565
Iteration 8/25 | Loss: 0.00333565
Iteration 9/25 | Loss: 0.00333565
Iteration 10/25 | Loss: 0.00333565
Iteration 11/25 | Loss: 0.00333565
Iteration 12/25 | Loss: 0.00333565
Iteration 13/25 | Loss: 0.00333565
Iteration 14/25 | Loss: 0.00333565
Iteration 15/25 | Loss: 0.00333565
Iteration 16/25 | Loss: 0.00333565
Iteration 17/25 | Loss: 0.00333565
Iteration 18/25 | Loss: 0.00333565
Iteration 19/25 | Loss: 0.00333565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0033356526400893927, 0.0033356526400893927, 0.0033356526400893927, 0.0033356526400893927, 0.0033356526400893927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033356526400893927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333565
Iteration 2/1000 | Loss: 0.00004746
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00001947
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001191
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001142
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001132
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001130
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001123
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001122
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001113
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001113
Iteration 56/1000 | Loss: 0.00001112
Iteration 57/1000 | Loss: 0.00001112
Iteration 58/1000 | Loss: 0.00001112
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001111
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001110
Iteration 64/1000 | Loss: 0.00001110
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001109
Iteration 68/1000 | Loss: 0.00001109
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001108
Iteration 71/1000 | Loss: 0.00001107
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001106
Iteration 79/1000 | Loss: 0.00001106
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001098
Iteration 105/1000 | Loss: 0.00001098
Iteration 106/1000 | Loss: 0.00001098
Iteration 107/1000 | Loss: 0.00001098
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001098
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001098
Iteration 126/1000 | Loss: 0.00001098
Iteration 127/1000 | Loss: 0.00001098
Iteration 128/1000 | Loss: 0.00001098
Iteration 129/1000 | Loss: 0.00001098
Iteration 130/1000 | Loss: 0.00001098
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.0978926184179727e-05, 1.0978926184179727e-05, 1.0978926184179727e-05, 1.0978926184179727e-05, 1.0978926184179727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0978926184179727e-05

Optimization complete. Final v2v error: 2.807710886001587 mm

Highest mean error: 3.4670021533966064 mm for frame 113

Lowest mean error: 2.2768709659576416 mm for frame 221

Saving results

Total time: 43.11618709564209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01123725
Iteration 2/25 | Loss: 0.01123725
Iteration 3/25 | Loss: 0.01123725
Iteration 4/25 | Loss: 0.00267027
Iteration 5/25 | Loss: 0.00233946
Iteration 6/25 | Loss: 0.00244430
Iteration 7/25 | Loss: 0.00247462
Iteration 8/25 | Loss: 0.00177807
Iteration 9/25 | Loss: 0.00140543
Iteration 10/25 | Loss: 0.00131756
Iteration 11/25 | Loss: 0.00129247
Iteration 12/25 | Loss: 0.00128731
Iteration 13/25 | Loss: 0.00128825
Iteration 14/25 | Loss: 0.00129347
Iteration 15/25 | Loss: 0.00129357
Iteration 16/25 | Loss: 0.00129447
Iteration 17/25 | Loss: 0.00129490
Iteration 18/25 | Loss: 0.00128541
Iteration 19/25 | Loss: 0.00128393
Iteration 20/25 | Loss: 0.00128555
Iteration 21/25 | Loss: 0.00128456
Iteration 22/25 | Loss: 0.00128132
Iteration 23/25 | Loss: 0.00127954
Iteration 24/25 | Loss: 0.00127700
Iteration 25/25 | Loss: 0.00127378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08253932
Iteration 2/25 | Loss: 0.00221662
Iteration 3/25 | Loss: 0.00221661
Iteration 4/25 | Loss: 0.00221661
Iteration 5/25 | Loss: 0.00221661
Iteration 6/25 | Loss: 0.00221661
Iteration 7/25 | Loss: 0.00221661
Iteration 8/25 | Loss: 0.00221661
Iteration 9/25 | Loss: 0.00221661
Iteration 10/25 | Loss: 0.00221661
Iteration 11/25 | Loss: 0.00221661
Iteration 12/25 | Loss: 0.00221661
Iteration 13/25 | Loss: 0.00221661
Iteration 14/25 | Loss: 0.00221661
Iteration 15/25 | Loss: 0.00221661
Iteration 16/25 | Loss: 0.00221661
Iteration 17/25 | Loss: 0.00221661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002216607565060258, 0.002216607565060258, 0.002216607565060258, 0.002216607565060258, 0.002216607565060258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002216607565060258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221661
Iteration 2/1000 | Loss: 0.00011334
Iteration 3/1000 | Loss: 0.00008699
Iteration 4/1000 | Loss: 0.00008757
Iteration 5/1000 | Loss: 0.00008234
Iteration 6/1000 | Loss: 0.00015683
Iteration 7/1000 | Loss: 0.00013165
Iteration 8/1000 | Loss: 0.00011051
Iteration 9/1000 | Loss: 0.00014614
Iteration 10/1000 | Loss: 0.00016305
Iteration 11/1000 | Loss: 0.00015773
Iteration 12/1000 | Loss: 0.00013332
Iteration 13/1000 | Loss: 0.00015810
Iteration 14/1000 | Loss: 0.00012253
Iteration 15/1000 | Loss: 0.00012607
Iteration 16/1000 | Loss: 0.00013418
Iteration 17/1000 | Loss: 0.00021781
Iteration 18/1000 | Loss: 0.00017983
Iteration 19/1000 | Loss: 0.00003953
Iteration 20/1000 | Loss: 0.00005349
Iteration 21/1000 | Loss: 0.00013782
Iteration 22/1000 | Loss: 0.00011059
Iteration 23/1000 | Loss: 0.00012150
Iteration 24/1000 | Loss: 0.00022952
Iteration 25/1000 | Loss: 0.00005189
Iteration 26/1000 | Loss: 0.00009378
Iteration 27/1000 | Loss: 0.00006061
Iteration 28/1000 | Loss: 0.00007961
Iteration 29/1000 | Loss: 0.00008592
Iteration 30/1000 | Loss: 0.00009617
Iteration 31/1000 | Loss: 0.00007469
Iteration 32/1000 | Loss: 0.00010051
Iteration 33/1000 | Loss: 0.00007224
Iteration 34/1000 | Loss: 0.00005881
Iteration 35/1000 | Loss: 0.00006512
Iteration 36/1000 | Loss: 0.00008104
Iteration 37/1000 | Loss: 0.00009995
Iteration 38/1000 | Loss: 0.00009254
Iteration 39/1000 | Loss: 0.00010630
Iteration 40/1000 | Loss: 0.00010273
Iteration 41/1000 | Loss: 0.00009370
Iteration 42/1000 | Loss: 0.00004342
Iteration 43/1000 | Loss: 0.00007192
Iteration 44/1000 | Loss: 0.00008784
Iteration 45/1000 | Loss: 0.00004982
Iteration 46/1000 | Loss: 0.00008811
Iteration 47/1000 | Loss: 0.00008521
Iteration 48/1000 | Loss: 0.00007554
Iteration 49/1000 | Loss: 0.00012883
Iteration 50/1000 | Loss: 0.00014122
Iteration 51/1000 | Loss: 0.00011350
Iteration 52/1000 | Loss: 0.00008189
Iteration 53/1000 | Loss: 0.00004335
Iteration 54/1000 | Loss: 0.00009933
Iteration 55/1000 | Loss: 0.00010469
Iteration 56/1000 | Loss: 0.00011543
Iteration 57/1000 | Loss: 0.00004697
Iteration 58/1000 | Loss: 0.00004181
Iteration 59/1000 | Loss: 0.00003904
Iteration 60/1000 | Loss: 0.00003710
Iteration 61/1000 | Loss: 0.00003576
Iteration 62/1000 | Loss: 0.00003497
Iteration 63/1000 | Loss: 0.00013579
Iteration 64/1000 | Loss: 0.00004085
Iteration 65/1000 | Loss: 0.00003755
Iteration 66/1000 | Loss: 0.00012025
Iteration 67/1000 | Loss: 0.00013149
Iteration 68/1000 | Loss: 0.00012446
Iteration 69/1000 | Loss: 0.00004701
Iteration 70/1000 | Loss: 0.00006045
Iteration 71/1000 | Loss: 0.00005080
Iteration 72/1000 | Loss: 0.00015593
Iteration 73/1000 | Loss: 0.00012342
Iteration 74/1000 | Loss: 0.00007842
Iteration 75/1000 | Loss: 0.00003645
Iteration 76/1000 | Loss: 0.00004365
Iteration 77/1000 | Loss: 0.00004145
Iteration 78/1000 | Loss: 0.00004557
Iteration 79/1000 | Loss: 0.00003959
Iteration 80/1000 | Loss: 0.00004833
Iteration 81/1000 | Loss: 0.00003832
Iteration 82/1000 | Loss: 0.00004821
Iteration 83/1000 | Loss: 0.00003849
Iteration 84/1000 | Loss: 0.00004436
Iteration 85/1000 | Loss: 0.00004641
Iteration 86/1000 | Loss: 0.00004347
Iteration 87/1000 | Loss: 0.00004552
Iteration 88/1000 | Loss: 0.00004341
Iteration 89/1000 | Loss: 0.00003534
Iteration 90/1000 | Loss: 0.00003441
Iteration 91/1000 | Loss: 0.00003374
Iteration 92/1000 | Loss: 0.00003322
Iteration 93/1000 | Loss: 0.00003287
Iteration 94/1000 | Loss: 0.00003266
Iteration 95/1000 | Loss: 0.00003260
Iteration 96/1000 | Loss: 0.00003256
Iteration 97/1000 | Loss: 0.00003252
Iteration 98/1000 | Loss: 0.00003250
Iteration 99/1000 | Loss: 0.00003245
Iteration 100/1000 | Loss: 0.00003245
Iteration 101/1000 | Loss: 0.00003245
Iteration 102/1000 | Loss: 0.00003243
Iteration 103/1000 | Loss: 0.00004584
Iteration 104/1000 | Loss: 0.00003415
Iteration 105/1000 | Loss: 0.00003562
Iteration 106/1000 | Loss: 0.00004655
Iteration 107/1000 | Loss: 0.00003447
Iteration 108/1000 | Loss: 0.00004394
Iteration 109/1000 | Loss: 0.00003240
Iteration 110/1000 | Loss: 0.00003202
Iteration 111/1000 | Loss: 0.00003188
Iteration 112/1000 | Loss: 0.00003172
Iteration 113/1000 | Loss: 0.00003170
Iteration 114/1000 | Loss: 0.00003170
Iteration 115/1000 | Loss: 0.00003167
Iteration 116/1000 | Loss: 0.00003166
Iteration 117/1000 | Loss: 0.00003166
Iteration 118/1000 | Loss: 0.00003165
Iteration 119/1000 | Loss: 0.00003163
Iteration 120/1000 | Loss: 0.00003161
Iteration 121/1000 | Loss: 0.00003159
Iteration 122/1000 | Loss: 0.00003158
Iteration 123/1000 | Loss: 0.00003158
Iteration 124/1000 | Loss: 0.00003157
Iteration 125/1000 | Loss: 0.00003156
Iteration 126/1000 | Loss: 0.00003156
Iteration 127/1000 | Loss: 0.00003155
Iteration 128/1000 | Loss: 0.00003155
Iteration 129/1000 | Loss: 0.00003155
Iteration 130/1000 | Loss: 0.00003155
Iteration 131/1000 | Loss: 0.00003154
Iteration 132/1000 | Loss: 0.00003154
Iteration 133/1000 | Loss: 0.00003154
Iteration 134/1000 | Loss: 0.00003154
Iteration 135/1000 | Loss: 0.00003154
Iteration 136/1000 | Loss: 0.00003154
Iteration 137/1000 | Loss: 0.00003154
Iteration 138/1000 | Loss: 0.00003154
Iteration 139/1000 | Loss: 0.00003154
Iteration 140/1000 | Loss: 0.00003153
Iteration 141/1000 | Loss: 0.00003153
Iteration 142/1000 | Loss: 0.00003153
Iteration 143/1000 | Loss: 0.00003153
Iteration 144/1000 | Loss: 0.00003153
Iteration 145/1000 | Loss: 0.00003153
Iteration 146/1000 | Loss: 0.00003152
Iteration 147/1000 | Loss: 0.00003152
Iteration 148/1000 | Loss: 0.00003152
Iteration 149/1000 | Loss: 0.00003152
Iteration 150/1000 | Loss: 0.00003152
Iteration 151/1000 | Loss: 0.00003151
Iteration 152/1000 | Loss: 0.00003151
Iteration 153/1000 | Loss: 0.00003151
Iteration 154/1000 | Loss: 0.00003151
Iteration 155/1000 | Loss: 0.00003151
Iteration 156/1000 | Loss: 0.00003151
Iteration 157/1000 | Loss: 0.00003150
Iteration 158/1000 | Loss: 0.00003150
Iteration 159/1000 | Loss: 0.00003150
Iteration 160/1000 | Loss: 0.00003150
Iteration 161/1000 | Loss: 0.00003149
Iteration 162/1000 | Loss: 0.00003149
Iteration 163/1000 | Loss: 0.00003149
Iteration 164/1000 | Loss: 0.00003149
Iteration 165/1000 | Loss: 0.00003149
Iteration 166/1000 | Loss: 0.00003149
Iteration 167/1000 | Loss: 0.00003149
Iteration 168/1000 | Loss: 0.00003149
Iteration 169/1000 | Loss: 0.00003149
Iteration 170/1000 | Loss: 0.00003149
Iteration 171/1000 | Loss: 0.00003149
Iteration 172/1000 | Loss: 0.00003149
Iteration 173/1000 | Loss: 0.00003149
Iteration 174/1000 | Loss: 0.00003149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.149018812109716e-05, 3.149018812109716e-05, 3.149018812109716e-05, 3.149018812109716e-05, 3.149018812109716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.149018812109716e-05

Optimization complete. Final v2v error: 4.261855125427246 mm

Highest mean error: 5.225447654724121 mm for frame 117

Lowest mean error: 4.008862018585205 mm for frame 0

Saving results

Total time: 224.98356342315674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951460
Iteration 2/25 | Loss: 0.00381363
Iteration 3/25 | Loss: 0.00284914
Iteration 4/25 | Loss: 0.00245299
Iteration 5/25 | Loss: 0.00239791
Iteration 6/25 | Loss: 0.00229393
Iteration 7/25 | Loss: 0.00206836
Iteration 8/25 | Loss: 0.00191993
Iteration 9/25 | Loss: 0.00185887
Iteration 10/25 | Loss: 0.00177589
Iteration 11/25 | Loss: 0.00179085
Iteration 12/25 | Loss: 0.00176350
Iteration 13/25 | Loss: 0.00176174
Iteration 14/25 | Loss: 0.00172076
Iteration 15/25 | Loss: 0.00171186
Iteration 16/25 | Loss: 0.00171014
Iteration 17/25 | Loss: 0.00170335
Iteration 18/25 | Loss: 0.00168413
Iteration 19/25 | Loss: 0.00169815
Iteration 20/25 | Loss: 0.00168256
Iteration 21/25 | Loss: 0.00171733
Iteration 22/25 | Loss: 0.00169268
Iteration 23/25 | Loss: 0.00169298
Iteration 24/25 | Loss: 0.00169102
Iteration 25/25 | Loss: 0.00167271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.84384322
Iteration 2/25 | Loss: 0.01273615
Iteration 3/25 | Loss: 0.00859567
Iteration 4/25 | Loss: 0.00858887
Iteration 5/25 | Loss: 0.00857521
Iteration 6/25 | Loss: 0.00857520
Iteration 7/25 | Loss: 0.00857520
Iteration 8/25 | Loss: 0.00857520
Iteration 9/25 | Loss: 0.00857520
Iteration 10/25 | Loss: 0.00857520
Iteration 11/25 | Loss: 0.00857520
Iteration 12/25 | Loss: 0.00857520
Iteration 13/25 | Loss: 0.00857520
Iteration 14/25 | Loss: 0.00857520
Iteration 15/25 | Loss: 0.00857520
Iteration 16/25 | Loss: 0.00857520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.008575198240578175, 0.008575198240578175, 0.008575198240578175, 0.008575198240578175, 0.008575198240578175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008575198240578175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00857520
Iteration 2/1000 | Loss: 0.00693706
Iteration 3/1000 | Loss: 0.00714802
Iteration 4/1000 | Loss: 0.00585311
Iteration 5/1000 | Loss: 0.00588184
Iteration 6/1000 | Loss: 0.00524467
Iteration 7/1000 | Loss: 0.00570848
Iteration 8/1000 | Loss: 0.00607862
Iteration 9/1000 | Loss: 0.00438687
Iteration 10/1000 | Loss: 0.00476477
Iteration 11/1000 | Loss: 0.00439284
Iteration 12/1000 | Loss: 0.00443880
Iteration 13/1000 | Loss: 0.00393025
Iteration 14/1000 | Loss: 0.00466884
Iteration 15/1000 | Loss: 0.00528135
Iteration 16/1000 | Loss: 0.00378369
Iteration 17/1000 | Loss: 0.00442649
Iteration 18/1000 | Loss: 0.00363735
Iteration 19/1000 | Loss: 0.00371948
Iteration 20/1000 | Loss: 0.00317916
Iteration 21/1000 | Loss: 0.00509838
Iteration 22/1000 | Loss: 0.00359268
Iteration 23/1000 | Loss: 0.00466575
Iteration 24/1000 | Loss: 0.00281476
Iteration 25/1000 | Loss: 0.00312768
Iteration 26/1000 | Loss: 0.00295236
Iteration 27/1000 | Loss: 0.00560382
Iteration 28/1000 | Loss: 0.00322194
Iteration 29/1000 | Loss: 0.00309523
Iteration 30/1000 | Loss: 0.00270280
Iteration 31/1000 | Loss: 0.00243354
Iteration 32/1000 | Loss: 0.00367721
Iteration 33/1000 | Loss: 0.00219195
Iteration 34/1000 | Loss: 0.00221155
Iteration 35/1000 | Loss: 0.00576952
Iteration 36/1000 | Loss: 0.00557161
Iteration 37/1000 | Loss: 0.00293013
Iteration 38/1000 | Loss: 0.00414978
Iteration 39/1000 | Loss: 0.00222788
Iteration 40/1000 | Loss: 0.00294254
Iteration 41/1000 | Loss: 0.00332811
Iteration 42/1000 | Loss: 0.00223340
Iteration 43/1000 | Loss: 0.00255418
Iteration 44/1000 | Loss: 0.00211479
Iteration 45/1000 | Loss: 0.00182257
Iteration 46/1000 | Loss: 0.00215698
Iteration 47/1000 | Loss: 0.00287717
Iteration 48/1000 | Loss: 0.00215487
Iteration 49/1000 | Loss: 0.00281667
Iteration 50/1000 | Loss: 0.00257896
Iteration 51/1000 | Loss: 0.00222786
Iteration 52/1000 | Loss: 0.00461152
Iteration 53/1000 | Loss: 0.00396113
Iteration 54/1000 | Loss: 0.00586592
Iteration 55/1000 | Loss: 0.00403365
Iteration 56/1000 | Loss: 0.00343687
Iteration 57/1000 | Loss: 0.00639119
Iteration 58/1000 | Loss: 0.00608992
Iteration 59/1000 | Loss: 0.00330619
Iteration 60/1000 | Loss: 0.00200095
Iteration 61/1000 | Loss: 0.00307582
Iteration 62/1000 | Loss: 0.00165997
Iteration 63/1000 | Loss: 0.00242955
Iteration 64/1000 | Loss: 0.00249181
Iteration 65/1000 | Loss: 0.00266658
Iteration 66/1000 | Loss: 0.00159065
Iteration 67/1000 | Loss: 0.00173203
Iteration 68/1000 | Loss: 0.00270297
Iteration 69/1000 | Loss: 0.00188283
Iteration 70/1000 | Loss: 0.00144067
Iteration 71/1000 | Loss: 0.00161705
Iteration 72/1000 | Loss: 0.00163686
Iteration 73/1000 | Loss: 0.00289595
Iteration 74/1000 | Loss: 0.00143987
Iteration 75/1000 | Loss: 0.00190491
Iteration 76/1000 | Loss: 0.00222988
Iteration 77/1000 | Loss: 0.00168221
Iteration 78/1000 | Loss: 0.00281052
Iteration 79/1000 | Loss: 0.00110001
Iteration 80/1000 | Loss: 0.00177552
Iteration 81/1000 | Loss: 0.00185720
Iteration 82/1000 | Loss: 0.00339842
Iteration 83/1000 | Loss: 0.00198282
Iteration 84/1000 | Loss: 0.00224516
Iteration 85/1000 | Loss: 0.00268885
Iteration 86/1000 | Loss: 0.00081011
Iteration 87/1000 | Loss: 0.00047776
Iteration 88/1000 | Loss: 0.00101749
Iteration 89/1000 | Loss: 0.00085916
Iteration 90/1000 | Loss: 0.00065477
Iteration 91/1000 | Loss: 0.00042106
Iteration 92/1000 | Loss: 0.00085706
Iteration 93/1000 | Loss: 0.00062294
Iteration 94/1000 | Loss: 0.00065127
Iteration 95/1000 | Loss: 0.00043660
Iteration 96/1000 | Loss: 0.00088205
Iteration 97/1000 | Loss: 0.00065318
Iteration 98/1000 | Loss: 0.00065205
Iteration 99/1000 | Loss: 0.00047778
Iteration 100/1000 | Loss: 0.00076155
Iteration 101/1000 | Loss: 0.00042802
Iteration 102/1000 | Loss: 0.00031272
Iteration 103/1000 | Loss: 0.00027908
Iteration 104/1000 | Loss: 0.00128604
Iteration 105/1000 | Loss: 0.00047784
Iteration 106/1000 | Loss: 0.00080882
Iteration 107/1000 | Loss: 0.00067856
Iteration 108/1000 | Loss: 0.00063601
Iteration 109/1000 | Loss: 0.00036546
Iteration 110/1000 | Loss: 0.00022344
Iteration 111/1000 | Loss: 0.00032649
Iteration 112/1000 | Loss: 0.00020770
Iteration 113/1000 | Loss: 0.00044057
Iteration 114/1000 | Loss: 0.00031764
Iteration 115/1000 | Loss: 0.00043875
Iteration 116/1000 | Loss: 0.00040447
Iteration 117/1000 | Loss: 0.00170876
Iteration 118/1000 | Loss: 0.00021802
Iteration 119/1000 | Loss: 0.00080312
Iteration 120/1000 | Loss: 0.00022793
Iteration 121/1000 | Loss: 0.00026000
Iteration 122/1000 | Loss: 0.00013731
Iteration 123/1000 | Loss: 0.00013003
Iteration 124/1000 | Loss: 0.00012558
Iteration 125/1000 | Loss: 0.00035509
Iteration 126/1000 | Loss: 0.00137133
Iteration 127/1000 | Loss: 0.00042626
Iteration 128/1000 | Loss: 0.00102277
Iteration 129/1000 | Loss: 0.00029100
Iteration 130/1000 | Loss: 0.00015859
Iteration 131/1000 | Loss: 0.00012555
Iteration 132/1000 | Loss: 0.00012310
Iteration 133/1000 | Loss: 0.00034129
Iteration 134/1000 | Loss: 0.00121095
Iteration 135/1000 | Loss: 0.00029554
Iteration 136/1000 | Loss: 0.00058688
Iteration 137/1000 | Loss: 0.00055027
Iteration 138/1000 | Loss: 0.00017623
Iteration 139/1000 | Loss: 0.00013375
Iteration 140/1000 | Loss: 0.00030371
Iteration 141/1000 | Loss: 0.00019999
Iteration 142/1000 | Loss: 0.00030421
Iteration 143/1000 | Loss: 0.00048178
Iteration 144/1000 | Loss: 0.00042883
Iteration 145/1000 | Loss: 0.00047431
Iteration 146/1000 | Loss: 0.00060673
Iteration 147/1000 | Loss: 0.00024606
Iteration 148/1000 | Loss: 0.00014344
Iteration 149/1000 | Loss: 0.00022679
Iteration 150/1000 | Loss: 0.00011957
Iteration 151/1000 | Loss: 0.00011863
Iteration 152/1000 | Loss: 0.00011601
Iteration 153/1000 | Loss: 0.00011675
Iteration 154/1000 | Loss: 0.00011679
Iteration 155/1000 | Loss: 0.00011627
Iteration 156/1000 | Loss: 0.00011665
Iteration 157/1000 | Loss: 0.00060667
Iteration 158/1000 | Loss: 0.00045713
Iteration 159/1000 | Loss: 0.00057479
Iteration 160/1000 | Loss: 0.00026160
Iteration 161/1000 | Loss: 0.00057688
Iteration 162/1000 | Loss: 0.00013469
Iteration 163/1000 | Loss: 0.00132544
Iteration 164/1000 | Loss: 0.00075697
Iteration 165/1000 | Loss: 0.00024796
Iteration 166/1000 | Loss: 0.00011672
Iteration 167/1000 | Loss: 0.00038938
Iteration 168/1000 | Loss: 0.00097114
Iteration 169/1000 | Loss: 0.00085593
Iteration 170/1000 | Loss: 0.00091352
Iteration 171/1000 | Loss: 0.00060748
Iteration 172/1000 | Loss: 0.00062677
Iteration 173/1000 | Loss: 0.00049571
Iteration 174/1000 | Loss: 0.00036473
Iteration 175/1000 | Loss: 0.00021224
Iteration 176/1000 | Loss: 0.00021218
Iteration 177/1000 | Loss: 0.00012287
Iteration 178/1000 | Loss: 0.00011704
Iteration 179/1000 | Loss: 0.00011322
Iteration 180/1000 | Loss: 0.00093623
Iteration 181/1000 | Loss: 0.00012855
Iteration 182/1000 | Loss: 0.00011680
Iteration 183/1000 | Loss: 0.00011273
Iteration 184/1000 | Loss: 0.00010826
Iteration 185/1000 | Loss: 0.00021867
Iteration 186/1000 | Loss: 0.00106884
Iteration 187/1000 | Loss: 0.00020476
Iteration 188/1000 | Loss: 0.00025946
Iteration 189/1000 | Loss: 0.00018130
Iteration 190/1000 | Loss: 0.00020039
Iteration 191/1000 | Loss: 0.00010795
Iteration 192/1000 | Loss: 0.00010634
Iteration 193/1000 | Loss: 0.00010565
Iteration 194/1000 | Loss: 0.00010353
Iteration 195/1000 | Loss: 0.00010222
Iteration 196/1000 | Loss: 0.00010389
Iteration 197/1000 | Loss: 0.00010281
Iteration 198/1000 | Loss: 0.00053213
Iteration 199/1000 | Loss: 0.00017964
Iteration 200/1000 | Loss: 0.00013460
Iteration 201/1000 | Loss: 0.00010711
Iteration 202/1000 | Loss: 0.00010374
Iteration 203/1000 | Loss: 0.00010390
Iteration 204/1000 | Loss: 0.00010323
Iteration 205/1000 | Loss: 0.00010302
Iteration 206/1000 | Loss: 0.00010119
Iteration 207/1000 | Loss: 0.00010222
Iteration 208/1000 | Loss: 0.00010102
Iteration 209/1000 | Loss: 0.00010041
Iteration 210/1000 | Loss: 0.00010040
Iteration 211/1000 | Loss: 0.00010160
Iteration 212/1000 | Loss: 0.00010105
Iteration 213/1000 | Loss: 0.00010124
Iteration 214/1000 | Loss: 0.00010123
Iteration 215/1000 | Loss: 0.00010101
Iteration 216/1000 | Loss: 0.00010110
Iteration 217/1000 | Loss: 0.00010094
Iteration 218/1000 | Loss: 0.00010074
Iteration 219/1000 | Loss: 0.00010100
Iteration 220/1000 | Loss: 0.00010050
Iteration 221/1000 | Loss: 0.00010082
Iteration 222/1000 | Loss: 0.00010068
Iteration 223/1000 | Loss: 0.00010049
Iteration 224/1000 | Loss: 0.00010041
Iteration 225/1000 | Loss: 0.00010048
Iteration 226/1000 | Loss: 0.00010029
Iteration 227/1000 | Loss: 0.00010054
Iteration 228/1000 | Loss: 0.00009901
Iteration 229/1000 | Loss: 0.00009928
Iteration 230/1000 | Loss: 0.00010100
Iteration 231/1000 | Loss: 0.00010037
Iteration 232/1000 | Loss: 0.00010014
Iteration 233/1000 | Loss: 0.00010037
Iteration 234/1000 | Loss: 0.00033387
Iteration 235/1000 | Loss: 0.00220414
Iteration 236/1000 | Loss: 0.00021003
Iteration 237/1000 | Loss: 0.00010710
Iteration 238/1000 | Loss: 0.00010306
Iteration 239/1000 | Loss: 0.00010267
Iteration 240/1000 | Loss: 0.00010046
Iteration 241/1000 | Loss: 0.00010119
Iteration 242/1000 | Loss: 0.00010009
Iteration 243/1000 | Loss: 0.00010006
Iteration 244/1000 | Loss: 0.00010178
Iteration 245/1000 | Loss: 0.00010146
Iteration 246/1000 | Loss: 0.00010056
Iteration 247/1000 | Loss: 0.00010111
Iteration 248/1000 | Loss: 0.00018185
Iteration 249/1000 | Loss: 0.00092111
Iteration 250/1000 | Loss: 0.00029700
Iteration 251/1000 | Loss: 0.00100254
Iteration 252/1000 | Loss: 0.00060550
Iteration 253/1000 | Loss: 0.00035264
Iteration 254/1000 | Loss: 0.00019060
Iteration 255/1000 | Loss: 0.00024034
Iteration 256/1000 | Loss: 0.00011981
Iteration 257/1000 | Loss: 0.00010607
Iteration 258/1000 | Loss: 0.00010138
Iteration 259/1000 | Loss: 0.00009722
Iteration 260/1000 | Loss: 0.00009495
Iteration 261/1000 | Loss: 0.00009341
Iteration 262/1000 | Loss: 0.00015510
Iteration 263/1000 | Loss: 0.00012588
Iteration 264/1000 | Loss: 0.00052820
Iteration 265/1000 | Loss: 0.00049186
Iteration 266/1000 | Loss: 0.00013144
Iteration 267/1000 | Loss: 0.00017174
Iteration 268/1000 | Loss: 0.00011763
Iteration 269/1000 | Loss: 0.00009214
Iteration 270/1000 | Loss: 0.00012480
Iteration 271/1000 | Loss: 0.00015303
Iteration 272/1000 | Loss: 0.00016696
Iteration 273/1000 | Loss: 0.00014641
Iteration 274/1000 | Loss: 0.00012701
Iteration 275/1000 | Loss: 0.00014165
Iteration 276/1000 | Loss: 0.00009873
Iteration 277/1000 | Loss: 0.00009118
Iteration 278/1000 | Loss: 0.00009008
Iteration 279/1000 | Loss: 0.00008913
Iteration 280/1000 | Loss: 0.00008816
Iteration 281/1000 | Loss: 0.00008721
Iteration 282/1000 | Loss: 0.00008616
Iteration 283/1000 | Loss: 0.00008554
Iteration 284/1000 | Loss: 0.00008495
Iteration 285/1000 | Loss: 0.00008465
Iteration 286/1000 | Loss: 0.00008440
Iteration 287/1000 | Loss: 0.00008433
Iteration 288/1000 | Loss: 0.00008400
Iteration 289/1000 | Loss: 0.00008382
Iteration 290/1000 | Loss: 0.00008374
Iteration 291/1000 | Loss: 0.00008363
Iteration 292/1000 | Loss: 0.00008380
Iteration 293/1000 | Loss: 0.00008380
Iteration 294/1000 | Loss: 0.00008379
Iteration 295/1000 | Loss: 0.00008379
Iteration 296/1000 | Loss: 0.00008379
Iteration 297/1000 | Loss: 0.00008359
Iteration 298/1000 | Loss: 0.00008340
Iteration 299/1000 | Loss: 0.00008315
Iteration 300/1000 | Loss: 0.00008311
Iteration 301/1000 | Loss: 0.00008310
Iteration 302/1000 | Loss: 0.00008309
Iteration 303/1000 | Loss: 0.00008303
Iteration 304/1000 | Loss: 0.00008294
Iteration 305/1000 | Loss: 0.00008289
Iteration 306/1000 | Loss: 0.00008303
Iteration 307/1000 | Loss: 0.00008281
Iteration 308/1000 | Loss: 0.00008278
Iteration 309/1000 | Loss: 0.00008278
Iteration 310/1000 | Loss: 0.00008278
Iteration 311/1000 | Loss: 0.00008278
Iteration 312/1000 | Loss: 0.00008278
Iteration 313/1000 | Loss: 0.00008278
Iteration 314/1000 | Loss: 0.00008277
Iteration 315/1000 | Loss: 0.00008277
Iteration 316/1000 | Loss: 0.00008270
Iteration 317/1000 | Loss: 0.00008268
Iteration 318/1000 | Loss: 0.00008265
Iteration 319/1000 | Loss: 0.00008264
Iteration 320/1000 | Loss: 0.00008264
Iteration 321/1000 | Loss: 0.00008255
Iteration 322/1000 | Loss: 0.00008244
Iteration 323/1000 | Loss: 0.00008258
Iteration 324/1000 | Loss: 0.00008254
Iteration 325/1000 | Loss: 0.00008233
Iteration 326/1000 | Loss: 0.00008231
Iteration 327/1000 | Loss: 0.00008231
Iteration 328/1000 | Loss: 0.00008231
Iteration 329/1000 | Loss: 0.00008231
Iteration 330/1000 | Loss: 0.00008231
Iteration 331/1000 | Loss: 0.00008230
Iteration 332/1000 | Loss: 0.00008230
Iteration 333/1000 | Loss: 0.00008230
Iteration 334/1000 | Loss: 0.00008230
Iteration 335/1000 | Loss: 0.00008230
Iteration 336/1000 | Loss: 0.00008230
Iteration 337/1000 | Loss: 0.00008230
Iteration 338/1000 | Loss: 0.00008230
Iteration 339/1000 | Loss: 0.00008230
Iteration 340/1000 | Loss: 0.00008221
Iteration 341/1000 | Loss: 0.00008220
Iteration 342/1000 | Loss: 0.00008219
Iteration 343/1000 | Loss: 0.00008218
Iteration 344/1000 | Loss: 0.00008218
Iteration 345/1000 | Loss: 0.00008217
Iteration 346/1000 | Loss: 0.00008217
Iteration 347/1000 | Loss: 0.00008217
Iteration 348/1000 | Loss: 0.00008217
Iteration 349/1000 | Loss: 0.00008217
Iteration 350/1000 | Loss: 0.00008217
Iteration 351/1000 | Loss: 0.00008217
Iteration 352/1000 | Loss: 0.00008216
Iteration 353/1000 | Loss: 0.00008216
Iteration 354/1000 | Loss: 0.00008216
Iteration 355/1000 | Loss: 0.00008215
Iteration 356/1000 | Loss: 0.00008215
Iteration 357/1000 | Loss: 0.00008215
Iteration 358/1000 | Loss: 0.00008215
Iteration 359/1000 | Loss: 0.00008214
Iteration 360/1000 | Loss: 0.00008214
Iteration 361/1000 | Loss: 0.00008214
Iteration 362/1000 | Loss: 0.00008214
Iteration 363/1000 | Loss: 0.00008213
Iteration 364/1000 | Loss: 0.00008212
Iteration 365/1000 | Loss: 0.00008212
Iteration 366/1000 | Loss: 0.00008212
Iteration 367/1000 | Loss: 0.00008212
Iteration 368/1000 | Loss: 0.00008212
Iteration 369/1000 | Loss: 0.00008212
Iteration 370/1000 | Loss: 0.00008212
Iteration 371/1000 | Loss: 0.00008212
Iteration 372/1000 | Loss: 0.00008211
Iteration 373/1000 | Loss: 0.00008210
Iteration 374/1000 | Loss: 0.00008210
Iteration 375/1000 | Loss: 0.00008210
Iteration 376/1000 | Loss: 0.00008209
Iteration 377/1000 | Loss: 0.00008209
Iteration 378/1000 | Loss: 0.00008208
Iteration 379/1000 | Loss: 0.00008208
Iteration 380/1000 | Loss: 0.00008208
Iteration 381/1000 | Loss: 0.00008207
Iteration 382/1000 | Loss: 0.00008207
Iteration 383/1000 | Loss: 0.00008207
Iteration 384/1000 | Loss: 0.00008207
Iteration 385/1000 | Loss: 0.00008206
Iteration 386/1000 | Loss: 0.00008206
Iteration 387/1000 | Loss: 0.00008206
Iteration 388/1000 | Loss: 0.00008206
Iteration 389/1000 | Loss: 0.00008206
Iteration 390/1000 | Loss: 0.00008205
Iteration 391/1000 | Loss: 0.00008205
Iteration 392/1000 | Loss: 0.00008205
Iteration 393/1000 | Loss: 0.00008205
Iteration 394/1000 | Loss: 0.00008205
Iteration 395/1000 | Loss: 0.00008205
Iteration 396/1000 | Loss: 0.00008205
Iteration 397/1000 | Loss: 0.00008204
Iteration 398/1000 | Loss: 0.00008204
Iteration 399/1000 | Loss: 0.00008204
Iteration 400/1000 | Loss: 0.00008204
Iteration 401/1000 | Loss: 0.00008203
Iteration 402/1000 | Loss: 0.00008203
Iteration 403/1000 | Loss: 0.00008203
Iteration 404/1000 | Loss: 0.00008203
Iteration 405/1000 | Loss: 0.00008202
Iteration 406/1000 | Loss: 0.00008202
Iteration 407/1000 | Loss: 0.00008201
Iteration 408/1000 | Loss: 0.00008201
Iteration 409/1000 | Loss: 0.00008201
Iteration 410/1000 | Loss: 0.00008201
Iteration 411/1000 | Loss: 0.00008201
Iteration 412/1000 | Loss: 0.00008200
Iteration 413/1000 | Loss: 0.00008200
Iteration 414/1000 | Loss: 0.00008200
Iteration 415/1000 | Loss: 0.00008199
Iteration 416/1000 | Loss: 0.00008199
Iteration 417/1000 | Loss: 0.00008199
Iteration 418/1000 | Loss: 0.00008199
Iteration 419/1000 | Loss: 0.00008199
Iteration 420/1000 | Loss: 0.00008198
Iteration 421/1000 | Loss: 0.00008198
Iteration 422/1000 | Loss: 0.00008198
Iteration 423/1000 | Loss: 0.00008198
Iteration 424/1000 | Loss: 0.00008197
Iteration 425/1000 | Loss: 0.00008197
Iteration 426/1000 | Loss: 0.00008196
Iteration 427/1000 | Loss: 0.00008196
Iteration 428/1000 | Loss: 0.00008196
Iteration 429/1000 | Loss: 0.00008196
Iteration 430/1000 | Loss: 0.00008196
Iteration 431/1000 | Loss: 0.00008196
Iteration 432/1000 | Loss: 0.00008196
Iteration 433/1000 | Loss: 0.00008196
Iteration 434/1000 | Loss: 0.00008196
Iteration 435/1000 | Loss: 0.00008196
Iteration 436/1000 | Loss: 0.00008195
Iteration 437/1000 | Loss: 0.00008195
Iteration 438/1000 | Loss: 0.00008195
Iteration 439/1000 | Loss: 0.00008195
Iteration 440/1000 | Loss: 0.00008195
Iteration 441/1000 | Loss: 0.00008194
Iteration 442/1000 | Loss: 0.00008194
Iteration 443/1000 | Loss: 0.00008194
Iteration 444/1000 | Loss: 0.00008194
Iteration 445/1000 | Loss: 0.00008194
Iteration 446/1000 | Loss: 0.00008194
Iteration 447/1000 | Loss: 0.00008194
Iteration 448/1000 | Loss: 0.00008193
Iteration 449/1000 | Loss: 0.00008193
Iteration 450/1000 | Loss: 0.00008193
Iteration 451/1000 | Loss: 0.00008193
Iteration 452/1000 | Loss: 0.00008193
Iteration 453/1000 | Loss: 0.00008193
Iteration 454/1000 | Loss: 0.00008193
Iteration 455/1000 | Loss: 0.00008192
Iteration 456/1000 | Loss: 0.00008192
Iteration 457/1000 | Loss: 0.00008192
Iteration 458/1000 | Loss: 0.00008192
Iteration 459/1000 | Loss: 0.00008192
Iteration 460/1000 | Loss: 0.00008192
Iteration 461/1000 | Loss: 0.00008192
Iteration 462/1000 | Loss: 0.00008192
Iteration 463/1000 | Loss: 0.00008191
Iteration 464/1000 | Loss: 0.00008191
Iteration 465/1000 | Loss: 0.00008204
Iteration 466/1000 | Loss: 0.00008204
Iteration 467/1000 | Loss: 0.00008204
Iteration 468/1000 | Loss: 0.00008204
Iteration 469/1000 | Loss: 0.00008203
Iteration 470/1000 | Loss: 0.00008203
Iteration 471/1000 | Loss: 0.00008201
Iteration 472/1000 | Loss: 0.00008200
Iteration 473/1000 | Loss: 0.00008199
Iteration 474/1000 | Loss: 0.00008199
Iteration 475/1000 | Loss: 0.00008198
Iteration 476/1000 | Loss: 0.00008198
Iteration 477/1000 | Loss: 0.00008198
Iteration 478/1000 | Loss: 0.00008198
Iteration 479/1000 | Loss: 0.00008197
Iteration 480/1000 | Loss: 0.00008197
Iteration 481/1000 | Loss: 0.00008197
Iteration 482/1000 | Loss: 0.00008196
Iteration 483/1000 | Loss: 0.00008196
Iteration 484/1000 | Loss: 0.00008191
Iteration 485/1000 | Loss: 0.00008190
Iteration 486/1000 | Loss: 0.00008187
Iteration 487/1000 | Loss: 0.00008187
Iteration 488/1000 | Loss: 0.00008184
Iteration 489/1000 | Loss: 0.00008184
Iteration 490/1000 | Loss: 0.00008184
Iteration 491/1000 | Loss: 0.00008184
Iteration 492/1000 | Loss: 0.00008184
Iteration 493/1000 | Loss: 0.00008184
Iteration 494/1000 | Loss: 0.00008183
Iteration 495/1000 | Loss: 0.00008183
Iteration 496/1000 | Loss: 0.00008183
Iteration 497/1000 | Loss: 0.00008183
Iteration 498/1000 | Loss: 0.00008182
Iteration 499/1000 | Loss: 0.00008182
Iteration 500/1000 | Loss: 0.00017357
Iteration 501/1000 | Loss: 0.00008879
Iteration 502/1000 | Loss: 0.00008414
Iteration 503/1000 | Loss: 0.00008270
Iteration 504/1000 | Loss: 0.00008131
Iteration 505/1000 | Loss: 0.00008068
Iteration 506/1000 | Loss: 0.00008034
Iteration 507/1000 | Loss: 0.00008010
Iteration 508/1000 | Loss: 0.00008009
Iteration 509/1000 | Loss: 0.00008005
Iteration 510/1000 | Loss: 0.00008004
Iteration 511/1000 | Loss: 0.00008004
Iteration 512/1000 | Loss: 0.00008004
Iteration 513/1000 | Loss: 0.00008003
Iteration 514/1000 | Loss: 0.00008003
Iteration 515/1000 | Loss: 0.00008003
Iteration 516/1000 | Loss: 0.00008003
Iteration 517/1000 | Loss: 0.00008002
Iteration 518/1000 | Loss: 0.00008002
Iteration 519/1000 | Loss: 0.00007999
Iteration 520/1000 | Loss: 0.00007999
Iteration 521/1000 | Loss: 0.00007997
Iteration 522/1000 | Loss: 0.00007997
Iteration 523/1000 | Loss: 0.00007997
Iteration 524/1000 | Loss: 0.00007997
Iteration 525/1000 | Loss: 0.00007997
Iteration 526/1000 | Loss: 0.00007997
Iteration 527/1000 | Loss: 0.00007997
Iteration 528/1000 | Loss: 0.00007997
Iteration 529/1000 | Loss: 0.00007996
Iteration 530/1000 | Loss: 0.00007994
Iteration 531/1000 | Loss: 0.00007994
Iteration 532/1000 | Loss: 0.00007993
Iteration 533/1000 | Loss: 0.00007992
Iteration 534/1000 | Loss: 0.00007992
Iteration 535/1000 | Loss: 0.00007992
Iteration 536/1000 | Loss: 0.00007991
Iteration 537/1000 | Loss: 0.00007991
Iteration 538/1000 | Loss: 0.00007991
Iteration 539/1000 | Loss: 0.00007991
Iteration 540/1000 | Loss: 0.00007991
Iteration 541/1000 | Loss: 0.00007991
Iteration 542/1000 | Loss: 0.00007991
Iteration 543/1000 | Loss: 0.00007991
Iteration 544/1000 | Loss: 0.00007991
Iteration 545/1000 | Loss: 0.00007991
Iteration 546/1000 | Loss: 0.00007991
Iteration 547/1000 | Loss: 0.00007990
Iteration 548/1000 | Loss: 0.00007990
Iteration 549/1000 | Loss: 0.00007990
Iteration 550/1000 | Loss: 0.00007990
Iteration 551/1000 | Loss: 0.00007990
Iteration 552/1000 | Loss: 0.00007990
Iteration 553/1000 | Loss: 0.00007990
Iteration 554/1000 | Loss: 0.00007990
Iteration 555/1000 | Loss: 0.00007990
Iteration 556/1000 | Loss: 0.00007990
Iteration 557/1000 | Loss: 0.00007990
Iteration 558/1000 | Loss: 0.00007990
Iteration 559/1000 | Loss: 0.00007990
Iteration 560/1000 | Loss: 0.00007989
Iteration 561/1000 | Loss: 0.00007989
Iteration 562/1000 | Loss: 0.00007989
Iteration 563/1000 | Loss: 0.00007989
Iteration 564/1000 | Loss: 0.00007989
Iteration 565/1000 | Loss: 0.00007989
Iteration 566/1000 | Loss: 0.00007988
Iteration 567/1000 | Loss: 0.00007988
Iteration 568/1000 | Loss: 0.00007988
Iteration 569/1000 | Loss: 0.00007988
Iteration 570/1000 | Loss: 0.00007988
Iteration 571/1000 | Loss: 0.00007988
Iteration 572/1000 | Loss: 0.00007988
Iteration 573/1000 | Loss: 0.00007988
Iteration 574/1000 | Loss: 0.00007988
Iteration 575/1000 | Loss: 0.00007988
Iteration 576/1000 | Loss: 0.00007988
Iteration 577/1000 | Loss: 0.00007988
Iteration 578/1000 | Loss: 0.00007988
Iteration 579/1000 | Loss: 0.00007988
Iteration 580/1000 | Loss: 0.00007988
Iteration 581/1000 | Loss: 0.00007988
Iteration 582/1000 | Loss: 0.00007988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 582. Stopping optimization.
Last 5 losses: [7.98808760009706e-05, 7.98808760009706e-05, 7.98808760009706e-05, 7.98808760009706e-05, 7.98808760009706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.98808760009706e-05

Optimization complete. Final v2v error: 4.418614864349365 mm

Highest mean error: 14.025739669799805 mm for frame 94

Lowest mean error: 2.4251620769500732 mm for frame 15

Saving results

Total time: 564.6393556594849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00342979
Iteration 2/25 | Loss: 0.00148879
Iteration 3/25 | Loss: 0.00118817
Iteration 4/25 | Loss: 0.00113807
Iteration 5/25 | Loss: 0.00113220
Iteration 6/25 | Loss: 0.00113098
Iteration 7/25 | Loss: 0.00113077
Iteration 8/25 | Loss: 0.00113077
Iteration 9/25 | Loss: 0.00113077
Iteration 10/25 | Loss: 0.00113077
Iteration 11/25 | Loss: 0.00113077
Iteration 12/25 | Loss: 0.00113077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011307700769975781, 0.0011307700769975781, 0.0011307700769975781, 0.0011307700769975781, 0.0011307700769975781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011307700769975781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14513457
Iteration 2/25 | Loss: 0.00395393
Iteration 3/25 | Loss: 0.00395393
Iteration 4/25 | Loss: 0.00395393
Iteration 5/25 | Loss: 0.00395393
Iteration 6/25 | Loss: 0.00395393
Iteration 7/25 | Loss: 0.00395393
Iteration 8/25 | Loss: 0.00395393
Iteration 9/25 | Loss: 0.00395393
Iteration 10/25 | Loss: 0.00395393
Iteration 11/25 | Loss: 0.00395393
Iteration 12/25 | Loss: 0.00395393
Iteration 13/25 | Loss: 0.00395393
Iteration 14/25 | Loss: 0.00395393
Iteration 15/25 | Loss: 0.00395393
Iteration 16/25 | Loss: 0.00395393
Iteration 17/25 | Loss: 0.00395393
Iteration 18/25 | Loss: 0.00395393
Iteration 19/25 | Loss: 0.00395393
Iteration 20/25 | Loss: 0.00395393
Iteration 21/25 | Loss: 0.00395393
Iteration 22/25 | Loss: 0.00395393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0039539276622235775, 0.0039539276622235775, 0.0039539276622235775, 0.0039539276622235775, 0.0039539276622235775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0039539276622235775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395393
Iteration 2/1000 | Loss: 0.00005811
Iteration 3/1000 | Loss: 0.00002791
Iteration 4/1000 | Loss: 0.00001524
Iteration 5/1000 | Loss: 0.00001328
Iteration 6/1000 | Loss: 0.00001219
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001137
Iteration 9/1000 | Loss: 0.00001105
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001038
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001025
Iteration 18/1000 | Loss: 0.00001025
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001022
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001016
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001013
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001010
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001006
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001005
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001003
Iteration 49/1000 | Loss: 0.00001003
Iteration 50/1000 | Loss: 0.00001002
Iteration 51/1000 | Loss: 0.00001002
Iteration 52/1000 | Loss: 0.00001002
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001001
Iteration 58/1000 | Loss: 0.00001001
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001000
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00000999
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000998
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000997
Iteration 73/1000 | Loss: 0.00000997
Iteration 74/1000 | Loss: 0.00000997
Iteration 75/1000 | Loss: 0.00000997
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000996
Iteration 78/1000 | Loss: 0.00000996
Iteration 79/1000 | Loss: 0.00000996
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000995
Iteration 82/1000 | Loss: 0.00000995
Iteration 83/1000 | Loss: 0.00000995
Iteration 84/1000 | Loss: 0.00000995
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000995
Iteration 87/1000 | Loss: 0.00000995
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000994
Iteration 101/1000 | Loss: 0.00000994
Iteration 102/1000 | Loss: 0.00000994
Iteration 103/1000 | Loss: 0.00000993
Iteration 104/1000 | Loss: 0.00000993
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000993
Iteration 107/1000 | Loss: 0.00000993
Iteration 108/1000 | Loss: 0.00000993
Iteration 109/1000 | Loss: 0.00000993
Iteration 110/1000 | Loss: 0.00000993
Iteration 111/1000 | Loss: 0.00000993
Iteration 112/1000 | Loss: 0.00000993
Iteration 113/1000 | Loss: 0.00000993
Iteration 114/1000 | Loss: 0.00000993
Iteration 115/1000 | Loss: 0.00000993
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000993
Iteration 125/1000 | Loss: 0.00000993
Iteration 126/1000 | Loss: 0.00000993
Iteration 127/1000 | Loss: 0.00000993
Iteration 128/1000 | Loss: 0.00000993
Iteration 129/1000 | Loss: 0.00000993
Iteration 130/1000 | Loss: 0.00000993
Iteration 131/1000 | Loss: 0.00000993
Iteration 132/1000 | Loss: 0.00000993
Iteration 133/1000 | Loss: 0.00000993
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [9.926045095198788e-06, 9.926045095198788e-06, 9.926045095198788e-06, 9.926045095198788e-06, 9.926045095198788e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.926045095198788e-06

Optimization complete. Final v2v error: 2.70737624168396 mm

Highest mean error: 3.0175273418426514 mm for frame 30

Lowest mean error: 2.365825891494751 mm for frame 9

Saving results

Total time: 36.517449378967285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378354
Iteration 2/25 | Loss: 0.00153305
Iteration 3/25 | Loss: 0.00120419
Iteration 4/25 | Loss: 0.00116863
Iteration 5/25 | Loss: 0.00116262
Iteration 6/25 | Loss: 0.00116133
Iteration 7/25 | Loss: 0.00116122
Iteration 8/25 | Loss: 0.00116122
Iteration 9/25 | Loss: 0.00116122
Iteration 10/25 | Loss: 0.00116122
Iteration 11/25 | Loss: 0.00116122
Iteration 12/25 | Loss: 0.00116122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011612240923568606, 0.0011612240923568606, 0.0011612240923568606, 0.0011612240923568606, 0.0011612240923568606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011612240923568606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08844125
Iteration 2/25 | Loss: 0.00307879
Iteration 3/25 | Loss: 0.00307879
Iteration 4/25 | Loss: 0.00307879
Iteration 5/25 | Loss: 0.00307878
Iteration 6/25 | Loss: 0.00307878
Iteration 7/25 | Loss: 0.00307878
Iteration 8/25 | Loss: 0.00307878
Iteration 9/25 | Loss: 0.00307878
Iteration 10/25 | Loss: 0.00307878
Iteration 11/25 | Loss: 0.00307878
Iteration 12/25 | Loss: 0.00307878
Iteration 13/25 | Loss: 0.00307878
Iteration 14/25 | Loss: 0.00307878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003078783629462123, 0.003078783629462123, 0.003078783629462123, 0.003078783629462123, 0.003078783629462123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003078783629462123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00307878
Iteration 2/1000 | Loss: 0.00005090
Iteration 3/1000 | Loss: 0.00002953
Iteration 4/1000 | Loss: 0.00001866
Iteration 5/1000 | Loss: 0.00001563
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001207
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001204
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001202
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001202
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001198
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001197
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001197
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001192
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001190
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001184
Iteration 82/1000 | Loss: 0.00001184
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001177
Iteration 121/1000 | Loss: 0.00001177
Iteration 122/1000 | Loss: 0.00001177
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001175
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001175
Iteration 137/1000 | Loss: 0.00001175
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001175
Iteration 142/1000 | Loss: 0.00001175
Iteration 143/1000 | Loss: 0.00001174
Iteration 144/1000 | Loss: 0.00001174
Iteration 145/1000 | Loss: 0.00001174
Iteration 146/1000 | Loss: 0.00001174
Iteration 147/1000 | Loss: 0.00001174
Iteration 148/1000 | Loss: 0.00001174
Iteration 149/1000 | Loss: 0.00001174
Iteration 150/1000 | Loss: 0.00001174
Iteration 151/1000 | Loss: 0.00001174
Iteration 152/1000 | Loss: 0.00001174
Iteration 153/1000 | Loss: 0.00001174
Iteration 154/1000 | Loss: 0.00001174
Iteration 155/1000 | Loss: 0.00001174
Iteration 156/1000 | Loss: 0.00001174
Iteration 157/1000 | Loss: 0.00001174
Iteration 158/1000 | Loss: 0.00001174
Iteration 159/1000 | Loss: 0.00001174
Iteration 160/1000 | Loss: 0.00001174
Iteration 161/1000 | Loss: 0.00001174
Iteration 162/1000 | Loss: 0.00001174
Iteration 163/1000 | Loss: 0.00001174
Iteration 164/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.17397275971598e-05, 1.17397275971598e-05, 1.17397275971598e-05, 1.17397275971598e-05, 1.17397275971598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.17397275971598e-05

Optimization complete. Final v2v error: 2.9459352493286133 mm

Highest mean error: 3.2749624252319336 mm for frame 128

Lowest mean error: 2.5839385986328125 mm for frame 4

Saving results

Total time: 38.492414236068726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397225
Iteration 2/25 | Loss: 0.00117925
Iteration 3/25 | Loss: 0.00110381
Iteration 4/25 | Loss: 0.00109092
Iteration 5/25 | Loss: 0.00108650
Iteration 6/25 | Loss: 0.00108498
Iteration 7/25 | Loss: 0.00108493
Iteration 8/25 | Loss: 0.00108493
Iteration 9/25 | Loss: 0.00108493
Iteration 10/25 | Loss: 0.00108493
Iteration 11/25 | Loss: 0.00108493
Iteration 12/25 | Loss: 0.00108493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010849281679838896, 0.0010849281679838896, 0.0010849281679838896, 0.0010849281679838896, 0.0010849281679838896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010849281679838896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58563888
Iteration 2/25 | Loss: 0.00300452
Iteration 3/25 | Loss: 0.00300452
Iteration 4/25 | Loss: 0.00300451
Iteration 5/25 | Loss: 0.00300451
Iteration 6/25 | Loss: 0.00300451
Iteration 7/25 | Loss: 0.00300451
Iteration 8/25 | Loss: 0.00300451
Iteration 9/25 | Loss: 0.00300451
Iteration 10/25 | Loss: 0.00300451
Iteration 11/25 | Loss: 0.00300451
Iteration 12/25 | Loss: 0.00300451
Iteration 13/25 | Loss: 0.00300451
Iteration 14/25 | Loss: 0.00300451
Iteration 15/25 | Loss: 0.00300451
Iteration 16/25 | Loss: 0.00300451
Iteration 17/25 | Loss: 0.00300451
Iteration 18/25 | Loss: 0.00300451
Iteration 19/25 | Loss: 0.00300451
Iteration 20/25 | Loss: 0.00300451
Iteration 21/25 | Loss: 0.00300451
Iteration 22/25 | Loss: 0.00300451
Iteration 23/25 | Loss: 0.00300451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003004512283951044, 0.003004512283951044, 0.003004512283951044, 0.003004512283951044, 0.003004512283951044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003004512283951044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300451
Iteration 2/1000 | Loss: 0.00003125
Iteration 3/1000 | Loss: 0.00001944
Iteration 4/1000 | Loss: 0.00001623
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001311
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001203
Iteration 12/1000 | Loss: 0.00001199
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001187
Iteration 19/1000 | Loss: 0.00001183
Iteration 20/1000 | Loss: 0.00001182
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001177
Iteration 25/1000 | Loss: 0.00001177
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001171
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001171
Iteration 30/1000 | Loss: 0.00001171
Iteration 31/1000 | Loss: 0.00001171
Iteration 32/1000 | Loss: 0.00001171
Iteration 33/1000 | Loss: 0.00001170
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00001169
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001168
Iteration 40/1000 | Loss: 0.00001168
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001167
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001166
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001161
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001160
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001160
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.1584966159716714e-05, 1.1584966159716714e-05, 1.1584966159716714e-05, 1.1584966159716714e-05, 1.1584966159716714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1584966159716714e-05

Optimization complete. Final v2v error: 2.8534023761749268 mm

Highest mean error: 3.4262325763702393 mm for frame 64

Lowest mean error: 2.5256707668304443 mm for frame 126

Saving results

Total time: 33.15583276748657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056672
Iteration 2/25 | Loss: 0.00180275
Iteration 3/25 | Loss: 0.00144933
Iteration 4/25 | Loss: 0.00134784
Iteration 5/25 | Loss: 0.00137707
Iteration 6/25 | Loss: 0.00132803
Iteration 7/25 | Loss: 0.00139339
Iteration 8/25 | Loss: 0.00122886
Iteration 9/25 | Loss: 0.00120374
Iteration 10/25 | Loss: 0.00120289
Iteration 11/25 | Loss: 0.00120149
Iteration 12/25 | Loss: 0.00119271
Iteration 13/25 | Loss: 0.00119568
Iteration 14/25 | Loss: 0.00119131
Iteration 15/25 | Loss: 0.00119018
Iteration 16/25 | Loss: 0.00118417
Iteration 17/25 | Loss: 0.00118800
Iteration 18/25 | Loss: 0.00118100
Iteration 19/25 | Loss: 0.00118437
Iteration 20/25 | Loss: 0.00118053
Iteration 21/25 | Loss: 0.00117975
Iteration 22/25 | Loss: 0.00117975
Iteration 23/25 | Loss: 0.00118125
Iteration 24/25 | Loss: 0.00119517
Iteration 25/25 | Loss: 0.00119734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20866001
Iteration 2/25 | Loss: 0.00345236
Iteration 3/25 | Loss: 0.00345236
Iteration 4/25 | Loss: 0.00345236
Iteration 5/25 | Loss: 0.00345236
Iteration 6/25 | Loss: 0.00345236
Iteration 7/25 | Loss: 0.00345236
Iteration 8/25 | Loss: 0.00345236
Iteration 9/25 | Loss: 0.00345236
Iteration 10/25 | Loss: 0.00345236
Iteration 11/25 | Loss: 0.00345236
Iteration 12/25 | Loss: 0.00345236
Iteration 13/25 | Loss: 0.00345236
Iteration 14/25 | Loss: 0.00345236
Iteration 15/25 | Loss: 0.00345236
Iteration 16/25 | Loss: 0.00345236
Iteration 17/25 | Loss: 0.00345236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0034523585345596075, 0.0034523585345596075, 0.0034523585345596075, 0.0034523585345596075, 0.0034523585345596075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034523585345596075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345236
Iteration 2/1000 | Loss: 0.00037811
Iteration 3/1000 | Loss: 0.00028037
Iteration 4/1000 | Loss: 0.00055191
Iteration 5/1000 | Loss: 0.00039797
Iteration 6/1000 | Loss: 0.00008647
Iteration 7/1000 | Loss: 0.00012786
Iteration 8/1000 | Loss: 0.00018435
Iteration 9/1000 | Loss: 0.00030696
Iteration 10/1000 | Loss: 0.00007322
Iteration 11/1000 | Loss: 0.00028149
Iteration 12/1000 | Loss: 0.00028320
Iteration 13/1000 | Loss: 0.00020296
Iteration 14/1000 | Loss: 0.00017555
Iteration 15/1000 | Loss: 0.00032477
Iteration 16/1000 | Loss: 0.00018082
Iteration 17/1000 | Loss: 0.00029618
Iteration 18/1000 | Loss: 0.00024440
Iteration 19/1000 | Loss: 0.00016169
Iteration 20/1000 | Loss: 0.00010473
Iteration 21/1000 | Loss: 0.00022971
Iteration 22/1000 | Loss: 0.00026326
Iteration 23/1000 | Loss: 0.00014362
Iteration 24/1000 | Loss: 0.00026483
Iteration 25/1000 | Loss: 0.00012902
Iteration 26/1000 | Loss: 0.00018976
Iteration 27/1000 | Loss: 0.00030127
Iteration 28/1000 | Loss: 0.00021551
Iteration 29/1000 | Loss: 0.00023968
Iteration 30/1000 | Loss: 0.00017869
Iteration 31/1000 | Loss: 0.00010423
Iteration 32/1000 | Loss: 0.00012220
Iteration 33/1000 | Loss: 0.00017813
Iteration 34/1000 | Loss: 0.00013247
Iteration 35/1000 | Loss: 0.00045893
Iteration 36/1000 | Loss: 0.00040539
Iteration 37/1000 | Loss: 0.00041122
Iteration 38/1000 | Loss: 0.00004491
Iteration 39/1000 | Loss: 0.00019222
Iteration 40/1000 | Loss: 0.00014166
Iteration 41/1000 | Loss: 0.00003121
Iteration 42/1000 | Loss: 0.00002404
Iteration 43/1000 | Loss: 0.00004984
Iteration 44/1000 | Loss: 0.00002502
Iteration 45/1000 | Loss: 0.00017444
Iteration 46/1000 | Loss: 0.00002811
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002202
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002031
Iteration 51/1000 | Loss: 0.00020303
Iteration 52/1000 | Loss: 0.00013987
Iteration 53/1000 | Loss: 0.00008485
Iteration 54/1000 | Loss: 0.00004624
Iteration 55/1000 | Loss: 0.00016718
Iteration 56/1000 | Loss: 0.00002426
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00016186
Iteration 61/1000 | Loss: 0.00031880
Iteration 62/1000 | Loss: 0.00003340
Iteration 63/1000 | Loss: 0.00026069
Iteration 64/1000 | Loss: 0.00021687
Iteration 65/1000 | Loss: 0.00020363
Iteration 66/1000 | Loss: 0.00002549
Iteration 67/1000 | Loss: 0.00002330
Iteration 68/1000 | Loss: 0.00030644
Iteration 69/1000 | Loss: 0.00003163
Iteration 70/1000 | Loss: 0.00002438
Iteration 71/1000 | Loss: 0.00023028
Iteration 72/1000 | Loss: 0.00036563
Iteration 73/1000 | Loss: 0.00035374
Iteration 74/1000 | Loss: 0.00009778
Iteration 75/1000 | Loss: 0.00011328
Iteration 76/1000 | Loss: 0.00007154
Iteration 77/1000 | Loss: 0.00005766
Iteration 78/1000 | Loss: 0.00006986
Iteration 79/1000 | Loss: 0.00005582
Iteration 80/1000 | Loss: 0.00014414
Iteration 81/1000 | Loss: 0.00003499
Iteration 82/1000 | Loss: 0.00060794
Iteration 83/1000 | Loss: 0.00027270
Iteration 84/1000 | Loss: 0.00002796
Iteration 85/1000 | Loss: 0.00002463
Iteration 86/1000 | Loss: 0.00004391
Iteration 87/1000 | Loss: 0.00002497
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001353
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001341
Iteration 99/1000 | Loss: 0.00001339
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001336
Iteration 105/1000 | Loss: 0.00001336
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001336
Iteration 108/1000 | Loss: 0.00001336
Iteration 109/1000 | Loss: 0.00001336
Iteration 110/1000 | Loss: 0.00001336
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001336
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001325
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001323
Iteration 147/1000 | Loss: 0.00001323
Iteration 148/1000 | Loss: 0.00001323
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001322
Iteration 152/1000 | Loss: 0.00001322
Iteration 153/1000 | Loss: 0.00001322
Iteration 154/1000 | Loss: 0.00001322
Iteration 155/1000 | Loss: 0.00001322
Iteration 156/1000 | Loss: 0.00001322
Iteration 157/1000 | Loss: 0.00001322
Iteration 158/1000 | Loss: 0.00001322
Iteration 159/1000 | Loss: 0.00001322
Iteration 160/1000 | Loss: 0.00001322
Iteration 161/1000 | Loss: 0.00001322
Iteration 162/1000 | Loss: 0.00001322
Iteration 163/1000 | Loss: 0.00001322
Iteration 164/1000 | Loss: 0.00001322
Iteration 165/1000 | Loss: 0.00001322
Iteration 166/1000 | Loss: 0.00001322
Iteration 167/1000 | Loss: 0.00001322
Iteration 168/1000 | Loss: 0.00001322
Iteration 169/1000 | Loss: 0.00001322
Iteration 170/1000 | Loss: 0.00001322
Iteration 171/1000 | Loss: 0.00001322
Iteration 172/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.3220119399193209e-05, 1.3220119399193209e-05, 1.3220119399193209e-05, 1.3220119399193209e-05, 1.3220119399193209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3220119399193209e-05

Optimization complete. Final v2v error: 2.959620237350464 mm

Highest mean error: 8.459976196289062 mm for frame 125

Lowest mean error: 2.4119017124176025 mm for frame 134

Saving results

Total time: 184.0897514820099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00280086
Iteration 2/25 | Loss: 0.00138039
Iteration 3/25 | Loss: 0.00123853
Iteration 4/25 | Loss: 0.00120240
Iteration 5/25 | Loss: 0.00119167
Iteration 6/25 | Loss: 0.00118861
Iteration 7/25 | Loss: 0.00118778
Iteration 8/25 | Loss: 0.00118772
Iteration 9/25 | Loss: 0.00118772
Iteration 10/25 | Loss: 0.00118772
Iteration 11/25 | Loss: 0.00118772
Iteration 12/25 | Loss: 0.00118772
Iteration 13/25 | Loss: 0.00118772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011877217330038548, 0.0011877217330038548, 0.0011877217330038548, 0.0011877217330038548, 0.0011877217330038548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011877217330038548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10881317
Iteration 2/25 | Loss: 0.00454203
Iteration 3/25 | Loss: 0.00454203
Iteration 4/25 | Loss: 0.00454203
Iteration 5/25 | Loss: 0.00454203
Iteration 6/25 | Loss: 0.00454203
Iteration 7/25 | Loss: 0.00454203
Iteration 8/25 | Loss: 0.00454203
Iteration 9/25 | Loss: 0.00454203
Iteration 10/25 | Loss: 0.00454203
Iteration 11/25 | Loss: 0.00454203
Iteration 12/25 | Loss: 0.00454203
Iteration 13/25 | Loss: 0.00454203
Iteration 14/25 | Loss: 0.00454203
Iteration 15/25 | Loss: 0.00454203
Iteration 16/25 | Loss: 0.00454203
Iteration 17/25 | Loss: 0.00454203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004542028531432152, 0.004542028531432152, 0.004542028531432152, 0.004542028531432152, 0.004542028531432152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004542028531432152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00454203
Iteration 2/1000 | Loss: 0.00007020
Iteration 3/1000 | Loss: 0.00003386
Iteration 4/1000 | Loss: 0.00002012
Iteration 5/1000 | Loss: 0.00001828
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001639
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001584
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001566
Iteration 25/1000 | Loss: 0.00001566
Iteration 26/1000 | Loss: 0.00001565
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001560
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001559
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001559
Iteration 39/1000 | Loss: 0.00001559
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001556
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001555
Iteration 57/1000 | Loss: 0.00001555
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001554
Iteration 68/1000 | Loss: 0.00001554
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001552
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001550
Iteration 95/1000 | Loss: 0.00001550
Iteration 96/1000 | Loss: 0.00001550
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001550
Iteration 100/1000 | Loss: 0.00001550
Iteration 101/1000 | Loss: 0.00001550
Iteration 102/1000 | Loss: 0.00001550
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001547
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001547
Iteration 143/1000 | Loss: 0.00001547
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001547
Iteration 146/1000 | Loss: 0.00001547
Iteration 147/1000 | Loss: 0.00001547
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001547
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001547
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Iteration 162/1000 | Loss: 0.00001547
Iteration 163/1000 | Loss: 0.00001547
Iteration 164/1000 | Loss: 0.00001547
Iteration 165/1000 | Loss: 0.00001547
Iteration 166/1000 | Loss: 0.00001547
Iteration 167/1000 | Loss: 0.00001547
Iteration 168/1000 | Loss: 0.00001547
Iteration 169/1000 | Loss: 0.00001547
Iteration 170/1000 | Loss: 0.00001547
Iteration 171/1000 | Loss: 0.00001547
Iteration 172/1000 | Loss: 0.00001547
Iteration 173/1000 | Loss: 0.00001547
Iteration 174/1000 | Loss: 0.00001547
Iteration 175/1000 | Loss: 0.00001547
Iteration 176/1000 | Loss: 0.00001547
Iteration 177/1000 | Loss: 0.00001547
Iteration 178/1000 | Loss: 0.00001547
Iteration 179/1000 | Loss: 0.00001547
Iteration 180/1000 | Loss: 0.00001547
Iteration 181/1000 | Loss: 0.00001547
Iteration 182/1000 | Loss: 0.00001547
Iteration 183/1000 | Loss: 0.00001547
Iteration 184/1000 | Loss: 0.00001547
Iteration 185/1000 | Loss: 0.00001547
Iteration 186/1000 | Loss: 0.00001547
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001547
Iteration 195/1000 | Loss: 0.00001547
Iteration 196/1000 | Loss: 0.00001547
Iteration 197/1000 | Loss: 0.00001547
Iteration 198/1000 | Loss: 0.00001547
Iteration 199/1000 | Loss: 0.00001547
Iteration 200/1000 | Loss: 0.00001547
Iteration 201/1000 | Loss: 0.00001547
Iteration 202/1000 | Loss: 0.00001547
Iteration 203/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.5468054698430933e-05, 1.5468054698430933e-05, 1.5468054698430933e-05, 1.5468054698430933e-05, 1.5468054698430933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5468054698430933e-05

Optimization complete. Final v2v error: 3.379061698913574 mm

Highest mean error: 3.7174761295318604 mm for frame 116

Lowest mean error: 3.049710750579834 mm for frame 2

Saving results

Total time: 37.09457445144653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541531
Iteration 2/25 | Loss: 0.00120082
Iteration 3/25 | Loss: 0.00111934
Iteration 4/25 | Loss: 0.00110774
Iteration 5/25 | Loss: 0.00110460
Iteration 6/25 | Loss: 0.00110367
Iteration 7/25 | Loss: 0.00110367
Iteration 8/25 | Loss: 0.00110367
Iteration 9/25 | Loss: 0.00110367
Iteration 10/25 | Loss: 0.00110367
Iteration 11/25 | Loss: 0.00110367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001103673712350428, 0.001103673712350428, 0.001103673712350428, 0.001103673712350428, 0.001103673712350428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001103673712350428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50670528
Iteration 2/25 | Loss: 0.00286728
Iteration 3/25 | Loss: 0.00286727
Iteration 4/25 | Loss: 0.00286727
Iteration 5/25 | Loss: 0.00286727
Iteration 6/25 | Loss: 0.00286726
Iteration 7/25 | Loss: 0.00286726
Iteration 8/25 | Loss: 0.00286726
Iteration 9/25 | Loss: 0.00286726
Iteration 10/25 | Loss: 0.00286726
Iteration 11/25 | Loss: 0.00286726
Iteration 12/25 | Loss: 0.00286726
Iteration 13/25 | Loss: 0.00286726
Iteration 14/25 | Loss: 0.00286726
Iteration 15/25 | Loss: 0.00286726
Iteration 16/25 | Loss: 0.00286726
Iteration 17/25 | Loss: 0.00286726
Iteration 18/25 | Loss: 0.00286726
Iteration 19/25 | Loss: 0.00286726
Iteration 20/25 | Loss: 0.00286726
Iteration 21/25 | Loss: 0.00286726
Iteration 22/25 | Loss: 0.00286726
Iteration 23/25 | Loss: 0.00286726
Iteration 24/25 | Loss: 0.00286726
Iteration 25/25 | Loss: 0.00286726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286726
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00001898
Iteration 4/1000 | Loss: 0.00001672
Iteration 5/1000 | Loss: 0.00001547
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001299
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001238
Iteration 19/1000 | Loss: 0.00001236
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001233
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001225
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001209
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001206
Iteration 85/1000 | Loss: 0.00001206
Iteration 86/1000 | Loss: 0.00001206
Iteration 87/1000 | Loss: 0.00001205
Iteration 88/1000 | Loss: 0.00001205
Iteration 89/1000 | Loss: 0.00001205
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001204
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001201
Iteration 117/1000 | Loss: 0.00001201
Iteration 118/1000 | Loss: 0.00001201
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1973676009802148e-05, 1.1973676009802148e-05, 1.1973676009802148e-05, 1.1973676009802148e-05, 1.1973676009802148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1973676009802148e-05

Optimization complete. Final v2v error: 2.947068929672241 mm

Highest mean error: 3.5070455074310303 mm for frame 72

Lowest mean error: 2.5942840576171875 mm for frame 25

Saving results

Total time: 38.477246046066284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847680
Iteration 2/25 | Loss: 0.00122119
Iteration 3/25 | Loss: 0.00111587
Iteration 4/25 | Loss: 0.00110609
Iteration 5/25 | Loss: 0.00110452
Iteration 6/25 | Loss: 0.00110452
Iteration 7/25 | Loss: 0.00110452
Iteration 8/25 | Loss: 0.00110452
Iteration 9/25 | Loss: 0.00110452
Iteration 10/25 | Loss: 0.00110452
Iteration 11/25 | Loss: 0.00110452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011045188875868917, 0.0011045188875868917, 0.0011045188875868917, 0.0011045188875868917, 0.0011045188875868917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011045188875868917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15287066
Iteration 2/25 | Loss: 0.00300144
Iteration 3/25 | Loss: 0.00300144
Iteration 4/25 | Loss: 0.00300144
Iteration 5/25 | Loss: 0.00300144
Iteration 6/25 | Loss: 0.00300144
Iteration 7/25 | Loss: 0.00300144
Iteration 8/25 | Loss: 0.00300144
Iteration 9/25 | Loss: 0.00300144
Iteration 10/25 | Loss: 0.00300144
Iteration 11/25 | Loss: 0.00300144
Iteration 12/25 | Loss: 0.00300144
Iteration 13/25 | Loss: 0.00300144
Iteration 14/25 | Loss: 0.00300144
Iteration 15/25 | Loss: 0.00300144
Iteration 16/25 | Loss: 0.00300144
Iteration 17/25 | Loss: 0.00300144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0030014358926564455, 0.0030014358926564455, 0.0030014358926564455, 0.0030014358926564455, 0.0030014358926564455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030014358926564455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300144
Iteration 2/1000 | Loss: 0.00002971
Iteration 3/1000 | Loss: 0.00001564
Iteration 4/1000 | Loss: 0.00001364
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001191
Iteration 7/1000 | Loss: 0.00001121
Iteration 8/1000 | Loss: 0.00001089
Iteration 9/1000 | Loss: 0.00001057
Iteration 10/1000 | Loss: 0.00001035
Iteration 11/1000 | Loss: 0.00001028
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001028
Iteration 14/1000 | Loss: 0.00001028
Iteration 15/1000 | Loss: 0.00001028
Iteration 16/1000 | Loss: 0.00001028
Iteration 17/1000 | Loss: 0.00001028
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001023
Iteration 21/1000 | Loss: 0.00001021
Iteration 22/1000 | Loss: 0.00001020
Iteration 23/1000 | Loss: 0.00001016
Iteration 24/1000 | Loss: 0.00001016
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001008
Iteration 28/1000 | Loss: 0.00001007
Iteration 29/1000 | Loss: 0.00001003
Iteration 30/1000 | Loss: 0.00001003
Iteration 31/1000 | Loss: 0.00001002
Iteration 32/1000 | Loss: 0.00001001
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00000999
Iteration 35/1000 | Loss: 0.00000998
Iteration 36/1000 | Loss: 0.00000998
Iteration 37/1000 | Loss: 0.00000997
Iteration 38/1000 | Loss: 0.00000997
Iteration 39/1000 | Loss: 0.00000996
Iteration 40/1000 | Loss: 0.00000996
Iteration 41/1000 | Loss: 0.00000995
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000991
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000990
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000990
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000989
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000988
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000987
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000987
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000987
Iteration 87/1000 | Loss: 0.00000987
Iteration 88/1000 | Loss: 0.00000987
Iteration 89/1000 | Loss: 0.00000987
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000986
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000986
Iteration 96/1000 | Loss: 0.00000986
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000985
Iteration 103/1000 | Loss: 0.00000985
Iteration 104/1000 | Loss: 0.00000985
Iteration 105/1000 | Loss: 0.00000985
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000985
Iteration 109/1000 | Loss: 0.00000985
Iteration 110/1000 | Loss: 0.00000985
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [9.852506991592236e-06, 9.852506991592236e-06, 9.852506991592236e-06, 9.852506991592236e-06, 9.852506991592236e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.852506991592236e-06

Optimization complete. Final v2v error: 2.6602048873901367 mm

Highest mean error: 2.8954880237579346 mm for frame 64

Lowest mean error: 2.3034844398498535 mm for frame 10

Saving results

Total time: 30.888431072235107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403235
Iteration 2/25 | Loss: 0.00125742
Iteration 3/25 | Loss: 0.00114872
Iteration 4/25 | Loss: 0.00113771
Iteration 5/25 | Loss: 0.00113482
Iteration 6/25 | Loss: 0.00113443
Iteration 7/25 | Loss: 0.00113443
Iteration 8/25 | Loss: 0.00113443
Iteration 9/25 | Loss: 0.00113443
Iteration 10/25 | Loss: 0.00113443
Iteration 11/25 | Loss: 0.00113443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001134434249252081, 0.001134434249252081, 0.001134434249252081, 0.001134434249252081, 0.001134434249252081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001134434249252081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43892753
Iteration 2/25 | Loss: 0.00295207
Iteration 3/25 | Loss: 0.00295207
Iteration 4/25 | Loss: 0.00295207
Iteration 5/25 | Loss: 0.00295207
Iteration 6/25 | Loss: 0.00295207
Iteration 7/25 | Loss: 0.00295207
Iteration 8/25 | Loss: 0.00295207
Iteration 9/25 | Loss: 0.00295206
Iteration 10/25 | Loss: 0.00295206
Iteration 11/25 | Loss: 0.00295206
Iteration 12/25 | Loss: 0.00295206
Iteration 13/25 | Loss: 0.00295206
Iteration 14/25 | Loss: 0.00295206
Iteration 15/25 | Loss: 0.00295206
Iteration 16/25 | Loss: 0.00295206
Iteration 17/25 | Loss: 0.00295206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002952064387500286, 0.002952064387500286, 0.002952064387500286, 0.002952064387500286, 0.002952064387500286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002952064387500286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295206
Iteration 2/1000 | Loss: 0.00003806
Iteration 3/1000 | Loss: 0.00002097
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001358
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001086
Iteration 10/1000 | Loss: 0.00001035
Iteration 11/1000 | Loss: 0.00001009
Iteration 12/1000 | Loss: 0.00000997
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000974
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000970
Iteration 17/1000 | Loss: 0.00000969
Iteration 18/1000 | Loss: 0.00000967
Iteration 19/1000 | Loss: 0.00000967
Iteration 20/1000 | Loss: 0.00000966
Iteration 21/1000 | Loss: 0.00000966
Iteration 22/1000 | Loss: 0.00000964
Iteration 23/1000 | Loss: 0.00000960
Iteration 24/1000 | Loss: 0.00000957
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000954
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000953
Iteration 30/1000 | Loss: 0.00000953
Iteration 31/1000 | Loss: 0.00000951
Iteration 32/1000 | Loss: 0.00000944
Iteration 33/1000 | Loss: 0.00000941
Iteration 34/1000 | Loss: 0.00000941
Iteration 35/1000 | Loss: 0.00000941
Iteration 36/1000 | Loss: 0.00000941
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000940
Iteration 39/1000 | Loss: 0.00000940
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000939
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000938
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000936
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000936
Iteration 48/1000 | Loss: 0.00000935
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000935
Iteration 51/1000 | Loss: 0.00000935
Iteration 52/1000 | Loss: 0.00000935
Iteration 53/1000 | Loss: 0.00000934
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000934
Iteration 56/1000 | Loss: 0.00000933
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000932
Iteration 59/1000 | Loss: 0.00000932
Iteration 60/1000 | Loss: 0.00000932
Iteration 61/1000 | Loss: 0.00000932
Iteration 62/1000 | Loss: 0.00000932
Iteration 63/1000 | Loss: 0.00000932
Iteration 64/1000 | Loss: 0.00000932
Iteration 65/1000 | Loss: 0.00000932
Iteration 66/1000 | Loss: 0.00000932
Iteration 67/1000 | Loss: 0.00000931
Iteration 68/1000 | Loss: 0.00000931
Iteration 69/1000 | Loss: 0.00000931
Iteration 70/1000 | Loss: 0.00000930
Iteration 71/1000 | Loss: 0.00000930
Iteration 72/1000 | Loss: 0.00000930
Iteration 73/1000 | Loss: 0.00000930
Iteration 74/1000 | Loss: 0.00000930
Iteration 75/1000 | Loss: 0.00000930
Iteration 76/1000 | Loss: 0.00000929
Iteration 77/1000 | Loss: 0.00000929
Iteration 78/1000 | Loss: 0.00000929
Iteration 79/1000 | Loss: 0.00000929
Iteration 80/1000 | Loss: 0.00000929
Iteration 81/1000 | Loss: 0.00000928
Iteration 82/1000 | Loss: 0.00000928
Iteration 83/1000 | Loss: 0.00000928
Iteration 84/1000 | Loss: 0.00000928
Iteration 85/1000 | Loss: 0.00000928
Iteration 86/1000 | Loss: 0.00000928
Iteration 87/1000 | Loss: 0.00000927
Iteration 88/1000 | Loss: 0.00000927
Iteration 89/1000 | Loss: 0.00000927
Iteration 90/1000 | Loss: 0.00000927
Iteration 91/1000 | Loss: 0.00000927
Iteration 92/1000 | Loss: 0.00000926
Iteration 93/1000 | Loss: 0.00000926
Iteration 94/1000 | Loss: 0.00000926
Iteration 95/1000 | Loss: 0.00000926
Iteration 96/1000 | Loss: 0.00000926
Iteration 97/1000 | Loss: 0.00000926
Iteration 98/1000 | Loss: 0.00000926
Iteration 99/1000 | Loss: 0.00000926
Iteration 100/1000 | Loss: 0.00000925
Iteration 101/1000 | Loss: 0.00000925
Iteration 102/1000 | Loss: 0.00000925
Iteration 103/1000 | Loss: 0.00000925
Iteration 104/1000 | Loss: 0.00000925
Iteration 105/1000 | Loss: 0.00000925
Iteration 106/1000 | Loss: 0.00000925
Iteration 107/1000 | Loss: 0.00000925
Iteration 108/1000 | Loss: 0.00000925
Iteration 109/1000 | Loss: 0.00000925
Iteration 110/1000 | Loss: 0.00000925
Iteration 111/1000 | Loss: 0.00000925
Iteration 112/1000 | Loss: 0.00000925
Iteration 113/1000 | Loss: 0.00000925
Iteration 114/1000 | Loss: 0.00000924
Iteration 115/1000 | Loss: 0.00000924
Iteration 116/1000 | Loss: 0.00000924
Iteration 117/1000 | Loss: 0.00000924
Iteration 118/1000 | Loss: 0.00000924
Iteration 119/1000 | Loss: 0.00000924
Iteration 120/1000 | Loss: 0.00000924
Iteration 121/1000 | Loss: 0.00000924
Iteration 122/1000 | Loss: 0.00000924
Iteration 123/1000 | Loss: 0.00000924
Iteration 124/1000 | Loss: 0.00000924
Iteration 125/1000 | Loss: 0.00000924
Iteration 126/1000 | Loss: 0.00000924
Iteration 127/1000 | Loss: 0.00000924
Iteration 128/1000 | Loss: 0.00000924
Iteration 129/1000 | Loss: 0.00000924
Iteration 130/1000 | Loss: 0.00000924
Iteration 131/1000 | Loss: 0.00000924
Iteration 132/1000 | Loss: 0.00000923
Iteration 133/1000 | Loss: 0.00000923
Iteration 134/1000 | Loss: 0.00000923
Iteration 135/1000 | Loss: 0.00000923
Iteration 136/1000 | Loss: 0.00000923
Iteration 137/1000 | Loss: 0.00000923
Iteration 138/1000 | Loss: 0.00000923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [9.233982382284012e-06, 9.233982382284012e-06, 9.233982382284012e-06, 9.233982382284012e-06, 9.233982382284012e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.233982382284012e-06

Optimization complete. Final v2v error: 2.5999364852905273 mm

Highest mean error: 2.8419716358184814 mm for frame 130

Lowest mean error: 2.3368802070617676 mm for frame 141

Saving results

Total time: 42.81216335296631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976137
Iteration 2/25 | Loss: 0.00272725
Iteration 3/25 | Loss: 0.00209848
Iteration 4/25 | Loss: 0.00192187
Iteration 5/25 | Loss: 0.00186659
Iteration 6/25 | Loss: 0.00181005
Iteration 7/25 | Loss: 0.00172557
Iteration 8/25 | Loss: 0.00167023
Iteration 9/25 | Loss: 0.00162028
Iteration 10/25 | Loss: 0.00157804
Iteration 11/25 | Loss: 0.00156276
Iteration 12/25 | Loss: 0.00153943
Iteration 13/25 | Loss: 0.00153402
Iteration 14/25 | Loss: 0.00153404
Iteration 15/25 | Loss: 0.00153207
Iteration 16/25 | Loss: 0.00153007
Iteration 17/25 | Loss: 0.00152862
Iteration 18/25 | Loss: 0.00152877
Iteration 19/25 | Loss: 0.00152842
Iteration 20/25 | Loss: 0.00152815
Iteration 21/25 | Loss: 0.00152722
Iteration 22/25 | Loss: 0.00152672
Iteration 23/25 | Loss: 0.00152774
Iteration 24/25 | Loss: 0.00152591
Iteration 25/25 | Loss: 0.00152578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09520388
Iteration 2/25 | Loss: 0.00631976
Iteration 3/25 | Loss: 0.00631976
Iteration 4/25 | Loss: 0.00631976
Iteration 5/25 | Loss: 0.00631976
Iteration 6/25 | Loss: 0.00631976
Iteration 7/25 | Loss: 0.00631976
Iteration 8/25 | Loss: 0.00631976
Iteration 9/25 | Loss: 0.00631976
Iteration 10/25 | Loss: 0.00631976
Iteration 11/25 | Loss: 0.00631976
Iteration 12/25 | Loss: 0.00631976
Iteration 13/25 | Loss: 0.00631976
Iteration 14/25 | Loss: 0.00631976
Iteration 15/25 | Loss: 0.00631976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.006319760344922543, 0.006319760344922543, 0.006319760344922543, 0.006319760344922543, 0.006319760344922543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006319760344922543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00631976
Iteration 2/1000 | Loss: 0.00050364
Iteration 3/1000 | Loss: 0.00036235
Iteration 4/1000 | Loss: 0.00029622
Iteration 5/1000 | Loss: 0.00026694
Iteration 6/1000 | Loss: 0.00024656
Iteration 7/1000 | Loss: 0.00023315
Iteration 8/1000 | Loss: 0.00024481
Iteration 9/1000 | Loss: 0.00022305
Iteration 10/1000 | Loss: 0.00022251
Iteration 11/1000 | Loss: 0.00021117
Iteration 12/1000 | Loss: 0.00023928
Iteration 13/1000 | Loss: 0.00020610
Iteration 14/1000 | Loss: 0.00020768
Iteration 15/1000 | Loss: 0.00022903
Iteration 16/1000 | Loss: 0.00021148
Iteration 17/1000 | Loss: 0.00019742
Iteration 18/1000 | Loss: 0.00126996
Iteration 19/1000 | Loss: 0.00393288
Iteration 20/1000 | Loss: 0.00370973
Iteration 21/1000 | Loss: 0.00052678
Iteration 22/1000 | Loss: 0.00033496
Iteration 23/1000 | Loss: 0.00026215
Iteration 24/1000 | Loss: 0.00020501
Iteration 25/1000 | Loss: 0.00016305
Iteration 26/1000 | Loss: 0.00013662
Iteration 27/1000 | Loss: 0.00012612
Iteration 28/1000 | Loss: 0.00011421
Iteration 29/1000 | Loss: 0.00010700
Iteration 30/1000 | Loss: 0.00010043
Iteration 31/1000 | Loss: 0.00009677
Iteration 32/1000 | Loss: 0.00009433
Iteration 33/1000 | Loss: 0.00009197
Iteration 34/1000 | Loss: 0.00009010
Iteration 35/1000 | Loss: 0.00008845
Iteration 36/1000 | Loss: 0.00008731
Iteration 37/1000 | Loss: 0.00008647
Iteration 38/1000 | Loss: 0.00008580
Iteration 39/1000 | Loss: 0.00008522
Iteration 40/1000 | Loss: 0.00008482
Iteration 41/1000 | Loss: 0.00008462
Iteration 42/1000 | Loss: 0.00008446
Iteration 43/1000 | Loss: 0.00008444
Iteration 44/1000 | Loss: 0.00008443
Iteration 45/1000 | Loss: 0.00008440
Iteration 46/1000 | Loss: 0.00008438
Iteration 47/1000 | Loss: 0.00008436
Iteration 48/1000 | Loss: 0.00008435
Iteration 49/1000 | Loss: 0.00008435
Iteration 50/1000 | Loss: 0.00008435
Iteration 51/1000 | Loss: 0.00008435
Iteration 52/1000 | Loss: 0.00008435
Iteration 53/1000 | Loss: 0.00008435
Iteration 54/1000 | Loss: 0.00008434
Iteration 55/1000 | Loss: 0.00008434
Iteration 56/1000 | Loss: 0.00008434
Iteration 57/1000 | Loss: 0.00008434
Iteration 58/1000 | Loss: 0.00008434
Iteration 59/1000 | Loss: 0.00008434
Iteration 60/1000 | Loss: 0.00008434
Iteration 61/1000 | Loss: 0.00008434
Iteration 62/1000 | Loss: 0.00008433
Iteration 63/1000 | Loss: 0.00008433
Iteration 64/1000 | Loss: 0.00008432
Iteration 65/1000 | Loss: 0.00008432
Iteration 66/1000 | Loss: 0.00008432
Iteration 67/1000 | Loss: 0.00008431
Iteration 68/1000 | Loss: 0.00008431
Iteration 69/1000 | Loss: 0.00008431
Iteration 70/1000 | Loss: 0.00008431
Iteration 71/1000 | Loss: 0.00008431
Iteration 72/1000 | Loss: 0.00008431
Iteration 73/1000 | Loss: 0.00008430
Iteration 74/1000 | Loss: 0.00008430
Iteration 75/1000 | Loss: 0.00008430
Iteration 76/1000 | Loss: 0.00008430
Iteration 77/1000 | Loss: 0.00008429
Iteration 78/1000 | Loss: 0.00008429
Iteration 79/1000 | Loss: 0.00008428
Iteration 80/1000 | Loss: 0.00008428
Iteration 81/1000 | Loss: 0.00008428
Iteration 82/1000 | Loss: 0.00008427
Iteration 83/1000 | Loss: 0.00008427
Iteration 84/1000 | Loss: 0.00008427
Iteration 85/1000 | Loss: 0.00008427
Iteration 86/1000 | Loss: 0.00008427
Iteration 87/1000 | Loss: 0.00008426
Iteration 88/1000 | Loss: 0.00008426
Iteration 89/1000 | Loss: 0.00008426
Iteration 90/1000 | Loss: 0.00008426
Iteration 91/1000 | Loss: 0.00008425
Iteration 92/1000 | Loss: 0.00008425
Iteration 93/1000 | Loss: 0.00008425
Iteration 94/1000 | Loss: 0.00008424
Iteration 95/1000 | Loss: 0.00008424
Iteration 96/1000 | Loss: 0.00008424
Iteration 97/1000 | Loss: 0.00008423
Iteration 98/1000 | Loss: 0.00008423
Iteration 99/1000 | Loss: 0.00008423
Iteration 100/1000 | Loss: 0.00008422
Iteration 101/1000 | Loss: 0.00008422
Iteration 102/1000 | Loss: 0.00008422
Iteration 103/1000 | Loss: 0.00008422
Iteration 104/1000 | Loss: 0.00008422
Iteration 105/1000 | Loss: 0.00008421
Iteration 106/1000 | Loss: 0.00008421
Iteration 107/1000 | Loss: 0.00008421
Iteration 108/1000 | Loss: 0.00008421
Iteration 109/1000 | Loss: 0.00008421
Iteration 110/1000 | Loss: 0.00008421
Iteration 111/1000 | Loss: 0.00008421
Iteration 112/1000 | Loss: 0.00008420
Iteration 113/1000 | Loss: 0.00008420
Iteration 114/1000 | Loss: 0.00008420
Iteration 115/1000 | Loss: 0.00008420
Iteration 116/1000 | Loss: 0.00008420
Iteration 117/1000 | Loss: 0.00008419
Iteration 118/1000 | Loss: 0.00008419
Iteration 119/1000 | Loss: 0.00008419
Iteration 120/1000 | Loss: 0.00008419
Iteration 121/1000 | Loss: 0.00008418
Iteration 122/1000 | Loss: 0.00008418
Iteration 123/1000 | Loss: 0.00008418
Iteration 124/1000 | Loss: 0.00008417
Iteration 125/1000 | Loss: 0.00008417
Iteration 126/1000 | Loss: 0.00008417
Iteration 127/1000 | Loss: 0.00008416
Iteration 128/1000 | Loss: 0.00008416
Iteration 129/1000 | Loss: 0.00008416
Iteration 130/1000 | Loss: 0.00008416
Iteration 131/1000 | Loss: 0.00008415
Iteration 132/1000 | Loss: 0.00008415
Iteration 133/1000 | Loss: 0.00008415
Iteration 134/1000 | Loss: 0.00008415
Iteration 135/1000 | Loss: 0.00008415
Iteration 136/1000 | Loss: 0.00008414
Iteration 137/1000 | Loss: 0.00008414
Iteration 138/1000 | Loss: 0.00008414
Iteration 139/1000 | Loss: 0.00008414
Iteration 140/1000 | Loss: 0.00008414
Iteration 141/1000 | Loss: 0.00008414
Iteration 142/1000 | Loss: 0.00008414
Iteration 143/1000 | Loss: 0.00008414
Iteration 144/1000 | Loss: 0.00008413
Iteration 145/1000 | Loss: 0.00008413
Iteration 146/1000 | Loss: 0.00008413
Iteration 147/1000 | Loss: 0.00008413
Iteration 148/1000 | Loss: 0.00008413
Iteration 149/1000 | Loss: 0.00008413
Iteration 150/1000 | Loss: 0.00008413
Iteration 151/1000 | Loss: 0.00008413
Iteration 152/1000 | Loss: 0.00008413
Iteration 153/1000 | Loss: 0.00008413
Iteration 154/1000 | Loss: 0.00008413
Iteration 155/1000 | Loss: 0.00008413
Iteration 156/1000 | Loss: 0.00008413
Iteration 157/1000 | Loss: 0.00008413
Iteration 158/1000 | Loss: 0.00008413
Iteration 159/1000 | Loss: 0.00008413
Iteration 160/1000 | Loss: 0.00008412
Iteration 161/1000 | Loss: 0.00008412
Iteration 162/1000 | Loss: 0.00008412
Iteration 163/1000 | Loss: 0.00008412
Iteration 164/1000 | Loss: 0.00008412
Iteration 165/1000 | Loss: 0.00008412
Iteration 166/1000 | Loss: 0.00008412
Iteration 167/1000 | Loss: 0.00008411
Iteration 168/1000 | Loss: 0.00008411
Iteration 169/1000 | Loss: 0.00008411
Iteration 170/1000 | Loss: 0.00008411
Iteration 171/1000 | Loss: 0.00008411
Iteration 172/1000 | Loss: 0.00008411
Iteration 173/1000 | Loss: 0.00008411
Iteration 174/1000 | Loss: 0.00008411
Iteration 175/1000 | Loss: 0.00008411
Iteration 176/1000 | Loss: 0.00008411
Iteration 177/1000 | Loss: 0.00008411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [8.411236922256649e-05, 8.411236922256649e-05, 8.411236922256649e-05, 8.411236922256649e-05, 8.411236922256649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.411236922256649e-05

Optimization complete. Final v2v error: 4.82127571105957 mm

Highest mean error: 11.316251754760742 mm for frame 95

Lowest mean error: 2.950127124786377 mm for frame 134

Saving results

Total time: 112.47088479995728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036875
Iteration 2/25 | Loss: 0.00313140
Iteration 3/25 | Loss: 0.00178663
Iteration 4/25 | Loss: 0.00156557
Iteration 5/25 | Loss: 0.00163984
Iteration 6/25 | Loss: 0.00163896
Iteration 7/25 | Loss: 0.00142589
Iteration 8/25 | Loss: 0.00138275
Iteration 9/25 | Loss: 0.00135361
Iteration 10/25 | Loss: 0.00147062
Iteration 11/25 | Loss: 0.00141610
Iteration 12/25 | Loss: 0.00137175
Iteration 13/25 | Loss: 0.00131358
Iteration 14/25 | Loss: 0.00132547
Iteration 15/25 | Loss: 0.00132613
Iteration 16/25 | Loss: 0.00134409
Iteration 17/25 | Loss: 0.00131622
Iteration 18/25 | Loss: 0.00131626
Iteration 19/25 | Loss: 0.00131252
Iteration 20/25 | Loss: 0.00131001
Iteration 21/25 | Loss: 0.00130690
Iteration 22/25 | Loss: 0.00130040
Iteration 23/25 | Loss: 0.00129431
Iteration 24/25 | Loss: 0.00129099
Iteration 25/25 | Loss: 0.00127379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74881393
Iteration 2/25 | Loss: 0.00211323
Iteration 3/25 | Loss: 0.00211322
Iteration 4/25 | Loss: 0.00211322
Iteration 5/25 | Loss: 0.00211322
Iteration 6/25 | Loss: 0.00211322
Iteration 7/25 | Loss: 0.00211322
Iteration 8/25 | Loss: 0.00211322
Iteration 9/25 | Loss: 0.00211322
Iteration 10/25 | Loss: 0.00211322
Iteration 11/25 | Loss: 0.00211322
Iteration 12/25 | Loss: 0.00211322
Iteration 13/25 | Loss: 0.00211322
Iteration 14/25 | Loss: 0.00211322
Iteration 15/25 | Loss: 0.00211322
Iteration 16/25 | Loss: 0.00211322
Iteration 17/25 | Loss: 0.00211322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021132156252861023, 0.0021132156252861023, 0.0021132156252861023, 0.0021132156252861023, 0.0021132156252861023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021132156252861023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211322
Iteration 2/1000 | Loss: 0.00039319
Iteration 3/1000 | Loss: 0.00108007
Iteration 4/1000 | Loss: 0.00146837
Iteration 5/1000 | Loss: 0.00137082
Iteration 6/1000 | Loss: 0.00034850
Iteration 7/1000 | Loss: 0.00056704
Iteration 8/1000 | Loss: 0.00073660
Iteration 9/1000 | Loss: 0.00067828
Iteration 10/1000 | Loss: 0.00036830
Iteration 11/1000 | Loss: 0.00059431
Iteration 12/1000 | Loss: 0.00059885
Iteration 13/1000 | Loss: 0.00028583
Iteration 14/1000 | Loss: 0.00078751
Iteration 15/1000 | Loss: 0.00070290
Iteration 16/1000 | Loss: 0.00079255
Iteration 17/1000 | Loss: 0.00060945
Iteration 18/1000 | Loss: 0.00052031
Iteration 19/1000 | Loss: 0.00020160
Iteration 20/1000 | Loss: 0.00072994
Iteration 21/1000 | Loss: 0.00029839
Iteration 22/1000 | Loss: 0.00045575
Iteration 23/1000 | Loss: 0.00016044
Iteration 24/1000 | Loss: 0.00019473
Iteration 25/1000 | Loss: 0.00019448
Iteration 26/1000 | Loss: 0.00020175
Iteration 27/1000 | Loss: 0.00020514
Iteration 28/1000 | Loss: 0.00015921
Iteration 29/1000 | Loss: 0.00017444
Iteration 30/1000 | Loss: 0.00012534
Iteration 31/1000 | Loss: 0.00014191
Iteration 32/1000 | Loss: 0.00014914
Iteration 33/1000 | Loss: 0.00017624
Iteration 34/1000 | Loss: 0.00017826
Iteration 35/1000 | Loss: 0.00012585
Iteration 36/1000 | Loss: 0.00014329
Iteration 37/1000 | Loss: 0.00058768
Iteration 38/1000 | Loss: 0.00065850
Iteration 39/1000 | Loss: 0.00091381
Iteration 40/1000 | Loss: 0.00016339
Iteration 41/1000 | Loss: 0.00019590
Iteration 42/1000 | Loss: 0.00020356
Iteration 43/1000 | Loss: 0.00080832
Iteration 44/1000 | Loss: 0.00093957
Iteration 45/1000 | Loss: 0.00019511
Iteration 46/1000 | Loss: 0.00021484
Iteration 47/1000 | Loss: 0.00016120
Iteration 48/1000 | Loss: 0.00017439
Iteration 49/1000 | Loss: 0.00017485
Iteration 50/1000 | Loss: 0.00021723
Iteration 51/1000 | Loss: 0.00018503
Iteration 52/1000 | Loss: 0.00017361
Iteration 53/1000 | Loss: 0.00016888
Iteration 54/1000 | Loss: 0.00017579
Iteration 55/1000 | Loss: 0.00019310
Iteration 56/1000 | Loss: 0.00017475
Iteration 57/1000 | Loss: 0.00019724
Iteration 58/1000 | Loss: 0.00013165
Iteration 59/1000 | Loss: 0.00017178
Iteration 60/1000 | Loss: 0.00015001
Iteration 61/1000 | Loss: 0.00020243
Iteration 62/1000 | Loss: 0.00018758
Iteration 63/1000 | Loss: 0.00020147
Iteration 64/1000 | Loss: 0.00017619
Iteration 65/1000 | Loss: 0.00019730
Iteration 66/1000 | Loss: 0.00017450
Iteration 67/1000 | Loss: 0.00019812
Iteration 68/1000 | Loss: 0.00017195
Iteration 69/1000 | Loss: 0.00017865
Iteration 70/1000 | Loss: 0.00020771
Iteration 71/1000 | Loss: 0.00021688
Iteration 72/1000 | Loss: 0.00019202
Iteration 73/1000 | Loss: 0.00033310
Iteration 74/1000 | Loss: 0.00016666
Iteration 75/1000 | Loss: 0.00014883
Iteration 76/1000 | Loss: 0.00011825
Iteration 77/1000 | Loss: 0.00016659
Iteration 78/1000 | Loss: 0.00018323
Iteration 79/1000 | Loss: 0.00012798
Iteration 80/1000 | Loss: 0.00017014
Iteration 81/1000 | Loss: 0.00034904
Iteration 82/1000 | Loss: 0.00014232
Iteration 83/1000 | Loss: 0.00014138
Iteration 84/1000 | Loss: 0.00015789
Iteration 85/1000 | Loss: 0.00018136
Iteration 86/1000 | Loss: 0.00016790
Iteration 87/1000 | Loss: 0.00036029
Iteration 88/1000 | Loss: 0.00017338
Iteration 89/1000 | Loss: 0.00016284
Iteration 90/1000 | Loss: 0.00011833
Iteration 91/1000 | Loss: 0.00015500
Iteration 92/1000 | Loss: 0.00017241
Iteration 93/1000 | Loss: 0.00017342
Iteration 94/1000 | Loss: 0.00011341
Iteration 95/1000 | Loss: 0.00014175
Iteration 96/1000 | Loss: 0.00014662
Iteration 97/1000 | Loss: 0.00014723
Iteration 98/1000 | Loss: 0.00012742
Iteration 99/1000 | Loss: 0.00011079
Iteration 100/1000 | Loss: 0.00021147
Iteration 101/1000 | Loss: 0.00016315
Iteration 102/1000 | Loss: 0.00012098
Iteration 103/1000 | Loss: 0.00011207
Iteration 104/1000 | Loss: 0.00019624
Iteration 105/1000 | Loss: 0.00015026
Iteration 106/1000 | Loss: 0.00010254
Iteration 107/1000 | Loss: 0.00010101
Iteration 108/1000 | Loss: 0.00013295
Iteration 109/1000 | Loss: 0.00014790
Iteration 110/1000 | Loss: 0.00015851
Iteration 111/1000 | Loss: 0.00014344
Iteration 112/1000 | Loss: 0.00017098
Iteration 113/1000 | Loss: 0.00013375
Iteration 114/1000 | Loss: 0.00016397
Iteration 115/1000 | Loss: 0.00013623
Iteration 116/1000 | Loss: 0.00014520
Iteration 117/1000 | Loss: 0.00013406
Iteration 118/1000 | Loss: 0.00013676
Iteration 119/1000 | Loss: 0.00011518
Iteration 120/1000 | Loss: 0.00011123
Iteration 121/1000 | Loss: 0.00017605
Iteration 122/1000 | Loss: 0.00017376
Iteration 123/1000 | Loss: 0.00015154
Iteration 124/1000 | Loss: 0.00013232
Iteration 125/1000 | Loss: 0.00015671
Iteration 126/1000 | Loss: 0.00014110
Iteration 127/1000 | Loss: 0.00013992
Iteration 128/1000 | Loss: 0.00015281
Iteration 129/1000 | Loss: 0.00013724
Iteration 130/1000 | Loss: 0.00014138
Iteration 131/1000 | Loss: 0.00007347
Iteration 132/1000 | Loss: 0.00012487
Iteration 133/1000 | Loss: 0.00013890
Iteration 134/1000 | Loss: 0.00014353
Iteration 135/1000 | Loss: 0.00012408
Iteration 136/1000 | Loss: 0.00010027
Iteration 137/1000 | Loss: 0.00010385
Iteration 138/1000 | Loss: 0.00013035
Iteration 139/1000 | Loss: 0.00010066
Iteration 140/1000 | Loss: 0.00010945
Iteration 141/1000 | Loss: 0.00012212
Iteration 142/1000 | Loss: 0.00012160
Iteration 143/1000 | Loss: 0.00012274
Iteration 144/1000 | Loss: 0.00012574
Iteration 145/1000 | Loss: 0.00012133
Iteration 146/1000 | Loss: 0.00008490
Iteration 147/1000 | Loss: 0.00007882
Iteration 148/1000 | Loss: 0.00010724
Iteration 149/1000 | Loss: 0.00011445
Iteration 150/1000 | Loss: 0.00009993
Iteration 151/1000 | Loss: 0.00009750
Iteration 152/1000 | Loss: 0.00011038
Iteration 153/1000 | Loss: 0.00011072
Iteration 154/1000 | Loss: 0.00009058
Iteration 155/1000 | Loss: 0.00009350
Iteration 156/1000 | Loss: 0.00007604
Iteration 157/1000 | Loss: 0.00005278
Iteration 158/1000 | Loss: 0.00005997
Iteration 159/1000 | Loss: 0.00005934
Iteration 160/1000 | Loss: 0.00007010
Iteration 161/1000 | Loss: 0.00005706
Iteration 162/1000 | Loss: 0.00006047
Iteration 163/1000 | Loss: 0.00006497
Iteration 164/1000 | Loss: 0.00007527
Iteration 165/1000 | Loss: 0.00007939
Iteration 166/1000 | Loss: 0.00007937
Iteration 167/1000 | Loss: 0.00007352
Iteration 168/1000 | Loss: 0.00007443
Iteration 169/1000 | Loss: 0.00009055
Iteration 170/1000 | Loss: 0.00006410
Iteration 171/1000 | Loss: 0.00005521
Iteration 172/1000 | Loss: 0.00005819
Iteration 173/1000 | Loss: 0.00006511
Iteration 174/1000 | Loss: 0.00007359
Iteration 175/1000 | Loss: 0.00007075
Iteration 176/1000 | Loss: 0.00006660
Iteration 177/1000 | Loss: 0.00004971
Iteration 178/1000 | Loss: 0.00007097
Iteration 179/1000 | Loss: 0.00007550
Iteration 180/1000 | Loss: 0.00006741
Iteration 181/1000 | Loss: 0.00005973
Iteration 182/1000 | Loss: 0.00005340
Iteration 183/1000 | Loss: 0.00006245
Iteration 184/1000 | Loss: 0.00006566
Iteration 185/1000 | Loss: 0.00005803
Iteration 186/1000 | Loss: 0.00005952
Iteration 187/1000 | Loss: 0.00006006
Iteration 188/1000 | Loss: 0.00006653
Iteration 189/1000 | Loss: 0.00004933
Iteration 190/1000 | Loss: 0.00004219
Iteration 191/1000 | Loss: 0.00006271
Iteration 192/1000 | Loss: 0.00006077
Iteration 193/1000 | Loss: 0.00005194
Iteration 194/1000 | Loss: 0.00004890
Iteration 195/1000 | Loss: 0.00007415
Iteration 196/1000 | Loss: 0.00005399
Iteration 197/1000 | Loss: 0.00006600
Iteration 198/1000 | Loss: 0.00004944
Iteration 199/1000 | Loss: 0.00006806
Iteration 200/1000 | Loss: 0.00006316
Iteration 201/1000 | Loss: 0.00005493
Iteration 202/1000 | Loss: 0.00003904
Iteration 203/1000 | Loss: 0.00003953
Iteration 204/1000 | Loss: 0.00003708
Iteration 205/1000 | Loss: 0.00003380
Iteration 206/1000 | Loss: 0.00005587
Iteration 207/1000 | Loss: 0.00005700
Iteration 208/1000 | Loss: 0.00003730
Iteration 209/1000 | Loss: 0.00003199
Iteration 210/1000 | Loss: 0.00002960
Iteration 211/1000 | Loss: 0.00002790
Iteration 212/1000 | Loss: 0.00002707
Iteration 213/1000 | Loss: 0.00002631
Iteration 214/1000 | Loss: 0.00002586
Iteration 215/1000 | Loss: 0.00002555
Iteration 216/1000 | Loss: 0.00002537
Iteration 217/1000 | Loss: 0.00002528
Iteration 218/1000 | Loss: 0.00002526
Iteration 219/1000 | Loss: 0.00002524
Iteration 220/1000 | Loss: 0.00002523
Iteration 221/1000 | Loss: 0.00002520
Iteration 222/1000 | Loss: 0.00002520
Iteration 223/1000 | Loss: 0.00002515
Iteration 224/1000 | Loss: 0.00002514
Iteration 225/1000 | Loss: 0.00002514
Iteration 226/1000 | Loss: 0.00002513
Iteration 227/1000 | Loss: 0.00002512
Iteration 228/1000 | Loss: 0.00002512
Iteration 229/1000 | Loss: 0.00002511
Iteration 230/1000 | Loss: 0.00002511
Iteration 231/1000 | Loss: 0.00002510
Iteration 232/1000 | Loss: 0.00002510
Iteration 233/1000 | Loss: 0.00002510
Iteration 234/1000 | Loss: 0.00002510
Iteration 235/1000 | Loss: 0.00002510
Iteration 236/1000 | Loss: 0.00002509
Iteration 237/1000 | Loss: 0.00002509
Iteration 238/1000 | Loss: 0.00002509
Iteration 239/1000 | Loss: 0.00002509
Iteration 240/1000 | Loss: 0.00002508
Iteration 241/1000 | Loss: 0.00002508
Iteration 242/1000 | Loss: 0.00002508
Iteration 243/1000 | Loss: 0.00002508
Iteration 244/1000 | Loss: 0.00002508
Iteration 245/1000 | Loss: 0.00002508
Iteration 246/1000 | Loss: 0.00002508
Iteration 247/1000 | Loss: 0.00002508
Iteration 248/1000 | Loss: 0.00002508
Iteration 249/1000 | Loss: 0.00002508
Iteration 250/1000 | Loss: 0.00002508
Iteration 251/1000 | Loss: 0.00002508
Iteration 252/1000 | Loss: 0.00002508
Iteration 253/1000 | Loss: 0.00002507
Iteration 254/1000 | Loss: 0.00002507
Iteration 255/1000 | Loss: 0.00002507
Iteration 256/1000 | Loss: 0.00002507
Iteration 257/1000 | Loss: 0.00002507
Iteration 258/1000 | Loss: 0.00002507
Iteration 259/1000 | Loss: 0.00002507
Iteration 260/1000 | Loss: 0.00002507
Iteration 261/1000 | Loss: 0.00002507
Iteration 262/1000 | Loss: 0.00002506
Iteration 263/1000 | Loss: 0.00002506
Iteration 264/1000 | Loss: 0.00002506
Iteration 265/1000 | Loss: 0.00002506
Iteration 266/1000 | Loss: 0.00002506
Iteration 267/1000 | Loss: 0.00002506
Iteration 268/1000 | Loss: 0.00002506
Iteration 269/1000 | Loss: 0.00002506
Iteration 270/1000 | Loss: 0.00002505
Iteration 271/1000 | Loss: 0.00002505
Iteration 272/1000 | Loss: 0.00002505
Iteration 273/1000 | Loss: 0.00002505
Iteration 274/1000 | Loss: 0.00002504
Iteration 275/1000 | Loss: 0.00002504
Iteration 276/1000 | Loss: 0.00002504
Iteration 277/1000 | Loss: 0.00002503
Iteration 278/1000 | Loss: 0.00002503
Iteration 279/1000 | Loss: 0.00002503
Iteration 280/1000 | Loss: 0.00002503
Iteration 281/1000 | Loss: 0.00002503
Iteration 282/1000 | Loss: 0.00002503
Iteration 283/1000 | Loss: 0.00002503
Iteration 284/1000 | Loss: 0.00002503
Iteration 285/1000 | Loss: 0.00002503
Iteration 286/1000 | Loss: 0.00002503
Iteration 287/1000 | Loss: 0.00002503
Iteration 288/1000 | Loss: 0.00002503
Iteration 289/1000 | Loss: 0.00002503
Iteration 290/1000 | Loss: 0.00002502
Iteration 291/1000 | Loss: 0.00002502
Iteration 292/1000 | Loss: 0.00002502
Iteration 293/1000 | Loss: 0.00002502
Iteration 294/1000 | Loss: 0.00002501
Iteration 295/1000 | Loss: 0.00002501
Iteration 296/1000 | Loss: 0.00002501
Iteration 297/1000 | Loss: 0.00002501
Iteration 298/1000 | Loss: 0.00002501
Iteration 299/1000 | Loss: 0.00002501
Iteration 300/1000 | Loss: 0.00002501
Iteration 301/1000 | Loss: 0.00002501
Iteration 302/1000 | Loss: 0.00002501
Iteration 303/1000 | Loss: 0.00002500
Iteration 304/1000 | Loss: 0.00002500
Iteration 305/1000 | Loss: 0.00002500
Iteration 306/1000 | Loss: 0.00002500
Iteration 307/1000 | Loss: 0.00002500
Iteration 308/1000 | Loss: 0.00002500
Iteration 309/1000 | Loss: 0.00002500
Iteration 310/1000 | Loss: 0.00002500
Iteration 311/1000 | Loss: 0.00002500
Iteration 312/1000 | Loss: 0.00002500
Iteration 313/1000 | Loss: 0.00002500
Iteration 314/1000 | Loss: 0.00002500
Iteration 315/1000 | Loss: 0.00002500
Iteration 316/1000 | Loss: 0.00002500
Iteration 317/1000 | Loss: 0.00002500
Iteration 318/1000 | Loss: 0.00002500
Iteration 319/1000 | Loss: 0.00002500
Iteration 320/1000 | Loss: 0.00002500
Iteration 321/1000 | Loss: 0.00002500
Iteration 322/1000 | Loss: 0.00002500
Iteration 323/1000 | Loss: 0.00002500
Iteration 324/1000 | Loss: 0.00002500
Iteration 325/1000 | Loss: 0.00002500
Iteration 326/1000 | Loss: 0.00002500
Iteration 327/1000 | Loss: 0.00002500
Iteration 328/1000 | Loss: 0.00002500
Iteration 329/1000 | Loss: 0.00002500
Iteration 330/1000 | Loss: 0.00002500
Iteration 331/1000 | Loss: 0.00002500
Iteration 332/1000 | Loss: 0.00002500
Iteration 333/1000 | Loss: 0.00002500
Iteration 334/1000 | Loss: 0.00002500
Iteration 335/1000 | Loss: 0.00002500
Iteration 336/1000 | Loss: 0.00002500
Iteration 337/1000 | Loss: 0.00002500
Iteration 338/1000 | Loss: 0.00002500
Iteration 339/1000 | Loss: 0.00002500
Iteration 340/1000 | Loss: 0.00002500
Iteration 341/1000 | Loss: 0.00002500
Iteration 342/1000 | Loss: 0.00002500
Iteration 343/1000 | Loss: 0.00002500
Iteration 344/1000 | Loss: 0.00002500
Iteration 345/1000 | Loss: 0.00002500
Iteration 346/1000 | Loss: 0.00002500
Iteration 347/1000 | Loss: 0.00002500
Iteration 348/1000 | Loss: 0.00002500
Iteration 349/1000 | Loss: 0.00002500
Iteration 350/1000 | Loss: 0.00002500
Iteration 351/1000 | Loss: 0.00002500
Iteration 352/1000 | Loss: 0.00002500
Iteration 353/1000 | Loss: 0.00002500
Iteration 354/1000 | Loss: 0.00002500
Iteration 355/1000 | Loss: 0.00002500
Iteration 356/1000 | Loss: 0.00002500
Iteration 357/1000 | Loss: 0.00002500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [2.4995913918246515e-05, 2.4995913918246515e-05, 2.4995913918246515e-05, 2.4995913918246515e-05, 2.4995913918246515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4995913918246515e-05

Optimization complete. Final v2v error: 3.9704208374023438 mm

Highest mean error: 5.752967834472656 mm for frame 18

Lowest mean error: 3.7581372261047363 mm for frame 33

Saving results

Total time: 405.80880331993103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947948
Iteration 2/25 | Loss: 0.00128201
Iteration 3/25 | Loss: 0.00115554
Iteration 4/25 | Loss: 0.00112633
Iteration 5/25 | Loss: 0.00111426
Iteration 6/25 | Loss: 0.00111030
Iteration 7/25 | Loss: 0.00110961
Iteration 8/25 | Loss: 0.00110961
Iteration 9/25 | Loss: 0.00110961
Iteration 10/25 | Loss: 0.00110961
Iteration 11/25 | Loss: 0.00110961
Iteration 12/25 | Loss: 0.00110961
Iteration 13/25 | Loss: 0.00110961
Iteration 14/25 | Loss: 0.00110961
Iteration 15/25 | Loss: 0.00110961
Iteration 16/25 | Loss: 0.00110961
Iteration 17/25 | Loss: 0.00110961
Iteration 18/25 | Loss: 0.00110961
Iteration 19/25 | Loss: 0.00110961
Iteration 20/25 | Loss: 0.00110961
Iteration 21/25 | Loss: 0.00110961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011096058879047632, 0.0011096058879047632, 0.0011096058879047632, 0.0011096058879047632, 0.0011096058879047632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011096058879047632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20830214
Iteration 2/25 | Loss: 0.00273987
Iteration 3/25 | Loss: 0.00273987
Iteration 4/25 | Loss: 0.00273987
Iteration 5/25 | Loss: 0.00273987
Iteration 6/25 | Loss: 0.00273987
Iteration 7/25 | Loss: 0.00273986
Iteration 8/25 | Loss: 0.00273986
Iteration 9/25 | Loss: 0.00273986
Iteration 10/25 | Loss: 0.00273986
Iteration 11/25 | Loss: 0.00273986
Iteration 12/25 | Loss: 0.00273986
Iteration 13/25 | Loss: 0.00273986
Iteration 14/25 | Loss: 0.00273986
Iteration 15/25 | Loss: 0.00273986
Iteration 16/25 | Loss: 0.00273986
Iteration 17/25 | Loss: 0.00273986
Iteration 18/25 | Loss: 0.00273986
Iteration 19/25 | Loss: 0.00273986
Iteration 20/25 | Loss: 0.00273986
Iteration 21/25 | Loss: 0.00273986
Iteration 22/25 | Loss: 0.00273986
Iteration 23/25 | Loss: 0.00273986
Iteration 24/25 | Loss: 0.00273986
Iteration 25/25 | Loss: 0.00273986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273986
Iteration 2/1000 | Loss: 0.00005098
Iteration 3/1000 | Loss: 0.00003368
Iteration 4/1000 | Loss: 0.00002552
Iteration 5/1000 | Loss: 0.00002328
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00001976
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001915
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001901
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001885
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001879
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001876
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001872
Iteration 30/1000 | Loss: 0.00001872
Iteration 31/1000 | Loss: 0.00001871
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001871
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001869
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001869
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001868
Iteration 51/1000 | Loss: 0.00001868
Iteration 52/1000 | Loss: 0.00001868
Iteration 53/1000 | Loss: 0.00001868
Iteration 54/1000 | Loss: 0.00001868
Iteration 55/1000 | Loss: 0.00001868
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001868
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001867
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001867
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001867
Iteration 74/1000 | Loss: 0.00001867
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001864
Iteration 94/1000 | Loss: 0.00001864
Iteration 95/1000 | Loss: 0.00001864
Iteration 96/1000 | Loss: 0.00001864
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001863
Iteration 102/1000 | Loss: 0.00001863
Iteration 103/1000 | Loss: 0.00001863
Iteration 104/1000 | Loss: 0.00001863
Iteration 105/1000 | Loss: 0.00001863
Iteration 106/1000 | Loss: 0.00001863
Iteration 107/1000 | Loss: 0.00001863
Iteration 108/1000 | Loss: 0.00001863
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001863
Iteration 113/1000 | Loss: 0.00001863
Iteration 114/1000 | Loss: 0.00001863
Iteration 115/1000 | Loss: 0.00001863
Iteration 116/1000 | Loss: 0.00001863
Iteration 117/1000 | Loss: 0.00001862
Iteration 118/1000 | Loss: 0.00001862
Iteration 119/1000 | Loss: 0.00001862
Iteration 120/1000 | Loss: 0.00001862
Iteration 121/1000 | Loss: 0.00001862
Iteration 122/1000 | Loss: 0.00001862
Iteration 123/1000 | Loss: 0.00001862
Iteration 124/1000 | Loss: 0.00001862
Iteration 125/1000 | Loss: 0.00001862
Iteration 126/1000 | Loss: 0.00001862
Iteration 127/1000 | Loss: 0.00001862
Iteration 128/1000 | Loss: 0.00001862
Iteration 129/1000 | Loss: 0.00001862
Iteration 130/1000 | Loss: 0.00001862
Iteration 131/1000 | Loss: 0.00001862
Iteration 132/1000 | Loss: 0.00001862
Iteration 133/1000 | Loss: 0.00001862
Iteration 134/1000 | Loss: 0.00001862
Iteration 135/1000 | Loss: 0.00001862
Iteration 136/1000 | Loss: 0.00001862
Iteration 137/1000 | Loss: 0.00001862
Iteration 138/1000 | Loss: 0.00001862
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001861
Iteration 143/1000 | Loss: 0.00001861
Iteration 144/1000 | Loss: 0.00001861
Iteration 145/1000 | Loss: 0.00001861
Iteration 146/1000 | Loss: 0.00001861
Iteration 147/1000 | Loss: 0.00001861
Iteration 148/1000 | Loss: 0.00001861
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001861
Iteration 152/1000 | Loss: 0.00001861
Iteration 153/1000 | Loss: 0.00001861
Iteration 154/1000 | Loss: 0.00001861
Iteration 155/1000 | Loss: 0.00001861
Iteration 156/1000 | Loss: 0.00001861
Iteration 157/1000 | Loss: 0.00001861
Iteration 158/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.8614695363794453e-05, 1.8614695363794453e-05, 1.8614695363794453e-05, 1.8614695363794453e-05, 1.8614695363794453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8614695363794453e-05

Optimization complete. Final v2v error: 3.4436116218566895 mm

Highest mean error: 5.882420063018799 mm for frame 70

Lowest mean error: 2.5766115188598633 mm for frame 0

Saving results

Total time: 36.89110255241394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855583
Iteration 2/25 | Loss: 0.00123436
Iteration 3/25 | Loss: 0.00111854
Iteration 4/25 | Loss: 0.00110561
Iteration 5/25 | Loss: 0.00110394
Iteration 6/25 | Loss: 0.00110394
Iteration 7/25 | Loss: 0.00110394
Iteration 8/25 | Loss: 0.00110394
Iteration 9/25 | Loss: 0.00110394
Iteration 10/25 | Loss: 0.00110394
Iteration 11/25 | Loss: 0.00110394
Iteration 12/25 | Loss: 0.00110394
Iteration 13/25 | Loss: 0.00110394
Iteration 14/25 | Loss: 0.00110394
Iteration 15/25 | Loss: 0.00110394
Iteration 16/25 | Loss: 0.00110394
Iteration 17/25 | Loss: 0.00110394
Iteration 18/25 | Loss: 0.00110394
Iteration 19/25 | Loss: 0.00110394
Iteration 20/25 | Loss: 0.00110394
Iteration 21/25 | Loss: 0.00110394
Iteration 22/25 | Loss: 0.00110394
Iteration 23/25 | Loss: 0.00110394
Iteration 24/25 | Loss: 0.00110394
Iteration 25/25 | Loss: 0.00110394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14709985
Iteration 2/25 | Loss: 0.00308251
Iteration 3/25 | Loss: 0.00308251
Iteration 4/25 | Loss: 0.00308251
Iteration 5/25 | Loss: 0.00308251
Iteration 6/25 | Loss: 0.00308251
Iteration 7/25 | Loss: 0.00308251
Iteration 8/25 | Loss: 0.00308251
Iteration 9/25 | Loss: 0.00308251
Iteration 10/25 | Loss: 0.00308251
Iteration 11/25 | Loss: 0.00308251
Iteration 12/25 | Loss: 0.00308251
Iteration 13/25 | Loss: 0.00308251
Iteration 14/25 | Loss: 0.00308251
Iteration 15/25 | Loss: 0.00308251
Iteration 16/25 | Loss: 0.00308251
Iteration 17/25 | Loss: 0.00308251
Iteration 18/25 | Loss: 0.00308251
Iteration 19/25 | Loss: 0.00308251
Iteration 20/25 | Loss: 0.00308251
Iteration 21/25 | Loss: 0.00308251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00308250542730093, 0.00308250542730093, 0.00308250542730093, 0.00308250542730093, 0.00308250542730093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00308250542730093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308251
Iteration 2/1000 | Loss: 0.00002813
Iteration 3/1000 | Loss: 0.00001640
Iteration 4/1000 | Loss: 0.00001420
Iteration 5/1000 | Loss: 0.00001276
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001114
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001045
Iteration 10/1000 | Loss: 0.00001008
Iteration 11/1000 | Loss: 0.00001006
Iteration 12/1000 | Loss: 0.00001005
Iteration 13/1000 | Loss: 0.00001000
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000984
Iteration 16/1000 | Loss: 0.00000980
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000980
Iteration 20/1000 | Loss: 0.00000980
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000980
Iteration 23/1000 | Loss: 0.00000980
Iteration 24/1000 | Loss: 0.00000979
Iteration 25/1000 | Loss: 0.00000979
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000978
Iteration 28/1000 | Loss: 0.00000977
Iteration 29/1000 | Loss: 0.00000975
Iteration 30/1000 | Loss: 0.00000974
Iteration 31/1000 | Loss: 0.00000974
Iteration 32/1000 | Loss: 0.00000973
Iteration 33/1000 | Loss: 0.00000968
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000966
Iteration 38/1000 | Loss: 0.00000966
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000962
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000961
Iteration 44/1000 | Loss: 0.00000961
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000960
Iteration 47/1000 | Loss: 0.00000959
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000957
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000957
Iteration 58/1000 | Loss: 0.00000957
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00000956
Iteration 61/1000 | Loss: 0.00000955
Iteration 62/1000 | Loss: 0.00000955
Iteration 63/1000 | Loss: 0.00000955
Iteration 64/1000 | Loss: 0.00000955
Iteration 65/1000 | Loss: 0.00000955
Iteration 66/1000 | Loss: 0.00000955
Iteration 67/1000 | Loss: 0.00000955
Iteration 68/1000 | Loss: 0.00000955
Iteration 69/1000 | Loss: 0.00000954
Iteration 70/1000 | Loss: 0.00000954
Iteration 71/1000 | Loss: 0.00000953
Iteration 72/1000 | Loss: 0.00000953
Iteration 73/1000 | Loss: 0.00000953
Iteration 74/1000 | Loss: 0.00000953
Iteration 75/1000 | Loss: 0.00000953
Iteration 76/1000 | Loss: 0.00000953
Iteration 77/1000 | Loss: 0.00000953
Iteration 78/1000 | Loss: 0.00000952
Iteration 79/1000 | Loss: 0.00000952
Iteration 80/1000 | Loss: 0.00000952
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000952
Iteration 83/1000 | Loss: 0.00000952
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000952
Iteration 86/1000 | Loss: 0.00000952
Iteration 87/1000 | Loss: 0.00000952
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000951
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000951
Iteration 97/1000 | Loss: 0.00000951
Iteration 98/1000 | Loss: 0.00000951
Iteration 99/1000 | Loss: 0.00000951
Iteration 100/1000 | Loss: 0.00000951
Iteration 101/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [9.509726623946335e-06, 9.509726623946335e-06, 9.509726623946335e-06, 9.509726623946335e-06, 9.509726623946335e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.509726623946335e-06

Optimization complete. Final v2v error: 2.6042673587799072 mm

Highest mean error: 2.832326889038086 mm for frame 25

Lowest mean error: 2.3202569484710693 mm for frame 53

Saving results

Total time: 34.607271909713745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396829
Iteration 2/25 | Loss: 0.00120959
Iteration 3/25 | Loss: 0.00111593
Iteration 4/25 | Loss: 0.00110354
Iteration 5/25 | Loss: 0.00109980
Iteration 6/25 | Loss: 0.00109838
Iteration 7/25 | Loss: 0.00109831
Iteration 8/25 | Loss: 0.00109831
Iteration 9/25 | Loss: 0.00109831
Iteration 10/25 | Loss: 0.00109831
Iteration 11/25 | Loss: 0.00109831
Iteration 12/25 | Loss: 0.00109831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010983141837641597, 0.0010983141837641597, 0.0010983141837641597, 0.0010983141837641597, 0.0010983141837641597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010983141837641597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37233472
Iteration 2/25 | Loss: 0.00301426
Iteration 3/25 | Loss: 0.00301426
Iteration 4/25 | Loss: 0.00301426
Iteration 5/25 | Loss: 0.00301426
Iteration 6/25 | Loss: 0.00301426
Iteration 7/25 | Loss: 0.00301426
Iteration 8/25 | Loss: 0.00301426
Iteration 9/25 | Loss: 0.00301426
Iteration 10/25 | Loss: 0.00301426
Iteration 11/25 | Loss: 0.00301426
Iteration 12/25 | Loss: 0.00301425
Iteration 13/25 | Loss: 0.00301425
Iteration 14/25 | Loss: 0.00301425
Iteration 15/25 | Loss: 0.00301425
Iteration 16/25 | Loss: 0.00301425
Iteration 17/25 | Loss: 0.00301425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0030142548494040966, 0.0030142548494040966, 0.0030142548494040966, 0.0030142548494040966, 0.0030142548494040966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030142548494040966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301425
Iteration 2/1000 | Loss: 0.00002744
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001226
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001126
Iteration 14/1000 | Loss: 0.00001124
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001113
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001103
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001103
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001101
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001100
Iteration 36/1000 | Loss: 0.00001100
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001099
Iteration 39/1000 | Loss: 0.00001099
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001098
Iteration 42/1000 | Loss: 0.00001098
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001098
Iteration 47/1000 | Loss: 0.00001098
Iteration 48/1000 | Loss: 0.00001097
Iteration 49/1000 | Loss: 0.00001097
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001093
Iteration 69/1000 | Loss: 0.00001093
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001092
Iteration 82/1000 | Loss: 0.00001092
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001091
Iteration 97/1000 | Loss: 0.00001091
Iteration 98/1000 | Loss: 0.00001091
Iteration 99/1000 | Loss: 0.00001091
Iteration 100/1000 | Loss: 0.00001091
Iteration 101/1000 | Loss: 0.00001091
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001091
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001091
Iteration 116/1000 | Loss: 0.00001091
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.0907957403105684e-05, 1.0907957403105684e-05, 1.0907957403105684e-05, 1.0907957403105684e-05, 1.0907957403105684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0907957403105684e-05

Optimization complete. Final v2v error: 2.8277387619018555 mm

Highest mean error: 3.2725465297698975 mm for frame 21

Lowest mean error: 2.4279439449310303 mm for frame 132

Saving results

Total time: 31.974356651306152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463961
Iteration 2/25 | Loss: 0.00138900
Iteration 3/25 | Loss: 0.00120211
Iteration 4/25 | Loss: 0.00118099
Iteration 5/25 | Loss: 0.00118012
Iteration 6/25 | Loss: 0.00118012
Iteration 7/25 | Loss: 0.00118012
Iteration 8/25 | Loss: 0.00118012
Iteration 9/25 | Loss: 0.00118012
Iteration 10/25 | Loss: 0.00118012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011801180662587285, 0.0011801180662587285, 0.0011801180662587285, 0.0011801180662587285, 0.0011801180662587285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011801180662587285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15282667
Iteration 2/25 | Loss: 0.00263907
Iteration 3/25 | Loss: 0.00263906
Iteration 4/25 | Loss: 0.00263906
Iteration 5/25 | Loss: 0.00263906
Iteration 6/25 | Loss: 0.00263906
Iteration 7/25 | Loss: 0.00263905
Iteration 8/25 | Loss: 0.00263905
Iteration 9/25 | Loss: 0.00263905
Iteration 10/25 | Loss: 0.00263905
Iteration 11/25 | Loss: 0.00263905
Iteration 12/25 | Loss: 0.00263905
Iteration 13/25 | Loss: 0.00263905
Iteration 14/25 | Loss: 0.00263905
Iteration 15/25 | Loss: 0.00263905
Iteration 16/25 | Loss: 0.00263905
Iteration 17/25 | Loss: 0.00263905
Iteration 18/25 | Loss: 0.00263905
Iteration 19/25 | Loss: 0.00263905
Iteration 20/25 | Loss: 0.00263905
Iteration 21/25 | Loss: 0.00263905
Iteration 22/25 | Loss: 0.00263905
Iteration 23/25 | Loss: 0.00263905
Iteration 24/25 | Loss: 0.00263905
Iteration 25/25 | Loss: 0.00263905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263905
Iteration 2/1000 | Loss: 0.00003611
Iteration 3/1000 | Loss: 0.00002365
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00002003
Iteration 6/1000 | Loss: 0.00001898
Iteration 7/1000 | Loss: 0.00001837
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001703
Iteration 15/1000 | Loss: 0.00001699
Iteration 16/1000 | Loss: 0.00001697
Iteration 17/1000 | Loss: 0.00001693
Iteration 18/1000 | Loss: 0.00001691
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001659
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001657
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001656
Iteration 37/1000 | Loss: 0.00001656
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001655
Iteration 44/1000 | Loss: 0.00001655
Iteration 45/1000 | Loss: 0.00001655
Iteration 46/1000 | Loss: 0.00001655
Iteration 47/1000 | Loss: 0.00001654
Iteration 48/1000 | Loss: 0.00001654
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001653
Iteration 51/1000 | Loss: 0.00001653
Iteration 52/1000 | Loss: 0.00001653
Iteration 53/1000 | Loss: 0.00001653
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001652
Iteration 56/1000 | Loss: 0.00001652
Iteration 57/1000 | Loss: 0.00001652
Iteration 58/1000 | Loss: 0.00001652
Iteration 59/1000 | Loss: 0.00001651
Iteration 60/1000 | Loss: 0.00001651
Iteration 61/1000 | Loss: 0.00001651
Iteration 62/1000 | Loss: 0.00001650
Iteration 63/1000 | Loss: 0.00001650
Iteration 64/1000 | Loss: 0.00001650
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001649
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001648
Iteration 73/1000 | Loss: 0.00001648
Iteration 74/1000 | Loss: 0.00001648
Iteration 75/1000 | Loss: 0.00001648
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001647
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001646
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001644
Iteration 104/1000 | Loss: 0.00001644
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001642
Iteration 112/1000 | Loss: 0.00001642
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001641
Iteration 115/1000 | Loss: 0.00001641
Iteration 116/1000 | Loss: 0.00001641
Iteration 117/1000 | Loss: 0.00001641
Iteration 118/1000 | Loss: 0.00001641
Iteration 119/1000 | Loss: 0.00001641
Iteration 120/1000 | Loss: 0.00001641
Iteration 121/1000 | Loss: 0.00001641
Iteration 122/1000 | Loss: 0.00001641
Iteration 123/1000 | Loss: 0.00001641
Iteration 124/1000 | Loss: 0.00001641
Iteration 125/1000 | Loss: 0.00001640
Iteration 126/1000 | Loss: 0.00001640
Iteration 127/1000 | Loss: 0.00001640
Iteration 128/1000 | Loss: 0.00001639
Iteration 129/1000 | Loss: 0.00001639
Iteration 130/1000 | Loss: 0.00001639
Iteration 131/1000 | Loss: 0.00001638
Iteration 132/1000 | Loss: 0.00001638
Iteration 133/1000 | Loss: 0.00001638
Iteration 134/1000 | Loss: 0.00001638
Iteration 135/1000 | Loss: 0.00001637
Iteration 136/1000 | Loss: 0.00001637
Iteration 137/1000 | Loss: 0.00001637
Iteration 138/1000 | Loss: 0.00001637
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001636
Iteration 141/1000 | Loss: 0.00001636
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001634
Iteration 149/1000 | Loss: 0.00001634
Iteration 150/1000 | Loss: 0.00001634
Iteration 151/1000 | Loss: 0.00001634
Iteration 152/1000 | Loss: 0.00001634
Iteration 153/1000 | Loss: 0.00001634
Iteration 154/1000 | Loss: 0.00001634
Iteration 155/1000 | Loss: 0.00001634
Iteration 156/1000 | Loss: 0.00001634
Iteration 157/1000 | Loss: 0.00001634
Iteration 158/1000 | Loss: 0.00001633
Iteration 159/1000 | Loss: 0.00001633
Iteration 160/1000 | Loss: 0.00001633
Iteration 161/1000 | Loss: 0.00001633
Iteration 162/1000 | Loss: 0.00001633
Iteration 163/1000 | Loss: 0.00001633
Iteration 164/1000 | Loss: 0.00001633
Iteration 165/1000 | Loss: 0.00001632
Iteration 166/1000 | Loss: 0.00001632
Iteration 167/1000 | Loss: 0.00001632
Iteration 168/1000 | Loss: 0.00001632
Iteration 169/1000 | Loss: 0.00001632
Iteration 170/1000 | Loss: 0.00001632
Iteration 171/1000 | Loss: 0.00001632
Iteration 172/1000 | Loss: 0.00001632
Iteration 173/1000 | Loss: 0.00001632
Iteration 174/1000 | Loss: 0.00001632
Iteration 175/1000 | Loss: 0.00001631
Iteration 176/1000 | Loss: 0.00001631
Iteration 177/1000 | Loss: 0.00001631
Iteration 178/1000 | Loss: 0.00001631
Iteration 179/1000 | Loss: 0.00001631
Iteration 180/1000 | Loss: 0.00001631
Iteration 181/1000 | Loss: 0.00001631
Iteration 182/1000 | Loss: 0.00001630
Iteration 183/1000 | Loss: 0.00001630
Iteration 184/1000 | Loss: 0.00001630
Iteration 185/1000 | Loss: 0.00001630
Iteration 186/1000 | Loss: 0.00001630
Iteration 187/1000 | Loss: 0.00001630
Iteration 188/1000 | Loss: 0.00001630
Iteration 189/1000 | Loss: 0.00001629
Iteration 190/1000 | Loss: 0.00001629
Iteration 191/1000 | Loss: 0.00001629
Iteration 192/1000 | Loss: 0.00001629
Iteration 193/1000 | Loss: 0.00001629
Iteration 194/1000 | Loss: 0.00001629
Iteration 195/1000 | Loss: 0.00001629
Iteration 196/1000 | Loss: 0.00001629
Iteration 197/1000 | Loss: 0.00001629
Iteration 198/1000 | Loss: 0.00001629
Iteration 199/1000 | Loss: 0.00001629
Iteration 200/1000 | Loss: 0.00001629
Iteration 201/1000 | Loss: 0.00001629
Iteration 202/1000 | Loss: 0.00001629
Iteration 203/1000 | Loss: 0.00001629
Iteration 204/1000 | Loss: 0.00001629
Iteration 205/1000 | Loss: 0.00001628
Iteration 206/1000 | Loss: 0.00001628
Iteration 207/1000 | Loss: 0.00001628
Iteration 208/1000 | Loss: 0.00001628
Iteration 209/1000 | Loss: 0.00001628
Iteration 210/1000 | Loss: 0.00001628
Iteration 211/1000 | Loss: 0.00001628
Iteration 212/1000 | Loss: 0.00001628
Iteration 213/1000 | Loss: 0.00001628
Iteration 214/1000 | Loss: 0.00001628
Iteration 215/1000 | Loss: 0.00001628
Iteration 216/1000 | Loss: 0.00001628
Iteration 217/1000 | Loss: 0.00001628
Iteration 218/1000 | Loss: 0.00001628
Iteration 219/1000 | Loss: 0.00001628
Iteration 220/1000 | Loss: 0.00001628
Iteration 221/1000 | Loss: 0.00001628
Iteration 222/1000 | Loss: 0.00001628
Iteration 223/1000 | Loss: 0.00001627
Iteration 224/1000 | Loss: 0.00001627
Iteration 225/1000 | Loss: 0.00001627
Iteration 226/1000 | Loss: 0.00001627
Iteration 227/1000 | Loss: 0.00001627
Iteration 228/1000 | Loss: 0.00001627
Iteration 229/1000 | Loss: 0.00001627
Iteration 230/1000 | Loss: 0.00001627
Iteration 231/1000 | Loss: 0.00001627
Iteration 232/1000 | Loss: 0.00001627
Iteration 233/1000 | Loss: 0.00001627
Iteration 234/1000 | Loss: 0.00001627
Iteration 235/1000 | Loss: 0.00001627
Iteration 236/1000 | Loss: 0.00001627
Iteration 237/1000 | Loss: 0.00001627
Iteration 238/1000 | Loss: 0.00001627
Iteration 239/1000 | Loss: 0.00001627
Iteration 240/1000 | Loss: 0.00001627
Iteration 241/1000 | Loss: 0.00001627
Iteration 242/1000 | Loss: 0.00001627
Iteration 243/1000 | Loss: 0.00001626
Iteration 244/1000 | Loss: 0.00001626
Iteration 245/1000 | Loss: 0.00001626
Iteration 246/1000 | Loss: 0.00001626
Iteration 247/1000 | Loss: 0.00001626
Iteration 248/1000 | Loss: 0.00001626
Iteration 249/1000 | Loss: 0.00001626
Iteration 250/1000 | Loss: 0.00001626
Iteration 251/1000 | Loss: 0.00001626
Iteration 252/1000 | Loss: 0.00001626
Iteration 253/1000 | Loss: 0.00001626
Iteration 254/1000 | Loss: 0.00001626
Iteration 255/1000 | Loss: 0.00001626
Iteration 256/1000 | Loss: 0.00001626
Iteration 257/1000 | Loss: 0.00001626
Iteration 258/1000 | Loss: 0.00001626
Iteration 259/1000 | Loss: 0.00001626
Iteration 260/1000 | Loss: 0.00001626
Iteration 261/1000 | Loss: 0.00001626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.6259327821899205e-05, 1.6259327821899205e-05, 1.6259327821899205e-05, 1.6259327821899205e-05, 1.6259327821899205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6259327821899205e-05

Optimization complete. Final v2v error: 3.402177095413208 mm

Highest mean error: 3.7906205654144287 mm for frame 200

Lowest mean error: 2.8913021087646484 mm for frame 78

Saving results

Total time: 48.55012655258179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077055
Iteration 2/25 | Loss: 0.00234638
Iteration 3/25 | Loss: 0.00173816
Iteration 4/25 | Loss: 0.00166126
Iteration 5/25 | Loss: 0.00151420
Iteration 6/25 | Loss: 0.00145734
Iteration 7/25 | Loss: 0.00142371
Iteration 8/25 | Loss: 0.00139146
Iteration 9/25 | Loss: 0.00138031
Iteration 10/25 | Loss: 0.00136166
Iteration 11/25 | Loss: 0.00135721
Iteration 12/25 | Loss: 0.00135188
Iteration 13/25 | Loss: 0.00135283
Iteration 14/25 | Loss: 0.00135653
Iteration 15/25 | Loss: 0.00134569
Iteration 16/25 | Loss: 0.00133792
Iteration 17/25 | Loss: 0.00133654
Iteration 18/25 | Loss: 0.00134098
Iteration 19/25 | Loss: 0.00133229
Iteration 20/25 | Loss: 0.00133175
Iteration 21/25 | Loss: 0.00133163
Iteration 22/25 | Loss: 0.00133159
Iteration 23/25 | Loss: 0.00133159
Iteration 24/25 | Loss: 0.00133159
Iteration 25/25 | Loss: 0.00133159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12911832
Iteration 2/25 | Loss: 0.00485075
Iteration 3/25 | Loss: 0.00466221
Iteration 4/25 | Loss: 0.00466221
Iteration 5/25 | Loss: 0.00466221
Iteration 6/25 | Loss: 0.00466221
Iteration 7/25 | Loss: 0.00466221
Iteration 8/25 | Loss: 0.00466221
Iteration 9/25 | Loss: 0.00466221
Iteration 10/25 | Loss: 0.00466221
Iteration 11/25 | Loss: 0.00466221
Iteration 12/25 | Loss: 0.00466221
Iteration 13/25 | Loss: 0.00466221
Iteration 14/25 | Loss: 0.00466221
Iteration 15/25 | Loss: 0.00466221
Iteration 16/25 | Loss: 0.00466221
Iteration 17/25 | Loss: 0.00466221
Iteration 18/25 | Loss: 0.00466221
Iteration 19/25 | Loss: 0.00466221
Iteration 20/25 | Loss: 0.00466221
Iteration 21/25 | Loss: 0.00466221
Iteration 22/25 | Loss: 0.00466221
Iteration 23/25 | Loss: 0.00466221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0046622068621218204, 0.0046622068621218204, 0.0046622068621218204, 0.0046622068621218204, 0.0046622068621218204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0046622068621218204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00466221
Iteration 2/1000 | Loss: 0.00072832
Iteration 3/1000 | Loss: 0.00142956
Iteration 4/1000 | Loss: 0.00107458
Iteration 5/1000 | Loss: 0.00042425
Iteration 6/1000 | Loss: 0.00019909
Iteration 7/1000 | Loss: 0.00025291
Iteration 8/1000 | Loss: 0.00014284
Iteration 9/1000 | Loss: 0.00027491
Iteration 10/1000 | Loss: 0.00028244
Iteration 11/1000 | Loss: 0.00013947
Iteration 12/1000 | Loss: 0.00013990
Iteration 13/1000 | Loss: 0.00081305
Iteration 14/1000 | Loss: 0.00053982
Iteration 15/1000 | Loss: 0.00084107
Iteration 16/1000 | Loss: 0.00011483
Iteration 17/1000 | Loss: 0.00014343
Iteration 18/1000 | Loss: 0.00009465
Iteration 19/1000 | Loss: 0.00014949
Iteration 20/1000 | Loss: 0.00046491
Iteration 21/1000 | Loss: 0.00020009
Iteration 22/1000 | Loss: 0.00011000
Iteration 23/1000 | Loss: 0.00014856
Iteration 24/1000 | Loss: 0.00038400
Iteration 25/1000 | Loss: 0.00012311
Iteration 26/1000 | Loss: 0.00017245
Iteration 27/1000 | Loss: 0.00027192
Iteration 28/1000 | Loss: 0.00009746
Iteration 29/1000 | Loss: 0.00008016
Iteration 30/1000 | Loss: 0.00007901
Iteration 31/1000 | Loss: 0.00013677
Iteration 32/1000 | Loss: 0.00020781
Iteration 33/1000 | Loss: 0.00022316
Iteration 34/1000 | Loss: 0.00008310
Iteration 35/1000 | Loss: 0.00006484
Iteration 36/1000 | Loss: 0.00006188
Iteration 37/1000 | Loss: 0.00012849
Iteration 38/1000 | Loss: 0.00008398
Iteration 39/1000 | Loss: 0.00005892
Iteration 40/1000 | Loss: 0.00006743
Iteration 41/1000 | Loss: 0.00005511
Iteration 42/1000 | Loss: 0.00011506
Iteration 43/1000 | Loss: 0.00005820
Iteration 44/1000 | Loss: 0.00007044
Iteration 45/1000 | Loss: 0.00005328
Iteration 46/1000 | Loss: 0.00005299
Iteration 47/1000 | Loss: 0.00009514
Iteration 48/1000 | Loss: 0.00005289
Iteration 49/1000 | Loss: 0.00005254
Iteration 50/1000 | Loss: 0.00005237
Iteration 51/1000 | Loss: 0.00005231
Iteration 52/1000 | Loss: 0.00005231
Iteration 53/1000 | Loss: 0.00005231
Iteration 54/1000 | Loss: 0.00005231
Iteration 55/1000 | Loss: 0.00005231
Iteration 56/1000 | Loss: 0.00005230
Iteration 57/1000 | Loss: 0.00005230
Iteration 58/1000 | Loss: 0.00005230
Iteration 59/1000 | Loss: 0.00005230
Iteration 60/1000 | Loss: 0.00005230
Iteration 61/1000 | Loss: 0.00005230
Iteration 62/1000 | Loss: 0.00005230
Iteration 63/1000 | Loss: 0.00005230
Iteration 64/1000 | Loss: 0.00005230
Iteration 65/1000 | Loss: 0.00005229
Iteration 66/1000 | Loss: 0.00005229
Iteration 67/1000 | Loss: 0.00005227
Iteration 68/1000 | Loss: 0.00005227
Iteration 69/1000 | Loss: 0.00005226
Iteration 70/1000 | Loss: 0.00005226
Iteration 71/1000 | Loss: 0.00005226
Iteration 72/1000 | Loss: 0.00005225
Iteration 73/1000 | Loss: 0.00005225
Iteration 74/1000 | Loss: 0.00005225
Iteration 75/1000 | Loss: 0.00005225
Iteration 76/1000 | Loss: 0.00005224
Iteration 77/1000 | Loss: 0.00005224
Iteration 78/1000 | Loss: 0.00005224
Iteration 79/1000 | Loss: 0.00005224
Iteration 80/1000 | Loss: 0.00005224
Iteration 81/1000 | Loss: 0.00005224
Iteration 82/1000 | Loss: 0.00005224
Iteration 83/1000 | Loss: 0.00005223
Iteration 84/1000 | Loss: 0.00005223
Iteration 85/1000 | Loss: 0.00005223
Iteration 86/1000 | Loss: 0.00005222
Iteration 87/1000 | Loss: 0.00005221
Iteration 88/1000 | Loss: 0.00005219
Iteration 89/1000 | Loss: 0.00005219
Iteration 90/1000 | Loss: 0.00005219
Iteration 91/1000 | Loss: 0.00005219
Iteration 92/1000 | Loss: 0.00005219
Iteration 93/1000 | Loss: 0.00005219
Iteration 94/1000 | Loss: 0.00005219
Iteration 95/1000 | Loss: 0.00005219
Iteration 96/1000 | Loss: 0.00005218
Iteration 97/1000 | Loss: 0.00005218
Iteration 98/1000 | Loss: 0.00005218
Iteration 99/1000 | Loss: 0.00005218
Iteration 100/1000 | Loss: 0.00005218
Iteration 101/1000 | Loss: 0.00005218
Iteration 102/1000 | Loss: 0.00005218
Iteration 103/1000 | Loss: 0.00005218
Iteration 104/1000 | Loss: 0.00005218
Iteration 105/1000 | Loss: 0.00005218
Iteration 106/1000 | Loss: 0.00005218
Iteration 107/1000 | Loss: 0.00005217
Iteration 108/1000 | Loss: 0.00005217
Iteration 109/1000 | Loss: 0.00005217
Iteration 110/1000 | Loss: 0.00005217
Iteration 111/1000 | Loss: 0.00005216
Iteration 112/1000 | Loss: 0.00005216
Iteration 113/1000 | Loss: 0.00005216
Iteration 114/1000 | Loss: 0.00005216
Iteration 115/1000 | Loss: 0.00005216
Iteration 116/1000 | Loss: 0.00005216
Iteration 117/1000 | Loss: 0.00005216
Iteration 118/1000 | Loss: 0.00005215
Iteration 119/1000 | Loss: 0.00005215
Iteration 120/1000 | Loss: 0.00005215
Iteration 121/1000 | Loss: 0.00005215
Iteration 122/1000 | Loss: 0.00005215
Iteration 123/1000 | Loss: 0.00005215
Iteration 124/1000 | Loss: 0.00005215
Iteration 125/1000 | Loss: 0.00005215
Iteration 126/1000 | Loss: 0.00005215
Iteration 127/1000 | Loss: 0.00005215
Iteration 128/1000 | Loss: 0.00005215
Iteration 129/1000 | Loss: 0.00005214
Iteration 130/1000 | Loss: 0.00005214
Iteration 131/1000 | Loss: 0.00005214
Iteration 132/1000 | Loss: 0.00005214
Iteration 133/1000 | Loss: 0.00005214
Iteration 134/1000 | Loss: 0.00005214
Iteration 135/1000 | Loss: 0.00005214
Iteration 136/1000 | Loss: 0.00005214
Iteration 137/1000 | Loss: 0.00005214
Iteration 138/1000 | Loss: 0.00005214
Iteration 139/1000 | Loss: 0.00005214
Iteration 140/1000 | Loss: 0.00005214
Iteration 141/1000 | Loss: 0.00005213
Iteration 142/1000 | Loss: 0.00005213
Iteration 143/1000 | Loss: 0.00005213
Iteration 144/1000 | Loss: 0.00005213
Iteration 145/1000 | Loss: 0.00005213
Iteration 146/1000 | Loss: 0.00005213
Iteration 147/1000 | Loss: 0.00005213
Iteration 148/1000 | Loss: 0.00005213
Iteration 149/1000 | Loss: 0.00005213
Iteration 150/1000 | Loss: 0.00005213
Iteration 151/1000 | Loss: 0.00005213
Iteration 152/1000 | Loss: 0.00005213
Iteration 153/1000 | Loss: 0.00005213
Iteration 154/1000 | Loss: 0.00005213
Iteration 155/1000 | Loss: 0.00005213
Iteration 156/1000 | Loss: 0.00005213
Iteration 157/1000 | Loss: 0.00005212
Iteration 158/1000 | Loss: 0.00005212
Iteration 159/1000 | Loss: 0.00005212
Iteration 160/1000 | Loss: 0.00005212
Iteration 161/1000 | Loss: 0.00005212
Iteration 162/1000 | Loss: 0.00005212
Iteration 163/1000 | Loss: 0.00005212
Iteration 164/1000 | Loss: 0.00005212
Iteration 165/1000 | Loss: 0.00005211
Iteration 166/1000 | Loss: 0.00005211
Iteration 167/1000 | Loss: 0.00005211
Iteration 168/1000 | Loss: 0.00005211
Iteration 169/1000 | Loss: 0.00005211
Iteration 170/1000 | Loss: 0.00005211
Iteration 171/1000 | Loss: 0.00005211
Iteration 172/1000 | Loss: 0.00005211
Iteration 173/1000 | Loss: 0.00005211
Iteration 174/1000 | Loss: 0.00005211
Iteration 175/1000 | Loss: 0.00005211
Iteration 176/1000 | Loss: 0.00005211
Iteration 177/1000 | Loss: 0.00005211
Iteration 178/1000 | Loss: 0.00005211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [5.211171810515225e-05, 5.211171810515225e-05, 5.211171810515225e-05, 5.211171810515225e-05, 5.211171810515225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.211171810515225e-05

Optimization complete. Final v2v error: 3.883371114730835 mm

Highest mean error: 12.494049072265625 mm for frame 31

Lowest mean error: 2.9167113304138184 mm for frame 85

Saving results

Total time: 110.66582083702087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914857
Iteration 2/25 | Loss: 0.00141593
Iteration 3/25 | Loss: 0.00126409
Iteration 4/25 | Loss: 0.00124594
Iteration 5/25 | Loss: 0.00123920
Iteration 6/25 | Loss: 0.00123805
Iteration 7/25 | Loss: 0.00123805
Iteration 8/25 | Loss: 0.00123805
Iteration 9/25 | Loss: 0.00123805
Iteration 10/25 | Loss: 0.00123805
Iteration 11/25 | Loss: 0.00123805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012380453990772367, 0.0012380453990772367, 0.0012380453990772367, 0.0012380453990772367, 0.0012380453990772367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012380453990772367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11562765
Iteration 2/25 | Loss: 0.00274022
Iteration 3/25 | Loss: 0.00274021
Iteration 4/25 | Loss: 0.00274021
Iteration 5/25 | Loss: 0.00274021
Iteration 6/25 | Loss: 0.00274021
Iteration 7/25 | Loss: 0.00274021
Iteration 8/25 | Loss: 0.00274021
Iteration 9/25 | Loss: 0.00274021
Iteration 10/25 | Loss: 0.00274021
Iteration 11/25 | Loss: 0.00274021
Iteration 12/25 | Loss: 0.00274021
Iteration 13/25 | Loss: 0.00274021
Iteration 14/25 | Loss: 0.00274021
Iteration 15/25 | Loss: 0.00274021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027402127161622047, 0.0027402127161622047, 0.0027402127161622047, 0.0027402127161622047, 0.0027402127161622047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027402127161622047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00274021
Iteration 2/1000 | Loss: 0.00005015
Iteration 3/1000 | Loss: 0.00003484
Iteration 4/1000 | Loss: 0.00002917
Iteration 5/1000 | Loss: 0.00002654
Iteration 6/1000 | Loss: 0.00002491
Iteration 7/1000 | Loss: 0.00002395
Iteration 8/1000 | Loss: 0.00002347
Iteration 9/1000 | Loss: 0.00002308
Iteration 10/1000 | Loss: 0.00002272
Iteration 11/1000 | Loss: 0.00002250
Iteration 12/1000 | Loss: 0.00002231
Iteration 13/1000 | Loss: 0.00002224
Iteration 14/1000 | Loss: 0.00002217
Iteration 15/1000 | Loss: 0.00002208
Iteration 16/1000 | Loss: 0.00002205
Iteration 17/1000 | Loss: 0.00002202
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00002196
Iteration 20/1000 | Loss: 0.00002196
Iteration 21/1000 | Loss: 0.00002194
Iteration 22/1000 | Loss: 0.00002194
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002191
Iteration 32/1000 | Loss: 0.00002191
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00002190
Iteration 36/1000 | Loss: 0.00002190
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002190
Iteration 40/1000 | Loss: 0.00002190
Iteration 41/1000 | Loss: 0.00002190
Iteration 42/1000 | Loss: 0.00002189
Iteration 43/1000 | Loss: 0.00002189
Iteration 44/1000 | Loss: 0.00002189
Iteration 45/1000 | Loss: 0.00002189
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00002189
Iteration 48/1000 | Loss: 0.00002189
Iteration 49/1000 | Loss: 0.00002189
Iteration 50/1000 | Loss: 0.00002189
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002189
Iteration 54/1000 | Loss: 0.00002189
Iteration 55/1000 | Loss: 0.00002188
Iteration 56/1000 | Loss: 0.00002188
Iteration 57/1000 | Loss: 0.00002188
Iteration 58/1000 | Loss: 0.00002188
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002187
Iteration 61/1000 | Loss: 0.00002187
Iteration 62/1000 | Loss: 0.00002187
Iteration 63/1000 | Loss: 0.00002187
Iteration 64/1000 | Loss: 0.00002187
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002187
Iteration 68/1000 | Loss: 0.00002187
Iteration 69/1000 | Loss: 0.00002187
Iteration 70/1000 | Loss: 0.00002187
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002186
Iteration 77/1000 | Loss: 0.00002186
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002186
Iteration 84/1000 | Loss: 0.00002186
Iteration 85/1000 | Loss: 0.00002186
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002186
Iteration 89/1000 | Loss: 0.00002186
Iteration 90/1000 | Loss: 0.00002186
Iteration 91/1000 | Loss: 0.00002185
Iteration 92/1000 | Loss: 0.00002185
Iteration 93/1000 | Loss: 0.00002185
Iteration 94/1000 | Loss: 0.00002185
Iteration 95/1000 | Loss: 0.00002185
Iteration 96/1000 | Loss: 0.00002185
Iteration 97/1000 | Loss: 0.00002185
Iteration 98/1000 | Loss: 0.00002185
Iteration 99/1000 | Loss: 0.00002185
Iteration 100/1000 | Loss: 0.00002185
Iteration 101/1000 | Loss: 0.00002185
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002185
Iteration 105/1000 | Loss: 0.00002185
Iteration 106/1000 | Loss: 0.00002185
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002185
Iteration 110/1000 | Loss: 0.00002185
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002185
Iteration 115/1000 | Loss: 0.00002185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.1849056793143973e-05, 2.1849056793143973e-05, 2.1849056793143973e-05, 2.1849056793143973e-05, 2.1849056793143973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1849056793143973e-05

Optimization complete. Final v2v error: 3.8721585273742676 mm

Highest mean error: 4.1581926345825195 mm for frame 132

Lowest mean error: 3.426826238632202 mm for frame 26

Saving results

Total time: 37.73841118812561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400941
Iteration 2/25 | Loss: 0.00131025
Iteration 3/25 | Loss: 0.00114504
Iteration 4/25 | Loss: 0.00113112
Iteration 5/25 | Loss: 0.00112701
Iteration 6/25 | Loss: 0.00112548
Iteration 7/25 | Loss: 0.00112548
Iteration 8/25 | Loss: 0.00112548
Iteration 9/25 | Loss: 0.00112548
Iteration 10/25 | Loss: 0.00112548
Iteration 11/25 | Loss: 0.00112548
Iteration 12/25 | Loss: 0.00112548
Iteration 13/25 | Loss: 0.00112548
Iteration 14/25 | Loss: 0.00112548
Iteration 15/25 | Loss: 0.00112548
Iteration 16/25 | Loss: 0.00112548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001125482376664877, 0.001125482376664877, 0.001125482376664877, 0.001125482376664877, 0.001125482376664877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001125482376664877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20641088
Iteration 2/25 | Loss: 0.00301188
Iteration 3/25 | Loss: 0.00301186
Iteration 4/25 | Loss: 0.00301186
Iteration 5/25 | Loss: 0.00301186
Iteration 6/25 | Loss: 0.00301186
Iteration 7/25 | Loss: 0.00301186
Iteration 8/25 | Loss: 0.00301186
Iteration 9/25 | Loss: 0.00301186
Iteration 10/25 | Loss: 0.00301186
Iteration 11/25 | Loss: 0.00301186
Iteration 12/25 | Loss: 0.00301186
Iteration 13/25 | Loss: 0.00301186
Iteration 14/25 | Loss: 0.00301186
Iteration 15/25 | Loss: 0.00301186
Iteration 16/25 | Loss: 0.00301186
Iteration 17/25 | Loss: 0.00301186
Iteration 18/25 | Loss: 0.00301186
Iteration 19/25 | Loss: 0.00301186
Iteration 20/25 | Loss: 0.00301186
Iteration 21/25 | Loss: 0.00301186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003011859953403473, 0.003011859953403473, 0.003011859953403473, 0.003011859953403473, 0.003011859953403473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003011859953403473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301186
Iteration 2/1000 | Loss: 0.00005176
Iteration 3/1000 | Loss: 0.00002613
Iteration 4/1000 | Loss: 0.00001938
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001533
Iteration 9/1000 | Loss: 0.00001489
Iteration 10/1000 | Loss: 0.00001460
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001406
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001404
Iteration 18/1000 | Loss: 0.00001403
Iteration 19/1000 | Loss: 0.00001403
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001397
Iteration 25/1000 | Loss: 0.00001396
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001396
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001393
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001390
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001387
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001384
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001378
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001377
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001373
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001372
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001369
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001367
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001366
Iteration 139/1000 | Loss: 0.00001366
Iteration 140/1000 | Loss: 0.00001366
Iteration 141/1000 | Loss: 0.00001366
Iteration 142/1000 | Loss: 0.00001366
Iteration 143/1000 | Loss: 0.00001366
Iteration 144/1000 | Loss: 0.00001366
Iteration 145/1000 | Loss: 0.00001366
Iteration 146/1000 | Loss: 0.00001366
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Iteration 156/1000 | Loss: 0.00001365
Iteration 157/1000 | Loss: 0.00001365
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001364
Iteration 162/1000 | Loss: 0.00001364
Iteration 163/1000 | Loss: 0.00001364
Iteration 164/1000 | Loss: 0.00001364
Iteration 165/1000 | Loss: 0.00001364
Iteration 166/1000 | Loss: 0.00001364
Iteration 167/1000 | Loss: 0.00001364
Iteration 168/1000 | Loss: 0.00001364
Iteration 169/1000 | Loss: 0.00001364
Iteration 170/1000 | Loss: 0.00001364
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001362
Iteration 185/1000 | Loss: 0.00001362
Iteration 186/1000 | Loss: 0.00001362
Iteration 187/1000 | Loss: 0.00001362
Iteration 188/1000 | Loss: 0.00001362
Iteration 189/1000 | Loss: 0.00001362
Iteration 190/1000 | Loss: 0.00001362
Iteration 191/1000 | Loss: 0.00001362
Iteration 192/1000 | Loss: 0.00001362
Iteration 193/1000 | Loss: 0.00001362
Iteration 194/1000 | Loss: 0.00001362
Iteration 195/1000 | Loss: 0.00001362
Iteration 196/1000 | Loss: 0.00001362
Iteration 197/1000 | Loss: 0.00001362
Iteration 198/1000 | Loss: 0.00001362
Iteration 199/1000 | Loss: 0.00001362
Iteration 200/1000 | Loss: 0.00001362
Iteration 201/1000 | Loss: 0.00001362
Iteration 202/1000 | Loss: 0.00001362
Iteration 203/1000 | Loss: 0.00001361
Iteration 204/1000 | Loss: 0.00001361
Iteration 205/1000 | Loss: 0.00001361
Iteration 206/1000 | Loss: 0.00001361
Iteration 207/1000 | Loss: 0.00001361
Iteration 208/1000 | Loss: 0.00001361
Iteration 209/1000 | Loss: 0.00001361
Iteration 210/1000 | Loss: 0.00001361
Iteration 211/1000 | Loss: 0.00001361
Iteration 212/1000 | Loss: 0.00001361
Iteration 213/1000 | Loss: 0.00001361
Iteration 214/1000 | Loss: 0.00001361
Iteration 215/1000 | Loss: 0.00001361
Iteration 216/1000 | Loss: 0.00001361
Iteration 217/1000 | Loss: 0.00001361
Iteration 218/1000 | Loss: 0.00001361
Iteration 219/1000 | Loss: 0.00001361
Iteration 220/1000 | Loss: 0.00001361
Iteration 221/1000 | Loss: 0.00001361
Iteration 222/1000 | Loss: 0.00001361
Iteration 223/1000 | Loss: 0.00001361
Iteration 224/1000 | Loss: 0.00001361
Iteration 225/1000 | Loss: 0.00001361
Iteration 226/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3614641829917673e-05, 1.3614641829917673e-05, 1.3614641829917673e-05, 1.3614641829917673e-05, 1.3614641829917673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3614641829917673e-05

Optimization complete. Final v2v error: 2.9356696605682373 mm

Highest mean error: 4.9428863525390625 mm for frame 88

Lowest mean error: 2.2317957878112793 mm for frame 3

Saving results

Total time: 41.6263267993927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00211471
Iteration 2/25 | Loss: 0.00127036
Iteration 3/25 | Loss: 0.00116124
Iteration 4/25 | Loss: 0.00113992
Iteration 5/25 | Loss: 0.00113232
Iteration 6/25 | Loss: 0.00113033
Iteration 7/25 | Loss: 0.00113033
Iteration 8/25 | Loss: 0.00113033
Iteration 9/25 | Loss: 0.00113033
Iteration 10/25 | Loss: 0.00113033
Iteration 11/25 | Loss: 0.00113033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011303279316052794, 0.0011303279316052794, 0.0011303279316052794, 0.0011303279316052794, 0.0011303279316052794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011303279316052794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11160195
Iteration 2/25 | Loss: 0.00549729
Iteration 3/25 | Loss: 0.00549729
Iteration 4/25 | Loss: 0.00549729
Iteration 5/25 | Loss: 0.00549729
Iteration 6/25 | Loss: 0.00549729
Iteration 7/25 | Loss: 0.00549729
Iteration 8/25 | Loss: 0.00549729
Iteration 9/25 | Loss: 0.00549729
Iteration 10/25 | Loss: 0.00549729
Iteration 11/25 | Loss: 0.00549729
Iteration 12/25 | Loss: 0.00549729
Iteration 13/25 | Loss: 0.00549729
Iteration 14/25 | Loss: 0.00549729
Iteration 15/25 | Loss: 0.00549729
Iteration 16/25 | Loss: 0.00549729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005497288890182972, 0.005497288890182972, 0.005497288890182972, 0.005497288890182972, 0.005497288890182972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005497288890182972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00549729
Iteration 2/1000 | Loss: 0.00006142
Iteration 3/1000 | Loss: 0.00002409
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001313
Iteration 10/1000 | Loss: 0.00001289
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001279
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001248
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001244
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001235
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001224
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001221
Iteration 179/1000 | Loss: 0.00001221
Iteration 180/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.2206032806716394e-05, 1.2206032806716394e-05, 1.2206032806716394e-05, 1.2206032806716394e-05, 1.2206032806716394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2206032806716394e-05

Optimization complete. Final v2v error: 2.9927220344543457 mm

Highest mean error: 3.4211106300354004 mm for frame 61

Lowest mean error: 2.590527057647705 mm for frame 150

Saving results

Total time: 35.487756967544556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050768
Iteration 2/25 | Loss: 0.00218613
Iteration 3/25 | Loss: 0.00176968
Iteration 4/25 | Loss: 0.00150293
Iteration 5/25 | Loss: 0.00141311
Iteration 6/25 | Loss: 0.00137017
Iteration 7/25 | Loss: 0.00131417
Iteration 8/25 | Loss: 0.00128072
Iteration 9/25 | Loss: 0.00126058
Iteration 10/25 | Loss: 0.00125283
Iteration 11/25 | Loss: 0.00124512
Iteration 12/25 | Loss: 0.00124458
Iteration 13/25 | Loss: 0.00122556
Iteration 14/25 | Loss: 0.00121828
Iteration 15/25 | Loss: 0.00121785
Iteration 16/25 | Loss: 0.00121705
Iteration 17/25 | Loss: 0.00121511
Iteration 18/25 | Loss: 0.00121547
Iteration 19/25 | Loss: 0.00121526
Iteration 20/25 | Loss: 0.00121569
Iteration 21/25 | Loss: 0.00121541
Iteration 22/25 | Loss: 0.00121446
Iteration 23/25 | Loss: 0.00121522
Iteration 24/25 | Loss: 0.00121923
Iteration 25/25 | Loss: 0.00121642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14505219
Iteration 2/25 | Loss: 0.00436974
Iteration 3/25 | Loss: 0.00411061
Iteration 4/25 | Loss: 0.00411060
Iteration 5/25 | Loss: 0.00411060
Iteration 6/25 | Loss: 0.00411059
Iteration 7/25 | Loss: 0.00411059
Iteration 8/25 | Loss: 0.00411059
Iteration 9/25 | Loss: 0.00411059
Iteration 10/25 | Loss: 0.00411059
Iteration 11/25 | Loss: 0.00411059
Iteration 12/25 | Loss: 0.00411059
Iteration 13/25 | Loss: 0.00411059
Iteration 14/25 | Loss: 0.00411059
Iteration 15/25 | Loss: 0.00411059
Iteration 16/25 | Loss: 0.00411059
Iteration 17/25 | Loss: 0.00411059
Iteration 18/25 | Loss: 0.00411059
Iteration 19/25 | Loss: 0.00411059
Iteration 20/25 | Loss: 0.00411059
Iteration 21/25 | Loss: 0.00411059
Iteration 22/25 | Loss: 0.00411059
Iteration 23/25 | Loss: 0.00411059
Iteration 24/25 | Loss: 0.00411059
Iteration 25/25 | Loss: 0.00411059

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00411059
Iteration 2/1000 | Loss: 0.00026168
Iteration 3/1000 | Loss: 0.00030699
Iteration 4/1000 | Loss: 0.00023761
Iteration 5/1000 | Loss: 0.00020585
Iteration 6/1000 | Loss: 0.00014741
Iteration 7/1000 | Loss: 0.00021946
Iteration 8/1000 | Loss: 0.00012143
Iteration 9/1000 | Loss: 0.00014112
Iteration 10/1000 | Loss: 0.00027793
Iteration 11/1000 | Loss: 0.00010519
Iteration 12/1000 | Loss: 0.00012810
Iteration 13/1000 | Loss: 0.00008686
Iteration 14/1000 | Loss: 0.00011152
Iteration 15/1000 | Loss: 0.00014249
Iteration 16/1000 | Loss: 0.00021840
Iteration 17/1000 | Loss: 0.00009633
Iteration 18/1000 | Loss: 0.00013070
Iteration 19/1000 | Loss: 0.00045884
Iteration 20/1000 | Loss: 0.00039629
Iteration 21/1000 | Loss: 0.00046702
Iteration 22/1000 | Loss: 0.00021199
Iteration 23/1000 | Loss: 0.00047002
Iteration 24/1000 | Loss: 0.00012947
Iteration 25/1000 | Loss: 0.00012538
Iteration 26/1000 | Loss: 0.00010114
Iteration 27/1000 | Loss: 0.00010524
Iteration 28/1000 | Loss: 0.00016053
Iteration 29/1000 | Loss: 0.00035950
Iteration 30/1000 | Loss: 0.00074139
Iteration 31/1000 | Loss: 0.00012959
Iteration 32/1000 | Loss: 0.00044090
Iteration 33/1000 | Loss: 0.00008581
Iteration 34/1000 | Loss: 0.00021176
Iteration 35/1000 | Loss: 0.00007567
Iteration 36/1000 | Loss: 0.00020700
Iteration 37/1000 | Loss: 0.00014696
Iteration 38/1000 | Loss: 0.00007199
Iteration 39/1000 | Loss: 0.00029013
Iteration 40/1000 | Loss: 0.00018112
Iteration 41/1000 | Loss: 0.00044999
Iteration 42/1000 | Loss: 0.00011837
Iteration 43/1000 | Loss: 0.00006709
Iteration 44/1000 | Loss: 0.00006888
Iteration 45/1000 | Loss: 0.00018754
Iteration 46/1000 | Loss: 0.00007855
Iteration 47/1000 | Loss: 0.00006777
Iteration 48/1000 | Loss: 0.00006539
Iteration 49/1000 | Loss: 0.00017564
Iteration 50/1000 | Loss: 0.00069917
Iteration 51/1000 | Loss: 0.00103022
Iteration 52/1000 | Loss: 0.00090510
Iteration 53/1000 | Loss: 0.00129846
Iteration 54/1000 | Loss: 0.00018716
Iteration 55/1000 | Loss: 0.00017652
Iteration 56/1000 | Loss: 0.00014066
Iteration 57/1000 | Loss: 0.00007710
Iteration 58/1000 | Loss: 0.00006949
Iteration 59/1000 | Loss: 0.00047052
Iteration 60/1000 | Loss: 0.00007277
Iteration 61/1000 | Loss: 0.00007608
Iteration 62/1000 | Loss: 0.00006007
Iteration 63/1000 | Loss: 0.00005387
Iteration 64/1000 | Loss: 0.00012222
Iteration 65/1000 | Loss: 0.00005338
Iteration 66/1000 | Loss: 0.00005117
Iteration 67/1000 | Loss: 0.00007079
Iteration 68/1000 | Loss: 0.00005956
Iteration 69/1000 | Loss: 0.00007542
Iteration 70/1000 | Loss: 0.00004996
Iteration 71/1000 | Loss: 0.00004964
Iteration 72/1000 | Loss: 0.00007109
Iteration 73/1000 | Loss: 0.00004925
Iteration 74/1000 | Loss: 0.00021711
Iteration 75/1000 | Loss: 0.00005883
Iteration 76/1000 | Loss: 0.00006110
Iteration 77/1000 | Loss: 0.00004979
Iteration 78/1000 | Loss: 0.00007070
Iteration 79/1000 | Loss: 0.00004807
Iteration 80/1000 | Loss: 0.00005547
Iteration 81/1000 | Loss: 0.00018890
Iteration 82/1000 | Loss: 0.00005598
Iteration 83/1000 | Loss: 0.00005034
Iteration 84/1000 | Loss: 0.00004785
Iteration 85/1000 | Loss: 0.00013399
Iteration 86/1000 | Loss: 0.00006663
Iteration 87/1000 | Loss: 0.00013915
Iteration 88/1000 | Loss: 0.00005771
Iteration 89/1000 | Loss: 0.00004885
Iteration 90/1000 | Loss: 0.00004606
Iteration 91/1000 | Loss: 0.00004509
Iteration 92/1000 | Loss: 0.00004939
Iteration 93/1000 | Loss: 0.00017769
Iteration 94/1000 | Loss: 0.00005910
Iteration 95/1000 | Loss: 0.00004682
Iteration 96/1000 | Loss: 0.00004461
Iteration 97/1000 | Loss: 0.00007995
Iteration 98/1000 | Loss: 0.00004373
Iteration 99/1000 | Loss: 0.00004214
Iteration 100/1000 | Loss: 0.00016253
Iteration 101/1000 | Loss: 0.00005109
Iteration 102/1000 | Loss: 0.00005060
Iteration 103/1000 | Loss: 0.00004403
Iteration 104/1000 | Loss: 0.00005634
Iteration 105/1000 | Loss: 0.00004825
Iteration 106/1000 | Loss: 0.00010162
Iteration 107/1000 | Loss: 0.00026861
Iteration 108/1000 | Loss: 0.00007494
Iteration 109/1000 | Loss: 0.00017396
Iteration 110/1000 | Loss: 0.00005099
Iteration 111/1000 | Loss: 0.00004620
Iteration 112/1000 | Loss: 0.00006062
Iteration 113/1000 | Loss: 0.00005232
Iteration 114/1000 | Loss: 0.00003997
Iteration 115/1000 | Loss: 0.00004757
Iteration 116/1000 | Loss: 0.00003827
Iteration 117/1000 | Loss: 0.00004312
Iteration 118/1000 | Loss: 0.00008630
Iteration 119/1000 | Loss: 0.00004131
Iteration 120/1000 | Loss: 0.00004024
Iteration 121/1000 | Loss: 0.00003872
Iteration 122/1000 | Loss: 0.00003695
Iteration 123/1000 | Loss: 0.00003695
Iteration 124/1000 | Loss: 0.00003690
Iteration 125/1000 | Loss: 0.00003689
Iteration 126/1000 | Loss: 0.00016645
Iteration 127/1000 | Loss: 0.00005641
Iteration 128/1000 | Loss: 0.00004545
Iteration 129/1000 | Loss: 0.00005367
Iteration 130/1000 | Loss: 0.00009142
Iteration 131/1000 | Loss: 0.00004274
Iteration 132/1000 | Loss: 0.00003523
Iteration 133/1000 | Loss: 0.00004662
Iteration 134/1000 | Loss: 0.00003446
Iteration 135/1000 | Loss: 0.00004375
Iteration 136/1000 | Loss: 0.00003435
Iteration 137/1000 | Loss: 0.00003456
Iteration 138/1000 | Loss: 0.00003826
Iteration 139/1000 | Loss: 0.00012233
Iteration 140/1000 | Loss: 0.00008739
Iteration 141/1000 | Loss: 0.00004175
Iteration 142/1000 | Loss: 0.00003589
Iteration 143/1000 | Loss: 0.00003780
Iteration 144/1000 | Loss: 0.00003529
Iteration 145/1000 | Loss: 0.00004510
Iteration 146/1000 | Loss: 0.00005573
Iteration 147/1000 | Loss: 0.00010823
Iteration 148/1000 | Loss: 0.00005703
Iteration 149/1000 | Loss: 0.00006833
Iteration 150/1000 | Loss: 0.00003631
Iteration 151/1000 | Loss: 0.00004486
Iteration 152/1000 | Loss: 0.00003284
Iteration 153/1000 | Loss: 0.00005170
Iteration 154/1000 | Loss: 0.00004121
Iteration 155/1000 | Loss: 0.00003226
Iteration 156/1000 | Loss: 0.00003102
Iteration 157/1000 | Loss: 0.00003085
Iteration 158/1000 | Loss: 0.00003080
Iteration 159/1000 | Loss: 0.00003333
Iteration 160/1000 | Loss: 0.00006048
Iteration 161/1000 | Loss: 0.00003369
Iteration 162/1000 | Loss: 0.00003119
Iteration 163/1000 | Loss: 0.00005817
Iteration 164/1000 | Loss: 0.00005134
Iteration 165/1000 | Loss: 0.00003057
Iteration 166/1000 | Loss: 0.00003053
Iteration 167/1000 | Loss: 0.00003053
Iteration 168/1000 | Loss: 0.00003052
Iteration 169/1000 | Loss: 0.00003051
Iteration 170/1000 | Loss: 0.00003051
Iteration 171/1000 | Loss: 0.00003051
Iteration 172/1000 | Loss: 0.00003050
Iteration 173/1000 | Loss: 0.00003050
Iteration 174/1000 | Loss: 0.00003049
Iteration 175/1000 | Loss: 0.00003049
Iteration 176/1000 | Loss: 0.00003049
Iteration 177/1000 | Loss: 0.00003404
Iteration 178/1000 | Loss: 0.00003047
Iteration 179/1000 | Loss: 0.00003046
Iteration 180/1000 | Loss: 0.00003045
Iteration 181/1000 | Loss: 0.00003045
Iteration 182/1000 | Loss: 0.00003045
Iteration 183/1000 | Loss: 0.00003045
Iteration 184/1000 | Loss: 0.00003045
Iteration 185/1000 | Loss: 0.00003045
Iteration 186/1000 | Loss: 0.00003045
Iteration 187/1000 | Loss: 0.00003045
Iteration 188/1000 | Loss: 0.00003045
Iteration 189/1000 | Loss: 0.00003045
Iteration 190/1000 | Loss: 0.00003045
Iteration 191/1000 | Loss: 0.00003045
Iteration 192/1000 | Loss: 0.00003045
Iteration 193/1000 | Loss: 0.00003045
Iteration 194/1000 | Loss: 0.00003045
Iteration 195/1000 | Loss: 0.00003045
Iteration 196/1000 | Loss: 0.00003045
Iteration 197/1000 | Loss: 0.00003045
Iteration 198/1000 | Loss: 0.00003045
Iteration 199/1000 | Loss: 0.00003045
Iteration 200/1000 | Loss: 0.00003045
Iteration 201/1000 | Loss: 0.00003045
Iteration 202/1000 | Loss: 0.00003045
Iteration 203/1000 | Loss: 0.00003045
Iteration 204/1000 | Loss: 0.00003045
Iteration 205/1000 | Loss: 0.00003045
Iteration 206/1000 | Loss: 0.00003045
Iteration 207/1000 | Loss: 0.00003045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [3.0445233278442174e-05, 3.0445233278442174e-05, 3.0445233278442174e-05, 3.0445233278442174e-05, 3.0445233278442174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0445233278442174e-05

Optimization complete. Final v2v error: 3.0570244789123535 mm

Highest mean error: 12.574542045593262 mm for frame 200

Lowest mean error: 2.1721065044403076 mm for frame 2

Saving results

Total time: 306.71464920043945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063662
Iteration 2/25 | Loss: 0.00219828
Iteration 3/25 | Loss: 0.00152733
Iteration 4/25 | Loss: 0.00141339
Iteration 5/25 | Loss: 0.00136839
Iteration 6/25 | Loss: 0.00133242
Iteration 7/25 | Loss: 0.00132018
Iteration 8/25 | Loss: 0.00129496
Iteration 9/25 | Loss: 0.00127934
Iteration 10/25 | Loss: 0.00127441
Iteration 11/25 | Loss: 0.00126946
Iteration 12/25 | Loss: 0.00126560
Iteration 13/25 | Loss: 0.00126419
Iteration 14/25 | Loss: 0.00126382
Iteration 15/25 | Loss: 0.00126383
Iteration 16/25 | Loss: 0.00126348
Iteration 17/25 | Loss: 0.00126523
Iteration 18/25 | Loss: 0.00126400
Iteration 19/25 | Loss: 0.00126407
Iteration 20/25 | Loss: 0.00126211
Iteration 21/25 | Loss: 0.00126006
Iteration 22/25 | Loss: 0.00126217
Iteration 23/25 | Loss: 0.00126092
Iteration 24/25 | Loss: 0.00125879
Iteration 25/25 | Loss: 0.00125782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11889017
Iteration 2/25 | Loss: 0.00362687
Iteration 3/25 | Loss: 0.00362686
Iteration 4/25 | Loss: 0.00362686
Iteration 5/25 | Loss: 0.00362686
Iteration 6/25 | Loss: 0.00362686
Iteration 7/25 | Loss: 0.00362686
Iteration 8/25 | Loss: 0.00362686
Iteration 9/25 | Loss: 0.00362686
Iteration 10/25 | Loss: 0.00362686
Iteration 11/25 | Loss: 0.00362686
Iteration 12/25 | Loss: 0.00362686
Iteration 13/25 | Loss: 0.00362686
Iteration 14/25 | Loss: 0.00362686
Iteration 15/25 | Loss: 0.00362686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003626856952905655, 0.003626856952905655, 0.003626856952905655, 0.003626856952905655, 0.003626856952905655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003626856952905655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362686
Iteration 2/1000 | Loss: 0.00033746
Iteration 3/1000 | Loss: 0.00011167
Iteration 4/1000 | Loss: 0.00009235
Iteration 5/1000 | Loss: 0.00007548
Iteration 6/1000 | Loss: 0.00006529
Iteration 7/1000 | Loss: 0.00005893
Iteration 8/1000 | Loss: 0.00005559
Iteration 9/1000 | Loss: 0.00005348
Iteration 10/1000 | Loss: 0.00045080
Iteration 11/1000 | Loss: 0.00029641
Iteration 12/1000 | Loss: 0.00005822
Iteration 13/1000 | Loss: 0.00004405
Iteration 14/1000 | Loss: 0.00003486
Iteration 15/1000 | Loss: 0.00002895
Iteration 16/1000 | Loss: 0.00002546
Iteration 17/1000 | Loss: 0.00002349
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001909
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001831
Iteration 27/1000 | Loss: 0.00001830
Iteration 28/1000 | Loss: 0.00001821
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001815
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001813
Iteration 36/1000 | Loss: 0.00001813
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001809
Iteration 50/1000 | Loss: 0.00001809
Iteration 51/1000 | Loss: 0.00001809
Iteration 52/1000 | Loss: 0.00001809
Iteration 53/1000 | Loss: 0.00001809
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001808
Iteration 67/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.80807473952882e-05, 1.80807473952882e-05, 1.80807473952882e-05, 1.80807473952882e-05, 1.80807473952882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.80807473952882e-05

Optimization complete. Final v2v error: 3.5275650024414062 mm

Highest mean error: 8.800398826599121 mm for frame 22

Lowest mean error: 2.9571406841278076 mm for frame 0

Saving results

Total time: 94.88859152793884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_it_4281/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_it_4281/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844386
Iteration 2/25 | Loss: 0.00137997
Iteration 3/25 | Loss: 0.00123078
Iteration 4/25 | Loss: 0.00120799
Iteration 5/25 | Loss: 0.00120354
Iteration 6/25 | Loss: 0.00120237
Iteration 7/25 | Loss: 0.00120190
Iteration 8/25 | Loss: 0.00120171
Iteration 9/25 | Loss: 0.00120171
Iteration 10/25 | Loss: 0.00120171
Iteration 11/25 | Loss: 0.00120171
Iteration 12/25 | Loss: 0.00120171
Iteration 13/25 | Loss: 0.00120171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001201710314489901, 0.001201710314489901, 0.001201710314489901, 0.001201710314489901, 0.001201710314489901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201710314489901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15227389
Iteration 2/25 | Loss: 0.00426647
Iteration 3/25 | Loss: 0.00426646
Iteration 4/25 | Loss: 0.00426646
Iteration 5/25 | Loss: 0.00426646
Iteration 6/25 | Loss: 0.00426646
Iteration 7/25 | Loss: 0.00426646
Iteration 8/25 | Loss: 0.00426646
Iteration 9/25 | Loss: 0.00426646
Iteration 10/25 | Loss: 0.00426646
Iteration 11/25 | Loss: 0.00426646
Iteration 12/25 | Loss: 0.00426646
Iteration 13/25 | Loss: 0.00426646
Iteration 14/25 | Loss: 0.00426646
Iteration 15/25 | Loss: 0.00426646
Iteration 16/25 | Loss: 0.00426646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004266458563506603, 0.004266458563506603, 0.004266458563506603, 0.004266458563506603, 0.004266458563506603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004266458563506603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00426646
Iteration 2/1000 | Loss: 0.00034289
Iteration 3/1000 | Loss: 0.00014524
Iteration 4/1000 | Loss: 0.00010526
Iteration 5/1000 | Loss: 0.00026591
Iteration 6/1000 | Loss: 0.00025979
Iteration 7/1000 | Loss: 0.00009123
Iteration 8/1000 | Loss: 0.00021478
Iteration 9/1000 | Loss: 0.00018979
Iteration 10/1000 | Loss: 0.00015603
Iteration 11/1000 | Loss: 0.00010651
Iteration 12/1000 | Loss: 0.00010597
Iteration 13/1000 | Loss: 0.00013315
Iteration 14/1000 | Loss: 0.00007490
Iteration 15/1000 | Loss: 0.00013213
Iteration 16/1000 | Loss: 0.00007431
Iteration 17/1000 | Loss: 0.00007053
Iteration 18/1000 | Loss: 0.00008517
Iteration 19/1000 | Loss: 0.00009012
Iteration 20/1000 | Loss: 0.00006776
Iteration 21/1000 | Loss: 0.00006616
Iteration 22/1000 | Loss: 0.00012279
Iteration 23/1000 | Loss: 0.00006610
Iteration 24/1000 | Loss: 0.00012757
Iteration 25/1000 | Loss: 0.00016161
Iteration 26/1000 | Loss: 0.00016918
Iteration 27/1000 | Loss: 0.00014819
Iteration 28/1000 | Loss: 0.00017279
Iteration 29/1000 | Loss: 0.00012783
Iteration 30/1000 | Loss: 0.00013209
Iteration 31/1000 | Loss: 0.00013256
Iteration 32/1000 | Loss: 0.00008604
Iteration 33/1000 | Loss: 0.00008855
Iteration 34/1000 | Loss: 0.00013586
Iteration 35/1000 | Loss: 0.00011612
Iteration 36/1000 | Loss: 0.00014839
Iteration 37/1000 | Loss: 0.00012291
Iteration 38/1000 | Loss: 0.00011164
Iteration 39/1000 | Loss: 0.00013121
Iteration 40/1000 | Loss: 0.00011707
Iteration 41/1000 | Loss: 0.00014419
Iteration 42/1000 | Loss: 0.00019404
Iteration 43/1000 | Loss: 0.00010929
Iteration 44/1000 | Loss: 0.00011409
Iteration 45/1000 | Loss: 0.00015193
Iteration 46/1000 | Loss: 0.00013968
Iteration 47/1000 | Loss: 0.00019429
Iteration 48/1000 | Loss: 0.00017057
Iteration 49/1000 | Loss: 0.00011890
Iteration 50/1000 | Loss: 0.00021417
Iteration 51/1000 | Loss: 0.00016847
Iteration 52/1000 | Loss: 0.00017183
Iteration 53/1000 | Loss: 0.00017096
Iteration 54/1000 | Loss: 0.00009584
Iteration 55/1000 | Loss: 0.00019095
Iteration 56/1000 | Loss: 0.00015735
Iteration 57/1000 | Loss: 0.00011268
Iteration 58/1000 | Loss: 0.00017553
Iteration 59/1000 | Loss: 0.00016148
Iteration 60/1000 | Loss: 0.00014377
Iteration 61/1000 | Loss: 0.00013110
Iteration 62/1000 | Loss: 0.00011010
Iteration 63/1000 | Loss: 0.00014111
Iteration 64/1000 | Loss: 0.00013979
Iteration 65/1000 | Loss: 0.00017030
Iteration 66/1000 | Loss: 0.00013678
Iteration 67/1000 | Loss: 0.00017039
Iteration 68/1000 | Loss: 0.00007159
Iteration 69/1000 | Loss: 0.00016451
Iteration 70/1000 | Loss: 0.00006386
Iteration 71/1000 | Loss: 0.00006264
Iteration 72/1000 | Loss: 0.00006142
Iteration 73/1000 | Loss: 0.00006085
Iteration 74/1000 | Loss: 0.00006044
Iteration 75/1000 | Loss: 0.00016762
Iteration 76/1000 | Loss: 0.00006415
Iteration 77/1000 | Loss: 0.00006084
Iteration 78/1000 | Loss: 0.00013915
Iteration 79/1000 | Loss: 0.00007076
Iteration 80/1000 | Loss: 0.00011427
Iteration 81/1000 | Loss: 0.00006692
Iteration 82/1000 | Loss: 0.00010129
Iteration 83/1000 | Loss: 0.00006334
Iteration 84/1000 | Loss: 0.00007808
Iteration 85/1000 | Loss: 0.00006174
Iteration 86/1000 | Loss: 0.00006092
Iteration 87/1000 | Loss: 0.00006048
Iteration 88/1000 | Loss: 0.00005998
Iteration 89/1000 | Loss: 0.00005948
Iteration 90/1000 | Loss: 0.00005904
Iteration 91/1000 | Loss: 0.00005867
Iteration 92/1000 | Loss: 0.00005826
Iteration 93/1000 | Loss: 0.00037412
Iteration 94/1000 | Loss: 0.00007160
Iteration 95/1000 | Loss: 0.00005914
Iteration 96/1000 | Loss: 0.00005694
Iteration 97/1000 | Loss: 0.00005602
Iteration 98/1000 | Loss: 0.00005529
Iteration 99/1000 | Loss: 0.00005471
Iteration 100/1000 | Loss: 0.00005435
Iteration 101/1000 | Loss: 0.00005407
Iteration 102/1000 | Loss: 0.00005378
Iteration 103/1000 | Loss: 0.00005352
Iteration 104/1000 | Loss: 0.00005334
Iteration 105/1000 | Loss: 0.00005329
Iteration 106/1000 | Loss: 0.00005324
Iteration 107/1000 | Loss: 0.00005323
Iteration 108/1000 | Loss: 0.00005322
Iteration 109/1000 | Loss: 0.00005315
Iteration 110/1000 | Loss: 0.00005299
Iteration 111/1000 | Loss: 0.00005287
Iteration 112/1000 | Loss: 0.00005285
Iteration 113/1000 | Loss: 0.00005283
Iteration 114/1000 | Loss: 0.00005282
Iteration 115/1000 | Loss: 0.00005282
Iteration 116/1000 | Loss: 0.00005282
Iteration 117/1000 | Loss: 0.00005282
Iteration 118/1000 | Loss: 0.00005282
Iteration 119/1000 | Loss: 0.00005282
Iteration 120/1000 | Loss: 0.00005282
Iteration 121/1000 | Loss: 0.00005282
Iteration 122/1000 | Loss: 0.00005282
Iteration 123/1000 | Loss: 0.00005282
Iteration 124/1000 | Loss: 0.00005281
Iteration 125/1000 | Loss: 0.00005280
Iteration 126/1000 | Loss: 0.00005280
Iteration 127/1000 | Loss: 0.00005279
Iteration 128/1000 | Loss: 0.00005279
Iteration 129/1000 | Loss: 0.00005279
Iteration 130/1000 | Loss: 0.00005278
Iteration 131/1000 | Loss: 0.00005278
Iteration 132/1000 | Loss: 0.00005278
Iteration 133/1000 | Loss: 0.00005277
Iteration 134/1000 | Loss: 0.00005277
Iteration 135/1000 | Loss: 0.00005276
Iteration 136/1000 | Loss: 0.00005276
Iteration 137/1000 | Loss: 0.00005275
Iteration 138/1000 | Loss: 0.00005275
Iteration 139/1000 | Loss: 0.00005275
Iteration 140/1000 | Loss: 0.00005275
Iteration 141/1000 | Loss: 0.00005274
Iteration 142/1000 | Loss: 0.00005274
Iteration 143/1000 | Loss: 0.00005274
Iteration 144/1000 | Loss: 0.00005273
Iteration 145/1000 | Loss: 0.00005273
Iteration 146/1000 | Loss: 0.00005272
Iteration 147/1000 | Loss: 0.00005270
Iteration 148/1000 | Loss: 0.00005269
Iteration 149/1000 | Loss: 0.00005268
Iteration 150/1000 | Loss: 0.00005264
Iteration 151/1000 | Loss: 0.00005264
Iteration 152/1000 | Loss: 0.00005263
Iteration 153/1000 | Loss: 0.00005263
Iteration 154/1000 | Loss: 0.00005262
Iteration 155/1000 | Loss: 0.00005262
Iteration 156/1000 | Loss: 0.00005262
Iteration 157/1000 | Loss: 0.00005262
Iteration 158/1000 | Loss: 0.00005262
Iteration 159/1000 | Loss: 0.00005261
Iteration 160/1000 | Loss: 0.00005261
Iteration 161/1000 | Loss: 0.00005261
Iteration 162/1000 | Loss: 0.00005261
Iteration 163/1000 | Loss: 0.00005260
Iteration 164/1000 | Loss: 0.00005260
Iteration 165/1000 | Loss: 0.00005259
Iteration 166/1000 | Loss: 0.00005258
Iteration 167/1000 | Loss: 0.00005258
Iteration 168/1000 | Loss: 0.00005255
Iteration 169/1000 | Loss: 0.00005254
Iteration 170/1000 | Loss: 0.00005242
Iteration 171/1000 | Loss: 0.00005241
Iteration 172/1000 | Loss: 0.00005240
Iteration 173/1000 | Loss: 0.00005239
Iteration 174/1000 | Loss: 0.00005236
Iteration 175/1000 | Loss: 0.00005233
Iteration 176/1000 | Loss: 0.00005232
Iteration 177/1000 | Loss: 0.00005231
Iteration 178/1000 | Loss: 0.00005231
Iteration 179/1000 | Loss: 0.00005231
Iteration 180/1000 | Loss: 0.00005231
Iteration 181/1000 | Loss: 0.00005231
Iteration 182/1000 | Loss: 0.00005230
Iteration 183/1000 | Loss: 0.00005225
Iteration 184/1000 | Loss: 0.00005225
Iteration 185/1000 | Loss: 0.00005225
Iteration 186/1000 | Loss: 0.00005225
Iteration 187/1000 | Loss: 0.00005224
Iteration 188/1000 | Loss: 0.00005223
Iteration 189/1000 | Loss: 0.00005223
Iteration 190/1000 | Loss: 0.00005223
Iteration 191/1000 | Loss: 0.00005222
Iteration 192/1000 | Loss: 0.00005222
Iteration 193/1000 | Loss: 0.00005222
Iteration 194/1000 | Loss: 0.00005222
Iteration 195/1000 | Loss: 0.00005222
Iteration 196/1000 | Loss: 0.00005221
Iteration 197/1000 | Loss: 0.00005221
Iteration 198/1000 | Loss: 0.00005221
Iteration 199/1000 | Loss: 0.00005221
Iteration 200/1000 | Loss: 0.00005221
Iteration 201/1000 | Loss: 0.00005221
Iteration 202/1000 | Loss: 0.00005221
Iteration 203/1000 | Loss: 0.00005220
Iteration 204/1000 | Loss: 0.00005220
Iteration 205/1000 | Loss: 0.00005219
Iteration 206/1000 | Loss: 0.00005219
Iteration 207/1000 | Loss: 0.00005219
Iteration 208/1000 | Loss: 0.00005218
Iteration 209/1000 | Loss: 0.00005218
Iteration 210/1000 | Loss: 0.00005216
Iteration 211/1000 | Loss: 0.00005216
Iteration 212/1000 | Loss: 0.00005216
Iteration 213/1000 | Loss: 0.00005215
Iteration 214/1000 | Loss: 0.00005215
Iteration 215/1000 | Loss: 0.00005215
Iteration 216/1000 | Loss: 0.00005214
Iteration 217/1000 | Loss: 0.00005214
Iteration 218/1000 | Loss: 0.00005214
Iteration 219/1000 | Loss: 0.00005213
Iteration 220/1000 | Loss: 0.00005213
Iteration 221/1000 | Loss: 0.00005213
Iteration 222/1000 | Loss: 0.00005212
Iteration 223/1000 | Loss: 0.00005212
Iteration 224/1000 | Loss: 0.00005211
Iteration 225/1000 | Loss: 0.00005211
Iteration 226/1000 | Loss: 0.00005209
Iteration 227/1000 | Loss: 0.00005209
Iteration 228/1000 | Loss: 0.00005208
Iteration 229/1000 | Loss: 0.00005208
Iteration 230/1000 | Loss: 0.00005208
Iteration 231/1000 | Loss: 0.00005207
Iteration 232/1000 | Loss: 0.00005207
Iteration 233/1000 | Loss: 0.00005206
Iteration 234/1000 | Loss: 0.00005206
Iteration 235/1000 | Loss: 0.00005206
Iteration 236/1000 | Loss: 0.00005205
Iteration 237/1000 | Loss: 0.00005204
Iteration 238/1000 | Loss: 0.00005203
Iteration 239/1000 | Loss: 0.00005202
Iteration 240/1000 | Loss: 0.00005202
Iteration 241/1000 | Loss: 0.00005202
Iteration 242/1000 | Loss: 0.00005199
Iteration 243/1000 | Loss: 0.00005197
Iteration 244/1000 | Loss: 0.00005196
Iteration 245/1000 | Loss: 0.00005196
Iteration 246/1000 | Loss: 0.00005195
Iteration 247/1000 | Loss: 0.00005195
Iteration 248/1000 | Loss: 0.00005191
Iteration 249/1000 | Loss: 0.00005188
Iteration 250/1000 | Loss: 0.00005188
Iteration 251/1000 | Loss: 0.00005187
Iteration 252/1000 | Loss: 0.00005187
Iteration 253/1000 | Loss: 0.00005187
Iteration 254/1000 | Loss: 0.00005187
Iteration 255/1000 | Loss: 0.00005186
Iteration 256/1000 | Loss: 0.00005186
Iteration 257/1000 | Loss: 0.00005185
Iteration 258/1000 | Loss: 0.00005185
Iteration 259/1000 | Loss: 0.00005185
Iteration 260/1000 | Loss: 0.00005185
Iteration 261/1000 | Loss: 0.00005185
Iteration 262/1000 | Loss: 0.00005185
Iteration 263/1000 | Loss: 0.00005184
Iteration 264/1000 | Loss: 0.00005184
Iteration 265/1000 | Loss: 0.00005184
Iteration 266/1000 | Loss: 0.00005184
Iteration 267/1000 | Loss: 0.00005184
Iteration 268/1000 | Loss: 0.00005184
Iteration 269/1000 | Loss: 0.00005183
Iteration 270/1000 | Loss: 0.00005183
Iteration 271/1000 | Loss: 0.00005183
Iteration 272/1000 | Loss: 0.00005182
Iteration 273/1000 | Loss: 0.00005182
Iteration 274/1000 | Loss: 0.00005182
Iteration 275/1000 | Loss: 0.00005180
Iteration 276/1000 | Loss: 0.00005180
Iteration 277/1000 | Loss: 0.00005180
Iteration 278/1000 | Loss: 0.00005180
Iteration 279/1000 | Loss: 0.00005180
Iteration 280/1000 | Loss: 0.00005180
Iteration 281/1000 | Loss: 0.00005180
Iteration 282/1000 | Loss: 0.00005179
Iteration 283/1000 | Loss: 0.00005179
Iteration 284/1000 | Loss: 0.00005179
Iteration 285/1000 | Loss: 0.00005179
Iteration 286/1000 | Loss: 0.00005179
Iteration 287/1000 | Loss: 0.00005179
Iteration 288/1000 | Loss: 0.00005179
Iteration 289/1000 | Loss: 0.00005179
Iteration 290/1000 | Loss: 0.00005179
Iteration 291/1000 | Loss: 0.00005179
Iteration 292/1000 | Loss: 0.00005179
Iteration 293/1000 | Loss: 0.00005179
Iteration 294/1000 | Loss: 0.00005178
Iteration 295/1000 | Loss: 0.00005178
Iteration 296/1000 | Loss: 0.00005178
Iteration 297/1000 | Loss: 0.00005177
Iteration 298/1000 | Loss: 0.00005177
Iteration 299/1000 | Loss: 0.00005177
Iteration 300/1000 | Loss: 0.00005176
Iteration 301/1000 | Loss: 0.00005176
Iteration 302/1000 | Loss: 0.00005176
Iteration 303/1000 | Loss: 0.00005176
Iteration 304/1000 | Loss: 0.00005176
Iteration 305/1000 | Loss: 0.00005176
Iteration 306/1000 | Loss: 0.00005176
Iteration 307/1000 | Loss: 0.00005176
Iteration 308/1000 | Loss: 0.00005176
Iteration 309/1000 | Loss: 0.00005175
Iteration 310/1000 | Loss: 0.00005175
Iteration 311/1000 | Loss: 0.00005175
Iteration 312/1000 | Loss: 0.00005175
Iteration 313/1000 | Loss: 0.00005175
Iteration 314/1000 | Loss: 0.00005175
Iteration 315/1000 | Loss: 0.00005175
Iteration 316/1000 | Loss: 0.00005174
Iteration 317/1000 | Loss: 0.00005174
Iteration 318/1000 | Loss: 0.00005174
Iteration 319/1000 | Loss: 0.00005174
Iteration 320/1000 | Loss: 0.00005174
Iteration 321/1000 | Loss: 0.00005174
Iteration 322/1000 | Loss: 0.00005173
Iteration 323/1000 | Loss: 0.00005173
Iteration 324/1000 | Loss: 0.00005173
Iteration 325/1000 | Loss: 0.00005173
Iteration 326/1000 | Loss: 0.00005173
Iteration 327/1000 | Loss: 0.00005173
Iteration 328/1000 | Loss: 0.00005173
Iteration 329/1000 | Loss: 0.00005173
Iteration 330/1000 | Loss: 0.00005172
Iteration 331/1000 | Loss: 0.00005172
Iteration 332/1000 | Loss: 0.00005172
Iteration 333/1000 | Loss: 0.00005172
Iteration 334/1000 | Loss: 0.00005172
Iteration 335/1000 | Loss: 0.00005172
Iteration 336/1000 | Loss: 0.00005171
Iteration 337/1000 | Loss: 0.00005171
Iteration 338/1000 | Loss: 0.00005171
Iteration 339/1000 | Loss: 0.00005170
Iteration 340/1000 | Loss: 0.00005170
Iteration 341/1000 | Loss: 0.00005170
Iteration 342/1000 | Loss: 0.00005170
Iteration 343/1000 | Loss: 0.00005169
Iteration 344/1000 | Loss: 0.00005169
Iteration 345/1000 | Loss: 0.00005169
Iteration 346/1000 | Loss: 0.00005169
Iteration 347/1000 | Loss: 0.00005169
Iteration 348/1000 | Loss: 0.00005169
Iteration 349/1000 | Loss: 0.00005169
Iteration 350/1000 | Loss: 0.00005169
Iteration 351/1000 | Loss: 0.00005169
Iteration 352/1000 | Loss: 0.00005169
Iteration 353/1000 | Loss: 0.00005169
Iteration 354/1000 | Loss: 0.00005169
Iteration 355/1000 | Loss: 0.00005169
Iteration 356/1000 | Loss: 0.00005169
Iteration 357/1000 | Loss: 0.00005169
Iteration 358/1000 | Loss: 0.00005169
Iteration 359/1000 | Loss: 0.00005169
Iteration 360/1000 | Loss: 0.00005169
Iteration 361/1000 | Loss: 0.00005169
Iteration 362/1000 | Loss: 0.00005169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [5.168638745089993e-05, 5.168638745089993e-05, 5.168638745089993e-05, 5.168638745089993e-05, 5.168638745089993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.168638745089993e-05

Optimization complete. Final v2v error: 3.3860056400299072 mm

Highest mean error: 12.679435729980469 mm for frame 85

Lowest mean error: 2.2817060947418213 mm for frame 200

Saving results

Total time: 195.1823592185974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006037
Iteration 2/25 | Loss: 0.00324314
Iteration 3/25 | Loss: 0.00197329
Iteration 4/25 | Loss: 0.00187049
Iteration 5/25 | Loss: 0.00168215
Iteration 6/25 | Loss: 0.00158992
Iteration 7/25 | Loss: 0.00151160
Iteration 8/25 | Loss: 0.00134097
Iteration 9/25 | Loss: 0.00150955
Iteration 10/25 | Loss: 0.00175696
Iteration 11/25 | Loss: 0.00114841
Iteration 12/25 | Loss: 0.00084221
Iteration 13/25 | Loss: 0.00080626
Iteration 14/25 | Loss: 0.00079800
Iteration 15/25 | Loss: 0.00079448
Iteration 16/25 | Loss: 0.00079359
Iteration 17/25 | Loss: 0.00079343
Iteration 18/25 | Loss: 0.00079335
Iteration 19/25 | Loss: 0.00079334
Iteration 20/25 | Loss: 0.00079334
Iteration 21/25 | Loss: 0.00079333
Iteration 22/25 | Loss: 0.00079333
Iteration 23/25 | Loss: 0.00079333
Iteration 24/25 | Loss: 0.00079333
Iteration 25/25 | Loss: 0.00079333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.14367580
Iteration 2/25 | Loss: 0.00113014
Iteration 3/25 | Loss: 0.00046364
Iteration 4/25 | Loss: 0.00046364
Iteration 5/25 | Loss: 0.00046364
Iteration 6/25 | Loss: 0.00046364
Iteration 7/25 | Loss: 0.00046364
Iteration 8/25 | Loss: 0.00046364
Iteration 9/25 | Loss: 0.00046364
Iteration 10/25 | Loss: 0.00046364
Iteration 11/25 | Loss: 0.00046364
Iteration 12/25 | Loss: 0.00046364
Iteration 13/25 | Loss: 0.00046364
Iteration 14/25 | Loss: 0.00046364
Iteration 15/25 | Loss: 0.00046364
Iteration 16/25 | Loss: 0.00046364
Iteration 17/25 | Loss: 0.00046364
Iteration 18/25 | Loss: 0.00046364
Iteration 19/25 | Loss: 0.00046364
Iteration 20/25 | Loss: 0.00046364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004636385419871658, 0.0004636385419871658, 0.0004636385419871658, 0.0004636385419871658, 0.0004636385419871658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004636385419871658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046364
Iteration 2/1000 | Loss: 0.00048078
Iteration 3/1000 | Loss: 0.00006664
Iteration 4/1000 | Loss: 0.00005803
Iteration 5/1000 | Loss: 0.00086203
Iteration 6/1000 | Loss: 0.00107911
Iteration 7/1000 | Loss: 0.00019547
Iteration 8/1000 | Loss: 0.00024348
Iteration 9/1000 | Loss: 0.00004108
Iteration 10/1000 | Loss: 0.00005121
Iteration 11/1000 | Loss: 0.00003062
Iteration 12/1000 | Loss: 0.00055375
Iteration 13/1000 | Loss: 0.00033860
Iteration 14/1000 | Loss: 0.00028038
Iteration 15/1000 | Loss: 0.00002833
Iteration 16/1000 | Loss: 0.00002648
Iteration 17/1000 | Loss: 0.00042127
Iteration 18/1000 | Loss: 0.00005309
Iteration 19/1000 | Loss: 0.00003696
Iteration 20/1000 | Loss: 0.00003016
Iteration 21/1000 | Loss: 0.00002639
Iteration 22/1000 | Loss: 0.00038256
Iteration 23/1000 | Loss: 0.00003162
Iteration 24/1000 | Loss: 0.00002601
Iteration 25/1000 | Loss: 0.00002509
Iteration 26/1000 | Loss: 0.00002481
Iteration 27/1000 | Loss: 0.00002454
Iteration 28/1000 | Loss: 0.00080402
Iteration 29/1000 | Loss: 0.00004547
Iteration 30/1000 | Loss: 0.00003314
Iteration 31/1000 | Loss: 0.00002473
Iteration 32/1000 | Loss: 0.00002418
Iteration 33/1000 | Loss: 0.00002409
Iteration 34/1000 | Loss: 0.00002403
Iteration 35/1000 | Loss: 0.00002402
Iteration 36/1000 | Loss: 0.00002402
Iteration 37/1000 | Loss: 0.00002397
Iteration 38/1000 | Loss: 0.00002396
Iteration 39/1000 | Loss: 0.00002396
Iteration 40/1000 | Loss: 0.00002395
Iteration 41/1000 | Loss: 0.00002395
Iteration 42/1000 | Loss: 0.00002395
Iteration 43/1000 | Loss: 0.00002394
Iteration 44/1000 | Loss: 0.00002394
Iteration 45/1000 | Loss: 0.00002393
Iteration 46/1000 | Loss: 0.00002392
Iteration 47/1000 | Loss: 0.00002392
Iteration 48/1000 | Loss: 0.00002392
Iteration 49/1000 | Loss: 0.00002391
Iteration 50/1000 | Loss: 0.00002391
Iteration 51/1000 | Loss: 0.00002391
Iteration 52/1000 | Loss: 0.00002391
Iteration 53/1000 | Loss: 0.00002390
Iteration 54/1000 | Loss: 0.00002390
Iteration 55/1000 | Loss: 0.00002390
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00002389
Iteration 58/1000 | Loss: 0.00002389
Iteration 59/1000 | Loss: 0.00002389
Iteration 60/1000 | Loss: 0.00002389
Iteration 61/1000 | Loss: 0.00002388
Iteration 62/1000 | Loss: 0.00002388
Iteration 63/1000 | Loss: 0.00002388
Iteration 64/1000 | Loss: 0.00002388
Iteration 65/1000 | Loss: 0.00002388
Iteration 66/1000 | Loss: 0.00002388
Iteration 67/1000 | Loss: 0.00002388
Iteration 68/1000 | Loss: 0.00002388
Iteration 69/1000 | Loss: 0.00002387
Iteration 70/1000 | Loss: 0.00002387
Iteration 71/1000 | Loss: 0.00002387
Iteration 72/1000 | Loss: 0.00002387
Iteration 73/1000 | Loss: 0.00002387
Iteration 74/1000 | Loss: 0.00002387
Iteration 75/1000 | Loss: 0.00002386
Iteration 76/1000 | Loss: 0.00002386
Iteration 77/1000 | Loss: 0.00002386
Iteration 78/1000 | Loss: 0.00002386
Iteration 79/1000 | Loss: 0.00002386
Iteration 80/1000 | Loss: 0.00002386
Iteration 81/1000 | Loss: 0.00002386
Iteration 82/1000 | Loss: 0.00002386
Iteration 83/1000 | Loss: 0.00002386
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002386
Iteration 90/1000 | Loss: 0.00002386
Iteration 91/1000 | Loss: 0.00002386
Iteration 92/1000 | Loss: 0.00002386
Iteration 93/1000 | Loss: 0.00002386
Iteration 94/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.385853986197617e-05, 2.385853986197617e-05, 2.385853986197617e-05, 2.385853986197617e-05, 2.385853986197617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.385853986197617e-05

Optimization complete. Final v2v error: 4.0999531745910645 mm

Highest mean error: 4.509542942047119 mm for frame 81

Lowest mean error: 3.7194111347198486 mm for frame 24

Saving results

Total time: 79.74932265281677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445657
Iteration 2/25 | Loss: 0.00082014
Iteration 3/25 | Loss: 0.00067374
Iteration 4/25 | Loss: 0.00064553
Iteration 5/25 | Loss: 0.00063618
Iteration 6/25 | Loss: 0.00063392
Iteration 7/25 | Loss: 0.00063335
Iteration 8/25 | Loss: 0.00063335
Iteration 9/25 | Loss: 0.00063335
Iteration 10/25 | Loss: 0.00063335
Iteration 11/25 | Loss: 0.00063335
Iteration 12/25 | Loss: 0.00063335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006333522615022957, 0.0006333522615022957, 0.0006333522615022957, 0.0006333522615022957, 0.0006333522615022957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006333522615022957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50844383
Iteration 2/25 | Loss: 0.00025650
Iteration 3/25 | Loss: 0.00025649
Iteration 4/25 | Loss: 0.00025649
Iteration 5/25 | Loss: 0.00025649
Iteration 6/25 | Loss: 0.00025649
Iteration 7/25 | Loss: 0.00025649
Iteration 8/25 | Loss: 0.00025649
Iteration 9/25 | Loss: 0.00025649
Iteration 10/25 | Loss: 0.00025649
Iteration 11/25 | Loss: 0.00025649
Iteration 12/25 | Loss: 0.00025649
Iteration 13/25 | Loss: 0.00025649
Iteration 14/25 | Loss: 0.00025649
Iteration 15/25 | Loss: 0.00025649
Iteration 16/25 | Loss: 0.00025649
Iteration 17/25 | Loss: 0.00025649
Iteration 18/25 | Loss: 0.00025649
Iteration 19/25 | Loss: 0.00025649
Iteration 20/25 | Loss: 0.00025649
Iteration 21/25 | Loss: 0.00025649
Iteration 22/25 | Loss: 0.00025649
Iteration 23/25 | Loss: 0.00025649
Iteration 24/25 | Loss: 0.00025649
Iteration 25/25 | Loss: 0.00025649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025649
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00002384
Iteration 4/1000 | Loss: 0.00002078
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00001900
Iteration 7/1000 | Loss: 0.00001863
Iteration 8/1000 | Loss: 0.00001825
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001814
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001787
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001773
Iteration 22/1000 | Loss: 0.00001769
Iteration 23/1000 | Loss: 0.00001768
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001758
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001752
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001751
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001747
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001740
Iteration 60/1000 | Loss: 0.00001740
Iteration 61/1000 | Loss: 0.00001739
Iteration 62/1000 | Loss: 0.00001738
Iteration 63/1000 | Loss: 0.00001738
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001737
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001734
Iteration 76/1000 | Loss: 0.00001734
Iteration 77/1000 | Loss: 0.00001734
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001727
Iteration 108/1000 | Loss: 0.00001727
Iteration 109/1000 | Loss: 0.00001727
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001726
Iteration 114/1000 | Loss: 0.00001726
Iteration 115/1000 | Loss: 0.00001726
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001725
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001725
Iteration 123/1000 | Loss: 0.00001725
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001724
Iteration 128/1000 | Loss: 0.00001724
Iteration 129/1000 | Loss: 0.00001724
Iteration 130/1000 | Loss: 0.00001724
Iteration 131/1000 | Loss: 0.00001723
Iteration 132/1000 | Loss: 0.00001723
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001722
Iteration 138/1000 | Loss: 0.00001722
Iteration 139/1000 | Loss: 0.00001722
Iteration 140/1000 | Loss: 0.00001722
Iteration 141/1000 | Loss: 0.00001722
Iteration 142/1000 | Loss: 0.00001722
Iteration 143/1000 | Loss: 0.00001722
Iteration 144/1000 | Loss: 0.00001722
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001721
Iteration 150/1000 | Loss: 0.00001721
Iteration 151/1000 | Loss: 0.00001721
Iteration 152/1000 | Loss: 0.00001721
Iteration 153/1000 | Loss: 0.00001721
Iteration 154/1000 | Loss: 0.00001721
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001721
Iteration 160/1000 | Loss: 0.00001720
Iteration 161/1000 | Loss: 0.00001720
Iteration 162/1000 | Loss: 0.00001720
Iteration 163/1000 | Loss: 0.00001720
Iteration 164/1000 | Loss: 0.00001720
Iteration 165/1000 | Loss: 0.00001720
Iteration 166/1000 | Loss: 0.00001720
Iteration 167/1000 | Loss: 0.00001720
Iteration 168/1000 | Loss: 0.00001720
Iteration 169/1000 | Loss: 0.00001720
Iteration 170/1000 | Loss: 0.00001720
Iteration 171/1000 | Loss: 0.00001720
Iteration 172/1000 | Loss: 0.00001720
Iteration 173/1000 | Loss: 0.00001720
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001720
Iteration 187/1000 | Loss: 0.00001720
Iteration 188/1000 | Loss: 0.00001720
Iteration 189/1000 | Loss: 0.00001720
Iteration 190/1000 | Loss: 0.00001720
Iteration 191/1000 | Loss: 0.00001720
Iteration 192/1000 | Loss: 0.00001720
Iteration 193/1000 | Loss: 0.00001720
Iteration 194/1000 | Loss: 0.00001720
Iteration 195/1000 | Loss: 0.00001720
Iteration 196/1000 | Loss: 0.00001720
Iteration 197/1000 | Loss: 0.00001720
Iteration 198/1000 | Loss: 0.00001720
Iteration 199/1000 | Loss: 0.00001720
Iteration 200/1000 | Loss: 0.00001720
Iteration 201/1000 | Loss: 0.00001720
Iteration 202/1000 | Loss: 0.00001720
Iteration 203/1000 | Loss: 0.00001720
Iteration 204/1000 | Loss: 0.00001720
Iteration 205/1000 | Loss: 0.00001720
Iteration 206/1000 | Loss: 0.00001720
Iteration 207/1000 | Loss: 0.00001720
Iteration 208/1000 | Loss: 0.00001720
Iteration 209/1000 | Loss: 0.00001720
Iteration 210/1000 | Loss: 0.00001720
Iteration 211/1000 | Loss: 0.00001720
Iteration 212/1000 | Loss: 0.00001720
Iteration 213/1000 | Loss: 0.00001720
Iteration 214/1000 | Loss: 0.00001720
Iteration 215/1000 | Loss: 0.00001720
Iteration 216/1000 | Loss: 0.00001720
Iteration 217/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.720013824524358e-05, 1.720013824524358e-05, 1.720013824524358e-05, 1.720013824524358e-05, 1.720013824524358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720013824524358e-05

Optimization complete. Final v2v error: 3.5133795738220215 mm

Highest mean error: 4.155189037322998 mm for frame 79

Lowest mean error: 3.171105146408081 mm for frame 35

Saving results

Total time: 39.9525363445282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040114
Iteration 2/25 | Loss: 0.00165348
Iteration 3/25 | Loss: 0.00111282
Iteration 4/25 | Loss: 0.00095652
Iteration 5/25 | Loss: 0.00096737
Iteration 6/25 | Loss: 0.00093428
Iteration 7/25 | Loss: 0.00087285
Iteration 8/25 | Loss: 0.00081982
Iteration 9/25 | Loss: 0.00080261
Iteration 10/25 | Loss: 0.00080573
Iteration 11/25 | Loss: 0.00080976
Iteration 12/25 | Loss: 0.00080583
Iteration 13/25 | Loss: 0.00079729
Iteration 14/25 | Loss: 0.00079020
Iteration 15/25 | Loss: 0.00079550
Iteration 16/25 | Loss: 0.00079316
Iteration 17/25 | Loss: 0.00078474
Iteration 18/25 | Loss: 0.00077785
Iteration 19/25 | Loss: 0.00078116
Iteration 20/25 | Loss: 0.00077753
Iteration 21/25 | Loss: 0.00077149
Iteration 22/25 | Loss: 0.00077451
Iteration 23/25 | Loss: 0.00077186
Iteration 24/25 | Loss: 0.00077222
Iteration 25/25 | Loss: 0.00076886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98038208
Iteration 2/25 | Loss: 0.00062694
Iteration 3/25 | Loss: 0.00062694
Iteration 4/25 | Loss: 0.00062694
Iteration 5/25 | Loss: 0.00062694
Iteration 6/25 | Loss: 0.00062694
Iteration 7/25 | Loss: 0.00062694
Iteration 8/25 | Loss: 0.00062694
Iteration 9/25 | Loss: 0.00062694
Iteration 10/25 | Loss: 0.00062694
Iteration 11/25 | Loss: 0.00062693
Iteration 12/25 | Loss: 0.00062693
Iteration 13/25 | Loss: 0.00062693
Iteration 14/25 | Loss: 0.00062693
Iteration 15/25 | Loss: 0.00062693
Iteration 16/25 | Loss: 0.00062693
Iteration 17/25 | Loss: 0.00062693
Iteration 18/25 | Loss: 0.00062693
Iteration 19/25 | Loss: 0.00062693
Iteration 20/25 | Loss: 0.00062693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000626934866886586, 0.000626934866886586, 0.000626934866886586, 0.000626934866886586, 0.000626934866886586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000626934866886586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062693
Iteration 2/1000 | Loss: 0.00031215
Iteration 3/1000 | Loss: 0.00029289
Iteration 4/1000 | Loss: 0.00031700
Iteration 5/1000 | Loss: 0.00019848
Iteration 6/1000 | Loss: 0.00033516
Iteration 7/1000 | Loss: 0.00038480
Iteration 8/1000 | Loss: 0.00010628
Iteration 9/1000 | Loss: 0.00006466
Iteration 10/1000 | Loss: 0.00007914
Iteration 11/1000 | Loss: 0.00019190
Iteration 12/1000 | Loss: 0.00023661
Iteration 13/1000 | Loss: 0.00026015
Iteration 14/1000 | Loss: 0.00026609
Iteration 15/1000 | Loss: 0.00031419
Iteration 16/1000 | Loss: 0.00032486
Iteration 17/1000 | Loss: 0.00024332
Iteration 18/1000 | Loss: 0.00025392
Iteration 19/1000 | Loss: 0.00029266
Iteration 20/1000 | Loss: 0.00034855
Iteration 21/1000 | Loss: 0.00031753
Iteration 22/1000 | Loss: 0.00019132
Iteration 23/1000 | Loss: 0.00022548
Iteration 24/1000 | Loss: 0.00019483
Iteration 25/1000 | Loss: 0.00019777
Iteration 26/1000 | Loss: 0.00018954
Iteration 27/1000 | Loss: 0.00025393
Iteration 28/1000 | Loss: 0.00024408
Iteration 29/1000 | Loss: 0.00020244
Iteration 30/1000 | Loss: 0.00054719
Iteration 31/1000 | Loss: 0.00004307
Iteration 32/1000 | Loss: 0.00003745
Iteration 33/1000 | Loss: 0.00006981
Iteration 34/1000 | Loss: 0.00022809
Iteration 35/1000 | Loss: 0.00015791
Iteration 36/1000 | Loss: 0.00003425
Iteration 37/1000 | Loss: 0.00017040
Iteration 38/1000 | Loss: 0.00011846
Iteration 39/1000 | Loss: 0.00017821
Iteration 40/1000 | Loss: 0.00011239
Iteration 41/1000 | Loss: 0.00007143
Iteration 42/1000 | Loss: 0.00004658
Iteration 43/1000 | Loss: 0.00003269
Iteration 44/1000 | Loss: 0.00013921
Iteration 45/1000 | Loss: 0.00004314
Iteration 46/1000 | Loss: 0.00003601
Iteration 47/1000 | Loss: 0.00003387
Iteration 48/1000 | Loss: 0.00003294
Iteration 49/1000 | Loss: 0.00003233
Iteration 50/1000 | Loss: 0.00016856
Iteration 51/1000 | Loss: 0.00012721
Iteration 52/1000 | Loss: 0.00006286
Iteration 53/1000 | Loss: 0.00010049
Iteration 54/1000 | Loss: 0.00018862
Iteration 55/1000 | Loss: 0.00020361
Iteration 56/1000 | Loss: 0.00004158
Iteration 57/1000 | Loss: 0.00004630
Iteration 58/1000 | Loss: 0.00002926
Iteration 59/1000 | Loss: 0.00004154
Iteration 60/1000 | Loss: 0.00002932
Iteration 61/1000 | Loss: 0.00002849
Iteration 62/1000 | Loss: 0.00002782
Iteration 63/1000 | Loss: 0.00002721
Iteration 64/1000 | Loss: 0.00002676
Iteration 65/1000 | Loss: 0.00002648
Iteration 66/1000 | Loss: 0.00002640
Iteration 67/1000 | Loss: 0.00002623
Iteration 68/1000 | Loss: 0.00002618
Iteration 69/1000 | Loss: 0.00002609
Iteration 70/1000 | Loss: 0.00002607
Iteration 71/1000 | Loss: 0.00002606
Iteration 72/1000 | Loss: 0.00002605
Iteration 73/1000 | Loss: 0.00002604
Iteration 74/1000 | Loss: 0.00002604
Iteration 75/1000 | Loss: 0.00002603
Iteration 76/1000 | Loss: 0.00002602
Iteration 77/1000 | Loss: 0.00002601
Iteration 78/1000 | Loss: 0.00002590
Iteration 79/1000 | Loss: 0.00002588
Iteration 80/1000 | Loss: 0.00002578
Iteration 81/1000 | Loss: 0.00002577
Iteration 82/1000 | Loss: 0.00002577
Iteration 83/1000 | Loss: 0.00002576
Iteration 84/1000 | Loss: 0.00002575
Iteration 85/1000 | Loss: 0.00002575
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002571
Iteration 88/1000 | Loss: 0.00002570
Iteration 89/1000 | Loss: 0.00002570
Iteration 90/1000 | Loss: 0.00002569
Iteration 91/1000 | Loss: 0.00002568
Iteration 92/1000 | Loss: 0.00002568
Iteration 93/1000 | Loss: 0.00002568
Iteration 94/1000 | Loss: 0.00002568
Iteration 95/1000 | Loss: 0.00002568
Iteration 96/1000 | Loss: 0.00002568
Iteration 97/1000 | Loss: 0.00002568
Iteration 98/1000 | Loss: 0.00002568
Iteration 99/1000 | Loss: 0.00002568
Iteration 100/1000 | Loss: 0.00002567
Iteration 101/1000 | Loss: 0.00002567
Iteration 102/1000 | Loss: 0.00002567
Iteration 103/1000 | Loss: 0.00002567
Iteration 104/1000 | Loss: 0.00002567
Iteration 105/1000 | Loss: 0.00002567
Iteration 106/1000 | Loss: 0.00002567
Iteration 107/1000 | Loss: 0.00002567
Iteration 108/1000 | Loss: 0.00002567
Iteration 109/1000 | Loss: 0.00002567
Iteration 110/1000 | Loss: 0.00002567
Iteration 111/1000 | Loss: 0.00002566
Iteration 112/1000 | Loss: 0.00002566
Iteration 113/1000 | Loss: 0.00002566
Iteration 114/1000 | Loss: 0.00002565
Iteration 115/1000 | Loss: 0.00002565
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002564
Iteration 118/1000 | Loss: 0.00002564
Iteration 119/1000 | Loss: 0.00002564
Iteration 120/1000 | Loss: 0.00002564
Iteration 121/1000 | Loss: 0.00002564
Iteration 122/1000 | Loss: 0.00002564
Iteration 123/1000 | Loss: 0.00002564
Iteration 124/1000 | Loss: 0.00002564
Iteration 125/1000 | Loss: 0.00002564
Iteration 126/1000 | Loss: 0.00002564
Iteration 127/1000 | Loss: 0.00002564
Iteration 128/1000 | Loss: 0.00002564
Iteration 129/1000 | Loss: 0.00002563
Iteration 130/1000 | Loss: 0.00002563
Iteration 131/1000 | Loss: 0.00002563
Iteration 132/1000 | Loss: 0.00002563
Iteration 133/1000 | Loss: 0.00002563
Iteration 134/1000 | Loss: 0.00002563
Iteration 135/1000 | Loss: 0.00002562
Iteration 136/1000 | Loss: 0.00002562
Iteration 137/1000 | Loss: 0.00002562
Iteration 138/1000 | Loss: 0.00002562
Iteration 139/1000 | Loss: 0.00002561
Iteration 140/1000 | Loss: 0.00002561
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002561
Iteration 143/1000 | Loss: 0.00002561
Iteration 144/1000 | Loss: 0.00002560
Iteration 145/1000 | Loss: 0.00002560
Iteration 146/1000 | Loss: 0.00002560
Iteration 147/1000 | Loss: 0.00002560
Iteration 148/1000 | Loss: 0.00002560
Iteration 149/1000 | Loss: 0.00002560
Iteration 150/1000 | Loss: 0.00002560
Iteration 151/1000 | Loss: 0.00002559
Iteration 152/1000 | Loss: 0.00002559
Iteration 153/1000 | Loss: 0.00002559
Iteration 154/1000 | Loss: 0.00002559
Iteration 155/1000 | Loss: 0.00002559
Iteration 156/1000 | Loss: 0.00002559
Iteration 157/1000 | Loss: 0.00002559
Iteration 158/1000 | Loss: 0.00002559
Iteration 159/1000 | Loss: 0.00002559
Iteration 160/1000 | Loss: 0.00002559
Iteration 161/1000 | Loss: 0.00002559
Iteration 162/1000 | Loss: 0.00002558
Iteration 163/1000 | Loss: 0.00002558
Iteration 164/1000 | Loss: 0.00002558
Iteration 165/1000 | Loss: 0.00002558
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002557
Iteration 168/1000 | Loss: 0.00002557
Iteration 169/1000 | Loss: 0.00002557
Iteration 170/1000 | Loss: 0.00002557
Iteration 171/1000 | Loss: 0.00002557
Iteration 172/1000 | Loss: 0.00002557
Iteration 173/1000 | Loss: 0.00002557
Iteration 174/1000 | Loss: 0.00002557
Iteration 175/1000 | Loss: 0.00002556
Iteration 176/1000 | Loss: 0.00002556
Iteration 177/1000 | Loss: 0.00002556
Iteration 178/1000 | Loss: 0.00002556
Iteration 179/1000 | Loss: 0.00002556
Iteration 180/1000 | Loss: 0.00002556
Iteration 181/1000 | Loss: 0.00002556
Iteration 182/1000 | Loss: 0.00002556
Iteration 183/1000 | Loss: 0.00002556
Iteration 184/1000 | Loss: 0.00002556
Iteration 185/1000 | Loss: 0.00002556
Iteration 186/1000 | Loss: 0.00002556
Iteration 187/1000 | Loss: 0.00002556
Iteration 188/1000 | Loss: 0.00002556
Iteration 189/1000 | Loss: 0.00002556
Iteration 190/1000 | Loss: 0.00002556
Iteration 191/1000 | Loss: 0.00002555
Iteration 192/1000 | Loss: 0.00002555
Iteration 193/1000 | Loss: 0.00002555
Iteration 194/1000 | Loss: 0.00002555
Iteration 195/1000 | Loss: 0.00002555
Iteration 196/1000 | Loss: 0.00002555
Iteration 197/1000 | Loss: 0.00002555
Iteration 198/1000 | Loss: 0.00002555
Iteration 199/1000 | Loss: 0.00002555
Iteration 200/1000 | Loss: 0.00002555
Iteration 201/1000 | Loss: 0.00002555
Iteration 202/1000 | Loss: 0.00002555
Iteration 203/1000 | Loss: 0.00002555
Iteration 204/1000 | Loss: 0.00002555
Iteration 205/1000 | Loss: 0.00002555
Iteration 206/1000 | Loss: 0.00002555
Iteration 207/1000 | Loss: 0.00002555
Iteration 208/1000 | Loss: 0.00002555
Iteration 209/1000 | Loss: 0.00002555
Iteration 210/1000 | Loss: 0.00002555
Iteration 211/1000 | Loss: 0.00002555
Iteration 212/1000 | Loss: 0.00002555
Iteration 213/1000 | Loss: 0.00002555
Iteration 214/1000 | Loss: 0.00002555
Iteration 215/1000 | Loss: 0.00002555
Iteration 216/1000 | Loss: 0.00002555
Iteration 217/1000 | Loss: 0.00002555
Iteration 218/1000 | Loss: 0.00002555
Iteration 219/1000 | Loss: 0.00002555
Iteration 220/1000 | Loss: 0.00002555
Iteration 221/1000 | Loss: 0.00002555
Iteration 222/1000 | Loss: 0.00002555
Iteration 223/1000 | Loss: 0.00002555
Iteration 224/1000 | Loss: 0.00002555
Iteration 225/1000 | Loss: 0.00002555
Iteration 226/1000 | Loss: 0.00002555
Iteration 227/1000 | Loss: 0.00002555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.5546902179485187e-05, 2.5546902179485187e-05, 2.5546902179485187e-05, 2.5546902179485187e-05, 2.5546902179485187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5546902179485187e-05

Optimization complete. Final v2v error: 4.1181745529174805 mm

Highest mean error: 5.917535781860352 mm for frame 115

Lowest mean error: 3.735553503036499 mm for frame 191

Saving results

Total time: 156.21420645713806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391022
Iteration 2/25 | Loss: 0.00075394
Iteration 3/25 | Loss: 0.00064050
Iteration 4/25 | Loss: 0.00061363
Iteration 5/25 | Loss: 0.00060918
Iteration 6/25 | Loss: 0.00060803
Iteration 7/25 | Loss: 0.00060779
Iteration 8/25 | Loss: 0.00060779
Iteration 9/25 | Loss: 0.00060779
Iteration 10/25 | Loss: 0.00060779
Iteration 11/25 | Loss: 0.00060779
Iteration 12/25 | Loss: 0.00060779
Iteration 13/25 | Loss: 0.00060779
Iteration 14/25 | Loss: 0.00060779
Iteration 15/25 | Loss: 0.00060779
Iteration 16/25 | Loss: 0.00060779
Iteration 17/25 | Loss: 0.00060779
Iteration 18/25 | Loss: 0.00060779
Iteration 19/25 | Loss: 0.00060779
Iteration 20/25 | Loss: 0.00060779
Iteration 21/25 | Loss: 0.00060779
Iteration 22/25 | Loss: 0.00060779
Iteration 23/25 | Loss: 0.00060779
Iteration 24/25 | Loss: 0.00060779
Iteration 25/25 | Loss: 0.00060779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.91659880
Iteration 2/25 | Loss: 0.00028811
Iteration 3/25 | Loss: 0.00028810
Iteration 4/25 | Loss: 0.00028810
Iteration 5/25 | Loss: 0.00028810
Iteration 6/25 | Loss: 0.00028810
Iteration 7/25 | Loss: 0.00028810
Iteration 8/25 | Loss: 0.00028810
Iteration 9/25 | Loss: 0.00028810
Iteration 10/25 | Loss: 0.00028810
Iteration 11/25 | Loss: 0.00028810
Iteration 12/25 | Loss: 0.00028810
Iteration 13/25 | Loss: 0.00028810
Iteration 14/25 | Loss: 0.00028810
Iteration 15/25 | Loss: 0.00028810
Iteration 16/25 | Loss: 0.00028810
Iteration 17/25 | Loss: 0.00028810
Iteration 18/25 | Loss: 0.00028810
Iteration 19/25 | Loss: 0.00028810
Iteration 20/25 | Loss: 0.00028810
Iteration 21/25 | Loss: 0.00028810
Iteration 22/25 | Loss: 0.00028810
Iteration 23/25 | Loss: 0.00028810
Iteration 24/25 | Loss: 0.00028810
Iteration 25/25 | Loss: 0.00028810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028810
Iteration 2/1000 | Loss: 0.00002851
Iteration 3/1000 | Loss: 0.00001746
Iteration 4/1000 | Loss: 0.00001614
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001487
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001410
Iteration 10/1000 | Loss: 0.00001408
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001397
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001380
Iteration 22/1000 | Loss: 0.00001379
Iteration 23/1000 | Loss: 0.00001379
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001377
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001376
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001373
Iteration 34/1000 | Loss: 0.00001373
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001363
Iteration 61/1000 | Loss: 0.00001363
Iteration 62/1000 | Loss: 0.00001363
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001362
Iteration 70/1000 | Loss: 0.00001362
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001361
Iteration 74/1000 | Loss: 0.00001361
Iteration 75/1000 | Loss: 0.00001360
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001360
Iteration 78/1000 | Loss: 0.00001360
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001354
Iteration 98/1000 | Loss: 0.00001354
Iteration 99/1000 | Loss: 0.00001354
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001353
Iteration 103/1000 | Loss: 0.00001353
Iteration 104/1000 | Loss: 0.00001353
Iteration 105/1000 | Loss: 0.00001353
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001350
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001350
Iteration 115/1000 | Loss: 0.00001350
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001350
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001350
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001350
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001350
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001350
Iteration 140/1000 | Loss: 0.00001350
Iteration 141/1000 | Loss: 0.00001350
Iteration 142/1000 | Loss: 0.00001350
Iteration 143/1000 | Loss: 0.00001350
Iteration 144/1000 | Loss: 0.00001350
Iteration 145/1000 | Loss: 0.00001350
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001350
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001350
Iteration 166/1000 | Loss: 0.00001350
Iteration 167/1000 | Loss: 0.00001350
Iteration 168/1000 | Loss: 0.00001350
Iteration 169/1000 | Loss: 0.00001350
Iteration 170/1000 | Loss: 0.00001350
Iteration 171/1000 | Loss: 0.00001350
Iteration 172/1000 | Loss: 0.00001350
Iteration 173/1000 | Loss: 0.00001350
Iteration 174/1000 | Loss: 0.00001350
Iteration 175/1000 | Loss: 0.00001350
Iteration 176/1000 | Loss: 0.00001350
Iteration 177/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.34957954287529e-05, 1.34957954287529e-05, 1.34957954287529e-05, 1.34957954287529e-05, 1.34957954287529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.34957954287529e-05

Optimization complete. Final v2v error: 3.14027738571167 mm

Highest mean error: 3.3664209842681885 mm for frame 45

Lowest mean error: 2.9330942630767822 mm for frame 88

Saving results

Total time: 34.935431718826294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806325
Iteration 2/25 | Loss: 0.00101820
Iteration 3/25 | Loss: 0.00080273
Iteration 4/25 | Loss: 0.00075338
Iteration 5/25 | Loss: 0.00073569
Iteration 6/25 | Loss: 0.00073067
Iteration 7/25 | Loss: 0.00072937
Iteration 8/25 | Loss: 0.00072907
Iteration 9/25 | Loss: 0.00072907
Iteration 10/25 | Loss: 0.00072895
Iteration 11/25 | Loss: 0.00072895
Iteration 12/25 | Loss: 0.00072895
Iteration 13/25 | Loss: 0.00072895
Iteration 14/25 | Loss: 0.00072895
Iteration 15/25 | Loss: 0.00072895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007289506029337645, 0.0007289506029337645, 0.0007289506029337645, 0.0007289506029337645, 0.0007289506029337645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007289506029337645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47796297
Iteration 2/25 | Loss: 0.00034148
Iteration 3/25 | Loss: 0.00034146
Iteration 4/25 | Loss: 0.00034146
Iteration 5/25 | Loss: 0.00034146
Iteration 6/25 | Loss: 0.00034146
Iteration 7/25 | Loss: 0.00034146
Iteration 8/25 | Loss: 0.00034146
Iteration 9/25 | Loss: 0.00034146
Iteration 10/25 | Loss: 0.00034146
Iteration 11/25 | Loss: 0.00034146
Iteration 12/25 | Loss: 0.00034146
Iteration 13/25 | Loss: 0.00034146
Iteration 14/25 | Loss: 0.00034146
Iteration 15/25 | Loss: 0.00034146
Iteration 16/25 | Loss: 0.00034146
Iteration 17/25 | Loss: 0.00034146
Iteration 18/25 | Loss: 0.00034146
Iteration 19/25 | Loss: 0.00034146
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00034146165126003325, 0.00034146165126003325, 0.00034146165126003325, 0.00034146165126003325, 0.00034146165126003325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034146165126003325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034146
Iteration 2/1000 | Loss: 0.00004737
Iteration 3/1000 | Loss: 0.00003322
Iteration 4/1000 | Loss: 0.00003032
Iteration 5/1000 | Loss: 0.00002869
Iteration 6/1000 | Loss: 0.00002734
Iteration 7/1000 | Loss: 0.00002674
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002585
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002534
Iteration 12/1000 | Loss: 0.00002517
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002513
Iteration 15/1000 | Loss: 0.00002510
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002508
Iteration 18/1000 | Loss: 0.00002508
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002506
Iteration 21/1000 | Loss: 0.00002506
Iteration 22/1000 | Loss: 0.00002506
Iteration 23/1000 | Loss: 0.00002506
Iteration 24/1000 | Loss: 0.00002506
Iteration 25/1000 | Loss: 0.00002505
Iteration 26/1000 | Loss: 0.00002505
Iteration 27/1000 | Loss: 0.00002504
Iteration 28/1000 | Loss: 0.00002504
Iteration 29/1000 | Loss: 0.00002504
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002503
Iteration 32/1000 | Loss: 0.00002502
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002501
Iteration 37/1000 | Loss: 0.00002500
Iteration 38/1000 | Loss: 0.00002500
Iteration 39/1000 | Loss: 0.00002499
Iteration 40/1000 | Loss: 0.00002499
Iteration 41/1000 | Loss: 0.00002499
Iteration 42/1000 | Loss: 0.00002498
Iteration 43/1000 | Loss: 0.00002498
Iteration 44/1000 | Loss: 0.00002498
Iteration 45/1000 | Loss: 0.00002497
Iteration 46/1000 | Loss: 0.00002497
Iteration 47/1000 | Loss: 0.00002497
Iteration 48/1000 | Loss: 0.00002497
Iteration 49/1000 | Loss: 0.00002497
Iteration 50/1000 | Loss: 0.00002497
Iteration 51/1000 | Loss: 0.00002497
Iteration 52/1000 | Loss: 0.00002496
Iteration 53/1000 | Loss: 0.00002496
Iteration 54/1000 | Loss: 0.00002496
Iteration 55/1000 | Loss: 0.00002496
Iteration 56/1000 | Loss: 0.00002496
Iteration 57/1000 | Loss: 0.00002496
Iteration 58/1000 | Loss: 0.00002496
Iteration 59/1000 | Loss: 0.00002496
Iteration 60/1000 | Loss: 0.00002496
Iteration 61/1000 | Loss: 0.00002496
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002496
Iteration 64/1000 | Loss: 0.00002496
Iteration 65/1000 | Loss: 0.00002496
Iteration 66/1000 | Loss: 0.00002496
Iteration 67/1000 | Loss: 0.00002496
Iteration 68/1000 | Loss: 0.00002496
Iteration 69/1000 | Loss: 0.00002496
Iteration 70/1000 | Loss: 0.00002496
Iteration 71/1000 | Loss: 0.00002496
Iteration 72/1000 | Loss: 0.00002496
Iteration 73/1000 | Loss: 0.00002496
Iteration 74/1000 | Loss: 0.00002496
Iteration 75/1000 | Loss: 0.00002496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.4963466785266064e-05, 2.4963466785266064e-05, 2.4963466785266064e-05, 2.4963466785266064e-05, 2.4963466785266064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4963466785266064e-05

Optimization complete. Final v2v error: 4.103898048400879 mm

Highest mean error: 4.986483097076416 mm for frame 153

Lowest mean error: 3.694673776626587 mm for frame 168

Saving results

Total time: 33.18126106262207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451396
Iteration 2/25 | Loss: 0.00085153
Iteration 3/25 | Loss: 0.00073567
Iteration 4/25 | Loss: 0.00071368
Iteration 5/25 | Loss: 0.00070673
Iteration 6/25 | Loss: 0.00070560
Iteration 7/25 | Loss: 0.00070554
Iteration 8/25 | Loss: 0.00070554
Iteration 9/25 | Loss: 0.00070554
Iteration 10/25 | Loss: 0.00070554
Iteration 11/25 | Loss: 0.00070554
Iteration 12/25 | Loss: 0.00070554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007055418100208044, 0.0007055418100208044, 0.0007055418100208044, 0.0007055418100208044, 0.0007055418100208044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007055418100208044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42785704
Iteration 2/25 | Loss: 0.00027848
Iteration 3/25 | Loss: 0.00027847
Iteration 4/25 | Loss: 0.00027847
Iteration 5/25 | Loss: 0.00027847
Iteration 6/25 | Loss: 0.00027847
Iteration 7/25 | Loss: 0.00027847
Iteration 8/25 | Loss: 0.00027847
Iteration 9/25 | Loss: 0.00027847
Iteration 10/25 | Loss: 0.00027847
Iteration 11/25 | Loss: 0.00027847
Iteration 12/25 | Loss: 0.00027847
Iteration 13/25 | Loss: 0.00027847
Iteration 14/25 | Loss: 0.00027847
Iteration 15/25 | Loss: 0.00027847
Iteration 16/25 | Loss: 0.00027847
Iteration 17/25 | Loss: 0.00027847
Iteration 18/25 | Loss: 0.00027847
Iteration 19/25 | Loss: 0.00027847
Iteration 20/25 | Loss: 0.00027847
Iteration 21/25 | Loss: 0.00027847
Iteration 22/25 | Loss: 0.00027847
Iteration 23/25 | Loss: 0.00027847
Iteration 24/25 | Loss: 0.00027847
Iteration 25/25 | Loss: 0.00027847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027847
Iteration 2/1000 | Loss: 0.00005509
Iteration 3/1000 | Loss: 0.00003610
Iteration 4/1000 | Loss: 0.00003309
Iteration 5/1000 | Loss: 0.00003089
Iteration 6/1000 | Loss: 0.00002961
Iteration 7/1000 | Loss: 0.00002868
Iteration 8/1000 | Loss: 0.00002800
Iteration 9/1000 | Loss: 0.00002767
Iteration 10/1000 | Loss: 0.00002744
Iteration 11/1000 | Loss: 0.00002726
Iteration 12/1000 | Loss: 0.00002717
Iteration 13/1000 | Loss: 0.00002715
Iteration 14/1000 | Loss: 0.00002711
Iteration 15/1000 | Loss: 0.00002708
Iteration 16/1000 | Loss: 0.00002707
Iteration 17/1000 | Loss: 0.00002707
Iteration 18/1000 | Loss: 0.00002707
Iteration 19/1000 | Loss: 0.00002706
Iteration 20/1000 | Loss: 0.00002706
Iteration 21/1000 | Loss: 0.00002705
Iteration 22/1000 | Loss: 0.00002704
Iteration 23/1000 | Loss: 0.00002704
Iteration 24/1000 | Loss: 0.00002704
Iteration 25/1000 | Loss: 0.00002704
Iteration 26/1000 | Loss: 0.00002704
Iteration 27/1000 | Loss: 0.00002703
Iteration 28/1000 | Loss: 0.00002702
Iteration 29/1000 | Loss: 0.00002702
Iteration 30/1000 | Loss: 0.00002701
Iteration 31/1000 | Loss: 0.00002701
Iteration 32/1000 | Loss: 0.00002700
Iteration 33/1000 | Loss: 0.00002700
Iteration 34/1000 | Loss: 0.00002698
Iteration 35/1000 | Loss: 0.00002696
Iteration 36/1000 | Loss: 0.00002686
Iteration 37/1000 | Loss: 0.00002683
Iteration 38/1000 | Loss: 0.00002682
Iteration 39/1000 | Loss: 0.00002682
Iteration 40/1000 | Loss: 0.00002682
Iteration 41/1000 | Loss: 0.00002681
Iteration 42/1000 | Loss: 0.00002681
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002678
Iteration 45/1000 | Loss: 0.00002678
Iteration 46/1000 | Loss: 0.00002677
Iteration 47/1000 | Loss: 0.00002677
Iteration 48/1000 | Loss: 0.00002677
Iteration 49/1000 | Loss: 0.00002677
Iteration 50/1000 | Loss: 0.00002677
Iteration 51/1000 | Loss: 0.00002676
Iteration 52/1000 | Loss: 0.00002676
Iteration 53/1000 | Loss: 0.00002676
Iteration 54/1000 | Loss: 0.00002676
Iteration 55/1000 | Loss: 0.00002676
Iteration 56/1000 | Loss: 0.00002676
Iteration 57/1000 | Loss: 0.00002676
Iteration 58/1000 | Loss: 0.00002675
Iteration 59/1000 | Loss: 0.00002675
Iteration 60/1000 | Loss: 0.00002675
Iteration 61/1000 | Loss: 0.00002675
Iteration 62/1000 | Loss: 0.00002675
Iteration 63/1000 | Loss: 0.00002675
Iteration 64/1000 | Loss: 0.00002674
Iteration 65/1000 | Loss: 0.00002674
Iteration 66/1000 | Loss: 0.00002674
Iteration 67/1000 | Loss: 0.00002674
Iteration 68/1000 | Loss: 0.00002674
Iteration 69/1000 | Loss: 0.00002674
Iteration 70/1000 | Loss: 0.00002674
Iteration 71/1000 | Loss: 0.00002674
Iteration 72/1000 | Loss: 0.00002674
Iteration 73/1000 | Loss: 0.00002673
Iteration 74/1000 | Loss: 0.00002673
Iteration 75/1000 | Loss: 0.00002673
Iteration 76/1000 | Loss: 0.00002673
Iteration 77/1000 | Loss: 0.00002673
Iteration 78/1000 | Loss: 0.00002673
Iteration 79/1000 | Loss: 0.00002673
Iteration 80/1000 | Loss: 0.00002672
Iteration 81/1000 | Loss: 0.00002672
Iteration 82/1000 | Loss: 0.00002672
Iteration 83/1000 | Loss: 0.00002672
Iteration 84/1000 | Loss: 0.00002671
Iteration 85/1000 | Loss: 0.00002671
Iteration 86/1000 | Loss: 0.00002671
Iteration 87/1000 | Loss: 0.00002671
Iteration 88/1000 | Loss: 0.00002671
Iteration 89/1000 | Loss: 0.00002671
Iteration 90/1000 | Loss: 0.00002671
Iteration 91/1000 | Loss: 0.00002671
Iteration 92/1000 | Loss: 0.00002671
Iteration 93/1000 | Loss: 0.00002671
Iteration 94/1000 | Loss: 0.00002671
Iteration 95/1000 | Loss: 0.00002671
Iteration 96/1000 | Loss: 0.00002671
Iteration 97/1000 | Loss: 0.00002670
Iteration 98/1000 | Loss: 0.00002670
Iteration 99/1000 | Loss: 0.00002670
Iteration 100/1000 | Loss: 0.00002669
Iteration 101/1000 | Loss: 0.00002669
Iteration 102/1000 | Loss: 0.00002669
Iteration 103/1000 | Loss: 0.00002669
Iteration 104/1000 | Loss: 0.00002669
Iteration 105/1000 | Loss: 0.00002669
Iteration 106/1000 | Loss: 0.00002669
Iteration 107/1000 | Loss: 0.00002669
Iteration 108/1000 | Loss: 0.00002669
Iteration 109/1000 | Loss: 0.00002669
Iteration 110/1000 | Loss: 0.00002669
Iteration 111/1000 | Loss: 0.00002669
Iteration 112/1000 | Loss: 0.00002669
Iteration 113/1000 | Loss: 0.00002668
Iteration 114/1000 | Loss: 0.00002668
Iteration 115/1000 | Loss: 0.00002668
Iteration 116/1000 | Loss: 0.00002668
Iteration 117/1000 | Loss: 0.00002667
Iteration 118/1000 | Loss: 0.00002667
Iteration 119/1000 | Loss: 0.00002667
Iteration 120/1000 | Loss: 0.00002667
Iteration 121/1000 | Loss: 0.00002667
Iteration 122/1000 | Loss: 0.00002666
Iteration 123/1000 | Loss: 0.00002666
Iteration 124/1000 | Loss: 0.00002666
Iteration 125/1000 | Loss: 0.00002666
Iteration 126/1000 | Loss: 0.00002666
Iteration 127/1000 | Loss: 0.00002666
Iteration 128/1000 | Loss: 0.00002666
Iteration 129/1000 | Loss: 0.00002665
Iteration 130/1000 | Loss: 0.00002665
Iteration 131/1000 | Loss: 0.00002665
Iteration 132/1000 | Loss: 0.00002665
Iteration 133/1000 | Loss: 0.00002665
Iteration 134/1000 | Loss: 0.00002665
Iteration 135/1000 | Loss: 0.00002665
Iteration 136/1000 | Loss: 0.00002665
Iteration 137/1000 | Loss: 0.00002665
Iteration 138/1000 | Loss: 0.00002665
Iteration 139/1000 | Loss: 0.00002664
Iteration 140/1000 | Loss: 0.00002664
Iteration 141/1000 | Loss: 0.00002664
Iteration 142/1000 | Loss: 0.00002664
Iteration 143/1000 | Loss: 0.00002664
Iteration 144/1000 | Loss: 0.00002664
Iteration 145/1000 | Loss: 0.00002664
Iteration 146/1000 | Loss: 0.00002664
Iteration 147/1000 | Loss: 0.00002664
Iteration 148/1000 | Loss: 0.00002664
Iteration 149/1000 | Loss: 0.00002664
Iteration 150/1000 | Loss: 0.00002664
Iteration 151/1000 | Loss: 0.00002664
Iteration 152/1000 | Loss: 0.00002664
Iteration 153/1000 | Loss: 0.00002664
Iteration 154/1000 | Loss: 0.00002664
Iteration 155/1000 | Loss: 0.00002664
Iteration 156/1000 | Loss: 0.00002664
Iteration 157/1000 | Loss: 0.00002664
Iteration 158/1000 | Loss: 0.00002664
Iteration 159/1000 | Loss: 0.00002664
Iteration 160/1000 | Loss: 0.00002664
Iteration 161/1000 | Loss: 0.00002664
Iteration 162/1000 | Loss: 0.00002664
Iteration 163/1000 | Loss: 0.00002664
Iteration 164/1000 | Loss: 0.00002664
Iteration 165/1000 | Loss: 0.00002664
Iteration 166/1000 | Loss: 0.00002664
Iteration 167/1000 | Loss: 0.00002664
Iteration 168/1000 | Loss: 0.00002664
Iteration 169/1000 | Loss: 0.00002664
Iteration 170/1000 | Loss: 0.00002664
Iteration 171/1000 | Loss: 0.00002664
Iteration 172/1000 | Loss: 0.00002664
Iteration 173/1000 | Loss: 0.00002664
Iteration 174/1000 | Loss: 0.00002664
Iteration 175/1000 | Loss: 0.00002664
Iteration 176/1000 | Loss: 0.00002664
Iteration 177/1000 | Loss: 0.00002664
Iteration 178/1000 | Loss: 0.00002664
Iteration 179/1000 | Loss: 0.00002664
Iteration 180/1000 | Loss: 0.00002664
Iteration 181/1000 | Loss: 0.00002664
Iteration 182/1000 | Loss: 0.00002664
Iteration 183/1000 | Loss: 0.00002664
Iteration 184/1000 | Loss: 0.00002664
Iteration 185/1000 | Loss: 0.00002664
Iteration 186/1000 | Loss: 0.00002664
Iteration 187/1000 | Loss: 0.00002664
Iteration 188/1000 | Loss: 0.00002664
Iteration 189/1000 | Loss: 0.00002664
Iteration 190/1000 | Loss: 0.00002664
Iteration 191/1000 | Loss: 0.00002664
Iteration 192/1000 | Loss: 0.00002664
Iteration 193/1000 | Loss: 0.00002664
Iteration 194/1000 | Loss: 0.00002664
Iteration 195/1000 | Loss: 0.00002664
Iteration 196/1000 | Loss: 0.00002664
Iteration 197/1000 | Loss: 0.00002664
Iteration 198/1000 | Loss: 0.00002664
Iteration 199/1000 | Loss: 0.00002664
Iteration 200/1000 | Loss: 0.00002664
Iteration 201/1000 | Loss: 0.00002664
Iteration 202/1000 | Loss: 0.00002664
Iteration 203/1000 | Loss: 0.00002664
Iteration 204/1000 | Loss: 0.00002664
Iteration 205/1000 | Loss: 0.00002664
Iteration 206/1000 | Loss: 0.00002664
Iteration 207/1000 | Loss: 0.00002664
Iteration 208/1000 | Loss: 0.00002664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.6638184863259085e-05, 2.6638184863259085e-05, 2.6638184863259085e-05, 2.6638184863259085e-05, 2.6638184863259085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6638184863259085e-05

Optimization complete. Final v2v error: 4.292457103729248 mm

Highest mean error: 4.492974758148193 mm for frame 131

Lowest mean error: 3.98797869682312 mm for frame 44

Saving results

Total time: 38.58767795562744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802912
Iteration 2/25 | Loss: 0.00101625
Iteration 3/25 | Loss: 0.00072505
Iteration 4/25 | Loss: 0.00066796
Iteration 5/25 | Loss: 0.00065602
Iteration 6/25 | Loss: 0.00065287
Iteration 7/25 | Loss: 0.00065244
Iteration 8/25 | Loss: 0.00065244
Iteration 9/25 | Loss: 0.00065244
Iteration 10/25 | Loss: 0.00065244
Iteration 11/25 | Loss: 0.00065244
Iteration 12/25 | Loss: 0.00065244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006524402415379882, 0.0006524402415379882, 0.0006524402415379882, 0.0006524402415379882, 0.0006524402415379882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006524402415379882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44425356
Iteration 2/25 | Loss: 0.00032323
Iteration 3/25 | Loss: 0.00032322
Iteration 4/25 | Loss: 0.00032322
Iteration 5/25 | Loss: 0.00032322
Iteration 6/25 | Loss: 0.00032322
Iteration 7/25 | Loss: 0.00032322
Iteration 8/25 | Loss: 0.00032322
Iteration 9/25 | Loss: 0.00032322
Iteration 10/25 | Loss: 0.00032322
Iteration 11/25 | Loss: 0.00032322
Iteration 12/25 | Loss: 0.00032322
Iteration 13/25 | Loss: 0.00032322
Iteration 14/25 | Loss: 0.00032322
Iteration 15/25 | Loss: 0.00032322
Iteration 16/25 | Loss: 0.00032322
Iteration 17/25 | Loss: 0.00032322
Iteration 18/25 | Loss: 0.00032322
Iteration 19/25 | Loss: 0.00032322
Iteration 20/25 | Loss: 0.00032322
Iteration 21/25 | Loss: 0.00032322
Iteration 22/25 | Loss: 0.00032322
Iteration 23/25 | Loss: 0.00032322
Iteration 24/25 | Loss: 0.00032322
Iteration 25/25 | Loss: 0.00032322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032322
Iteration 2/1000 | Loss: 0.00002397
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001784
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001653
Iteration 7/1000 | Loss: 0.00001617
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001543
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001538
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001533
Iteration 27/1000 | Loss: 0.00001532
Iteration 28/1000 | Loss: 0.00001531
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001518
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001516
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001515
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001514
Iteration 89/1000 | Loss: 0.00001514
Iteration 90/1000 | Loss: 0.00001514
Iteration 91/1000 | Loss: 0.00001514
Iteration 92/1000 | Loss: 0.00001514
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001513
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001512
Iteration 104/1000 | Loss: 0.00001512
Iteration 105/1000 | Loss: 0.00001512
Iteration 106/1000 | Loss: 0.00001511
Iteration 107/1000 | Loss: 0.00001511
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001509
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001507
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001507
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001507
Iteration 143/1000 | Loss: 0.00001507
Iteration 144/1000 | Loss: 0.00001507
Iteration 145/1000 | Loss: 0.00001507
Iteration 146/1000 | Loss: 0.00001507
Iteration 147/1000 | Loss: 0.00001507
Iteration 148/1000 | Loss: 0.00001507
Iteration 149/1000 | Loss: 0.00001507
Iteration 150/1000 | Loss: 0.00001507
Iteration 151/1000 | Loss: 0.00001507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.5073234862938989e-05, 1.5073234862938989e-05, 1.5073234862938989e-05, 1.5073234862938989e-05, 1.5073234862938989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5073234862938989e-05

Optimization complete. Final v2v error: 3.2295944690704346 mm

Highest mean error: 3.765867233276367 mm for frame 159

Lowest mean error: 2.84712815284729 mm for frame 5

Saving results

Total time: 39.91479468345642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629700
Iteration 2/25 | Loss: 0.00076812
Iteration 3/25 | Loss: 0.00065534
Iteration 4/25 | Loss: 0.00063108
Iteration 5/25 | Loss: 0.00061989
Iteration 6/25 | Loss: 0.00061820
Iteration 7/25 | Loss: 0.00061786
Iteration 8/25 | Loss: 0.00061786
Iteration 9/25 | Loss: 0.00061785
Iteration 10/25 | Loss: 0.00061785
Iteration 11/25 | Loss: 0.00061785
Iteration 12/25 | Loss: 0.00061785
Iteration 13/25 | Loss: 0.00061785
Iteration 14/25 | Loss: 0.00061785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006178506300784647, 0.0006178506300784647, 0.0006178506300784647, 0.0006178506300784647, 0.0006178506300784647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006178506300784647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75501156
Iteration 2/25 | Loss: 0.00028906
Iteration 3/25 | Loss: 0.00028906
Iteration 4/25 | Loss: 0.00028906
Iteration 5/25 | Loss: 0.00028905
Iteration 6/25 | Loss: 0.00028905
Iteration 7/25 | Loss: 0.00028905
Iteration 8/25 | Loss: 0.00028905
Iteration 9/25 | Loss: 0.00028905
Iteration 10/25 | Loss: 0.00028905
Iteration 11/25 | Loss: 0.00028905
Iteration 12/25 | Loss: 0.00028905
Iteration 13/25 | Loss: 0.00028905
Iteration 14/25 | Loss: 0.00028905
Iteration 15/25 | Loss: 0.00028905
Iteration 16/25 | Loss: 0.00028905
Iteration 17/25 | Loss: 0.00028905
Iteration 18/25 | Loss: 0.00028905
Iteration 19/25 | Loss: 0.00028905
Iteration 20/25 | Loss: 0.00028905
Iteration 21/25 | Loss: 0.00028905
Iteration 22/25 | Loss: 0.00028905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00028905324870720506, 0.00028905324870720506, 0.00028905324870720506, 0.00028905324870720506, 0.00028905324870720506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028905324870720506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028905
Iteration 2/1000 | Loss: 0.00002664
Iteration 3/1000 | Loss: 0.00001755
Iteration 4/1000 | Loss: 0.00001582
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001425
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001401
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001341
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001337
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001336
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001328
Iteration 103/1000 | Loss: 0.00001328
Iteration 104/1000 | Loss: 0.00001328
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001326
Iteration 122/1000 | Loss: 0.00001326
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001325
Iteration 130/1000 | Loss: 0.00001325
Iteration 131/1000 | Loss: 0.00001325
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001324
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Iteration 147/1000 | Loss: 0.00001324
Iteration 148/1000 | Loss: 0.00001324
Iteration 149/1000 | Loss: 0.00001324
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Iteration 152/1000 | Loss: 0.00001323
Iteration 153/1000 | Loss: 0.00001323
Iteration 154/1000 | Loss: 0.00001323
Iteration 155/1000 | Loss: 0.00001323
Iteration 156/1000 | Loss: 0.00001323
Iteration 157/1000 | Loss: 0.00001323
Iteration 158/1000 | Loss: 0.00001323
Iteration 159/1000 | Loss: 0.00001323
Iteration 160/1000 | Loss: 0.00001323
Iteration 161/1000 | Loss: 0.00001323
Iteration 162/1000 | Loss: 0.00001323
Iteration 163/1000 | Loss: 0.00001323
Iteration 164/1000 | Loss: 0.00001322
Iteration 165/1000 | Loss: 0.00001322
Iteration 166/1000 | Loss: 0.00001322
Iteration 167/1000 | Loss: 0.00001322
Iteration 168/1000 | Loss: 0.00001322
Iteration 169/1000 | Loss: 0.00001322
Iteration 170/1000 | Loss: 0.00001322
Iteration 171/1000 | Loss: 0.00001322
Iteration 172/1000 | Loss: 0.00001322
Iteration 173/1000 | Loss: 0.00001322
Iteration 174/1000 | Loss: 0.00001322
Iteration 175/1000 | Loss: 0.00001322
Iteration 176/1000 | Loss: 0.00001322
Iteration 177/1000 | Loss: 0.00001322
Iteration 178/1000 | Loss: 0.00001322
Iteration 179/1000 | Loss: 0.00001322
Iteration 180/1000 | Loss: 0.00001322
Iteration 181/1000 | Loss: 0.00001322
Iteration 182/1000 | Loss: 0.00001322
Iteration 183/1000 | Loss: 0.00001321
Iteration 184/1000 | Loss: 0.00001321
Iteration 185/1000 | Loss: 0.00001321
Iteration 186/1000 | Loss: 0.00001321
Iteration 187/1000 | Loss: 0.00001321
Iteration 188/1000 | Loss: 0.00001321
Iteration 189/1000 | Loss: 0.00001321
Iteration 190/1000 | Loss: 0.00001321
Iteration 191/1000 | Loss: 0.00001321
Iteration 192/1000 | Loss: 0.00001321
Iteration 193/1000 | Loss: 0.00001321
Iteration 194/1000 | Loss: 0.00001321
Iteration 195/1000 | Loss: 0.00001321
Iteration 196/1000 | Loss: 0.00001321
Iteration 197/1000 | Loss: 0.00001321
Iteration 198/1000 | Loss: 0.00001321
Iteration 199/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.3209857570473105e-05, 1.3209857570473105e-05, 1.3209857570473105e-05, 1.3209857570473105e-05, 1.3209857570473105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3209857570473105e-05

Optimization complete. Final v2v error: 3.097113847732544 mm

Highest mean error: 3.408323049545288 mm for frame 79

Lowest mean error: 2.962952136993408 mm for frame 10

Saving results

Total time: 38.5677387714386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018619
Iteration 2/25 | Loss: 0.01018618
Iteration 3/25 | Loss: 0.00306051
Iteration 4/25 | Loss: 0.00200237
Iteration 5/25 | Loss: 0.00184728
Iteration 6/25 | Loss: 0.00169592
Iteration 7/25 | Loss: 0.00173243
Iteration 8/25 | Loss: 0.00166666
Iteration 9/25 | Loss: 0.00152189
Iteration 10/25 | Loss: 0.00146851
Iteration 11/25 | Loss: 0.00141454
Iteration 12/25 | Loss: 0.00133342
Iteration 13/25 | Loss: 0.00130784
Iteration 14/25 | Loss: 0.00125046
Iteration 15/25 | Loss: 0.00124620
Iteration 16/25 | Loss: 0.00122606
Iteration 17/25 | Loss: 0.00121320
Iteration 18/25 | Loss: 0.00120730
Iteration 19/25 | Loss: 0.00120179
Iteration 20/25 | Loss: 0.00119174
Iteration 21/25 | Loss: 0.00117940
Iteration 22/25 | Loss: 0.00117098
Iteration 23/25 | Loss: 0.00117414
Iteration 24/25 | Loss: 0.00117015
Iteration 25/25 | Loss: 0.00116879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51435173
Iteration 2/25 | Loss: 0.00561755
Iteration 3/25 | Loss: 0.00306408
Iteration 4/25 | Loss: 0.00306408
Iteration 5/25 | Loss: 0.00306408
Iteration 6/25 | Loss: 0.00306408
Iteration 7/25 | Loss: 0.00306408
Iteration 8/25 | Loss: 0.00306407
Iteration 9/25 | Loss: 0.00306407
Iteration 10/25 | Loss: 0.00306407
Iteration 11/25 | Loss: 0.00306407
Iteration 12/25 | Loss: 0.00306407
Iteration 13/25 | Loss: 0.00306407
Iteration 14/25 | Loss: 0.00306407
Iteration 15/25 | Loss: 0.00306407
Iteration 16/25 | Loss: 0.00306407
Iteration 17/25 | Loss: 0.00306407
Iteration 18/25 | Loss: 0.00306407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003064072923734784, 0.003064072923734784, 0.003064072923734784, 0.003064072923734784, 0.003064072923734784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003064072923734784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306407
Iteration 2/1000 | Loss: 0.00538339
Iteration 3/1000 | Loss: 0.00094360
Iteration 4/1000 | Loss: 0.00092103
Iteration 5/1000 | Loss: 0.00081072
Iteration 6/1000 | Loss: 0.00074355
Iteration 7/1000 | Loss: 0.00082885
Iteration 8/1000 | Loss: 0.00061323
Iteration 9/1000 | Loss: 0.00149898
Iteration 10/1000 | Loss: 0.00155678
Iteration 11/1000 | Loss: 0.00153300
Iteration 12/1000 | Loss: 0.00079801
Iteration 13/1000 | Loss: 0.00058401
Iteration 14/1000 | Loss: 0.00049143
Iteration 15/1000 | Loss: 0.00061184
Iteration 16/1000 | Loss: 0.00036133
Iteration 17/1000 | Loss: 0.00083293
Iteration 18/1000 | Loss: 0.00048988
Iteration 19/1000 | Loss: 0.00049446
Iteration 20/1000 | Loss: 0.00046231
Iteration 21/1000 | Loss: 0.00055336
Iteration 22/1000 | Loss: 0.00090815
Iteration 23/1000 | Loss: 0.00084879
Iteration 24/1000 | Loss: 0.00086370
Iteration 25/1000 | Loss: 0.00047847
Iteration 26/1000 | Loss: 0.00047788
Iteration 27/1000 | Loss: 0.00079338
Iteration 28/1000 | Loss: 0.00079179
Iteration 29/1000 | Loss: 0.00070285
Iteration 30/1000 | Loss: 0.00091526
Iteration 31/1000 | Loss: 0.00056127
Iteration 32/1000 | Loss: 0.00090915
Iteration 33/1000 | Loss: 0.00045939
Iteration 34/1000 | Loss: 0.00038646
Iteration 35/1000 | Loss: 0.00035037
Iteration 36/1000 | Loss: 0.00028657
Iteration 37/1000 | Loss: 0.00047425
Iteration 38/1000 | Loss: 0.00041066
Iteration 39/1000 | Loss: 0.00032920
Iteration 40/1000 | Loss: 0.00045685
Iteration 41/1000 | Loss: 0.00048981
Iteration 42/1000 | Loss: 0.00037545
Iteration 43/1000 | Loss: 0.00060919
Iteration 44/1000 | Loss: 0.00083980
Iteration 45/1000 | Loss: 0.00051661
Iteration 46/1000 | Loss: 0.00040776
Iteration 47/1000 | Loss: 0.00037162
Iteration 48/1000 | Loss: 0.00024586
Iteration 49/1000 | Loss: 0.00040314
Iteration 50/1000 | Loss: 0.00052919
Iteration 51/1000 | Loss: 0.00053646
Iteration 52/1000 | Loss: 0.00049397
Iteration 53/1000 | Loss: 0.00047391
Iteration 54/1000 | Loss: 0.00038173
Iteration 55/1000 | Loss: 0.00066691
Iteration 56/1000 | Loss: 0.00115078
Iteration 57/1000 | Loss: 0.00072149
Iteration 58/1000 | Loss: 0.00052956
Iteration 59/1000 | Loss: 0.00067142
Iteration 60/1000 | Loss: 0.00039403
Iteration 61/1000 | Loss: 0.00069835
Iteration 62/1000 | Loss: 0.00037660
Iteration 63/1000 | Loss: 0.00042590
Iteration 64/1000 | Loss: 0.00026142
Iteration 65/1000 | Loss: 0.00023040
Iteration 66/1000 | Loss: 0.00028316
Iteration 67/1000 | Loss: 0.00031965
Iteration 68/1000 | Loss: 0.00037233
Iteration 69/1000 | Loss: 0.00115632
Iteration 70/1000 | Loss: 0.00057699
Iteration 71/1000 | Loss: 0.00024155
Iteration 72/1000 | Loss: 0.00021932
Iteration 73/1000 | Loss: 0.00027858
Iteration 74/1000 | Loss: 0.00037003
Iteration 75/1000 | Loss: 0.00025019
Iteration 76/1000 | Loss: 0.00022147
Iteration 77/1000 | Loss: 0.00021372
Iteration 78/1000 | Loss: 0.00020105
Iteration 79/1000 | Loss: 0.00021398
Iteration 80/1000 | Loss: 0.00022383
Iteration 81/1000 | Loss: 0.00020997
Iteration 82/1000 | Loss: 0.00033628
Iteration 83/1000 | Loss: 0.00054100
Iteration 84/1000 | Loss: 0.00074618
Iteration 85/1000 | Loss: 0.00020905
Iteration 86/1000 | Loss: 0.00019989
Iteration 87/1000 | Loss: 0.00019828
Iteration 88/1000 | Loss: 0.00043315
Iteration 89/1000 | Loss: 0.00037565
Iteration 90/1000 | Loss: 0.00039994
Iteration 91/1000 | Loss: 0.00031927
Iteration 92/1000 | Loss: 0.00021989
Iteration 93/1000 | Loss: 0.00044582
Iteration 94/1000 | Loss: 0.00020820
Iteration 95/1000 | Loss: 0.00022602
Iteration 96/1000 | Loss: 0.00023164
Iteration 97/1000 | Loss: 0.00019572
Iteration 98/1000 | Loss: 0.00023074
Iteration 99/1000 | Loss: 0.00022385
Iteration 100/1000 | Loss: 0.00019275
Iteration 101/1000 | Loss: 0.00021775
Iteration 102/1000 | Loss: 0.00026569
Iteration 103/1000 | Loss: 0.00028773
Iteration 104/1000 | Loss: 0.00024969
Iteration 105/1000 | Loss: 0.00043173
Iteration 106/1000 | Loss: 0.00037522
Iteration 107/1000 | Loss: 0.00046149
Iteration 108/1000 | Loss: 0.00069047
Iteration 109/1000 | Loss: 0.00026397
Iteration 110/1000 | Loss: 0.00031083
Iteration 111/1000 | Loss: 0.00062752
Iteration 112/1000 | Loss: 0.00036435
Iteration 113/1000 | Loss: 0.00034625
Iteration 114/1000 | Loss: 0.00027548
Iteration 115/1000 | Loss: 0.00031114
Iteration 116/1000 | Loss: 0.00027137
Iteration 117/1000 | Loss: 0.00117123
Iteration 118/1000 | Loss: 0.00032299
Iteration 119/1000 | Loss: 0.00027200
Iteration 120/1000 | Loss: 0.00025528
Iteration 121/1000 | Loss: 0.00021927
Iteration 122/1000 | Loss: 0.00055682
Iteration 123/1000 | Loss: 0.00035747
Iteration 124/1000 | Loss: 0.00038315
Iteration 125/1000 | Loss: 0.00045053
Iteration 126/1000 | Loss: 0.00028238
Iteration 127/1000 | Loss: 0.00038607
Iteration 128/1000 | Loss: 0.00037354
Iteration 129/1000 | Loss: 0.00021455
Iteration 130/1000 | Loss: 0.00055706
Iteration 131/1000 | Loss: 0.00019294
Iteration 132/1000 | Loss: 0.00018324
Iteration 133/1000 | Loss: 0.00041645
Iteration 134/1000 | Loss: 0.00020224
Iteration 135/1000 | Loss: 0.00018756
Iteration 136/1000 | Loss: 0.00045736
Iteration 137/1000 | Loss: 0.00020739
Iteration 138/1000 | Loss: 0.00043049
Iteration 139/1000 | Loss: 0.00026888
Iteration 140/1000 | Loss: 0.00020746
Iteration 141/1000 | Loss: 0.00021972
Iteration 142/1000 | Loss: 0.00046775
Iteration 143/1000 | Loss: 0.00020705
Iteration 144/1000 | Loss: 0.00053869
Iteration 145/1000 | Loss: 0.00128194
Iteration 146/1000 | Loss: 0.00127800
Iteration 147/1000 | Loss: 0.00150145
Iteration 148/1000 | Loss: 0.00083326
Iteration 149/1000 | Loss: 0.00047385
Iteration 150/1000 | Loss: 0.00018513
Iteration 151/1000 | Loss: 0.00018570
Iteration 152/1000 | Loss: 0.00018193
Iteration 153/1000 | Loss: 0.00017625
Iteration 154/1000 | Loss: 0.00018282
Iteration 155/1000 | Loss: 0.00018806
Iteration 156/1000 | Loss: 0.00018375
Iteration 157/1000 | Loss: 0.00017677
Iteration 158/1000 | Loss: 0.00018246
Iteration 159/1000 | Loss: 0.00017679
Iteration 160/1000 | Loss: 0.00018023
Iteration 161/1000 | Loss: 0.00017621
Iteration 162/1000 | Loss: 0.00018826
Iteration 163/1000 | Loss: 0.00018174
Iteration 164/1000 | Loss: 0.00017949
Iteration 165/1000 | Loss: 0.00017560
Iteration 166/1000 | Loss: 0.00018485
Iteration 167/1000 | Loss: 0.00018236
Iteration 168/1000 | Loss: 0.00018309
Iteration 169/1000 | Loss: 0.00017912
Iteration 170/1000 | Loss: 0.00017838
Iteration 171/1000 | Loss: 0.00019338
Iteration 172/1000 | Loss: 0.00019826
Iteration 173/1000 | Loss: 0.00018864
Iteration 174/1000 | Loss: 0.00018908
Iteration 175/1000 | Loss: 0.00018898
Iteration 176/1000 | Loss: 0.00018787
Iteration 177/1000 | Loss: 0.00019211
Iteration 178/1000 | Loss: 0.00018393
Iteration 179/1000 | Loss: 0.00035736
Iteration 180/1000 | Loss: 0.00017598
Iteration 181/1000 | Loss: 0.00016270
Iteration 182/1000 | Loss: 0.00015905
Iteration 183/1000 | Loss: 0.00015752
Iteration 184/1000 | Loss: 0.00015642
Iteration 185/1000 | Loss: 0.00039091
Iteration 186/1000 | Loss: 0.00026175
Iteration 187/1000 | Loss: 0.00016435
Iteration 188/1000 | Loss: 0.00015920
Iteration 189/1000 | Loss: 0.00028446
Iteration 190/1000 | Loss: 0.00016501
Iteration 191/1000 | Loss: 0.00016100
Iteration 192/1000 | Loss: 0.00052333
Iteration 193/1000 | Loss: 0.00016235
Iteration 194/1000 | Loss: 0.00015839
Iteration 195/1000 | Loss: 0.00034148
Iteration 196/1000 | Loss: 0.00015634
Iteration 197/1000 | Loss: 0.00022526
Iteration 198/1000 | Loss: 0.00020394
Iteration 199/1000 | Loss: 0.00092160
Iteration 200/1000 | Loss: 0.00026129
Iteration 201/1000 | Loss: 0.00028393
Iteration 202/1000 | Loss: 0.00023333
Iteration 203/1000 | Loss: 0.00035756
Iteration 204/1000 | Loss: 0.00032158
Iteration 205/1000 | Loss: 0.00015669
Iteration 206/1000 | Loss: 0.00029538
Iteration 207/1000 | Loss: 0.00072974
Iteration 208/1000 | Loss: 0.00034267
Iteration 209/1000 | Loss: 0.00027709
Iteration 210/1000 | Loss: 0.00027100
Iteration 211/1000 | Loss: 0.00053108
Iteration 212/1000 | Loss: 0.00034598
Iteration 213/1000 | Loss: 0.00048731
Iteration 214/1000 | Loss: 0.00032783
Iteration 215/1000 | Loss: 0.00016351
Iteration 216/1000 | Loss: 0.00015366
Iteration 217/1000 | Loss: 0.00015134
Iteration 218/1000 | Loss: 0.00014947
Iteration 219/1000 | Loss: 0.00014834
Iteration 220/1000 | Loss: 0.00014735
Iteration 221/1000 | Loss: 0.00014672
Iteration 222/1000 | Loss: 0.00014630
Iteration 223/1000 | Loss: 0.00014600
Iteration 224/1000 | Loss: 0.00014997
Iteration 225/1000 | Loss: 0.00036756
Iteration 226/1000 | Loss: 0.00018312
Iteration 227/1000 | Loss: 0.00016821
Iteration 228/1000 | Loss: 0.00015045
Iteration 229/1000 | Loss: 0.00015004
Iteration 230/1000 | Loss: 0.00014898
Iteration 231/1000 | Loss: 0.00014663
Iteration 232/1000 | Loss: 0.00014797
Iteration 233/1000 | Loss: 0.00014634
Iteration 234/1000 | Loss: 0.00014917
Iteration 235/1000 | Loss: 0.00014760
Iteration 236/1000 | Loss: 0.00015695
Iteration 237/1000 | Loss: 0.00051389
Iteration 238/1000 | Loss: 0.00020367
Iteration 239/1000 | Loss: 0.00016386
Iteration 240/1000 | Loss: 0.00015020
Iteration 241/1000 | Loss: 0.00015865
Iteration 242/1000 | Loss: 0.00015883
Iteration 243/1000 | Loss: 0.00015287
Iteration 244/1000 | Loss: 0.00015082
Iteration 245/1000 | Loss: 0.00015897
Iteration 246/1000 | Loss: 0.00015762
Iteration 247/1000 | Loss: 0.00015774
Iteration 248/1000 | Loss: 0.00015558
Iteration 249/1000 | Loss: 0.00014834
Iteration 250/1000 | Loss: 0.00015054
Iteration 251/1000 | Loss: 0.00014807
Iteration 252/1000 | Loss: 0.00014728
Iteration 253/1000 | Loss: 0.00014781
Iteration 254/1000 | Loss: 0.00015499
Iteration 255/1000 | Loss: 0.00015659
Iteration 256/1000 | Loss: 0.00014770
Iteration 257/1000 | Loss: 0.00015330
Iteration 258/1000 | Loss: 0.00015590
Iteration 259/1000 | Loss: 0.00015193
Iteration 260/1000 | Loss: 0.00015404
Iteration 261/1000 | Loss: 0.00015351
Iteration 262/1000 | Loss: 0.00015273
Iteration 263/1000 | Loss: 0.00015162
Iteration 264/1000 | Loss: 0.00015586
Iteration 265/1000 | Loss: 0.00015650
Iteration 266/1000 | Loss: 0.00015420
Iteration 267/1000 | Loss: 0.00015216
Iteration 268/1000 | Loss: 0.00015670
Iteration 269/1000 | Loss: 0.00015579
Iteration 270/1000 | Loss: 0.00015521
Iteration 271/1000 | Loss: 0.00015668
Iteration 272/1000 | Loss: 0.00015081
Iteration 273/1000 | Loss: 0.00015936
Iteration 274/1000 | Loss: 0.00015108
Iteration 275/1000 | Loss: 0.00015063
Iteration 276/1000 | Loss: 0.00015830
Iteration 277/1000 | Loss: 0.00015081
Iteration 278/1000 | Loss: 0.00015362
Iteration 279/1000 | Loss: 0.00015914
Iteration 280/1000 | Loss: 0.00015135
Iteration 281/1000 | Loss: 0.00015837
Iteration 282/1000 | Loss: 0.00016432
Iteration 283/1000 | Loss: 0.00015362
Iteration 284/1000 | Loss: 0.00015856
Iteration 285/1000 | Loss: 0.00015511
Iteration 286/1000 | Loss: 0.00015570
Iteration 287/1000 | Loss: 0.00015532
Iteration 288/1000 | Loss: 0.00015103
Iteration 289/1000 | Loss: 0.00015312
Iteration 290/1000 | Loss: 0.00015500
Iteration 291/1000 | Loss: 0.00015243
Iteration 292/1000 | Loss: 0.00015699
Iteration 293/1000 | Loss: 0.00043777
Iteration 294/1000 | Loss: 0.00017828
Iteration 295/1000 | Loss: 0.00015344
Iteration 296/1000 | Loss: 0.00015421
Iteration 297/1000 | Loss: 0.00022897
Iteration 298/1000 | Loss: 0.00014624
Iteration 299/1000 | Loss: 0.00014469
Iteration 300/1000 | Loss: 0.00014381
Iteration 301/1000 | Loss: 0.00033010
Iteration 302/1000 | Loss: 0.00018090
Iteration 303/1000 | Loss: 0.00016576
Iteration 304/1000 | Loss: 0.00014373
Iteration 305/1000 | Loss: 0.00014335
Iteration 306/1000 | Loss: 0.00014315
Iteration 307/1000 | Loss: 0.00014310
Iteration 308/1000 | Loss: 0.00014307
Iteration 309/1000 | Loss: 0.00014307
Iteration 310/1000 | Loss: 0.00014302
Iteration 311/1000 | Loss: 0.00014302
Iteration 312/1000 | Loss: 0.00014301
Iteration 313/1000 | Loss: 0.00014301
Iteration 314/1000 | Loss: 0.00014301
Iteration 315/1000 | Loss: 0.00014301
Iteration 316/1000 | Loss: 0.00014301
Iteration 317/1000 | Loss: 0.00014300
Iteration 318/1000 | Loss: 0.00014300
Iteration 319/1000 | Loss: 0.00014300
Iteration 320/1000 | Loss: 0.00014300
Iteration 321/1000 | Loss: 0.00014300
Iteration 322/1000 | Loss: 0.00014300
Iteration 323/1000 | Loss: 0.00014300
Iteration 324/1000 | Loss: 0.00014300
Iteration 325/1000 | Loss: 0.00014299
Iteration 326/1000 | Loss: 0.00014299
Iteration 327/1000 | Loss: 0.00014299
Iteration 328/1000 | Loss: 0.00014298
Iteration 329/1000 | Loss: 0.00014298
Iteration 330/1000 | Loss: 0.00014298
Iteration 331/1000 | Loss: 0.00014298
Iteration 332/1000 | Loss: 0.00014297
Iteration 333/1000 | Loss: 0.00014297
Iteration 334/1000 | Loss: 0.00014297
Iteration 335/1000 | Loss: 0.00014297
Iteration 336/1000 | Loss: 0.00014297
Iteration 337/1000 | Loss: 0.00014297
Iteration 338/1000 | Loss: 0.00014297
Iteration 339/1000 | Loss: 0.00014297
Iteration 340/1000 | Loss: 0.00014297
Iteration 341/1000 | Loss: 0.00014296
Iteration 342/1000 | Loss: 0.00014296
Iteration 343/1000 | Loss: 0.00014296
Iteration 344/1000 | Loss: 0.00014296
Iteration 345/1000 | Loss: 0.00014296
Iteration 346/1000 | Loss: 0.00014296
Iteration 347/1000 | Loss: 0.00014296
Iteration 348/1000 | Loss: 0.00014296
Iteration 349/1000 | Loss: 0.00014296
Iteration 350/1000 | Loss: 0.00014296
Iteration 351/1000 | Loss: 0.00014296
Iteration 352/1000 | Loss: 0.00014296
Iteration 353/1000 | Loss: 0.00014296
Iteration 354/1000 | Loss: 0.00014295
Iteration 355/1000 | Loss: 0.00014295
Iteration 356/1000 | Loss: 0.00014295
Iteration 357/1000 | Loss: 0.00014295
Iteration 358/1000 | Loss: 0.00014295
Iteration 359/1000 | Loss: 0.00014295
Iteration 360/1000 | Loss: 0.00014295
Iteration 361/1000 | Loss: 0.00014295
Iteration 362/1000 | Loss: 0.00014295
Iteration 363/1000 | Loss: 0.00014295
Iteration 364/1000 | Loss: 0.00014295
Iteration 365/1000 | Loss: 0.00014295
Iteration 366/1000 | Loss: 0.00014294
Iteration 367/1000 | Loss: 0.00014294
Iteration 368/1000 | Loss: 0.00014294
Iteration 369/1000 | Loss: 0.00014294
Iteration 370/1000 | Loss: 0.00014294
Iteration 371/1000 | Loss: 0.00014294
Iteration 372/1000 | Loss: 0.00014294
Iteration 373/1000 | Loss: 0.00014294
Iteration 374/1000 | Loss: 0.00014294
Iteration 375/1000 | Loss: 0.00014294
Iteration 376/1000 | Loss: 0.00014294
Iteration 377/1000 | Loss: 0.00014294
Iteration 378/1000 | Loss: 0.00014294
Iteration 379/1000 | Loss: 0.00014294
Iteration 380/1000 | Loss: 0.00014294
Iteration 381/1000 | Loss: 0.00014294
Iteration 382/1000 | Loss: 0.00014294
Iteration 383/1000 | Loss: 0.00014294
Iteration 384/1000 | Loss: 0.00014294
Iteration 385/1000 | Loss: 0.00014294
Iteration 386/1000 | Loss: 0.00014294
Iteration 387/1000 | Loss: 0.00014294
Iteration 388/1000 | Loss: 0.00014294
Iteration 389/1000 | Loss: 0.00014294
Iteration 390/1000 | Loss: 0.00014294
Iteration 391/1000 | Loss: 0.00014294
Iteration 392/1000 | Loss: 0.00014294
Iteration 393/1000 | Loss: 0.00014294
Iteration 394/1000 | Loss: 0.00014294
Iteration 395/1000 | Loss: 0.00014294
Iteration 396/1000 | Loss: 0.00014294
Iteration 397/1000 | Loss: 0.00014294
Iteration 398/1000 | Loss: 0.00014294
Iteration 399/1000 | Loss: 0.00014294
Iteration 400/1000 | Loss: 0.00014294
Iteration 401/1000 | Loss: 0.00014294
Iteration 402/1000 | Loss: 0.00014294
Iteration 403/1000 | Loss: 0.00014294
Iteration 404/1000 | Loss: 0.00014294
Iteration 405/1000 | Loss: 0.00014294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [0.00014293883577920496, 0.00014293883577920496, 0.00014293883577920496, 0.00014293883577920496, 0.00014293883577920496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014293883577920496

Optimization complete. Final v2v error: 6.5626983642578125 mm

Highest mean error: 12.50033187866211 mm for frame 134

Lowest mean error: 3.6455605030059814 mm for frame 102

Saving results

Total time: 531.190847158432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371213
Iteration 2/25 | Loss: 0.00102356
Iteration 3/25 | Loss: 0.00067992
Iteration 4/25 | Loss: 0.00064432
Iteration 5/25 | Loss: 0.00063060
Iteration 6/25 | Loss: 0.00062758
Iteration 7/25 | Loss: 0.00062665
Iteration 8/25 | Loss: 0.00062653
Iteration 9/25 | Loss: 0.00062653
Iteration 10/25 | Loss: 0.00062653
Iteration 11/25 | Loss: 0.00062653
Iteration 12/25 | Loss: 0.00062653
Iteration 13/25 | Loss: 0.00062653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006265279371291399, 0.0006265279371291399, 0.0006265279371291399, 0.0006265279371291399, 0.0006265279371291399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006265279371291399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55616999
Iteration 2/25 | Loss: 0.00028407
Iteration 3/25 | Loss: 0.00028407
Iteration 4/25 | Loss: 0.00028407
Iteration 5/25 | Loss: 0.00028407
Iteration 6/25 | Loss: 0.00028407
Iteration 7/25 | Loss: 0.00028407
Iteration 8/25 | Loss: 0.00028407
Iteration 9/25 | Loss: 0.00028407
Iteration 10/25 | Loss: 0.00028407
Iteration 11/25 | Loss: 0.00028407
Iteration 12/25 | Loss: 0.00028407
Iteration 13/25 | Loss: 0.00028407
Iteration 14/25 | Loss: 0.00028407
Iteration 15/25 | Loss: 0.00028407
Iteration 16/25 | Loss: 0.00028407
Iteration 17/25 | Loss: 0.00028407
Iteration 18/25 | Loss: 0.00028407
Iteration 19/25 | Loss: 0.00028407
Iteration 20/25 | Loss: 0.00028407
Iteration 21/25 | Loss: 0.00028407
Iteration 22/25 | Loss: 0.00028407
Iteration 23/25 | Loss: 0.00028407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002840654633473605, 0.0002840654633473605, 0.0002840654633473605, 0.0002840654633473605, 0.0002840654633473605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002840654633473605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028407
Iteration 2/1000 | Loss: 0.00002735
Iteration 3/1000 | Loss: 0.00001891
Iteration 4/1000 | Loss: 0.00001748
Iteration 5/1000 | Loss: 0.00001674
Iteration 6/1000 | Loss: 0.00001632
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001549
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001530
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001526
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001525
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001524
Iteration 30/1000 | Loss: 0.00001524
Iteration 31/1000 | Loss: 0.00001524
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001523
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001520
Iteration 43/1000 | Loss: 0.00001520
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001520
Iteration 48/1000 | Loss: 0.00001520
Iteration 49/1000 | Loss: 0.00001520
Iteration 50/1000 | Loss: 0.00001520
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001519
Iteration 53/1000 | Loss: 0.00001519
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001516
Iteration 66/1000 | Loss: 0.00001516
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001514
Iteration 76/1000 | Loss: 0.00001514
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001507
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001506
Iteration 140/1000 | Loss: 0.00001506
Iteration 141/1000 | Loss: 0.00001506
Iteration 142/1000 | Loss: 0.00001506
Iteration 143/1000 | Loss: 0.00001506
Iteration 144/1000 | Loss: 0.00001506
Iteration 145/1000 | Loss: 0.00001506
Iteration 146/1000 | Loss: 0.00001506
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001506
Iteration 149/1000 | Loss: 0.00001506
Iteration 150/1000 | Loss: 0.00001505
Iteration 151/1000 | Loss: 0.00001505
Iteration 152/1000 | Loss: 0.00001505
Iteration 153/1000 | Loss: 0.00001505
Iteration 154/1000 | Loss: 0.00001505
Iteration 155/1000 | Loss: 0.00001505
Iteration 156/1000 | Loss: 0.00001505
Iteration 157/1000 | Loss: 0.00001505
Iteration 158/1000 | Loss: 0.00001505
Iteration 159/1000 | Loss: 0.00001505
Iteration 160/1000 | Loss: 0.00001505
Iteration 161/1000 | Loss: 0.00001505
Iteration 162/1000 | Loss: 0.00001504
Iteration 163/1000 | Loss: 0.00001504
Iteration 164/1000 | Loss: 0.00001504
Iteration 165/1000 | Loss: 0.00001504
Iteration 166/1000 | Loss: 0.00001504
Iteration 167/1000 | Loss: 0.00001504
Iteration 168/1000 | Loss: 0.00001504
Iteration 169/1000 | Loss: 0.00001503
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001503
Iteration 173/1000 | Loss: 0.00001503
Iteration 174/1000 | Loss: 0.00001503
Iteration 175/1000 | Loss: 0.00001503
Iteration 176/1000 | Loss: 0.00001503
Iteration 177/1000 | Loss: 0.00001503
Iteration 178/1000 | Loss: 0.00001503
Iteration 179/1000 | Loss: 0.00001503
Iteration 180/1000 | Loss: 0.00001503
Iteration 181/1000 | Loss: 0.00001503
Iteration 182/1000 | Loss: 0.00001503
Iteration 183/1000 | Loss: 0.00001503
Iteration 184/1000 | Loss: 0.00001503
Iteration 185/1000 | Loss: 0.00001503
Iteration 186/1000 | Loss: 0.00001503
Iteration 187/1000 | Loss: 0.00001503
Iteration 188/1000 | Loss: 0.00001503
Iteration 189/1000 | Loss: 0.00001503
Iteration 190/1000 | Loss: 0.00001503
Iteration 191/1000 | Loss: 0.00001503
Iteration 192/1000 | Loss: 0.00001503
Iteration 193/1000 | Loss: 0.00001503
Iteration 194/1000 | Loss: 0.00001503
Iteration 195/1000 | Loss: 0.00001503
Iteration 196/1000 | Loss: 0.00001503
Iteration 197/1000 | Loss: 0.00001503
Iteration 198/1000 | Loss: 0.00001503
Iteration 199/1000 | Loss: 0.00001503
Iteration 200/1000 | Loss: 0.00001503
Iteration 201/1000 | Loss: 0.00001503
Iteration 202/1000 | Loss: 0.00001503
Iteration 203/1000 | Loss: 0.00001503
Iteration 204/1000 | Loss: 0.00001503
Iteration 205/1000 | Loss: 0.00001503
Iteration 206/1000 | Loss: 0.00001503
Iteration 207/1000 | Loss: 0.00001503
Iteration 208/1000 | Loss: 0.00001503
Iteration 209/1000 | Loss: 0.00001503
Iteration 210/1000 | Loss: 0.00001503
Iteration 211/1000 | Loss: 0.00001503
Iteration 212/1000 | Loss: 0.00001503
Iteration 213/1000 | Loss: 0.00001503
Iteration 214/1000 | Loss: 0.00001503
Iteration 215/1000 | Loss: 0.00001503
Iteration 216/1000 | Loss: 0.00001503
Iteration 217/1000 | Loss: 0.00001503
Iteration 218/1000 | Loss: 0.00001503
Iteration 219/1000 | Loss: 0.00001503
Iteration 220/1000 | Loss: 0.00001503
Iteration 221/1000 | Loss: 0.00001503
Iteration 222/1000 | Loss: 0.00001503
Iteration 223/1000 | Loss: 0.00001503
Iteration 224/1000 | Loss: 0.00001503
Iteration 225/1000 | Loss: 0.00001503
Iteration 226/1000 | Loss: 0.00001503
Iteration 227/1000 | Loss: 0.00001503
Iteration 228/1000 | Loss: 0.00001503
Iteration 229/1000 | Loss: 0.00001503
Iteration 230/1000 | Loss: 0.00001503
Iteration 231/1000 | Loss: 0.00001503
Iteration 232/1000 | Loss: 0.00001503
Iteration 233/1000 | Loss: 0.00001503
Iteration 234/1000 | Loss: 0.00001503
Iteration 235/1000 | Loss: 0.00001503
Iteration 236/1000 | Loss: 0.00001503
Iteration 237/1000 | Loss: 0.00001503
Iteration 238/1000 | Loss: 0.00001503
Iteration 239/1000 | Loss: 0.00001503
Iteration 240/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.5029619135020766e-05, 1.5029619135020766e-05, 1.5029619135020766e-05, 1.5029619135020766e-05, 1.5029619135020766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5029619135020766e-05

Optimization complete. Final v2v error: 3.302567481994629 mm

Highest mean error: 3.5771303176879883 mm for frame 21

Lowest mean error: 2.7981700897216797 mm for frame 5

Saving results

Total time: 37.097880363464355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429506
Iteration 2/25 | Loss: 0.00086467
Iteration 3/25 | Loss: 0.00068458
Iteration 4/25 | Loss: 0.00065707
Iteration 5/25 | Loss: 0.00064897
Iteration 6/25 | Loss: 0.00064812
Iteration 7/25 | Loss: 0.00064793
Iteration 8/25 | Loss: 0.00064793
Iteration 9/25 | Loss: 0.00064793
Iteration 10/25 | Loss: 0.00064793
Iteration 11/25 | Loss: 0.00064793
Iteration 12/25 | Loss: 0.00064793
Iteration 13/25 | Loss: 0.00064793
Iteration 14/25 | Loss: 0.00064793
Iteration 15/25 | Loss: 0.00064793
Iteration 16/25 | Loss: 0.00064793
Iteration 17/25 | Loss: 0.00064793
Iteration 18/25 | Loss: 0.00064793
Iteration 19/25 | Loss: 0.00064793
Iteration 20/25 | Loss: 0.00064793
Iteration 21/25 | Loss: 0.00064793
Iteration 22/25 | Loss: 0.00064793
Iteration 23/25 | Loss: 0.00064793
Iteration 24/25 | Loss: 0.00064793
Iteration 25/25 | Loss: 0.00064793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16168988
Iteration 2/25 | Loss: 0.00030205
Iteration 3/25 | Loss: 0.00030204
Iteration 4/25 | Loss: 0.00030204
Iteration 5/25 | Loss: 0.00030204
Iteration 6/25 | Loss: 0.00030204
Iteration 7/25 | Loss: 0.00030204
Iteration 8/25 | Loss: 0.00030204
Iteration 9/25 | Loss: 0.00030204
Iteration 10/25 | Loss: 0.00030204
Iteration 11/25 | Loss: 0.00030204
Iteration 12/25 | Loss: 0.00030204
Iteration 13/25 | Loss: 0.00030204
Iteration 14/25 | Loss: 0.00030204
Iteration 15/25 | Loss: 0.00030204
Iteration 16/25 | Loss: 0.00030204
Iteration 17/25 | Loss: 0.00030204
Iteration 18/25 | Loss: 0.00030204
Iteration 19/25 | Loss: 0.00030204
Iteration 20/25 | Loss: 0.00030204
Iteration 21/25 | Loss: 0.00030204
Iteration 22/25 | Loss: 0.00030204
Iteration 23/25 | Loss: 0.00030204
Iteration 24/25 | Loss: 0.00030204
Iteration 25/25 | Loss: 0.00030204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030204
Iteration 2/1000 | Loss: 0.00003728
Iteration 3/1000 | Loss: 0.00002421
Iteration 4/1000 | Loss: 0.00002198
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002006
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001926
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00001901
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001892
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00001883
Iteration 20/1000 | Loss: 0.00001878
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001877
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001877
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001867
Iteration 33/1000 | Loss: 0.00001867
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001867
Iteration 41/1000 | Loss: 0.00001867
Iteration 42/1000 | Loss: 0.00001866
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001864
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00001860
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001859
Iteration 76/1000 | Loss: 0.00001859
Iteration 77/1000 | Loss: 0.00001859
Iteration 78/1000 | Loss: 0.00001859
Iteration 79/1000 | Loss: 0.00001859
Iteration 80/1000 | Loss: 0.00001859
Iteration 81/1000 | Loss: 0.00001859
Iteration 82/1000 | Loss: 0.00001859
Iteration 83/1000 | Loss: 0.00001859
Iteration 84/1000 | Loss: 0.00001859
Iteration 85/1000 | Loss: 0.00001859
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.859003350546118e-05, 1.859003350546118e-05, 1.859003350546118e-05, 1.859003350546118e-05, 1.859003350546118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.859003350546118e-05

Optimization complete. Final v2v error: 3.618581533432007 mm

Highest mean error: 3.6445400714874268 mm for frame 88

Lowest mean error: 3.5777602195739746 mm for frame 25

Saving results

Total time: 28.192058086395264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117551
Iteration 2/25 | Loss: 0.00200256
Iteration 3/25 | Loss: 0.00101830
Iteration 4/25 | Loss: 0.00087508
Iteration 5/25 | Loss: 0.00086308
Iteration 6/25 | Loss: 0.00084040
Iteration 7/25 | Loss: 0.00080841
Iteration 8/25 | Loss: 0.00079056
Iteration 9/25 | Loss: 0.00077411
Iteration 10/25 | Loss: 0.00077574
Iteration 11/25 | Loss: 0.00077017
Iteration 12/25 | Loss: 0.00076039
Iteration 13/25 | Loss: 0.00075709
Iteration 14/25 | Loss: 0.00075298
Iteration 15/25 | Loss: 0.00075141
Iteration 16/25 | Loss: 0.00075426
Iteration 17/25 | Loss: 0.00075313
Iteration 18/25 | Loss: 0.00075011
Iteration 19/25 | Loss: 0.00075246
Iteration 20/25 | Loss: 0.00074868
Iteration 21/25 | Loss: 0.00074843
Iteration 22/25 | Loss: 0.00074831
Iteration 23/25 | Loss: 0.00074831
Iteration 24/25 | Loss: 0.00074831
Iteration 25/25 | Loss: 0.00074831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77875054
Iteration 2/25 | Loss: 0.00035607
Iteration 3/25 | Loss: 0.00035607
Iteration 4/25 | Loss: 0.00035607
Iteration 5/25 | Loss: 0.00035606
Iteration 6/25 | Loss: 0.00035606
Iteration 7/25 | Loss: 0.00035606
Iteration 8/25 | Loss: 0.00035606
Iteration 9/25 | Loss: 0.00035606
Iteration 10/25 | Loss: 0.00035606
Iteration 11/25 | Loss: 0.00035606
Iteration 12/25 | Loss: 0.00035606
Iteration 13/25 | Loss: 0.00035606
Iteration 14/25 | Loss: 0.00035606
Iteration 15/25 | Loss: 0.00035606
Iteration 16/25 | Loss: 0.00035606
Iteration 17/25 | Loss: 0.00035606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00035606269375421107, 0.00035606269375421107, 0.00035606269375421107, 0.00035606269375421107, 0.00035606269375421107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035606269375421107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035606
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00002282
Iteration 4/1000 | Loss: 0.00002117
Iteration 5/1000 | Loss: 0.00002015
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001901
Iteration 8/1000 | Loss: 0.00001874
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001834
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001828
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001819
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001816
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001813
Iteration 24/1000 | Loss: 0.00001813
Iteration 25/1000 | Loss: 0.00001813
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001813
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001810
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001807
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001805
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001804
Iteration 70/1000 | Loss: 0.00001803
Iteration 71/1000 | Loss: 0.00001803
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001802
Iteration 76/1000 | Loss: 0.00001802
Iteration 77/1000 | Loss: 0.00001802
Iteration 78/1000 | Loss: 0.00001802
Iteration 79/1000 | Loss: 0.00001802
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.801746475393884e-05, 1.801746475393884e-05, 1.801746475393884e-05, 1.801746475393884e-05, 1.801746475393884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.801746475393884e-05

Optimization complete. Final v2v error: 3.483741521835327 mm

Highest mean error: 4.616724967956543 mm for frame 182

Lowest mean error: 3.0180251598358154 mm for frame 112

Saving results

Total time: 68.14030408859253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068691
Iteration 2/25 | Loss: 0.00266337
Iteration 3/25 | Loss: 0.00151197
Iteration 4/25 | Loss: 0.00111888
Iteration 5/25 | Loss: 0.00100209
Iteration 6/25 | Loss: 0.00096116
Iteration 7/25 | Loss: 0.00092353
Iteration 8/25 | Loss: 0.00091748
Iteration 9/25 | Loss: 0.00089326
Iteration 10/25 | Loss: 0.00083749
Iteration 11/25 | Loss: 0.00081202
Iteration 12/25 | Loss: 0.00080891
Iteration 13/25 | Loss: 0.00079523
Iteration 14/25 | Loss: 0.00078258
Iteration 15/25 | Loss: 0.00077942
Iteration 16/25 | Loss: 0.00077202
Iteration 17/25 | Loss: 0.00077327
Iteration 18/25 | Loss: 0.00076790
Iteration 19/25 | Loss: 0.00076714
Iteration 20/25 | Loss: 0.00077404
Iteration 21/25 | Loss: 0.00076658
Iteration 22/25 | Loss: 0.00076618
Iteration 23/25 | Loss: 0.00076345
Iteration 24/25 | Loss: 0.00076090
Iteration 25/25 | Loss: 0.00075157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49332881
Iteration 2/25 | Loss: 0.00118619
Iteration 3/25 | Loss: 0.00095066
Iteration 4/25 | Loss: 0.00095066
Iteration 5/25 | Loss: 0.00095066
Iteration 6/25 | Loss: 0.00095066
Iteration 7/25 | Loss: 0.00095066
Iteration 8/25 | Loss: 0.00095066
Iteration 9/25 | Loss: 0.00095066
Iteration 10/25 | Loss: 0.00095066
Iteration 11/25 | Loss: 0.00095066
Iteration 12/25 | Loss: 0.00095066
Iteration 13/25 | Loss: 0.00095066
Iteration 14/25 | Loss: 0.00095066
Iteration 15/25 | Loss: 0.00095065
Iteration 16/25 | Loss: 0.00095065
Iteration 17/25 | Loss: 0.00095065
Iteration 18/25 | Loss: 0.00095065
Iteration 19/25 | Loss: 0.00095065
Iteration 20/25 | Loss: 0.00095065
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009506548522040248, 0.0009506548522040248, 0.0009506548522040248, 0.0009506548522040248, 0.0009506548522040248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009506548522040248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095065
Iteration 2/1000 | Loss: 0.00047961
Iteration 3/1000 | Loss: 0.00066893
Iteration 4/1000 | Loss: 0.00036549
Iteration 5/1000 | Loss: 0.00092697
Iteration 6/1000 | Loss: 0.00044858
Iteration 7/1000 | Loss: 0.00010510
Iteration 8/1000 | Loss: 0.00022903
Iteration 9/1000 | Loss: 0.00068956
Iteration 10/1000 | Loss: 0.00015299
Iteration 11/1000 | Loss: 0.00009168
Iteration 12/1000 | Loss: 0.00032023
Iteration 13/1000 | Loss: 0.00012301
Iteration 14/1000 | Loss: 0.00015635
Iteration 15/1000 | Loss: 0.00040546
Iteration 16/1000 | Loss: 0.00013881
Iteration 17/1000 | Loss: 0.00013308
Iteration 18/1000 | Loss: 0.00005227
Iteration 19/1000 | Loss: 0.00005004
Iteration 20/1000 | Loss: 0.00015738
Iteration 21/1000 | Loss: 0.00005948
Iteration 22/1000 | Loss: 0.00065505
Iteration 23/1000 | Loss: 0.00018356
Iteration 24/1000 | Loss: 0.00005754
Iteration 25/1000 | Loss: 0.00014619
Iteration 26/1000 | Loss: 0.00014565
Iteration 27/1000 | Loss: 0.00009712
Iteration 28/1000 | Loss: 0.00038425
Iteration 29/1000 | Loss: 0.00029069
Iteration 30/1000 | Loss: 0.00030300
Iteration 31/1000 | Loss: 0.00013379
Iteration 32/1000 | Loss: 0.00011899
Iteration 33/1000 | Loss: 0.00002917
Iteration 34/1000 | Loss: 0.00002739
Iteration 35/1000 | Loss: 0.00002643
Iteration 36/1000 | Loss: 0.00018607
Iteration 37/1000 | Loss: 0.00047808
Iteration 38/1000 | Loss: 0.00029338
Iteration 39/1000 | Loss: 0.00002909
Iteration 40/1000 | Loss: 0.00005234
Iteration 41/1000 | Loss: 0.00002913
Iteration 42/1000 | Loss: 0.00007652
Iteration 43/1000 | Loss: 0.00003179
Iteration 44/1000 | Loss: 0.00002525
Iteration 45/1000 | Loss: 0.00037415
Iteration 46/1000 | Loss: 0.00006630
Iteration 47/1000 | Loss: 0.00014023
Iteration 48/1000 | Loss: 0.00003004
Iteration 49/1000 | Loss: 0.00002497
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00031255
Iteration 52/1000 | Loss: 0.00023967
Iteration 53/1000 | Loss: 0.00033164
Iteration 54/1000 | Loss: 0.00019982
Iteration 55/1000 | Loss: 0.00003536
Iteration 56/1000 | Loss: 0.00002324
Iteration 57/1000 | Loss: 0.00002292
Iteration 58/1000 | Loss: 0.00002279
Iteration 59/1000 | Loss: 0.00009730
Iteration 60/1000 | Loss: 0.00031346
Iteration 61/1000 | Loss: 0.00044039
Iteration 62/1000 | Loss: 0.00015238
Iteration 63/1000 | Loss: 0.00012493
Iteration 64/1000 | Loss: 0.00006656
Iteration 65/1000 | Loss: 0.00005753
Iteration 66/1000 | Loss: 0.00015915
Iteration 67/1000 | Loss: 0.00010644
Iteration 68/1000 | Loss: 0.00003773
Iteration 69/1000 | Loss: 0.00002609
Iteration 70/1000 | Loss: 0.00009441
Iteration 71/1000 | Loss: 0.00002281
Iteration 72/1000 | Loss: 0.00002209
Iteration 73/1000 | Loss: 0.00007659
Iteration 74/1000 | Loss: 0.00008484
Iteration 75/1000 | Loss: 0.00005817
Iteration 76/1000 | Loss: 0.00002719
Iteration 77/1000 | Loss: 0.00002226
Iteration 78/1000 | Loss: 0.00005761
Iteration 79/1000 | Loss: 0.00007735
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00008648
Iteration 83/1000 | Loss: 0.00001804
Iteration 84/1000 | Loss: 0.00009664
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00003306
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00005781
Iteration 91/1000 | Loss: 0.00016558
Iteration 92/1000 | Loss: 0.00004228
Iteration 93/1000 | Loss: 0.00002188
Iteration 94/1000 | Loss: 0.00001980
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00016100
Iteration 97/1000 | Loss: 0.00002250
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00002175
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001618
Iteration 105/1000 | Loss: 0.00008701
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001593
Iteration 109/1000 | Loss: 0.00001590
Iteration 110/1000 | Loss: 0.00001590
Iteration 111/1000 | Loss: 0.00001589
Iteration 112/1000 | Loss: 0.00001588
Iteration 113/1000 | Loss: 0.00001588
Iteration 114/1000 | Loss: 0.00001588
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00008444
Iteration 117/1000 | Loss: 0.00016064
Iteration 118/1000 | Loss: 0.00001926
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00001626
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001498
Iteration 123/1000 | Loss: 0.00001471
Iteration 124/1000 | Loss: 0.00001462
Iteration 125/1000 | Loss: 0.00001461
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001445
Iteration 134/1000 | Loss: 0.00001445
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001441
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001439
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001434
Iteration 144/1000 | Loss: 0.00001434
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001433
Iteration 148/1000 | Loss: 0.00001432
Iteration 149/1000 | Loss: 0.00001432
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001431
Iteration 153/1000 | Loss: 0.00001431
Iteration 154/1000 | Loss: 0.00001431
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001431
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001430
Iteration 163/1000 | Loss: 0.00001430
Iteration 164/1000 | Loss: 0.00001429
Iteration 165/1000 | Loss: 0.00001429
Iteration 166/1000 | Loss: 0.00001429
Iteration 167/1000 | Loss: 0.00001429
Iteration 168/1000 | Loss: 0.00001429
Iteration 169/1000 | Loss: 0.00001429
Iteration 170/1000 | Loss: 0.00001428
Iteration 171/1000 | Loss: 0.00001428
Iteration 172/1000 | Loss: 0.00001428
Iteration 173/1000 | Loss: 0.00001427
Iteration 174/1000 | Loss: 0.00001427
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001425
Iteration 182/1000 | Loss: 0.00001425
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001424
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001423
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001423
Iteration 196/1000 | Loss: 0.00001423
Iteration 197/1000 | Loss: 0.00001423
Iteration 198/1000 | Loss: 0.00001423
Iteration 199/1000 | Loss: 0.00001423
Iteration 200/1000 | Loss: 0.00001423
Iteration 201/1000 | Loss: 0.00001423
Iteration 202/1000 | Loss: 0.00001423
Iteration 203/1000 | Loss: 0.00001423
Iteration 204/1000 | Loss: 0.00001422
Iteration 205/1000 | Loss: 0.00001422
Iteration 206/1000 | Loss: 0.00001422
Iteration 207/1000 | Loss: 0.00001422
Iteration 208/1000 | Loss: 0.00001422
Iteration 209/1000 | Loss: 0.00001422
Iteration 210/1000 | Loss: 0.00001422
Iteration 211/1000 | Loss: 0.00001422
Iteration 212/1000 | Loss: 0.00001422
Iteration 213/1000 | Loss: 0.00001422
Iteration 214/1000 | Loss: 0.00001422
Iteration 215/1000 | Loss: 0.00001422
Iteration 216/1000 | Loss: 0.00001422
Iteration 217/1000 | Loss: 0.00001422
Iteration 218/1000 | Loss: 0.00001421
Iteration 219/1000 | Loss: 0.00001421
Iteration 220/1000 | Loss: 0.00001421
Iteration 221/1000 | Loss: 0.00001421
Iteration 222/1000 | Loss: 0.00001421
Iteration 223/1000 | Loss: 0.00001421
Iteration 224/1000 | Loss: 0.00001421
Iteration 225/1000 | Loss: 0.00001421
Iteration 226/1000 | Loss: 0.00001421
Iteration 227/1000 | Loss: 0.00001421
Iteration 228/1000 | Loss: 0.00001421
Iteration 229/1000 | Loss: 0.00001421
Iteration 230/1000 | Loss: 0.00001421
Iteration 231/1000 | Loss: 0.00001421
Iteration 232/1000 | Loss: 0.00001421
Iteration 233/1000 | Loss: 0.00001420
Iteration 234/1000 | Loss: 0.00001420
Iteration 235/1000 | Loss: 0.00001420
Iteration 236/1000 | Loss: 0.00001420
Iteration 237/1000 | Loss: 0.00001420
Iteration 238/1000 | Loss: 0.00001420
Iteration 239/1000 | Loss: 0.00001420
Iteration 240/1000 | Loss: 0.00001420
Iteration 241/1000 | Loss: 0.00001420
Iteration 242/1000 | Loss: 0.00001420
Iteration 243/1000 | Loss: 0.00001420
Iteration 244/1000 | Loss: 0.00001420
Iteration 245/1000 | Loss: 0.00001419
Iteration 246/1000 | Loss: 0.00001419
Iteration 247/1000 | Loss: 0.00001419
Iteration 248/1000 | Loss: 0.00001419
Iteration 249/1000 | Loss: 0.00001419
Iteration 250/1000 | Loss: 0.00001419
Iteration 251/1000 | Loss: 0.00001419
Iteration 252/1000 | Loss: 0.00001419
Iteration 253/1000 | Loss: 0.00001419
Iteration 254/1000 | Loss: 0.00001419
Iteration 255/1000 | Loss: 0.00001419
Iteration 256/1000 | Loss: 0.00001419
Iteration 257/1000 | Loss: 0.00001419
Iteration 258/1000 | Loss: 0.00001419
Iteration 259/1000 | Loss: 0.00001419
Iteration 260/1000 | Loss: 0.00001419
Iteration 261/1000 | Loss: 0.00001419
Iteration 262/1000 | Loss: 0.00001419
Iteration 263/1000 | Loss: 0.00001418
Iteration 264/1000 | Loss: 0.00001418
Iteration 265/1000 | Loss: 0.00001418
Iteration 266/1000 | Loss: 0.00001418
Iteration 267/1000 | Loss: 0.00001418
Iteration 268/1000 | Loss: 0.00001418
Iteration 269/1000 | Loss: 0.00001418
Iteration 270/1000 | Loss: 0.00001418
Iteration 271/1000 | Loss: 0.00001418
Iteration 272/1000 | Loss: 0.00001418
Iteration 273/1000 | Loss: 0.00001418
Iteration 274/1000 | Loss: 0.00001418
Iteration 275/1000 | Loss: 0.00001418
Iteration 276/1000 | Loss: 0.00001418
Iteration 277/1000 | Loss: 0.00001418
Iteration 278/1000 | Loss: 0.00001418
Iteration 279/1000 | Loss: 0.00001418
Iteration 280/1000 | Loss: 0.00001418
Iteration 281/1000 | Loss: 0.00001418
Iteration 282/1000 | Loss: 0.00001418
Iteration 283/1000 | Loss: 0.00001418
Iteration 284/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.4177978300722316e-05, 1.4177978300722316e-05, 1.4177978300722316e-05, 1.4177978300722316e-05, 1.4177978300722316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4177978300722316e-05

Optimization complete. Final v2v error: 3.1448543071746826 mm

Highest mean error: 8.961499214172363 mm for frame 224

Lowest mean error: 2.716784954071045 mm for frame 41

Saving results

Total time: 240.14160823822021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822450
Iteration 2/25 | Loss: 0.00075011
Iteration 3/25 | Loss: 0.00062094
Iteration 4/25 | Loss: 0.00059969
Iteration 5/25 | Loss: 0.00059490
Iteration 6/25 | Loss: 0.00059392
Iteration 7/25 | Loss: 0.00059392
Iteration 8/25 | Loss: 0.00059392
Iteration 9/25 | Loss: 0.00059392
Iteration 10/25 | Loss: 0.00059392
Iteration 11/25 | Loss: 0.00059392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005939185502938926, 0.0005939185502938926, 0.0005939185502938926, 0.0005939185502938926, 0.0005939185502938926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005939185502938926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46098614
Iteration 2/25 | Loss: 0.00027735
Iteration 3/25 | Loss: 0.00027735
Iteration 4/25 | Loss: 0.00027735
Iteration 5/25 | Loss: 0.00027735
Iteration 6/25 | Loss: 0.00027735
Iteration 7/25 | Loss: 0.00027735
Iteration 8/25 | Loss: 0.00027735
Iteration 9/25 | Loss: 0.00027735
Iteration 10/25 | Loss: 0.00027735
Iteration 11/25 | Loss: 0.00027735
Iteration 12/25 | Loss: 0.00027735
Iteration 13/25 | Loss: 0.00027734
Iteration 14/25 | Loss: 0.00027734
Iteration 15/25 | Loss: 0.00027734
Iteration 16/25 | Loss: 0.00027734
Iteration 17/25 | Loss: 0.00027734
Iteration 18/25 | Loss: 0.00027734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002773449814412743, 0.0002773449814412743, 0.0002773449814412743, 0.0002773449814412743, 0.0002773449814412743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002773449814412743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027734
Iteration 2/1000 | Loss: 0.00002254
Iteration 3/1000 | Loss: 0.00001303
Iteration 4/1000 | Loss: 0.00001197
Iteration 5/1000 | Loss: 0.00001135
Iteration 6/1000 | Loss: 0.00001112
Iteration 7/1000 | Loss: 0.00001082
Iteration 8/1000 | Loss: 0.00001079
Iteration 9/1000 | Loss: 0.00001074
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001073
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001067
Iteration 15/1000 | Loss: 0.00001067
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001058
Iteration 19/1000 | Loss: 0.00001057
Iteration 20/1000 | Loss: 0.00001056
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001052
Iteration 24/1000 | Loss: 0.00001051
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001039
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001033
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001033
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001032
Iteration 62/1000 | Loss: 0.00001032
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001030
Iteration 66/1000 | Loss: 0.00001030
Iteration 67/1000 | Loss: 0.00001030
Iteration 68/1000 | Loss: 0.00001029
Iteration 69/1000 | Loss: 0.00001029
Iteration 70/1000 | Loss: 0.00001029
Iteration 71/1000 | Loss: 0.00001029
Iteration 72/1000 | Loss: 0.00001029
Iteration 73/1000 | Loss: 0.00001029
Iteration 74/1000 | Loss: 0.00001029
Iteration 75/1000 | Loss: 0.00001028
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001028
Iteration 78/1000 | Loss: 0.00001028
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001028
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001028
Iteration 84/1000 | Loss: 0.00001028
Iteration 85/1000 | Loss: 0.00001028
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001027
Iteration 88/1000 | Loss: 0.00001027
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001027
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001027
Iteration 96/1000 | Loss: 0.00001027
Iteration 97/1000 | Loss: 0.00001027
Iteration 98/1000 | Loss: 0.00001026
Iteration 99/1000 | Loss: 0.00001026
Iteration 100/1000 | Loss: 0.00001026
Iteration 101/1000 | Loss: 0.00001026
Iteration 102/1000 | Loss: 0.00001025
Iteration 103/1000 | Loss: 0.00001025
Iteration 104/1000 | Loss: 0.00001025
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001025
Iteration 108/1000 | Loss: 0.00001025
Iteration 109/1000 | Loss: 0.00001025
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001024
Iteration 112/1000 | Loss: 0.00001024
Iteration 113/1000 | Loss: 0.00001024
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001024
Iteration 116/1000 | Loss: 0.00001024
Iteration 117/1000 | Loss: 0.00001024
Iteration 118/1000 | Loss: 0.00001024
Iteration 119/1000 | Loss: 0.00001024
Iteration 120/1000 | Loss: 0.00001024
Iteration 121/1000 | Loss: 0.00001024
Iteration 122/1000 | Loss: 0.00001024
Iteration 123/1000 | Loss: 0.00001023
Iteration 124/1000 | Loss: 0.00001023
Iteration 125/1000 | Loss: 0.00001023
Iteration 126/1000 | Loss: 0.00001023
Iteration 127/1000 | Loss: 0.00001023
Iteration 128/1000 | Loss: 0.00001023
Iteration 129/1000 | Loss: 0.00001023
Iteration 130/1000 | Loss: 0.00001023
Iteration 131/1000 | Loss: 0.00001023
Iteration 132/1000 | Loss: 0.00001023
Iteration 133/1000 | Loss: 0.00001022
Iteration 134/1000 | Loss: 0.00001022
Iteration 135/1000 | Loss: 0.00001022
Iteration 136/1000 | Loss: 0.00001022
Iteration 137/1000 | Loss: 0.00001022
Iteration 138/1000 | Loss: 0.00001022
Iteration 139/1000 | Loss: 0.00001022
Iteration 140/1000 | Loss: 0.00001021
Iteration 141/1000 | Loss: 0.00001021
Iteration 142/1000 | Loss: 0.00001021
Iteration 143/1000 | Loss: 0.00001021
Iteration 144/1000 | Loss: 0.00001021
Iteration 145/1000 | Loss: 0.00001021
Iteration 146/1000 | Loss: 0.00001020
Iteration 147/1000 | Loss: 0.00001020
Iteration 148/1000 | Loss: 0.00001020
Iteration 149/1000 | Loss: 0.00001020
Iteration 150/1000 | Loss: 0.00001020
Iteration 151/1000 | Loss: 0.00001020
Iteration 152/1000 | Loss: 0.00001019
Iteration 153/1000 | Loss: 0.00001019
Iteration 154/1000 | Loss: 0.00001019
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001019
Iteration 168/1000 | Loss: 0.00001019
Iteration 169/1000 | Loss: 0.00001018
Iteration 170/1000 | Loss: 0.00001018
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001018
Iteration 174/1000 | Loss: 0.00001018
Iteration 175/1000 | Loss: 0.00001018
Iteration 176/1000 | Loss: 0.00001018
Iteration 177/1000 | Loss: 0.00001018
Iteration 178/1000 | Loss: 0.00001018
Iteration 179/1000 | Loss: 0.00001018
Iteration 180/1000 | Loss: 0.00001018
Iteration 181/1000 | Loss: 0.00001018
Iteration 182/1000 | Loss: 0.00001018
Iteration 183/1000 | Loss: 0.00001017
Iteration 184/1000 | Loss: 0.00001017
Iteration 185/1000 | Loss: 0.00001017
Iteration 186/1000 | Loss: 0.00001017
Iteration 187/1000 | Loss: 0.00001017
Iteration 188/1000 | Loss: 0.00001017
Iteration 189/1000 | Loss: 0.00001017
Iteration 190/1000 | Loss: 0.00001017
Iteration 191/1000 | Loss: 0.00001017
Iteration 192/1000 | Loss: 0.00001017
Iteration 193/1000 | Loss: 0.00001017
Iteration 194/1000 | Loss: 0.00001017
Iteration 195/1000 | Loss: 0.00001017
Iteration 196/1000 | Loss: 0.00001017
Iteration 197/1000 | Loss: 0.00001017
Iteration 198/1000 | Loss: 0.00001017
Iteration 199/1000 | Loss: 0.00001017
Iteration 200/1000 | Loss: 0.00001017
Iteration 201/1000 | Loss: 0.00001017
Iteration 202/1000 | Loss: 0.00001017
Iteration 203/1000 | Loss: 0.00001017
Iteration 204/1000 | Loss: 0.00001017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0173462214879692e-05, 1.0173462214879692e-05, 1.0173462214879692e-05, 1.0173462214879692e-05, 1.0173462214879692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0173462214879692e-05

Optimization complete. Final v2v error: 2.68145751953125 mm

Highest mean error: 2.84853458404541 mm for frame 86

Lowest mean error: 2.534125804901123 mm for frame 153

Saving results

Total time: 34.36924076080322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015813
Iteration 2/25 | Loss: 0.00321382
Iteration 3/25 | Loss: 0.00155927
Iteration 4/25 | Loss: 0.00140574
Iteration 5/25 | Loss: 0.00135296
Iteration 6/25 | Loss: 0.00133493
Iteration 7/25 | Loss: 0.00132735
Iteration 8/25 | Loss: 0.00132631
Iteration 9/25 | Loss: 0.00132457
Iteration 10/25 | Loss: 0.00132233
Iteration 11/25 | Loss: 0.00132350
Iteration 12/25 | Loss: 0.00131837
Iteration 13/25 | Loss: 0.00131689
Iteration 14/25 | Loss: 0.00131559
Iteration 15/25 | Loss: 0.00131238
Iteration 16/25 | Loss: 0.00131151
Iteration 17/25 | Loss: 0.00131113
Iteration 18/25 | Loss: 0.00131110
Iteration 19/25 | Loss: 0.00131109
Iteration 20/25 | Loss: 0.00131109
Iteration 21/25 | Loss: 0.00131109
Iteration 22/25 | Loss: 0.00131109
Iteration 23/25 | Loss: 0.00131109
Iteration 24/25 | Loss: 0.00131109
Iteration 25/25 | Loss: 0.00131109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42574120
Iteration 2/25 | Loss: 0.00488963
Iteration 3/25 | Loss: 0.00488963
Iteration 4/25 | Loss: 0.00488963
Iteration 5/25 | Loss: 0.00488963
Iteration 6/25 | Loss: 0.00488963
Iteration 7/25 | Loss: 0.00488963
Iteration 8/25 | Loss: 0.00488963
Iteration 9/25 | Loss: 0.00488963
Iteration 10/25 | Loss: 0.00488963
Iteration 11/25 | Loss: 0.00488963
Iteration 12/25 | Loss: 0.00488963
Iteration 13/25 | Loss: 0.00488963
Iteration 14/25 | Loss: 0.00488963
Iteration 15/25 | Loss: 0.00488963
Iteration 16/25 | Loss: 0.00488963
Iteration 17/25 | Loss: 0.00488963
Iteration 18/25 | Loss: 0.00488963
Iteration 19/25 | Loss: 0.00488963
Iteration 20/25 | Loss: 0.00488963
Iteration 21/25 | Loss: 0.00488963
Iteration 22/25 | Loss: 0.00488963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0048896316438913345, 0.0048896316438913345, 0.0048896316438913345, 0.0048896316438913345, 0.0048896316438913345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0048896316438913345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00488963
Iteration 2/1000 | Loss: 0.00086827
Iteration 3/1000 | Loss: 0.00059696
Iteration 4/1000 | Loss: 0.00048045
Iteration 5/1000 | Loss: 0.00042595
Iteration 6/1000 | Loss: 0.00037650
Iteration 7/1000 | Loss: 0.00034482
Iteration 8/1000 | Loss: 0.00032100
Iteration 9/1000 | Loss: 0.00030488
Iteration 10/1000 | Loss: 0.00209450
Iteration 11/1000 | Loss: 0.01485352
Iteration 12/1000 | Loss: 0.00073297
Iteration 13/1000 | Loss: 0.00035250
Iteration 14/1000 | Loss: 0.00024557
Iteration 15/1000 | Loss: 0.00017390
Iteration 16/1000 | Loss: 0.00011796
Iteration 17/1000 | Loss: 0.00008109
Iteration 18/1000 | Loss: 0.00005662
Iteration 19/1000 | Loss: 0.00004377
Iteration 20/1000 | Loss: 0.00003654
Iteration 21/1000 | Loss: 0.00013088
Iteration 22/1000 | Loss: 0.00002954
Iteration 23/1000 | Loss: 0.00002684
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002162
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00001914
Iteration 28/1000 | Loss: 0.00001842
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001684
Iteration 34/1000 | Loss: 0.00001682
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001674
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001667
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001664
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001663
Iteration 63/1000 | Loss: 0.00001663
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001662
Iteration 68/1000 | Loss: 0.00001662
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001662
Iteration 71/1000 | Loss: 0.00001662
Iteration 72/1000 | Loss: 0.00001662
Iteration 73/1000 | Loss: 0.00001662
Iteration 74/1000 | Loss: 0.00001662
Iteration 75/1000 | Loss: 0.00001662
Iteration 76/1000 | Loss: 0.00001662
Iteration 77/1000 | Loss: 0.00001662
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001661
Iteration 80/1000 | Loss: 0.00001661
Iteration 81/1000 | Loss: 0.00001661
Iteration 82/1000 | Loss: 0.00001661
Iteration 83/1000 | Loss: 0.00001661
Iteration 84/1000 | Loss: 0.00001661
Iteration 85/1000 | Loss: 0.00001661
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001661
Iteration 91/1000 | Loss: 0.00001661
Iteration 92/1000 | Loss: 0.00001661
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001659
Iteration 100/1000 | Loss: 0.00001659
Iteration 101/1000 | Loss: 0.00001659
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001658
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001658
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001658
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Iteration 122/1000 | Loss: 0.00001657
Iteration 123/1000 | Loss: 0.00001657
Iteration 124/1000 | Loss: 0.00001657
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.656671338423621e-05, 1.656671338423621e-05, 1.656671338423621e-05, 1.656671338423621e-05, 1.656671338423621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.656671338423621e-05

Optimization complete. Final v2v error: 3.4557180404663086 mm

Highest mean error: 3.855027437210083 mm for frame 223

Lowest mean error: 3.255096435546875 mm for frame 102

Saving results

Total time: 92.81645202636719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055541
Iteration 2/25 | Loss: 0.01055540
Iteration 3/25 | Loss: 0.00322051
Iteration 4/25 | Loss: 0.00227227
Iteration 5/25 | Loss: 0.00197486
Iteration 6/25 | Loss: 0.00143992
Iteration 7/25 | Loss: 0.00135897
Iteration 8/25 | Loss: 0.00123713
Iteration 9/25 | Loss: 0.00117225
Iteration 10/25 | Loss: 0.00111224
Iteration 11/25 | Loss: 0.00107332
Iteration 12/25 | Loss: 0.00106089
Iteration 13/25 | Loss: 0.00095874
Iteration 14/25 | Loss: 0.00090527
Iteration 15/25 | Loss: 0.00091461
Iteration 16/25 | Loss: 0.00085473
Iteration 17/25 | Loss: 0.00084959
Iteration 18/25 | Loss: 0.00083335
Iteration 19/25 | Loss: 0.00081708
Iteration 20/25 | Loss: 0.00081175
Iteration 21/25 | Loss: 0.00082840
Iteration 22/25 | Loss: 0.00080960
Iteration 23/25 | Loss: 0.00081171
Iteration 24/25 | Loss: 0.00080143
Iteration 25/25 | Loss: 0.00079868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51993036
Iteration 2/25 | Loss: 0.00114871
Iteration 3/25 | Loss: 0.00098944
Iteration 4/25 | Loss: 0.00098944
Iteration 5/25 | Loss: 0.00098944
Iteration 6/25 | Loss: 0.00098944
Iteration 7/25 | Loss: 0.00098944
Iteration 8/25 | Loss: 0.00098944
Iteration 9/25 | Loss: 0.00098944
Iteration 10/25 | Loss: 0.00098944
Iteration 11/25 | Loss: 0.00098944
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009894357062876225, 0.0009894357062876225, 0.0009894357062876225, 0.0009894357062876225, 0.0009894357062876225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009894357062876225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098944
Iteration 2/1000 | Loss: 0.00118392
Iteration 3/1000 | Loss: 0.00113202
Iteration 4/1000 | Loss: 0.00158749
Iteration 5/1000 | Loss: 0.00040641
Iteration 6/1000 | Loss: 0.00064758
Iteration 7/1000 | Loss: 0.00072448
Iteration 8/1000 | Loss: 0.00021341
Iteration 9/1000 | Loss: 0.00021679
Iteration 10/1000 | Loss: 0.00090113
Iteration 11/1000 | Loss: 0.00035669
Iteration 12/1000 | Loss: 0.00097813
Iteration 13/1000 | Loss: 0.00073752
Iteration 14/1000 | Loss: 0.00082466
Iteration 15/1000 | Loss: 0.00030163
Iteration 16/1000 | Loss: 0.00042877
Iteration 17/1000 | Loss: 0.00024242
Iteration 18/1000 | Loss: 0.00013377
Iteration 19/1000 | Loss: 0.00047270
Iteration 20/1000 | Loss: 0.00022737
Iteration 21/1000 | Loss: 0.00033775
Iteration 22/1000 | Loss: 0.00025678
Iteration 23/1000 | Loss: 0.00029061
Iteration 24/1000 | Loss: 0.00041045
Iteration 25/1000 | Loss: 0.00029908
Iteration 26/1000 | Loss: 0.00024400
Iteration 27/1000 | Loss: 0.00042138
Iteration 28/1000 | Loss: 0.00099562
Iteration 29/1000 | Loss: 0.00046380
Iteration 30/1000 | Loss: 0.00027063
Iteration 31/1000 | Loss: 0.00036987
Iteration 32/1000 | Loss: 0.00012277
Iteration 33/1000 | Loss: 0.00009348
Iteration 34/1000 | Loss: 0.00015979
Iteration 35/1000 | Loss: 0.00021613
Iteration 36/1000 | Loss: 0.00018287
Iteration 37/1000 | Loss: 0.00017075
Iteration 38/1000 | Loss: 0.00029336
Iteration 39/1000 | Loss: 0.00037356
Iteration 40/1000 | Loss: 0.00026186
Iteration 41/1000 | Loss: 0.00014508
Iteration 42/1000 | Loss: 0.00012201
Iteration 43/1000 | Loss: 0.00005398
Iteration 44/1000 | Loss: 0.00004022
Iteration 45/1000 | Loss: 0.00005071
Iteration 46/1000 | Loss: 0.00025389
Iteration 47/1000 | Loss: 0.00050413
Iteration 48/1000 | Loss: 0.00003278
Iteration 49/1000 | Loss: 0.00033821
Iteration 50/1000 | Loss: 0.00013001
Iteration 51/1000 | Loss: 0.00014771
Iteration 52/1000 | Loss: 0.00002997
Iteration 53/1000 | Loss: 0.00002781
Iteration 54/1000 | Loss: 0.00007376
Iteration 55/1000 | Loss: 0.00002573
Iteration 56/1000 | Loss: 0.00005052
Iteration 57/1000 | Loss: 0.00002428
Iteration 58/1000 | Loss: 0.00012769
Iteration 59/1000 | Loss: 0.00025823
Iteration 60/1000 | Loss: 0.00013195
Iteration 61/1000 | Loss: 0.00003178
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00014103
Iteration 64/1000 | Loss: 0.00008986
Iteration 65/1000 | Loss: 0.00006427
Iteration 66/1000 | Loss: 0.00005483
Iteration 67/1000 | Loss: 0.00004150
Iteration 68/1000 | Loss: 0.00002312
Iteration 69/1000 | Loss: 0.00002293
Iteration 70/1000 | Loss: 0.00002277
Iteration 71/1000 | Loss: 0.00002272
Iteration 72/1000 | Loss: 0.00007378
Iteration 73/1000 | Loss: 0.00034414
Iteration 74/1000 | Loss: 0.00002290
Iteration 75/1000 | Loss: 0.00002246
Iteration 76/1000 | Loss: 0.00002240
Iteration 77/1000 | Loss: 0.00002240
Iteration 78/1000 | Loss: 0.00002240
Iteration 79/1000 | Loss: 0.00002240
Iteration 80/1000 | Loss: 0.00002240
Iteration 81/1000 | Loss: 0.00002240
Iteration 82/1000 | Loss: 0.00002240
Iteration 83/1000 | Loss: 0.00002240
Iteration 84/1000 | Loss: 0.00002240
Iteration 85/1000 | Loss: 0.00002240
Iteration 86/1000 | Loss: 0.00002239
Iteration 87/1000 | Loss: 0.00002239
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002239
Iteration 90/1000 | Loss: 0.00002239
Iteration 91/1000 | Loss: 0.00002238
Iteration 92/1000 | Loss: 0.00002238
Iteration 93/1000 | Loss: 0.00002238
Iteration 94/1000 | Loss: 0.00002238
Iteration 95/1000 | Loss: 0.00002238
Iteration 96/1000 | Loss: 0.00002238
Iteration 97/1000 | Loss: 0.00002237
Iteration 98/1000 | Loss: 0.00005269
Iteration 99/1000 | Loss: 0.00002294
Iteration 100/1000 | Loss: 0.00002238
Iteration 101/1000 | Loss: 0.00002237
Iteration 102/1000 | Loss: 0.00002237
Iteration 103/1000 | Loss: 0.00002237
Iteration 104/1000 | Loss: 0.00002237
Iteration 105/1000 | Loss: 0.00002236
Iteration 106/1000 | Loss: 0.00002236
Iteration 107/1000 | Loss: 0.00002236
Iteration 108/1000 | Loss: 0.00002235
Iteration 109/1000 | Loss: 0.00002235
Iteration 110/1000 | Loss: 0.00002235
Iteration 111/1000 | Loss: 0.00003038
Iteration 112/1000 | Loss: 0.00002518
Iteration 113/1000 | Loss: 0.00002464
Iteration 114/1000 | Loss: 0.00002233
Iteration 115/1000 | Loss: 0.00002233
Iteration 116/1000 | Loss: 0.00002233
Iteration 117/1000 | Loss: 0.00002233
Iteration 118/1000 | Loss: 0.00002233
Iteration 119/1000 | Loss: 0.00002233
Iteration 120/1000 | Loss: 0.00002233
Iteration 121/1000 | Loss: 0.00002233
Iteration 122/1000 | Loss: 0.00002233
Iteration 123/1000 | Loss: 0.00002232
Iteration 124/1000 | Loss: 0.00002232
Iteration 125/1000 | Loss: 0.00002232
Iteration 126/1000 | Loss: 0.00002232
Iteration 127/1000 | Loss: 0.00002232
Iteration 128/1000 | Loss: 0.00002232
Iteration 129/1000 | Loss: 0.00002231
Iteration 130/1000 | Loss: 0.00002231
Iteration 131/1000 | Loss: 0.00002231
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002230
Iteration 134/1000 | Loss: 0.00002230
Iteration 135/1000 | Loss: 0.00002230
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002229
Iteration 138/1000 | Loss: 0.00002229
Iteration 139/1000 | Loss: 0.00002229
Iteration 140/1000 | Loss: 0.00002229
Iteration 141/1000 | Loss: 0.00002228
Iteration 142/1000 | Loss: 0.00002228
Iteration 143/1000 | Loss: 0.00002228
Iteration 144/1000 | Loss: 0.00002228
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002228
Iteration 148/1000 | Loss: 0.00002228
Iteration 149/1000 | Loss: 0.00002228
Iteration 150/1000 | Loss: 0.00002228
Iteration 151/1000 | Loss: 0.00002228
Iteration 152/1000 | Loss: 0.00002228
Iteration 153/1000 | Loss: 0.00002228
Iteration 154/1000 | Loss: 0.00002228
Iteration 155/1000 | Loss: 0.00002228
Iteration 156/1000 | Loss: 0.00002228
Iteration 157/1000 | Loss: 0.00002227
Iteration 158/1000 | Loss: 0.00002227
Iteration 159/1000 | Loss: 0.00002226
Iteration 160/1000 | Loss: 0.00002226
Iteration 161/1000 | Loss: 0.00002226
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Iteration 165/1000 | Loss: 0.00002224
Iteration 166/1000 | Loss: 0.00002224
Iteration 167/1000 | Loss: 0.00002224
Iteration 168/1000 | Loss: 0.00002224
Iteration 169/1000 | Loss: 0.00002224
Iteration 170/1000 | Loss: 0.00002224
Iteration 171/1000 | Loss: 0.00002224
Iteration 172/1000 | Loss: 0.00002224
Iteration 173/1000 | Loss: 0.00002224
Iteration 174/1000 | Loss: 0.00002224
Iteration 175/1000 | Loss: 0.00002224
Iteration 176/1000 | Loss: 0.00002224
Iteration 177/1000 | Loss: 0.00002224
Iteration 178/1000 | Loss: 0.00002224
Iteration 179/1000 | Loss: 0.00002224
Iteration 180/1000 | Loss: 0.00002224
Iteration 181/1000 | Loss: 0.00002224
Iteration 182/1000 | Loss: 0.00002224
Iteration 183/1000 | Loss: 0.00002224
Iteration 184/1000 | Loss: 0.00002224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.223570118076168e-05, 2.223570118076168e-05, 2.223570118076168e-05, 2.223570118076168e-05, 2.223570118076168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.223570118076168e-05

Optimization complete. Final v2v error: 3.565717935562134 mm

Highest mean error: 11.471688270568848 mm for frame 199

Lowest mean error: 3.12996244430542 mm for frame 13

Saving results

Total time: 178.6553978919983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408888
Iteration 2/25 | Loss: 0.00088853
Iteration 3/25 | Loss: 0.00074509
Iteration 4/25 | Loss: 0.00071480
Iteration 5/25 | Loss: 0.00070492
Iteration 6/25 | Loss: 0.00070351
Iteration 7/25 | Loss: 0.00070303
Iteration 8/25 | Loss: 0.00070298
Iteration 9/25 | Loss: 0.00070298
Iteration 10/25 | Loss: 0.00070298
Iteration 11/25 | Loss: 0.00070298
Iteration 12/25 | Loss: 0.00070298
Iteration 13/25 | Loss: 0.00070298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007029800326563418, 0.0007029800326563418, 0.0007029800326563418, 0.0007029800326563418, 0.0007029800326563418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007029800326563418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45405638
Iteration 2/25 | Loss: 0.00030991
Iteration 3/25 | Loss: 0.00030991
Iteration 4/25 | Loss: 0.00030991
Iteration 5/25 | Loss: 0.00030991
Iteration 6/25 | Loss: 0.00030991
Iteration 7/25 | Loss: 0.00030991
Iteration 8/25 | Loss: 0.00030991
Iteration 9/25 | Loss: 0.00030991
Iteration 10/25 | Loss: 0.00030991
Iteration 11/25 | Loss: 0.00030991
Iteration 12/25 | Loss: 0.00030991
Iteration 13/25 | Loss: 0.00030991
Iteration 14/25 | Loss: 0.00030991
Iteration 15/25 | Loss: 0.00030991
Iteration 16/25 | Loss: 0.00030991
Iteration 17/25 | Loss: 0.00030991
Iteration 18/25 | Loss: 0.00030991
Iteration 19/25 | Loss: 0.00030991
Iteration 20/25 | Loss: 0.00030991
Iteration 21/25 | Loss: 0.00030991
Iteration 22/25 | Loss: 0.00030991
Iteration 23/25 | Loss: 0.00030991
Iteration 24/25 | Loss: 0.00030991
Iteration 25/25 | Loss: 0.00030991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030991
Iteration 2/1000 | Loss: 0.00004444
Iteration 3/1000 | Loss: 0.00003251
Iteration 4/1000 | Loss: 0.00003065
Iteration 5/1000 | Loss: 0.00002918
Iteration 6/1000 | Loss: 0.00002822
Iteration 7/1000 | Loss: 0.00002747
Iteration 8/1000 | Loss: 0.00002696
Iteration 9/1000 | Loss: 0.00002675
Iteration 10/1000 | Loss: 0.00002652
Iteration 11/1000 | Loss: 0.00002635
Iteration 12/1000 | Loss: 0.00002620
Iteration 13/1000 | Loss: 0.00002619
Iteration 14/1000 | Loss: 0.00002615
Iteration 15/1000 | Loss: 0.00002615
Iteration 16/1000 | Loss: 0.00002613
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00002610
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00002609
Iteration 22/1000 | Loss: 0.00002609
Iteration 23/1000 | Loss: 0.00002609
Iteration 24/1000 | Loss: 0.00002608
Iteration 25/1000 | Loss: 0.00002607
Iteration 26/1000 | Loss: 0.00002604
Iteration 27/1000 | Loss: 0.00002604
Iteration 28/1000 | Loss: 0.00002604
Iteration 29/1000 | Loss: 0.00002604
Iteration 30/1000 | Loss: 0.00002603
Iteration 31/1000 | Loss: 0.00002602
Iteration 32/1000 | Loss: 0.00002601
Iteration 33/1000 | Loss: 0.00002597
Iteration 34/1000 | Loss: 0.00002597
Iteration 35/1000 | Loss: 0.00002597
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00002593
Iteration 38/1000 | Loss: 0.00002590
Iteration 39/1000 | Loss: 0.00002588
Iteration 40/1000 | Loss: 0.00002587
Iteration 41/1000 | Loss: 0.00002586
Iteration 42/1000 | Loss: 0.00002585
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002584
Iteration 45/1000 | Loss: 0.00002584
Iteration 46/1000 | Loss: 0.00002584
Iteration 47/1000 | Loss: 0.00002584
Iteration 48/1000 | Loss: 0.00002584
Iteration 49/1000 | Loss: 0.00002584
Iteration 50/1000 | Loss: 0.00002584
Iteration 51/1000 | Loss: 0.00002583
Iteration 52/1000 | Loss: 0.00002583
Iteration 53/1000 | Loss: 0.00002583
Iteration 54/1000 | Loss: 0.00002583
Iteration 55/1000 | Loss: 0.00002582
Iteration 56/1000 | Loss: 0.00002582
Iteration 57/1000 | Loss: 0.00002582
Iteration 58/1000 | Loss: 0.00002582
Iteration 59/1000 | Loss: 0.00002581
Iteration 60/1000 | Loss: 0.00002581
Iteration 61/1000 | Loss: 0.00002581
Iteration 62/1000 | Loss: 0.00002581
Iteration 63/1000 | Loss: 0.00002581
Iteration 64/1000 | Loss: 0.00002581
Iteration 65/1000 | Loss: 0.00002581
Iteration 66/1000 | Loss: 0.00002581
Iteration 67/1000 | Loss: 0.00002580
Iteration 68/1000 | Loss: 0.00002580
Iteration 69/1000 | Loss: 0.00002580
Iteration 70/1000 | Loss: 0.00002580
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002580
Iteration 73/1000 | Loss: 0.00002580
Iteration 74/1000 | Loss: 0.00002580
Iteration 75/1000 | Loss: 0.00002580
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002580
Iteration 78/1000 | Loss: 0.00002580
Iteration 79/1000 | Loss: 0.00002580
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002579
Iteration 82/1000 | Loss: 0.00002579
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002579
Iteration 85/1000 | Loss: 0.00002579
Iteration 86/1000 | Loss: 0.00002579
Iteration 87/1000 | Loss: 0.00002579
Iteration 88/1000 | Loss: 0.00002579
Iteration 89/1000 | Loss: 0.00002579
Iteration 90/1000 | Loss: 0.00002579
Iteration 91/1000 | Loss: 0.00002579
Iteration 92/1000 | Loss: 0.00002579
Iteration 93/1000 | Loss: 0.00002578
Iteration 94/1000 | Loss: 0.00002578
Iteration 95/1000 | Loss: 0.00002578
Iteration 96/1000 | Loss: 0.00002578
Iteration 97/1000 | Loss: 0.00002578
Iteration 98/1000 | Loss: 0.00002578
Iteration 99/1000 | Loss: 0.00002578
Iteration 100/1000 | Loss: 0.00002578
Iteration 101/1000 | Loss: 0.00002578
Iteration 102/1000 | Loss: 0.00002577
Iteration 103/1000 | Loss: 0.00002577
Iteration 104/1000 | Loss: 0.00002577
Iteration 105/1000 | Loss: 0.00002577
Iteration 106/1000 | Loss: 0.00002577
Iteration 107/1000 | Loss: 0.00002577
Iteration 108/1000 | Loss: 0.00002577
Iteration 109/1000 | Loss: 0.00002577
Iteration 110/1000 | Loss: 0.00002577
Iteration 111/1000 | Loss: 0.00002577
Iteration 112/1000 | Loss: 0.00002577
Iteration 113/1000 | Loss: 0.00002577
Iteration 114/1000 | Loss: 0.00002577
Iteration 115/1000 | Loss: 0.00002576
Iteration 116/1000 | Loss: 0.00002576
Iteration 117/1000 | Loss: 0.00002576
Iteration 118/1000 | Loss: 0.00002576
Iteration 119/1000 | Loss: 0.00002576
Iteration 120/1000 | Loss: 0.00002576
Iteration 121/1000 | Loss: 0.00002576
Iteration 122/1000 | Loss: 0.00002576
Iteration 123/1000 | Loss: 0.00002576
Iteration 124/1000 | Loss: 0.00002576
Iteration 125/1000 | Loss: 0.00002576
Iteration 126/1000 | Loss: 0.00002575
Iteration 127/1000 | Loss: 0.00002575
Iteration 128/1000 | Loss: 0.00002575
Iteration 129/1000 | Loss: 0.00002575
Iteration 130/1000 | Loss: 0.00002574
Iteration 131/1000 | Loss: 0.00002574
Iteration 132/1000 | Loss: 0.00002574
Iteration 133/1000 | Loss: 0.00002574
Iteration 134/1000 | Loss: 0.00002574
Iteration 135/1000 | Loss: 0.00002574
Iteration 136/1000 | Loss: 0.00002574
Iteration 137/1000 | Loss: 0.00002574
Iteration 138/1000 | Loss: 0.00002574
Iteration 139/1000 | Loss: 0.00002574
Iteration 140/1000 | Loss: 0.00002574
Iteration 141/1000 | Loss: 0.00002574
Iteration 142/1000 | Loss: 0.00002573
Iteration 143/1000 | Loss: 0.00002573
Iteration 144/1000 | Loss: 0.00002573
Iteration 145/1000 | Loss: 0.00002573
Iteration 146/1000 | Loss: 0.00002573
Iteration 147/1000 | Loss: 0.00002573
Iteration 148/1000 | Loss: 0.00002573
Iteration 149/1000 | Loss: 0.00002573
Iteration 150/1000 | Loss: 0.00002573
Iteration 151/1000 | Loss: 0.00002573
Iteration 152/1000 | Loss: 0.00002573
Iteration 153/1000 | Loss: 0.00002572
Iteration 154/1000 | Loss: 0.00002572
Iteration 155/1000 | Loss: 0.00002572
Iteration 156/1000 | Loss: 0.00002572
Iteration 157/1000 | Loss: 0.00002572
Iteration 158/1000 | Loss: 0.00002572
Iteration 159/1000 | Loss: 0.00002572
Iteration 160/1000 | Loss: 0.00002572
Iteration 161/1000 | Loss: 0.00002572
Iteration 162/1000 | Loss: 0.00002572
Iteration 163/1000 | Loss: 0.00002572
Iteration 164/1000 | Loss: 0.00002572
Iteration 165/1000 | Loss: 0.00002572
Iteration 166/1000 | Loss: 0.00002572
Iteration 167/1000 | Loss: 0.00002572
Iteration 168/1000 | Loss: 0.00002572
Iteration 169/1000 | Loss: 0.00002572
Iteration 170/1000 | Loss: 0.00002572
Iteration 171/1000 | Loss: 0.00002572
Iteration 172/1000 | Loss: 0.00002572
Iteration 173/1000 | Loss: 0.00002571
Iteration 174/1000 | Loss: 0.00002571
Iteration 175/1000 | Loss: 0.00002571
Iteration 176/1000 | Loss: 0.00002571
Iteration 177/1000 | Loss: 0.00002571
Iteration 178/1000 | Loss: 0.00002571
Iteration 179/1000 | Loss: 0.00002571
Iteration 180/1000 | Loss: 0.00002571
Iteration 181/1000 | Loss: 0.00002571
Iteration 182/1000 | Loss: 0.00002571
Iteration 183/1000 | Loss: 0.00002571
Iteration 184/1000 | Loss: 0.00002571
Iteration 185/1000 | Loss: 0.00002571
Iteration 186/1000 | Loss: 0.00002571
Iteration 187/1000 | Loss: 0.00002571
Iteration 188/1000 | Loss: 0.00002571
Iteration 189/1000 | Loss: 0.00002571
Iteration 190/1000 | Loss: 0.00002571
Iteration 191/1000 | Loss: 0.00002571
Iteration 192/1000 | Loss: 0.00002571
Iteration 193/1000 | Loss: 0.00002571
Iteration 194/1000 | Loss: 0.00002571
Iteration 195/1000 | Loss: 0.00002571
Iteration 196/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.5709734472911805e-05, 2.5709734472911805e-05, 2.5709734472911805e-05, 2.5709734472911805e-05, 2.5709734472911805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5709734472911805e-05

Optimization complete. Final v2v error: 4.197175025939941 mm

Highest mean error: 4.58112907409668 mm for frame 15

Lowest mean error: 3.80949330329895 mm for frame 33

Saving results

Total time: 39.62541151046753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041444
Iteration 2/25 | Loss: 0.00308191
Iteration 3/25 | Loss: 0.00151735
Iteration 4/25 | Loss: 0.00131876
Iteration 5/25 | Loss: 0.00127059
Iteration 6/25 | Loss: 0.00125512
Iteration 7/25 | Loss: 0.00123313
Iteration 8/25 | Loss: 0.00120778
Iteration 9/25 | Loss: 0.00118754
Iteration 10/25 | Loss: 0.00117787
Iteration 11/25 | Loss: 0.00117067
Iteration 12/25 | Loss: 0.00116164
Iteration 13/25 | Loss: 0.00114681
Iteration 14/25 | Loss: 0.00112895
Iteration 15/25 | Loss: 0.00112270
Iteration 16/25 | Loss: 0.00112150
Iteration 17/25 | Loss: 0.00112376
Iteration 18/25 | Loss: 0.00111981
Iteration 19/25 | Loss: 0.00111908
Iteration 20/25 | Loss: 0.00111892
Iteration 21/25 | Loss: 0.00111885
Iteration 22/25 | Loss: 0.00111885
Iteration 23/25 | Loss: 0.00111885
Iteration 24/25 | Loss: 0.00111884
Iteration 25/25 | Loss: 0.00111884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42986441
Iteration 2/25 | Loss: 0.00358288
Iteration 3/25 | Loss: 0.00358288
Iteration 4/25 | Loss: 0.00358288
Iteration 5/25 | Loss: 0.00358288
Iteration 6/25 | Loss: 0.00358288
Iteration 7/25 | Loss: 0.00358288
Iteration 8/25 | Loss: 0.00358288
Iteration 9/25 | Loss: 0.00358288
Iteration 10/25 | Loss: 0.00358288
Iteration 11/25 | Loss: 0.00358288
Iteration 12/25 | Loss: 0.00358288
Iteration 13/25 | Loss: 0.00358288
Iteration 14/25 | Loss: 0.00358288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003582875244319439, 0.003582875244319439, 0.003582875244319439, 0.003582875244319439, 0.003582875244319439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003582875244319439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00358288
Iteration 2/1000 | Loss: 0.00081028
Iteration 3/1000 | Loss: 0.00073140
Iteration 4/1000 | Loss: 0.00044697
Iteration 5/1000 | Loss: 0.00036908
Iteration 6/1000 | Loss: 0.00031174
Iteration 7/1000 | Loss: 0.00082043
Iteration 8/1000 | Loss: 0.00030440
Iteration 9/1000 | Loss: 0.00037714
Iteration 10/1000 | Loss: 0.00026580
Iteration 11/1000 | Loss: 0.00020280
Iteration 12/1000 | Loss: 0.00028025
Iteration 13/1000 | Loss: 0.00020540
Iteration 14/1000 | Loss: 0.00065087
Iteration 15/1000 | Loss: 0.00018737
Iteration 16/1000 | Loss: 0.00094975
Iteration 17/1000 | Loss: 0.01469190
Iteration 18/1000 | Loss: 0.00863404
Iteration 19/1000 | Loss: 0.00073230
Iteration 20/1000 | Loss: 0.00067772
Iteration 21/1000 | Loss: 0.00039985
Iteration 22/1000 | Loss: 0.00099222
Iteration 23/1000 | Loss: 0.00048090
Iteration 24/1000 | Loss: 0.00040022
Iteration 25/1000 | Loss: 0.00013066
Iteration 26/1000 | Loss: 0.00066768
Iteration 27/1000 | Loss: 0.00030306
Iteration 28/1000 | Loss: 0.00008289
Iteration 29/1000 | Loss: 0.00020098
Iteration 30/1000 | Loss: 0.00005694
Iteration 31/1000 | Loss: 0.00028710
Iteration 32/1000 | Loss: 0.00017885
Iteration 33/1000 | Loss: 0.00010778
Iteration 34/1000 | Loss: 0.00031135
Iteration 35/1000 | Loss: 0.00028330
Iteration 36/1000 | Loss: 0.00026553
Iteration 37/1000 | Loss: 0.00029588
Iteration 38/1000 | Loss: 0.00029338
Iteration 39/1000 | Loss: 0.00084140
Iteration 40/1000 | Loss: 0.00031040
Iteration 41/1000 | Loss: 0.00024767
Iteration 42/1000 | Loss: 0.00028061
Iteration 43/1000 | Loss: 0.00025484
Iteration 44/1000 | Loss: 0.00006291
Iteration 45/1000 | Loss: 0.00002629
Iteration 46/1000 | Loss: 0.00002344
Iteration 47/1000 | Loss: 0.00001987
Iteration 48/1000 | Loss: 0.00001802
Iteration 49/1000 | Loss: 0.00001654
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001497
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001370
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001340
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001340
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001337
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001334
Iteration 97/1000 | Loss: 0.00001334
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001333
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001331
Iteration 109/1000 | Loss: 0.00001331
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001328
Iteration 138/1000 | Loss: 0.00001328
Iteration 139/1000 | Loss: 0.00001328
Iteration 140/1000 | Loss: 0.00001328
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001328
Iteration 143/1000 | Loss: 0.00001328
Iteration 144/1000 | Loss: 0.00001328
Iteration 145/1000 | Loss: 0.00001328
Iteration 146/1000 | Loss: 0.00001328
Iteration 147/1000 | Loss: 0.00001328
Iteration 148/1000 | Loss: 0.00001328
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001326
Iteration 167/1000 | Loss: 0.00001326
Iteration 168/1000 | Loss: 0.00001326
Iteration 169/1000 | Loss: 0.00001326
Iteration 170/1000 | Loss: 0.00001326
Iteration 171/1000 | Loss: 0.00001326
Iteration 172/1000 | Loss: 0.00001326
Iteration 173/1000 | Loss: 0.00001326
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3262153515825048e-05, 1.3262153515825048e-05, 1.3262153515825048e-05, 1.3262153515825048e-05, 1.3262153515825048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3262153515825048e-05

Optimization complete. Final v2v error: 3.10890793800354 mm

Highest mean error: 3.826510429382324 mm for frame 142

Lowest mean error: 2.9502949714660645 mm for frame 159

Saving results

Total time: 134.97798800468445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045898
Iteration 2/25 | Loss: 0.00194731
Iteration 3/25 | Loss: 0.00136530
Iteration 4/25 | Loss: 0.00123162
Iteration 5/25 | Loss: 0.00109371
Iteration 6/25 | Loss: 0.00128187
Iteration 7/25 | Loss: 0.00113514
Iteration 8/25 | Loss: 0.00100743
Iteration 9/25 | Loss: 0.00087317
Iteration 10/25 | Loss: 0.00080169
Iteration 11/25 | Loss: 0.00074925
Iteration 12/25 | Loss: 0.00072484
Iteration 13/25 | Loss: 0.00070181
Iteration 14/25 | Loss: 0.00069510
Iteration 15/25 | Loss: 0.00068867
Iteration 16/25 | Loss: 0.00068103
Iteration 17/25 | Loss: 0.00067322
Iteration 18/25 | Loss: 0.00066906
Iteration 19/25 | Loss: 0.00066479
Iteration 20/25 | Loss: 0.00066250
Iteration 21/25 | Loss: 0.00066205
Iteration 22/25 | Loss: 0.00066191
Iteration 23/25 | Loss: 0.00066183
Iteration 24/25 | Loss: 0.00066135
Iteration 25/25 | Loss: 0.00065984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50313222
Iteration 2/25 | Loss: 0.00022441
Iteration 3/25 | Loss: 0.00022441
Iteration 4/25 | Loss: 0.00022441
Iteration 5/25 | Loss: 0.00022441
Iteration 6/25 | Loss: 0.00022441
Iteration 7/25 | Loss: 0.00022441
Iteration 8/25 | Loss: 0.00022441
Iteration 9/25 | Loss: 0.00022441
Iteration 10/25 | Loss: 0.00022441
Iteration 11/25 | Loss: 0.00022441
Iteration 12/25 | Loss: 0.00022441
Iteration 13/25 | Loss: 0.00022441
Iteration 14/25 | Loss: 0.00022441
Iteration 15/25 | Loss: 0.00022441
Iteration 16/25 | Loss: 0.00022441
Iteration 17/25 | Loss: 0.00022441
Iteration 18/25 | Loss: 0.00022441
Iteration 19/25 | Loss: 0.00022441
Iteration 20/25 | Loss: 0.00022441
Iteration 21/25 | Loss: 0.00022441
Iteration 22/25 | Loss: 0.00022441
Iteration 23/25 | Loss: 0.00022441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00022440942120738328, 0.00022440942120738328, 0.00022440942120738328, 0.00022440942120738328, 0.00022440942120738328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022440942120738328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022441
Iteration 2/1000 | Loss: 0.00003549
Iteration 3/1000 | Loss: 0.00002839
Iteration 4/1000 | Loss: 0.00002537
Iteration 5/1000 | Loss: 0.00002335
Iteration 6/1000 | Loss: 0.00002211
Iteration 7/1000 | Loss: 0.00002558
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002056
Iteration 10/1000 | Loss: 0.00002013
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00002013
Iteration 14/1000 | Loss: 0.00002042
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001853
Iteration 17/1000 | Loss: 0.00001822
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001776
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001772
Iteration 32/1000 | Loss: 0.00001772
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001771
Iteration 35/1000 | Loss: 0.00001771
Iteration 36/1000 | Loss: 0.00001771
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001770
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001768
Iteration 47/1000 | Loss: 0.00001768
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001767
Iteration 51/1000 | Loss: 0.00001767
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001766
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001765
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001764
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001763
Iteration 75/1000 | Loss: 0.00001763
Iteration 76/1000 | Loss: 0.00001763
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001763
Iteration 79/1000 | Loss: 0.00001763
Iteration 80/1000 | Loss: 0.00001763
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.762790088832844e-05, 1.762790088832844e-05, 1.762790088832844e-05, 1.762790088832844e-05, 1.762790088832844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.762790088832844e-05

Optimization complete. Final v2v error: 3.5070197582244873 mm

Highest mean error: 4.032642841339111 mm for frame 150

Lowest mean error: 3.2750325202941895 mm for frame 42

Saving results

Total time: 76.56058311462402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935832
Iteration 2/25 | Loss: 0.00103892
Iteration 3/25 | Loss: 0.00072265
Iteration 4/25 | Loss: 0.00066675
Iteration 5/25 | Loss: 0.00065153
Iteration 6/25 | Loss: 0.00064945
Iteration 7/25 | Loss: 0.00064934
Iteration 8/25 | Loss: 0.00064934
Iteration 9/25 | Loss: 0.00064934
Iteration 10/25 | Loss: 0.00064934
Iteration 11/25 | Loss: 0.00064934
Iteration 12/25 | Loss: 0.00064934
Iteration 13/25 | Loss: 0.00064934
Iteration 14/25 | Loss: 0.00064934
Iteration 15/25 | Loss: 0.00064934
Iteration 16/25 | Loss: 0.00064934
Iteration 17/25 | Loss: 0.00064934
Iteration 18/25 | Loss: 0.00064934
Iteration 19/25 | Loss: 0.00064934
Iteration 20/25 | Loss: 0.00064934
Iteration 21/25 | Loss: 0.00064934
Iteration 22/25 | Loss: 0.00064934
Iteration 23/25 | Loss: 0.00064934
Iteration 24/25 | Loss: 0.00064934
Iteration 25/25 | Loss: 0.00064934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45688617
Iteration 2/25 | Loss: 0.00017731
Iteration 3/25 | Loss: 0.00017728
Iteration 4/25 | Loss: 0.00017728
Iteration 5/25 | Loss: 0.00017728
Iteration 6/25 | Loss: 0.00017728
Iteration 7/25 | Loss: 0.00017728
Iteration 8/25 | Loss: 0.00017728
Iteration 9/25 | Loss: 0.00017728
Iteration 10/25 | Loss: 0.00017728
Iteration 11/25 | Loss: 0.00017728
Iteration 12/25 | Loss: 0.00017728
Iteration 13/25 | Loss: 0.00017728
Iteration 14/25 | Loss: 0.00017728
Iteration 15/25 | Loss: 0.00017728
Iteration 16/25 | Loss: 0.00017728
Iteration 17/25 | Loss: 0.00017728
Iteration 18/25 | Loss: 0.00017728
Iteration 19/25 | Loss: 0.00017728
Iteration 20/25 | Loss: 0.00017728
Iteration 21/25 | Loss: 0.00017728
Iteration 22/25 | Loss: 0.00017728
Iteration 23/25 | Loss: 0.00017728
Iteration 24/25 | Loss: 0.00017728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00017727749946061522, 0.00017727749946061522, 0.00017727749946061522, 0.00017727749946061522, 0.00017727749946061522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017727749946061522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017728
Iteration 2/1000 | Loss: 0.00002316
Iteration 3/1000 | Loss: 0.00001823
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001299
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001297
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001294
Iteration 141/1000 | Loss: 0.00001294
Iteration 142/1000 | Loss: 0.00001294
Iteration 143/1000 | Loss: 0.00001294
Iteration 144/1000 | Loss: 0.00001294
Iteration 145/1000 | Loss: 0.00001294
Iteration 146/1000 | Loss: 0.00001294
Iteration 147/1000 | Loss: 0.00001294
Iteration 148/1000 | Loss: 0.00001294
Iteration 149/1000 | Loss: 0.00001294
Iteration 150/1000 | Loss: 0.00001294
Iteration 151/1000 | Loss: 0.00001294
Iteration 152/1000 | Loss: 0.00001294
Iteration 153/1000 | Loss: 0.00001294
Iteration 154/1000 | Loss: 0.00001294
Iteration 155/1000 | Loss: 0.00001294
Iteration 156/1000 | Loss: 0.00001294
Iteration 157/1000 | Loss: 0.00001294
Iteration 158/1000 | Loss: 0.00001294
Iteration 159/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.2935758604726288e-05, 1.2935758604726288e-05, 1.2935758604726288e-05, 1.2935758604726288e-05, 1.2935758604726288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2935758604726288e-05

Optimization complete. Final v2v error: 3.097179651260376 mm

Highest mean error: 3.295264482498169 mm for frame 96

Lowest mean error: 2.835050106048584 mm for frame 166

Saving results

Total time: 39.506258964538574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845159
Iteration 2/25 | Loss: 0.00105502
Iteration 3/25 | Loss: 0.00074660
Iteration 4/25 | Loss: 0.00070598
Iteration 5/25 | Loss: 0.00069986
Iteration 6/25 | Loss: 0.00069871
Iteration 7/25 | Loss: 0.00069871
Iteration 8/25 | Loss: 0.00069871
Iteration 9/25 | Loss: 0.00069871
Iteration 10/25 | Loss: 0.00069871
Iteration 11/25 | Loss: 0.00069871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006987094529904425, 0.0006987094529904425, 0.0006987094529904425, 0.0006987094529904425, 0.0006987094529904425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006987094529904425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97342098
Iteration 2/25 | Loss: 0.00027670
Iteration 3/25 | Loss: 0.00027670
Iteration 4/25 | Loss: 0.00027670
Iteration 5/25 | Loss: 0.00027670
Iteration 6/25 | Loss: 0.00027670
Iteration 7/25 | Loss: 0.00027670
Iteration 8/25 | Loss: 0.00027669
Iteration 9/25 | Loss: 0.00027669
Iteration 10/25 | Loss: 0.00027669
Iteration 11/25 | Loss: 0.00027669
Iteration 12/25 | Loss: 0.00027669
Iteration 13/25 | Loss: 0.00027669
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00027669474366120994, 0.00027669474366120994, 0.00027669474366120994, 0.00027669474366120994, 0.00027669474366120994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027669474366120994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027669
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00002479
Iteration 4/1000 | Loss: 0.00002322
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002116
Iteration 7/1000 | Loss: 0.00002080
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002043
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002037
Iteration 13/1000 | Loss: 0.00002037
Iteration 14/1000 | Loss: 0.00002031
Iteration 15/1000 | Loss: 0.00002028
Iteration 16/1000 | Loss: 0.00002026
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002025
Iteration 19/1000 | Loss: 0.00002025
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002023
Iteration 22/1000 | Loss: 0.00002022
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002022
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00002021
Iteration 29/1000 | Loss: 0.00002020
Iteration 30/1000 | Loss: 0.00002020
Iteration 31/1000 | Loss: 0.00002020
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002020
Iteration 34/1000 | Loss: 0.00002019
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002019
Iteration 39/1000 | Loss: 0.00002019
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002018
Iteration 42/1000 | Loss: 0.00002018
Iteration 43/1000 | Loss: 0.00002018
Iteration 44/1000 | Loss: 0.00002018
Iteration 45/1000 | Loss: 0.00002018
Iteration 46/1000 | Loss: 0.00002018
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002018
Iteration 49/1000 | Loss: 0.00002018
Iteration 50/1000 | Loss: 0.00002018
Iteration 51/1000 | Loss: 0.00002017
Iteration 52/1000 | Loss: 0.00002017
Iteration 53/1000 | Loss: 0.00002017
Iteration 54/1000 | Loss: 0.00002016
Iteration 55/1000 | Loss: 0.00002016
Iteration 56/1000 | Loss: 0.00002016
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00002016
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00002016
Iteration 61/1000 | Loss: 0.00002016
Iteration 62/1000 | Loss: 0.00002016
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002016
Iteration 65/1000 | Loss: 0.00002016
Iteration 66/1000 | Loss: 0.00002016
Iteration 67/1000 | Loss: 0.00002016
Iteration 68/1000 | Loss: 0.00002016
Iteration 69/1000 | Loss: 0.00002016
Iteration 70/1000 | Loss: 0.00002016
Iteration 71/1000 | Loss: 0.00002016
Iteration 72/1000 | Loss: 0.00002016
Iteration 73/1000 | Loss: 0.00002016
Iteration 74/1000 | Loss: 0.00002016
Iteration 75/1000 | Loss: 0.00002016
Iteration 76/1000 | Loss: 0.00002016
Iteration 77/1000 | Loss: 0.00002016
Iteration 78/1000 | Loss: 0.00002016
Iteration 79/1000 | Loss: 0.00002016
Iteration 80/1000 | Loss: 0.00002016
Iteration 81/1000 | Loss: 0.00002016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.0157205653958954e-05, 2.0157205653958954e-05, 2.0157205653958954e-05, 2.0157205653958954e-05, 2.0157205653958954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0157205653958954e-05

Optimization complete. Final v2v error: 3.8423585891723633 mm

Highest mean error: 4.170629978179932 mm for frame 0

Lowest mean error: 3.6694562435150146 mm for frame 205

Saving results

Total time: 29.26036047935486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527710
Iteration 2/25 | Loss: 0.00106228
Iteration 3/25 | Loss: 0.00077764
Iteration 4/25 | Loss: 0.00074102
Iteration 5/25 | Loss: 0.00073427
Iteration 6/25 | Loss: 0.00073263
Iteration 7/25 | Loss: 0.00073227
Iteration 8/25 | Loss: 0.00073227
Iteration 9/25 | Loss: 0.00073227
Iteration 10/25 | Loss: 0.00073227
Iteration 11/25 | Loss: 0.00073227
Iteration 12/25 | Loss: 0.00073227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000732271233573556, 0.000732271233573556, 0.000732271233573556, 0.000732271233573556, 0.000732271233573556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000732271233573556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.17740917
Iteration 2/25 | Loss: 0.00031456
Iteration 3/25 | Loss: 0.00031456
Iteration 4/25 | Loss: 0.00031456
Iteration 5/25 | Loss: 0.00031456
Iteration 6/25 | Loss: 0.00031456
Iteration 7/25 | Loss: 0.00031456
Iteration 8/25 | Loss: 0.00031456
Iteration 9/25 | Loss: 0.00031456
Iteration 10/25 | Loss: 0.00031456
Iteration 11/25 | Loss: 0.00031456
Iteration 12/25 | Loss: 0.00031456
Iteration 13/25 | Loss: 0.00031456
Iteration 14/25 | Loss: 0.00031456
Iteration 15/25 | Loss: 0.00031456
Iteration 16/25 | Loss: 0.00031456
Iteration 17/25 | Loss: 0.00031456
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003145579539705068, 0.0003145579539705068, 0.0003145579539705068, 0.0003145579539705068, 0.0003145579539705068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003145579539705068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031456
Iteration 2/1000 | Loss: 0.00003392
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00002564
Iteration 5/1000 | Loss: 0.00002481
Iteration 6/1000 | Loss: 0.00002404
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002330
Iteration 9/1000 | Loss: 0.00002316
Iteration 10/1000 | Loss: 0.00002310
Iteration 11/1000 | Loss: 0.00002307
Iteration 12/1000 | Loss: 0.00002307
Iteration 13/1000 | Loss: 0.00002300
Iteration 14/1000 | Loss: 0.00002291
Iteration 15/1000 | Loss: 0.00002289
Iteration 16/1000 | Loss: 0.00002287
Iteration 17/1000 | Loss: 0.00002286
Iteration 18/1000 | Loss: 0.00002286
Iteration 19/1000 | Loss: 0.00002286
Iteration 20/1000 | Loss: 0.00002285
Iteration 21/1000 | Loss: 0.00002285
Iteration 22/1000 | Loss: 0.00002285
Iteration 23/1000 | Loss: 0.00002284
Iteration 24/1000 | Loss: 0.00002284
Iteration 25/1000 | Loss: 0.00002283
Iteration 26/1000 | Loss: 0.00002283
Iteration 27/1000 | Loss: 0.00002282
Iteration 28/1000 | Loss: 0.00002282
Iteration 29/1000 | Loss: 0.00002282
Iteration 30/1000 | Loss: 0.00002282
Iteration 31/1000 | Loss: 0.00002282
Iteration 32/1000 | Loss: 0.00002282
Iteration 33/1000 | Loss: 0.00002282
Iteration 34/1000 | Loss: 0.00002282
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002282
Iteration 39/1000 | Loss: 0.00002282
Iteration 40/1000 | Loss: 0.00002282
Iteration 41/1000 | Loss: 0.00002282
Iteration 42/1000 | Loss: 0.00002282
Iteration 43/1000 | Loss: 0.00002282
Iteration 44/1000 | Loss: 0.00002281
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002281
Iteration 48/1000 | Loss: 0.00002280
Iteration 49/1000 | Loss: 0.00002280
Iteration 50/1000 | Loss: 0.00002279
Iteration 51/1000 | Loss: 0.00002279
Iteration 52/1000 | Loss: 0.00002279
Iteration 53/1000 | Loss: 0.00002279
Iteration 54/1000 | Loss: 0.00002279
Iteration 55/1000 | Loss: 0.00002279
Iteration 56/1000 | Loss: 0.00002279
Iteration 57/1000 | Loss: 0.00002279
Iteration 58/1000 | Loss: 0.00002279
Iteration 59/1000 | Loss: 0.00002279
Iteration 60/1000 | Loss: 0.00002278
Iteration 61/1000 | Loss: 0.00002278
Iteration 62/1000 | Loss: 0.00002278
Iteration 63/1000 | Loss: 0.00002278
Iteration 64/1000 | Loss: 0.00002277
Iteration 65/1000 | Loss: 0.00002277
Iteration 66/1000 | Loss: 0.00002277
Iteration 67/1000 | Loss: 0.00002277
Iteration 68/1000 | Loss: 0.00002277
Iteration 69/1000 | Loss: 0.00002277
Iteration 70/1000 | Loss: 0.00002276
Iteration 71/1000 | Loss: 0.00002276
Iteration 72/1000 | Loss: 0.00002276
Iteration 73/1000 | Loss: 0.00002276
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002276
Iteration 76/1000 | Loss: 0.00002276
Iteration 77/1000 | Loss: 0.00002276
Iteration 78/1000 | Loss: 0.00002276
Iteration 79/1000 | Loss: 0.00002276
Iteration 80/1000 | Loss: 0.00002275
Iteration 81/1000 | Loss: 0.00002275
Iteration 82/1000 | Loss: 0.00002275
Iteration 83/1000 | Loss: 0.00002275
Iteration 84/1000 | Loss: 0.00002275
Iteration 85/1000 | Loss: 0.00002275
Iteration 86/1000 | Loss: 0.00002275
Iteration 87/1000 | Loss: 0.00002275
Iteration 88/1000 | Loss: 0.00002275
Iteration 89/1000 | Loss: 0.00002275
Iteration 90/1000 | Loss: 0.00002274
Iteration 91/1000 | Loss: 0.00002274
Iteration 92/1000 | Loss: 0.00002274
Iteration 93/1000 | Loss: 0.00002274
Iteration 94/1000 | Loss: 0.00002274
Iteration 95/1000 | Loss: 0.00002274
Iteration 96/1000 | Loss: 0.00002274
Iteration 97/1000 | Loss: 0.00002274
Iteration 98/1000 | Loss: 0.00002274
Iteration 99/1000 | Loss: 0.00002274
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002273
Iteration 102/1000 | Loss: 0.00002273
Iteration 103/1000 | Loss: 0.00002273
Iteration 104/1000 | Loss: 0.00002273
Iteration 105/1000 | Loss: 0.00002273
Iteration 106/1000 | Loss: 0.00002273
Iteration 107/1000 | Loss: 0.00002273
Iteration 108/1000 | Loss: 0.00002273
Iteration 109/1000 | Loss: 0.00002273
Iteration 110/1000 | Loss: 0.00002273
Iteration 111/1000 | Loss: 0.00002273
Iteration 112/1000 | Loss: 0.00002273
Iteration 113/1000 | Loss: 0.00002273
Iteration 114/1000 | Loss: 0.00002273
Iteration 115/1000 | Loss: 0.00002273
Iteration 116/1000 | Loss: 0.00002273
Iteration 117/1000 | Loss: 0.00002273
Iteration 118/1000 | Loss: 0.00002273
Iteration 119/1000 | Loss: 0.00002273
Iteration 120/1000 | Loss: 0.00002273
Iteration 121/1000 | Loss: 0.00002273
Iteration 122/1000 | Loss: 0.00002273
Iteration 123/1000 | Loss: 0.00002273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.2731770513928495e-05, 2.2731770513928495e-05, 2.2731770513928495e-05, 2.2731770513928495e-05, 2.2731770513928495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2731770513928495e-05

Optimization complete. Final v2v error: 4.019284725189209 mm

Highest mean error: 4.496761798858643 mm for frame 58

Lowest mean error: 3.680053472518921 mm for frame 51

Saving results

Total time: 29.8458890914917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828695
Iteration 2/25 | Loss: 0.00103334
Iteration 3/25 | Loss: 0.00076292
Iteration 4/25 | Loss: 0.00071398
Iteration 5/25 | Loss: 0.00069761
Iteration 6/25 | Loss: 0.00069334
Iteration 7/25 | Loss: 0.00069309
Iteration 8/25 | Loss: 0.00069309
Iteration 9/25 | Loss: 0.00069309
Iteration 10/25 | Loss: 0.00069309
Iteration 11/25 | Loss: 0.00069309
Iteration 12/25 | Loss: 0.00069309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006930919480510056, 0.0006930919480510056, 0.0006930919480510056, 0.0006930919480510056, 0.0006930919480510056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006930919480510056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86652100
Iteration 2/25 | Loss: 0.00034427
Iteration 3/25 | Loss: 0.00034427
Iteration 4/25 | Loss: 0.00034427
Iteration 5/25 | Loss: 0.00034427
Iteration 6/25 | Loss: 0.00034427
Iteration 7/25 | Loss: 0.00034427
Iteration 8/25 | Loss: 0.00034427
Iteration 9/25 | Loss: 0.00034426
Iteration 10/25 | Loss: 0.00034426
Iteration 11/25 | Loss: 0.00034426
Iteration 12/25 | Loss: 0.00034426
Iteration 13/25 | Loss: 0.00034426
Iteration 14/25 | Loss: 0.00034426
Iteration 15/25 | Loss: 0.00034426
Iteration 16/25 | Loss: 0.00034426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00034426478669047356, 0.00034426478669047356, 0.00034426478669047356, 0.00034426478669047356, 0.00034426478669047356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034426478669047356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034426
Iteration 2/1000 | Loss: 0.00003336
Iteration 3/1000 | Loss: 0.00002393
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001859
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00001817
Iteration 12/1000 | Loss: 0.00001813
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001791
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001788
Iteration 19/1000 | Loss: 0.00001787
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001783
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001774
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001773
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001771
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001769
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001769
Iteration 63/1000 | Loss: 0.00001768
Iteration 64/1000 | Loss: 0.00001768
Iteration 65/1000 | Loss: 0.00001768
Iteration 66/1000 | Loss: 0.00001767
Iteration 67/1000 | Loss: 0.00001767
Iteration 68/1000 | Loss: 0.00001767
Iteration 69/1000 | Loss: 0.00001767
Iteration 70/1000 | Loss: 0.00001766
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001765
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001764
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00001764
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001763
Iteration 87/1000 | Loss: 0.00001763
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.7634009054745547e-05, 1.7634009054745547e-05, 1.7634009054745547e-05, 1.7634009054745547e-05, 1.7634009054745547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7634009054745547e-05

Optimization complete. Final v2v error: 3.570828437805176 mm

Highest mean error: 4.135568618774414 mm for frame 169

Lowest mean error: 3.2039005756378174 mm for frame 42

Saving results

Total time: 36.57906222343445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_023/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_023/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101860
Iteration 2/25 | Loss: 0.00173912
Iteration 3/25 | Loss: 0.00100215
Iteration 4/25 | Loss: 0.00089826
Iteration 5/25 | Loss: 0.00087822
Iteration 6/25 | Loss: 0.00085506
Iteration 7/25 | Loss: 0.00081301
Iteration 8/25 | Loss: 0.00080221
Iteration 9/25 | Loss: 0.00079433
Iteration 10/25 | Loss: 0.00078878
Iteration 11/25 | Loss: 0.00078407
Iteration 12/25 | Loss: 0.00078685
Iteration 13/25 | Loss: 0.00078472
Iteration 14/25 | Loss: 0.00078142
Iteration 15/25 | Loss: 0.00078343
Iteration 16/25 | Loss: 0.00078059
Iteration 17/25 | Loss: 0.00077774
Iteration 18/25 | Loss: 0.00077321
Iteration 19/25 | Loss: 0.00077132
Iteration 20/25 | Loss: 0.00077738
Iteration 21/25 | Loss: 0.00077666
Iteration 22/25 | Loss: 0.00078053
Iteration 23/25 | Loss: 0.00077801
Iteration 24/25 | Loss: 0.00077228
Iteration 25/25 | Loss: 0.00076899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24868345
Iteration 2/25 | Loss: 0.00040948
Iteration 3/25 | Loss: 0.00040947
Iteration 4/25 | Loss: 0.00039684
Iteration 5/25 | Loss: 0.00039683
Iteration 6/25 | Loss: 0.00039683
Iteration 7/25 | Loss: 0.00039683
Iteration 8/25 | Loss: 0.00039683
Iteration 9/25 | Loss: 0.00039683
Iteration 10/25 | Loss: 0.00039683
Iteration 11/25 | Loss: 0.00039683
Iteration 12/25 | Loss: 0.00039683
Iteration 13/25 | Loss: 0.00039683
Iteration 14/25 | Loss: 0.00039683
Iteration 15/25 | Loss: 0.00039683
Iteration 16/25 | Loss: 0.00039683
Iteration 17/25 | Loss: 0.00039683
Iteration 18/25 | Loss: 0.00039683
Iteration 19/25 | Loss: 0.00039683
Iteration 20/25 | Loss: 0.00039683
Iteration 21/25 | Loss: 0.00039683
Iteration 22/25 | Loss: 0.00039683
Iteration 23/25 | Loss: 0.00039683
Iteration 24/25 | Loss: 0.00039683
Iteration 25/25 | Loss: 0.00039683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039683
Iteration 2/1000 | Loss: 0.00004283
Iteration 3/1000 | Loss: 0.00022887
Iteration 4/1000 | Loss: 0.00023173
Iteration 5/1000 | Loss: 0.00011656
Iteration 6/1000 | Loss: 0.00030582
Iteration 7/1000 | Loss: 0.00031952
Iteration 8/1000 | Loss: 0.00023812
Iteration 9/1000 | Loss: 0.00032266
Iteration 10/1000 | Loss: 0.00015662
Iteration 11/1000 | Loss: 0.00010074
Iteration 12/1000 | Loss: 0.00029995
Iteration 13/1000 | Loss: 0.00038460
Iteration 14/1000 | Loss: 0.00020322
Iteration 15/1000 | Loss: 0.00021003
Iteration 16/1000 | Loss: 0.00017618
Iteration 17/1000 | Loss: 0.00015312
Iteration 18/1000 | Loss: 0.00005697
Iteration 19/1000 | Loss: 0.00022177
Iteration 20/1000 | Loss: 0.00009461
Iteration 21/1000 | Loss: 0.00024063
Iteration 22/1000 | Loss: 0.00020139
Iteration 23/1000 | Loss: 0.00015375
Iteration 24/1000 | Loss: 0.00030084
Iteration 25/1000 | Loss: 0.00030381
Iteration 26/1000 | Loss: 0.00003629
Iteration 27/1000 | Loss: 0.00009979
Iteration 28/1000 | Loss: 0.00024084
Iteration 29/1000 | Loss: 0.00009099
Iteration 30/1000 | Loss: 0.00005055
Iteration 31/1000 | Loss: 0.00004212
Iteration 32/1000 | Loss: 0.00003599
Iteration 33/1000 | Loss: 0.00003132
Iteration 34/1000 | Loss: 0.00003008
Iteration 35/1000 | Loss: 0.00002908
Iteration 36/1000 | Loss: 0.00005300
Iteration 37/1000 | Loss: 0.00003161
Iteration 38/1000 | Loss: 0.00003233
Iteration 39/1000 | Loss: 0.00002737
Iteration 40/1000 | Loss: 0.00004874
Iteration 41/1000 | Loss: 0.00002662
Iteration 42/1000 | Loss: 0.00003560
Iteration 43/1000 | Loss: 0.00026816
Iteration 44/1000 | Loss: 0.00011884
Iteration 45/1000 | Loss: 0.00011319
Iteration 46/1000 | Loss: 0.00002807
Iteration 47/1000 | Loss: 0.00005046
Iteration 48/1000 | Loss: 0.00002567
Iteration 49/1000 | Loss: 0.00018268
Iteration 50/1000 | Loss: 0.00004485
Iteration 51/1000 | Loss: 0.00003897
Iteration 52/1000 | Loss: 0.00003185
Iteration 53/1000 | Loss: 0.00002878
Iteration 54/1000 | Loss: 0.00002630
Iteration 55/1000 | Loss: 0.00002529
Iteration 56/1000 | Loss: 0.00005513
Iteration 57/1000 | Loss: 0.00002322
Iteration 58/1000 | Loss: 0.00002240
Iteration 59/1000 | Loss: 0.00003506
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002071
Iteration 63/1000 | Loss: 0.00002647
Iteration 64/1000 | Loss: 0.00002065
Iteration 65/1000 | Loss: 0.00002065
Iteration 66/1000 | Loss: 0.00002064
Iteration 67/1000 | Loss: 0.00002063
Iteration 68/1000 | Loss: 0.00002063
Iteration 69/1000 | Loss: 0.00002063
Iteration 70/1000 | Loss: 0.00002062
Iteration 71/1000 | Loss: 0.00002062
Iteration 72/1000 | Loss: 0.00002062
Iteration 73/1000 | Loss: 0.00002062
Iteration 74/1000 | Loss: 0.00002062
Iteration 75/1000 | Loss: 0.00002062
Iteration 76/1000 | Loss: 0.00002062
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002061
Iteration 81/1000 | Loss: 0.00002061
Iteration 82/1000 | Loss: 0.00002061
Iteration 83/1000 | Loss: 0.00002061
Iteration 84/1000 | Loss: 0.00002061
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002058
Iteration 90/1000 | Loss: 0.00002058
Iteration 91/1000 | Loss: 0.00002057
Iteration 92/1000 | Loss: 0.00002057
Iteration 93/1000 | Loss: 0.00002056
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002055
Iteration 96/1000 | Loss: 0.00002055
Iteration 97/1000 | Loss: 0.00002055
Iteration 98/1000 | Loss: 0.00002055
Iteration 99/1000 | Loss: 0.00002055
Iteration 100/1000 | Loss: 0.00002055
Iteration 101/1000 | Loss: 0.00002055
Iteration 102/1000 | Loss: 0.00002054
Iteration 103/1000 | Loss: 0.00002054
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002054
Iteration 106/1000 | Loss: 0.00002054
Iteration 107/1000 | Loss: 0.00002054
Iteration 108/1000 | Loss: 0.00002054
Iteration 109/1000 | Loss: 0.00002054
Iteration 110/1000 | Loss: 0.00002054
Iteration 111/1000 | Loss: 0.00002054
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002053
Iteration 115/1000 | Loss: 0.00002053
Iteration 116/1000 | Loss: 0.00002053
Iteration 117/1000 | Loss: 0.00002052
Iteration 118/1000 | Loss: 0.00002052
Iteration 119/1000 | Loss: 0.00002051
Iteration 120/1000 | Loss: 0.00002051
Iteration 121/1000 | Loss: 0.00002051
Iteration 122/1000 | Loss: 0.00002051
Iteration 123/1000 | Loss: 0.00002051
Iteration 124/1000 | Loss: 0.00002051
Iteration 125/1000 | Loss: 0.00002050
Iteration 126/1000 | Loss: 0.00002050
Iteration 127/1000 | Loss: 0.00002050
Iteration 128/1000 | Loss: 0.00002050
Iteration 129/1000 | Loss: 0.00002049
Iteration 130/1000 | Loss: 0.00002049
Iteration 131/1000 | Loss: 0.00002049
Iteration 132/1000 | Loss: 0.00002049
Iteration 133/1000 | Loss: 0.00002048
Iteration 134/1000 | Loss: 0.00002048
Iteration 135/1000 | Loss: 0.00002047
Iteration 136/1000 | Loss: 0.00002047
Iteration 137/1000 | Loss: 0.00002047
Iteration 138/1000 | Loss: 0.00002047
Iteration 139/1000 | Loss: 0.00002047
Iteration 140/1000 | Loss: 0.00002047
Iteration 141/1000 | Loss: 0.00002047
Iteration 142/1000 | Loss: 0.00002047
Iteration 143/1000 | Loss: 0.00002047
Iteration 144/1000 | Loss: 0.00002047
Iteration 145/1000 | Loss: 0.00002047
Iteration 146/1000 | Loss: 0.00002047
Iteration 147/1000 | Loss: 0.00002047
Iteration 148/1000 | Loss: 0.00002047
Iteration 149/1000 | Loss: 0.00002047
Iteration 150/1000 | Loss: 0.00002046
Iteration 151/1000 | Loss: 0.00002046
Iteration 152/1000 | Loss: 0.00002046
Iteration 153/1000 | Loss: 0.00002046
Iteration 154/1000 | Loss: 0.00002046
Iteration 155/1000 | Loss: 0.00002046
Iteration 156/1000 | Loss: 0.00002046
Iteration 157/1000 | Loss: 0.00002046
Iteration 158/1000 | Loss: 0.00002046
Iteration 159/1000 | Loss: 0.00002046
Iteration 160/1000 | Loss: 0.00002046
Iteration 161/1000 | Loss: 0.00002046
Iteration 162/1000 | Loss: 0.00002046
Iteration 163/1000 | Loss: 0.00002046
Iteration 164/1000 | Loss: 0.00002045
Iteration 165/1000 | Loss: 0.00002045
Iteration 166/1000 | Loss: 0.00002045
Iteration 167/1000 | Loss: 0.00002045
Iteration 168/1000 | Loss: 0.00002045
Iteration 169/1000 | Loss: 0.00002045
Iteration 170/1000 | Loss: 0.00002045
Iteration 171/1000 | Loss: 0.00002045
Iteration 172/1000 | Loss: 0.00002045
Iteration 173/1000 | Loss: 0.00002045
Iteration 174/1000 | Loss: 0.00002045
Iteration 175/1000 | Loss: 0.00002045
Iteration 176/1000 | Loss: 0.00002045
Iteration 177/1000 | Loss: 0.00002045
Iteration 178/1000 | Loss: 0.00002045
Iteration 179/1000 | Loss: 0.00002045
Iteration 180/1000 | Loss: 0.00002045
Iteration 181/1000 | Loss: 0.00002045
Iteration 182/1000 | Loss: 0.00002045
Iteration 183/1000 | Loss: 0.00002044
Iteration 184/1000 | Loss: 0.00002044
Iteration 185/1000 | Loss: 0.00002044
Iteration 186/1000 | Loss: 0.00002044
Iteration 187/1000 | Loss: 0.00002044
Iteration 188/1000 | Loss: 0.00002044
Iteration 189/1000 | Loss: 0.00002044
Iteration 190/1000 | Loss: 0.00002044
Iteration 191/1000 | Loss: 0.00002044
Iteration 192/1000 | Loss: 0.00002044
Iteration 193/1000 | Loss: 0.00002044
Iteration 194/1000 | Loss: 0.00002044
Iteration 195/1000 | Loss: 0.00002044
Iteration 196/1000 | Loss: 0.00002044
Iteration 197/1000 | Loss: 0.00002043
Iteration 198/1000 | Loss: 0.00002043
Iteration 199/1000 | Loss: 0.00002043
Iteration 200/1000 | Loss: 0.00002043
Iteration 201/1000 | Loss: 0.00002043
Iteration 202/1000 | Loss: 0.00002043
Iteration 203/1000 | Loss: 0.00002043
Iteration 204/1000 | Loss: 0.00002043
Iteration 205/1000 | Loss: 0.00002043
Iteration 206/1000 | Loss: 0.00002043
Iteration 207/1000 | Loss: 0.00002043
Iteration 208/1000 | Loss: 0.00002043
Iteration 209/1000 | Loss: 0.00002043
Iteration 210/1000 | Loss: 0.00002043
Iteration 211/1000 | Loss: 0.00002042
Iteration 212/1000 | Loss: 0.00002042
Iteration 213/1000 | Loss: 0.00002042
Iteration 214/1000 | Loss: 0.00002042
Iteration 215/1000 | Loss: 0.00002042
Iteration 216/1000 | Loss: 0.00002042
Iteration 217/1000 | Loss: 0.00002042
Iteration 218/1000 | Loss: 0.00002042
Iteration 219/1000 | Loss: 0.00002042
Iteration 220/1000 | Loss: 0.00002042
Iteration 221/1000 | Loss: 0.00002042
Iteration 222/1000 | Loss: 0.00002042
Iteration 223/1000 | Loss: 0.00002042
Iteration 224/1000 | Loss: 0.00002042
Iteration 225/1000 | Loss: 0.00002042
Iteration 226/1000 | Loss: 0.00002042
Iteration 227/1000 | Loss: 0.00002042
Iteration 228/1000 | Loss: 0.00002042
Iteration 229/1000 | Loss: 0.00002042
Iteration 230/1000 | Loss: 0.00002042
Iteration 231/1000 | Loss: 0.00002042
Iteration 232/1000 | Loss: 0.00002042
Iteration 233/1000 | Loss: 0.00002042
Iteration 234/1000 | Loss: 0.00002042
Iteration 235/1000 | Loss: 0.00002042
Iteration 236/1000 | Loss: 0.00002042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.0417170162545517e-05, 2.0417170162545517e-05, 2.0417170162545517e-05, 2.0417170162545517e-05, 2.0417170162545517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0417170162545517e-05

Optimization complete. Final v2v error: 3.840580463409424 mm

Highest mean error: 5.083464622497559 mm for frame 182

Lowest mean error: 3.4886655807495117 mm for frame 33

Saving results

Total time: 162.9581925868988
