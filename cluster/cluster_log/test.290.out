Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=290, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 16240-16295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007494
Iteration 2/25 | Loss: 0.00191904
Iteration 3/25 | Loss: 0.00169821
Iteration 4/25 | Loss: 0.00154944
Iteration 5/25 | Loss: 0.00140844
Iteration 6/25 | Loss: 0.00142044
Iteration 7/25 | Loss: 0.00141837
Iteration 8/25 | Loss: 0.00138814
Iteration 9/25 | Loss: 0.00132136
Iteration 10/25 | Loss: 0.00131105
Iteration 11/25 | Loss: 0.00129280
Iteration 12/25 | Loss: 0.00127658
Iteration 13/25 | Loss: 0.00127100
Iteration 14/25 | Loss: 0.00127214
Iteration 15/25 | Loss: 0.00126688
Iteration 16/25 | Loss: 0.00127018
Iteration 17/25 | Loss: 0.00126544
Iteration 18/25 | Loss: 0.00126219
Iteration 19/25 | Loss: 0.00126765
Iteration 20/25 | Loss: 0.00126532
Iteration 21/25 | Loss: 0.00126845
Iteration 22/25 | Loss: 0.00126337
Iteration 23/25 | Loss: 0.00125983
Iteration 24/25 | Loss: 0.00126147
Iteration 25/25 | Loss: 0.00126152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49490440
Iteration 2/25 | Loss: 0.00128091
Iteration 3/25 | Loss: 0.00103201
Iteration 4/25 | Loss: 0.00103201
Iteration 5/25 | Loss: 0.00103200
Iteration 6/25 | Loss: 0.00103200
Iteration 7/25 | Loss: 0.00103200
Iteration 8/25 | Loss: 0.00103200
Iteration 9/25 | Loss: 0.00103200
Iteration 10/25 | Loss: 0.00103200
Iteration 11/25 | Loss: 0.00103200
Iteration 12/25 | Loss: 0.00103200
Iteration 13/25 | Loss: 0.00103200
Iteration 14/25 | Loss: 0.00103200
Iteration 15/25 | Loss: 0.00103200
Iteration 16/25 | Loss: 0.00103200
Iteration 17/25 | Loss: 0.00103200
Iteration 18/25 | Loss: 0.00103200
Iteration 19/25 | Loss: 0.00103200
Iteration 20/25 | Loss: 0.00103200
Iteration 21/25 | Loss: 0.00103200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001032001688145101, 0.001032001688145101, 0.001032001688145101, 0.001032001688145101, 0.001032001688145101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001032001688145101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103200
Iteration 2/1000 | Loss: 0.00075696
Iteration 3/1000 | Loss: 0.00015519
Iteration 4/1000 | Loss: 0.00017109
Iteration 5/1000 | Loss: 0.00005320
Iteration 6/1000 | Loss: 0.00025314
Iteration 7/1000 | Loss: 0.00061589
Iteration 8/1000 | Loss: 0.00010095
Iteration 9/1000 | Loss: 0.00030530
Iteration 10/1000 | Loss: 0.00003848
Iteration 11/1000 | Loss: 0.00003228
Iteration 12/1000 | Loss: 0.00080430
Iteration 13/1000 | Loss: 0.00014581
Iteration 14/1000 | Loss: 0.00006497
Iteration 15/1000 | Loss: 0.00003419
Iteration 16/1000 | Loss: 0.00002162
Iteration 17/1000 | Loss: 0.00004481
Iteration 18/1000 | Loss: 0.00009781
Iteration 19/1000 | Loss: 0.00004091
Iteration 20/1000 | Loss: 0.00003591
Iteration 21/1000 | Loss: 0.00005367
Iteration 22/1000 | Loss: 0.00006093
Iteration 23/1000 | Loss: 0.00003519
Iteration 24/1000 | Loss: 0.00003332
Iteration 25/1000 | Loss: 0.00007107
Iteration 26/1000 | Loss: 0.00039067
Iteration 27/1000 | Loss: 0.00003198
Iteration 28/1000 | Loss: 0.00003436
Iteration 29/1000 | Loss: 0.00016689
Iteration 30/1000 | Loss: 0.00013008
Iteration 31/1000 | Loss: 0.00024543
Iteration 32/1000 | Loss: 0.00033739
Iteration 33/1000 | Loss: 0.00004802
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00007856
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001391
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001350
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001324
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001315
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001308
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001304
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001299
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001299
Iteration 146/1000 | Loss: 0.00001299
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001299
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2986924048163928e-05, 1.2986924048163928e-05, 1.2986924048163928e-05, 1.2986924048163928e-05, 1.2986924048163928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2986924048163928e-05

Optimization complete. Final v2v error: 3.066835880279541 mm

Highest mean error: 3.7897627353668213 mm for frame 88

Lowest mean error: 2.8926193714141846 mm for frame 12

Saving results

Total time: 121.49470782279968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926967
Iteration 2/25 | Loss: 0.00240963
Iteration 3/25 | Loss: 0.00190409
Iteration 4/25 | Loss: 0.00182217
Iteration 5/25 | Loss: 0.00183717
Iteration 6/25 | Loss: 0.00177056
Iteration 7/25 | Loss: 0.00165891
Iteration 8/25 | Loss: 0.00157947
Iteration 9/25 | Loss: 0.00153351
Iteration 10/25 | Loss: 0.00149155
Iteration 11/25 | Loss: 0.00147133
Iteration 12/25 | Loss: 0.00147893
Iteration 13/25 | Loss: 0.00148393
Iteration 14/25 | Loss: 0.00146739
Iteration 15/25 | Loss: 0.00145192
Iteration 16/25 | Loss: 0.00143484
Iteration 17/25 | Loss: 0.00142207
Iteration 18/25 | Loss: 0.00141040
Iteration 19/25 | Loss: 0.00141833
Iteration 20/25 | Loss: 0.00141340
Iteration 21/25 | Loss: 0.00140291
Iteration 22/25 | Loss: 0.00140651
Iteration 23/25 | Loss: 0.00140156
Iteration 24/25 | Loss: 0.00140166
Iteration 25/25 | Loss: 0.00139102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20968223
Iteration 2/25 | Loss: 0.00144135
Iteration 3/25 | Loss: 0.00141096
Iteration 4/25 | Loss: 0.00141091
Iteration 5/25 | Loss: 0.00141091
Iteration 6/25 | Loss: 0.00141091
Iteration 7/25 | Loss: 0.00141091
Iteration 8/25 | Loss: 0.00141091
Iteration 9/25 | Loss: 0.00141091
Iteration 10/25 | Loss: 0.00141091
Iteration 11/25 | Loss: 0.00141091
Iteration 12/25 | Loss: 0.00141091
Iteration 13/25 | Loss: 0.00141091
Iteration 14/25 | Loss: 0.00141091
Iteration 15/25 | Loss: 0.00141091
Iteration 16/25 | Loss: 0.00141091
Iteration 17/25 | Loss: 0.00141091
Iteration 18/25 | Loss: 0.00141091
Iteration 19/25 | Loss: 0.00141091
Iteration 20/25 | Loss: 0.00141091
Iteration 21/25 | Loss: 0.00141091
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014109105104580522, 0.0014109105104580522, 0.0014109105104580522, 0.0014109105104580522, 0.0014109105104580522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014109105104580522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141091
Iteration 2/1000 | Loss: 0.00020337
Iteration 3/1000 | Loss: 0.00019943
Iteration 4/1000 | Loss: 0.00029394
Iteration 5/1000 | Loss: 0.00025735
Iteration 6/1000 | Loss: 0.00007743
Iteration 7/1000 | Loss: 0.00059242
Iteration 8/1000 | Loss: 0.00009309
Iteration 9/1000 | Loss: 0.00020792
Iteration 10/1000 | Loss: 0.00011998
Iteration 11/1000 | Loss: 0.00011587
Iteration 12/1000 | Loss: 0.00005986
Iteration 13/1000 | Loss: 0.00005410
Iteration 14/1000 | Loss: 0.00005110
Iteration 15/1000 | Loss: 0.00004896
Iteration 16/1000 | Loss: 0.00006376
Iteration 17/1000 | Loss: 0.00099921
Iteration 18/1000 | Loss: 0.00024849
Iteration 19/1000 | Loss: 0.00015184
Iteration 20/1000 | Loss: 0.00028091
Iteration 21/1000 | Loss: 0.00010523
Iteration 22/1000 | Loss: 0.00008390
Iteration 23/1000 | Loss: 0.00005180
Iteration 24/1000 | Loss: 0.00007998
Iteration 25/1000 | Loss: 0.00033529
Iteration 26/1000 | Loss: 0.00024518
Iteration 27/1000 | Loss: 0.00014974
Iteration 28/1000 | Loss: 0.00017249
Iteration 29/1000 | Loss: 0.00005432
Iteration 30/1000 | Loss: 0.00032449
Iteration 31/1000 | Loss: 0.00018400
Iteration 32/1000 | Loss: 0.00034512
Iteration 33/1000 | Loss: 0.00020803
Iteration 34/1000 | Loss: 0.00020095
Iteration 35/1000 | Loss: 0.00025771
Iteration 36/1000 | Loss: 0.00019552
Iteration 37/1000 | Loss: 0.00024366
Iteration 38/1000 | Loss: 0.00013986
Iteration 39/1000 | Loss: 0.00013311
Iteration 40/1000 | Loss: 0.00029870
Iteration 41/1000 | Loss: 0.00032738
Iteration 42/1000 | Loss: 0.00022234
Iteration 43/1000 | Loss: 0.00023138
Iteration 44/1000 | Loss: 0.00038504
Iteration 45/1000 | Loss: 0.00043603
Iteration 46/1000 | Loss: 0.00017666
Iteration 47/1000 | Loss: 0.00005146
Iteration 48/1000 | Loss: 0.00004729
Iteration 49/1000 | Loss: 0.00004466
Iteration 50/1000 | Loss: 0.00036524
Iteration 51/1000 | Loss: 0.00027219
Iteration 52/1000 | Loss: 0.00005498
Iteration 53/1000 | Loss: 0.00007071
Iteration 54/1000 | Loss: 0.00004538
Iteration 55/1000 | Loss: 0.00004369
Iteration 56/1000 | Loss: 0.00005397
Iteration 57/1000 | Loss: 0.00004123
Iteration 58/1000 | Loss: 0.00003975
Iteration 59/1000 | Loss: 0.00003825
Iteration 60/1000 | Loss: 0.00003763
Iteration 61/1000 | Loss: 0.00003701
Iteration 62/1000 | Loss: 0.00076303
Iteration 63/1000 | Loss: 0.00006843
Iteration 64/1000 | Loss: 0.00052154
Iteration 65/1000 | Loss: 0.00003784
Iteration 66/1000 | Loss: 0.00003616
Iteration 67/1000 | Loss: 0.00003558
Iteration 68/1000 | Loss: 0.00147067
Iteration 69/1000 | Loss: 0.00010451
Iteration 70/1000 | Loss: 0.00005669
Iteration 71/1000 | Loss: 0.00004446
Iteration 72/1000 | Loss: 0.00003953
Iteration 73/1000 | Loss: 0.00003682
Iteration 74/1000 | Loss: 0.00003488
Iteration 75/1000 | Loss: 0.00003370
Iteration 76/1000 | Loss: 0.00003309
Iteration 77/1000 | Loss: 0.00003264
Iteration 78/1000 | Loss: 0.00003230
Iteration 79/1000 | Loss: 0.00003208
Iteration 80/1000 | Loss: 0.00003204
Iteration 81/1000 | Loss: 0.00003204
Iteration 82/1000 | Loss: 0.00003200
Iteration 83/1000 | Loss: 0.00003199
Iteration 84/1000 | Loss: 0.00003199
Iteration 85/1000 | Loss: 0.00003198
Iteration 86/1000 | Loss: 0.00003191
Iteration 87/1000 | Loss: 0.00003186
Iteration 88/1000 | Loss: 0.00003183
Iteration 89/1000 | Loss: 0.00003182
Iteration 90/1000 | Loss: 0.00003182
Iteration 91/1000 | Loss: 0.00003181
Iteration 92/1000 | Loss: 0.00003179
Iteration 93/1000 | Loss: 0.00003179
Iteration 94/1000 | Loss: 0.00003178
Iteration 95/1000 | Loss: 0.00003177
Iteration 96/1000 | Loss: 0.00003174
Iteration 97/1000 | Loss: 0.00003172
Iteration 98/1000 | Loss: 0.00003171
Iteration 99/1000 | Loss: 0.00003170
Iteration 100/1000 | Loss: 0.00003169
Iteration 101/1000 | Loss: 0.00003168
Iteration 102/1000 | Loss: 0.00003168
Iteration 103/1000 | Loss: 0.00003167
Iteration 104/1000 | Loss: 0.00003166
Iteration 105/1000 | Loss: 0.00003166
Iteration 106/1000 | Loss: 0.00003166
Iteration 107/1000 | Loss: 0.00003165
Iteration 108/1000 | Loss: 0.00003165
Iteration 109/1000 | Loss: 0.00003165
Iteration 110/1000 | Loss: 0.00003164
Iteration 111/1000 | Loss: 0.00003164
Iteration 112/1000 | Loss: 0.00003160
Iteration 113/1000 | Loss: 0.00003159
Iteration 114/1000 | Loss: 0.00003159
Iteration 115/1000 | Loss: 0.00003158
Iteration 116/1000 | Loss: 0.00003158
Iteration 117/1000 | Loss: 0.00003157
Iteration 118/1000 | Loss: 0.00003157
Iteration 119/1000 | Loss: 0.00003156
Iteration 120/1000 | Loss: 0.00003156
Iteration 121/1000 | Loss: 0.00003156
Iteration 122/1000 | Loss: 0.00003155
Iteration 123/1000 | Loss: 0.00003155
Iteration 124/1000 | Loss: 0.00003155
Iteration 125/1000 | Loss: 0.00003154
Iteration 126/1000 | Loss: 0.00003154
Iteration 127/1000 | Loss: 0.00003154
Iteration 128/1000 | Loss: 0.00003154
Iteration 129/1000 | Loss: 0.00003153
Iteration 130/1000 | Loss: 0.00003153
Iteration 131/1000 | Loss: 0.00003153
Iteration 132/1000 | Loss: 0.00003153
Iteration 133/1000 | Loss: 0.00003153
Iteration 134/1000 | Loss: 0.00003153
Iteration 135/1000 | Loss: 0.00003153
Iteration 136/1000 | Loss: 0.00003153
Iteration 137/1000 | Loss: 0.00003153
Iteration 138/1000 | Loss: 0.00003153
Iteration 139/1000 | Loss: 0.00003153
Iteration 140/1000 | Loss: 0.00003153
Iteration 141/1000 | Loss: 0.00003152
Iteration 142/1000 | Loss: 0.00003152
Iteration 143/1000 | Loss: 0.00003152
Iteration 144/1000 | Loss: 0.00003152
Iteration 145/1000 | Loss: 0.00003152
Iteration 146/1000 | Loss: 0.00003152
Iteration 147/1000 | Loss: 0.00003152
Iteration 148/1000 | Loss: 0.00003152
Iteration 149/1000 | Loss: 0.00003151
Iteration 150/1000 | Loss: 0.00003151
Iteration 151/1000 | Loss: 0.00003151
Iteration 152/1000 | Loss: 0.00003151
Iteration 153/1000 | Loss: 0.00003151
Iteration 154/1000 | Loss: 0.00003151
Iteration 155/1000 | Loss: 0.00003151
Iteration 156/1000 | Loss: 0.00003151
Iteration 157/1000 | Loss: 0.00003150
Iteration 158/1000 | Loss: 0.00003150
Iteration 159/1000 | Loss: 0.00003150
Iteration 160/1000 | Loss: 0.00003150
Iteration 161/1000 | Loss: 0.00003150
Iteration 162/1000 | Loss: 0.00003150
Iteration 163/1000 | Loss: 0.00003150
Iteration 164/1000 | Loss: 0.00003150
Iteration 165/1000 | Loss: 0.00003150
Iteration 166/1000 | Loss: 0.00003150
Iteration 167/1000 | Loss: 0.00003150
Iteration 168/1000 | Loss: 0.00003149
Iteration 169/1000 | Loss: 0.00003149
Iteration 170/1000 | Loss: 0.00003149
Iteration 171/1000 | Loss: 0.00003149
Iteration 172/1000 | Loss: 0.00003149
Iteration 173/1000 | Loss: 0.00003148
Iteration 174/1000 | Loss: 0.00003148
Iteration 175/1000 | Loss: 0.00003148
Iteration 176/1000 | Loss: 0.00003148
Iteration 177/1000 | Loss: 0.00003147
Iteration 178/1000 | Loss: 0.00003147
Iteration 179/1000 | Loss: 0.00003147
Iteration 180/1000 | Loss: 0.00003147
Iteration 181/1000 | Loss: 0.00003146
Iteration 182/1000 | Loss: 0.00003146
Iteration 183/1000 | Loss: 0.00003146
Iteration 184/1000 | Loss: 0.00003146
Iteration 185/1000 | Loss: 0.00003145
Iteration 186/1000 | Loss: 0.00003145
Iteration 187/1000 | Loss: 0.00003145
Iteration 188/1000 | Loss: 0.00003145
Iteration 189/1000 | Loss: 0.00003145
Iteration 190/1000 | Loss: 0.00003145
Iteration 191/1000 | Loss: 0.00003144
Iteration 192/1000 | Loss: 0.00003144
Iteration 193/1000 | Loss: 0.00003144
Iteration 194/1000 | Loss: 0.00003144
Iteration 195/1000 | Loss: 0.00003144
Iteration 196/1000 | Loss: 0.00003144
Iteration 197/1000 | Loss: 0.00003144
Iteration 198/1000 | Loss: 0.00003144
Iteration 199/1000 | Loss: 0.00003144
Iteration 200/1000 | Loss: 0.00003144
Iteration 201/1000 | Loss: 0.00003143
Iteration 202/1000 | Loss: 0.00003143
Iteration 203/1000 | Loss: 0.00003143
Iteration 204/1000 | Loss: 0.00003143
Iteration 205/1000 | Loss: 0.00003143
Iteration 206/1000 | Loss: 0.00003143
Iteration 207/1000 | Loss: 0.00003142
Iteration 208/1000 | Loss: 0.00003142
Iteration 209/1000 | Loss: 0.00003142
Iteration 210/1000 | Loss: 0.00003142
Iteration 211/1000 | Loss: 0.00003142
Iteration 212/1000 | Loss: 0.00003142
Iteration 213/1000 | Loss: 0.00003142
Iteration 214/1000 | Loss: 0.00003142
Iteration 215/1000 | Loss: 0.00003142
Iteration 216/1000 | Loss: 0.00003142
Iteration 217/1000 | Loss: 0.00003142
Iteration 218/1000 | Loss: 0.00003141
Iteration 219/1000 | Loss: 0.00003141
Iteration 220/1000 | Loss: 0.00023271
Iteration 221/1000 | Loss: 0.00023271
Iteration 222/1000 | Loss: 0.00021248
Iteration 223/1000 | Loss: 0.00017037
Iteration 224/1000 | Loss: 0.00023568
Iteration 225/1000 | Loss: 0.00008072
Iteration 226/1000 | Loss: 0.00012620
Iteration 227/1000 | Loss: 0.00007976
Iteration 228/1000 | Loss: 0.00016901
Iteration 229/1000 | Loss: 0.00015612
Iteration 230/1000 | Loss: 0.00027409
Iteration 231/1000 | Loss: 0.00003422
Iteration 232/1000 | Loss: 0.00003280
Iteration 233/1000 | Loss: 0.00003217
Iteration 234/1000 | Loss: 0.00003175
Iteration 235/1000 | Loss: 0.00003169
Iteration 236/1000 | Loss: 0.00003168
Iteration 237/1000 | Loss: 0.00017933
Iteration 238/1000 | Loss: 0.00014838
Iteration 239/1000 | Loss: 0.00003594
Iteration 240/1000 | Loss: 0.00003320
Iteration 241/1000 | Loss: 0.00003214
Iteration 242/1000 | Loss: 0.00003177
Iteration 243/1000 | Loss: 0.00003146
Iteration 244/1000 | Loss: 0.00003143
Iteration 245/1000 | Loss: 0.00003142
Iteration 246/1000 | Loss: 0.00003142
Iteration 247/1000 | Loss: 0.00003141
Iteration 248/1000 | Loss: 0.00003140
Iteration 249/1000 | Loss: 0.00003140
Iteration 250/1000 | Loss: 0.00003140
Iteration 251/1000 | Loss: 0.00003140
Iteration 252/1000 | Loss: 0.00003140
Iteration 253/1000 | Loss: 0.00003139
Iteration 254/1000 | Loss: 0.00003139
Iteration 255/1000 | Loss: 0.00003139
Iteration 256/1000 | Loss: 0.00003138
Iteration 257/1000 | Loss: 0.00003138
Iteration 258/1000 | Loss: 0.00003138
Iteration 259/1000 | Loss: 0.00003137
Iteration 260/1000 | Loss: 0.00003137
Iteration 261/1000 | Loss: 0.00003137
Iteration 262/1000 | Loss: 0.00003137
Iteration 263/1000 | Loss: 0.00003137
Iteration 264/1000 | Loss: 0.00003137
Iteration 265/1000 | Loss: 0.00003137
Iteration 266/1000 | Loss: 0.00003137
Iteration 267/1000 | Loss: 0.00003137
Iteration 268/1000 | Loss: 0.00003137
Iteration 269/1000 | Loss: 0.00003136
Iteration 270/1000 | Loss: 0.00003136
Iteration 271/1000 | Loss: 0.00003136
Iteration 272/1000 | Loss: 0.00003136
Iteration 273/1000 | Loss: 0.00003136
Iteration 274/1000 | Loss: 0.00003136
Iteration 275/1000 | Loss: 0.00003136
Iteration 276/1000 | Loss: 0.00003136
Iteration 277/1000 | Loss: 0.00003136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [3.136427403660491e-05, 3.136427403660491e-05, 3.136427403660491e-05, 3.136427403660491e-05, 3.136427403660491e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.136427403660491e-05

Optimization complete. Final v2v error: 4.1810760498046875 mm

Highest mean error: 12.393704414367676 mm for frame 4

Lowest mean error: 3.009115695953369 mm for frame 93

Saving results

Total time: 213.1251928806305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00223488
Iteration 2/25 | Loss: 0.00134706
Iteration 3/25 | Loss: 0.00124352
Iteration 4/25 | Loss: 0.00121631
Iteration 5/25 | Loss: 0.00121183
Iteration 6/25 | Loss: 0.00121075
Iteration 7/25 | Loss: 0.00121075
Iteration 8/25 | Loss: 0.00121075
Iteration 9/25 | Loss: 0.00121075
Iteration 10/25 | Loss: 0.00121075
Iteration 11/25 | Loss: 0.00121075
Iteration 12/25 | Loss: 0.00121075
Iteration 13/25 | Loss: 0.00121075
Iteration 14/25 | Loss: 0.00121075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012107468210160732, 0.0012107468210160732, 0.0012107468210160732, 0.0012107468210160732, 0.0012107468210160732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012107468210160732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42279553
Iteration 2/25 | Loss: 0.00091457
Iteration 3/25 | Loss: 0.00091457
Iteration 4/25 | Loss: 0.00091457
Iteration 5/25 | Loss: 0.00091457
Iteration 6/25 | Loss: 0.00091457
Iteration 7/25 | Loss: 0.00091457
Iteration 8/25 | Loss: 0.00091457
Iteration 9/25 | Loss: 0.00091457
Iteration 10/25 | Loss: 0.00091457
Iteration 11/25 | Loss: 0.00091456
Iteration 12/25 | Loss: 0.00091456
Iteration 13/25 | Loss: 0.00091456
Iteration 14/25 | Loss: 0.00091456
Iteration 15/25 | Loss: 0.00091457
Iteration 16/25 | Loss: 0.00091457
Iteration 17/25 | Loss: 0.00091457
Iteration 18/25 | Loss: 0.00091457
Iteration 19/25 | Loss: 0.00091457
Iteration 20/25 | Loss: 0.00091457
Iteration 21/25 | Loss: 0.00091457
Iteration 22/25 | Loss: 0.00091457
Iteration 23/25 | Loss: 0.00091457
Iteration 24/25 | Loss: 0.00091457
Iteration 25/25 | Loss: 0.00091457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091457
Iteration 2/1000 | Loss: 0.00004684
Iteration 3/1000 | Loss: 0.00003110
Iteration 4/1000 | Loss: 0.00002472
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002182
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002040
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001939
Iteration 13/1000 | Loss: 0.00001928
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001913
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001903
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00001884
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001871
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001869
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001863
Iteration 58/1000 | Loss: 0.00001863
Iteration 59/1000 | Loss: 0.00001862
Iteration 60/1000 | Loss: 0.00001862
Iteration 61/1000 | Loss: 0.00001862
Iteration 62/1000 | Loss: 0.00001861
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001854
Iteration 112/1000 | Loss: 0.00001854
Iteration 113/1000 | Loss: 0.00001854
Iteration 114/1000 | Loss: 0.00001854
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.8538044969318435e-05, 1.8538044969318435e-05, 1.8538044969318435e-05, 1.8538044969318435e-05, 1.8538044969318435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8538044969318435e-05

Optimization complete. Final v2v error: 3.670149087905884 mm

Highest mean error: 4.126674652099609 mm for frame 61

Lowest mean error: 3.302746057510376 mm for frame 34

Saving results

Total time: 39.1967990398407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006072
Iteration 2/25 | Loss: 0.00209380
Iteration 3/25 | Loss: 0.00159037
Iteration 4/25 | Loss: 0.00149482
Iteration 5/25 | Loss: 0.00147792
Iteration 6/25 | Loss: 0.00143695
Iteration 7/25 | Loss: 0.00139959
Iteration 8/25 | Loss: 0.00136106
Iteration 9/25 | Loss: 0.00134686
Iteration 10/25 | Loss: 0.00133391
Iteration 11/25 | Loss: 0.00132613
Iteration 12/25 | Loss: 0.00131739
Iteration 13/25 | Loss: 0.00130723
Iteration 14/25 | Loss: 0.00131394
Iteration 15/25 | Loss: 0.00130677
Iteration 16/25 | Loss: 0.00130453
Iteration 17/25 | Loss: 0.00129661
Iteration 18/25 | Loss: 0.00129967
Iteration 19/25 | Loss: 0.00131077
Iteration 20/25 | Loss: 0.00129520
Iteration 21/25 | Loss: 0.00130005
Iteration 22/25 | Loss: 0.00129688
Iteration 23/25 | Loss: 0.00129463
Iteration 24/25 | Loss: 0.00129461
Iteration 25/25 | Loss: 0.00129461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53534281
Iteration 2/25 | Loss: 0.00155988
Iteration 3/25 | Loss: 0.00120689
Iteration 4/25 | Loss: 0.00120689
Iteration 5/25 | Loss: 0.00120689
Iteration 6/25 | Loss: 0.00120689
Iteration 7/25 | Loss: 0.00120689
Iteration 8/25 | Loss: 0.00120689
Iteration 9/25 | Loss: 0.00120689
Iteration 10/25 | Loss: 0.00120689
Iteration 11/25 | Loss: 0.00120689
Iteration 12/25 | Loss: 0.00120689
Iteration 13/25 | Loss: 0.00120689
Iteration 14/25 | Loss: 0.00120689
Iteration 15/25 | Loss: 0.00120689
Iteration 16/25 | Loss: 0.00120689
Iteration 17/25 | Loss: 0.00120689
Iteration 18/25 | Loss: 0.00120689
Iteration 19/25 | Loss: 0.00120689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012068874202668667, 0.0012068874202668667, 0.0012068874202668667, 0.0012068874202668667, 0.0012068874202668667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012068874202668667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120689
Iteration 2/1000 | Loss: 0.00025967
Iteration 3/1000 | Loss: 0.00008849
Iteration 4/1000 | Loss: 0.00025388
Iteration 5/1000 | Loss: 0.00002934
Iteration 6/1000 | Loss: 0.00002753
Iteration 7/1000 | Loss: 0.00002557
Iteration 8/1000 | Loss: 0.00002421
Iteration 9/1000 | Loss: 0.00033736
Iteration 10/1000 | Loss: 0.00022446
Iteration 11/1000 | Loss: 0.00005895
Iteration 12/1000 | Loss: 0.00023637
Iteration 13/1000 | Loss: 0.00012180
Iteration 14/1000 | Loss: 0.00014026
Iteration 15/1000 | Loss: 0.00002621
Iteration 16/1000 | Loss: 0.00002363
Iteration 17/1000 | Loss: 0.00002198
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002047
Iteration 21/1000 | Loss: 0.00002015
Iteration 22/1000 | Loss: 0.00002323
Iteration 23/1000 | Loss: 0.00001988
Iteration 24/1000 | Loss: 0.00001961
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001940
Iteration 29/1000 | Loss: 0.00001940
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001940
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001939
Iteration 36/1000 | Loss: 0.00001939
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001934
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00001932
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001931
Iteration 45/1000 | Loss: 0.00001930
Iteration 46/1000 | Loss: 0.00001930
Iteration 47/1000 | Loss: 0.00001930
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001929
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001928
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001926
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001924
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001917
Iteration 70/1000 | Loss: 0.00001916
Iteration 71/1000 | Loss: 0.00001916
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001911
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001910
Iteration 87/1000 | Loss: 0.00001910
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001909
Iteration 95/1000 | Loss: 0.00001909
Iteration 96/1000 | Loss: 0.00001909
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001908
Iteration 105/1000 | Loss: 0.00001908
Iteration 106/1000 | Loss: 0.00001908
Iteration 107/1000 | Loss: 0.00001908
Iteration 108/1000 | Loss: 0.00001908
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001907
Iteration 125/1000 | Loss: 0.00001907
Iteration 126/1000 | Loss: 0.00001907
Iteration 127/1000 | Loss: 0.00001907
Iteration 128/1000 | Loss: 0.00001907
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001906
Iteration 132/1000 | Loss: 0.00001906
Iteration 133/1000 | Loss: 0.00001906
Iteration 134/1000 | Loss: 0.00001906
Iteration 135/1000 | Loss: 0.00001906
Iteration 136/1000 | Loss: 0.00001906
Iteration 137/1000 | Loss: 0.00001906
Iteration 138/1000 | Loss: 0.00001906
Iteration 139/1000 | Loss: 0.00001906
Iteration 140/1000 | Loss: 0.00001906
Iteration 141/1000 | Loss: 0.00001906
Iteration 142/1000 | Loss: 0.00001906
Iteration 143/1000 | Loss: 0.00001906
Iteration 144/1000 | Loss: 0.00001906
Iteration 145/1000 | Loss: 0.00001906
Iteration 146/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.906335455714725e-05, 1.906335455714725e-05, 1.906335455714725e-05, 1.906335455714725e-05, 1.906335455714725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.906335455714725e-05

Optimization complete. Final v2v error: 3.2717597484588623 mm

Highest mean error: 11.050158500671387 mm for frame 125

Lowest mean error: 2.887835741043091 mm for frame 24

Saving results

Total time: 93.36856079101562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781875
Iteration 2/25 | Loss: 0.00193466
Iteration 3/25 | Loss: 0.00140667
Iteration 4/25 | Loss: 0.00135849
Iteration 5/25 | Loss: 0.00135252
Iteration 6/25 | Loss: 0.00135122
Iteration 7/25 | Loss: 0.00135122
Iteration 8/25 | Loss: 0.00135122
Iteration 9/25 | Loss: 0.00135122
Iteration 10/25 | Loss: 0.00135122
Iteration 11/25 | Loss: 0.00135122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013512235600501299, 0.0013512235600501299, 0.0013512235600501299, 0.0013512235600501299, 0.0013512235600501299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013512235600501299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43570244
Iteration 2/25 | Loss: 0.00088917
Iteration 3/25 | Loss: 0.00088914
Iteration 4/25 | Loss: 0.00088914
Iteration 5/25 | Loss: 0.00088914
Iteration 6/25 | Loss: 0.00088914
Iteration 7/25 | Loss: 0.00088914
Iteration 8/25 | Loss: 0.00088914
Iteration 9/25 | Loss: 0.00088914
Iteration 10/25 | Loss: 0.00088914
Iteration 11/25 | Loss: 0.00088914
Iteration 12/25 | Loss: 0.00088914
Iteration 13/25 | Loss: 0.00088914
Iteration 14/25 | Loss: 0.00088914
Iteration 15/25 | Loss: 0.00088914
Iteration 16/25 | Loss: 0.00088914
Iteration 17/25 | Loss: 0.00088914
Iteration 18/25 | Loss: 0.00088914
Iteration 19/25 | Loss: 0.00088914
Iteration 20/25 | Loss: 0.00088914
Iteration 21/25 | Loss: 0.00088914
Iteration 22/25 | Loss: 0.00088914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008891407051123679, 0.0008891407051123679, 0.0008891407051123679, 0.0008891407051123679, 0.0008891407051123679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008891407051123679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088914
Iteration 2/1000 | Loss: 0.00004676
Iteration 3/1000 | Loss: 0.00003272
Iteration 4/1000 | Loss: 0.00002614
Iteration 5/1000 | Loss: 0.00002469
Iteration 6/1000 | Loss: 0.00002390
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002301
Iteration 9/1000 | Loss: 0.00002281
Iteration 10/1000 | Loss: 0.00002262
Iteration 11/1000 | Loss: 0.00002260
Iteration 12/1000 | Loss: 0.00002260
Iteration 13/1000 | Loss: 0.00002259
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002244
Iteration 16/1000 | Loss: 0.00002242
Iteration 17/1000 | Loss: 0.00002234
Iteration 18/1000 | Loss: 0.00002232
Iteration 19/1000 | Loss: 0.00002232
Iteration 20/1000 | Loss: 0.00002231
Iteration 21/1000 | Loss: 0.00002231
Iteration 22/1000 | Loss: 0.00002230
Iteration 23/1000 | Loss: 0.00002230
Iteration 24/1000 | Loss: 0.00002230
Iteration 25/1000 | Loss: 0.00002230
Iteration 26/1000 | Loss: 0.00002230
Iteration 27/1000 | Loss: 0.00002230
Iteration 28/1000 | Loss: 0.00002230
Iteration 29/1000 | Loss: 0.00002230
Iteration 30/1000 | Loss: 0.00002230
Iteration 31/1000 | Loss: 0.00002227
Iteration 32/1000 | Loss: 0.00002227
Iteration 33/1000 | Loss: 0.00002226
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00002225
Iteration 36/1000 | Loss: 0.00002225
Iteration 37/1000 | Loss: 0.00002224
Iteration 38/1000 | Loss: 0.00002224
Iteration 39/1000 | Loss: 0.00002223
Iteration 40/1000 | Loss: 0.00002223
Iteration 41/1000 | Loss: 0.00002223
Iteration 42/1000 | Loss: 0.00002223
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00002222
Iteration 45/1000 | Loss: 0.00002221
Iteration 46/1000 | Loss: 0.00002221
Iteration 47/1000 | Loss: 0.00002220
Iteration 48/1000 | Loss: 0.00002220
Iteration 49/1000 | Loss: 0.00002220
Iteration 50/1000 | Loss: 0.00002219
Iteration 51/1000 | Loss: 0.00002219
Iteration 52/1000 | Loss: 0.00002219
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002219
Iteration 55/1000 | Loss: 0.00002219
Iteration 56/1000 | Loss: 0.00002219
Iteration 57/1000 | Loss: 0.00002218
Iteration 58/1000 | Loss: 0.00002217
Iteration 59/1000 | Loss: 0.00002216
Iteration 60/1000 | Loss: 0.00002216
Iteration 61/1000 | Loss: 0.00002215
Iteration 62/1000 | Loss: 0.00002215
Iteration 63/1000 | Loss: 0.00002215
Iteration 64/1000 | Loss: 0.00002215
Iteration 65/1000 | Loss: 0.00002215
Iteration 66/1000 | Loss: 0.00002215
Iteration 67/1000 | Loss: 0.00002215
Iteration 68/1000 | Loss: 0.00002214
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002214
Iteration 71/1000 | Loss: 0.00002213
Iteration 72/1000 | Loss: 0.00002213
Iteration 73/1000 | Loss: 0.00002213
Iteration 74/1000 | Loss: 0.00002213
Iteration 75/1000 | Loss: 0.00002212
Iteration 76/1000 | Loss: 0.00002212
Iteration 77/1000 | Loss: 0.00002211
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002211
Iteration 81/1000 | Loss: 0.00002211
Iteration 82/1000 | Loss: 0.00002211
Iteration 83/1000 | Loss: 0.00002210
Iteration 84/1000 | Loss: 0.00002210
Iteration 85/1000 | Loss: 0.00002210
Iteration 86/1000 | Loss: 0.00002210
Iteration 87/1000 | Loss: 0.00002210
Iteration 88/1000 | Loss: 0.00002210
Iteration 89/1000 | Loss: 0.00002210
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00002209
Iteration 92/1000 | Loss: 0.00002209
Iteration 93/1000 | Loss: 0.00002209
Iteration 94/1000 | Loss: 0.00002209
Iteration 95/1000 | Loss: 0.00002209
Iteration 96/1000 | Loss: 0.00002209
Iteration 97/1000 | Loss: 0.00002208
Iteration 98/1000 | Loss: 0.00002208
Iteration 99/1000 | Loss: 0.00002208
Iteration 100/1000 | Loss: 0.00002208
Iteration 101/1000 | Loss: 0.00002208
Iteration 102/1000 | Loss: 0.00002208
Iteration 103/1000 | Loss: 0.00002208
Iteration 104/1000 | Loss: 0.00002208
Iteration 105/1000 | Loss: 0.00002208
Iteration 106/1000 | Loss: 0.00002208
Iteration 107/1000 | Loss: 0.00002208
Iteration 108/1000 | Loss: 0.00002208
Iteration 109/1000 | Loss: 0.00002208
Iteration 110/1000 | Loss: 0.00002208
Iteration 111/1000 | Loss: 0.00002208
Iteration 112/1000 | Loss: 0.00002208
Iteration 113/1000 | Loss: 0.00002208
Iteration 114/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.208231126132887e-05, 2.208231126132887e-05, 2.208231126132887e-05, 2.208231126132887e-05, 2.208231126132887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.208231126132887e-05

Optimization complete. Final v2v error: 3.951129913330078 mm

Highest mean error: 4.50863790512085 mm for frame 135

Lowest mean error: 3.693690776824951 mm for frame 111

Saving results

Total time: 33.15443253517151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764326
Iteration 2/25 | Loss: 0.00187938
Iteration 3/25 | Loss: 0.00149565
Iteration 4/25 | Loss: 0.00144462
Iteration 5/25 | Loss: 0.00143433
Iteration 6/25 | Loss: 0.00143116
Iteration 7/25 | Loss: 0.00142961
Iteration 8/25 | Loss: 0.00143553
Iteration 9/25 | Loss: 0.00142929
Iteration 10/25 | Loss: 0.00142311
Iteration 11/25 | Loss: 0.00142220
Iteration 12/25 | Loss: 0.00142202
Iteration 13/25 | Loss: 0.00142192
Iteration 14/25 | Loss: 0.00142180
Iteration 15/25 | Loss: 0.00142164
Iteration 16/25 | Loss: 0.00142148
Iteration 17/25 | Loss: 0.00142356
Iteration 18/25 | Loss: 0.00141980
Iteration 19/25 | Loss: 0.00141924
Iteration 20/25 | Loss: 0.00141913
Iteration 21/25 | Loss: 0.00141912
Iteration 22/25 | Loss: 0.00141912
Iteration 23/25 | Loss: 0.00141912
Iteration 24/25 | Loss: 0.00141912
Iteration 25/25 | Loss: 0.00141912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54840118
Iteration 2/25 | Loss: 0.00098567
Iteration 3/25 | Loss: 0.00098566
Iteration 4/25 | Loss: 0.00098566
Iteration 5/25 | Loss: 0.00098565
Iteration 6/25 | Loss: 0.00098565
Iteration 7/25 | Loss: 0.00098565
Iteration 8/25 | Loss: 0.00098565
Iteration 9/25 | Loss: 0.00098565
Iteration 10/25 | Loss: 0.00098565
Iteration 11/25 | Loss: 0.00098565
Iteration 12/25 | Loss: 0.00098565
Iteration 13/25 | Loss: 0.00098565
Iteration 14/25 | Loss: 0.00098565
Iteration 15/25 | Loss: 0.00098565
Iteration 16/25 | Loss: 0.00098565
Iteration 17/25 | Loss: 0.00098565
Iteration 18/25 | Loss: 0.00098565
Iteration 19/25 | Loss: 0.00098565
Iteration 20/25 | Loss: 0.00098565
Iteration 21/25 | Loss: 0.00098565
Iteration 22/25 | Loss: 0.00098565
Iteration 23/25 | Loss: 0.00098565
Iteration 24/25 | Loss: 0.00098565
Iteration 25/25 | Loss: 0.00098565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098565
Iteration 2/1000 | Loss: 0.00008089
Iteration 3/1000 | Loss: 0.00004457
Iteration 4/1000 | Loss: 0.00003564
Iteration 5/1000 | Loss: 0.00003173
Iteration 6/1000 | Loss: 0.00002996
Iteration 7/1000 | Loss: 0.00002862
Iteration 8/1000 | Loss: 0.00002780
Iteration 9/1000 | Loss: 0.00002697
Iteration 10/1000 | Loss: 0.00002640
Iteration 11/1000 | Loss: 0.00002602
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00002525
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002510
Iteration 18/1000 | Loss: 0.00002508
Iteration 19/1000 | Loss: 0.00002508
Iteration 20/1000 | Loss: 0.00002507
Iteration 21/1000 | Loss: 0.00002506
Iteration 22/1000 | Loss: 0.00002505
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002499
Iteration 25/1000 | Loss: 0.00002498
Iteration 26/1000 | Loss: 0.00002497
Iteration 27/1000 | Loss: 0.00002495
Iteration 28/1000 | Loss: 0.00002495
Iteration 29/1000 | Loss: 0.00002495
Iteration 30/1000 | Loss: 0.00002494
Iteration 31/1000 | Loss: 0.00002494
Iteration 32/1000 | Loss: 0.00002494
Iteration 33/1000 | Loss: 0.00002493
Iteration 34/1000 | Loss: 0.00002492
Iteration 35/1000 | Loss: 0.00002491
Iteration 36/1000 | Loss: 0.00002491
Iteration 37/1000 | Loss: 0.00002490
Iteration 38/1000 | Loss: 0.00002490
Iteration 39/1000 | Loss: 0.00002489
Iteration 40/1000 | Loss: 0.00002489
Iteration 41/1000 | Loss: 0.00002488
Iteration 42/1000 | Loss: 0.00002486
Iteration 43/1000 | Loss: 0.00002486
Iteration 44/1000 | Loss: 0.00002485
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002484
Iteration 48/1000 | Loss: 0.00002484
Iteration 49/1000 | Loss: 0.00002484
Iteration 50/1000 | Loss: 0.00002484
Iteration 51/1000 | Loss: 0.00002484
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00002484
Iteration 54/1000 | Loss: 0.00002483
Iteration 55/1000 | Loss: 0.00002483
Iteration 56/1000 | Loss: 0.00002483
Iteration 57/1000 | Loss: 0.00002483
Iteration 58/1000 | Loss: 0.00002482
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00002482
Iteration 61/1000 | Loss: 0.00002481
Iteration 62/1000 | Loss: 0.00002481
Iteration 63/1000 | Loss: 0.00002481
Iteration 64/1000 | Loss: 0.00002481
Iteration 65/1000 | Loss: 0.00002481
Iteration 66/1000 | Loss: 0.00002481
Iteration 67/1000 | Loss: 0.00002481
Iteration 68/1000 | Loss: 0.00002481
Iteration 69/1000 | Loss: 0.00002481
Iteration 70/1000 | Loss: 0.00002481
Iteration 71/1000 | Loss: 0.00002480
Iteration 72/1000 | Loss: 0.00002480
Iteration 73/1000 | Loss: 0.00002480
Iteration 74/1000 | Loss: 0.00002480
Iteration 75/1000 | Loss: 0.00002480
Iteration 76/1000 | Loss: 0.00002479
Iteration 77/1000 | Loss: 0.00002479
Iteration 78/1000 | Loss: 0.00002479
Iteration 79/1000 | Loss: 0.00002479
Iteration 80/1000 | Loss: 0.00002479
Iteration 81/1000 | Loss: 0.00002479
Iteration 82/1000 | Loss: 0.00002479
Iteration 83/1000 | Loss: 0.00002478
Iteration 84/1000 | Loss: 0.00002478
Iteration 85/1000 | Loss: 0.00002478
Iteration 86/1000 | Loss: 0.00002477
Iteration 87/1000 | Loss: 0.00002477
Iteration 88/1000 | Loss: 0.00002477
Iteration 89/1000 | Loss: 0.00002476
Iteration 90/1000 | Loss: 0.00002476
Iteration 91/1000 | Loss: 0.00002476
Iteration 92/1000 | Loss: 0.00002476
Iteration 93/1000 | Loss: 0.00002475
Iteration 94/1000 | Loss: 0.00002475
Iteration 95/1000 | Loss: 0.00002475
Iteration 96/1000 | Loss: 0.00002475
Iteration 97/1000 | Loss: 0.00002474
Iteration 98/1000 | Loss: 0.00002474
Iteration 99/1000 | Loss: 0.00002474
Iteration 100/1000 | Loss: 0.00002474
Iteration 101/1000 | Loss: 0.00002474
Iteration 102/1000 | Loss: 0.00002474
Iteration 103/1000 | Loss: 0.00002474
Iteration 104/1000 | Loss: 0.00002474
Iteration 105/1000 | Loss: 0.00002474
Iteration 106/1000 | Loss: 0.00002473
Iteration 107/1000 | Loss: 0.00002473
Iteration 108/1000 | Loss: 0.00002473
Iteration 109/1000 | Loss: 0.00002473
Iteration 110/1000 | Loss: 0.00002473
Iteration 111/1000 | Loss: 0.00002473
Iteration 112/1000 | Loss: 0.00002472
Iteration 113/1000 | Loss: 0.00002472
Iteration 114/1000 | Loss: 0.00002472
Iteration 115/1000 | Loss: 0.00002472
Iteration 116/1000 | Loss: 0.00002472
Iteration 117/1000 | Loss: 0.00002471
Iteration 118/1000 | Loss: 0.00002471
Iteration 119/1000 | Loss: 0.00002471
Iteration 120/1000 | Loss: 0.00002470
Iteration 121/1000 | Loss: 0.00002470
Iteration 122/1000 | Loss: 0.00002470
Iteration 123/1000 | Loss: 0.00002470
Iteration 124/1000 | Loss: 0.00002470
Iteration 125/1000 | Loss: 0.00002469
Iteration 126/1000 | Loss: 0.00002469
Iteration 127/1000 | Loss: 0.00002469
Iteration 128/1000 | Loss: 0.00002469
Iteration 129/1000 | Loss: 0.00002468
Iteration 130/1000 | Loss: 0.00002468
Iteration 131/1000 | Loss: 0.00002468
Iteration 132/1000 | Loss: 0.00002468
Iteration 133/1000 | Loss: 0.00002468
Iteration 134/1000 | Loss: 0.00002468
Iteration 135/1000 | Loss: 0.00002468
Iteration 136/1000 | Loss: 0.00002468
Iteration 137/1000 | Loss: 0.00002468
Iteration 138/1000 | Loss: 0.00002468
Iteration 139/1000 | Loss: 0.00002468
Iteration 140/1000 | Loss: 0.00002467
Iteration 141/1000 | Loss: 0.00002467
Iteration 142/1000 | Loss: 0.00002467
Iteration 143/1000 | Loss: 0.00002466
Iteration 144/1000 | Loss: 0.00002466
Iteration 145/1000 | Loss: 0.00002466
Iteration 146/1000 | Loss: 0.00002466
Iteration 147/1000 | Loss: 0.00002465
Iteration 148/1000 | Loss: 0.00002465
Iteration 149/1000 | Loss: 0.00002465
Iteration 150/1000 | Loss: 0.00002465
Iteration 151/1000 | Loss: 0.00002465
Iteration 152/1000 | Loss: 0.00002465
Iteration 153/1000 | Loss: 0.00002465
Iteration 154/1000 | Loss: 0.00002465
Iteration 155/1000 | Loss: 0.00002464
Iteration 156/1000 | Loss: 0.00002464
Iteration 157/1000 | Loss: 0.00002464
Iteration 158/1000 | Loss: 0.00002464
Iteration 159/1000 | Loss: 0.00002464
Iteration 160/1000 | Loss: 0.00002464
Iteration 161/1000 | Loss: 0.00002464
Iteration 162/1000 | Loss: 0.00002464
Iteration 163/1000 | Loss: 0.00002464
Iteration 164/1000 | Loss: 0.00002464
Iteration 165/1000 | Loss: 0.00002464
Iteration 166/1000 | Loss: 0.00002464
Iteration 167/1000 | Loss: 0.00002464
Iteration 168/1000 | Loss: 0.00002463
Iteration 169/1000 | Loss: 0.00002463
Iteration 170/1000 | Loss: 0.00002463
Iteration 171/1000 | Loss: 0.00002463
Iteration 172/1000 | Loss: 0.00002463
Iteration 173/1000 | Loss: 0.00002463
Iteration 174/1000 | Loss: 0.00002463
Iteration 175/1000 | Loss: 0.00002463
Iteration 176/1000 | Loss: 0.00002463
Iteration 177/1000 | Loss: 0.00002463
Iteration 178/1000 | Loss: 0.00002463
Iteration 179/1000 | Loss: 0.00002463
Iteration 180/1000 | Loss: 0.00002463
Iteration 181/1000 | Loss: 0.00002463
Iteration 182/1000 | Loss: 0.00002463
Iteration 183/1000 | Loss: 0.00002463
Iteration 184/1000 | Loss: 0.00002462
Iteration 185/1000 | Loss: 0.00002462
Iteration 186/1000 | Loss: 0.00002462
Iteration 187/1000 | Loss: 0.00002462
Iteration 188/1000 | Loss: 0.00002462
Iteration 189/1000 | Loss: 0.00002462
Iteration 190/1000 | Loss: 0.00002462
Iteration 191/1000 | Loss: 0.00002462
Iteration 192/1000 | Loss: 0.00002462
Iteration 193/1000 | Loss: 0.00002462
Iteration 194/1000 | Loss: 0.00002462
Iteration 195/1000 | Loss: 0.00002462
Iteration 196/1000 | Loss: 0.00002462
Iteration 197/1000 | Loss: 0.00002462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.462307566020172e-05, 2.462307566020172e-05, 2.462307566020172e-05, 2.462307566020172e-05, 2.462307566020172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.462307566020172e-05

Optimization complete. Final v2v error: 4.150991439819336 mm

Highest mean error: 5.722750186920166 mm for frame 34

Lowest mean error: 3.519655466079712 mm for frame 0

Saving results

Total time: 70.51418018341064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531352
Iteration 2/25 | Loss: 0.00133357
Iteration 3/25 | Loss: 0.00127196
Iteration 4/25 | Loss: 0.00126328
Iteration 5/25 | Loss: 0.00126035
Iteration 6/25 | Loss: 0.00125988
Iteration 7/25 | Loss: 0.00125988
Iteration 8/25 | Loss: 0.00125988
Iteration 9/25 | Loss: 0.00125988
Iteration 10/25 | Loss: 0.00125988
Iteration 11/25 | Loss: 0.00125988
Iteration 12/25 | Loss: 0.00125988
Iteration 13/25 | Loss: 0.00125988
Iteration 14/25 | Loss: 0.00125988
Iteration 15/25 | Loss: 0.00125988
Iteration 16/25 | Loss: 0.00125988
Iteration 17/25 | Loss: 0.00125988
Iteration 18/25 | Loss: 0.00125988
Iteration 19/25 | Loss: 0.00125988
Iteration 20/25 | Loss: 0.00125988
Iteration 21/25 | Loss: 0.00125988
Iteration 22/25 | Loss: 0.00125988
Iteration 23/25 | Loss: 0.00125988
Iteration 24/25 | Loss: 0.00125988
Iteration 25/25 | Loss: 0.00125988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.78271055
Iteration 2/25 | Loss: 0.00080580
Iteration 3/25 | Loss: 0.00080579
Iteration 4/25 | Loss: 0.00080579
Iteration 5/25 | Loss: 0.00080579
Iteration 6/25 | Loss: 0.00080579
Iteration 7/25 | Loss: 0.00080579
Iteration 8/25 | Loss: 0.00080579
Iteration 9/25 | Loss: 0.00080579
Iteration 10/25 | Loss: 0.00080579
Iteration 11/25 | Loss: 0.00080579
Iteration 12/25 | Loss: 0.00080579
Iteration 13/25 | Loss: 0.00080579
Iteration 14/25 | Loss: 0.00080579
Iteration 15/25 | Loss: 0.00080579
Iteration 16/25 | Loss: 0.00080579
Iteration 17/25 | Loss: 0.00080579
Iteration 18/25 | Loss: 0.00080579
Iteration 19/25 | Loss: 0.00080579
Iteration 20/25 | Loss: 0.00080579
Iteration 21/25 | Loss: 0.00080579
Iteration 22/25 | Loss: 0.00080579
Iteration 23/25 | Loss: 0.00080579
Iteration 24/25 | Loss: 0.00080579
Iteration 25/25 | Loss: 0.00080579

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080579
Iteration 2/1000 | Loss: 0.00002708
Iteration 3/1000 | Loss: 0.00001915
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001614
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001434
Iteration 22/1000 | Loss: 0.00001433
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001430
Iteration 27/1000 | Loss: 0.00001429
Iteration 28/1000 | Loss: 0.00001429
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001422
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001421
Iteration 40/1000 | Loss: 0.00001421
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001417
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001412
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001408
Iteration 54/1000 | Loss: 0.00001408
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001401
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001392
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001391
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001390
Iteration 97/1000 | Loss: 0.00001390
Iteration 98/1000 | Loss: 0.00001390
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001386
Iteration 112/1000 | Loss: 0.00001386
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001386
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001385
Iteration 119/1000 | Loss: 0.00001385
Iteration 120/1000 | Loss: 0.00001385
Iteration 121/1000 | Loss: 0.00001385
Iteration 122/1000 | Loss: 0.00001385
Iteration 123/1000 | Loss: 0.00001385
Iteration 124/1000 | Loss: 0.00001385
Iteration 125/1000 | Loss: 0.00001385
Iteration 126/1000 | Loss: 0.00001385
Iteration 127/1000 | Loss: 0.00001384
Iteration 128/1000 | Loss: 0.00001384
Iteration 129/1000 | Loss: 0.00001384
Iteration 130/1000 | Loss: 0.00001383
Iteration 131/1000 | Loss: 0.00001383
Iteration 132/1000 | Loss: 0.00001383
Iteration 133/1000 | Loss: 0.00001383
Iteration 134/1000 | Loss: 0.00001383
Iteration 135/1000 | Loss: 0.00001383
Iteration 136/1000 | Loss: 0.00001383
Iteration 137/1000 | Loss: 0.00001382
Iteration 138/1000 | Loss: 0.00001382
Iteration 139/1000 | Loss: 0.00001382
Iteration 140/1000 | Loss: 0.00001382
Iteration 141/1000 | Loss: 0.00001382
Iteration 142/1000 | Loss: 0.00001382
Iteration 143/1000 | Loss: 0.00001382
Iteration 144/1000 | Loss: 0.00001382
Iteration 145/1000 | Loss: 0.00001382
Iteration 146/1000 | Loss: 0.00001382
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001381
Iteration 150/1000 | Loss: 0.00001381
Iteration 151/1000 | Loss: 0.00001381
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001380
Iteration 160/1000 | Loss: 0.00001380
Iteration 161/1000 | Loss: 0.00001380
Iteration 162/1000 | Loss: 0.00001380
Iteration 163/1000 | Loss: 0.00001380
Iteration 164/1000 | Loss: 0.00001380
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001379
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001377
Iteration 202/1000 | Loss: 0.00001377
Iteration 203/1000 | Loss: 0.00001377
Iteration 204/1000 | Loss: 0.00001377
Iteration 205/1000 | Loss: 0.00001377
Iteration 206/1000 | Loss: 0.00001377
Iteration 207/1000 | Loss: 0.00001377
Iteration 208/1000 | Loss: 0.00001377
Iteration 209/1000 | Loss: 0.00001377
Iteration 210/1000 | Loss: 0.00001377
Iteration 211/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.3767155905952677e-05, 1.3767155905952677e-05, 1.3767155905952677e-05, 1.3767155905952677e-05, 1.3767155905952677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3767155905952677e-05

Optimization complete. Final v2v error: 3.157691240310669 mm

Highest mean error: 3.867703676223755 mm for frame 70

Lowest mean error: 2.8446733951568604 mm for frame 25

Saving results

Total time: 43.14656209945679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391718
Iteration 2/25 | Loss: 0.00127936
Iteration 3/25 | Loss: 0.00122703
Iteration 4/25 | Loss: 0.00121736
Iteration 5/25 | Loss: 0.00121486
Iteration 6/25 | Loss: 0.00121486
Iteration 7/25 | Loss: 0.00121486
Iteration 8/25 | Loss: 0.00121486
Iteration 9/25 | Loss: 0.00121486
Iteration 10/25 | Loss: 0.00121486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012148618698120117, 0.0012148618698120117, 0.0012148618698120117, 0.0012148618698120117, 0.0012148618698120117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012148618698120117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51524305
Iteration 2/25 | Loss: 0.00079516
Iteration 3/25 | Loss: 0.00079516
Iteration 4/25 | Loss: 0.00079516
Iteration 5/25 | Loss: 0.00079516
Iteration 6/25 | Loss: 0.00079516
Iteration 7/25 | Loss: 0.00079515
Iteration 8/25 | Loss: 0.00079515
Iteration 9/25 | Loss: 0.00079515
Iteration 10/25 | Loss: 0.00079515
Iteration 11/25 | Loss: 0.00079515
Iteration 12/25 | Loss: 0.00079515
Iteration 13/25 | Loss: 0.00079515
Iteration 14/25 | Loss: 0.00079515
Iteration 15/25 | Loss: 0.00079515
Iteration 16/25 | Loss: 0.00079515
Iteration 17/25 | Loss: 0.00079515
Iteration 18/25 | Loss: 0.00079515
Iteration 19/25 | Loss: 0.00079515
Iteration 20/25 | Loss: 0.00079515
Iteration 21/25 | Loss: 0.00079515
Iteration 22/25 | Loss: 0.00079515
Iteration 23/25 | Loss: 0.00079515
Iteration 24/25 | Loss: 0.00079515
Iteration 25/25 | Loss: 0.00079515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079515
Iteration 2/1000 | Loss: 0.00001908
Iteration 3/1000 | Loss: 0.00001393
Iteration 4/1000 | Loss: 0.00001305
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001168
Iteration 7/1000 | Loss: 0.00001148
Iteration 8/1000 | Loss: 0.00001135
Iteration 9/1000 | Loss: 0.00001132
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001110
Iteration 12/1000 | Loss: 0.00001096
Iteration 13/1000 | Loss: 0.00001096
Iteration 14/1000 | Loss: 0.00001088
Iteration 15/1000 | Loss: 0.00001088
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001087
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001073
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001071
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001068
Iteration 26/1000 | Loss: 0.00001068
Iteration 27/1000 | Loss: 0.00001067
Iteration 28/1000 | Loss: 0.00001066
Iteration 29/1000 | Loss: 0.00001066
Iteration 30/1000 | Loss: 0.00001065
Iteration 31/1000 | Loss: 0.00001065
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001064
Iteration 34/1000 | Loss: 0.00001064
Iteration 35/1000 | Loss: 0.00001064
Iteration 36/1000 | Loss: 0.00001063
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001063
Iteration 39/1000 | Loss: 0.00001060
Iteration 40/1000 | Loss: 0.00001060
Iteration 41/1000 | Loss: 0.00001060
Iteration 42/1000 | Loss: 0.00001060
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001059
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001055
Iteration 51/1000 | Loss: 0.00001055
Iteration 52/1000 | Loss: 0.00001055
Iteration 53/1000 | Loss: 0.00001055
Iteration 54/1000 | Loss: 0.00001055
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001054
Iteration 58/1000 | Loss: 0.00001054
Iteration 59/1000 | Loss: 0.00001054
Iteration 60/1000 | Loss: 0.00001054
Iteration 61/1000 | Loss: 0.00001054
Iteration 62/1000 | Loss: 0.00001054
Iteration 63/1000 | Loss: 0.00001053
Iteration 64/1000 | Loss: 0.00001053
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001050
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001049
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001048
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001044
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001041
Iteration 88/1000 | Loss: 0.00001039
Iteration 89/1000 | Loss: 0.00001039
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001038
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001034
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001033
Iteration 101/1000 | Loss: 0.00001032
Iteration 102/1000 | Loss: 0.00001031
Iteration 103/1000 | Loss: 0.00001031
Iteration 104/1000 | Loss: 0.00001031
Iteration 105/1000 | Loss: 0.00001031
Iteration 106/1000 | Loss: 0.00001030
Iteration 107/1000 | Loss: 0.00001030
Iteration 108/1000 | Loss: 0.00001030
Iteration 109/1000 | Loss: 0.00001030
Iteration 110/1000 | Loss: 0.00001030
Iteration 111/1000 | Loss: 0.00001030
Iteration 112/1000 | Loss: 0.00001030
Iteration 113/1000 | Loss: 0.00001029
Iteration 114/1000 | Loss: 0.00001029
Iteration 115/1000 | Loss: 0.00001029
Iteration 116/1000 | Loss: 0.00001029
Iteration 117/1000 | Loss: 0.00001029
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001028
Iteration 121/1000 | Loss: 0.00001027
Iteration 122/1000 | Loss: 0.00001027
Iteration 123/1000 | Loss: 0.00001027
Iteration 124/1000 | Loss: 0.00001027
Iteration 125/1000 | Loss: 0.00001025
Iteration 126/1000 | Loss: 0.00001023
Iteration 127/1000 | Loss: 0.00001023
Iteration 128/1000 | Loss: 0.00001023
Iteration 129/1000 | Loss: 0.00001023
Iteration 130/1000 | Loss: 0.00001022
Iteration 131/1000 | Loss: 0.00001022
Iteration 132/1000 | Loss: 0.00001022
Iteration 133/1000 | Loss: 0.00001022
Iteration 134/1000 | Loss: 0.00001022
Iteration 135/1000 | Loss: 0.00001022
Iteration 136/1000 | Loss: 0.00001021
Iteration 137/1000 | Loss: 0.00001021
Iteration 138/1000 | Loss: 0.00001021
Iteration 139/1000 | Loss: 0.00001021
Iteration 140/1000 | Loss: 0.00001021
Iteration 141/1000 | Loss: 0.00001021
Iteration 142/1000 | Loss: 0.00001021
Iteration 143/1000 | Loss: 0.00001021
Iteration 144/1000 | Loss: 0.00001020
Iteration 145/1000 | Loss: 0.00001020
Iteration 146/1000 | Loss: 0.00001020
Iteration 147/1000 | Loss: 0.00001020
Iteration 148/1000 | Loss: 0.00001020
Iteration 149/1000 | Loss: 0.00001020
Iteration 150/1000 | Loss: 0.00001020
Iteration 151/1000 | Loss: 0.00001020
Iteration 152/1000 | Loss: 0.00001020
Iteration 153/1000 | Loss: 0.00001020
Iteration 154/1000 | Loss: 0.00001019
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001019
Iteration 168/1000 | Loss: 0.00001019
Iteration 169/1000 | Loss: 0.00001018
Iteration 170/1000 | Loss: 0.00001018
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001018
Iteration 174/1000 | Loss: 0.00001018
Iteration 175/1000 | Loss: 0.00001018
Iteration 176/1000 | Loss: 0.00001018
Iteration 177/1000 | Loss: 0.00001018
Iteration 178/1000 | Loss: 0.00001018
Iteration 179/1000 | Loss: 0.00001018
Iteration 180/1000 | Loss: 0.00001018
Iteration 181/1000 | Loss: 0.00001018
Iteration 182/1000 | Loss: 0.00001018
Iteration 183/1000 | Loss: 0.00001018
Iteration 184/1000 | Loss: 0.00001018
Iteration 185/1000 | Loss: 0.00001018
Iteration 186/1000 | Loss: 0.00001018
Iteration 187/1000 | Loss: 0.00001018
Iteration 188/1000 | Loss: 0.00001018
Iteration 189/1000 | Loss: 0.00001018
Iteration 190/1000 | Loss: 0.00001018
Iteration 191/1000 | Loss: 0.00001018
Iteration 192/1000 | Loss: 0.00001018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.0175896022701636e-05, 1.0175896022701636e-05, 1.0175896022701636e-05, 1.0175896022701636e-05, 1.0175896022701636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0175896022701636e-05

Optimization complete. Final v2v error: 2.764871597290039 mm

Highest mean error: 2.8405508995056152 mm for frame 148

Lowest mean error: 2.712651252746582 mm for frame 58

Saving results

Total time: 39.3934760093689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821017
Iteration 2/25 | Loss: 0.00137245
Iteration 3/25 | Loss: 0.00128299
Iteration 4/25 | Loss: 0.00127370
Iteration 5/25 | Loss: 0.00127107
Iteration 6/25 | Loss: 0.00127107
Iteration 7/25 | Loss: 0.00127107
Iteration 8/25 | Loss: 0.00127107
Iteration 9/25 | Loss: 0.00127107
Iteration 10/25 | Loss: 0.00127107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012710671871900558, 0.0012710671871900558, 0.0012710671871900558, 0.0012710671871900558, 0.0012710671871900558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012710671871900558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80278254
Iteration 2/25 | Loss: 0.00090789
Iteration 3/25 | Loss: 0.00090789
Iteration 4/25 | Loss: 0.00090788
Iteration 5/25 | Loss: 0.00090788
Iteration 6/25 | Loss: 0.00090788
Iteration 7/25 | Loss: 0.00090788
Iteration 8/25 | Loss: 0.00090788
Iteration 9/25 | Loss: 0.00090788
Iteration 10/25 | Loss: 0.00090788
Iteration 11/25 | Loss: 0.00090788
Iteration 12/25 | Loss: 0.00090788
Iteration 13/25 | Loss: 0.00090788
Iteration 14/25 | Loss: 0.00090788
Iteration 15/25 | Loss: 0.00090788
Iteration 16/25 | Loss: 0.00090788
Iteration 17/25 | Loss: 0.00090788
Iteration 18/25 | Loss: 0.00090788
Iteration 19/25 | Loss: 0.00090788
Iteration 20/25 | Loss: 0.00090788
Iteration 21/25 | Loss: 0.00090788
Iteration 22/25 | Loss: 0.00090788
Iteration 23/25 | Loss: 0.00090788
Iteration 24/25 | Loss: 0.00090788
Iteration 25/25 | Loss: 0.00090788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090788
Iteration 2/1000 | Loss: 0.00002431
Iteration 3/1000 | Loss: 0.00001929
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001573
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001464
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001423
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001411
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001405
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001387
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001380
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001372
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001368
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001367
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001364
Iteration 86/1000 | Loss: 0.00001364
Iteration 87/1000 | Loss: 0.00001364
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001363
Iteration 96/1000 | Loss: 0.00001363
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001362
Iteration 118/1000 | Loss: 0.00001362
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001361
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001361
Iteration 125/1000 | Loss: 0.00001361
Iteration 126/1000 | Loss: 0.00001361
Iteration 127/1000 | Loss: 0.00001361
Iteration 128/1000 | Loss: 0.00001361
Iteration 129/1000 | Loss: 0.00001361
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001360
Iteration 132/1000 | Loss: 0.00001360
Iteration 133/1000 | Loss: 0.00001360
Iteration 134/1000 | Loss: 0.00001360
Iteration 135/1000 | Loss: 0.00001360
Iteration 136/1000 | Loss: 0.00001360
Iteration 137/1000 | Loss: 0.00001360
Iteration 138/1000 | Loss: 0.00001360
Iteration 139/1000 | Loss: 0.00001360
Iteration 140/1000 | Loss: 0.00001359
Iteration 141/1000 | Loss: 0.00001359
Iteration 142/1000 | Loss: 0.00001359
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Iteration 145/1000 | Loss: 0.00001359
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001359
Iteration 158/1000 | Loss: 0.00001359
Iteration 159/1000 | Loss: 0.00001359
Iteration 160/1000 | Loss: 0.00001359
Iteration 161/1000 | Loss: 0.00001359
Iteration 162/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.359033467451809e-05, 1.359033467451809e-05, 1.359033467451809e-05, 1.359033467451809e-05, 1.359033467451809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.359033467451809e-05

Optimization complete. Final v2v error: 3.1486804485321045 mm

Highest mean error: 3.8676917552948 mm for frame 106

Lowest mean error: 2.919339418411255 mm for frame 71

Saving results

Total time: 39.3418288230896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571680
Iteration 2/25 | Loss: 0.00157597
Iteration 3/25 | Loss: 0.00138896
Iteration 4/25 | Loss: 0.00136479
Iteration 5/25 | Loss: 0.00135964
Iteration 6/25 | Loss: 0.00135795
Iteration 7/25 | Loss: 0.00135747
Iteration 8/25 | Loss: 0.00135747
Iteration 9/25 | Loss: 0.00135747
Iteration 10/25 | Loss: 0.00135747
Iteration 11/25 | Loss: 0.00135747
Iteration 12/25 | Loss: 0.00135747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013574680779129267, 0.0013574680779129267, 0.0013574680779129267, 0.0013574680779129267, 0.0013574680779129267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013574680779129267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51971149
Iteration 2/25 | Loss: 0.00086890
Iteration 3/25 | Loss: 0.00086890
Iteration 4/25 | Loss: 0.00086890
Iteration 5/25 | Loss: 0.00086890
Iteration 6/25 | Loss: 0.00086890
Iteration 7/25 | Loss: 0.00086890
Iteration 8/25 | Loss: 0.00086890
Iteration 9/25 | Loss: 0.00086890
Iteration 10/25 | Loss: 0.00086890
Iteration 11/25 | Loss: 0.00086890
Iteration 12/25 | Loss: 0.00086890
Iteration 13/25 | Loss: 0.00086890
Iteration 14/25 | Loss: 0.00086890
Iteration 15/25 | Loss: 0.00086890
Iteration 16/25 | Loss: 0.00086890
Iteration 17/25 | Loss: 0.00086890
Iteration 18/25 | Loss: 0.00086890
Iteration 19/25 | Loss: 0.00086890
Iteration 20/25 | Loss: 0.00086890
Iteration 21/25 | Loss: 0.00086890
Iteration 22/25 | Loss: 0.00086890
Iteration 23/25 | Loss: 0.00086890
Iteration 24/25 | Loss: 0.00086890
Iteration 25/25 | Loss: 0.00086890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086890
Iteration 2/1000 | Loss: 0.00005127
Iteration 3/1000 | Loss: 0.00003740
Iteration 4/1000 | Loss: 0.00003408
Iteration 5/1000 | Loss: 0.00003203
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00003115
Iteration 8/1000 | Loss: 0.00002886
Iteration 9/1000 | Loss: 0.00002815
Iteration 10/1000 | Loss: 0.00026469
Iteration 11/1000 | Loss: 0.00051419
Iteration 12/1000 | Loss: 0.00041397
Iteration 13/1000 | Loss: 0.00020838
Iteration 14/1000 | Loss: 0.00004981
Iteration 15/1000 | Loss: 0.00004040
Iteration 16/1000 | Loss: 0.00003876
Iteration 17/1000 | Loss: 0.00003459
Iteration 18/1000 | Loss: 0.00020407
Iteration 19/1000 | Loss: 0.00003283
Iteration 20/1000 | Loss: 0.00002964
Iteration 21/1000 | Loss: 0.00002889
Iteration 22/1000 | Loss: 0.00002789
Iteration 23/1000 | Loss: 0.00023039
Iteration 24/1000 | Loss: 0.00042322
Iteration 25/1000 | Loss: 0.00011126
Iteration 26/1000 | Loss: 0.00003986
Iteration 27/1000 | Loss: 0.00025132
Iteration 28/1000 | Loss: 0.00016965
Iteration 29/1000 | Loss: 0.00003448
Iteration 30/1000 | Loss: 0.00002678
Iteration 31/1000 | Loss: 0.00024258
Iteration 32/1000 | Loss: 0.00013139
Iteration 33/1000 | Loss: 0.00024805
Iteration 34/1000 | Loss: 0.00008156
Iteration 35/1000 | Loss: 0.00024209
Iteration 36/1000 | Loss: 0.00003208
Iteration 37/1000 | Loss: 0.00002855
Iteration 38/1000 | Loss: 0.00002666
Iteration 39/1000 | Loss: 0.00002570
Iteration 40/1000 | Loss: 0.00002503
Iteration 41/1000 | Loss: 0.00002470
Iteration 42/1000 | Loss: 0.00002447
Iteration 43/1000 | Loss: 0.00021625
Iteration 44/1000 | Loss: 0.00003004
Iteration 45/1000 | Loss: 0.00002756
Iteration 46/1000 | Loss: 0.00002660
Iteration 47/1000 | Loss: 0.00002556
Iteration 48/1000 | Loss: 0.00002512
Iteration 49/1000 | Loss: 0.00003093
Iteration 50/1000 | Loss: 0.00002717
Iteration 51/1000 | Loss: 0.00019147
Iteration 52/1000 | Loss: 0.00014632
Iteration 53/1000 | Loss: 0.00010083
Iteration 54/1000 | Loss: 0.00018207
Iteration 55/1000 | Loss: 0.00008112
Iteration 56/1000 | Loss: 0.00002627
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002383
Iteration 59/1000 | Loss: 0.00002788
Iteration 60/1000 | Loss: 0.00002681
Iteration 61/1000 | Loss: 0.00002488
Iteration 62/1000 | Loss: 0.00002328
Iteration 63/1000 | Loss: 0.00002324
Iteration 64/1000 | Loss: 0.00002324
Iteration 65/1000 | Loss: 0.00002324
Iteration 66/1000 | Loss: 0.00002324
Iteration 67/1000 | Loss: 0.00002324
Iteration 68/1000 | Loss: 0.00002324
Iteration 69/1000 | Loss: 0.00002324
Iteration 70/1000 | Loss: 0.00002607
Iteration 71/1000 | Loss: 0.00002610
Iteration 72/1000 | Loss: 0.00002583
Iteration 73/1000 | Loss: 0.00002349
Iteration 74/1000 | Loss: 0.00002323
Iteration 75/1000 | Loss: 0.00002318
Iteration 76/1000 | Loss: 0.00002318
Iteration 77/1000 | Loss: 0.00002317
Iteration 78/1000 | Loss: 0.00002316
Iteration 79/1000 | Loss: 0.00002316
Iteration 80/1000 | Loss: 0.00002315
Iteration 81/1000 | Loss: 0.00002312
Iteration 82/1000 | Loss: 0.00002303
Iteration 83/1000 | Loss: 0.00002296
Iteration 84/1000 | Loss: 0.00025451
Iteration 85/1000 | Loss: 0.00005017
Iteration 86/1000 | Loss: 0.00003209
Iteration 87/1000 | Loss: 0.00002681
Iteration 88/1000 | Loss: 0.00002532
Iteration 89/1000 | Loss: 0.00002464
Iteration 90/1000 | Loss: 0.00002432
Iteration 91/1000 | Loss: 0.00002394
Iteration 92/1000 | Loss: 0.00002387
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00033975
Iteration 95/1000 | Loss: 0.00010729
Iteration 96/1000 | Loss: 0.00016397
Iteration 97/1000 | Loss: 0.00010366
Iteration 98/1000 | Loss: 0.00002463
Iteration 99/1000 | Loss: 0.00002355
Iteration 100/1000 | Loss: 0.00002272
Iteration 101/1000 | Loss: 0.00002242
Iteration 102/1000 | Loss: 0.00002220
Iteration 103/1000 | Loss: 0.00002210
Iteration 104/1000 | Loss: 0.00002203
Iteration 105/1000 | Loss: 0.00002199
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002197
Iteration 108/1000 | Loss: 0.00002196
Iteration 109/1000 | Loss: 0.00002196
Iteration 110/1000 | Loss: 0.00002195
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002193
Iteration 114/1000 | Loss: 0.00002193
Iteration 115/1000 | Loss: 0.00002193
Iteration 116/1000 | Loss: 0.00002192
Iteration 117/1000 | Loss: 0.00002192
Iteration 118/1000 | Loss: 0.00002192
Iteration 119/1000 | Loss: 0.00002192
Iteration 120/1000 | Loss: 0.00002191
Iteration 121/1000 | Loss: 0.00002191
Iteration 122/1000 | Loss: 0.00002191
Iteration 123/1000 | Loss: 0.00002191
Iteration 124/1000 | Loss: 0.00002191
Iteration 125/1000 | Loss: 0.00002190
Iteration 126/1000 | Loss: 0.00002190
Iteration 127/1000 | Loss: 0.00002190
Iteration 128/1000 | Loss: 0.00002190
Iteration 129/1000 | Loss: 0.00002190
Iteration 130/1000 | Loss: 0.00002190
Iteration 131/1000 | Loss: 0.00002190
Iteration 132/1000 | Loss: 0.00002190
Iteration 133/1000 | Loss: 0.00002189
Iteration 134/1000 | Loss: 0.00002189
Iteration 135/1000 | Loss: 0.00002189
Iteration 136/1000 | Loss: 0.00002189
Iteration 137/1000 | Loss: 0.00002188
Iteration 138/1000 | Loss: 0.00002188
Iteration 139/1000 | Loss: 0.00002188
Iteration 140/1000 | Loss: 0.00002188
Iteration 141/1000 | Loss: 0.00002188
Iteration 142/1000 | Loss: 0.00002188
Iteration 143/1000 | Loss: 0.00002188
Iteration 144/1000 | Loss: 0.00002188
Iteration 145/1000 | Loss: 0.00002188
Iteration 146/1000 | Loss: 0.00002188
Iteration 147/1000 | Loss: 0.00002187
Iteration 148/1000 | Loss: 0.00002187
Iteration 149/1000 | Loss: 0.00002187
Iteration 150/1000 | Loss: 0.00002187
Iteration 151/1000 | Loss: 0.00002187
Iteration 152/1000 | Loss: 0.00002187
Iteration 153/1000 | Loss: 0.00002187
Iteration 154/1000 | Loss: 0.00002187
Iteration 155/1000 | Loss: 0.00002187
Iteration 156/1000 | Loss: 0.00002186
Iteration 157/1000 | Loss: 0.00002186
Iteration 158/1000 | Loss: 0.00002186
Iteration 159/1000 | Loss: 0.00002186
Iteration 160/1000 | Loss: 0.00002186
Iteration 161/1000 | Loss: 0.00002186
Iteration 162/1000 | Loss: 0.00002186
Iteration 163/1000 | Loss: 0.00002186
Iteration 164/1000 | Loss: 0.00002186
Iteration 165/1000 | Loss: 0.00002186
Iteration 166/1000 | Loss: 0.00002186
Iteration 167/1000 | Loss: 0.00002186
Iteration 168/1000 | Loss: 0.00002186
Iteration 169/1000 | Loss: 0.00002186
Iteration 170/1000 | Loss: 0.00002186
Iteration 171/1000 | Loss: 0.00002186
Iteration 172/1000 | Loss: 0.00002186
Iteration 173/1000 | Loss: 0.00002186
Iteration 174/1000 | Loss: 0.00002186
Iteration 175/1000 | Loss: 0.00002186
Iteration 176/1000 | Loss: 0.00002186
Iteration 177/1000 | Loss: 0.00002186
Iteration 178/1000 | Loss: 0.00002186
Iteration 179/1000 | Loss: 0.00002186
Iteration 180/1000 | Loss: 0.00002186
Iteration 181/1000 | Loss: 0.00002186
Iteration 182/1000 | Loss: 0.00002186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.185568337154109e-05, 2.185568337154109e-05, 2.185568337154109e-05, 2.185568337154109e-05, 2.185568337154109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.185568337154109e-05

Optimization complete. Final v2v error: 3.8842051029205322 mm

Highest mean error: 6.6348090171813965 mm for frame 148

Lowest mean error: 3.0490574836730957 mm for frame 74

Saving results

Total time: 160.5210783481598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472052
Iteration 2/25 | Loss: 0.00159501
Iteration 3/25 | Loss: 0.00138641
Iteration 4/25 | Loss: 0.00136031
Iteration 5/25 | Loss: 0.00135645
Iteration 6/25 | Loss: 0.00135608
Iteration 7/25 | Loss: 0.00135608
Iteration 8/25 | Loss: 0.00135608
Iteration 9/25 | Loss: 0.00135608
Iteration 10/25 | Loss: 0.00135608
Iteration 11/25 | Loss: 0.00135608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013560816878452897, 0.0013560816878452897, 0.0013560816878452897, 0.0013560816878452897, 0.0013560816878452897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013560816878452897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42570233
Iteration 2/25 | Loss: 0.00088873
Iteration 3/25 | Loss: 0.00088871
Iteration 4/25 | Loss: 0.00088871
Iteration 5/25 | Loss: 0.00088871
Iteration 6/25 | Loss: 0.00088871
Iteration 7/25 | Loss: 0.00088871
Iteration 8/25 | Loss: 0.00088871
Iteration 9/25 | Loss: 0.00088871
Iteration 10/25 | Loss: 0.00088871
Iteration 11/25 | Loss: 0.00088871
Iteration 12/25 | Loss: 0.00088871
Iteration 13/25 | Loss: 0.00088871
Iteration 14/25 | Loss: 0.00088871
Iteration 15/25 | Loss: 0.00088871
Iteration 16/25 | Loss: 0.00088871
Iteration 17/25 | Loss: 0.00088871
Iteration 18/25 | Loss: 0.00088871
Iteration 19/25 | Loss: 0.00088871
Iteration 20/25 | Loss: 0.00088871
Iteration 21/25 | Loss: 0.00088871
Iteration 22/25 | Loss: 0.00088871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008887069998309016, 0.0008887069998309016, 0.0008887069998309016, 0.0008887069998309016, 0.0008887069998309016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008887069998309016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088871
Iteration 2/1000 | Loss: 0.00005564
Iteration 3/1000 | Loss: 0.00003272
Iteration 4/1000 | Loss: 0.00002957
Iteration 5/1000 | Loss: 0.00002795
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002550
Iteration 8/1000 | Loss: 0.00002475
Iteration 9/1000 | Loss: 0.00002394
Iteration 10/1000 | Loss: 0.00002338
Iteration 11/1000 | Loss: 0.00002305
Iteration 12/1000 | Loss: 0.00002285
Iteration 13/1000 | Loss: 0.00002270
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002252
Iteration 16/1000 | Loss: 0.00002246
Iteration 17/1000 | Loss: 0.00002237
Iteration 18/1000 | Loss: 0.00002236
Iteration 19/1000 | Loss: 0.00002232
Iteration 20/1000 | Loss: 0.00002230
Iteration 21/1000 | Loss: 0.00002228
Iteration 22/1000 | Loss: 0.00002228
Iteration 23/1000 | Loss: 0.00002227
Iteration 24/1000 | Loss: 0.00002226
Iteration 25/1000 | Loss: 0.00002226
Iteration 26/1000 | Loss: 0.00002226
Iteration 27/1000 | Loss: 0.00002225
Iteration 28/1000 | Loss: 0.00002225
Iteration 29/1000 | Loss: 0.00002225
Iteration 30/1000 | Loss: 0.00002225
Iteration 31/1000 | Loss: 0.00002224
Iteration 32/1000 | Loss: 0.00002224
Iteration 33/1000 | Loss: 0.00002224
Iteration 34/1000 | Loss: 0.00002223
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002222
Iteration 38/1000 | Loss: 0.00002222
Iteration 39/1000 | Loss: 0.00002222
Iteration 40/1000 | Loss: 0.00002221
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00002220
Iteration 43/1000 | Loss: 0.00002219
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002219
Iteration 46/1000 | Loss: 0.00002218
Iteration 47/1000 | Loss: 0.00002218
Iteration 48/1000 | Loss: 0.00002218
Iteration 49/1000 | Loss: 0.00002218
Iteration 50/1000 | Loss: 0.00002218
Iteration 51/1000 | Loss: 0.00002218
Iteration 52/1000 | Loss: 0.00002218
Iteration 53/1000 | Loss: 0.00002218
Iteration 54/1000 | Loss: 0.00002216
Iteration 55/1000 | Loss: 0.00002216
Iteration 56/1000 | Loss: 0.00002216
Iteration 57/1000 | Loss: 0.00002216
Iteration 58/1000 | Loss: 0.00002216
Iteration 59/1000 | Loss: 0.00002216
Iteration 60/1000 | Loss: 0.00002216
Iteration 61/1000 | Loss: 0.00002216
Iteration 62/1000 | Loss: 0.00002215
Iteration 63/1000 | Loss: 0.00002215
Iteration 64/1000 | Loss: 0.00002215
Iteration 65/1000 | Loss: 0.00002215
Iteration 66/1000 | Loss: 0.00002215
Iteration 67/1000 | Loss: 0.00002215
Iteration 68/1000 | Loss: 0.00002215
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002213
Iteration 71/1000 | Loss: 0.00002213
Iteration 72/1000 | Loss: 0.00002213
Iteration 73/1000 | Loss: 0.00002212
Iteration 74/1000 | Loss: 0.00002212
Iteration 75/1000 | Loss: 0.00002212
Iteration 76/1000 | Loss: 0.00002211
Iteration 77/1000 | Loss: 0.00002211
Iteration 78/1000 | Loss: 0.00002210
Iteration 79/1000 | Loss: 0.00002210
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002209
Iteration 82/1000 | Loss: 0.00002209
Iteration 83/1000 | Loss: 0.00002209
Iteration 84/1000 | Loss: 0.00002209
Iteration 85/1000 | Loss: 0.00002208
Iteration 86/1000 | Loss: 0.00002208
Iteration 87/1000 | Loss: 0.00002208
Iteration 88/1000 | Loss: 0.00002208
Iteration 89/1000 | Loss: 0.00002208
Iteration 90/1000 | Loss: 0.00002208
Iteration 91/1000 | Loss: 0.00002208
Iteration 92/1000 | Loss: 0.00002208
Iteration 93/1000 | Loss: 0.00002208
Iteration 94/1000 | Loss: 0.00002208
Iteration 95/1000 | Loss: 0.00002208
Iteration 96/1000 | Loss: 0.00002208
Iteration 97/1000 | Loss: 0.00002207
Iteration 98/1000 | Loss: 0.00002207
Iteration 99/1000 | Loss: 0.00002206
Iteration 100/1000 | Loss: 0.00002206
Iteration 101/1000 | Loss: 0.00002206
Iteration 102/1000 | Loss: 0.00002206
Iteration 103/1000 | Loss: 0.00002206
Iteration 104/1000 | Loss: 0.00002206
Iteration 105/1000 | Loss: 0.00002206
Iteration 106/1000 | Loss: 0.00002206
Iteration 107/1000 | Loss: 0.00002206
Iteration 108/1000 | Loss: 0.00002206
Iteration 109/1000 | Loss: 0.00002206
Iteration 110/1000 | Loss: 0.00002206
Iteration 111/1000 | Loss: 0.00002206
Iteration 112/1000 | Loss: 0.00002205
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002205
Iteration 115/1000 | Loss: 0.00002205
Iteration 116/1000 | Loss: 0.00002204
Iteration 117/1000 | Loss: 0.00002204
Iteration 118/1000 | Loss: 0.00002204
Iteration 119/1000 | Loss: 0.00002204
Iteration 120/1000 | Loss: 0.00002204
Iteration 121/1000 | Loss: 0.00002203
Iteration 122/1000 | Loss: 0.00002203
Iteration 123/1000 | Loss: 0.00002203
Iteration 124/1000 | Loss: 0.00002202
Iteration 125/1000 | Loss: 0.00002202
Iteration 126/1000 | Loss: 0.00002202
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002201
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002200
Iteration 133/1000 | Loss: 0.00002200
Iteration 134/1000 | Loss: 0.00002200
Iteration 135/1000 | Loss: 0.00002200
Iteration 136/1000 | Loss: 0.00002200
Iteration 137/1000 | Loss: 0.00002200
Iteration 138/1000 | Loss: 0.00002200
Iteration 139/1000 | Loss: 0.00002199
Iteration 140/1000 | Loss: 0.00002199
Iteration 141/1000 | Loss: 0.00002199
Iteration 142/1000 | Loss: 0.00002199
Iteration 143/1000 | Loss: 0.00002199
Iteration 144/1000 | Loss: 0.00002199
Iteration 145/1000 | Loss: 0.00002199
Iteration 146/1000 | Loss: 0.00002199
Iteration 147/1000 | Loss: 0.00002199
Iteration 148/1000 | Loss: 0.00002198
Iteration 149/1000 | Loss: 0.00002198
Iteration 150/1000 | Loss: 0.00002198
Iteration 151/1000 | Loss: 0.00002198
Iteration 152/1000 | Loss: 0.00002198
Iteration 153/1000 | Loss: 0.00002198
Iteration 154/1000 | Loss: 0.00002198
Iteration 155/1000 | Loss: 0.00002198
Iteration 156/1000 | Loss: 0.00002198
Iteration 157/1000 | Loss: 0.00002197
Iteration 158/1000 | Loss: 0.00002197
Iteration 159/1000 | Loss: 0.00002197
Iteration 160/1000 | Loss: 0.00002197
Iteration 161/1000 | Loss: 0.00002197
Iteration 162/1000 | Loss: 0.00002197
Iteration 163/1000 | Loss: 0.00002197
Iteration 164/1000 | Loss: 0.00002197
Iteration 165/1000 | Loss: 0.00002197
Iteration 166/1000 | Loss: 0.00002196
Iteration 167/1000 | Loss: 0.00002196
Iteration 168/1000 | Loss: 0.00002196
Iteration 169/1000 | Loss: 0.00002196
Iteration 170/1000 | Loss: 0.00002196
Iteration 171/1000 | Loss: 0.00002196
Iteration 172/1000 | Loss: 0.00002195
Iteration 173/1000 | Loss: 0.00002195
Iteration 174/1000 | Loss: 0.00002195
Iteration 175/1000 | Loss: 0.00002195
Iteration 176/1000 | Loss: 0.00002195
Iteration 177/1000 | Loss: 0.00002195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.1953877876512706e-05, 2.1953877876512706e-05, 2.1953877876512706e-05, 2.1953877876512706e-05, 2.1953877876512706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1953877876512706e-05

Optimization complete. Final v2v error: 3.924467086791992 mm

Highest mean error: 4.218486309051514 mm for frame 234

Lowest mean error: 3.4998300075531006 mm for frame 38

Saving results

Total time: 48.84568381309509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039591
Iteration 2/25 | Loss: 0.00175732
Iteration 3/25 | Loss: 0.00147232
Iteration 4/25 | Loss: 0.00144474
Iteration 5/25 | Loss: 0.00143851
Iteration 6/25 | Loss: 0.00143725
Iteration 7/25 | Loss: 0.00143708
Iteration 8/25 | Loss: 0.00143708
Iteration 9/25 | Loss: 0.00143708
Iteration 10/25 | Loss: 0.00143708
Iteration 11/25 | Loss: 0.00143708
Iteration 12/25 | Loss: 0.00143708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001437080674804747, 0.001437080674804747, 0.001437080674804747, 0.001437080674804747, 0.001437080674804747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001437080674804747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51645309
Iteration 2/25 | Loss: 0.00096386
Iteration 3/25 | Loss: 0.00096384
Iteration 4/25 | Loss: 0.00096384
Iteration 5/25 | Loss: 0.00096384
Iteration 6/25 | Loss: 0.00096384
Iteration 7/25 | Loss: 0.00096384
Iteration 8/25 | Loss: 0.00096384
Iteration 9/25 | Loss: 0.00096384
Iteration 10/25 | Loss: 0.00096384
Iteration 11/25 | Loss: 0.00096384
Iteration 12/25 | Loss: 0.00096384
Iteration 13/25 | Loss: 0.00096384
Iteration 14/25 | Loss: 0.00096384
Iteration 15/25 | Loss: 0.00096384
Iteration 16/25 | Loss: 0.00096384
Iteration 17/25 | Loss: 0.00096384
Iteration 18/25 | Loss: 0.00096384
Iteration 19/25 | Loss: 0.00096384
Iteration 20/25 | Loss: 0.00096384
Iteration 21/25 | Loss: 0.00096384
Iteration 22/25 | Loss: 0.00096384
Iteration 23/25 | Loss: 0.00096384
Iteration 24/25 | Loss: 0.00096384
Iteration 25/25 | Loss: 0.00096384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009638375486247241, 0.0009638375486247241, 0.0009638375486247241, 0.0009638375486247241, 0.0009638375486247241]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009638375486247241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096384
Iteration 2/1000 | Loss: 0.00007612
Iteration 3/1000 | Loss: 0.00004552
Iteration 4/1000 | Loss: 0.00003967
Iteration 5/1000 | Loss: 0.00003796
Iteration 6/1000 | Loss: 0.00003664
Iteration 7/1000 | Loss: 0.00003560
Iteration 8/1000 | Loss: 0.00003477
Iteration 9/1000 | Loss: 0.00003434
Iteration 10/1000 | Loss: 0.00003395
Iteration 11/1000 | Loss: 0.00003364
Iteration 12/1000 | Loss: 0.00003339
Iteration 13/1000 | Loss: 0.00003323
Iteration 14/1000 | Loss: 0.00003308
Iteration 15/1000 | Loss: 0.00003304
Iteration 16/1000 | Loss: 0.00003300
Iteration 17/1000 | Loss: 0.00003300
Iteration 18/1000 | Loss: 0.00003299
Iteration 19/1000 | Loss: 0.00003299
Iteration 20/1000 | Loss: 0.00003299
Iteration 21/1000 | Loss: 0.00003298
Iteration 22/1000 | Loss: 0.00003298
Iteration 23/1000 | Loss: 0.00003296
Iteration 24/1000 | Loss: 0.00003296
Iteration 25/1000 | Loss: 0.00003295
Iteration 26/1000 | Loss: 0.00003295
Iteration 27/1000 | Loss: 0.00003295
Iteration 28/1000 | Loss: 0.00003295
Iteration 29/1000 | Loss: 0.00003295
Iteration 30/1000 | Loss: 0.00003294
Iteration 31/1000 | Loss: 0.00003294
Iteration 32/1000 | Loss: 0.00003293
Iteration 33/1000 | Loss: 0.00003293
Iteration 34/1000 | Loss: 0.00003292
Iteration 35/1000 | Loss: 0.00003291
Iteration 36/1000 | Loss: 0.00003291
Iteration 37/1000 | Loss: 0.00003291
Iteration 38/1000 | Loss: 0.00003291
Iteration 39/1000 | Loss: 0.00003291
Iteration 40/1000 | Loss: 0.00003291
Iteration 41/1000 | Loss: 0.00003291
Iteration 42/1000 | Loss: 0.00003291
Iteration 43/1000 | Loss: 0.00003291
Iteration 44/1000 | Loss: 0.00003290
Iteration 45/1000 | Loss: 0.00003290
Iteration 46/1000 | Loss: 0.00003290
Iteration 47/1000 | Loss: 0.00003290
Iteration 48/1000 | Loss: 0.00003290
Iteration 49/1000 | Loss: 0.00003289
Iteration 50/1000 | Loss: 0.00003289
Iteration 51/1000 | Loss: 0.00003289
Iteration 52/1000 | Loss: 0.00003289
Iteration 53/1000 | Loss: 0.00003288
Iteration 54/1000 | Loss: 0.00003288
Iteration 55/1000 | Loss: 0.00003287
Iteration 56/1000 | Loss: 0.00003287
Iteration 57/1000 | Loss: 0.00003287
Iteration 58/1000 | Loss: 0.00003287
Iteration 59/1000 | Loss: 0.00003286
Iteration 60/1000 | Loss: 0.00003286
Iteration 61/1000 | Loss: 0.00003286
Iteration 62/1000 | Loss: 0.00003285
Iteration 63/1000 | Loss: 0.00003285
Iteration 64/1000 | Loss: 0.00003285
Iteration 65/1000 | Loss: 0.00003285
Iteration 66/1000 | Loss: 0.00003285
Iteration 67/1000 | Loss: 0.00003284
Iteration 68/1000 | Loss: 0.00003284
Iteration 69/1000 | Loss: 0.00003284
Iteration 70/1000 | Loss: 0.00003284
Iteration 71/1000 | Loss: 0.00003284
Iteration 72/1000 | Loss: 0.00003284
Iteration 73/1000 | Loss: 0.00003284
Iteration 74/1000 | Loss: 0.00003284
Iteration 75/1000 | Loss: 0.00003284
Iteration 76/1000 | Loss: 0.00003284
Iteration 77/1000 | Loss: 0.00003284
Iteration 78/1000 | Loss: 0.00003284
Iteration 79/1000 | Loss: 0.00003284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.2837440812727436e-05, 3.2837440812727436e-05, 3.2837440812727436e-05, 3.2837440812727436e-05, 3.2837440812727436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2837440812727436e-05

Optimization complete. Final v2v error: 4.758900165557861 mm

Highest mean error: 5.64647912979126 mm for frame 194

Lowest mean error: 4.254438877105713 mm for frame 101

Saving results

Total time: 41.00563597679138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738027
Iteration 2/25 | Loss: 0.00165544
Iteration 3/25 | Loss: 0.00141219
Iteration 4/25 | Loss: 0.00139189
Iteration 5/25 | Loss: 0.00138637
Iteration 6/25 | Loss: 0.00136160
Iteration 7/25 | Loss: 0.00135997
Iteration 8/25 | Loss: 0.00135909
Iteration 9/25 | Loss: 0.00135847
Iteration 10/25 | Loss: 0.00135775
Iteration 11/25 | Loss: 0.00135948
Iteration 12/25 | Loss: 0.00135769
Iteration 13/25 | Loss: 0.00135591
Iteration 14/25 | Loss: 0.00135567
Iteration 15/25 | Loss: 0.00135565
Iteration 16/25 | Loss: 0.00135565
Iteration 17/25 | Loss: 0.00135564
Iteration 18/25 | Loss: 0.00135564
Iteration 19/25 | Loss: 0.00135564
Iteration 20/25 | Loss: 0.00135564
Iteration 21/25 | Loss: 0.00135564
Iteration 22/25 | Loss: 0.00135564
Iteration 23/25 | Loss: 0.00135564
Iteration 24/25 | Loss: 0.00135564
Iteration 25/25 | Loss: 0.00135563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77272606
Iteration 2/25 | Loss: 0.00153525
Iteration 3/25 | Loss: 0.00089978
Iteration 4/25 | Loss: 0.00089978
Iteration 5/25 | Loss: 0.00089978
Iteration 6/25 | Loss: 0.00089978
Iteration 7/25 | Loss: 0.00089978
Iteration 8/25 | Loss: 0.00089978
Iteration 9/25 | Loss: 0.00089977
Iteration 10/25 | Loss: 0.00089977
Iteration 11/25 | Loss: 0.00089977
Iteration 12/25 | Loss: 0.00089977
Iteration 13/25 | Loss: 0.00089977
Iteration 14/25 | Loss: 0.00089977
Iteration 15/25 | Loss: 0.00089977
Iteration 16/25 | Loss: 0.00089977
Iteration 17/25 | Loss: 0.00089977
Iteration 18/25 | Loss: 0.00089977
Iteration 19/25 | Loss: 0.00089977
Iteration 20/25 | Loss: 0.00089977
Iteration 21/25 | Loss: 0.00089977
Iteration 22/25 | Loss: 0.00089977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008997742552310228, 0.0008997742552310228, 0.0008997742552310228, 0.0008997742552310228, 0.0008997742552310228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008997742552310228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089977
Iteration 2/1000 | Loss: 0.00007920
Iteration 3/1000 | Loss: 0.00050562
Iteration 4/1000 | Loss: 0.00099770
Iteration 5/1000 | Loss: 0.00004253
Iteration 6/1000 | Loss: 0.00003926
Iteration 7/1000 | Loss: 0.00056839
Iteration 8/1000 | Loss: 0.00007794
Iteration 9/1000 | Loss: 0.00003683
Iteration 10/1000 | Loss: 0.00026617
Iteration 11/1000 | Loss: 0.00003588
Iteration 12/1000 | Loss: 0.00015008
Iteration 13/1000 | Loss: 0.00003584
Iteration 14/1000 | Loss: 0.00003406
Iteration 15/1000 | Loss: 0.00003318
Iteration 16/1000 | Loss: 0.00003233
Iteration 17/1000 | Loss: 0.00003157
Iteration 18/1000 | Loss: 0.00003111
Iteration 19/1000 | Loss: 0.00003084
Iteration 20/1000 | Loss: 0.00003064
Iteration 21/1000 | Loss: 0.00003044
Iteration 22/1000 | Loss: 0.00003037
Iteration 23/1000 | Loss: 0.00003032
Iteration 24/1000 | Loss: 0.00003029
Iteration 25/1000 | Loss: 0.00003028
Iteration 26/1000 | Loss: 0.00003020
Iteration 27/1000 | Loss: 0.00003020
Iteration 28/1000 | Loss: 0.00003017
Iteration 29/1000 | Loss: 0.00003017
Iteration 30/1000 | Loss: 0.00003016
Iteration 31/1000 | Loss: 0.00003016
Iteration 32/1000 | Loss: 0.00003015
Iteration 33/1000 | Loss: 0.00003015
Iteration 34/1000 | Loss: 0.00003014
Iteration 35/1000 | Loss: 0.00003013
Iteration 36/1000 | Loss: 0.00003012
Iteration 37/1000 | Loss: 0.00003012
Iteration 38/1000 | Loss: 0.00003011
Iteration 39/1000 | Loss: 0.00003011
Iteration 40/1000 | Loss: 0.00003010
Iteration 41/1000 | Loss: 0.00003010
Iteration 42/1000 | Loss: 0.00003009
Iteration 43/1000 | Loss: 0.00003006
Iteration 44/1000 | Loss: 0.00003006
Iteration 45/1000 | Loss: 0.00003006
Iteration 46/1000 | Loss: 0.00003005
Iteration 47/1000 | Loss: 0.00003005
Iteration 48/1000 | Loss: 0.00003002
Iteration 49/1000 | Loss: 0.00003001
Iteration 50/1000 | Loss: 0.00003001
Iteration 51/1000 | Loss: 0.00003001
Iteration 52/1000 | Loss: 0.00003001
Iteration 53/1000 | Loss: 0.00003001
Iteration 54/1000 | Loss: 0.00003000
Iteration 55/1000 | Loss: 0.00003000
Iteration 56/1000 | Loss: 0.00003000
Iteration 57/1000 | Loss: 0.00003000
Iteration 58/1000 | Loss: 0.00002999
Iteration 59/1000 | Loss: 0.00002998
Iteration 60/1000 | Loss: 0.00002998
Iteration 61/1000 | Loss: 0.00002998
Iteration 62/1000 | Loss: 0.00002998
Iteration 63/1000 | Loss: 0.00002998
Iteration 64/1000 | Loss: 0.00002998
Iteration 65/1000 | Loss: 0.00002998
Iteration 66/1000 | Loss: 0.00002998
Iteration 67/1000 | Loss: 0.00002998
Iteration 68/1000 | Loss: 0.00002998
Iteration 69/1000 | Loss: 0.00002997
Iteration 70/1000 | Loss: 0.00002996
Iteration 71/1000 | Loss: 0.00002996
Iteration 72/1000 | Loss: 0.00002996
Iteration 73/1000 | Loss: 0.00002996
Iteration 74/1000 | Loss: 0.00002996
Iteration 75/1000 | Loss: 0.00002996
Iteration 76/1000 | Loss: 0.00002996
Iteration 77/1000 | Loss: 0.00002996
Iteration 78/1000 | Loss: 0.00002996
Iteration 79/1000 | Loss: 0.00002996
Iteration 80/1000 | Loss: 0.00002996
Iteration 81/1000 | Loss: 0.00002996
Iteration 82/1000 | Loss: 0.00002995
Iteration 83/1000 | Loss: 0.00002995
Iteration 84/1000 | Loss: 0.00002995
Iteration 85/1000 | Loss: 0.00002995
Iteration 86/1000 | Loss: 0.00002995
Iteration 87/1000 | Loss: 0.00002995
Iteration 88/1000 | Loss: 0.00002995
Iteration 89/1000 | Loss: 0.00002995
Iteration 90/1000 | Loss: 0.00002995
Iteration 91/1000 | Loss: 0.00002995
Iteration 92/1000 | Loss: 0.00002995
Iteration 93/1000 | Loss: 0.00002995
Iteration 94/1000 | Loss: 0.00002995
Iteration 95/1000 | Loss: 0.00002995
Iteration 96/1000 | Loss: 0.00002995
Iteration 97/1000 | Loss: 0.00002995
Iteration 98/1000 | Loss: 0.00002995
Iteration 99/1000 | Loss: 0.00002995
Iteration 100/1000 | Loss: 0.00002995
Iteration 101/1000 | Loss: 0.00002995
Iteration 102/1000 | Loss: 0.00002995
Iteration 103/1000 | Loss: 0.00002995
Iteration 104/1000 | Loss: 0.00002995
Iteration 105/1000 | Loss: 0.00002995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.994541500811465e-05, 2.994541500811465e-05, 2.994541500811465e-05, 2.994541500811465e-05, 2.994541500811465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.994541500811465e-05

Optimization complete. Final v2v error: 4.199517250061035 mm

Highest mean error: 12.839884757995605 mm for frame 210

Lowest mean error: 3.4227027893066406 mm for frame 230

Saving results

Total time: 74.04206156730652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423364
Iteration 2/25 | Loss: 0.00132294
Iteration 3/25 | Loss: 0.00126226
Iteration 4/25 | Loss: 0.00125327
Iteration 5/25 | Loss: 0.00125089
Iteration 6/25 | Loss: 0.00125089
Iteration 7/25 | Loss: 0.00125089
Iteration 8/25 | Loss: 0.00125089
Iteration 9/25 | Loss: 0.00125089
Iteration 10/25 | Loss: 0.00125089
Iteration 11/25 | Loss: 0.00125089
Iteration 12/25 | Loss: 0.00125089
Iteration 13/25 | Loss: 0.00125089
Iteration 14/25 | Loss: 0.00125089
Iteration 15/25 | Loss: 0.00125089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012508933432400227, 0.0012508933432400227, 0.0012508933432400227, 0.0012508933432400227, 0.0012508933432400227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012508933432400227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.51062894
Iteration 2/25 | Loss: 0.00079374
Iteration 3/25 | Loss: 0.00079374
Iteration 4/25 | Loss: 0.00079374
Iteration 5/25 | Loss: 0.00079374
Iteration 6/25 | Loss: 0.00079374
Iteration 7/25 | Loss: 0.00079374
Iteration 8/25 | Loss: 0.00079374
Iteration 9/25 | Loss: 0.00079374
Iteration 10/25 | Loss: 0.00079374
Iteration 11/25 | Loss: 0.00079374
Iteration 12/25 | Loss: 0.00079374
Iteration 13/25 | Loss: 0.00079374
Iteration 14/25 | Loss: 0.00079374
Iteration 15/25 | Loss: 0.00079374
Iteration 16/25 | Loss: 0.00079374
Iteration 17/25 | Loss: 0.00079374
Iteration 18/25 | Loss: 0.00079374
Iteration 19/25 | Loss: 0.00079374
Iteration 20/25 | Loss: 0.00079374
Iteration 21/25 | Loss: 0.00079374
Iteration 22/25 | Loss: 0.00079374
Iteration 23/25 | Loss: 0.00079374
Iteration 24/25 | Loss: 0.00079374
Iteration 25/25 | Loss: 0.00079374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079374
Iteration 2/1000 | Loss: 0.00002856
Iteration 3/1000 | Loss: 0.00001790
Iteration 4/1000 | Loss: 0.00001632
Iteration 5/1000 | Loss: 0.00001551
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001468
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001357
Iteration 14/1000 | Loss: 0.00001344
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001340
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001312
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001311
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001307
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001297
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001283
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00001281
Iteration 109/1000 | Loss: 0.00001281
Iteration 110/1000 | Loss: 0.00001281
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001276
Iteration 144/1000 | Loss: 0.00001276
Iteration 145/1000 | Loss: 0.00001276
Iteration 146/1000 | Loss: 0.00001276
Iteration 147/1000 | Loss: 0.00001276
Iteration 148/1000 | Loss: 0.00001276
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001275
Iteration 151/1000 | Loss: 0.00001275
Iteration 152/1000 | Loss: 0.00001275
Iteration 153/1000 | Loss: 0.00001275
Iteration 154/1000 | Loss: 0.00001275
Iteration 155/1000 | Loss: 0.00001274
Iteration 156/1000 | Loss: 0.00001274
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001274
Iteration 160/1000 | Loss: 0.00001274
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Iteration 164/1000 | Loss: 0.00001273
Iteration 165/1000 | Loss: 0.00001273
Iteration 166/1000 | Loss: 0.00001273
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001272
Iteration 171/1000 | Loss: 0.00001272
Iteration 172/1000 | Loss: 0.00001272
Iteration 173/1000 | Loss: 0.00001272
Iteration 174/1000 | Loss: 0.00001271
Iteration 175/1000 | Loss: 0.00001271
Iteration 176/1000 | Loss: 0.00001271
Iteration 177/1000 | Loss: 0.00001271
Iteration 178/1000 | Loss: 0.00001271
Iteration 179/1000 | Loss: 0.00001271
Iteration 180/1000 | Loss: 0.00001271
Iteration 181/1000 | Loss: 0.00001271
Iteration 182/1000 | Loss: 0.00001271
Iteration 183/1000 | Loss: 0.00001271
Iteration 184/1000 | Loss: 0.00001271
Iteration 185/1000 | Loss: 0.00001271
Iteration 186/1000 | Loss: 0.00001271
Iteration 187/1000 | Loss: 0.00001271
Iteration 188/1000 | Loss: 0.00001271
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.271278779313434e-05, 1.271278779313434e-05, 1.271278779313434e-05, 1.271278779313434e-05, 1.271278779313434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.271278779313434e-05

Optimization complete. Final v2v error: 3.018747091293335 mm

Highest mean error: 3.376924514770508 mm for frame 156

Lowest mean error: 2.790677309036255 mm for frame 132

Saving results

Total time: 47.43102836608887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711592
Iteration 2/25 | Loss: 0.00151863
Iteration 3/25 | Loss: 0.00139872
Iteration 4/25 | Loss: 0.00138431
Iteration 5/25 | Loss: 0.00138141
Iteration 6/25 | Loss: 0.00138141
Iteration 7/25 | Loss: 0.00138141
Iteration 8/25 | Loss: 0.00138141
Iteration 9/25 | Loss: 0.00138141
Iteration 10/25 | Loss: 0.00138141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013814108679071069, 0.0013814108679071069, 0.0013814108679071069, 0.0013814108679071069, 0.0013814108679071069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013814108679071069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.19894981
Iteration 2/25 | Loss: 0.00111578
Iteration 3/25 | Loss: 0.00111576
Iteration 4/25 | Loss: 0.00111575
Iteration 5/25 | Loss: 0.00111575
Iteration 6/25 | Loss: 0.00111575
Iteration 7/25 | Loss: 0.00111575
Iteration 8/25 | Loss: 0.00111575
Iteration 9/25 | Loss: 0.00111575
Iteration 10/25 | Loss: 0.00111575
Iteration 11/25 | Loss: 0.00111575
Iteration 12/25 | Loss: 0.00111575
Iteration 13/25 | Loss: 0.00111575
Iteration 14/25 | Loss: 0.00111575
Iteration 15/25 | Loss: 0.00111575
Iteration 16/25 | Loss: 0.00111575
Iteration 17/25 | Loss: 0.00111575
Iteration 18/25 | Loss: 0.00111575
Iteration 19/25 | Loss: 0.00111575
Iteration 20/25 | Loss: 0.00111575
Iteration 21/25 | Loss: 0.00111575
Iteration 22/25 | Loss: 0.00111575
Iteration 23/25 | Loss: 0.00111575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001115752849727869, 0.001115752849727869, 0.001115752849727869, 0.001115752849727869, 0.001115752849727869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001115752849727869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111575
Iteration 2/1000 | Loss: 0.00004451
Iteration 3/1000 | Loss: 0.00003204
Iteration 4/1000 | Loss: 0.00002944
Iteration 5/1000 | Loss: 0.00002771
Iteration 6/1000 | Loss: 0.00002671
Iteration 7/1000 | Loss: 0.00002629
Iteration 8/1000 | Loss: 0.00002592
Iteration 9/1000 | Loss: 0.00002568
Iteration 10/1000 | Loss: 0.00002546
Iteration 11/1000 | Loss: 0.00002546
Iteration 12/1000 | Loss: 0.00002534
Iteration 13/1000 | Loss: 0.00002526
Iteration 14/1000 | Loss: 0.00002523
Iteration 15/1000 | Loss: 0.00002514
Iteration 16/1000 | Loss: 0.00002512
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00002511
Iteration 19/1000 | Loss: 0.00002511
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002510
Iteration 22/1000 | Loss: 0.00002510
Iteration 23/1000 | Loss: 0.00002509
Iteration 24/1000 | Loss: 0.00002509
Iteration 25/1000 | Loss: 0.00002509
Iteration 26/1000 | Loss: 0.00002508
Iteration 27/1000 | Loss: 0.00002508
Iteration 28/1000 | Loss: 0.00002508
Iteration 29/1000 | Loss: 0.00002508
Iteration 30/1000 | Loss: 0.00002508
Iteration 31/1000 | Loss: 0.00002508
Iteration 32/1000 | Loss: 0.00002508
Iteration 33/1000 | Loss: 0.00002507
Iteration 34/1000 | Loss: 0.00002507
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002507
Iteration 37/1000 | Loss: 0.00002506
Iteration 38/1000 | Loss: 0.00002506
Iteration 39/1000 | Loss: 0.00002505
Iteration 40/1000 | Loss: 0.00002505
Iteration 41/1000 | Loss: 0.00002505
Iteration 42/1000 | Loss: 0.00002505
Iteration 43/1000 | Loss: 0.00002505
Iteration 44/1000 | Loss: 0.00002505
Iteration 45/1000 | Loss: 0.00002504
Iteration 46/1000 | Loss: 0.00002504
Iteration 47/1000 | Loss: 0.00002504
Iteration 48/1000 | Loss: 0.00002504
Iteration 49/1000 | Loss: 0.00002504
Iteration 50/1000 | Loss: 0.00002504
Iteration 51/1000 | Loss: 0.00002504
Iteration 52/1000 | Loss: 0.00002504
Iteration 53/1000 | Loss: 0.00002504
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002502
Iteration 56/1000 | Loss: 0.00002501
Iteration 57/1000 | Loss: 0.00002501
Iteration 58/1000 | Loss: 0.00002501
Iteration 59/1000 | Loss: 0.00002501
Iteration 60/1000 | Loss: 0.00002500
Iteration 61/1000 | Loss: 0.00002500
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00002499
Iteration 66/1000 | Loss: 0.00002499
Iteration 67/1000 | Loss: 0.00002499
Iteration 68/1000 | Loss: 0.00002499
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002498
Iteration 71/1000 | Loss: 0.00002498
Iteration 72/1000 | Loss: 0.00002498
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00002498
Iteration 75/1000 | Loss: 0.00002498
Iteration 76/1000 | Loss: 0.00002498
Iteration 77/1000 | Loss: 0.00002498
Iteration 78/1000 | Loss: 0.00002498
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002498
Iteration 85/1000 | Loss: 0.00002498
Iteration 86/1000 | Loss: 0.00002498
Iteration 87/1000 | Loss: 0.00002497
Iteration 88/1000 | Loss: 0.00002497
Iteration 89/1000 | Loss: 0.00002497
Iteration 90/1000 | Loss: 0.00002497
Iteration 91/1000 | Loss: 0.00002497
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002497
Iteration 94/1000 | Loss: 0.00002497
Iteration 95/1000 | Loss: 0.00002497
Iteration 96/1000 | Loss: 0.00002497
Iteration 97/1000 | Loss: 0.00002497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.497401692380663e-05, 2.497401692380663e-05, 2.497401692380663e-05, 2.497401692380663e-05, 2.497401692380663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.497401692380663e-05

Optimization complete. Final v2v error: 4.168875694274902 mm

Highest mean error: 4.56075382232666 mm for frame 59

Lowest mean error: 3.6685149669647217 mm for frame 238

Saving results

Total time: 34.51969385147095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412443
Iteration 2/25 | Loss: 0.00141933
Iteration 3/25 | Loss: 0.00127072
Iteration 4/25 | Loss: 0.00125884
Iteration 5/25 | Loss: 0.00125699
Iteration 6/25 | Loss: 0.00125648
Iteration 7/25 | Loss: 0.00125648
Iteration 8/25 | Loss: 0.00125648
Iteration 9/25 | Loss: 0.00125648
Iteration 10/25 | Loss: 0.00125648
Iteration 11/25 | Loss: 0.00125648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00125647964887321, 0.00125647964887321, 0.00125647964887321, 0.00125647964887321, 0.00125647964887321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00125647964887321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41956699
Iteration 2/25 | Loss: 0.00065984
Iteration 3/25 | Loss: 0.00065983
Iteration 4/25 | Loss: 0.00065983
Iteration 5/25 | Loss: 0.00065983
Iteration 6/25 | Loss: 0.00065983
Iteration 7/25 | Loss: 0.00065983
Iteration 8/25 | Loss: 0.00065983
Iteration 9/25 | Loss: 0.00065983
Iteration 10/25 | Loss: 0.00065983
Iteration 11/25 | Loss: 0.00065983
Iteration 12/25 | Loss: 0.00065983
Iteration 13/25 | Loss: 0.00065983
Iteration 14/25 | Loss: 0.00065983
Iteration 15/25 | Loss: 0.00065983
Iteration 16/25 | Loss: 0.00065983
Iteration 17/25 | Loss: 0.00065983
Iteration 18/25 | Loss: 0.00065983
Iteration 19/25 | Loss: 0.00065983
Iteration 20/25 | Loss: 0.00065983
Iteration 21/25 | Loss: 0.00065983
Iteration 22/25 | Loss: 0.00065983
Iteration 23/25 | Loss: 0.00065983
Iteration 24/25 | Loss: 0.00065983
Iteration 25/25 | Loss: 0.00065983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065983
Iteration 2/1000 | Loss: 0.00003043
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001925
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001710
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001603
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001578
Iteration 11/1000 | Loss: 0.00001573
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001533
Iteration 18/1000 | Loss: 0.00001532
Iteration 19/1000 | Loss: 0.00001527
Iteration 20/1000 | Loss: 0.00001515
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001513
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001503
Iteration 30/1000 | Loss: 0.00001503
Iteration 31/1000 | Loss: 0.00001502
Iteration 32/1000 | Loss: 0.00001502
Iteration 33/1000 | Loss: 0.00001502
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001486
Iteration 54/1000 | Loss: 0.00001486
Iteration 55/1000 | Loss: 0.00001485
Iteration 56/1000 | Loss: 0.00001485
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001481
Iteration 61/1000 | Loss: 0.00001481
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001469
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001468
Iteration 88/1000 | Loss: 0.00001468
Iteration 89/1000 | Loss: 0.00001468
Iteration 90/1000 | Loss: 0.00001468
Iteration 91/1000 | Loss: 0.00001468
Iteration 92/1000 | Loss: 0.00001468
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001467
Iteration 103/1000 | Loss: 0.00001467
Iteration 104/1000 | Loss: 0.00001467
Iteration 105/1000 | Loss: 0.00001467
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001466
Iteration 109/1000 | Loss: 0.00001466
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00001466
Iteration 112/1000 | Loss: 0.00001466
Iteration 113/1000 | Loss: 0.00001466
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001465
Iteration 116/1000 | Loss: 0.00001465
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001465
Iteration 119/1000 | Loss: 0.00001465
Iteration 120/1000 | Loss: 0.00001465
Iteration 121/1000 | Loss: 0.00001465
Iteration 122/1000 | Loss: 0.00001465
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001464
Iteration 132/1000 | Loss: 0.00001464
Iteration 133/1000 | Loss: 0.00001464
Iteration 134/1000 | Loss: 0.00001464
Iteration 135/1000 | Loss: 0.00001464
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001463
Iteration 152/1000 | Loss: 0.00001463
Iteration 153/1000 | Loss: 0.00001463
Iteration 154/1000 | Loss: 0.00001463
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Iteration 159/1000 | Loss: 0.00001462
Iteration 160/1000 | Loss: 0.00001462
Iteration 161/1000 | Loss: 0.00001462
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001460
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001460
Iteration 176/1000 | Loss: 0.00001460
Iteration 177/1000 | Loss: 0.00001460
Iteration 178/1000 | Loss: 0.00001460
Iteration 179/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.4600860595237464e-05, 1.4600860595237464e-05, 1.4600860595237464e-05, 1.4600860595237464e-05, 1.4600860595237464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4600860595237464e-05

Optimization complete. Final v2v error: 3.269814968109131 mm

Highest mean error: 3.378767967224121 mm for frame 25

Lowest mean error: 3.1625380516052246 mm for frame 127

Saving results

Total time: 41.32035183906555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473757
Iteration 2/25 | Loss: 0.00143194
Iteration 3/25 | Loss: 0.00131352
Iteration 4/25 | Loss: 0.00129933
Iteration 5/25 | Loss: 0.00129556
Iteration 6/25 | Loss: 0.00129499
Iteration 7/25 | Loss: 0.00129499
Iteration 8/25 | Loss: 0.00129499
Iteration 9/25 | Loss: 0.00129499
Iteration 10/25 | Loss: 0.00129499
Iteration 11/25 | Loss: 0.00129499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012949861120432615, 0.0012949861120432615, 0.0012949861120432615, 0.0012949861120432615, 0.0012949861120432615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012949861120432615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15260792
Iteration 2/25 | Loss: 0.00094638
Iteration 3/25 | Loss: 0.00094638
Iteration 4/25 | Loss: 0.00094638
Iteration 5/25 | Loss: 0.00094638
Iteration 6/25 | Loss: 0.00094638
Iteration 7/25 | Loss: 0.00094638
Iteration 8/25 | Loss: 0.00094638
Iteration 9/25 | Loss: 0.00094638
Iteration 10/25 | Loss: 0.00094638
Iteration 11/25 | Loss: 0.00094637
Iteration 12/25 | Loss: 0.00094637
Iteration 13/25 | Loss: 0.00094637
Iteration 14/25 | Loss: 0.00094637
Iteration 15/25 | Loss: 0.00094637
Iteration 16/25 | Loss: 0.00094637
Iteration 17/25 | Loss: 0.00094637
Iteration 18/25 | Loss: 0.00094637
Iteration 19/25 | Loss: 0.00094637
Iteration 20/25 | Loss: 0.00094637
Iteration 21/25 | Loss: 0.00094637
Iteration 22/25 | Loss: 0.00094637
Iteration 23/25 | Loss: 0.00094637
Iteration 24/25 | Loss: 0.00094637
Iteration 25/25 | Loss: 0.00094637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094637
Iteration 2/1000 | Loss: 0.00002832
Iteration 3/1000 | Loss: 0.00002089
Iteration 4/1000 | Loss: 0.00001961
Iteration 5/1000 | Loss: 0.00001881
Iteration 6/1000 | Loss: 0.00001819
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001710
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001665
Iteration 12/1000 | Loss: 0.00001649
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001628
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001624
Iteration 20/1000 | Loss: 0.00001624
Iteration 21/1000 | Loss: 0.00001623
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001618
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001610
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001603
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001601
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001600
Iteration 58/1000 | Loss: 0.00001600
Iteration 59/1000 | Loss: 0.00001600
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001599
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001599
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001598
Iteration 70/1000 | Loss: 0.00001598
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001597
Iteration 73/1000 | Loss: 0.00001597
Iteration 74/1000 | Loss: 0.00001597
Iteration 75/1000 | Loss: 0.00001597
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001596
Iteration 78/1000 | Loss: 0.00001596
Iteration 79/1000 | Loss: 0.00001596
Iteration 80/1000 | Loss: 0.00001596
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001594
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001593
Iteration 93/1000 | Loss: 0.00001593
Iteration 94/1000 | Loss: 0.00001593
Iteration 95/1000 | Loss: 0.00001593
Iteration 96/1000 | Loss: 0.00001593
Iteration 97/1000 | Loss: 0.00001593
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001592
Iteration 100/1000 | Loss: 0.00001592
Iteration 101/1000 | Loss: 0.00001592
Iteration 102/1000 | Loss: 0.00001592
Iteration 103/1000 | Loss: 0.00001592
Iteration 104/1000 | Loss: 0.00001592
Iteration 105/1000 | Loss: 0.00001592
Iteration 106/1000 | Loss: 0.00001592
Iteration 107/1000 | Loss: 0.00001592
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001591
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001591
Iteration 113/1000 | Loss: 0.00001591
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001590
Iteration 119/1000 | Loss: 0.00001590
Iteration 120/1000 | Loss: 0.00001590
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001589
Iteration 123/1000 | Loss: 0.00001589
Iteration 124/1000 | Loss: 0.00001589
Iteration 125/1000 | Loss: 0.00001589
Iteration 126/1000 | Loss: 0.00001589
Iteration 127/1000 | Loss: 0.00001589
Iteration 128/1000 | Loss: 0.00001589
Iteration 129/1000 | Loss: 0.00001589
Iteration 130/1000 | Loss: 0.00001589
Iteration 131/1000 | Loss: 0.00001589
Iteration 132/1000 | Loss: 0.00001589
Iteration 133/1000 | Loss: 0.00001589
Iteration 134/1000 | Loss: 0.00001589
Iteration 135/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.5888608686509542e-05, 1.5888608686509542e-05, 1.5888608686509542e-05, 1.5888608686509542e-05, 1.5888608686509542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5888608686509542e-05

Optimization complete. Final v2v error: 3.3779335021972656 mm

Highest mean error: 3.704129695892334 mm for frame 83

Lowest mean error: 3.0850369930267334 mm for frame 10

Saving results

Total time: 38.41575002670288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786881
Iteration 2/25 | Loss: 0.00148447
Iteration 3/25 | Loss: 0.00127978
Iteration 4/25 | Loss: 0.00126502
Iteration 5/25 | Loss: 0.00125965
Iteration 6/25 | Loss: 0.00125806
Iteration 7/25 | Loss: 0.00125789
Iteration 8/25 | Loss: 0.00125789
Iteration 9/25 | Loss: 0.00125789
Iteration 10/25 | Loss: 0.00125789
Iteration 11/25 | Loss: 0.00125789
Iteration 12/25 | Loss: 0.00125789
Iteration 13/25 | Loss: 0.00125789
Iteration 14/25 | Loss: 0.00125789
Iteration 15/25 | Loss: 0.00125789
Iteration 16/25 | Loss: 0.00125789
Iteration 17/25 | Loss: 0.00125789
Iteration 18/25 | Loss: 0.00125789
Iteration 19/25 | Loss: 0.00125789
Iteration 20/25 | Loss: 0.00125789
Iteration 21/25 | Loss: 0.00125789
Iteration 22/25 | Loss: 0.00125789
Iteration 23/25 | Loss: 0.00125789
Iteration 24/25 | Loss: 0.00125789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012578893220052123, 0.0012578893220052123, 0.0012578893220052123, 0.0012578893220052123, 0.0012578893220052123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012578893220052123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21937132
Iteration 2/25 | Loss: 0.00089936
Iteration 3/25 | Loss: 0.00089936
Iteration 4/25 | Loss: 0.00089936
Iteration 5/25 | Loss: 0.00089936
Iteration 6/25 | Loss: 0.00089936
Iteration 7/25 | Loss: 0.00089936
Iteration 8/25 | Loss: 0.00089936
Iteration 9/25 | Loss: 0.00089936
Iteration 10/25 | Loss: 0.00089936
Iteration 11/25 | Loss: 0.00089936
Iteration 12/25 | Loss: 0.00089936
Iteration 13/25 | Loss: 0.00089936
Iteration 14/25 | Loss: 0.00089936
Iteration 15/25 | Loss: 0.00089936
Iteration 16/25 | Loss: 0.00089936
Iteration 17/25 | Loss: 0.00089936
Iteration 18/25 | Loss: 0.00089936
Iteration 19/25 | Loss: 0.00089936
Iteration 20/25 | Loss: 0.00089936
Iteration 21/25 | Loss: 0.00089936
Iteration 22/25 | Loss: 0.00089936
Iteration 23/25 | Loss: 0.00089936
Iteration 24/25 | Loss: 0.00089936
Iteration 25/25 | Loss: 0.00089936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089936
Iteration 2/1000 | Loss: 0.00005465
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00002457
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002025
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001727
Iteration 11/1000 | Loss: 0.00001699
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001643
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001640
Iteration 17/1000 | Loss: 0.00001639
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001633
Iteration 22/1000 | Loss: 0.00001628
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001612
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001595
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001594
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001586
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001583
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001572
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001571
Iteration 68/1000 | Loss: 0.00001571
Iteration 69/1000 | Loss: 0.00001571
Iteration 70/1000 | Loss: 0.00001571
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001570
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001570
Iteration 80/1000 | Loss: 0.00001569
Iteration 81/1000 | Loss: 0.00001569
Iteration 82/1000 | Loss: 0.00001569
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001569
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001569
Iteration 87/1000 | Loss: 0.00001569
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001569
Iteration 90/1000 | Loss: 0.00001569
Iteration 91/1000 | Loss: 0.00001569
Iteration 92/1000 | Loss: 0.00001569
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001568
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001568
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001566
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001565
Iteration 103/1000 | Loss: 0.00001565
Iteration 104/1000 | Loss: 0.00001565
Iteration 105/1000 | Loss: 0.00001565
Iteration 106/1000 | Loss: 0.00001565
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001564
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001564
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001563
Iteration 113/1000 | Loss: 0.00001563
Iteration 114/1000 | Loss: 0.00001563
Iteration 115/1000 | Loss: 0.00001563
Iteration 116/1000 | Loss: 0.00001563
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001562
Iteration 119/1000 | Loss: 0.00001562
Iteration 120/1000 | Loss: 0.00001562
Iteration 121/1000 | Loss: 0.00001562
Iteration 122/1000 | Loss: 0.00001562
Iteration 123/1000 | Loss: 0.00001562
Iteration 124/1000 | Loss: 0.00001561
Iteration 125/1000 | Loss: 0.00001561
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001556
Iteration 144/1000 | Loss: 0.00001556
Iteration 145/1000 | Loss: 0.00001556
Iteration 146/1000 | Loss: 0.00001556
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001556
Iteration 150/1000 | Loss: 0.00001556
Iteration 151/1000 | Loss: 0.00001555
Iteration 152/1000 | Loss: 0.00001555
Iteration 153/1000 | Loss: 0.00001555
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001555
Iteration 156/1000 | Loss: 0.00001555
Iteration 157/1000 | Loss: 0.00001554
Iteration 158/1000 | Loss: 0.00001554
Iteration 159/1000 | Loss: 0.00001554
Iteration 160/1000 | Loss: 0.00001554
Iteration 161/1000 | Loss: 0.00001554
Iteration 162/1000 | Loss: 0.00001554
Iteration 163/1000 | Loss: 0.00001554
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001554
Iteration 166/1000 | Loss: 0.00001554
Iteration 167/1000 | Loss: 0.00001554
Iteration 168/1000 | Loss: 0.00001554
Iteration 169/1000 | Loss: 0.00001554
Iteration 170/1000 | Loss: 0.00001554
Iteration 171/1000 | Loss: 0.00001554
Iteration 172/1000 | Loss: 0.00001554
Iteration 173/1000 | Loss: 0.00001554
Iteration 174/1000 | Loss: 0.00001554
Iteration 175/1000 | Loss: 0.00001554
Iteration 176/1000 | Loss: 0.00001554
Iteration 177/1000 | Loss: 0.00001554
Iteration 178/1000 | Loss: 0.00001554
Iteration 179/1000 | Loss: 0.00001554
Iteration 180/1000 | Loss: 0.00001554
Iteration 181/1000 | Loss: 0.00001554
Iteration 182/1000 | Loss: 0.00001554
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001554
Iteration 185/1000 | Loss: 0.00001554
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001554
Iteration 188/1000 | Loss: 0.00001554
Iteration 189/1000 | Loss: 0.00001554
Iteration 190/1000 | Loss: 0.00001554
Iteration 191/1000 | Loss: 0.00001554
Iteration 192/1000 | Loss: 0.00001554
Iteration 193/1000 | Loss: 0.00001554
Iteration 194/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.553764559503179e-05, 1.553764559503179e-05, 1.553764559503179e-05, 1.553764559503179e-05, 1.553764559503179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.553764559503179e-05

Optimization complete. Final v2v error: 3.2827227115631104 mm

Highest mean error: 4.508586406707764 mm for frame 69

Lowest mean error: 2.7985293865203857 mm for frame 103

Saving results

Total time: 48.009507179260254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992638
Iteration 2/25 | Loss: 0.00214765
Iteration 3/25 | Loss: 0.00168390
Iteration 4/25 | Loss: 0.00159747
Iteration 5/25 | Loss: 0.00172082
Iteration 6/25 | Loss: 0.00165163
Iteration 7/25 | Loss: 0.00149923
Iteration 8/25 | Loss: 0.00144043
Iteration 9/25 | Loss: 0.00137776
Iteration 10/25 | Loss: 0.00133714
Iteration 11/25 | Loss: 0.00132066
Iteration 12/25 | Loss: 0.00131407
Iteration 13/25 | Loss: 0.00130901
Iteration 14/25 | Loss: 0.00130569
Iteration 15/25 | Loss: 0.00130333
Iteration 16/25 | Loss: 0.00130124
Iteration 17/25 | Loss: 0.00129950
Iteration 18/25 | Loss: 0.00129701
Iteration 19/25 | Loss: 0.00129486
Iteration 20/25 | Loss: 0.00129406
Iteration 21/25 | Loss: 0.00129378
Iteration 22/25 | Loss: 0.00129360
Iteration 23/25 | Loss: 0.00129254
Iteration 24/25 | Loss: 0.00129212
Iteration 25/25 | Loss: 0.00129192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48225331
Iteration 2/25 | Loss: 0.00071102
Iteration 3/25 | Loss: 0.00071102
Iteration 4/25 | Loss: 0.00071102
Iteration 5/25 | Loss: 0.00071101
Iteration 6/25 | Loss: 0.00071101
Iteration 7/25 | Loss: 0.00071101
Iteration 8/25 | Loss: 0.00071101
Iteration 9/25 | Loss: 0.00071101
Iteration 10/25 | Loss: 0.00071101
Iteration 11/25 | Loss: 0.00071101
Iteration 12/25 | Loss: 0.00071101
Iteration 13/25 | Loss: 0.00071101
Iteration 14/25 | Loss: 0.00071101
Iteration 15/25 | Loss: 0.00071101
Iteration 16/25 | Loss: 0.00071101
Iteration 17/25 | Loss: 0.00071101
Iteration 18/25 | Loss: 0.00071101
Iteration 19/25 | Loss: 0.00071101
Iteration 20/25 | Loss: 0.00071101
Iteration 21/25 | Loss: 0.00071101
Iteration 22/25 | Loss: 0.00071101
Iteration 23/25 | Loss: 0.00071101
Iteration 24/25 | Loss: 0.00071101
Iteration 25/25 | Loss: 0.00071101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071101
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00002395
Iteration 4/1000 | Loss: 0.00002356
Iteration 5/1000 | Loss: 0.00002140
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001981
Iteration 9/1000 | Loss: 0.00001953
Iteration 10/1000 | Loss: 0.00001929
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001910
Iteration 13/1000 | Loss: 0.00001907
Iteration 14/1000 | Loss: 0.00001905
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001898
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00019597
Iteration 19/1000 | Loss: 0.00012917
Iteration 20/1000 | Loss: 0.00017127
Iteration 21/1000 | Loss: 0.00012277
Iteration 22/1000 | Loss: 0.00016305
Iteration 23/1000 | Loss: 0.00012658
Iteration 24/1000 | Loss: 0.00001901
Iteration 25/1000 | Loss: 0.00012615
Iteration 26/1000 | Loss: 0.00010916
Iteration 27/1000 | Loss: 0.00001935
Iteration 28/1000 | Loss: 0.00012640
Iteration 29/1000 | Loss: 0.00002603
Iteration 30/1000 | Loss: 0.00010922
Iteration 31/1000 | Loss: 0.00012346
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00013623
Iteration 34/1000 | Loss: 0.00004053
Iteration 35/1000 | Loss: 0.00009447
Iteration 36/1000 | Loss: 0.00004321
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00018542
Iteration 40/1000 | Loss: 0.00002089
Iteration 41/1000 | Loss: 0.00014281
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00014762
Iteration 44/1000 | Loss: 0.00003480
Iteration 45/1000 | Loss: 0.00010412
Iteration 46/1000 | Loss: 0.00003287
Iteration 47/1000 | Loss: 0.00007510
Iteration 48/1000 | Loss: 0.00002703
Iteration 49/1000 | Loss: 0.00009233
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001911
Iteration 52/1000 | Loss: 0.00001872
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001826
Iteration 56/1000 | Loss: 0.00002059
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001755
Iteration 61/1000 | Loss: 0.00001751
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001749
Iteration 64/1000 | Loss: 0.00001749
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001743
Iteration 79/1000 | Loss: 0.00001743
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001740
Iteration 90/1000 | Loss: 0.00001739
Iteration 91/1000 | Loss: 0.00001739
Iteration 92/1000 | Loss: 0.00001739
Iteration 93/1000 | Loss: 0.00001739
Iteration 94/1000 | Loss: 0.00001739
Iteration 95/1000 | Loss: 0.00001739
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00001738
Iteration 98/1000 | Loss: 0.00001738
Iteration 99/1000 | Loss: 0.00001738
Iteration 100/1000 | Loss: 0.00001738
Iteration 101/1000 | Loss: 0.00001737
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001736
Iteration 106/1000 | Loss: 0.00001736
Iteration 107/1000 | Loss: 0.00001736
Iteration 108/1000 | Loss: 0.00001736
Iteration 109/1000 | Loss: 0.00001736
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001736
Iteration 114/1000 | Loss: 0.00001736
Iteration 115/1000 | Loss: 0.00001736
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001735
Iteration 118/1000 | Loss: 0.00001735
Iteration 119/1000 | Loss: 0.00001735
Iteration 120/1000 | Loss: 0.00001735
Iteration 121/1000 | Loss: 0.00001734
Iteration 122/1000 | Loss: 0.00001734
Iteration 123/1000 | Loss: 0.00001734
Iteration 124/1000 | Loss: 0.00001734
Iteration 125/1000 | Loss: 0.00001734
Iteration 126/1000 | Loss: 0.00001734
Iteration 127/1000 | Loss: 0.00001733
Iteration 128/1000 | Loss: 0.00001733
Iteration 129/1000 | Loss: 0.00001733
Iteration 130/1000 | Loss: 0.00001733
Iteration 131/1000 | Loss: 0.00001733
Iteration 132/1000 | Loss: 0.00001733
Iteration 133/1000 | Loss: 0.00001733
Iteration 134/1000 | Loss: 0.00001733
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001732
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001732
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Iteration 145/1000 | Loss: 0.00001732
Iteration 146/1000 | Loss: 0.00001732
Iteration 147/1000 | Loss: 0.00001732
Iteration 148/1000 | Loss: 0.00001732
Iteration 149/1000 | Loss: 0.00001732
Iteration 150/1000 | Loss: 0.00001732
Iteration 151/1000 | Loss: 0.00001732
Iteration 152/1000 | Loss: 0.00001732
Iteration 153/1000 | Loss: 0.00001732
Iteration 154/1000 | Loss: 0.00001732
Iteration 155/1000 | Loss: 0.00001732
Iteration 156/1000 | Loss: 0.00001732
Iteration 157/1000 | Loss: 0.00001731
Iteration 158/1000 | Loss: 0.00001731
Iteration 159/1000 | Loss: 0.00001731
Iteration 160/1000 | Loss: 0.00001731
Iteration 161/1000 | Loss: 0.00001731
Iteration 162/1000 | Loss: 0.00001731
Iteration 163/1000 | Loss: 0.00001731
Iteration 164/1000 | Loss: 0.00001731
Iteration 165/1000 | Loss: 0.00001731
Iteration 166/1000 | Loss: 0.00001731
Iteration 167/1000 | Loss: 0.00001731
Iteration 168/1000 | Loss: 0.00001731
Iteration 169/1000 | Loss: 0.00001731
Iteration 170/1000 | Loss: 0.00001731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.7314820070168935e-05, 1.7314820070168935e-05, 1.7314820070168935e-05, 1.7314820070168935e-05, 1.7314820070168935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7314820070168935e-05

Optimization complete. Final v2v error: 3.4934310913085938 mm

Highest mean error: 3.982529878616333 mm for frame 118

Lowest mean error: 3.2617814540863037 mm for frame 44

Saving results

Total time: 133.29051303863525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00571554
Iteration 2/25 | Loss: 0.00153884
Iteration 3/25 | Loss: 0.00135581
Iteration 4/25 | Loss: 0.00127347
Iteration 5/25 | Loss: 0.00126906
Iteration 6/25 | Loss: 0.00126735
Iteration 7/25 | Loss: 0.00126722
Iteration 8/25 | Loss: 0.00126499
Iteration 9/25 | Loss: 0.00125987
Iteration 10/25 | Loss: 0.00125764
Iteration 11/25 | Loss: 0.00125737
Iteration 12/25 | Loss: 0.00125729
Iteration 13/25 | Loss: 0.00125729
Iteration 14/25 | Loss: 0.00125729
Iteration 15/25 | Loss: 0.00125729
Iteration 16/25 | Loss: 0.00125729
Iteration 17/25 | Loss: 0.00125728
Iteration 18/25 | Loss: 0.00125728
Iteration 19/25 | Loss: 0.00125728
Iteration 20/25 | Loss: 0.00125728
Iteration 21/25 | Loss: 0.00125727
Iteration 22/25 | Loss: 0.00125727
Iteration 23/25 | Loss: 0.00125727
Iteration 24/25 | Loss: 0.00125727
Iteration 25/25 | Loss: 0.00125727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.23664522
Iteration 2/25 | Loss: 0.00081640
Iteration 3/25 | Loss: 0.00081639
Iteration 4/25 | Loss: 0.00081639
Iteration 5/25 | Loss: 0.00081639
Iteration 6/25 | Loss: 0.00081639
Iteration 7/25 | Loss: 0.00081639
Iteration 8/25 | Loss: 0.00081639
Iteration 9/25 | Loss: 0.00081639
Iteration 10/25 | Loss: 0.00081639
Iteration 11/25 | Loss: 0.00081639
Iteration 12/25 | Loss: 0.00081639
Iteration 13/25 | Loss: 0.00081639
Iteration 14/25 | Loss: 0.00081639
Iteration 15/25 | Loss: 0.00081639
Iteration 16/25 | Loss: 0.00081639
Iteration 17/25 | Loss: 0.00081639
Iteration 18/25 | Loss: 0.00081639
Iteration 19/25 | Loss: 0.00081639
Iteration 20/25 | Loss: 0.00081639
Iteration 21/25 | Loss: 0.00081639
Iteration 22/25 | Loss: 0.00081639
Iteration 23/25 | Loss: 0.00081639
Iteration 24/25 | Loss: 0.00081639
Iteration 25/25 | Loss: 0.00081639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081639
Iteration 2/1000 | Loss: 0.00006134
Iteration 3/1000 | Loss: 0.00002036
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00004991
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00002237
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001535
Iteration 12/1000 | Loss: 0.00001503
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001483
Iteration 17/1000 | Loss: 0.00001479
Iteration 18/1000 | Loss: 0.00001478
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00005166
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001452
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001452
Iteration 42/1000 | Loss: 0.00001452
Iteration 43/1000 | Loss: 0.00001452
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001451
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.4514284885080997e-05, 1.4514284885080997e-05, 1.4514284885080997e-05, 1.4514284885080997e-05, 1.4514284885080997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4514284885080997e-05

Optimization complete. Final v2v error: 3.2391555309295654 mm

Highest mean error: 3.9529433250427246 mm for frame 171

Lowest mean error: 2.9354007244110107 mm for frame 188

Saving results

Total time: 50.475186824798584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987014
Iteration 2/25 | Loss: 0.00987014
Iteration 3/25 | Loss: 0.00245480
Iteration 4/25 | Loss: 0.00189231
Iteration 5/25 | Loss: 0.00182378
Iteration 6/25 | Loss: 0.00179982
Iteration 7/25 | Loss: 0.00178359
Iteration 8/25 | Loss: 0.00176351
Iteration 9/25 | Loss: 0.00173926
Iteration 10/25 | Loss: 0.00172555
Iteration 11/25 | Loss: 0.00170888
Iteration 12/25 | Loss: 0.00170437
Iteration 13/25 | Loss: 0.00168941
Iteration 14/25 | Loss: 0.00167602
Iteration 15/25 | Loss: 0.00167306
Iteration 16/25 | Loss: 0.00167265
Iteration 17/25 | Loss: 0.00167252
Iteration 18/25 | Loss: 0.00167252
Iteration 19/25 | Loss: 0.00167252
Iteration 20/25 | Loss: 0.00167252
Iteration 21/25 | Loss: 0.00167252
Iteration 22/25 | Loss: 0.00167252
Iteration 23/25 | Loss: 0.00167252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016725176246836782, 0.0016725176246836782, 0.0016725176246836782, 0.0016725176246836782, 0.0016725176246836782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016725176246836782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39193964
Iteration 2/25 | Loss: 0.00315465
Iteration 3/25 | Loss: 0.00306231
Iteration 4/25 | Loss: 0.00306230
Iteration 5/25 | Loss: 0.00306230
Iteration 6/25 | Loss: 0.00306230
Iteration 7/25 | Loss: 0.00306230
Iteration 8/25 | Loss: 0.00306230
Iteration 9/25 | Loss: 0.00306230
Iteration 10/25 | Loss: 0.00306230
Iteration 11/25 | Loss: 0.00306230
Iteration 12/25 | Loss: 0.00306230
Iteration 13/25 | Loss: 0.00306230
Iteration 14/25 | Loss: 0.00306230
Iteration 15/25 | Loss: 0.00306230
Iteration 16/25 | Loss: 0.00306230
Iteration 17/25 | Loss: 0.00306230
Iteration 18/25 | Loss: 0.00306230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003062302013859153, 0.003062302013859153, 0.003062302013859153, 0.003062302013859153, 0.003062302013859153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003062302013859153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306230
Iteration 2/1000 | Loss: 0.00051591
Iteration 3/1000 | Loss: 0.00032928
Iteration 4/1000 | Loss: 0.00040969
Iteration 5/1000 | Loss: 0.00048883
Iteration 6/1000 | Loss: 0.00037199
Iteration 7/1000 | Loss: 0.00063853
Iteration 8/1000 | Loss: 0.00029735
Iteration 9/1000 | Loss: 0.00037356
Iteration 10/1000 | Loss: 0.00190808
Iteration 11/1000 | Loss: 0.00070986
Iteration 12/1000 | Loss: 0.00174877
Iteration 13/1000 | Loss: 0.00223109
Iteration 14/1000 | Loss: 0.00121416
Iteration 15/1000 | Loss: 0.00025897
Iteration 16/1000 | Loss: 0.00029530
Iteration 17/1000 | Loss: 0.00044582
Iteration 18/1000 | Loss: 0.00020701
Iteration 19/1000 | Loss: 0.00019704
Iteration 20/1000 | Loss: 0.00019227
Iteration 21/1000 | Loss: 0.00020644
Iteration 22/1000 | Loss: 0.00096882
Iteration 23/1000 | Loss: 0.00263830
Iteration 24/1000 | Loss: 0.00983179
Iteration 25/1000 | Loss: 0.00118823
Iteration 26/1000 | Loss: 0.00044442
Iteration 27/1000 | Loss: 0.00033635
Iteration 28/1000 | Loss: 0.00017894
Iteration 29/1000 | Loss: 0.00023596
Iteration 30/1000 | Loss: 0.00014448
Iteration 31/1000 | Loss: 0.00018079
Iteration 32/1000 | Loss: 0.00005515
Iteration 33/1000 | Loss: 0.00010998
Iteration 34/1000 | Loss: 0.00003833
Iteration 35/1000 | Loss: 0.00005899
Iteration 36/1000 | Loss: 0.00002958
Iteration 37/1000 | Loss: 0.00003027
Iteration 38/1000 | Loss: 0.00002353
Iteration 39/1000 | Loss: 0.00002610
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001434
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001430
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001423
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001415
Iteration 72/1000 | Loss: 0.00001415
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001410
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001407
Iteration 98/1000 | Loss: 0.00001407
Iteration 99/1000 | Loss: 0.00001407
Iteration 100/1000 | Loss: 0.00001407
Iteration 101/1000 | Loss: 0.00001407
Iteration 102/1000 | Loss: 0.00001406
Iteration 103/1000 | Loss: 0.00001406
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001406
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.4019544323673472e-05, 1.4019544323673472e-05, 1.4019544323673472e-05, 1.4019544323673472e-05, 1.4019544323673472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4019544323673472e-05

Optimization complete. Final v2v error: 3.226363182067871 mm

Highest mean error: 3.415177583694458 mm for frame 129

Lowest mean error: 3.075024366378784 mm for frame 183

Saving results

Total time: 119.52660918235779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488043
Iteration 2/25 | Loss: 0.00152961
Iteration 3/25 | Loss: 0.00131688
Iteration 4/25 | Loss: 0.00129123
Iteration 5/25 | Loss: 0.00128828
Iteration 6/25 | Loss: 0.00128762
Iteration 7/25 | Loss: 0.00128762
Iteration 8/25 | Loss: 0.00128762
Iteration 9/25 | Loss: 0.00128762
Iteration 10/25 | Loss: 0.00128762
Iteration 11/25 | Loss: 0.00128762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012876209802925587, 0.0012876209802925587, 0.0012876209802925587, 0.0012876209802925587, 0.0012876209802925587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012876209802925587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44688964
Iteration 2/25 | Loss: 0.00083448
Iteration 3/25 | Loss: 0.00083447
Iteration 4/25 | Loss: 0.00083447
Iteration 5/25 | Loss: 0.00083447
Iteration 6/25 | Loss: 0.00083447
Iteration 7/25 | Loss: 0.00083447
Iteration 8/25 | Loss: 0.00083447
Iteration 9/25 | Loss: 0.00083447
Iteration 10/25 | Loss: 0.00083447
Iteration 11/25 | Loss: 0.00083447
Iteration 12/25 | Loss: 0.00083447
Iteration 13/25 | Loss: 0.00083447
Iteration 14/25 | Loss: 0.00083447
Iteration 15/25 | Loss: 0.00083447
Iteration 16/25 | Loss: 0.00083447
Iteration 17/25 | Loss: 0.00083447
Iteration 18/25 | Loss: 0.00083447
Iteration 19/25 | Loss: 0.00083447
Iteration 20/25 | Loss: 0.00083447
Iteration 21/25 | Loss: 0.00083447
Iteration 22/25 | Loss: 0.00083447
Iteration 23/25 | Loss: 0.00083447
Iteration 24/25 | Loss: 0.00083447
Iteration 25/25 | Loss: 0.00083447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083447
Iteration 2/1000 | Loss: 0.00003691
Iteration 3/1000 | Loss: 0.00002115
Iteration 4/1000 | Loss: 0.00001874
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001573
Iteration 10/1000 | Loss: 0.00001551
Iteration 11/1000 | Loss: 0.00001548
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001535
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001521
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001519
Iteration 22/1000 | Loss: 0.00001519
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00001511
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001510
Iteration 36/1000 | Loss: 0.00001510
Iteration 37/1000 | Loss: 0.00001510
Iteration 38/1000 | Loss: 0.00001510
Iteration 39/1000 | Loss: 0.00001510
Iteration 40/1000 | Loss: 0.00001510
Iteration 41/1000 | Loss: 0.00001509
Iteration 42/1000 | Loss: 0.00001509
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001509
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001501
Iteration 48/1000 | Loss: 0.00001501
Iteration 49/1000 | Loss: 0.00001500
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001495
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001491
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001490
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001489
Iteration 77/1000 | Loss: 0.00001489
Iteration 78/1000 | Loss: 0.00001489
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001488
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001484
Iteration 97/1000 | Loss: 0.00001484
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001480
Iteration 118/1000 | Loss: 0.00001480
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001479
Iteration 124/1000 | Loss: 0.00001479
Iteration 125/1000 | Loss: 0.00001479
Iteration 126/1000 | Loss: 0.00001479
Iteration 127/1000 | Loss: 0.00001479
Iteration 128/1000 | Loss: 0.00001479
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001478
Iteration 131/1000 | Loss: 0.00001478
Iteration 132/1000 | Loss: 0.00001478
Iteration 133/1000 | Loss: 0.00001478
Iteration 134/1000 | Loss: 0.00001477
Iteration 135/1000 | Loss: 0.00001477
Iteration 136/1000 | Loss: 0.00001477
Iteration 137/1000 | Loss: 0.00001477
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001476
Iteration 141/1000 | Loss: 0.00001476
Iteration 142/1000 | Loss: 0.00001476
Iteration 143/1000 | Loss: 0.00001476
Iteration 144/1000 | Loss: 0.00001475
Iteration 145/1000 | Loss: 0.00001475
Iteration 146/1000 | Loss: 0.00001475
Iteration 147/1000 | Loss: 0.00001475
Iteration 148/1000 | Loss: 0.00001475
Iteration 149/1000 | Loss: 0.00001475
Iteration 150/1000 | Loss: 0.00001475
Iteration 151/1000 | Loss: 0.00001475
Iteration 152/1000 | Loss: 0.00001474
Iteration 153/1000 | Loss: 0.00001474
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001474
Iteration 156/1000 | Loss: 0.00001474
Iteration 157/1000 | Loss: 0.00001474
Iteration 158/1000 | Loss: 0.00001474
Iteration 159/1000 | Loss: 0.00001474
Iteration 160/1000 | Loss: 0.00001474
Iteration 161/1000 | Loss: 0.00001474
Iteration 162/1000 | Loss: 0.00001474
Iteration 163/1000 | Loss: 0.00001474
Iteration 164/1000 | Loss: 0.00001474
Iteration 165/1000 | Loss: 0.00001474
Iteration 166/1000 | Loss: 0.00001473
Iteration 167/1000 | Loss: 0.00001473
Iteration 168/1000 | Loss: 0.00001473
Iteration 169/1000 | Loss: 0.00001473
Iteration 170/1000 | Loss: 0.00001473
Iteration 171/1000 | Loss: 0.00001473
Iteration 172/1000 | Loss: 0.00001473
Iteration 173/1000 | Loss: 0.00001473
Iteration 174/1000 | Loss: 0.00001473
Iteration 175/1000 | Loss: 0.00001473
Iteration 176/1000 | Loss: 0.00001472
Iteration 177/1000 | Loss: 0.00001472
Iteration 178/1000 | Loss: 0.00001472
Iteration 179/1000 | Loss: 0.00001472
Iteration 180/1000 | Loss: 0.00001472
Iteration 181/1000 | Loss: 0.00001472
Iteration 182/1000 | Loss: 0.00001472
Iteration 183/1000 | Loss: 0.00001472
Iteration 184/1000 | Loss: 0.00001472
Iteration 185/1000 | Loss: 0.00001472
Iteration 186/1000 | Loss: 0.00001472
Iteration 187/1000 | Loss: 0.00001472
Iteration 188/1000 | Loss: 0.00001472
Iteration 189/1000 | Loss: 0.00001472
Iteration 190/1000 | Loss: 0.00001472
Iteration 191/1000 | Loss: 0.00001472
Iteration 192/1000 | Loss: 0.00001472
Iteration 193/1000 | Loss: 0.00001472
Iteration 194/1000 | Loss: 0.00001472
Iteration 195/1000 | Loss: 0.00001472
Iteration 196/1000 | Loss: 0.00001472
Iteration 197/1000 | Loss: 0.00001472
Iteration 198/1000 | Loss: 0.00001472
Iteration 199/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4722068954142742e-05, 1.4722068954142742e-05, 1.4722068954142742e-05, 1.4722068954142742e-05, 1.4722068954142742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4722068954142742e-05

Optimization complete. Final v2v error: 3.1555094718933105 mm

Highest mean error: 4.473211765289307 mm for frame 81

Lowest mean error: 2.757070302963257 mm for frame 158

Saving results

Total time: 42.72997045516968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767909
Iteration 2/25 | Loss: 0.00142997
Iteration 3/25 | Loss: 0.00129651
Iteration 4/25 | Loss: 0.00126917
Iteration 5/25 | Loss: 0.00127970
Iteration 6/25 | Loss: 0.00126749
Iteration 7/25 | Loss: 0.00124601
Iteration 8/25 | Loss: 0.00123813
Iteration 9/25 | Loss: 0.00123687
Iteration 10/25 | Loss: 0.00123719
Iteration 11/25 | Loss: 0.00123609
Iteration 12/25 | Loss: 0.00123584
Iteration 13/25 | Loss: 0.00123388
Iteration 14/25 | Loss: 0.00123344
Iteration 15/25 | Loss: 0.00123332
Iteration 16/25 | Loss: 0.00123332
Iteration 17/25 | Loss: 0.00123332
Iteration 18/25 | Loss: 0.00123331
Iteration 19/25 | Loss: 0.00123331
Iteration 20/25 | Loss: 0.00123331
Iteration 21/25 | Loss: 0.00123331
Iteration 22/25 | Loss: 0.00123331
Iteration 23/25 | Loss: 0.00123331
Iteration 24/25 | Loss: 0.00123331
Iteration 25/25 | Loss: 0.00123331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41646469
Iteration 2/25 | Loss: 0.00064216
Iteration 3/25 | Loss: 0.00064214
Iteration 4/25 | Loss: 0.00064214
Iteration 5/25 | Loss: 0.00064214
Iteration 6/25 | Loss: 0.00064214
Iteration 7/25 | Loss: 0.00064214
Iteration 8/25 | Loss: 0.00064214
Iteration 9/25 | Loss: 0.00064214
Iteration 10/25 | Loss: 0.00064214
Iteration 11/25 | Loss: 0.00064214
Iteration 12/25 | Loss: 0.00064214
Iteration 13/25 | Loss: 0.00064213
Iteration 14/25 | Loss: 0.00064213
Iteration 15/25 | Loss: 0.00064213
Iteration 16/25 | Loss: 0.00064213
Iteration 17/25 | Loss: 0.00064213
Iteration 18/25 | Loss: 0.00064213
Iteration 19/25 | Loss: 0.00064213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006421348662115633, 0.0006421348662115633, 0.0006421348662115633, 0.0006421348662115633, 0.0006421348662115633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006421348662115633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064213
Iteration 2/1000 | Loss: 0.00004948
Iteration 3/1000 | Loss: 0.00003505
Iteration 4/1000 | Loss: 0.00003041
Iteration 5/1000 | Loss: 0.00002931
Iteration 6/1000 | Loss: 0.00002805
Iteration 7/1000 | Loss: 0.00002735
Iteration 8/1000 | Loss: 0.00002672
Iteration 9/1000 | Loss: 0.00002625
Iteration 10/1000 | Loss: 0.00002592
Iteration 11/1000 | Loss: 0.00002562
Iteration 12/1000 | Loss: 0.00002540
Iteration 13/1000 | Loss: 0.00002521
Iteration 14/1000 | Loss: 0.00007850
Iteration 15/1000 | Loss: 0.00006758
Iteration 16/1000 | Loss: 0.00002671
Iteration 17/1000 | Loss: 0.00002497
Iteration 18/1000 | Loss: 0.00002482
Iteration 19/1000 | Loss: 0.00002479
Iteration 20/1000 | Loss: 0.00002479
Iteration 21/1000 | Loss: 0.00002477
Iteration 22/1000 | Loss: 0.00002476
Iteration 23/1000 | Loss: 0.00002475
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002471
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002470
Iteration 28/1000 | Loss: 0.00002470
Iteration 29/1000 | Loss: 0.00002470
Iteration 30/1000 | Loss: 0.00002469
Iteration 31/1000 | Loss: 0.00002469
Iteration 32/1000 | Loss: 0.00002469
Iteration 33/1000 | Loss: 0.00002468
Iteration 34/1000 | Loss: 0.00002468
Iteration 35/1000 | Loss: 0.00002468
Iteration 36/1000 | Loss: 0.00002467
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002466
Iteration 39/1000 | Loss: 0.00002464
Iteration 40/1000 | Loss: 0.00002463
Iteration 41/1000 | Loss: 0.00002462
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002460
Iteration 46/1000 | Loss: 0.00002460
Iteration 47/1000 | Loss: 0.00002460
Iteration 48/1000 | Loss: 0.00002459
Iteration 49/1000 | Loss: 0.00002458
Iteration 50/1000 | Loss: 0.00002458
Iteration 51/1000 | Loss: 0.00002458
Iteration 52/1000 | Loss: 0.00002457
Iteration 53/1000 | Loss: 0.00002457
Iteration 54/1000 | Loss: 0.00002457
Iteration 55/1000 | Loss: 0.00002456
Iteration 56/1000 | Loss: 0.00002455
Iteration 57/1000 | Loss: 0.00002455
Iteration 58/1000 | Loss: 0.00002454
Iteration 59/1000 | Loss: 0.00007793
Iteration 60/1000 | Loss: 0.00006199
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00007156
Iteration 63/1000 | Loss: 0.00006684
Iteration 64/1000 | Loss: 0.00002461
Iteration 65/1000 | Loss: 0.00002455
Iteration 66/1000 | Loss: 0.00002453
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002452
Iteration 69/1000 | Loss: 0.00002452
Iteration 70/1000 | Loss: 0.00002452
Iteration 71/1000 | Loss: 0.00002451
Iteration 72/1000 | Loss: 0.00002451
Iteration 73/1000 | Loss: 0.00002451
Iteration 74/1000 | Loss: 0.00002451
Iteration 75/1000 | Loss: 0.00002450
Iteration 76/1000 | Loss: 0.00002450
Iteration 77/1000 | Loss: 0.00002450
Iteration 78/1000 | Loss: 0.00002450
Iteration 79/1000 | Loss: 0.00002450
Iteration 80/1000 | Loss: 0.00002449
Iteration 81/1000 | Loss: 0.00002449
Iteration 82/1000 | Loss: 0.00002449
Iteration 83/1000 | Loss: 0.00002449
Iteration 84/1000 | Loss: 0.00002448
Iteration 85/1000 | Loss: 0.00002448
Iteration 86/1000 | Loss: 0.00002448
Iteration 87/1000 | Loss: 0.00002447
Iteration 88/1000 | Loss: 0.00002447
Iteration 89/1000 | Loss: 0.00002447
Iteration 90/1000 | Loss: 0.00002447
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002446
Iteration 93/1000 | Loss: 0.00002446
Iteration 94/1000 | Loss: 0.00002446
Iteration 95/1000 | Loss: 0.00002446
Iteration 96/1000 | Loss: 0.00002446
Iteration 97/1000 | Loss: 0.00002446
Iteration 98/1000 | Loss: 0.00002446
Iteration 99/1000 | Loss: 0.00002446
Iteration 100/1000 | Loss: 0.00002446
Iteration 101/1000 | Loss: 0.00002446
Iteration 102/1000 | Loss: 0.00002446
Iteration 103/1000 | Loss: 0.00002446
Iteration 104/1000 | Loss: 0.00002446
Iteration 105/1000 | Loss: 0.00002446
Iteration 106/1000 | Loss: 0.00002446
Iteration 107/1000 | Loss: 0.00002446
Iteration 108/1000 | Loss: 0.00002446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.445840618747752e-05, 2.445840618747752e-05, 2.445840618747752e-05, 2.445840618747752e-05, 2.445840618747752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.445840618747752e-05

Optimization complete. Final v2v error: 4.1346821784973145 mm

Highest mean error: 6.167513847351074 mm for frame 26

Lowest mean error: 3.4113876819610596 mm for frame 110

Saving results

Total time: 78.12156128883362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920558
Iteration 2/25 | Loss: 0.00165212
Iteration 3/25 | Loss: 0.00154437
Iteration 4/25 | Loss: 0.00137134
Iteration 5/25 | Loss: 0.00134898
Iteration 6/25 | Loss: 0.00134570
Iteration 7/25 | Loss: 0.00134070
Iteration 8/25 | Loss: 0.00133221
Iteration 9/25 | Loss: 0.00132628
Iteration 10/25 | Loss: 0.00132401
Iteration 11/25 | Loss: 0.00132435
Iteration 12/25 | Loss: 0.00132029
Iteration 13/25 | Loss: 0.00132307
Iteration 14/25 | Loss: 0.00132002
Iteration 15/25 | Loss: 0.00131889
Iteration 16/25 | Loss: 0.00131880
Iteration 17/25 | Loss: 0.00131880
Iteration 18/25 | Loss: 0.00131880
Iteration 19/25 | Loss: 0.00131880
Iteration 20/25 | Loss: 0.00131880
Iteration 21/25 | Loss: 0.00131880
Iteration 22/25 | Loss: 0.00131880
Iteration 23/25 | Loss: 0.00131880
Iteration 24/25 | Loss: 0.00131879
Iteration 25/25 | Loss: 0.00131879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54391730
Iteration 2/25 | Loss: 0.00096574
Iteration 3/25 | Loss: 0.00085794
Iteration 4/25 | Loss: 0.00085794
Iteration 5/25 | Loss: 0.00085794
Iteration 6/25 | Loss: 0.00085794
Iteration 7/25 | Loss: 0.00085794
Iteration 8/25 | Loss: 0.00085794
Iteration 9/25 | Loss: 0.00085794
Iteration 10/25 | Loss: 0.00085794
Iteration 11/25 | Loss: 0.00085794
Iteration 12/25 | Loss: 0.00085794
Iteration 13/25 | Loss: 0.00085794
Iteration 14/25 | Loss: 0.00085794
Iteration 15/25 | Loss: 0.00085794
Iteration 16/25 | Loss: 0.00085794
Iteration 17/25 | Loss: 0.00085794
Iteration 18/25 | Loss: 0.00085794
Iteration 19/25 | Loss: 0.00085794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008579398272559047, 0.0008579398272559047, 0.0008579398272559047, 0.0008579398272559047, 0.0008579398272559047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008579398272559047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085794
Iteration 2/1000 | Loss: 0.00013172
Iteration 3/1000 | Loss: 0.00017748
Iteration 4/1000 | Loss: 0.00002675
Iteration 5/1000 | Loss: 0.00002521
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00003899
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002195
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002112
Iteration 13/1000 | Loss: 0.00004312
Iteration 14/1000 | Loss: 0.00002076
Iteration 15/1000 | Loss: 0.00045958
Iteration 16/1000 | Loss: 0.00002956
Iteration 17/1000 | Loss: 0.00002679
Iteration 18/1000 | Loss: 0.00004990
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00005007
Iteration 21/1000 | Loss: 0.00001988
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00002596
Iteration 24/1000 | Loss: 0.00002090
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00002123
Iteration 31/1000 | Loss: 0.00002783
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00002871
Iteration 35/1000 | Loss: 0.00002147
Iteration 36/1000 | Loss: 0.00001917
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001838
Iteration 46/1000 | Loss: 0.00001838
Iteration 47/1000 | Loss: 0.00001838
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001836
Iteration 65/1000 | Loss: 0.00001836
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001834
Iteration 72/1000 | Loss: 0.00001834
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001833
Iteration 81/1000 | Loss: 0.00001833
Iteration 82/1000 | Loss: 0.00001833
Iteration 83/1000 | Loss: 0.00001833
Iteration 84/1000 | Loss: 0.00001833
Iteration 85/1000 | Loss: 0.00001833
Iteration 86/1000 | Loss: 0.00001833
Iteration 87/1000 | Loss: 0.00001833
Iteration 88/1000 | Loss: 0.00001833
Iteration 89/1000 | Loss: 0.00001833
Iteration 90/1000 | Loss: 0.00001833
Iteration 91/1000 | Loss: 0.00001833
Iteration 92/1000 | Loss: 0.00001833
Iteration 93/1000 | Loss: 0.00001833
Iteration 94/1000 | Loss: 0.00001833
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.833229725889396e-05, 1.833229725889396e-05, 1.833229725889396e-05, 1.833229725889396e-05, 1.833229725889396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.833229725889396e-05

Optimization complete. Final v2v error: 3.6103744506835938 mm

Highest mean error: 4.641825199127197 mm for frame 178

Lowest mean error: 3.0072243213653564 mm for frame 200

Saving results

Total time: 87.02779388427734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736356
Iteration 2/25 | Loss: 0.00206104
Iteration 3/25 | Loss: 0.00177536
Iteration 4/25 | Loss: 0.00167090
Iteration 5/25 | Loss: 0.00165058
Iteration 6/25 | Loss: 0.00156470
Iteration 7/25 | Loss: 0.00155044
Iteration 8/25 | Loss: 0.00154751
Iteration 9/25 | Loss: 0.00154701
Iteration 10/25 | Loss: 0.00154663
Iteration 11/25 | Loss: 0.00154655
Iteration 12/25 | Loss: 0.00154655
Iteration 13/25 | Loss: 0.00154655
Iteration 14/25 | Loss: 0.00154655
Iteration 15/25 | Loss: 0.00154655
Iteration 16/25 | Loss: 0.00154655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015465505421161652, 0.0015465505421161652, 0.0015465505421161652, 0.0015465505421161652, 0.0015465505421161652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015465505421161652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42241740
Iteration 2/25 | Loss: 0.00115022
Iteration 3/25 | Loss: 0.00115020
Iteration 4/25 | Loss: 0.00115020
Iteration 5/25 | Loss: 0.00115020
Iteration 6/25 | Loss: 0.00115020
Iteration 7/25 | Loss: 0.00115019
Iteration 8/25 | Loss: 0.00115019
Iteration 9/25 | Loss: 0.00115019
Iteration 10/25 | Loss: 0.00115019
Iteration 11/25 | Loss: 0.00115019
Iteration 12/25 | Loss: 0.00115019
Iteration 13/25 | Loss: 0.00115019
Iteration 14/25 | Loss: 0.00115019
Iteration 15/25 | Loss: 0.00115019
Iteration 16/25 | Loss: 0.00115019
Iteration 17/25 | Loss: 0.00115019
Iteration 18/25 | Loss: 0.00115019
Iteration 19/25 | Loss: 0.00115019
Iteration 20/25 | Loss: 0.00115019
Iteration 21/25 | Loss: 0.00115019
Iteration 22/25 | Loss: 0.00115019
Iteration 23/25 | Loss: 0.00115019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011501932749524713, 0.0011501932749524713, 0.0011501932749524713, 0.0011501932749524713, 0.0011501932749524713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011501932749524713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115019
Iteration 2/1000 | Loss: 0.00009745
Iteration 3/1000 | Loss: 0.00007080
Iteration 4/1000 | Loss: 0.00006398
Iteration 5/1000 | Loss: 0.00006046
Iteration 6/1000 | Loss: 0.00005802
Iteration 7/1000 | Loss: 0.00005670
Iteration 8/1000 | Loss: 0.00005553
Iteration 9/1000 | Loss: 0.00005477
Iteration 10/1000 | Loss: 0.00005408
Iteration 11/1000 | Loss: 0.00005359
Iteration 12/1000 | Loss: 0.00005334
Iteration 13/1000 | Loss: 0.00005312
Iteration 14/1000 | Loss: 0.00005298
Iteration 15/1000 | Loss: 0.00005288
Iteration 16/1000 | Loss: 0.00005280
Iteration 17/1000 | Loss: 0.00005274
Iteration 18/1000 | Loss: 0.00005269
Iteration 19/1000 | Loss: 0.00005269
Iteration 20/1000 | Loss: 0.00005267
Iteration 21/1000 | Loss: 0.00005267
Iteration 22/1000 | Loss: 0.00005265
Iteration 23/1000 | Loss: 0.00005264
Iteration 24/1000 | Loss: 0.00005263
Iteration 25/1000 | Loss: 0.00005263
Iteration 26/1000 | Loss: 0.00005263
Iteration 27/1000 | Loss: 0.00005263
Iteration 28/1000 | Loss: 0.00005263
Iteration 29/1000 | Loss: 0.00005263
Iteration 30/1000 | Loss: 0.00005263
Iteration 31/1000 | Loss: 0.00005263
Iteration 32/1000 | Loss: 0.00005262
Iteration 33/1000 | Loss: 0.00005262
Iteration 34/1000 | Loss: 0.00005261
Iteration 35/1000 | Loss: 0.00005261
Iteration 36/1000 | Loss: 0.00005260
Iteration 37/1000 | Loss: 0.00005260
Iteration 38/1000 | Loss: 0.00005260
Iteration 39/1000 | Loss: 0.00005260
Iteration 40/1000 | Loss: 0.00005260
Iteration 41/1000 | Loss: 0.00005260
Iteration 42/1000 | Loss: 0.00005260
Iteration 43/1000 | Loss: 0.00005260
Iteration 44/1000 | Loss: 0.00005260
Iteration 45/1000 | Loss: 0.00005260
Iteration 46/1000 | Loss: 0.00005260
Iteration 47/1000 | Loss: 0.00005260
Iteration 48/1000 | Loss: 0.00005259
Iteration 49/1000 | Loss: 0.00005259
Iteration 50/1000 | Loss: 0.00005259
Iteration 51/1000 | Loss: 0.00005259
Iteration 52/1000 | Loss: 0.00005259
Iteration 53/1000 | Loss: 0.00005259
Iteration 54/1000 | Loss: 0.00005258
Iteration 55/1000 | Loss: 0.00005257
Iteration 56/1000 | Loss: 0.00005257
Iteration 57/1000 | Loss: 0.00005257
Iteration 58/1000 | Loss: 0.00005257
Iteration 59/1000 | Loss: 0.00005256
Iteration 60/1000 | Loss: 0.00005256
Iteration 61/1000 | Loss: 0.00005255
Iteration 62/1000 | Loss: 0.00005255
Iteration 63/1000 | Loss: 0.00005254
Iteration 64/1000 | Loss: 0.00005254
Iteration 65/1000 | Loss: 0.00005253
Iteration 66/1000 | Loss: 0.00005253
Iteration 67/1000 | Loss: 0.00005253
Iteration 68/1000 | Loss: 0.00005252
Iteration 69/1000 | Loss: 0.00005251
Iteration 70/1000 | Loss: 0.00005251
Iteration 71/1000 | Loss: 0.00005250
Iteration 72/1000 | Loss: 0.00005249
Iteration 73/1000 | Loss: 0.00005249
Iteration 74/1000 | Loss: 0.00005248
Iteration 75/1000 | Loss: 0.00005248
Iteration 76/1000 | Loss: 0.00005248
Iteration 77/1000 | Loss: 0.00005248
Iteration 78/1000 | Loss: 0.00005248
Iteration 79/1000 | Loss: 0.00005248
Iteration 80/1000 | Loss: 0.00005248
Iteration 81/1000 | Loss: 0.00005248
Iteration 82/1000 | Loss: 0.00005247
Iteration 83/1000 | Loss: 0.00005247
Iteration 84/1000 | Loss: 0.00005247
Iteration 85/1000 | Loss: 0.00005246
Iteration 86/1000 | Loss: 0.00005246
Iteration 87/1000 | Loss: 0.00005246
Iteration 88/1000 | Loss: 0.00005246
Iteration 89/1000 | Loss: 0.00005245
Iteration 90/1000 | Loss: 0.00005245
Iteration 91/1000 | Loss: 0.00005245
Iteration 92/1000 | Loss: 0.00005244
Iteration 93/1000 | Loss: 0.00005244
Iteration 94/1000 | Loss: 0.00005244
Iteration 95/1000 | Loss: 0.00005244
Iteration 96/1000 | Loss: 0.00005243
Iteration 97/1000 | Loss: 0.00005243
Iteration 98/1000 | Loss: 0.00005243
Iteration 99/1000 | Loss: 0.00005243
Iteration 100/1000 | Loss: 0.00005243
Iteration 101/1000 | Loss: 0.00005243
Iteration 102/1000 | Loss: 0.00005243
Iteration 103/1000 | Loss: 0.00005243
Iteration 104/1000 | Loss: 0.00005243
Iteration 105/1000 | Loss: 0.00005243
Iteration 106/1000 | Loss: 0.00005243
Iteration 107/1000 | Loss: 0.00005243
Iteration 108/1000 | Loss: 0.00005243
Iteration 109/1000 | Loss: 0.00005243
Iteration 110/1000 | Loss: 0.00005243
Iteration 111/1000 | Loss: 0.00005243
Iteration 112/1000 | Loss: 0.00005243
Iteration 113/1000 | Loss: 0.00005243
Iteration 114/1000 | Loss: 0.00005243
Iteration 115/1000 | Loss: 0.00005243
Iteration 116/1000 | Loss: 0.00005243
Iteration 117/1000 | Loss: 0.00005243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [5.2426850743358955e-05, 5.2426850743358955e-05, 5.2426850743358955e-05, 5.2426850743358955e-05, 5.2426850743358955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.2426850743358955e-05

Optimization complete. Final v2v error: 5.966677188873291 mm

Highest mean error: 6.889503479003906 mm for frame 33

Lowest mean error: 5.206995487213135 mm for frame 217

Saving results

Total time: 56.516204595565796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757948
Iteration 2/25 | Loss: 0.00148402
Iteration 3/25 | Loss: 0.00129354
Iteration 4/25 | Loss: 0.00127044
Iteration 5/25 | Loss: 0.00126557
Iteration 6/25 | Loss: 0.00126493
Iteration 7/25 | Loss: 0.00126493
Iteration 8/25 | Loss: 0.00126493
Iteration 9/25 | Loss: 0.00126493
Iteration 10/25 | Loss: 0.00126493
Iteration 11/25 | Loss: 0.00126493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012649315176531672, 0.0012649315176531672, 0.0012649315176531672, 0.0012649315176531672, 0.0012649315176531672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012649315176531672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39510751
Iteration 2/25 | Loss: 0.00064144
Iteration 3/25 | Loss: 0.00064140
Iteration 4/25 | Loss: 0.00064140
Iteration 5/25 | Loss: 0.00064140
Iteration 6/25 | Loss: 0.00064140
Iteration 7/25 | Loss: 0.00064140
Iteration 8/25 | Loss: 0.00064140
Iteration 9/25 | Loss: 0.00064140
Iteration 10/25 | Loss: 0.00064140
Iteration 11/25 | Loss: 0.00064140
Iteration 12/25 | Loss: 0.00064140
Iteration 13/25 | Loss: 0.00064140
Iteration 14/25 | Loss: 0.00064140
Iteration 15/25 | Loss: 0.00064140
Iteration 16/25 | Loss: 0.00064140
Iteration 17/25 | Loss: 0.00064140
Iteration 18/25 | Loss: 0.00064140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006413975497707725, 0.0006413975497707725, 0.0006413975497707725, 0.0006413975497707725, 0.0006413975497707725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006413975497707725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064140
Iteration 2/1000 | Loss: 0.00004096
Iteration 3/1000 | Loss: 0.00003059
Iteration 4/1000 | Loss: 0.00002535
Iteration 5/1000 | Loss: 0.00002397
Iteration 6/1000 | Loss: 0.00002272
Iteration 7/1000 | Loss: 0.00002182
Iteration 8/1000 | Loss: 0.00002122
Iteration 9/1000 | Loss: 0.00002077
Iteration 10/1000 | Loss: 0.00002043
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00002015
Iteration 13/1000 | Loss: 0.00001993
Iteration 14/1000 | Loss: 0.00001991
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001959
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001936
Iteration 21/1000 | Loss: 0.00001926
Iteration 22/1000 | Loss: 0.00001925
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001911
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001908
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001905
Iteration 37/1000 | Loss: 0.00001905
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001904
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001903
Iteration 48/1000 | Loss: 0.00001901
Iteration 49/1000 | Loss: 0.00001901
Iteration 50/1000 | Loss: 0.00001901
Iteration 51/1000 | Loss: 0.00001901
Iteration 52/1000 | Loss: 0.00001901
Iteration 53/1000 | Loss: 0.00001901
Iteration 54/1000 | Loss: 0.00001901
Iteration 55/1000 | Loss: 0.00001901
Iteration 56/1000 | Loss: 0.00001901
Iteration 57/1000 | Loss: 0.00001901
Iteration 58/1000 | Loss: 0.00001901
Iteration 59/1000 | Loss: 0.00001901
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001900
Iteration 62/1000 | Loss: 0.00001899
Iteration 63/1000 | Loss: 0.00001899
Iteration 64/1000 | Loss: 0.00001899
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001898
Iteration 68/1000 | Loss: 0.00001898
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001898
Iteration 72/1000 | Loss: 0.00001898
Iteration 73/1000 | Loss: 0.00001898
Iteration 74/1000 | Loss: 0.00001898
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001897
Iteration 77/1000 | Loss: 0.00001897
Iteration 78/1000 | Loss: 0.00001897
Iteration 79/1000 | Loss: 0.00001897
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001895
Iteration 82/1000 | Loss: 0.00001895
Iteration 83/1000 | Loss: 0.00001895
Iteration 84/1000 | Loss: 0.00001895
Iteration 85/1000 | Loss: 0.00001894
Iteration 86/1000 | Loss: 0.00001894
Iteration 87/1000 | Loss: 0.00001894
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Iteration 97/1000 | Loss: 0.00001892
Iteration 98/1000 | Loss: 0.00001892
Iteration 99/1000 | Loss: 0.00001892
Iteration 100/1000 | Loss: 0.00001892
Iteration 101/1000 | Loss: 0.00001892
Iteration 102/1000 | Loss: 0.00001892
Iteration 103/1000 | Loss: 0.00001892
Iteration 104/1000 | Loss: 0.00001892
Iteration 105/1000 | Loss: 0.00001892
Iteration 106/1000 | Loss: 0.00001892
Iteration 107/1000 | Loss: 0.00001892
Iteration 108/1000 | Loss: 0.00001892
Iteration 109/1000 | Loss: 0.00001892
Iteration 110/1000 | Loss: 0.00001892
Iteration 111/1000 | Loss: 0.00001892
Iteration 112/1000 | Loss: 0.00001892
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001892
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.8915005057351664e-05, 1.8915005057351664e-05, 1.8915005057351664e-05, 1.8915005057351664e-05, 1.8915005057351664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8915005057351664e-05

Optimization complete. Final v2v error: 3.7243058681488037 mm

Highest mean error: 4.067887783050537 mm for frame 202

Lowest mean error: 3.5223567485809326 mm for frame 216

Saving results

Total time: 43.297908544540405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767204
Iteration 2/25 | Loss: 0.00150702
Iteration 3/25 | Loss: 0.00131624
Iteration 4/25 | Loss: 0.00130409
Iteration 5/25 | Loss: 0.00130168
Iteration 6/25 | Loss: 0.00130168
Iteration 7/25 | Loss: 0.00130168
Iteration 8/25 | Loss: 0.00130168
Iteration 9/25 | Loss: 0.00130168
Iteration 10/25 | Loss: 0.00130168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013016824377700686, 0.0013016824377700686, 0.0013016824377700686, 0.0013016824377700686, 0.0013016824377700686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013016824377700686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39850414
Iteration 2/25 | Loss: 0.00072884
Iteration 3/25 | Loss: 0.00072884
Iteration 4/25 | Loss: 0.00072883
Iteration 5/25 | Loss: 0.00072883
Iteration 6/25 | Loss: 0.00072883
Iteration 7/25 | Loss: 0.00072883
Iteration 8/25 | Loss: 0.00072883
Iteration 9/25 | Loss: 0.00072883
Iteration 10/25 | Loss: 0.00072883
Iteration 11/25 | Loss: 0.00072883
Iteration 12/25 | Loss: 0.00072883
Iteration 13/25 | Loss: 0.00072883
Iteration 14/25 | Loss: 0.00072883
Iteration 15/25 | Loss: 0.00072883
Iteration 16/25 | Loss: 0.00072883
Iteration 17/25 | Loss: 0.00072883
Iteration 18/25 | Loss: 0.00072883
Iteration 19/25 | Loss: 0.00072883
Iteration 20/25 | Loss: 0.00072883
Iteration 21/25 | Loss: 0.00072883
Iteration 22/25 | Loss: 0.00072883
Iteration 23/25 | Loss: 0.00072883
Iteration 24/25 | Loss: 0.00072883
Iteration 25/25 | Loss: 0.00072883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072883
Iteration 2/1000 | Loss: 0.00003786
Iteration 3/1000 | Loss: 0.00002621
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002177
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001827
Iteration 12/1000 | Loss: 0.00001825
Iteration 13/1000 | Loss: 0.00001806
Iteration 14/1000 | Loss: 0.00001804
Iteration 15/1000 | Loss: 0.00001790
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001770
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001754
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001735
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001733
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001732
Iteration 54/1000 | Loss: 0.00001732
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001731
Iteration 58/1000 | Loss: 0.00001731
Iteration 59/1000 | Loss: 0.00001731
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001727
Iteration 71/1000 | Loss: 0.00001727
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001725
Iteration 77/1000 | Loss: 0.00001725
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001724
Iteration 86/1000 | Loss: 0.00001724
Iteration 87/1000 | Loss: 0.00001724
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00001724
Iteration 91/1000 | Loss: 0.00001724
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001723
Iteration 94/1000 | Loss: 0.00001723
Iteration 95/1000 | Loss: 0.00001723
Iteration 96/1000 | Loss: 0.00001723
Iteration 97/1000 | Loss: 0.00001722
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001721
Iteration 101/1000 | Loss: 0.00001721
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001720
Iteration 105/1000 | Loss: 0.00001720
Iteration 106/1000 | Loss: 0.00001720
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001720
Iteration 109/1000 | Loss: 0.00001720
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001718
Iteration 122/1000 | Loss: 0.00001718
Iteration 123/1000 | Loss: 0.00001718
Iteration 124/1000 | Loss: 0.00001718
Iteration 125/1000 | Loss: 0.00001718
Iteration 126/1000 | Loss: 0.00001718
Iteration 127/1000 | Loss: 0.00001718
Iteration 128/1000 | Loss: 0.00001717
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00001717
Iteration 131/1000 | Loss: 0.00001717
Iteration 132/1000 | Loss: 0.00001717
Iteration 133/1000 | Loss: 0.00001717
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001717
Iteration 137/1000 | Loss: 0.00001717
Iteration 138/1000 | Loss: 0.00001717
Iteration 139/1000 | Loss: 0.00001717
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7153332009911537e-05, 1.7153332009911537e-05, 1.7153332009911537e-05, 1.7153332009911537e-05, 1.7153332009911537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7153332009911537e-05

Optimization complete. Final v2v error: 3.5039713382720947 mm

Highest mean error: 4.045719623565674 mm for frame 161

Lowest mean error: 2.980926036834717 mm for frame 5

Saving results

Total time: 46.57593035697937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471660
Iteration 2/25 | Loss: 0.00158882
Iteration 3/25 | Loss: 0.00138487
Iteration 4/25 | Loss: 0.00135948
Iteration 5/25 | Loss: 0.00135563
Iteration 6/25 | Loss: 0.00135536
Iteration 7/25 | Loss: 0.00135536
Iteration 8/25 | Loss: 0.00135536
Iteration 9/25 | Loss: 0.00135536
Iteration 10/25 | Loss: 0.00135536
Iteration 11/25 | Loss: 0.00135536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013553627068176866, 0.0013553627068176866, 0.0013553627068176866, 0.0013553627068176866, 0.0013553627068176866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013553627068176866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42128646
Iteration 2/25 | Loss: 0.00087934
Iteration 3/25 | Loss: 0.00087933
Iteration 4/25 | Loss: 0.00087933
Iteration 5/25 | Loss: 0.00087933
Iteration 6/25 | Loss: 0.00087933
Iteration 7/25 | Loss: 0.00087932
Iteration 8/25 | Loss: 0.00087932
Iteration 9/25 | Loss: 0.00087932
Iteration 10/25 | Loss: 0.00087932
Iteration 11/25 | Loss: 0.00087932
Iteration 12/25 | Loss: 0.00087932
Iteration 13/25 | Loss: 0.00087932
Iteration 14/25 | Loss: 0.00087932
Iteration 15/25 | Loss: 0.00087932
Iteration 16/25 | Loss: 0.00087932
Iteration 17/25 | Loss: 0.00087932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008793237502686679, 0.0008793237502686679, 0.0008793237502686679, 0.0008793237502686679, 0.0008793237502686679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008793237502686679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087932
Iteration 2/1000 | Loss: 0.00005569
Iteration 3/1000 | Loss: 0.00003321
Iteration 4/1000 | Loss: 0.00002985
Iteration 5/1000 | Loss: 0.00002794
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002537
Iteration 8/1000 | Loss: 0.00002472
Iteration 9/1000 | Loss: 0.00002397
Iteration 10/1000 | Loss: 0.00002339
Iteration 11/1000 | Loss: 0.00002306
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002277
Iteration 14/1000 | Loss: 0.00002273
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002272
Iteration 17/1000 | Loss: 0.00002272
Iteration 18/1000 | Loss: 0.00002267
Iteration 19/1000 | Loss: 0.00002266
Iteration 20/1000 | Loss: 0.00002266
Iteration 21/1000 | Loss: 0.00002265
Iteration 22/1000 | Loss: 0.00002261
Iteration 23/1000 | Loss: 0.00002256
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002241
Iteration 26/1000 | Loss: 0.00002237
Iteration 27/1000 | Loss: 0.00002236
Iteration 28/1000 | Loss: 0.00002236
Iteration 29/1000 | Loss: 0.00002236
Iteration 30/1000 | Loss: 0.00002232
Iteration 31/1000 | Loss: 0.00002231
Iteration 32/1000 | Loss: 0.00002229
Iteration 33/1000 | Loss: 0.00002229
Iteration 34/1000 | Loss: 0.00002229
Iteration 35/1000 | Loss: 0.00002229
Iteration 36/1000 | Loss: 0.00002229
Iteration 37/1000 | Loss: 0.00002229
Iteration 38/1000 | Loss: 0.00002227
Iteration 39/1000 | Loss: 0.00002227
Iteration 40/1000 | Loss: 0.00002225
Iteration 41/1000 | Loss: 0.00002225
Iteration 42/1000 | Loss: 0.00002224
Iteration 43/1000 | Loss: 0.00002224
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00002222
Iteration 46/1000 | Loss: 0.00002222
Iteration 47/1000 | Loss: 0.00002222
Iteration 48/1000 | Loss: 0.00002221
Iteration 49/1000 | Loss: 0.00002221
Iteration 50/1000 | Loss: 0.00002221
Iteration 51/1000 | Loss: 0.00002220
Iteration 52/1000 | Loss: 0.00002220
Iteration 53/1000 | Loss: 0.00002220
Iteration 54/1000 | Loss: 0.00002219
Iteration 55/1000 | Loss: 0.00002219
Iteration 56/1000 | Loss: 0.00002219
Iteration 57/1000 | Loss: 0.00002219
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002216
Iteration 60/1000 | Loss: 0.00002216
Iteration 61/1000 | Loss: 0.00002216
Iteration 62/1000 | Loss: 0.00002216
Iteration 63/1000 | Loss: 0.00002216
Iteration 64/1000 | Loss: 0.00002216
Iteration 65/1000 | Loss: 0.00002216
Iteration 66/1000 | Loss: 0.00002215
Iteration 67/1000 | Loss: 0.00002215
Iteration 68/1000 | Loss: 0.00002215
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002214
Iteration 71/1000 | Loss: 0.00002213
Iteration 72/1000 | Loss: 0.00002213
Iteration 73/1000 | Loss: 0.00002213
Iteration 74/1000 | Loss: 0.00002213
Iteration 75/1000 | Loss: 0.00002212
Iteration 76/1000 | Loss: 0.00002212
Iteration 77/1000 | Loss: 0.00002212
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002210
Iteration 82/1000 | Loss: 0.00002210
Iteration 83/1000 | Loss: 0.00002210
Iteration 84/1000 | Loss: 0.00002209
Iteration 85/1000 | Loss: 0.00002209
Iteration 86/1000 | Loss: 0.00002209
Iteration 87/1000 | Loss: 0.00002209
Iteration 88/1000 | Loss: 0.00002209
Iteration 89/1000 | Loss: 0.00002209
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00002209
Iteration 92/1000 | Loss: 0.00002208
Iteration 93/1000 | Loss: 0.00002207
Iteration 94/1000 | Loss: 0.00002207
Iteration 95/1000 | Loss: 0.00002207
Iteration 96/1000 | Loss: 0.00002207
Iteration 97/1000 | Loss: 0.00002207
Iteration 98/1000 | Loss: 0.00002207
Iteration 99/1000 | Loss: 0.00002207
Iteration 100/1000 | Loss: 0.00002206
Iteration 101/1000 | Loss: 0.00002206
Iteration 102/1000 | Loss: 0.00002206
Iteration 103/1000 | Loss: 0.00002205
Iteration 104/1000 | Loss: 0.00002205
Iteration 105/1000 | Loss: 0.00002205
Iteration 106/1000 | Loss: 0.00002204
Iteration 107/1000 | Loss: 0.00002204
Iteration 108/1000 | Loss: 0.00002204
Iteration 109/1000 | Loss: 0.00002203
Iteration 110/1000 | Loss: 0.00002203
Iteration 111/1000 | Loss: 0.00002203
Iteration 112/1000 | Loss: 0.00002203
Iteration 113/1000 | Loss: 0.00002203
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002202
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002202
Iteration 122/1000 | Loss: 0.00002202
Iteration 123/1000 | Loss: 0.00002202
Iteration 124/1000 | Loss: 0.00002202
Iteration 125/1000 | Loss: 0.00002202
Iteration 126/1000 | Loss: 0.00002202
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002202
Iteration 130/1000 | Loss: 0.00002202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.2020760297891684e-05, 2.2020760297891684e-05, 2.2020760297891684e-05, 2.2020760297891684e-05, 2.2020760297891684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2020760297891684e-05

Optimization complete. Final v2v error: 3.935485601425171 mm

Highest mean error: 4.234358787536621 mm for frame 163

Lowest mean error: 3.5255746841430664 mm for frame 27

Saving results

Total time: 45.268736124038696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962536
Iteration 2/25 | Loss: 0.00171444
Iteration 3/25 | Loss: 0.00141510
Iteration 4/25 | Loss: 0.00135187
Iteration 5/25 | Loss: 0.00134703
Iteration 6/25 | Loss: 0.00134051
Iteration 7/25 | Loss: 0.00132996
Iteration 8/25 | Loss: 0.00132299
Iteration 9/25 | Loss: 0.00132153
Iteration 10/25 | Loss: 0.00131964
Iteration 11/25 | Loss: 0.00131944
Iteration 12/25 | Loss: 0.00131988
Iteration 13/25 | Loss: 0.00131980
Iteration 14/25 | Loss: 0.00132060
Iteration 15/25 | Loss: 0.00132098
Iteration 16/25 | Loss: 0.00131973
Iteration 17/25 | Loss: 0.00131612
Iteration 18/25 | Loss: 0.00131510
Iteration 19/25 | Loss: 0.00131441
Iteration 20/25 | Loss: 0.00131408
Iteration 21/25 | Loss: 0.00131405
Iteration 22/25 | Loss: 0.00131405
Iteration 23/25 | Loss: 0.00131405
Iteration 24/25 | Loss: 0.00131404
Iteration 25/25 | Loss: 0.00131404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52449203
Iteration 2/25 | Loss: 0.00115005
Iteration 3/25 | Loss: 0.00115004
Iteration 4/25 | Loss: 0.00115004
Iteration 5/25 | Loss: 0.00115004
Iteration 6/25 | Loss: 0.00115004
Iteration 7/25 | Loss: 0.00115004
Iteration 8/25 | Loss: 0.00115004
Iteration 9/25 | Loss: 0.00115004
Iteration 10/25 | Loss: 0.00115004
Iteration 11/25 | Loss: 0.00115004
Iteration 12/25 | Loss: 0.00115004
Iteration 13/25 | Loss: 0.00115004
Iteration 14/25 | Loss: 0.00115004
Iteration 15/25 | Loss: 0.00115004
Iteration 16/25 | Loss: 0.00115004
Iteration 17/25 | Loss: 0.00115004
Iteration 18/25 | Loss: 0.00115004
Iteration 19/25 | Loss: 0.00115004
Iteration 20/25 | Loss: 0.00115004
Iteration 21/25 | Loss: 0.00115004
Iteration 22/25 | Loss: 0.00115004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011500362306833267, 0.0011500362306833267, 0.0011500362306833267, 0.0011500362306833267, 0.0011500362306833267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011500362306833267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115004
Iteration 2/1000 | Loss: 0.00003650
Iteration 3/1000 | Loss: 0.00002313
Iteration 4/1000 | Loss: 0.00001991
Iteration 5/1000 | Loss: 0.00001900
Iteration 6/1000 | Loss: 0.00001813
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001629
Iteration 16/1000 | Loss: 0.00001626
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001620
Iteration 21/1000 | Loss: 0.00001620
Iteration 22/1000 | Loss: 0.00001619
Iteration 23/1000 | Loss: 0.00001619
Iteration 24/1000 | Loss: 0.00001616
Iteration 25/1000 | Loss: 0.00001616
Iteration 26/1000 | Loss: 0.00001615
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001611
Iteration 30/1000 | Loss: 0.00001611
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001608
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001608
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001605
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001605
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001605
Iteration 57/1000 | Loss: 0.00001605
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.6046666132751852e-05, 1.6046666132751852e-05, 1.6046666132751852e-05, 1.6046666132751852e-05, 1.6046666132751852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6046666132751852e-05

Optimization complete. Final v2v error: 3.4224042892456055 mm

Highest mean error: 4.015589714050293 mm for frame 177

Lowest mean error: 3.134860038757324 mm for frame 95

Saving results

Total time: 66.5229640007019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513987
Iteration 2/25 | Loss: 0.00149667
Iteration 3/25 | Loss: 0.00136357
Iteration 4/25 | Loss: 0.00135244
Iteration 5/25 | Loss: 0.00134924
Iteration 6/25 | Loss: 0.00134921
Iteration 7/25 | Loss: 0.00134921
Iteration 8/25 | Loss: 0.00134921
Iteration 9/25 | Loss: 0.00134921
Iteration 10/25 | Loss: 0.00134921
Iteration 11/25 | Loss: 0.00134921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013492105063050985, 0.0013492105063050985, 0.0013492105063050985, 0.0013492105063050985, 0.0013492105063050985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013492105063050985

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43972266
Iteration 2/25 | Loss: 0.00103341
Iteration 3/25 | Loss: 0.00103340
Iteration 4/25 | Loss: 0.00103340
Iteration 5/25 | Loss: 0.00103340
Iteration 6/25 | Loss: 0.00103340
Iteration 7/25 | Loss: 0.00103340
Iteration 8/25 | Loss: 0.00103340
Iteration 9/25 | Loss: 0.00103340
Iteration 10/25 | Loss: 0.00103340
Iteration 11/25 | Loss: 0.00103340
Iteration 12/25 | Loss: 0.00103340
Iteration 13/25 | Loss: 0.00103340
Iteration 14/25 | Loss: 0.00103340
Iteration 15/25 | Loss: 0.00103340
Iteration 16/25 | Loss: 0.00103340
Iteration 17/25 | Loss: 0.00103340
Iteration 18/25 | Loss: 0.00103340
Iteration 19/25 | Loss: 0.00103340
Iteration 20/25 | Loss: 0.00103340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010333998361602426, 0.0010333998361602426, 0.0010333998361602426, 0.0010333998361602426, 0.0010333998361602426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010333998361602426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103340
Iteration 2/1000 | Loss: 0.00004001
Iteration 3/1000 | Loss: 0.00002858
Iteration 4/1000 | Loss: 0.00002562
Iteration 5/1000 | Loss: 0.00002412
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00002213
Iteration 8/1000 | Loss: 0.00002160
Iteration 9/1000 | Loss: 0.00002125
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002056
Iteration 12/1000 | Loss: 0.00002033
Iteration 13/1000 | Loss: 0.00002013
Iteration 14/1000 | Loss: 0.00001996
Iteration 15/1000 | Loss: 0.00001993
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001971
Iteration 28/1000 | Loss: 0.00001970
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001970
Iteration 31/1000 | Loss: 0.00001969
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001967
Iteration 34/1000 | Loss: 0.00001967
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001965
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001963
Iteration 41/1000 | Loss: 0.00001963
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001962
Iteration 44/1000 | Loss: 0.00001962
Iteration 45/1000 | Loss: 0.00001961
Iteration 46/1000 | Loss: 0.00001961
Iteration 47/1000 | Loss: 0.00001960
Iteration 48/1000 | Loss: 0.00001960
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001959
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001957
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001956
Iteration 72/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.955988227564376e-05, 1.955988227564376e-05, 1.955988227564376e-05, 1.955988227564376e-05, 1.955988227564376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.955988227564376e-05

Optimization complete. Final v2v error: 3.6662049293518066 mm

Highest mean error: 4.422701835632324 mm for frame 209

Lowest mean error: 3.172964334487915 mm for frame 59

Saving results

Total time: 39.80413579940796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791595
Iteration 2/25 | Loss: 0.00135258
Iteration 3/25 | Loss: 0.00125912
Iteration 4/25 | Loss: 0.00124462
Iteration 5/25 | Loss: 0.00124035
Iteration 6/25 | Loss: 0.00124035
Iteration 7/25 | Loss: 0.00124035
Iteration 8/25 | Loss: 0.00124035
Iteration 9/25 | Loss: 0.00124035
Iteration 10/25 | Loss: 0.00124035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012403520522639155, 0.0012403520522639155, 0.0012403520522639155, 0.0012403520522639155, 0.0012403520522639155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012403520522639155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47319102
Iteration 2/25 | Loss: 0.00075914
Iteration 3/25 | Loss: 0.00075913
Iteration 4/25 | Loss: 0.00075913
Iteration 5/25 | Loss: 0.00075913
Iteration 6/25 | Loss: 0.00075913
Iteration 7/25 | Loss: 0.00075913
Iteration 8/25 | Loss: 0.00075913
Iteration 9/25 | Loss: 0.00075913
Iteration 10/25 | Loss: 0.00075913
Iteration 11/25 | Loss: 0.00075913
Iteration 12/25 | Loss: 0.00075913
Iteration 13/25 | Loss: 0.00075913
Iteration 14/25 | Loss: 0.00075913
Iteration 15/25 | Loss: 0.00075913
Iteration 16/25 | Loss: 0.00075913
Iteration 17/25 | Loss: 0.00075913
Iteration 18/25 | Loss: 0.00075913
Iteration 19/25 | Loss: 0.00075913
Iteration 20/25 | Loss: 0.00075913
Iteration 21/25 | Loss: 0.00075913
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000759128772187978, 0.000759128772187978, 0.000759128772187978, 0.000759128772187978, 0.000759128772187978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000759128772187978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075913
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00002271
Iteration 4/1000 | Loss: 0.00001992
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001689
Iteration 9/1000 | Loss: 0.00001658
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001618
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001599
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001593
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001581
Iteration 22/1000 | Loss: 0.00001580
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001560
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001556
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001553
Iteration 58/1000 | Loss: 0.00001553
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001551
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001547
Iteration 78/1000 | Loss: 0.00001547
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001544
Iteration 89/1000 | Loss: 0.00001544
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001539
Iteration 100/1000 | Loss: 0.00001539
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001538
Iteration 103/1000 | Loss: 0.00001538
Iteration 104/1000 | Loss: 0.00001538
Iteration 105/1000 | Loss: 0.00001538
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001537
Iteration 108/1000 | Loss: 0.00001537
Iteration 109/1000 | Loss: 0.00001537
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001536
Iteration 113/1000 | Loss: 0.00001536
Iteration 114/1000 | Loss: 0.00001536
Iteration 115/1000 | Loss: 0.00001536
Iteration 116/1000 | Loss: 0.00001536
Iteration 117/1000 | Loss: 0.00001536
Iteration 118/1000 | Loss: 0.00001536
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001535
Iteration 121/1000 | Loss: 0.00001535
Iteration 122/1000 | Loss: 0.00001535
Iteration 123/1000 | Loss: 0.00001535
Iteration 124/1000 | Loss: 0.00001535
Iteration 125/1000 | Loss: 0.00001534
Iteration 126/1000 | Loss: 0.00001534
Iteration 127/1000 | Loss: 0.00001534
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001532
Iteration 132/1000 | Loss: 0.00001532
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001532
Iteration 135/1000 | Loss: 0.00001531
Iteration 136/1000 | Loss: 0.00001531
Iteration 137/1000 | Loss: 0.00001531
Iteration 138/1000 | Loss: 0.00001531
Iteration 139/1000 | Loss: 0.00001530
Iteration 140/1000 | Loss: 0.00001530
Iteration 141/1000 | Loss: 0.00001529
Iteration 142/1000 | Loss: 0.00001529
Iteration 143/1000 | Loss: 0.00001529
Iteration 144/1000 | Loss: 0.00001529
Iteration 145/1000 | Loss: 0.00001529
Iteration 146/1000 | Loss: 0.00001529
Iteration 147/1000 | Loss: 0.00001529
Iteration 148/1000 | Loss: 0.00001528
Iteration 149/1000 | Loss: 0.00001528
Iteration 150/1000 | Loss: 0.00001528
Iteration 151/1000 | Loss: 0.00001528
Iteration 152/1000 | Loss: 0.00001528
Iteration 153/1000 | Loss: 0.00001528
Iteration 154/1000 | Loss: 0.00001528
Iteration 155/1000 | Loss: 0.00001528
Iteration 156/1000 | Loss: 0.00001527
Iteration 157/1000 | Loss: 0.00001527
Iteration 158/1000 | Loss: 0.00001527
Iteration 159/1000 | Loss: 0.00001527
Iteration 160/1000 | Loss: 0.00001527
Iteration 161/1000 | Loss: 0.00001527
Iteration 162/1000 | Loss: 0.00001527
Iteration 163/1000 | Loss: 0.00001527
Iteration 164/1000 | Loss: 0.00001527
Iteration 165/1000 | Loss: 0.00001527
Iteration 166/1000 | Loss: 0.00001526
Iteration 167/1000 | Loss: 0.00001526
Iteration 168/1000 | Loss: 0.00001526
Iteration 169/1000 | Loss: 0.00001526
Iteration 170/1000 | Loss: 0.00001526
Iteration 171/1000 | Loss: 0.00001526
Iteration 172/1000 | Loss: 0.00001526
Iteration 173/1000 | Loss: 0.00001526
Iteration 174/1000 | Loss: 0.00001526
Iteration 175/1000 | Loss: 0.00001526
Iteration 176/1000 | Loss: 0.00001526
Iteration 177/1000 | Loss: 0.00001526
Iteration 178/1000 | Loss: 0.00001526
Iteration 179/1000 | Loss: 0.00001526
Iteration 180/1000 | Loss: 0.00001526
Iteration 181/1000 | Loss: 0.00001526
Iteration 182/1000 | Loss: 0.00001526
Iteration 183/1000 | Loss: 0.00001526
Iteration 184/1000 | Loss: 0.00001526
Iteration 185/1000 | Loss: 0.00001526
Iteration 186/1000 | Loss: 0.00001525
Iteration 187/1000 | Loss: 0.00001525
Iteration 188/1000 | Loss: 0.00001525
Iteration 189/1000 | Loss: 0.00001525
Iteration 190/1000 | Loss: 0.00001525
Iteration 191/1000 | Loss: 0.00001525
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001525
Iteration 195/1000 | Loss: 0.00001525
Iteration 196/1000 | Loss: 0.00001525
Iteration 197/1000 | Loss: 0.00001525
Iteration 198/1000 | Loss: 0.00001525
Iteration 199/1000 | Loss: 0.00001525
Iteration 200/1000 | Loss: 0.00001525
Iteration 201/1000 | Loss: 0.00001525
Iteration 202/1000 | Loss: 0.00001525
Iteration 203/1000 | Loss: 0.00001525
Iteration 204/1000 | Loss: 0.00001525
Iteration 205/1000 | Loss: 0.00001525
Iteration 206/1000 | Loss: 0.00001525
Iteration 207/1000 | Loss: 0.00001525
Iteration 208/1000 | Loss: 0.00001524
Iteration 209/1000 | Loss: 0.00001524
Iteration 210/1000 | Loss: 0.00001524
Iteration 211/1000 | Loss: 0.00001524
Iteration 212/1000 | Loss: 0.00001524
Iteration 213/1000 | Loss: 0.00001524
Iteration 214/1000 | Loss: 0.00001524
Iteration 215/1000 | Loss: 0.00001524
Iteration 216/1000 | Loss: 0.00001524
Iteration 217/1000 | Loss: 0.00001524
Iteration 218/1000 | Loss: 0.00001524
Iteration 219/1000 | Loss: 0.00001524
Iteration 220/1000 | Loss: 0.00001524
Iteration 221/1000 | Loss: 0.00001524
Iteration 222/1000 | Loss: 0.00001524
Iteration 223/1000 | Loss: 0.00001524
Iteration 224/1000 | Loss: 0.00001524
Iteration 225/1000 | Loss: 0.00001524
Iteration 226/1000 | Loss: 0.00001524
Iteration 227/1000 | Loss: 0.00001524
Iteration 228/1000 | Loss: 0.00001524
Iteration 229/1000 | Loss: 0.00001524
Iteration 230/1000 | Loss: 0.00001524
Iteration 231/1000 | Loss: 0.00001524
Iteration 232/1000 | Loss: 0.00001524
Iteration 233/1000 | Loss: 0.00001524
Iteration 234/1000 | Loss: 0.00001524
Iteration 235/1000 | Loss: 0.00001524
Iteration 236/1000 | Loss: 0.00001524
Iteration 237/1000 | Loss: 0.00001524
Iteration 238/1000 | Loss: 0.00001524
Iteration 239/1000 | Loss: 0.00001524
Iteration 240/1000 | Loss: 0.00001524
Iteration 241/1000 | Loss: 0.00001524
Iteration 242/1000 | Loss: 0.00001524
Iteration 243/1000 | Loss: 0.00001524
Iteration 244/1000 | Loss: 0.00001524
Iteration 245/1000 | Loss: 0.00001524
Iteration 246/1000 | Loss: 0.00001524
Iteration 247/1000 | Loss: 0.00001524
Iteration 248/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.5236705621646252e-05, 1.5236705621646252e-05, 1.5236705621646252e-05, 1.5236705621646252e-05, 1.5236705621646252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5236705621646252e-05

Optimization complete. Final v2v error: 3.358546018600464 mm

Highest mean error: 3.6605348587036133 mm for frame 154

Lowest mean error: 3.2063608169555664 mm for frame 66

Saving results

Total time: 49.87503743171692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430423
Iteration 2/25 | Loss: 0.00144929
Iteration 3/25 | Loss: 0.00128436
Iteration 4/25 | Loss: 0.00126417
Iteration 5/25 | Loss: 0.00126102
Iteration 6/25 | Loss: 0.00126042
Iteration 7/25 | Loss: 0.00126042
Iteration 8/25 | Loss: 0.00126042
Iteration 9/25 | Loss: 0.00126042
Iteration 10/25 | Loss: 0.00126042
Iteration 11/25 | Loss: 0.00126042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012604249641299248, 0.0012604249641299248, 0.0012604249641299248, 0.0012604249641299248, 0.0012604249641299248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012604249641299248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43346846
Iteration 2/25 | Loss: 0.00079438
Iteration 3/25 | Loss: 0.00079438
Iteration 4/25 | Loss: 0.00079438
Iteration 5/25 | Loss: 0.00079438
Iteration 6/25 | Loss: 0.00079438
Iteration 7/25 | Loss: 0.00079438
Iteration 8/25 | Loss: 0.00079438
Iteration 9/25 | Loss: 0.00079438
Iteration 10/25 | Loss: 0.00079438
Iteration 11/25 | Loss: 0.00079438
Iteration 12/25 | Loss: 0.00079438
Iteration 13/25 | Loss: 0.00079438
Iteration 14/25 | Loss: 0.00079438
Iteration 15/25 | Loss: 0.00079438
Iteration 16/25 | Loss: 0.00079438
Iteration 17/25 | Loss: 0.00079438
Iteration 18/25 | Loss: 0.00079438
Iteration 19/25 | Loss: 0.00079438
Iteration 20/25 | Loss: 0.00079438
Iteration 21/25 | Loss: 0.00079438
Iteration 22/25 | Loss: 0.00079438
Iteration 23/25 | Loss: 0.00079438
Iteration 24/25 | Loss: 0.00079438
Iteration 25/25 | Loss: 0.00079438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079438
Iteration 2/1000 | Loss: 0.00003165
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001766
Iteration 5/1000 | Loss: 0.00001633
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001435
Iteration 9/1000 | Loss: 0.00001430
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001372
Iteration 12/1000 | Loss: 0.00001366
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001350
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001344
Iteration 24/1000 | Loss: 0.00001343
Iteration 25/1000 | Loss: 0.00001342
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001340
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001339
Iteration 30/1000 | Loss: 0.00001339
Iteration 31/1000 | Loss: 0.00001337
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001324
Iteration 57/1000 | Loss: 0.00001324
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001322
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00001322
Iteration 74/1000 | Loss: 0.00001322
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001322
Iteration 77/1000 | Loss: 0.00001322
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001313
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001310
Iteration 92/1000 | Loss: 0.00001310
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001310
Iteration 96/1000 | Loss: 0.00001310
Iteration 97/1000 | Loss: 0.00001310
Iteration 98/1000 | Loss: 0.00001310
Iteration 99/1000 | Loss: 0.00001310
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001309
Iteration 102/1000 | Loss: 0.00001309
Iteration 103/1000 | Loss: 0.00001308
Iteration 104/1000 | Loss: 0.00001307
Iteration 105/1000 | Loss: 0.00001307
Iteration 106/1000 | Loss: 0.00001307
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001306
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001306
Iteration 111/1000 | Loss: 0.00001306
Iteration 112/1000 | Loss: 0.00001306
Iteration 113/1000 | Loss: 0.00001306
Iteration 114/1000 | Loss: 0.00001306
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001303
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001297
Iteration 149/1000 | Loss: 0.00001297
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001296
Iteration 155/1000 | Loss: 0.00001296
Iteration 156/1000 | Loss: 0.00001296
Iteration 157/1000 | Loss: 0.00001295
Iteration 158/1000 | Loss: 0.00001295
Iteration 159/1000 | Loss: 0.00001295
Iteration 160/1000 | Loss: 0.00001295
Iteration 161/1000 | Loss: 0.00001295
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001294
Iteration 164/1000 | Loss: 0.00001294
Iteration 165/1000 | Loss: 0.00001294
Iteration 166/1000 | Loss: 0.00001294
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001293
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001291
Iteration 176/1000 | Loss: 0.00001291
Iteration 177/1000 | Loss: 0.00001290
Iteration 178/1000 | Loss: 0.00001290
Iteration 179/1000 | Loss: 0.00001290
Iteration 180/1000 | Loss: 0.00001290
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001290
Iteration 183/1000 | Loss: 0.00001290
Iteration 184/1000 | Loss: 0.00001290
Iteration 185/1000 | Loss: 0.00001289
Iteration 186/1000 | Loss: 0.00001289
Iteration 187/1000 | Loss: 0.00001289
Iteration 188/1000 | Loss: 0.00001289
Iteration 189/1000 | Loss: 0.00001289
Iteration 190/1000 | Loss: 0.00001289
Iteration 191/1000 | Loss: 0.00001289
Iteration 192/1000 | Loss: 0.00001289
Iteration 193/1000 | Loss: 0.00001289
Iteration 194/1000 | Loss: 0.00001289
Iteration 195/1000 | Loss: 0.00001289
Iteration 196/1000 | Loss: 0.00001289
Iteration 197/1000 | Loss: 0.00001289
Iteration 198/1000 | Loss: 0.00001288
Iteration 199/1000 | Loss: 0.00001288
Iteration 200/1000 | Loss: 0.00001288
Iteration 201/1000 | Loss: 0.00001288
Iteration 202/1000 | Loss: 0.00001288
Iteration 203/1000 | Loss: 0.00001288
Iteration 204/1000 | Loss: 0.00001287
Iteration 205/1000 | Loss: 0.00001287
Iteration 206/1000 | Loss: 0.00001287
Iteration 207/1000 | Loss: 0.00001287
Iteration 208/1000 | Loss: 0.00001287
Iteration 209/1000 | Loss: 0.00001287
Iteration 210/1000 | Loss: 0.00001287
Iteration 211/1000 | Loss: 0.00001287
Iteration 212/1000 | Loss: 0.00001287
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001286
Iteration 219/1000 | Loss: 0.00001286
Iteration 220/1000 | Loss: 0.00001286
Iteration 221/1000 | Loss: 0.00001286
Iteration 222/1000 | Loss: 0.00001286
Iteration 223/1000 | Loss: 0.00001286
Iteration 224/1000 | Loss: 0.00001286
Iteration 225/1000 | Loss: 0.00001286
Iteration 226/1000 | Loss: 0.00001286
Iteration 227/1000 | Loss: 0.00001286
Iteration 228/1000 | Loss: 0.00001286
Iteration 229/1000 | Loss: 0.00001285
Iteration 230/1000 | Loss: 0.00001285
Iteration 231/1000 | Loss: 0.00001285
Iteration 232/1000 | Loss: 0.00001285
Iteration 233/1000 | Loss: 0.00001285
Iteration 234/1000 | Loss: 0.00001285
Iteration 235/1000 | Loss: 0.00001285
Iteration 236/1000 | Loss: 0.00001285
Iteration 237/1000 | Loss: 0.00001285
Iteration 238/1000 | Loss: 0.00001285
Iteration 239/1000 | Loss: 0.00001285
Iteration 240/1000 | Loss: 0.00001285
Iteration 241/1000 | Loss: 0.00001285
Iteration 242/1000 | Loss: 0.00001284
Iteration 243/1000 | Loss: 0.00001284
Iteration 244/1000 | Loss: 0.00001284
Iteration 245/1000 | Loss: 0.00001284
Iteration 246/1000 | Loss: 0.00001284
Iteration 247/1000 | Loss: 0.00001284
Iteration 248/1000 | Loss: 0.00001284
Iteration 249/1000 | Loss: 0.00001284
Iteration 250/1000 | Loss: 0.00001284
Iteration 251/1000 | Loss: 0.00001284
Iteration 252/1000 | Loss: 0.00001284
Iteration 253/1000 | Loss: 0.00001284
Iteration 254/1000 | Loss: 0.00001284
Iteration 255/1000 | Loss: 0.00001284
Iteration 256/1000 | Loss: 0.00001284
Iteration 257/1000 | Loss: 0.00001284
Iteration 258/1000 | Loss: 0.00001284
Iteration 259/1000 | Loss: 0.00001284
Iteration 260/1000 | Loss: 0.00001284
Iteration 261/1000 | Loss: 0.00001284
Iteration 262/1000 | Loss: 0.00001284
Iteration 263/1000 | Loss: 0.00001284
Iteration 264/1000 | Loss: 0.00001284
Iteration 265/1000 | Loss: 0.00001284
Iteration 266/1000 | Loss: 0.00001284
Iteration 267/1000 | Loss: 0.00001284
Iteration 268/1000 | Loss: 0.00001284
Iteration 269/1000 | Loss: 0.00001284
Iteration 270/1000 | Loss: 0.00001284
Iteration 271/1000 | Loss: 0.00001284
Iteration 272/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.2839528608310502e-05, 1.2839528608310502e-05, 1.2839528608310502e-05, 1.2839528608310502e-05, 1.2839528608310502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2839528608310502e-05

Optimization complete. Final v2v error: 3.0659213066101074 mm

Highest mean error: 3.6014151573181152 mm for frame 90

Lowest mean error: 2.8808233737945557 mm for frame 201

Saving results

Total time: 51.16720390319824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803096
Iteration 2/25 | Loss: 0.00148009
Iteration 3/25 | Loss: 0.00135333
Iteration 4/25 | Loss: 0.00133859
Iteration 5/25 | Loss: 0.00133478
Iteration 6/25 | Loss: 0.00133383
Iteration 7/25 | Loss: 0.00133383
Iteration 8/25 | Loss: 0.00133383
Iteration 9/25 | Loss: 0.00133383
Iteration 10/25 | Loss: 0.00133383
Iteration 11/25 | Loss: 0.00133383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013338349526748061, 0.0013338349526748061, 0.0013338349526748061, 0.0013338349526748061, 0.0013338349526748061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013338349526748061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42708266
Iteration 2/25 | Loss: 0.00190031
Iteration 3/25 | Loss: 0.00190030
Iteration 4/25 | Loss: 0.00190030
Iteration 5/25 | Loss: 0.00190030
Iteration 6/25 | Loss: 0.00190030
Iteration 7/25 | Loss: 0.00190030
Iteration 8/25 | Loss: 0.00190030
Iteration 9/25 | Loss: 0.00190030
Iteration 10/25 | Loss: 0.00190030
Iteration 11/25 | Loss: 0.00190030
Iteration 12/25 | Loss: 0.00190030
Iteration 13/25 | Loss: 0.00190030
Iteration 14/25 | Loss: 0.00190030
Iteration 15/25 | Loss: 0.00190030
Iteration 16/25 | Loss: 0.00190030
Iteration 17/25 | Loss: 0.00190030
Iteration 18/25 | Loss: 0.00190030
Iteration 19/25 | Loss: 0.00190030
Iteration 20/25 | Loss: 0.00190030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019002978224307299, 0.0019002978224307299, 0.0019002978224307299, 0.0019002978224307299, 0.0019002978224307299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019002978224307299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190030
Iteration 2/1000 | Loss: 0.00015028
Iteration 3/1000 | Loss: 0.00008868
Iteration 4/1000 | Loss: 0.00007549
Iteration 5/1000 | Loss: 0.00006777
Iteration 6/1000 | Loss: 0.00006508
Iteration 7/1000 | Loss: 0.00006229
Iteration 8/1000 | Loss: 0.00006062
Iteration 9/1000 | Loss: 0.00005882
Iteration 10/1000 | Loss: 0.00005734
Iteration 11/1000 | Loss: 0.00005656
Iteration 12/1000 | Loss: 0.00005619
Iteration 13/1000 | Loss: 0.00005582
Iteration 14/1000 | Loss: 0.00005554
Iteration 15/1000 | Loss: 0.00005542
Iteration 16/1000 | Loss: 0.00005528
Iteration 17/1000 | Loss: 0.00005510
Iteration 18/1000 | Loss: 0.00005506
Iteration 19/1000 | Loss: 0.00005487
Iteration 20/1000 | Loss: 0.00005485
Iteration 21/1000 | Loss: 0.00005473
Iteration 22/1000 | Loss: 0.00005454
Iteration 23/1000 | Loss: 0.00005418
Iteration 24/1000 | Loss: 0.00023036
Iteration 25/1000 | Loss: 0.00005821
Iteration 26/1000 | Loss: 0.00022135
Iteration 27/1000 | Loss: 0.00024240
Iteration 28/1000 | Loss: 0.00022864
Iteration 29/1000 | Loss: 0.00025095
Iteration 30/1000 | Loss: 0.00005749
Iteration 31/1000 | Loss: 0.00005542
Iteration 32/1000 | Loss: 0.00005465
Iteration 33/1000 | Loss: 0.00005405
Iteration 34/1000 | Loss: 0.00005355
Iteration 35/1000 | Loss: 0.00005317
Iteration 36/1000 | Loss: 0.00005263
Iteration 37/1000 | Loss: 0.00005213
Iteration 38/1000 | Loss: 0.00005162
Iteration 39/1000 | Loss: 0.00005094
Iteration 40/1000 | Loss: 0.00005028
Iteration 41/1000 | Loss: 0.00004973
Iteration 42/1000 | Loss: 0.00004935
Iteration 43/1000 | Loss: 0.00004896
Iteration 44/1000 | Loss: 0.00004862
Iteration 45/1000 | Loss: 0.00004836
Iteration 46/1000 | Loss: 0.00004816
Iteration 47/1000 | Loss: 0.00004801
Iteration 48/1000 | Loss: 0.00004795
Iteration 49/1000 | Loss: 0.00004794
Iteration 50/1000 | Loss: 0.00004793
Iteration 51/1000 | Loss: 0.00004777
Iteration 52/1000 | Loss: 0.00004774
Iteration 53/1000 | Loss: 0.00004757
Iteration 54/1000 | Loss: 0.00095307
Iteration 55/1000 | Loss: 0.00067521
Iteration 56/1000 | Loss: 0.00121282
Iteration 57/1000 | Loss: 0.00026507
Iteration 58/1000 | Loss: 0.00184333
Iteration 59/1000 | Loss: 0.00075631
Iteration 60/1000 | Loss: 0.00065193
Iteration 61/1000 | Loss: 0.00079248
Iteration 62/1000 | Loss: 0.00064153
Iteration 63/1000 | Loss: 0.00132731
Iteration 64/1000 | Loss: 0.00148614
Iteration 65/1000 | Loss: 0.00116139
Iteration 66/1000 | Loss: 0.00121348
Iteration 67/1000 | Loss: 0.00087519
Iteration 68/1000 | Loss: 0.00100719
Iteration 69/1000 | Loss: 0.00081505
Iteration 70/1000 | Loss: 0.00050593
Iteration 71/1000 | Loss: 0.00013486
Iteration 72/1000 | Loss: 0.00031228
Iteration 73/1000 | Loss: 0.00011346
Iteration 74/1000 | Loss: 0.00007323
Iteration 75/1000 | Loss: 0.00042824
Iteration 76/1000 | Loss: 0.00022147
Iteration 77/1000 | Loss: 0.00021679
Iteration 78/1000 | Loss: 0.00005300
Iteration 79/1000 | Loss: 0.00049236
Iteration 80/1000 | Loss: 0.00013405
Iteration 81/1000 | Loss: 0.00037180
Iteration 82/1000 | Loss: 0.00097133
Iteration 83/1000 | Loss: 0.00006792
Iteration 84/1000 | Loss: 0.00061537
Iteration 85/1000 | Loss: 0.00059848
Iteration 86/1000 | Loss: 0.00033223
Iteration 87/1000 | Loss: 0.00032108
Iteration 88/1000 | Loss: 0.00012921
Iteration 89/1000 | Loss: 0.00020767
Iteration 90/1000 | Loss: 0.00010189
Iteration 91/1000 | Loss: 0.00021069
Iteration 92/1000 | Loss: 0.00032091
Iteration 93/1000 | Loss: 0.00032452
Iteration 94/1000 | Loss: 0.00018400
Iteration 95/1000 | Loss: 0.00022672
Iteration 96/1000 | Loss: 0.00031906
Iteration 97/1000 | Loss: 0.00025745
Iteration 98/1000 | Loss: 0.00014930
Iteration 99/1000 | Loss: 0.00019114
Iteration 100/1000 | Loss: 0.00014089
Iteration 101/1000 | Loss: 0.00023239
Iteration 102/1000 | Loss: 0.00018768
Iteration 103/1000 | Loss: 0.00026743
Iteration 104/1000 | Loss: 0.00012071
Iteration 105/1000 | Loss: 0.00020108
Iteration 106/1000 | Loss: 0.00006322
Iteration 107/1000 | Loss: 0.00004527
Iteration 108/1000 | Loss: 0.00008753
Iteration 109/1000 | Loss: 0.00003458
Iteration 110/1000 | Loss: 0.00003312
Iteration 111/1000 | Loss: 0.00003244
Iteration 112/1000 | Loss: 0.00003178
Iteration 113/1000 | Loss: 0.00003107
Iteration 114/1000 | Loss: 0.00003032
Iteration 115/1000 | Loss: 0.00002987
Iteration 116/1000 | Loss: 0.00002958
Iteration 117/1000 | Loss: 0.00002954
Iteration 118/1000 | Loss: 0.00002950
Iteration 119/1000 | Loss: 0.00002948
Iteration 120/1000 | Loss: 0.00002943
Iteration 121/1000 | Loss: 0.00002941
Iteration 122/1000 | Loss: 0.00002938
Iteration 123/1000 | Loss: 0.00002937
Iteration 124/1000 | Loss: 0.00002937
Iteration 125/1000 | Loss: 0.00002937
Iteration 126/1000 | Loss: 0.00002936
Iteration 127/1000 | Loss: 0.00002935
Iteration 128/1000 | Loss: 0.00002934
Iteration 129/1000 | Loss: 0.00002915
Iteration 130/1000 | Loss: 0.00002909
Iteration 131/1000 | Loss: 0.00002906
Iteration 132/1000 | Loss: 0.00002906
Iteration 133/1000 | Loss: 0.00002905
Iteration 134/1000 | Loss: 0.00002905
Iteration 135/1000 | Loss: 0.00002904
Iteration 136/1000 | Loss: 0.00002904
Iteration 137/1000 | Loss: 0.00002903
Iteration 138/1000 | Loss: 0.00002903
Iteration 139/1000 | Loss: 0.00002902
Iteration 140/1000 | Loss: 0.00002902
Iteration 141/1000 | Loss: 0.00002902
Iteration 142/1000 | Loss: 0.00002902
Iteration 143/1000 | Loss: 0.00002902
Iteration 144/1000 | Loss: 0.00002902
Iteration 145/1000 | Loss: 0.00002902
Iteration 146/1000 | Loss: 0.00002902
Iteration 147/1000 | Loss: 0.00002902
Iteration 148/1000 | Loss: 0.00002901
Iteration 149/1000 | Loss: 0.00002900
Iteration 150/1000 | Loss: 0.00002899
Iteration 151/1000 | Loss: 0.00002899
Iteration 152/1000 | Loss: 0.00002898
Iteration 153/1000 | Loss: 0.00002896
Iteration 154/1000 | Loss: 0.00002896
Iteration 155/1000 | Loss: 0.00002896
Iteration 156/1000 | Loss: 0.00002896
Iteration 157/1000 | Loss: 0.00002896
Iteration 158/1000 | Loss: 0.00002896
Iteration 159/1000 | Loss: 0.00002896
Iteration 160/1000 | Loss: 0.00002896
Iteration 161/1000 | Loss: 0.00002895
Iteration 162/1000 | Loss: 0.00002895
Iteration 163/1000 | Loss: 0.00002895
Iteration 164/1000 | Loss: 0.00002895
Iteration 165/1000 | Loss: 0.00002895
Iteration 166/1000 | Loss: 0.00002895
Iteration 167/1000 | Loss: 0.00002895
Iteration 168/1000 | Loss: 0.00002895
Iteration 169/1000 | Loss: 0.00002895
Iteration 170/1000 | Loss: 0.00002894
Iteration 171/1000 | Loss: 0.00002893
Iteration 172/1000 | Loss: 0.00002893
Iteration 173/1000 | Loss: 0.00002893
Iteration 174/1000 | Loss: 0.00002893
Iteration 175/1000 | Loss: 0.00002892
Iteration 176/1000 | Loss: 0.00002892
Iteration 177/1000 | Loss: 0.00002892
Iteration 178/1000 | Loss: 0.00002892
Iteration 179/1000 | Loss: 0.00002892
Iteration 180/1000 | Loss: 0.00002891
Iteration 181/1000 | Loss: 0.00002891
Iteration 182/1000 | Loss: 0.00002891
Iteration 183/1000 | Loss: 0.00002891
Iteration 184/1000 | Loss: 0.00002890
Iteration 185/1000 | Loss: 0.00002890
Iteration 186/1000 | Loss: 0.00002890
Iteration 187/1000 | Loss: 0.00002889
Iteration 188/1000 | Loss: 0.00002889
Iteration 189/1000 | Loss: 0.00002889
Iteration 190/1000 | Loss: 0.00002888
Iteration 191/1000 | Loss: 0.00002887
Iteration 192/1000 | Loss: 0.00002887
Iteration 193/1000 | Loss: 0.00002887
Iteration 194/1000 | Loss: 0.00002887
Iteration 195/1000 | Loss: 0.00002886
Iteration 196/1000 | Loss: 0.00002886
Iteration 197/1000 | Loss: 0.00002885
Iteration 198/1000 | Loss: 0.00002884
Iteration 199/1000 | Loss: 0.00002884
Iteration 200/1000 | Loss: 0.00002884
Iteration 201/1000 | Loss: 0.00002884
Iteration 202/1000 | Loss: 0.00002884
Iteration 203/1000 | Loss: 0.00002883
Iteration 204/1000 | Loss: 0.00002883
Iteration 205/1000 | Loss: 0.00002883
Iteration 206/1000 | Loss: 0.00002883
Iteration 207/1000 | Loss: 0.00002883
Iteration 208/1000 | Loss: 0.00002883
Iteration 209/1000 | Loss: 0.00002883
Iteration 210/1000 | Loss: 0.00002883
Iteration 211/1000 | Loss: 0.00002883
Iteration 212/1000 | Loss: 0.00002882
Iteration 213/1000 | Loss: 0.00002882
Iteration 214/1000 | Loss: 0.00002882
Iteration 215/1000 | Loss: 0.00002882
Iteration 216/1000 | Loss: 0.00002882
Iteration 217/1000 | Loss: 0.00002881
Iteration 218/1000 | Loss: 0.00002881
Iteration 219/1000 | Loss: 0.00002881
Iteration 220/1000 | Loss: 0.00002881
Iteration 221/1000 | Loss: 0.00002881
Iteration 222/1000 | Loss: 0.00002881
Iteration 223/1000 | Loss: 0.00002881
Iteration 224/1000 | Loss: 0.00002881
Iteration 225/1000 | Loss: 0.00002881
Iteration 226/1000 | Loss: 0.00002881
Iteration 227/1000 | Loss: 0.00002881
Iteration 228/1000 | Loss: 0.00002881
Iteration 229/1000 | Loss: 0.00002881
Iteration 230/1000 | Loss: 0.00002880
Iteration 231/1000 | Loss: 0.00002880
Iteration 232/1000 | Loss: 0.00002880
Iteration 233/1000 | Loss: 0.00002880
Iteration 234/1000 | Loss: 0.00002880
Iteration 235/1000 | Loss: 0.00002880
Iteration 236/1000 | Loss: 0.00002880
Iteration 237/1000 | Loss: 0.00002879
Iteration 238/1000 | Loss: 0.00002879
Iteration 239/1000 | Loss: 0.00002879
Iteration 240/1000 | Loss: 0.00002878
Iteration 241/1000 | Loss: 0.00002878
Iteration 242/1000 | Loss: 0.00002877
Iteration 243/1000 | Loss: 0.00002877
Iteration 244/1000 | Loss: 0.00002877
Iteration 245/1000 | Loss: 0.00002877
Iteration 246/1000 | Loss: 0.00002877
Iteration 247/1000 | Loss: 0.00002877
Iteration 248/1000 | Loss: 0.00002877
Iteration 249/1000 | Loss: 0.00002877
Iteration 250/1000 | Loss: 0.00002877
Iteration 251/1000 | Loss: 0.00002877
Iteration 252/1000 | Loss: 0.00002876
Iteration 253/1000 | Loss: 0.00002876
Iteration 254/1000 | Loss: 0.00002876
Iteration 255/1000 | Loss: 0.00002876
Iteration 256/1000 | Loss: 0.00002876
Iteration 257/1000 | Loss: 0.00002876
Iteration 258/1000 | Loss: 0.00002875
Iteration 259/1000 | Loss: 0.00002875
Iteration 260/1000 | Loss: 0.00002875
Iteration 261/1000 | Loss: 0.00002875
Iteration 262/1000 | Loss: 0.00002875
Iteration 263/1000 | Loss: 0.00002875
Iteration 264/1000 | Loss: 0.00002875
Iteration 265/1000 | Loss: 0.00002875
Iteration 266/1000 | Loss: 0.00002875
Iteration 267/1000 | Loss: 0.00002874
Iteration 268/1000 | Loss: 0.00002874
Iteration 269/1000 | Loss: 0.00002874
Iteration 270/1000 | Loss: 0.00002874
Iteration 271/1000 | Loss: 0.00002874
Iteration 272/1000 | Loss: 0.00002874
Iteration 273/1000 | Loss: 0.00002874
Iteration 274/1000 | Loss: 0.00002874
Iteration 275/1000 | Loss: 0.00002874
Iteration 276/1000 | Loss: 0.00002874
Iteration 277/1000 | Loss: 0.00002874
Iteration 278/1000 | Loss: 0.00002874
Iteration 279/1000 | Loss: 0.00002874
Iteration 280/1000 | Loss: 0.00002874
Iteration 281/1000 | Loss: 0.00002874
Iteration 282/1000 | Loss: 0.00002874
Iteration 283/1000 | Loss: 0.00002873
Iteration 284/1000 | Loss: 0.00002873
Iteration 285/1000 | Loss: 0.00002873
Iteration 286/1000 | Loss: 0.00002873
Iteration 287/1000 | Loss: 0.00002873
Iteration 288/1000 | Loss: 0.00002873
Iteration 289/1000 | Loss: 0.00002873
Iteration 290/1000 | Loss: 0.00002873
Iteration 291/1000 | Loss: 0.00002873
Iteration 292/1000 | Loss: 0.00002872
Iteration 293/1000 | Loss: 0.00002872
Iteration 294/1000 | Loss: 0.00002872
Iteration 295/1000 | Loss: 0.00002872
Iteration 296/1000 | Loss: 0.00002872
Iteration 297/1000 | Loss: 0.00002872
Iteration 298/1000 | Loss: 0.00002872
Iteration 299/1000 | Loss: 0.00002872
Iteration 300/1000 | Loss: 0.00002872
Iteration 301/1000 | Loss: 0.00002872
Iteration 302/1000 | Loss: 0.00002872
Iteration 303/1000 | Loss: 0.00002871
Iteration 304/1000 | Loss: 0.00002871
Iteration 305/1000 | Loss: 0.00002871
Iteration 306/1000 | Loss: 0.00002871
Iteration 307/1000 | Loss: 0.00002871
Iteration 308/1000 | Loss: 0.00002871
Iteration 309/1000 | Loss: 0.00002871
Iteration 310/1000 | Loss: 0.00002871
Iteration 311/1000 | Loss: 0.00002871
Iteration 312/1000 | Loss: 0.00002871
Iteration 313/1000 | Loss: 0.00002871
Iteration 314/1000 | Loss: 0.00002871
Iteration 315/1000 | Loss: 0.00002871
Iteration 316/1000 | Loss: 0.00002871
Iteration 317/1000 | Loss: 0.00002871
Iteration 318/1000 | Loss: 0.00002871
Iteration 319/1000 | Loss: 0.00002871
Iteration 320/1000 | Loss: 0.00002871
Iteration 321/1000 | Loss: 0.00002871
Iteration 322/1000 | Loss: 0.00002870
Iteration 323/1000 | Loss: 0.00002870
Iteration 324/1000 | Loss: 0.00002870
Iteration 325/1000 | Loss: 0.00002870
Iteration 326/1000 | Loss: 0.00002870
Iteration 327/1000 | Loss: 0.00002870
Iteration 328/1000 | Loss: 0.00002870
Iteration 329/1000 | Loss: 0.00002870
Iteration 330/1000 | Loss: 0.00002870
Iteration 331/1000 | Loss: 0.00002870
Iteration 332/1000 | Loss: 0.00002870
Iteration 333/1000 | Loss: 0.00002870
Iteration 334/1000 | Loss: 0.00002870
Iteration 335/1000 | Loss: 0.00002870
Iteration 336/1000 | Loss: 0.00002870
Iteration 337/1000 | Loss: 0.00002870
Iteration 338/1000 | Loss: 0.00002870
Iteration 339/1000 | Loss: 0.00002870
Iteration 340/1000 | Loss: 0.00002870
Iteration 341/1000 | Loss: 0.00002869
Iteration 342/1000 | Loss: 0.00002869
Iteration 343/1000 | Loss: 0.00002869
Iteration 344/1000 | Loss: 0.00002869
Iteration 345/1000 | Loss: 0.00002869
Iteration 346/1000 | Loss: 0.00002869
Iteration 347/1000 | Loss: 0.00002869
Iteration 348/1000 | Loss: 0.00002869
Iteration 349/1000 | Loss: 0.00002869
Iteration 350/1000 | Loss: 0.00002869
Iteration 351/1000 | Loss: 0.00002869
Iteration 352/1000 | Loss: 0.00002869
Iteration 353/1000 | Loss: 0.00002869
Iteration 354/1000 | Loss: 0.00002869
Iteration 355/1000 | Loss: 0.00002869
Iteration 356/1000 | Loss: 0.00002869
Iteration 357/1000 | Loss: 0.00002869
Iteration 358/1000 | Loss: 0.00002869
Iteration 359/1000 | Loss: 0.00002869
Iteration 360/1000 | Loss: 0.00002869
Iteration 361/1000 | Loss: 0.00002869
Iteration 362/1000 | Loss: 0.00002869
Iteration 363/1000 | Loss: 0.00002869
Iteration 364/1000 | Loss: 0.00002869
Iteration 365/1000 | Loss: 0.00002869
Iteration 366/1000 | Loss: 0.00002869
Iteration 367/1000 | Loss: 0.00002869
Iteration 368/1000 | Loss: 0.00002869
Iteration 369/1000 | Loss: 0.00002869
Iteration 370/1000 | Loss: 0.00002869
Iteration 371/1000 | Loss: 0.00002869
Iteration 372/1000 | Loss: 0.00002869
Iteration 373/1000 | Loss: 0.00002869
Iteration 374/1000 | Loss: 0.00002869
Iteration 375/1000 | Loss: 0.00002869
Iteration 376/1000 | Loss: 0.00002869
Iteration 377/1000 | Loss: 0.00002869
Iteration 378/1000 | Loss: 0.00002869
Iteration 379/1000 | Loss: 0.00002869
Iteration 380/1000 | Loss: 0.00002869
Iteration 381/1000 | Loss: 0.00002869
Iteration 382/1000 | Loss: 0.00002869
Iteration 383/1000 | Loss: 0.00002869
Iteration 384/1000 | Loss: 0.00002869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 384. Stopping optimization.
Last 5 losses: [2.8686337827821262e-05, 2.8686337827821262e-05, 2.8686337827821262e-05, 2.8686337827821262e-05, 2.8686337827821262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8686337827821262e-05

Optimization complete. Final v2v error: 3.3358492851257324 mm

Highest mean error: 12.132942199707031 mm for frame 100

Lowest mean error: 2.613361358642578 mm for frame 54

Saving results

Total time: 197.78041458129883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00701872
Iteration 2/25 | Loss: 0.00141627
Iteration 3/25 | Loss: 0.00130650
Iteration 4/25 | Loss: 0.00127643
Iteration 5/25 | Loss: 0.00126784
Iteration 6/25 | Loss: 0.00126748
Iteration 7/25 | Loss: 0.00126410
Iteration 8/25 | Loss: 0.00126172
Iteration 9/25 | Loss: 0.00126303
Iteration 10/25 | Loss: 0.00126093
Iteration 11/25 | Loss: 0.00126267
Iteration 12/25 | Loss: 0.00125999
Iteration 13/25 | Loss: 0.00125985
Iteration 14/25 | Loss: 0.00125985
Iteration 15/25 | Loss: 0.00125985
Iteration 16/25 | Loss: 0.00125985
Iteration 17/25 | Loss: 0.00125984
Iteration 18/25 | Loss: 0.00125984
Iteration 19/25 | Loss: 0.00125984
Iteration 20/25 | Loss: 0.00125984
Iteration 21/25 | Loss: 0.00125984
Iteration 22/25 | Loss: 0.00125984
Iteration 23/25 | Loss: 0.00125984
Iteration 24/25 | Loss: 0.00125984
Iteration 25/25 | Loss: 0.00125982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63614845
Iteration 2/25 | Loss: 0.00083261
Iteration 3/25 | Loss: 0.00083260
Iteration 4/25 | Loss: 0.00083260
Iteration 5/25 | Loss: 0.00083260
Iteration 6/25 | Loss: 0.00083260
Iteration 7/25 | Loss: 0.00083260
Iteration 8/25 | Loss: 0.00083260
Iteration 9/25 | Loss: 0.00083260
Iteration 10/25 | Loss: 0.00083260
Iteration 11/25 | Loss: 0.00083260
Iteration 12/25 | Loss: 0.00083260
Iteration 13/25 | Loss: 0.00083260
Iteration 14/25 | Loss: 0.00083260
Iteration 15/25 | Loss: 0.00083260
Iteration 16/25 | Loss: 0.00083260
Iteration 17/25 | Loss: 0.00083260
Iteration 18/25 | Loss: 0.00083260
Iteration 19/25 | Loss: 0.00083260
Iteration 20/25 | Loss: 0.00083260
Iteration 21/25 | Loss: 0.00083260
Iteration 22/25 | Loss: 0.00083260
Iteration 23/25 | Loss: 0.00083260
Iteration 24/25 | Loss: 0.00083260
Iteration 25/25 | Loss: 0.00083260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083260
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00004566
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001696
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00028209
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001466
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001457
Iteration 17/1000 | Loss: 0.00001456
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001454
Iteration 20/1000 | Loss: 0.00001453
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001426
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001414
Iteration 55/1000 | Loss: 0.00001414
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001413
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001409
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001407
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001405
Iteration 77/1000 | Loss: 0.00001405
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001401
Iteration 88/1000 | Loss: 0.00001401
Iteration 89/1000 | Loss: 0.00001401
Iteration 90/1000 | Loss: 0.00001400
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001399
Iteration 96/1000 | Loss: 0.00001399
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001396
Iteration 121/1000 | Loss: 0.00001396
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001396
Iteration 129/1000 | Loss: 0.00001396
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001396
Iteration 143/1000 | Loss: 0.00001396
Iteration 144/1000 | Loss: 0.00001396
Iteration 145/1000 | Loss: 0.00001396
Iteration 146/1000 | Loss: 0.00001396
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001396
Iteration 149/1000 | Loss: 0.00001396
Iteration 150/1000 | Loss: 0.00001396
Iteration 151/1000 | Loss: 0.00001396
Iteration 152/1000 | Loss: 0.00001396
Iteration 153/1000 | Loss: 0.00001396
Iteration 154/1000 | Loss: 0.00001396
Iteration 155/1000 | Loss: 0.00001396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3960711839899886e-05, 1.3960711839899886e-05, 1.3960711839899886e-05, 1.3960711839899886e-05, 1.3960711839899886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3960711839899886e-05

Optimization complete. Final v2v error: 3.191354751586914 mm

Highest mean error: 4.094468116760254 mm for frame 140

Lowest mean error: 2.9582881927490234 mm for frame 26

Saving results

Total time: 64.01226782798767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010583
Iteration 2/25 | Loss: 0.00251460
Iteration 3/25 | Loss: 0.00220930
Iteration 4/25 | Loss: 0.00213814
Iteration 5/25 | Loss: 0.00211120
Iteration 6/25 | Loss: 0.00207947
Iteration 7/25 | Loss: 0.00205828
Iteration 8/25 | Loss: 0.00202063
Iteration 9/25 | Loss: 0.00200709
Iteration 10/25 | Loss: 0.00200190
Iteration 11/25 | Loss: 0.00200232
Iteration 12/25 | Loss: 0.00199706
Iteration 13/25 | Loss: 0.00199783
Iteration 14/25 | Loss: 0.00199407
Iteration 15/25 | Loss: 0.00199314
Iteration 16/25 | Loss: 0.00199284
Iteration 17/25 | Loss: 0.00199275
Iteration 18/25 | Loss: 0.00199263
Iteration 19/25 | Loss: 0.00199254
Iteration 20/25 | Loss: 0.00199245
Iteration 21/25 | Loss: 0.00199241
Iteration 22/25 | Loss: 0.00199240
Iteration 23/25 | Loss: 0.00199240
Iteration 24/25 | Loss: 0.00199240
Iteration 25/25 | Loss: 0.00199240

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29134631
Iteration 2/25 | Loss: 0.00956607
Iteration 3/25 | Loss: 0.00956607
Iteration 4/25 | Loss: 0.00956607
Iteration 5/25 | Loss: 0.00956607
Iteration 6/25 | Loss: 0.00956607
Iteration 7/25 | Loss: 0.00956607
Iteration 8/25 | Loss: 0.00956607
Iteration 9/25 | Loss: 0.00956606
Iteration 10/25 | Loss: 0.00956606
Iteration 11/25 | Loss: 0.00956606
Iteration 12/25 | Loss: 0.00956606
Iteration 13/25 | Loss: 0.00956606
Iteration 14/25 | Loss: 0.00956606
Iteration 15/25 | Loss: 0.00956606
Iteration 16/25 | Loss: 0.00956606
Iteration 17/25 | Loss: 0.00956606
Iteration 18/25 | Loss: 0.00956606
Iteration 19/25 | Loss: 0.00956606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.009566063992679119, 0.009566063992679119, 0.009566063992679119, 0.009566063992679119, 0.009566063992679119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009566063992679119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00956606
Iteration 2/1000 | Loss: 0.00112416
Iteration 3/1000 | Loss: 0.00142571
Iteration 4/1000 | Loss: 0.00062547
Iteration 5/1000 | Loss: 0.00057044
Iteration 6/1000 | Loss: 0.00052827
Iteration 7/1000 | Loss: 0.00048438
Iteration 8/1000 | Loss: 0.00046108
Iteration 9/1000 | Loss: 0.00044748
Iteration 10/1000 | Loss: 0.00043365
Iteration 11/1000 | Loss: 0.00042554
Iteration 12/1000 | Loss: 0.00042029
Iteration 13/1000 | Loss: 0.00041584
Iteration 14/1000 | Loss: 0.00041213
Iteration 15/1000 | Loss: 0.00040946
Iteration 16/1000 | Loss: 0.00040751
Iteration 17/1000 | Loss: 0.00040580
Iteration 18/1000 | Loss: 0.00040452
Iteration 19/1000 | Loss: 0.00040346
Iteration 20/1000 | Loss: 0.00040254
Iteration 21/1000 | Loss: 0.00040184
Iteration 22/1000 | Loss: 0.00040120
Iteration 23/1000 | Loss: 0.00040070
Iteration 24/1000 | Loss: 0.00040033
Iteration 25/1000 | Loss: 0.00039989
Iteration 26/1000 | Loss: 0.00039958
Iteration 27/1000 | Loss: 0.00039928
Iteration 28/1000 | Loss: 0.00039901
Iteration 29/1000 | Loss: 0.00039873
Iteration 30/1000 | Loss: 0.00039833
Iteration 31/1000 | Loss: 0.00039763
Iteration 32/1000 | Loss: 0.00039615
Iteration 33/1000 | Loss: 0.00038948
Iteration 34/1000 | Loss: 0.00038048
Iteration 35/1000 | Loss: 0.00036764
Iteration 36/1000 | Loss: 0.00036030
Iteration 37/1000 | Loss: 0.00035297
Iteration 38/1000 | Loss: 0.00034712
Iteration 39/1000 | Loss: 0.00034007
Iteration 40/1000 | Loss: 0.00033350
Iteration 41/1000 | Loss: 0.00032879
Iteration 42/1000 | Loss: 0.00032445
Iteration 43/1000 | Loss: 0.00032112
Iteration 44/1000 | Loss: 0.00031733
Iteration 45/1000 | Loss: 0.00031436
Iteration 46/1000 | Loss: 0.00031103
Iteration 47/1000 | Loss: 0.00030888
Iteration 48/1000 | Loss: 0.00030741
Iteration 49/1000 | Loss: 0.00030607
Iteration 50/1000 | Loss: 0.00030490
Iteration 51/1000 | Loss: 0.00030425
Iteration 52/1000 | Loss: 0.00030326
Iteration 53/1000 | Loss: 0.00030240
Iteration 54/1000 | Loss: 0.00030190
Iteration 55/1000 | Loss: 0.00030157
Iteration 56/1000 | Loss: 0.00030121
Iteration 57/1000 | Loss: 0.00030086
Iteration 58/1000 | Loss: 0.00030060
Iteration 59/1000 | Loss: 0.00030038
Iteration 60/1000 | Loss: 0.00030014
Iteration 61/1000 | Loss: 0.00029986
Iteration 62/1000 | Loss: 0.00029956
Iteration 63/1000 | Loss: 0.00029916
Iteration 64/1000 | Loss: 0.00029866
Iteration 65/1000 | Loss: 0.00029805
Iteration 66/1000 | Loss: 0.00029762
Iteration 67/1000 | Loss: 0.00029709
Iteration 68/1000 | Loss: 0.00029652
Iteration 69/1000 | Loss: 0.00029598
Iteration 70/1000 | Loss: 0.00029546
Iteration 71/1000 | Loss: 0.00029472
Iteration 72/1000 | Loss: 0.00029376
Iteration 73/1000 | Loss: 0.00083966
Iteration 74/1000 | Loss: 0.00043216
Iteration 75/1000 | Loss: 0.00029399
Iteration 76/1000 | Loss: 0.00028847
Iteration 77/1000 | Loss: 0.01785768
Iteration 78/1000 | Loss: 0.00249005
Iteration 79/1000 | Loss: 0.00029822
Iteration 80/1000 | Loss: 0.00029165
Iteration 81/1000 | Loss: 0.01164131
Iteration 82/1000 | Loss: 0.00134385
Iteration 83/1000 | Loss: 0.00030025
Iteration 84/1000 | Loss: 0.00038382
Iteration 85/1000 | Loss: 0.01027426
Iteration 86/1000 | Loss: 0.01360665
Iteration 87/1000 | Loss: 0.00363220
Iteration 88/1000 | Loss: 0.00208689
Iteration 89/1000 | Loss: 0.00083198
Iteration 90/1000 | Loss: 0.00041245
Iteration 91/1000 | Loss: 0.00118793
Iteration 92/1000 | Loss: 0.00094383
Iteration 93/1000 | Loss: 0.00040287
Iteration 94/1000 | Loss: 0.00041628
Iteration 95/1000 | Loss: 0.00078501
Iteration 96/1000 | Loss: 0.00042544
Iteration 97/1000 | Loss: 0.00013696
Iteration 98/1000 | Loss: 0.00010667
Iteration 99/1000 | Loss: 0.00009061
Iteration 100/1000 | Loss: 0.00020400
Iteration 101/1000 | Loss: 0.00015457
Iteration 102/1000 | Loss: 0.00006052
Iteration 103/1000 | Loss: 0.00008840
Iteration 104/1000 | Loss: 0.00005014
Iteration 105/1000 | Loss: 0.00004495
Iteration 106/1000 | Loss: 0.00004363
Iteration 107/1000 | Loss: 0.00003802
Iteration 108/1000 | Loss: 0.00003868
Iteration 109/1000 | Loss: 0.00003380
Iteration 110/1000 | Loss: 0.00003861
Iteration 111/1000 | Loss: 0.00003526
Iteration 112/1000 | Loss: 0.00002792
Iteration 113/1000 | Loss: 0.00002872
Iteration 114/1000 | Loss: 0.00002641
Iteration 115/1000 | Loss: 0.00003164
Iteration 116/1000 | Loss: 0.00002740
Iteration 117/1000 | Loss: 0.00057460
Iteration 118/1000 | Loss: 0.00019903
Iteration 119/1000 | Loss: 0.00055939
Iteration 120/1000 | Loss: 0.00005996
Iteration 121/1000 | Loss: 0.00022689
Iteration 122/1000 | Loss: 0.00017566
Iteration 123/1000 | Loss: 0.00002570
Iteration 124/1000 | Loss: 0.00002304
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001887
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001788
Iteration 131/1000 | Loss: 0.00001786
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001697
Iteration 136/1000 | Loss: 0.00001678
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001664
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001653
Iteration 144/1000 | Loss: 0.00001653
Iteration 145/1000 | Loss: 0.00001653
Iteration 146/1000 | Loss: 0.00001652
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001652
Iteration 150/1000 | Loss: 0.00001652
Iteration 151/1000 | Loss: 0.00001652
Iteration 152/1000 | Loss: 0.00001652
Iteration 153/1000 | Loss: 0.00001651
Iteration 154/1000 | Loss: 0.00001651
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001648
Iteration 157/1000 | Loss: 0.00001648
Iteration 158/1000 | Loss: 0.00001647
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001646
Iteration 161/1000 | Loss: 0.00001646
Iteration 162/1000 | Loss: 0.00001646
Iteration 163/1000 | Loss: 0.00001646
Iteration 164/1000 | Loss: 0.00001645
Iteration 165/1000 | Loss: 0.00001645
Iteration 166/1000 | Loss: 0.00001644
Iteration 167/1000 | Loss: 0.00001644
Iteration 168/1000 | Loss: 0.00001643
Iteration 169/1000 | Loss: 0.00001643
Iteration 170/1000 | Loss: 0.00001642
Iteration 171/1000 | Loss: 0.00001642
Iteration 172/1000 | Loss: 0.00001638
Iteration 173/1000 | Loss: 0.00001636
Iteration 174/1000 | Loss: 0.00001633
Iteration 175/1000 | Loss: 0.00001631
Iteration 176/1000 | Loss: 0.00001631
Iteration 177/1000 | Loss: 0.00001628
Iteration 178/1000 | Loss: 0.00001627
Iteration 179/1000 | Loss: 0.00001621
Iteration 180/1000 | Loss: 0.00001620
Iteration 181/1000 | Loss: 0.00001620
Iteration 182/1000 | Loss: 0.00001619
Iteration 183/1000 | Loss: 0.00001619
Iteration 184/1000 | Loss: 0.00001618
Iteration 185/1000 | Loss: 0.00001617
Iteration 186/1000 | Loss: 0.00001617
Iteration 187/1000 | Loss: 0.00001616
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001615
Iteration 191/1000 | Loss: 0.00001614
Iteration 192/1000 | Loss: 0.00001614
Iteration 193/1000 | Loss: 0.00001613
Iteration 194/1000 | Loss: 0.00001610
Iteration 195/1000 | Loss: 0.00001609
Iteration 196/1000 | Loss: 0.00001609
Iteration 197/1000 | Loss: 0.00001608
Iteration 198/1000 | Loss: 0.00001608
Iteration 199/1000 | Loss: 0.00001607
Iteration 200/1000 | Loss: 0.00001606
Iteration 201/1000 | Loss: 0.00001606
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001605
Iteration 204/1000 | Loss: 0.00001605
Iteration 205/1000 | Loss: 0.00001604
Iteration 206/1000 | Loss: 0.00001603
Iteration 207/1000 | Loss: 0.00001602
Iteration 208/1000 | Loss: 0.00001602
Iteration 209/1000 | Loss: 0.00001601
Iteration 210/1000 | Loss: 0.00001601
Iteration 211/1000 | Loss: 0.00001601
Iteration 212/1000 | Loss: 0.00001601
Iteration 213/1000 | Loss: 0.00001601
Iteration 214/1000 | Loss: 0.00001601
Iteration 215/1000 | Loss: 0.00001601
Iteration 216/1000 | Loss: 0.00001600
Iteration 217/1000 | Loss: 0.00001600
Iteration 218/1000 | Loss: 0.00001600
Iteration 219/1000 | Loss: 0.00001600
Iteration 220/1000 | Loss: 0.00001600
Iteration 221/1000 | Loss: 0.00001600
Iteration 222/1000 | Loss: 0.00001600
Iteration 223/1000 | Loss: 0.00001600
Iteration 224/1000 | Loss: 0.00001600
Iteration 225/1000 | Loss: 0.00001600
Iteration 226/1000 | Loss: 0.00001599
Iteration 227/1000 | Loss: 0.00001598
Iteration 228/1000 | Loss: 0.00001598
Iteration 229/1000 | Loss: 0.00001598
Iteration 230/1000 | Loss: 0.00001598
Iteration 231/1000 | Loss: 0.00001598
Iteration 232/1000 | Loss: 0.00001598
Iteration 233/1000 | Loss: 0.00001598
Iteration 234/1000 | Loss: 0.00001597
Iteration 235/1000 | Loss: 0.00001597
Iteration 236/1000 | Loss: 0.00001596
Iteration 237/1000 | Loss: 0.00001596
Iteration 238/1000 | Loss: 0.00001596
Iteration 239/1000 | Loss: 0.00001596
Iteration 240/1000 | Loss: 0.00001595
Iteration 241/1000 | Loss: 0.00001595
Iteration 242/1000 | Loss: 0.00001595
Iteration 243/1000 | Loss: 0.00001595
Iteration 244/1000 | Loss: 0.00001595
Iteration 245/1000 | Loss: 0.00001595
Iteration 246/1000 | Loss: 0.00001595
Iteration 247/1000 | Loss: 0.00001594
Iteration 248/1000 | Loss: 0.00001594
Iteration 249/1000 | Loss: 0.00001594
Iteration 250/1000 | Loss: 0.00001594
Iteration 251/1000 | Loss: 0.00001594
Iteration 252/1000 | Loss: 0.00001593
Iteration 253/1000 | Loss: 0.00001593
Iteration 254/1000 | Loss: 0.00001593
Iteration 255/1000 | Loss: 0.00001593
Iteration 256/1000 | Loss: 0.00001593
Iteration 257/1000 | Loss: 0.00001592
Iteration 258/1000 | Loss: 0.00001592
Iteration 259/1000 | Loss: 0.00001592
Iteration 260/1000 | Loss: 0.00001592
Iteration 261/1000 | Loss: 0.00001592
Iteration 262/1000 | Loss: 0.00001592
Iteration 263/1000 | Loss: 0.00001592
Iteration 264/1000 | Loss: 0.00001592
Iteration 265/1000 | Loss: 0.00001591
Iteration 266/1000 | Loss: 0.00001591
Iteration 267/1000 | Loss: 0.00001591
Iteration 268/1000 | Loss: 0.00001591
Iteration 269/1000 | Loss: 0.00001591
Iteration 270/1000 | Loss: 0.00001591
Iteration 271/1000 | Loss: 0.00001591
Iteration 272/1000 | Loss: 0.00001591
Iteration 273/1000 | Loss: 0.00001591
Iteration 274/1000 | Loss: 0.00001591
Iteration 275/1000 | Loss: 0.00001591
Iteration 276/1000 | Loss: 0.00001591
Iteration 277/1000 | Loss: 0.00001591
Iteration 278/1000 | Loss: 0.00001591
Iteration 279/1000 | Loss: 0.00001590
Iteration 280/1000 | Loss: 0.00001590
Iteration 281/1000 | Loss: 0.00001590
Iteration 282/1000 | Loss: 0.00001590
Iteration 283/1000 | Loss: 0.00001590
Iteration 284/1000 | Loss: 0.00001590
Iteration 285/1000 | Loss: 0.00001590
Iteration 286/1000 | Loss: 0.00001590
Iteration 287/1000 | Loss: 0.00001590
Iteration 288/1000 | Loss: 0.00001590
Iteration 289/1000 | Loss: 0.00001590
Iteration 290/1000 | Loss: 0.00001590
Iteration 291/1000 | Loss: 0.00001589
Iteration 292/1000 | Loss: 0.00001589
Iteration 293/1000 | Loss: 0.00001589
Iteration 294/1000 | Loss: 0.00001589
Iteration 295/1000 | Loss: 0.00001589
Iteration 296/1000 | Loss: 0.00001589
Iteration 297/1000 | Loss: 0.00001589
Iteration 298/1000 | Loss: 0.00001589
Iteration 299/1000 | Loss: 0.00001589
Iteration 300/1000 | Loss: 0.00001589
Iteration 301/1000 | Loss: 0.00001588
Iteration 302/1000 | Loss: 0.00001588
Iteration 303/1000 | Loss: 0.00001588
Iteration 304/1000 | Loss: 0.00001588
Iteration 305/1000 | Loss: 0.00001588
Iteration 306/1000 | Loss: 0.00001588
Iteration 307/1000 | Loss: 0.00001588
Iteration 308/1000 | Loss: 0.00001587
Iteration 309/1000 | Loss: 0.00001587
Iteration 310/1000 | Loss: 0.00001587
Iteration 311/1000 | Loss: 0.00001587
Iteration 312/1000 | Loss: 0.00001587
Iteration 313/1000 | Loss: 0.00001587
Iteration 314/1000 | Loss: 0.00001587
Iteration 315/1000 | Loss: 0.00001587
Iteration 316/1000 | Loss: 0.00001587
Iteration 317/1000 | Loss: 0.00001587
Iteration 318/1000 | Loss: 0.00001587
Iteration 319/1000 | Loss: 0.00001587
Iteration 320/1000 | Loss: 0.00001586
Iteration 321/1000 | Loss: 0.00001586
Iteration 322/1000 | Loss: 0.00001586
Iteration 323/1000 | Loss: 0.00001586
Iteration 324/1000 | Loss: 0.00001586
Iteration 325/1000 | Loss: 0.00001586
Iteration 326/1000 | Loss: 0.00001586
Iteration 327/1000 | Loss: 0.00001586
Iteration 328/1000 | Loss: 0.00001586
Iteration 329/1000 | Loss: 0.00001586
Iteration 330/1000 | Loss: 0.00001586
Iteration 331/1000 | Loss: 0.00001586
Iteration 332/1000 | Loss: 0.00001586
Iteration 333/1000 | Loss: 0.00001586
Iteration 334/1000 | Loss: 0.00001586
Iteration 335/1000 | Loss: 0.00001586
Iteration 336/1000 | Loss: 0.00001586
Iteration 337/1000 | Loss: 0.00001586
Iteration 338/1000 | Loss: 0.00001586
Iteration 339/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 339. Stopping optimization.
Last 5 losses: [1.5864428860368207e-05, 1.5864428860368207e-05, 1.5864428860368207e-05, 1.5864428860368207e-05, 1.5864428860368207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5864428860368207e-05

Optimization complete. Final v2v error: 3.354497194290161 mm

Highest mean error: 4.146751880645752 mm for frame 191

Lowest mean error: 3.2635650634765625 mm for frame 64

Saving results

Total time: 281.7966077327728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055543
Iteration 2/25 | Loss: 0.00270556
Iteration 3/25 | Loss: 0.00200459
Iteration 4/25 | Loss: 0.00160054
Iteration 5/25 | Loss: 0.00152989
Iteration 6/25 | Loss: 0.00152256
Iteration 7/25 | Loss: 0.00145203
Iteration 8/25 | Loss: 0.00142670
Iteration 9/25 | Loss: 0.00137636
Iteration 10/25 | Loss: 0.00139055
Iteration 11/25 | Loss: 0.00134766
Iteration 12/25 | Loss: 0.00134172
Iteration 13/25 | Loss: 0.00133863
Iteration 14/25 | Loss: 0.00133545
Iteration 15/25 | Loss: 0.00133350
Iteration 16/25 | Loss: 0.00133441
Iteration 17/25 | Loss: 0.00133534
Iteration 18/25 | Loss: 0.00133448
Iteration 19/25 | Loss: 0.00133531
Iteration 20/25 | Loss: 0.00133612
Iteration 21/25 | Loss: 0.00133442
Iteration 22/25 | Loss: 0.00133431
Iteration 23/25 | Loss: 0.00133536
Iteration 24/25 | Loss: 0.00133454
Iteration 25/25 | Loss: 0.00133509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.95773315
Iteration 2/25 | Loss: 0.00117874
Iteration 3/25 | Loss: 0.00116644
Iteration 4/25 | Loss: 0.00116651
Iteration 5/25 | Loss: 0.00116098
Iteration 6/25 | Loss: 0.00116098
Iteration 7/25 | Loss: 0.00116098
Iteration 8/25 | Loss: 0.00116098
Iteration 9/25 | Loss: 0.00116098
Iteration 10/25 | Loss: 0.00116098
Iteration 11/25 | Loss: 0.00116098
Iteration 12/25 | Loss: 0.00116097
Iteration 13/25 | Loss: 0.00116097
Iteration 14/25 | Loss: 0.00116097
Iteration 15/25 | Loss: 0.00116097
Iteration 16/25 | Loss: 0.00116097
Iteration 17/25 | Loss: 0.00116097
Iteration 18/25 | Loss: 0.00116097
Iteration 19/25 | Loss: 0.00116097
Iteration 20/25 | Loss: 0.00116097
Iteration 21/25 | Loss: 0.00116097
Iteration 22/25 | Loss: 0.00116097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011609743814915419, 0.0011609743814915419, 0.0011609743814915419, 0.0011609743814915419, 0.0011609743814915419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011609743814915419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116097
Iteration 2/1000 | Loss: 0.00010860
Iteration 3/1000 | Loss: 0.00004463
Iteration 4/1000 | Loss: 0.00003182
Iteration 5/1000 | Loss: 0.00002815
Iteration 6/1000 | Loss: 0.00007203
Iteration 7/1000 | Loss: 0.00004258
Iteration 8/1000 | Loss: 0.00004860
Iteration 9/1000 | Loss: 0.00003528
Iteration 10/1000 | Loss: 0.00004105
Iteration 11/1000 | Loss: 0.00006737
Iteration 12/1000 | Loss: 0.00002830
Iteration 13/1000 | Loss: 0.00003627
Iteration 14/1000 | Loss: 0.00004219
Iteration 15/1000 | Loss: 0.00004421
Iteration 16/1000 | Loss: 0.00004171
Iteration 17/1000 | Loss: 0.00004079
Iteration 18/1000 | Loss: 0.00003356
Iteration 19/1000 | Loss: 0.00004074
Iteration 20/1000 | Loss: 0.00004123
Iteration 21/1000 | Loss: 0.00008481
Iteration 22/1000 | Loss: 0.00010264
Iteration 23/1000 | Loss: 0.00006173
Iteration 24/1000 | Loss: 0.00003449
Iteration 25/1000 | Loss: 0.00003897
Iteration 26/1000 | Loss: 0.00004489
Iteration 27/1000 | Loss: 0.00004466
Iteration 28/1000 | Loss: 0.00005671
Iteration 29/1000 | Loss: 0.00002840
Iteration 30/1000 | Loss: 0.00003731
Iteration 31/1000 | Loss: 0.00003735
Iteration 32/1000 | Loss: 0.00004016
Iteration 33/1000 | Loss: 0.00005119
Iteration 34/1000 | Loss: 0.00004954
Iteration 35/1000 | Loss: 0.00008526
Iteration 36/1000 | Loss: 0.00004470
Iteration 37/1000 | Loss: 0.00003708
Iteration 38/1000 | Loss: 0.00005632
Iteration 39/1000 | Loss: 0.00005745
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00004552
Iteration 42/1000 | Loss: 0.00002400
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00004910
Iteration 45/1000 | Loss: 0.00002215
Iteration 46/1000 | Loss: 0.00002342
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001980
Iteration 49/1000 | Loss: 0.00002433
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00005748
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001969
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001968
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001965
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001964
Iteration 64/1000 | Loss: 0.00001964
Iteration 65/1000 | Loss: 0.00001963
Iteration 66/1000 | Loss: 0.00001963
Iteration 67/1000 | Loss: 0.00001962
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001961
Iteration 70/1000 | Loss: 0.00001961
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001952
Iteration 74/1000 | Loss: 0.00001949
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00001957
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001945
Iteration 91/1000 | Loss: 0.00001945
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00001944
Iteration 96/1000 | Loss: 0.00001944
Iteration 97/1000 | Loss: 0.00001944
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001943
Iteration 100/1000 | Loss: 0.00001943
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00004979
Iteration 104/1000 | Loss: 0.00003446
Iteration 105/1000 | Loss: 0.00001965
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001933
Iteration 108/1000 | Loss: 0.00001933
Iteration 109/1000 | Loss: 0.00001933
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001931
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001931
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001931
Iteration 125/1000 | Loss: 0.00001931
Iteration 126/1000 | Loss: 0.00001931
Iteration 127/1000 | Loss: 0.00001930
Iteration 128/1000 | Loss: 0.00001930
Iteration 129/1000 | Loss: 0.00001930
Iteration 130/1000 | Loss: 0.00001930
Iteration 131/1000 | Loss: 0.00001930
Iteration 132/1000 | Loss: 0.00001930
Iteration 133/1000 | Loss: 0.00001930
Iteration 134/1000 | Loss: 0.00001930
Iteration 135/1000 | Loss: 0.00001930
Iteration 136/1000 | Loss: 0.00001929
Iteration 137/1000 | Loss: 0.00001929
Iteration 138/1000 | Loss: 0.00001929
Iteration 139/1000 | Loss: 0.00001929
Iteration 140/1000 | Loss: 0.00001929
Iteration 141/1000 | Loss: 0.00001929
Iteration 142/1000 | Loss: 0.00001928
Iteration 143/1000 | Loss: 0.00001928
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001927
Iteration 146/1000 | Loss: 0.00001927
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001927
Iteration 149/1000 | Loss: 0.00001927
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001925
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001925
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001925
Iteration 159/1000 | Loss: 0.00001925
Iteration 160/1000 | Loss: 0.00001925
Iteration 161/1000 | Loss: 0.00001925
Iteration 162/1000 | Loss: 0.00001924
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001924
Iteration 167/1000 | Loss: 0.00001924
Iteration 168/1000 | Loss: 0.00001924
Iteration 169/1000 | Loss: 0.00001924
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001923
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Iteration 176/1000 | Loss: 0.00001923
Iteration 177/1000 | Loss: 0.00001923
Iteration 178/1000 | Loss: 0.00001923
Iteration 179/1000 | Loss: 0.00001923
Iteration 180/1000 | Loss: 0.00001923
Iteration 181/1000 | Loss: 0.00001923
Iteration 182/1000 | Loss: 0.00001923
Iteration 183/1000 | Loss: 0.00001923
Iteration 184/1000 | Loss: 0.00001923
Iteration 185/1000 | Loss: 0.00001923
Iteration 186/1000 | Loss: 0.00001923
Iteration 187/1000 | Loss: 0.00001923
Iteration 188/1000 | Loss: 0.00001923
Iteration 189/1000 | Loss: 0.00001923
Iteration 190/1000 | Loss: 0.00001922
Iteration 191/1000 | Loss: 0.00001922
Iteration 192/1000 | Loss: 0.00001922
Iteration 193/1000 | Loss: 0.00001922
Iteration 194/1000 | Loss: 0.00001922
Iteration 195/1000 | Loss: 0.00001922
Iteration 196/1000 | Loss: 0.00001922
Iteration 197/1000 | Loss: 0.00001922
Iteration 198/1000 | Loss: 0.00001922
Iteration 199/1000 | Loss: 0.00001922
Iteration 200/1000 | Loss: 0.00001922
Iteration 201/1000 | Loss: 0.00001922
Iteration 202/1000 | Loss: 0.00001922
Iteration 203/1000 | Loss: 0.00001922
Iteration 204/1000 | Loss: 0.00001922
Iteration 205/1000 | Loss: 0.00001922
Iteration 206/1000 | Loss: 0.00001922
Iteration 207/1000 | Loss: 0.00001922
Iteration 208/1000 | Loss: 0.00001922
Iteration 209/1000 | Loss: 0.00001922
Iteration 210/1000 | Loss: 0.00001922
Iteration 211/1000 | Loss: 0.00001922
Iteration 212/1000 | Loss: 0.00001922
Iteration 213/1000 | Loss: 0.00001922
Iteration 214/1000 | Loss: 0.00001922
Iteration 215/1000 | Loss: 0.00001922
Iteration 216/1000 | Loss: 0.00001922
Iteration 217/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.921503098856192e-05, 1.921503098856192e-05, 1.921503098856192e-05, 1.921503098856192e-05, 1.921503098856192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.921503098856192e-05

Optimization complete. Final v2v error: 3.498645782470703 mm

Highest mean error: 10.648390769958496 mm for frame 150

Lowest mean error: 3.2084693908691406 mm for frame 79

Saving results

Total time: 137.37756490707397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824702
Iteration 2/25 | Loss: 0.00150711
Iteration 3/25 | Loss: 0.00129176
Iteration 4/25 | Loss: 0.00126352
Iteration 5/25 | Loss: 0.00125879
Iteration 6/25 | Loss: 0.00125770
Iteration 7/25 | Loss: 0.00125771
Iteration 8/25 | Loss: 0.00125770
Iteration 9/25 | Loss: 0.00125770
Iteration 10/25 | Loss: 0.00125771
Iteration 11/25 | Loss: 0.00125770
Iteration 12/25 | Loss: 0.00125770
Iteration 13/25 | Loss: 0.00125770
Iteration 14/25 | Loss: 0.00125770
Iteration 15/25 | Loss: 0.00125770
Iteration 16/25 | Loss: 0.00125770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012577048037201166, 0.0012577048037201166, 0.0012577048037201166, 0.0012577048037201166, 0.0012577048037201166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012577048037201166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43123603
Iteration 2/25 | Loss: 0.00077628
Iteration 3/25 | Loss: 0.00077628
Iteration 4/25 | Loss: 0.00077628
Iteration 5/25 | Loss: 0.00077628
Iteration 6/25 | Loss: 0.00077628
Iteration 7/25 | Loss: 0.00077628
Iteration 8/25 | Loss: 0.00077628
Iteration 9/25 | Loss: 0.00077628
Iteration 10/25 | Loss: 0.00077628
Iteration 11/25 | Loss: 0.00077628
Iteration 12/25 | Loss: 0.00077628
Iteration 13/25 | Loss: 0.00077628
Iteration 14/25 | Loss: 0.00077628
Iteration 15/25 | Loss: 0.00077628
Iteration 16/25 | Loss: 0.00077628
Iteration 17/25 | Loss: 0.00077628
Iteration 18/25 | Loss: 0.00077628
Iteration 19/25 | Loss: 0.00077628
Iteration 20/25 | Loss: 0.00077628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00077627639984712, 0.00077627639984712, 0.00077627639984712, 0.00077627639984712, 0.00077627639984712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00077627639984712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077628
Iteration 2/1000 | Loss: 0.00003485
Iteration 3/1000 | Loss: 0.00002454
Iteration 4/1000 | Loss: 0.00001993
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001598
Iteration 8/1000 | Loss: 0.00001561
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001443
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001432
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001426
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00001422
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001415
Iteration 27/1000 | Loss: 0.00001415
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001408
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001407
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001406
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001404
Iteration 60/1000 | Loss: 0.00001404
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001398
Iteration 84/1000 | Loss: 0.00001397
Iteration 85/1000 | Loss: 0.00001397
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.3974601642985363e-05, 1.3974601642985363e-05, 1.3974601642985363e-05, 1.3974601642985363e-05, 1.3974601642985363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3974601642985363e-05

Optimization complete. Final v2v error: 3.1994431018829346 mm

Highest mean error: 3.9158408641815186 mm for frame 114

Lowest mean error: 2.8312201499938965 mm for frame 32

Saving results

Total time: 38.42899203300476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002574
Iteration 2/25 | Loss: 0.00163535
Iteration 3/25 | Loss: 0.00135995
Iteration 4/25 | Loss: 0.00132708
Iteration 5/25 | Loss: 0.00132065
Iteration 6/25 | Loss: 0.00131915
Iteration 7/25 | Loss: 0.00131723
Iteration 8/25 | Loss: 0.00131652
Iteration 9/25 | Loss: 0.00131609
Iteration 10/25 | Loss: 0.00131604
Iteration 11/25 | Loss: 0.00131604
Iteration 12/25 | Loss: 0.00131603
Iteration 13/25 | Loss: 0.00131603
Iteration 14/25 | Loss: 0.00131603
Iteration 15/25 | Loss: 0.00131603
Iteration 16/25 | Loss: 0.00131603
Iteration 17/25 | Loss: 0.00131603
Iteration 18/25 | Loss: 0.00131603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013160331873223186, 0.0013160331873223186, 0.0013160331873223186, 0.0013160331873223186, 0.0013160331873223186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013160331873223186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54855895
Iteration 2/25 | Loss: 0.00100351
Iteration 3/25 | Loss: 0.00100351
Iteration 4/25 | Loss: 0.00100351
Iteration 5/25 | Loss: 0.00100351
Iteration 6/25 | Loss: 0.00100351
Iteration 7/25 | Loss: 0.00100351
Iteration 8/25 | Loss: 0.00100351
Iteration 9/25 | Loss: 0.00100351
Iteration 10/25 | Loss: 0.00100351
Iteration 11/25 | Loss: 0.00100351
Iteration 12/25 | Loss: 0.00100351
Iteration 13/25 | Loss: 0.00100351
Iteration 14/25 | Loss: 0.00100351
Iteration 15/25 | Loss: 0.00100351
Iteration 16/25 | Loss: 0.00100351
Iteration 17/25 | Loss: 0.00100351
Iteration 18/25 | Loss: 0.00100351
Iteration 19/25 | Loss: 0.00100351
Iteration 20/25 | Loss: 0.00100351
Iteration 21/25 | Loss: 0.00100351
Iteration 22/25 | Loss: 0.00100351
Iteration 23/25 | Loss: 0.00100351
Iteration 24/25 | Loss: 0.00100351
Iteration 25/25 | Loss: 0.00100351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100351
Iteration 2/1000 | Loss: 0.00003546
Iteration 3/1000 | Loss: 0.00002562
Iteration 4/1000 | Loss: 0.00002348
Iteration 5/1000 | Loss: 0.00002221
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002057
Iteration 9/1000 | Loss: 0.00002015
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001907
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001892
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001886
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001881
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001878
Iteration 34/1000 | Loss: 0.00001878
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001877
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001877
Iteration 40/1000 | Loss: 0.00001877
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001874
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001874
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001874
Iteration 68/1000 | Loss: 0.00001874
Iteration 69/1000 | Loss: 0.00001874
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001874
Iteration 72/1000 | Loss: 0.00001874
Iteration 73/1000 | Loss: 0.00001874
Iteration 74/1000 | Loss: 0.00001874
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Iteration 82/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.8737551727099344e-05, 1.8737551727099344e-05, 1.8737551727099344e-05, 1.8737551727099344e-05, 1.8737551727099344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8737551727099344e-05

Optimization complete. Final v2v error: 3.703131675720215 mm

Highest mean error: 4.185730457305908 mm for frame 170

Lowest mean error: 3.244150161743164 mm for frame 201

Saving results

Total time: 44.09012246131897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009239
Iteration 2/25 | Loss: 0.01009239
Iteration 3/25 | Loss: 0.01009239
Iteration 4/25 | Loss: 0.01009239
Iteration 5/25 | Loss: 0.01009239
Iteration 6/25 | Loss: 0.01009239
Iteration 7/25 | Loss: 0.01009239
Iteration 8/25 | Loss: 0.01009239
Iteration 9/25 | Loss: 0.01009239
Iteration 10/25 | Loss: 0.01009238
Iteration 11/25 | Loss: 0.01009238
Iteration 12/25 | Loss: 0.01009238
Iteration 13/25 | Loss: 0.01009238
Iteration 14/25 | Loss: 0.01009238
Iteration 15/25 | Loss: 0.01009237
Iteration 16/25 | Loss: 0.01009237
Iteration 17/25 | Loss: 0.01009237
Iteration 18/25 | Loss: 0.01009237
Iteration 19/25 | Loss: 0.01009237
Iteration 20/25 | Loss: 0.01009237
Iteration 21/25 | Loss: 0.01009237
Iteration 22/25 | Loss: 0.01009237
Iteration 23/25 | Loss: 0.01009236
Iteration 24/25 | Loss: 0.01009236
Iteration 25/25 | Loss: 0.01009236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51894331
Iteration 2/25 | Loss: 0.17402463
Iteration 3/25 | Loss: 0.17378999
Iteration 4/25 | Loss: 0.17376557
Iteration 5/25 | Loss: 0.17376553
Iteration 6/25 | Loss: 0.17376551
Iteration 7/25 | Loss: 0.17376550
Iteration 8/25 | Loss: 0.17377642
Iteration 9/25 | Loss: 0.17376556
Iteration 10/25 | Loss: 0.17376553
Iteration 11/25 | Loss: 0.17376551
Iteration 12/25 | Loss: 0.17376551
Iteration 13/25 | Loss: 0.17376550
Iteration 14/25 | Loss: 0.17376550
Iteration 15/25 | Loss: 0.17376550
Iteration 16/25 | Loss: 0.17376548
Iteration 17/25 | Loss: 0.17376550
Iteration 18/25 | Loss: 0.17376550
Iteration 19/25 | Loss: 0.17376548
Iteration 20/25 | Loss: 0.17376548
Iteration 21/25 | Loss: 0.17376548
Iteration 22/25 | Loss: 0.17376550
Iteration 23/25 | Loss: 0.17376548
Iteration 24/25 | Loss: 0.17376550
Iteration 25/25 | Loss: 0.17376548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17376548
Iteration 2/1000 | Loss: 0.01299778
Iteration 3/1000 | Loss: 0.00368624
Iteration 4/1000 | Loss: 0.00067589
Iteration 5/1000 | Loss: 0.00038171
Iteration 6/1000 | Loss: 0.00040705
Iteration 7/1000 | Loss: 0.00017884
Iteration 8/1000 | Loss: 0.00016436
Iteration 9/1000 | Loss: 0.00028001
Iteration 10/1000 | Loss: 0.00015400
Iteration 11/1000 | Loss: 0.00005236
Iteration 12/1000 | Loss: 0.00016860
Iteration 13/1000 | Loss: 0.00005651
Iteration 14/1000 | Loss: 0.00003600
Iteration 15/1000 | Loss: 0.00003798
Iteration 16/1000 | Loss: 0.00003054
Iteration 17/1000 | Loss: 0.00002885
Iteration 18/1000 | Loss: 0.00005594
Iteration 19/1000 | Loss: 0.00002685
Iteration 20/1000 | Loss: 0.00005751
Iteration 21/1000 | Loss: 0.00002528
Iteration 22/1000 | Loss: 0.00002440
Iteration 23/1000 | Loss: 0.00003730
Iteration 24/1000 | Loss: 0.00002340
Iteration 25/1000 | Loss: 0.00003775
Iteration 26/1000 | Loss: 0.00002300
Iteration 27/1000 | Loss: 0.00002259
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00003875
Iteration 30/1000 | Loss: 0.00002190
Iteration 31/1000 | Loss: 0.00004102
Iteration 32/1000 | Loss: 0.00023269
Iteration 33/1000 | Loss: 0.00006180
Iteration 34/1000 | Loss: 0.00003015
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002643
Iteration 37/1000 | Loss: 0.00002153
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002145
Iteration 41/1000 | Loss: 0.00002143
Iteration 42/1000 | Loss: 0.00002143
Iteration 43/1000 | Loss: 0.00002141
Iteration 44/1000 | Loss: 0.00002138
Iteration 45/1000 | Loss: 0.00002136
Iteration 46/1000 | Loss: 0.00002134
Iteration 47/1000 | Loss: 0.00004071
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00002122
Iteration 51/1000 | Loss: 0.00002122
Iteration 52/1000 | Loss: 0.00002122
Iteration 53/1000 | Loss: 0.00002122
Iteration 54/1000 | Loss: 0.00002122
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00002121
Iteration 57/1000 | Loss: 0.00002121
Iteration 58/1000 | Loss: 0.00002121
Iteration 59/1000 | Loss: 0.00002121
Iteration 60/1000 | Loss: 0.00002121
Iteration 61/1000 | Loss: 0.00002121
Iteration 62/1000 | Loss: 0.00002121
Iteration 63/1000 | Loss: 0.00002121
Iteration 64/1000 | Loss: 0.00002121
Iteration 65/1000 | Loss: 0.00002121
Iteration 66/1000 | Loss: 0.00002120
Iteration 67/1000 | Loss: 0.00002120
Iteration 68/1000 | Loss: 0.00002120
Iteration 69/1000 | Loss: 0.00002120
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00002119
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002114
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002112
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002111
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002110
Iteration 99/1000 | Loss: 0.00002110
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002109
Iteration 106/1000 | Loss: 0.00002109
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002108
Iteration 109/1000 | Loss: 0.00002108
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002108
Iteration 114/1000 | Loss: 0.00002108
Iteration 115/1000 | Loss: 0.00002108
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002107
Iteration 124/1000 | Loss: 0.00002107
Iteration 125/1000 | Loss: 0.00002107
Iteration 126/1000 | Loss: 0.00002107
Iteration 127/1000 | Loss: 0.00002107
Iteration 128/1000 | Loss: 0.00002107
Iteration 129/1000 | Loss: 0.00002107
Iteration 130/1000 | Loss: 0.00002107
Iteration 131/1000 | Loss: 0.00002107
Iteration 132/1000 | Loss: 0.00002106
Iteration 133/1000 | Loss: 0.00002106
Iteration 134/1000 | Loss: 0.00002106
Iteration 135/1000 | Loss: 0.00002106
Iteration 136/1000 | Loss: 0.00002106
Iteration 137/1000 | Loss: 0.00002106
Iteration 138/1000 | Loss: 0.00004729
Iteration 139/1000 | Loss: 0.00002168
Iteration 140/1000 | Loss: 0.00002809
Iteration 141/1000 | Loss: 0.00002107
Iteration 142/1000 | Loss: 0.00002107
Iteration 143/1000 | Loss: 0.00002105
Iteration 144/1000 | Loss: 0.00002105
Iteration 145/1000 | Loss: 0.00002105
Iteration 146/1000 | Loss: 0.00002104
Iteration 147/1000 | Loss: 0.00002104
Iteration 148/1000 | Loss: 0.00002104
Iteration 149/1000 | Loss: 0.00002103
Iteration 150/1000 | Loss: 0.00002103
Iteration 151/1000 | Loss: 0.00002103
Iteration 152/1000 | Loss: 0.00002103
Iteration 153/1000 | Loss: 0.00002103
Iteration 154/1000 | Loss: 0.00002103
Iteration 155/1000 | Loss: 0.00002103
Iteration 156/1000 | Loss: 0.00002103
Iteration 157/1000 | Loss: 0.00002103
Iteration 158/1000 | Loss: 0.00002103
Iteration 159/1000 | Loss: 0.00002103
Iteration 160/1000 | Loss: 0.00002103
Iteration 161/1000 | Loss: 0.00002102
Iteration 162/1000 | Loss: 0.00002102
Iteration 163/1000 | Loss: 0.00002102
Iteration 164/1000 | Loss: 0.00002102
Iteration 165/1000 | Loss: 0.00002102
Iteration 166/1000 | Loss: 0.00002102
Iteration 167/1000 | Loss: 0.00002102
Iteration 168/1000 | Loss: 0.00002102
Iteration 169/1000 | Loss: 0.00002102
Iteration 170/1000 | Loss: 0.00002102
Iteration 171/1000 | Loss: 0.00002102
Iteration 172/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.1024692614446394e-05, 2.1024692614446394e-05, 2.1024692614446394e-05, 2.1024692614446394e-05, 2.1024692614446394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1024692614446394e-05

Optimization complete. Final v2v error: 3.8245320320129395 mm

Highest mean error: 4.224883556365967 mm for frame 74

Lowest mean error: 3.497138738632202 mm for frame 221

Saving results

Total time: 86.90993213653564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693724
Iteration 2/25 | Loss: 0.00162426
Iteration 3/25 | Loss: 0.00138142
Iteration 4/25 | Loss: 0.00135155
Iteration 5/25 | Loss: 0.00134915
Iteration 6/25 | Loss: 0.00134915
Iteration 7/25 | Loss: 0.00134915
Iteration 8/25 | Loss: 0.00134915
Iteration 9/25 | Loss: 0.00134915
Iteration 10/25 | Loss: 0.00134915
Iteration 11/25 | Loss: 0.00134915
Iteration 12/25 | Loss: 0.00134915
Iteration 13/25 | Loss: 0.00134915
Iteration 14/25 | Loss: 0.00134915
Iteration 15/25 | Loss: 0.00134915
Iteration 16/25 | Loss: 0.00134915
Iteration 17/25 | Loss: 0.00134915
Iteration 18/25 | Loss: 0.00134915
Iteration 19/25 | Loss: 0.00134915
Iteration 20/25 | Loss: 0.00134915
Iteration 21/25 | Loss: 0.00134915
Iteration 22/25 | Loss: 0.00134915
Iteration 23/25 | Loss: 0.00134915
Iteration 24/25 | Loss: 0.00134915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013491527643054724, 0.0013491527643054724, 0.0013491527643054724, 0.0013491527643054724, 0.0013491527643054724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013491527643054724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39593613
Iteration 2/25 | Loss: 0.00081905
Iteration 3/25 | Loss: 0.00081904
Iteration 4/25 | Loss: 0.00081903
Iteration 5/25 | Loss: 0.00081903
Iteration 6/25 | Loss: 0.00081903
Iteration 7/25 | Loss: 0.00081903
Iteration 8/25 | Loss: 0.00081903
Iteration 9/25 | Loss: 0.00081903
Iteration 10/25 | Loss: 0.00081903
Iteration 11/25 | Loss: 0.00081903
Iteration 12/25 | Loss: 0.00081903
Iteration 13/25 | Loss: 0.00081903
Iteration 14/25 | Loss: 0.00081903
Iteration 15/25 | Loss: 0.00081903
Iteration 16/25 | Loss: 0.00081903
Iteration 17/25 | Loss: 0.00081903
Iteration 18/25 | Loss: 0.00081903
Iteration 19/25 | Loss: 0.00081903
Iteration 20/25 | Loss: 0.00081903
Iteration 21/25 | Loss: 0.00081903
Iteration 22/25 | Loss: 0.00081903
Iteration 23/25 | Loss: 0.00081903
Iteration 24/25 | Loss: 0.00081903
Iteration 25/25 | Loss: 0.00081903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081903
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002287
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002169
Iteration 8/1000 | Loss: 0.00002127
Iteration 9/1000 | Loss: 0.00002095
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002053
Iteration 13/1000 | Loss: 0.00002030
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00001985
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001959
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001941
Iteration 22/1000 | Loss: 0.00001933
Iteration 23/1000 | Loss: 0.00001928
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001919
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001918
Iteration 39/1000 | Loss: 0.00001918
Iteration 40/1000 | Loss: 0.00001917
Iteration 41/1000 | Loss: 0.00001915
Iteration 42/1000 | Loss: 0.00001915
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001914
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001914
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001913
Iteration 50/1000 | Loss: 0.00001912
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001911
Iteration 54/1000 | Loss: 0.00001911
Iteration 55/1000 | Loss: 0.00001911
Iteration 56/1000 | Loss: 0.00001911
Iteration 57/1000 | Loss: 0.00001911
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001906
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001906
Iteration 86/1000 | Loss: 0.00001906
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001905
Iteration 96/1000 | Loss: 0.00001905
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001904
Iteration 106/1000 | Loss: 0.00001904
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001904
Iteration 109/1000 | Loss: 0.00001904
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001904
Iteration 112/1000 | Loss: 0.00001904
Iteration 113/1000 | Loss: 0.00001904
Iteration 114/1000 | Loss: 0.00001904
Iteration 115/1000 | Loss: 0.00001904
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001904
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001903
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001903
Iteration 131/1000 | Loss: 0.00001903
Iteration 132/1000 | Loss: 0.00001903
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001902
Iteration 135/1000 | Loss: 0.00001902
Iteration 136/1000 | Loss: 0.00001902
Iteration 137/1000 | Loss: 0.00001902
Iteration 138/1000 | Loss: 0.00001901
Iteration 139/1000 | Loss: 0.00001901
Iteration 140/1000 | Loss: 0.00001901
Iteration 141/1000 | Loss: 0.00001901
Iteration 142/1000 | Loss: 0.00001901
Iteration 143/1000 | Loss: 0.00001901
Iteration 144/1000 | Loss: 0.00001901
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001901
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Iteration 153/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.9007995433639735e-05, 1.9007995433639735e-05, 1.9007995433639735e-05, 1.9007995433639735e-05, 1.9007995433639735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9007995433639735e-05

Optimization complete. Final v2v error: 3.704878091812134 mm

Highest mean error: 3.873391628265381 mm for frame 91

Lowest mean error: 3.543694257736206 mm for frame 194

Saving results

Total time: 47.497501611709595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012294
Iteration 2/25 | Loss: 0.00164484
Iteration 3/25 | Loss: 0.00136609
Iteration 4/25 | Loss: 0.00133222
Iteration 5/25 | Loss: 0.00132053
Iteration 6/25 | Loss: 0.00131536
Iteration 7/25 | Loss: 0.00131608
Iteration 8/25 | Loss: 0.00131473
Iteration 9/25 | Loss: 0.00130819
Iteration 10/25 | Loss: 0.00130818
Iteration 11/25 | Loss: 0.00130817
Iteration 12/25 | Loss: 0.00130817
Iteration 13/25 | Loss: 0.00130817
Iteration 14/25 | Loss: 0.00130817
Iteration 15/25 | Loss: 0.00130817
Iteration 16/25 | Loss: 0.00130817
Iteration 17/25 | Loss: 0.00130817
Iteration 18/25 | Loss: 0.00130817
Iteration 19/25 | Loss: 0.00130817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013081731740385294, 0.0013081731740385294, 0.0013081731740385294, 0.0013081731740385294, 0.0013081731740385294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013081731740385294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53203857
Iteration 2/25 | Loss: 0.00099364
Iteration 3/25 | Loss: 0.00099363
Iteration 4/25 | Loss: 0.00099363
Iteration 5/25 | Loss: 0.00099363
Iteration 6/25 | Loss: 0.00099363
Iteration 7/25 | Loss: 0.00099363
Iteration 8/25 | Loss: 0.00099363
Iteration 9/25 | Loss: 0.00099363
Iteration 10/25 | Loss: 0.00099363
Iteration 11/25 | Loss: 0.00099363
Iteration 12/25 | Loss: 0.00099363
Iteration 13/25 | Loss: 0.00099363
Iteration 14/25 | Loss: 0.00099363
Iteration 15/25 | Loss: 0.00099363
Iteration 16/25 | Loss: 0.00099363
Iteration 17/25 | Loss: 0.00099363
Iteration 18/25 | Loss: 0.00099363
Iteration 19/25 | Loss: 0.00099363
Iteration 20/25 | Loss: 0.00099363
Iteration 21/25 | Loss: 0.00099363
Iteration 22/25 | Loss: 0.00099363
Iteration 23/25 | Loss: 0.00099363
Iteration 24/25 | Loss: 0.00099363
Iteration 25/25 | Loss: 0.00099363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099363
Iteration 2/1000 | Loss: 0.00004316
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002462
Iteration 5/1000 | Loss: 0.00002290
Iteration 6/1000 | Loss: 0.00002177
Iteration 7/1000 | Loss: 0.00002097
Iteration 8/1000 | Loss: 0.00002041
Iteration 9/1000 | Loss: 0.00002011
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001939
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001892
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001889
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001867
Iteration 22/1000 | Loss: 0.00001867
Iteration 23/1000 | Loss: 0.00001863
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001861
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00001859
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001858
Iteration 31/1000 | Loss: 0.00001858
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00001857
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001855
Iteration 43/1000 | Loss: 0.00001855
Iteration 44/1000 | Loss: 0.00001855
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001848
Iteration 86/1000 | Loss: 0.00001848
Iteration 87/1000 | Loss: 0.00001848
Iteration 88/1000 | Loss: 0.00001848
Iteration 89/1000 | Loss: 0.00001848
Iteration 90/1000 | Loss: 0.00001848
Iteration 91/1000 | Loss: 0.00001847
Iteration 92/1000 | Loss: 0.00001847
Iteration 93/1000 | Loss: 0.00001847
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001847
Iteration 96/1000 | Loss: 0.00001847
Iteration 97/1000 | Loss: 0.00001847
Iteration 98/1000 | Loss: 0.00001847
Iteration 99/1000 | Loss: 0.00001847
Iteration 100/1000 | Loss: 0.00001847
Iteration 101/1000 | Loss: 0.00001847
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001846
Iteration 107/1000 | Loss: 0.00001846
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001846
Iteration 125/1000 | Loss: 0.00001846
Iteration 126/1000 | Loss: 0.00001846
Iteration 127/1000 | Loss: 0.00001846
Iteration 128/1000 | Loss: 0.00001846
Iteration 129/1000 | Loss: 0.00001846
Iteration 130/1000 | Loss: 0.00001846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.8459542843629606e-05, 1.8459542843629606e-05, 1.8459542843629606e-05, 1.8459542843629606e-05, 1.8459542843629606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8459542843629606e-05

Optimization complete. Final v2v error: 3.7378013134002686 mm

Highest mean error: 3.929128885269165 mm for frame 3

Lowest mean error: 3.5493359565734863 mm for frame 92

Saving results

Total time: 52.979674100875854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532689
Iteration 2/25 | Loss: 0.00141186
Iteration 3/25 | Loss: 0.00134030
Iteration 4/25 | Loss: 0.00133041
Iteration 5/25 | Loss: 0.00132716
Iteration 6/25 | Loss: 0.00132716
Iteration 7/25 | Loss: 0.00132716
Iteration 8/25 | Loss: 0.00132716
Iteration 9/25 | Loss: 0.00132716
Iteration 10/25 | Loss: 0.00132716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013271564384922385, 0.0013271564384922385, 0.0013271564384922385, 0.0013271564384922385, 0.0013271564384922385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013271564384922385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42757976
Iteration 2/25 | Loss: 0.00097436
Iteration 3/25 | Loss: 0.00097436
Iteration 4/25 | Loss: 0.00097436
Iteration 5/25 | Loss: 0.00097436
Iteration 6/25 | Loss: 0.00097436
Iteration 7/25 | Loss: 0.00097436
Iteration 8/25 | Loss: 0.00097435
Iteration 9/25 | Loss: 0.00097435
Iteration 10/25 | Loss: 0.00097435
Iteration 11/25 | Loss: 0.00097435
Iteration 12/25 | Loss: 0.00097435
Iteration 13/25 | Loss: 0.00097435
Iteration 14/25 | Loss: 0.00097435
Iteration 15/25 | Loss: 0.00097435
Iteration 16/25 | Loss: 0.00097435
Iteration 17/25 | Loss: 0.00097435
Iteration 18/25 | Loss: 0.00097435
Iteration 19/25 | Loss: 0.00097435
Iteration 20/25 | Loss: 0.00097435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009743545087985694, 0.0009743545087985694, 0.0009743545087985694, 0.0009743545087985694, 0.0009743545087985694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009743545087985694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097435
Iteration 2/1000 | Loss: 0.00003594
Iteration 3/1000 | Loss: 0.00002574
Iteration 4/1000 | Loss: 0.00002260
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00002034
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001883
Iteration 14/1000 | Loss: 0.00001874
Iteration 15/1000 | Loss: 0.00001867
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001837
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001834
Iteration 22/1000 | Loss: 0.00001833
Iteration 23/1000 | Loss: 0.00001832
Iteration 24/1000 | Loss: 0.00001830
Iteration 25/1000 | Loss: 0.00001829
Iteration 26/1000 | Loss: 0.00001827
Iteration 27/1000 | Loss: 0.00001826
Iteration 28/1000 | Loss: 0.00001826
Iteration 29/1000 | Loss: 0.00001820
Iteration 30/1000 | Loss: 0.00001817
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001798
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001796
Iteration 57/1000 | Loss: 0.00001796
Iteration 58/1000 | Loss: 0.00001796
Iteration 59/1000 | Loss: 0.00001796
Iteration 60/1000 | Loss: 0.00001796
Iteration 61/1000 | Loss: 0.00001796
Iteration 62/1000 | Loss: 0.00001795
Iteration 63/1000 | Loss: 0.00001794
Iteration 64/1000 | Loss: 0.00001794
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001793
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001792
Iteration 72/1000 | Loss: 0.00001792
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001790
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001789
Iteration 80/1000 | Loss: 0.00001789
Iteration 81/1000 | Loss: 0.00001789
Iteration 82/1000 | Loss: 0.00001789
Iteration 83/1000 | Loss: 0.00001789
Iteration 84/1000 | Loss: 0.00001789
Iteration 85/1000 | Loss: 0.00001788
Iteration 86/1000 | Loss: 0.00001788
Iteration 87/1000 | Loss: 0.00001788
Iteration 88/1000 | Loss: 0.00001787
Iteration 89/1000 | Loss: 0.00001787
Iteration 90/1000 | Loss: 0.00001787
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001784
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001783
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001782
Iteration 103/1000 | Loss: 0.00001782
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Iteration 109/1000 | Loss: 0.00001781
Iteration 110/1000 | Loss: 0.00001780
Iteration 111/1000 | Loss: 0.00001780
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001780
Iteration 114/1000 | Loss: 0.00001780
Iteration 115/1000 | Loss: 0.00001780
Iteration 116/1000 | Loss: 0.00001780
Iteration 117/1000 | Loss: 0.00001779
Iteration 118/1000 | Loss: 0.00001779
Iteration 119/1000 | Loss: 0.00001779
Iteration 120/1000 | Loss: 0.00001779
Iteration 121/1000 | Loss: 0.00001779
Iteration 122/1000 | Loss: 0.00001779
Iteration 123/1000 | Loss: 0.00001779
Iteration 124/1000 | Loss: 0.00001778
Iteration 125/1000 | Loss: 0.00001778
Iteration 126/1000 | Loss: 0.00001778
Iteration 127/1000 | Loss: 0.00001778
Iteration 128/1000 | Loss: 0.00001778
Iteration 129/1000 | Loss: 0.00001778
Iteration 130/1000 | Loss: 0.00001778
Iteration 131/1000 | Loss: 0.00001777
Iteration 132/1000 | Loss: 0.00001777
Iteration 133/1000 | Loss: 0.00001777
Iteration 134/1000 | Loss: 0.00001777
Iteration 135/1000 | Loss: 0.00001777
Iteration 136/1000 | Loss: 0.00001777
Iteration 137/1000 | Loss: 0.00001776
Iteration 138/1000 | Loss: 0.00001776
Iteration 139/1000 | Loss: 0.00001776
Iteration 140/1000 | Loss: 0.00001776
Iteration 141/1000 | Loss: 0.00001775
Iteration 142/1000 | Loss: 0.00001775
Iteration 143/1000 | Loss: 0.00001775
Iteration 144/1000 | Loss: 0.00001775
Iteration 145/1000 | Loss: 0.00001775
Iteration 146/1000 | Loss: 0.00001775
Iteration 147/1000 | Loss: 0.00001775
Iteration 148/1000 | Loss: 0.00001775
Iteration 149/1000 | Loss: 0.00001775
Iteration 150/1000 | Loss: 0.00001775
Iteration 151/1000 | Loss: 0.00001775
Iteration 152/1000 | Loss: 0.00001774
Iteration 153/1000 | Loss: 0.00001774
Iteration 154/1000 | Loss: 0.00001774
Iteration 155/1000 | Loss: 0.00001774
Iteration 156/1000 | Loss: 0.00001774
Iteration 157/1000 | Loss: 0.00001774
Iteration 158/1000 | Loss: 0.00001774
Iteration 159/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.7744023352861404e-05, 1.7744023352861404e-05, 1.7744023352861404e-05, 1.7744023352861404e-05, 1.7744023352861404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7744023352861404e-05

Optimization complete. Final v2v error: 3.5301809310913086 mm

Highest mean error: 3.9926939010620117 mm for frame 143

Lowest mean error: 2.9651198387145996 mm for frame 38

Saving results

Total time: 48.43766808509827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434209
Iteration 2/25 | Loss: 0.00138986
Iteration 3/25 | Loss: 0.00130837
Iteration 4/25 | Loss: 0.00128830
Iteration 5/25 | Loss: 0.00128101
Iteration 6/25 | Loss: 0.00127979
Iteration 7/25 | Loss: 0.00127979
Iteration 8/25 | Loss: 0.00127979
Iteration 9/25 | Loss: 0.00127979
Iteration 10/25 | Loss: 0.00127979
Iteration 11/25 | Loss: 0.00127979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001279786811210215, 0.001279786811210215, 0.001279786811210215, 0.001279786811210215, 0.001279786811210215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001279786811210215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19062877
Iteration 2/25 | Loss: 0.00093607
Iteration 3/25 | Loss: 0.00093607
Iteration 4/25 | Loss: 0.00093607
Iteration 5/25 | Loss: 0.00093607
Iteration 6/25 | Loss: 0.00093607
Iteration 7/25 | Loss: 0.00093607
Iteration 8/25 | Loss: 0.00093607
Iteration 9/25 | Loss: 0.00093607
Iteration 10/25 | Loss: 0.00093606
Iteration 11/25 | Loss: 0.00093606
Iteration 12/25 | Loss: 0.00093606
Iteration 13/25 | Loss: 0.00093606
Iteration 14/25 | Loss: 0.00093606
Iteration 15/25 | Loss: 0.00093606
Iteration 16/25 | Loss: 0.00093606
Iteration 17/25 | Loss: 0.00093606
Iteration 18/25 | Loss: 0.00093606
Iteration 19/25 | Loss: 0.00093606
Iteration 20/25 | Loss: 0.00093606
Iteration 21/25 | Loss: 0.00093606
Iteration 22/25 | Loss: 0.00093606
Iteration 23/25 | Loss: 0.00093606
Iteration 24/25 | Loss: 0.00093606
Iteration 25/25 | Loss: 0.00093606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000936064519919455, 0.000936064519919455, 0.000936064519919455, 0.000936064519919455, 0.000936064519919455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000936064519919455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093606
Iteration 2/1000 | Loss: 0.00003150
Iteration 3/1000 | Loss: 0.00002535
Iteration 4/1000 | Loss: 0.00002359
Iteration 5/1000 | Loss: 0.00002284
Iteration 6/1000 | Loss: 0.00002229
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002133
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002066
Iteration 12/1000 | Loss: 0.00002057
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002054
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002038
Iteration 17/1000 | Loss: 0.00002027
Iteration 18/1000 | Loss: 0.00002021
Iteration 19/1000 | Loss: 0.00002014
Iteration 20/1000 | Loss: 0.00002013
Iteration 21/1000 | Loss: 0.00002012
Iteration 22/1000 | Loss: 0.00002010
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002006
Iteration 26/1000 | Loss: 0.00002002
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002000
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001994
Iteration 33/1000 | Loss: 0.00001990
Iteration 34/1000 | Loss: 0.00001990
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001986
Iteration 43/1000 | Loss: 0.00001986
Iteration 44/1000 | Loss: 0.00001985
Iteration 45/1000 | Loss: 0.00001985
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001984
Iteration 48/1000 | Loss: 0.00001984
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00001983
Iteration 51/1000 | Loss: 0.00001983
Iteration 52/1000 | Loss: 0.00001983
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001981
Iteration 58/1000 | Loss: 0.00001981
Iteration 59/1000 | Loss: 0.00001981
Iteration 60/1000 | Loss: 0.00001980
Iteration 61/1000 | Loss: 0.00001980
Iteration 62/1000 | Loss: 0.00001979
Iteration 63/1000 | Loss: 0.00001979
Iteration 64/1000 | Loss: 0.00001979
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001978
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001972
Iteration 83/1000 | Loss: 0.00001972
Iteration 84/1000 | Loss: 0.00001972
Iteration 85/1000 | Loss: 0.00001971
Iteration 86/1000 | Loss: 0.00001971
Iteration 87/1000 | Loss: 0.00001971
Iteration 88/1000 | Loss: 0.00001971
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001969
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00001969
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001967
Iteration 120/1000 | Loss: 0.00001967
Iteration 121/1000 | Loss: 0.00001967
Iteration 122/1000 | Loss: 0.00001967
Iteration 123/1000 | Loss: 0.00001967
Iteration 124/1000 | Loss: 0.00001967
Iteration 125/1000 | Loss: 0.00001967
Iteration 126/1000 | Loss: 0.00001967
Iteration 127/1000 | Loss: 0.00001967
Iteration 128/1000 | Loss: 0.00001967
Iteration 129/1000 | Loss: 0.00001967
Iteration 130/1000 | Loss: 0.00001966
Iteration 131/1000 | Loss: 0.00001966
Iteration 132/1000 | Loss: 0.00001966
Iteration 133/1000 | Loss: 0.00001966
Iteration 134/1000 | Loss: 0.00001966
Iteration 135/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.9664950741571374e-05, 1.9664950741571374e-05, 1.9664950741571374e-05, 1.9664950741571374e-05, 1.9664950741571374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9664950741571374e-05

Optimization complete. Final v2v error: 3.7724759578704834 mm

Highest mean error: 4.316886901855469 mm for frame 88

Lowest mean error: 3.442615270614624 mm for frame 3

Saving results

Total time: 38.513763189315796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039102
Iteration 2/25 | Loss: 0.00150059
Iteration 3/25 | Loss: 0.00129772
Iteration 4/25 | Loss: 0.00128031
Iteration 5/25 | Loss: 0.00127739
Iteration 6/25 | Loss: 0.00126663
Iteration 7/25 | Loss: 0.00126234
Iteration 8/25 | Loss: 0.00126035
Iteration 9/25 | Loss: 0.00125946
Iteration 10/25 | Loss: 0.00125873
Iteration 11/25 | Loss: 0.00125824
Iteration 12/25 | Loss: 0.00125781
Iteration 13/25 | Loss: 0.00125761
Iteration 14/25 | Loss: 0.00125757
Iteration 15/25 | Loss: 0.00125757
Iteration 16/25 | Loss: 0.00125757
Iteration 17/25 | Loss: 0.00125757
Iteration 18/25 | Loss: 0.00125757
Iteration 19/25 | Loss: 0.00125756
Iteration 20/25 | Loss: 0.00125756
Iteration 21/25 | Loss: 0.00125756
Iteration 22/25 | Loss: 0.00125756
Iteration 23/25 | Loss: 0.00125756
Iteration 24/25 | Loss: 0.00125756
Iteration 25/25 | Loss: 0.00125756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36813307
Iteration 2/25 | Loss: 0.00071358
Iteration 3/25 | Loss: 0.00071358
Iteration 4/25 | Loss: 0.00071358
Iteration 5/25 | Loss: 0.00071358
Iteration 6/25 | Loss: 0.00071358
Iteration 7/25 | Loss: 0.00071358
Iteration 8/25 | Loss: 0.00071358
Iteration 9/25 | Loss: 0.00071358
Iteration 10/25 | Loss: 0.00071358
Iteration 11/25 | Loss: 0.00071358
Iteration 12/25 | Loss: 0.00071358
Iteration 13/25 | Loss: 0.00071358
Iteration 14/25 | Loss: 0.00071358
Iteration 15/25 | Loss: 0.00071358
Iteration 16/25 | Loss: 0.00071358
Iteration 17/25 | Loss: 0.00071358
Iteration 18/25 | Loss: 0.00071358
Iteration 19/25 | Loss: 0.00071358
Iteration 20/25 | Loss: 0.00071358
Iteration 21/25 | Loss: 0.00071358
Iteration 22/25 | Loss: 0.00071358
Iteration 23/25 | Loss: 0.00071358
Iteration 24/25 | Loss: 0.00071358
Iteration 25/25 | Loss: 0.00071358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071358
Iteration 2/1000 | Loss: 0.00002800
Iteration 3/1000 | Loss: 0.00002203
Iteration 4/1000 | Loss: 0.00001954
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001697
Iteration 9/1000 | Loss: 0.00001667
Iteration 10/1000 | Loss: 0.00001637
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001594
Iteration 13/1000 | Loss: 0.00001594
Iteration 14/1000 | Loss: 0.00061170
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00008547
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00008632
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001320
Iteration 26/1000 | Loss: 0.00001320
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001319
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001319
Iteration 33/1000 | Loss: 0.00001319
Iteration 34/1000 | Loss: 0.00001314
Iteration 35/1000 | Loss: 0.00001310
Iteration 36/1000 | Loss: 0.00001310
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001307
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001285
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001284
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001279
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001278
Iteration 132/1000 | Loss: 0.00001278
Iteration 133/1000 | Loss: 0.00001278
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001277
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001276
Iteration 149/1000 | Loss: 0.00001276
Iteration 150/1000 | Loss: 0.00001276
Iteration 151/1000 | Loss: 0.00001276
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001276
Iteration 155/1000 | Loss: 0.00001276
Iteration 156/1000 | Loss: 0.00001276
Iteration 157/1000 | Loss: 0.00001276
Iteration 158/1000 | Loss: 0.00001276
Iteration 159/1000 | Loss: 0.00001276
Iteration 160/1000 | Loss: 0.00001276
Iteration 161/1000 | Loss: 0.00001276
Iteration 162/1000 | Loss: 0.00001276
Iteration 163/1000 | Loss: 0.00001276
Iteration 164/1000 | Loss: 0.00001276
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001275
Iteration 167/1000 | Loss: 0.00001275
Iteration 168/1000 | Loss: 0.00001275
Iteration 169/1000 | Loss: 0.00001275
Iteration 170/1000 | Loss: 0.00001275
Iteration 171/1000 | Loss: 0.00001275
Iteration 172/1000 | Loss: 0.00001275
Iteration 173/1000 | Loss: 0.00001275
Iteration 174/1000 | Loss: 0.00001275
Iteration 175/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.2754821000271477e-05, 1.2754821000271477e-05, 1.2754821000271477e-05, 1.2754821000271477e-05, 1.2754821000271477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2754821000271477e-05

Optimization complete. Final v2v error: 3.0295846462249756 mm

Highest mean error: 3.6201560497283936 mm for frame 62

Lowest mean error: 2.879610776901245 mm for frame 15

Saving results

Total time: 65.48622131347656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975304
Iteration 2/25 | Loss: 0.00975304
Iteration 3/25 | Loss: 0.00975304
Iteration 4/25 | Loss: 0.00332794
Iteration 5/25 | Loss: 0.00224802
Iteration 6/25 | Loss: 0.00202822
Iteration 7/25 | Loss: 0.00199295
Iteration 8/25 | Loss: 0.00188401
Iteration 9/25 | Loss: 0.00182022
Iteration 10/25 | Loss: 0.00177522
Iteration 11/25 | Loss: 0.00176735
Iteration 12/25 | Loss: 0.00176139
Iteration 13/25 | Loss: 0.00174704
Iteration 14/25 | Loss: 0.00173591
Iteration 15/25 | Loss: 0.00172761
Iteration 16/25 | Loss: 0.00171814
Iteration 17/25 | Loss: 0.00171604
Iteration 18/25 | Loss: 0.00171601
Iteration 19/25 | Loss: 0.00171533
Iteration 20/25 | Loss: 0.00171203
Iteration 21/25 | Loss: 0.00171182
Iteration 22/25 | Loss: 0.00171108
Iteration 23/25 | Loss: 0.00171047
Iteration 24/25 | Loss: 0.00171120
Iteration 25/25 | Loss: 0.00171697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43189120
Iteration 2/25 | Loss: 0.00401232
Iteration 3/25 | Loss: 0.00394422
Iteration 4/25 | Loss: 0.00396089
Iteration 5/25 | Loss: 0.00396088
Iteration 6/25 | Loss: 0.00394363
Iteration 7/25 | Loss: 0.00394345
Iteration 8/25 | Loss: 0.00394344
Iteration 9/25 | Loss: 0.00394344
Iteration 10/25 | Loss: 0.00394344
Iteration 11/25 | Loss: 0.00394344
Iteration 12/25 | Loss: 0.00394344
Iteration 13/25 | Loss: 0.00394344
Iteration 14/25 | Loss: 0.00394344
Iteration 15/25 | Loss: 0.00394344
Iteration 16/25 | Loss: 0.00394344
Iteration 17/25 | Loss: 0.00394344
Iteration 18/25 | Loss: 0.00394344
Iteration 19/25 | Loss: 0.00394344
Iteration 20/25 | Loss: 0.00394344
Iteration 21/25 | Loss: 0.00394344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003943440970033407, 0.003943440970033407, 0.003943440970033407, 0.003943440970033407, 0.003943440970033407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003943440970033407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00394344
Iteration 2/1000 | Loss: 0.00078842
Iteration 3/1000 | Loss: 0.00062987
Iteration 4/1000 | Loss: 0.00078495
Iteration 5/1000 | Loss: 0.00124985
Iteration 6/1000 | Loss: 0.00127147
Iteration 7/1000 | Loss: 0.00300152
Iteration 8/1000 | Loss: 0.00283603
Iteration 9/1000 | Loss: 0.00156438
Iteration 10/1000 | Loss: 0.00057198
Iteration 11/1000 | Loss: 0.00082027
Iteration 12/1000 | Loss: 0.00309528
Iteration 13/1000 | Loss: 0.00086420
Iteration 14/1000 | Loss: 0.00109120
Iteration 15/1000 | Loss: 0.00032379
Iteration 16/1000 | Loss: 0.00091995
Iteration 17/1000 | Loss: 0.00035538
Iteration 18/1000 | Loss: 0.00135539
Iteration 19/1000 | Loss: 0.00041280
Iteration 20/1000 | Loss: 0.00075018
Iteration 21/1000 | Loss: 0.00093576
Iteration 22/1000 | Loss: 0.00035533
Iteration 23/1000 | Loss: 0.00127080
Iteration 24/1000 | Loss: 0.00090103
Iteration 25/1000 | Loss: 0.00158253
Iteration 26/1000 | Loss: 0.00029356
Iteration 27/1000 | Loss: 0.00035151
Iteration 28/1000 | Loss: 0.00072208
Iteration 29/1000 | Loss: 0.00037550
Iteration 30/1000 | Loss: 0.00053714
Iteration 31/1000 | Loss: 0.00028939
Iteration 32/1000 | Loss: 0.00043617
Iteration 33/1000 | Loss: 0.00209559
Iteration 34/1000 | Loss: 0.00095314
Iteration 35/1000 | Loss: 0.00040376
Iteration 36/1000 | Loss: 0.00023991
Iteration 37/1000 | Loss: 0.00064860
Iteration 38/1000 | Loss: 0.00047887
Iteration 39/1000 | Loss: 0.00057937
Iteration 40/1000 | Loss: 0.00098043
Iteration 41/1000 | Loss: 0.00094908
Iteration 42/1000 | Loss: 0.00062911
Iteration 43/1000 | Loss: 0.00083349
Iteration 44/1000 | Loss: 0.00074639
Iteration 45/1000 | Loss: 0.00079080
Iteration 46/1000 | Loss: 0.00066572
Iteration 47/1000 | Loss: 0.00078649
Iteration 48/1000 | Loss: 0.00098362
Iteration 49/1000 | Loss: 0.00088585
Iteration 50/1000 | Loss: 0.00076492
Iteration 51/1000 | Loss: 0.00110905
Iteration 52/1000 | Loss: 0.00057160
Iteration 53/1000 | Loss: 0.00063243
Iteration 54/1000 | Loss: 0.00062569
Iteration 55/1000 | Loss: 0.00070471
Iteration 56/1000 | Loss: 0.00100376
Iteration 57/1000 | Loss: 0.00085892
Iteration 58/1000 | Loss: 0.00052880
Iteration 59/1000 | Loss: 0.00066506
Iteration 60/1000 | Loss: 0.00068592
Iteration 61/1000 | Loss: 0.00093646
Iteration 62/1000 | Loss: 0.00059373
Iteration 63/1000 | Loss: 0.00038891
Iteration 64/1000 | Loss: 0.00083876
Iteration 65/1000 | Loss: 0.00091564
Iteration 66/1000 | Loss: 0.00020174
Iteration 67/1000 | Loss: 0.00029801
Iteration 68/1000 | Loss: 0.00018849
Iteration 69/1000 | Loss: 0.00055273
Iteration 70/1000 | Loss: 0.00060759
Iteration 71/1000 | Loss: 0.00052995
Iteration 72/1000 | Loss: 0.00021954
Iteration 73/1000 | Loss: 0.00033339
Iteration 74/1000 | Loss: 0.00042226
Iteration 75/1000 | Loss: 0.00078325
Iteration 76/1000 | Loss: 0.00045988
Iteration 77/1000 | Loss: 0.00035997
Iteration 78/1000 | Loss: 0.00032994
Iteration 79/1000 | Loss: 0.00045030
Iteration 80/1000 | Loss: 0.00039100
Iteration 81/1000 | Loss: 0.00035988
Iteration 82/1000 | Loss: 0.00022359
Iteration 83/1000 | Loss: 0.00025000
Iteration 84/1000 | Loss: 0.00023322
Iteration 85/1000 | Loss: 0.00019159
Iteration 86/1000 | Loss: 0.00029470
Iteration 87/1000 | Loss: 0.00041537
Iteration 88/1000 | Loss: 0.00101322
Iteration 89/1000 | Loss: 0.00046228
Iteration 90/1000 | Loss: 0.00041763
Iteration 91/1000 | Loss: 0.00028598
Iteration 92/1000 | Loss: 0.00045588
Iteration 93/1000 | Loss: 0.00069599
Iteration 94/1000 | Loss: 0.00039485
Iteration 95/1000 | Loss: 0.00047315
Iteration 96/1000 | Loss: 0.00041594
Iteration 97/1000 | Loss: 0.00044308
Iteration 98/1000 | Loss: 0.00039236
Iteration 99/1000 | Loss: 0.00038569
Iteration 100/1000 | Loss: 0.00019638
Iteration 101/1000 | Loss: 0.00015166
Iteration 102/1000 | Loss: 0.00051611
Iteration 103/1000 | Loss: 0.00100472
Iteration 104/1000 | Loss: 0.00043004
Iteration 105/1000 | Loss: 0.00022267
Iteration 106/1000 | Loss: 0.00020330
Iteration 107/1000 | Loss: 0.00031688
Iteration 108/1000 | Loss: 0.00121211
Iteration 109/1000 | Loss: 0.00049080
Iteration 110/1000 | Loss: 0.00183619
Iteration 111/1000 | Loss: 0.00291629
Iteration 112/1000 | Loss: 0.00175264
Iteration 113/1000 | Loss: 0.00042021
Iteration 114/1000 | Loss: 0.00113297
Iteration 115/1000 | Loss: 0.00020388
Iteration 116/1000 | Loss: 0.00116556
Iteration 117/1000 | Loss: 0.00056410
Iteration 118/1000 | Loss: 0.00068770
Iteration 119/1000 | Loss: 0.00161090
Iteration 120/1000 | Loss: 0.00039353
Iteration 121/1000 | Loss: 0.00016460
Iteration 122/1000 | Loss: 0.00012891
Iteration 123/1000 | Loss: 0.00142366
Iteration 124/1000 | Loss: 0.00051540
Iteration 125/1000 | Loss: 0.00014070
Iteration 126/1000 | Loss: 0.00013258
Iteration 127/1000 | Loss: 0.00055669
Iteration 128/1000 | Loss: 0.00008691
Iteration 129/1000 | Loss: 0.00013503
Iteration 130/1000 | Loss: 0.00028168
Iteration 131/1000 | Loss: 0.00027504
Iteration 132/1000 | Loss: 0.00039249
Iteration 133/1000 | Loss: 0.00018254
Iteration 134/1000 | Loss: 0.00009879
Iteration 135/1000 | Loss: 0.00012366
Iteration 136/1000 | Loss: 0.00009624
Iteration 137/1000 | Loss: 0.00019868
Iteration 138/1000 | Loss: 0.00070995
Iteration 139/1000 | Loss: 0.00298153
Iteration 140/1000 | Loss: 0.00167350
Iteration 141/1000 | Loss: 0.00166625
Iteration 142/1000 | Loss: 0.00010084
Iteration 143/1000 | Loss: 0.00021963
Iteration 144/1000 | Loss: 0.00026059
Iteration 145/1000 | Loss: 0.00048596
Iteration 146/1000 | Loss: 0.00006146
Iteration 147/1000 | Loss: 0.00007645
Iteration 148/1000 | Loss: 0.00008915
Iteration 149/1000 | Loss: 0.00008609
Iteration 150/1000 | Loss: 0.00005036
Iteration 151/1000 | Loss: 0.00011064
Iteration 152/1000 | Loss: 0.00004851
Iteration 153/1000 | Loss: 0.00030025
Iteration 154/1000 | Loss: 0.00078451
Iteration 155/1000 | Loss: 0.00025916
Iteration 156/1000 | Loss: 0.00014104
Iteration 157/1000 | Loss: 0.00006554
Iteration 158/1000 | Loss: 0.00033070
Iteration 159/1000 | Loss: 0.00023895
Iteration 160/1000 | Loss: 0.00005294
Iteration 161/1000 | Loss: 0.00006508
Iteration 162/1000 | Loss: 0.00008780
Iteration 163/1000 | Loss: 0.00012461
Iteration 164/1000 | Loss: 0.00005682
Iteration 165/1000 | Loss: 0.00030587
Iteration 166/1000 | Loss: 0.00024683
Iteration 167/1000 | Loss: 0.00005184
Iteration 168/1000 | Loss: 0.00004599
Iteration 169/1000 | Loss: 0.00004212
Iteration 170/1000 | Loss: 0.00014013
Iteration 171/1000 | Loss: 0.00022446
Iteration 172/1000 | Loss: 0.00010374
Iteration 173/1000 | Loss: 0.00006634
Iteration 174/1000 | Loss: 0.00003948
Iteration 175/1000 | Loss: 0.00014278
Iteration 176/1000 | Loss: 0.00028266
Iteration 177/1000 | Loss: 0.00014207
Iteration 178/1000 | Loss: 0.00014600
Iteration 179/1000 | Loss: 0.00004190
Iteration 180/1000 | Loss: 0.00003563
Iteration 181/1000 | Loss: 0.00003340
Iteration 182/1000 | Loss: 0.00015100
Iteration 183/1000 | Loss: 0.00003121
Iteration 184/1000 | Loss: 0.00007697
Iteration 185/1000 | Loss: 0.00002985
Iteration 186/1000 | Loss: 0.00018220
Iteration 187/1000 | Loss: 0.00003562
Iteration 188/1000 | Loss: 0.00003132
Iteration 189/1000 | Loss: 0.00008780
Iteration 190/1000 | Loss: 0.00002876
Iteration 191/1000 | Loss: 0.00007107
Iteration 192/1000 | Loss: 0.00002655
Iteration 193/1000 | Loss: 0.00035586
Iteration 194/1000 | Loss: 0.00004495
Iteration 195/1000 | Loss: 0.00004222
Iteration 196/1000 | Loss: 0.00002907
Iteration 197/1000 | Loss: 0.00005604
Iteration 198/1000 | Loss: 0.00004982
Iteration 199/1000 | Loss: 0.00002313
Iteration 200/1000 | Loss: 0.00002247
Iteration 201/1000 | Loss: 0.00005198
Iteration 202/1000 | Loss: 0.00002166
Iteration 203/1000 | Loss: 0.00002134
Iteration 204/1000 | Loss: 0.00004933
Iteration 205/1000 | Loss: 0.00002261
Iteration 206/1000 | Loss: 0.00002093
Iteration 207/1000 | Loss: 0.00002081
Iteration 208/1000 | Loss: 0.00002079
Iteration 209/1000 | Loss: 0.00002078
Iteration 210/1000 | Loss: 0.00002078
Iteration 211/1000 | Loss: 0.00002078
Iteration 212/1000 | Loss: 0.00002077
Iteration 213/1000 | Loss: 0.00002077
Iteration 214/1000 | Loss: 0.00002076
Iteration 215/1000 | Loss: 0.00002071
Iteration 216/1000 | Loss: 0.00002070
Iteration 217/1000 | Loss: 0.00002069
Iteration 218/1000 | Loss: 0.00002059
Iteration 219/1000 | Loss: 0.00003888
Iteration 220/1000 | Loss: 0.00002050
Iteration 221/1000 | Loss: 0.00002045
Iteration 222/1000 | Loss: 0.00002657
Iteration 223/1000 | Loss: 0.00002043
Iteration 224/1000 | Loss: 0.00002036
Iteration 225/1000 | Loss: 0.00002035
Iteration 226/1000 | Loss: 0.00002035
Iteration 227/1000 | Loss: 0.00002035
Iteration 228/1000 | Loss: 0.00002034
Iteration 229/1000 | Loss: 0.00002034
Iteration 230/1000 | Loss: 0.00002034
Iteration 231/1000 | Loss: 0.00002034
Iteration 232/1000 | Loss: 0.00002034
Iteration 233/1000 | Loss: 0.00002033
Iteration 234/1000 | Loss: 0.00002031
Iteration 235/1000 | Loss: 0.00002027
Iteration 236/1000 | Loss: 0.00002027
Iteration 237/1000 | Loss: 0.00002026
Iteration 238/1000 | Loss: 0.00002026
Iteration 239/1000 | Loss: 0.00005098
Iteration 240/1000 | Loss: 0.00002297
Iteration 241/1000 | Loss: 0.00002173
Iteration 242/1000 | Loss: 0.00002013
Iteration 243/1000 | Loss: 0.00002012
Iteration 244/1000 | Loss: 0.00002012
Iteration 245/1000 | Loss: 0.00002007
Iteration 246/1000 | Loss: 0.00002006
Iteration 247/1000 | Loss: 0.00002004
Iteration 248/1000 | Loss: 0.00002003
Iteration 249/1000 | Loss: 0.00002003
Iteration 250/1000 | Loss: 0.00002002
Iteration 251/1000 | Loss: 0.00002002
Iteration 252/1000 | Loss: 0.00002001
Iteration 253/1000 | Loss: 0.00002000
Iteration 254/1000 | Loss: 0.00014619
Iteration 255/1000 | Loss: 0.00021851
Iteration 256/1000 | Loss: 0.00009945
Iteration 257/1000 | Loss: 0.00012507
Iteration 258/1000 | Loss: 0.00021116
Iteration 259/1000 | Loss: 0.00021036
Iteration 260/1000 | Loss: 0.00054199
Iteration 261/1000 | Loss: 0.00022905
Iteration 262/1000 | Loss: 0.00015979
Iteration 263/1000 | Loss: 0.00024250
Iteration 264/1000 | Loss: 0.00056517
Iteration 265/1000 | Loss: 0.00044466
Iteration 266/1000 | Loss: 0.00011673
Iteration 267/1000 | Loss: 0.00023583
Iteration 268/1000 | Loss: 0.00009584
Iteration 269/1000 | Loss: 0.00009382
Iteration 270/1000 | Loss: 0.00008250
Iteration 271/1000 | Loss: 0.00013359
Iteration 272/1000 | Loss: 0.00013405
Iteration 273/1000 | Loss: 0.00078974
Iteration 274/1000 | Loss: 0.00002531
Iteration 275/1000 | Loss: 0.00003147
Iteration 276/1000 | Loss: 0.00003103
Iteration 277/1000 | Loss: 0.00002068
Iteration 278/1000 | Loss: 0.00007395
Iteration 279/1000 | Loss: 0.00037858
Iteration 280/1000 | Loss: 0.00015332
Iteration 281/1000 | Loss: 0.00002412
Iteration 282/1000 | Loss: 0.00003508
Iteration 283/1000 | Loss: 0.00003225
Iteration 284/1000 | Loss: 0.00009115
Iteration 285/1000 | Loss: 0.00009242
Iteration 286/1000 | Loss: 0.00007325
Iteration 287/1000 | Loss: 0.00002482
Iteration 288/1000 | Loss: 0.00002007
Iteration 289/1000 | Loss: 0.00002005
Iteration 290/1000 | Loss: 0.00001901
Iteration 291/1000 | Loss: 0.00004029
Iteration 292/1000 | Loss: 0.00004508
Iteration 293/1000 | Loss: 0.00002200
Iteration 294/1000 | Loss: 0.00001886
Iteration 295/1000 | Loss: 0.00001886
Iteration 296/1000 | Loss: 0.00001886
Iteration 297/1000 | Loss: 0.00001886
Iteration 298/1000 | Loss: 0.00001886
Iteration 299/1000 | Loss: 0.00001886
Iteration 300/1000 | Loss: 0.00001885
Iteration 301/1000 | Loss: 0.00001885
Iteration 302/1000 | Loss: 0.00001884
Iteration 303/1000 | Loss: 0.00001884
Iteration 304/1000 | Loss: 0.00001884
Iteration 305/1000 | Loss: 0.00001884
Iteration 306/1000 | Loss: 0.00001884
Iteration 307/1000 | Loss: 0.00001884
Iteration 308/1000 | Loss: 0.00001884
Iteration 309/1000 | Loss: 0.00001884
Iteration 310/1000 | Loss: 0.00001884
Iteration 311/1000 | Loss: 0.00001883
Iteration 312/1000 | Loss: 0.00001883
Iteration 313/1000 | Loss: 0.00001883
Iteration 314/1000 | Loss: 0.00001883
Iteration 315/1000 | Loss: 0.00001882
Iteration 316/1000 | Loss: 0.00001882
Iteration 317/1000 | Loss: 0.00001882
Iteration 318/1000 | Loss: 0.00002413
Iteration 319/1000 | Loss: 0.00001977
Iteration 320/1000 | Loss: 0.00001880
Iteration 321/1000 | Loss: 0.00001880
Iteration 322/1000 | Loss: 0.00001879
Iteration 323/1000 | Loss: 0.00001879
Iteration 324/1000 | Loss: 0.00001879
Iteration 325/1000 | Loss: 0.00001879
Iteration 326/1000 | Loss: 0.00001879
Iteration 327/1000 | Loss: 0.00001879
Iteration 328/1000 | Loss: 0.00001879
Iteration 329/1000 | Loss: 0.00002121
Iteration 330/1000 | Loss: 0.00001918
Iteration 331/1000 | Loss: 0.00001882
Iteration 332/1000 | Loss: 0.00001882
Iteration 333/1000 | Loss: 0.00001880
Iteration 334/1000 | Loss: 0.00001880
Iteration 335/1000 | Loss: 0.00001879
Iteration 336/1000 | Loss: 0.00001879
Iteration 337/1000 | Loss: 0.00001879
Iteration 338/1000 | Loss: 0.00001879
Iteration 339/1000 | Loss: 0.00001879
Iteration 340/1000 | Loss: 0.00001879
Iteration 341/1000 | Loss: 0.00001879
Iteration 342/1000 | Loss: 0.00001879
Iteration 343/1000 | Loss: 0.00001879
Iteration 344/1000 | Loss: 0.00001879
Iteration 345/1000 | Loss: 0.00001879
Iteration 346/1000 | Loss: 0.00001879
Iteration 347/1000 | Loss: 0.00001879
Iteration 348/1000 | Loss: 0.00001879
Iteration 349/1000 | Loss: 0.00001879
Iteration 350/1000 | Loss: 0.00001878
Iteration 351/1000 | Loss: 0.00001878
Iteration 352/1000 | Loss: 0.00001878
Iteration 353/1000 | Loss: 0.00001878
Iteration 354/1000 | Loss: 0.00001878
Iteration 355/1000 | Loss: 0.00001878
Iteration 356/1000 | Loss: 0.00001878
Iteration 357/1000 | Loss: 0.00001878
Iteration 358/1000 | Loss: 0.00001878
Iteration 359/1000 | Loss: 0.00001878
Iteration 360/1000 | Loss: 0.00001877
Iteration 361/1000 | Loss: 0.00001877
Iteration 362/1000 | Loss: 0.00001877
Iteration 363/1000 | Loss: 0.00001877
Iteration 364/1000 | Loss: 0.00001877
Iteration 365/1000 | Loss: 0.00001877
Iteration 366/1000 | Loss: 0.00001877
Iteration 367/1000 | Loss: 0.00001877
Iteration 368/1000 | Loss: 0.00001877
Iteration 369/1000 | Loss: 0.00001877
Iteration 370/1000 | Loss: 0.00001877
Iteration 371/1000 | Loss: 0.00001877
Iteration 372/1000 | Loss: 0.00001877
Iteration 373/1000 | Loss: 0.00001886
Iteration 374/1000 | Loss: 0.00001877
Iteration 375/1000 | Loss: 0.00001877
Iteration 376/1000 | Loss: 0.00001877
Iteration 377/1000 | Loss: 0.00001877
Iteration 378/1000 | Loss: 0.00001877
Iteration 379/1000 | Loss: 0.00001877
Iteration 380/1000 | Loss: 0.00001877
Iteration 381/1000 | Loss: 0.00001877
Iteration 382/1000 | Loss: 0.00001877
Iteration 383/1000 | Loss: 0.00001877
Iteration 384/1000 | Loss: 0.00001877
Iteration 385/1000 | Loss: 0.00001877
Iteration 386/1000 | Loss: 0.00001876
Iteration 387/1000 | Loss: 0.00001876
Iteration 388/1000 | Loss: 0.00001876
Iteration 389/1000 | Loss: 0.00001876
Iteration 390/1000 | Loss: 0.00001876
Iteration 391/1000 | Loss: 0.00001876
Iteration 392/1000 | Loss: 0.00001876
Iteration 393/1000 | Loss: 0.00001876
Iteration 394/1000 | Loss: 0.00001876
Iteration 395/1000 | Loss: 0.00001876
Iteration 396/1000 | Loss: 0.00001876
Iteration 397/1000 | Loss: 0.00001876
Iteration 398/1000 | Loss: 0.00001876
Iteration 399/1000 | Loss: 0.00001876
Iteration 400/1000 | Loss: 0.00001876
Iteration 401/1000 | Loss: 0.00001876
Iteration 402/1000 | Loss: 0.00001876
Iteration 403/1000 | Loss: 0.00001876
Iteration 404/1000 | Loss: 0.00001876
Iteration 405/1000 | Loss: 0.00001875
Iteration 406/1000 | Loss: 0.00001875
Iteration 407/1000 | Loss: 0.00003808
Iteration 408/1000 | Loss: 0.00003926
Iteration 409/1000 | Loss: 0.00010310
Iteration 410/1000 | Loss: 0.00004249
Iteration 411/1000 | Loss: 0.00006367
Iteration 412/1000 | Loss: 0.00005130
Iteration 413/1000 | Loss: 0.00002140
Iteration 414/1000 | Loss: 0.00007628
Iteration 415/1000 | Loss: 0.00001872
Iteration 416/1000 | Loss: 0.00001862
Iteration 417/1000 | Loss: 0.00001861
Iteration 418/1000 | Loss: 0.00001861
Iteration 419/1000 | Loss: 0.00001861
Iteration 420/1000 | Loss: 0.00001861
Iteration 421/1000 | Loss: 0.00001861
Iteration 422/1000 | Loss: 0.00001861
Iteration 423/1000 | Loss: 0.00001860
Iteration 424/1000 | Loss: 0.00001860
Iteration 425/1000 | Loss: 0.00001860
Iteration 426/1000 | Loss: 0.00001860
Iteration 427/1000 | Loss: 0.00001860
Iteration 428/1000 | Loss: 0.00001859
Iteration 429/1000 | Loss: 0.00001859
Iteration 430/1000 | Loss: 0.00001859
Iteration 431/1000 | Loss: 0.00001859
Iteration 432/1000 | Loss: 0.00001859
Iteration 433/1000 | Loss: 0.00001859
Iteration 434/1000 | Loss: 0.00001859
Iteration 435/1000 | Loss: 0.00001859
Iteration 436/1000 | Loss: 0.00001859
Iteration 437/1000 | Loss: 0.00001859
Iteration 438/1000 | Loss: 0.00001859
Iteration 439/1000 | Loss: 0.00001859
Iteration 440/1000 | Loss: 0.00001859
Iteration 441/1000 | Loss: 0.00001859
Iteration 442/1000 | Loss: 0.00001859
Iteration 443/1000 | Loss: 0.00001859
Iteration 444/1000 | Loss: 0.00001859
Iteration 445/1000 | Loss: 0.00001859
Iteration 446/1000 | Loss: 0.00001859
Iteration 447/1000 | Loss: 0.00001859
Iteration 448/1000 | Loss: 0.00001859
Iteration 449/1000 | Loss: 0.00001859
Iteration 450/1000 | Loss: 0.00001859
Iteration 451/1000 | Loss: 0.00001859
Iteration 452/1000 | Loss: 0.00001859
Iteration 453/1000 | Loss: 0.00001859
Iteration 454/1000 | Loss: 0.00001859
Iteration 455/1000 | Loss: 0.00001859
Iteration 456/1000 | Loss: 0.00001859
Iteration 457/1000 | Loss: 0.00001859
Iteration 458/1000 | Loss: 0.00001859
Iteration 459/1000 | Loss: 0.00001859
Iteration 460/1000 | Loss: 0.00001859
Iteration 461/1000 | Loss: 0.00001859
Iteration 462/1000 | Loss: 0.00001859
Iteration 463/1000 | Loss: 0.00001859
Iteration 464/1000 | Loss: 0.00001859
Iteration 465/1000 | Loss: 0.00001859
Iteration 466/1000 | Loss: 0.00001859
Iteration 467/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 467. Stopping optimization.
Last 5 losses: [1.859049007180147e-05, 1.859049007180147e-05, 1.859049007180147e-05, 1.859049007180147e-05, 1.859049007180147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.859049007180147e-05

Optimization complete. Final v2v error: 3.3353919982910156 mm

Highest mean error: 11.297296524047852 mm for frame 167

Lowest mean error: 2.965632200241089 mm for frame 127

Saving results

Total time: 495.47114610671997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943973
Iteration 2/25 | Loss: 0.00253041
Iteration 3/25 | Loss: 0.00204668
Iteration 4/25 | Loss: 0.00206194
Iteration 5/25 | Loss: 0.00190467
Iteration 6/25 | Loss: 0.00168630
Iteration 7/25 | Loss: 0.00155667
Iteration 8/25 | Loss: 0.00152863
Iteration 9/25 | Loss: 0.00150987
Iteration 10/25 | Loss: 0.00149731
Iteration 11/25 | Loss: 0.00147812
Iteration 12/25 | Loss: 0.00147851
Iteration 13/25 | Loss: 0.00147394
Iteration 14/25 | Loss: 0.00147203
Iteration 15/25 | Loss: 0.00147564
Iteration 16/25 | Loss: 0.00147271
Iteration 17/25 | Loss: 0.00146803
Iteration 18/25 | Loss: 0.00146662
Iteration 19/25 | Loss: 0.00146410
Iteration 20/25 | Loss: 0.00146332
Iteration 21/25 | Loss: 0.00146306
Iteration 22/25 | Loss: 0.00146291
Iteration 23/25 | Loss: 0.00146247
Iteration 24/25 | Loss: 0.00146199
Iteration 25/25 | Loss: 0.00146608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36692035
Iteration 2/25 | Loss: 0.00166356
Iteration 3/25 | Loss: 0.00166355
Iteration 4/25 | Loss: 0.00166355
Iteration 5/25 | Loss: 0.00166355
Iteration 6/25 | Loss: 0.00166355
Iteration 7/25 | Loss: 0.00166355
Iteration 8/25 | Loss: 0.00166355
Iteration 9/25 | Loss: 0.00166355
Iteration 10/25 | Loss: 0.00166355
Iteration 11/25 | Loss: 0.00166355
Iteration 12/25 | Loss: 0.00166355
Iteration 13/25 | Loss: 0.00166355
Iteration 14/25 | Loss: 0.00166355
Iteration 15/25 | Loss: 0.00166355
Iteration 16/25 | Loss: 0.00166355
Iteration 17/25 | Loss: 0.00166355
Iteration 18/25 | Loss: 0.00166355
Iteration 19/25 | Loss: 0.00166355
Iteration 20/25 | Loss: 0.00166355
Iteration 21/25 | Loss: 0.00166355
Iteration 22/25 | Loss: 0.00166355
Iteration 23/25 | Loss: 0.00166355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016635509673506021, 0.0016635509673506021, 0.0016635509673506021, 0.0016635509673506021, 0.0016635509673506021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016635509673506021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166355
Iteration 2/1000 | Loss: 0.00015222
Iteration 3/1000 | Loss: 0.00011211
Iteration 4/1000 | Loss: 0.00014339
Iteration 5/1000 | Loss: 0.00010315
Iteration 6/1000 | Loss: 0.00008274
Iteration 7/1000 | Loss: 0.00007792
Iteration 8/1000 | Loss: 0.00007497
Iteration 9/1000 | Loss: 0.00007265
Iteration 10/1000 | Loss: 0.00007064
Iteration 11/1000 | Loss: 0.00006926
Iteration 12/1000 | Loss: 0.00006841
Iteration 13/1000 | Loss: 0.00006790
Iteration 14/1000 | Loss: 0.00006750
Iteration 15/1000 | Loss: 0.00006715
Iteration 16/1000 | Loss: 0.00006686
Iteration 17/1000 | Loss: 0.00006642
Iteration 18/1000 | Loss: 0.00006611
Iteration 19/1000 | Loss: 0.00006584
Iteration 20/1000 | Loss: 0.00006544
Iteration 21/1000 | Loss: 0.00006501
Iteration 22/1000 | Loss: 0.00006463
Iteration 23/1000 | Loss: 0.00006439
Iteration 24/1000 | Loss: 0.00006415
Iteration 25/1000 | Loss: 0.00006395
Iteration 26/1000 | Loss: 0.00006391
Iteration 27/1000 | Loss: 0.00006365
Iteration 28/1000 | Loss: 0.00006344
Iteration 29/1000 | Loss: 0.00006341
Iteration 30/1000 | Loss: 0.00006329
Iteration 31/1000 | Loss: 0.00006312
Iteration 32/1000 | Loss: 0.00006297
Iteration 33/1000 | Loss: 0.00006297
Iteration 34/1000 | Loss: 0.00006296
Iteration 35/1000 | Loss: 0.00006292
Iteration 36/1000 | Loss: 0.00006289
Iteration 37/1000 | Loss: 0.00006289
Iteration 38/1000 | Loss: 0.00006288
Iteration 39/1000 | Loss: 0.00006285
Iteration 40/1000 | Loss: 0.00006285
Iteration 41/1000 | Loss: 0.00006284
Iteration 42/1000 | Loss: 0.00006282
Iteration 43/1000 | Loss: 0.00006282
Iteration 44/1000 | Loss: 0.00006282
Iteration 45/1000 | Loss: 0.00006282
Iteration 46/1000 | Loss: 0.00006282
Iteration 47/1000 | Loss: 0.00006282
Iteration 48/1000 | Loss: 0.00006282
Iteration 49/1000 | Loss: 0.00006282
Iteration 50/1000 | Loss: 0.00006282
Iteration 51/1000 | Loss: 0.00006281
Iteration 52/1000 | Loss: 0.00006281
Iteration 53/1000 | Loss: 0.00006281
Iteration 54/1000 | Loss: 0.00006278
Iteration 55/1000 | Loss: 0.00006278
Iteration 56/1000 | Loss: 0.00006278
Iteration 57/1000 | Loss: 0.00006278
Iteration 58/1000 | Loss: 0.00006278
Iteration 59/1000 | Loss: 0.00006278
Iteration 60/1000 | Loss: 0.00006278
Iteration 61/1000 | Loss: 0.00006278
Iteration 62/1000 | Loss: 0.00006278
Iteration 63/1000 | Loss: 0.00006277
Iteration 64/1000 | Loss: 0.00006277
Iteration 65/1000 | Loss: 0.00006276
Iteration 66/1000 | Loss: 0.00006276
Iteration 67/1000 | Loss: 0.00006276
Iteration 68/1000 | Loss: 0.00006276
Iteration 69/1000 | Loss: 0.00006276
Iteration 70/1000 | Loss: 0.00006275
Iteration 71/1000 | Loss: 0.00006275
Iteration 72/1000 | Loss: 0.00006275
Iteration 73/1000 | Loss: 0.00006275
Iteration 74/1000 | Loss: 0.00006275
Iteration 75/1000 | Loss: 0.00006275
Iteration 76/1000 | Loss: 0.00006275
Iteration 77/1000 | Loss: 0.00006275
Iteration 78/1000 | Loss: 0.00006274
Iteration 79/1000 | Loss: 0.00006274
Iteration 80/1000 | Loss: 0.00006273
Iteration 81/1000 | Loss: 0.00006273
Iteration 82/1000 | Loss: 0.00006273
Iteration 83/1000 | Loss: 0.00006273
Iteration 84/1000 | Loss: 0.00006273
Iteration 85/1000 | Loss: 0.00006272
Iteration 86/1000 | Loss: 0.00006272
Iteration 87/1000 | Loss: 0.00006272
Iteration 88/1000 | Loss: 0.00006272
Iteration 89/1000 | Loss: 0.00006272
Iteration 90/1000 | Loss: 0.00006272
Iteration 91/1000 | Loss: 0.00006272
Iteration 92/1000 | Loss: 0.00006272
Iteration 93/1000 | Loss: 0.00006272
Iteration 94/1000 | Loss: 0.00006272
Iteration 95/1000 | Loss: 0.00006272
Iteration 96/1000 | Loss: 0.00006272
Iteration 97/1000 | Loss: 0.00006272
Iteration 98/1000 | Loss: 0.00006272
Iteration 99/1000 | Loss: 0.00006272
Iteration 100/1000 | Loss: 0.00006271
Iteration 101/1000 | Loss: 0.00006271
Iteration 102/1000 | Loss: 0.00006271
Iteration 103/1000 | Loss: 0.00006271
Iteration 104/1000 | Loss: 0.00006271
Iteration 105/1000 | Loss: 0.00006271
Iteration 106/1000 | Loss: 0.00006271
Iteration 107/1000 | Loss: 0.00006270
Iteration 108/1000 | Loss: 0.00006270
Iteration 109/1000 | Loss: 0.00006270
Iteration 110/1000 | Loss: 0.00006270
Iteration 111/1000 | Loss: 0.00006270
Iteration 112/1000 | Loss: 0.00006270
Iteration 113/1000 | Loss: 0.00006270
Iteration 114/1000 | Loss: 0.00006270
Iteration 115/1000 | Loss: 0.00006270
Iteration 116/1000 | Loss: 0.00006270
Iteration 117/1000 | Loss: 0.00006270
Iteration 118/1000 | Loss: 0.00006270
Iteration 119/1000 | Loss: 0.00006269
Iteration 120/1000 | Loss: 0.00006269
Iteration 121/1000 | Loss: 0.00006269
Iteration 122/1000 | Loss: 0.00006269
Iteration 123/1000 | Loss: 0.00006269
Iteration 124/1000 | Loss: 0.00006269
Iteration 125/1000 | Loss: 0.00006269
Iteration 126/1000 | Loss: 0.00006269
Iteration 127/1000 | Loss: 0.00006269
Iteration 128/1000 | Loss: 0.00006269
Iteration 129/1000 | Loss: 0.00006269
Iteration 130/1000 | Loss: 0.00006269
Iteration 131/1000 | Loss: 0.00006269
Iteration 132/1000 | Loss: 0.00006269
Iteration 133/1000 | Loss: 0.00006269
Iteration 134/1000 | Loss: 0.00006269
Iteration 135/1000 | Loss: 0.00006269
Iteration 136/1000 | Loss: 0.00006269
Iteration 137/1000 | Loss: 0.00006269
Iteration 138/1000 | Loss: 0.00006269
Iteration 139/1000 | Loss: 0.00006269
Iteration 140/1000 | Loss: 0.00006269
Iteration 141/1000 | Loss: 0.00006269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [6.268684228416532e-05, 6.268684228416532e-05, 6.268684228416532e-05, 6.268684228416532e-05, 6.268684228416532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.268684228416532e-05

Optimization complete. Final v2v error: 4.911869525909424 mm

Highest mean error: 12.036818504333496 mm for frame 67

Lowest mean error: 3.8247287273406982 mm for frame 151

Saving results

Total time: 99.32511615753174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002896
Iteration 2/25 | Loss: 0.01002895
Iteration 3/25 | Loss: 0.00246955
Iteration 4/25 | Loss: 0.00178032
Iteration 5/25 | Loss: 0.00161254
Iteration 6/25 | Loss: 0.00156069
Iteration 7/25 | Loss: 0.00146332
Iteration 8/25 | Loss: 0.00142146
Iteration 9/25 | Loss: 0.00141536
Iteration 10/25 | Loss: 0.00142276
Iteration 11/25 | Loss: 0.00139602
Iteration 12/25 | Loss: 0.00138532
Iteration 13/25 | Loss: 0.00138138
Iteration 14/25 | Loss: 0.00138017
Iteration 15/25 | Loss: 0.00137869
Iteration 16/25 | Loss: 0.00137832
Iteration 17/25 | Loss: 0.00137671
Iteration 18/25 | Loss: 0.00137659
Iteration 19/25 | Loss: 0.00137654
Iteration 20/25 | Loss: 0.00137654
Iteration 21/25 | Loss: 0.00137653
Iteration 22/25 | Loss: 0.00137653
Iteration 23/25 | Loss: 0.00137653
Iteration 24/25 | Loss: 0.00137653
Iteration 25/25 | Loss: 0.00137653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38560188
Iteration 2/25 | Loss: 0.00104160
Iteration 3/25 | Loss: 0.00104160
Iteration 4/25 | Loss: 0.00104160
Iteration 5/25 | Loss: 0.00104160
Iteration 6/25 | Loss: 0.00104160
Iteration 7/25 | Loss: 0.00104160
Iteration 8/25 | Loss: 0.00104160
Iteration 9/25 | Loss: 0.00104160
Iteration 10/25 | Loss: 0.00104160
Iteration 11/25 | Loss: 0.00104160
Iteration 12/25 | Loss: 0.00104160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001041596056893468, 0.001041596056893468, 0.001041596056893468, 0.001041596056893468, 0.001041596056893468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001041596056893468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104160
Iteration 2/1000 | Loss: 0.00005155
Iteration 3/1000 | Loss: 0.00003673
Iteration 4/1000 | Loss: 0.00003319
Iteration 5/1000 | Loss: 0.00011189
Iteration 6/1000 | Loss: 0.00018092
Iteration 7/1000 | Loss: 0.00031443
Iteration 8/1000 | Loss: 0.00010156
Iteration 9/1000 | Loss: 0.00002963
Iteration 10/1000 | Loss: 0.00002661
Iteration 11/1000 | Loss: 0.00002485
Iteration 12/1000 | Loss: 0.00002397
Iteration 13/1000 | Loss: 0.00002333
Iteration 14/1000 | Loss: 0.00002284
Iteration 15/1000 | Loss: 0.00002242
Iteration 16/1000 | Loss: 0.00002211
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002168
Iteration 19/1000 | Loss: 0.00002154
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002150
Iteration 22/1000 | Loss: 0.00002149
Iteration 23/1000 | Loss: 0.00002149
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002147
Iteration 26/1000 | Loss: 0.00002146
Iteration 27/1000 | Loss: 0.00002146
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002143
Iteration 32/1000 | Loss: 0.00002143
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002140
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002139
Iteration 44/1000 | Loss: 0.00002139
Iteration 45/1000 | Loss: 0.00002139
Iteration 46/1000 | Loss: 0.00002139
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002138
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002137
Iteration 54/1000 | Loss: 0.00002137
Iteration 55/1000 | Loss: 0.00002137
Iteration 56/1000 | Loss: 0.00002137
Iteration 57/1000 | Loss: 0.00002137
Iteration 58/1000 | Loss: 0.00002137
Iteration 59/1000 | Loss: 0.00002137
Iteration 60/1000 | Loss: 0.00002137
Iteration 61/1000 | Loss: 0.00002136
Iteration 62/1000 | Loss: 0.00002136
Iteration 63/1000 | Loss: 0.00002136
Iteration 64/1000 | Loss: 0.00002136
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002136
Iteration 68/1000 | Loss: 0.00002136
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002135
Iteration 73/1000 | Loss: 0.00002135
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002135
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002135
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002134
Iteration 80/1000 | Loss: 0.00002134
Iteration 81/1000 | Loss: 0.00002134
Iteration 82/1000 | Loss: 0.00002134
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002134
Iteration 87/1000 | Loss: 0.00002134
Iteration 88/1000 | Loss: 0.00002134
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002134
Iteration 91/1000 | Loss: 0.00002134
Iteration 92/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.134194437530823e-05, 2.134194437530823e-05, 2.134194437530823e-05, 2.134194437530823e-05, 2.134194437530823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.134194437530823e-05

Optimization complete. Final v2v error: 3.943190336227417 mm

Highest mean error: 4.263270854949951 mm for frame 3

Lowest mean error: 3.7816083431243896 mm for frame 12

Saving results

Total time: 71.33433699607849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530217
Iteration 2/25 | Loss: 0.00153913
Iteration 3/25 | Loss: 0.00134852
Iteration 4/25 | Loss: 0.00133477
Iteration 5/25 | Loss: 0.00133272
Iteration 6/25 | Loss: 0.00133272
Iteration 7/25 | Loss: 0.00133272
Iteration 8/25 | Loss: 0.00133272
Iteration 9/25 | Loss: 0.00133272
Iteration 10/25 | Loss: 0.00133272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013327242340892553, 0.0013327242340892553, 0.0013327242340892553, 0.0013327242340892553, 0.0013327242340892553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013327242340892553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42249906
Iteration 2/25 | Loss: 0.00067099
Iteration 3/25 | Loss: 0.00067099
Iteration 4/25 | Loss: 0.00067099
Iteration 5/25 | Loss: 0.00067099
Iteration 6/25 | Loss: 0.00067099
Iteration 7/25 | Loss: 0.00067099
Iteration 8/25 | Loss: 0.00067099
Iteration 9/25 | Loss: 0.00067099
Iteration 10/25 | Loss: 0.00067099
Iteration 11/25 | Loss: 0.00067099
Iteration 12/25 | Loss: 0.00067099
Iteration 13/25 | Loss: 0.00067099
Iteration 14/25 | Loss: 0.00067099
Iteration 15/25 | Loss: 0.00067099
Iteration 16/25 | Loss: 0.00067099
Iteration 17/25 | Loss: 0.00067099
Iteration 18/25 | Loss: 0.00067099
Iteration 19/25 | Loss: 0.00067099
Iteration 20/25 | Loss: 0.00067099
Iteration 21/25 | Loss: 0.00067099
Iteration 22/25 | Loss: 0.00067099
Iteration 23/25 | Loss: 0.00067099
Iteration 24/25 | Loss: 0.00067099
Iteration 25/25 | Loss: 0.00067099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067099
Iteration 2/1000 | Loss: 0.00004257
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002370
Iteration 7/1000 | Loss: 0.00002306
Iteration 8/1000 | Loss: 0.00002263
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002194
Iteration 11/1000 | Loss: 0.00002188
Iteration 12/1000 | Loss: 0.00002188
Iteration 13/1000 | Loss: 0.00002180
Iteration 14/1000 | Loss: 0.00002176
Iteration 15/1000 | Loss: 0.00002171
Iteration 16/1000 | Loss: 0.00002168
Iteration 17/1000 | Loss: 0.00002164
Iteration 18/1000 | Loss: 0.00002164
Iteration 19/1000 | Loss: 0.00002162
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002159
Iteration 23/1000 | Loss: 0.00002155
Iteration 24/1000 | Loss: 0.00002154
Iteration 25/1000 | Loss: 0.00002154
Iteration 26/1000 | Loss: 0.00002153
Iteration 27/1000 | Loss: 0.00002152
Iteration 28/1000 | Loss: 0.00002152
Iteration 29/1000 | Loss: 0.00002152
Iteration 30/1000 | Loss: 0.00002152
Iteration 31/1000 | Loss: 0.00002152
Iteration 32/1000 | Loss: 0.00002151
Iteration 33/1000 | Loss: 0.00002151
Iteration 34/1000 | Loss: 0.00002151
Iteration 35/1000 | Loss: 0.00002151
Iteration 36/1000 | Loss: 0.00002151
Iteration 37/1000 | Loss: 0.00002151
Iteration 38/1000 | Loss: 0.00002150
Iteration 39/1000 | Loss: 0.00002150
Iteration 40/1000 | Loss: 0.00002150
Iteration 41/1000 | Loss: 0.00002150
Iteration 42/1000 | Loss: 0.00002150
Iteration 43/1000 | Loss: 0.00002150
Iteration 44/1000 | Loss: 0.00002150
Iteration 45/1000 | Loss: 0.00002150
Iteration 46/1000 | Loss: 0.00002150
Iteration 47/1000 | Loss: 0.00002149
Iteration 48/1000 | Loss: 0.00002149
Iteration 49/1000 | Loss: 0.00002149
Iteration 50/1000 | Loss: 0.00002149
Iteration 51/1000 | Loss: 0.00002149
Iteration 52/1000 | Loss: 0.00002149
Iteration 53/1000 | Loss: 0.00002148
Iteration 54/1000 | Loss: 0.00002148
Iteration 55/1000 | Loss: 0.00002148
Iteration 56/1000 | Loss: 0.00002148
Iteration 57/1000 | Loss: 0.00002147
Iteration 58/1000 | Loss: 0.00002147
Iteration 59/1000 | Loss: 0.00002147
Iteration 60/1000 | Loss: 0.00002147
Iteration 61/1000 | Loss: 0.00002147
Iteration 62/1000 | Loss: 0.00002147
Iteration 63/1000 | Loss: 0.00002147
Iteration 64/1000 | Loss: 0.00002146
Iteration 65/1000 | Loss: 0.00002146
Iteration 66/1000 | Loss: 0.00002146
Iteration 67/1000 | Loss: 0.00002146
Iteration 68/1000 | Loss: 0.00002146
Iteration 69/1000 | Loss: 0.00002146
Iteration 70/1000 | Loss: 0.00002146
Iteration 71/1000 | Loss: 0.00002146
Iteration 72/1000 | Loss: 0.00002146
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00002144
Iteration 77/1000 | Loss: 0.00002144
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002142
Iteration 81/1000 | Loss: 0.00002142
Iteration 82/1000 | Loss: 0.00002142
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002141
Iteration 85/1000 | Loss: 0.00002141
Iteration 86/1000 | Loss: 0.00002141
Iteration 87/1000 | Loss: 0.00002141
Iteration 88/1000 | Loss: 0.00002141
Iteration 89/1000 | Loss: 0.00002141
Iteration 90/1000 | Loss: 0.00002141
Iteration 91/1000 | Loss: 0.00002141
Iteration 92/1000 | Loss: 0.00002140
Iteration 93/1000 | Loss: 0.00002140
Iteration 94/1000 | Loss: 0.00002140
Iteration 95/1000 | Loss: 0.00002140
Iteration 96/1000 | Loss: 0.00002140
Iteration 97/1000 | Loss: 0.00002139
Iteration 98/1000 | Loss: 0.00002139
Iteration 99/1000 | Loss: 0.00002139
Iteration 100/1000 | Loss: 0.00002139
Iteration 101/1000 | Loss: 0.00002139
Iteration 102/1000 | Loss: 0.00002139
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00002139
Iteration 105/1000 | Loss: 0.00002139
Iteration 106/1000 | Loss: 0.00002138
Iteration 107/1000 | Loss: 0.00002138
Iteration 108/1000 | Loss: 0.00002138
Iteration 109/1000 | Loss: 0.00002138
Iteration 110/1000 | Loss: 0.00002137
Iteration 111/1000 | Loss: 0.00002137
Iteration 112/1000 | Loss: 0.00002137
Iteration 113/1000 | Loss: 0.00002137
Iteration 114/1000 | Loss: 0.00002137
Iteration 115/1000 | Loss: 0.00002137
Iteration 116/1000 | Loss: 0.00002137
Iteration 117/1000 | Loss: 0.00002137
Iteration 118/1000 | Loss: 0.00002137
Iteration 119/1000 | Loss: 0.00002137
Iteration 120/1000 | Loss: 0.00002137
Iteration 121/1000 | Loss: 0.00002137
Iteration 122/1000 | Loss: 0.00002137
Iteration 123/1000 | Loss: 0.00002137
Iteration 124/1000 | Loss: 0.00002137
Iteration 125/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.13677831197856e-05, 2.13677831197856e-05, 2.13677831197856e-05, 2.13677831197856e-05, 2.13677831197856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.13677831197856e-05

Optimization complete. Final v2v error: 3.8896241188049316 mm

Highest mean error: 4.2471466064453125 mm for frame 162

Lowest mean error: 3.680116891860962 mm for frame 205

Saving results

Total time: 34.1898992061615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00359221
Iteration 2/25 | Loss: 0.00140178
Iteration 3/25 | Loss: 0.00125626
Iteration 4/25 | Loss: 0.00123896
Iteration 5/25 | Loss: 0.00123444
Iteration 6/25 | Loss: 0.00123344
Iteration 7/25 | Loss: 0.00123344
Iteration 8/25 | Loss: 0.00123344
Iteration 9/25 | Loss: 0.00123344
Iteration 10/25 | Loss: 0.00123344
Iteration 11/25 | Loss: 0.00123344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012334411730989814, 0.0012334411730989814, 0.0012334411730989814, 0.0012334411730989814, 0.0012334411730989814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012334411730989814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42236471
Iteration 2/25 | Loss: 0.00076361
Iteration 3/25 | Loss: 0.00076360
Iteration 4/25 | Loss: 0.00076360
Iteration 5/25 | Loss: 0.00076360
Iteration 6/25 | Loss: 0.00076359
Iteration 7/25 | Loss: 0.00076359
Iteration 8/25 | Loss: 0.00076359
Iteration 9/25 | Loss: 0.00076359
Iteration 10/25 | Loss: 0.00076359
Iteration 11/25 | Loss: 0.00076359
Iteration 12/25 | Loss: 0.00076359
Iteration 13/25 | Loss: 0.00076359
Iteration 14/25 | Loss: 0.00076359
Iteration 15/25 | Loss: 0.00076359
Iteration 16/25 | Loss: 0.00076359
Iteration 17/25 | Loss: 0.00076359
Iteration 18/25 | Loss: 0.00076359
Iteration 19/25 | Loss: 0.00076359
Iteration 20/25 | Loss: 0.00076359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007635935326106846, 0.0007635935326106846, 0.0007635935326106846, 0.0007635935326106846, 0.0007635935326106846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007635935326106846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076359
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001396
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001374
Iteration 31/1000 | Loss: 0.00001373
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001366
Iteration 41/1000 | Loss: 0.00001363
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.3541799489757977e-05, 1.3541799489757977e-05, 1.3541799489757977e-05, 1.3541799489757977e-05, 1.3541799489757977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3541799489757977e-05

Optimization complete. Final v2v error: 3.1416704654693604 mm

Highest mean error: 3.4474704265594482 mm for frame 162

Lowest mean error: 2.8942348957061768 mm for frame 69

Saving results

Total time: 40.48336672782898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457930
Iteration 2/25 | Loss: 0.00137846
Iteration 3/25 | Loss: 0.00127848
Iteration 4/25 | Loss: 0.00126727
Iteration 5/25 | Loss: 0.00126411
Iteration 6/25 | Loss: 0.00126411
Iteration 7/25 | Loss: 0.00126411
Iteration 8/25 | Loss: 0.00126411
Iteration 9/25 | Loss: 0.00126411
Iteration 10/25 | Loss: 0.00126411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012641127686947584, 0.0012641127686947584, 0.0012641127686947584, 0.0012641127686947584, 0.0012641127686947584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012641127686947584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43400729
Iteration 2/25 | Loss: 0.00088777
Iteration 3/25 | Loss: 0.00088777
Iteration 4/25 | Loss: 0.00088777
Iteration 5/25 | Loss: 0.00088777
Iteration 6/25 | Loss: 0.00088777
Iteration 7/25 | Loss: 0.00088777
Iteration 8/25 | Loss: 0.00088777
Iteration 9/25 | Loss: 0.00088777
Iteration 10/25 | Loss: 0.00088777
Iteration 11/25 | Loss: 0.00088777
Iteration 12/25 | Loss: 0.00088777
Iteration 13/25 | Loss: 0.00088777
Iteration 14/25 | Loss: 0.00088777
Iteration 15/25 | Loss: 0.00088777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008877707878127694, 0.0008877707878127694, 0.0008877707878127694, 0.0008877707878127694, 0.0008877707878127694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008877707878127694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088777
Iteration 2/1000 | Loss: 0.00002497
Iteration 3/1000 | Loss: 0.00001833
Iteration 4/1000 | Loss: 0.00001695
Iteration 5/1000 | Loss: 0.00001618
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001420
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001398
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001364
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001353
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001326
Iteration 92/1000 | Loss: 0.00001326
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001325
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.3248943105281796e-05, 1.3248943105281796e-05, 1.3248943105281796e-05, 1.3248943105281796e-05, 1.3248943105281796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3248943105281796e-05

Optimization complete. Final v2v error: 3.122187376022339 mm

Highest mean error: 3.543480634689331 mm for frame 221

Lowest mean error: 2.931049346923828 mm for frame 22

Saving results

Total time: 41.54419469833374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029528
Iteration 2/25 | Loss: 0.00206150
Iteration 3/25 | Loss: 0.00159300
Iteration 4/25 | Loss: 0.00153814
Iteration 5/25 | Loss: 0.00148504
Iteration 6/25 | Loss: 0.00146210
Iteration 7/25 | Loss: 0.00144838
Iteration 8/25 | Loss: 0.00146441
Iteration 9/25 | Loss: 0.00145283
Iteration 10/25 | Loss: 0.00144317
Iteration 11/25 | Loss: 0.00142442
Iteration 12/25 | Loss: 0.00141848
Iteration 13/25 | Loss: 0.00141637
Iteration 14/25 | Loss: 0.00143629
Iteration 15/25 | Loss: 0.00141148
Iteration 16/25 | Loss: 0.00140499
Iteration 17/25 | Loss: 0.00140251
Iteration 18/25 | Loss: 0.00140475
Iteration 19/25 | Loss: 0.00140545
Iteration 20/25 | Loss: 0.00140110
Iteration 21/25 | Loss: 0.00139974
Iteration 22/25 | Loss: 0.00139935
Iteration 23/25 | Loss: 0.00139925
Iteration 24/25 | Loss: 0.00139925
Iteration 25/25 | Loss: 0.00139924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43086767
Iteration 2/25 | Loss: 0.00079067
Iteration 3/25 | Loss: 0.00079067
Iteration 4/25 | Loss: 0.00079067
Iteration 5/25 | Loss: 0.00079067
Iteration 6/25 | Loss: 0.00079067
Iteration 7/25 | Loss: 0.00079067
Iteration 8/25 | Loss: 0.00079067
Iteration 9/25 | Loss: 0.00079067
Iteration 10/25 | Loss: 0.00079067
Iteration 11/25 | Loss: 0.00079067
Iteration 12/25 | Loss: 0.00079067
Iteration 13/25 | Loss: 0.00079067
Iteration 14/25 | Loss: 0.00079067
Iteration 15/25 | Loss: 0.00079067
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007906727842055261, 0.0007906727842055261, 0.0007906727842055261, 0.0007906727842055261, 0.0007906727842055261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007906727842055261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079067
Iteration 2/1000 | Loss: 0.00010427
Iteration 3/1000 | Loss: 0.00030714
Iteration 4/1000 | Loss: 0.00004663
Iteration 5/1000 | Loss: 0.00009228
Iteration 6/1000 | Loss: 0.00012969
Iteration 7/1000 | Loss: 0.00003564
Iteration 8/1000 | Loss: 0.00006362
Iteration 9/1000 | Loss: 0.00022032
Iteration 10/1000 | Loss: 0.00015573
Iteration 11/1000 | Loss: 0.00006860
Iteration 12/1000 | Loss: 0.00004604
Iteration 13/1000 | Loss: 0.00004734
Iteration 14/1000 | Loss: 0.00003247
Iteration 15/1000 | Loss: 0.00007771
Iteration 16/1000 | Loss: 0.00009032
Iteration 17/1000 | Loss: 0.00003010
Iteration 18/1000 | Loss: 0.00008341
Iteration 19/1000 | Loss: 0.00002903
Iteration 20/1000 | Loss: 0.00002879
Iteration 21/1000 | Loss: 0.00011702
Iteration 22/1000 | Loss: 0.00015409
Iteration 23/1000 | Loss: 0.00022198
Iteration 24/1000 | Loss: 0.00003040
Iteration 25/1000 | Loss: 0.00002818
Iteration 26/1000 | Loss: 0.00002679
Iteration 27/1000 | Loss: 0.00002615
Iteration 28/1000 | Loss: 0.00002589
Iteration 29/1000 | Loss: 0.00002571
Iteration 30/1000 | Loss: 0.00004830
Iteration 31/1000 | Loss: 0.00003015
Iteration 32/1000 | Loss: 0.00002599
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002537
Iteration 35/1000 | Loss: 0.00002537
Iteration 36/1000 | Loss: 0.00002536
Iteration 37/1000 | Loss: 0.00002536
Iteration 38/1000 | Loss: 0.00002536
Iteration 39/1000 | Loss: 0.00002536
Iteration 40/1000 | Loss: 0.00002536
Iteration 41/1000 | Loss: 0.00002536
Iteration 42/1000 | Loss: 0.00002536
Iteration 43/1000 | Loss: 0.00002536
Iteration 44/1000 | Loss: 0.00002536
Iteration 45/1000 | Loss: 0.00002536
Iteration 46/1000 | Loss: 0.00002536
Iteration 47/1000 | Loss: 0.00002535
Iteration 48/1000 | Loss: 0.00002535
Iteration 49/1000 | Loss: 0.00002535
Iteration 50/1000 | Loss: 0.00002535
Iteration 51/1000 | Loss: 0.00002535
Iteration 52/1000 | Loss: 0.00002535
Iteration 53/1000 | Loss: 0.00002535
Iteration 54/1000 | Loss: 0.00002535
Iteration 55/1000 | Loss: 0.00002535
Iteration 56/1000 | Loss: 0.00002534
Iteration 57/1000 | Loss: 0.00002534
Iteration 58/1000 | Loss: 0.00002534
Iteration 59/1000 | Loss: 0.00002534
Iteration 60/1000 | Loss: 0.00002534
Iteration 61/1000 | Loss: 0.00002534
Iteration 62/1000 | Loss: 0.00002534
Iteration 63/1000 | Loss: 0.00002534
Iteration 64/1000 | Loss: 0.00002534
Iteration 65/1000 | Loss: 0.00002534
Iteration 66/1000 | Loss: 0.00002534
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002533
Iteration 70/1000 | Loss: 0.00002533
Iteration 71/1000 | Loss: 0.00002532
Iteration 72/1000 | Loss: 0.00002532
Iteration 73/1000 | Loss: 0.00002531
Iteration 74/1000 | Loss: 0.00002531
Iteration 75/1000 | Loss: 0.00002530
Iteration 76/1000 | Loss: 0.00002530
Iteration 77/1000 | Loss: 0.00002530
Iteration 78/1000 | Loss: 0.00002529
Iteration 79/1000 | Loss: 0.00002529
Iteration 80/1000 | Loss: 0.00002529
Iteration 81/1000 | Loss: 0.00002529
Iteration 82/1000 | Loss: 0.00002529
Iteration 83/1000 | Loss: 0.00002528
Iteration 84/1000 | Loss: 0.00002528
Iteration 85/1000 | Loss: 0.00002528
Iteration 86/1000 | Loss: 0.00002528
Iteration 87/1000 | Loss: 0.00002528
Iteration 88/1000 | Loss: 0.00002528
Iteration 89/1000 | Loss: 0.00002528
Iteration 90/1000 | Loss: 0.00002528
Iteration 91/1000 | Loss: 0.00002528
Iteration 92/1000 | Loss: 0.00002528
Iteration 93/1000 | Loss: 0.00002527
Iteration 94/1000 | Loss: 0.00002527
Iteration 95/1000 | Loss: 0.00002527
Iteration 96/1000 | Loss: 0.00002527
Iteration 97/1000 | Loss: 0.00002527
Iteration 98/1000 | Loss: 0.00002527
Iteration 99/1000 | Loss: 0.00002526
Iteration 100/1000 | Loss: 0.00002526
Iteration 101/1000 | Loss: 0.00002526
Iteration 102/1000 | Loss: 0.00002526
Iteration 103/1000 | Loss: 0.00002525
Iteration 104/1000 | Loss: 0.00002524
Iteration 105/1000 | Loss: 0.00002524
Iteration 106/1000 | Loss: 0.00002523
Iteration 107/1000 | Loss: 0.00002523
Iteration 108/1000 | Loss: 0.00002523
Iteration 109/1000 | Loss: 0.00002523
Iteration 110/1000 | Loss: 0.00002523
Iteration 111/1000 | Loss: 0.00002523
Iteration 112/1000 | Loss: 0.00002523
Iteration 113/1000 | Loss: 0.00002523
Iteration 114/1000 | Loss: 0.00002523
Iteration 115/1000 | Loss: 0.00002523
Iteration 116/1000 | Loss: 0.00002523
Iteration 117/1000 | Loss: 0.00002523
Iteration 118/1000 | Loss: 0.00002522
Iteration 119/1000 | Loss: 0.00002522
Iteration 120/1000 | Loss: 0.00002522
Iteration 121/1000 | Loss: 0.00002522
Iteration 122/1000 | Loss: 0.00002522
Iteration 123/1000 | Loss: 0.00002521
Iteration 124/1000 | Loss: 0.00002521
Iteration 125/1000 | Loss: 0.00002521
Iteration 126/1000 | Loss: 0.00002521
Iteration 127/1000 | Loss: 0.00002521
Iteration 128/1000 | Loss: 0.00002521
Iteration 129/1000 | Loss: 0.00002520
Iteration 130/1000 | Loss: 0.00002520
Iteration 131/1000 | Loss: 0.00002520
Iteration 132/1000 | Loss: 0.00002520
Iteration 133/1000 | Loss: 0.00002520
Iteration 134/1000 | Loss: 0.00002520
Iteration 135/1000 | Loss: 0.00002520
Iteration 136/1000 | Loss: 0.00002520
Iteration 137/1000 | Loss: 0.00002520
Iteration 138/1000 | Loss: 0.00002520
Iteration 139/1000 | Loss: 0.00002520
Iteration 140/1000 | Loss: 0.00002520
Iteration 141/1000 | Loss: 0.00002520
Iteration 142/1000 | Loss: 0.00002520
Iteration 143/1000 | Loss: 0.00002520
Iteration 144/1000 | Loss: 0.00002520
Iteration 145/1000 | Loss: 0.00002520
Iteration 146/1000 | Loss: 0.00002520
Iteration 147/1000 | Loss: 0.00002520
Iteration 148/1000 | Loss: 0.00002520
Iteration 149/1000 | Loss: 0.00002520
Iteration 150/1000 | Loss: 0.00002520
Iteration 151/1000 | Loss: 0.00002520
Iteration 152/1000 | Loss: 0.00002520
Iteration 153/1000 | Loss: 0.00002520
Iteration 154/1000 | Loss: 0.00002520
Iteration 155/1000 | Loss: 0.00002520
Iteration 156/1000 | Loss: 0.00002520
Iteration 157/1000 | Loss: 0.00002520
Iteration 158/1000 | Loss: 0.00002520
Iteration 159/1000 | Loss: 0.00002520
Iteration 160/1000 | Loss: 0.00002520
Iteration 161/1000 | Loss: 0.00002520
Iteration 162/1000 | Loss: 0.00002520
Iteration 163/1000 | Loss: 0.00002520
Iteration 164/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.5200482923537493e-05, 2.5200482923537493e-05, 2.5200482923537493e-05, 2.5200482923537493e-05, 2.5200482923537493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5200482923537493e-05

Optimization complete. Final v2v error: 4.199519157409668 mm

Highest mean error: 5.234407424926758 mm for frame 87

Lowest mean error: 3.6337287425994873 mm for frame 133

Saving results

Total time: 107.8422122001648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00650114
Iteration 2/25 | Loss: 0.00163113
Iteration 3/25 | Loss: 0.00150272
Iteration 4/25 | Loss: 0.00149226
Iteration 5/25 | Loss: 0.00148972
Iteration 6/25 | Loss: 0.00148971
Iteration 7/25 | Loss: 0.00148971
Iteration 8/25 | Loss: 0.00148971
Iteration 9/25 | Loss: 0.00148971
Iteration 10/25 | Loss: 0.00148971
Iteration 11/25 | Loss: 0.00148971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00148971495218575, 0.00148971495218575, 0.00148971495218575, 0.00148971495218575, 0.00148971495218575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00148971495218575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66098005
Iteration 2/25 | Loss: 0.00103844
Iteration 3/25 | Loss: 0.00103843
Iteration 4/25 | Loss: 0.00103843
Iteration 5/25 | Loss: 0.00103843
Iteration 6/25 | Loss: 0.00103843
Iteration 7/25 | Loss: 0.00103843
Iteration 8/25 | Loss: 0.00103843
Iteration 9/25 | Loss: 0.00103843
Iteration 10/25 | Loss: 0.00103842
Iteration 11/25 | Loss: 0.00103842
Iteration 12/25 | Loss: 0.00103842
Iteration 13/25 | Loss: 0.00103842
Iteration 14/25 | Loss: 0.00103842
Iteration 15/25 | Loss: 0.00103842
Iteration 16/25 | Loss: 0.00103842
Iteration 17/25 | Loss: 0.00103842
Iteration 18/25 | Loss: 0.00103842
Iteration 19/25 | Loss: 0.00103842
Iteration 20/25 | Loss: 0.00103842
Iteration 21/25 | Loss: 0.00103842
Iteration 22/25 | Loss: 0.00103842
Iteration 23/25 | Loss: 0.00103842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010384240886196494, 0.0010384240886196494, 0.0010384240886196494, 0.0010384240886196494, 0.0010384240886196494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010384240886196494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103842
Iteration 2/1000 | Loss: 0.00005095
Iteration 3/1000 | Loss: 0.00003590
Iteration 4/1000 | Loss: 0.00003332
Iteration 5/1000 | Loss: 0.00003200
Iteration 6/1000 | Loss: 0.00003126
Iteration 7/1000 | Loss: 0.00003081
Iteration 8/1000 | Loss: 0.00003023
Iteration 9/1000 | Loss: 0.00002980
Iteration 10/1000 | Loss: 0.00002947
Iteration 11/1000 | Loss: 0.00002916
Iteration 12/1000 | Loss: 0.00002891
Iteration 13/1000 | Loss: 0.00002874
Iteration 14/1000 | Loss: 0.00002869
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00002868
Iteration 17/1000 | Loss: 0.00002866
Iteration 18/1000 | Loss: 0.00002866
Iteration 19/1000 | Loss: 0.00002855
Iteration 20/1000 | Loss: 0.00002852
Iteration 21/1000 | Loss: 0.00002848
Iteration 22/1000 | Loss: 0.00002844
Iteration 23/1000 | Loss: 0.00002841
Iteration 24/1000 | Loss: 0.00002836
Iteration 25/1000 | Loss: 0.00002835
Iteration 26/1000 | Loss: 0.00002830
Iteration 27/1000 | Loss: 0.00002828
Iteration 28/1000 | Loss: 0.00002828
Iteration 29/1000 | Loss: 0.00002827
Iteration 30/1000 | Loss: 0.00002827
Iteration 31/1000 | Loss: 0.00002827
Iteration 32/1000 | Loss: 0.00002827
Iteration 33/1000 | Loss: 0.00002827
Iteration 34/1000 | Loss: 0.00002825
Iteration 35/1000 | Loss: 0.00002824
Iteration 36/1000 | Loss: 0.00002824
Iteration 37/1000 | Loss: 0.00002823
Iteration 38/1000 | Loss: 0.00002823
Iteration 39/1000 | Loss: 0.00002823
Iteration 40/1000 | Loss: 0.00002823
Iteration 41/1000 | Loss: 0.00002822
Iteration 42/1000 | Loss: 0.00002822
Iteration 43/1000 | Loss: 0.00002822
Iteration 44/1000 | Loss: 0.00002822
Iteration 45/1000 | Loss: 0.00002822
Iteration 46/1000 | Loss: 0.00002822
Iteration 47/1000 | Loss: 0.00002822
Iteration 48/1000 | Loss: 0.00002821
Iteration 49/1000 | Loss: 0.00002821
Iteration 50/1000 | Loss: 0.00002821
Iteration 51/1000 | Loss: 0.00002821
Iteration 52/1000 | Loss: 0.00002821
Iteration 53/1000 | Loss: 0.00002821
Iteration 54/1000 | Loss: 0.00002821
Iteration 55/1000 | Loss: 0.00002821
Iteration 56/1000 | Loss: 0.00002821
Iteration 57/1000 | Loss: 0.00002821
Iteration 58/1000 | Loss: 0.00002821
Iteration 59/1000 | Loss: 0.00002821
Iteration 60/1000 | Loss: 0.00002821
Iteration 61/1000 | Loss: 0.00002821
Iteration 62/1000 | Loss: 0.00002820
Iteration 63/1000 | Loss: 0.00002820
Iteration 64/1000 | Loss: 0.00002820
Iteration 65/1000 | Loss: 0.00002820
Iteration 66/1000 | Loss: 0.00002820
Iteration 67/1000 | Loss: 0.00002820
Iteration 68/1000 | Loss: 0.00002820
Iteration 69/1000 | Loss: 0.00002820
Iteration 70/1000 | Loss: 0.00002820
Iteration 71/1000 | Loss: 0.00002820
Iteration 72/1000 | Loss: 0.00002819
Iteration 73/1000 | Loss: 0.00002819
Iteration 74/1000 | Loss: 0.00002819
Iteration 75/1000 | Loss: 0.00002819
Iteration 76/1000 | Loss: 0.00002819
Iteration 77/1000 | Loss: 0.00002819
Iteration 78/1000 | Loss: 0.00002819
Iteration 79/1000 | Loss: 0.00002819
Iteration 80/1000 | Loss: 0.00002819
Iteration 81/1000 | Loss: 0.00002819
Iteration 82/1000 | Loss: 0.00002819
Iteration 83/1000 | Loss: 0.00002819
Iteration 84/1000 | Loss: 0.00002818
Iteration 85/1000 | Loss: 0.00002818
Iteration 86/1000 | Loss: 0.00002818
Iteration 87/1000 | Loss: 0.00002818
Iteration 88/1000 | Loss: 0.00002818
Iteration 89/1000 | Loss: 0.00002818
Iteration 90/1000 | Loss: 0.00002818
Iteration 91/1000 | Loss: 0.00002818
Iteration 92/1000 | Loss: 0.00002818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.8182510504848324e-05, 2.8182510504848324e-05, 2.8182510504848324e-05, 2.8182510504848324e-05, 2.8182510504848324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8182510504848324e-05

Optimization complete. Final v2v error: 4.258898735046387 mm

Highest mean error: 4.935730934143066 mm for frame 173

Lowest mean error: 4.022221565246582 mm for frame 106

Saving results

Total time: 41.48242163658142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020740
Iteration 2/25 | Loss: 0.00182374
Iteration 3/25 | Loss: 0.00151484
Iteration 4/25 | Loss: 0.00150880
Iteration 5/25 | Loss: 0.00138154
Iteration 6/25 | Loss: 0.00140626
Iteration 7/25 | Loss: 0.00135908
Iteration 8/25 | Loss: 0.00133587
Iteration 9/25 | Loss: 0.00132617
Iteration 10/25 | Loss: 0.00131445
Iteration 11/25 | Loss: 0.00130902
Iteration 12/25 | Loss: 0.00130088
Iteration 13/25 | Loss: 0.00129751
Iteration 14/25 | Loss: 0.00129634
Iteration 15/25 | Loss: 0.00129748
Iteration 16/25 | Loss: 0.00129473
Iteration 17/25 | Loss: 0.00129108
Iteration 18/25 | Loss: 0.00129204
Iteration 19/25 | Loss: 0.00129236
Iteration 20/25 | Loss: 0.00129330
Iteration 21/25 | Loss: 0.00129315
Iteration 22/25 | Loss: 0.00129448
Iteration 23/25 | Loss: 0.00129252
Iteration 24/25 | Loss: 0.00129281
Iteration 25/25 | Loss: 0.00129277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45508480
Iteration 2/25 | Loss: 0.00133258
Iteration 3/25 | Loss: 0.00109418
Iteration 4/25 | Loss: 0.00109416
Iteration 5/25 | Loss: 0.00109415
Iteration 6/25 | Loss: 0.00109415
Iteration 7/25 | Loss: 0.00109415
Iteration 8/25 | Loss: 0.00109415
Iteration 9/25 | Loss: 0.00109415
Iteration 10/25 | Loss: 0.00109415
Iteration 11/25 | Loss: 0.00109415
Iteration 12/25 | Loss: 0.00109415
Iteration 13/25 | Loss: 0.00109415
Iteration 14/25 | Loss: 0.00109415
Iteration 15/25 | Loss: 0.00109415
Iteration 16/25 | Loss: 0.00109415
Iteration 17/25 | Loss: 0.00109415
Iteration 18/25 | Loss: 0.00109415
Iteration 19/25 | Loss: 0.00109415
Iteration 20/25 | Loss: 0.00109415
Iteration 21/25 | Loss: 0.00109415
Iteration 22/25 | Loss: 0.00109415
Iteration 23/25 | Loss: 0.00109415
Iteration 24/25 | Loss: 0.00109415
Iteration 25/25 | Loss: 0.00109415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109415
Iteration 2/1000 | Loss: 0.00014718
Iteration 3/1000 | Loss: 0.00006397
Iteration 4/1000 | Loss: 0.00019861
Iteration 5/1000 | Loss: 0.00014350
Iteration 6/1000 | Loss: 0.00005660
Iteration 7/1000 | Loss: 0.00004772
Iteration 8/1000 | Loss: 0.00004232
Iteration 9/1000 | Loss: 0.00023678
Iteration 10/1000 | Loss: 0.00005558
Iteration 11/1000 | Loss: 0.00004864
Iteration 12/1000 | Loss: 0.00017188
Iteration 13/1000 | Loss: 0.00020073
Iteration 14/1000 | Loss: 0.00032286
Iteration 15/1000 | Loss: 0.00005869
Iteration 16/1000 | Loss: 0.00004325
Iteration 17/1000 | Loss: 0.00008861
Iteration 18/1000 | Loss: 0.00004323
Iteration 19/1000 | Loss: 0.00003837
Iteration 20/1000 | Loss: 0.00003536
Iteration 21/1000 | Loss: 0.00003391
Iteration 22/1000 | Loss: 0.00003292
Iteration 23/1000 | Loss: 0.00003224
Iteration 24/1000 | Loss: 0.00043882
Iteration 25/1000 | Loss: 0.00382191
Iteration 26/1000 | Loss: 0.00331304
Iteration 27/1000 | Loss: 0.00284874
Iteration 28/1000 | Loss: 0.00124834
Iteration 29/1000 | Loss: 0.00061314
Iteration 30/1000 | Loss: 0.00012006
Iteration 31/1000 | Loss: 0.00013018
Iteration 32/1000 | Loss: 0.00005522
Iteration 33/1000 | Loss: 0.00038592
Iteration 34/1000 | Loss: 0.00003671
Iteration 35/1000 | Loss: 0.00003038
Iteration 36/1000 | Loss: 0.00002541
Iteration 37/1000 | Loss: 0.00002195
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001796
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001278
Iteration 93/1000 | Loss: 0.00001278
Iteration 94/1000 | Loss: 0.00001278
Iteration 95/1000 | Loss: 0.00001278
Iteration 96/1000 | Loss: 0.00001278
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001275
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001272
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001268
Iteration 127/1000 | Loss: 0.00001268
Iteration 128/1000 | Loss: 0.00001268
Iteration 129/1000 | Loss: 0.00001268
Iteration 130/1000 | Loss: 0.00001268
Iteration 131/1000 | Loss: 0.00001268
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001268
Iteration 134/1000 | Loss: 0.00001268
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001266
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001266
Iteration 143/1000 | Loss: 0.00001266
Iteration 144/1000 | Loss: 0.00001266
Iteration 145/1000 | Loss: 0.00001266
Iteration 146/1000 | Loss: 0.00001266
Iteration 147/1000 | Loss: 0.00001266
Iteration 148/1000 | Loss: 0.00001266
Iteration 149/1000 | Loss: 0.00001266
Iteration 150/1000 | Loss: 0.00001266
Iteration 151/1000 | Loss: 0.00001266
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001265
Iteration 155/1000 | Loss: 0.00001265
Iteration 156/1000 | Loss: 0.00001265
Iteration 157/1000 | Loss: 0.00001265
Iteration 158/1000 | Loss: 0.00001265
Iteration 159/1000 | Loss: 0.00001265
Iteration 160/1000 | Loss: 0.00001265
Iteration 161/1000 | Loss: 0.00001265
Iteration 162/1000 | Loss: 0.00001265
Iteration 163/1000 | Loss: 0.00001264
Iteration 164/1000 | Loss: 0.00001264
Iteration 165/1000 | Loss: 0.00001264
Iteration 166/1000 | Loss: 0.00001264
Iteration 167/1000 | Loss: 0.00001264
Iteration 168/1000 | Loss: 0.00001264
Iteration 169/1000 | Loss: 0.00001264
Iteration 170/1000 | Loss: 0.00001264
Iteration 171/1000 | Loss: 0.00001264
Iteration 172/1000 | Loss: 0.00001264
Iteration 173/1000 | Loss: 0.00001264
Iteration 174/1000 | Loss: 0.00001264
Iteration 175/1000 | Loss: 0.00001264
Iteration 176/1000 | Loss: 0.00001264
Iteration 177/1000 | Loss: 0.00001264
Iteration 178/1000 | Loss: 0.00001264
Iteration 179/1000 | Loss: 0.00001264
Iteration 180/1000 | Loss: 0.00001264
Iteration 181/1000 | Loss: 0.00001263
Iteration 182/1000 | Loss: 0.00001263
Iteration 183/1000 | Loss: 0.00001263
Iteration 184/1000 | Loss: 0.00001263
Iteration 185/1000 | Loss: 0.00001263
Iteration 186/1000 | Loss: 0.00001263
Iteration 187/1000 | Loss: 0.00001263
Iteration 188/1000 | Loss: 0.00001263
Iteration 189/1000 | Loss: 0.00001263
Iteration 190/1000 | Loss: 0.00001263
Iteration 191/1000 | Loss: 0.00001263
Iteration 192/1000 | Loss: 0.00001263
Iteration 193/1000 | Loss: 0.00001263
Iteration 194/1000 | Loss: 0.00001263
Iteration 195/1000 | Loss: 0.00001263
Iteration 196/1000 | Loss: 0.00001263
Iteration 197/1000 | Loss: 0.00001263
Iteration 198/1000 | Loss: 0.00001263
Iteration 199/1000 | Loss: 0.00001263
Iteration 200/1000 | Loss: 0.00001263
Iteration 201/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.2629015145648737e-05, 1.2629015145648737e-05, 1.2629015145648737e-05, 1.2629015145648737e-05, 1.2629015145648737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2629015145648737e-05

Optimization complete. Final v2v error: 3.0176656246185303 mm

Highest mean error: 3.5496654510498047 mm for frame 96

Lowest mean error: 2.821770668029785 mm for frame 25

Saving results

Total time: 124.53010892868042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815548
Iteration 2/25 | Loss: 0.00137032
Iteration 3/25 | Loss: 0.00125936
Iteration 4/25 | Loss: 0.00124788
Iteration 5/25 | Loss: 0.00124588
Iteration 6/25 | Loss: 0.00124539
Iteration 7/25 | Loss: 0.00124539
Iteration 8/25 | Loss: 0.00124539
Iteration 9/25 | Loss: 0.00124539
Iteration 10/25 | Loss: 0.00124539
Iteration 11/25 | Loss: 0.00124539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001245393417775631, 0.001245393417775631, 0.001245393417775631, 0.001245393417775631, 0.001245393417775631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001245393417775631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41832316
Iteration 2/25 | Loss: 0.00075764
Iteration 3/25 | Loss: 0.00075761
Iteration 4/25 | Loss: 0.00075761
Iteration 5/25 | Loss: 0.00075761
Iteration 6/25 | Loss: 0.00075761
Iteration 7/25 | Loss: 0.00075761
Iteration 8/25 | Loss: 0.00075761
Iteration 9/25 | Loss: 0.00075761
Iteration 10/25 | Loss: 0.00075761
Iteration 11/25 | Loss: 0.00075761
Iteration 12/25 | Loss: 0.00075761
Iteration 13/25 | Loss: 0.00075761
Iteration 14/25 | Loss: 0.00075761
Iteration 15/25 | Loss: 0.00075761
Iteration 16/25 | Loss: 0.00075761
Iteration 17/25 | Loss: 0.00075761
Iteration 18/25 | Loss: 0.00075761
Iteration 19/25 | Loss: 0.00075761
Iteration 20/25 | Loss: 0.00075761
Iteration 21/25 | Loss: 0.00075761
Iteration 22/25 | Loss: 0.00075761
Iteration 23/25 | Loss: 0.00075761
Iteration 24/25 | Loss: 0.00075761
Iteration 25/25 | Loss: 0.00075761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007576093194074929, 0.0007576093194074929, 0.0007576093194074929, 0.0007576093194074929, 0.0007576093194074929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007576093194074929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075761
Iteration 2/1000 | Loss: 0.00002559
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001157
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001141
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001128
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001112
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001104
Iteration 29/1000 | Loss: 0.00001103
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001096
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001094
Iteration 42/1000 | Loss: 0.00001094
Iteration 43/1000 | Loss: 0.00001093
Iteration 44/1000 | Loss: 0.00001093
Iteration 45/1000 | Loss: 0.00001093
Iteration 46/1000 | Loss: 0.00001092
Iteration 47/1000 | Loss: 0.00001092
Iteration 48/1000 | Loss: 0.00001092
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001090
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001089
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001088
Iteration 70/1000 | Loss: 0.00001088
Iteration 71/1000 | Loss: 0.00001087
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001085
Iteration 79/1000 | Loss: 0.00001085
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001084
Iteration 83/1000 | Loss: 0.00001084
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001083
Iteration 87/1000 | Loss: 0.00001083
Iteration 88/1000 | Loss: 0.00001083
Iteration 89/1000 | Loss: 0.00001083
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001082
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001081
Iteration 103/1000 | Loss: 0.00001081
Iteration 104/1000 | Loss: 0.00001080
Iteration 105/1000 | Loss: 0.00001080
Iteration 106/1000 | Loss: 0.00001080
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001078
Iteration 116/1000 | Loss: 0.00001078
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001078
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001077
Iteration 122/1000 | Loss: 0.00001077
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001073
Iteration 139/1000 | Loss: 0.00001073
Iteration 140/1000 | Loss: 0.00001073
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001071
Iteration 151/1000 | Loss: 0.00001071
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001070
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001070
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001070
Iteration 160/1000 | Loss: 0.00001070
Iteration 161/1000 | Loss: 0.00001070
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Iteration 166/1000 | Loss: 0.00001069
Iteration 167/1000 | Loss: 0.00001069
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001068
Iteration 171/1000 | Loss: 0.00001068
Iteration 172/1000 | Loss: 0.00001068
Iteration 173/1000 | Loss: 0.00001068
Iteration 174/1000 | Loss: 0.00001068
Iteration 175/1000 | Loss: 0.00001068
Iteration 176/1000 | Loss: 0.00001068
Iteration 177/1000 | Loss: 0.00001068
Iteration 178/1000 | Loss: 0.00001068
Iteration 179/1000 | Loss: 0.00001068
Iteration 180/1000 | Loss: 0.00001068
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001067
Iteration 190/1000 | Loss: 0.00001067
Iteration 191/1000 | Loss: 0.00001067
Iteration 192/1000 | Loss: 0.00001067
Iteration 193/1000 | Loss: 0.00001067
Iteration 194/1000 | Loss: 0.00001067
Iteration 195/1000 | Loss: 0.00001067
Iteration 196/1000 | Loss: 0.00001067
Iteration 197/1000 | Loss: 0.00001066
Iteration 198/1000 | Loss: 0.00001066
Iteration 199/1000 | Loss: 0.00001066
Iteration 200/1000 | Loss: 0.00001066
Iteration 201/1000 | Loss: 0.00001066
Iteration 202/1000 | Loss: 0.00001066
Iteration 203/1000 | Loss: 0.00001066
Iteration 204/1000 | Loss: 0.00001066
Iteration 205/1000 | Loss: 0.00001066
Iteration 206/1000 | Loss: 0.00001066
Iteration 207/1000 | Loss: 0.00001066
Iteration 208/1000 | Loss: 0.00001066
Iteration 209/1000 | Loss: 0.00001066
Iteration 210/1000 | Loss: 0.00001066
Iteration 211/1000 | Loss: 0.00001066
Iteration 212/1000 | Loss: 0.00001066
Iteration 213/1000 | Loss: 0.00001066
Iteration 214/1000 | Loss: 0.00001066
Iteration 215/1000 | Loss: 0.00001066
Iteration 216/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.0663894499884918e-05, 1.0663894499884918e-05, 1.0663894499884918e-05, 1.0663894499884918e-05, 1.0663894499884918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0663894499884918e-05

Optimization complete. Final v2v error: 2.793205976486206 mm

Highest mean error: 3.1698203086853027 mm for frame 0

Lowest mean error: 2.6762149333953857 mm for frame 91

Saving results

Total time: 40.54815649986267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996086
Iteration 2/25 | Loss: 0.00297323
Iteration 3/25 | Loss: 0.00234031
Iteration 4/25 | Loss: 0.00216871
Iteration 5/25 | Loss: 0.00209580
Iteration 6/25 | Loss: 0.00206334
Iteration 7/25 | Loss: 0.00203949
Iteration 8/25 | Loss: 0.00201412
Iteration 9/25 | Loss: 0.00200820
Iteration 10/25 | Loss: 0.00200623
Iteration 11/25 | Loss: 0.00200540
Iteration 12/25 | Loss: 0.00200512
Iteration 13/25 | Loss: 0.00200500
Iteration 14/25 | Loss: 0.00200489
Iteration 15/25 | Loss: 0.00200473
Iteration 16/25 | Loss: 0.00200432
Iteration 17/25 | Loss: 0.00200387
Iteration 18/25 | Loss: 0.00200367
Iteration 19/25 | Loss: 0.00200363
Iteration 20/25 | Loss: 0.00200363
Iteration 21/25 | Loss: 0.00200362
Iteration 22/25 | Loss: 0.00200362
Iteration 23/25 | Loss: 0.00200362
Iteration 24/25 | Loss: 0.00200362
Iteration 25/25 | Loss: 0.00200362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41573036
Iteration 2/25 | Loss: 0.00326517
Iteration 3/25 | Loss: 0.00326517
Iteration 4/25 | Loss: 0.00326517
Iteration 5/25 | Loss: 0.00326517
Iteration 6/25 | Loss: 0.00326517
Iteration 7/25 | Loss: 0.00326517
Iteration 8/25 | Loss: 0.00326517
Iteration 9/25 | Loss: 0.00326517
Iteration 10/25 | Loss: 0.00326517
Iteration 11/25 | Loss: 0.00326517
Iteration 12/25 | Loss: 0.00326517
Iteration 13/25 | Loss: 0.00326517
Iteration 14/25 | Loss: 0.00326517
Iteration 15/25 | Loss: 0.00326517
Iteration 16/25 | Loss: 0.00326517
Iteration 17/25 | Loss: 0.00326517
Iteration 18/25 | Loss: 0.00326517
Iteration 19/25 | Loss: 0.00326517
Iteration 20/25 | Loss: 0.00326517
Iteration 21/25 | Loss: 0.00326517
Iteration 22/25 | Loss: 0.00326517
Iteration 23/25 | Loss: 0.00326517
Iteration 24/25 | Loss: 0.00326517
Iteration 25/25 | Loss: 0.00326517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326517
Iteration 2/1000 | Loss: 0.00053975
Iteration 3/1000 | Loss: 0.00042629
Iteration 4/1000 | Loss: 0.00039109
Iteration 5/1000 | Loss: 0.00035998
Iteration 6/1000 | Loss: 0.00033996
Iteration 7/1000 | Loss: 0.00031934
Iteration 8/1000 | Loss: 0.00030571
Iteration 9/1000 | Loss: 0.00048944
Iteration 10/1000 | Loss: 0.01577202
Iteration 11/1000 | Loss: 0.00387460
Iteration 12/1000 | Loss: 0.00054003
Iteration 13/1000 | Loss: 0.00027042
Iteration 14/1000 | Loss: 0.00018019
Iteration 15/1000 | Loss: 0.00012817
Iteration 16/1000 | Loss: 0.00009583
Iteration 17/1000 | Loss: 0.00006784
Iteration 18/1000 | Loss: 0.00005213
Iteration 19/1000 | Loss: 0.00004385
Iteration 20/1000 | Loss: 0.00003746
Iteration 21/1000 | Loss: 0.00003313
Iteration 22/1000 | Loss: 0.00002995
Iteration 23/1000 | Loss: 0.00002749
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002365
Iteration 26/1000 | Loss: 0.00002253
Iteration 27/1000 | Loss: 0.00002164
Iteration 28/1000 | Loss: 0.00002094
Iteration 29/1000 | Loss: 0.00002057
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002006
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001983
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001975
Iteration 37/1000 | Loss: 0.00001974
Iteration 38/1000 | Loss: 0.00001973
Iteration 39/1000 | Loss: 0.00001972
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001971
Iteration 43/1000 | Loss: 0.00001971
Iteration 44/1000 | Loss: 0.00001971
Iteration 45/1000 | Loss: 0.00001971
Iteration 46/1000 | Loss: 0.00001971
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001970
Iteration 49/1000 | Loss: 0.00001970
Iteration 50/1000 | Loss: 0.00001969
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001968
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001968
Iteration 57/1000 | Loss: 0.00001968
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001967
Iteration 61/1000 | Loss: 0.00001967
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001961
Iteration 104/1000 | Loss: 0.00001961
Iteration 105/1000 | Loss: 0.00001961
Iteration 106/1000 | Loss: 0.00001961
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001959
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001959
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001958
Iteration 132/1000 | Loss: 0.00001958
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001957
Iteration 135/1000 | Loss: 0.00001957
Iteration 136/1000 | Loss: 0.00001957
Iteration 137/1000 | Loss: 0.00001957
Iteration 138/1000 | Loss: 0.00001957
Iteration 139/1000 | Loss: 0.00001957
Iteration 140/1000 | Loss: 0.00001957
Iteration 141/1000 | Loss: 0.00001957
Iteration 142/1000 | Loss: 0.00001957
Iteration 143/1000 | Loss: 0.00001956
Iteration 144/1000 | Loss: 0.00001956
Iteration 145/1000 | Loss: 0.00001956
Iteration 146/1000 | Loss: 0.00001956
Iteration 147/1000 | Loss: 0.00001956
Iteration 148/1000 | Loss: 0.00001956
Iteration 149/1000 | Loss: 0.00001956
Iteration 150/1000 | Loss: 0.00001956
Iteration 151/1000 | Loss: 0.00001956
Iteration 152/1000 | Loss: 0.00001956
Iteration 153/1000 | Loss: 0.00001956
Iteration 154/1000 | Loss: 0.00001956
Iteration 155/1000 | Loss: 0.00001956
Iteration 156/1000 | Loss: 0.00001956
Iteration 157/1000 | Loss: 0.00001955
Iteration 158/1000 | Loss: 0.00001955
Iteration 159/1000 | Loss: 0.00001955
Iteration 160/1000 | Loss: 0.00001955
Iteration 161/1000 | Loss: 0.00001955
Iteration 162/1000 | Loss: 0.00001955
Iteration 163/1000 | Loss: 0.00001955
Iteration 164/1000 | Loss: 0.00001954
Iteration 165/1000 | Loss: 0.00001954
Iteration 166/1000 | Loss: 0.00001954
Iteration 167/1000 | Loss: 0.00001954
Iteration 168/1000 | Loss: 0.00001954
Iteration 169/1000 | Loss: 0.00001954
Iteration 170/1000 | Loss: 0.00001954
Iteration 171/1000 | Loss: 0.00001954
Iteration 172/1000 | Loss: 0.00001954
Iteration 173/1000 | Loss: 0.00001954
Iteration 174/1000 | Loss: 0.00001954
Iteration 175/1000 | Loss: 0.00001954
Iteration 176/1000 | Loss: 0.00001954
Iteration 177/1000 | Loss: 0.00001954
Iteration 178/1000 | Loss: 0.00001954
Iteration 179/1000 | Loss: 0.00001953
Iteration 180/1000 | Loss: 0.00001953
Iteration 181/1000 | Loss: 0.00001953
Iteration 182/1000 | Loss: 0.00001953
Iteration 183/1000 | Loss: 0.00001953
Iteration 184/1000 | Loss: 0.00001953
Iteration 185/1000 | Loss: 0.00001953
Iteration 186/1000 | Loss: 0.00001953
Iteration 187/1000 | Loss: 0.00001953
Iteration 188/1000 | Loss: 0.00001953
Iteration 189/1000 | Loss: 0.00001953
Iteration 190/1000 | Loss: 0.00001953
Iteration 191/1000 | Loss: 0.00001953
Iteration 192/1000 | Loss: 0.00001953
Iteration 193/1000 | Loss: 0.00001953
Iteration 194/1000 | Loss: 0.00001953
Iteration 195/1000 | Loss: 0.00001953
Iteration 196/1000 | Loss: 0.00001953
Iteration 197/1000 | Loss: 0.00001953
Iteration 198/1000 | Loss: 0.00001953
Iteration 199/1000 | Loss: 0.00001953
Iteration 200/1000 | Loss: 0.00001953
Iteration 201/1000 | Loss: 0.00001953
Iteration 202/1000 | Loss: 0.00001953
Iteration 203/1000 | Loss: 0.00001952
Iteration 204/1000 | Loss: 0.00001952
Iteration 205/1000 | Loss: 0.00001952
Iteration 206/1000 | Loss: 0.00001952
Iteration 207/1000 | Loss: 0.00001952
Iteration 208/1000 | Loss: 0.00001952
Iteration 209/1000 | Loss: 0.00001952
Iteration 210/1000 | Loss: 0.00001952
Iteration 211/1000 | Loss: 0.00001952
Iteration 212/1000 | Loss: 0.00001952
Iteration 213/1000 | Loss: 0.00001952
Iteration 214/1000 | Loss: 0.00001952
Iteration 215/1000 | Loss: 0.00001952
Iteration 216/1000 | Loss: 0.00001952
Iteration 217/1000 | Loss: 0.00001952
Iteration 218/1000 | Loss: 0.00001952
Iteration 219/1000 | Loss: 0.00001952
Iteration 220/1000 | Loss: 0.00001952
Iteration 221/1000 | Loss: 0.00001952
Iteration 222/1000 | Loss: 0.00001952
Iteration 223/1000 | Loss: 0.00001952
Iteration 224/1000 | Loss: 0.00001952
Iteration 225/1000 | Loss: 0.00001952
Iteration 226/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.9524904928402975e-05, 1.9524904928402975e-05, 1.9524904928402975e-05, 1.9524904928402975e-05, 1.9524904928402975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9524904928402975e-05

Optimization complete. Final v2v error: 3.7435648441314697 mm

Highest mean error: 3.8403396606445312 mm for frame 15

Lowest mean error: 3.699608087539673 mm for frame 65

Saving results

Total time: 89.00110507011414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997798
Iteration 2/25 | Loss: 0.00228759
Iteration 3/25 | Loss: 0.00184354
Iteration 4/25 | Loss: 0.00159014
Iteration 5/25 | Loss: 0.00161178
Iteration 6/25 | Loss: 0.00152905
Iteration 7/25 | Loss: 0.00150881
Iteration 8/25 | Loss: 0.00149242
Iteration 9/25 | Loss: 0.00147093
Iteration 10/25 | Loss: 0.00146583
Iteration 11/25 | Loss: 0.00145553
Iteration 12/25 | Loss: 0.00145406
Iteration 13/25 | Loss: 0.00143984
Iteration 14/25 | Loss: 0.00144455
Iteration 15/25 | Loss: 0.00143300
Iteration 16/25 | Loss: 0.00142038
Iteration 17/25 | Loss: 0.00142376
Iteration 18/25 | Loss: 0.00141520
Iteration 19/25 | Loss: 0.00140957
Iteration 20/25 | Loss: 0.00141003
Iteration 21/25 | Loss: 0.00141358
Iteration 22/25 | Loss: 0.00141265
Iteration 23/25 | Loss: 0.00141016
Iteration 24/25 | Loss: 0.00140781
Iteration 25/25 | Loss: 0.00141076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42228901
Iteration 2/25 | Loss: 0.00208525
Iteration 3/25 | Loss: 0.00192701
Iteration 4/25 | Loss: 0.00192696
Iteration 5/25 | Loss: 0.00192696
Iteration 6/25 | Loss: 0.00192696
Iteration 7/25 | Loss: 0.00192696
Iteration 8/25 | Loss: 0.00192696
Iteration 9/25 | Loss: 0.00192696
Iteration 10/25 | Loss: 0.00192696
Iteration 11/25 | Loss: 0.00192696
Iteration 12/25 | Loss: 0.00192696
Iteration 13/25 | Loss: 0.00192696
Iteration 14/25 | Loss: 0.00192696
Iteration 15/25 | Loss: 0.00192696
Iteration 16/25 | Loss: 0.00192696
Iteration 17/25 | Loss: 0.00192696
Iteration 18/25 | Loss: 0.00192696
Iteration 19/25 | Loss: 0.00192696
Iteration 20/25 | Loss: 0.00192696
Iteration 21/25 | Loss: 0.00192696
Iteration 22/25 | Loss: 0.00192696
Iteration 23/25 | Loss: 0.00192696
Iteration 24/25 | Loss: 0.00192696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019269550684839487, 0.0019269550684839487, 0.0019269550684839487, 0.0019269550684839487, 0.0019269550684839487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019269550684839487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192696
Iteration 2/1000 | Loss: 0.00149518
Iteration 3/1000 | Loss: 0.00018060
Iteration 4/1000 | Loss: 0.00015834
Iteration 5/1000 | Loss: 0.00108171
Iteration 6/1000 | Loss: 0.00113227
Iteration 7/1000 | Loss: 0.00058482
Iteration 8/1000 | Loss: 0.00012566
Iteration 9/1000 | Loss: 0.00021429
Iteration 10/1000 | Loss: 0.00012506
Iteration 11/1000 | Loss: 0.00011973
Iteration 12/1000 | Loss: 0.00032538
Iteration 13/1000 | Loss: 0.00215632
Iteration 14/1000 | Loss: 0.00105157
Iteration 15/1000 | Loss: 0.00314615
Iteration 16/1000 | Loss: 0.00249385
Iteration 17/1000 | Loss: 0.00113159
Iteration 18/1000 | Loss: 0.00064204
Iteration 19/1000 | Loss: 0.00035719
Iteration 20/1000 | Loss: 0.00035583
Iteration 21/1000 | Loss: 0.00019398
Iteration 22/1000 | Loss: 0.00040612
Iteration 23/1000 | Loss: 0.00047305
Iteration 24/1000 | Loss: 0.00112155
Iteration 25/1000 | Loss: 0.00108621
Iteration 26/1000 | Loss: 0.00114836
Iteration 27/1000 | Loss: 0.00062694
Iteration 28/1000 | Loss: 0.00045681
Iteration 29/1000 | Loss: 0.00024085
Iteration 30/1000 | Loss: 0.00025114
Iteration 31/1000 | Loss: 0.00011417
Iteration 32/1000 | Loss: 0.00010630
Iteration 33/1000 | Loss: 0.00010945
Iteration 34/1000 | Loss: 0.00047658
Iteration 35/1000 | Loss: 0.00054285
Iteration 36/1000 | Loss: 0.00044262
Iteration 37/1000 | Loss: 0.00011468
Iteration 38/1000 | Loss: 0.00036902
Iteration 39/1000 | Loss: 0.00033523
Iteration 40/1000 | Loss: 0.00010004
Iteration 41/1000 | Loss: 0.00010849
Iteration 42/1000 | Loss: 0.00008303
Iteration 43/1000 | Loss: 0.00065776
Iteration 44/1000 | Loss: 0.00039848
Iteration 45/1000 | Loss: 0.00046404
Iteration 46/1000 | Loss: 0.00035543
Iteration 47/1000 | Loss: 0.00018865
Iteration 48/1000 | Loss: 0.00072713
Iteration 49/1000 | Loss: 0.00020403
Iteration 50/1000 | Loss: 0.00008373
Iteration 51/1000 | Loss: 0.00024330
Iteration 52/1000 | Loss: 0.00014292
Iteration 53/1000 | Loss: 0.00027709
Iteration 54/1000 | Loss: 0.00056435
Iteration 55/1000 | Loss: 0.00037871
Iteration 56/1000 | Loss: 0.00021515
Iteration 57/1000 | Loss: 0.00040206
Iteration 58/1000 | Loss: 0.00057870
Iteration 59/1000 | Loss: 0.00041070
Iteration 60/1000 | Loss: 0.00031222
Iteration 61/1000 | Loss: 0.00029004
Iteration 62/1000 | Loss: 0.00034820
Iteration 63/1000 | Loss: 0.00014969
Iteration 64/1000 | Loss: 0.00021194
Iteration 65/1000 | Loss: 0.00034079
Iteration 66/1000 | Loss: 0.00599562
Iteration 67/1000 | Loss: 0.00084201
Iteration 68/1000 | Loss: 0.00488210
Iteration 69/1000 | Loss: 0.00044699
Iteration 70/1000 | Loss: 0.00265958
Iteration 71/1000 | Loss: 0.00082561
Iteration 72/1000 | Loss: 0.00025041
Iteration 73/1000 | Loss: 0.00085797
Iteration 74/1000 | Loss: 0.00072242
Iteration 75/1000 | Loss: 0.00028736
Iteration 76/1000 | Loss: 0.00016371
Iteration 77/1000 | Loss: 0.00012724
Iteration 78/1000 | Loss: 0.00005027
Iteration 79/1000 | Loss: 0.00011615
Iteration 80/1000 | Loss: 0.00004154
Iteration 81/1000 | Loss: 0.00118599
Iteration 82/1000 | Loss: 0.00003598
Iteration 83/1000 | Loss: 0.00032449
Iteration 84/1000 | Loss: 0.00053212
Iteration 85/1000 | Loss: 0.00009844
Iteration 86/1000 | Loss: 0.00025890
Iteration 87/1000 | Loss: 0.00053963
Iteration 88/1000 | Loss: 0.00185257
Iteration 89/1000 | Loss: 0.00011462
Iteration 90/1000 | Loss: 0.00015859
Iteration 91/1000 | Loss: 0.00014806
Iteration 92/1000 | Loss: 0.00004501
Iteration 93/1000 | Loss: 0.00004664
Iteration 94/1000 | Loss: 0.00020236
Iteration 95/1000 | Loss: 0.00004710
Iteration 96/1000 | Loss: 0.00030294
Iteration 97/1000 | Loss: 0.00005407
Iteration 98/1000 | Loss: 0.00004829
Iteration 99/1000 | Loss: 0.00004852
Iteration 100/1000 | Loss: 0.00018631
Iteration 101/1000 | Loss: 0.00039964
Iteration 102/1000 | Loss: 0.00006178
Iteration 103/1000 | Loss: 0.00009615
Iteration 104/1000 | Loss: 0.00013726
Iteration 105/1000 | Loss: 0.00002975
Iteration 106/1000 | Loss: 0.00003809
Iteration 107/1000 | Loss: 0.00004911
Iteration 108/1000 | Loss: 0.00002616
Iteration 109/1000 | Loss: 0.00003410
Iteration 110/1000 | Loss: 0.00004356
Iteration 111/1000 | Loss: 0.00003130
Iteration 112/1000 | Loss: 0.00003561
Iteration 113/1000 | Loss: 0.00002403
Iteration 114/1000 | Loss: 0.00004125
Iteration 115/1000 | Loss: 0.00002751
Iteration 116/1000 | Loss: 0.00014955
Iteration 117/1000 | Loss: 0.00017775
Iteration 118/1000 | Loss: 0.00004096
Iteration 119/1000 | Loss: 0.00004158
Iteration 120/1000 | Loss: 0.00014351
Iteration 121/1000 | Loss: 0.00006486
Iteration 122/1000 | Loss: 0.00006939
Iteration 123/1000 | Loss: 0.00004491
Iteration 124/1000 | Loss: 0.00004956
Iteration 125/1000 | Loss: 0.00002308
Iteration 126/1000 | Loss: 0.00010032
Iteration 127/1000 | Loss: 0.00003983
Iteration 128/1000 | Loss: 0.00005813
Iteration 129/1000 | Loss: 0.00005532
Iteration 130/1000 | Loss: 0.00004487
Iteration 131/1000 | Loss: 0.00004167
Iteration 132/1000 | Loss: 0.00003991
Iteration 133/1000 | Loss: 0.00003982
Iteration 134/1000 | Loss: 0.00004080
Iteration 135/1000 | Loss: 0.00003796
Iteration 136/1000 | Loss: 0.00004972
Iteration 137/1000 | Loss: 0.00004796
Iteration 138/1000 | Loss: 0.00004005
Iteration 139/1000 | Loss: 0.00003388
Iteration 140/1000 | Loss: 0.00003654
Iteration 141/1000 | Loss: 0.00003804
Iteration 142/1000 | Loss: 0.00004871
Iteration 143/1000 | Loss: 0.00003986
Iteration 144/1000 | Loss: 0.00007872
Iteration 145/1000 | Loss: 0.00004666
Iteration 146/1000 | Loss: 0.00006298
Iteration 147/1000 | Loss: 0.00004044
Iteration 148/1000 | Loss: 0.00004395
Iteration 149/1000 | Loss: 0.00003930
Iteration 150/1000 | Loss: 0.00003982
Iteration 151/1000 | Loss: 0.00003821
Iteration 152/1000 | Loss: 0.00003970
Iteration 153/1000 | Loss: 0.00003385
Iteration 154/1000 | Loss: 0.00015877
Iteration 155/1000 | Loss: 0.00004446
Iteration 156/1000 | Loss: 0.00003667
Iteration 157/1000 | Loss: 0.00003997
Iteration 158/1000 | Loss: 0.00003722
Iteration 159/1000 | Loss: 0.00008914
Iteration 160/1000 | Loss: 0.00004826
Iteration 161/1000 | Loss: 0.00004152
Iteration 162/1000 | Loss: 0.00003900
Iteration 163/1000 | Loss: 0.00002424
Iteration 164/1000 | Loss: 0.00004013
Iteration 165/1000 | Loss: 0.00003916
Iteration 166/1000 | Loss: 0.00004008
Iteration 167/1000 | Loss: 0.00003751
Iteration 168/1000 | Loss: 0.00004053
Iteration 169/1000 | Loss: 0.00003836
Iteration 170/1000 | Loss: 0.00004068
Iteration 171/1000 | Loss: 0.00003671
Iteration 172/1000 | Loss: 0.00004195
Iteration 173/1000 | Loss: 0.00004096
Iteration 174/1000 | Loss: 0.00004037
Iteration 175/1000 | Loss: 0.00003809
Iteration 176/1000 | Loss: 0.00004027
Iteration 177/1000 | Loss: 0.00003932
Iteration 178/1000 | Loss: 0.00003992
Iteration 179/1000 | Loss: 0.00003997
Iteration 180/1000 | Loss: 0.00003764
Iteration 181/1000 | Loss: 0.00003722
Iteration 182/1000 | Loss: 0.00004004
Iteration 183/1000 | Loss: 0.00004484
Iteration 184/1000 | Loss: 0.00002677
Iteration 185/1000 | Loss: 0.00002757
Iteration 186/1000 | Loss: 0.00004608
Iteration 187/1000 | Loss: 0.00004012
Iteration 188/1000 | Loss: 0.00002854
Iteration 189/1000 | Loss: 0.00003948
Iteration 190/1000 | Loss: 0.00003038
Iteration 191/1000 | Loss: 0.00003061
Iteration 192/1000 | Loss: 0.00003602
Iteration 193/1000 | Loss: 0.00003660
Iteration 194/1000 | Loss: 0.00003968
Iteration 195/1000 | Loss: 0.00001885
Iteration 196/1000 | Loss: 0.00003581
Iteration 197/1000 | Loss: 0.00002542
Iteration 198/1000 | Loss: 0.00003672
Iteration 199/1000 | Loss: 0.00003423
Iteration 200/1000 | Loss: 0.00002957
Iteration 201/1000 | Loss: 0.00003824
Iteration 202/1000 | Loss: 0.00003136
Iteration 203/1000 | Loss: 0.00003668
Iteration 204/1000 | Loss: 0.00003976
Iteration 205/1000 | Loss: 0.00003739
Iteration 206/1000 | Loss: 0.00002659
Iteration 207/1000 | Loss: 0.00002607
Iteration 208/1000 | Loss: 0.00014497
Iteration 209/1000 | Loss: 0.00005427
Iteration 210/1000 | Loss: 0.00003921
Iteration 211/1000 | Loss: 0.00003731
Iteration 212/1000 | Loss: 0.00003918
Iteration 213/1000 | Loss: 0.00003715
Iteration 214/1000 | Loss: 0.00003946
Iteration 215/1000 | Loss: 0.00003596
Iteration 216/1000 | Loss: 0.00013901
Iteration 217/1000 | Loss: 0.00004829
Iteration 218/1000 | Loss: 0.00003849
Iteration 219/1000 | Loss: 0.00003591
Iteration 220/1000 | Loss: 0.00003711
Iteration 221/1000 | Loss: 0.00003593
Iteration 222/1000 | Loss: 0.00003792
Iteration 223/1000 | Loss: 0.00005479
Iteration 224/1000 | Loss: 0.00004085
Iteration 225/1000 | Loss: 0.00003718
Iteration 226/1000 | Loss: 0.00003938
Iteration 227/1000 | Loss: 0.00003627
Iteration 228/1000 | Loss: 0.00005089
Iteration 229/1000 | Loss: 0.00004234
Iteration 230/1000 | Loss: 0.00004582
Iteration 231/1000 | Loss: 0.00003487
Iteration 232/1000 | Loss: 0.00003500
Iteration 233/1000 | Loss: 0.00003803
Iteration 234/1000 | Loss: 0.00003386
Iteration 235/1000 | Loss: 0.00003663
Iteration 236/1000 | Loss: 0.00003333
Iteration 237/1000 | Loss: 0.00004914
Iteration 238/1000 | Loss: 0.00003982
Iteration 239/1000 | Loss: 0.00006089
Iteration 240/1000 | Loss: 0.00003786
Iteration 241/1000 | Loss: 0.00002668
Iteration 242/1000 | Loss: 0.00006061
Iteration 243/1000 | Loss: 0.00003837
Iteration 244/1000 | Loss: 0.00021007
Iteration 245/1000 | Loss: 0.00003798
Iteration 246/1000 | Loss: 0.00023787
Iteration 247/1000 | Loss: 0.00006560
Iteration 248/1000 | Loss: 0.00002559
Iteration 249/1000 | Loss: 0.00006823
Iteration 250/1000 | Loss: 0.00004160
Iteration 251/1000 | Loss: 0.00004884
Iteration 252/1000 | Loss: 0.00003880
Iteration 253/1000 | Loss: 0.00003418
Iteration 254/1000 | Loss: 0.00002652
Iteration 255/1000 | Loss: 0.00003378
Iteration 256/1000 | Loss: 0.00007181
Iteration 257/1000 | Loss: 0.00005113
Iteration 258/1000 | Loss: 0.00003591
Iteration 259/1000 | Loss: 0.00002880
Iteration 260/1000 | Loss: 0.00003805
Iteration 261/1000 | Loss: 0.00002698
Iteration 262/1000 | Loss: 0.00004242
Iteration 263/1000 | Loss: 0.00003082
Iteration 264/1000 | Loss: 0.00011635
Iteration 265/1000 | Loss: 0.00004054
Iteration 266/1000 | Loss: 0.00005605
Iteration 267/1000 | Loss: 0.00002054
Iteration 268/1000 | Loss: 0.00003184
Iteration 269/1000 | Loss: 0.00001695
Iteration 270/1000 | Loss: 0.00005491
Iteration 271/1000 | Loss: 0.00001650
Iteration 272/1000 | Loss: 0.00001594
Iteration 273/1000 | Loss: 0.00001560
Iteration 274/1000 | Loss: 0.00001534
Iteration 275/1000 | Loss: 0.00001528
Iteration 276/1000 | Loss: 0.00001524
Iteration 277/1000 | Loss: 0.00001523
Iteration 278/1000 | Loss: 0.00001522
Iteration 279/1000 | Loss: 0.00001516
Iteration 280/1000 | Loss: 0.00001515
Iteration 281/1000 | Loss: 0.00001513
Iteration 282/1000 | Loss: 0.00001513
Iteration 283/1000 | Loss: 0.00001510
Iteration 284/1000 | Loss: 0.00001507
Iteration 285/1000 | Loss: 0.00001507
Iteration 286/1000 | Loss: 0.00001503
Iteration 287/1000 | Loss: 0.00001500
Iteration 288/1000 | Loss: 0.00001500
Iteration 289/1000 | Loss: 0.00001493
Iteration 290/1000 | Loss: 0.00001492
Iteration 291/1000 | Loss: 0.00001491
Iteration 292/1000 | Loss: 0.00001491
Iteration 293/1000 | Loss: 0.00001491
Iteration 294/1000 | Loss: 0.00001487
Iteration 295/1000 | Loss: 0.00044354
Iteration 296/1000 | Loss: 0.00070379
Iteration 297/1000 | Loss: 0.00255734
Iteration 298/1000 | Loss: 0.00002283
Iteration 299/1000 | Loss: 0.00001908
Iteration 300/1000 | Loss: 0.00001748
Iteration 301/1000 | Loss: 0.00018840
Iteration 302/1000 | Loss: 0.00018925
Iteration 303/1000 | Loss: 0.00002775
Iteration 304/1000 | Loss: 0.00002431
Iteration 305/1000 | Loss: 0.00002167
Iteration 306/1000 | Loss: 0.00001880
Iteration 307/1000 | Loss: 0.00001783
Iteration 308/1000 | Loss: 0.00001871
Iteration 309/1000 | Loss: 0.00021667
Iteration 310/1000 | Loss: 0.00001687
Iteration 311/1000 | Loss: 0.00001495
Iteration 312/1000 | Loss: 0.00001464
Iteration 313/1000 | Loss: 0.00001440
Iteration 314/1000 | Loss: 0.00001428
Iteration 315/1000 | Loss: 0.00001427
Iteration 316/1000 | Loss: 0.00001422
Iteration 317/1000 | Loss: 0.00001419
Iteration 318/1000 | Loss: 0.00001413
Iteration 319/1000 | Loss: 0.00001411
Iteration 320/1000 | Loss: 0.00001411
Iteration 321/1000 | Loss: 0.00001410
Iteration 322/1000 | Loss: 0.00001410
Iteration 323/1000 | Loss: 0.00001409
Iteration 324/1000 | Loss: 0.00001409
Iteration 325/1000 | Loss: 0.00001408
Iteration 326/1000 | Loss: 0.00001408
Iteration 327/1000 | Loss: 0.00001407
Iteration 328/1000 | Loss: 0.00001407
Iteration 329/1000 | Loss: 0.00001407
Iteration 330/1000 | Loss: 0.00001407
Iteration 331/1000 | Loss: 0.00001407
Iteration 332/1000 | Loss: 0.00001406
Iteration 333/1000 | Loss: 0.00001406
Iteration 334/1000 | Loss: 0.00001406
Iteration 335/1000 | Loss: 0.00001406
Iteration 336/1000 | Loss: 0.00001406
Iteration 337/1000 | Loss: 0.00001405
Iteration 338/1000 | Loss: 0.00001405
Iteration 339/1000 | Loss: 0.00001404
Iteration 340/1000 | Loss: 0.00001404
Iteration 341/1000 | Loss: 0.00001403
Iteration 342/1000 | Loss: 0.00001403
Iteration 343/1000 | Loss: 0.00001403
Iteration 344/1000 | Loss: 0.00001403
Iteration 345/1000 | Loss: 0.00001402
Iteration 346/1000 | Loss: 0.00001402
Iteration 347/1000 | Loss: 0.00001401
Iteration 348/1000 | Loss: 0.00001401
Iteration 349/1000 | Loss: 0.00001400
Iteration 350/1000 | Loss: 0.00001400
Iteration 351/1000 | Loss: 0.00001400
Iteration 352/1000 | Loss: 0.00001400
Iteration 353/1000 | Loss: 0.00001400
Iteration 354/1000 | Loss: 0.00001400
Iteration 355/1000 | Loss: 0.00001400
Iteration 356/1000 | Loss: 0.00001399
Iteration 357/1000 | Loss: 0.00001399
Iteration 358/1000 | Loss: 0.00001399
Iteration 359/1000 | Loss: 0.00001399
Iteration 360/1000 | Loss: 0.00001399
Iteration 361/1000 | Loss: 0.00001399
Iteration 362/1000 | Loss: 0.00001399
Iteration 363/1000 | Loss: 0.00001399
Iteration 364/1000 | Loss: 0.00001399
Iteration 365/1000 | Loss: 0.00001399
Iteration 366/1000 | Loss: 0.00001399
Iteration 367/1000 | Loss: 0.00001398
Iteration 368/1000 | Loss: 0.00001398
Iteration 369/1000 | Loss: 0.00001398
Iteration 370/1000 | Loss: 0.00001398
Iteration 371/1000 | Loss: 0.00001398
Iteration 372/1000 | Loss: 0.00001398
Iteration 373/1000 | Loss: 0.00001398
Iteration 374/1000 | Loss: 0.00001398
Iteration 375/1000 | Loss: 0.00001398
Iteration 376/1000 | Loss: 0.00001398
Iteration 377/1000 | Loss: 0.00001398
Iteration 378/1000 | Loss: 0.00001398
Iteration 379/1000 | Loss: 0.00001398
Iteration 380/1000 | Loss: 0.00001398
Iteration 381/1000 | Loss: 0.00001398
Iteration 382/1000 | Loss: 0.00001398
Iteration 383/1000 | Loss: 0.00001398
Iteration 384/1000 | Loss: 0.00001398
Iteration 385/1000 | Loss: 0.00001398
Iteration 386/1000 | Loss: 0.00001397
Iteration 387/1000 | Loss: 0.00001397
Iteration 388/1000 | Loss: 0.00001397
Iteration 389/1000 | Loss: 0.00001397
Iteration 390/1000 | Loss: 0.00001397
Iteration 391/1000 | Loss: 0.00001397
Iteration 392/1000 | Loss: 0.00001397
Iteration 393/1000 | Loss: 0.00001397
Iteration 394/1000 | Loss: 0.00001397
Iteration 395/1000 | Loss: 0.00001397
Iteration 396/1000 | Loss: 0.00001397
Iteration 397/1000 | Loss: 0.00001397
Iteration 398/1000 | Loss: 0.00001397
Iteration 399/1000 | Loss: 0.00001397
Iteration 400/1000 | Loss: 0.00001397
Iteration 401/1000 | Loss: 0.00001397
Iteration 402/1000 | Loss: 0.00001397
Iteration 403/1000 | Loss: 0.00001397
Iteration 404/1000 | Loss: 0.00001397
Iteration 405/1000 | Loss: 0.00001397
Iteration 406/1000 | Loss: 0.00001396
Iteration 407/1000 | Loss: 0.00001396
Iteration 408/1000 | Loss: 0.00001396
Iteration 409/1000 | Loss: 0.00001396
Iteration 410/1000 | Loss: 0.00001396
Iteration 411/1000 | Loss: 0.00001396
Iteration 412/1000 | Loss: 0.00001396
Iteration 413/1000 | Loss: 0.00001396
Iteration 414/1000 | Loss: 0.00001396
Iteration 415/1000 | Loss: 0.00001396
Iteration 416/1000 | Loss: 0.00001396
Iteration 417/1000 | Loss: 0.00001396
Iteration 418/1000 | Loss: 0.00001396
Iteration 419/1000 | Loss: 0.00001396
Iteration 420/1000 | Loss: 0.00001396
Iteration 421/1000 | Loss: 0.00001396
Iteration 422/1000 | Loss: 0.00001396
Iteration 423/1000 | Loss: 0.00001396
Iteration 424/1000 | Loss: 0.00001396
Iteration 425/1000 | Loss: 0.00001396
Iteration 426/1000 | Loss: 0.00001396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 426. Stopping optimization.
Last 5 losses: [1.3959547686681617e-05, 1.3959547686681617e-05, 1.3959547686681617e-05, 1.3959547686681617e-05, 1.3959547686681617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3959547686681617e-05

Optimization complete. Final v2v error: 3.0933902263641357 mm

Highest mean error: 5.257021427154541 mm for frame 55

Lowest mean error: 2.81026554107666 mm for frame 100

Saving results

Total time: 468.0405833721161
