Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=264, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14784-14839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944993
Iteration 2/25 | Loss: 0.00136287
Iteration 3/25 | Loss: 0.00084025
Iteration 4/25 | Loss: 0.00077469
Iteration 5/25 | Loss: 0.00075009
Iteration 6/25 | Loss: 0.00074069
Iteration 7/25 | Loss: 0.00073921
Iteration 8/25 | Loss: 0.00073897
Iteration 9/25 | Loss: 0.00073897
Iteration 10/25 | Loss: 0.00073897
Iteration 11/25 | Loss: 0.00073897
Iteration 12/25 | Loss: 0.00073897
Iteration 13/25 | Loss: 0.00073897
Iteration 14/25 | Loss: 0.00073897
Iteration 15/25 | Loss: 0.00073897
Iteration 16/25 | Loss: 0.00073897
Iteration 17/25 | Loss: 0.00073897
Iteration 18/25 | Loss: 0.00073897
Iteration 19/25 | Loss: 0.00073897
Iteration 20/25 | Loss: 0.00073897
Iteration 21/25 | Loss: 0.00073897
Iteration 22/25 | Loss: 0.00073897
Iteration 23/25 | Loss: 0.00073897
Iteration 24/25 | Loss: 0.00073897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007389735546894372, 0.0007389735546894372, 0.0007389735546894372, 0.0007389735546894372, 0.0007389735546894372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007389735546894372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10425186
Iteration 2/25 | Loss: 0.00023805
Iteration 3/25 | Loss: 0.00023803
Iteration 4/25 | Loss: 0.00023803
Iteration 5/25 | Loss: 0.00023803
Iteration 6/25 | Loss: 0.00023803
Iteration 7/25 | Loss: 0.00023803
Iteration 8/25 | Loss: 0.00023803
Iteration 9/25 | Loss: 0.00023803
Iteration 10/25 | Loss: 0.00023803
Iteration 11/25 | Loss: 0.00023803
Iteration 12/25 | Loss: 0.00023803
Iteration 13/25 | Loss: 0.00023803
Iteration 14/25 | Loss: 0.00023803
Iteration 15/25 | Loss: 0.00023803
Iteration 16/25 | Loss: 0.00023803
Iteration 17/25 | Loss: 0.00023803
Iteration 18/25 | Loss: 0.00023803
Iteration 19/25 | Loss: 0.00023803
Iteration 20/25 | Loss: 0.00023803
Iteration 21/25 | Loss: 0.00023803
Iteration 22/25 | Loss: 0.00023803
Iteration 23/25 | Loss: 0.00023803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00023803049407433718, 0.00023803049407433718, 0.00023803049407433718, 0.00023803049407433718, 0.00023803049407433718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023803049407433718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023803
Iteration 2/1000 | Loss: 0.00004530
Iteration 3/1000 | Loss: 0.00003633
Iteration 4/1000 | Loss: 0.00003324
Iteration 5/1000 | Loss: 0.00003126
Iteration 6/1000 | Loss: 0.00003018
Iteration 7/1000 | Loss: 0.00002920
Iteration 8/1000 | Loss: 0.00002843
Iteration 9/1000 | Loss: 0.00002809
Iteration 10/1000 | Loss: 0.00002779
Iteration 11/1000 | Loss: 0.00002760
Iteration 12/1000 | Loss: 0.00002754
Iteration 13/1000 | Loss: 0.00002740
Iteration 14/1000 | Loss: 0.00002739
Iteration 15/1000 | Loss: 0.00002735
Iteration 16/1000 | Loss: 0.00002734
Iteration 17/1000 | Loss: 0.00002734
Iteration 18/1000 | Loss: 0.00002728
Iteration 19/1000 | Loss: 0.00002723
Iteration 20/1000 | Loss: 0.00002721
Iteration 21/1000 | Loss: 0.00002720
Iteration 22/1000 | Loss: 0.00002720
Iteration 23/1000 | Loss: 0.00002720
Iteration 24/1000 | Loss: 0.00002719
Iteration 25/1000 | Loss: 0.00002718
Iteration 26/1000 | Loss: 0.00002718
Iteration 27/1000 | Loss: 0.00002718
Iteration 28/1000 | Loss: 0.00002717
Iteration 29/1000 | Loss: 0.00002717
Iteration 30/1000 | Loss: 0.00002717
Iteration 31/1000 | Loss: 0.00002717
Iteration 32/1000 | Loss: 0.00002717
Iteration 33/1000 | Loss: 0.00002717
Iteration 34/1000 | Loss: 0.00002717
Iteration 35/1000 | Loss: 0.00002717
Iteration 36/1000 | Loss: 0.00002717
Iteration 37/1000 | Loss: 0.00002717
Iteration 38/1000 | Loss: 0.00002717
Iteration 39/1000 | Loss: 0.00002716
Iteration 40/1000 | Loss: 0.00002716
Iteration 41/1000 | Loss: 0.00002716
Iteration 42/1000 | Loss: 0.00002716
Iteration 43/1000 | Loss: 0.00002716
Iteration 44/1000 | Loss: 0.00002716
Iteration 45/1000 | Loss: 0.00002716
Iteration 46/1000 | Loss: 0.00002715
Iteration 47/1000 | Loss: 0.00002715
Iteration 48/1000 | Loss: 0.00002714
Iteration 49/1000 | Loss: 0.00002711
Iteration 50/1000 | Loss: 0.00002710
Iteration 51/1000 | Loss: 0.00002708
Iteration 52/1000 | Loss: 0.00002708
Iteration 53/1000 | Loss: 0.00002707
Iteration 54/1000 | Loss: 0.00002706
Iteration 55/1000 | Loss: 0.00002706
Iteration 56/1000 | Loss: 0.00002704
Iteration 57/1000 | Loss: 0.00002704
Iteration 58/1000 | Loss: 0.00002703
Iteration 59/1000 | Loss: 0.00002702
Iteration 60/1000 | Loss: 0.00002702
Iteration 61/1000 | Loss: 0.00002701
Iteration 62/1000 | Loss: 0.00002701
Iteration 63/1000 | Loss: 0.00002701
Iteration 64/1000 | Loss: 0.00002701
Iteration 65/1000 | Loss: 0.00002701
Iteration 66/1000 | Loss: 0.00002701
Iteration 67/1000 | Loss: 0.00002701
Iteration 68/1000 | Loss: 0.00002701
Iteration 69/1000 | Loss: 0.00002701
Iteration 70/1000 | Loss: 0.00002701
Iteration 71/1000 | Loss: 0.00002701
Iteration 72/1000 | Loss: 0.00002700
Iteration 73/1000 | Loss: 0.00002699
Iteration 74/1000 | Loss: 0.00002693
Iteration 75/1000 | Loss: 0.00002692
Iteration 76/1000 | Loss: 0.00002691
Iteration 77/1000 | Loss: 0.00002689
Iteration 78/1000 | Loss: 0.00002687
Iteration 79/1000 | Loss: 0.00002687
Iteration 80/1000 | Loss: 0.00002687
Iteration 81/1000 | Loss: 0.00002686
Iteration 82/1000 | Loss: 0.00002686
Iteration 83/1000 | Loss: 0.00002685
Iteration 84/1000 | Loss: 0.00002684
Iteration 85/1000 | Loss: 0.00002684
Iteration 86/1000 | Loss: 0.00002684
Iteration 87/1000 | Loss: 0.00002684
Iteration 88/1000 | Loss: 0.00002684
Iteration 89/1000 | Loss: 0.00002684
Iteration 90/1000 | Loss: 0.00002684
Iteration 91/1000 | Loss: 0.00002684
Iteration 92/1000 | Loss: 0.00002683
Iteration 93/1000 | Loss: 0.00002683
Iteration 94/1000 | Loss: 0.00002682
Iteration 95/1000 | Loss: 0.00002681
Iteration 96/1000 | Loss: 0.00002681
Iteration 97/1000 | Loss: 0.00002681
Iteration 98/1000 | Loss: 0.00002681
Iteration 99/1000 | Loss: 0.00002681
Iteration 100/1000 | Loss: 0.00002680
Iteration 101/1000 | Loss: 0.00002680
Iteration 102/1000 | Loss: 0.00002680
Iteration 103/1000 | Loss: 0.00002680
Iteration 104/1000 | Loss: 0.00002680
Iteration 105/1000 | Loss: 0.00002680
Iteration 106/1000 | Loss: 0.00002680
Iteration 107/1000 | Loss: 0.00002680
Iteration 108/1000 | Loss: 0.00002680
Iteration 109/1000 | Loss: 0.00002680
Iteration 110/1000 | Loss: 0.00002679
Iteration 111/1000 | Loss: 0.00002679
Iteration 112/1000 | Loss: 0.00002679
Iteration 113/1000 | Loss: 0.00002679
Iteration 114/1000 | Loss: 0.00002678
Iteration 115/1000 | Loss: 0.00002678
Iteration 116/1000 | Loss: 0.00002678
Iteration 117/1000 | Loss: 0.00002678
Iteration 118/1000 | Loss: 0.00002678
Iteration 119/1000 | Loss: 0.00002678
Iteration 120/1000 | Loss: 0.00002678
Iteration 121/1000 | Loss: 0.00002678
Iteration 122/1000 | Loss: 0.00002678
Iteration 123/1000 | Loss: 0.00002678
Iteration 124/1000 | Loss: 0.00002678
Iteration 125/1000 | Loss: 0.00002678
Iteration 126/1000 | Loss: 0.00002677
Iteration 127/1000 | Loss: 0.00002677
Iteration 128/1000 | Loss: 0.00002677
Iteration 129/1000 | Loss: 0.00002677
Iteration 130/1000 | Loss: 0.00002677
Iteration 131/1000 | Loss: 0.00002677
Iteration 132/1000 | Loss: 0.00002677
Iteration 133/1000 | Loss: 0.00002677
Iteration 134/1000 | Loss: 0.00002677
Iteration 135/1000 | Loss: 0.00002676
Iteration 136/1000 | Loss: 0.00002676
Iteration 137/1000 | Loss: 0.00002676
Iteration 138/1000 | Loss: 0.00002676
Iteration 139/1000 | Loss: 0.00002676
Iteration 140/1000 | Loss: 0.00002676
Iteration 141/1000 | Loss: 0.00002676
Iteration 142/1000 | Loss: 0.00002676
Iteration 143/1000 | Loss: 0.00002676
Iteration 144/1000 | Loss: 0.00002676
Iteration 145/1000 | Loss: 0.00002675
Iteration 146/1000 | Loss: 0.00002675
Iteration 147/1000 | Loss: 0.00002675
Iteration 148/1000 | Loss: 0.00002675
Iteration 149/1000 | Loss: 0.00002675
Iteration 150/1000 | Loss: 0.00002675
Iteration 151/1000 | Loss: 0.00002675
Iteration 152/1000 | Loss: 0.00002675
Iteration 153/1000 | Loss: 0.00002675
Iteration 154/1000 | Loss: 0.00002675
Iteration 155/1000 | Loss: 0.00002675
Iteration 156/1000 | Loss: 0.00002675
Iteration 157/1000 | Loss: 0.00002674
Iteration 158/1000 | Loss: 0.00002674
Iteration 159/1000 | Loss: 0.00002674
Iteration 160/1000 | Loss: 0.00002674
Iteration 161/1000 | Loss: 0.00002674
Iteration 162/1000 | Loss: 0.00002674
Iteration 163/1000 | Loss: 0.00002673
Iteration 164/1000 | Loss: 0.00002673
Iteration 165/1000 | Loss: 0.00002673
Iteration 166/1000 | Loss: 0.00002673
Iteration 167/1000 | Loss: 0.00002673
Iteration 168/1000 | Loss: 0.00002673
Iteration 169/1000 | Loss: 0.00002673
Iteration 170/1000 | Loss: 0.00002673
Iteration 171/1000 | Loss: 0.00002673
Iteration 172/1000 | Loss: 0.00002672
Iteration 173/1000 | Loss: 0.00002672
Iteration 174/1000 | Loss: 0.00002672
Iteration 175/1000 | Loss: 0.00002672
Iteration 176/1000 | Loss: 0.00002672
Iteration 177/1000 | Loss: 0.00002672
Iteration 178/1000 | Loss: 0.00002672
Iteration 179/1000 | Loss: 0.00002672
Iteration 180/1000 | Loss: 0.00002672
Iteration 181/1000 | Loss: 0.00002672
Iteration 182/1000 | Loss: 0.00002672
Iteration 183/1000 | Loss: 0.00002672
Iteration 184/1000 | Loss: 0.00002671
Iteration 185/1000 | Loss: 0.00002671
Iteration 186/1000 | Loss: 0.00002671
Iteration 187/1000 | Loss: 0.00002671
Iteration 188/1000 | Loss: 0.00002671
Iteration 189/1000 | Loss: 0.00002671
Iteration 190/1000 | Loss: 0.00002671
Iteration 191/1000 | Loss: 0.00002671
Iteration 192/1000 | Loss: 0.00002671
Iteration 193/1000 | Loss: 0.00002671
Iteration 194/1000 | Loss: 0.00002671
Iteration 195/1000 | Loss: 0.00002671
Iteration 196/1000 | Loss: 0.00002671
Iteration 197/1000 | Loss: 0.00002671
Iteration 198/1000 | Loss: 0.00002671
Iteration 199/1000 | Loss: 0.00002671
Iteration 200/1000 | Loss: 0.00002670
Iteration 201/1000 | Loss: 0.00002670
Iteration 202/1000 | Loss: 0.00002670
Iteration 203/1000 | Loss: 0.00002670
Iteration 204/1000 | Loss: 0.00002670
Iteration 205/1000 | Loss: 0.00002670
Iteration 206/1000 | Loss: 0.00002670
Iteration 207/1000 | Loss: 0.00002670
Iteration 208/1000 | Loss: 0.00002670
Iteration 209/1000 | Loss: 0.00002670
Iteration 210/1000 | Loss: 0.00002670
Iteration 211/1000 | Loss: 0.00002670
Iteration 212/1000 | Loss: 0.00002670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.6703646653913893e-05, 2.6703646653913893e-05, 2.6703646653913893e-05, 2.6703646653913893e-05, 2.6703646653913893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6703646653913893e-05

Optimization complete. Final v2v error: 4.188554763793945 mm

Highest mean error: 5.4170308113098145 mm for frame 103

Lowest mean error: 3.3759243488311768 mm for frame 119

Saving results

Total time: 49.17659139633179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821564
Iteration 2/25 | Loss: 0.00203631
Iteration 3/25 | Loss: 0.00108067
Iteration 4/25 | Loss: 0.00100995
Iteration 5/25 | Loss: 0.00099270
Iteration 6/25 | Loss: 0.00098626
Iteration 7/25 | Loss: 0.00098496
Iteration 8/25 | Loss: 0.00098486
Iteration 9/25 | Loss: 0.00098486
Iteration 10/25 | Loss: 0.00098486
Iteration 11/25 | Loss: 0.00098486
Iteration 12/25 | Loss: 0.00098486
Iteration 13/25 | Loss: 0.00098486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000984858488664031, 0.000984858488664031, 0.000984858488664031, 0.000984858488664031, 0.000984858488664031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000984858488664031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61443013
Iteration 2/25 | Loss: 0.00035406
Iteration 3/25 | Loss: 0.00035406
Iteration 4/25 | Loss: 0.00035406
Iteration 5/25 | Loss: 0.00035406
Iteration 6/25 | Loss: 0.00035406
Iteration 7/25 | Loss: 0.00035406
Iteration 8/25 | Loss: 0.00035406
Iteration 9/25 | Loss: 0.00035406
Iteration 10/25 | Loss: 0.00035406
Iteration 11/25 | Loss: 0.00035406
Iteration 12/25 | Loss: 0.00035406
Iteration 13/25 | Loss: 0.00035406
Iteration 14/25 | Loss: 0.00035406
Iteration 15/25 | Loss: 0.00035406
Iteration 16/25 | Loss: 0.00035406
Iteration 17/25 | Loss: 0.00035406
Iteration 18/25 | Loss: 0.00035406
Iteration 19/25 | Loss: 0.00035406
Iteration 20/25 | Loss: 0.00035406
Iteration 21/25 | Loss: 0.00035406
Iteration 22/25 | Loss: 0.00035406
Iteration 23/25 | Loss: 0.00035406
Iteration 24/25 | Loss: 0.00035406
Iteration 25/25 | Loss: 0.00035406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035406
Iteration 2/1000 | Loss: 0.00006969
Iteration 3/1000 | Loss: 0.00005180
Iteration 4/1000 | Loss: 0.00004599
Iteration 5/1000 | Loss: 0.00004366
Iteration 6/1000 | Loss: 0.00004256
Iteration 7/1000 | Loss: 0.00004173
Iteration 8/1000 | Loss: 0.00004089
Iteration 9/1000 | Loss: 0.00004035
Iteration 10/1000 | Loss: 0.00004000
Iteration 11/1000 | Loss: 0.00003971
Iteration 12/1000 | Loss: 0.00003941
Iteration 13/1000 | Loss: 0.00003918
Iteration 14/1000 | Loss: 0.00003886
Iteration 15/1000 | Loss: 0.00003865
Iteration 16/1000 | Loss: 0.00003848
Iteration 17/1000 | Loss: 0.00003835
Iteration 18/1000 | Loss: 0.00003823
Iteration 19/1000 | Loss: 0.00003811
Iteration 20/1000 | Loss: 0.00003811
Iteration 21/1000 | Loss: 0.00003808
Iteration 22/1000 | Loss: 0.00003807
Iteration 23/1000 | Loss: 0.00003806
Iteration 24/1000 | Loss: 0.00003805
Iteration 25/1000 | Loss: 0.00003804
Iteration 26/1000 | Loss: 0.00003804
Iteration 27/1000 | Loss: 0.00003804
Iteration 28/1000 | Loss: 0.00003803
Iteration 29/1000 | Loss: 0.00003803
Iteration 30/1000 | Loss: 0.00003802
Iteration 31/1000 | Loss: 0.00003798
Iteration 32/1000 | Loss: 0.00003797
Iteration 33/1000 | Loss: 0.00003797
Iteration 34/1000 | Loss: 0.00003797
Iteration 35/1000 | Loss: 0.00003797
Iteration 36/1000 | Loss: 0.00003797
Iteration 37/1000 | Loss: 0.00003797
Iteration 38/1000 | Loss: 0.00003796
Iteration 39/1000 | Loss: 0.00003796
Iteration 40/1000 | Loss: 0.00003795
Iteration 41/1000 | Loss: 0.00003795
Iteration 42/1000 | Loss: 0.00003795
Iteration 43/1000 | Loss: 0.00003794
Iteration 44/1000 | Loss: 0.00003794
Iteration 45/1000 | Loss: 0.00003794
Iteration 46/1000 | Loss: 0.00003794
Iteration 47/1000 | Loss: 0.00003793
Iteration 48/1000 | Loss: 0.00003793
Iteration 49/1000 | Loss: 0.00003791
Iteration 50/1000 | Loss: 0.00003791
Iteration 51/1000 | Loss: 0.00003791
Iteration 52/1000 | Loss: 0.00003791
Iteration 53/1000 | Loss: 0.00003791
Iteration 54/1000 | Loss: 0.00003791
Iteration 55/1000 | Loss: 0.00003791
Iteration 56/1000 | Loss: 0.00003791
Iteration 57/1000 | Loss: 0.00003790
Iteration 58/1000 | Loss: 0.00003790
Iteration 59/1000 | Loss: 0.00003789
Iteration 60/1000 | Loss: 0.00003789
Iteration 61/1000 | Loss: 0.00003789
Iteration 62/1000 | Loss: 0.00003789
Iteration 63/1000 | Loss: 0.00003789
Iteration 64/1000 | Loss: 0.00003789
Iteration 65/1000 | Loss: 0.00003789
Iteration 66/1000 | Loss: 0.00003789
Iteration 67/1000 | Loss: 0.00003788
Iteration 68/1000 | Loss: 0.00003788
Iteration 69/1000 | Loss: 0.00003788
Iteration 70/1000 | Loss: 0.00003788
Iteration 71/1000 | Loss: 0.00003788
Iteration 72/1000 | Loss: 0.00003788
Iteration 73/1000 | Loss: 0.00003788
Iteration 74/1000 | Loss: 0.00003788
Iteration 75/1000 | Loss: 0.00003787
Iteration 76/1000 | Loss: 0.00003787
Iteration 77/1000 | Loss: 0.00003787
Iteration 78/1000 | Loss: 0.00003787
Iteration 79/1000 | Loss: 0.00003786
Iteration 80/1000 | Loss: 0.00003786
Iteration 81/1000 | Loss: 0.00003786
Iteration 82/1000 | Loss: 0.00003786
Iteration 83/1000 | Loss: 0.00003786
Iteration 84/1000 | Loss: 0.00003786
Iteration 85/1000 | Loss: 0.00003786
Iteration 86/1000 | Loss: 0.00003785
Iteration 87/1000 | Loss: 0.00003785
Iteration 88/1000 | Loss: 0.00003785
Iteration 89/1000 | Loss: 0.00003785
Iteration 90/1000 | Loss: 0.00003785
Iteration 91/1000 | Loss: 0.00003785
Iteration 92/1000 | Loss: 0.00003785
Iteration 93/1000 | Loss: 0.00003785
Iteration 94/1000 | Loss: 0.00003784
Iteration 95/1000 | Loss: 0.00003784
Iteration 96/1000 | Loss: 0.00003784
Iteration 97/1000 | Loss: 0.00003783
Iteration 98/1000 | Loss: 0.00003783
Iteration 99/1000 | Loss: 0.00003782
Iteration 100/1000 | Loss: 0.00003782
Iteration 101/1000 | Loss: 0.00003782
Iteration 102/1000 | Loss: 0.00003782
Iteration 103/1000 | Loss: 0.00003782
Iteration 104/1000 | Loss: 0.00003782
Iteration 105/1000 | Loss: 0.00003782
Iteration 106/1000 | Loss: 0.00003782
Iteration 107/1000 | Loss: 0.00003781
Iteration 108/1000 | Loss: 0.00003781
Iteration 109/1000 | Loss: 0.00003781
Iteration 110/1000 | Loss: 0.00003781
Iteration 111/1000 | Loss: 0.00003781
Iteration 112/1000 | Loss: 0.00003781
Iteration 113/1000 | Loss: 0.00003781
Iteration 114/1000 | Loss: 0.00003781
Iteration 115/1000 | Loss: 0.00003781
Iteration 116/1000 | Loss: 0.00003781
Iteration 117/1000 | Loss: 0.00003781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [3.781364648602903e-05, 3.781364648602903e-05, 3.781364648602903e-05, 3.781364648602903e-05, 3.781364648602903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.781364648602903e-05

Optimization complete. Final v2v error: 4.806608200073242 mm

Highest mean error: 5.5380144119262695 mm for frame 61

Lowest mean error: 3.47015643119812 mm for frame 11

Saving results

Total time: 46.69657874107361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472920
Iteration 2/25 | Loss: 0.00092243
Iteration 3/25 | Loss: 0.00073411
Iteration 4/25 | Loss: 0.00068853
Iteration 5/25 | Loss: 0.00067842
Iteration 6/25 | Loss: 0.00067573
Iteration 7/25 | Loss: 0.00067446
Iteration 8/25 | Loss: 0.00067432
Iteration 9/25 | Loss: 0.00067432
Iteration 10/25 | Loss: 0.00067432
Iteration 11/25 | Loss: 0.00067432
Iteration 12/25 | Loss: 0.00067432
Iteration 13/25 | Loss: 0.00067432
Iteration 14/25 | Loss: 0.00067432
Iteration 15/25 | Loss: 0.00067432
Iteration 16/25 | Loss: 0.00067432
Iteration 17/25 | Loss: 0.00067432
Iteration 18/25 | Loss: 0.00067432
Iteration 19/25 | Loss: 0.00067432
Iteration 20/25 | Loss: 0.00067432
Iteration 21/25 | Loss: 0.00067432
Iteration 22/25 | Loss: 0.00067432
Iteration 23/25 | Loss: 0.00067432
Iteration 24/25 | Loss: 0.00067432
Iteration 25/25 | Loss: 0.00067432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48463976
Iteration 2/25 | Loss: 0.00049365
Iteration 3/25 | Loss: 0.00049365
Iteration 4/25 | Loss: 0.00049365
Iteration 5/25 | Loss: 0.00049365
Iteration 6/25 | Loss: 0.00049365
Iteration 7/25 | Loss: 0.00049365
Iteration 8/25 | Loss: 0.00049365
Iteration 9/25 | Loss: 0.00049365
Iteration 10/25 | Loss: 0.00049365
Iteration 11/25 | Loss: 0.00049365
Iteration 12/25 | Loss: 0.00049365
Iteration 13/25 | Loss: 0.00049365
Iteration 14/25 | Loss: 0.00049365
Iteration 15/25 | Loss: 0.00049365
Iteration 16/25 | Loss: 0.00049365
Iteration 17/25 | Loss: 0.00049365
Iteration 18/25 | Loss: 0.00049365
Iteration 19/25 | Loss: 0.00049365
Iteration 20/25 | Loss: 0.00049365
Iteration 21/25 | Loss: 0.00049365
Iteration 22/25 | Loss: 0.00049365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004936460172757506, 0.0004936460172757506, 0.0004936460172757506, 0.0004936460172757506, 0.0004936460172757506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004936460172757506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049365
Iteration 2/1000 | Loss: 0.00004361
Iteration 3/1000 | Loss: 0.00003479
Iteration 4/1000 | Loss: 0.00002762
Iteration 5/1000 | Loss: 0.00002556
Iteration 6/1000 | Loss: 0.00002393
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002220
Iteration 9/1000 | Loss: 0.00002177
Iteration 10/1000 | Loss: 0.00002152
Iteration 11/1000 | Loss: 0.00002133
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002101
Iteration 14/1000 | Loss: 0.00002093
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002079
Iteration 17/1000 | Loss: 0.00002077
Iteration 18/1000 | Loss: 0.00002076
Iteration 19/1000 | Loss: 0.00002076
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00002074
Iteration 22/1000 | Loss: 0.00002073
Iteration 23/1000 | Loss: 0.00002073
Iteration 24/1000 | Loss: 0.00002072
Iteration 25/1000 | Loss: 0.00002071
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002070
Iteration 28/1000 | Loss: 0.00002069
Iteration 29/1000 | Loss: 0.00002069
Iteration 30/1000 | Loss: 0.00002069
Iteration 31/1000 | Loss: 0.00002069
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002067
Iteration 36/1000 | Loss: 0.00002067
Iteration 37/1000 | Loss: 0.00002066
Iteration 38/1000 | Loss: 0.00002065
Iteration 39/1000 | Loss: 0.00002065
Iteration 40/1000 | Loss: 0.00002064
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00002061
Iteration 44/1000 | Loss: 0.00002061
Iteration 45/1000 | Loss: 0.00002060
Iteration 46/1000 | Loss: 0.00002060
Iteration 47/1000 | Loss: 0.00002059
Iteration 48/1000 | Loss: 0.00002059
Iteration 49/1000 | Loss: 0.00002059
Iteration 50/1000 | Loss: 0.00002058
Iteration 51/1000 | Loss: 0.00002058
Iteration 52/1000 | Loss: 0.00002058
Iteration 53/1000 | Loss: 0.00002058
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002057
Iteration 56/1000 | Loss: 0.00002057
Iteration 57/1000 | Loss: 0.00002057
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002056
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002056
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00002055
Iteration 65/1000 | Loss: 0.00002055
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002053
Iteration 72/1000 | Loss: 0.00002053
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002052
Iteration 77/1000 | Loss: 0.00002052
Iteration 78/1000 | Loss: 0.00002052
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002051
Iteration 86/1000 | Loss: 0.00002051
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002050
Iteration 91/1000 | Loss: 0.00002050
Iteration 92/1000 | Loss: 0.00002050
Iteration 93/1000 | Loss: 0.00002049
Iteration 94/1000 | Loss: 0.00002049
Iteration 95/1000 | Loss: 0.00002049
Iteration 96/1000 | Loss: 0.00002048
Iteration 97/1000 | Loss: 0.00002048
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002047
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002047
Iteration 106/1000 | Loss: 0.00002047
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002045
Iteration 114/1000 | Loss: 0.00002045
Iteration 115/1000 | Loss: 0.00002045
Iteration 116/1000 | Loss: 0.00002045
Iteration 117/1000 | Loss: 0.00002045
Iteration 118/1000 | Loss: 0.00002045
Iteration 119/1000 | Loss: 0.00002045
Iteration 120/1000 | Loss: 0.00002045
Iteration 121/1000 | Loss: 0.00002045
Iteration 122/1000 | Loss: 0.00002045
Iteration 123/1000 | Loss: 0.00002044
Iteration 124/1000 | Loss: 0.00002044
Iteration 125/1000 | Loss: 0.00002044
Iteration 126/1000 | Loss: 0.00002044
Iteration 127/1000 | Loss: 0.00002044
Iteration 128/1000 | Loss: 0.00002044
Iteration 129/1000 | Loss: 0.00002044
Iteration 130/1000 | Loss: 0.00002044
Iteration 131/1000 | Loss: 0.00002044
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002044
Iteration 136/1000 | Loss: 0.00002044
Iteration 137/1000 | Loss: 0.00002044
Iteration 138/1000 | Loss: 0.00002043
Iteration 139/1000 | Loss: 0.00002043
Iteration 140/1000 | Loss: 0.00002043
Iteration 141/1000 | Loss: 0.00002043
Iteration 142/1000 | Loss: 0.00002043
Iteration 143/1000 | Loss: 0.00002043
Iteration 144/1000 | Loss: 0.00002043
Iteration 145/1000 | Loss: 0.00002043
Iteration 146/1000 | Loss: 0.00002043
Iteration 147/1000 | Loss: 0.00002043
Iteration 148/1000 | Loss: 0.00002043
Iteration 149/1000 | Loss: 0.00002043
Iteration 150/1000 | Loss: 0.00002043
Iteration 151/1000 | Loss: 0.00002043
Iteration 152/1000 | Loss: 0.00002043
Iteration 153/1000 | Loss: 0.00002043
Iteration 154/1000 | Loss: 0.00002043
Iteration 155/1000 | Loss: 0.00002043
Iteration 156/1000 | Loss: 0.00002042
Iteration 157/1000 | Loss: 0.00002042
Iteration 158/1000 | Loss: 0.00002042
Iteration 159/1000 | Loss: 0.00002042
Iteration 160/1000 | Loss: 0.00002042
Iteration 161/1000 | Loss: 0.00002042
Iteration 162/1000 | Loss: 0.00002042
Iteration 163/1000 | Loss: 0.00002042
Iteration 164/1000 | Loss: 0.00002042
Iteration 165/1000 | Loss: 0.00002042
Iteration 166/1000 | Loss: 0.00002042
Iteration 167/1000 | Loss: 0.00002041
Iteration 168/1000 | Loss: 0.00002041
Iteration 169/1000 | Loss: 0.00002041
Iteration 170/1000 | Loss: 0.00002041
Iteration 171/1000 | Loss: 0.00002041
Iteration 172/1000 | Loss: 0.00002041
Iteration 173/1000 | Loss: 0.00002041
Iteration 174/1000 | Loss: 0.00002041
Iteration 175/1000 | Loss: 0.00002041
Iteration 176/1000 | Loss: 0.00002041
Iteration 177/1000 | Loss: 0.00002041
Iteration 178/1000 | Loss: 0.00002041
Iteration 179/1000 | Loss: 0.00002041
Iteration 180/1000 | Loss: 0.00002041
Iteration 181/1000 | Loss: 0.00002041
Iteration 182/1000 | Loss: 0.00002041
Iteration 183/1000 | Loss: 0.00002041
Iteration 184/1000 | Loss: 0.00002041
Iteration 185/1000 | Loss: 0.00002041
Iteration 186/1000 | Loss: 0.00002041
Iteration 187/1000 | Loss: 0.00002041
Iteration 188/1000 | Loss: 0.00002041
Iteration 189/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.041319021373056e-05, 2.041319021373056e-05, 2.041319021373056e-05, 2.041319021373056e-05, 2.041319021373056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.041319021373056e-05

Optimization complete. Final v2v error: 3.671849250793457 mm

Highest mean error: 4.430757999420166 mm for frame 177

Lowest mean error: 3.332141399383545 mm for frame 151

Saving results

Total time: 46.02090668678284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872671
Iteration 2/25 | Loss: 0.00169578
Iteration 3/25 | Loss: 0.00093192
Iteration 4/25 | Loss: 0.00075790
Iteration 5/25 | Loss: 0.00073991
Iteration 6/25 | Loss: 0.00073760
Iteration 7/25 | Loss: 0.00073760
Iteration 8/25 | Loss: 0.00073760
Iteration 9/25 | Loss: 0.00073760
Iteration 10/25 | Loss: 0.00073760
Iteration 11/25 | Loss: 0.00073760
Iteration 12/25 | Loss: 0.00073760
Iteration 13/25 | Loss: 0.00073760
Iteration 14/25 | Loss: 0.00073760
Iteration 15/25 | Loss: 0.00073760
Iteration 16/25 | Loss: 0.00073760
Iteration 17/25 | Loss: 0.00073760
Iteration 18/25 | Loss: 0.00073760
Iteration 19/25 | Loss: 0.00073760
Iteration 20/25 | Loss: 0.00073760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007375965942628682, 0.0007375965942628682, 0.0007375965942628682, 0.0007375965942628682, 0.0007375965942628682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007375965942628682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39463925
Iteration 2/25 | Loss: 0.00043973
Iteration 3/25 | Loss: 0.00043973
Iteration 4/25 | Loss: 0.00043973
Iteration 5/25 | Loss: 0.00043972
Iteration 6/25 | Loss: 0.00043972
Iteration 7/25 | Loss: 0.00043972
Iteration 8/25 | Loss: 0.00043972
Iteration 9/25 | Loss: 0.00043972
Iteration 10/25 | Loss: 0.00043972
Iteration 11/25 | Loss: 0.00043972
Iteration 12/25 | Loss: 0.00043972
Iteration 13/25 | Loss: 0.00043972
Iteration 14/25 | Loss: 0.00043972
Iteration 15/25 | Loss: 0.00043972
Iteration 16/25 | Loss: 0.00043972
Iteration 17/25 | Loss: 0.00043972
Iteration 18/25 | Loss: 0.00043972
Iteration 19/25 | Loss: 0.00043972
Iteration 20/25 | Loss: 0.00043972
Iteration 21/25 | Loss: 0.00043972
Iteration 22/25 | Loss: 0.00043972
Iteration 23/25 | Loss: 0.00043972
Iteration 24/25 | Loss: 0.00043972
Iteration 25/25 | Loss: 0.00043972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043972
Iteration 2/1000 | Loss: 0.00004447
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00002008
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001962
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001939
Iteration 16/1000 | Loss: 0.00001939
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001939
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001939
Iteration 21/1000 | Loss: 0.00001939
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001939
Iteration 24/1000 | Loss: 0.00001939
Iteration 25/1000 | Loss: 0.00001939
Iteration 26/1000 | Loss: 0.00001938
Iteration 27/1000 | Loss: 0.00001938
Iteration 28/1000 | Loss: 0.00001938
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00001937
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001936
Iteration 33/1000 | Loss: 0.00001935
Iteration 34/1000 | Loss: 0.00001935
Iteration 35/1000 | Loss: 0.00001935
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001926
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001924
Iteration 63/1000 | Loss: 0.00001924
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.9232686099712737e-05, 1.9232686099712737e-05, 1.9232686099712737e-05, 1.9232686099712737e-05, 1.9232686099712737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9232686099712737e-05

Optimization complete. Final v2v error: 3.6434946060180664 mm

Highest mean error: 4.039298057556152 mm for frame 18

Lowest mean error: 3.0666141510009766 mm for frame 0

Saving results

Total time: 35.63819217681885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428731
Iteration 2/25 | Loss: 0.00080496
Iteration 3/25 | Loss: 0.00068866
Iteration 4/25 | Loss: 0.00065831
Iteration 5/25 | Loss: 0.00064713
Iteration 6/25 | Loss: 0.00064433
Iteration 7/25 | Loss: 0.00064307
Iteration 8/25 | Loss: 0.00064296
Iteration 9/25 | Loss: 0.00064296
Iteration 10/25 | Loss: 0.00064296
Iteration 11/25 | Loss: 0.00064296
Iteration 12/25 | Loss: 0.00064296
Iteration 13/25 | Loss: 0.00064296
Iteration 14/25 | Loss: 0.00064296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006429621134884655, 0.0006429621134884655, 0.0006429621134884655, 0.0006429621134884655, 0.0006429621134884655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006429621134884655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85871089
Iteration 2/25 | Loss: 0.00023369
Iteration 3/25 | Loss: 0.00023368
Iteration 4/25 | Loss: 0.00023368
Iteration 5/25 | Loss: 0.00023368
Iteration 6/25 | Loss: 0.00023368
Iteration 7/25 | Loss: 0.00023368
Iteration 8/25 | Loss: 0.00023368
Iteration 9/25 | Loss: 0.00023368
Iteration 10/25 | Loss: 0.00023368
Iteration 11/25 | Loss: 0.00023368
Iteration 12/25 | Loss: 0.00023368
Iteration 13/25 | Loss: 0.00023368
Iteration 14/25 | Loss: 0.00023368
Iteration 15/25 | Loss: 0.00023368
Iteration 16/25 | Loss: 0.00023368
Iteration 17/25 | Loss: 0.00023368
Iteration 18/25 | Loss: 0.00023368
Iteration 19/25 | Loss: 0.00023368
Iteration 20/25 | Loss: 0.00023368
Iteration 21/25 | Loss: 0.00023368
Iteration 22/25 | Loss: 0.00023368
Iteration 23/25 | Loss: 0.00023368
Iteration 24/25 | Loss: 0.00023368
Iteration 25/25 | Loss: 0.00023368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023368
Iteration 2/1000 | Loss: 0.00002409
Iteration 3/1000 | Loss: 0.00002032
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001787
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001756
Iteration 10/1000 | Loss: 0.00001742
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001724
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001714
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001712
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001711
Iteration 46/1000 | Loss: 0.00001711
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001710
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001707
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001699
Iteration 89/1000 | Loss: 0.00001699
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001698
Iteration 92/1000 | Loss: 0.00001698
Iteration 93/1000 | Loss: 0.00001698
Iteration 94/1000 | Loss: 0.00001698
Iteration 95/1000 | Loss: 0.00001698
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001697
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001696
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001696
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001693
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001693
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001693
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001692
Iteration 138/1000 | Loss: 0.00001692
Iteration 139/1000 | Loss: 0.00001692
Iteration 140/1000 | Loss: 0.00001692
Iteration 141/1000 | Loss: 0.00001692
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001692
Iteration 144/1000 | Loss: 0.00001692
Iteration 145/1000 | Loss: 0.00001692
Iteration 146/1000 | Loss: 0.00001692
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001691
Iteration 155/1000 | Loss: 0.00001691
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001690
Iteration 164/1000 | Loss: 0.00001690
Iteration 165/1000 | Loss: 0.00001690
Iteration 166/1000 | Loss: 0.00001690
Iteration 167/1000 | Loss: 0.00001690
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001689
Iteration 172/1000 | Loss: 0.00001689
Iteration 173/1000 | Loss: 0.00001689
Iteration 174/1000 | Loss: 0.00001689
Iteration 175/1000 | Loss: 0.00001689
Iteration 176/1000 | Loss: 0.00001689
Iteration 177/1000 | Loss: 0.00001689
Iteration 178/1000 | Loss: 0.00001689
Iteration 179/1000 | Loss: 0.00001689
Iteration 180/1000 | Loss: 0.00001689
Iteration 181/1000 | Loss: 0.00001689
Iteration 182/1000 | Loss: 0.00001689
Iteration 183/1000 | Loss: 0.00001689
Iteration 184/1000 | Loss: 0.00001688
Iteration 185/1000 | Loss: 0.00001688
Iteration 186/1000 | Loss: 0.00001688
Iteration 187/1000 | Loss: 0.00001688
Iteration 188/1000 | Loss: 0.00001688
Iteration 189/1000 | Loss: 0.00001688
Iteration 190/1000 | Loss: 0.00001688
Iteration 191/1000 | Loss: 0.00001688
Iteration 192/1000 | Loss: 0.00001688
Iteration 193/1000 | Loss: 0.00001688
Iteration 194/1000 | Loss: 0.00001688
Iteration 195/1000 | Loss: 0.00001688
Iteration 196/1000 | Loss: 0.00001688
Iteration 197/1000 | Loss: 0.00001688
Iteration 198/1000 | Loss: 0.00001688
Iteration 199/1000 | Loss: 0.00001688
Iteration 200/1000 | Loss: 0.00001688
Iteration 201/1000 | Loss: 0.00001688
Iteration 202/1000 | Loss: 0.00001688
Iteration 203/1000 | Loss: 0.00001688
Iteration 204/1000 | Loss: 0.00001688
Iteration 205/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.6876305380719714e-05, 1.6876305380719714e-05, 1.6876305380719714e-05, 1.6876305380719714e-05, 1.6876305380719714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6876305380719714e-05

Optimization complete. Final v2v error: 3.4783782958984375 mm

Highest mean error: 3.809830665588379 mm for frame 158

Lowest mean error: 3.2702462673187256 mm for frame 181

Saving results

Total time: 44.861109256744385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860705
Iteration 2/25 | Loss: 0.00130033
Iteration 3/25 | Loss: 0.00087312
Iteration 4/25 | Loss: 0.00080749
Iteration 5/25 | Loss: 0.00079679
Iteration 6/25 | Loss: 0.00081983
Iteration 7/25 | Loss: 0.00082296
Iteration 8/25 | Loss: 0.00080163
Iteration 9/25 | Loss: 0.00076799
Iteration 10/25 | Loss: 0.00075929
Iteration 11/25 | Loss: 0.00075094
Iteration 12/25 | Loss: 0.00074458
Iteration 13/25 | Loss: 0.00074244
Iteration 14/25 | Loss: 0.00074121
Iteration 15/25 | Loss: 0.00074073
Iteration 16/25 | Loss: 0.00074057
Iteration 17/25 | Loss: 0.00074053
Iteration 18/25 | Loss: 0.00074053
Iteration 19/25 | Loss: 0.00074053
Iteration 20/25 | Loss: 0.00074053
Iteration 21/25 | Loss: 0.00074053
Iteration 22/25 | Loss: 0.00074053
Iteration 23/25 | Loss: 0.00074053
Iteration 24/25 | Loss: 0.00074053
Iteration 25/25 | Loss: 0.00074053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39856517
Iteration 2/25 | Loss: 0.00032519
Iteration 3/25 | Loss: 0.00032515
Iteration 4/25 | Loss: 0.00032515
Iteration 5/25 | Loss: 0.00032514
Iteration 6/25 | Loss: 0.00032514
Iteration 7/25 | Loss: 0.00032514
Iteration 8/25 | Loss: 0.00032514
Iteration 9/25 | Loss: 0.00032514
Iteration 10/25 | Loss: 0.00032514
Iteration 11/25 | Loss: 0.00032514
Iteration 12/25 | Loss: 0.00032514
Iteration 13/25 | Loss: 0.00032514
Iteration 14/25 | Loss: 0.00032514
Iteration 15/25 | Loss: 0.00032514
Iteration 16/25 | Loss: 0.00032514
Iteration 17/25 | Loss: 0.00032514
Iteration 18/25 | Loss: 0.00032514
Iteration 19/25 | Loss: 0.00032514
Iteration 20/25 | Loss: 0.00032514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00032514307531528175, 0.00032514307531528175, 0.00032514307531528175, 0.00032514307531528175, 0.00032514307531528175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032514307531528175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032514
Iteration 2/1000 | Loss: 0.00003155
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00002223
Iteration 5/1000 | Loss: 0.00002114
Iteration 6/1000 | Loss: 0.00002059
Iteration 7/1000 | Loss: 0.00002011
Iteration 8/1000 | Loss: 0.00002005
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001945
Iteration 23/1000 | Loss: 0.00001945
Iteration 24/1000 | Loss: 0.00001943
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001943
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001939
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001939
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001938
Iteration 37/1000 | Loss: 0.00001937
Iteration 38/1000 | Loss: 0.00001937
Iteration 39/1000 | Loss: 0.00001936
Iteration 40/1000 | Loss: 0.00001936
Iteration 41/1000 | Loss: 0.00001936
Iteration 42/1000 | Loss: 0.00001936
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001935
Iteration 45/1000 | Loss: 0.00001934
Iteration 46/1000 | Loss: 0.00001934
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001934
Iteration 49/1000 | Loss: 0.00001933
Iteration 50/1000 | Loss: 0.00001933
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001933
Iteration 55/1000 | Loss: 0.00001931
Iteration 56/1000 | Loss: 0.00001931
Iteration 57/1000 | Loss: 0.00001931
Iteration 58/1000 | Loss: 0.00001930
Iteration 59/1000 | Loss: 0.00001930
Iteration 60/1000 | Loss: 0.00001930
Iteration 61/1000 | Loss: 0.00001930
Iteration 62/1000 | Loss: 0.00001930
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00001929
Iteration 66/1000 | Loss: 0.00001927
Iteration 67/1000 | Loss: 0.00001927
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001925
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001922
Iteration 80/1000 | Loss: 0.00001922
Iteration 81/1000 | Loss: 0.00001921
Iteration 82/1000 | Loss: 0.00001921
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001920
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001919
Iteration 91/1000 | Loss: 0.00001919
Iteration 92/1000 | Loss: 0.00001919
Iteration 93/1000 | Loss: 0.00001919
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001919
Iteration 96/1000 | Loss: 0.00001919
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001918
Iteration 99/1000 | Loss: 0.00001918
Iteration 100/1000 | Loss: 0.00001918
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001917
Iteration 103/1000 | Loss: 0.00001917
Iteration 104/1000 | Loss: 0.00001917
Iteration 105/1000 | Loss: 0.00001917
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001917
Iteration 110/1000 | Loss: 0.00001917
Iteration 111/1000 | Loss: 0.00001917
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001916
Iteration 119/1000 | Loss: 0.00001916
Iteration 120/1000 | Loss: 0.00001916
Iteration 121/1000 | Loss: 0.00001916
Iteration 122/1000 | Loss: 0.00001915
Iteration 123/1000 | Loss: 0.00001915
Iteration 124/1000 | Loss: 0.00001915
Iteration 125/1000 | Loss: 0.00001915
Iteration 126/1000 | Loss: 0.00001915
Iteration 127/1000 | Loss: 0.00001915
Iteration 128/1000 | Loss: 0.00001915
Iteration 129/1000 | Loss: 0.00001915
Iteration 130/1000 | Loss: 0.00001915
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001915
Iteration 134/1000 | Loss: 0.00001915
Iteration 135/1000 | Loss: 0.00001914
Iteration 136/1000 | Loss: 0.00001914
Iteration 137/1000 | Loss: 0.00001914
Iteration 138/1000 | Loss: 0.00001914
Iteration 139/1000 | Loss: 0.00001914
Iteration 140/1000 | Loss: 0.00001914
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001914
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001913
Iteration 145/1000 | Loss: 0.00001913
Iteration 146/1000 | Loss: 0.00001913
Iteration 147/1000 | Loss: 0.00001913
Iteration 148/1000 | Loss: 0.00001913
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001913
Iteration 152/1000 | Loss: 0.00001913
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001913
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001913
Iteration 159/1000 | Loss: 0.00001913
Iteration 160/1000 | Loss: 0.00001912
Iteration 161/1000 | Loss: 0.00001912
Iteration 162/1000 | Loss: 0.00001912
Iteration 163/1000 | Loss: 0.00001912
Iteration 164/1000 | Loss: 0.00001912
Iteration 165/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.912477455334738e-05, 1.912477455334738e-05, 1.912477455334738e-05, 1.912477455334738e-05, 1.912477455334738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.912477455334738e-05

Optimization complete. Final v2v error: 3.638007640838623 mm

Highest mean error: 4.25263786315918 mm for frame 65

Lowest mean error: 3.2524755001068115 mm for frame 34

Saving results

Total time: 64.28387069702148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00314360
Iteration 2/25 | Loss: 0.00080746
Iteration 3/25 | Loss: 0.00063343
Iteration 4/25 | Loss: 0.00058497
Iteration 5/25 | Loss: 0.00057443
Iteration 6/25 | Loss: 0.00057182
Iteration 7/25 | Loss: 0.00057116
Iteration 8/25 | Loss: 0.00057116
Iteration 9/25 | Loss: 0.00057116
Iteration 10/25 | Loss: 0.00057116
Iteration 11/25 | Loss: 0.00057116
Iteration 12/25 | Loss: 0.00057116
Iteration 13/25 | Loss: 0.00057116
Iteration 14/25 | Loss: 0.00057116
Iteration 15/25 | Loss: 0.00057116
Iteration 16/25 | Loss: 0.00057116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005711602861993015, 0.0005711602861993015, 0.0005711602861993015, 0.0005711602861993015, 0.0005711602861993015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005711602861993015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46504080
Iteration 2/25 | Loss: 0.00022744
Iteration 3/25 | Loss: 0.00022743
Iteration 4/25 | Loss: 0.00022743
Iteration 5/25 | Loss: 0.00022743
Iteration 6/25 | Loss: 0.00022743
Iteration 7/25 | Loss: 0.00022743
Iteration 8/25 | Loss: 0.00022743
Iteration 9/25 | Loss: 0.00022743
Iteration 10/25 | Loss: 0.00022743
Iteration 11/25 | Loss: 0.00022743
Iteration 12/25 | Loss: 0.00022743
Iteration 13/25 | Loss: 0.00022743
Iteration 14/25 | Loss: 0.00022743
Iteration 15/25 | Loss: 0.00022743
Iteration 16/25 | Loss: 0.00022743
Iteration 17/25 | Loss: 0.00022743
Iteration 18/25 | Loss: 0.00022743
Iteration 19/25 | Loss: 0.00022743
Iteration 20/25 | Loss: 0.00022743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00022743141744285822, 0.00022743141744285822, 0.00022743141744285822, 0.00022743141744285822, 0.00022743141744285822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022743141744285822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022743
Iteration 2/1000 | Loss: 0.00002591
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001521
Iteration 5/1000 | Loss: 0.00001437
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001316
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001270
Iteration 39/1000 | Loss: 0.00001270
Iteration 40/1000 | Loss: 0.00001270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [1.2696795238298364e-05, 1.2696795238298364e-05, 1.2696795238298364e-05, 1.2696795238298364e-05, 1.2696795238298364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2696795238298364e-05

Optimization complete. Final v2v error: 3.0142102241516113 mm

Highest mean error: 3.2331016063690186 mm for frame 5

Lowest mean error: 2.75606107711792 mm for frame 29

Saving results

Total time: 29.79004979133606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838493
Iteration 2/25 | Loss: 0.00122983
Iteration 3/25 | Loss: 0.00082564
Iteration 4/25 | Loss: 0.00077067
Iteration 5/25 | Loss: 0.00075431
Iteration 6/25 | Loss: 0.00075087
Iteration 7/25 | Loss: 0.00075007
Iteration 8/25 | Loss: 0.00075007
Iteration 9/25 | Loss: 0.00075007
Iteration 10/25 | Loss: 0.00075007
Iteration 11/25 | Loss: 0.00075007
Iteration 12/25 | Loss: 0.00075007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007500717183575034, 0.0007500717183575034, 0.0007500717183575034, 0.0007500717183575034, 0.0007500717183575034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007500717183575034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09342599
Iteration 2/25 | Loss: 0.00036348
Iteration 3/25 | Loss: 0.00036348
Iteration 4/25 | Loss: 0.00036348
Iteration 5/25 | Loss: 0.00036348
Iteration 6/25 | Loss: 0.00036348
Iteration 7/25 | Loss: 0.00036348
Iteration 8/25 | Loss: 0.00036348
Iteration 9/25 | Loss: 0.00036348
Iteration 10/25 | Loss: 0.00036348
Iteration 11/25 | Loss: 0.00036348
Iteration 12/25 | Loss: 0.00036348
Iteration 13/25 | Loss: 0.00036348
Iteration 14/25 | Loss: 0.00036348
Iteration 15/25 | Loss: 0.00036348
Iteration 16/25 | Loss: 0.00036348
Iteration 17/25 | Loss: 0.00036348
Iteration 18/25 | Loss: 0.00036348
Iteration 19/25 | Loss: 0.00036348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00036347549757920206, 0.00036347549757920206, 0.00036347549757920206, 0.00036347549757920206, 0.00036347549757920206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036347549757920206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036348
Iteration 2/1000 | Loss: 0.00005951
Iteration 3/1000 | Loss: 0.00004369
Iteration 4/1000 | Loss: 0.00003786
Iteration 5/1000 | Loss: 0.00003572
Iteration 6/1000 | Loss: 0.00003454
Iteration 7/1000 | Loss: 0.00003359
Iteration 8/1000 | Loss: 0.00003281
Iteration 9/1000 | Loss: 0.00003221
Iteration 10/1000 | Loss: 0.00003170
Iteration 11/1000 | Loss: 0.00003138
Iteration 12/1000 | Loss: 0.00003114
Iteration 13/1000 | Loss: 0.00003092
Iteration 14/1000 | Loss: 0.00003074
Iteration 15/1000 | Loss: 0.00003054
Iteration 16/1000 | Loss: 0.00003040
Iteration 17/1000 | Loss: 0.00003030
Iteration 18/1000 | Loss: 0.00003028
Iteration 19/1000 | Loss: 0.00003022
Iteration 20/1000 | Loss: 0.00003021
Iteration 21/1000 | Loss: 0.00003020
Iteration 22/1000 | Loss: 0.00003019
Iteration 23/1000 | Loss: 0.00003019
Iteration 24/1000 | Loss: 0.00003015
Iteration 25/1000 | Loss: 0.00003014
Iteration 26/1000 | Loss: 0.00003014
Iteration 27/1000 | Loss: 0.00003013
Iteration 28/1000 | Loss: 0.00003013
Iteration 29/1000 | Loss: 0.00003012
Iteration 30/1000 | Loss: 0.00003012
Iteration 31/1000 | Loss: 0.00003010
Iteration 32/1000 | Loss: 0.00003010
Iteration 33/1000 | Loss: 0.00003008
Iteration 34/1000 | Loss: 0.00003008
Iteration 35/1000 | Loss: 0.00003007
Iteration 36/1000 | Loss: 0.00003007
Iteration 37/1000 | Loss: 0.00003007
Iteration 38/1000 | Loss: 0.00003006
Iteration 39/1000 | Loss: 0.00003006
Iteration 40/1000 | Loss: 0.00003006
Iteration 41/1000 | Loss: 0.00003005
Iteration 42/1000 | Loss: 0.00003005
Iteration 43/1000 | Loss: 0.00003005
Iteration 44/1000 | Loss: 0.00003004
Iteration 45/1000 | Loss: 0.00003004
Iteration 46/1000 | Loss: 0.00003003
Iteration 47/1000 | Loss: 0.00003003
Iteration 48/1000 | Loss: 0.00003003
Iteration 49/1000 | Loss: 0.00003002
Iteration 50/1000 | Loss: 0.00003002
Iteration 51/1000 | Loss: 0.00003002
Iteration 52/1000 | Loss: 0.00003002
Iteration 53/1000 | Loss: 0.00003002
Iteration 54/1000 | Loss: 0.00003002
Iteration 55/1000 | Loss: 0.00003002
Iteration 56/1000 | Loss: 0.00003001
Iteration 57/1000 | Loss: 0.00003001
Iteration 58/1000 | Loss: 0.00003001
Iteration 59/1000 | Loss: 0.00002999
Iteration 60/1000 | Loss: 0.00002999
Iteration 61/1000 | Loss: 0.00002998
Iteration 62/1000 | Loss: 0.00002997
Iteration 63/1000 | Loss: 0.00002997
Iteration 64/1000 | Loss: 0.00002997
Iteration 65/1000 | Loss: 0.00002996
Iteration 66/1000 | Loss: 0.00002996
Iteration 67/1000 | Loss: 0.00002992
Iteration 68/1000 | Loss: 0.00002992
Iteration 69/1000 | Loss: 0.00002991
Iteration 70/1000 | Loss: 0.00002991
Iteration 71/1000 | Loss: 0.00002990
Iteration 72/1000 | Loss: 0.00002990
Iteration 73/1000 | Loss: 0.00002990
Iteration 74/1000 | Loss: 0.00002990
Iteration 75/1000 | Loss: 0.00002990
Iteration 76/1000 | Loss: 0.00002989
Iteration 77/1000 | Loss: 0.00002989
Iteration 78/1000 | Loss: 0.00002989
Iteration 79/1000 | Loss: 0.00002989
Iteration 80/1000 | Loss: 0.00002989
Iteration 81/1000 | Loss: 0.00002988
Iteration 82/1000 | Loss: 0.00002988
Iteration 83/1000 | Loss: 0.00002988
Iteration 84/1000 | Loss: 0.00002988
Iteration 85/1000 | Loss: 0.00002988
Iteration 86/1000 | Loss: 0.00002987
Iteration 87/1000 | Loss: 0.00002987
Iteration 88/1000 | Loss: 0.00002987
Iteration 89/1000 | Loss: 0.00002987
Iteration 90/1000 | Loss: 0.00002987
Iteration 91/1000 | Loss: 0.00002986
Iteration 92/1000 | Loss: 0.00002986
Iteration 93/1000 | Loss: 0.00002986
Iteration 94/1000 | Loss: 0.00002986
Iteration 95/1000 | Loss: 0.00002985
Iteration 96/1000 | Loss: 0.00002985
Iteration 97/1000 | Loss: 0.00002984
Iteration 98/1000 | Loss: 0.00002984
Iteration 99/1000 | Loss: 0.00002984
Iteration 100/1000 | Loss: 0.00002983
Iteration 101/1000 | Loss: 0.00002983
Iteration 102/1000 | Loss: 0.00002983
Iteration 103/1000 | Loss: 0.00002983
Iteration 104/1000 | Loss: 0.00002983
Iteration 105/1000 | Loss: 0.00002983
Iteration 106/1000 | Loss: 0.00002983
Iteration 107/1000 | Loss: 0.00002982
Iteration 108/1000 | Loss: 0.00002982
Iteration 109/1000 | Loss: 0.00002982
Iteration 110/1000 | Loss: 0.00002982
Iteration 111/1000 | Loss: 0.00002981
Iteration 112/1000 | Loss: 0.00002981
Iteration 113/1000 | Loss: 0.00002980
Iteration 114/1000 | Loss: 0.00002980
Iteration 115/1000 | Loss: 0.00002980
Iteration 116/1000 | Loss: 0.00002979
Iteration 117/1000 | Loss: 0.00002979
Iteration 118/1000 | Loss: 0.00002979
Iteration 119/1000 | Loss: 0.00002979
Iteration 120/1000 | Loss: 0.00002979
Iteration 121/1000 | Loss: 0.00002979
Iteration 122/1000 | Loss: 0.00002979
Iteration 123/1000 | Loss: 0.00002979
Iteration 124/1000 | Loss: 0.00002979
Iteration 125/1000 | Loss: 0.00002978
Iteration 126/1000 | Loss: 0.00002977
Iteration 127/1000 | Loss: 0.00002977
Iteration 128/1000 | Loss: 0.00002977
Iteration 129/1000 | Loss: 0.00002977
Iteration 130/1000 | Loss: 0.00002977
Iteration 131/1000 | Loss: 0.00002977
Iteration 132/1000 | Loss: 0.00002977
Iteration 133/1000 | Loss: 0.00002976
Iteration 134/1000 | Loss: 0.00002976
Iteration 135/1000 | Loss: 0.00002976
Iteration 136/1000 | Loss: 0.00002976
Iteration 137/1000 | Loss: 0.00002976
Iteration 138/1000 | Loss: 0.00002976
Iteration 139/1000 | Loss: 0.00002976
Iteration 140/1000 | Loss: 0.00002976
Iteration 141/1000 | Loss: 0.00002976
Iteration 142/1000 | Loss: 0.00002976
Iteration 143/1000 | Loss: 0.00002976
Iteration 144/1000 | Loss: 0.00002976
Iteration 145/1000 | Loss: 0.00002976
Iteration 146/1000 | Loss: 0.00002976
Iteration 147/1000 | Loss: 0.00002976
Iteration 148/1000 | Loss: 0.00002975
Iteration 149/1000 | Loss: 0.00002975
Iteration 150/1000 | Loss: 0.00002975
Iteration 151/1000 | Loss: 0.00002975
Iteration 152/1000 | Loss: 0.00002975
Iteration 153/1000 | Loss: 0.00002975
Iteration 154/1000 | Loss: 0.00002975
Iteration 155/1000 | Loss: 0.00002974
Iteration 156/1000 | Loss: 0.00002974
Iteration 157/1000 | Loss: 0.00002974
Iteration 158/1000 | Loss: 0.00002974
Iteration 159/1000 | Loss: 0.00002974
Iteration 160/1000 | Loss: 0.00002973
Iteration 161/1000 | Loss: 0.00002973
Iteration 162/1000 | Loss: 0.00002973
Iteration 163/1000 | Loss: 0.00002973
Iteration 164/1000 | Loss: 0.00002973
Iteration 165/1000 | Loss: 0.00002972
Iteration 166/1000 | Loss: 0.00002972
Iteration 167/1000 | Loss: 0.00002972
Iteration 168/1000 | Loss: 0.00002972
Iteration 169/1000 | Loss: 0.00002972
Iteration 170/1000 | Loss: 0.00002972
Iteration 171/1000 | Loss: 0.00002972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.972454240079969e-05, 2.972454240079969e-05, 2.972454240079969e-05, 2.972454240079969e-05, 2.972454240079969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.972454240079969e-05

Optimization complete. Final v2v error: 4.3578290939331055 mm

Highest mean error: 5.9227728843688965 mm for frame 35

Lowest mean error: 3.5151724815368652 mm for frame 0

Saving results

Total time: 53.78878307342529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819984
Iteration 2/25 | Loss: 0.00145090
Iteration 3/25 | Loss: 0.00083003
Iteration 4/25 | Loss: 0.00069740
Iteration 5/25 | Loss: 0.00068046
Iteration 6/25 | Loss: 0.00067758
Iteration 7/25 | Loss: 0.00067718
Iteration 8/25 | Loss: 0.00067718
Iteration 9/25 | Loss: 0.00067718
Iteration 10/25 | Loss: 0.00067718
Iteration 11/25 | Loss: 0.00067718
Iteration 12/25 | Loss: 0.00067718
Iteration 13/25 | Loss: 0.00067718
Iteration 14/25 | Loss: 0.00067718
Iteration 15/25 | Loss: 0.00067718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006771776243112981, 0.0006771776243112981, 0.0006771776243112981, 0.0006771776243112981, 0.0006771776243112981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006771776243112981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49983358
Iteration 2/25 | Loss: 0.00024361
Iteration 3/25 | Loss: 0.00024361
Iteration 4/25 | Loss: 0.00024361
Iteration 5/25 | Loss: 0.00024361
Iteration 6/25 | Loss: 0.00024361
Iteration 7/25 | Loss: 0.00024361
Iteration 8/25 | Loss: 0.00024361
Iteration 9/25 | Loss: 0.00024361
Iteration 10/25 | Loss: 0.00024361
Iteration 11/25 | Loss: 0.00024361
Iteration 12/25 | Loss: 0.00024361
Iteration 13/25 | Loss: 0.00024361
Iteration 14/25 | Loss: 0.00024361
Iteration 15/25 | Loss: 0.00024361
Iteration 16/25 | Loss: 0.00024361
Iteration 17/25 | Loss: 0.00024361
Iteration 18/25 | Loss: 0.00024361
Iteration 19/25 | Loss: 0.00024361
Iteration 20/25 | Loss: 0.00024361
Iteration 21/25 | Loss: 0.00024361
Iteration 22/25 | Loss: 0.00024361
Iteration 23/25 | Loss: 0.00024361
Iteration 24/25 | Loss: 0.00024361
Iteration 25/25 | Loss: 0.00024361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024361
Iteration 2/1000 | Loss: 0.00002676
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001905
Iteration 5/1000 | Loss: 0.00001812
Iteration 6/1000 | Loss: 0.00001775
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001668
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001667
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001665
Iteration 21/1000 | Loss: 0.00001664
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001657
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001656
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001651
Iteration 34/1000 | Loss: 0.00001651
Iteration 35/1000 | Loss: 0.00001651
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001649
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001648
Iteration 49/1000 | Loss: 0.00001648
Iteration 50/1000 | Loss: 0.00001648
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001647
Iteration 56/1000 | Loss: 0.00001647
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001646
Iteration 60/1000 | Loss: 0.00001646
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001645
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001645
Iteration 66/1000 | Loss: 0.00001645
Iteration 67/1000 | Loss: 0.00001645
Iteration 68/1000 | Loss: 0.00001645
Iteration 69/1000 | Loss: 0.00001645
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001644
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001644
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001643
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001642
Iteration 84/1000 | Loss: 0.00001641
Iteration 85/1000 | Loss: 0.00001641
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00001640
Iteration 88/1000 | Loss: 0.00001640
Iteration 89/1000 | Loss: 0.00001640
Iteration 90/1000 | Loss: 0.00001640
Iteration 91/1000 | Loss: 0.00001640
Iteration 92/1000 | Loss: 0.00001640
Iteration 93/1000 | Loss: 0.00001640
Iteration 94/1000 | Loss: 0.00001640
Iteration 95/1000 | Loss: 0.00001640
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001639
Iteration 99/1000 | Loss: 0.00001639
Iteration 100/1000 | Loss: 0.00001639
Iteration 101/1000 | Loss: 0.00001639
Iteration 102/1000 | Loss: 0.00001639
Iteration 103/1000 | Loss: 0.00001639
Iteration 104/1000 | Loss: 0.00001638
Iteration 105/1000 | Loss: 0.00001638
Iteration 106/1000 | Loss: 0.00001638
Iteration 107/1000 | Loss: 0.00001638
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001638
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001638
Iteration 126/1000 | Loss: 0.00001638
Iteration 127/1000 | Loss: 0.00001638
Iteration 128/1000 | Loss: 0.00001638
Iteration 129/1000 | Loss: 0.00001638
Iteration 130/1000 | Loss: 0.00001638
Iteration 131/1000 | Loss: 0.00001638
Iteration 132/1000 | Loss: 0.00001638
Iteration 133/1000 | Loss: 0.00001638
Iteration 134/1000 | Loss: 0.00001638
Iteration 135/1000 | Loss: 0.00001638
Iteration 136/1000 | Loss: 0.00001638
Iteration 137/1000 | Loss: 0.00001638
Iteration 138/1000 | Loss: 0.00001638
Iteration 139/1000 | Loss: 0.00001638
Iteration 140/1000 | Loss: 0.00001638
Iteration 141/1000 | Loss: 0.00001638
Iteration 142/1000 | Loss: 0.00001638
Iteration 143/1000 | Loss: 0.00001638
Iteration 144/1000 | Loss: 0.00001638
Iteration 145/1000 | Loss: 0.00001638
Iteration 146/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.6379830412915908e-05, 1.6379830412915908e-05, 1.6379830412915908e-05, 1.6379830412915908e-05, 1.6379830412915908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6379830412915908e-05

Optimization complete. Final v2v error: 3.350916862487793 mm

Highest mean error: 3.83524227142334 mm for frame 135

Lowest mean error: 3.0408520698547363 mm for frame 113

Saving results

Total time: 32.812602043151855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339936
Iteration 2/25 | Loss: 0.00066641
Iteration 3/25 | Loss: 0.00055896
Iteration 4/25 | Loss: 0.00054243
Iteration 5/25 | Loss: 0.00053821
Iteration 6/25 | Loss: 0.00053710
Iteration 7/25 | Loss: 0.00053707
Iteration 8/25 | Loss: 0.00053707
Iteration 9/25 | Loss: 0.00053707
Iteration 10/25 | Loss: 0.00053707
Iteration 11/25 | Loss: 0.00053707
Iteration 12/25 | Loss: 0.00053707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005370742874220014, 0.0005370742874220014, 0.0005370742874220014, 0.0005370742874220014, 0.0005370742874220014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005370742874220014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72168410
Iteration 2/25 | Loss: 0.00021472
Iteration 3/25 | Loss: 0.00021472
Iteration 4/25 | Loss: 0.00021471
Iteration 5/25 | Loss: 0.00021471
Iteration 6/25 | Loss: 0.00021471
Iteration 7/25 | Loss: 0.00021471
Iteration 8/25 | Loss: 0.00021471
Iteration 9/25 | Loss: 0.00021471
Iteration 10/25 | Loss: 0.00021471
Iteration 11/25 | Loss: 0.00021471
Iteration 12/25 | Loss: 0.00021471
Iteration 13/25 | Loss: 0.00021471
Iteration 14/25 | Loss: 0.00021471
Iteration 15/25 | Loss: 0.00021471
Iteration 16/25 | Loss: 0.00021471
Iteration 17/25 | Loss: 0.00021471
Iteration 18/25 | Loss: 0.00021471
Iteration 19/25 | Loss: 0.00021471
Iteration 20/25 | Loss: 0.00021471
Iteration 21/25 | Loss: 0.00021471
Iteration 22/25 | Loss: 0.00021471
Iteration 23/25 | Loss: 0.00021471
Iteration 24/25 | Loss: 0.00021471
Iteration 25/25 | Loss: 0.00021471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021471
Iteration 2/1000 | Loss: 0.00001540
Iteration 3/1000 | Loss: 0.00001310
Iteration 4/1000 | Loss: 0.00001205
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001112
Iteration 7/1000 | Loss: 0.00001111
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001085
Iteration 10/1000 | Loss: 0.00001075
Iteration 11/1000 | Loss: 0.00001067
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001065
Iteration 14/1000 | Loss: 0.00001065
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001063
Iteration 19/1000 | Loss: 0.00001062
Iteration 20/1000 | Loss: 0.00001062
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001061
Iteration 27/1000 | Loss: 0.00001060
Iteration 28/1000 | Loss: 0.00001060
Iteration 29/1000 | Loss: 0.00001059
Iteration 30/1000 | Loss: 0.00001058
Iteration 31/1000 | Loss: 0.00001058
Iteration 32/1000 | Loss: 0.00001057
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001057
Iteration 35/1000 | Loss: 0.00001056
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001053
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001053
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001050
Iteration 49/1000 | Loss: 0.00001050
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001050
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001048
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001048
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001045
Iteration 71/1000 | Loss: 0.00001045
Iteration 72/1000 | Loss: 0.00001044
Iteration 73/1000 | Loss: 0.00001044
Iteration 74/1000 | Loss: 0.00001044
Iteration 75/1000 | Loss: 0.00001044
Iteration 76/1000 | Loss: 0.00001044
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001044
Iteration 80/1000 | Loss: 0.00001044
Iteration 81/1000 | Loss: 0.00001044
Iteration 82/1000 | Loss: 0.00001044
Iteration 83/1000 | Loss: 0.00001044
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001041
Iteration 89/1000 | Loss: 0.00001041
Iteration 90/1000 | Loss: 0.00001040
Iteration 91/1000 | Loss: 0.00001040
Iteration 92/1000 | Loss: 0.00001040
Iteration 93/1000 | Loss: 0.00001040
Iteration 94/1000 | Loss: 0.00001040
Iteration 95/1000 | Loss: 0.00001040
Iteration 96/1000 | Loss: 0.00001039
Iteration 97/1000 | Loss: 0.00001039
Iteration 98/1000 | Loss: 0.00001039
Iteration 99/1000 | Loss: 0.00001039
Iteration 100/1000 | Loss: 0.00001039
Iteration 101/1000 | Loss: 0.00001039
Iteration 102/1000 | Loss: 0.00001039
Iteration 103/1000 | Loss: 0.00001039
Iteration 104/1000 | Loss: 0.00001038
Iteration 105/1000 | Loss: 0.00001038
Iteration 106/1000 | Loss: 0.00001038
Iteration 107/1000 | Loss: 0.00001038
Iteration 108/1000 | Loss: 0.00001038
Iteration 109/1000 | Loss: 0.00001038
Iteration 110/1000 | Loss: 0.00001038
Iteration 111/1000 | Loss: 0.00001038
Iteration 112/1000 | Loss: 0.00001037
Iteration 113/1000 | Loss: 0.00001037
Iteration 114/1000 | Loss: 0.00001037
Iteration 115/1000 | Loss: 0.00001037
Iteration 116/1000 | Loss: 0.00001037
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001034
Iteration 127/1000 | Loss: 0.00001034
Iteration 128/1000 | Loss: 0.00001033
Iteration 129/1000 | Loss: 0.00001033
Iteration 130/1000 | Loss: 0.00001033
Iteration 131/1000 | Loss: 0.00001033
Iteration 132/1000 | Loss: 0.00001032
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001032
Iteration 137/1000 | Loss: 0.00001032
Iteration 138/1000 | Loss: 0.00001031
Iteration 139/1000 | Loss: 0.00001031
Iteration 140/1000 | Loss: 0.00001031
Iteration 141/1000 | Loss: 0.00001031
Iteration 142/1000 | Loss: 0.00001031
Iteration 143/1000 | Loss: 0.00001031
Iteration 144/1000 | Loss: 0.00001030
Iteration 145/1000 | Loss: 0.00001030
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001030
Iteration 148/1000 | Loss: 0.00001030
Iteration 149/1000 | Loss: 0.00001030
Iteration 150/1000 | Loss: 0.00001030
Iteration 151/1000 | Loss: 0.00001030
Iteration 152/1000 | Loss: 0.00001030
Iteration 153/1000 | Loss: 0.00001030
Iteration 154/1000 | Loss: 0.00001030
Iteration 155/1000 | Loss: 0.00001030
Iteration 156/1000 | Loss: 0.00001030
Iteration 157/1000 | Loss: 0.00001030
Iteration 158/1000 | Loss: 0.00001030
Iteration 159/1000 | Loss: 0.00001030
Iteration 160/1000 | Loss: 0.00001029
Iteration 161/1000 | Loss: 0.00001029
Iteration 162/1000 | Loss: 0.00001029
Iteration 163/1000 | Loss: 0.00001029
Iteration 164/1000 | Loss: 0.00001029
Iteration 165/1000 | Loss: 0.00001029
Iteration 166/1000 | Loss: 0.00001029
Iteration 167/1000 | Loss: 0.00001029
Iteration 168/1000 | Loss: 0.00001029
Iteration 169/1000 | Loss: 0.00001029
Iteration 170/1000 | Loss: 0.00001029
Iteration 171/1000 | Loss: 0.00001029
Iteration 172/1000 | Loss: 0.00001029
Iteration 173/1000 | Loss: 0.00001029
Iteration 174/1000 | Loss: 0.00001029
Iteration 175/1000 | Loss: 0.00001029
Iteration 176/1000 | Loss: 0.00001029
Iteration 177/1000 | Loss: 0.00001029
Iteration 178/1000 | Loss: 0.00001029
Iteration 179/1000 | Loss: 0.00001029
Iteration 180/1000 | Loss: 0.00001029
Iteration 181/1000 | Loss: 0.00001029
Iteration 182/1000 | Loss: 0.00001029
Iteration 183/1000 | Loss: 0.00001029
Iteration 184/1000 | Loss: 0.00001029
Iteration 185/1000 | Loss: 0.00001029
Iteration 186/1000 | Loss: 0.00001029
Iteration 187/1000 | Loss: 0.00001029
Iteration 188/1000 | Loss: 0.00001029
Iteration 189/1000 | Loss: 0.00001029
Iteration 190/1000 | Loss: 0.00001029
Iteration 191/1000 | Loss: 0.00001029
Iteration 192/1000 | Loss: 0.00001029
Iteration 193/1000 | Loss: 0.00001029
Iteration 194/1000 | Loss: 0.00001029
Iteration 195/1000 | Loss: 0.00001029
Iteration 196/1000 | Loss: 0.00001029
Iteration 197/1000 | Loss: 0.00001029
Iteration 198/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.0293205377820414e-05, 1.0293205377820414e-05, 1.0293205377820414e-05, 1.0293205377820414e-05, 1.0293205377820414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0293205377820414e-05

Optimization complete. Final v2v error: 2.7135095596313477 mm

Highest mean error: 3.184964895248413 mm for frame 130

Lowest mean error: 2.5554075241088867 mm for frame 118

Saving results

Total time: 39.64262580871582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00664457
Iteration 2/25 | Loss: 0.00124669
Iteration 3/25 | Loss: 0.00077003
Iteration 4/25 | Loss: 0.00068532
Iteration 5/25 | Loss: 0.00066384
Iteration 6/25 | Loss: 0.00065964
Iteration 7/25 | Loss: 0.00065508
Iteration 8/25 | Loss: 0.00065843
Iteration 9/25 | Loss: 0.00065422
Iteration 10/25 | Loss: 0.00065309
Iteration 11/25 | Loss: 0.00065245
Iteration 12/25 | Loss: 0.00065221
Iteration 13/25 | Loss: 0.00065204
Iteration 14/25 | Loss: 0.00065204
Iteration 15/25 | Loss: 0.00065203
Iteration 16/25 | Loss: 0.00065203
Iteration 17/25 | Loss: 0.00065203
Iteration 18/25 | Loss: 0.00065203
Iteration 19/25 | Loss: 0.00065203
Iteration 20/25 | Loss: 0.00065203
Iteration 21/25 | Loss: 0.00065203
Iteration 22/25 | Loss: 0.00065203
Iteration 23/25 | Loss: 0.00065202
Iteration 24/25 | Loss: 0.00065202
Iteration 25/25 | Loss: 0.00065202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96931660
Iteration 2/25 | Loss: 0.00057386
Iteration 3/25 | Loss: 0.00057386
Iteration 4/25 | Loss: 0.00057385
Iteration 5/25 | Loss: 0.00057385
Iteration 6/25 | Loss: 0.00057385
Iteration 7/25 | Loss: 0.00057385
Iteration 8/25 | Loss: 0.00057385
Iteration 9/25 | Loss: 0.00057385
Iteration 10/25 | Loss: 0.00057385
Iteration 11/25 | Loss: 0.00057385
Iteration 12/25 | Loss: 0.00057385
Iteration 13/25 | Loss: 0.00057385
Iteration 14/25 | Loss: 0.00057385
Iteration 15/25 | Loss: 0.00057385
Iteration 16/25 | Loss: 0.00057385
Iteration 17/25 | Loss: 0.00057385
Iteration 18/25 | Loss: 0.00057385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005738526815548539, 0.0005738526815548539, 0.0005738526815548539, 0.0005738526815548539, 0.0005738526815548539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005738526815548539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057385
Iteration 2/1000 | Loss: 0.00007318
Iteration 3/1000 | Loss: 0.00005090
Iteration 4/1000 | Loss: 0.00004228
Iteration 5/1000 | Loss: 0.00005550
Iteration 6/1000 | Loss: 0.00003514
Iteration 7/1000 | Loss: 0.00005116
Iteration 8/1000 | Loss: 0.00005632
Iteration 9/1000 | Loss: 0.00003384
Iteration 10/1000 | Loss: 0.00002943
Iteration 11/1000 | Loss: 0.00002816
Iteration 12/1000 | Loss: 0.00002778
Iteration 13/1000 | Loss: 0.00003325
Iteration 14/1000 | Loss: 0.00002725
Iteration 15/1000 | Loss: 0.00006268
Iteration 16/1000 | Loss: 0.00003084
Iteration 17/1000 | Loss: 0.00003941
Iteration 18/1000 | Loss: 0.00002661
Iteration 19/1000 | Loss: 0.00002655
Iteration 20/1000 | Loss: 0.00003996
Iteration 21/1000 | Loss: 0.00002626
Iteration 22/1000 | Loss: 0.00002610
Iteration 23/1000 | Loss: 0.00002592
Iteration 24/1000 | Loss: 0.00002578
Iteration 25/1000 | Loss: 0.00002577
Iteration 26/1000 | Loss: 0.00002574
Iteration 27/1000 | Loss: 0.00002573
Iteration 28/1000 | Loss: 0.00005302
Iteration 29/1000 | Loss: 0.00002556
Iteration 30/1000 | Loss: 0.00002555
Iteration 31/1000 | Loss: 0.00002554
Iteration 32/1000 | Loss: 0.00002553
Iteration 33/1000 | Loss: 0.00002553
Iteration 34/1000 | Loss: 0.00002553
Iteration 35/1000 | Loss: 0.00002552
Iteration 36/1000 | Loss: 0.00002552
Iteration 37/1000 | Loss: 0.00002552
Iteration 38/1000 | Loss: 0.00002552
Iteration 39/1000 | Loss: 0.00002552
Iteration 40/1000 | Loss: 0.00002552
Iteration 41/1000 | Loss: 0.00002552
Iteration 42/1000 | Loss: 0.00002551
Iteration 43/1000 | Loss: 0.00002551
Iteration 44/1000 | Loss: 0.00002548
Iteration 45/1000 | Loss: 0.00002547
Iteration 46/1000 | Loss: 0.00002547
Iteration 47/1000 | Loss: 0.00002546
Iteration 48/1000 | Loss: 0.00002546
Iteration 49/1000 | Loss: 0.00002545
Iteration 50/1000 | Loss: 0.00002545
Iteration 51/1000 | Loss: 0.00002545
Iteration 52/1000 | Loss: 0.00002545
Iteration 53/1000 | Loss: 0.00002545
Iteration 54/1000 | Loss: 0.00002545
Iteration 55/1000 | Loss: 0.00002545
Iteration 56/1000 | Loss: 0.00002545
Iteration 57/1000 | Loss: 0.00002544
Iteration 58/1000 | Loss: 0.00002544
Iteration 59/1000 | Loss: 0.00002544
Iteration 60/1000 | Loss: 0.00002543
Iteration 61/1000 | Loss: 0.00002543
Iteration 62/1000 | Loss: 0.00002542
Iteration 63/1000 | Loss: 0.00002542
Iteration 64/1000 | Loss: 0.00002541
Iteration 65/1000 | Loss: 0.00002541
Iteration 66/1000 | Loss: 0.00002541
Iteration 67/1000 | Loss: 0.00002541
Iteration 68/1000 | Loss: 0.00002541
Iteration 69/1000 | Loss: 0.00002540
Iteration 70/1000 | Loss: 0.00004158
Iteration 71/1000 | Loss: 0.00002540
Iteration 72/1000 | Loss: 0.00002539
Iteration 73/1000 | Loss: 0.00002539
Iteration 74/1000 | Loss: 0.00002539
Iteration 75/1000 | Loss: 0.00002538
Iteration 76/1000 | Loss: 0.00002538
Iteration 77/1000 | Loss: 0.00002538
Iteration 78/1000 | Loss: 0.00002538
Iteration 79/1000 | Loss: 0.00002537
Iteration 80/1000 | Loss: 0.00002537
Iteration 81/1000 | Loss: 0.00002537
Iteration 82/1000 | Loss: 0.00002537
Iteration 83/1000 | Loss: 0.00002537
Iteration 84/1000 | Loss: 0.00002537
Iteration 85/1000 | Loss: 0.00002537
Iteration 86/1000 | Loss: 0.00002536
Iteration 87/1000 | Loss: 0.00002536
Iteration 88/1000 | Loss: 0.00002536
Iteration 89/1000 | Loss: 0.00002535
Iteration 90/1000 | Loss: 0.00002535
Iteration 91/1000 | Loss: 0.00002535
Iteration 92/1000 | Loss: 0.00002534
Iteration 93/1000 | Loss: 0.00002534
Iteration 94/1000 | Loss: 0.00002534
Iteration 95/1000 | Loss: 0.00002533
Iteration 96/1000 | Loss: 0.00002533
Iteration 97/1000 | Loss: 0.00002533
Iteration 98/1000 | Loss: 0.00002532
Iteration 99/1000 | Loss: 0.00002532
Iteration 100/1000 | Loss: 0.00002532
Iteration 101/1000 | Loss: 0.00002531
Iteration 102/1000 | Loss: 0.00002531
Iteration 103/1000 | Loss: 0.00002531
Iteration 104/1000 | Loss: 0.00002531
Iteration 105/1000 | Loss: 0.00002531
Iteration 106/1000 | Loss: 0.00002531
Iteration 107/1000 | Loss: 0.00002531
Iteration 108/1000 | Loss: 0.00002530
Iteration 109/1000 | Loss: 0.00002530
Iteration 110/1000 | Loss: 0.00002530
Iteration 111/1000 | Loss: 0.00002530
Iteration 112/1000 | Loss: 0.00002529
Iteration 113/1000 | Loss: 0.00002529
Iteration 114/1000 | Loss: 0.00002529
Iteration 115/1000 | Loss: 0.00002528
Iteration 116/1000 | Loss: 0.00002528
Iteration 117/1000 | Loss: 0.00002528
Iteration 118/1000 | Loss: 0.00002528
Iteration 119/1000 | Loss: 0.00002528
Iteration 120/1000 | Loss: 0.00002528
Iteration 121/1000 | Loss: 0.00002528
Iteration 122/1000 | Loss: 0.00002527
Iteration 123/1000 | Loss: 0.00002527
Iteration 124/1000 | Loss: 0.00002527
Iteration 125/1000 | Loss: 0.00006028
Iteration 126/1000 | Loss: 0.00002558
Iteration 127/1000 | Loss: 0.00002526
Iteration 128/1000 | Loss: 0.00002526
Iteration 129/1000 | Loss: 0.00002525
Iteration 130/1000 | Loss: 0.00002525
Iteration 131/1000 | Loss: 0.00002525
Iteration 132/1000 | Loss: 0.00002524
Iteration 133/1000 | Loss: 0.00002524
Iteration 134/1000 | Loss: 0.00002524
Iteration 135/1000 | Loss: 0.00002523
Iteration 136/1000 | Loss: 0.00002523
Iteration 137/1000 | Loss: 0.00002523
Iteration 138/1000 | Loss: 0.00051628
Iteration 139/1000 | Loss: 0.00121406
Iteration 140/1000 | Loss: 0.00003564
Iteration 141/1000 | Loss: 0.00002618
Iteration 142/1000 | Loss: 0.00002291
Iteration 143/1000 | Loss: 0.00002089
Iteration 144/1000 | Loss: 0.00003555
Iteration 145/1000 | Loss: 0.00001842
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001710
Iteration 149/1000 | Loss: 0.00004061
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001652
Iteration 152/1000 | Loss: 0.00001651
Iteration 153/1000 | Loss: 0.00004326
Iteration 154/1000 | Loss: 0.00001648
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001635
Iteration 158/1000 | Loss: 0.00001635
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001635
Iteration 161/1000 | Loss: 0.00001635
Iteration 162/1000 | Loss: 0.00001635
Iteration 163/1000 | Loss: 0.00001635
Iteration 164/1000 | Loss: 0.00001635
Iteration 165/1000 | Loss: 0.00001635
Iteration 166/1000 | Loss: 0.00001635
Iteration 167/1000 | Loss: 0.00001635
Iteration 168/1000 | Loss: 0.00001635
Iteration 169/1000 | Loss: 0.00001635
Iteration 170/1000 | Loss: 0.00001634
Iteration 171/1000 | Loss: 0.00001633
Iteration 172/1000 | Loss: 0.00001633
Iteration 173/1000 | Loss: 0.00001633
Iteration 174/1000 | Loss: 0.00001633
Iteration 175/1000 | Loss: 0.00001632
Iteration 176/1000 | Loss: 0.00001632
Iteration 177/1000 | Loss: 0.00001632
Iteration 178/1000 | Loss: 0.00001632
Iteration 179/1000 | Loss: 0.00001632
Iteration 180/1000 | Loss: 0.00001632
Iteration 181/1000 | Loss: 0.00001632
Iteration 182/1000 | Loss: 0.00001631
Iteration 183/1000 | Loss: 0.00001631
Iteration 184/1000 | Loss: 0.00001631
Iteration 185/1000 | Loss: 0.00001631
Iteration 186/1000 | Loss: 0.00001630
Iteration 187/1000 | Loss: 0.00001630
Iteration 188/1000 | Loss: 0.00001630
Iteration 189/1000 | Loss: 0.00001630
Iteration 190/1000 | Loss: 0.00001630
Iteration 191/1000 | Loss: 0.00001630
Iteration 192/1000 | Loss: 0.00001630
Iteration 193/1000 | Loss: 0.00001629
Iteration 194/1000 | Loss: 0.00001629
Iteration 195/1000 | Loss: 0.00001629
Iteration 196/1000 | Loss: 0.00001628
Iteration 197/1000 | Loss: 0.00001627
Iteration 198/1000 | Loss: 0.00004300
Iteration 199/1000 | Loss: 0.00001624
Iteration 200/1000 | Loss: 0.00001622
Iteration 201/1000 | Loss: 0.00001621
Iteration 202/1000 | Loss: 0.00001621
Iteration 203/1000 | Loss: 0.00001621
Iteration 204/1000 | Loss: 0.00002428
Iteration 205/1000 | Loss: 0.00001622
Iteration 206/1000 | Loss: 0.00001621
Iteration 207/1000 | Loss: 0.00001621
Iteration 208/1000 | Loss: 0.00001621
Iteration 209/1000 | Loss: 0.00001620
Iteration 210/1000 | Loss: 0.00001620
Iteration 211/1000 | Loss: 0.00001620
Iteration 212/1000 | Loss: 0.00001620
Iteration 213/1000 | Loss: 0.00001620
Iteration 214/1000 | Loss: 0.00001620
Iteration 215/1000 | Loss: 0.00001620
Iteration 216/1000 | Loss: 0.00001619
Iteration 217/1000 | Loss: 0.00001619
Iteration 218/1000 | Loss: 0.00001619
Iteration 219/1000 | Loss: 0.00001619
Iteration 220/1000 | Loss: 0.00001619
Iteration 221/1000 | Loss: 0.00001619
Iteration 222/1000 | Loss: 0.00001619
Iteration 223/1000 | Loss: 0.00001619
Iteration 224/1000 | Loss: 0.00001619
Iteration 225/1000 | Loss: 0.00001619
Iteration 226/1000 | Loss: 0.00001619
Iteration 227/1000 | Loss: 0.00001619
Iteration 228/1000 | Loss: 0.00001619
Iteration 229/1000 | Loss: 0.00001619
Iteration 230/1000 | Loss: 0.00001619
Iteration 231/1000 | Loss: 0.00001619
Iteration 232/1000 | Loss: 0.00001618
Iteration 233/1000 | Loss: 0.00001618
Iteration 234/1000 | Loss: 0.00001618
Iteration 235/1000 | Loss: 0.00001618
Iteration 236/1000 | Loss: 0.00001618
Iteration 237/1000 | Loss: 0.00001618
Iteration 238/1000 | Loss: 0.00001618
Iteration 239/1000 | Loss: 0.00001618
Iteration 240/1000 | Loss: 0.00001618
Iteration 241/1000 | Loss: 0.00001618
Iteration 242/1000 | Loss: 0.00001617
Iteration 243/1000 | Loss: 0.00001617
Iteration 244/1000 | Loss: 0.00001617
Iteration 245/1000 | Loss: 0.00001617
Iteration 246/1000 | Loss: 0.00001617
Iteration 247/1000 | Loss: 0.00001617
Iteration 248/1000 | Loss: 0.00001617
Iteration 249/1000 | Loss: 0.00001617
Iteration 250/1000 | Loss: 0.00001617
Iteration 251/1000 | Loss: 0.00001617
Iteration 252/1000 | Loss: 0.00001617
Iteration 253/1000 | Loss: 0.00001617
Iteration 254/1000 | Loss: 0.00001617
Iteration 255/1000 | Loss: 0.00001617
Iteration 256/1000 | Loss: 0.00001617
Iteration 257/1000 | Loss: 0.00001616
Iteration 258/1000 | Loss: 0.00001616
Iteration 259/1000 | Loss: 0.00001616
Iteration 260/1000 | Loss: 0.00001616
Iteration 261/1000 | Loss: 0.00001616
Iteration 262/1000 | Loss: 0.00001616
Iteration 263/1000 | Loss: 0.00001616
Iteration 264/1000 | Loss: 0.00001616
Iteration 265/1000 | Loss: 0.00001616
Iteration 266/1000 | Loss: 0.00001616
Iteration 267/1000 | Loss: 0.00001616
Iteration 268/1000 | Loss: 0.00001616
Iteration 269/1000 | Loss: 0.00001615
Iteration 270/1000 | Loss: 0.00001615
Iteration 271/1000 | Loss: 0.00001615
Iteration 272/1000 | Loss: 0.00001615
Iteration 273/1000 | Loss: 0.00001615
Iteration 274/1000 | Loss: 0.00001615
Iteration 275/1000 | Loss: 0.00001615
Iteration 276/1000 | Loss: 0.00001615
Iteration 277/1000 | Loss: 0.00001615
Iteration 278/1000 | Loss: 0.00001615
Iteration 279/1000 | Loss: 0.00001615
Iteration 280/1000 | Loss: 0.00001615
Iteration 281/1000 | Loss: 0.00001615
Iteration 282/1000 | Loss: 0.00001615
Iteration 283/1000 | Loss: 0.00001615
Iteration 284/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.6153568139998242e-05, 1.6153568139998242e-05, 1.6153568139998242e-05, 1.6153568139998242e-05, 1.6153568139998242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6153568139998242e-05

Optimization complete. Final v2v error: 3.3260321617126465 mm

Highest mean error: 4.205211162567139 mm for frame 175

Lowest mean error: 2.72519588470459 mm for frame 139

Saving results

Total time: 116.52865314483643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042749
Iteration 2/25 | Loss: 0.00300449
Iteration 3/25 | Loss: 0.00194726
Iteration 4/25 | Loss: 0.00167588
Iteration 5/25 | Loss: 0.00133562
Iteration 6/25 | Loss: 0.00115590
Iteration 7/25 | Loss: 0.00089354
Iteration 8/25 | Loss: 0.00081788
Iteration 9/25 | Loss: 0.00080375
Iteration 10/25 | Loss: 0.00074996
Iteration 11/25 | Loss: 0.00074241
Iteration 12/25 | Loss: 0.00074017
Iteration 13/25 | Loss: 0.00073782
Iteration 14/25 | Loss: 0.00073519
Iteration 15/25 | Loss: 0.00073853
Iteration 16/25 | Loss: 0.00073182
Iteration 17/25 | Loss: 0.00072968
Iteration 18/25 | Loss: 0.00072903
Iteration 19/25 | Loss: 0.00072897
Iteration 20/25 | Loss: 0.00072897
Iteration 21/25 | Loss: 0.00072897
Iteration 22/25 | Loss: 0.00072897
Iteration 23/25 | Loss: 0.00072897
Iteration 24/25 | Loss: 0.00072897
Iteration 25/25 | Loss: 0.00072897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47576630
Iteration 2/25 | Loss: 0.00052028
Iteration 3/25 | Loss: 0.00052028
Iteration 4/25 | Loss: 0.00052028
Iteration 5/25 | Loss: 0.00052028
Iteration 6/25 | Loss: 0.00052028
Iteration 7/25 | Loss: 0.00052028
Iteration 8/25 | Loss: 0.00052028
Iteration 9/25 | Loss: 0.00052028
Iteration 10/25 | Loss: 0.00052028
Iteration 11/25 | Loss: 0.00052028
Iteration 12/25 | Loss: 0.00052028
Iteration 13/25 | Loss: 0.00052028
Iteration 14/25 | Loss: 0.00052028
Iteration 15/25 | Loss: 0.00052028
Iteration 16/25 | Loss: 0.00052028
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005202808533795178, 0.0005202808533795178, 0.0005202808533795178, 0.0005202808533795178, 0.0005202808533795178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005202808533795178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052028
Iteration 2/1000 | Loss: 0.00006500
Iteration 3/1000 | Loss: 0.00032975
Iteration 4/1000 | Loss: 0.00023975
Iteration 5/1000 | Loss: 0.00011131
Iteration 6/1000 | Loss: 0.00013510
Iteration 7/1000 | Loss: 0.00017455
Iteration 8/1000 | Loss: 0.00004499
Iteration 9/1000 | Loss: 0.00003944
Iteration 10/1000 | Loss: 0.00003478
Iteration 11/1000 | Loss: 0.00003225
Iteration 12/1000 | Loss: 0.00003054
Iteration 13/1000 | Loss: 0.00002972
Iteration 14/1000 | Loss: 0.00089572
Iteration 15/1000 | Loss: 0.00063056
Iteration 16/1000 | Loss: 0.00004378
Iteration 17/1000 | Loss: 0.00003237
Iteration 18/1000 | Loss: 0.00002607
Iteration 19/1000 | Loss: 0.00002229
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001637
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001614
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001610
Iteration 33/1000 | Loss: 0.00001610
Iteration 34/1000 | Loss: 0.00001610
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001607
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001605
Iteration 48/1000 | Loss: 0.00001602
Iteration 49/1000 | Loss: 0.00001602
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001602
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001601
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001599
Iteration 65/1000 | Loss: 0.00001599
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001598
Iteration 70/1000 | Loss: 0.00001598
Iteration 71/1000 | Loss: 0.00001598
Iteration 72/1000 | Loss: 0.00001598
Iteration 73/1000 | Loss: 0.00001597
Iteration 74/1000 | Loss: 0.00001597
Iteration 75/1000 | Loss: 0.00001597
Iteration 76/1000 | Loss: 0.00001597
Iteration 77/1000 | Loss: 0.00001597
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001597
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001596
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001596
Iteration 88/1000 | Loss: 0.00001596
Iteration 89/1000 | Loss: 0.00001596
Iteration 90/1000 | Loss: 0.00001596
Iteration 91/1000 | Loss: 0.00001596
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001596
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001595
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001594
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001594
Iteration 120/1000 | Loss: 0.00001594
Iteration 121/1000 | Loss: 0.00001594
Iteration 122/1000 | Loss: 0.00001594
Iteration 123/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.5938523574732244e-05, 1.5938523574732244e-05, 1.5938523574732244e-05, 1.5938523574732244e-05, 1.5938523574732244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5938523574732244e-05

Optimization complete. Final v2v error: 3.3059253692626953 mm

Highest mean error: 3.8886218070983887 mm for frame 156

Lowest mean error: 3.047149658203125 mm for frame 3

Saving results

Total time: 76.75800156593323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370378
Iteration 2/25 | Loss: 0.00067433
Iteration 3/25 | Loss: 0.00058827
Iteration 4/25 | Loss: 0.00056610
Iteration 5/25 | Loss: 0.00056268
Iteration 6/25 | Loss: 0.00056230
Iteration 7/25 | Loss: 0.00056230
Iteration 8/25 | Loss: 0.00056230
Iteration 9/25 | Loss: 0.00056230
Iteration 10/25 | Loss: 0.00056230
Iteration 11/25 | Loss: 0.00056230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005622992175631225, 0.0005622992175631225, 0.0005622992175631225, 0.0005622992175631225, 0.0005622992175631225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005622992175631225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00679207
Iteration 2/25 | Loss: 0.00021971
Iteration 3/25 | Loss: 0.00021971
Iteration 4/25 | Loss: 0.00021971
Iteration 5/25 | Loss: 0.00021971
Iteration 6/25 | Loss: 0.00021971
Iteration 7/25 | Loss: 0.00021971
Iteration 8/25 | Loss: 0.00021971
Iteration 9/25 | Loss: 0.00021971
Iteration 10/25 | Loss: 0.00021971
Iteration 11/25 | Loss: 0.00021971
Iteration 12/25 | Loss: 0.00021971
Iteration 13/25 | Loss: 0.00021971
Iteration 14/25 | Loss: 0.00021971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00021971127716824412, 0.00021971127716824412, 0.00021971127716824412, 0.00021971127716824412, 0.00021971127716824412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021971127716824412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021971
Iteration 2/1000 | Loss: 0.00001825
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001329
Iteration 5/1000 | Loss: 0.00001275
Iteration 6/1000 | Loss: 0.00001244
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001177
Iteration 17/1000 | Loss: 0.00001177
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001165
Iteration 32/1000 | Loss: 0.00001165
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001158
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001150
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001149
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001146
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001143
Iteration 98/1000 | Loss: 0.00001143
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001141
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001140
Iteration 117/1000 | Loss: 0.00001140
Iteration 118/1000 | Loss: 0.00001140
Iteration 119/1000 | Loss: 0.00001140
Iteration 120/1000 | Loss: 0.00001140
Iteration 121/1000 | Loss: 0.00001140
Iteration 122/1000 | Loss: 0.00001140
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001138
Iteration 134/1000 | Loss: 0.00001138
Iteration 135/1000 | Loss: 0.00001138
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001138
Iteration 138/1000 | Loss: 0.00001138
Iteration 139/1000 | Loss: 0.00001138
Iteration 140/1000 | Loss: 0.00001138
Iteration 141/1000 | Loss: 0.00001138
Iteration 142/1000 | Loss: 0.00001138
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Iteration 164/1000 | Loss: 0.00001137
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001137
Iteration 172/1000 | Loss: 0.00001137
Iteration 173/1000 | Loss: 0.00001137
Iteration 174/1000 | Loss: 0.00001137
Iteration 175/1000 | Loss: 0.00001137
Iteration 176/1000 | Loss: 0.00001137
Iteration 177/1000 | Loss: 0.00001137
Iteration 178/1000 | Loss: 0.00001137
Iteration 179/1000 | Loss: 0.00001137
Iteration 180/1000 | Loss: 0.00001137
Iteration 181/1000 | Loss: 0.00001137
Iteration 182/1000 | Loss: 0.00001137
Iteration 183/1000 | Loss: 0.00001137
Iteration 184/1000 | Loss: 0.00001137
Iteration 185/1000 | Loss: 0.00001137
Iteration 186/1000 | Loss: 0.00001137
Iteration 187/1000 | Loss: 0.00001137
Iteration 188/1000 | Loss: 0.00001137
Iteration 189/1000 | Loss: 0.00001137
Iteration 190/1000 | Loss: 0.00001137
Iteration 191/1000 | Loss: 0.00001137
Iteration 192/1000 | Loss: 0.00001137
Iteration 193/1000 | Loss: 0.00001137
Iteration 194/1000 | Loss: 0.00001137
Iteration 195/1000 | Loss: 0.00001137
Iteration 196/1000 | Loss: 0.00001137
Iteration 197/1000 | Loss: 0.00001137
Iteration 198/1000 | Loss: 0.00001137
Iteration 199/1000 | Loss: 0.00001137
Iteration 200/1000 | Loss: 0.00001137
Iteration 201/1000 | Loss: 0.00001137
Iteration 202/1000 | Loss: 0.00001137
Iteration 203/1000 | Loss: 0.00001137
Iteration 204/1000 | Loss: 0.00001137
Iteration 205/1000 | Loss: 0.00001137
Iteration 206/1000 | Loss: 0.00001137
Iteration 207/1000 | Loss: 0.00001137
Iteration 208/1000 | Loss: 0.00001137
Iteration 209/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.1371559594408609e-05, 1.1371559594408609e-05, 1.1371559594408609e-05, 1.1371559594408609e-05, 1.1371559594408609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1371559594408609e-05

Optimization complete. Final v2v error: 2.8489034175872803 mm

Highest mean error: 2.9889309406280518 mm for frame 114

Lowest mean error: 2.695469379425049 mm for frame 46

Saving results

Total time: 34.93290901184082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409518
Iteration 2/25 | Loss: 0.00069140
Iteration 3/25 | Loss: 0.00057357
Iteration 4/25 | Loss: 0.00055912
Iteration 5/25 | Loss: 0.00055463
Iteration 6/25 | Loss: 0.00055362
Iteration 7/25 | Loss: 0.00055362
Iteration 8/25 | Loss: 0.00055362
Iteration 9/25 | Loss: 0.00055362
Iteration 10/25 | Loss: 0.00055362
Iteration 11/25 | Loss: 0.00055362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005536199314519763, 0.0005536199314519763, 0.0005536199314519763, 0.0005536199314519763, 0.0005536199314519763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005536199314519763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13795328
Iteration 2/25 | Loss: 0.00020086
Iteration 3/25 | Loss: 0.00020086
Iteration 4/25 | Loss: 0.00020086
Iteration 5/25 | Loss: 0.00020086
Iteration 6/25 | Loss: 0.00020086
Iteration 7/25 | Loss: 0.00020086
Iteration 8/25 | Loss: 0.00020086
Iteration 9/25 | Loss: 0.00020086
Iteration 10/25 | Loss: 0.00020086
Iteration 11/25 | Loss: 0.00020086
Iteration 12/25 | Loss: 0.00020086
Iteration 13/25 | Loss: 0.00020086
Iteration 14/25 | Loss: 0.00020086
Iteration 15/25 | Loss: 0.00020086
Iteration 16/25 | Loss: 0.00020086
Iteration 17/25 | Loss: 0.00020086
Iteration 18/25 | Loss: 0.00020086
Iteration 19/25 | Loss: 0.00020086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00020086122094653547, 0.00020086122094653547, 0.00020086122094653547, 0.00020086122094653547, 0.00020086122094653547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020086122094653547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020086
Iteration 2/1000 | Loss: 0.00001904
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001351
Iteration 5/1000 | Loss: 0.00001270
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001160
Iteration 16/1000 | Loss: 0.00001160
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001148
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001136
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001135
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001133
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001133
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001132
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001126
Iteration 73/1000 | Loss: 0.00001126
Iteration 74/1000 | Loss: 0.00001126
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001122
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001117
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001115
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001114
Iteration 125/1000 | Loss: 0.00001114
Iteration 126/1000 | Loss: 0.00001114
Iteration 127/1000 | Loss: 0.00001114
Iteration 128/1000 | Loss: 0.00001114
Iteration 129/1000 | Loss: 0.00001114
Iteration 130/1000 | Loss: 0.00001114
Iteration 131/1000 | Loss: 0.00001114
Iteration 132/1000 | Loss: 0.00001114
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001113
Iteration 137/1000 | Loss: 0.00001113
Iteration 138/1000 | Loss: 0.00001113
Iteration 139/1000 | Loss: 0.00001113
Iteration 140/1000 | Loss: 0.00001113
Iteration 141/1000 | Loss: 0.00001113
Iteration 142/1000 | Loss: 0.00001113
Iteration 143/1000 | Loss: 0.00001113
Iteration 144/1000 | Loss: 0.00001113
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001113
Iteration 148/1000 | Loss: 0.00001113
Iteration 149/1000 | Loss: 0.00001113
Iteration 150/1000 | Loss: 0.00001113
Iteration 151/1000 | Loss: 0.00001112
Iteration 152/1000 | Loss: 0.00001112
Iteration 153/1000 | Loss: 0.00001112
Iteration 154/1000 | Loss: 0.00001112
Iteration 155/1000 | Loss: 0.00001112
Iteration 156/1000 | Loss: 0.00001112
Iteration 157/1000 | Loss: 0.00001112
Iteration 158/1000 | Loss: 0.00001112
Iteration 159/1000 | Loss: 0.00001112
Iteration 160/1000 | Loss: 0.00001112
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001112
Iteration 163/1000 | Loss: 0.00001112
Iteration 164/1000 | Loss: 0.00001112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1121714123873971e-05, 1.1121714123873971e-05, 1.1121714123873971e-05, 1.1121714123873971e-05, 1.1121714123873971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1121714123873971e-05

Optimization complete. Final v2v error: 2.8661630153656006 mm

Highest mean error: 3.080836296081543 mm for frame 88

Lowest mean error: 2.7211861610412598 mm for frame 183

Saving results

Total time: 36.130455017089844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778075
Iteration 2/25 | Loss: 0.00077507
Iteration 3/25 | Loss: 0.00065188
Iteration 4/25 | Loss: 0.00061181
Iteration 5/25 | Loss: 0.00060046
Iteration 6/25 | Loss: 0.00059801
Iteration 7/25 | Loss: 0.00059758
Iteration 8/25 | Loss: 0.00059758
Iteration 9/25 | Loss: 0.00059758
Iteration 10/25 | Loss: 0.00059758
Iteration 11/25 | Loss: 0.00059758
Iteration 12/25 | Loss: 0.00059758
Iteration 13/25 | Loss: 0.00059758
Iteration 14/25 | Loss: 0.00059758
Iteration 15/25 | Loss: 0.00059758
Iteration 16/25 | Loss: 0.00059758
Iteration 17/25 | Loss: 0.00059758
Iteration 18/25 | Loss: 0.00059758
Iteration 19/25 | Loss: 0.00059758
Iteration 20/25 | Loss: 0.00059758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005975833628326654, 0.0005975833628326654, 0.0005975833628326654, 0.0005975833628326654, 0.0005975833628326654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005975833628326654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32470942
Iteration 2/25 | Loss: 0.00020783
Iteration 3/25 | Loss: 0.00020776
Iteration 4/25 | Loss: 0.00020776
Iteration 5/25 | Loss: 0.00020776
Iteration 6/25 | Loss: 0.00020776
Iteration 7/25 | Loss: 0.00020776
Iteration 8/25 | Loss: 0.00020776
Iteration 9/25 | Loss: 0.00020776
Iteration 10/25 | Loss: 0.00020776
Iteration 11/25 | Loss: 0.00020776
Iteration 12/25 | Loss: 0.00020776
Iteration 13/25 | Loss: 0.00020776
Iteration 14/25 | Loss: 0.00020776
Iteration 15/25 | Loss: 0.00020776
Iteration 16/25 | Loss: 0.00020776
Iteration 17/25 | Loss: 0.00020776
Iteration 18/25 | Loss: 0.00020776
Iteration 19/25 | Loss: 0.00020776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00020776230667252094, 0.00020776230667252094, 0.00020776230667252094, 0.00020776230667252094, 0.00020776230667252094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020776230667252094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020776
Iteration 2/1000 | Loss: 0.00002908
Iteration 3/1000 | Loss: 0.00001986
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001688
Iteration 6/1000 | Loss: 0.00001634
Iteration 7/1000 | Loss: 0.00001576
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001533
Iteration 10/1000 | Loss: 0.00001519
Iteration 11/1000 | Loss: 0.00001517
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001511
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001499
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001496
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001495
Iteration 22/1000 | Loss: 0.00001495
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001493
Iteration 25/1000 | Loss: 0.00001493
Iteration 26/1000 | Loss: 0.00001492
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001490
Iteration 29/1000 | Loss: 0.00001489
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001484
Iteration 47/1000 | Loss: 0.00001484
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001483
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001482
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001480
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001480
Iteration 83/1000 | Loss: 0.00001480
Iteration 84/1000 | Loss: 0.00001480
Iteration 85/1000 | Loss: 0.00001480
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.4795559764024802e-05, 1.4795559764024802e-05, 1.4795559764024802e-05, 1.4795559764024802e-05, 1.4795559764024802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4795559764024802e-05

Optimization complete. Final v2v error: 3.251197576522827 mm

Highest mean error: 4.324233055114746 mm for frame 5

Lowest mean error: 2.9094231128692627 mm for frame 101

Saving results

Total time: 34.100661277770996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411077
Iteration 2/25 | Loss: 0.00090129
Iteration 3/25 | Loss: 0.00059646
Iteration 4/25 | Loss: 0.00057330
Iteration 5/25 | Loss: 0.00056431
Iteration 6/25 | Loss: 0.00056190
Iteration 7/25 | Loss: 0.00056131
Iteration 8/25 | Loss: 0.00056123
Iteration 9/25 | Loss: 0.00056123
Iteration 10/25 | Loss: 0.00056123
Iteration 11/25 | Loss: 0.00056123
Iteration 12/25 | Loss: 0.00056123
Iteration 13/25 | Loss: 0.00056123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005612251115962863, 0.0005612251115962863, 0.0005612251115962863, 0.0005612251115962863, 0.0005612251115962863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005612251115962863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54816091
Iteration 2/25 | Loss: 0.00022634
Iteration 3/25 | Loss: 0.00022634
Iteration 4/25 | Loss: 0.00022634
Iteration 5/25 | Loss: 0.00022634
Iteration 6/25 | Loss: 0.00022634
Iteration 7/25 | Loss: 0.00022634
Iteration 8/25 | Loss: 0.00022633
Iteration 9/25 | Loss: 0.00022633
Iteration 10/25 | Loss: 0.00022633
Iteration 11/25 | Loss: 0.00022633
Iteration 12/25 | Loss: 0.00022633
Iteration 13/25 | Loss: 0.00022633
Iteration 14/25 | Loss: 0.00022633
Iteration 15/25 | Loss: 0.00022633
Iteration 16/25 | Loss: 0.00022633
Iteration 17/25 | Loss: 0.00022633
Iteration 18/25 | Loss: 0.00022633
Iteration 19/25 | Loss: 0.00022633
Iteration 20/25 | Loss: 0.00022633
Iteration 21/25 | Loss: 0.00022633
Iteration 22/25 | Loss: 0.00022633
Iteration 23/25 | Loss: 0.00022633
Iteration 24/25 | Loss: 0.00022633
Iteration 25/25 | Loss: 0.00022633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022633
Iteration 2/1000 | Loss: 0.00002284
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001470
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001371
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001360
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001347
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001347
Iteration 38/1000 | Loss: 0.00001347
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001346
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001332
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001331
Iteration 68/1000 | Loss: 0.00001331
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001323
Iteration 87/1000 | Loss: 0.00001323
Iteration 88/1000 | Loss: 0.00001323
Iteration 89/1000 | Loss: 0.00001323
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001322
Iteration 94/1000 | Loss: 0.00001322
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001321
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001321
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001316
Iteration 119/1000 | Loss: 0.00001316
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001315
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001315
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001315
Iteration 130/1000 | Loss: 0.00001315
Iteration 131/1000 | Loss: 0.00001314
Iteration 132/1000 | Loss: 0.00001314
Iteration 133/1000 | Loss: 0.00001314
Iteration 134/1000 | Loss: 0.00001314
Iteration 135/1000 | Loss: 0.00001314
Iteration 136/1000 | Loss: 0.00001314
Iteration 137/1000 | Loss: 0.00001314
Iteration 138/1000 | Loss: 0.00001314
Iteration 139/1000 | Loss: 0.00001314
Iteration 140/1000 | Loss: 0.00001314
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001314
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Iteration 162/1000 | Loss: 0.00001314
Iteration 163/1000 | Loss: 0.00001314
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001314
Iteration 169/1000 | Loss: 0.00001314
Iteration 170/1000 | Loss: 0.00001314
Iteration 171/1000 | Loss: 0.00001314
Iteration 172/1000 | Loss: 0.00001314
Iteration 173/1000 | Loss: 0.00001314
Iteration 174/1000 | Loss: 0.00001314
Iteration 175/1000 | Loss: 0.00001314
Iteration 176/1000 | Loss: 0.00001314
Iteration 177/1000 | Loss: 0.00001314
Iteration 178/1000 | Loss: 0.00001314
Iteration 179/1000 | Loss: 0.00001314
Iteration 180/1000 | Loss: 0.00001314
Iteration 181/1000 | Loss: 0.00001314
Iteration 182/1000 | Loss: 0.00001314
Iteration 183/1000 | Loss: 0.00001314
Iteration 184/1000 | Loss: 0.00001314
Iteration 185/1000 | Loss: 0.00001314
Iteration 186/1000 | Loss: 0.00001314
Iteration 187/1000 | Loss: 0.00001314
Iteration 188/1000 | Loss: 0.00001314
Iteration 189/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3136291272530798e-05, 1.3136291272530798e-05, 1.3136291272530798e-05, 1.3136291272530798e-05, 1.3136291272530798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3136291272530798e-05

Optimization complete. Final v2v error: 3.00661301612854 mm

Highest mean error: 3.7200846672058105 mm for frame 34

Lowest mean error: 2.4661600589752197 mm for frame 145

Saving results

Total time: 36.86080884933472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742729
Iteration 2/25 | Loss: 0.00168803
Iteration 3/25 | Loss: 0.00101649
Iteration 4/25 | Loss: 0.00087260
Iteration 5/25 | Loss: 0.00082677
Iteration 6/25 | Loss: 0.00080887
Iteration 7/25 | Loss: 0.00080308
Iteration 8/25 | Loss: 0.00080334
Iteration 9/25 | Loss: 0.00079715
Iteration 10/25 | Loss: 0.00079612
Iteration 11/25 | Loss: 0.00078802
Iteration 12/25 | Loss: 0.00079245
Iteration 13/25 | Loss: 0.00078812
Iteration 14/25 | Loss: 0.00079156
Iteration 15/25 | Loss: 0.00078920
Iteration 16/25 | Loss: 0.00078998
Iteration 17/25 | Loss: 0.00079072
Iteration 18/25 | Loss: 0.00079154
Iteration 19/25 | Loss: 0.00078645
Iteration 20/25 | Loss: 0.00078862
Iteration 21/25 | Loss: 0.00078806
Iteration 22/25 | Loss: 0.00078007
Iteration 23/25 | Loss: 0.00078935
Iteration 24/25 | Loss: 0.00078844
Iteration 25/25 | Loss: 0.00078788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.21457481
Iteration 2/25 | Loss: 0.00134856
Iteration 3/25 | Loss: 0.00134759
Iteration 4/25 | Loss: 0.00134759
Iteration 5/25 | Loss: 0.00134759
Iteration 6/25 | Loss: 0.00134759
Iteration 7/25 | Loss: 0.00134759
Iteration 8/25 | Loss: 0.00134759
Iteration 9/25 | Loss: 0.00134759
Iteration 10/25 | Loss: 0.00134759
Iteration 11/25 | Loss: 0.00134758
Iteration 12/25 | Loss: 0.00134758
Iteration 13/25 | Loss: 0.00134758
Iteration 14/25 | Loss: 0.00134758
Iteration 15/25 | Loss: 0.00134758
Iteration 16/25 | Loss: 0.00134758
Iteration 17/25 | Loss: 0.00134758
Iteration 18/25 | Loss: 0.00134758
Iteration 19/25 | Loss: 0.00134758
Iteration 20/25 | Loss: 0.00134758
Iteration 21/25 | Loss: 0.00134758
Iteration 22/25 | Loss: 0.00134758
Iteration 23/25 | Loss: 0.00134758
Iteration 24/25 | Loss: 0.00134758
Iteration 25/25 | Loss: 0.00134758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134758
Iteration 2/1000 | Loss: 0.00042387
Iteration 3/1000 | Loss: 0.00061197
Iteration 4/1000 | Loss: 0.00061008
Iteration 5/1000 | Loss: 0.00042664
Iteration 6/1000 | Loss: 0.00044278
Iteration 7/1000 | Loss: 0.00052251
Iteration 8/1000 | Loss: 0.00026309
Iteration 9/1000 | Loss: 0.00049633
Iteration 10/1000 | Loss: 0.00020199
Iteration 11/1000 | Loss: 0.00077360
Iteration 12/1000 | Loss: 0.00057554
Iteration 13/1000 | Loss: 0.00102633
Iteration 14/1000 | Loss: 0.00086817
Iteration 15/1000 | Loss: 0.00185995
Iteration 16/1000 | Loss: 0.00078748
Iteration 17/1000 | Loss: 0.00039617
Iteration 18/1000 | Loss: 0.00096120
Iteration 19/1000 | Loss: 0.00087094
Iteration 20/1000 | Loss: 0.00059119
Iteration 21/1000 | Loss: 0.00049635
Iteration 22/1000 | Loss: 0.00071231
Iteration 23/1000 | Loss: 0.00709787
Iteration 24/1000 | Loss: 0.00064268
Iteration 25/1000 | Loss: 0.00018801
Iteration 26/1000 | Loss: 0.00034098
Iteration 27/1000 | Loss: 0.00033778
Iteration 28/1000 | Loss: 0.00028970
Iteration 29/1000 | Loss: 0.00676031
Iteration 30/1000 | Loss: 0.00237686
Iteration 31/1000 | Loss: 0.00554124
Iteration 32/1000 | Loss: 0.00028619
Iteration 33/1000 | Loss: 0.00039659
Iteration 34/1000 | Loss: 0.00038693
Iteration 35/1000 | Loss: 0.00047358
Iteration 36/1000 | Loss: 0.00078785
Iteration 37/1000 | Loss: 0.00039796
Iteration 38/1000 | Loss: 0.00236873
Iteration 39/1000 | Loss: 0.00042863
Iteration 40/1000 | Loss: 0.00055114
Iteration 41/1000 | Loss: 0.00718632
Iteration 42/1000 | Loss: 0.00119525
Iteration 43/1000 | Loss: 0.00080626
Iteration 44/1000 | Loss: 0.00092072
Iteration 45/1000 | Loss: 0.00074490
Iteration 46/1000 | Loss: 0.00121374
Iteration 47/1000 | Loss: 0.00034767
Iteration 48/1000 | Loss: 0.00008808
Iteration 49/1000 | Loss: 0.00007739
Iteration 50/1000 | Loss: 0.00007157
Iteration 51/1000 | Loss: 0.00006710
Iteration 52/1000 | Loss: 0.00006457
Iteration 53/1000 | Loss: 0.00006255
Iteration 54/1000 | Loss: 0.00134957
Iteration 55/1000 | Loss: 0.00022295
Iteration 56/1000 | Loss: 0.00025488
Iteration 57/1000 | Loss: 0.00041544
Iteration 58/1000 | Loss: 0.00046464
Iteration 59/1000 | Loss: 0.00039158
Iteration 60/1000 | Loss: 0.00080424
Iteration 61/1000 | Loss: 0.00082784
Iteration 62/1000 | Loss: 0.00098708
Iteration 63/1000 | Loss: 0.00007298
Iteration 64/1000 | Loss: 0.00017912
Iteration 65/1000 | Loss: 0.00013575
Iteration 66/1000 | Loss: 0.00016290
Iteration 67/1000 | Loss: 0.00024144
Iteration 68/1000 | Loss: 0.00032542
Iteration 69/1000 | Loss: 0.00035821
Iteration 70/1000 | Loss: 0.00054808
Iteration 71/1000 | Loss: 0.00034544
Iteration 72/1000 | Loss: 0.00054735
Iteration 73/1000 | Loss: 0.00040254
Iteration 74/1000 | Loss: 0.00046520
Iteration 75/1000 | Loss: 0.00040281
Iteration 76/1000 | Loss: 0.00029884
Iteration 77/1000 | Loss: 0.00052042
Iteration 78/1000 | Loss: 0.00028358
Iteration 79/1000 | Loss: 0.00121236
Iteration 80/1000 | Loss: 0.00097357
Iteration 81/1000 | Loss: 0.00041398
Iteration 82/1000 | Loss: 0.00047888
Iteration 83/1000 | Loss: 0.00097090
Iteration 84/1000 | Loss: 0.00094315
Iteration 85/1000 | Loss: 0.00049637
Iteration 86/1000 | Loss: 0.00006675
Iteration 87/1000 | Loss: 0.00048260
Iteration 88/1000 | Loss: 0.00005916
Iteration 89/1000 | Loss: 0.00037745
Iteration 90/1000 | Loss: 0.00024231
Iteration 91/1000 | Loss: 0.00044163
Iteration 92/1000 | Loss: 0.00053738
Iteration 93/1000 | Loss: 0.00168285
Iteration 94/1000 | Loss: 0.00130611
Iteration 95/1000 | Loss: 0.00047698
Iteration 96/1000 | Loss: 0.00062734
Iteration 97/1000 | Loss: 0.00073917
Iteration 98/1000 | Loss: 0.00062600
Iteration 99/1000 | Loss: 0.00148941
Iteration 100/1000 | Loss: 0.00102136
Iteration 101/1000 | Loss: 0.00021633
Iteration 102/1000 | Loss: 0.00026453
Iteration 103/1000 | Loss: 0.00062122
Iteration 104/1000 | Loss: 0.00018338
Iteration 105/1000 | Loss: 0.00013267
Iteration 106/1000 | Loss: 0.00007290
Iteration 107/1000 | Loss: 0.00113336
Iteration 108/1000 | Loss: 0.00047260
Iteration 109/1000 | Loss: 0.00046903
Iteration 110/1000 | Loss: 0.00056504
Iteration 111/1000 | Loss: 0.00082066
Iteration 112/1000 | Loss: 0.00032933
Iteration 113/1000 | Loss: 0.00038254
Iteration 114/1000 | Loss: 0.00010406
Iteration 115/1000 | Loss: 0.00017266
Iteration 116/1000 | Loss: 0.00007831
Iteration 117/1000 | Loss: 0.00055370
Iteration 118/1000 | Loss: 0.00043802
Iteration 119/1000 | Loss: 0.00007361
Iteration 120/1000 | Loss: 0.00005536
Iteration 121/1000 | Loss: 0.00005275
Iteration 122/1000 | Loss: 0.00005940
Iteration 123/1000 | Loss: 0.00005395
Iteration 124/1000 | Loss: 0.00005123
Iteration 125/1000 | Loss: 0.00004878
Iteration 126/1000 | Loss: 0.00004738
Iteration 127/1000 | Loss: 0.00004834
Iteration 128/1000 | Loss: 0.00118291
Iteration 129/1000 | Loss: 0.00375872
Iteration 130/1000 | Loss: 0.00185620
Iteration 131/1000 | Loss: 0.00012379
Iteration 132/1000 | Loss: 0.00007571
Iteration 133/1000 | Loss: 0.00005508
Iteration 134/1000 | Loss: 0.00004959
Iteration 135/1000 | Loss: 0.00004718
Iteration 136/1000 | Loss: 0.00004582
Iteration 137/1000 | Loss: 0.00004471
Iteration 138/1000 | Loss: 0.00004391
Iteration 139/1000 | Loss: 0.00004297
Iteration 140/1000 | Loss: 0.00004223
Iteration 141/1000 | Loss: 0.00004177
Iteration 142/1000 | Loss: 0.00005072
Iteration 143/1000 | Loss: 0.00004268
Iteration 144/1000 | Loss: 0.00004189
Iteration 145/1000 | Loss: 0.00005318
Iteration 146/1000 | Loss: 0.00128763
Iteration 147/1000 | Loss: 0.00103324
Iteration 148/1000 | Loss: 0.00168200
Iteration 149/1000 | Loss: 0.00077264
Iteration 150/1000 | Loss: 0.00085053
Iteration 151/1000 | Loss: 0.00009338
Iteration 152/1000 | Loss: 0.00007141
Iteration 153/1000 | Loss: 0.00008495
Iteration 154/1000 | Loss: 0.00006057
Iteration 155/1000 | Loss: 0.00007380
Iteration 156/1000 | Loss: 0.00007776
Iteration 157/1000 | Loss: 0.00005860
Iteration 158/1000 | Loss: 0.00005066
Iteration 159/1000 | Loss: 0.00004775
Iteration 160/1000 | Loss: 0.00004554
Iteration 161/1000 | Loss: 0.00005518
Iteration 162/1000 | Loss: 0.00004275
Iteration 163/1000 | Loss: 0.00004153
Iteration 164/1000 | Loss: 0.00006458
Iteration 165/1000 | Loss: 0.00122533
Iteration 166/1000 | Loss: 0.00008228
Iteration 167/1000 | Loss: 0.00005337
Iteration 168/1000 | Loss: 0.00004614
Iteration 169/1000 | Loss: 0.00004331
Iteration 170/1000 | Loss: 0.00004191
Iteration 171/1000 | Loss: 0.00004054
Iteration 172/1000 | Loss: 0.00003993
Iteration 173/1000 | Loss: 0.00004926
Iteration 174/1000 | Loss: 0.00004760
Iteration 175/1000 | Loss: 0.00004370
Iteration 176/1000 | Loss: 0.00004630
Iteration 177/1000 | Loss: 0.00004411
Iteration 178/1000 | Loss: 0.00004805
Iteration 179/1000 | Loss: 0.00004581
Iteration 180/1000 | Loss: 0.00004022
Iteration 181/1000 | Loss: 0.00003910
Iteration 182/1000 | Loss: 0.00004661
Iteration 183/1000 | Loss: 0.00004332
Iteration 184/1000 | Loss: 0.00004662
Iteration 185/1000 | Loss: 0.00004339
Iteration 186/1000 | Loss: 0.00003939
Iteration 187/1000 | Loss: 0.00003868
Iteration 188/1000 | Loss: 0.00004135
Iteration 189/1000 | Loss: 0.00003851
Iteration 190/1000 | Loss: 0.00003847
Iteration 191/1000 | Loss: 0.00003847
Iteration 192/1000 | Loss: 0.00003843
Iteration 193/1000 | Loss: 0.00003835
Iteration 194/1000 | Loss: 0.00003835
Iteration 195/1000 | Loss: 0.00003833
Iteration 196/1000 | Loss: 0.00003833
Iteration 197/1000 | Loss: 0.00003829
Iteration 198/1000 | Loss: 0.00003815
Iteration 199/1000 | Loss: 0.00120029
Iteration 200/1000 | Loss: 0.00096554
Iteration 201/1000 | Loss: 0.00005853
Iteration 202/1000 | Loss: 0.00004714
Iteration 203/1000 | Loss: 0.00004202
Iteration 204/1000 | Loss: 0.00003822
Iteration 205/1000 | Loss: 0.00004203
Iteration 206/1000 | Loss: 0.00003911
Iteration 207/1000 | Loss: 0.00003785
Iteration 208/1000 | Loss: 0.00003694
Iteration 209/1000 | Loss: 0.00003628
Iteration 210/1000 | Loss: 0.00003575
Iteration 211/1000 | Loss: 0.00003552
Iteration 212/1000 | Loss: 0.00003529
Iteration 213/1000 | Loss: 0.00003495
Iteration 214/1000 | Loss: 0.00004227
Iteration 215/1000 | Loss: 0.00003753
Iteration 216/1000 | Loss: 0.00004185
Iteration 217/1000 | Loss: 0.00003824
Iteration 218/1000 | Loss: 0.00003710
Iteration 219/1000 | Loss: 0.00003574
Iteration 220/1000 | Loss: 0.00003484
Iteration 221/1000 | Loss: 0.00003416
Iteration 222/1000 | Loss: 0.00003361
Iteration 223/1000 | Loss: 0.00003324
Iteration 224/1000 | Loss: 0.00003310
Iteration 225/1000 | Loss: 0.00003306
Iteration 226/1000 | Loss: 0.00003300
Iteration 227/1000 | Loss: 0.00003299
Iteration 228/1000 | Loss: 0.00003299
Iteration 229/1000 | Loss: 0.00003299
Iteration 230/1000 | Loss: 0.00003298
Iteration 231/1000 | Loss: 0.00003298
Iteration 232/1000 | Loss: 0.00003297
Iteration 233/1000 | Loss: 0.00003297
Iteration 234/1000 | Loss: 0.00003296
Iteration 235/1000 | Loss: 0.00003295
Iteration 236/1000 | Loss: 0.00003295
Iteration 237/1000 | Loss: 0.00003294
Iteration 238/1000 | Loss: 0.00003294
Iteration 239/1000 | Loss: 0.00003294
Iteration 240/1000 | Loss: 0.00003293
Iteration 241/1000 | Loss: 0.00003293
Iteration 242/1000 | Loss: 0.00003292
Iteration 243/1000 | Loss: 0.00003292
Iteration 244/1000 | Loss: 0.00003292
Iteration 245/1000 | Loss: 0.00003291
Iteration 246/1000 | Loss: 0.00003291
Iteration 247/1000 | Loss: 0.00003291
Iteration 248/1000 | Loss: 0.00003290
Iteration 249/1000 | Loss: 0.00003290
Iteration 250/1000 | Loss: 0.00003290
Iteration 251/1000 | Loss: 0.00003290
Iteration 252/1000 | Loss: 0.00003290
Iteration 253/1000 | Loss: 0.00003289
Iteration 254/1000 | Loss: 0.00003289
Iteration 255/1000 | Loss: 0.00003289
Iteration 256/1000 | Loss: 0.00003289
Iteration 257/1000 | Loss: 0.00003289
Iteration 258/1000 | Loss: 0.00003289
Iteration 259/1000 | Loss: 0.00003288
Iteration 260/1000 | Loss: 0.00003288
Iteration 261/1000 | Loss: 0.00003288
Iteration 262/1000 | Loss: 0.00003288
Iteration 263/1000 | Loss: 0.00003287
Iteration 264/1000 | Loss: 0.00003287
Iteration 265/1000 | Loss: 0.00003287
Iteration 266/1000 | Loss: 0.00003286
Iteration 267/1000 | Loss: 0.00003286
Iteration 268/1000 | Loss: 0.00003286
Iteration 269/1000 | Loss: 0.00003286
Iteration 270/1000 | Loss: 0.00003286
Iteration 271/1000 | Loss: 0.00003286
Iteration 272/1000 | Loss: 0.00003285
Iteration 273/1000 | Loss: 0.00003285
Iteration 274/1000 | Loss: 0.00003285
Iteration 275/1000 | Loss: 0.00003285
Iteration 276/1000 | Loss: 0.00003284
Iteration 277/1000 | Loss: 0.00003284
Iteration 278/1000 | Loss: 0.00003284
Iteration 279/1000 | Loss: 0.00003284
Iteration 280/1000 | Loss: 0.00003284
Iteration 281/1000 | Loss: 0.00003284
Iteration 282/1000 | Loss: 0.00003284
Iteration 283/1000 | Loss: 0.00003284
Iteration 284/1000 | Loss: 0.00003284
Iteration 285/1000 | Loss: 0.00003283
Iteration 286/1000 | Loss: 0.00003283
Iteration 287/1000 | Loss: 0.00003283
Iteration 288/1000 | Loss: 0.00003283
Iteration 289/1000 | Loss: 0.00003283
Iteration 290/1000 | Loss: 0.00003283
Iteration 291/1000 | Loss: 0.00003283
Iteration 292/1000 | Loss: 0.00003283
Iteration 293/1000 | Loss: 0.00003283
Iteration 294/1000 | Loss: 0.00003283
Iteration 295/1000 | Loss: 0.00003283
Iteration 296/1000 | Loss: 0.00003283
Iteration 297/1000 | Loss: 0.00003283
Iteration 298/1000 | Loss: 0.00003283
Iteration 299/1000 | Loss: 0.00003283
Iteration 300/1000 | Loss: 0.00003283
Iteration 301/1000 | Loss: 0.00003283
Iteration 302/1000 | Loss: 0.00003283
Iteration 303/1000 | Loss: 0.00003283
Iteration 304/1000 | Loss: 0.00003283
Iteration 305/1000 | Loss: 0.00003283
Iteration 306/1000 | Loss: 0.00003283
Iteration 307/1000 | Loss: 0.00003283
Iteration 308/1000 | Loss: 0.00003283
Iteration 309/1000 | Loss: 0.00003283
Iteration 310/1000 | Loss: 0.00003283
Iteration 311/1000 | Loss: 0.00003283
Iteration 312/1000 | Loss: 0.00003283
Iteration 313/1000 | Loss: 0.00003283
Iteration 314/1000 | Loss: 0.00003283
Iteration 315/1000 | Loss: 0.00003283
Iteration 316/1000 | Loss: 0.00003283
Iteration 317/1000 | Loss: 0.00003283
Iteration 318/1000 | Loss: 0.00003283
Iteration 319/1000 | Loss: 0.00003283
Iteration 320/1000 | Loss: 0.00003283
Iteration 321/1000 | Loss: 0.00003283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [3.2830539566930383e-05, 3.2830539566930383e-05, 3.2830539566930383e-05, 3.2830539566930383e-05, 3.2830539566930383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2830539566930383e-05

Optimization complete. Final v2v error: 4.297380447387695 mm

Highest mean error: 12.641973495483398 mm for frame 126

Lowest mean error: 3.144129514694214 mm for frame 14

Saving results

Total time: 365.07558012008667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835434
Iteration 2/25 | Loss: 0.00112011
Iteration 3/25 | Loss: 0.00079975
Iteration 4/25 | Loss: 0.00073939
Iteration 5/25 | Loss: 0.00071928
Iteration 6/25 | Loss: 0.00071470
Iteration 7/25 | Loss: 0.00071340
Iteration 8/25 | Loss: 0.00071307
Iteration 9/25 | Loss: 0.00071307
Iteration 10/25 | Loss: 0.00071307
Iteration 11/25 | Loss: 0.00071307
Iteration 12/25 | Loss: 0.00071307
Iteration 13/25 | Loss: 0.00071307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007130650337785482, 0.0007130650337785482, 0.0007130650337785482, 0.0007130650337785482, 0.0007130650337785482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007130650337785482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23675537
Iteration 2/25 | Loss: 0.00024366
Iteration 3/25 | Loss: 0.00024363
Iteration 4/25 | Loss: 0.00024363
Iteration 5/25 | Loss: 0.00024363
Iteration 6/25 | Loss: 0.00024363
Iteration 7/25 | Loss: 0.00024363
Iteration 8/25 | Loss: 0.00024363
Iteration 9/25 | Loss: 0.00024363
Iteration 10/25 | Loss: 0.00024363
Iteration 11/25 | Loss: 0.00024363
Iteration 12/25 | Loss: 0.00024363
Iteration 13/25 | Loss: 0.00024363
Iteration 14/25 | Loss: 0.00024363
Iteration 15/25 | Loss: 0.00024363
Iteration 16/25 | Loss: 0.00024363
Iteration 17/25 | Loss: 0.00024363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024362944532185793, 0.00024362944532185793, 0.00024362944532185793, 0.00024362944532185793, 0.00024362944532185793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024362944532185793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024363
Iteration 2/1000 | Loss: 0.00004420
Iteration 3/1000 | Loss: 0.00003479
Iteration 4/1000 | Loss: 0.00003091
Iteration 5/1000 | Loss: 0.00002949
Iteration 6/1000 | Loss: 0.00002837
Iteration 7/1000 | Loss: 0.00002769
Iteration 8/1000 | Loss: 0.00002706
Iteration 9/1000 | Loss: 0.00002665
Iteration 10/1000 | Loss: 0.00002632
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00002581
Iteration 13/1000 | Loss: 0.00002562
Iteration 14/1000 | Loss: 0.00002546
Iteration 15/1000 | Loss: 0.00002535
Iteration 16/1000 | Loss: 0.00002526
Iteration 17/1000 | Loss: 0.00002523
Iteration 18/1000 | Loss: 0.00002523
Iteration 19/1000 | Loss: 0.00002521
Iteration 20/1000 | Loss: 0.00002520
Iteration 21/1000 | Loss: 0.00002519
Iteration 22/1000 | Loss: 0.00002518
Iteration 23/1000 | Loss: 0.00002517
Iteration 24/1000 | Loss: 0.00002517
Iteration 25/1000 | Loss: 0.00002516
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00002515
Iteration 28/1000 | Loss: 0.00002515
Iteration 29/1000 | Loss: 0.00002515
Iteration 30/1000 | Loss: 0.00002514
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002514
Iteration 33/1000 | Loss: 0.00002513
Iteration 34/1000 | Loss: 0.00002513
Iteration 35/1000 | Loss: 0.00002513
Iteration 36/1000 | Loss: 0.00002512
Iteration 37/1000 | Loss: 0.00002512
Iteration 38/1000 | Loss: 0.00002511
Iteration 39/1000 | Loss: 0.00002511
Iteration 40/1000 | Loss: 0.00002511
Iteration 41/1000 | Loss: 0.00002510
Iteration 42/1000 | Loss: 0.00002510
Iteration 43/1000 | Loss: 0.00002510
Iteration 44/1000 | Loss: 0.00002510
Iteration 45/1000 | Loss: 0.00002510
Iteration 46/1000 | Loss: 0.00002510
Iteration 47/1000 | Loss: 0.00002510
Iteration 48/1000 | Loss: 0.00002510
Iteration 49/1000 | Loss: 0.00002510
Iteration 50/1000 | Loss: 0.00002510
Iteration 51/1000 | Loss: 0.00002510
Iteration 52/1000 | Loss: 0.00002510
Iteration 53/1000 | Loss: 0.00002509
Iteration 54/1000 | Loss: 0.00002509
Iteration 55/1000 | Loss: 0.00002509
Iteration 56/1000 | Loss: 0.00002509
Iteration 57/1000 | Loss: 0.00002509
Iteration 58/1000 | Loss: 0.00002508
Iteration 59/1000 | Loss: 0.00002508
Iteration 60/1000 | Loss: 0.00002508
Iteration 61/1000 | Loss: 0.00002508
Iteration 62/1000 | Loss: 0.00002508
Iteration 63/1000 | Loss: 0.00002507
Iteration 64/1000 | Loss: 0.00002507
Iteration 65/1000 | Loss: 0.00002507
Iteration 66/1000 | Loss: 0.00002506
Iteration 67/1000 | Loss: 0.00002506
Iteration 68/1000 | Loss: 0.00002506
Iteration 69/1000 | Loss: 0.00002506
Iteration 70/1000 | Loss: 0.00002505
Iteration 71/1000 | Loss: 0.00002505
Iteration 72/1000 | Loss: 0.00002505
Iteration 73/1000 | Loss: 0.00002505
Iteration 74/1000 | Loss: 0.00002505
Iteration 75/1000 | Loss: 0.00002505
Iteration 76/1000 | Loss: 0.00002504
Iteration 77/1000 | Loss: 0.00002504
Iteration 78/1000 | Loss: 0.00002504
Iteration 79/1000 | Loss: 0.00002504
Iteration 80/1000 | Loss: 0.00002504
Iteration 81/1000 | Loss: 0.00002504
Iteration 82/1000 | Loss: 0.00002504
Iteration 83/1000 | Loss: 0.00002504
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002503
Iteration 87/1000 | Loss: 0.00002503
Iteration 88/1000 | Loss: 0.00002503
Iteration 89/1000 | Loss: 0.00002503
Iteration 90/1000 | Loss: 0.00002502
Iteration 91/1000 | Loss: 0.00002502
Iteration 92/1000 | Loss: 0.00002502
Iteration 93/1000 | Loss: 0.00002502
Iteration 94/1000 | Loss: 0.00002501
Iteration 95/1000 | Loss: 0.00002501
Iteration 96/1000 | Loss: 0.00002501
Iteration 97/1000 | Loss: 0.00002501
Iteration 98/1000 | Loss: 0.00002501
Iteration 99/1000 | Loss: 0.00002500
Iteration 100/1000 | Loss: 0.00002500
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00002500
Iteration 103/1000 | Loss: 0.00002500
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002500
Iteration 107/1000 | Loss: 0.00002500
Iteration 108/1000 | Loss: 0.00002500
Iteration 109/1000 | Loss: 0.00002499
Iteration 110/1000 | Loss: 0.00002499
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002499
Iteration 113/1000 | Loss: 0.00002499
Iteration 114/1000 | Loss: 0.00002499
Iteration 115/1000 | Loss: 0.00002499
Iteration 116/1000 | Loss: 0.00002499
Iteration 117/1000 | Loss: 0.00002499
Iteration 118/1000 | Loss: 0.00002499
Iteration 119/1000 | Loss: 0.00002499
Iteration 120/1000 | Loss: 0.00002499
Iteration 121/1000 | Loss: 0.00002499
Iteration 122/1000 | Loss: 0.00002498
Iteration 123/1000 | Loss: 0.00002498
Iteration 124/1000 | Loss: 0.00002498
Iteration 125/1000 | Loss: 0.00002498
Iteration 126/1000 | Loss: 0.00002498
Iteration 127/1000 | Loss: 0.00002498
Iteration 128/1000 | Loss: 0.00002498
Iteration 129/1000 | Loss: 0.00002498
Iteration 130/1000 | Loss: 0.00002498
Iteration 131/1000 | Loss: 0.00002498
Iteration 132/1000 | Loss: 0.00002498
Iteration 133/1000 | Loss: 0.00002497
Iteration 134/1000 | Loss: 0.00002497
Iteration 135/1000 | Loss: 0.00002497
Iteration 136/1000 | Loss: 0.00002497
Iteration 137/1000 | Loss: 0.00002497
Iteration 138/1000 | Loss: 0.00002497
Iteration 139/1000 | Loss: 0.00002497
Iteration 140/1000 | Loss: 0.00002497
Iteration 141/1000 | Loss: 0.00002497
Iteration 142/1000 | Loss: 0.00002497
Iteration 143/1000 | Loss: 0.00002497
Iteration 144/1000 | Loss: 0.00002497
Iteration 145/1000 | Loss: 0.00002497
Iteration 146/1000 | Loss: 0.00002497
Iteration 147/1000 | Loss: 0.00002497
Iteration 148/1000 | Loss: 0.00002497
Iteration 149/1000 | Loss: 0.00002497
Iteration 150/1000 | Loss: 0.00002497
Iteration 151/1000 | Loss: 0.00002497
Iteration 152/1000 | Loss: 0.00002497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.497327841410879e-05, 2.497327841410879e-05, 2.497327841410879e-05, 2.497327841410879e-05, 2.497327841410879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.497327841410879e-05

Optimization complete. Final v2v error: 4.097092151641846 mm

Highest mean error: 5.346006393432617 mm for frame 10

Lowest mean error: 3.133165121078491 mm for frame 214

Saving results

Total time: 48.38863658905029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806899
Iteration 2/25 | Loss: 0.00106765
Iteration 3/25 | Loss: 0.00077700
Iteration 4/25 | Loss: 0.00071147
Iteration 5/25 | Loss: 0.00069130
Iteration 6/25 | Loss: 0.00068497
Iteration 7/25 | Loss: 0.00068283
Iteration 8/25 | Loss: 0.00068249
Iteration 9/25 | Loss: 0.00068249
Iteration 10/25 | Loss: 0.00068249
Iteration 11/25 | Loss: 0.00068249
Iteration 12/25 | Loss: 0.00068249
Iteration 13/25 | Loss: 0.00068249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006824887823313475, 0.0006824887823313475, 0.0006824887823313475, 0.0006824887823313475, 0.0006824887823313475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006824887823313475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66227496
Iteration 2/25 | Loss: 0.00036623
Iteration 3/25 | Loss: 0.00036623
Iteration 4/25 | Loss: 0.00036623
Iteration 5/25 | Loss: 0.00036623
Iteration 6/25 | Loss: 0.00036623
Iteration 7/25 | Loss: 0.00036623
Iteration 8/25 | Loss: 0.00036623
Iteration 9/25 | Loss: 0.00036623
Iteration 10/25 | Loss: 0.00036623
Iteration 11/25 | Loss: 0.00036623
Iteration 12/25 | Loss: 0.00036623
Iteration 13/25 | Loss: 0.00036623
Iteration 14/25 | Loss: 0.00036623
Iteration 15/25 | Loss: 0.00036623
Iteration 16/25 | Loss: 0.00036623
Iteration 17/25 | Loss: 0.00036623
Iteration 18/25 | Loss: 0.00036623
Iteration 19/25 | Loss: 0.00036623
Iteration 20/25 | Loss: 0.00036623
Iteration 21/25 | Loss: 0.00036623
Iteration 22/25 | Loss: 0.00036623
Iteration 23/25 | Loss: 0.00036623
Iteration 24/25 | Loss: 0.00036623
Iteration 25/25 | Loss: 0.00036623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036623
Iteration 2/1000 | Loss: 0.00005162
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002298
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002026
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001956
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001888
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001867
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001863
Iteration 58/1000 | Loss: 0.00001863
Iteration 59/1000 | Loss: 0.00001862
Iteration 60/1000 | Loss: 0.00001862
Iteration 61/1000 | Loss: 0.00001862
Iteration 62/1000 | Loss: 0.00001861
Iteration 63/1000 | Loss: 0.00001861
Iteration 64/1000 | Loss: 0.00001861
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00001852
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001851
Iteration 123/1000 | Loss: 0.00001851
Iteration 124/1000 | Loss: 0.00001851
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001851
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.8499587895348668e-05, 1.8499587895348668e-05, 1.8499587895348668e-05, 1.8499587895348668e-05, 1.8499587895348668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8499587895348668e-05

Optimization complete. Final v2v error: 3.5938472747802734 mm

Highest mean error: 5.1787896156311035 mm for frame 176

Lowest mean error: 2.681124687194824 mm for frame 129

Saving results

Total time: 46.354332447052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104024
Iteration 2/25 | Loss: 0.00366420
Iteration 3/25 | Loss: 0.00201790
Iteration 4/25 | Loss: 0.00171031
Iteration 5/25 | Loss: 0.00155935
Iteration 6/25 | Loss: 0.00151383
Iteration 7/25 | Loss: 0.00143312
Iteration 8/25 | Loss: 0.00139142
Iteration 9/25 | Loss: 0.00140432
Iteration 10/25 | Loss: 0.00134556
Iteration 11/25 | Loss: 0.00124687
Iteration 12/25 | Loss: 0.00125479
Iteration 13/25 | Loss: 0.00117289
Iteration 14/25 | Loss: 0.00114577
Iteration 15/25 | Loss: 0.00109236
Iteration 16/25 | Loss: 0.00107487
Iteration 17/25 | Loss: 0.00106859
Iteration 18/25 | Loss: 0.00106707
Iteration 19/25 | Loss: 0.00106323
Iteration 20/25 | Loss: 0.00108065
Iteration 21/25 | Loss: 0.00107079
Iteration 22/25 | Loss: 0.00109476
Iteration 23/25 | Loss: 0.00109211
Iteration 24/25 | Loss: 0.00107878
Iteration 25/25 | Loss: 0.00106500

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17948711
Iteration 2/25 | Loss: 0.00281234
Iteration 3/25 | Loss: 0.00276490
Iteration 4/25 | Loss: 0.00276490
Iteration 5/25 | Loss: 0.00276490
Iteration 6/25 | Loss: 0.00276490
Iteration 7/25 | Loss: 0.00276490
Iteration 8/25 | Loss: 0.00276490
Iteration 9/25 | Loss: 0.00276490
Iteration 10/25 | Loss: 0.00276490
Iteration 11/25 | Loss: 0.00276490
Iteration 12/25 | Loss: 0.00276490
Iteration 13/25 | Loss: 0.00276490
Iteration 14/25 | Loss: 0.00276490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002764899516478181, 0.002764899516478181, 0.002764899516478181, 0.002764899516478181, 0.002764899516478181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002764899516478181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276490
Iteration 2/1000 | Loss: 0.00090729
Iteration 3/1000 | Loss: 0.00108520
Iteration 4/1000 | Loss: 0.00054832
Iteration 5/1000 | Loss: 0.00078219
Iteration 6/1000 | Loss: 0.00115732
Iteration 7/1000 | Loss: 0.00037732
Iteration 8/1000 | Loss: 0.00079833
Iteration 9/1000 | Loss: 0.00036790
Iteration 10/1000 | Loss: 0.00026962
Iteration 11/1000 | Loss: 0.00027111
Iteration 12/1000 | Loss: 0.00017459
Iteration 13/1000 | Loss: 0.00044752
Iteration 14/1000 | Loss: 0.00016379
Iteration 15/1000 | Loss: 0.00038350
Iteration 16/1000 | Loss: 0.00049174
Iteration 17/1000 | Loss: 0.00016905
Iteration 18/1000 | Loss: 0.00014219
Iteration 19/1000 | Loss: 0.00032030
Iteration 20/1000 | Loss: 0.00024755
Iteration 21/1000 | Loss: 0.00018081
Iteration 22/1000 | Loss: 0.00014807
Iteration 23/1000 | Loss: 0.00041675
Iteration 24/1000 | Loss: 0.00192700
Iteration 25/1000 | Loss: 0.00381803
Iteration 26/1000 | Loss: 0.00276026
Iteration 27/1000 | Loss: 0.00022847
Iteration 28/1000 | Loss: 0.00034882
Iteration 29/1000 | Loss: 0.00017695
Iteration 30/1000 | Loss: 0.00026843
Iteration 31/1000 | Loss: 0.00011169
Iteration 32/1000 | Loss: 0.00008679
Iteration 33/1000 | Loss: 0.00020628
Iteration 34/1000 | Loss: 0.00008478
Iteration 35/1000 | Loss: 0.00061107
Iteration 36/1000 | Loss: 0.00062843
Iteration 37/1000 | Loss: 0.00021049
Iteration 38/1000 | Loss: 0.00012173
Iteration 39/1000 | Loss: 0.00026847
Iteration 40/1000 | Loss: 0.00006244
Iteration 41/1000 | Loss: 0.00005423
Iteration 42/1000 | Loss: 0.00005154
Iteration 43/1000 | Loss: 0.00004047
Iteration 44/1000 | Loss: 0.00014007
Iteration 45/1000 | Loss: 0.00003695
Iteration 46/1000 | Loss: 0.00003587
Iteration 47/1000 | Loss: 0.00003517
Iteration 48/1000 | Loss: 0.00003473
Iteration 49/1000 | Loss: 0.00009575
Iteration 50/1000 | Loss: 0.00005425
Iteration 51/1000 | Loss: 0.00005852
Iteration 52/1000 | Loss: 0.00003745
Iteration 53/1000 | Loss: 0.00003472
Iteration 54/1000 | Loss: 0.00003401
Iteration 55/1000 | Loss: 0.00003359
Iteration 56/1000 | Loss: 0.00003328
Iteration 57/1000 | Loss: 0.00003307
Iteration 58/1000 | Loss: 0.00003300
Iteration 59/1000 | Loss: 0.00003299
Iteration 60/1000 | Loss: 0.00003298
Iteration 61/1000 | Loss: 0.00003298
Iteration 62/1000 | Loss: 0.00003298
Iteration 63/1000 | Loss: 0.00003297
Iteration 64/1000 | Loss: 0.00003297
Iteration 65/1000 | Loss: 0.00003284
Iteration 66/1000 | Loss: 0.00003280
Iteration 67/1000 | Loss: 0.00003279
Iteration 68/1000 | Loss: 0.00003278
Iteration 69/1000 | Loss: 0.00003277
Iteration 70/1000 | Loss: 0.00003276
Iteration 71/1000 | Loss: 0.00003275
Iteration 72/1000 | Loss: 0.00003275
Iteration 73/1000 | Loss: 0.00003273
Iteration 74/1000 | Loss: 0.00003273
Iteration 75/1000 | Loss: 0.00003273
Iteration 76/1000 | Loss: 0.00003273
Iteration 77/1000 | Loss: 0.00003273
Iteration 78/1000 | Loss: 0.00003273
Iteration 79/1000 | Loss: 0.00003273
Iteration 80/1000 | Loss: 0.00003273
Iteration 81/1000 | Loss: 0.00003273
Iteration 82/1000 | Loss: 0.00003273
Iteration 83/1000 | Loss: 0.00003272
Iteration 84/1000 | Loss: 0.00003272
Iteration 85/1000 | Loss: 0.00003272
Iteration 86/1000 | Loss: 0.00003272
Iteration 87/1000 | Loss: 0.00003272
Iteration 88/1000 | Loss: 0.00003272
Iteration 89/1000 | Loss: 0.00003272
Iteration 90/1000 | Loss: 0.00003272
Iteration 91/1000 | Loss: 0.00003272
Iteration 92/1000 | Loss: 0.00003271
Iteration 93/1000 | Loss: 0.00003271
Iteration 94/1000 | Loss: 0.00003271
Iteration 95/1000 | Loss: 0.00003271
Iteration 96/1000 | Loss: 0.00003270
Iteration 97/1000 | Loss: 0.00003269
Iteration 98/1000 | Loss: 0.00003269
Iteration 99/1000 | Loss: 0.00003268
Iteration 100/1000 | Loss: 0.00003268
Iteration 101/1000 | Loss: 0.00003268
Iteration 102/1000 | Loss: 0.00003268
Iteration 103/1000 | Loss: 0.00003268
Iteration 104/1000 | Loss: 0.00003268
Iteration 105/1000 | Loss: 0.00003268
Iteration 106/1000 | Loss: 0.00003268
Iteration 107/1000 | Loss: 0.00003268
Iteration 108/1000 | Loss: 0.00003268
Iteration 109/1000 | Loss: 0.00003267
Iteration 110/1000 | Loss: 0.00003267
Iteration 111/1000 | Loss: 0.00003267
Iteration 112/1000 | Loss: 0.00003267
Iteration 113/1000 | Loss: 0.00003267
Iteration 114/1000 | Loss: 0.00003267
Iteration 115/1000 | Loss: 0.00003266
Iteration 116/1000 | Loss: 0.00003265
Iteration 117/1000 | Loss: 0.00003265
Iteration 118/1000 | Loss: 0.00003264
Iteration 119/1000 | Loss: 0.00003264
Iteration 120/1000 | Loss: 0.00003264
Iteration 121/1000 | Loss: 0.00003263
Iteration 122/1000 | Loss: 0.00003263
Iteration 123/1000 | Loss: 0.00003263
Iteration 124/1000 | Loss: 0.00003263
Iteration 125/1000 | Loss: 0.00003263
Iteration 126/1000 | Loss: 0.00003263
Iteration 127/1000 | Loss: 0.00003263
Iteration 128/1000 | Loss: 0.00003263
Iteration 129/1000 | Loss: 0.00003263
Iteration 130/1000 | Loss: 0.00003263
Iteration 131/1000 | Loss: 0.00003263
Iteration 132/1000 | Loss: 0.00003262
Iteration 133/1000 | Loss: 0.00003262
Iteration 134/1000 | Loss: 0.00003262
Iteration 135/1000 | Loss: 0.00003262
Iteration 136/1000 | Loss: 0.00003262
Iteration 137/1000 | Loss: 0.00003262
Iteration 138/1000 | Loss: 0.00003262
Iteration 139/1000 | Loss: 0.00003262
Iteration 140/1000 | Loss: 0.00003261
Iteration 141/1000 | Loss: 0.00003261
Iteration 142/1000 | Loss: 0.00003261
Iteration 143/1000 | Loss: 0.00003261
Iteration 144/1000 | Loss: 0.00003261
Iteration 145/1000 | Loss: 0.00003261
Iteration 146/1000 | Loss: 0.00003261
Iteration 147/1000 | Loss: 0.00003261
Iteration 148/1000 | Loss: 0.00003261
Iteration 149/1000 | Loss: 0.00003261
Iteration 150/1000 | Loss: 0.00003261
Iteration 151/1000 | Loss: 0.00003261
Iteration 152/1000 | Loss: 0.00003261
Iteration 153/1000 | Loss: 0.00003261
Iteration 154/1000 | Loss: 0.00003261
Iteration 155/1000 | Loss: 0.00003260
Iteration 156/1000 | Loss: 0.00003260
Iteration 157/1000 | Loss: 0.00003260
Iteration 158/1000 | Loss: 0.00003260
Iteration 159/1000 | Loss: 0.00003260
Iteration 160/1000 | Loss: 0.00003260
Iteration 161/1000 | Loss: 0.00003260
Iteration 162/1000 | Loss: 0.00003260
Iteration 163/1000 | Loss: 0.00003260
Iteration 164/1000 | Loss: 0.00003260
Iteration 165/1000 | Loss: 0.00003260
Iteration 166/1000 | Loss: 0.00003260
Iteration 167/1000 | Loss: 0.00003260
Iteration 168/1000 | Loss: 0.00003260
Iteration 169/1000 | Loss: 0.00003260
Iteration 170/1000 | Loss: 0.00003260
Iteration 171/1000 | Loss: 0.00003260
Iteration 172/1000 | Loss: 0.00003260
Iteration 173/1000 | Loss: 0.00003260
Iteration 174/1000 | Loss: 0.00003260
Iteration 175/1000 | Loss: 0.00003260
Iteration 176/1000 | Loss: 0.00003260
Iteration 177/1000 | Loss: 0.00003259
Iteration 178/1000 | Loss: 0.00003259
Iteration 179/1000 | Loss: 0.00003259
Iteration 180/1000 | Loss: 0.00003259
Iteration 181/1000 | Loss: 0.00003259
Iteration 182/1000 | Loss: 0.00003259
Iteration 183/1000 | Loss: 0.00003259
Iteration 184/1000 | Loss: 0.00003259
Iteration 185/1000 | Loss: 0.00003259
Iteration 186/1000 | Loss: 0.00003259
Iteration 187/1000 | Loss: 0.00003259
Iteration 188/1000 | Loss: 0.00003259
Iteration 189/1000 | Loss: 0.00003259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [3.259413279010914e-05, 3.259413279010914e-05, 3.259413279010914e-05, 3.259413279010914e-05, 3.259413279010914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.259413279010914e-05

Optimization complete. Final v2v error: 4.39365816116333 mm

Highest mean error: 6.250423908233643 mm for frame 34

Lowest mean error: 3.6838817596435547 mm for frame 73

Saving results

Total time: 133.22215104103088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963638
Iteration 2/25 | Loss: 0.00394888
Iteration 3/25 | Loss: 0.00295629
Iteration 4/25 | Loss: 0.00220473
Iteration 5/25 | Loss: 0.00213289
Iteration 6/25 | Loss: 0.00232771
Iteration 7/25 | Loss: 0.00218041
Iteration 8/25 | Loss: 0.00201654
Iteration 9/25 | Loss: 0.00194255
Iteration 10/25 | Loss: 0.00201191
Iteration 11/25 | Loss: 0.00183881
Iteration 12/25 | Loss: 0.00170276
Iteration 13/25 | Loss: 0.00160147
Iteration 14/25 | Loss: 0.00158951
Iteration 15/25 | Loss: 0.00159514
Iteration 16/25 | Loss: 0.00153232
Iteration 17/25 | Loss: 0.00152250
Iteration 18/25 | Loss: 0.00144742
Iteration 19/25 | Loss: 0.00149070
Iteration 20/25 | Loss: 0.00148982
Iteration 21/25 | Loss: 0.00145511
Iteration 22/25 | Loss: 0.00150037
Iteration 23/25 | Loss: 0.00146750
Iteration 24/25 | Loss: 0.00148211
Iteration 25/25 | Loss: 0.00142950

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02412701
Iteration 2/25 | Loss: 0.01618848
Iteration 3/25 | Loss: 0.00959823
Iteration 4/25 | Loss: 0.00958835
Iteration 5/25 | Loss: 0.00958834
Iteration 6/25 | Loss: 0.00958834
Iteration 7/25 | Loss: 0.00958834
Iteration 8/25 | Loss: 0.00958834
Iteration 9/25 | Loss: 0.00958834
Iteration 10/25 | Loss: 0.00958834
Iteration 11/25 | Loss: 0.00958834
Iteration 12/25 | Loss: 0.00958834
Iteration 13/25 | Loss: 0.00958834
Iteration 14/25 | Loss: 0.00958834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.009588342159986496, 0.009588342159986496, 0.009588342159986496, 0.009588342159986496, 0.009588342159986496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009588342159986496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00958834
Iteration 2/1000 | Loss: 0.01404543
Iteration 3/1000 | Loss: 0.01382618
Iteration 4/1000 | Loss: 0.01508240
Iteration 5/1000 | Loss: 0.01325669
Iteration 6/1000 | Loss: 0.01340298
Iteration 7/1000 | Loss: 0.01004823
Iteration 8/1000 | Loss: 0.00889635
Iteration 9/1000 | Loss: 0.01641322
Iteration 10/1000 | Loss: 0.00983447
Iteration 11/1000 | Loss: 0.01438775
Iteration 12/1000 | Loss: 0.00949909
Iteration 13/1000 | Loss: 0.01020323
Iteration 14/1000 | Loss: 0.01107015
Iteration 15/1000 | Loss: 0.01447302
Iteration 16/1000 | Loss: 0.01208905
Iteration 17/1000 | Loss: 0.01457144
Iteration 18/1000 | Loss: 0.00906356
Iteration 19/1000 | Loss: 0.01032518
Iteration 20/1000 | Loss: 0.01109556
Iteration 21/1000 | Loss: 0.01093240
Iteration 22/1000 | Loss: 0.01234562
Iteration 23/1000 | Loss: 0.00725949
Iteration 24/1000 | Loss: 0.01141599
Iteration 25/1000 | Loss: 0.01000792
Iteration 26/1000 | Loss: 0.00826475
Iteration 27/1000 | Loss: 0.00934105
Iteration 28/1000 | Loss: 0.01354599
Iteration 29/1000 | Loss: 0.00909563
Iteration 30/1000 | Loss: 0.01376229
Iteration 31/1000 | Loss: 0.01088024
Iteration 32/1000 | Loss: 0.00791757
Iteration 33/1000 | Loss: 0.00694406
Iteration 34/1000 | Loss: 0.01348538
Iteration 35/1000 | Loss: 0.00984346
Iteration 36/1000 | Loss: 0.01111425
Iteration 37/1000 | Loss: 0.01069366
Iteration 38/1000 | Loss: 0.01373223
Iteration 39/1000 | Loss: 0.01125291
Iteration 40/1000 | Loss: 0.01153660
Iteration 41/1000 | Loss: 0.01359175
Iteration 42/1000 | Loss: 0.01093994
Iteration 43/1000 | Loss: 0.00997019
Iteration 44/1000 | Loss: 0.00841945
Iteration 45/1000 | Loss: 0.01492393
Iteration 46/1000 | Loss: 0.01108332
Iteration 47/1000 | Loss: 0.01020442
Iteration 48/1000 | Loss: 0.01149929
Iteration 49/1000 | Loss: 0.01464582
Iteration 50/1000 | Loss: 0.01120449
Iteration 51/1000 | Loss: 0.01329290
Iteration 52/1000 | Loss: 0.01106970
Iteration 53/1000 | Loss: 0.01166553
Iteration 54/1000 | Loss: 0.00888585
Iteration 55/1000 | Loss: 0.01069064
Iteration 56/1000 | Loss: 0.01117932
Iteration 57/1000 | Loss: 0.01011615
Iteration 58/1000 | Loss: 0.00970702
Iteration 59/1000 | Loss: 0.00833319
Iteration 60/1000 | Loss: 0.00804492
Iteration 61/1000 | Loss: 0.01222198
Iteration 62/1000 | Loss: 0.00836857
Iteration 63/1000 | Loss: 0.01014144
Iteration 64/1000 | Loss: 0.00961042
Iteration 65/1000 | Loss: 0.00983066
Iteration 66/1000 | Loss: 0.01119890
Iteration 67/1000 | Loss: 0.00937108
Iteration 68/1000 | Loss: 0.00661862
Iteration 69/1000 | Loss: 0.00717344
Iteration 70/1000 | Loss: 0.00846300
Iteration 71/1000 | Loss: 0.00699672
Iteration 72/1000 | Loss: 0.01225027
Iteration 73/1000 | Loss: 0.01032453
Iteration 74/1000 | Loss: 0.01130820
Iteration 75/1000 | Loss: 0.00834992
Iteration 76/1000 | Loss: 0.01098508
Iteration 77/1000 | Loss: 0.01175378
Iteration 78/1000 | Loss: 0.00713248
Iteration 79/1000 | Loss: 0.00711565
Iteration 80/1000 | Loss: 0.00959202
Iteration 81/1000 | Loss: 0.01067862
Iteration 82/1000 | Loss: 0.01243660
Iteration 83/1000 | Loss: 0.00970361
Iteration 84/1000 | Loss: 0.01008703
Iteration 85/1000 | Loss: 0.00822338
Iteration 86/1000 | Loss: 0.00986918
Iteration 87/1000 | Loss: 0.00743594
Iteration 88/1000 | Loss: 0.00903241
Iteration 89/1000 | Loss: 0.00913513
Iteration 90/1000 | Loss: 0.00743680
Iteration 91/1000 | Loss: 0.00963915
Iteration 92/1000 | Loss: 0.00818411
Iteration 93/1000 | Loss: 0.00770844
Iteration 94/1000 | Loss: 0.01134489
Iteration 95/1000 | Loss: 0.01287162
Iteration 96/1000 | Loss: 0.01352534
Iteration 97/1000 | Loss: 0.01234530
Iteration 98/1000 | Loss: 0.01504257
Iteration 99/1000 | Loss: 0.00820978
Iteration 100/1000 | Loss: 0.00888333
Iteration 101/1000 | Loss: 0.00943730
Iteration 102/1000 | Loss: 0.00705381
Iteration 103/1000 | Loss: 0.00840683
Iteration 104/1000 | Loss: 0.01187795
Iteration 105/1000 | Loss: 0.01033641
Iteration 106/1000 | Loss: 0.00942393
Iteration 107/1000 | Loss: 0.01184526
Iteration 108/1000 | Loss: 0.01022728
Iteration 109/1000 | Loss: 0.00856395
Iteration 110/1000 | Loss: 0.01302596
Iteration 111/1000 | Loss: 0.00854316
Iteration 112/1000 | Loss: 0.01049868
Iteration 113/1000 | Loss: 0.01041292
Iteration 114/1000 | Loss: 0.01165800
Iteration 115/1000 | Loss: 0.01008133
Iteration 116/1000 | Loss: 0.00938449
Iteration 117/1000 | Loss: 0.00730123
Iteration 118/1000 | Loss: 0.01404147
Iteration 119/1000 | Loss: 0.00772599
Iteration 120/1000 | Loss: 0.00721858
Iteration 121/1000 | Loss: 0.01890576
Iteration 122/1000 | Loss: 0.01132832
Iteration 123/1000 | Loss: 0.00931255
Iteration 124/1000 | Loss: 0.00992407
Iteration 125/1000 | Loss: 0.01333745
Iteration 126/1000 | Loss: 0.01084376
Iteration 127/1000 | Loss: 0.00613741
Iteration 128/1000 | Loss: 0.01262917
Iteration 129/1000 | Loss: 0.00846612
Iteration 130/1000 | Loss: 0.00810552
Iteration 131/1000 | Loss: 0.00926726
Iteration 132/1000 | Loss: 0.00773056
Iteration 133/1000 | Loss: 0.00868170
Iteration 134/1000 | Loss: 0.00773770
Iteration 135/1000 | Loss: 0.00786288
Iteration 136/1000 | Loss: 0.00863418
Iteration 137/1000 | Loss: 0.01353044
Iteration 138/1000 | Loss: 0.01058266
Iteration 139/1000 | Loss: 0.01041696
Iteration 140/1000 | Loss: 0.00865818
Iteration 141/1000 | Loss: 0.00652113
Iteration 142/1000 | Loss: 0.00946435
Iteration 143/1000 | Loss: 0.00730604
Iteration 144/1000 | Loss: 0.00694132
Iteration 145/1000 | Loss: 0.00669182
Iteration 146/1000 | Loss: 0.00680951
Iteration 147/1000 | Loss: 0.01153650
Iteration 148/1000 | Loss: 0.01334784
Iteration 149/1000 | Loss: 0.00771683
Iteration 150/1000 | Loss: 0.01021634
Iteration 151/1000 | Loss: 0.00669210
Iteration 152/1000 | Loss: 0.00754143
Iteration 153/1000 | Loss: 0.01126496
Iteration 154/1000 | Loss: 0.00816191
Iteration 155/1000 | Loss: 0.00892691
Iteration 156/1000 | Loss: 0.00598239
Iteration 157/1000 | Loss: 0.00795971
Iteration 158/1000 | Loss: 0.00690461
Iteration 159/1000 | Loss: 0.00700233
Iteration 160/1000 | Loss: 0.01003459
Iteration 161/1000 | Loss: 0.01043824
Iteration 162/1000 | Loss: 0.00919216
Iteration 163/1000 | Loss: 0.01019480
Iteration 164/1000 | Loss: 0.00659758
Iteration 165/1000 | Loss: 0.01012271
Iteration 166/1000 | Loss: 0.00588325
Iteration 167/1000 | Loss: 0.00730315
Iteration 168/1000 | Loss: 0.00776154
Iteration 169/1000 | Loss: 0.01100912
Iteration 170/1000 | Loss: 0.00810327
Iteration 171/1000 | Loss: 0.01199773
Iteration 172/1000 | Loss: 0.01187835
Iteration 173/1000 | Loss: 0.00775414
Iteration 174/1000 | Loss: 0.01180910
Iteration 175/1000 | Loss: 0.00795304
Iteration 176/1000 | Loss: 0.00610258
Iteration 177/1000 | Loss: 0.00802988
Iteration 178/1000 | Loss: 0.00995878
Iteration 179/1000 | Loss: 0.00953037
Iteration 180/1000 | Loss: 0.01065765
Iteration 181/1000 | Loss: 0.00817549
Iteration 182/1000 | Loss: 0.00819127
Iteration 183/1000 | Loss: 0.00731442
Iteration 184/1000 | Loss: 0.00619854
Iteration 185/1000 | Loss: 0.00755644
Iteration 186/1000 | Loss: 0.00820529
Iteration 187/1000 | Loss: 0.00783217
Iteration 188/1000 | Loss: 0.00830294
Iteration 189/1000 | Loss: 0.01272077
Iteration 190/1000 | Loss: 0.01276186
Iteration 191/1000 | Loss: 0.00725639
Iteration 192/1000 | Loss: 0.00726702
Iteration 193/1000 | Loss: 0.00839334
Iteration 194/1000 | Loss: 0.00580867
Iteration 195/1000 | Loss: 0.00787694
Iteration 196/1000 | Loss: 0.00832986
Iteration 197/1000 | Loss: 0.01358490
Iteration 198/1000 | Loss: 0.01021030
Iteration 199/1000 | Loss: 0.01240714
Iteration 200/1000 | Loss: 0.01129230
Iteration 201/1000 | Loss: 0.01097486
Iteration 202/1000 | Loss: 0.00914987
Iteration 203/1000 | Loss: 0.00592361
Iteration 204/1000 | Loss: 0.00571262
Iteration 205/1000 | Loss: 0.00861524
Iteration 206/1000 | Loss: 0.00985373
Iteration 207/1000 | Loss: 0.00826347
Iteration 208/1000 | Loss: 0.00906737
Iteration 209/1000 | Loss: 0.00732855
Iteration 210/1000 | Loss: 0.00572032
Iteration 211/1000 | Loss: 0.00763601
Iteration 212/1000 | Loss: 0.00891249
Iteration 213/1000 | Loss: 0.00661689
Iteration 214/1000 | Loss: 0.00643870
Iteration 215/1000 | Loss: 0.00877413
Iteration 216/1000 | Loss: 0.00875372
Iteration 217/1000 | Loss: 0.00698094
Iteration 218/1000 | Loss: 0.00910837
Iteration 219/1000 | Loss: 0.00559093
Iteration 220/1000 | Loss: 0.00576692
Iteration 221/1000 | Loss: 0.00668561
Iteration 222/1000 | Loss: 0.00670926
Iteration 223/1000 | Loss: 0.00545057
Iteration 224/1000 | Loss: 0.00560740
Iteration 225/1000 | Loss: 0.00954696
Iteration 226/1000 | Loss: 0.00688625
Iteration 227/1000 | Loss: 0.00770913
Iteration 228/1000 | Loss: 0.00973190
Iteration 229/1000 | Loss: 0.00672500
Iteration 230/1000 | Loss: 0.00691723
Iteration 231/1000 | Loss: 0.00740611
Iteration 232/1000 | Loss: 0.00733075
Iteration 233/1000 | Loss: 0.01042855
Iteration 234/1000 | Loss: 0.00723807
Iteration 235/1000 | Loss: 0.00547795
Iteration 236/1000 | Loss: 0.00656587
Iteration 237/1000 | Loss: 0.00549749
Iteration 238/1000 | Loss: 0.00796994
Iteration 239/1000 | Loss: 0.00678867
Iteration 240/1000 | Loss: 0.00582286
Iteration 241/1000 | Loss: 0.00499934
Iteration 242/1000 | Loss: 0.00459679
Iteration 243/1000 | Loss: 0.00460624
Iteration 244/1000 | Loss: 0.00827129
Iteration 245/1000 | Loss: 0.00721476
Iteration 246/1000 | Loss: 0.00449133
Iteration 247/1000 | Loss: 0.00441816
Iteration 248/1000 | Loss: 0.00814862
Iteration 249/1000 | Loss: 0.00479439
Iteration 250/1000 | Loss: 0.00458944
Iteration 251/1000 | Loss: 0.00353821
Iteration 252/1000 | Loss: 0.00708036
Iteration 253/1000 | Loss: 0.00485772
Iteration 254/1000 | Loss: 0.00537703
Iteration 255/1000 | Loss: 0.00374905
Iteration 256/1000 | Loss: 0.00742741
Iteration 257/1000 | Loss: 0.00421051
Iteration 258/1000 | Loss: 0.00714512
Iteration 259/1000 | Loss: 0.00508108
Iteration 260/1000 | Loss: 0.00610888
Iteration 261/1000 | Loss: 0.00495463
Iteration 262/1000 | Loss: 0.00701745
Iteration 263/1000 | Loss: 0.00714858
Iteration 264/1000 | Loss: 0.00596179
Iteration 265/1000 | Loss: 0.00477319
Iteration 266/1000 | Loss: 0.00917955
Iteration 267/1000 | Loss: 0.00788688
Iteration 268/1000 | Loss: 0.00519478
Iteration 269/1000 | Loss: 0.00464807
Iteration 270/1000 | Loss: 0.00640066
Iteration 271/1000 | Loss: 0.00487322
Iteration 272/1000 | Loss: 0.00632549
Iteration 273/1000 | Loss: 0.00723874
Iteration 274/1000 | Loss: 0.00515705
Iteration 275/1000 | Loss: 0.00356243
Iteration 276/1000 | Loss: 0.00150563
Iteration 277/1000 | Loss: 0.00247173
Iteration 278/1000 | Loss: 0.00146662
Iteration 279/1000 | Loss: 0.00164224
Iteration 280/1000 | Loss: 0.00232185
Iteration 281/1000 | Loss: 0.00223935
Iteration 282/1000 | Loss: 0.00266186
Iteration 283/1000 | Loss: 0.00159803
Iteration 284/1000 | Loss: 0.00272352
Iteration 285/1000 | Loss: 0.00153424
Iteration 286/1000 | Loss: 0.00173664
Iteration 287/1000 | Loss: 0.00136088
Iteration 288/1000 | Loss: 0.00193009
Iteration 289/1000 | Loss: 0.00331884
Iteration 290/1000 | Loss: 0.00240076
Iteration 291/1000 | Loss: 0.00218178
Iteration 292/1000 | Loss: 0.00305835
Iteration 293/1000 | Loss: 0.00178422
Iteration 294/1000 | Loss: 0.00141383
Iteration 295/1000 | Loss: 0.00179069
Iteration 296/1000 | Loss: 0.00381672
Iteration 297/1000 | Loss: 0.00107198
Iteration 298/1000 | Loss: 0.00055126
Iteration 299/1000 | Loss: 0.00115836
Iteration 300/1000 | Loss: 0.00074911
Iteration 301/1000 | Loss: 0.00177191
Iteration 302/1000 | Loss: 0.00215389
Iteration 303/1000 | Loss: 0.00264977
Iteration 304/1000 | Loss: 0.00257572
Iteration 305/1000 | Loss: 0.00223989
Iteration 306/1000 | Loss: 0.00071736
Iteration 307/1000 | Loss: 0.00480878
Iteration 308/1000 | Loss: 0.00161352
Iteration 309/1000 | Loss: 0.00151362
Iteration 310/1000 | Loss: 0.00151415
Iteration 311/1000 | Loss: 0.00098609
Iteration 312/1000 | Loss: 0.00210836
Iteration 313/1000 | Loss: 0.00280200
Iteration 314/1000 | Loss: 0.00147230
Iteration 315/1000 | Loss: 0.00081593
Iteration 316/1000 | Loss: 0.00170438
Iteration 317/1000 | Loss: 0.00084676
Iteration 318/1000 | Loss: 0.00145744
Iteration 319/1000 | Loss: 0.00248577
Iteration 320/1000 | Loss: 0.00072644
Iteration 321/1000 | Loss: 0.00183682
Iteration 322/1000 | Loss: 0.00239968
Iteration 323/1000 | Loss: 0.00095648
Iteration 324/1000 | Loss: 0.00075181
Iteration 325/1000 | Loss: 0.00031951
Iteration 326/1000 | Loss: 0.00065468
Iteration 327/1000 | Loss: 0.00021118
Iteration 328/1000 | Loss: 0.00020773
Iteration 329/1000 | Loss: 0.00274421
Iteration 330/1000 | Loss: 0.00149125
Iteration 331/1000 | Loss: 0.00112021
Iteration 332/1000 | Loss: 0.00172658
Iteration 333/1000 | Loss: 0.00054930
Iteration 334/1000 | Loss: 0.00103660
Iteration 335/1000 | Loss: 0.00076356
Iteration 336/1000 | Loss: 0.00061797
Iteration 337/1000 | Loss: 0.00056775
Iteration 338/1000 | Loss: 0.00122375
Iteration 339/1000 | Loss: 0.00182544
Iteration 340/1000 | Loss: 0.00303658
Iteration 341/1000 | Loss: 0.00168526
Iteration 342/1000 | Loss: 0.00153879
Iteration 343/1000 | Loss: 0.00070700
Iteration 344/1000 | Loss: 0.00116682
Iteration 345/1000 | Loss: 0.00081944
Iteration 346/1000 | Loss: 0.00089052
Iteration 347/1000 | Loss: 0.00072389
Iteration 348/1000 | Loss: 0.00062627
Iteration 349/1000 | Loss: 0.00050551
Iteration 350/1000 | Loss: 0.00068946
Iteration 351/1000 | Loss: 0.00053723
Iteration 352/1000 | Loss: 0.00093914
Iteration 353/1000 | Loss: 0.00074081
Iteration 354/1000 | Loss: 0.00060855
Iteration 355/1000 | Loss: 0.00028589
Iteration 356/1000 | Loss: 0.00023963
Iteration 357/1000 | Loss: 0.00070224
Iteration 358/1000 | Loss: 0.00018421
Iteration 359/1000 | Loss: 0.00016552
Iteration 360/1000 | Loss: 0.00063906
Iteration 361/1000 | Loss: 0.00216189
Iteration 362/1000 | Loss: 0.00041050
Iteration 363/1000 | Loss: 0.00031993
Iteration 364/1000 | Loss: 0.00011602
Iteration 365/1000 | Loss: 0.00010908
Iteration 366/1000 | Loss: 0.00028200
Iteration 367/1000 | Loss: 0.00193359
Iteration 368/1000 | Loss: 0.00147421
Iteration 369/1000 | Loss: 0.00162106
Iteration 370/1000 | Loss: 0.00171900
Iteration 371/1000 | Loss: 0.00124433
Iteration 372/1000 | Loss: 0.00063439
Iteration 373/1000 | Loss: 0.00051728
Iteration 374/1000 | Loss: 0.00135903
Iteration 375/1000 | Loss: 0.00140932
Iteration 376/1000 | Loss: 0.00016325
Iteration 377/1000 | Loss: 0.00033197
Iteration 378/1000 | Loss: 0.00036823
Iteration 379/1000 | Loss: 0.00032968
Iteration 380/1000 | Loss: 0.00031849
Iteration 381/1000 | Loss: 0.00026498
Iteration 382/1000 | Loss: 0.00059158
Iteration 383/1000 | Loss: 0.00046705
Iteration 384/1000 | Loss: 0.00076176
Iteration 385/1000 | Loss: 0.00103158
Iteration 386/1000 | Loss: 0.00128764
Iteration 387/1000 | Loss: 0.00044719
Iteration 388/1000 | Loss: 0.00028987
Iteration 389/1000 | Loss: 0.00040641
Iteration 390/1000 | Loss: 0.00036553
Iteration 391/1000 | Loss: 0.00346905
Iteration 392/1000 | Loss: 0.00027822
Iteration 393/1000 | Loss: 0.00018855
Iteration 394/1000 | Loss: 0.00031645
Iteration 395/1000 | Loss: 0.00037496
Iteration 396/1000 | Loss: 0.00057014
Iteration 397/1000 | Loss: 0.00027838
Iteration 398/1000 | Loss: 0.00010229
Iteration 399/1000 | Loss: 0.00010811
Iteration 400/1000 | Loss: 0.00010010
Iteration 401/1000 | Loss: 0.00008935
Iteration 402/1000 | Loss: 0.00031124
Iteration 403/1000 | Loss: 0.00084154
Iteration 404/1000 | Loss: 0.00042336
Iteration 405/1000 | Loss: 0.00066203
Iteration 406/1000 | Loss: 0.00017413
Iteration 407/1000 | Loss: 0.00010755
Iteration 408/1000 | Loss: 0.00014411
Iteration 409/1000 | Loss: 0.00008556
Iteration 410/1000 | Loss: 0.00008317
Iteration 411/1000 | Loss: 0.00008058
Iteration 412/1000 | Loss: 0.00007851
Iteration 413/1000 | Loss: 0.00007718
Iteration 414/1000 | Loss: 0.00049692
Iteration 415/1000 | Loss: 0.00256191
Iteration 416/1000 | Loss: 0.00576004
Iteration 417/1000 | Loss: 0.00162582
Iteration 418/1000 | Loss: 0.00293878
Iteration 419/1000 | Loss: 0.00402444
Iteration 420/1000 | Loss: 0.00478392
Iteration 421/1000 | Loss: 0.00148650
Iteration 422/1000 | Loss: 0.00312753
Iteration 423/1000 | Loss: 0.00148692
Iteration 424/1000 | Loss: 0.00149247
Iteration 425/1000 | Loss: 0.00234872
Iteration 426/1000 | Loss: 0.00114705
Iteration 427/1000 | Loss: 0.00600016
Iteration 428/1000 | Loss: 0.00348594
Iteration 429/1000 | Loss: 0.00139242
Iteration 430/1000 | Loss: 0.00247767
Iteration 431/1000 | Loss: 0.00303021
Iteration 432/1000 | Loss: 0.00153416
Iteration 433/1000 | Loss: 0.00099884
Iteration 434/1000 | Loss: 0.00039074
Iteration 435/1000 | Loss: 0.00011215
Iteration 436/1000 | Loss: 0.00016960
Iteration 437/1000 | Loss: 0.00009444
Iteration 438/1000 | Loss: 0.00035684
Iteration 439/1000 | Loss: 0.00011985
Iteration 440/1000 | Loss: 0.00012425
Iteration 441/1000 | Loss: 0.00008166
Iteration 442/1000 | Loss: 0.00007433
Iteration 443/1000 | Loss: 0.00047453
Iteration 444/1000 | Loss: 0.00028270
Iteration 445/1000 | Loss: 0.00025885
Iteration 446/1000 | Loss: 0.00160888
Iteration 447/1000 | Loss: 0.00006841
Iteration 448/1000 | Loss: 0.00006547
Iteration 449/1000 | Loss: 0.00025048
Iteration 450/1000 | Loss: 0.00035932
Iteration 451/1000 | Loss: 0.00045982
Iteration 452/1000 | Loss: 0.00022179
Iteration 453/1000 | Loss: 0.00023188
Iteration 454/1000 | Loss: 0.00032526
Iteration 455/1000 | Loss: 0.00020207
Iteration 456/1000 | Loss: 0.00006610
Iteration 457/1000 | Loss: 0.00011937
Iteration 458/1000 | Loss: 0.00006209
Iteration 459/1000 | Loss: 0.00005880
Iteration 460/1000 | Loss: 0.00038266
Iteration 461/1000 | Loss: 0.00084714
Iteration 462/1000 | Loss: 0.00044960
Iteration 463/1000 | Loss: 0.00030997
Iteration 464/1000 | Loss: 0.00032534
Iteration 465/1000 | Loss: 0.00052798
Iteration 466/1000 | Loss: 0.00019270
Iteration 467/1000 | Loss: 0.00049443
Iteration 468/1000 | Loss: 0.00018030
Iteration 469/1000 | Loss: 0.00019813
Iteration 470/1000 | Loss: 0.00015310
Iteration 471/1000 | Loss: 0.00033551
Iteration 472/1000 | Loss: 0.00015822
Iteration 473/1000 | Loss: 0.00016237
Iteration 474/1000 | Loss: 0.00014117
Iteration 475/1000 | Loss: 0.00012752
Iteration 476/1000 | Loss: 0.00006248
Iteration 477/1000 | Loss: 0.00011609
Iteration 478/1000 | Loss: 0.00005305
Iteration 479/1000 | Loss: 0.00005109
Iteration 480/1000 | Loss: 0.00020241
Iteration 481/1000 | Loss: 0.00052681
Iteration 482/1000 | Loss: 0.00015971
Iteration 483/1000 | Loss: 0.00026704
Iteration 484/1000 | Loss: 0.00013106
Iteration 485/1000 | Loss: 0.00006181
Iteration 486/1000 | Loss: 0.00005371
Iteration 487/1000 | Loss: 0.00004841
Iteration 488/1000 | Loss: 0.00011416
Iteration 489/1000 | Loss: 0.00006484
Iteration 490/1000 | Loss: 0.00024463
Iteration 491/1000 | Loss: 0.00007583
Iteration 492/1000 | Loss: 0.00028382
Iteration 493/1000 | Loss: 0.00013088
Iteration 494/1000 | Loss: 0.00010995
Iteration 495/1000 | Loss: 0.00004881
Iteration 496/1000 | Loss: 0.00019472
Iteration 497/1000 | Loss: 0.00007750
Iteration 498/1000 | Loss: 0.00006408
Iteration 499/1000 | Loss: 0.00024698
Iteration 500/1000 | Loss: 0.00006525
Iteration 501/1000 | Loss: 0.00006797
Iteration 502/1000 | Loss: 0.00004629
Iteration 503/1000 | Loss: 0.00015008
Iteration 504/1000 | Loss: 0.00004574
Iteration 505/1000 | Loss: 0.00004522
Iteration 506/1000 | Loss: 0.00004483
Iteration 507/1000 | Loss: 0.00004444
Iteration 508/1000 | Loss: 0.00038058
Iteration 509/1000 | Loss: 0.00094851
Iteration 510/1000 | Loss: 0.00015951
Iteration 511/1000 | Loss: 0.00019775
Iteration 512/1000 | Loss: 0.00005127
Iteration 513/1000 | Loss: 0.00017734
Iteration 514/1000 | Loss: 0.00004519
Iteration 515/1000 | Loss: 0.00004316
Iteration 516/1000 | Loss: 0.00017782
Iteration 517/1000 | Loss: 0.00004103
Iteration 518/1000 | Loss: 0.00003990
Iteration 519/1000 | Loss: 0.00003940
Iteration 520/1000 | Loss: 0.00014656
Iteration 521/1000 | Loss: 0.00003926
Iteration 522/1000 | Loss: 0.00003845
Iteration 523/1000 | Loss: 0.00003840
Iteration 524/1000 | Loss: 0.00003835
Iteration 525/1000 | Loss: 0.00017747
Iteration 526/1000 | Loss: 0.00003880
Iteration 527/1000 | Loss: 0.00003821
Iteration 528/1000 | Loss: 0.00004230
Iteration 529/1000 | Loss: 0.00003837
Iteration 530/1000 | Loss: 0.00003888
Iteration 531/1000 | Loss: 0.00003800
Iteration 532/1000 | Loss: 0.00003798
Iteration 533/1000 | Loss: 0.00003798
Iteration 534/1000 | Loss: 0.00003798
Iteration 535/1000 | Loss: 0.00003798
Iteration 536/1000 | Loss: 0.00003798
Iteration 537/1000 | Loss: 0.00003798
Iteration 538/1000 | Loss: 0.00003798
Iteration 539/1000 | Loss: 0.00003790
Iteration 540/1000 | Loss: 0.00003768
Iteration 541/1000 | Loss: 0.00003760
Iteration 542/1000 | Loss: 0.00003759
Iteration 543/1000 | Loss: 0.00003759
Iteration 544/1000 | Loss: 0.00045238
Iteration 545/1000 | Loss: 0.00035634
Iteration 546/1000 | Loss: 0.00005021
Iteration 547/1000 | Loss: 0.00003899
Iteration 548/1000 | Loss: 0.00003939
Iteration 549/1000 | Loss: 0.00014048
Iteration 550/1000 | Loss: 0.00004061
Iteration 551/1000 | Loss: 0.00003828
Iteration 552/1000 | Loss: 0.00003764
Iteration 553/1000 | Loss: 0.00003750
Iteration 554/1000 | Loss: 0.00003749
Iteration 555/1000 | Loss: 0.00003749
Iteration 556/1000 | Loss: 0.00003748
Iteration 557/1000 | Loss: 0.00003747
Iteration 558/1000 | Loss: 0.00003745
Iteration 559/1000 | Loss: 0.00042080
Iteration 560/1000 | Loss: 0.00031774
Iteration 561/1000 | Loss: 0.00004056
Iteration 562/1000 | Loss: 0.00003844
Iteration 563/1000 | Loss: 0.00003891
Iteration 564/1000 | Loss: 0.00016045
Iteration 565/1000 | Loss: 0.00003798
Iteration 566/1000 | Loss: 0.00043689
Iteration 567/1000 | Loss: 0.00035721
Iteration 568/1000 | Loss: 0.00005501
Iteration 569/1000 | Loss: 0.00005385
Iteration 570/1000 | Loss: 0.00004027
Iteration 571/1000 | Loss: 0.00003864
Iteration 572/1000 | Loss: 0.00003810
Iteration 573/1000 | Loss: 0.00016624
Iteration 574/1000 | Loss: 0.00005111
Iteration 575/1000 | Loss: 0.00003836
Iteration 576/1000 | Loss: 0.00003780
Iteration 577/1000 | Loss: 0.00033670
Iteration 578/1000 | Loss: 0.00005010
Iteration 579/1000 | Loss: 0.00003808
Iteration 580/1000 | Loss: 0.00015913
Iteration 581/1000 | Loss: 0.00007268
Iteration 582/1000 | Loss: 0.00004201
Iteration 583/1000 | Loss: 0.00015028
Iteration 584/1000 | Loss: 0.00014511
Iteration 585/1000 | Loss: 0.00003752
Iteration 586/1000 | Loss: 0.00003667
Iteration 587/1000 | Loss: 0.00003618
Iteration 588/1000 | Loss: 0.00003578
Iteration 589/1000 | Loss: 0.00003547
Iteration 590/1000 | Loss: 0.00003534
Iteration 591/1000 | Loss: 0.00003533
Iteration 592/1000 | Loss: 0.00003532
Iteration 593/1000 | Loss: 0.00003529
Iteration 594/1000 | Loss: 0.00003523
Iteration 595/1000 | Loss: 0.00003523
Iteration 596/1000 | Loss: 0.00003523
Iteration 597/1000 | Loss: 0.00003521
Iteration 598/1000 | Loss: 0.00003521
Iteration 599/1000 | Loss: 0.00003521
Iteration 600/1000 | Loss: 0.00003521
Iteration 601/1000 | Loss: 0.00003520
Iteration 602/1000 | Loss: 0.00003520
Iteration 603/1000 | Loss: 0.00003520
Iteration 604/1000 | Loss: 0.00003520
Iteration 605/1000 | Loss: 0.00003520
Iteration 606/1000 | Loss: 0.00003520
Iteration 607/1000 | Loss: 0.00003520
Iteration 608/1000 | Loss: 0.00003520
Iteration 609/1000 | Loss: 0.00003520
Iteration 610/1000 | Loss: 0.00003518
Iteration 611/1000 | Loss: 0.00003518
Iteration 612/1000 | Loss: 0.00003518
Iteration 613/1000 | Loss: 0.00003517
Iteration 614/1000 | Loss: 0.00003516
Iteration 615/1000 | Loss: 0.00003516
Iteration 616/1000 | Loss: 0.00003515
Iteration 617/1000 | Loss: 0.00003515
Iteration 618/1000 | Loss: 0.00003514
Iteration 619/1000 | Loss: 0.00003514
Iteration 620/1000 | Loss: 0.00003514
Iteration 621/1000 | Loss: 0.00003513
Iteration 622/1000 | Loss: 0.00003513
Iteration 623/1000 | Loss: 0.00003513
Iteration 624/1000 | Loss: 0.00003512
Iteration 625/1000 | Loss: 0.00003512
Iteration 626/1000 | Loss: 0.00003512
Iteration 627/1000 | Loss: 0.00003512
Iteration 628/1000 | Loss: 0.00003511
Iteration 629/1000 | Loss: 0.00003511
Iteration 630/1000 | Loss: 0.00003511
Iteration 631/1000 | Loss: 0.00003510
Iteration 632/1000 | Loss: 0.00003510
Iteration 633/1000 | Loss: 0.00003510
Iteration 634/1000 | Loss: 0.00003510
Iteration 635/1000 | Loss: 0.00003509
Iteration 636/1000 | Loss: 0.00003509
Iteration 637/1000 | Loss: 0.00003509
Iteration 638/1000 | Loss: 0.00003509
Iteration 639/1000 | Loss: 0.00003509
Iteration 640/1000 | Loss: 0.00003509
Iteration 641/1000 | Loss: 0.00003509
Iteration 642/1000 | Loss: 0.00003509
Iteration 643/1000 | Loss: 0.00003509
Iteration 644/1000 | Loss: 0.00003509
Iteration 645/1000 | Loss: 0.00003509
Iteration 646/1000 | Loss: 0.00003509
Iteration 647/1000 | Loss: 0.00003509
Iteration 648/1000 | Loss: 0.00003509
Iteration 649/1000 | Loss: 0.00003509
Iteration 650/1000 | Loss: 0.00003509
Iteration 651/1000 | Loss: 0.00003509
Iteration 652/1000 | Loss: 0.00003509
Iteration 653/1000 | Loss: 0.00003509
Iteration 654/1000 | Loss: 0.00003509
Iteration 655/1000 | Loss: 0.00003509
Iteration 656/1000 | Loss: 0.00003509
Iteration 657/1000 | Loss: 0.00003509
Iteration 658/1000 | Loss: 0.00003509
Iteration 659/1000 | Loss: 0.00003509
Iteration 660/1000 | Loss: 0.00003509
Iteration 661/1000 | Loss: 0.00003509
Iteration 662/1000 | Loss: 0.00003509
Iteration 663/1000 | Loss: 0.00003509
Iteration 664/1000 | Loss: 0.00003509
Iteration 665/1000 | Loss: 0.00003509
Iteration 666/1000 | Loss: 0.00003509
Iteration 667/1000 | Loss: 0.00003509
Iteration 668/1000 | Loss: 0.00003509
Iteration 669/1000 | Loss: 0.00003509
Iteration 670/1000 | Loss: 0.00003509
Iteration 671/1000 | Loss: 0.00003509
Iteration 672/1000 | Loss: 0.00003509
Iteration 673/1000 | Loss: 0.00003509
Iteration 674/1000 | Loss: 0.00003509
Iteration 675/1000 | Loss: 0.00003509
Iteration 676/1000 | Loss: 0.00003509
Iteration 677/1000 | Loss: 0.00003509
Iteration 678/1000 | Loss: 0.00003509
Iteration 679/1000 | Loss: 0.00003509
Iteration 680/1000 | Loss: 0.00003509
Iteration 681/1000 | Loss: 0.00003509
Iteration 682/1000 | Loss: 0.00003509
Iteration 683/1000 | Loss: 0.00003509
Iteration 684/1000 | Loss: 0.00003509
Iteration 685/1000 | Loss: 0.00003509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 685. Stopping optimization.
Last 5 losses: [3.508833469823003e-05, 3.508833469823003e-05, 3.508833469823003e-05, 3.508833469823003e-05, 3.508833469823003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.508833469823003e-05

Optimization complete. Final v2v error: 3.65453839302063 mm

Highest mean error: 13.014041900634766 mm for frame 96

Lowest mean error: 2.4527323246002197 mm for frame 8

Saving results

Total time: 972.0458624362946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376660
Iteration 2/25 | Loss: 0.00066092
Iteration 3/25 | Loss: 0.00054114
Iteration 4/25 | Loss: 0.00052608
Iteration 5/25 | Loss: 0.00052136
Iteration 6/25 | Loss: 0.00051979
Iteration 7/25 | Loss: 0.00051946
Iteration 8/25 | Loss: 0.00051946
Iteration 9/25 | Loss: 0.00051946
Iteration 10/25 | Loss: 0.00051946
Iteration 11/25 | Loss: 0.00051946
Iteration 12/25 | Loss: 0.00051946
Iteration 13/25 | Loss: 0.00051946
Iteration 14/25 | Loss: 0.00051946
Iteration 15/25 | Loss: 0.00051946
Iteration 16/25 | Loss: 0.00051946
Iteration 17/25 | Loss: 0.00051946
Iteration 18/25 | Loss: 0.00051946
Iteration 19/25 | Loss: 0.00051946
Iteration 20/25 | Loss: 0.00051946
Iteration 21/25 | Loss: 0.00051946
Iteration 22/25 | Loss: 0.00051946
Iteration 23/25 | Loss: 0.00051946
Iteration 24/25 | Loss: 0.00051946
Iteration 25/25 | Loss: 0.00051946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46834910
Iteration 2/25 | Loss: 0.00017347
Iteration 3/25 | Loss: 0.00017346
Iteration 4/25 | Loss: 0.00017346
Iteration 5/25 | Loss: 0.00017346
Iteration 6/25 | Loss: 0.00017346
Iteration 7/25 | Loss: 0.00017346
Iteration 8/25 | Loss: 0.00017346
Iteration 9/25 | Loss: 0.00017346
Iteration 10/25 | Loss: 0.00017346
Iteration 11/25 | Loss: 0.00017346
Iteration 12/25 | Loss: 0.00017346
Iteration 13/25 | Loss: 0.00017346
Iteration 14/25 | Loss: 0.00017346
Iteration 15/25 | Loss: 0.00017346
Iteration 16/25 | Loss: 0.00017346
Iteration 17/25 | Loss: 0.00017346
Iteration 18/25 | Loss: 0.00017346
Iteration 19/25 | Loss: 0.00017346
Iteration 20/25 | Loss: 0.00017346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00017346025560982525, 0.00017346025560982525, 0.00017346025560982525, 0.00017346025560982525, 0.00017346025560982525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017346025560982525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017346
Iteration 2/1000 | Loss: 0.00001466
Iteration 3/1000 | Loss: 0.00000979
Iteration 4/1000 | Loss: 0.00000894
Iteration 5/1000 | Loss: 0.00000847
Iteration 6/1000 | Loss: 0.00000824
Iteration 7/1000 | Loss: 0.00000823
Iteration 8/1000 | Loss: 0.00000822
Iteration 9/1000 | Loss: 0.00000822
Iteration 10/1000 | Loss: 0.00000815
Iteration 11/1000 | Loss: 0.00000813
Iteration 12/1000 | Loss: 0.00000809
Iteration 13/1000 | Loss: 0.00000809
Iteration 14/1000 | Loss: 0.00000808
Iteration 15/1000 | Loss: 0.00000808
Iteration 16/1000 | Loss: 0.00000808
Iteration 17/1000 | Loss: 0.00000808
Iteration 18/1000 | Loss: 0.00000807
Iteration 19/1000 | Loss: 0.00000807
Iteration 20/1000 | Loss: 0.00000807
Iteration 21/1000 | Loss: 0.00000806
Iteration 22/1000 | Loss: 0.00000805
Iteration 23/1000 | Loss: 0.00000805
Iteration 24/1000 | Loss: 0.00000804
Iteration 25/1000 | Loss: 0.00000804
Iteration 26/1000 | Loss: 0.00000804
Iteration 27/1000 | Loss: 0.00000803
Iteration 28/1000 | Loss: 0.00000803
Iteration 29/1000 | Loss: 0.00000802
Iteration 30/1000 | Loss: 0.00000802
Iteration 31/1000 | Loss: 0.00000802
Iteration 32/1000 | Loss: 0.00000802
Iteration 33/1000 | Loss: 0.00000802
Iteration 34/1000 | Loss: 0.00000801
Iteration 35/1000 | Loss: 0.00000801
Iteration 36/1000 | Loss: 0.00000801
Iteration 37/1000 | Loss: 0.00000801
Iteration 38/1000 | Loss: 0.00000801
Iteration 39/1000 | Loss: 0.00000801
Iteration 40/1000 | Loss: 0.00000801
Iteration 41/1000 | Loss: 0.00000801
Iteration 42/1000 | Loss: 0.00000800
Iteration 43/1000 | Loss: 0.00000798
Iteration 44/1000 | Loss: 0.00000798
Iteration 45/1000 | Loss: 0.00000798
Iteration 46/1000 | Loss: 0.00000798
Iteration 47/1000 | Loss: 0.00000798
Iteration 48/1000 | Loss: 0.00000798
Iteration 49/1000 | Loss: 0.00000798
Iteration 50/1000 | Loss: 0.00000797
Iteration 51/1000 | Loss: 0.00000797
Iteration 52/1000 | Loss: 0.00000797
Iteration 53/1000 | Loss: 0.00000796
Iteration 54/1000 | Loss: 0.00000796
Iteration 55/1000 | Loss: 0.00000795
Iteration 56/1000 | Loss: 0.00000795
Iteration 57/1000 | Loss: 0.00000794
Iteration 58/1000 | Loss: 0.00000794
Iteration 59/1000 | Loss: 0.00000793
Iteration 60/1000 | Loss: 0.00000793
Iteration 61/1000 | Loss: 0.00000793
Iteration 62/1000 | Loss: 0.00000792
Iteration 63/1000 | Loss: 0.00000792
Iteration 64/1000 | Loss: 0.00000792
Iteration 65/1000 | Loss: 0.00000792
Iteration 66/1000 | Loss: 0.00000792
Iteration 67/1000 | Loss: 0.00000791
Iteration 68/1000 | Loss: 0.00000791
Iteration 69/1000 | Loss: 0.00000791
Iteration 70/1000 | Loss: 0.00000791
Iteration 71/1000 | Loss: 0.00000791
Iteration 72/1000 | Loss: 0.00000791
Iteration 73/1000 | Loss: 0.00000791
Iteration 74/1000 | Loss: 0.00000791
Iteration 75/1000 | Loss: 0.00000790
Iteration 76/1000 | Loss: 0.00000790
Iteration 77/1000 | Loss: 0.00000789
Iteration 78/1000 | Loss: 0.00000789
Iteration 79/1000 | Loss: 0.00000789
Iteration 80/1000 | Loss: 0.00000788
Iteration 81/1000 | Loss: 0.00000788
Iteration 82/1000 | Loss: 0.00000788
Iteration 83/1000 | Loss: 0.00000787
Iteration 84/1000 | Loss: 0.00000787
Iteration 85/1000 | Loss: 0.00000787
Iteration 86/1000 | Loss: 0.00000787
Iteration 87/1000 | Loss: 0.00000787
Iteration 88/1000 | Loss: 0.00000787
Iteration 89/1000 | Loss: 0.00000786
Iteration 90/1000 | Loss: 0.00000786
Iteration 91/1000 | Loss: 0.00000786
Iteration 92/1000 | Loss: 0.00000786
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000785
Iteration 96/1000 | Loss: 0.00000785
Iteration 97/1000 | Loss: 0.00000785
Iteration 98/1000 | Loss: 0.00000784
Iteration 99/1000 | Loss: 0.00000784
Iteration 100/1000 | Loss: 0.00000784
Iteration 101/1000 | Loss: 0.00000784
Iteration 102/1000 | Loss: 0.00000784
Iteration 103/1000 | Loss: 0.00000784
Iteration 104/1000 | Loss: 0.00000784
Iteration 105/1000 | Loss: 0.00000784
Iteration 106/1000 | Loss: 0.00000783
Iteration 107/1000 | Loss: 0.00000783
Iteration 108/1000 | Loss: 0.00000783
Iteration 109/1000 | Loss: 0.00000782
Iteration 110/1000 | Loss: 0.00000782
Iteration 111/1000 | Loss: 0.00000782
Iteration 112/1000 | Loss: 0.00000782
Iteration 113/1000 | Loss: 0.00000782
Iteration 114/1000 | Loss: 0.00000781
Iteration 115/1000 | Loss: 0.00000781
Iteration 116/1000 | Loss: 0.00000781
Iteration 117/1000 | Loss: 0.00000780
Iteration 118/1000 | Loss: 0.00000780
Iteration 119/1000 | Loss: 0.00000779
Iteration 120/1000 | Loss: 0.00000779
Iteration 121/1000 | Loss: 0.00000779
Iteration 122/1000 | Loss: 0.00000779
Iteration 123/1000 | Loss: 0.00000779
Iteration 124/1000 | Loss: 0.00000779
Iteration 125/1000 | Loss: 0.00000779
Iteration 126/1000 | Loss: 0.00000779
Iteration 127/1000 | Loss: 0.00000778
Iteration 128/1000 | Loss: 0.00000778
Iteration 129/1000 | Loss: 0.00000778
Iteration 130/1000 | Loss: 0.00000777
Iteration 131/1000 | Loss: 0.00000777
Iteration 132/1000 | Loss: 0.00000777
Iteration 133/1000 | Loss: 0.00000777
Iteration 134/1000 | Loss: 0.00000777
Iteration 135/1000 | Loss: 0.00000777
Iteration 136/1000 | Loss: 0.00000776
Iteration 137/1000 | Loss: 0.00000776
Iteration 138/1000 | Loss: 0.00000776
Iteration 139/1000 | Loss: 0.00000776
Iteration 140/1000 | Loss: 0.00000775
Iteration 141/1000 | Loss: 0.00000775
Iteration 142/1000 | Loss: 0.00000775
Iteration 143/1000 | Loss: 0.00000775
Iteration 144/1000 | Loss: 0.00000775
Iteration 145/1000 | Loss: 0.00000774
Iteration 146/1000 | Loss: 0.00000774
Iteration 147/1000 | Loss: 0.00000774
Iteration 148/1000 | Loss: 0.00000774
Iteration 149/1000 | Loss: 0.00000774
Iteration 150/1000 | Loss: 0.00000774
Iteration 151/1000 | Loss: 0.00000774
Iteration 152/1000 | Loss: 0.00000774
Iteration 153/1000 | Loss: 0.00000774
Iteration 154/1000 | Loss: 0.00000774
Iteration 155/1000 | Loss: 0.00000774
Iteration 156/1000 | Loss: 0.00000774
Iteration 157/1000 | Loss: 0.00000774
Iteration 158/1000 | Loss: 0.00000774
Iteration 159/1000 | Loss: 0.00000774
Iteration 160/1000 | Loss: 0.00000774
Iteration 161/1000 | Loss: 0.00000774
Iteration 162/1000 | Loss: 0.00000774
Iteration 163/1000 | Loss: 0.00000774
Iteration 164/1000 | Loss: 0.00000774
Iteration 165/1000 | Loss: 0.00000774
Iteration 166/1000 | Loss: 0.00000774
Iteration 167/1000 | Loss: 0.00000774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [7.74011277826503e-06, 7.74011277826503e-06, 7.74011277826503e-06, 7.74011277826503e-06, 7.74011277826503e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.74011277826503e-06

Optimization complete. Final v2v error: 2.3804256916046143 mm

Highest mean error: 2.582075595855713 mm for frame 77

Lowest mean error: 2.3044240474700928 mm for frame 130

Saving results

Total time: 31.148176431655884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372754
Iteration 2/25 | Loss: 0.00085777
Iteration 3/25 | Loss: 0.00067056
Iteration 4/25 | Loss: 0.00063116
Iteration 5/25 | Loss: 0.00061867
Iteration 6/25 | Loss: 0.00061414
Iteration 7/25 | Loss: 0.00061328
Iteration 8/25 | Loss: 0.00061328
Iteration 9/25 | Loss: 0.00061328
Iteration 10/25 | Loss: 0.00061328
Iteration 11/25 | Loss: 0.00061328
Iteration 12/25 | Loss: 0.00061328
Iteration 13/25 | Loss: 0.00061328
Iteration 14/25 | Loss: 0.00061328
Iteration 15/25 | Loss: 0.00061328
Iteration 16/25 | Loss: 0.00061328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006132792332209647, 0.0006132792332209647, 0.0006132792332209647, 0.0006132792332209647, 0.0006132792332209647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006132792332209647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92094821
Iteration 2/25 | Loss: 0.00024556
Iteration 3/25 | Loss: 0.00024556
Iteration 4/25 | Loss: 0.00024556
Iteration 5/25 | Loss: 0.00024556
Iteration 6/25 | Loss: 0.00024556
Iteration 7/25 | Loss: 0.00024556
Iteration 8/25 | Loss: 0.00024556
Iteration 9/25 | Loss: 0.00024556
Iteration 10/25 | Loss: 0.00024556
Iteration 11/25 | Loss: 0.00024556
Iteration 12/25 | Loss: 0.00024556
Iteration 13/25 | Loss: 0.00024556
Iteration 14/25 | Loss: 0.00024556
Iteration 15/25 | Loss: 0.00024556
Iteration 16/25 | Loss: 0.00024556
Iteration 17/25 | Loss: 0.00024556
Iteration 18/25 | Loss: 0.00024556
Iteration 19/25 | Loss: 0.00024556
Iteration 20/25 | Loss: 0.00024556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002455558569636196, 0.0002455558569636196, 0.0002455558569636196, 0.0002455558569636196, 0.0002455558569636196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002455558569636196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024556
Iteration 2/1000 | Loss: 0.00002489
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001611
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001389
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001368
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001368
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001366
Iteration 40/1000 | Loss: 0.00001366
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.3382833458308596e-05, 1.3382833458308596e-05, 1.3382833458308596e-05, 1.3382833458308596e-05, 1.3382833458308596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3382833458308596e-05

Optimization complete. Final v2v error: 3.0890352725982666 mm

Highest mean error: 3.1641945838928223 mm for frame 9

Lowest mean error: 3.041705369949341 mm for frame 89

Saving results

Total time: 32.25907373428345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458653
Iteration 2/25 | Loss: 0.00075180
Iteration 3/25 | Loss: 0.00062176
Iteration 4/25 | Loss: 0.00059822
Iteration 5/25 | Loss: 0.00059167
Iteration 6/25 | Loss: 0.00059047
Iteration 7/25 | Loss: 0.00059047
Iteration 8/25 | Loss: 0.00059047
Iteration 9/25 | Loss: 0.00059047
Iteration 10/25 | Loss: 0.00059047
Iteration 11/25 | Loss: 0.00059047
Iteration 12/25 | Loss: 0.00059047
Iteration 13/25 | Loss: 0.00059047
Iteration 14/25 | Loss: 0.00059047
Iteration 15/25 | Loss: 0.00059047
Iteration 16/25 | Loss: 0.00059047
Iteration 17/25 | Loss: 0.00059047
Iteration 18/25 | Loss: 0.00059047
Iteration 19/25 | Loss: 0.00059047
Iteration 20/25 | Loss: 0.00059047
Iteration 21/25 | Loss: 0.00059047
Iteration 22/25 | Loss: 0.00059047
Iteration 23/25 | Loss: 0.00059047
Iteration 24/25 | Loss: 0.00059047
Iteration 25/25 | Loss: 0.00059047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.53304768
Iteration 2/25 | Loss: 0.00020501
Iteration 3/25 | Loss: 0.00020501
Iteration 4/25 | Loss: 0.00020501
Iteration 5/25 | Loss: 0.00020501
Iteration 6/25 | Loss: 0.00020501
Iteration 7/25 | Loss: 0.00020501
Iteration 8/25 | Loss: 0.00020501
Iteration 9/25 | Loss: 0.00020501
Iteration 10/25 | Loss: 0.00020501
Iteration 11/25 | Loss: 0.00020501
Iteration 12/25 | Loss: 0.00020501
Iteration 13/25 | Loss: 0.00020501
Iteration 14/25 | Loss: 0.00020501
Iteration 15/25 | Loss: 0.00020501
Iteration 16/25 | Loss: 0.00020501
Iteration 17/25 | Loss: 0.00020501
Iteration 18/25 | Loss: 0.00020501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00020500879327300936, 0.00020500879327300936, 0.00020500879327300936, 0.00020500879327300936, 0.00020500879327300936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020500879327300936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020501
Iteration 2/1000 | Loss: 0.00001792
Iteration 3/1000 | Loss: 0.00001520
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001353
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001296
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001283
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001275
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001261
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001258
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001257
Iteration 48/1000 | Loss: 0.00001257
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001253
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001249
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001246
Iteration 75/1000 | Loss: 0.00001246
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001244
Iteration 95/1000 | Loss: 0.00001244
Iteration 96/1000 | Loss: 0.00001244
Iteration 97/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.243906172021525e-05, 1.243906172021525e-05, 1.243906172021525e-05, 1.243906172021525e-05, 1.243906172021525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.243906172021525e-05

Optimization complete. Final v2v error: 2.991664171218872 mm

Highest mean error: 3.19195294380188 mm for frame 95

Lowest mean error: 2.8199806213378906 mm for frame 36

Saving results

Total time: 36.33449649810791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864359
Iteration 2/25 | Loss: 0.00206935
Iteration 3/25 | Loss: 0.00113121
Iteration 4/25 | Loss: 0.00098385
Iteration 5/25 | Loss: 0.00094575
Iteration 6/25 | Loss: 0.00093268
Iteration 7/25 | Loss: 0.00089798
Iteration 8/25 | Loss: 0.00082862
Iteration 9/25 | Loss: 0.00083657
Iteration 10/25 | Loss: 0.00080830
Iteration 11/25 | Loss: 0.00078639
Iteration 12/25 | Loss: 0.00077829
Iteration 13/25 | Loss: 0.00078329
Iteration 14/25 | Loss: 0.00078535
Iteration 15/25 | Loss: 0.00077370
Iteration 16/25 | Loss: 0.00077765
Iteration 17/25 | Loss: 0.00077482
Iteration 18/25 | Loss: 0.00076968
Iteration 19/25 | Loss: 0.00077016
Iteration 20/25 | Loss: 0.00077202
Iteration 21/25 | Loss: 0.00076647
Iteration 22/25 | Loss: 0.00076834
Iteration 23/25 | Loss: 0.00076963
Iteration 24/25 | Loss: 0.00076687
Iteration 25/25 | Loss: 0.00076887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50998843
Iteration 2/25 | Loss: 0.00065726
Iteration 3/25 | Loss: 0.00065722
Iteration 4/25 | Loss: 0.00065722
Iteration 5/25 | Loss: 0.00065722
Iteration 6/25 | Loss: 0.00065722
Iteration 7/25 | Loss: 0.00065722
Iteration 8/25 | Loss: 0.00065722
Iteration 9/25 | Loss: 0.00065722
Iteration 10/25 | Loss: 0.00065722
Iteration 11/25 | Loss: 0.00065722
Iteration 12/25 | Loss: 0.00065722
Iteration 13/25 | Loss: 0.00065722
Iteration 14/25 | Loss: 0.00065722
Iteration 15/25 | Loss: 0.00065722
Iteration 16/25 | Loss: 0.00065722
Iteration 17/25 | Loss: 0.00065722
Iteration 18/25 | Loss: 0.00065722
Iteration 19/25 | Loss: 0.00065722
Iteration 20/25 | Loss: 0.00065722
Iteration 21/25 | Loss: 0.00065722
Iteration 22/25 | Loss: 0.00065722
Iteration 23/25 | Loss: 0.00065722
Iteration 24/25 | Loss: 0.00065722
Iteration 25/25 | Loss: 0.00065722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065722
Iteration 2/1000 | Loss: 0.00014852
Iteration 3/1000 | Loss: 0.00020154
Iteration 4/1000 | Loss: 0.00017764
Iteration 5/1000 | Loss: 0.00018795
Iteration 6/1000 | Loss: 0.00027188
Iteration 7/1000 | Loss: 0.00020398
Iteration 8/1000 | Loss: 0.00015456
Iteration 9/1000 | Loss: 0.00022540
Iteration 10/1000 | Loss: 0.00025622
Iteration 11/1000 | Loss: 0.00022274
Iteration 12/1000 | Loss: 0.00022506
Iteration 13/1000 | Loss: 0.00023353
Iteration 14/1000 | Loss: 0.00024147
Iteration 15/1000 | Loss: 0.00025599
Iteration 16/1000 | Loss: 0.00025223
Iteration 17/1000 | Loss: 0.00026584
Iteration 18/1000 | Loss: 0.00025974
Iteration 19/1000 | Loss: 0.00027559
Iteration 20/1000 | Loss: 0.00022007
Iteration 21/1000 | Loss: 0.00011394
Iteration 22/1000 | Loss: 0.00010771
Iteration 23/1000 | Loss: 0.00016382
Iteration 24/1000 | Loss: 0.00016583
Iteration 25/1000 | Loss: 0.00029996
Iteration 26/1000 | Loss: 0.00017428
Iteration 27/1000 | Loss: 0.00016775
Iteration 28/1000 | Loss: 0.00016606
Iteration 29/1000 | Loss: 0.00019291
Iteration 30/1000 | Loss: 0.00015035
Iteration 31/1000 | Loss: 0.00020957
Iteration 32/1000 | Loss: 0.00020983
Iteration 33/1000 | Loss: 0.00021021
Iteration 34/1000 | Loss: 0.00021486
Iteration 35/1000 | Loss: 0.00022271
Iteration 36/1000 | Loss: 0.00020134
Iteration 37/1000 | Loss: 0.00022615
Iteration 38/1000 | Loss: 0.00033745
Iteration 39/1000 | Loss: 0.00016570
Iteration 40/1000 | Loss: 0.00027594
Iteration 41/1000 | Loss: 0.00018603
Iteration 42/1000 | Loss: 0.00015858
Iteration 43/1000 | Loss: 0.00021986
Iteration 44/1000 | Loss: 0.00021475
Iteration 45/1000 | Loss: 0.00022376
Iteration 46/1000 | Loss: 0.00014038
Iteration 47/1000 | Loss: 0.00019640
Iteration 48/1000 | Loss: 0.00018686
Iteration 49/1000 | Loss: 0.00015520
Iteration 50/1000 | Loss: 0.00014843
Iteration 51/1000 | Loss: 0.00020887
Iteration 52/1000 | Loss: 0.00018829
Iteration 53/1000 | Loss: 0.00018741
Iteration 54/1000 | Loss: 0.00023386
Iteration 55/1000 | Loss: 0.00031222
Iteration 56/1000 | Loss: 0.00012170
Iteration 57/1000 | Loss: 0.00013067
Iteration 58/1000 | Loss: 0.00014612
Iteration 59/1000 | Loss: 0.00009830
Iteration 60/1000 | Loss: 0.00011875
Iteration 61/1000 | Loss: 0.00010892
Iteration 62/1000 | Loss: 0.00013819
Iteration 63/1000 | Loss: 0.00013880
Iteration 64/1000 | Loss: 0.00012629
Iteration 65/1000 | Loss: 0.00013968
Iteration 66/1000 | Loss: 0.00011758
Iteration 67/1000 | Loss: 0.00014765
Iteration 68/1000 | Loss: 0.00011490
Iteration 69/1000 | Loss: 0.00013574
Iteration 70/1000 | Loss: 0.00012953
Iteration 71/1000 | Loss: 0.00011618
Iteration 72/1000 | Loss: 0.00010405
Iteration 73/1000 | Loss: 0.00014794
Iteration 74/1000 | Loss: 0.00013205
Iteration 75/1000 | Loss: 0.00006748
Iteration 76/1000 | Loss: 0.00007867
Iteration 77/1000 | Loss: 0.00012201
Iteration 78/1000 | Loss: 0.00012714
Iteration 79/1000 | Loss: 0.00014529
Iteration 80/1000 | Loss: 0.00014405
Iteration 81/1000 | Loss: 0.00010157
Iteration 82/1000 | Loss: 0.00009614
Iteration 83/1000 | Loss: 0.00012962
Iteration 84/1000 | Loss: 0.00010630
Iteration 85/1000 | Loss: 0.00011594
Iteration 86/1000 | Loss: 0.00011628
Iteration 87/1000 | Loss: 0.00006843
Iteration 88/1000 | Loss: 0.00011062
Iteration 89/1000 | Loss: 0.00011371
Iteration 90/1000 | Loss: 0.00011063
Iteration 91/1000 | Loss: 0.00007508
Iteration 92/1000 | Loss: 0.00009194
Iteration 93/1000 | Loss: 0.00010770
Iteration 94/1000 | Loss: 0.00015315
Iteration 95/1000 | Loss: 0.00010150
Iteration 96/1000 | Loss: 0.00015010
Iteration 97/1000 | Loss: 0.00011456
Iteration 98/1000 | Loss: 0.00009131
Iteration 99/1000 | Loss: 0.00010766
Iteration 100/1000 | Loss: 0.00005482
Iteration 101/1000 | Loss: 0.00007157
Iteration 102/1000 | Loss: 0.00010255
Iteration 103/1000 | Loss: 0.00012194
Iteration 104/1000 | Loss: 0.00006643
Iteration 105/1000 | Loss: 0.00003672
Iteration 106/1000 | Loss: 0.00010375
Iteration 107/1000 | Loss: 0.00003691
Iteration 108/1000 | Loss: 0.00003073
Iteration 109/1000 | Loss: 0.00003034
Iteration 110/1000 | Loss: 0.00002644
Iteration 111/1000 | Loss: 0.00003759
Iteration 112/1000 | Loss: 0.00003418
Iteration 113/1000 | Loss: 0.00005057
Iteration 114/1000 | Loss: 0.00004623
Iteration 115/1000 | Loss: 0.00004605
Iteration 116/1000 | Loss: 0.00003495
Iteration 117/1000 | Loss: 0.00002838
Iteration 118/1000 | Loss: 0.00004600
Iteration 119/1000 | Loss: 0.00002793
Iteration 120/1000 | Loss: 0.00002766
Iteration 121/1000 | Loss: 0.00003049
Iteration 122/1000 | Loss: 0.00002581
Iteration 123/1000 | Loss: 0.00002562
Iteration 124/1000 | Loss: 0.00002672
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002266
Iteration 127/1000 | Loss: 0.00003352
Iteration 128/1000 | Loss: 0.00003535
Iteration 129/1000 | Loss: 0.00004085
Iteration 130/1000 | Loss: 0.00003079
Iteration 131/1000 | Loss: 0.00002865
Iteration 132/1000 | Loss: 0.00003081
Iteration 133/1000 | Loss: 0.00002513
Iteration 134/1000 | Loss: 0.00002973
Iteration 135/1000 | Loss: 0.00002451
Iteration 136/1000 | Loss: 0.00002605
Iteration 137/1000 | Loss: 0.00002181
Iteration 138/1000 | Loss: 0.00003966
Iteration 139/1000 | Loss: 0.00002103
Iteration 140/1000 | Loss: 0.00001938
Iteration 141/1000 | Loss: 0.00001838
Iteration 142/1000 | Loss: 0.00001803
Iteration 143/1000 | Loss: 0.00001774
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001758
Iteration 146/1000 | Loss: 0.00001758
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001757
Iteration 149/1000 | Loss: 0.00001757
Iteration 150/1000 | Loss: 0.00001756
Iteration 151/1000 | Loss: 0.00001756
Iteration 152/1000 | Loss: 0.00001755
Iteration 153/1000 | Loss: 0.00001753
Iteration 154/1000 | Loss: 0.00001752
Iteration 155/1000 | Loss: 0.00001752
Iteration 156/1000 | Loss: 0.00001752
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001749
Iteration 159/1000 | Loss: 0.00001749
Iteration 160/1000 | Loss: 0.00001748
Iteration 161/1000 | Loss: 0.00001743
Iteration 162/1000 | Loss: 0.00001743
Iteration 163/1000 | Loss: 0.00001740
Iteration 164/1000 | Loss: 0.00001738
Iteration 165/1000 | Loss: 0.00001738
Iteration 166/1000 | Loss: 0.00001737
Iteration 167/1000 | Loss: 0.00001732
Iteration 168/1000 | Loss: 0.00001729
Iteration 169/1000 | Loss: 0.00001728
Iteration 170/1000 | Loss: 0.00001728
Iteration 171/1000 | Loss: 0.00001728
Iteration 172/1000 | Loss: 0.00001727
Iteration 173/1000 | Loss: 0.00001727
Iteration 174/1000 | Loss: 0.00001727
Iteration 175/1000 | Loss: 0.00001727
Iteration 176/1000 | Loss: 0.00001727
Iteration 177/1000 | Loss: 0.00001726
Iteration 178/1000 | Loss: 0.00001726
Iteration 179/1000 | Loss: 0.00001726
Iteration 180/1000 | Loss: 0.00001726
Iteration 181/1000 | Loss: 0.00001726
Iteration 182/1000 | Loss: 0.00001725
Iteration 183/1000 | Loss: 0.00001725
Iteration 184/1000 | Loss: 0.00001725
Iteration 185/1000 | Loss: 0.00001725
Iteration 186/1000 | Loss: 0.00001725
Iteration 187/1000 | Loss: 0.00001725
Iteration 188/1000 | Loss: 0.00001725
Iteration 189/1000 | Loss: 0.00001725
Iteration 190/1000 | Loss: 0.00001725
Iteration 191/1000 | Loss: 0.00001725
Iteration 192/1000 | Loss: 0.00001725
Iteration 193/1000 | Loss: 0.00001725
Iteration 194/1000 | Loss: 0.00001724
Iteration 195/1000 | Loss: 0.00001724
Iteration 196/1000 | Loss: 0.00001724
Iteration 197/1000 | Loss: 0.00001724
Iteration 198/1000 | Loss: 0.00001723
Iteration 199/1000 | Loss: 0.00001723
Iteration 200/1000 | Loss: 0.00001723
Iteration 201/1000 | Loss: 0.00001723
Iteration 202/1000 | Loss: 0.00001723
Iteration 203/1000 | Loss: 0.00001723
Iteration 204/1000 | Loss: 0.00001722
Iteration 205/1000 | Loss: 0.00001722
Iteration 206/1000 | Loss: 0.00001722
Iteration 207/1000 | Loss: 0.00001722
Iteration 208/1000 | Loss: 0.00001722
Iteration 209/1000 | Loss: 0.00001722
Iteration 210/1000 | Loss: 0.00001722
Iteration 211/1000 | Loss: 0.00001722
Iteration 212/1000 | Loss: 0.00001722
Iteration 213/1000 | Loss: 0.00001722
Iteration 214/1000 | Loss: 0.00001722
Iteration 215/1000 | Loss: 0.00001722
Iteration 216/1000 | Loss: 0.00001722
Iteration 217/1000 | Loss: 0.00001721
Iteration 218/1000 | Loss: 0.00001721
Iteration 219/1000 | Loss: 0.00001721
Iteration 220/1000 | Loss: 0.00001721
Iteration 221/1000 | Loss: 0.00001721
Iteration 222/1000 | Loss: 0.00001721
Iteration 223/1000 | Loss: 0.00001721
Iteration 224/1000 | Loss: 0.00001721
Iteration 225/1000 | Loss: 0.00001721
Iteration 226/1000 | Loss: 0.00001721
Iteration 227/1000 | Loss: 0.00001721
Iteration 228/1000 | Loss: 0.00001721
Iteration 229/1000 | Loss: 0.00001721
Iteration 230/1000 | Loss: 0.00001721
Iteration 231/1000 | Loss: 0.00001721
Iteration 232/1000 | Loss: 0.00001721
Iteration 233/1000 | Loss: 0.00001721
Iteration 234/1000 | Loss: 0.00001721
Iteration 235/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.720532418403309e-05, 1.720532418403309e-05, 1.720532418403309e-05, 1.720532418403309e-05, 1.720532418403309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720532418403309e-05

Optimization complete. Final v2v error: 3.473724126815796 mm

Highest mean error: 4.536495685577393 mm for frame 195

Lowest mean error: 2.9689764976501465 mm for frame 101

Saving results

Total time: 290.1983861923218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979973
Iteration 2/25 | Loss: 0.00263959
Iteration 3/25 | Loss: 0.00193488
Iteration 4/25 | Loss: 0.00174005
Iteration 5/25 | Loss: 0.00148204
Iteration 6/25 | Loss: 0.00157220
Iteration 7/25 | Loss: 0.00162074
Iteration 8/25 | Loss: 0.00161253
Iteration 9/25 | Loss: 0.00141107
Iteration 10/25 | Loss: 0.00113258
Iteration 11/25 | Loss: 0.00096304
Iteration 12/25 | Loss: 0.00090905
Iteration 13/25 | Loss: 0.00088752
Iteration 14/25 | Loss: 0.00088283
Iteration 15/25 | Loss: 0.00084618
Iteration 16/25 | Loss: 0.00081189
Iteration 17/25 | Loss: 0.00079422
Iteration 18/25 | Loss: 0.00077783
Iteration 19/25 | Loss: 0.00077588
Iteration 20/25 | Loss: 0.00076921
Iteration 21/25 | Loss: 0.00076305
Iteration 22/25 | Loss: 0.00076478
Iteration 23/25 | Loss: 0.00076119
Iteration 24/25 | Loss: 0.00075432
Iteration 25/25 | Loss: 0.00075179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46878064
Iteration 2/25 | Loss: 0.00101572
Iteration 3/25 | Loss: 0.00101571
Iteration 4/25 | Loss: 0.00101571
Iteration 5/25 | Loss: 0.00101571
Iteration 6/25 | Loss: 0.00101571
Iteration 7/25 | Loss: 0.00101571
Iteration 8/25 | Loss: 0.00101571
Iteration 9/25 | Loss: 0.00101571
Iteration 10/25 | Loss: 0.00101571
Iteration 11/25 | Loss: 0.00101571
Iteration 12/25 | Loss: 0.00101571
Iteration 13/25 | Loss: 0.00101571
Iteration 14/25 | Loss: 0.00101571
Iteration 15/25 | Loss: 0.00101571
Iteration 16/25 | Loss: 0.00101571
Iteration 17/25 | Loss: 0.00101571
Iteration 18/25 | Loss: 0.00101571
Iteration 19/25 | Loss: 0.00101571
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010157107608392835, 0.0010157107608392835, 0.0010157107608392835, 0.0010157107608392835, 0.0010157107608392835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010157107608392835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101571
Iteration 2/1000 | Loss: 0.00015865
Iteration 3/1000 | Loss: 0.00011814
Iteration 4/1000 | Loss: 0.00010129
Iteration 5/1000 | Loss: 0.00008929
Iteration 6/1000 | Loss: 0.00007834
Iteration 7/1000 | Loss: 0.00007209
Iteration 8/1000 | Loss: 0.00006866
Iteration 9/1000 | Loss: 0.00649050
Iteration 10/1000 | Loss: 0.00037901
Iteration 11/1000 | Loss: 0.00007306
Iteration 12/1000 | Loss: 0.00005027
Iteration 13/1000 | Loss: 0.00003727
Iteration 14/1000 | Loss: 0.00002956
Iteration 15/1000 | Loss: 0.00002549
Iteration 16/1000 | Loss: 0.00002241
Iteration 17/1000 | Loss: 0.00002032
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001831
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00028804
Iteration 24/1000 | Loss: 0.00002468
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00001969
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001886
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001767
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001710
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00029126
Iteration 40/1000 | Loss: 0.00002511
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00028912
Iteration 46/1000 | Loss: 0.00002848
Iteration 47/1000 | Loss: 0.00002395
Iteration 48/1000 | Loss: 0.00002088
Iteration 49/1000 | Loss: 0.00002009
Iteration 50/1000 | Loss: 0.00028342
Iteration 51/1000 | Loss: 0.00002849
Iteration 52/1000 | Loss: 0.00002360
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002045
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001803
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001725
Iteration 62/1000 | Loss: 0.00028830
Iteration 63/1000 | Loss: 0.00044658
Iteration 64/1000 | Loss: 0.00006271
Iteration 65/1000 | Loss: 0.00014545
Iteration 66/1000 | Loss: 0.00003429
Iteration 67/1000 | Loss: 0.00002593
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001667
Iteration 74/1000 | Loss: 0.00001641
Iteration 75/1000 | Loss: 0.00001629
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001596
Iteration 90/1000 | Loss: 0.00001596
Iteration 91/1000 | Loss: 0.00001596
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001592
Iteration 100/1000 | Loss: 0.00001592
Iteration 101/1000 | Loss: 0.00001591
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001589
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001582
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001582
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001579
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001577
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001574
Iteration 131/1000 | Loss: 0.00001574
Iteration 132/1000 | Loss: 0.00001573
Iteration 133/1000 | Loss: 0.00001573
Iteration 134/1000 | Loss: 0.00001573
Iteration 135/1000 | Loss: 0.00001573
Iteration 136/1000 | Loss: 0.00001573
Iteration 137/1000 | Loss: 0.00001573
Iteration 138/1000 | Loss: 0.00001573
Iteration 139/1000 | Loss: 0.00001573
Iteration 140/1000 | Loss: 0.00001573
Iteration 141/1000 | Loss: 0.00001573
Iteration 142/1000 | Loss: 0.00001572
Iteration 143/1000 | Loss: 0.00001572
Iteration 144/1000 | Loss: 0.00001572
Iteration 145/1000 | Loss: 0.00001571
Iteration 146/1000 | Loss: 0.00001571
Iteration 147/1000 | Loss: 0.00001570
Iteration 148/1000 | Loss: 0.00001570
Iteration 149/1000 | Loss: 0.00001570
Iteration 150/1000 | Loss: 0.00001569
Iteration 151/1000 | Loss: 0.00001569
Iteration 152/1000 | Loss: 0.00001569
Iteration 153/1000 | Loss: 0.00001569
Iteration 154/1000 | Loss: 0.00001568
Iteration 155/1000 | Loss: 0.00001568
Iteration 156/1000 | Loss: 0.00001568
Iteration 157/1000 | Loss: 0.00001567
Iteration 158/1000 | Loss: 0.00001567
Iteration 159/1000 | Loss: 0.00001566
Iteration 160/1000 | Loss: 0.00001566
Iteration 161/1000 | Loss: 0.00001566
Iteration 162/1000 | Loss: 0.00001565
Iteration 163/1000 | Loss: 0.00001565
Iteration 164/1000 | Loss: 0.00001565
Iteration 165/1000 | Loss: 0.00001565
Iteration 166/1000 | Loss: 0.00001565
Iteration 167/1000 | Loss: 0.00001565
Iteration 168/1000 | Loss: 0.00001565
Iteration 169/1000 | Loss: 0.00001565
Iteration 170/1000 | Loss: 0.00001565
Iteration 171/1000 | Loss: 0.00001565
Iteration 172/1000 | Loss: 0.00001564
Iteration 173/1000 | Loss: 0.00001564
Iteration 174/1000 | Loss: 0.00001564
Iteration 175/1000 | Loss: 0.00001564
Iteration 176/1000 | Loss: 0.00001564
Iteration 177/1000 | Loss: 0.00001563
Iteration 178/1000 | Loss: 0.00001563
Iteration 179/1000 | Loss: 0.00001563
Iteration 180/1000 | Loss: 0.00001562
Iteration 181/1000 | Loss: 0.00001562
Iteration 182/1000 | Loss: 0.00001562
Iteration 183/1000 | Loss: 0.00001562
Iteration 184/1000 | Loss: 0.00001562
Iteration 185/1000 | Loss: 0.00001562
Iteration 186/1000 | Loss: 0.00001562
Iteration 187/1000 | Loss: 0.00001562
Iteration 188/1000 | Loss: 0.00001562
Iteration 189/1000 | Loss: 0.00001562
Iteration 190/1000 | Loss: 0.00001562
Iteration 191/1000 | Loss: 0.00001561
Iteration 192/1000 | Loss: 0.00001561
Iteration 193/1000 | Loss: 0.00001561
Iteration 194/1000 | Loss: 0.00001561
Iteration 195/1000 | Loss: 0.00001561
Iteration 196/1000 | Loss: 0.00001560
Iteration 197/1000 | Loss: 0.00001560
Iteration 198/1000 | Loss: 0.00001560
Iteration 199/1000 | Loss: 0.00001560
Iteration 200/1000 | Loss: 0.00001560
Iteration 201/1000 | Loss: 0.00001560
Iteration 202/1000 | Loss: 0.00001560
Iteration 203/1000 | Loss: 0.00001560
Iteration 204/1000 | Loss: 0.00001560
Iteration 205/1000 | Loss: 0.00001560
Iteration 206/1000 | Loss: 0.00001560
Iteration 207/1000 | Loss: 0.00001560
Iteration 208/1000 | Loss: 0.00001559
Iteration 209/1000 | Loss: 0.00001559
Iteration 210/1000 | Loss: 0.00001559
Iteration 211/1000 | Loss: 0.00001559
Iteration 212/1000 | Loss: 0.00001559
Iteration 213/1000 | Loss: 0.00001559
Iteration 214/1000 | Loss: 0.00001559
Iteration 215/1000 | Loss: 0.00001559
Iteration 216/1000 | Loss: 0.00001559
Iteration 217/1000 | Loss: 0.00001559
Iteration 218/1000 | Loss: 0.00001559
Iteration 219/1000 | Loss: 0.00001559
Iteration 220/1000 | Loss: 0.00001558
Iteration 221/1000 | Loss: 0.00001558
Iteration 222/1000 | Loss: 0.00001558
Iteration 223/1000 | Loss: 0.00001558
Iteration 224/1000 | Loss: 0.00001558
Iteration 225/1000 | Loss: 0.00001558
Iteration 226/1000 | Loss: 0.00001557
Iteration 227/1000 | Loss: 0.00001557
Iteration 228/1000 | Loss: 0.00001557
Iteration 229/1000 | Loss: 0.00001557
Iteration 230/1000 | Loss: 0.00001557
Iteration 231/1000 | Loss: 0.00001557
Iteration 232/1000 | Loss: 0.00001557
Iteration 233/1000 | Loss: 0.00001557
Iteration 234/1000 | Loss: 0.00001557
Iteration 235/1000 | Loss: 0.00001557
Iteration 236/1000 | Loss: 0.00001557
Iteration 237/1000 | Loss: 0.00001557
Iteration 238/1000 | Loss: 0.00001557
Iteration 239/1000 | Loss: 0.00001557
Iteration 240/1000 | Loss: 0.00001557
Iteration 241/1000 | Loss: 0.00001557
Iteration 242/1000 | Loss: 0.00001557
Iteration 243/1000 | Loss: 0.00001557
Iteration 244/1000 | Loss: 0.00001557
Iteration 245/1000 | Loss: 0.00001557
Iteration 246/1000 | Loss: 0.00001557
Iteration 247/1000 | Loss: 0.00001557
Iteration 248/1000 | Loss: 0.00001557
Iteration 249/1000 | Loss: 0.00001557
Iteration 250/1000 | Loss: 0.00001557
Iteration 251/1000 | Loss: 0.00001557
Iteration 252/1000 | Loss: 0.00001557
Iteration 253/1000 | Loss: 0.00001557
Iteration 254/1000 | Loss: 0.00001557
Iteration 255/1000 | Loss: 0.00001557
Iteration 256/1000 | Loss: 0.00001557
Iteration 257/1000 | Loss: 0.00001557
Iteration 258/1000 | Loss: 0.00001557
Iteration 259/1000 | Loss: 0.00001557
Iteration 260/1000 | Loss: 0.00001557
Iteration 261/1000 | Loss: 0.00001557
Iteration 262/1000 | Loss: 0.00001557
Iteration 263/1000 | Loss: 0.00001557
Iteration 264/1000 | Loss: 0.00001557
Iteration 265/1000 | Loss: 0.00001557
Iteration 266/1000 | Loss: 0.00001557
Iteration 267/1000 | Loss: 0.00001557
Iteration 268/1000 | Loss: 0.00001557
Iteration 269/1000 | Loss: 0.00001557
Iteration 270/1000 | Loss: 0.00001557
Iteration 271/1000 | Loss: 0.00001557
Iteration 272/1000 | Loss: 0.00001557
Iteration 273/1000 | Loss: 0.00001557
Iteration 274/1000 | Loss: 0.00001557
Iteration 275/1000 | Loss: 0.00001557
Iteration 276/1000 | Loss: 0.00001557
Iteration 277/1000 | Loss: 0.00001557
Iteration 278/1000 | Loss: 0.00001557
Iteration 279/1000 | Loss: 0.00001557
Iteration 280/1000 | Loss: 0.00001557
Iteration 281/1000 | Loss: 0.00001557
Iteration 282/1000 | Loss: 0.00001557
Iteration 283/1000 | Loss: 0.00001557
Iteration 284/1000 | Loss: 0.00001557
Iteration 285/1000 | Loss: 0.00001557
Iteration 286/1000 | Loss: 0.00001557
Iteration 287/1000 | Loss: 0.00001557
Iteration 288/1000 | Loss: 0.00001557
Iteration 289/1000 | Loss: 0.00001557
Iteration 290/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.5566114598186687e-05, 1.5566114598186687e-05, 1.5566114598186687e-05, 1.5566114598186687e-05, 1.5566114598186687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5566114598186687e-05

Optimization complete. Final v2v error: 3.3342247009277344 mm

Highest mean error: 5.577767848968506 mm for frame 71

Lowest mean error: 3.0854485034942627 mm for frame 17

Saving results

Total time: 157.76944756507874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540390
Iteration 2/25 | Loss: 0.00083090
Iteration 3/25 | Loss: 0.00064281
Iteration 4/25 | Loss: 0.00061088
Iteration 5/25 | Loss: 0.00060352
Iteration 6/25 | Loss: 0.00060117
Iteration 7/25 | Loss: 0.00060048
Iteration 8/25 | Loss: 0.00060047
Iteration 9/25 | Loss: 0.00060047
Iteration 10/25 | Loss: 0.00060047
Iteration 11/25 | Loss: 0.00060047
Iteration 12/25 | Loss: 0.00060047
Iteration 13/25 | Loss: 0.00060047
Iteration 14/25 | Loss: 0.00060047
Iteration 15/25 | Loss: 0.00060047
Iteration 16/25 | Loss: 0.00060047
Iteration 17/25 | Loss: 0.00060047
Iteration 18/25 | Loss: 0.00060047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006004747119732201, 0.0006004747119732201, 0.0006004747119732201, 0.0006004747119732201, 0.0006004747119732201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006004747119732201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16923857
Iteration 2/25 | Loss: 0.00019364
Iteration 3/25 | Loss: 0.00019364
Iteration 4/25 | Loss: 0.00019364
Iteration 5/25 | Loss: 0.00019363
Iteration 6/25 | Loss: 0.00019363
Iteration 7/25 | Loss: 0.00019363
Iteration 8/25 | Loss: 0.00019363
Iteration 9/25 | Loss: 0.00019363
Iteration 10/25 | Loss: 0.00019363
Iteration 11/25 | Loss: 0.00019363
Iteration 12/25 | Loss: 0.00019363
Iteration 13/25 | Loss: 0.00019363
Iteration 14/25 | Loss: 0.00019363
Iteration 15/25 | Loss: 0.00019363
Iteration 16/25 | Loss: 0.00019363
Iteration 17/25 | Loss: 0.00019363
Iteration 18/25 | Loss: 0.00019363
Iteration 19/25 | Loss: 0.00019363
Iteration 20/25 | Loss: 0.00019363
Iteration 21/25 | Loss: 0.00019363
Iteration 22/25 | Loss: 0.00019363
Iteration 23/25 | Loss: 0.00019363
Iteration 24/25 | Loss: 0.00019363
Iteration 25/25 | Loss: 0.00019363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019363
Iteration 2/1000 | Loss: 0.00001978
Iteration 3/1000 | Loss: 0.00001428
Iteration 4/1000 | Loss: 0.00001312
Iteration 5/1000 | Loss: 0.00001265
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001184
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001184
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001176
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001170
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001167
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001159
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001157
Iteration 44/1000 | Loss: 0.00001157
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001156
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001148
Iteration 68/1000 | Loss: 0.00001148
Iteration 69/1000 | Loss: 0.00001148
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001147
Iteration 72/1000 | Loss: 0.00001147
Iteration 73/1000 | Loss: 0.00001146
Iteration 74/1000 | Loss: 0.00001146
Iteration 75/1000 | Loss: 0.00001146
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001143
Iteration 97/1000 | Loss: 0.00001143
Iteration 98/1000 | Loss: 0.00001143
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001142
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001141
Iteration 114/1000 | Loss: 0.00001141
Iteration 115/1000 | Loss: 0.00001141
Iteration 116/1000 | Loss: 0.00001141
Iteration 117/1000 | Loss: 0.00001141
Iteration 118/1000 | Loss: 0.00001140
Iteration 119/1000 | Loss: 0.00001140
Iteration 120/1000 | Loss: 0.00001140
Iteration 121/1000 | Loss: 0.00001140
Iteration 122/1000 | Loss: 0.00001140
Iteration 123/1000 | Loss: 0.00001140
Iteration 124/1000 | Loss: 0.00001140
Iteration 125/1000 | Loss: 0.00001140
Iteration 126/1000 | Loss: 0.00001140
Iteration 127/1000 | Loss: 0.00001140
Iteration 128/1000 | Loss: 0.00001140
Iteration 129/1000 | Loss: 0.00001140
Iteration 130/1000 | Loss: 0.00001140
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001139
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001138
Iteration 152/1000 | Loss: 0.00001138
Iteration 153/1000 | Loss: 0.00001138
Iteration 154/1000 | Loss: 0.00001138
Iteration 155/1000 | Loss: 0.00001138
Iteration 156/1000 | Loss: 0.00001138
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Iteration 164/1000 | Loss: 0.00001137
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001136
Iteration 167/1000 | Loss: 0.00001136
Iteration 168/1000 | Loss: 0.00001136
Iteration 169/1000 | Loss: 0.00001136
Iteration 170/1000 | Loss: 0.00001136
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001136
Iteration 174/1000 | Loss: 0.00001136
Iteration 175/1000 | Loss: 0.00001136
Iteration 176/1000 | Loss: 0.00001136
Iteration 177/1000 | Loss: 0.00001136
Iteration 178/1000 | Loss: 0.00001136
Iteration 179/1000 | Loss: 0.00001136
Iteration 180/1000 | Loss: 0.00001136
Iteration 181/1000 | Loss: 0.00001136
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001135
Iteration 188/1000 | Loss: 0.00001135
Iteration 189/1000 | Loss: 0.00001135
Iteration 190/1000 | Loss: 0.00001135
Iteration 191/1000 | Loss: 0.00001135
Iteration 192/1000 | Loss: 0.00001135
Iteration 193/1000 | Loss: 0.00001135
Iteration 194/1000 | Loss: 0.00001135
Iteration 195/1000 | Loss: 0.00001135
Iteration 196/1000 | Loss: 0.00001135
Iteration 197/1000 | Loss: 0.00001135
Iteration 198/1000 | Loss: 0.00001135
Iteration 199/1000 | Loss: 0.00001135
Iteration 200/1000 | Loss: 0.00001135
Iteration 201/1000 | Loss: 0.00001135
Iteration 202/1000 | Loss: 0.00001134
Iteration 203/1000 | Loss: 0.00001134
Iteration 204/1000 | Loss: 0.00001134
Iteration 205/1000 | Loss: 0.00001134
Iteration 206/1000 | Loss: 0.00001134
Iteration 207/1000 | Loss: 0.00001134
Iteration 208/1000 | Loss: 0.00001134
Iteration 209/1000 | Loss: 0.00001134
Iteration 210/1000 | Loss: 0.00001134
Iteration 211/1000 | Loss: 0.00001134
Iteration 212/1000 | Loss: 0.00001134
Iteration 213/1000 | Loss: 0.00001134
Iteration 214/1000 | Loss: 0.00001134
Iteration 215/1000 | Loss: 0.00001134
Iteration 216/1000 | Loss: 0.00001134
Iteration 217/1000 | Loss: 0.00001134
Iteration 218/1000 | Loss: 0.00001134
Iteration 219/1000 | Loss: 0.00001134
Iteration 220/1000 | Loss: 0.00001134
Iteration 221/1000 | Loss: 0.00001134
Iteration 222/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.1344360245857388e-05, 1.1344360245857388e-05, 1.1344360245857388e-05, 1.1344360245857388e-05, 1.1344360245857388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1344360245857388e-05

Optimization complete. Final v2v error: 2.8405139446258545 mm

Highest mean error: 3.325084686279297 mm for frame 62

Lowest mean error: 2.59128737449646 mm for frame 14

Saving results

Total time: 38.83396315574646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444792
Iteration 2/25 | Loss: 0.00111916
Iteration 3/25 | Loss: 0.00069029
Iteration 4/25 | Loss: 0.00063242
Iteration 5/25 | Loss: 0.00061940
Iteration 6/25 | Loss: 0.00061639
Iteration 7/25 | Loss: 0.00061572
Iteration 8/25 | Loss: 0.00061572
Iteration 9/25 | Loss: 0.00061572
Iteration 10/25 | Loss: 0.00061572
Iteration 11/25 | Loss: 0.00061572
Iteration 12/25 | Loss: 0.00061572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006157232564873993, 0.0006157232564873993, 0.0006157232564873993, 0.0006157232564873993, 0.0006157232564873993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006157232564873993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46084833
Iteration 2/25 | Loss: 0.00024872
Iteration 3/25 | Loss: 0.00024871
Iteration 4/25 | Loss: 0.00024871
Iteration 5/25 | Loss: 0.00024871
Iteration 6/25 | Loss: 0.00024871
Iteration 7/25 | Loss: 0.00024870
Iteration 8/25 | Loss: 0.00024870
Iteration 9/25 | Loss: 0.00024870
Iteration 10/25 | Loss: 0.00024870
Iteration 11/25 | Loss: 0.00024870
Iteration 12/25 | Loss: 0.00024870
Iteration 13/25 | Loss: 0.00024870
Iteration 14/25 | Loss: 0.00024870
Iteration 15/25 | Loss: 0.00024870
Iteration 16/25 | Loss: 0.00024870
Iteration 17/25 | Loss: 0.00024870
Iteration 18/25 | Loss: 0.00024870
Iteration 19/25 | Loss: 0.00024870
Iteration 20/25 | Loss: 0.00024870
Iteration 21/25 | Loss: 0.00024870
Iteration 22/25 | Loss: 0.00024870
Iteration 23/25 | Loss: 0.00024870
Iteration 24/25 | Loss: 0.00024870
Iteration 25/25 | Loss: 0.00024870

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024870
Iteration 2/1000 | Loss: 0.00001775
Iteration 3/1000 | Loss: 0.00001381
Iteration 4/1000 | Loss: 0.00001263
Iteration 5/1000 | Loss: 0.00001207
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001161
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001150
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001135
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001134
Iteration 17/1000 | Loss: 0.00001133
Iteration 18/1000 | Loss: 0.00001132
Iteration 19/1000 | Loss: 0.00001132
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001130
Iteration 23/1000 | Loss: 0.00001129
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001119
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001116
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001114
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001108
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001107
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001095
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001092
Iteration 118/1000 | Loss: 0.00001092
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001092
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001092
Iteration 123/1000 | Loss: 0.00001092
Iteration 124/1000 | Loss: 0.00001092
Iteration 125/1000 | Loss: 0.00001092
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.0915749953710474e-05, 1.0915749953710474e-05, 1.0915749953710474e-05, 1.0915749953710474e-05, 1.0915749953710474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0915749953710474e-05

Optimization complete. Final v2v error: 2.726471185684204 mm

Highest mean error: 2.984917402267456 mm for frame 136

Lowest mean error: 2.5822572708129883 mm for frame 26

Saving results

Total time: 31.125918865203857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048267
Iteration 2/25 | Loss: 0.00159312
Iteration 3/25 | Loss: 0.00098723
Iteration 4/25 | Loss: 0.00093468
Iteration 5/25 | Loss: 0.00092033
Iteration 6/25 | Loss: 0.00091662
Iteration 7/25 | Loss: 0.00091604
Iteration 8/25 | Loss: 0.00091604
Iteration 9/25 | Loss: 0.00091604
Iteration 10/25 | Loss: 0.00091604
Iteration 11/25 | Loss: 0.00091604
Iteration 12/25 | Loss: 0.00091604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009160395129583776, 0.0009160395129583776, 0.0009160395129583776, 0.0009160395129583776, 0.0009160395129583776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009160395129583776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58285093
Iteration 2/25 | Loss: 0.00022419
Iteration 3/25 | Loss: 0.00022418
Iteration 4/25 | Loss: 0.00022418
Iteration 5/25 | Loss: 0.00022418
Iteration 6/25 | Loss: 0.00022418
Iteration 7/25 | Loss: 0.00022418
Iteration 8/25 | Loss: 0.00022418
Iteration 9/25 | Loss: 0.00022418
Iteration 10/25 | Loss: 0.00022418
Iteration 11/25 | Loss: 0.00022418
Iteration 12/25 | Loss: 0.00022418
Iteration 13/25 | Loss: 0.00022418
Iteration 14/25 | Loss: 0.00022418
Iteration 15/25 | Loss: 0.00022418
Iteration 16/25 | Loss: 0.00022418
Iteration 17/25 | Loss: 0.00022418
Iteration 18/25 | Loss: 0.00022418
Iteration 19/25 | Loss: 0.00022418
Iteration 20/25 | Loss: 0.00022418
Iteration 21/25 | Loss: 0.00022418
Iteration 22/25 | Loss: 0.00022418
Iteration 23/25 | Loss: 0.00022418
Iteration 24/25 | Loss: 0.00022418
Iteration 25/25 | Loss: 0.00022418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022418
Iteration 2/1000 | Loss: 0.00006270
Iteration 3/1000 | Loss: 0.00004575
Iteration 4/1000 | Loss: 0.00004205
Iteration 5/1000 | Loss: 0.00004025
Iteration 6/1000 | Loss: 0.00003939
Iteration 7/1000 | Loss: 0.00003862
Iteration 8/1000 | Loss: 0.00003807
Iteration 9/1000 | Loss: 0.00003765
Iteration 10/1000 | Loss: 0.00003729
Iteration 11/1000 | Loss: 0.00003701
Iteration 12/1000 | Loss: 0.00003679
Iteration 13/1000 | Loss: 0.00003657
Iteration 14/1000 | Loss: 0.00003642
Iteration 15/1000 | Loss: 0.00003639
Iteration 16/1000 | Loss: 0.00003623
Iteration 17/1000 | Loss: 0.00003615
Iteration 18/1000 | Loss: 0.00003604
Iteration 19/1000 | Loss: 0.00003602
Iteration 20/1000 | Loss: 0.00003599
Iteration 21/1000 | Loss: 0.00003599
Iteration 22/1000 | Loss: 0.00003596
Iteration 23/1000 | Loss: 0.00003595
Iteration 24/1000 | Loss: 0.00003593
Iteration 25/1000 | Loss: 0.00003593
Iteration 26/1000 | Loss: 0.00003592
Iteration 27/1000 | Loss: 0.00003589
Iteration 28/1000 | Loss: 0.00003589
Iteration 29/1000 | Loss: 0.00003589
Iteration 30/1000 | Loss: 0.00003582
Iteration 31/1000 | Loss: 0.00003581
Iteration 32/1000 | Loss: 0.00003579
Iteration 33/1000 | Loss: 0.00003578
Iteration 34/1000 | Loss: 0.00003576
Iteration 35/1000 | Loss: 0.00003572
Iteration 36/1000 | Loss: 0.00003572
Iteration 37/1000 | Loss: 0.00003572
Iteration 38/1000 | Loss: 0.00003571
Iteration 39/1000 | Loss: 0.00003570
Iteration 40/1000 | Loss: 0.00003570
Iteration 41/1000 | Loss: 0.00003570
Iteration 42/1000 | Loss: 0.00003570
Iteration 43/1000 | Loss: 0.00003570
Iteration 44/1000 | Loss: 0.00003570
Iteration 45/1000 | Loss: 0.00003570
Iteration 46/1000 | Loss: 0.00003569
Iteration 47/1000 | Loss: 0.00003569
Iteration 48/1000 | Loss: 0.00003569
Iteration 49/1000 | Loss: 0.00003569
Iteration 50/1000 | Loss: 0.00003569
Iteration 51/1000 | Loss: 0.00003569
Iteration 52/1000 | Loss: 0.00003569
Iteration 53/1000 | Loss: 0.00003569
Iteration 54/1000 | Loss: 0.00003569
Iteration 55/1000 | Loss: 0.00003569
Iteration 56/1000 | Loss: 0.00003569
Iteration 57/1000 | Loss: 0.00003569
Iteration 58/1000 | Loss: 0.00003568
Iteration 59/1000 | Loss: 0.00003568
Iteration 60/1000 | Loss: 0.00003568
Iteration 61/1000 | Loss: 0.00003568
Iteration 62/1000 | Loss: 0.00003568
Iteration 63/1000 | Loss: 0.00003567
Iteration 64/1000 | Loss: 0.00003567
Iteration 65/1000 | Loss: 0.00003567
Iteration 66/1000 | Loss: 0.00003567
Iteration 67/1000 | Loss: 0.00003566
Iteration 68/1000 | Loss: 0.00003566
Iteration 69/1000 | Loss: 0.00003566
Iteration 70/1000 | Loss: 0.00003565
Iteration 71/1000 | Loss: 0.00003565
Iteration 72/1000 | Loss: 0.00003564
Iteration 73/1000 | Loss: 0.00003564
Iteration 74/1000 | Loss: 0.00003564
Iteration 75/1000 | Loss: 0.00003563
Iteration 76/1000 | Loss: 0.00003563
Iteration 77/1000 | Loss: 0.00003563
Iteration 78/1000 | Loss: 0.00003563
Iteration 79/1000 | Loss: 0.00003562
Iteration 80/1000 | Loss: 0.00003562
Iteration 81/1000 | Loss: 0.00003562
Iteration 82/1000 | Loss: 0.00003562
Iteration 83/1000 | Loss: 0.00003562
Iteration 84/1000 | Loss: 0.00003561
Iteration 85/1000 | Loss: 0.00003561
Iteration 86/1000 | Loss: 0.00003561
Iteration 87/1000 | Loss: 0.00003561
Iteration 88/1000 | Loss: 0.00003561
Iteration 89/1000 | Loss: 0.00003561
Iteration 90/1000 | Loss: 0.00003561
Iteration 91/1000 | Loss: 0.00003561
Iteration 92/1000 | Loss: 0.00003561
Iteration 93/1000 | Loss: 0.00003560
Iteration 94/1000 | Loss: 0.00003560
Iteration 95/1000 | Loss: 0.00003560
Iteration 96/1000 | Loss: 0.00003560
Iteration 97/1000 | Loss: 0.00003560
Iteration 98/1000 | Loss: 0.00003559
Iteration 99/1000 | Loss: 0.00003559
Iteration 100/1000 | Loss: 0.00003559
Iteration 101/1000 | Loss: 0.00003559
Iteration 102/1000 | Loss: 0.00003559
Iteration 103/1000 | Loss: 0.00003559
Iteration 104/1000 | Loss: 0.00003559
Iteration 105/1000 | Loss: 0.00003558
Iteration 106/1000 | Loss: 0.00003558
Iteration 107/1000 | Loss: 0.00003558
Iteration 108/1000 | Loss: 0.00003558
Iteration 109/1000 | Loss: 0.00003558
Iteration 110/1000 | Loss: 0.00003558
Iteration 111/1000 | Loss: 0.00003558
Iteration 112/1000 | Loss: 0.00003558
Iteration 113/1000 | Loss: 0.00003558
Iteration 114/1000 | Loss: 0.00003558
Iteration 115/1000 | Loss: 0.00003557
Iteration 116/1000 | Loss: 0.00003557
Iteration 117/1000 | Loss: 0.00003557
Iteration 118/1000 | Loss: 0.00003557
Iteration 119/1000 | Loss: 0.00003557
Iteration 120/1000 | Loss: 0.00003557
Iteration 121/1000 | Loss: 0.00003556
Iteration 122/1000 | Loss: 0.00003556
Iteration 123/1000 | Loss: 0.00003556
Iteration 124/1000 | Loss: 0.00003556
Iteration 125/1000 | Loss: 0.00003556
Iteration 126/1000 | Loss: 0.00003556
Iteration 127/1000 | Loss: 0.00003556
Iteration 128/1000 | Loss: 0.00003556
Iteration 129/1000 | Loss: 0.00003555
Iteration 130/1000 | Loss: 0.00003555
Iteration 131/1000 | Loss: 0.00003555
Iteration 132/1000 | Loss: 0.00003555
Iteration 133/1000 | Loss: 0.00003555
Iteration 134/1000 | Loss: 0.00003555
Iteration 135/1000 | Loss: 0.00003555
Iteration 136/1000 | Loss: 0.00003555
Iteration 137/1000 | Loss: 0.00003555
Iteration 138/1000 | Loss: 0.00003555
Iteration 139/1000 | Loss: 0.00003554
Iteration 140/1000 | Loss: 0.00003554
Iteration 141/1000 | Loss: 0.00003554
Iteration 142/1000 | Loss: 0.00003554
Iteration 143/1000 | Loss: 0.00003554
Iteration 144/1000 | Loss: 0.00003554
Iteration 145/1000 | Loss: 0.00003554
Iteration 146/1000 | Loss: 0.00003554
Iteration 147/1000 | Loss: 0.00003554
Iteration 148/1000 | Loss: 0.00003554
Iteration 149/1000 | Loss: 0.00003554
Iteration 150/1000 | Loss: 0.00003554
Iteration 151/1000 | Loss: 0.00003554
Iteration 152/1000 | Loss: 0.00003553
Iteration 153/1000 | Loss: 0.00003553
Iteration 154/1000 | Loss: 0.00003553
Iteration 155/1000 | Loss: 0.00003553
Iteration 156/1000 | Loss: 0.00003553
Iteration 157/1000 | Loss: 0.00003553
Iteration 158/1000 | Loss: 0.00003553
Iteration 159/1000 | Loss: 0.00003553
Iteration 160/1000 | Loss: 0.00003553
Iteration 161/1000 | Loss: 0.00003553
Iteration 162/1000 | Loss: 0.00003553
Iteration 163/1000 | Loss: 0.00003553
Iteration 164/1000 | Loss: 0.00003553
Iteration 165/1000 | Loss: 0.00003553
Iteration 166/1000 | Loss: 0.00003553
Iteration 167/1000 | Loss: 0.00003553
Iteration 168/1000 | Loss: 0.00003553
Iteration 169/1000 | Loss: 0.00003553
Iteration 170/1000 | Loss: 0.00003553
Iteration 171/1000 | Loss: 0.00003553
Iteration 172/1000 | Loss: 0.00003553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [3.5528319131117314e-05, 3.5528319131117314e-05, 3.5528319131117314e-05, 3.5528319131117314e-05, 3.5528319131117314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5528319131117314e-05

Optimization complete. Final v2v error: 4.798868656158447 mm

Highest mean error: 5.848291397094727 mm for frame 40

Lowest mean error: 4.055582523345947 mm for frame 65

Saving results

Total time: 48.244043827056885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086990
Iteration 2/25 | Loss: 0.01086989
Iteration 3/25 | Loss: 0.00164204
Iteration 4/25 | Loss: 0.00116956
Iteration 5/25 | Loss: 0.00097786
Iteration 6/25 | Loss: 0.00085029
Iteration 7/25 | Loss: 0.00082051
Iteration 8/25 | Loss: 0.00080260
Iteration 9/25 | Loss: 0.00075883
Iteration 10/25 | Loss: 0.00073834
Iteration 11/25 | Loss: 0.00074046
Iteration 12/25 | Loss: 0.00074854
Iteration 13/25 | Loss: 0.00073268
Iteration 14/25 | Loss: 0.00073845
Iteration 15/25 | Loss: 0.00072741
Iteration 16/25 | Loss: 0.00073110
Iteration 17/25 | Loss: 0.00072186
Iteration 18/25 | Loss: 0.00072355
Iteration 19/25 | Loss: 0.00072458
Iteration 20/25 | Loss: 0.00072587
Iteration 21/25 | Loss: 0.00072304
Iteration 22/25 | Loss: 0.00072325
Iteration 23/25 | Loss: 0.00072479
Iteration 24/25 | Loss: 0.00072478
Iteration 25/25 | Loss: 0.00072443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54481030
Iteration 2/25 | Loss: 0.00039538
Iteration 3/25 | Loss: 0.00032300
Iteration 4/25 | Loss: 0.00032300
Iteration 5/25 | Loss: 0.00032300
Iteration 6/25 | Loss: 0.00032300
Iteration 7/25 | Loss: 0.00032300
Iteration 8/25 | Loss: 0.00032300
Iteration 9/25 | Loss: 0.00032300
Iteration 10/25 | Loss: 0.00032300
Iteration 11/25 | Loss: 0.00032299
Iteration 12/25 | Loss: 0.00032299
Iteration 13/25 | Loss: 0.00032299
Iteration 14/25 | Loss: 0.00032299
Iteration 15/25 | Loss: 0.00032299
Iteration 16/25 | Loss: 0.00032299
Iteration 17/25 | Loss: 0.00032299
Iteration 18/25 | Loss: 0.00032299
Iteration 19/25 | Loss: 0.00032299
Iteration 20/25 | Loss: 0.00032299
Iteration 21/25 | Loss: 0.00032299
Iteration 22/25 | Loss: 0.00032299
Iteration 23/25 | Loss: 0.00032299
Iteration 24/25 | Loss: 0.00032299
Iteration 25/25 | Loss: 0.00032299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032299
Iteration 2/1000 | Loss: 0.00040678
Iteration 3/1000 | Loss: 0.00012014
Iteration 4/1000 | Loss: 0.00011708
Iteration 5/1000 | Loss: 0.00052064
Iteration 6/1000 | Loss: 0.00014610
Iteration 7/1000 | Loss: 0.00011389
Iteration 8/1000 | Loss: 0.00012255
Iteration 9/1000 | Loss: 0.00011309
Iteration 10/1000 | Loss: 0.00011859
Iteration 11/1000 | Loss: 0.00012245
Iteration 12/1000 | Loss: 0.00019290
Iteration 13/1000 | Loss: 0.00015921
Iteration 14/1000 | Loss: 0.00017022
Iteration 15/1000 | Loss: 0.00010676
Iteration 16/1000 | Loss: 0.00010702
Iteration 17/1000 | Loss: 0.00004226
Iteration 18/1000 | Loss: 0.00009363
Iteration 19/1000 | Loss: 0.00002414
Iteration 20/1000 | Loss: 0.00012513
Iteration 21/1000 | Loss: 0.00002233
Iteration 22/1000 | Loss: 0.00002186
Iteration 23/1000 | Loss: 0.00002155
Iteration 24/1000 | Loss: 0.00002122
Iteration 25/1000 | Loss: 0.00002094
Iteration 26/1000 | Loss: 0.00002056
Iteration 27/1000 | Loss: 0.00002033
Iteration 28/1000 | Loss: 0.00077110
Iteration 29/1000 | Loss: 0.00002671
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001879
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001749
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001678
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001617
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001601
Iteration 43/1000 | Loss: 0.00001601
Iteration 44/1000 | Loss: 0.00001600
Iteration 45/1000 | Loss: 0.00001597
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001594
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001593
Iteration 58/1000 | Loss: 0.00001593
Iteration 59/1000 | Loss: 0.00001592
Iteration 60/1000 | Loss: 0.00001592
Iteration 61/1000 | Loss: 0.00001592
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001592
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001591
Iteration 67/1000 | Loss: 0.00001591
Iteration 68/1000 | Loss: 0.00001591
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001586
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001586
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001586
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001586
Iteration 122/1000 | Loss: 0.00001586
Iteration 123/1000 | Loss: 0.00001586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.585718200658448e-05, 1.585718200658448e-05, 1.585718200658448e-05, 1.585718200658448e-05, 1.585718200658448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.585718200658448e-05

Optimization complete. Final v2v error: 3.290858745574951 mm

Highest mean error: 9.019047737121582 mm for frame 110

Lowest mean error: 2.8831193447113037 mm for frame 11

Saving results

Total time: 102.012047290802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106355
Iteration 2/25 | Loss: 0.00248156
Iteration 3/25 | Loss: 0.00127864
Iteration 4/25 | Loss: 0.00108272
Iteration 5/25 | Loss: 0.00098728
Iteration 6/25 | Loss: 0.00090413
Iteration 7/25 | Loss: 0.00085399
Iteration 8/25 | Loss: 0.00084485
Iteration 9/25 | Loss: 0.00079626
Iteration 10/25 | Loss: 0.00075770
Iteration 11/25 | Loss: 0.00073997
Iteration 12/25 | Loss: 0.00071147
Iteration 13/25 | Loss: 0.00069150
Iteration 14/25 | Loss: 0.00069006
Iteration 15/25 | Loss: 0.00068292
Iteration 16/25 | Loss: 0.00068161
Iteration 17/25 | Loss: 0.00068095
Iteration 18/25 | Loss: 0.00069160
Iteration 19/25 | Loss: 0.00068106
Iteration 20/25 | Loss: 0.00068055
Iteration 21/25 | Loss: 0.00068055
Iteration 22/25 | Loss: 0.00068054
Iteration 23/25 | Loss: 0.00068054
Iteration 24/25 | Loss: 0.00068054
Iteration 25/25 | Loss: 0.00068054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40199816
Iteration 2/25 | Loss: 0.00095169
Iteration 3/25 | Loss: 0.00081918
Iteration 4/25 | Loss: 0.00081917
Iteration 5/25 | Loss: 0.00081917
Iteration 6/25 | Loss: 0.00081917
Iteration 7/25 | Loss: 0.00081917
Iteration 8/25 | Loss: 0.00081917
Iteration 9/25 | Loss: 0.00081917
Iteration 10/25 | Loss: 0.00081917
Iteration 11/25 | Loss: 0.00081917
Iteration 12/25 | Loss: 0.00081917
Iteration 13/25 | Loss: 0.00081917
Iteration 14/25 | Loss: 0.00081917
Iteration 15/25 | Loss: 0.00081917
Iteration 16/25 | Loss: 0.00081917
Iteration 17/25 | Loss: 0.00081917
Iteration 18/25 | Loss: 0.00081917
Iteration 19/25 | Loss: 0.00081917
Iteration 20/25 | Loss: 0.00081917
Iteration 21/25 | Loss: 0.00081917
Iteration 22/25 | Loss: 0.00081917
Iteration 23/25 | Loss: 0.00081917
Iteration 24/25 | Loss: 0.00081917
Iteration 25/25 | Loss: 0.00081917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081917
Iteration 2/1000 | Loss: 0.00016116
Iteration 3/1000 | Loss: 0.00010909
Iteration 4/1000 | Loss: 0.00008705
Iteration 5/1000 | Loss: 0.00007493
Iteration 6/1000 | Loss: 0.00006922
Iteration 7/1000 | Loss: 0.00006537
Iteration 8/1000 | Loss: 0.00006255
Iteration 9/1000 | Loss: 0.00006002
Iteration 10/1000 | Loss: 0.00289349
Iteration 11/1000 | Loss: 0.00815198
Iteration 12/1000 | Loss: 0.00036582
Iteration 13/1000 | Loss: 0.00018082
Iteration 14/1000 | Loss: 0.00006193
Iteration 15/1000 | Loss: 0.00028076
Iteration 16/1000 | Loss: 0.00003235
Iteration 17/1000 | Loss: 0.00016755
Iteration 18/1000 | Loss: 0.00044443
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00011719
Iteration 21/1000 | Loss: 0.00004094
Iteration 22/1000 | Loss: 0.00009887
Iteration 23/1000 | Loss: 0.00007677
Iteration 24/1000 | Loss: 0.00004641
Iteration 25/1000 | Loss: 0.00011333
Iteration 26/1000 | Loss: 0.00003014
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00014893
Iteration 29/1000 | Loss: 0.00004552
Iteration 30/1000 | Loss: 0.00034453
Iteration 31/1000 | Loss: 0.00005219
Iteration 32/1000 | Loss: 0.00008577
Iteration 33/1000 | Loss: 0.00004327
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00003399
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001248
Iteration 39/1000 | Loss: 0.00001247
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001226
Iteration 45/1000 | Loss: 0.00004347
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00003716
Iteration 48/1000 | Loss: 0.00004551
Iteration 49/1000 | Loss: 0.00011689
Iteration 50/1000 | Loss: 0.00010754
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00003556
Iteration 53/1000 | Loss: 0.00009039
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00003188
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00003443
Iteration 64/1000 | Loss: 0.00003261
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00002498
Iteration 68/1000 | Loss: 0.00001224
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00001220
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00002434
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001213
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001213
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001212
Iteration 112/1000 | Loss: 0.00001212
Iteration 113/1000 | Loss: 0.00002482
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001214
Iteration 116/1000 | Loss: 0.00001214
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001211
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001210
Iteration 133/1000 | Loss: 0.00001210
Iteration 134/1000 | Loss: 0.00001210
Iteration 135/1000 | Loss: 0.00001210
Iteration 136/1000 | Loss: 0.00001210
Iteration 137/1000 | Loss: 0.00001210
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Iteration 143/1000 | Loss: 0.00001209
Iteration 144/1000 | Loss: 0.00001209
Iteration 145/1000 | Loss: 0.00001209
Iteration 146/1000 | Loss: 0.00001209
Iteration 147/1000 | Loss: 0.00001209
Iteration 148/1000 | Loss: 0.00001209
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00003230
Iteration 151/1000 | Loss: 0.00002278
Iteration 152/1000 | Loss: 0.00001341
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001214
Iteration 155/1000 | Loss: 0.00001214
Iteration 156/1000 | Loss: 0.00001214
Iteration 157/1000 | Loss: 0.00001214
Iteration 158/1000 | Loss: 0.00001214
Iteration 159/1000 | Loss: 0.00001214
Iteration 160/1000 | Loss: 0.00001214
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001213
Iteration 163/1000 | Loss: 0.00001213
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001213
Iteration 166/1000 | Loss: 0.00001213
Iteration 167/1000 | Loss: 0.00001213
Iteration 168/1000 | Loss: 0.00001213
Iteration 169/1000 | Loss: 0.00001213
Iteration 170/1000 | Loss: 0.00001213
Iteration 171/1000 | Loss: 0.00001213
Iteration 172/1000 | Loss: 0.00001213
Iteration 173/1000 | Loss: 0.00001213
Iteration 174/1000 | Loss: 0.00001213
Iteration 175/1000 | Loss: 0.00001213
Iteration 176/1000 | Loss: 0.00001213
Iteration 177/1000 | Loss: 0.00001213
Iteration 178/1000 | Loss: 0.00001213
Iteration 179/1000 | Loss: 0.00001213
Iteration 180/1000 | Loss: 0.00001213
Iteration 181/1000 | Loss: 0.00001213
Iteration 182/1000 | Loss: 0.00001213
Iteration 183/1000 | Loss: 0.00001213
Iteration 184/1000 | Loss: 0.00001213
Iteration 185/1000 | Loss: 0.00001213
Iteration 186/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2132582924095914e-05, 1.2132582924095914e-05, 1.2132582924095914e-05, 1.2132582924095914e-05, 1.2132582924095914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2132582924095914e-05

Optimization complete. Final v2v error: 2.964557647705078 mm

Highest mean error: 3.5848441123962402 mm for frame 89

Lowest mean error: 2.8251004219055176 mm for frame 17

Saving results

Total time: 128.27783918380737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424563
Iteration 2/25 | Loss: 0.00075348
Iteration 3/25 | Loss: 0.00057548
Iteration 4/25 | Loss: 0.00056128
Iteration 5/25 | Loss: 0.00055505
Iteration 6/25 | Loss: 0.00055352
Iteration 7/25 | Loss: 0.00055339
Iteration 8/25 | Loss: 0.00055339
Iteration 9/25 | Loss: 0.00055339
Iteration 10/25 | Loss: 0.00055339
Iteration 11/25 | Loss: 0.00055339
Iteration 12/25 | Loss: 0.00055339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000553392746951431, 0.000553392746951431, 0.000553392746951431, 0.000553392746951431, 0.000553392746951431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000553392746951431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49284685
Iteration 2/25 | Loss: 0.00023316
Iteration 3/25 | Loss: 0.00023316
Iteration 4/25 | Loss: 0.00023316
Iteration 5/25 | Loss: 0.00023315
Iteration 6/25 | Loss: 0.00023315
Iteration 7/25 | Loss: 0.00023315
Iteration 8/25 | Loss: 0.00023315
Iteration 9/25 | Loss: 0.00023315
Iteration 10/25 | Loss: 0.00023315
Iteration 11/25 | Loss: 0.00023315
Iteration 12/25 | Loss: 0.00023315
Iteration 13/25 | Loss: 0.00023315
Iteration 14/25 | Loss: 0.00023315
Iteration 15/25 | Loss: 0.00023315
Iteration 16/25 | Loss: 0.00023315
Iteration 17/25 | Loss: 0.00023315
Iteration 18/25 | Loss: 0.00023315
Iteration 19/25 | Loss: 0.00023315
Iteration 20/25 | Loss: 0.00023315
Iteration 21/25 | Loss: 0.00023315
Iteration 22/25 | Loss: 0.00023315
Iteration 23/25 | Loss: 0.00023315
Iteration 24/25 | Loss: 0.00023315
Iteration 25/25 | Loss: 0.00023315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023315
Iteration 2/1000 | Loss: 0.00001808
Iteration 3/1000 | Loss: 0.00001191
Iteration 4/1000 | Loss: 0.00001117
Iteration 5/1000 | Loss: 0.00001073
Iteration 6/1000 | Loss: 0.00001054
Iteration 7/1000 | Loss: 0.00001049
Iteration 8/1000 | Loss: 0.00001046
Iteration 9/1000 | Loss: 0.00001045
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001029
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001023
Iteration 22/1000 | Loss: 0.00001018
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001018
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00001013
Iteration 35/1000 | Loss: 0.00001013
Iteration 36/1000 | Loss: 0.00001013
Iteration 37/1000 | Loss: 0.00001013
Iteration 38/1000 | Loss: 0.00001012
Iteration 39/1000 | Loss: 0.00001012
Iteration 40/1000 | Loss: 0.00001012
Iteration 41/1000 | Loss: 0.00001012
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001011
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001007
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001006
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001005
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001005
Iteration 67/1000 | Loss: 0.00001005
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001005
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001004
Iteration 74/1000 | Loss: 0.00001004
Iteration 75/1000 | Loss: 0.00001004
Iteration 76/1000 | Loss: 0.00001003
Iteration 77/1000 | Loss: 0.00001003
Iteration 78/1000 | Loss: 0.00001003
Iteration 79/1000 | Loss: 0.00001003
Iteration 80/1000 | Loss: 0.00001003
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00001002
Iteration 85/1000 | Loss: 0.00001002
Iteration 86/1000 | Loss: 0.00001002
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001002
Iteration 89/1000 | Loss: 0.00001002
Iteration 90/1000 | Loss: 0.00001002
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001001
Iteration 93/1000 | Loss: 0.00001001
Iteration 94/1000 | Loss: 0.00001001
Iteration 95/1000 | Loss: 0.00001001
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001001
Iteration 104/1000 | Loss: 0.00001001
Iteration 105/1000 | Loss: 0.00001001
Iteration 106/1000 | Loss: 0.00001001
Iteration 107/1000 | Loss: 0.00001001
Iteration 108/1000 | Loss: 0.00001001
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001001
Iteration 113/1000 | Loss: 0.00001001
Iteration 114/1000 | Loss: 0.00001001
Iteration 115/1000 | Loss: 0.00001001
Iteration 116/1000 | Loss: 0.00001001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.0010221558331978e-05, 1.0010221558331978e-05, 1.0010221558331978e-05, 1.0010221558331978e-05, 1.0010221558331978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0010221558331978e-05

Optimization complete. Final v2v error: 2.653552293777466 mm

Highest mean error: 2.796091079711914 mm for frame 195

Lowest mean error: 2.551586627960205 mm for frame 17

Saving results

Total time: 31.907479763031006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740920
Iteration 2/25 | Loss: 0.00098285
Iteration 3/25 | Loss: 0.00071295
Iteration 4/25 | Loss: 0.00066083
Iteration 5/25 | Loss: 0.00065504
Iteration 6/25 | Loss: 0.00065366
Iteration 7/25 | Loss: 0.00065351
Iteration 8/25 | Loss: 0.00065351
Iteration 9/25 | Loss: 0.00065351
Iteration 10/25 | Loss: 0.00065351
Iteration 11/25 | Loss: 0.00065351
Iteration 12/25 | Loss: 0.00065351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006535074207931757, 0.0006535074207931757, 0.0006535074207931757, 0.0006535074207931757, 0.0006535074207931757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006535074207931757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39912438
Iteration 2/25 | Loss: 0.00031064
Iteration 3/25 | Loss: 0.00031062
Iteration 4/25 | Loss: 0.00031062
Iteration 5/25 | Loss: 0.00031062
Iteration 6/25 | Loss: 0.00031062
Iteration 7/25 | Loss: 0.00031062
Iteration 8/25 | Loss: 0.00031062
Iteration 9/25 | Loss: 0.00031062
Iteration 10/25 | Loss: 0.00031062
Iteration 11/25 | Loss: 0.00031062
Iteration 12/25 | Loss: 0.00031062
Iteration 13/25 | Loss: 0.00031062
Iteration 14/25 | Loss: 0.00031062
Iteration 15/25 | Loss: 0.00031062
Iteration 16/25 | Loss: 0.00031062
Iteration 17/25 | Loss: 0.00031062
Iteration 18/25 | Loss: 0.00031062
Iteration 19/25 | Loss: 0.00031062
Iteration 20/25 | Loss: 0.00031062
Iteration 21/25 | Loss: 0.00031062
Iteration 22/25 | Loss: 0.00031062
Iteration 23/25 | Loss: 0.00031062
Iteration 24/25 | Loss: 0.00031062
Iteration 25/25 | Loss: 0.00031062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031062
Iteration 2/1000 | Loss: 0.00002885
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00002062
Iteration 5/1000 | Loss: 0.00001929
Iteration 6/1000 | Loss: 0.00001868
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00001784
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001750
Iteration 13/1000 | Loss: 0.00001740
Iteration 14/1000 | Loss: 0.00001737
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001716
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001714
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001708
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001707
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001707
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001707
Iteration 52/1000 | Loss: 0.00001707
Iteration 53/1000 | Loss: 0.00001707
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001707
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001707
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00001704
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001703
Iteration 95/1000 | Loss: 0.00001703
Iteration 96/1000 | Loss: 0.00001703
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00001703
Iteration 105/1000 | Loss: 0.00001703
Iteration 106/1000 | Loss: 0.00001703
Iteration 107/1000 | Loss: 0.00001703
Iteration 108/1000 | Loss: 0.00001703
Iteration 109/1000 | Loss: 0.00001703
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001703
Iteration 112/1000 | Loss: 0.00001703
Iteration 113/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.7027534340741113e-05, 1.7027534340741113e-05, 1.7027534340741113e-05, 1.7027534340741113e-05, 1.7027534340741113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7027534340741113e-05

Optimization complete. Final v2v error: 3.4759442806243896 mm

Highest mean error: 3.9304842948913574 mm for frame 0

Lowest mean error: 3.1444950103759766 mm for frame 75

Saving results

Total time: 31.895545721054077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01127166
Iteration 2/25 | Loss: 0.00218077
Iteration 3/25 | Loss: 0.00119796
Iteration 4/25 | Loss: 0.00099020
Iteration 5/25 | Loss: 0.00091906
Iteration 6/25 | Loss: 0.00089351
Iteration 7/25 | Loss: 0.00087893
Iteration 8/25 | Loss: 0.00086527
Iteration 9/25 | Loss: 0.00085754
Iteration 10/25 | Loss: 0.00085351
Iteration 11/25 | Loss: 0.00085323
Iteration 12/25 | Loss: 0.00084752
Iteration 13/25 | Loss: 0.00084442
Iteration 14/25 | Loss: 0.00084388
Iteration 15/25 | Loss: 0.00084376
Iteration 16/25 | Loss: 0.00084376
Iteration 17/25 | Loss: 0.00084376
Iteration 18/25 | Loss: 0.00084376
Iteration 19/25 | Loss: 0.00084376
Iteration 20/25 | Loss: 0.00084376
Iteration 21/25 | Loss: 0.00084376
Iteration 22/25 | Loss: 0.00084376
Iteration 23/25 | Loss: 0.00084376
Iteration 24/25 | Loss: 0.00084376
Iteration 25/25 | Loss: 0.00084375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53905487
Iteration 2/25 | Loss: 0.00086439
Iteration 3/25 | Loss: 0.00086439
Iteration 4/25 | Loss: 0.00086439
Iteration 5/25 | Loss: 0.00086438
Iteration 6/25 | Loss: 0.00086438
Iteration 7/25 | Loss: 0.00086438
Iteration 8/25 | Loss: 0.00086438
Iteration 9/25 | Loss: 0.00086438
Iteration 10/25 | Loss: 0.00086438
Iteration 11/25 | Loss: 0.00086438
Iteration 12/25 | Loss: 0.00086438
Iteration 13/25 | Loss: 0.00086438
Iteration 14/25 | Loss: 0.00086438
Iteration 15/25 | Loss: 0.00086438
Iteration 16/25 | Loss: 0.00086438
Iteration 17/25 | Loss: 0.00086438
Iteration 18/25 | Loss: 0.00086438
Iteration 19/25 | Loss: 0.00086438
Iteration 20/25 | Loss: 0.00086438
Iteration 21/25 | Loss: 0.00086438
Iteration 22/25 | Loss: 0.00086438
Iteration 23/25 | Loss: 0.00086438
Iteration 24/25 | Loss: 0.00086438
Iteration 25/25 | Loss: 0.00086438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086438
Iteration 2/1000 | Loss: 0.00015655
Iteration 3/1000 | Loss: 0.00013873
Iteration 4/1000 | Loss: 0.00009255
Iteration 5/1000 | Loss: 0.00006812
Iteration 6/1000 | Loss: 0.00006051
Iteration 7/1000 | Loss: 0.00005764
Iteration 8/1000 | Loss: 0.00007738
Iteration 9/1000 | Loss: 0.00019810
Iteration 10/1000 | Loss: 0.00007678
Iteration 11/1000 | Loss: 0.00005248
Iteration 12/1000 | Loss: 0.00007384
Iteration 13/1000 | Loss: 0.00007658
Iteration 14/1000 | Loss: 0.00007706
Iteration 15/1000 | Loss: 0.00005432
Iteration 16/1000 | Loss: 0.00005117
Iteration 17/1000 | Loss: 0.00004790
Iteration 18/1000 | Loss: 0.00004764
Iteration 19/1000 | Loss: 0.00008816
Iteration 20/1000 | Loss: 0.00005764
Iteration 21/1000 | Loss: 0.00004926
Iteration 22/1000 | Loss: 0.00004152
Iteration 23/1000 | Loss: 0.00004891
Iteration 24/1000 | Loss: 0.00006145
Iteration 25/1000 | Loss: 0.00006140
Iteration 26/1000 | Loss: 0.00005666
Iteration 27/1000 | Loss: 0.00006929
Iteration 28/1000 | Loss: 0.00005925
Iteration 29/1000 | Loss: 0.00007168
Iteration 30/1000 | Loss: 0.00005916
Iteration 31/1000 | Loss: 0.00004664
Iteration 32/1000 | Loss: 0.00004953
Iteration 33/1000 | Loss: 0.00006905
Iteration 34/1000 | Loss: 0.00011993
Iteration 35/1000 | Loss: 0.00004404
Iteration 36/1000 | Loss: 0.00004768
Iteration 37/1000 | Loss: 0.00004835
Iteration 38/1000 | Loss: 0.00006579
Iteration 39/1000 | Loss: 0.00006904
Iteration 40/1000 | Loss: 0.00005859
Iteration 41/1000 | Loss: 0.00004959
Iteration 42/1000 | Loss: 0.00004639
Iteration 43/1000 | Loss: 0.00005245
Iteration 44/1000 | Loss: 0.00004107
Iteration 45/1000 | Loss: 0.00003927
Iteration 46/1000 | Loss: 0.00004938
Iteration 47/1000 | Loss: 0.00005412
Iteration 48/1000 | Loss: 0.00003837
Iteration 49/1000 | Loss: 0.00007104
Iteration 50/1000 | Loss: 0.00006861
Iteration 51/1000 | Loss: 0.00008069
Iteration 52/1000 | Loss: 0.00007176
Iteration 53/1000 | Loss: 0.00020910
Iteration 54/1000 | Loss: 0.00017327
Iteration 55/1000 | Loss: 0.00005186
Iteration 56/1000 | Loss: 0.00007675
Iteration 57/1000 | Loss: 0.00008230
Iteration 58/1000 | Loss: 0.00007334
Iteration 59/1000 | Loss: 0.00007216
Iteration 60/1000 | Loss: 0.00008569
Iteration 61/1000 | Loss: 0.00005845
Iteration 62/1000 | Loss: 0.00008442
Iteration 63/1000 | Loss: 0.00005766
Iteration 64/1000 | Loss: 0.00007019
Iteration 65/1000 | Loss: 0.00005645
Iteration 66/1000 | Loss: 0.00005818
Iteration 67/1000 | Loss: 0.00005541
Iteration 68/1000 | Loss: 0.00005237
Iteration 69/1000 | Loss: 0.00005879
Iteration 70/1000 | Loss: 0.00006030
Iteration 71/1000 | Loss: 0.00005547
Iteration 72/1000 | Loss: 0.00005553
Iteration 73/1000 | Loss: 0.00006406
Iteration 74/1000 | Loss: 0.00006311
Iteration 75/1000 | Loss: 0.00006268
Iteration 76/1000 | Loss: 0.00004436
Iteration 77/1000 | Loss: 0.00005153
Iteration 78/1000 | Loss: 0.00005444
Iteration 79/1000 | Loss: 0.00003957
Iteration 80/1000 | Loss: 0.00003681
Iteration 81/1000 | Loss: 0.00003539
Iteration 82/1000 | Loss: 0.00004876
Iteration 83/1000 | Loss: 0.00004479
Iteration 84/1000 | Loss: 0.00003685
Iteration 85/1000 | Loss: 0.00005167
Iteration 86/1000 | Loss: 0.00004415
Iteration 87/1000 | Loss: 0.00004918
Iteration 88/1000 | Loss: 0.00004227
Iteration 89/1000 | Loss: 0.00004417
Iteration 90/1000 | Loss: 0.00009991
Iteration 91/1000 | Loss: 0.00004654
Iteration 92/1000 | Loss: 0.00004595
Iteration 93/1000 | Loss: 0.00004138
Iteration 94/1000 | Loss: 0.00004251
Iteration 95/1000 | Loss: 0.00003854
Iteration 96/1000 | Loss: 0.00004212
Iteration 97/1000 | Loss: 0.00011185
Iteration 98/1000 | Loss: 0.00005245
Iteration 99/1000 | Loss: 0.00004648
Iteration 100/1000 | Loss: 0.00004476
Iteration 101/1000 | Loss: 0.00003433
Iteration 102/1000 | Loss: 0.00003886
Iteration 103/1000 | Loss: 0.00012973
Iteration 104/1000 | Loss: 0.00005893
Iteration 105/1000 | Loss: 0.00004129
Iteration 106/1000 | Loss: 0.00003453
Iteration 107/1000 | Loss: 0.00006234
Iteration 108/1000 | Loss: 0.00004672
Iteration 109/1000 | Loss: 0.00005200
Iteration 110/1000 | Loss: 0.00003922
Iteration 111/1000 | Loss: 0.00004474
Iteration 112/1000 | Loss: 0.00004382
Iteration 113/1000 | Loss: 0.00003869
Iteration 114/1000 | Loss: 0.00003677
Iteration 115/1000 | Loss: 0.00004003
Iteration 116/1000 | Loss: 0.00003740
Iteration 117/1000 | Loss: 0.00004156
Iteration 118/1000 | Loss: 0.00003478
Iteration 119/1000 | Loss: 0.00003350
Iteration 120/1000 | Loss: 0.00003306
Iteration 121/1000 | Loss: 0.00003297
Iteration 122/1000 | Loss: 0.00003296
Iteration 123/1000 | Loss: 0.00003287
Iteration 124/1000 | Loss: 0.00003274
Iteration 125/1000 | Loss: 0.00003273
Iteration 126/1000 | Loss: 0.00003271
Iteration 127/1000 | Loss: 0.00003261
Iteration 128/1000 | Loss: 0.00003256
Iteration 129/1000 | Loss: 0.00003255
Iteration 130/1000 | Loss: 0.00003254
Iteration 131/1000 | Loss: 0.00003251
Iteration 132/1000 | Loss: 0.00003249
Iteration 133/1000 | Loss: 0.00003249
Iteration 134/1000 | Loss: 0.00003249
Iteration 135/1000 | Loss: 0.00003249
Iteration 136/1000 | Loss: 0.00003248
Iteration 137/1000 | Loss: 0.00003248
Iteration 138/1000 | Loss: 0.00003240
Iteration 139/1000 | Loss: 0.00003240
Iteration 140/1000 | Loss: 0.00003240
Iteration 141/1000 | Loss: 0.00003239
Iteration 142/1000 | Loss: 0.00003239
Iteration 143/1000 | Loss: 0.00003239
Iteration 144/1000 | Loss: 0.00003239
Iteration 145/1000 | Loss: 0.00003239
Iteration 146/1000 | Loss: 0.00003239
Iteration 147/1000 | Loss: 0.00003238
Iteration 148/1000 | Loss: 0.00003238
Iteration 149/1000 | Loss: 0.00003238
Iteration 150/1000 | Loss: 0.00003237
Iteration 151/1000 | Loss: 0.00003236
Iteration 152/1000 | Loss: 0.00003235
Iteration 153/1000 | Loss: 0.00003234
Iteration 154/1000 | Loss: 0.00003233
Iteration 155/1000 | Loss: 0.00003232
Iteration 156/1000 | Loss: 0.00003232
Iteration 157/1000 | Loss: 0.00003231
Iteration 158/1000 | Loss: 0.00003231
Iteration 159/1000 | Loss: 0.00003230
Iteration 160/1000 | Loss: 0.00003230
Iteration 161/1000 | Loss: 0.00003230
Iteration 162/1000 | Loss: 0.00003230
Iteration 163/1000 | Loss: 0.00003229
Iteration 164/1000 | Loss: 0.00003229
Iteration 165/1000 | Loss: 0.00003228
Iteration 166/1000 | Loss: 0.00003228
Iteration 167/1000 | Loss: 0.00003228
Iteration 168/1000 | Loss: 0.00003227
Iteration 169/1000 | Loss: 0.00003227
Iteration 170/1000 | Loss: 0.00003226
Iteration 171/1000 | Loss: 0.00003226
Iteration 172/1000 | Loss: 0.00003225
Iteration 173/1000 | Loss: 0.00003225
Iteration 174/1000 | Loss: 0.00003225
Iteration 175/1000 | Loss: 0.00003224
Iteration 176/1000 | Loss: 0.00003222
Iteration 177/1000 | Loss: 0.00003222
Iteration 178/1000 | Loss: 0.00003222
Iteration 179/1000 | Loss: 0.00003222
Iteration 180/1000 | Loss: 0.00003221
Iteration 181/1000 | Loss: 0.00003220
Iteration 182/1000 | Loss: 0.00003220
Iteration 183/1000 | Loss: 0.00003220
Iteration 184/1000 | Loss: 0.00003220
Iteration 185/1000 | Loss: 0.00003220
Iteration 186/1000 | Loss: 0.00003220
Iteration 187/1000 | Loss: 0.00003219
Iteration 188/1000 | Loss: 0.00003219
Iteration 189/1000 | Loss: 0.00003219
Iteration 190/1000 | Loss: 0.00003219
Iteration 191/1000 | Loss: 0.00003219
Iteration 192/1000 | Loss: 0.00003219
Iteration 193/1000 | Loss: 0.00003219
Iteration 194/1000 | Loss: 0.00003218
Iteration 195/1000 | Loss: 0.00003218
Iteration 196/1000 | Loss: 0.00003218
Iteration 197/1000 | Loss: 0.00003217
Iteration 198/1000 | Loss: 0.00003216
Iteration 199/1000 | Loss: 0.00003216
Iteration 200/1000 | Loss: 0.00003215
Iteration 201/1000 | Loss: 0.00003215
Iteration 202/1000 | Loss: 0.00003214
Iteration 203/1000 | Loss: 0.00003214
Iteration 204/1000 | Loss: 0.00003213
Iteration 205/1000 | Loss: 0.00003213
Iteration 206/1000 | Loss: 0.00003212
Iteration 207/1000 | Loss: 0.00003212
Iteration 208/1000 | Loss: 0.00003212
Iteration 209/1000 | Loss: 0.00003211
Iteration 210/1000 | Loss: 0.00003211
Iteration 211/1000 | Loss: 0.00003211
Iteration 212/1000 | Loss: 0.00003211
Iteration 213/1000 | Loss: 0.00003211
Iteration 214/1000 | Loss: 0.00003211
Iteration 215/1000 | Loss: 0.00003211
Iteration 216/1000 | Loss: 0.00003211
Iteration 217/1000 | Loss: 0.00003211
Iteration 218/1000 | Loss: 0.00003211
Iteration 219/1000 | Loss: 0.00003211
Iteration 220/1000 | Loss: 0.00003210
Iteration 221/1000 | Loss: 0.00003210
Iteration 222/1000 | Loss: 0.00003210
Iteration 223/1000 | Loss: 0.00003209
Iteration 224/1000 | Loss: 0.00003209
Iteration 225/1000 | Loss: 0.00003209
Iteration 226/1000 | Loss: 0.00003209
Iteration 227/1000 | Loss: 0.00003209
Iteration 228/1000 | Loss: 0.00003209
Iteration 229/1000 | Loss: 0.00003208
Iteration 230/1000 | Loss: 0.00003208
Iteration 231/1000 | Loss: 0.00003206
Iteration 232/1000 | Loss: 0.00003206
Iteration 233/1000 | Loss: 0.00003206
Iteration 234/1000 | Loss: 0.00003206
Iteration 235/1000 | Loss: 0.00003206
Iteration 236/1000 | Loss: 0.00003206
Iteration 237/1000 | Loss: 0.00003206
Iteration 238/1000 | Loss: 0.00003206
Iteration 239/1000 | Loss: 0.00003205
Iteration 240/1000 | Loss: 0.00003205
Iteration 241/1000 | Loss: 0.00003205
Iteration 242/1000 | Loss: 0.00003205
Iteration 243/1000 | Loss: 0.00003205
Iteration 244/1000 | Loss: 0.00003205
Iteration 245/1000 | Loss: 0.00003204
Iteration 246/1000 | Loss: 0.00003204
Iteration 247/1000 | Loss: 0.00003202
Iteration 248/1000 | Loss: 0.00003202
Iteration 249/1000 | Loss: 0.00003202
Iteration 250/1000 | Loss: 0.00003202
Iteration 251/1000 | Loss: 0.00003202
Iteration 252/1000 | Loss: 0.00003202
Iteration 253/1000 | Loss: 0.00003202
Iteration 254/1000 | Loss: 0.00003202
Iteration 255/1000 | Loss: 0.00003202
Iteration 256/1000 | Loss: 0.00003202
Iteration 257/1000 | Loss: 0.00003202
Iteration 258/1000 | Loss: 0.00003202
Iteration 259/1000 | Loss: 0.00003202
Iteration 260/1000 | Loss: 0.00003201
Iteration 261/1000 | Loss: 0.00003201
Iteration 262/1000 | Loss: 0.00003200
Iteration 263/1000 | Loss: 0.00003200
Iteration 264/1000 | Loss: 0.00003199
Iteration 265/1000 | Loss: 0.00003199
Iteration 266/1000 | Loss: 0.00003198
Iteration 267/1000 | Loss: 0.00003198
Iteration 268/1000 | Loss: 0.00003198
Iteration 269/1000 | Loss: 0.00003198
Iteration 270/1000 | Loss: 0.00003198
Iteration 271/1000 | Loss: 0.00003198
Iteration 272/1000 | Loss: 0.00003198
Iteration 273/1000 | Loss: 0.00003198
Iteration 274/1000 | Loss: 0.00003197
Iteration 275/1000 | Loss: 0.00003197
Iteration 276/1000 | Loss: 0.00003196
Iteration 277/1000 | Loss: 0.00003196
Iteration 278/1000 | Loss: 0.00003196
Iteration 279/1000 | Loss: 0.00003196
Iteration 280/1000 | Loss: 0.00003196
Iteration 281/1000 | Loss: 0.00003196
Iteration 282/1000 | Loss: 0.00003196
Iteration 283/1000 | Loss: 0.00003195
Iteration 284/1000 | Loss: 0.00003195
Iteration 285/1000 | Loss: 0.00003195
Iteration 286/1000 | Loss: 0.00003195
Iteration 287/1000 | Loss: 0.00003195
Iteration 288/1000 | Loss: 0.00003195
Iteration 289/1000 | Loss: 0.00003194
Iteration 290/1000 | Loss: 0.00003194
Iteration 291/1000 | Loss: 0.00003194
Iteration 292/1000 | Loss: 0.00003194
Iteration 293/1000 | Loss: 0.00003194
Iteration 294/1000 | Loss: 0.00003194
Iteration 295/1000 | Loss: 0.00003194
Iteration 296/1000 | Loss: 0.00003194
Iteration 297/1000 | Loss: 0.00003193
Iteration 298/1000 | Loss: 0.00003193
Iteration 299/1000 | Loss: 0.00003193
Iteration 300/1000 | Loss: 0.00003193
Iteration 301/1000 | Loss: 0.00003193
Iteration 302/1000 | Loss: 0.00003193
Iteration 303/1000 | Loss: 0.00003193
Iteration 304/1000 | Loss: 0.00003193
Iteration 305/1000 | Loss: 0.00003192
Iteration 306/1000 | Loss: 0.00003192
Iteration 307/1000 | Loss: 0.00003192
Iteration 308/1000 | Loss: 0.00003192
Iteration 309/1000 | Loss: 0.00003192
Iteration 310/1000 | Loss: 0.00003192
Iteration 311/1000 | Loss: 0.00003192
Iteration 312/1000 | Loss: 0.00003192
Iteration 313/1000 | Loss: 0.00003192
Iteration 314/1000 | Loss: 0.00003192
Iteration 315/1000 | Loss: 0.00003192
Iteration 316/1000 | Loss: 0.00003192
Iteration 317/1000 | Loss: 0.00003192
Iteration 318/1000 | Loss: 0.00003192
Iteration 319/1000 | Loss: 0.00003192
Iteration 320/1000 | Loss: 0.00003192
Iteration 321/1000 | Loss: 0.00003192
Iteration 322/1000 | Loss: 0.00003192
Iteration 323/1000 | Loss: 0.00003192
Iteration 324/1000 | Loss: 0.00003191
Iteration 325/1000 | Loss: 0.00003191
Iteration 326/1000 | Loss: 0.00003191
Iteration 327/1000 | Loss: 0.00003191
Iteration 328/1000 | Loss: 0.00003191
Iteration 329/1000 | Loss: 0.00003191
Iteration 330/1000 | Loss: 0.00003191
Iteration 331/1000 | Loss: 0.00003191
Iteration 332/1000 | Loss: 0.00003191
Iteration 333/1000 | Loss: 0.00003191
Iteration 334/1000 | Loss: 0.00003191
Iteration 335/1000 | Loss: 0.00003191
Iteration 336/1000 | Loss: 0.00003191
Iteration 337/1000 | Loss: 0.00003191
Iteration 338/1000 | Loss: 0.00003190
Iteration 339/1000 | Loss: 0.00003190
Iteration 340/1000 | Loss: 0.00003190
Iteration 341/1000 | Loss: 0.00003190
Iteration 342/1000 | Loss: 0.00003190
Iteration 343/1000 | Loss: 0.00003190
Iteration 344/1000 | Loss: 0.00003190
Iteration 345/1000 | Loss: 0.00003190
Iteration 346/1000 | Loss: 0.00003190
Iteration 347/1000 | Loss: 0.00003190
Iteration 348/1000 | Loss: 0.00003190
Iteration 349/1000 | Loss: 0.00003190
Iteration 350/1000 | Loss: 0.00003190
Iteration 351/1000 | Loss: 0.00003190
Iteration 352/1000 | Loss: 0.00003190
Iteration 353/1000 | Loss: 0.00003190
Iteration 354/1000 | Loss: 0.00003190
Iteration 355/1000 | Loss: 0.00003190
Iteration 356/1000 | Loss: 0.00003190
Iteration 357/1000 | Loss: 0.00003190
Iteration 358/1000 | Loss: 0.00003190
Iteration 359/1000 | Loss: 0.00003190
Iteration 360/1000 | Loss: 0.00003190
Iteration 361/1000 | Loss: 0.00003190
Iteration 362/1000 | Loss: 0.00003190
Iteration 363/1000 | Loss: 0.00003190
Iteration 364/1000 | Loss: 0.00003190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 364. Stopping optimization.
Last 5 losses: [3.190269853803329e-05, 3.190269853803329e-05, 3.190269853803329e-05, 3.190269853803329e-05, 3.190269853803329e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.190269853803329e-05

Optimization complete. Final v2v error: 4.472271919250488 mm

Highest mean error: 6.437399864196777 mm for frame 192

Lowest mean error: 3.0910122394561768 mm for frame 5

Saving results

Total time: 250.49902653694153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418138
Iteration 2/25 | Loss: 0.00079861
Iteration 3/25 | Loss: 0.00061159
Iteration 4/25 | Loss: 0.00057842
Iteration 5/25 | Loss: 0.00056602
Iteration 6/25 | Loss: 0.00056223
Iteration 7/25 | Loss: 0.00056112
Iteration 8/25 | Loss: 0.00056098
Iteration 9/25 | Loss: 0.00056098
Iteration 10/25 | Loss: 0.00056098
Iteration 11/25 | Loss: 0.00056098
Iteration 12/25 | Loss: 0.00056098
Iteration 13/25 | Loss: 0.00056098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005609766230918467, 0.0005609766230918467, 0.0005609766230918467, 0.0005609766230918467, 0.0005609766230918467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005609766230918467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45222902
Iteration 2/25 | Loss: 0.00023034
Iteration 3/25 | Loss: 0.00023033
Iteration 4/25 | Loss: 0.00023033
Iteration 5/25 | Loss: 0.00023033
Iteration 6/25 | Loss: 0.00023033
Iteration 7/25 | Loss: 0.00023033
Iteration 8/25 | Loss: 0.00023033
Iteration 9/25 | Loss: 0.00023033
Iteration 10/25 | Loss: 0.00023033
Iteration 11/25 | Loss: 0.00023033
Iteration 12/25 | Loss: 0.00023033
Iteration 13/25 | Loss: 0.00023033
Iteration 14/25 | Loss: 0.00023033
Iteration 15/25 | Loss: 0.00023033
Iteration 16/25 | Loss: 0.00023033
Iteration 17/25 | Loss: 0.00023033
Iteration 18/25 | Loss: 0.00023033
Iteration 19/25 | Loss: 0.00023033
Iteration 20/25 | Loss: 0.00023033
Iteration 21/25 | Loss: 0.00023033
Iteration 22/25 | Loss: 0.00023033
Iteration 23/25 | Loss: 0.00023033
Iteration 24/25 | Loss: 0.00023033
Iteration 25/25 | Loss: 0.00023033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023033
Iteration 2/1000 | Loss: 0.00001929
Iteration 3/1000 | Loss: 0.00001430
Iteration 4/1000 | Loss: 0.00001329
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001202
Iteration 7/1000 | Loss: 0.00001188
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001135
Iteration 13/1000 | Loss: 0.00001134
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001117
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001105
Iteration 28/1000 | Loss: 0.00001105
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001103
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001102
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001100
Iteration 41/1000 | Loss: 0.00001100
Iteration 42/1000 | Loss: 0.00001100
Iteration 43/1000 | Loss: 0.00001099
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001098
Iteration 47/1000 | Loss: 0.00001098
Iteration 48/1000 | Loss: 0.00001098
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001097
Iteration 54/1000 | Loss: 0.00001097
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001093
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001093
Iteration 86/1000 | Loss: 0.00001093
Iteration 87/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.0932242730632424e-05, 1.0932242730632424e-05, 1.0932242730632424e-05, 1.0932242730632424e-05, 1.0932242730632424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0932242730632424e-05

Optimization complete. Final v2v error: 2.8244643211364746 mm

Highest mean error: 3.755194664001465 mm for frame 61

Lowest mean error: 2.5384976863861084 mm for frame 92

Saving results

Total time: 36.71412014961243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847816
Iteration 2/25 | Loss: 0.00109920
Iteration 3/25 | Loss: 0.00070247
Iteration 4/25 | Loss: 0.00066519
Iteration 5/25 | Loss: 0.00065885
Iteration 6/25 | Loss: 0.00065700
Iteration 7/25 | Loss: 0.00065678
Iteration 8/25 | Loss: 0.00065678
Iteration 9/25 | Loss: 0.00065678
Iteration 10/25 | Loss: 0.00065678
Iteration 11/25 | Loss: 0.00065678
Iteration 12/25 | Loss: 0.00065678
Iteration 13/25 | Loss: 0.00065678
Iteration 14/25 | Loss: 0.00065678
Iteration 15/25 | Loss: 0.00065678
Iteration 16/25 | Loss: 0.00065678
Iteration 17/25 | Loss: 0.00065678
Iteration 18/25 | Loss: 0.00065678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006567814853042364, 0.0006567814853042364, 0.0006567814853042364, 0.0006567814853042364, 0.0006567814853042364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006567814853042364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32466948
Iteration 2/25 | Loss: 0.00031489
Iteration 3/25 | Loss: 0.00031486
Iteration 4/25 | Loss: 0.00031485
Iteration 5/25 | Loss: 0.00031485
Iteration 6/25 | Loss: 0.00031485
Iteration 7/25 | Loss: 0.00031485
Iteration 8/25 | Loss: 0.00031485
Iteration 9/25 | Loss: 0.00031485
Iteration 10/25 | Loss: 0.00031485
Iteration 11/25 | Loss: 0.00031485
Iteration 12/25 | Loss: 0.00031485
Iteration 13/25 | Loss: 0.00031485
Iteration 14/25 | Loss: 0.00031485
Iteration 15/25 | Loss: 0.00031485
Iteration 16/25 | Loss: 0.00031485
Iteration 17/25 | Loss: 0.00031485
Iteration 18/25 | Loss: 0.00031485
Iteration 19/25 | Loss: 0.00031485
Iteration 20/25 | Loss: 0.00031485
Iteration 21/25 | Loss: 0.00031485
Iteration 22/25 | Loss: 0.00031485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003148524556308985, 0.0003148524556308985, 0.0003148524556308985, 0.0003148524556308985, 0.0003148524556308985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003148524556308985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031485
Iteration 2/1000 | Loss: 0.00002335
Iteration 3/1000 | Loss: 0.00001710
Iteration 4/1000 | Loss: 0.00001574
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001423
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001408
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001403
Iteration 19/1000 | Loss: 0.00001403
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001402
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001402
Iteration 27/1000 | Loss: 0.00001402
Iteration 28/1000 | Loss: 0.00001402
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001400
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001398
Iteration 39/1000 | Loss: 0.00001398
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001394
Iteration 61/1000 | Loss: 0.00001394
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001390
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001389
Iteration 105/1000 | Loss: 0.00001389
Iteration 106/1000 | Loss: 0.00001389
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001388
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001388
Iteration 113/1000 | Loss: 0.00001388
Iteration 114/1000 | Loss: 0.00001388
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001387
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001387
Iteration 131/1000 | Loss: 0.00001387
Iteration 132/1000 | Loss: 0.00001387
Iteration 133/1000 | Loss: 0.00001387
Iteration 134/1000 | Loss: 0.00001387
Iteration 135/1000 | Loss: 0.00001387
Iteration 136/1000 | Loss: 0.00001387
Iteration 137/1000 | Loss: 0.00001387
Iteration 138/1000 | Loss: 0.00001387
Iteration 139/1000 | Loss: 0.00001387
Iteration 140/1000 | Loss: 0.00001387
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001387
Iteration 150/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.3874619980924763e-05, 1.3874619980924763e-05, 1.3874619980924763e-05, 1.3874619980924763e-05, 1.3874619980924763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3874619980924763e-05

Optimization complete. Final v2v error: 3.180997848510742 mm

Highest mean error: 3.385577440261841 mm for frame 157

Lowest mean error: 2.946280002593994 mm for frame 70

Saving results

Total time: 36.22950315475464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385480
Iteration 2/25 | Loss: 0.00075352
Iteration 3/25 | Loss: 0.00059159
Iteration 4/25 | Loss: 0.00056963
Iteration 5/25 | Loss: 0.00056292
Iteration 6/25 | Loss: 0.00056132
Iteration 7/25 | Loss: 0.00056068
Iteration 8/25 | Loss: 0.00056059
Iteration 9/25 | Loss: 0.00056059
Iteration 10/25 | Loss: 0.00056059
Iteration 11/25 | Loss: 0.00056059
Iteration 12/25 | Loss: 0.00056059
Iteration 13/25 | Loss: 0.00056059
Iteration 14/25 | Loss: 0.00056059
Iteration 15/25 | Loss: 0.00056059
Iteration 16/25 | Loss: 0.00056059
Iteration 17/25 | Loss: 0.00056059
Iteration 18/25 | Loss: 0.00056059
Iteration 19/25 | Loss: 0.00056059
Iteration 20/25 | Loss: 0.00056059
Iteration 21/25 | Loss: 0.00056059
Iteration 22/25 | Loss: 0.00056059
Iteration 23/25 | Loss: 0.00056059
Iteration 24/25 | Loss: 0.00056059
Iteration 25/25 | Loss: 0.00056059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52064490
Iteration 2/25 | Loss: 0.00025257
Iteration 3/25 | Loss: 0.00025257
Iteration 4/25 | Loss: 0.00025257
Iteration 5/25 | Loss: 0.00025257
Iteration 6/25 | Loss: 0.00025257
Iteration 7/25 | Loss: 0.00025257
Iteration 8/25 | Loss: 0.00025257
Iteration 9/25 | Loss: 0.00025257
Iteration 10/25 | Loss: 0.00025257
Iteration 11/25 | Loss: 0.00025257
Iteration 12/25 | Loss: 0.00025257
Iteration 13/25 | Loss: 0.00025257
Iteration 14/25 | Loss: 0.00025257
Iteration 15/25 | Loss: 0.00025257
Iteration 16/25 | Loss: 0.00025257
Iteration 17/25 | Loss: 0.00025257
Iteration 18/25 | Loss: 0.00025257
Iteration 19/25 | Loss: 0.00025257
Iteration 20/25 | Loss: 0.00025257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00025256795925088227, 0.00025256795925088227, 0.00025256795925088227, 0.00025256795925088227, 0.00025256795925088227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025256795925088227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025257
Iteration 2/1000 | Loss: 0.00002233
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001267
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001166
Iteration 7/1000 | Loss: 0.00001139
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001117
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001089
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001085
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001082
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001071
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001069
Iteration 26/1000 | Loss: 0.00001069
Iteration 27/1000 | Loss: 0.00001069
Iteration 28/1000 | Loss: 0.00001068
Iteration 29/1000 | Loss: 0.00001067
Iteration 30/1000 | Loss: 0.00001067
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001067
Iteration 33/1000 | Loss: 0.00001067
Iteration 34/1000 | Loss: 0.00001066
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001064
Iteration 38/1000 | Loss: 0.00001064
Iteration 39/1000 | Loss: 0.00001064
Iteration 40/1000 | Loss: 0.00001063
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001063
Iteration 44/1000 | Loss: 0.00001062
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001061
Iteration 47/1000 | Loss: 0.00001061
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001060
Iteration 51/1000 | Loss: 0.00001060
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001059
Iteration 54/1000 | Loss: 0.00001059
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001057
Iteration 61/1000 | Loss: 0.00001057
Iteration 62/1000 | Loss: 0.00001057
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001056
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001055
Iteration 72/1000 | Loss: 0.00001055
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001055
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001054
Iteration 79/1000 | Loss: 0.00001054
Iteration 80/1000 | Loss: 0.00001054
Iteration 81/1000 | Loss: 0.00001054
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001054
Iteration 88/1000 | Loss: 0.00001054
Iteration 89/1000 | Loss: 0.00001054
Iteration 90/1000 | Loss: 0.00001054
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001053
Iteration 94/1000 | Loss: 0.00001053
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001053
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001052
Iteration 102/1000 | Loss: 0.00001052
Iteration 103/1000 | Loss: 0.00001052
Iteration 104/1000 | Loss: 0.00001052
Iteration 105/1000 | Loss: 0.00001052
Iteration 106/1000 | Loss: 0.00001052
Iteration 107/1000 | Loss: 0.00001052
Iteration 108/1000 | Loss: 0.00001052
Iteration 109/1000 | Loss: 0.00001052
Iteration 110/1000 | Loss: 0.00001051
Iteration 111/1000 | Loss: 0.00001051
Iteration 112/1000 | Loss: 0.00001051
Iteration 113/1000 | Loss: 0.00001051
Iteration 114/1000 | Loss: 0.00001051
Iteration 115/1000 | Loss: 0.00001051
Iteration 116/1000 | Loss: 0.00001050
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001050
Iteration 120/1000 | Loss: 0.00001050
Iteration 121/1000 | Loss: 0.00001050
Iteration 122/1000 | Loss: 0.00001050
Iteration 123/1000 | Loss: 0.00001049
Iteration 124/1000 | Loss: 0.00001049
Iteration 125/1000 | Loss: 0.00001049
Iteration 126/1000 | Loss: 0.00001049
Iteration 127/1000 | Loss: 0.00001049
Iteration 128/1000 | Loss: 0.00001049
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001049
Iteration 133/1000 | Loss: 0.00001049
Iteration 134/1000 | Loss: 0.00001049
Iteration 135/1000 | Loss: 0.00001049
Iteration 136/1000 | Loss: 0.00001049
Iteration 137/1000 | Loss: 0.00001049
Iteration 138/1000 | Loss: 0.00001049
Iteration 139/1000 | Loss: 0.00001048
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001048
Iteration 142/1000 | Loss: 0.00001048
Iteration 143/1000 | Loss: 0.00001048
Iteration 144/1000 | Loss: 0.00001048
Iteration 145/1000 | Loss: 0.00001048
Iteration 146/1000 | Loss: 0.00001048
Iteration 147/1000 | Loss: 0.00001048
Iteration 148/1000 | Loss: 0.00001047
Iteration 149/1000 | Loss: 0.00001047
Iteration 150/1000 | Loss: 0.00001047
Iteration 151/1000 | Loss: 0.00001047
Iteration 152/1000 | Loss: 0.00001047
Iteration 153/1000 | Loss: 0.00001047
Iteration 154/1000 | Loss: 0.00001047
Iteration 155/1000 | Loss: 0.00001047
Iteration 156/1000 | Loss: 0.00001047
Iteration 157/1000 | Loss: 0.00001047
Iteration 158/1000 | Loss: 0.00001047
Iteration 159/1000 | Loss: 0.00001047
Iteration 160/1000 | Loss: 0.00001047
Iteration 161/1000 | Loss: 0.00001047
Iteration 162/1000 | Loss: 0.00001047
Iteration 163/1000 | Loss: 0.00001047
Iteration 164/1000 | Loss: 0.00001047
Iteration 165/1000 | Loss: 0.00001047
Iteration 166/1000 | Loss: 0.00001046
Iteration 167/1000 | Loss: 0.00001046
Iteration 168/1000 | Loss: 0.00001046
Iteration 169/1000 | Loss: 0.00001046
Iteration 170/1000 | Loss: 0.00001046
Iteration 171/1000 | Loss: 0.00001046
Iteration 172/1000 | Loss: 0.00001046
Iteration 173/1000 | Loss: 0.00001046
Iteration 174/1000 | Loss: 0.00001046
Iteration 175/1000 | Loss: 0.00001046
Iteration 176/1000 | Loss: 0.00001046
Iteration 177/1000 | Loss: 0.00001046
Iteration 178/1000 | Loss: 0.00001045
Iteration 179/1000 | Loss: 0.00001045
Iteration 180/1000 | Loss: 0.00001045
Iteration 181/1000 | Loss: 0.00001045
Iteration 182/1000 | Loss: 0.00001045
Iteration 183/1000 | Loss: 0.00001045
Iteration 184/1000 | Loss: 0.00001045
Iteration 185/1000 | Loss: 0.00001045
Iteration 186/1000 | Loss: 0.00001045
Iteration 187/1000 | Loss: 0.00001045
Iteration 188/1000 | Loss: 0.00001045
Iteration 189/1000 | Loss: 0.00001045
Iteration 190/1000 | Loss: 0.00001045
Iteration 191/1000 | Loss: 0.00001045
Iteration 192/1000 | Loss: 0.00001045
Iteration 193/1000 | Loss: 0.00001045
Iteration 194/1000 | Loss: 0.00001045
Iteration 195/1000 | Loss: 0.00001044
Iteration 196/1000 | Loss: 0.00001044
Iteration 197/1000 | Loss: 0.00001044
Iteration 198/1000 | Loss: 0.00001044
Iteration 199/1000 | Loss: 0.00001044
Iteration 200/1000 | Loss: 0.00001044
Iteration 201/1000 | Loss: 0.00001044
Iteration 202/1000 | Loss: 0.00001044
Iteration 203/1000 | Loss: 0.00001044
Iteration 204/1000 | Loss: 0.00001044
Iteration 205/1000 | Loss: 0.00001044
Iteration 206/1000 | Loss: 0.00001044
Iteration 207/1000 | Loss: 0.00001044
Iteration 208/1000 | Loss: 0.00001044
Iteration 209/1000 | Loss: 0.00001044
Iteration 210/1000 | Loss: 0.00001044
Iteration 211/1000 | Loss: 0.00001044
Iteration 212/1000 | Loss: 0.00001044
Iteration 213/1000 | Loss: 0.00001044
Iteration 214/1000 | Loss: 0.00001044
Iteration 215/1000 | Loss: 0.00001044
Iteration 216/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.044139571604319e-05, 1.044139571604319e-05, 1.044139571604319e-05, 1.044139571604319e-05, 1.044139571604319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.044139571604319e-05

Optimization complete. Final v2v error: 2.752202272415161 mm

Highest mean error: 3.718688726425171 mm for frame 35

Lowest mean error: 2.4783005714416504 mm for frame 70

Saving results

Total time: 38.31491041183472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00325013
Iteration 2/25 | Loss: 0.00075651
Iteration 3/25 | Loss: 0.00064775
Iteration 4/25 | Loss: 0.00061380
Iteration 5/25 | Loss: 0.00060796
Iteration 6/25 | Loss: 0.00060636
Iteration 7/25 | Loss: 0.00060587
Iteration 8/25 | Loss: 0.00060587
Iteration 9/25 | Loss: 0.00060587
Iteration 10/25 | Loss: 0.00060587
Iteration 11/25 | Loss: 0.00060587
Iteration 12/25 | Loss: 0.00060587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006058717845007777, 0.0006058717845007777, 0.0006058717845007777, 0.0006058717845007777, 0.0006058717845007777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006058717845007777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.88455677
Iteration 2/25 | Loss: 0.00030014
Iteration 3/25 | Loss: 0.00030009
Iteration 4/25 | Loss: 0.00030009
Iteration 5/25 | Loss: 0.00030009
Iteration 6/25 | Loss: 0.00030009
Iteration 7/25 | Loss: 0.00030009
Iteration 8/25 | Loss: 0.00030009
Iteration 9/25 | Loss: 0.00030009
Iteration 10/25 | Loss: 0.00030009
Iteration 11/25 | Loss: 0.00030009
Iteration 12/25 | Loss: 0.00030009
Iteration 13/25 | Loss: 0.00030009
Iteration 14/25 | Loss: 0.00030009
Iteration 15/25 | Loss: 0.00030009
Iteration 16/25 | Loss: 0.00030009
Iteration 17/25 | Loss: 0.00030009
Iteration 18/25 | Loss: 0.00030009
Iteration 19/25 | Loss: 0.00030009
Iteration 20/25 | Loss: 0.00030009
Iteration 21/25 | Loss: 0.00030009
Iteration 22/25 | Loss: 0.00030009
Iteration 23/25 | Loss: 0.00030009
Iteration 24/25 | Loss: 0.00030009
Iteration 25/25 | Loss: 0.00030009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030009
Iteration 2/1000 | Loss: 0.00002678
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001640
Iteration 6/1000 | Loss: 0.00001598
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001467
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001457
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001455
Iteration 22/1000 | Loss: 0.00001454
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001449
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001439
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001438
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001437
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001435
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001432
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001431
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001430
Iteration 76/1000 | Loss: 0.00001430
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001428
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4256237591325771e-05, 1.4256237591325771e-05, 1.4256237591325771e-05, 1.4256237591325771e-05, 1.4256237591325771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4256237591325771e-05

Optimization complete. Final v2v error: 3.177022933959961 mm

Highest mean error: 3.5274994373321533 mm for frame 153

Lowest mean error: 2.795156478881836 mm for frame 200

Saving results

Total time: 38.14816164970398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075837
Iteration 2/25 | Loss: 0.00096567
Iteration 3/25 | Loss: 0.00064426
Iteration 4/25 | Loss: 0.00059631
Iteration 5/25 | Loss: 0.00058991
Iteration 6/25 | Loss: 0.00058763
Iteration 7/25 | Loss: 0.00058722
Iteration 8/25 | Loss: 0.00058722
Iteration 9/25 | Loss: 0.00058722
Iteration 10/25 | Loss: 0.00058722
Iteration 11/25 | Loss: 0.00058722
Iteration 12/25 | Loss: 0.00058722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005872190813533962, 0.0005872190813533962, 0.0005872190813533962, 0.0005872190813533962, 0.0005872190813533962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005872190813533962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98162770
Iteration 2/25 | Loss: 0.00029714
Iteration 3/25 | Loss: 0.00029713
Iteration 4/25 | Loss: 0.00029713
Iteration 5/25 | Loss: 0.00029712
Iteration 6/25 | Loss: 0.00029712
Iteration 7/25 | Loss: 0.00029712
Iteration 8/25 | Loss: 0.00029712
Iteration 9/25 | Loss: 0.00029712
Iteration 10/25 | Loss: 0.00029712
Iteration 11/25 | Loss: 0.00029712
Iteration 12/25 | Loss: 0.00029712
Iteration 13/25 | Loss: 0.00029712
Iteration 14/25 | Loss: 0.00029712
Iteration 15/25 | Loss: 0.00029712
Iteration 16/25 | Loss: 0.00029712
Iteration 17/25 | Loss: 0.00029712
Iteration 18/25 | Loss: 0.00029712
Iteration 19/25 | Loss: 0.00029712
Iteration 20/25 | Loss: 0.00029712
Iteration 21/25 | Loss: 0.00029712
Iteration 22/25 | Loss: 0.00029712
Iteration 23/25 | Loss: 0.00029712
Iteration 24/25 | Loss: 0.00029712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002971226640511304, 0.0002971226640511304, 0.0002971226640511304, 0.0002971226640511304, 0.0002971226640511304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002971226640511304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029712
Iteration 2/1000 | Loss: 0.00002341
Iteration 3/1000 | Loss: 0.00001763
Iteration 4/1000 | Loss: 0.00001453
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001155
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001152
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001144
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001141
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001138
Iteration 83/1000 | Loss: 0.00001138
Iteration 84/1000 | Loss: 0.00001138
Iteration 85/1000 | Loss: 0.00001138
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001137
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001135
Iteration 107/1000 | Loss: 0.00001135
Iteration 108/1000 | Loss: 0.00001135
Iteration 109/1000 | Loss: 0.00001135
Iteration 110/1000 | Loss: 0.00001135
Iteration 111/1000 | Loss: 0.00001135
Iteration 112/1000 | Loss: 0.00001135
Iteration 113/1000 | Loss: 0.00001135
Iteration 114/1000 | Loss: 0.00001135
Iteration 115/1000 | Loss: 0.00001135
Iteration 116/1000 | Loss: 0.00001135
Iteration 117/1000 | Loss: 0.00001135
Iteration 118/1000 | Loss: 0.00001135
Iteration 119/1000 | Loss: 0.00001135
Iteration 120/1000 | Loss: 0.00001135
Iteration 121/1000 | Loss: 0.00001135
Iteration 122/1000 | Loss: 0.00001135
Iteration 123/1000 | Loss: 0.00001135
Iteration 124/1000 | Loss: 0.00001135
Iteration 125/1000 | Loss: 0.00001135
Iteration 126/1000 | Loss: 0.00001135
Iteration 127/1000 | Loss: 0.00001135
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001135
Iteration 134/1000 | Loss: 0.00001135
Iteration 135/1000 | Loss: 0.00001135
Iteration 136/1000 | Loss: 0.00001135
Iteration 137/1000 | Loss: 0.00001135
Iteration 138/1000 | Loss: 0.00001135
Iteration 139/1000 | Loss: 0.00001135
Iteration 140/1000 | Loss: 0.00001135
Iteration 141/1000 | Loss: 0.00001135
Iteration 142/1000 | Loss: 0.00001135
Iteration 143/1000 | Loss: 0.00001135
Iteration 144/1000 | Loss: 0.00001135
Iteration 145/1000 | Loss: 0.00001135
Iteration 146/1000 | Loss: 0.00001135
Iteration 147/1000 | Loss: 0.00001135
Iteration 148/1000 | Loss: 0.00001135
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001135
Iteration 154/1000 | Loss: 0.00001135
Iteration 155/1000 | Loss: 0.00001135
Iteration 156/1000 | Loss: 0.00001135
Iteration 157/1000 | Loss: 0.00001135
Iteration 158/1000 | Loss: 0.00001135
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001135
Iteration 163/1000 | Loss: 0.00001135
Iteration 164/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1350698514434043e-05, 1.1350698514434043e-05, 1.1350698514434043e-05, 1.1350698514434043e-05, 1.1350698514434043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1350698514434043e-05

Optimization complete. Final v2v error: 2.777743339538574 mm

Highest mean error: 3.584855318069458 mm for frame 0

Lowest mean error: 2.459942579269409 mm for frame 113

Saving results

Total time: 34.77539110183716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997770
Iteration 2/25 | Loss: 0.00281086
Iteration 3/25 | Loss: 0.00175144
Iteration 4/25 | Loss: 0.00143015
Iteration 5/25 | Loss: 0.00143978
Iteration 6/25 | Loss: 0.00140558
Iteration 7/25 | Loss: 0.00146004
Iteration 8/25 | Loss: 0.00144238
Iteration 9/25 | Loss: 0.00122384
Iteration 10/25 | Loss: 0.00117335
Iteration 11/25 | Loss: 0.00112458
Iteration 12/25 | Loss: 0.00111631
Iteration 13/25 | Loss: 0.00110190
Iteration 14/25 | Loss: 0.00109059
Iteration 15/25 | Loss: 0.00109911
Iteration 16/25 | Loss: 0.00107973
Iteration 17/25 | Loss: 0.00107703
Iteration 18/25 | Loss: 0.00107635
Iteration 19/25 | Loss: 0.00107508
Iteration 20/25 | Loss: 0.00107534
Iteration 21/25 | Loss: 0.00107321
Iteration 22/25 | Loss: 0.00107252
Iteration 23/25 | Loss: 0.00107203
Iteration 24/25 | Loss: 0.00107279
Iteration 25/25 | Loss: 0.00107197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83613729
Iteration 2/25 | Loss: 0.00159715
Iteration 3/25 | Loss: 0.00159713
Iteration 4/25 | Loss: 0.00159713
Iteration 5/25 | Loss: 0.00159713
Iteration 6/25 | Loss: 0.00159713
Iteration 7/25 | Loss: 0.00159713
Iteration 8/25 | Loss: 0.00159713
Iteration 9/25 | Loss: 0.00159713
Iteration 10/25 | Loss: 0.00159713
Iteration 11/25 | Loss: 0.00159713
Iteration 12/25 | Loss: 0.00159713
Iteration 13/25 | Loss: 0.00159713
Iteration 14/25 | Loss: 0.00159713
Iteration 15/25 | Loss: 0.00159713
Iteration 16/25 | Loss: 0.00159713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015971309039741755, 0.0015971309039741755, 0.0015971309039741755, 0.0015971309039741755, 0.0015971309039741755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015971309039741755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159713
Iteration 2/1000 | Loss: 0.00261636
Iteration 3/1000 | Loss: 0.00037648
Iteration 4/1000 | Loss: 0.00041447
Iteration 5/1000 | Loss: 0.00020379
Iteration 6/1000 | Loss: 0.00021809
Iteration 7/1000 | Loss: 0.00012734
Iteration 8/1000 | Loss: 0.00040186
Iteration 9/1000 | Loss: 0.00016538
Iteration 10/1000 | Loss: 0.00007877
Iteration 11/1000 | Loss: 0.00007158
Iteration 12/1000 | Loss: 0.00006441
Iteration 13/1000 | Loss: 0.00006127
Iteration 14/1000 | Loss: 0.00006268
Iteration 15/1000 | Loss: 0.00005951
Iteration 16/1000 | Loss: 0.00005785
Iteration 17/1000 | Loss: 0.00005872
Iteration 18/1000 | Loss: 0.00005822
Iteration 19/1000 | Loss: 0.00005862
Iteration 20/1000 | Loss: 0.00005716
Iteration 21/1000 | Loss: 0.00005488
Iteration 22/1000 | Loss: 0.00005842
Iteration 23/1000 | Loss: 0.00005636
Iteration 24/1000 | Loss: 0.00005421
Iteration 25/1000 | Loss: 0.00005850
Iteration 26/1000 | Loss: 0.00005612
Iteration 27/1000 | Loss: 0.00005369
Iteration 28/1000 | Loss: 0.00005839
Iteration 29/1000 | Loss: 0.00005579
Iteration 30/1000 | Loss: 0.00005826
Iteration 31/1000 | Loss: 0.00005614
Iteration 32/1000 | Loss: 0.00005331
Iteration 33/1000 | Loss: 0.00005833
Iteration 34/1000 | Loss: 0.00005567
Iteration 35/1000 | Loss: 0.00005800
Iteration 36/1000 | Loss: 0.00005585
Iteration 37/1000 | Loss: 0.00005308
Iteration 38/1000 | Loss: 0.00005784
Iteration 39/1000 | Loss: 0.00005714
Iteration 40/1000 | Loss: 0.00005741
Iteration 41/1000 | Loss: 0.00005879
Iteration 42/1000 | Loss: 0.00005691
Iteration 43/1000 | Loss: 0.00005769
Iteration 44/1000 | Loss: 0.00005668
Iteration 45/1000 | Loss: 0.00005813
Iteration 46/1000 | Loss: 0.00005625
Iteration 47/1000 | Loss: 0.00005373
Iteration 48/1000 | Loss: 0.00005527
Iteration 49/1000 | Loss: 0.00005357
Iteration 50/1000 | Loss: 0.00005259
Iteration 51/1000 | Loss: 0.00005615
Iteration 52/1000 | Loss: 0.00006243
Iteration 53/1000 | Loss: 0.00005383
Iteration 54/1000 | Loss: 0.00005819
Iteration 55/1000 | Loss: 0.00006277
Iteration 56/1000 | Loss: 0.00005588
Iteration 57/1000 | Loss: 0.00005464
Iteration 58/1000 | Loss: 0.00005316
Iteration 59/1000 | Loss: 0.00005257
Iteration 60/1000 | Loss: 0.00005225
Iteration 61/1000 | Loss: 0.00005205
Iteration 62/1000 | Loss: 0.00005201
Iteration 63/1000 | Loss: 0.00005197
Iteration 64/1000 | Loss: 0.00005195
Iteration 65/1000 | Loss: 0.00005195
Iteration 66/1000 | Loss: 0.00005193
Iteration 67/1000 | Loss: 0.00005193
Iteration 68/1000 | Loss: 0.00005191
Iteration 69/1000 | Loss: 0.00005191
Iteration 70/1000 | Loss: 0.00005191
Iteration 71/1000 | Loss: 0.00005191
Iteration 72/1000 | Loss: 0.00005191
Iteration 73/1000 | Loss: 0.00005191
Iteration 74/1000 | Loss: 0.00005191
Iteration 75/1000 | Loss: 0.00005190
Iteration 76/1000 | Loss: 0.00005190
Iteration 77/1000 | Loss: 0.00005190
Iteration 78/1000 | Loss: 0.00005190
Iteration 79/1000 | Loss: 0.00005189
Iteration 80/1000 | Loss: 0.00005189
Iteration 81/1000 | Loss: 0.00005189
Iteration 82/1000 | Loss: 0.00005189
Iteration 83/1000 | Loss: 0.00005189
Iteration 84/1000 | Loss: 0.00005189
Iteration 85/1000 | Loss: 0.00005189
Iteration 86/1000 | Loss: 0.00005188
Iteration 87/1000 | Loss: 0.00005188
Iteration 88/1000 | Loss: 0.00005188
Iteration 89/1000 | Loss: 0.00005188
Iteration 90/1000 | Loss: 0.00005188
Iteration 91/1000 | Loss: 0.00005188
Iteration 92/1000 | Loss: 0.00005188
Iteration 93/1000 | Loss: 0.00005188
Iteration 94/1000 | Loss: 0.00005188
Iteration 95/1000 | Loss: 0.00005188
Iteration 96/1000 | Loss: 0.00005187
Iteration 97/1000 | Loss: 0.00005187
Iteration 98/1000 | Loss: 0.00005187
Iteration 99/1000 | Loss: 0.00005187
Iteration 100/1000 | Loss: 0.00005187
Iteration 101/1000 | Loss: 0.00005187
Iteration 102/1000 | Loss: 0.00005187
Iteration 103/1000 | Loss: 0.00005187
Iteration 104/1000 | Loss: 0.00005187
Iteration 105/1000 | Loss: 0.00005187
Iteration 106/1000 | Loss: 0.00005186
Iteration 107/1000 | Loss: 0.00005186
Iteration 108/1000 | Loss: 0.00005186
Iteration 109/1000 | Loss: 0.00005186
Iteration 110/1000 | Loss: 0.00005186
Iteration 111/1000 | Loss: 0.00005186
Iteration 112/1000 | Loss: 0.00005186
Iteration 113/1000 | Loss: 0.00005186
Iteration 114/1000 | Loss: 0.00005186
Iteration 115/1000 | Loss: 0.00005186
Iteration 116/1000 | Loss: 0.00005186
Iteration 117/1000 | Loss: 0.00005186
Iteration 118/1000 | Loss: 0.00005185
Iteration 119/1000 | Loss: 0.00005185
Iteration 120/1000 | Loss: 0.00005185
Iteration 121/1000 | Loss: 0.00005185
Iteration 122/1000 | Loss: 0.00005185
Iteration 123/1000 | Loss: 0.00005185
Iteration 124/1000 | Loss: 0.00005185
Iteration 125/1000 | Loss: 0.00005185
Iteration 126/1000 | Loss: 0.00005185
Iteration 127/1000 | Loss: 0.00005185
Iteration 128/1000 | Loss: 0.00005185
Iteration 129/1000 | Loss: 0.00005185
Iteration 130/1000 | Loss: 0.00005185
Iteration 131/1000 | Loss: 0.00005185
Iteration 132/1000 | Loss: 0.00005184
Iteration 133/1000 | Loss: 0.00005184
Iteration 134/1000 | Loss: 0.00005184
Iteration 135/1000 | Loss: 0.00005184
Iteration 136/1000 | Loss: 0.00005184
Iteration 137/1000 | Loss: 0.00005184
Iteration 138/1000 | Loss: 0.00005184
Iteration 139/1000 | Loss: 0.00005184
Iteration 140/1000 | Loss: 0.00005183
Iteration 141/1000 | Loss: 0.00005183
Iteration 142/1000 | Loss: 0.00005183
Iteration 143/1000 | Loss: 0.00005183
Iteration 144/1000 | Loss: 0.00005183
Iteration 145/1000 | Loss: 0.00005183
Iteration 146/1000 | Loss: 0.00005183
Iteration 147/1000 | Loss: 0.00005183
Iteration 148/1000 | Loss: 0.00005183
Iteration 149/1000 | Loss: 0.00005183
Iteration 150/1000 | Loss: 0.00005183
Iteration 151/1000 | Loss: 0.00005183
Iteration 152/1000 | Loss: 0.00005183
Iteration 153/1000 | Loss: 0.00005183
Iteration 154/1000 | Loss: 0.00005183
Iteration 155/1000 | Loss: 0.00005183
Iteration 156/1000 | Loss: 0.00005183
Iteration 157/1000 | Loss: 0.00005183
Iteration 158/1000 | Loss: 0.00005183
Iteration 159/1000 | Loss: 0.00005183
Iteration 160/1000 | Loss: 0.00005183
Iteration 161/1000 | Loss: 0.00005183
Iteration 162/1000 | Loss: 0.00005183
Iteration 163/1000 | Loss: 0.00005183
Iteration 164/1000 | Loss: 0.00005183
Iteration 165/1000 | Loss: 0.00005183
Iteration 166/1000 | Loss: 0.00005183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [5.182727545616217e-05, 5.182727545616217e-05, 5.182727545616217e-05, 5.182727545616217e-05, 5.182727545616217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.182727545616217e-05

Optimization complete. Final v2v error: 5.026735305786133 mm

Highest mean error: 6.344949722290039 mm for frame 19

Lowest mean error: 3.346278429031372 mm for frame 107

Saving results

Total time: 158.78556180000305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393549
Iteration 2/25 | Loss: 0.00080874
Iteration 3/25 | Loss: 0.00062062
Iteration 4/25 | Loss: 0.00058621
Iteration 5/25 | Loss: 0.00057785
Iteration 6/25 | Loss: 0.00057479
Iteration 7/25 | Loss: 0.00057412
Iteration 8/25 | Loss: 0.00057412
Iteration 9/25 | Loss: 0.00057412
Iteration 10/25 | Loss: 0.00057412
Iteration 11/25 | Loss: 0.00057412
Iteration 12/25 | Loss: 0.00057412
Iteration 13/25 | Loss: 0.00057412
Iteration 14/25 | Loss: 0.00057412
Iteration 15/25 | Loss: 0.00057412
Iteration 16/25 | Loss: 0.00057412
Iteration 17/25 | Loss: 0.00057412
Iteration 18/25 | Loss: 0.00057412
Iteration 19/25 | Loss: 0.00057412
Iteration 20/25 | Loss: 0.00057412
Iteration 21/25 | Loss: 0.00057412
Iteration 22/25 | Loss: 0.00057412
Iteration 23/25 | Loss: 0.00057412
Iteration 24/25 | Loss: 0.00057412
Iteration 25/25 | Loss: 0.00057412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45627141
Iteration 2/25 | Loss: 0.00019954
Iteration 3/25 | Loss: 0.00019954
Iteration 4/25 | Loss: 0.00019954
Iteration 5/25 | Loss: 0.00019954
Iteration 6/25 | Loss: 0.00019954
Iteration 7/25 | Loss: 0.00019954
Iteration 8/25 | Loss: 0.00019954
Iteration 9/25 | Loss: 0.00019954
Iteration 10/25 | Loss: 0.00019954
Iteration 11/25 | Loss: 0.00019954
Iteration 12/25 | Loss: 0.00019953
Iteration 13/25 | Loss: 0.00019953
Iteration 14/25 | Loss: 0.00019953
Iteration 15/25 | Loss: 0.00019953
Iteration 16/25 | Loss: 0.00019953
Iteration 17/25 | Loss: 0.00019953
Iteration 18/25 | Loss: 0.00019953
Iteration 19/25 | Loss: 0.00019953
Iteration 20/25 | Loss: 0.00019953
Iteration 21/25 | Loss: 0.00019953
Iteration 22/25 | Loss: 0.00019953
Iteration 23/25 | Loss: 0.00019953
Iteration 24/25 | Loss: 0.00019953
Iteration 25/25 | Loss: 0.00019953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019953
Iteration 2/1000 | Loss: 0.00002535
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001458
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001366
Iteration 13/1000 | Loss: 0.00001362
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001352
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001348
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001345
Iteration 22/1000 | Loss: 0.00001344
Iteration 23/1000 | Loss: 0.00001344
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Iteration 82/1000 | Loss: 0.00001328
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.3223042515164707e-05, 1.3223042515164707e-05, 1.3223042515164707e-05, 1.3223042515164707e-05, 1.3223042515164707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3223042515164707e-05

Optimization complete. Final v2v error: 3.0321221351623535 mm

Highest mean error: 3.3993098735809326 mm for frame 104

Lowest mean error: 2.73124098777771 mm for frame 19

Saving results

Total time: 38.220879554748535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947642
Iteration 2/25 | Loss: 0.00138912
Iteration 3/25 | Loss: 0.00095224
Iteration 4/25 | Loss: 0.00077963
Iteration 5/25 | Loss: 0.00074727
Iteration 6/25 | Loss: 0.00073351
Iteration 7/25 | Loss: 0.00073112
Iteration 8/25 | Loss: 0.00072952
Iteration 9/25 | Loss: 0.00072684
Iteration 10/25 | Loss: 0.00072608
Iteration 11/25 | Loss: 0.00072526
Iteration 12/25 | Loss: 0.00072472
Iteration 13/25 | Loss: 0.00072582
Iteration 14/25 | Loss: 0.00071828
Iteration 15/25 | Loss: 0.00071563
Iteration 16/25 | Loss: 0.00071527
Iteration 17/25 | Loss: 0.00071527
Iteration 18/25 | Loss: 0.00071527
Iteration 19/25 | Loss: 0.00071526
Iteration 20/25 | Loss: 0.00071526
Iteration 21/25 | Loss: 0.00071526
Iteration 22/25 | Loss: 0.00071526
Iteration 23/25 | Loss: 0.00071526
Iteration 24/25 | Loss: 0.00071526
Iteration 25/25 | Loss: 0.00071526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43916965
Iteration 2/25 | Loss: 0.00032558
Iteration 3/25 | Loss: 0.00032558
Iteration 4/25 | Loss: 0.00032558
Iteration 5/25 | Loss: 0.00032558
Iteration 6/25 | Loss: 0.00032558
Iteration 7/25 | Loss: 0.00032558
Iteration 8/25 | Loss: 0.00032558
Iteration 9/25 | Loss: 0.00032558
Iteration 10/25 | Loss: 0.00032557
Iteration 11/25 | Loss: 0.00032557
Iteration 12/25 | Loss: 0.00032557
Iteration 13/25 | Loss: 0.00032557
Iteration 14/25 | Loss: 0.00032557
Iteration 15/25 | Loss: 0.00032557
Iteration 16/25 | Loss: 0.00032557
Iteration 17/25 | Loss: 0.00032557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00032557471422478557, 0.00032557471422478557, 0.00032557471422478557, 0.00032557471422478557, 0.00032557471422478557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032557471422478557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032557
Iteration 2/1000 | Loss: 0.00003067
Iteration 3/1000 | Loss: 0.00002379
Iteration 4/1000 | Loss: 0.00002237
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002108
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00036058
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002001
Iteration 12/1000 | Loss: 0.00001930
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001895
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001882
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001880
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001875
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001875
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001872
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001871
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001871
Iteration 45/1000 | Loss: 0.00001871
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001869
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001863
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001857
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001853
Iteration 76/1000 | Loss: 0.00001853
Iteration 77/1000 | Loss: 0.00001853
Iteration 78/1000 | Loss: 0.00001853
Iteration 79/1000 | Loss: 0.00001853
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001851
Iteration 102/1000 | Loss: 0.00001851
Iteration 103/1000 | Loss: 0.00001851
Iteration 104/1000 | Loss: 0.00001851
Iteration 105/1000 | Loss: 0.00001851
Iteration 106/1000 | Loss: 0.00001851
Iteration 107/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.8511716916691512e-05, 1.8511716916691512e-05, 1.8511716916691512e-05, 1.8511716916691512e-05, 1.8511716916691512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8511716916691512e-05

Optimization complete. Final v2v error: 3.5411152839660645 mm

Highest mean error: 4.745027542114258 mm for frame 98

Lowest mean error: 3.2376697063446045 mm for frame 186

Saving results

Total time: 63.681267976760864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417577
Iteration 2/25 | Loss: 0.00082847
Iteration 3/25 | Loss: 0.00070309
Iteration 4/25 | Loss: 0.00065777
Iteration 5/25 | Loss: 0.00064902
Iteration 6/25 | Loss: 0.00064786
Iteration 7/25 | Loss: 0.00064742
Iteration 8/25 | Loss: 0.00064742
Iteration 9/25 | Loss: 0.00064742
Iteration 10/25 | Loss: 0.00064742
Iteration 11/25 | Loss: 0.00064742
Iteration 12/25 | Loss: 0.00064742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006474188412539661, 0.0006474188412539661, 0.0006474188412539661, 0.0006474188412539661, 0.0006474188412539661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006474188412539661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46518230
Iteration 2/25 | Loss: 0.00022647
Iteration 3/25 | Loss: 0.00022647
Iteration 4/25 | Loss: 0.00022647
Iteration 5/25 | Loss: 0.00022647
Iteration 6/25 | Loss: 0.00022647
Iteration 7/25 | Loss: 0.00022647
Iteration 8/25 | Loss: 0.00022647
Iteration 9/25 | Loss: 0.00022647
Iteration 10/25 | Loss: 0.00022647
Iteration 11/25 | Loss: 0.00022647
Iteration 12/25 | Loss: 0.00022647
Iteration 13/25 | Loss: 0.00022647
Iteration 14/25 | Loss: 0.00022647
Iteration 15/25 | Loss: 0.00022647
Iteration 16/25 | Loss: 0.00022647
Iteration 17/25 | Loss: 0.00022647
Iteration 18/25 | Loss: 0.00022647
Iteration 19/25 | Loss: 0.00022647
Iteration 20/25 | Loss: 0.00022647
Iteration 21/25 | Loss: 0.00022647
Iteration 22/25 | Loss: 0.00022647
Iteration 23/25 | Loss: 0.00022647
Iteration 24/25 | Loss: 0.00022647
Iteration 25/25 | Loss: 0.00022647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022647
Iteration 2/1000 | Loss: 0.00003693
Iteration 3/1000 | Loss: 0.00002569
Iteration 4/1000 | Loss: 0.00002341
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002046
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00002017
Iteration 13/1000 | Loss: 0.00002004
Iteration 14/1000 | Loss: 0.00001998
Iteration 15/1000 | Loss: 0.00001996
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001992
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001991
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001990
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001987
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001986
Iteration 41/1000 | Loss: 0.00001986
Iteration 42/1000 | Loss: 0.00001986
Iteration 43/1000 | Loss: 0.00001985
Iteration 44/1000 | Loss: 0.00001985
Iteration 45/1000 | Loss: 0.00001985
Iteration 46/1000 | Loss: 0.00001984
Iteration 47/1000 | Loss: 0.00001984
Iteration 48/1000 | Loss: 0.00001984
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00001983
Iteration 51/1000 | Loss: 0.00001983
Iteration 52/1000 | Loss: 0.00001983
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001982
Iteration 58/1000 | Loss: 0.00001982
Iteration 59/1000 | Loss: 0.00001981
Iteration 60/1000 | Loss: 0.00001981
Iteration 61/1000 | Loss: 0.00001981
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001981
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001979
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001979
Iteration 70/1000 | Loss: 0.00001978
Iteration 71/1000 | Loss: 0.00001978
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001978
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001977
Iteration 79/1000 | Loss: 0.00001977
Iteration 80/1000 | Loss: 0.00001977
Iteration 81/1000 | Loss: 0.00001977
Iteration 82/1000 | Loss: 0.00001977
Iteration 83/1000 | Loss: 0.00001977
Iteration 84/1000 | Loss: 0.00001976
Iteration 85/1000 | Loss: 0.00001976
Iteration 86/1000 | Loss: 0.00001975
Iteration 87/1000 | Loss: 0.00001975
Iteration 88/1000 | Loss: 0.00001975
Iteration 89/1000 | Loss: 0.00001975
Iteration 90/1000 | Loss: 0.00001975
Iteration 91/1000 | Loss: 0.00001975
Iteration 92/1000 | Loss: 0.00001974
Iteration 93/1000 | Loss: 0.00001974
Iteration 94/1000 | Loss: 0.00001974
Iteration 95/1000 | Loss: 0.00001974
Iteration 96/1000 | Loss: 0.00001974
Iteration 97/1000 | Loss: 0.00001973
Iteration 98/1000 | Loss: 0.00001973
Iteration 99/1000 | Loss: 0.00001973
Iteration 100/1000 | Loss: 0.00001972
Iteration 101/1000 | Loss: 0.00001972
Iteration 102/1000 | Loss: 0.00001972
Iteration 103/1000 | Loss: 0.00001972
Iteration 104/1000 | Loss: 0.00001971
Iteration 105/1000 | Loss: 0.00001971
Iteration 106/1000 | Loss: 0.00001971
Iteration 107/1000 | Loss: 0.00001971
Iteration 108/1000 | Loss: 0.00001971
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001970
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001969
Iteration 117/1000 | Loss: 0.00001969
Iteration 118/1000 | Loss: 0.00001969
Iteration 119/1000 | Loss: 0.00001969
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00001968
Iteration 128/1000 | Loss: 0.00001968
Iteration 129/1000 | Loss: 0.00001968
Iteration 130/1000 | Loss: 0.00001968
Iteration 131/1000 | Loss: 0.00001968
Iteration 132/1000 | Loss: 0.00001968
Iteration 133/1000 | Loss: 0.00001968
Iteration 134/1000 | Loss: 0.00001968
Iteration 135/1000 | Loss: 0.00001968
Iteration 136/1000 | Loss: 0.00001968
Iteration 137/1000 | Loss: 0.00001968
Iteration 138/1000 | Loss: 0.00001967
Iteration 139/1000 | Loss: 0.00001967
Iteration 140/1000 | Loss: 0.00001967
Iteration 141/1000 | Loss: 0.00001967
Iteration 142/1000 | Loss: 0.00001967
Iteration 143/1000 | Loss: 0.00001967
Iteration 144/1000 | Loss: 0.00001967
Iteration 145/1000 | Loss: 0.00001967
Iteration 146/1000 | Loss: 0.00001966
Iteration 147/1000 | Loss: 0.00001966
Iteration 148/1000 | Loss: 0.00001966
Iteration 149/1000 | Loss: 0.00001966
Iteration 150/1000 | Loss: 0.00001966
Iteration 151/1000 | Loss: 0.00001965
Iteration 152/1000 | Loss: 0.00001965
Iteration 153/1000 | Loss: 0.00001965
Iteration 154/1000 | Loss: 0.00001965
Iteration 155/1000 | Loss: 0.00001965
Iteration 156/1000 | Loss: 0.00001965
Iteration 157/1000 | Loss: 0.00001965
Iteration 158/1000 | Loss: 0.00001965
Iteration 159/1000 | Loss: 0.00001965
Iteration 160/1000 | Loss: 0.00001965
Iteration 161/1000 | Loss: 0.00001965
Iteration 162/1000 | Loss: 0.00001965
Iteration 163/1000 | Loss: 0.00001965
Iteration 164/1000 | Loss: 0.00001964
Iteration 165/1000 | Loss: 0.00001964
Iteration 166/1000 | Loss: 0.00001964
Iteration 167/1000 | Loss: 0.00001964
Iteration 168/1000 | Loss: 0.00001964
Iteration 169/1000 | Loss: 0.00001964
Iteration 170/1000 | Loss: 0.00001964
Iteration 171/1000 | Loss: 0.00001964
Iteration 172/1000 | Loss: 0.00001964
Iteration 173/1000 | Loss: 0.00001964
Iteration 174/1000 | Loss: 0.00001964
Iteration 175/1000 | Loss: 0.00001964
Iteration 176/1000 | Loss: 0.00001964
Iteration 177/1000 | Loss: 0.00001964
Iteration 178/1000 | Loss: 0.00001964
Iteration 179/1000 | Loss: 0.00001964
Iteration 180/1000 | Loss: 0.00001964
Iteration 181/1000 | Loss: 0.00001964
Iteration 182/1000 | Loss: 0.00001964
Iteration 183/1000 | Loss: 0.00001964
Iteration 184/1000 | Loss: 0.00001964
Iteration 185/1000 | Loss: 0.00001964
Iteration 186/1000 | Loss: 0.00001964
Iteration 187/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.964401417353656e-05, 1.964401417353656e-05, 1.964401417353656e-05, 1.964401417353656e-05, 1.964401417353656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.964401417353656e-05

Optimization complete. Final v2v error: 3.69699764251709 mm

Highest mean error: 4.567784786224365 mm for frame 83

Lowest mean error: 3.3115057945251465 mm for frame 46

Saving results

Total time: 36.70845007896423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502607
Iteration 2/25 | Loss: 0.00079081
Iteration 3/25 | Loss: 0.00063397
Iteration 4/25 | Loss: 0.00060314
Iteration 5/25 | Loss: 0.00059243
Iteration 6/25 | Loss: 0.00058984
Iteration 7/25 | Loss: 0.00058897
Iteration 8/25 | Loss: 0.00058879
Iteration 9/25 | Loss: 0.00058879
Iteration 10/25 | Loss: 0.00058879
Iteration 11/25 | Loss: 0.00058879
Iteration 12/25 | Loss: 0.00058879
Iteration 13/25 | Loss: 0.00058879
Iteration 14/25 | Loss: 0.00058879
Iteration 15/25 | Loss: 0.00058879
Iteration 16/25 | Loss: 0.00058879
Iteration 17/25 | Loss: 0.00058879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005887884180992842, 0.0005887884180992842, 0.0005887884180992842, 0.0005887884180992842, 0.0005887884180992842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005887884180992842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58177757
Iteration 2/25 | Loss: 0.00023552
Iteration 3/25 | Loss: 0.00023548
Iteration 4/25 | Loss: 0.00023548
Iteration 5/25 | Loss: 0.00023548
Iteration 6/25 | Loss: 0.00023548
Iteration 7/25 | Loss: 0.00023548
Iteration 8/25 | Loss: 0.00023548
Iteration 9/25 | Loss: 0.00023548
Iteration 10/25 | Loss: 0.00023548
Iteration 11/25 | Loss: 0.00023548
Iteration 12/25 | Loss: 0.00023548
Iteration 13/25 | Loss: 0.00023548
Iteration 14/25 | Loss: 0.00023548
Iteration 15/25 | Loss: 0.00023548
Iteration 16/25 | Loss: 0.00023548
Iteration 17/25 | Loss: 0.00023548
Iteration 18/25 | Loss: 0.00023548
Iteration 19/25 | Loss: 0.00023548
Iteration 20/25 | Loss: 0.00023548
Iteration 21/25 | Loss: 0.00023548
Iteration 22/25 | Loss: 0.00023548
Iteration 23/25 | Loss: 0.00023548
Iteration 24/25 | Loss: 0.00023548
Iteration 25/25 | Loss: 0.00023548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023548
Iteration 2/1000 | Loss: 0.00002561
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001376
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001330
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001326
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001323
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00001320
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001319
Iteration 30/1000 | Loss: 0.00001318
Iteration 31/1000 | Loss: 0.00001317
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001313
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001298
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001295
Iteration 57/1000 | Loss: 0.00001295
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001294
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001293
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001290
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001283
Iteration 98/1000 | Loss: 0.00001282
Iteration 99/1000 | Loss: 0.00001282
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001282
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001281
Iteration 105/1000 | Loss: 0.00001281
Iteration 106/1000 | Loss: 0.00001281
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001280
Iteration 110/1000 | Loss: 0.00001280
Iteration 111/1000 | Loss: 0.00001280
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001280
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001280
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001279
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001279
Iteration 135/1000 | Loss: 0.00001279
Iteration 136/1000 | Loss: 0.00001278
Iteration 137/1000 | Loss: 0.00001278
Iteration 138/1000 | Loss: 0.00001278
Iteration 139/1000 | Loss: 0.00001278
Iteration 140/1000 | Loss: 0.00001278
Iteration 141/1000 | Loss: 0.00001278
Iteration 142/1000 | Loss: 0.00001278
Iteration 143/1000 | Loss: 0.00001278
Iteration 144/1000 | Loss: 0.00001278
Iteration 145/1000 | Loss: 0.00001278
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001278
Iteration 156/1000 | Loss: 0.00001278
Iteration 157/1000 | Loss: 0.00001278
Iteration 158/1000 | Loss: 0.00001278
Iteration 159/1000 | Loss: 0.00001278
Iteration 160/1000 | Loss: 0.00001278
Iteration 161/1000 | Loss: 0.00001278
Iteration 162/1000 | Loss: 0.00001278
Iteration 163/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2782546946255025e-05, 1.2782546946255025e-05, 1.2782546946255025e-05, 1.2782546946255025e-05, 1.2782546946255025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2782546946255025e-05

Optimization complete. Final v2v error: 3.0025103092193604 mm

Highest mean error: 3.408409357070923 mm for frame 111

Lowest mean error: 2.6219475269317627 mm for frame 143

Saving results

Total time: 38.74630045890808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866076
Iteration 2/25 | Loss: 0.00103435
Iteration 3/25 | Loss: 0.00068751
Iteration 4/25 | Loss: 0.00062806
Iteration 5/25 | Loss: 0.00060888
Iteration 6/25 | Loss: 0.00060213
Iteration 7/25 | Loss: 0.00060772
Iteration 8/25 | Loss: 0.00060726
Iteration 9/25 | Loss: 0.00060711
Iteration 10/25 | Loss: 0.00060650
Iteration 11/25 | Loss: 0.00060209
Iteration 12/25 | Loss: 0.00060044
Iteration 13/25 | Loss: 0.00060173
Iteration 14/25 | Loss: 0.00060100
Iteration 15/25 | Loss: 0.00059996
Iteration 16/25 | Loss: 0.00059995
Iteration 17/25 | Loss: 0.00059997
Iteration 18/25 | Loss: 0.00059751
Iteration 19/25 | Loss: 0.00059677
Iteration 20/25 | Loss: 0.00059663
Iteration 21/25 | Loss: 0.00059660
Iteration 22/25 | Loss: 0.00059660
Iteration 23/25 | Loss: 0.00059660
Iteration 24/25 | Loss: 0.00059660
Iteration 25/25 | Loss: 0.00059660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71769238
Iteration 2/25 | Loss: 0.00024588
Iteration 3/25 | Loss: 0.00024588
Iteration 4/25 | Loss: 0.00024588
Iteration 5/25 | Loss: 0.00024588
Iteration 6/25 | Loss: 0.00024588
Iteration 7/25 | Loss: 0.00024588
Iteration 8/25 | Loss: 0.00024588
Iteration 9/25 | Loss: 0.00024588
Iteration 10/25 | Loss: 0.00024588
Iteration 11/25 | Loss: 0.00024588
Iteration 12/25 | Loss: 0.00024588
Iteration 13/25 | Loss: 0.00024588
Iteration 14/25 | Loss: 0.00024588
Iteration 15/25 | Loss: 0.00024588
Iteration 16/25 | Loss: 0.00024588
Iteration 17/25 | Loss: 0.00024588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000245876784902066, 0.000245876784902066, 0.000245876784902066, 0.000245876784902066, 0.000245876784902066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000245876784902066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024588
Iteration 2/1000 | Loss: 0.00002976
Iteration 3/1000 | Loss: 0.00005345
Iteration 4/1000 | Loss: 0.00002463
Iteration 5/1000 | Loss: 0.00003618
Iteration 6/1000 | Loss: 0.00004122
Iteration 7/1000 | Loss: 0.00003899
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002716
Iteration 10/1000 | Loss: 0.00003614
Iteration 11/1000 | Loss: 0.00003398
Iteration 12/1000 | Loss: 0.00003654
Iteration 13/1000 | Loss: 0.00004124
Iteration 14/1000 | Loss: 0.00003421
Iteration 15/1000 | Loss: 0.00004066
Iteration 16/1000 | Loss: 0.00002791
Iteration 17/1000 | Loss: 0.00037517
Iteration 18/1000 | Loss: 0.00006596
Iteration 19/1000 | Loss: 0.00003099
Iteration 20/1000 | Loss: 0.00001860
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001507
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001442
Iteration 31/1000 | Loss: 0.00001442
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001438
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001432
Iteration 45/1000 | Loss: 0.00001431
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001418
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001417
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001417
Iteration 104/1000 | Loss: 0.00001417
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001416
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001415
Iteration 112/1000 | Loss: 0.00001414
Iteration 113/1000 | Loss: 0.00001414
Iteration 114/1000 | Loss: 0.00001414
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001409
Iteration 121/1000 | Loss: 0.00001409
Iteration 122/1000 | Loss: 0.00001409
Iteration 123/1000 | Loss: 0.00001409
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001408
Iteration 127/1000 | Loss: 0.00001408
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001408
Iteration 130/1000 | Loss: 0.00001408
Iteration 131/1000 | Loss: 0.00001407
Iteration 132/1000 | Loss: 0.00001407
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001407
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001406
Iteration 140/1000 | Loss: 0.00001406
Iteration 141/1000 | Loss: 0.00001406
Iteration 142/1000 | Loss: 0.00001406
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001405
Iteration 145/1000 | Loss: 0.00001405
Iteration 146/1000 | Loss: 0.00001405
Iteration 147/1000 | Loss: 0.00001405
Iteration 148/1000 | Loss: 0.00001405
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001403
Iteration 158/1000 | Loss: 0.00001403
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001402
Iteration 165/1000 | Loss: 0.00001402
Iteration 166/1000 | Loss: 0.00001402
Iteration 167/1000 | Loss: 0.00001402
Iteration 168/1000 | Loss: 0.00001402
Iteration 169/1000 | Loss: 0.00001402
Iteration 170/1000 | Loss: 0.00001402
Iteration 171/1000 | Loss: 0.00001402
Iteration 172/1000 | Loss: 0.00001402
Iteration 173/1000 | Loss: 0.00001402
Iteration 174/1000 | Loss: 0.00001402
Iteration 175/1000 | Loss: 0.00001402
Iteration 176/1000 | Loss: 0.00001402
Iteration 177/1000 | Loss: 0.00001402
Iteration 178/1000 | Loss: 0.00001401
Iteration 179/1000 | Loss: 0.00001401
Iteration 180/1000 | Loss: 0.00001401
Iteration 181/1000 | Loss: 0.00001401
Iteration 182/1000 | Loss: 0.00001401
Iteration 183/1000 | Loss: 0.00001401
Iteration 184/1000 | Loss: 0.00001401
Iteration 185/1000 | Loss: 0.00001401
Iteration 186/1000 | Loss: 0.00001401
Iteration 187/1000 | Loss: 0.00001401
Iteration 188/1000 | Loss: 0.00001401
Iteration 189/1000 | Loss: 0.00001401
Iteration 190/1000 | Loss: 0.00001401
Iteration 191/1000 | Loss: 0.00001400
Iteration 192/1000 | Loss: 0.00001400
Iteration 193/1000 | Loss: 0.00001400
Iteration 194/1000 | Loss: 0.00001400
Iteration 195/1000 | Loss: 0.00001400
Iteration 196/1000 | Loss: 0.00001400
Iteration 197/1000 | Loss: 0.00001400
Iteration 198/1000 | Loss: 0.00001399
Iteration 199/1000 | Loss: 0.00001399
Iteration 200/1000 | Loss: 0.00001399
Iteration 201/1000 | Loss: 0.00001399
Iteration 202/1000 | Loss: 0.00001399
Iteration 203/1000 | Loss: 0.00001399
Iteration 204/1000 | Loss: 0.00001399
Iteration 205/1000 | Loss: 0.00001399
Iteration 206/1000 | Loss: 0.00001399
Iteration 207/1000 | Loss: 0.00001399
Iteration 208/1000 | Loss: 0.00001399
Iteration 209/1000 | Loss: 0.00001399
Iteration 210/1000 | Loss: 0.00001399
Iteration 211/1000 | Loss: 0.00001399
Iteration 212/1000 | Loss: 0.00001399
Iteration 213/1000 | Loss: 0.00001399
Iteration 214/1000 | Loss: 0.00001398
Iteration 215/1000 | Loss: 0.00001398
Iteration 216/1000 | Loss: 0.00001398
Iteration 217/1000 | Loss: 0.00001398
Iteration 218/1000 | Loss: 0.00001398
Iteration 219/1000 | Loss: 0.00001398
Iteration 220/1000 | Loss: 0.00001398
Iteration 221/1000 | Loss: 0.00001398
Iteration 222/1000 | Loss: 0.00001398
Iteration 223/1000 | Loss: 0.00001398
Iteration 224/1000 | Loss: 0.00001398
Iteration 225/1000 | Loss: 0.00001398
Iteration 226/1000 | Loss: 0.00001398
Iteration 227/1000 | Loss: 0.00001398
Iteration 228/1000 | Loss: 0.00001398
Iteration 229/1000 | Loss: 0.00001398
Iteration 230/1000 | Loss: 0.00001398
Iteration 231/1000 | Loss: 0.00001398
Iteration 232/1000 | Loss: 0.00001398
Iteration 233/1000 | Loss: 0.00001398
Iteration 234/1000 | Loss: 0.00001398
Iteration 235/1000 | Loss: 0.00001398
Iteration 236/1000 | Loss: 0.00001398
Iteration 237/1000 | Loss: 0.00001398
Iteration 238/1000 | Loss: 0.00001398
Iteration 239/1000 | Loss: 0.00001398
Iteration 240/1000 | Loss: 0.00001398
Iteration 241/1000 | Loss: 0.00001398
Iteration 242/1000 | Loss: 0.00001398
Iteration 243/1000 | Loss: 0.00001398
Iteration 244/1000 | Loss: 0.00001398
Iteration 245/1000 | Loss: 0.00001398
Iteration 246/1000 | Loss: 0.00001398
Iteration 247/1000 | Loss: 0.00001398
Iteration 248/1000 | Loss: 0.00001398
Iteration 249/1000 | Loss: 0.00001398
Iteration 250/1000 | Loss: 0.00001398
Iteration 251/1000 | Loss: 0.00001398
Iteration 252/1000 | Loss: 0.00001398
Iteration 253/1000 | Loss: 0.00001398
Iteration 254/1000 | Loss: 0.00001398
Iteration 255/1000 | Loss: 0.00001398
Iteration 256/1000 | Loss: 0.00001398
Iteration 257/1000 | Loss: 0.00001398
Iteration 258/1000 | Loss: 0.00001398
Iteration 259/1000 | Loss: 0.00001398
Iteration 260/1000 | Loss: 0.00001398
Iteration 261/1000 | Loss: 0.00001398
Iteration 262/1000 | Loss: 0.00001398
Iteration 263/1000 | Loss: 0.00001398
Iteration 264/1000 | Loss: 0.00001398
Iteration 265/1000 | Loss: 0.00001398
Iteration 266/1000 | Loss: 0.00001398
Iteration 267/1000 | Loss: 0.00001398
Iteration 268/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.3981456504552625e-05, 1.3981456504552625e-05, 1.3981456504552625e-05, 1.3981456504552625e-05, 1.3981456504552625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3981456504552625e-05

Optimization complete. Final v2v error: 3.173175811767578 mm

Highest mean error: 4.062900543212891 mm for frame 195

Lowest mean error: 2.7570717334747314 mm for frame 14

Saving results

Total time: 99.96351790428162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00338744
Iteration 2/25 | Loss: 0.00096608
Iteration 3/25 | Loss: 0.00066818
Iteration 4/25 | Loss: 0.00060787
Iteration 5/25 | Loss: 0.00059577
Iteration 6/25 | Loss: 0.00059354
Iteration 7/25 | Loss: 0.00059327
Iteration 8/25 | Loss: 0.00059327
Iteration 9/25 | Loss: 0.00059327
Iteration 10/25 | Loss: 0.00059327
Iteration 11/25 | Loss: 0.00059327
Iteration 12/25 | Loss: 0.00059327
Iteration 13/25 | Loss: 0.00059327
Iteration 14/25 | Loss: 0.00059327
Iteration 15/25 | Loss: 0.00059327
Iteration 16/25 | Loss: 0.00059327
Iteration 17/25 | Loss: 0.00059327
Iteration 18/25 | Loss: 0.00059327
Iteration 19/25 | Loss: 0.00059327
Iteration 20/25 | Loss: 0.00059327
Iteration 21/25 | Loss: 0.00059327
Iteration 22/25 | Loss: 0.00059327
Iteration 23/25 | Loss: 0.00059327
Iteration 24/25 | Loss: 0.00059327
Iteration 25/25 | Loss: 0.00059327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46320510
Iteration 2/25 | Loss: 0.00022948
Iteration 3/25 | Loss: 0.00022948
Iteration 4/25 | Loss: 0.00022948
Iteration 5/25 | Loss: 0.00022948
Iteration 6/25 | Loss: 0.00022947
Iteration 7/25 | Loss: 0.00022947
Iteration 8/25 | Loss: 0.00022947
Iteration 9/25 | Loss: 0.00022947
Iteration 10/25 | Loss: 0.00022947
Iteration 11/25 | Loss: 0.00022947
Iteration 12/25 | Loss: 0.00022947
Iteration 13/25 | Loss: 0.00022947
Iteration 14/25 | Loss: 0.00022947
Iteration 15/25 | Loss: 0.00022947
Iteration 16/25 | Loss: 0.00022947
Iteration 17/25 | Loss: 0.00022947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00022947364777792245, 0.00022947364777792245, 0.00022947364777792245, 0.00022947364777792245, 0.00022947364777792245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022947364777792245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022947
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001840
Iteration 4/1000 | Loss: 0.00001624
Iteration 5/1000 | Loss: 0.00001531
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001312
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001310
Iteration 20/1000 | Loss: 0.00001310
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001305
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001299
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001299
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001298
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001298
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001298
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.297362814511871e-05, 1.297362814511871e-05, 1.297362814511871e-05, 1.297362814511871e-05, 1.297362814511871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.297362814511871e-05

Optimization complete. Final v2v error: 3.0176570415496826 mm

Highest mean error: 3.3475916385650635 mm for frame 26

Lowest mean error: 2.662060022354126 mm for frame 105

Saving results

Total time: 37.61067771911621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378620
Iteration 2/25 | Loss: 0.00087952
Iteration 3/25 | Loss: 0.00061169
Iteration 4/25 | Loss: 0.00059387
Iteration 5/25 | Loss: 0.00058660
Iteration 6/25 | Loss: 0.00058452
Iteration 7/25 | Loss: 0.00058438
Iteration 8/25 | Loss: 0.00058438
Iteration 9/25 | Loss: 0.00058438
Iteration 10/25 | Loss: 0.00058438
Iteration 11/25 | Loss: 0.00058438
Iteration 12/25 | Loss: 0.00058438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005843756371177733, 0.0005843756371177733, 0.0005843756371177733, 0.0005843756371177733, 0.0005843756371177733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005843756371177733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73754907
Iteration 2/25 | Loss: 0.00025514
Iteration 3/25 | Loss: 0.00025514
Iteration 4/25 | Loss: 0.00025514
Iteration 5/25 | Loss: 0.00025514
Iteration 6/25 | Loss: 0.00025514
Iteration 7/25 | Loss: 0.00025514
Iteration 8/25 | Loss: 0.00025514
Iteration 9/25 | Loss: 0.00025514
Iteration 10/25 | Loss: 0.00025514
Iteration 11/25 | Loss: 0.00025514
Iteration 12/25 | Loss: 0.00025514
Iteration 13/25 | Loss: 0.00025514
Iteration 14/25 | Loss: 0.00025514
Iteration 15/25 | Loss: 0.00025514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00025513858417980373, 0.00025513858417980373, 0.00025513858417980373, 0.00025513858417980373, 0.00025513858417980373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025513858417980373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025514
Iteration 2/1000 | Loss: 0.00002019
Iteration 3/1000 | Loss: 0.00001395
Iteration 4/1000 | Loss: 0.00001258
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001168
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001121
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001090
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001083
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001081
Iteration 15/1000 | Loss: 0.00001075
Iteration 16/1000 | Loss: 0.00001075
Iteration 17/1000 | Loss: 0.00001074
Iteration 18/1000 | Loss: 0.00001074
Iteration 19/1000 | Loss: 0.00001073
Iteration 20/1000 | Loss: 0.00001073
Iteration 21/1000 | Loss: 0.00001071
Iteration 22/1000 | Loss: 0.00001071
Iteration 23/1000 | Loss: 0.00001071
Iteration 24/1000 | Loss: 0.00001071
Iteration 25/1000 | Loss: 0.00001070
Iteration 26/1000 | Loss: 0.00001069
Iteration 27/1000 | Loss: 0.00001069
Iteration 28/1000 | Loss: 0.00001069
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001068
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001067
Iteration 35/1000 | Loss: 0.00001067
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001064
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001064
Iteration 49/1000 | Loss: 0.00001063
Iteration 50/1000 | Loss: 0.00001063
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001062
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001061
Iteration 58/1000 | Loss: 0.00001061
Iteration 59/1000 | Loss: 0.00001061
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001060
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001060
Iteration 73/1000 | Loss: 0.00001060
Iteration 74/1000 | Loss: 0.00001060
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001060
Iteration 77/1000 | Loss: 0.00001060
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001059
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001059
Iteration 84/1000 | Loss: 0.00001059
Iteration 85/1000 | Loss: 0.00001059
Iteration 86/1000 | Loss: 0.00001059
Iteration 87/1000 | Loss: 0.00001059
Iteration 88/1000 | Loss: 0.00001059
Iteration 89/1000 | Loss: 0.00001059
Iteration 90/1000 | Loss: 0.00001059
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.0593211300147232e-05, 1.0593211300147232e-05, 1.0593211300147232e-05, 1.0593211300147232e-05, 1.0593211300147232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0593211300147232e-05

Optimization complete. Final v2v error: 2.7742514610290527 mm

Highest mean error: 3.0572681427001953 mm for frame 170

Lowest mean error: 2.594360589981079 mm for frame 20

Saving results

Total time: 34.663899421691895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017887
Iteration 2/25 | Loss: 0.00288597
Iteration 3/25 | Loss: 0.00205131
Iteration 4/25 | Loss: 0.00196936
Iteration 5/25 | Loss: 0.00157989
Iteration 6/25 | Loss: 0.00164436
Iteration 7/25 | Loss: 0.00145301
Iteration 8/25 | Loss: 0.00120115
Iteration 9/25 | Loss: 0.00110219
Iteration 10/25 | Loss: 0.00105469
Iteration 11/25 | Loss: 0.00104717
Iteration 12/25 | Loss: 0.00103276
Iteration 13/25 | Loss: 0.00101343
Iteration 14/25 | Loss: 0.00100805
Iteration 15/25 | Loss: 0.00100145
Iteration 16/25 | Loss: 0.00099600
Iteration 17/25 | Loss: 0.00099259
Iteration 18/25 | Loss: 0.00098603
Iteration 19/25 | Loss: 0.00097840
Iteration 20/25 | Loss: 0.00097492
Iteration 21/25 | Loss: 0.00097394
Iteration 22/25 | Loss: 0.00097272
Iteration 23/25 | Loss: 0.00097442
Iteration 24/25 | Loss: 0.00097248
Iteration 25/25 | Loss: 0.00097248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44541645
Iteration 2/25 | Loss: 0.00310295
Iteration 3/25 | Loss: 0.00244421
Iteration 4/25 | Loss: 0.00244421
Iteration 5/25 | Loss: 0.00244421
Iteration 6/25 | Loss: 0.00244421
Iteration 7/25 | Loss: 0.00244421
Iteration 8/25 | Loss: 0.00244421
Iteration 9/25 | Loss: 0.00244421
Iteration 10/25 | Loss: 0.00244421
Iteration 11/25 | Loss: 0.00244421
Iteration 12/25 | Loss: 0.00244421
Iteration 13/25 | Loss: 0.00244421
Iteration 14/25 | Loss: 0.00244421
Iteration 15/25 | Loss: 0.00244421
Iteration 16/25 | Loss: 0.00244421
Iteration 17/25 | Loss: 0.00244421
Iteration 18/25 | Loss: 0.00244421
Iteration 19/25 | Loss: 0.00244421
Iteration 20/25 | Loss: 0.00244421
Iteration 21/25 | Loss: 0.00244421
Iteration 22/25 | Loss: 0.00244421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0024442109279334545, 0.0024442109279334545, 0.0024442109279334545, 0.0024442109279334545, 0.0024442109279334545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024442109279334545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244421
Iteration 2/1000 | Loss: 0.00072984
Iteration 3/1000 | Loss: 0.00057491
Iteration 4/1000 | Loss: 0.00094571
Iteration 5/1000 | Loss: 0.00029607
Iteration 6/1000 | Loss: 0.00030800
Iteration 7/1000 | Loss: 0.00033420
Iteration 8/1000 | Loss: 0.00126981
Iteration 9/1000 | Loss: 0.00032109
Iteration 10/1000 | Loss: 0.00082744
Iteration 11/1000 | Loss: 0.00022648
Iteration 12/1000 | Loss: 0.00020788
Iteration 13/1000 | Loss: 0.00039520
Iteration 14/1000 | Loss: 0.00021245
Iteration 15/1000 | Loss: 0.00033452
Iteration 16/1000 | Loss: 0.00016883
Iteration 17/1000 | Loss: 0.00041303
Iteration 18/1000 | Loss: 0.00060611
Iteration 19/1000 | Loss: 0.00507445
Iteration 20/1000 | Loss: 0.00268460
Iteration 21/1000 | Loss: 0.00158594
Iteration 22/1000 | Loss: 0.00199736
Iteration 23/1000 | Loss: 0.00032843
Iteration 24/1000 | Loss: 0.00101414
Iteration 25/1000 | Loss: 0.00050964
Iteration 26/1000 | Loss: 0.00058214
Iteration 27/1000 | Loss: 0.00036083
Iteration 28/1000 | Loss: 0.00023994
Iteration 29/1000 | Loss: 0.00010593
Iteration 30/1000 | Loss: 0.00007899
Iteration 31/1000 | Loss: 0.00019148
Iteration 32/1000 | Loss: 0.00005419
Iteration 33/1000 | Loss: 0.00021563
Iteration 34/1000 | Loss: 0.00005892
Iteration 35/1000 | Loss: 0.00005522
Iteration 36/1000 | Loss: 0.00017272
Iteration 37/1000 | Loss: 0.00006790
Iteration 38/1000 | Loss: 0.00007863
Iteration 39/1000 | Loss: 0.00008571
Iteration 40/1000 | Loss: 0.00013131
Iteration 41/1000 | Loss: 0.00073080
Iteration 42/1000 | Loss: 0.00029546
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002325
Iteration 45/1000 | Loss: 0.00008860
Iteration 46/1000 | Loss: 0.00006966
Iteration 47/1000 | Loss: 0.00002342
Iteration 48/1000 | Loss: 0.00002817
Iteration 49/1000 | Loss: 0.00003208
Iteration 50/1000 | Loss: 0.00003020
Iteration 51/1000 | Loss: 0.00002452
Iteration 52/1000 | Loss: 0.00002012
Iteration 53/1000 | Loss: 0.00003031
Iteration 54/1000 | Loss: 0.00002042
Iteration 55/1000 | Loss: 0.00001979
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001978
Iteration 60/1000 | Loss: 0.00001977
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001971
Iteration 67/1000 | Loss: 0.00001970
Iteration 68/1000 | Loss: 0.00001970
Iteration 69/1000 | Loss: 0.00001970
Iteration 70/1000 | Loss: 0.00001969
Iteration 71/1000 | Loss: 0.00001969
Iteration 72/1000 | Loss: 0.00001969
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001968
Iteration 75/1000 | Loss: 0.00001967
Iteration 76/1000 | Loss: 0.00001967
Iteration 77/1000 | Loss: 0.00001967
Iteration 78/1000 | Loss: 0.00001966
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00002225
Iteration 81/1000 | Loss: 0.00003315
Iteration 82/1000 | Loss: 0.00002351
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001957
Iteration 94/1000 | Loss: 0.00001957
Iteration 95/1000 | Loss: 0.00001957
Iteration 96/1000 | Loss: 0.00001957
Iteration 97/1000 | Loss: 0.00001957
Iteration 98/1000 | Loss: 0.00001957
Iteration 99/1000 | Loss: 0.00001957
Iteration 100/1000 | Loss: 0.00001957
Iteration 101/1000 | Loss: 0.00001957
Iteration 102/1000 | Loss: 0.00001957
Iteration 103/1000 | Loss: 0.00001957
Iteration 104/1000 | Loss: 0.00001957
Iteration 105/1000 | Loss: 0.00001957
Iteration 106/1000 | Loss: 0.00001957
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00001956
Iteration 109/1000 | Loss: 0.00001956
Iteration 110/1000 | Loss: 0.00001956
Iteration 111/1000 | Loss: 0.00001956
Iteration 112/1000 | Loss: 0.00001956
Iteration 113/1000 | Loss: 0.00001956
Iteration 114/1000 | Loss: 0.00001956
Iteration 115/1000 | Loss: 0.00001956
Iteration 116/1000 | Loss: 0.00001956
Iteration 117/1000 | Loss: 0.00001956
Iteration 118/1000 | Loss: 0.00001956
Iteration 119/1000 | Loss: 0.00001956
Iteration 120/1000 | Loss: 0.00001956
Iteration 121/1000 | Loss: 0.00001956
Iteration 122/1000 | Loss: 0.00001956
Iteration 123/1000 | Loss: 0.00001956
Iteration 124/1000 | Loss: 0.00001956
Iteration 125/1000 | Loss: 0.00001956
Iteration 126/1000 | Loss: 0.00001956
Iteration 127/1000 | Loss: 0.00001956
Iteration 128/1000 | Loss: 0.00001956
Iteration 129/1000 | Loss: 0.00001956
Iteration 130/1000 | Loss: 0.00001956
Iteration 131/1000 | Loss: 0.00001956
Iteration 132/1000 | Loss: 0.00001956
Iteration 133/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.955598963832017e-05, 1.955598963832017e-05, 1.955598963832017e-05, 1.955598963832017e-05, 1.955598963832017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.955598963832017e-05

Optimization complete. Final v2v error: 3.712585926055908 mm

Highest mean error: 5.625874042510986 mm for frame 18

Lowest mean error: 3.297825813293457 mm for frame 56

Saving results

Total time: 145.7392098903656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729419
Iteration 2/25 | Loss: 0.00089972
Iteration 3/25 | Loss: 0.00066945
Iteration 4/25 | Loss: 0.00062967
Iteration 5/25 | Loss: 0.00060626
Iteration 6/25 | Loss: 0.00059097
Iteration 7/25 | Loss: 0.00058667
Iteration 8/25 | Loss: 0.00058623
Iteration 9/25 | Loss: 0.00058607
Iteration 10/25 | Loss: 0.00058606
Iteration 11/25 | Loss: 0.00058606
Iteration 12/25 | Loss: 0.00058606
Iteration 13/25 | Loss: 0.00058606
Iteration 14/25 | Loss: 0.00058606
Iteration 15/25 | Loss: 0.00058606
Iteration 16/25 | Loss: 0.00058606
Iteration 17/25 | Loss: 0.00058606
Iteration 18/25 | Loss: 0.00058606
Iteration 19/25 | Loss: 0.00058605
Iteration 20/25 | Loss: 0.00058605
Iteration 21/25 | Loss: 0.00058605
Iteration 22/25 | Loss: 0.00058605
Iteration 23/25 | Loss: 0.00058605
Iteration 24/25 | Loss: 0.00058605
Iteration 25/25 | Loss: 0.00058605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08102179
Iteration 2/25 | Loss: 0.00022042
Iteration 3/25 | Loss: 0.00022042
Iteration 4/25 | Loss: 0.00022041
Iteration 5/25 | Loss: 0.00022041
Iteration 6/25 | Loss: 0.00022041
Iteration 7/25 | Loss: 0.00022041
Iteration 8/25 | Loss: 0.00022041
Iteration 9/25 | Loss: 0.00022041
Iteration 10/25 | Loss: 0.00022041
Iteration 11/25 | Loss: 0.00022041
Iteration 12/25 | Loss: 0.00022041
Iteration 13/25 | Loss: 0.00022041
Iteration 14/25 | Loss: 0.00022041
Iteration 15/25 | Loss: 0.00022041
Iteration 16/25 | Loss: 0.00022041
Iteration 17/25 | Loss: 0.00022041
Iteration 18/25 | Loss: 0.00022041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00022041270858608186, 0.00022041270858608186, 0.00022041270858608186, 0.00022041270858608186, 0.00022041270858608186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022041270858608186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022041
Iteration 2/1000 | Loss: 0.00002025
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001395
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001394
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001373
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001369
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001352
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001332
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001331
Iteration 45/1000 | Loss: 0.00001331
Iteration 46/1000 | Loss: 0.00001331
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.3225912880443502e-05, 1.3225912880443502e-05, 1.3225912880443502e-05, 1.3225912880443502e-05, 1.3225912880443502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3225912880443502e-05

Optimization complete. Final v2v error: 3.080256462097168 mm

Highest mean error: 3.384556293487549 mm for frame 84

Lowest mean error: 2.9093880653381348 mm for frame 1

Saving results

Total time: 46.10329985618591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010524
Iteration 2/25 | Loss: 0.00163216
Iteration 3/25 | Loss: 0.00095507
Iteration 4/25 | Loss: 0.00089167
Iteration 5/25 | Loss: 0.00087323
Iteration 6/25 | Loss: 0.00086959
Iteration 7/25 | Loss: 0.00086898
Iteration 8/25 | Loss: 0.00086898
Iteration 9/25 | Loss: 0.00086898
Iteration 10/25 | Loss: 0.00086898
Iteration 11/25 | Loss: 0.00086898
Iteration 12/25 | Loss: 0.00086898
Iteration 13/25 | Loss: 0.00086898
Iteration 14/25 | Loss: 0.00086898
Iteration 15/25 | Loss: 0.00086898
Iteration 16/25 | Loss: 0.00086898
Iteration 17/25 | Loss: 0.00086898
Iteration 18/25 | Loss: 0.00086898
Iteration 19/25 | Loss: 0.00086898
Iteration 20/25 | Loss: 0.00086898
Iteration 21/25 | Loss: 0.00086898
Iteration 22/25 | Loss: 0.00086898
Iteration 23/25 | Loss: 0.00086898
Iteration 24/25 | Loss: 0.00086898
Iteration 25/25 | Loss: 0.00086898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84490597
Iteration 2/25 | Loss: 0.00025302
Iteration 3/25 | Loss: 0.00025301
Iteration 4/25 | Loss: 0.00025301
Iteration 5/25 | Loss: 0.00025301
Iteration 6/25 | Loss: 0.00025301
Iteration 7/25 | Loss: 0.00025301
Iteration 8/25 | Loss: 0.00025301
Iteration 9/25 | Loss: 0.00025301
Iteration 10/25 | Loss: 0.00025301
Iteration 11/25 | Loss: 0.00025301
Iteration 12/25 | Loss: 0.00025301
Iteration 13/25 | Loss: 0.00025301
Iteration 14/25 | Loss: 0.00025301
Iteration 15/25 | Loss: 0.00025301
Iteration 16/25 | Loss: 0.00025301
Iteration 17/25 | Loss: 0.00025301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002530094934627414, 0.0002530094934627414, 0.0002530094934627414, 0.0002530094934627414, 0.0002530094934627414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002530094934627414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025301
Iteration 2/1000 | Loss: 0.00006163
Iteration 3/1000 | Loss: 0.00004418
Iteration 4/1000 | Loss: 0.00004035
Iteration 5/1000 | Loss: 0.00003887
Iteration 6/1000 | Loss: 0.00003798
Iteration 7/1000 | Loss: 0.00003730
Iteration 8/1000 | Loss: 0.00003666
Iteration 9/1000 | Loss: 0.00003631
Iteration 10/1000 | Loss: 0.00003597
Iteration 11/1000 | Loss: 0.00003573
Iteration 12/1000 | Loss: 0.00003551
Iteration 13/1000 | Loss: 0.00003531
Iteration 14/1000 | Loss: 0.00003521
Iteration 15/1000 | Loss: 0.00003517
Iteration 16/1000 | Loss: 0.00003516
Iteration 17/1000 | Loss: 0.00003511
Iteration 18/1000 | Loss: 0.00003497
Iteration 19/1000 | Loss: 0.00003485
Iteration 20/1000 | Loss: 0.00003484
Iteration 21/1000 | Loss: 0.00003479
Iteration 22/1000 | Loss: 0.00003479
Iteration 23/1000 | Loss: 0.00003478
Iteration 24/1000 | Loss: 0.00003477
Iteration 25/1000 | Loss: 0.00003477
Iteration 26/1000 | Loss: 0.00003475
Iteration 27/1000 | Loss: 0.00003474
Iteration 28/1000 | Loss: 0.00003473
Iteration 29/1000 | Loss: 0.00003472
Iteration 30/1000 | Loss: 0.00003471
Iteration 31/1000 | Loss: 0.00003467
Iteration 32/1000 | Loss: 0.00003467
Iteration 33/1000 | Loss: 0.00003465
Iteration 34/1000 | Loss: 0.00003464
Iteration 35/1000 | Loss: 0.00003464
Iteration 36/1000 | Loss: 0.00003461
Iteration 37/1000 | Loss: 0.00003461
Iteration 38/1000 | Loss: 0.00003458
Iteration 39/1000 | Loss: 0.00003458
Iteration 40/1000 | Loss: 0.00003457
Iteration 41/1000 | Loss: 0.00003455
Iteration 42/1000 | Loss: 0.00003455
Iteration 43/1000 | Loss: 0.00003453
Iteration 44/1000 | Loss: 0.00003453
Iteration 45/1000 | Loss: 0.00003453
Iteration 46/1000 | Loss: 0.00003453
Iteration 47/1000 | Loss: 0.00003453
Iteration 48/1000 | Loss: 0.00003453
Iteration 49/1000 | Loss: 0.00003453
Iteration 50/1000 | Loss: 0.00003453
Iteration 51/1000 | Loss: 0.00003452
Iteration 52/1000 | Loss: 0.00003452
Iteration 53/1000 | Loss: 0.00003451
Iteration 54/1000 | Loss: 0.00003451
Iteration 55/1000 | Loss: 0.00003451
Iteration 56/1000 | Loss: 0.00003450
Iteration 57/1000 | Loss: 0.00003450
Iteration 58/1000 | Loss: 0.00003447
Iteration 59/1000 | Loss: 0.00003447
Iteration 60/1000 | Loss: 0.00003447
Iteration 61/1000 | Loss: 0.00003446
Iteration 62/1000 | Loss: 0.00003446
Iteration 63/1000 | Loss: 0.00003446
Iteration 64/1000 | Loss: 0.00003445
Iteration 65/1000 | Loss: 0.00003445
Iteration 66/1000 | Loss: 0.00003445
Iteration 67/1000 | Loss: 0.00003445
Iteration 68/1000 | Loss: 0.00003445
Iteration 69/1000 | Loss: 0.00003445
Iteration 70/1000 | Loss: 0.00003445
Iteration 71/1000 | Loss: 0.00003445
Iteration 72/1000 | Loss: 0.00003444
Iteration 73/1000 | Loss: 0.00003444
Iteration 74/1000 | Loss: 0.00003444
Iteration 75/1000 | Loss: 0.00003443
Iteration 76/1000 | Loss: 0.00003443
Iteration 77/1000 | Loss: 0.00003443
Iteration 78/1000 | Loss: 0.00003443
Iteration 79/1000 | Loss: 0.00003443
Iteration 80/1000 | Loss: 0.00003443
Iteration 81/1000 | Loss: 0.00003442
Iteration 82/1000 | Loss: 0.00003442
Iteration 83/1000 | Loss: 0.00003442
Iteration 84/1000 | Loss: 0.00003442
Iteration 85/1000 | Loss: 0.00003442
Iteration 86/1000 | Loss: 0.00003441
Iteration 87/1000 | Loss: 0.00003441
Iteration 88/1000 | Loss: 0.00003441
Iteration 89/1000 | Loss: 0.00003441
Iteration 90/1000 | Loss: 0.00003441
Iteration 91/1000 | Loss: 0.00003441
Iteration 92/1000 | Loss: 0.00003441
Iteration 93/1000 | Loss: 0.00003441
Iteration 94/1000 | Loss: 0.00003441
Iteration 95/1000 | Loss: 0.00003441
Iteration 96/1000 | Loss: 0.00003441
Iteration 97/1000 | Loss: 0.00003441
Iteration 98/1000 | Loss: 0.00003441
Iteration 99/1000 | Loss: 0.00003441
Iteration 100/1000 | Loss: 0.00003441
Iteration 101/1000 | Loss: 0.00003441
Iteration 102/1000 | Loss: 0.00003441
Iteration 103/1000 | Loss: 0.00003441
Iteration 104/1000 | Loss: 0.00003440
Iteration 105/1000 | Loss: 0.00003440
Iteration 106/1000 | Loss: 0.00003440
Iteration 107/1000 | Loss: 0.00003440
Iteration 108/1000 | Loss: 0.00003440
Iteration 109/1000 | Loss: 0.00003440
Iteration 110/1000 | Loss: 0.00003440
Iteration 111/1000 | Loss: 0.00003440
Iteration 112/1000 | Loss: 0.00003440
Iteration 113/1000 | Loss: 0.00003440
Iteration 114/1000 | Loss: 0.00003440
Iteration 115/1000 | Loss: 0.00003440
Iteration 116/1000 | Loss: 0.00003440
Iteration 117/1000 | Loss: 0.00003440
Iteration 118/1000 | Loss: 0.00003440
Iteration 119/1000 | Loss: 0.00003440
Iteration 120/1000 | Loss: 0.00003440
Iteration 121/1000 | Loss: 0.00003440
Iteration 122/1000 | Loss: 0.00003440
Iteration 123/1000 | Loss: 0.00003440
Iteration 124/1000 | Loss: 0.00003440
Iteration 125/1000 | Loss: 0.00003440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.4404634789098054e-05, 3.4404634789098054e-05, 3.4404634789098054e-05, 3.4404634789098054e-05, 3.4404634789098054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4404634789098054e-05

Optimization complete. Final v2v error: 4.695015907287598 mm

Highest mean error: 5.800795078277588 mm for frame 105

Lowest mean error: 3.7453529834747314 mm for frame 32

Saving results

Total time: 45.89468240737915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101378
Iteration 2/25 | Loss: 0.00166520
Iteration 3/25 | Loss: 0.00099271
Iteration 4/25 | Loss: 0.00083089
Iteration 5/25 | Loss: 0.00080136
Iteration 6/25 | Loss: 0.00079563
Iteration 7/25 | Loss: 0.00079404
Iteration 8/25 | Loss: 0.00079386
Iteration 9/25 | Loss: 0.00079386
Iteration 10/25 | Loss: 0.00079386
Iteration 11/25 | Loss: 0.00079386
Iteration 12/25 | Loss: 0.00079386
Iteration 13/25 | Loss: 0.00079386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007938591297715902, 0.0007938591297715902, 0.0007938591297715902, 0.0007938591297715902, 0.0007938591297715902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007938591297715902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92357916
Iteration 2/25 | Loss: 0.00047622
Iteration 3/25 | Loss: 0.00047621
Iteration 4/25 | Loss: 0.00047621
Iteration 5/25 | Loss: 0.00047621
Iteration 6/25 | Loss: 0.00047621
Iteration 7/25 | Loss: 0.00047621
Iteration 8/25 | Loss: 0.00047621
Iteration 9/25 | Loss: 0.00047621
Iteration 10/25 | Loss: 0.00047621
Iteration 11/25 | Loss: 0.00047621
Iteration 12/25 | Loss: 0.00047621
Iteration 13/25 | Loss: 0.00047621
Iteration 14/25 | Loss: 0.00047621
Iteration 15/25 | Loss: 0.00047621
Iteration 16/25 | Loss: 0.00047621
Iteration 17/25 | Loss: 0.00047621
Iteration 18/25 | Loss: 0.00047621
Iteration 19/25 | Loss: 0.00047621
Iteration 20/25 | Loss: 0.00047621
Iteration 21/25 | Loss: 0.00047621
Iteration 22/25 | Loss: 0.00047621
Iteration 23/25 | Loss: 0.00047621
Iteration 24/25 | Loss: 0.00047621
Iteration 25/25 | Loss: 0.00047621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047621
Iteration 2/1000 | Loss: 0.00003992
Iteration 3/1000 | Loss: 0.00003269
Iteration 4/1000 | Loss: 0.00003080
Iteration 5/1000 | Loss: 0.00003016
Iteration 6/1000 | Loss: 0.00002954
Iteration 7/1000 | Loss: 0.00002910
Iteration 8/1000 | Loss: 0.00002873
Iteration 9/1000 | Loss: 0.00002838
Iteration 10/1000 | Loss: 0.00002832
Iteration 11/1000 | Loss: 0.00002818
Iteration 12/1000 | Loss: 0.00002802
Iteration 13/1000 | Loss: 0.00002790
Iteration 14/1000 | Loss: 0.00002788
Iteration 15/1000 | Loss: 0.00002787
Iteration 16/1000 | Loss: 0.00002776
Iteration 17/1000 | Loss: 0.00002776
Iteration 18/1000 | Loss: 0.00002774
Iteration 19/1000 | Loss: 0.00002774
Iteration 20/1000 | Loss: 0.00002771
Iteration 21/1000 | Loss: 0.00002770
Iteration 22/1000 | Loss: 0.00002768
Iteration 23/1000 | Loss: 0.00002767
Iteration 24/1000 | Loss: 0.00002767
Iteration 25/1000 | Loss: 0.00002766
Iteration 26/1000 | Loss: 0.00002766
Iteration 27/1000 | Loss: 0.00002766
Iteration 28/1000 | Loss: 0.00002766
Iteration 29/1000 | Loss: 0.00002766
Iteration 30/1000 | Loss: 0.00002766
Iteration 31/1000 | Loss: 0.00002766
Iteration 32/1000 | Loss: 0.00002765
Iteration 33/1000 | Loss: 0.00002765
Iteration 34/1000 | Loss: 0.00002765
Iteration 35/1000 | Loss: 0.00002765
Iteration 36/1000 | Loss: 0.00002765
Iteration 37/1000 | Loss: 0.00002765
Iteration 38/1000 | Loss: 0.00002763
Iteration 39/1000 | Loss: 0.00002763
Iteration 40/1000 | Loss: 0.00002762
Iteration 41/1000 | Loss: 0.00002762
Iteration 42/1000 | Loss: 0.00002762
Iteration 43/1000 | Loss: 0.00002761
Iteration 44/1000 | Loss: 0.00002761
Iteration 45/1000 | Loss: 0.00002760
Iteration 46/1000 | Loss: 0.00002760
Iteration 47/1000 | Loss: 0.00002760
Iteration 48/1000 | Loss: 0.00002760
Iteration 49/1000 | Loss: 0.00002759
Iteration 50/1000 | Loss: 0.00002759
Iteration 51/1000 | Loss: 0.00002759
Iteration 52/1000 | Loss: 0.00002759
Iteration 53/1000 | Loss: 0.00002759
Iteration 54/1000 | Loss: 0.00002759
Iteration 55/1000 | Loss: 0.00002759
Iteration 56/1000 | Loss: 0.00002759
Iteration 57/1000 | Loss: 0.00002758
Iteration 58/1000 | Loss: 0.00002758
Iteration 59/1000 | Loss: 0.00002758
Iteration 60/1000 | Loss: 0.00002757
Iteration 61/1000 | Loss: 0.00002757
Iteration 62/1000 | Loss: 0.00002757
Iteration 63/1000 | Loss: 0.00002757
Iteration 64/1000 | Loss: 0.00002756
Iteration 65/1000 | Loss: 0.00002756
Iteration 66/1000 | Loss: 0.00002756
Iteration 67/1000 | Loss: 0.00002755
Iteration 68/1000 | Loss: 0.00002755
Iteration 69/1000 | Loss: 0.00002755
Iteration 70/1000 | Loss: 0.00002755
Iteration 71/1000 | Loss: 0.00002755
Iteration 72/1000 | Loss: 0.00002755
Iteration 73/1000 | Loss: 0.00002754
Iteration 74/1000 | Loss: 0.00002754
Iteration 75/1000 | Loss: 0.00002754
Iteration 76/1000 | Loss: 0.00002754
Iteration 77/1000 | Loss: 0.00002754
Iteration 78/1000 | Loss: 0.00002754
Iteration 79/1000 | Loss: 0.00002754
Iteration 80/1000 | Loss: 0.00002753
Iteration 81/1000 | Loss: 0.00002753
Iteration 82/1000 | Loss: 0.00002753
Iteration 83/1000 | Loss: 0.00002753
Iteration 84/1000 | Loss: 0.00002753
Iteration 85/1000 | Loss: 0.00002753
Iteration 86/1000 | Loss: 0.00002752
Iteration 87/1000 | Loss: 0.00002752
Iteration 88/1000 | Loss: 0.00002752
Iteration 89/1000 | Loss: 0.00002752
Iteration 90/1000 | Loss: 0.00002751
Iteration 91/1000 | Loss: 0.00002751
Iteration 92/1000 | Loss: 0.00002751
Iteration 93/1000 | Loss: 0.00002750
Iteration 94/1000 | Loss: 0.00002750
Iteration 95/1000 | Loss: 0.00002749
Iteration 96/1000 | Loss: 0.00002749
Iteration 97/1000 | Loss: 0.00002749
Iteration 98/1000 | Loss: 0.00002748
Iteration 99/1000 | Loss: 0.00002747
Iteration 100/1000 | Loss: 0.00002747
Iteration 101/1000 | Loss: 0.00002747
Iteration 102/1000 | Loss: 0.00002745
Iteration 103/1000 | Loss: 0.00002745
Iteration 104/1000 | Loss: 0.00002745
Iteration 105/1000 | Loss: 0.00002744
Iteration 106/1000 | Loss: 0.00002743
Iteration 107/1000 | Loss: 0.00002742
Iteration 108/1000 | Loss: 0.00002742
Iteration 109/1000 | Loss: 0.00002742
Iteration 110/1000 | Loss: 0.00002741
Iteration 111/1000 | Loss: 0.00002741
Iteration 112/1000 | Loss: 0.00002741
Iteration 113/1000 | Loss: 0.00002740
Iteration 114/1000 | Loss: 0.00002740
Iteration 115/1000 | Loss: 0.00002740
Iteration 116/1000 | Loss: 0.00002739
Iteration 117/1000 | Loss: 0.00002739
Iteration 118/1000 | Loss: 0.00002739
Iteration 119/1000 | Loss: 0.00002738
Iteration 120/1000 | Loss: 0.00002738
Iteration 121/1000 | Loss: 0.00002737
Iteration 122/1000 | Loss: 0.00002737
Iteration 123/1000 | Loss: 0.00002736
Iteration 124/1000 | Loss: 0.00002736
Iteration 125/1000 | Loss: 0.00002733
Iteration 126/1000 | Loss: 0.00002733
Iteration 127/1000 | Loss: 0.00002732
Iteration 128/1000 | Loss: 0.00002732
Iteration 129/1000 | Loss: 0.00002731
Iteration 130/1000 | Loss: 0.00002731
Iteration 131/1000 | Loss: 0.00002730
Iteration 132/1000 | Loss: 0.00002730
Iteration 133/1000 | Loss: 0.00002730
Iteration 134/1000 | Loss: 0.00002729
Iteration 135/1000 | Loss: 0.00002729
Iteration 136/1000 | Loss: 0.00002729
Iteration 137/1000 | Loss: 0.00002729
Iteration 138/1000 | Loss: 0.00002729
Iteration 139/1000 | Loss: 0.00002729
Iteration 140/1000 | Loss: 0.00002729
Iteration 141/1000 | Loss: 0.00002729
Iteration 142/1000 | Loss: 0.00002729
Iteration 143/1000 | Loss: 0.00002729
Iteration 144/1000 | Loss: 0.00002729
Iteration 145/1000 | Loss: 0.00002729
Iteration 146/1000 | Loss: 0.00002729
Iteration 147/1000 | Loss: 0.00002728
Iteration 148/1000 | Loss: 0.00002727
Iteration 149/1000 | Loss: 0.00002727
Iteration 150/1000 | Loss: 0.00002727
Iteration 151/1000 | Loss: 0.00002727
Iteration 152/1000 | Loss: 0.00002727
Iteration 153/1000 | Loss: 0.00002726
Iteration 154/1000 | Loss: 0.00002726
Iteration 155/1000 | Loss: 0.00002726
Iteration 156/1000 | Loss: 0.00002726
Iteration 157/1000 | Loss: 0.00002726
Iteration 158/1000 | Loss: 0.00002726
Iteration 159/1000 | Loss: 0.00002726
Iteration 160/1000 | Loss: 0.00002726
Iteration 161/1000 | Loss: 0.00002726
Iteration 162/1000 | Loss: 0.00002726
Iteration 163/1000 | Loss: 0.00002726
Iteration 164/1000 | Loss: 0.00002726
Iteration 165/1000 | Loss: 0.00002726
Iteration 166/1000 | Loss: 0.00002726
Iteration 167/1000 | Loss: 0.00002726
Iteration 168/1000 | Loss: 0.00002726
Iteration 169/1000 | Loss: 0.00002726
Iteration 170/1000 | Loss: 0.00002726
Iteration 171/1000 | Loss: 0.00002726
Iteration 172/1000 | Loss: 0.00002726
Iteration 173/1000 | Loss: 0.00002726
Iteration 174/1000 | Loss: 0.00002726
Iteration 175/1000 | Loss: 0.00002726
Iteration 176/1000 | Loss: 0.00002726
Iteration 177/1000 | Loss: 0.00002726
Iteration 178/1000 | Loss: 0.00002726
Iteration 179/1000 | Loss: 0.00002726
Iteration 180/1000 | Loss: 0.00002726
Iteration 181/1000 | Loss: 0.00002726
Iteration 182/1000 | Loss: 0.00002726
Iteration 183/1000 | Loss: 0.00002726
Iteration 184/1000 | Loss: 0.00002726
Iteration 185/1000 | Loss: 0.00002726
Iteration 186/1000 | Loss: 0.00002726
Iteration 187/1000 | Loss: 0.00002726
Iteration 188/1000 | Loss: 0.00002726
Iteration 189/1000 | Loss: 0.00002726
Iteration 190/1000 | Loss: 0.00002726
Iteration 191/1000 | Loss: 0.00002726
Iteration 192/1000 | Loss: 0.00002726
Iteration 193/1000 | Loss: 0.00002726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.7260533897788264e-05, 2.7260533897788264e-05, 2.7260533897788264e-05, 2.7260533897788264e-05, 2.7260533897788264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7260533897788264e-05

Optimization complete. Final v2v error: 4.271608829498291 mm

Highest mean error: 4.7820587158203125 mm for frame 100

Lowest mean error: 3.7815442085266113 mm for frame 154

Saving results

Total time: 49.13609743118286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775812
Iteration 2/25 | Loss: 0.00096867
Iteration 3/25 | Loss: 0.00072210
Iteration 4/25 | Loss: 0.00067381
Iteration 5/25 | Loss: 0.00066936
Iteration 6/25 | Loss: 0.00066893
Iteration 7/25 | Loss: 0.00066893
Iteration 8/25 | Loss: 0.00066893
Iteration 9/25 | Loss: 0.00066893
Iteration 10/25 | Loss: 0.00066893
Iteration 11/25 | Loss: 0.00066893
Iteration 12/25 | Loss: 0.00066893
Iteration 13/25 | Loss: 0.00066893
Iteration 14/25 | Loss: 0.00066893
Iteration 15/25 | Loss: 0.00066893
Iteration 16/25 | Loss: 0.00066893
Iteration 17/25 | Loss: 0.00066893
Iteration 18/25 | Loss: 0.00066893
Iteration 19/25 | Loss: 0.00066893
Iteration 20/25 | Loss: 0.00066893
Iteration 21/25 | Loss: 0.00066893
Iteration 22/25 | Loss: 0.00066893
Iteration 23/25 | Loss: 0.00066893
Iteration 24/25 | Loss: 0.00066893
Iteration 25/25 | Loss: 0.00066893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43323123
Iteration 2/25 | Loss: 0.00024059
Iteration 3/25 | Loss: 0.00024055
Iteration 4/25 | Loss: 0.00024055
Iteration 5/25 | Loss: 0.00024055
Iteration 6/25 | Loss: 0.00024055
Iteration 7/25 | Loss: 0.00024055
Iteration 8/25 | Loss: 0.00024055
Iteration 9/25 | Loss: 0.00024055
Iteration 10/25 | Loss: 0.00024055
Iteration 11/25 | Loss: 0.00024055
Iteration 12/25 | Loss: 0.00024055
Iteration 13/25 | Loss: 0.00024055
Iteration 14/25 | Loss: 0.00024055
Iteration 15/25 | Loss: 0.00024055
Iteration 16/25 | Loss: 0.00024055
Iteration 17/25 | Loss: 0.00024055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024054561799857765, 0.00024054561799857765, 0.00024054561799857765, 0.00024054561799857765, 0.00024054561799857765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024054561799857765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024055
Iteration 2/1000 | Loss: 0.00003741
Iteration 3/1000 | Loss: 0.00002836
Iteration 4/1000 | Loss: 0.00002493
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002203
Iteration 7/1000 | Loss: 0.00002112
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001994
Iteration 14/1000 | Loss: 0.00001985
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001982
Iteration 17/1000 | Loss: 0.00001978
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001977
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001970
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001970
Iteration 31/1000 | Loss: 0.00001970
Iteration 32/1000 | Loss: 0.00001970
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001969
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001968
Iteration 38/1000 | Loss: 0.00001968
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001967
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.9666136722662486e-05, 1.9666136722662486e-05, 1.9666136722662486e-05, 1.9666136722662486e-05, 1.9666136722662486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9666136722662486e-05

Optimization complete. Final v2v error: 3.7487375736236572 mm

Highest mean error: 4.04290246963501 mm for frame 230

Lowest mean error: 3.541071891784668 mm for frame 29

Saving results

Total time: 32.73702621459961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049926
Iteration 2/25 | Loss: 0.00190488
Iteration 3/25 | Loss: 0.00119796
Iteration 4/25 | Loss: 0.00121210
Iteration 5/25 | Loss: 0.00099518
Iteration 6/25 | Loss: 0.00099308
Iteration 7/25 | Loss: 0.00098465
Iteration 8/25 | Loss: 0.00080945
Iteration 9/25 | Loss: 0.00075205
Iteration 10/25 | Loss: 0.00070019
Iteration 11/25 | Loss: 0.00069424
Iteration 12/25 | Loss: 0.00073031
Iteration 13/25 | Loss: 0.00067546
Iteration 14/25 | Loss: 0.00065523
Iteration 15/25 | Loss: 0.00064300
Iteration 16/25 | Loss: 0.00063681
Iteration 17/25 | Loss: 0.00062542
Iteration 18/25 | Loss: 0.00062744
Iteration 19/25 | Loss: 0.00062694
Iteration 20/25 | Loss: 0.00062632
Iteration 21/25 | Loss: 0.00063001
Iteration 22/25 | Loss: 0.00062850
Iteration 23/25 | Loss: 0.00062311
Iteration 24/25 | Loss: 0.00062319
Iteration 25/25 | Loss: 0.00062566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54637647
Iteration 2/25 | Loss: 0.00052754
Iteration 3/25 | Loss: 0.00052754
Iteration 4/25 | Loss: 0.00052754
Iteration 5/25 | Loss: 0.00052754
Iteration 6/25 | Loss: 0.00052754
Iteration 7/25 | Loss: 0.00052754
Iteration 8/25 | Loss: 0.00052754
Iteration 9/25 | Loss: 0.00052754
Iteration 10/25 | Loss: 0.00052754
Iteration 11/25 | Loss: 0.00052754
Iteration 12/25 | Loss: 0.00052754
Iteration 13/25 | Loss: 0.00052754
Iteration 14/25 | Loss: 0.00052754
Iteration 15/25 | Loss: 0.00052754
Iteration 16/25 | Loss: 0.00052754
Iteration 17/25 | Loss: 0.00052754
Iteration 18/25 | Loss: 0.00052754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005275373114272952, 0.0005275373114272952, 0.0005275373114272952, 0.0005275373114272952, 0.0005275373114272952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005275373114272952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052754
Iteration 2/1000 | Loss: 0.00039921
Iteration 3/1000 | Loss: 0.00037867
Iteration 4/1000 | Loss: 0.00028524
Iteration 5/1000 | Loss: 0.00032838
Iteration 6/1000 | Loss: 0.00027229
Iteration 7/1000 | Loss: 0.00019527
Iteration 8/1000 | Loss: 0.00023325
Iteration 9/1000 | Loss: 0.00023942
Iteration 10/1000 | Loss: 0.00016784
Iteration 11/1000 | Loss: 0.00024651
Iteration 12/1000 | Loss: 0.00022393
Iteration 13/1000 | Loss: 0.00017181
Iteration 14/1000 | Loss: 0.00023096
Iteration 15/1000 | Loss: 0.00024107
Iteration 16/1000 | Loss: 0.00025481
Iteration 17/1000 | Loss: 0.00012893
Iteration 18/1000 | Loss: 0.00021970
Iteration 19/1000 | Loss: 0.00017509
Iteration 20/1000 | Loss: 0.00026176
Iteration 21/1000 | Loss: 0.00019807
Iteration 22/1000 | Loss: 0.00009269
Iteration 23/1000 | Loss: 0.00023373
Iteration 24/1000 | Loss: 0.00012794
Iteration 25/1000 | Loss: 0.00012355
Iteration 26/1000 | Loss: 0.00019277
Iteration 27/1000 | Loss: 0.00022988
Iteration 28/1000 | Loss: 0.00017769
Iteration 29/1000 | Loss: 0.00026191
Iteration 30/1000 | Loss: 0.00040469
Iteration 31/1000 | Loss: 0.00003019
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002279
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00001998
Iteration 38/1000 | Loss: 0.00032845
Iteration 39/1000 | Loss: 0.00931482
Iteration 40/1000 | Loss: 0.00004871
Iteration 41/1000 | Loss: 0.00025031
Iteration 42/1000 | Loss: 0.00036751
Iteration 43/1000 | Loss: 0.00021171
Iteration 44/1000 | Loss: 0.00034818
Iteration 45/1000 | Loss: 0.00002253
Iteration 46/1000 | Loss: 0.00019296
Iteration 47/1000 | Loss: 0.00013330
Iteration 48/1000 | Loss: 0.00015848
Iteration 49/1000 | Loss: 0.00013976
Iteration 50/1000 | Loss: 0.00007295
Iteration 51/1000 | Loss: 0.00019220
Iteration 52/1000 | Loss: 0.00002038
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001877
Iteration 55/1000 | Loss: 0.00003747
Iteration 56/1000 | Loss: 0.00003024
Iteration 57/1000 | Loss: 0.00003436
Iteration 58/1000 | Loss: 0.00003086
Iteration 59/1000 | Loss: 0.00035118
Iteration 60/1000 | Loss: 0.00003492
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002005
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001497
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001494
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001492
Iteration 78/1000 | Loss: 0.00001492
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001485
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001475
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001471
Iteration 125/1000 | Loss: 0.00001471
Iteration 126/1000 | Loss: 0.00001471
Iteration 127/1000 | Loss: 0.00001471
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001470
Iteration 131/1000 | Loss: 0.00001470
Iteration 132/1000 | Loss: 0.00001470
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001469
Iteration 135/1000 | Loss: 0.00001469
Iteration 136/1000 | Loss: 0.00001469
Iteration 137/1000 | Loss: 0.00001469
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001465
Iteration 154/1000 | Loss: 0.00001465
Iteration 155/1000 | Loss: 0.00001465
Iteration 156/1000 | Loss: 0.00001465
Iteration 157/1000 | Loss: 0.00001465
Iteration 158/1000 | Loss: 0.00001465
Iteration 159/1000 | Loss: 0.00001465
Iteration 160/1000 | Loss: 0.00001465
Iteration 161/1000 | Loss: 0.00001465
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001464
Iteration 166/1000 | Loss: 0.00001464
Iteration 167/1000 | Loss: 0.00001464
Iteration 168/1000 | Loss: 0.00001464
Iteration 169/1000 | Loss: 0.00001464
Iteration 170/1000 | Loss: 0.00001464
Iteration 171/1000 | Loss: 0.00001464
Iteration 172/1000 | Loss: 0.00001464
Iteration 173/1000 | Loss: 0.00001463
Iteration 174/1000 | Loss: 0.00001463
Iteration 175/1000 | Loss: 0.00001463
Iteration 176/1000 | Loss: 0.00001463
Iteration 177/1000 | Loss: 0.00001463
Iteration 178/1000 | Loss: 0.00001463
Iteration 179/1000 | Loss: 0.00001463
Iteration 180/1000 | Loss: 0.00001463
Iteration 181/1000 | Loss: 0.00001463
Iteration 182/1000 | Loss: 0.00001463
Iteration 183/1000 | Loss: 0.00001463
Iteration 184/1000 | Loss: 0.00001463
Iteration 185/1000 | Loss: 0.00001463
Iteration 186/1000 | Loss: 0.00001462
Iteration 187/1000 | Loss: 0.00001462
Iteration 188/1000 | Loss: 0.00001462
Iteration 189/1000 | Loss: 0.00001462
Iteration 190/1000 | Loss: 0.00001462
Iteration 191/1000 | Loss: 0.00001462
Iteration 192/1000 | Loss: 0.00001462
Iteration 193/1000 | Loss: 0.00001462
Iteration 194/1000 | Loss: 0.00001462
Iteration 195/1000 | Loss: 0.00001462
Iteration 196/1000 | Loss: 0.00001462
Iteration 197/1000 | Loss: 0.00001462
Iteration 198/1000 | Loss: 0.00001462
Iteration 199/1000 | Loss: 0.00001462
Iteration 200/1000 | Loss: 0.00001462
Iteration 201/1000 | Loss: 0.00001462
Iteration 202/1000 | Loss: 0.00001462
Iteration 203/1000 | Loss: 0.00001462
Iteration 204/1000 | Loss: 0.00001462
Iteration 205/1000 | Loss: 0.00001462
Iteration 206/1000 | Loss: 0.00001462
Iteration 207/1000 | Loss: 0.00001462
Iteration 208/1000 | Loss: 0.00001462
Iteration 209/1000 | Loss: 0.00001462
Iteration 210/1000 | Loss: 0.00001462
Iteration 211/1000 | Loss: 0.00001462
Iteration 212/1000 | Loss: 0.00001462
Iteration 213/1000 | Loss: 0.00001462
Iteration 214/1000 | Loss: 0.00001462
Iteration 215/1000 | Loss: 0.00001462
Iteration 216/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.4616693079005927e-05, 1.4616693079005927e-05, 1.4616693079005927e-05, 1.4616693079005927e-05, 1.4616693079005927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4616693079005927e-05

Optimization complete. Final v2v error: 3.1698222160339355 mm

Highest mean error: 4.196483135223389 mm for frame 61

Lowest mean error: 2.6127920150756836 mm for frame 14

Saving results

Total time: 152.87463784217834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814358
Iteration 2/25 | Loss: 0.00087545
Iteration 3/25 | Loss: 0.00066165
Iteration 4/25 | Loss: 0.00061474
Iteration 5/25 | Loss: 0.00059979
Iteration 6/25 | Loss: 0.00059737
Iteration 7/25 | Loss: 0.00059634
Iteration 8/25 | Loss: 0.00059621
Iteration 9/25 | Loss: 0.00059621
Iteration 10/25 | Loss: 0.00059621
Iteration 11/25 | Loss: 0.00059621
Iteration 12/25 | Loss: 0.00059621
Iteration 13/25 | Loss: 0.00059621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005962064606137574, 0.0005962064606137574, 0.0005962064606137574, 0.0005962064606137574, 0.0005962064606137574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005962064606137574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36227131
Iteration 2/25 | Loss: 0.00026761
Iteration 3/25 | Loss: 0.00026761
Iteration 4/25 | Loss: 0.00026761
Iteration 5/25 | Loss: 0.00026761
Iteration 6/25 | Loss: 0.00026761
Iteration 7/25 | Loss: 0.00026761
Iteration 8/25 | Loss: 0.00026761
Iteration 9/25 | Loss: 0.00026761
Iteration 10/25 | Loss: 0.00026761
Iteration 11/25 | Loss: 0.00026761
Iteration 12/25 | Loss: 0.00026761
Iteration 13/25 | Loss: 0.00026761
Iteration 14/25 | Loss: 0.00026761
Iteration 15/25 | Loss: 0.00026761
Iteration 16/25 | Loss: 0.00026761
Iteration 17/25 | Loss: 0.00026761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002676074218470603, 0.0002676074218470603, 0.0002676074218470603, 0.0002676074218470603, 0.0002676074218470603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002676074218470603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026761
Iteration 2/1000 | Loss: 0.00002112
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001404
Iteration 9/1000 | Loss: 0.00001392
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001338
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001331
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001327
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001322
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001321
Iteration 78/1000 | Loss: 0.00001321
Iteration 79/1000 | Loss: 0.00001321
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001321
Iteration 92/1000 | Loss: 0.00001321
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001321
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.3208354175731074e-05, 1.3208354175731074e-05, 1.3208354175731074e-05, 1.3208354175731074e-05, 1.3208354175731074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3208354175731074e-05

Optimization complete. Final v2v error: 3.0975449085235596 mm

Highest mean error: 3.266439914703369 mm for frame 123

Lowest mean error: 2.9772093296051025 mm for frame 43

Saving results

Total time: 29.362763166427612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082688
Iteration 2/25 | Loss: 0.01082687
Iteration 3/25 | Loss: 0.00237001
Iteration 4/25 | Loss: 0.00166276
Iteration 5/25 | Loss: 0.00127891
Iteration 6/25 | Loss: 0.00115397
Iteration 7/25 | Loss: 0.00111286
Iteration 8/25 | Loss: 0.00101574
Iteration 9/25 | Loss: 0.00098702
Iteration 10/25 | Loss: 0.00083832
Iteration 11/25 | Loss: 0.00082671
Iteration 12/25 | Loss: 0.00077510
Iteration 13/25 | Loss: 0.00079477
Iteration 14/25 | Loss: 0.00076601
Iteration 15/25 | Loss: 0.00071381
Iteration 16/25 | Loss: 0.00075064
Iteration 17/25 | Loss: 0.00068406
Iteration 18/25 | Loss: 0.00068667
Iteration 19/25 | Loss: 0.00067020
Iteration 20/25 | Loss: 0.00068419
Iteration 21/25 | Loss: 0.00066887
Iteration 22/25 | Loss: 0.00066536
Iteration 23/25 | Loss: 0.00066424
Iteration 24/25 | Loss: 0.00066392
Iteration 25/25 | Loss: 0.00066378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47665238
Iteration 2/25 | Loss: 0.00041396
Iteration 3/25 | Loss: 0.00037445
Iteration 4/25 | Loss: 0.00037445
Iteration 5/25 | Loss: 0.00037445
Iteration 6/25 | Loss: 0.00037445
Iteration 7/25 | Loss: 0.00037445
Iteration 8/25 | Loss: 0.00037445
Iteration 9/25 | Loss: 0.00037445
Iteration 10/25 | Loss: 0.00037445
Iteration 11/25 | Loss: 0.00037445
Iteration 12/25 | Loss: 0.00037445
Iteration 13/25 | Loss: 0.00037445
Iteration 14/25 | Loss: 0.00037445
Iteration 15/25 | Loss: 0.00037445
Iteration 16/25 | Loss: 0.00037445
Iteration 17/25 | Loss: 0.00037445
Iteration 18/25 | Loss: 0.00037445
Iteration 19/25 | Loss: 0.00037445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003744502319023013, 0.0003744502319023013, 0.0003744502319023013, 0.0003744502319023013, 0.0003744502319023013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003744502319023013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037445
Iteration 2/1000 | Loss: 0.00007932
Iteration 3/1000 | Loss: 0.00005693
Iteration 4/1000 | Loss: 0.00002587
Iteration 5/1000 | Loss: 0.00011171
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00011973
Iteration 8/1000 | Loss: 0.00009686
Iteration 9/1000 | Loss: 0.00002476
Iteration 10/1000 | Loss: 0.00003829
Iteration 11/1000 | Loss: 0.00003463
Iteration 12/1000 | Loss: 0.00008068
Iteration 13/1000 | Loss: 0.00003174
Iteration 14/1000 | Loss: 0.00002546
Iteration 15/1000 | Loss: 0.00003689
Iteration 16/1000 | Loss: 0.00003175
Iteration 17/1000 | Loss: 0.00004594
Iteration 18/1000 | Loss: 0.00004087
Iteration 19/1000 | Loss: 0.00006254
Iteration 20/1000 | Loss: 0.00002714
Iteration 21/1000 | Loss: 0.00002279
Iteration 22/1000 | Loss: 0.00004035
Iteration 23/1000 | Loss: 0.00002266
Iteration 24/1000 | Loss: 0.00004583
Iteration 25/1000 | Loss: 0.00002268
Iteration 26/1000 | Loss: 0.00036003
Iteration 27/1000 | Loss: 0.00004791
Iteration 28/1000 | Loss: 0.00084866
Iteration 29/1000 | Loss: 0.00009354
Iteration 30/1000 | Loss: 0.00014905
Iteration 31/1000 | Loss: 0.00004224
Iteration 32/1000 | Loss: 0.00002497
Iteration 33/1000 | Loss: 0.00008023
Iteration 34/1000 | Loss: 0.00002414
Iteration 35/1000 | Loss: 0.00002378
Iteration 36/1000 | Loss: 0.00010946
Iteration 37/1000 | Loss: 0.00002093
Iteration 38/1000 | Loss: 0.00005140
Iteration 39/1000 | Loss: 0.00008806
Iteration 40/1000 | Loss: 0.00009416
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00001964
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00009751
Iteration 45/1000 | Loss: 0.00002964
Iteration 46/1000 | Loss: 0.00002176
Iteration 47/1000 | Loss: 0.00002142
Iteration 48/1000 | Loss: 0.00002166
Iteration 49/1000 | Loss: 0.00012557
Iteration 50/1000 | Loss: 0.00005493
Iteration 51/1000 | Loss: 0.00002634
Iteration 52/1000 | Loss: 0.00003626
Iteration 53/1000 | Loss: 0.00005574
Iteration 54/1000 | Loss: 0.00002817
Iteration 55/1000 | Loss: 0.00002480
Iteration 56/1000 | Loss: 0.00002636
Iteration 57/1000 | Loss: 0.00002440
Iteration 58/1000 | Loss: 0.00003047
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00004701
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00003290
Iteration 63/1000 | Loss: 0.00002476
Iteration 64/1000 | Loss: 0.00003506
Iteration 65/1000 | Loss: 0.00002395
Iteration 66/1000 | Loss: 0.00004517
Iteration 67/1000 | Loss: 0.00002396
Iteration 68/1000 | Loss: 0.00002314
Iteration 69/1000 | Loss: 0.00003767
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002583
Iteration 72/1000 | Loss: 0.00002996
Iteration 73/1000 | Loss: 0.00003652
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00004882
Iteration 77/1000 | Loss: 0.00002295
Iteration 78/1000 | Loss: 0.00006450
Iteration 79/1000 | Loss: 0.00002235
Iteration 80/1000 | Loss: 0.00002597
Iteration 81/1000 | Loss: 0.00002483
Iteration 82/1000 | Loss: 0.00002642
Iteration 83/1000 | Loss: 0.00002584
Iteration 84/1000 | Loss: 0.00002162
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002137
Iteration 87/1000 | Loss: 0.00002515
Iteration 88/1000 | Loss: 0.00002361
Iteration 89/1000 | Loss: 0.00002171
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002922
Iteration 93/1000 | Loss: 0.00002351
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002649
Iteration 96/1000 | Loss: 0.00002266
Iteration 97/1000 | Loss: 0.00002622
Iteration 98/1000 | Loss: 0.00005868
Iteration 99/1000 | Loss: 0.00002261
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00015965
Iteration 103/1000 | Loss: 0.00002080
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001575
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001536
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001528
Iteration 110/1000 | Loss: 0.00001527
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001524
Iteration 121/1000 | Loss: 0.00001524
Iteration 122/1000 | Loss: 0.00001524
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001519
Iteration 125/1000 | Loss: 0.00001519
Iteration 126/1000 | Loss: 0.00001519
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001519
Iteration 129/1000 | Loss: 0.00001519
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001518
Iteration 132/1000 | Loss: 0.00001518
Iteration 133/1000 | Loss: 0.00001518
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001517
Iteration 139/1000 | Loss: 0.00001517
Iteration 140/1000 | Loss: 0.00001517
Iteration 141/1000 | Loss: 0.00001517
Iteration 142/1000 | Loss: 0.00001517
Iteration 143/1000 | Loss: 0.00001517
Iteration 144/1000 | Loss: 0.00001517
Iteration 145/1000 | Loss: 0.00001517
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001517
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001517
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.5165232071012724e-05, 1.5165232071012724e-05, 1.5165232071012724e-05, 1.5165232071012724e-05, 1.5165232071012724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5165232071012724e-05

Optimization complete. Final v2v error: 3.2913811206817627 mm

Highest mean error: 8.996297836303711 mm for frame 229

Lowest mean error: 2.8499834537506104 mm for frame 138

Saving results

Total time: 222.72183513641357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038483
Iteration 2/25 | Loss: 0.00178626
Iteration 3/25 | Loss: 0.00105552
Iteration 4/25 | Loss: 0.00088010
Iteration 5/25 | Loss: 0.00080942
Iteration 6/25 | Loss: 0.00084375
Iteration 7/25 | Loss: 0.00090973
Iteration 8/25 | Loss: 0.00077792
Iteration 9/25 | Loss: 0.00073617
Iteration 10/25 | Loss: 0.00072457
Iteration 11/25 | Loss: 0.00071353
Iteration 12/25 | Loss: 0.00068684
Iteration 13/25 | Loss: 0.00068462
Iteration 14/25 | Loss: 0.00067299
Iteration 15/25 | Loss: 0.00066993
Iteration 16/25 | Loss: 0.00064967
Iteration 17/25 | Loss: 0.00064753
Iteration 18/25 | Loss: 0.00063976
Iteration 19/25 | Loss: 0.00063068
Iteration 20/25 | Loss: 0.00063246
Iteration 21/25 | Loss: 0.00062250
Iteration 22/25 | Loss: 0.00063133
Iteration 23/25 | Loss: 0.00063430
Iteration 24/25 | Loss: 0.00062203
Iteration 25/25 | Loss: 0.00062802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43875563
Iteration 2/25 | Loss: 0.00055775
Iteration 3/25 | Loss: 0.00048785
Iteration 4/25 | Loss: 0.00042696
Iteration 5/25 | Loss: 0.00042696
Iteration 6/25 | Loss: 0.00042696
Iteration 7/25 | Loss: 0.00042696
Iteration 8/25 | Loss: 0.00042696
Iteration 9/25 | Loss: 0.00042696
Iteration 10/25 | Loss: 0.00042696
Iteration 11/25 | Loss: 0.00042696
Iteration 12/25 | Loss: 0.00042696
Iteration 13/25 | Loss: 0.00042696
Iteration 14/25 | Loss: 0.00042696
Iteration 15/25 | Loss: 0.00042696
Iteration 16/25 | Loss: 0.00042696
Iteration 17/25 | Loss: 0.00042696
Iteration 18/25 | Loss: 0.00042696
Iteration 19/25 | Loss: 0.00042696
Iteration 20/25 | Loss: 0.00042696
Iteration 21/25 | Loss: 0.00042696
Iteration 22/25 | Loss: 0.00042696
Iteration 23/25 | Loss: 0.00042696
Iteration 24/25 | Loss: 0.00042696
Iteration 25/25 | Loss: 0.00042696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042696
Iteration 2/1000 | Loss: 0.00019289
Iteration 3/1000 | Loss: 0.00026931
Iteration 4/1000 | Loss: 0.00003055
Iteration 5/1000 | Loss: 0.00002475
Iteration 6/1000 | Loss: 0.00052591
Iteration 7/1000 | Loss: 0.00032265
Iteration 8/1000 | Loss: 0.00056383
Iteration 9/1000 | Loss: 0.00034545
Iteration 10/1000 | Loss: 0.00061629
Iteration 11/1000 | Loss: 0.00022583
Iteration 12/1000 | Loss: 0.00024697
Iteration 13/1000 | Loss: 0.00012923
Iteration 14/1000 | Loss: 0.00023641
Iteration 15/1000 | Loss: 0.00003355
Iteration 16/1000 | Loss: 0.00002674
Iteration 17/1000 | Loss: 0.00002437
Iteration 18/1000 | Loss: 0.00002303
Iteration 19/1000 | Loss: 0.00046223
Iteration 20/1000 | Loss: 0.00003819
Iteration 21/1000 | Loss: 0.00002710
Iteration 22/1000 | Loss: 0.00003286
Iteration 23/1000 | Loss: 0.00001868
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00006545
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001522
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00007094
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00002460
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00003478
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001437
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001436
Iteration 54/1000 | Loss: 0.00001436
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00004792
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001426
Iteration 120/1000 | Loss: 0.00001426
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001426
Iteration 130/1000 | Loss: 0.00001426
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001426
Iteration 133/1000 | Loss: 0.00001426
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001426
Iteration 137/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.4264613128034398e-05, 1.4264613128034398e-05, 1.4264613128034398e-05, 1.4264613128034398e-05, 1.4264613128034398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4264613128034398e-05

Optimization complete. Final v2v error: 3.070594310760498 mm

Highest mean error: 5.318661212921143 mm for frame 38

Lowest mean error: 2.5703835487365723 mm for frame 26

Saving results

Total time: 107.9273738861084
