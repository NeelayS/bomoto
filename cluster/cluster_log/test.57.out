Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=57, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3192-3247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894842
Iteration 2/25 | Loss: 0.00164658
Iteration 3/25 | Loss: 0.00141719
Iteration 4/25 | Loss: 0.00140918
Iteration 5/25 | Loss: 0.00140748
Iteration 6/25 | Loss: 0.00140727
Iteration 7/25 | Loss: 0.00140727
Iteration 8/25 | Loss: 0.00140727
Iteration 9/25 | Loss: 0.00140727
Iteration 10/25 | Loss: 0.00140727
Iteration 11/25 | Loss: 0.00140727
Iteration 12/25 | Loss: 0.00140727
Iteration 13/25 | Loss: 0.00140727
Iteration 14/25 | Loss: 0.00140727
Iteration 15/25 | Loss: 0.00140727
Iteration 16/25 | Loss: 0.00140727
Iteration 17/25 | Loss: 0.00140727
Iteration 18/25 | Loss: 0.00140727
Iteration 19/25 | Loss: 0.00140727
Iteration 20/25 | Loss: 0.00140727
Iteration 21/25 | Loss: 0.00140727
Iteration 22/25 | Loss: 0.00140727
Iteration 23/25 | Loss: 0.00140727
Iteration 24/25 | Loss: 0.00140727
Iteration 25/25 | Loss: 0.00140727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08999252
Iteration 2/25 | Loss: 0.00115478
Iteration 3/25 | Loss: 0.00115477
Iteration 4/25 | Loss: 0.00115477
Iteration 5/25 | Loss: 0.00115477
Iteration 6/25 | Loss: 0.00115477
Iteration 7/25 | Loss: 0.00115477
Iteration 8/25 | Loss: 0.00115477
Iteration 9/25 | Loss: 0.00115477
Iteration 10/25 | Loss: 0.00115477
Iteration 11/25 | Loss: 0.00115477
Iteration 12/25 | Loss: 0.00115477
Iteration 13/25 | Loss: 0.00115477
Iteration 14/25 | Loss: 0.00115477
Iteration 15/25 | Loss: 0.00115477
Iteration 16/25 | Loss: 0.00115477
Iteration 17/25 | Loss: 0.00115477
Iteration 18/25 | Loss: 0.00115477
Iteration 19/25 | Loss: 0.00115477
Iteration 20/25 | Loss: 0.00115477
Iteration 21/25 | Loss: 0.00115477
Iteration 22/25 | Loss: 0.00115477
Iteration 23/25 | Loss: 0.00115477
Iteration 24/25 | Loss: 0.00115477
Iteration 25/25 | Loss: 0.00115477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115477
Iteration 2/1000 | Loss: 0.00004217
Iteration 3/1000 | Loss: 0.00002674
Iteration 4/1000 | Loss: 0.00002264
Iteration 5/1000 | Loss: 0.00002140
Iteration 6/1000 | Loss: 0.00002066
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001990
Iteration 9/1000 | Loss: 0.00001976
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001943
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001942
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001926
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001914
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001907
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001905
Iteration 24/1000 | Loss: 0.00001905
Iteration 25/1000 | Loss: 0.00001905
Iteration 26/1000 | Loss: 0.00001904
Iteration 27/1000 | Loss: 0.00001904
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001903
Iteration 30/1000 | Loss: 0.00001903
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001902
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001902
Iteration 35/1000 | Loss: 0.00001902
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001901
Iteration 38/1000 | Loss: 0.00001901
Iteration 39/1000 | Loss: 0.00001900
Iteration 40/1000 | Loss: 0.00001900
Iteration 41/1000 | Loss: 0.00001899
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001893
Iteration 53/1000 | Loss: 0.00001893
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001892
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001891
Iteration 62/1000 | Loss: 0.00001891
Iteration 63/1000 | Loss: 0.00001891
Iteration 64/1000 | Loss: 0.00001891
Iteration 65/1000 | Loss: 0.00001891
Iteration 66/1000 | Loss: 0.00001891
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00001890
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001889
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001889
Iteration 79/1000 | Loss: 0.00001889
Iteration 80/1000 | Loss: 0.00001889
Iteration 81/1000 | Loss: 0.00001889
Iteration 82/1000 | Loss: 0.00001889
Iteration 83/1000 | Loss: 0.00001888
Iteration 84/1000 | Loss: 0.00001888
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001882
Iteration 111/1000 | Loss: 0.00001882
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001882
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001882
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001882
Iteration 123/1000 | Loss: 0.00001882
Iteration 124/1000 | Loss: 0.00001882
Iteration 125/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.8823457139660604e-05, 1.8823457139660604e-05, 1.8823457139660604e-05, 1.8823457139660604e-05, 1.8823457139660604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8823457139660604e-05

Optimization complete. Final v2v error: 3.7793893814086914 mm

Highest mean error: 4.078283309936523 mm for frame 107

Lowest mean error: 3.531353235244751 mm for frame 46

Saving results

Total time: 78.82484936714172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056567
Iteration 2/25 | Loss: 0.00174806
Iteration 3/25 | Loss: 0.00143618
Iteration 4/25 | Loss: 0.00139849
Iteration 5/25 | Loss: 0.00139327
Iteration 6/25 | Loss: 0.00138923
Iteration 7/25 | Loss: 0.00138293
Iteration 8/25 | Loss: 0.00138314
Iteration 9/25 | Loss: 0.00138100
Iteration 10/25 | Loss: 0.00137991
Iteration 11/25 | Loss: 0.00137892
Iteration 12/25 | Loss: 0.00137832
Iteration 13/25 | Loss: 0.00137815
Iteration 14/25 | Loss: 0.00137805
Iteration 15/25 | Loss: 0.00137804
Iteration 16/25 | Loss: 0.00137804
Iteration 17/25 | Loss: 0.00137804
Iteration 18/25 | Loss: 0.00137804
Iteration 19/25 | Loss: 0.00137804
Iteration 20/25 | Loss: 0.00137804
Iteration 21/25 | Loss: 0.00137804
Iteration 22/25 | Loss: 0.00137804
Iteration 23/25 | Loss: 0.00137803
Iteration 24/25 | Loss: 0.00137803
Iteration 25/25 | Loss: 0.00137803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.73016119
Iteration 2/25 | Loss: 0.00237265
Iteration 3/25 | Loss: 0.00237265
Iteration 4/25 | Loss: 0.00237265
Iteration 5/25 | Loss: 0.00237265
Iteration 6/25 | Loss: 0.00237265
Iteration 7/25 | Loss: 0.00237265
Iteration 8/25 | Loss: 0.00237265
Iteration 9/25 | Loss: 0.00237265
Iteration 10/25 | Loss: 0.00237265
Iteration 11/25 | Loss: 0.00237265
Iteration 12/25 | Loss: 0.00237265
Iteration 13/25 | Loss: 0.00237265
Iteration 14/25 | Loss: 0.00237265
Iteration 15/25 | Loss: 0.00237265
Iteration 16/25 | Loss: 0.00237265
Iteration 17/25 | Loss: 0.00237265
Iteration 18/25 | Loss: 0.00237265
Iteration 19/25 | Loss: 0.00237265
Iteration 20/25 | Loss: 0.00237265
Iteration 21/25 | Loss: 0.00237265
Iteration 22/25 | Loss: 0.00237265
Iteration 23/25 | Loss: 0.00237265
Iteration 24/25 | Loss: 0.00237265
Iteration 25/25 | Loss: 0.00237265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237265
Iteration 2/1000 | Loss: 0.00005235
Iteration 3/1000 | Loss: 0.00003177
Iteration 4/1000 | Loss: 0.00002658
Iteration 5/1000 | Loss: 0.00002361
Iteration 6/1000 | Loss: 0.00002255
Iteration 7/1000 | Loss: 0.00002165
Iteration 8/1000 | Loss: 0.00002073
Iteration 9/1000 | Loss: 0.00002026
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001926
Iteration 15/1000 | Loss: 0.00001921
Iteration 16/1000 | Loss: 0.00001916
Iteration 17/1000 | Loss: 0.00001909
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001903
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001901
Iteration 25/1000 | Loss: 0.00001900
Iteration 26/1000 | Loss: 0.00001899
Iteration 27/1000 | Loss: 0.00001899
Iteration 28/1000 | Loss: 0.00001898
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001897
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001888
Iteration 38/1000 | Loss: 0.00001888
Iteration 39/1000 | Loss: 0.00001887
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001886
Iteration 42/1000 | Loss: 0.00001886
Iteration 43/1000 | Loss: 0.00001885
Iteration 44/1000 | Loss: 0.00001885
Iteration 45/1000 | Loss: 0.00001884
Iteration 46/1000 | Loss: 0.00001884
Iteration 47/1000 | Loss: 0.00001884
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00001883
Iteration 51/1000 | Loss: 0.00001882
Iteration 52/1000 | Loss: 0.00001882
Iteration 53/1000 | Loss: 0.00001882
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001880
Iteration 61/1000 | Loss: 0.00001880
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001880
Iteration 65/1000 | Loss: 0.00001880
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001879
Iteration 69/1000 | Loss: 0.00001879
Iteration 70/1000 | Loss: 0.00001879
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001876
Iteration 77/1000 | Loss: 0.00001876
Iteration 78/1000 | Loss: 0.00001876
Iteration 79/1000 | Loss: 0.00001875
Iteration 80/1000 | Loss: 0.00001875
Iteration 81/1000 | Loss: 0.00001875
Iteration 82/1000 | Loss: 0.00001874
Iteration 83/1000 | Loss: 0.00001874
Iteration 84/1000 | Loss: 0.00001874
Iteration 85/1000 | Loss: 0.00001874
Iteration 86/1000 | Loss: 0.00001874
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001873
Iteration 92/1000 | Loss: 0.00001873
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001870
Iteration 103/1000 | Loss: 0.00001870
Iteration 104/1000 | Loss: 0.00001870
Iteration 105/1000 | Loss: 0.00001870
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001869
Iteration 108/1000 | Loss: 0.00001869
Iteration 109/1000 | Loss: 0.00001869
Iteration 110/1000 | Loss: 0.00001869
Iteration 111/1000 | Loss: 0.00001869
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001867
Iteration 118/1000 | Loss: 0.00001867
Iteration 119/1000 | Loss: 0.00001867
Iteration 120/1000 | Loss: 0.00001867
Iteration 121/1000 | Loss: 0.00001866
Iteration 122/1000 | Loss: 0.00001866
Iteration 123/1000 | Loss: 0.00001866
Iteration 124/1000 | Loss: 0.00001866
Iteration 125/1000 | Loss: 0.00001866
Iteration 126/1000 | Loss: 0.00001866
Iteration 127/1000 | Loss: 0.00001866
Iteration 128/1000 | Loss: 0.00001866
Iteration 129/1000 | Loss: 0.00001866
Iteration 130/1000 | Loss: 0.00001866
Iteration 131/1000 | Loss: 0.00001865
Iteration 132/1000 | Loss: 0.00001865
Iteration 133/1000 | Loss: 0.00001865
Iteration 134/1000 | Loss: 0.00001865
Iteration 135/1000 | Loss: 0.00001865
Iteration 136/1000 | Loss: 0.00001865
Iteration 137/1000 | Loss: 0.00001865
Iteration 138/1000 | Loss: 0.00001865
Iteration 139/1000 | Loss: 0.00001865
Iteration 140/1000 | Loss: 0.00001865
Iteration 141/1000 | Loss: 0.00001865
Iteration 142/1000 | Loss: 0.00001865
Iteration 143/1000 | Loss: 0.00001865
Iteration 144/1000 | Loss: 0.00001865
Iteration 145/1000 | Loss: 0.00001864
Iteration 146/1000 | Loss: 0.00001864
Iteration 147/1000 | Loss: 0.00001864
Iteration 148/1000 | Loss: 0.00001864
Iteration 149/1000 | Loss: 0.00001864
Iteration 150/1000 | Loss: 0.00001864
Iteration 151/1000 | Loss: 0.00001864
Iteration 152/1000 | Loss: 0.00001864
Iteration 153/1000 | Loss: 0.00001863
Iteration 154/1000 | Loss: 0.00001863
Iteration 155/1000 | Loss: 0.00001863
Iteration 156/1000 | Loss: 0.00001863
Iteration 157/1000 | Loss: 0.00001863
Iteration 158/1000 | Loss: 0.00001863
Iteration 159/1000 | Loss: 0.00001863
Iteration 160/1000 | Loss: 0.00001863
Iteration 161/1000 | Loss: 0.00001863
Iteration 162/1000 | Loss: 0.00001863
Iteration 163/1000 | Loss: 0.00001863
Iteration 164/1000 | Loss: 0.00001863
Iteration 165/1000 | Loss: 0.00001863
Iteration 166/1000 | Loss: 0.00001862
Iteration 167/1000 | Loss: 0.00001862
Iteration 168/1000 | Loss: 0.00001862
Iteration 169/1000 | Loss: 0.00001862
Iteration 170/1000 | Loss: 0.00001862
Iteration 171/1000 | Loss: 0.00001862
Iteration 172/1000 | Loss: 0.00001862
Iteration 173/1000 | Loss: 0.00001862
Iteration 174/1000 | Loss: 0.00001862
Iteration 175/1000 | Loss: 0.00001862
Iteration 176/1000 | Loss: 0.00001862
Iteration 177/1000 | Loss: 0.00001862
Iteration 178/1000 | Loss: 0.00001862
Iteration 179/1000 | Loss: 0.00001861
Iteration 180/1000 | Loss: 0.00001861
Iteration 181/1000 | Loss: 0.00001861
Iteration 182/1000 | Loss: 0.00001861
Iteration 183/1000 | Loss: 0.00001861
Iteration 184/1000 | Loss: 0.00001861
Iteration 185/1000 | Loss: 0.00001861
Iteration 186/1000 | Loss: 0.00001861
Iteration 187/1000 | Loss: 0.00001861
Iteration 188/1000 | Loss: 0.00001861
Iteration 189/1000 | Loss: 0.00001861
Iteration 190/1000 | Loss: 0.00001861
Iteration 191/1000 | Loss: 0.00001861
Iteration 192/1000 | Loss: 0.00001861
Iteration 193/1000 | Loss: 0.00001861
Iteration 194/1000 | Loss: 0.00001861
Iteration 195/1000 | Loss: 0.00001861
Iteration 196/1000 | Loss: 0.00001861
Iteration 197/1000 | Loss: 0.00001861
Iteration 198/1000 | Loss: 0.00001861
Iteration 199/1000 | Loss: 0.00001861
Iteration 200/1000 | Loss: 0.00001861
Iteration 201/1000 | Loss: 0.00001861
Iteration 202/1000 | Loss: 0.00001861
Iteration 203/1000 | Loss: 0.00001861
Iteration 204/1000 | Loss: 0.00001861
Iteration 205/1000 | Loss: 0.00001861
Iteration 206/1000 | Loss: 0.00001861
Iteration 207/1000 | Loss: 0.00001861
Iteration 208/1000 | Loss: 0.00001861
Iteration 209/1000 | Loss: 0.00001861
Iteration 210/1000 | Loss: 0.00001861
Iteration 211/1000 | Loss: 0.00001861
Iteration 212/1000 | Loss: 0.00001861
Iteration 213/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.8606051526148804e-05, 1.8606051526148804e-05, 1.8606051526148804e-05, 1.8606051526148804e-05, 1.8606051526148804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8606051526148804e-05

Optimization complete. Final v2v error: 3.7043988704681396 mm

Highest mean error: 8.801477432250977 mm for frame 132

Lowest mean error: 3.410177230834961 mm for frame 57

Saving results

Total time: 176.9517183303833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922476
Iteration 2/25 | Loss: 0.00259140
Iteration 3/25 | Loss: 0.00204797
Iteration 4/25 | Loss: 0.00196623
Iteration 5/25 | Loss: 0.00191201
Iteration 6/25 | Loss: 0.00182903
Iteration 7/25 | Loss: 0.00174929
Iteration 8/25 | Loss: 0.00167857
Iteration 9/25 | Loss: 0.00162151
Iteration 10/25 | Loss: 0.00159133
Iteration 11/25 | Loss: 0.00157737
Iteration 12/25 | Loss: 0.00156161
Iteration 13/25 | Loss: 0.00155322
Iteration 14/25 | Loss: 0.00153952
Iteration 15/25 | Loss: 0.00152694
Iteration 16/25 | Loss: 0.00151852
Iteration 17/25 | Loss: 0.00151762
Iteration 18/25 | Loss: 0.00151492
Iteration 19/25 | Loss: 0.00150701
Iteration 20/25 | Loss: 0.00150614
Iteration 21/25 | Loss: 0.00150489
Iteration 22/25 | Loss: 0.00150584
Iteration 23/25 | Loss: 0.00150555
Iteration 24/25 | Loss: 0.00150683
Iteration 25/25 | Loss: 0.00150494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16260576
Iteration 2/25 | Loss: 0.00161769
Iteration 3/25 | Loss: 0.00161767
Iteration 4/25 | Loss: 0.00161767
Iteration 5/25 | Loss: 0.00161767
Iteration 6/25 | Loss: 0.00161767
Iteration 7/25 | Loss: 0.00161767
Iteration 8/25 | Loss: 0.00161767
Iteration 9/25 | Loss: 0.00161767
Iteration 10/25 | Loss: 0.00161767
Iteration 11/25 | Loss: 0.00161767
Iteration 12/25 | Loss: 0.00161767
Iteration 13/25 | Loss: 0.00161767
Iteration 14/25 | Loss: 0.00161767
Iteration 15/25 | Loss: 0.00161767
Iteration 16/25 | Loss: 0.00161767
Iteration 17/25 | Loss: 0.00161767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016176699427887797, 0.0016176699427887797, 0.0016176699427887797, 0.0016176699427887797, 0.0016176699427887797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016176699427887797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161767
Iteration 2/1000 | Loss: 0.00010122
Iteration 3/1000 | Loss: 0.00010818
Iteration 4/1000 | Loss: 0.00011682
Iteration 5/1000 | Loss: 0.00010872
Iteration 6/1000 | Loss: 0.00013276
Iteration 7/1000 | Loss: 0.00009343
Iteration 8/1000 | Loss: 0.00014691
Iteration 9/1000 | Loss: 0.00013336
Iteration 10/1000 | Loss: 0.00010968
Iteration 11/1000 | Loss: 0.00009922
Iteration 12/1000 | Loss: 0.00010218
Iteration 13/1000 | Loss: 0.00013413
Iteration 14/1000 | Loss: 0.00010856
Iteration 15/1000 | Loss: 0.00011583
Iteration 16/1000 | Loss: 0.00010393
Iteration 17/1000 | Loss: 0.00007197
Iteration 18/1000 | Loss: 0.00007333
Iteration 19/1000 | Loss: 0.00008548
Iteration 20/1000 | Loss: 0.00009472
Iteration 21/1000 | Loss: 0.00008159
Iteration 22/1000 | Loss: 0.00010436
Iteration 23/1000 | Loss: 0.00009750
Iteration 24/1000 | Loss: 0.00009735
Iteration 25/1000 | Loss: 0.00008714
Iteration 26/1000 | Loss: 0.00007540
Iteration 27/1000 | Loss: 0.00006302
Iteration 28/1000 | Loss: 0.00009470
Iteration 29/1000 | Loss: 0.00009476
Iteration 30/1000 | Loss: 0.00009894
Iteration 31/1000 | Loss: 0.00009349
Iteration 32/1000 | Loss: 0.00009760
Iteration 33/1000 | Loss: 0.00009976
Iteration 34/1000 | Loss: 0.00005488
Iteration 35/1000 | Loss: 0.00008832
Iteration 36/1000 | Loss: 0.00004417
Iteration 37/1000 | Loss: 0.00005565
Iteration 38/1000 | Loss: 0.00007492
Iteration 39/1000 | Loss: 0.00007812
Iteration 40/1000 | Loss: 0.00007742
Iteration 41/1000 | Loss: 0.00006531
Iteration 42/1000 | Loss: 0.00008445
Iteration 43/1000 | Loss: 0.00008290
Iteration 44/1000 | Loss: 0.00008425
Iteration 45/1000 | Loss: 0.00007986
Iteration 46/1000 | Loss: 0.00009023
Iteration 47/1000 | Loss: 0.00013839
Iteration 48/1000 | Loss: 0.00008792
Iteration 49/1000 | Loss: 0.00010440
Iteration 50/1000 | Loss: 0.00009206
Iteration 51/1000 | Loss: 0.00008311
Iteration 52/1000 | Loss: 0.00008558
Iteration 53/1000 | Loss: 0.00008474
Iteration 54/1000 | Loss: 0.00008502
Iteration 55/1000 | Loss: 0.00009203
Iteration 56/1000 | Loss: 0.00010156
Iteration 57/1000 | Loss: 0.00009682
Iteration 58/1000 | Loss: 0.00009763
Iteration 59/1000 | Loss: 0.00007884
Iteration 60/1000 | Loss: 0.00008220
Iteration 61/1000 | Loss: 0.00009066
Iteration 62/1000 | Loss: 0.00005340
Iteration 63/1000 | Loss: 0.00006917
Iteration 64/1000 | Loss: 0.00006668
Iteration 65/1000 | Loss: 0.00006843
Iteration 66/1000 | Loss: 0.00006174
Iteration 67/1000 | Loss: 0.00006779
Iteration 68/1000 | Loss: 0.00008439
Iteration 69/1000 | Loss: 0.00006957
Iteration 70/1000 | Loss: 0.00007700
Iteration 71/1000 | Loss: 0.00006931
Iteration 72/1000 | Loss: 0.00004166
Iteration 73/1000 | Loss: 0.00004277
Iteration 74/1000 | Loss: 0.00004306
Iteration 75/1000 | Loss: 0.00006320
Iteration 76/1000 | Loss: 0.00005213
Iteration 77/1000 | Loss: 0.00005671
Iteration 78/1000 | Loss: 0.00005292
Iteration 79/1000 | Loss: 0.00003847
Iteration 80/1000 | Loss: 0.00003883
Iteration 81/1000 | Loss: 0.00005043
Iteration 82/1000 | Loss: 0.00005130
Iteration 83/1000 | Loss: 0.00004256
Iteration 84/1000 | Loss: 0.00004324
Iteration 85/1000 | Loss: 0.00006320
Iteration 86/1000 | Loss: 0.00005856
Iteration 87/1000 | Loss: 0.00007043
Iteration 88/1000 | Loss: 0.00005872
Iteration 89/1000 | Loss: 0.00004373
Iteration 90/1000 | Loss: 0.00003041
Iteration 91/1000 | Loss: 0.00002718
Iteration 92/1000 | Loss: 0.00002503
Iteration 93/1000 | Loss: 0.00002380
Iteration 94/1000 | Loss: 0.00002301
Iteration 95/1000 | Loss: 0.00002243
Iteration 96/1000 | Loss: 0.00002210
Iteration 97/1000 | Loss: 0.00002180
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002162
Iteration 100/1000 | Loss: 0.00002162
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002146
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002141
Iteration 111/1000 | Loss: 0.00002141
Iteration 112/1000 | Loss: 0.00002141
Iteration 113/1000 | Loss: 0.00002141
Iteration 114/1000 | Loss: 0.00002140
Iteration 115/1000 | Loss: 0.00002140
Iteration 116/1000 | Loss: 0.00002140
Iteration 117/1000 | Loss: 0.00002140
Iteration 118/1000 | Loss: 0.00002140
Iteration 119/1000 | Loss: 0.00002139
Iteration 120/1000 | Loss: 0.00002139
Iteration 121/1000 | Loss: 0.00002139
Iteration 122/1000 | Loss: 0.00002139
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002138
Iteration 126/1000 | Loss: 0.00002138
Iteration 127/1000 | Loss: 0.00002138
Iteration 128/1000 | Loss: 0.00002138
Iteration 129/1000 | Loss: 0.00002138
Iteration 130/1000 | Loss: 0.00002138
Iteration 131/1000 | Loss: 0.00002138
Iteration 132/1000 | Loss: 0.00002137
Iteration 133/1000 | Loss: 0.00002137
Iteration 134/1000 | Loss: 0.00002137
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002136
Iteration 138/1000 | Loss: 0.00002136
Iteration 139/1000 | Loss: 0.00002135
Iteration 140/1000 | Loss: 0.00002135
Iteration 141/1000 | Loss: 0.00002135
Iteration 142/1000 | Loss: 0.00002135
Iteration 143/1000 | Loss: 0.00002135
Iteration 144/1000 | Loss: 0.00002134
Iteration 145/1000 | Loss: 0.00002134
Iteration 146/1000 | Loss: 0.00002134
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002134
Iteration 149/1000 | Loss: 0.00002134
Iteration 150/1000 | Loss: 0.00002134
Iteration 151/1000 | Loss: 0.00002134
Iteration 152/1000 | Loss: 0.00002134
Iteration 153/1000 | Loss: 0.00002134
Iteration 154/1000 | Loss: 0.00002134
Iteration 155/1000 | Loss: 0.00002134
Iteration 156/1000 | Loss: 0.00002133
Iteration 157/1000 | Loss: 0.00002133
Iteration 158/1000 | Loss: 0.00002133
Iteration 159/1000 | Loss: 0.00002132
Iteration 160/1000 | Loss: 0.00002132
Iteration 161/1000 | Loss: 0.00002132
Iteration 162/1000 | Loss: 0.00002132
Iteration 163/1000 | Loss: 0.00002132
Iteration 164/1000 | Loss: 0.00002132
Iteration 165/1000 | Loss: 0.00002132
Iteration 166/1000 | Loss: 0.00002132
Iteration 167/1000 | Loss: 0.00002132
Iteration 168/1000 | Loss: 0.00002132
Iteration 169/1000 | Loss: 0.00002132
Iteration 170/1000 | Loss: 0.00002132
Iteration 171/1000 | Loss: 0.00002132
Iteration 172/1000 | Loss: 0.00002132
Iteration 173/1000 | Loss: 0.00002132
Iteration 174/1000 | Loss: 0.00002132
Iteration 175/1000 | Loss: 0.00002132
Iteration 176/1000 | Loss: 0.00002132
Iteration 177/1000 | Loss: 0.00002132
Iteration 178/1000 | Loss: 0.00002132
Iteration 179/1000 | Loss: 0.00002131
Iteration 180/1000 | Loss: 0.00002131
Iteration 181/1000 | Loss: 0.00002131
Iteration 182/1000 | Loss: 0.00002131
Iteration 183/1000 | Loss: 0.00002131
Iteration 184/1000 | Loss: 0.00002131
Iteration 185/1000 | Loss: 0.00002131
Iteration 186/1000 | Loss: 0.00002131
Iteration 187/1000 | Loss: 0.00002131
Iteration 188/1000 | Loss: 0.00002131
Iteration 189/1000 | Loss: 0.00002131
Iteration 190/1000 | Loss: 0.00002131
Iteration 191/1000 | Loss: 0.00002131
Iteration 192/1000 | Loss: 0.00002131
Iteration 193/1000 | Loss: 0.00002131
Iteration 194/1000 | Loss: 0.00002130
Iteration 195/1000 | Loss: 0.00002130
Iteration 196/1000 | Loss: 0.00002130
Iteration 197/1000 | Loss: 0.00002130
Iteration 198/1000 | Loss: 0.00002130
Iteration 199/1000 | Loss: 0.00002130
Iteration 200/1000 | Loss: 0.00002130
Iteration 201/1000 | Loss: 0.00002130
Iteration 202/1000 | Loss: 0.00002130
Iteration 203/1000 | Loss: 0.00002130
Iteration 204/1000 | Loss: 0.00002130
Iteration 205/1000 | Loss: 0.00002130
Iteration 206/1000 | Loss: 0.00002130
Iteration 207/1000 | Loss: 0.00002130
Iteration 208/1000 | Loss: 0.00002130
Iteration 209/1000 | Loss: 0.00002130
Iteration 210/1000 | Loss: 0.00002129
Iteration 211/1000 | Loss: 0.00002129
Iteration 212/1000 | Loss: 0.00002129
Iteration 213/1000 | Loss: 0.00002129
Iteration 214/1000 | Loss: 0.00002129
Iteration 215/1000 | Loss: 0.00002129
Iteration 216/1000 | Loss: 0.00002129
Iteration 217/1000 | Loss: 0.00002129
Iteration 218/1000 | Loss: 0.00002129
Iteration 219/1000 | Loss: 0.00002129
Iteration 220/1000 | Loss: 0.00002129
Iteration 221/1000 | Loss: 0.00002129
Iteration 222/1000 | Loss: 0.00002129
Iteration 223/1000 | Loss: 0.00002129
Iteration 224/1000 | Loss: 0.00002129
Iteration 225/1000 | Loss: 0.00002129
Iteration 226/1000 | Loss: 0.00002129
Iteration 227/1000 | Loss: 0.00002129
Iteration 228/1000 | Loss: 0.00002129
Iteration 229/1000 | Loss: 0.00002129
Iteration 230/1000 | Loss: 0.00002129
Iteration 231/1000 | Loss: 0.00002129
Iteration 232/1000 | Loss: 0.00002129
Iteration 233/1000 | Loss: 0.00002129
Iteration 234/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.1289040887495503e-05, 2.1289040887495503e-05, 2.1289040887495503e-05, 2.1289040887495503e-05, 2.1289040887495503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1289040887495503e-05

Optimization complete. Final v2v error: 4.047607898712158 mm

Highest mean error: 4.514321327209473 mm for frame 153

Lowest mean error: 3.8445615768432617 mm for frame 43

Saving results

Total time: 582.4866766929626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122406
Iteration 2/25 | Loss: 0.00269767
Iteration 3/25 | Loss: 0.00188591
Iteration 4/25 | Loss: 0.00188789
Iteration 5/25 | Loss: 0.00186876
Iteration 6/25 | Loss: 0.00165974
Iteration 7/25 | Loss: 0.00157778
Iteration 8/25 | Loss: 0.00155626
Iteration 9/25 | Loss: 0.00156248
Iteration 10/25 | Loss: 0.00154665
Iteration 11/25 | Loss: 0.00150426
Iteration 12/25 | Loss: 0.00145082
Iteration 13/25 | Loss: 0.00142516
Iteration 14/25 | Loss: 0.00142723
Iteration 15/25 | Loss: 0.00142216
Iteration 16/25 | Loss: 0.00140997
Iteration 17/25 | Loss: 0.00141263
Iteration 18/25 | Loss: 0.00140580
Iteration 19/25 | Loss: 0.00140723
Iteration 20/25 | Loss: 0.00140386
Iteration 21/25 | Loss: 0.00141074
Iteration 22/25 | Loss: 0.00141080
Iteration 23/25 | Loss: 0.00140881
Iteration 24/25 | Loss: 0.00141037
Iteration 25/25 | Loss: 0.00140946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26550019
Iteration 2/25 | Loss: 0.00262902
Iteration 3/25 | Loss: 0.00262902
Iteration 4/25 | Loss: 0.00262901
Iteration 5/25 | Loss: 0.00262901
Iteration 6/25 | Loss: 0.00262901
Iteration 7/25 | Loss: 0.00262901
Iteration 8/25 | Loss: 0.00262901
Iteration 9/25 | Loss: 0.00262901
Iteration 10/25 | Loss: 0.00262901
Iteration 11/25 | Loss: 0.00262901
Iteration 12/25 | Loss: 0.00262901
Iteration 13/25 | Loss: 0.00262901
Iteration 14/25 | Loss: 0.00262901
Iteration 15/25 | Loss: 0.00262901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002629013964906335, 0.002629013964906335, 0.002629013964906335, 0.002629013964906335, 0.002629013964906335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002629013964906335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262901
Iteration 2/1000 | Loss: 0.00048300
Iteration 3/1000 | Loss: 0.00025171
Iteration 4/1000 | Loss: 0.00026095
Iteration 5/1000 | Loss: 0.00020715
Iteration 6/1000 | Loss: 0.00024718
Iteration 7/1000 | Loss: 0.00036805
Iteration 8/1000 | Loss: 0.00033630
Iteration 9/1000 | Loss: 0.00027391
Iteration 10/1000 | Loss: 0.00020873
Iteration 11/1000 | Loss: 0.00018106
Iteration 12/1000 | Loss: 0.00020704
Iteration 13/1000 | Loss: 0.00019832
Iteration 14/1000 | Loss: 0.00029814
Iteration 15/1000 | Loss: 0.00019933
Iteration 16/1000 | Loss: 0.00017675
Iteration 17/1000 | Loss: 0.00008041
Iteration 18/1000 | Loss: 0.00014227
Iteration 19/1000 | Loss: 0.00014513
Iteration 20/1000 | Loss: 0.00022451
Iteration 21/1000 | Loss: 0.00025724
Iteration 22/1000 | Loss: 0.00009234
Iteration 23/1000 | Loss: 0.00027916
Iteration 24/1000 | Loss: 0.00021408
Iteration 25/1000 | Loss: 0.00029067
Iteration 26/1000 | Loss: 0.00021583
Iteration 27/1000 | Loss: 0.00010207
Iteration 28/1000 | Loss: 0.00016688
Iteration 29/1000 | Loss: 0.00024918
Iteration 30/1000 | Loss: 0.00020455
Iteration 31/1000 | Loss: 0.00019995
Iteration 32/1000 | Loss: 0.00013624
Iteration 33/1000 | Loss: 0.00033199
Iteration 34/1000 | Loss: 0.00016964
Iteration 35/1000 | Loss: 0.00021431
Iteration 36/1000 | Loss: 0.00030339
Iteration 37/1000 | Loss: 0.00031642
Iteration 38/1000 | Loss: 0.00023227
Iteration 39/1000 | Loss: 0.00032745
Iteration 40/1000 | Loss: 0.00017590
Iteration 41/1000 | Loss: 0.00021878
Iteration 42/1000 | Loss: 0.00024253
Iteration 43/1000 | Loss: 0.00013408
Iteration 44/1000 | Loss: 0.00007491
Iteration 45/1000 | Loss: 0.00023748
Iteration 46/1000 | Loss: 0.00015415
Iteration 47/1000 | Loss: 0.00016978
Iteration 48/1000 | Loss: 0.00047631
Iteration 49/1000 | Loss: 0.00069276
Iteration 50/1000 | Loss: 0.00095762
Iteration 51/1000 | Loss: 0.00071127
Iteration 52/1000 | Loss: 0.00041971
Iteration 53/1000 | Loss: 0.00031215
Iteration 54/1000 | Loss: 0.00007811
Iteration 55/1000 | Loss: 0.00008301
Iteration 56/1000 | Loss: 0.00020919
Iteration 57/1000 | Loss: 0.00014988
Iteration 58/1000 | Loss: 0.00010664
Iteration 59/1000 | Loss: 0.00004171
Iteration 60/1000 | Loss: 0.00004095
Iteration 61/1000 | Loss: 0.00005125
Iteration 62/1000 | Loss: 0.00004143
Iteration 63/1000 | Loss: 0.00005171
Iteration 64/1000 | Loss: 0.00005348
Iteration 65/1000 | Loss: 0.00009865
Iteration 66/1000 | Loss: 0.00004731
Iteration 67/1000 | Loss: 0.00007933
Iteration 68/1000 | Loss: 0.00010937
Iteration 69/1000 | Loss: 0.00009018
Iteration 70/1000 | Loss: 0.00007935
Iteration 71/1000 | Loss: 0.00008971
Iteration 72/1000 | Loss: 0.00004807
Iteration 73/1000 | Loss: 0.00016075
Iteration 74/1000 | Loss: 0.00020098
Iteration 75/1000 | Loss: 0.00075234
Iteration 76/1000 | Loss: 0.00032646
Iteration 77/1000 | Loss: 0.00018090
Iteration 78/1000 | Loss: 0.00016169
Iteration 79/1000 | Loss: 0.00054208
Iteration 80/1000 | Loss: 0.00006244
Iteration 81/1000 | Loss: 0.00029045
Iteration 82/1000 | Loss: 0.00022036
Iteration 83/1000 | Loss: 0.00004682
Iteration 84/1000 | Loss: 0.00003605
Iteration 85/1000 | Loss: 0.00003610
Iteration 86/1000 | Loss: 0.00004724
Iteration 87/1000 | Loss: 0.00003208
Iteration 88/1000 | Loss: 0.00003246
Iteration 89/1000 | Loss: 0.00004095
Iteration 90/1000 | Loss: 0.00004221
Iteration 91/1000 | Loss: 0.00004181
Iteration 92/1000 | Loss: 0.00004482
Iteration 93/1000 | Loss: 0.00003312
Iteration 94/1000 | Loss: 0.00003687
Iteration 95/1000 | Loss: 0.00003360
Iteration 96/1000 | Loss: 0.00003783
Iteration 97/1000 | Loss: 0.00003476
Iteration 98/1000 | Loss: 0.00003726
Iteration 99/1000 | Loss: 0.00004271
Iteration 100/1000 | Loss: 0.00004766
Iteration 101/1000 | Loss: 0.00003494
Iteration 102/1000 | Loss: 0.00003922
Iteration 103/1000 | Loss: 0.00005322
Iteration 104/1000 | Loss: 0.00005197
Iteration 105/1000 | Loss: 0.00003432
Iteration 106/1000 | Loss: 0.00004450
Iteration 107/1000 | Loss: 0.00004515
Iteration 108/1000 | Loss: 0.00004577
Iteration 109/1000 | Loss: 0.00004508
Iteration 110/1000 | Loss: 0.00004137
Iteration 111/1000 | Loss: 0.00003602
Iteration 112/1000 | Loss: 0.00003480
Iteration 113/1000 | Loss: 0.00004336
Iteration 114/1000 | Loss: 0.00004525
Iteration 115/1000 | Loss: 0.00004612
Iteration 116/1000 | Loss: 0.00004746
Iteration 117/1000 | Loss: 0.00004054
Iteration 118/1000 | Loss: 0.00003700
Iteration 119/1000 | Loss: 0.00005461
Iteration 120/1000 | Loss: 0.00004827
Iteration 121/1000 | Loss: 0.00005113
Iteration 122/1000 | Loss: 0.00002972
Iteration 123/1000 | Loss: 0.00003330
Iteration 124/1000 | Loss: 0.00004274
Iteration 125/1000 | Loss: 0.00003481
Iteration 126/1000 | Loss: 0.00004186
Iteration 127/1000 | Loss: 0.00003371
Iteration 128/1000 | Loss: 0.00004149
Iteration 129/1000 | Loss: 0.00003384
Iteration 130/1000 | Loss: 0.00004485
Iteration 131/1000 | Loss: 0.00004417
Iteration 132/1000 | Loss: 0.00004346
Iteration 133/1000 | Loss: 0.00004060
Iteration 134/1000 | Loss: 0.00004238
Iteration 135/1000 | Loss: 0.00004288
Iteration 136/1000 | Loss: 0.00004568
Iteration 137/1000 | Loss: 0.00004302
Iteration 138/1000 | Loss: 0.00004570
Iteration 139/1000 | Loss: 0.00003070
Iteration 140/1000 | Loss: 0.00004348
Iteration 141/1000 | Loss: 0.00004311
Iteration 142/1000 | Loss: 0.00004215
Iteration 143/1000 | Loss: 0.00004702
Iteration 144/1000 | Loss: 0.00004535
Iteration 145/1000 | Loss: 0.00004300
Iteration 146/1000 | Loss: 0.00004433
Iteration 147/1000 | Loss: 0.00004355
Iteration 148/1000 | Loss: 0.00004388
Iteration 149/1000 | Loss: 0.00004530
Iteration 150/1000 | Loss: 0.00004427
Iteration 151/1000 | Loss: 0.00004288
Iteration 152/1000 | Loss: 0.00004509
Iteration 153/1000 | Loss: 0.00004356
Iteration 154/1000 | Loss: 0.00004276
Iteration 155/1000 | Loss: 0.00004491
Iteration 156/1000 | Loss: 0.00003630
Iteration 157/1000 | Loss: 0.00004707
Iteration 158/1000 | Loss: 0.00004305
Iteration 159/1000 | Loss: 0.00004394
Iteration 160/1000 | Loss: 0.00004276
Iteration 161/1000 | Loss: 0.00004618
Iteration 162/1000 | Loss: 0.00004475
Iteration 163/1000 | Loss: 0.00003807
Iteration 164/1000 | Loss: 0.00004311
Iteration 165/1000 | Loss: 0.00004443
Iteration 166/1000 | Loss: 0.00004289
Iteration 167/1000 | Loss: 0.00004427
Iteration 168/1000 | Loss: 0.00004227
Iteration 169/1000 | Loss: 0.00004605
Iteration 170/1000 | Loss: 0.00004693
Iteration 171/1000 | Loss: 0.00004387
Iteration 172/1000 | Loss: 0.00004451
Iteration 173/1000 | Loss: 0.00003474
Iteration 174/1000 | Loss: 0.00006500
Iteration 175/1000 | Loss: 0.00004312
Iteration 176/1000 | Loss: 0.00004763
Iteration 177/1000 | Loss: 0.00004598
Iteration 178/1000 | Loss: 0.00003214
Iteration 179/1000 | Loss: 0.00004602
Iteration 180/1000 | Loss: 0.00004274
Iteration 181/1000 | Loss: 0.00004382
Iteration 182/1000 | Loss: 0.00004439
Iteration 183/1000 | Loss: 0.00004355
Iteration 184/1000 | Loss: 0.00004635
Iteration 185/1000 | Loss: 0.00004616
Iteration 186/1000 | Loss: 0.00004224
Iteration 187/1000 | Loss: 0.00004230
Iteration 188/1000 | Loss: 0.00004482
Iteration 189/1000 | Loss: 0.00004295
Iteration 190/1000 | Loss: 0.00004394
Iteration 191/1000 | Loss: 0.00004714
Iteration 192/1000 | Loss: 0.00004537
Iteration 193/1000 | Loss: 0.00004226
Iteration 194/1000 | Loss: 0.00003388
Iteration 195/1000 | Loss: 0.00004377
Iteration 196/1000 | Loss: 0.00004152
Iteration 197/1000 | Loss: 0.00004203
Iteration 198/1000 | Loss: 0.00004218
Iteration 199/1000 | Loss: 0.00004466
Iteration 200/1000 | Loss: 0.00004458
Iteration 201/1000 | Loss: 0.00004540
Iteration 202/1000 | Loss: 0.00004297
Iteration 203/1000 | Loss: 0.00004313
Iteration 204/1000 | Loss: 0.00004329
Iteration 205/1000 | Loss: 0.00004271
Iteration 206/1000 | Loss: 0.00004349
Iteration 207/1000 | Loss: 0.00004189
Iteration 208/1000 | Loss: 0.00004441
Iteration 209/1000 | Loss: 0.00004282
Iteration 210/1000 | Loss: 0.00004319
Iteration 211/1000 | Loss: 0.00004321
Iteration 212/1000 | Loss: 0.00004332
Iteration 213/1000 | Loss: 0.00004223
Iteration 214/1000 | Loss: 0.00004331
Iteration 215/1000 | Loss: 0.00004491
Iteration 216/1000 | Loss: 0.00004577
Iteration 217/1000 | Loss: 0.00004559
Iteration 218/1000 | Loss: 0.00004254
Iteration 219/1000 | Loss: 0.00004242
Iteration 220/1000 | Loss: 0.00004231
Iteration 221/1000 | Loss: 0.00004231
Iteration 222/1000 | Loss: 0.00004683
Iteration 223/1000 | Loss: 0.00004316
Iteration 224/1000 | Loss: 0.00004364
Iteration 225/1000 | Loss: 0.00004502
Iteration 226/1000 | Loss: 0.00004640
Iteration 227/1000 | Loss: 0.00004588
Iteration 228/1000 | Loss: 0.00004185
Iteration 229/1000 | Loss: 0.00004212
Iteration 230/1000 | Loss: 0.00002700
Iteration 231/1000 | Loss: 0.00003687
Iteration 232/1000 | Loss: 0.00004457
Iteration 233/1000 | Loss: 0.00004236
Iteration 234/1000 | Loss: 0.00004254
Iteration 235/1000 | Loss: 0.00004245
Iteration 236/1000 | Loss: 0.00004221
Iteration 237/1000 | Loss: 0.00004229
Iteration 238/1000 | Loss: 0.00004553
Iteration 239/1000 | Loss: 0.00004305
Iteration 240/1000 | Loss: 0.00004211
Iteration 241/1000 | Loss: 0.00004061
Iteration 242/1000 | Loss: 0.00004406
Iteration 243/1000 | Loss: 0.00004236
Iteration 244/1000 | Loss: 0.00004687
Iteration 245/1000 | Loss: 0.00004219
Iteration 246/1000 | Loss: 0.00004646
Iteration 247/1000 | Loss: 0.00004222
Iteration 248/1000 | Loss: 0.00004406
Iteration 249/1000 | Loss: 0.00004282
Iteration 250/1000 | Loss: 0.00004348
Iteration 251/1000 | Loss: 0.00004267
Iteration 252/1000 | Loss: 0.00004325
Iteration 253/1000 | Loss: 0.00004378
Iteration 254/1000 | Loss: 0.00004310
Iteration 255/1000 | Loss: 0.00004407
Iteration 256/1000 | Loss: 0.00004305
Iteration 257/1000 | Loss: 0.00004340
Iteration 258/1000 | Loss: 0.00004299
Iteration 259/1000 | Loss: 0.00004299
Iteration 260/1000 | Loss: 0.00004368
Iteration 261/1000 | Loss: 0.00004328
Iteration 262/1000 | Loss: 0.00004212
Iteration 263/1000 | Loss: 0.00004064
Iteration 264/1000 | Loss: 0.00004610
Iteration 265/1000 | Loss: 0.00004252
Iteration 266/1000 | Loss: 0.00004322
Iteration 267/1000 | Loss: 0.00004239
Iteration 268/1000 | Loss: 0.00004205
Iteration 269/1000 | Loss: 0.00004249
Iteration 270/1000 | Loss: 0.00004201
Iteration 271/1000 | Loss: 0.00004183
Iteration 272/1000 | Loss: 0.00004252
Iteration 273/1000 | Loss: 0.00004149
Iteration 274/1000 | Loss: 0.00004185
Iteration 275/1000 | Loss: 0.00004120
Iteration 276/1000 | Loss: 0.00004124
Iteration 277/1000 | Loss: 0.00004428
Iteration 278/1000 | Loss: 0.00004217
Iteration 279/1000 | Loss: 0.00004460
Iteration 280/1000 | Loss: 0.00004287
Iteration 281/1000 | Loss: 0.00004291
Iteration 282/1000 | Loss: 0.00004296
Iteration 283/1000 | Loss: 0.00004565
Iteration 284/1000 | Loss: 0.00004249
Iteration 285/1000 | Loss: 0.00003776
Iteration 286/1000 | Loss: 0.00003586
Iteration 287/1000 | Loss: 0.00004162
Iteration 288/1000 | Loss: 0.00004101
Iteration 289/1000 | Loss: 0.00004453
Iteration 290/1000 | Loss: 0.00004090
Iteration 291/1000 | Loss: 0.00004447
Iteration 292/1000 | Loss: 0.00004478
Iteration 293/1000 | Loss: 0.00004320
Iteration 294/1000 | Loss: 0.00004352
Iteration 295/1000 | Loss: 0.00004270
Iteration 296/1000 | Loss: 0.00004252
Iteration 297/1000 | Loss: 0.00004230
Iteration 298/1000 | Loss: 0.00004608
Iteration 299/1000 | Loss: 0.00004205
Iteration 300/1000 | Loss: 0.00004526
Iteration 301/1000 | Loss: 0.00004200
Iteration 302/1000 | Loss: 0.00004174
Iteration 303/1000 | Loss: 0.00004307
Iteration 304/1000 | Loss: 0.00004264
Iteration 305/1000 | Loss: 0.00004273
Iteration 306/1000 | Loss: 0.00004225
Iteration 307/1000 | Loss: 0.00004304
Iteration 308/1000 | Loss: 0.00004232
Iteration 309/1000 | Loss: 0.00004272
Iteration 310/1000 | Loss: 0.00003658
Iteration 311/1000 | Loss: 0.00004485
Iteration 312/1000 | Loss: 0.00004678
Iteration 313/1000 | Loss: 0.00004321
Iteration 314/1000 | Loss: 0.00004235
Iteration 315/1000 | Loss: 0.00004207
Iteration 316/1000 | Loss: 0.00004207
Iteration 317/1000 | Loss: 0.00004194
Iteration 318/1000 | Loss: 0.00004484
Iteration 319/1000 | Loss: 0.00004488
Iteration 320/1000 | Loss: 0.00004329
Iteration 321/1000 | Loss: 0.00004285
Iteration 322/1000 | Loss: 0.00004639
Iteration 323/1000 | Loss: 0.00004406
Iteration 324/1000 | Loss: 0.00004650
Iteration 325/1000 | Loss: 0.00004684
Iteration 326/1000 | Loss: 0.00004696
Iteration 327/1000 | Loss: 0.00004517
Iteration 328/1000 | Loss: 0.00004403
Iteration 329/1000 | Loss: 0.00004173
Iteration 330/1000 | Loss: 0.00004142
Iteration 331/1000 | Loss: 0.00004242
Iteration 332/1000 | Loss: 0.00004108
Iteration 333/1000 | Loss: 0.00004326
Iteration 334/1000 | Loss: 0.00004169
Iteration 335/1000 | Loss: 0.00004307
Iteration 336/1000 | Loss: 0.00004165
Iteration 337/1000 | Loss: 0.00004174
Iteration 338/1000 | Loss: 0.00004459
Iteration 339/1000 | Loss: 0.00004364
Iteration 340/1000 | Loss: 0.00004454
Iteration 341/1000 | Loss: 0.00004330
Iteration 342/1000 | Loss: 0.00004106
Iteration 343/1000 | Loss: 0.00004197
Iteration 344/1000 | Loss: 0.00004165
Iteration 345/1000 | Loss: 0.00004198
Iteration 346/1000 | Loss: 0.00004451
Iteration 347/1000 | Loss: 0.00004649
Iteration 348/1000 | Loss: 0.00004388
Iteration 349/1000 | Loss: 0.00004615
Iteration 350/1000 | Loss: 0.00004744
Iteration 351/1000 | Loss: 0.00004442
Iteration 352/1000 | Loss: 0.00004321
Iteration 353/1000 | Loss: 0.00004146
Iteration 354/1000 | Loss: 0.00004082
Iteration 355/1000 | Loss: 0.00003378
Iteration 356/1000 | Loss: 0.00004195
Iteration 357/1000 | Loss: 0.00003030
Iteration 358/1000 | Loss: 0.00004406
Iteration 359/1000 | Loss: 0.00004265
Iteration 360/1000 | Loss: 0.00004250
Iteration 361/1000 | Loss: 0.00004305
Iteration 362/1000 | Loss: 0.00004271
Iteration 363/1000 | Loss: 0.00004393
Iteration 364/1000 | Loss: 0.00004415
Iteration 365/1000 | Loss: 0.00004220
Iteration 366/1000 | Loss: 0.00004293
Iteration 367/1000 | Loss: 0.00004236
Iteration 368/1000 | Loss: 0.00004238
Iteration 369/1000 | Loss: 0.00004405
Iteration 370/1000 | Loss: 0.00004263
Iteration 371/1000 | Loss: 0.00003596
Iteration 372/1000 | Loss: 0.00004251
Iteration 373/1000 | Loss: 0.00004540
Iteration 374/1000 | Loss: 0.00004357
Iteration 375/1000 | Loss: 0.00004399
Iteration 376/1000 | Loss: 0.00004365
Iteration 377/1000 | Loss: 0.00004677
Iteration 378/1000 | Loss: 0.00004179
Iteration 379/1000 | Loss: 0.00004432
Iteration 380/1000 | Loss: 0.00004136
Iteration 381/1000 | Loss: 0.00004413
Iteration 382/1000 | Loss: 0.00004411
Iteration 383/1000 | Loss: 0.00004677
Iteration 384/1000 | Loss: 0.00004297
Iteration 385/1000 | Loss: 0.00004379
Iteration 386/1000 | Loss: 0.00004164
Iteration 387/1000 | Loss: 0.00004298
Iteration 388/1000 | Loss: 0.00004734
Iteration 389/1000 | Loss: 0.00002781
Iteration 390/1000 | Loss: 0.00004277
Iteration 391/1000 | Loss: 0.00004400
Iteration 392/1000 | Loss: 0.00004277
Iteration 393/1000 | Loss: 0.00004365
Iteration 394/1000 | Loss: 0.00004543
Iteration 395/1000 | Loss: 0.00004412
Iteration 396/1000 | Loss: 0.00004192
Iteration 397/1000 | Loss: 0.00004183
Iteration 398/1000 | Loss: 0.00004448
Iteration 399/1000 | Loss: 0.00004258
Iteration 400/1000 | Loss: 0.00004171
Iteration 401/1000 | Loss: 0.00004106
Iteration 402/1000 | Loss: 0.00004163
Iteration 403/1000 | Loss: 0.00004250
Iteration 404/1000 | Loss: 0.00004247
Iteration 405/1000 | Loss: 0.00004630
Iteration 406/1000 | Loss: 0.00004325
Iteration 407/1000 | Loss: 0.00004581
Iteration 408/1000 | Loss: 0.00004321
Iteration 409/1000 | Loss: 0.00004081
Iteration 410/1000 | Loss: 0.00004368
Iteration 411/1000 | Loss: 0.00004468
Iteration 412/1000 | Loss: 0.00004160
Iteration 413/1000 | Loss: 0.00004324
Iteration 414/1000 | Loss: 0.00004170
Iteration 415/1000 | Loss: 0.00004277
Iteration 416/1000 | Loss: 0.00004272
Iteration 417/1000 | Loss: 0.00004357
Iteration 418/1000 | Loss: 0.00004177
Iteration 419/1000 | Loss: 0.00004407
Iteration 420/1000 | Loss: 0.00004219
Iteration 421/1000 | Loss: 0.00004260
Iteration 422/1000 | Loss: 0.00004217
Iteration 423/1000 | Loss: 0.00004489
Iteration 424/1000 | Loss: 0.00004274
Iteration 425/1000 | Loss: 0.00003191
Iteration 426/1000 | Loss: 0.00005314
Iteration 427/1000 | Loss: 0.00004220
Iteration 428/1000 | Loss: 0.00004376
Iteration 429/1000 | Loss: 0.00004107
Iteration 430/1000 | Loss: 0.00004458
Iteration 431/1000 | Loss: 0.00004769
Iteration 432/1000 | Loss: 0.00005007
Iteration 433/1000 | Loss: 0.00004535
Iteration 434/1000 | Loss: 0.00004516
Iteration 435/1000 | Loss: 0.00004351
Iteration 436/1000 | Loss: 0.00004283
Iteration 437/1000 | Loss: 0.00004121
Iteration 438/1000 | Loss: 0.00004403
Iteration 439/1000 | Loss: 0.00004140
Iteration 440/1000 | Loss: 0.00004407
Iteration 441/1000 | Loss: 0.00004750
Iteration 442/1000 | Loss: 0.00004403
Iteration 443/1000 | Loss: 0.00004478
Iteration 444/1000 | Loss: 0.00004306
Iteration 445/1000 | Loss: 0.00004547
Iteration 446/1000 | Loss: 0.00004807
Iteration 447/1000 | Loss: 0.00004392
Iteration 448/1000 | Loss: 0.00003489
Iteration 449/1000 | Loss: 0.00003421
Iteration 450/1000 | Loss: 0.00003682
Iteration 451/1000 | Loss: 0.00004310
Iteration 452/1000 | Loss: 0.00004646
Iteration 453/1000 | Loss: 0.00004417
Iteration 454/1000 | Loss: 0.00004412
Iteration 455/1000 | Loss: 0.00004240
Iteration 456/1000 | Loss: 0.00004231
Iteration 457/1000 | Loss: 0.00004796
Iteration 458/1000 | Loss: 0.00004231
Iteration 459/1000 | Loss: 0.00004629
Iteration 460/1000 | Loss: 0.00004139
Iteration 461/1000 | Loss: 0.00003978
Iteration 462/1000 | Loss: 0.00004726
Iteration 463/1000 | Loss: 0.00004040
Iteration 464/1000 | Loss: 0.00004522
Iteration 465/1000 | Loss: 0.00005992
Iteration 466/1000 | Loss: 0.00002871
Iteration 467/1000 | Loss: 0.00002603
Iteration 468/1000 | Loss: 0.00002525
Iteration 469/1000 | Loss: 0.00002448
Iteration 470/1000 | Loss: 0.00002418
Iteration 471/1000 | Loss: 0.00002410
Iteration 472/1000 | Loss: 0.00002408
Iteration 473/1000 | Loss: 0.00002407
Iteration 474/1000 | Loss: 0.00002406
Iteration 475/1000 | Loss: 0.00002401
Iteration 476/1000 | Loss: 0.00002400
Iteration 477/1000 | Loss: 0.00002398
Iteration 478/1000 | Loss: 0.00002398
Iteration 479/1000 | Loss: 0.00002398
Iteration 480/1000 | Loss: 0.00002398
Iteration 481/1000 | Loss: 0.00002397
Iteration 482/1000 | Loss: 0.00002396
Iteration 483/1000 | Loss: 0.00002396
Iteration 484/1000 | Loss: 0.00002396
Iteration 485/1000 | Loss: 0.00002395
Iteration 486/1000 | Loss: 0.00002395
Iteration 487/1000 | Loss: 0.00002394
Iteration 488/1000 | Loss: 0.00002394
Iteration 489/1000 | Loss: 0.00002394
Iteration 490/1000 | Loss: 0.00002393
Iteration 491/1000 | Loss: 0.00002393
Iteration 492/1000 | Loss: 0.00002392
Iteration 493/1000 | Loss: 0.00002392
Iteration 494/1000 | Loss: 0.00002391
Iteration 495/1000 | Loss: 0.00002391
Iteration 496/1000 | Loss: 0.00002390
Iteration 497/1000 | Loss: 0.00002390
Iteration 498/1000 | Loss: 0.00002390
Iteration 499/1000 | Loss: 0.00002390
Iteration 500/1000 | Loss: 0.00002389
Iteration 501/1000 | Loss: 0.00002389
Iteration 502/1000 | Loss: 0.00002389
Iteration 503/1000 | Loss: 0.00002389
Iteration 504/1000 | Loss: 0.00002389
Iteration 505/1000 | Loss: 0.00002389
Iteration 506/1000 | Loss: 0.00002389
Iteration 507/1000 | Loss: 0.00002389
Iteration 508/1000 | Loss: 0.00002389
Iteration 509/1000 | Loss: 0.00002389
Iteration 510/1000 | Loss: 0.00002388
Iteration 511/1000 | Loss: 0.00002388
Iteration 512/1000 | Loss: 0.00002388
Iteration 513/1000 | Loss: 0.00002388
Iteration 514/1000 | Loss: 0.00002388
Iteration 515/1000 | Loss: 0.00002388
Iteration 516/1000 | Loss: 0.00002388
Iteration 517/1000 | Loss: 0.00002388
Iteration 518/1000 | Loss: 0.00002388
Iteration 519/1000 | Loss: 0.00002387
Iteration 520/1000 | Loss: 0.00002387
Iteration 521/1000 | Loss: 0.00002387
Iteration 522/1000 | Loss: 0.00002387
Iteration 523/1000 | Loss: 0.00002387
Iteration 524/1000 | Loss: 0.00002387
Iteration 525/1000 | Loss: 0.00002387
Iteration 526/1000 | Loss: 0.00002387
Iteration 527/1000 | Loss: 0.00002386
Iteration 528/1000 | Loss: 0.00002386
Iteration 529/1000 | Loss: 0.00002386
Iteration 530/1000 | Loss: 0.00002386
Iteration 531/1000 | Loss: 0.00002386
Iteration 532/1000 | Loss: 0.00002386
Iteration 533/1000 | Loss: 0.00002386
Iteration 534/1000 | Loss: 0.00002386
Iteration 535/1000 | Loss: 0.00002386
Iteration 536/1000 | Loss: 0.00002386
Iteration 537/1000 | Loss: 0.00002386
Iteration 538/1000 | Loss: 0.00002386
Iteration 539/1000 | Loss: 0.00002386
Iteration 540/1000 | Loss: 0.00002386
Iteration 541/1000 | Loss: 0.00002385
Iteration 542/1000 | Loss: 0.00002385
Iteration 543/1000 | Loss: 0.00002385
Iteration 544/1000 | Loss: 0.00002385
Iteration 545/1000 | Loss: 0.00002385
Iteration 546/1000 | Loss: 0.00002385
Iteration 547/1000 | Loss: 0.00002385
Iteration 548/1000 | Loss: 0.00002385
Iteration 549/1000 | Loss: 0.00002385
Iteration 550/1000 | Loss: 0.00002385
Iteration 551/1000 | Loss: 0.00002385
Iteration 552/1000 | Loss: 0.00002385
Iteration 553/1000 | Loss: 0.00002385
Iteration 554/1000 | Loss: 0.00002385
Iteration 555/1000 | Loss: 0.00002385
Iteration 556/1000 | Loss: 0.00002384
Iteration 557/1000 | Loss: 0.00002384
Iteration 558/1000 | Loss: 0.00002384
Iteration 559/1000 | Loss: 0.00002384
Iteration 560/1000 | Loss: 0.00002384
Iteration 561/1000 | Loss: 0.00002384
Iteration 562/1000 | Loss: 0.00002384
Iteration 563/1000 | Loss: 0.00002384
Iteration 564/1000 | Loss: 0.00002384
Iteration 565/1000 | Loss: 0.00002384
Iteration 566/1000 | Loss: 0.00002384
Iteration 567/1000 | Loss: 0.00002384
Iteration 568/1000 | Loss: 0.00002384
Iteration 569/1000 | Loss: 0.00002384
Iteration 570/1000 | Loss: 0.00002383
Iteration 571/1000 | Loss: 0.00002383
Iteration 572/1000 | Loss: 0.00002383
Iteration 573/1000 | Loss: 0.00002383
Iteration 574/1000 | Loss: 0.00002383
Iteration 575/1000 | Loss: 0.00002383
Iteration 576/1000 | Loss: 0.00002383
Iteration 577/1000 | Loss: 0.00002383
Iteration 578/1000 | Loss: 0.00002383
Iteration 579/1000 | Loss: 0.00002382
Iteration 580/1000 | Loss: 0.00002382
Iteration 581/1000 | Loss: 0.00002382
Iteration 582/1000 | Loss: 0.00002382
Iteration 583/1000 | Loss: 0.00002382
Iteration 584/1000 | Loss: 0.00002382
Iteration 585/1000 | Loss: 0.00002382
Iteration 586/1000 | Loss: 0.00002382
Iteration 587/1000 | Loss: 0.00002382
Iteration 588/1000 | Loss: 0.00002382
Iteration 589/1000 | Loss: 0.00002382
Iteration 590/1000 | Loss: 0.00002382
Iteration 591/1000 | Loss: 0.00002381
Iteration 592/1000 | Loss: 0.00002381
Iteration 593/1000 | Loss: 0.00002381
Iteration 594/1000 | Loss: 0.00002381
Iteration 595/1000 | Loss: 0.00002381
Iteration 596/1000 | Loss: 0.00002381
Iteration 597/1000 | Loss: 0.00002381
Iteration 598/1000 | Loss: 0.00002381
Iteration 599/1000 | Loss: 0.00002381
Iteration 600/1000 | Loss: 0.00002381
Iteration 601/1000 | Loss: 0.00002381
Iteration 602/1000 | Loss: 0.00002381
Iteration 603/1000 | Loss: 0.00002381
Iteration 604/1000 | Loss: 0.00002381
Iteration 605/1000 | Loss: 0.00002381
Iteration 606/1000 | Loss: 0.00002381
Iteration 607/1000 | Loss: 0.00002380
Iteration 608/1000 | Loss: 0.00002380
Iteration 609/1000 | Loss: 0.00002380
Iteration 610/1000 | Loss: 0.00002380
Iteration 611/1000 | Loss: 0.00002380
Iteration 612/1000 | Loss: 0.00002380
Iteration 613/1000 | Loss: 0.00002380
Iteration 614/1000 | Loss: 0.00002380
Iteration 615/1000 | Loss: 0.00002380
Iteration 616/1000 | Loss: 0.00002380
Iteration 617/1000 | Loss: 0.00002380
Iteration 618/1000 | Loss: 0.00002380
Iteration 619/1000 | Loss: 0.00002379
Iteration 620/1000 | Loss: 0.00002379
Iteration 621/1000 | Loss: 0.00002379
Iteration 622/1000 | Loss: 0.00002379
Iteration 623/1000 | Loss: 0.00002378
Iteration 624/1000 | Loss: 0.00002378
Iteration 625/1000 | Loss: 0.00002378
Iteration 626/1000 | Loss: 0.00002378
Iteration 627/1000 | Loss: 0.00002378
Iteration 628/1000 | Loss: 0.00002378
Iteration 629/1000 | Loss: 0.00002378
Iteration 630/1000 | Loss: 0.00002378
Iteration 631/1000 | Loss: 0.00002378
Iteration 632/1000 | Loss: 0.00002378
Iteration 633/1000 | Loss: 0.00002378
Iteration 634/1000 | Loss: 0.00002378
Iteration 635/1000 | Loss: 0.00002378
Iteration 636/1000 | Loss: 0.00002378
Iteration 637/1000 | Loss: 0.00002378
Iteration 638/1000 | Loss: 0.00002378
Iteration 639/1000 | Loss: 0.00002378
Iteration 640/1000 | Loss: 0.00002377
Iteration 641/1000 | Loss: 0.00002377
Iteration 642/1000 | Loss: 0.00002377
Iteration 643/1000 | Loss: 0.00002377
Iteration 644/1000 | Loss: 0.00002377
Iteration 645/1000 | Loss: 0.00002377
Iteration 646/1000 | Loss: 0.00002377
Iteration 647/1000 | Loss: 0.00002377
Iteration 648/1000 | Loss: 0.00002377
Iteration 649/1000 | Loss: 0.00002377
Iteration 650/1000 | Loss: 0.00002377
Iteration 651/1000 | Loss: 0.00002377
Iteration 652/1000 | Loss: 0.00002377
Iteration 653/1000 | Loss: 0.00002377
Iteration 654/1000 | Loss: 0.00002377
Iteration 655/1000 | Loss: 0.00002377
Iteration 656/1000 | Loss: 0.00002377
Iteration 657/1000 | Loss: 0.00002377
Iteration 658/1000 | Loss: 0.00002377
Iteration 659/1000 | Loss: 0.00002377
Iteration 660/1000 | Loss: 0.00002377
Iteration 661/1000 | Loss: 0.00002377
Iteration 662/1000 | Loss: 0.00002377
Iteration 663/1000 | Loss: 0.00002377
Iteration 664/1000 | Loss: 0.00002377
Iteration 665/1000 | Loss: 0.00002377
Iteration 666/1000 | Loss: 0.00002377
Iteration 667/1000 | Loss: 0.00002377
Iteration 668/1000 | Loss: 0.00002377
Iteration 669/1000 | Loss: 0.00002377
Iteration 670/1000 | Loss: 0.00002377
Iteration 671/1000 | Loss: 0.00002377
Iteration 672/1000 | Loss: 0.00002377
Iteration 673/1000 | Loss: 0.00002377
Iteration 674/1000 | Loss: 0.00002377
Iteration 675/1000 | Loss: 0.00002377
Iteration 676/1000 | Loss: 0.00002377
Iteration 677/1000 | Loss: 0.00002377
Iteration 678/1000 | Loss: 0.00002377
Iteration 679/1000 | Loss: 0.00002377
Iteration 680/1000 | Loss: 0.00002377
Iteration 681/1000 | Loss: 0.00002377
Iteration 682/1000 | Loss: 0.00002377
Iteration 683/1000 | Loss: 0.00002377
Iteration 684/1000 | Loss: 0.00002377
Iteration 685/1000 | Loss: 0.00002377
Iteration 686/1000 | Loss: 0.00002377
Iteration 687/1000 | Loss: 0.00002377
Iteration 688/1000 | Loss: 0.00002377
Iteration 689/1000 | Loss: 0.00002377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 689. Stopping optimization.
Last 5 losses: [2.377409145992715e-05, 2.377409145992715e-05, 2.377409145992715e-05, 2.377409145992715e-05, 2.377409145992715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.377409145992715e-05

Optimization complete. Final v2v error: 3.8408985137939453 mm

Highest mean error: 18.835142135620117 mm for frame 134

Lowest mean error: 3.352004289627075 mm for frame 94

Saving results

Total time: 1604.5212914943695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392190
Iteration 2/25 | Loss: 0.00143400
Iteration 3/25 | Loss: 0.00135347
Iteration 4/25 | Loss: 0.00134560
Iteration 5/25 | Loss: 0.00134277
Iteration 6/25 | Loss: 0.00134234
Iteration 7/25 | Loss: 0.00134234
Iteration 8/25 | Loss: 0.00134234
Iteration 9/25 | Loss: 0.00134234
Iteration 10/25 | Loss: 0.00134234
Iteration 11/25 | Loss: 0.00134234
Iteration 12/25 | Loss: 0.00134234
Iteration 13/25 | Loss: 0.00134234
Iteration 14/25 | Loss: 0.00134234
Iteration 15/25 | Loss: 0.00134234
Iteration 16/25 | Loss: 0.00134234
Iteration 17/25 | Loss: 0.00134234
Iteration 18/25 | Loss: 0.00134234
Iteration 19/25 | Loss: 0.00134234
Iteration 20/25 | Loss: 0.00134234
Iteration 21/25 | Loss: 0.00134234
Iteration 22/25 | Loss: 0.00134234
Iteration 23/25 | Loss: 0.00134234
Iteration 24/25 | Loss: 0.00134234
Iteration 25/25 | Loss: 0.00134234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013423440977931023, 0.0013423440977931023, 0.0013423440977931023, 0.0013423440977931023, 0.0013423440977931023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013423440977931023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45114136
Iteration 2/25 | Loss: 0.00227636
Iteration 3/25 | Loss: 0.00227636
Iteration 4/25 | Loss: 0.00227636
Iteration 5/25 | Loss: 0.00227636
Iteration 6/25 | Loss: 0.00227636
Iteration 7/25 | Loss: 0.00227636
Iteration 8/25 | Loss: 0.00227636
Iteration 9/25 | Loss: 0.00227636
Iteration 10/25 | Loss: 0.00227636
Iteration 11/25 | Loss: 0.00227636
Iteration 12/25 | Loss: 0.00227636
Iteration 13/25 | Loss: 0.00227636
Iteration 14/25 | Loss: 0.00227636
Iteration 15/25 | Loss: 0.00227636
Iteration 16/25 | Loss: 0.00227636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002276358660310507, 0.002276358660310507, 0.002276358660310507, 0.002276358660310507, 0.002276358660310507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002276358660310507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227636
Iteration 2/1000 | Loss: 0.00002908
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001978
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001893
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001859
Iteration 10/1000 | Loss: 0.00001852
Iteration 11/1000 | Loss: 0.00001849
Iteration 12/1000 | Loss: 0.00001848
Iteration 13/1000 | Loss: 0.00001847
Iteration 14/1000 | Loss: 0.00001845
Iteration 15/1000 | Loss: 0.00001845
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001843
Iteration 19/1000 | Loss: 0.00001842
Iteration 20/1000 | Loss: 0.00001838
Iteration 21/1000 | Loss: 0.00001837
Iteration 22/1000 | Loss: 0.00001836
Iteration 23/1000 | Loss: 0.00001835
Iteration 24/1000 | Loss: 0.00001834
Iteration 25/1000 | Loss: 0.00001833
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001832
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001828
Iteration 30/1000 | Loss: 0.00001827
Iteration 31/1000 | Loss: 0.00001825
Iteration 32/1000 | Loss: 0.00001825
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001823
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001818
Iteration 39/1000 | Loss: 0.00001817
Iteration 40/1000 | Loss: 0.00001817
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001808
Iteration 43/1000 | Loss: 0.00001805
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00001803
Iteration 47/1000 | Loss: 0.00001802
Iteration 48/1000 | Loss: 0.00001802
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001802
Iteration 51/1000 | Loss: 0.00001801
Iteration 52/1000 | Loss: 0.00001801
Iteration 53/1000 | Loss: 0.00001801
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001800
Iteration 57/1000 | Loss: 0.00001800
Iteration 58/1000 | Loss: 0.00001800
Iteration 59/1000 | Loss: 0.00001799
Iteration 60/1000 | Loss: 0.00001799
Iteration 61/1000 | Loss: 0.00001799
Iteration 62/1000 | Loss: 0.00001799
Iteration 63/1000 | Loss: 0.00001799
Iteration 64/1000 | Loss: 0.00001798
Iteration 65/1000 | Loss: 0.00001798
Iteration 66/1000 | Loss: 0.00001798
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001796
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001795
Iteration 77/1000 | Loss: 0.00001795
Iteration 78/1000 | Loss: 0.00001795
Iteration 79/1000 | Loss: 0.00001795
Iteration 80/1000 | Loss: 0.00001795
Iteration 81/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.7953319911612198e-05, 1.7953319911612198e-05, 1.7953319911612198e-05, 1.7953319911612198e-05, 1.7953319911612198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7953319911612198e-05

Optimization complete. Final v2v error: 3.5749330520629883 mm

Highest mean error: 3.9980714321136475 mm for frame 140

Lowest mean error: 3.3277480602264404 mm for frame 154

Saving results

Total time: 98.54518055915833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556273
Iteration 2/25 | Loss: 0.00166023
Iteration 3/25 | Loss: 0.00148686
Iteration 4/25 | Loss: 0.00147275
Iteration 5/25 | Loss: 0.00147069
Iteration 6/25 | Loss: 0.00147048
Iteration 7/25 | Loss: 0.00147048
Iteration 8/25 | Loss: 0.00147048
Iteration 9/25 | Loss: 0.00147048
Iteration 10/25 | Loss: 0.00147048
Iteration 11/25 | Loss: 0.00147048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00147047929931432, 0.00147047929931432, 0.00147047929931432, 0.00147047929931432, 0.00147047929931432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00147047929931432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29507196
Iteration 2/25 | Loss: 0.00173294
Iteration 3/25 | Loss: 0.00173294
Iteration 4/25 | Loss: 0.00173293
Iteration 5/25 | Loss: 0.00173293
Iteration 6/25 | Loss: 0.00173293
Iteration 7/25 | Loss: 0.00173293
Iteration 8/25 | Loss: 0.00173293
Iteration 9/25 | Loss: 0.00173293
Iteration 10/25 | Loss: 0.00173293
Iteration 11/25 | Loss: 0.00173293
Iteration 12/25 | Loss: 0.00173293
Iteration 13/25 | Loss: 0.00173293
Iteration 14/25 | Loss: 0.00173293
Iteration 15/25 | Loss: 0.00173293
Iteration 16/25 | Loss: 0.00173293
Iteration 17/25 | Loss: 0.00173293
Iteration 18/25 | Loss: 0.00173293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017329315887764096, 0.0017329315887764096, 0.0017329315887764096, 0.0017329315887764096, 0.0017329315887764096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017329315887764096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173293
Iteration 2/1000 | Loss: 0.00005764
Iteration 3/1000 | Loss: 0.00003751
Iteration 4/1000 | Loss: 0.00002906
Iteration 5/1000 | Loss: 0.00002619
Iteration 6/1000 | Loss: 0.00002476
Iteration 7/1000 | Loss: 0.00002363
Iteration 8/1000 | Loss: 0.00002295
Iteration 9/1000 | Loss: 0.00002264
Iteration 10/1000 | Loss: 0.00002233
Iteration 11/1000 | Loss: 0.00002214
Iteration 12/1000 | Loss: 0.00002194
Iteration 13/1000 | Loss: 0.00002185
Iteration 14/1000 | Loss: 0.00002180
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002178
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002175
Iteration 22/1000 | Loss: 0.00002173
Iteration 23/1000 | Loss: 0.00002172
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002168
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002168
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002166
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002166
Iteration 38/1000 | Loss: 0.00002166
Iteration 39/1000 | Loss: 0.00002166
Iteration 40/1000 | Loss: 0.00002166
Iteration 41/1000 | Loss: 0.00002165
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002163
Iteration 45/1000 | Loss: 0.00002163
Iteration 46/1000 | Loss: 0.00002162
Iteration 47/1000 | Loss: 0.00002162
Iteration 48/1000 | Loss: 0.00002161
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002161
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00002160
Iteration 55/1000 | Loss: 0.00002160
Iteration 56/1000 | Loss: 0.00002160
Iteration 57/1000 | Loss: 0.00002160
Iteration 58/1000 | Loss: 0.00002160
Iteration 59/1000 | Loss: 0.00002160
Iteration 60/1000 | Loss: 0.00002160
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002160
Iteration 68/1000 | Loss: 0.00002160
Iteration 69/1000 | Loss: 0.00002160
Iteration 70/1000 | Loss: 0.00002160
Iteration 71/1000 | Loss: 0.00002160
Iteration 72/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.1600397303700447e-05, 2.1600397303700447e-05, 2.1600397303700447e-05, 2.1600397303700447e-05, 2.1600397303700447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1600397303700447e-05

Optimization complete. Final v2v error: 4.0184760093688965 mm

Highest mean error: 4.8668293952941895 mm for frame 79

Lowest mean error: 3.6271591186523438 mm for frame 32

Saving results

Total time: 71.46080136299133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981419
Iteration 2/25 | Loss: 0.00169675
Iteration 3/25 | Loss: 0.00155806
Iteration 4/25 | Loss: 0.00153117
Iteration 5/25 | Loss: 0.00152362
Iteration 6/25 | Loss: 0.00152153
Iteration 7/25 | Loss: 0.00152114
Iteration 8/25 | Loss: 0.00152114
Iteration 9/25 | Loss: 0.00152114
Iteration 10/25 | Loss: 0.00152114
Iteration 11/25 | Loss: 0.00152114
Iteration 12/25 | Loss: 0.00152114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001521144062280655, 0.001521144062280655, 0.001521144062280655, 0.001521144062280655, 0.001521144062280655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001521144062280655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.20376331
Iteration 2/25 | Loss: 0.00186165
Iteration 3/25 | Loss: 0.00186165
Iteration 4/25 | Loss: 0.00186165
Iteration 5/25 | Loss: 0.00186165
Iteration 6/25 | Loss: 0.00186165
Iteration 7/25 | Loss: 0.00186165
Iteration 8/25 | Loss: 0.00186165
Iteration 9/25 | Loss: 0.00186165
Iteration 10/25 | Loss: 0.00186165
Iteration 11/25 | Loss: 0.00186165
Iteration 12/25 | Loss: 0.00186165
Iteration 13/25 | Loss: 0.00186165
Iteration 14/25 | Loss: 0.00186165
Iteration 15/25 | Loss: 0.00186165
Iteration 16/25 | Loss: 0.00186165
Iteration 17/25 | Loss: 0.00186165
Iteration 18/25 | Loss: 0.00186165
Iteration 19/25 | Loss: 0.00186165
Iteration 20/25 | Loss: 0.00186165
Iteration 21/25 | Loss: 0.00186165
Iteration 22/25 | Loss: 0.00186165
Iteration 23/25 | Loss: 0.00186165
Iteration 24/25 | Loss: 0.00186165
Iteration 25/25 | Loss: 0.00186165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186165
Iteration 2/1000 | Loss: 0.00008435
Iteration 3/1000 | Loss: 0.00005693
Iteration 4/1000 | Loss: 0.00004459
Iteration 5/1000 | Loss: 0.00004014
Iteration 6/1000 | Loss: 0.00003822
Iteration 7/1000 | Loss: 0.00003727
Iteration 8/1000 | Loss: 0.00003560
Iteration 9/1000 | Loss: 0.00003462
Iteration 10/1000 | Loss: 0.00003401
Iteration 11/1000 | Loss: 0.00003357
Iteration 12/1000 | Loss: 0.00003316
Iteration 13/1000 | Loss: 0.00003283
Iteration 14/1000 | Loss: 0.00003248
Iteration 15/1000 | Loss: 0.00003232
Iteration 16/1000 | Loss: 0.00003229
Iteration 17/1000 | Loss: 0.00003225
Iteration 18/1000 | Loss: 0.00003224
Iteration 19/1000 | Loss: 0.00003223
Iteration 20/1000 | Loss: 0.00003220
Iteration 21/1000 | Loss: 0.00003218
Iteration 22/1000 | Loss: 0.00003215
Iteration 23/1000 | Loss: 0.00003207
Iteration 24/1000 | Loss: 0.00003198
Iteration 25/1000 | Loss: 0.00003190
Iteration 26/1000 | Loss: 0.00003189
Iteration 27/1000 | Loss: 0.00003189
Iteration 28/1000 | Loss: 0.00003186
Iteration 29/1000 | Loss: 0.00003186
Iteration 30/1000 | Loss: 0.00003185
Iteration 31/1000 | Loss: 0.00003185
Iteration 32/1000 | Loss: 0.00003185
Iteration 33/1000 | Loss: 0.00003185
Iteration 34/1000 | Loss: 0.00003185
Iteration 35/1000 | Loss: 0.00003184
Iteration 36/1000 | Loss: 0.00003184
Iteration 37/1000 | Loss: 0.00003183
Iteration 38/1000 | Loss: 0.00003182
Iteration 39/1000 | Loss: 0.00003182
Iteration 40/1000 | Loss: 0.00003182
Iteration 41/1000 | Loss: 0.00003181
Iteration 42/1000 | Loss: 0.00003181
Iteration 43/1000 | Loss: 0.00003181
Iteration 44/1000 | Loss: 0.00003181
Iteration 45/1000 | Loss: 0.00003181
Iteration 46/1000 | Loss: 0.00003180
Iteration 47/1000 | Loss: 0.00003180
Iteration 48/1000 | Loss: 0.00003180
Iteration 49/1000 | Loss: 0.00003180
Iteration 50/1000 | Loss: 0.00003180
Iteration 51/1000 | Loss: 0.00003180
Iteration 52/1000 | Loss: 0.00003180
Iteration 53/1000 | Loss: 0.00003179
Iteration 54/1000 | Loss: 0.00003179
Iteration 55/1000 | Loss: 0.00003178
Iteration 56/1000 | Loss: 0.00003178
Iteration 57/1000 | Loss: 0.00003177
Iteration 58/1000 | Loss: 0.00003177
Iteration 59/1000 | Loss: 0.00003177
Iteration 60/1000 | Loss: 0.00003177
Iteration 61/1000 | Loss: 0.00003177
Iteration 62/1000 | Loss: 0.00003177
Iteration 63/1000 | Loss: 0.00003176
Iteration 64/1000 | Loss: 0.00003176
Iteration 65/1000 | Loss: 0.00003176
Iteration 66/1000 | Loss: 0.00003176
Iteration 67/1000 | Loss: 0.00003176
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003175
Iteration 73/1000 | Loss: 0.00003175
Iteration 74/1000 | Loss: 0.00003175
Iteration 75/1000 | Loss: 0.00003175
Iteration 76/1000 | Loss: 0.00003174
Iteration 77/1000 | Loss: 0.00003174
Iteration 78/1000 | Loss: 0.00003174
Iteration 79/1000 | Loss: 0.00003174
Iteration 80/1000 | Loss: 0.00003173
Iteration 81/1000 | Loss: 0.00003173
Iteration 82/1000 | Loss: 0.00003172
Iteration 83/1000 | Loss: 0.00003172
Iteration 84/1000 | Loss: 0.00003171
Iteration 85/1000 | Loss: 0.00003171
Iteration 86/1000 | Loss: 0.00003170
Iteration 87/1000 | Loss: 0.00003170
Iteration 88/1000 | Loss: 0.00003170
Iteration 89/1000 | Loss: 0.00003170
Iteration 90/1000 | Loss: 0.00003170
Iteration 91/1000 | Loss: 0.00003170
Iteration 92/1000 | Loss: 0.00003170
Iteration 93/1000 | Loss: 0.00003169
Iteration 94/1000 | Loss: 0.00003169
Iteration 95/1000 | Loss: 0.00003169
Iteration 96/1000 | Loss: 0.00003169
Iteration 97/1000 | Loss: 0.00003169
Iteration 98/1000 | Loss: 0.00003169
Iteration 99/1000 | Loss: 0.00003169
Iteration 100/1000 | Loss: 0.00003168
Iteration 101/1000 | Loss: 0.00003168
Iteration 102/1000 | Loss: 0.00003168
Iteration 103/1000 | Loss: 0.00003168
Iteration 104/1000 | Loss: 0.00003168
Iteration 105/1000 | Loss: 0.00003168
Iteration 106/1000 | Loss: 0.00003168
Iteration 107/1000 | Loss: 0.00003168
Iteration 108/1000 | Loss: 0.00003168
Iteration 109/1000 | Loss: 0.00003168
Iteration 110/1000 | Loss: 0.00003168
Iteration 111/1000 | Loss: 0.00003168
Iteration 112/1000 | Loss: 0.00003168
Iteration 113/1000 | Loss: 0.00003168
Iteration 114/1000 | Loss: 0.00003168
Iteration 115/1000 | Loss: 0.00003168
Iteration 116/1000 | Loss: 0.00003168
Iteration 117/1000 | Loss: 0.00003168
Iteration 118/1000 | Loss: 0.00003168
Iteration 119/1000 | Loss: 0.00003168
Iteration 120/1000 | Loss: 0.00003168
Iteration 121/1000 | Loss: 0.00003168
Iteration 122/1000 | Loss: 0.00003168
Iteration 123/1000 | Loss: 0.00003168
Iteration 124/1000 | Loss: 0.00003168
Iteration 125/1000 | Loss: 0.00003168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.168128750985488e-05, 3.168128750985488e-05, 3.168128750985488e-05, 3.168128750985488e-05, 3.168128750985488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.168128750985488e-05

Optimization complete. Final v2v error: 4.901010036468506 mm

Highest mean error: 5.166601181030273 mm for frame 112

Lowest mean error: 4.514431476593018 mm for frame 15

Saving results

Total time: 99.6883111000061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_2463/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_2463/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932642
Iteration 2/25 | Loss: 0.00190402
Iteration 3/25 | Loss: 0.00172642
Iteration 4/25 | Loss: 0.00167282
Iteration 5/25 | Loss: 0.00168463
Iteration 6/25 | Loss: 0.00167499
Iteration 7/25 | Loss: 0.00167271
Iteration 8/25 | Loss: 0.00163870
Iteration 9/25 | Loss: 0.00162036
Iteration 10/25 | Loss: 0.00161518
Iteration 11/25 | Loss: 0.00160768
Iteration 12/25 | Loss: 0.00160521
Iteration 13/25 | Loss: 0.00159687
Iteration 14/25 | Loss: 0.00159384
Iteration 15/25 | Loss: 0.00159311
Iteration 16/25 | Loss: 0.00159282
Iteration 17/25 | Loss: 0.00159404
Iteration 18/25 | Loss: 0.00159416
Iteration 19/25 | Loss: 0.00158224
Iteration 20/25 | Loss: 0.00157953
Iteration 21/25 | Loss: 0.00157643
Iteration 22/25 | Loss: 0.00156936
Iteration 23/25 | Loss: 0.00156696
Iteration 24/25 | Loss: 0.00156680
Iteration 25/25 | Loss: 0.00156364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17760432
Iteration 2/25 | Loss: 0.00424963
Iteration 3/25 | Loss: 0.00424962
Iteration 4/25 | Loss: 0.00424962
Iteration 5/25 | Loss: 0.00424962
Iteration 6/25 | Loss: 0.00424962
Iteration 7/25 | Loss: 0.00424962
Iteration 8/25 | Loss: 0.00424962
Iteration 9/25 | Loss: 0.00424962
Iteration 10/25 | Loss: 0.00424962
Iteration 11/25 | Loss: 0.00424962
Iteration 12/25 | Loss: 0.00424962
Iteration 13/25 | Loss: 0.00424962
Iteration 14/25 | Loss: 0.00424962
Iteration 15/25 | Loss: 0.00424962
Iteration 16/25 | Loss: 0.00424962
Iteration 17/25 | Loss: 0.00424962
Iteration 18/25 | Loss: 0.00424962
Iteration 19/25 | Loss: 0.00424962
Iteration 20/25 | Loss: 0.00424962
Iteration 21/25 | Loss: 0.00424962
Iteration 22/25 | Loss: 0.00424962
Iteration 23/25 | Loss: 0.00424962
Iteration 24/25 | Loss: 0.00424962
Iteration 25/25 | Loss: 0.00424962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00424962
Iteration 2/1000 | Loss: 0.00048154
Iteration 3/1000 | Loss: 0.00023434
Iteration 4/1000 | Loss: 0.00022468
Iteration 5/1000 | Loss: 0.00021149
Iteration 6/1000 | Loss: 0.00024508
Iteration 7/1000 | Loss: 0.00025426
Iteration 8/1000 | Loss: 0.00024120
Iteration 9/1000 | Loss: 0.00014698
Iteration 10/1000 | Loss: 0.00032571
Iteration 11/1000 | Loss: 0.00022321
Iteration 12/1000 | Loss: 0.00087214
Iteration 13/1000 | Loss: 0.00065685
Iteration 14/1000 | Loss: 0.00064501
Iteration 15/1000 | Loss: 0.00038542
Iteration 16/1000 | Loss: 0.00017948
Iteration 17/1000 | Loss: 0.00043343
Iteration 18/1000 | Loss: 0.00017080
Iteration 19/1000 | Loss: 0.00018327
Iteration 20/1000 | Loss: 0.00010191
Iteration 21/1000 | Loss: 0.00062621
Iteration 22/1000 | Loss: 0.00045097
Iteration 23/1000 | Loss: 0.00020532
Iteration 24/1000 | Loss: 0.00011755
Iteration 25/1000 | Loss: 0.00026272
Iteration 26/1000 | Loss: 0.00009646
Iteration 27/1000 | Loss: 0.00013512
Iteration 28/1000 | Loss: 0.00008554
Iteration 29/1000 | Loss: 0.00008099
Iteration 30/1000 | Loss: 0.00016285
Iteration 31/1000 | Loss: 0.00057074
Iteration 32/1000 | Loss: 0.00034311
Iteration 33/1000 | Loss: 0.00009393
Iteration 34/1000 | Loss: 0.00008104
Iteration 35/1000 | Loss: 0.00036382
Iteration 36/1000 | Loss: 0.00007808
Iteration 37/1000 | Loss: 0.00007249
Iteration 38/1000 | Loss: 0.00006954
Iteration 39/1000 | Loss: 0.00006700
Iteration 40/1000 | Loss: 0.00054050
Iteration 41/1000 | Loss: 0.00185859
Iteration 42/1000 | Loss: 0.00036880
Iteration 43/1000 | Loss: 0.00049678
Iteration 44/1000 | Loss: 0.00007707
Iteration 45/1000 | Loss: 0.00006812
Iteration 46/1000 | Loss: 0.00045947
Iteration 47/1000 | Loss: 0.00025515
Iteration 48/1000 | Loss: 0.00039881
Iteration 49/1000 | Loss: 0.00022612
Iteration 50/1000 | Loss: 0.00036894
Iteration 51/1000 | Loss: 0.00011955
Iteration 52/1000 | Loss: 0.00007263
Iteration 53/1000 | Loss: 0.00006351
Iteration 54/1000 | Loss: 0.00045236
Iteration 55/1000 | Loss: 0.00020110
Iteration 56/1000 | Loss: 0.00006172
Iteration 57/1000 | Loss: 0.00152132
Iteration 58/1000 | Loss: 0.00029076
Iteration 59/1000 | Loss: 0.00007121
Iteration 60/1000 | Loss: 0.00006196
Iteration 61/1000 | Loss: 0.00042201
Iteration 62/1000 | Loss: 0.00024587
Iteration 63/1000 | Loss: 0.00006116
Iteration 64/1000 | Loss: 0.00046210
Iteration 65/1000 | Loss: 0.00010462
Iteration 66/1000 | Loss: 0.00006047
Iteration 67/1000 | Loss: 0.00173430
Iteration 68/1000 | Loss: 0.00049047
Iteration 69/1000 | Loss: 0.00081637
Iteration 70/1000 | Loss: 0.00014031
Iteration 71/1000 | Loss: 0.00008475
Iteration 72/1000 | Loss: 0.00006834
Iteration 73/1000 | Loss: 0.00005889
Iteration 74/1000 | Loss: 0.00005442
Iteration 75/1000 | Loss: 0.00005082
Iteration 76/1000 | Loss: 0.00041936
Iteration 77/1000 | Loss: 0.00022278
Iteration 78/1000 | Loss: 0.00009529
Iteration 79/1000 | Loss: 0.00032494
Iteration 80/1000 | Loss: 0.00005055
Iteration 81/1000 | Loss: 0.00004845
Iteration 82/1000 | Loss: 0.00013814
Iteration 83/1000 | Loss: 0.00011965
Iteration 84/1000 | Loss: 0.00008763
Iteration 85/1000 | Loss: 0.00013584
Iteration 86/1000 | Loss: 0.00023050
Iteration 87/1000 | Loss: 0.00009372
Iteration 88/1000 | Loss: 0.00005240
Iteration 89/1000 | Loss: 0.00014716
Iteration 90/1000 | Loss: 0.00018871
Iteration 91/1000 | Loss: 0.00008166
Iteration 92/1000 | Loss: 0.00022524
Iteration 93/1000 | Loss: 0.00009630
Iteration 94/1000 | Loss: 0.00004828
Iteration 95/1000 | Loss: 0.00004675
Iteration 96/1000 | Loss: 0.00004635
Iteration 97/1000 | Loss: 0.00004624
Iteration 98/1000 | Loss: 0.00004599
Iteration 99/1000 | Loss: 0.00004572
Iteration 100/1000 | Loss: 0.00004545
Iteration 101/1000 | Loss: 0.00004517
Iteration 102/1000 | Loss: 0.00028559
Iteration 103/1000 | Loss: 0.00009145
Iteration 104/1000 | Loss: 0.00006772
Iteration 105/1000 | Loss: 0.00029459
Iteration 106/1000 | Loss: 0.00009829
Iteration 107/1000 | Loss: 0.00004851
Iteration 108/1000 | Loss: 0.00004447
Iteration 109/1000 | Loss: 0.00004276
Iteration 110/1000 | Loss: 0.00004226
Iteration 111/1000 | Loss: 0.00004199
Iteration 112/1000 | Loss: 0.00004192
Iteration 113/1000 | Loss: 0.00004192
Iteration 114/1000 | Loss: 0.00004190
Iteration 115/1000 | Loss: 0.00004190
Iteration 116/1000 | Loss: 0.00004189
Iteration 117/1000 | Loss: 0.00004188
Iteration 118/1000 | Loss: 0.00004182
Iteration 119/1000 | Loss: 0.00004172
Iteration 120/1000 | Loss: 0.00004169
Iteration 121/1000 | Loss: 0.00004168
Iteration 122/1000 | Loss: 0.00004168
Iteration 123/1000 | Loss: 0.00004164
Iteration 124/1000 | Loss: 0.00004157
Iteration 125/1000 | Loss: 0.00004154
Iteration 126/1000 | Loss: 0.00004154
Iteration 127/1000 | Loss: 0.00004154
Iteration 128/1000 | Loss: 0.00004153
Iteration 129/1000 | Loss: 0.00004152
Iteration 130/1000 | Loss: 0.00004150
Iteration 131/1000 | Loss: 0.00004150
Iteration 132/1000 | Loss: 0.00004150
Iteration 133/1000 | Loss: 0.00004149
Iteration 134/1000 | Loss: 0.00004149
Iteration 135/1000 | Loss: 0.00004149
Iteration 136/1000 | Loss: 0.00004148
Iteration 137/1000 | Loss: 0.00004147
Iteration 138/1000 | Loss: 0.00004145
Iteration 139/1000 | Loss: 0.00004145
Iteration 140/1000 | Loss: 0.00004144
Iteration 141/1000 | Loss: 0.00004144
Iteration 142/1000 | Loss: 0.00004143
Iteration 143/1000 | Loss: 0.00004143
Iteration 144/1000 | Loss: 0.00004143
Iteration 145/1000 | Loss: 0.00004143
Iteration 146/1000 | Loss: 0.00004142
Iteration 147/1000 | Loss: 0.00004142
Iteration 148/1000 | Loss: 0.00004142
Iteration 149/1000 | Loss: 0.00004141
Iteration 150/1000 | Loss: 0.00004141
Iteration 151/1000 | Loss: 0.00004141
Iteration 152/1000 | Loss: 0.00004140
Iteration 153/1000 | Loss: 0.00004140
Iteration 154/1000 | Loss: 0.00004139
Iteration 155/1000 | Loss: 0.00004139
Iteration 156/1000 | Loss: 0.00004139
Iteration 157/1000 | Loss: 0.00004139
Iteration 158/1000 | Loss: 0.00004138
Iteration 159/1000 | Loss: 0.00004138
Iteration 160/1000 | Loss: 0.00004137
Iteration 161/1000 | Loss: 0.00004137
Iteration 162/1000 | Loss: 0.00004137
Iteration 163/1000 | Loss: 0.00004137
Iteration 164/1000 | Loss: 0.00004137
Iteration 165/1000 | Loss: 0.00004137
Iteration 166/1000 | Loss: 0.00004137
Iteration 167/1000 | Loss: 0.00004137
Iteration 168/1000 | Loss: 0.00004136
Iteration 169/1000 | Loss: 0.00004136
Iteration 170/1000 | Loss: 0.00004136
Iteration 171/1000 | Loss: 0.00004135
Iteration 172/1000 | Loss: 0.00004135
Iteration 173/1000 | Loss: 0.00004135
Iteration 174/1000 | Loss: 0.00004135
Iteration 175/1000 | Loss: 0.00004134
Iteration 176/1000 | Loss: 0.00004134
Iteration 177/1000 | Loss: 0.00004134
Iteration 178/1000 | Loss: 0.00004133
Iteration 179/1000 | Loss: 0.00004133
Iteration 180/1000 | Loss: 0.00004131
Iteration 181/1000 | Loss: 0.00004128
Iteration 182/1000 | Loss: 0.00004128
Iteration 183/1000 | Loss: 0.00004127
Iteration 184/1000 | Loss: 0.00004127
Iteration 185/1000 | Loss: 0.00004127
Iteration 186/1000 | Loss: 0.00004127
Iteration 187/1000 | Loss: 0.00004126
Iteration 188/1000 | Loss: 0.00004126
Iteration 189/1000 | Loss: 0.00004126
Iteration 190/1000 | Loss: 0.00004126
Iteration 191/1000 | Loss: 0.00004126
Iteration 192/1000 | Loss: 0.00004126
Iteration 193/1000 | Loss: 0.00004126
Iteration 194/1000 | Loss: 0.00004126
Iteration 195/1000 | Loss: 0.00004126
Iteration 196/1000 | Loss: 0.00004126
Iteration 197/1000 | Loss: 0.00004126
Iteration 198/1000 | Loss: 0.00004125
Iteration 199/1000 | Loss: 0.00004125
Iteration 200/1000 | Loss: 0.00004125
Iteration 201/1000 | Loss: 0.00004125
Iteration 202/1000 | Loss: 0.00004125
Iteration 203/1000 | Loss: 0.00004125
Iteration 204/1000 | Loss: 0.00004125
Iteration 205/1000 | Loss: 0.00004125
Iteration 206/1000 | Loss: 0.00004125
Iteration 207/1000 | Loss: 0.00004125
Iteration 208/1000 | Loss: 0.00004125
Iteration 209/1000 | Loss: 0.00004125
Iteration 210/1000 | Loss: 0.00004124
Iteration 211/1000 | Loss: 0.00004124
Iteration 212/1000 | Loss: 0.00004124
Iteration 213/1000 | Loss: 0.00004124
Iteration 214/1000 | Loss: 0.00004124
Iteration 215/1000 | Loss: 0.00004121
Iteration 216/1000 | Loss: 0.00004120
Iteration 217/1000 | Loss: 0.00004120
Iteration 218/1000 | Loss: 0.00004119
Iteration 219/1000 | Loss: 0.00004119
Iteration 220/1000 | Loss: 0.00004118
Iteration 221/1000 | Loss: 0.00004117
Iteration 222/1000 | Loss: 0.00004117
Iteration 223/1000 | Loss: 0.00004116
Iteration 224/1000 | Loss: 0.00004116
Iteration 225/1000 | Loss: 0.00004114
Iteration 226/1000 | Loss: 0.00004114
Iteration 227/1000 | Loss: 0.00004114
Iteration 228/1000 | Loss: 0.00004113
Iteration 229/1000 | Loss: 0.00004113
Iteration 230/1000 | Loss: 0.00004112
Iteration 231/1000 | Loss: 0.00004111
Iteration 232/1000 | Loss: 0.00004111
Iteration 233/1000 | Loss: 0.00004111
Iteration 234/1000 | Loss: 0.00004110
Iteration 235/1000 | Loss: 0.00004110
Iteration 236/1000 | Loss: 0.00004110
Iteration 237/1000 | Loss: 0.00004110
Iteration 238/1000 | Loss: 0.00004109
Iteration 239/1000 | Loss: 0.00004109
Iteration 240/1000 | Loss: 0.00004109
Iteration 241/1000 | Loss: 0.00004109
Iteration 242/1000 | Loss: 0.00004109
Iteration 243/1000 | Loss: 0.00004109
Iteration 244/1000 | Loss: 0.00004109
Iteration 245/1000 | Loss: 0.00004109
Iteration 246/1000 | Loss: 0.00004109
Iteration 247/1000 | Loss: 0.00004109
Iteration 248/1000 | Loss: 0.00004108
Iteration 249/1000 | Loss: 0.00004108
Iteration 250/1000 | Loss: 0.00004108
Iteration 251/1000 | Loss: 0.00004108
Iteration 252/1000 | Loss: 0.00004108
Iteration 253/1000 | Loss: 0.00004108
Iteration 254/1000 | Loss: 0.00004107
Iteration 255/1000 | Loss: 0.00004107
Iteration 256/1000 | Loss: 0.00004107
Iteration 257/1000 | Loss: 0.00004106
Iteration 258/1000 | Loss: 0.00004106
Iteration 259/1000 | Loss: 0.00004106
Iteration 260/1000 | Loss: 0.00004106
Iteration 261/1000 | Loss: 0.00004106
Iteration 262/1000 | Loss: 0.00004106
Iteration 263/1000 | Loss: 0.00004105
Iteration 264/1000 | Loss: 0.00004105
Iteration 265/1000 | Loss: 0.00004105
Iteration 266/1000 | Loss: 0.00004105
Iteration 267/1000 | Loss: 0.00004104
Iteration 268/1000 | Loss: 0.00004104
Iteration 269/1000 | Loss: 0.00004104
Iteration 270/1000 | Loss: 0.00004104
Iteration 271/1000 | Loss: 0.00004104
Iteration 272/1000 | Loss: 0.00004103
Iteration 273/1000 | Loss: 0.00004103
Iteration 274/1000 | Loss: 0.00004103
Iteration 275/1000 | Loss: 0.00004103
Iteration 276/1000 | Loss: 0.00004102
Iteration 277/1000 | Loss: 0.00004102
Iteration 278/1000 | Loss: 0.00004102
Iteration 279/1000 | Loss: 0.00004102
Iteration 280/1000 | Loss: 0.00004102
Iteration 281/1000 | Loss: 0.00004102
Iteration 282/1000 | Loss: 0.00004102
Iteration 283/1000 | Loss: 0.00004102
Iteration 284/1000 | Loss: 0.00004102
Iteration 285/1000 | Loss: 0.00004101
Iteration 286/1000 | Loss: 0.00004101
Iteration 287/1000 | Loss: 0.00004101
Iteration 288/1000 | Loss: 0.00004101
Iteration 289/1000 | Loss: 0.00004101
Iteration 290/1000 | Loss: 0.00004101
Iteration 291/1000 | Loss: 0.00004100
Iteration 292/1000 | Loss: 0.00004100
Iteration 293/1000 | Loss: 0.00004100
Iteration 294/1000 | Loss: 0.00004100
Iteration 295/1000 | Loss: 0.00004100
Iteration 296/1000 | Loss: 0.00004100
Iteration 297/1000 | Loss: 0.00004100
Iteration 298/1000 | Loss: 0.00004100
Iteration 299/1000 | Loss: 0.00004100
Iteration 300/1000 | Loss: 0.00004100
Iteration 301/1000 | Loss: 0.00004099
Iteration 302/1000 | Loss: 0.00004099
Iteration 303/1000 | Loss: 0.00004099
Iteration 304/1000 | Loss: 0.00004099
Iteration 305/1000 | Loss: 0.00004099
Iteration 306/1000 | Loss: 0.00004099
Iteration 307/1000 | Loss: 0.00004098
Iteration 308/1000 | Loss: 0.00004098
Iteration 309/1000 | Loss: 0.00004098
Iteration 310/1000 | Loss: 0.00004098
Iteration 311/1000 | Loss: 0.00004098
Iteration 312/1000 | Loss: 0.00004098
Iteration 313/1000 | Loss: 0.00004098
Iteration 314/1000 | Loss: 0.00004098
Iteration 315/1000 | Loss: 0.00004098
Iteration 316/1000 | Loss: 0.00004098
Iteration 317/1000 | Loss: 0.00004098
Iteration 318/1000 | Loss: 0.00004098
Iteration 319/1000 | Loss: 0.00004097
Iteration 320/1000 | Loss: 0.00004097
Iteration 321/1000 | Loss: 0.00004097
Iteration 322/1000 | Loss: 0.00004097
Iteration 323/1000 | Loss: 0.00004097
Iteration 324/1000 | Loss: 0.00004097
Iteration 325/1000 | Loss: 0.00004097
Iteration 326/1000 | Loss: 0.00004097
Iteration 327/1000 | Loss: 0.00004097
Iteration 328/1000 | Loss: 0.00004097
Iteration 329/1000 | Loss: 0.00004097
Iteration 330/1000 | Loss: 0.00004097
Iteration 331/1000 | Loss: 0.00004097
Iteration 332/1000 | Loss: 0.00004097
Iteration 333/1000 | Loss: 0.00004097
Iteration 334/1000 | Loss: 0.00004097
Iteration 335/1000 | Loss: 0.00004096
Iteration 336/1000 | Loss: 0.00004096
Iteration 337/1000 | Loss: 0.00004096
Iteration 338/1000 | Loss: 0.00004096
Iteration 339/1000 | Loss: 0.00004096
Iteration 340/1000 | Loss: 0.00004096
Iteration 341/1000 | Loss: 0.00004096
Iteration 342/1000 | Loss: 0.00004096
Iteration 343/1000 | Loss: 0.00004096
Iteration 344/1000 | Loss: 0.00004096
Iteration 345/1000 | Loss: 0.00004096
Iteration 346/1000 | Loss: 0.00004096
Iteration 347/1000 | Loss: 0.00004096
Iteration 348/1000 | Loss: 0.00004096
Iteration 349/1000 | Loss: 0.00004096
Iteration 350/1000 | Loss: 0.00004096
Iteration 351/1000 | Loss: 0.00004096
Iteration 352/1000 | Loss: 0.00004096
Iteration 353/1000 | Loss: 0.00004096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [4.0963932406157255e-05, 4.0963932406157255e-05, 4.0963932406157255e-05, 4.0963932406157255e-05, 4.0963932406157255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0963932406157255e-05

Optimization complete. Final v2v error: 4.171844005584717 mm

Highest mean error: 12.675325393676758 mm for frame 111

Lowest mean error: 3.551602602005005 mm for frame 183

Saving results

Total time: 583.6823222637177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070870
Iteration 2/25 | Loss: 0.00182454
Iteration 3/25 | Loss: 0.00114847
Iteration 4/25 | Loss: 0.00104239
Iteration 5/25 | Loss: 0.00101905
Iteration 6/25 | Loss: 0.00101513
Iteration 7/25 | Loss: 0.00100708
Iteration 8/25 | Loss: 0.00100982
Iteration 9/25 | Loss: 0.00098044
Iteration 10/25 | Loss: 0.00097002
Iteration 11/25 | Loss: 0.00096806
Iteration 12/25 | Loss: 0.00096767
Iteration 13/25 | Loss: 0.00096755
Iteration 14/25 | Loss: 0.00096755
Iteration 15/25 | Loss: 0.00096755
Iteration 16/25 | Loss: 0.00096755
Iteration 17/25 | Loss: 0.00096755
Iteration 18/25 | Loss: 0.00096755
Iteration 19/25 | Loss: 0.00096755
Iteration 20/25 | Loss: 0.00096755
Iteration 21/25 | Loss: 0.00096755
Iteration 22/25 | Loss: 0.00096755
Iteration 23/25 | Loss: 0.00096755
Iteration 24/25 | Loss: 0.00096755
Iteration 25/25 | Loss: 0.00096755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42442691
Iteration 2/25 | Loss: 0.00037396
Iteration 3/25 | Loss: 0.00037396
Iteration 4/25 | Loss: 0.00037396
Iteration 5/25 | Loss: 0.00037396
Iteration 6/25 | Loss: 0.00037396
Iteration 7/25 | Loss: 0.00037396
Iteration 8/25 | Loss: 0.00037396
Iteration 9/25 | Loss: 0.00037396
Iteration 10/25 | Loss: 0.00037396
Iteration 11/25 | Loss: 0.00037396
Iteration 12/25 | Loss: 0.00037396
Iteration 13/25 | Loss: 0.00037396
Iteration 14/25 | Loss: 0.00037396
Iteration 15/25 | Loss: 0.00037396
Iteration 16/25 | Loss: 0.00037396
Iteration 17/25 | Loss: 0.00037396
Iteration 18/25 | Loss: 0.00037396
Iteration 19/25 | Loss: 0.00037396
Iteration 20/25 | Loss: 0.00037396
Iteration 21/25 | Loss: 0.00037396
Iteration 22/25 | Loss: 0.00037396
Iteration 23/25 | Loss: 0.00037396
Iteration 24/25 | Loss: 0.00037396
Iteration 25/25 | Loss: 0.00037396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037396
Iteration 2/1000 | Loss: 0.00003332
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001845
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00001702
Iteration 9/1000 | Loss: 0.00001675
Iteration 10/1000 | Loss: 0.00001673
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001650
Iteration 13/1000 | Loss: 0.00001636
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001629
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001626
Iteration 20/1000 | Loss: 0.00001626
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001622
Iteration 30/1000 | Loss: 0.00001622
Iteration 31/1000 | Loss: 0.00001622
Iteration 32/1000 | Loss: 0.00001622
Iteration 33/1000 | Loss: 0.00001622
Iteration 34/1000 | Loss: 0.00001622
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001622
Iteration 40/1000 | Loss: 0.00001622
Iteration 41/1000 | Loss: 0.00001622
Iteration 42/1000 | Loss: 0.00001622
Iteration 43/1000 | Loss: 0.00001621
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001618
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001617
Iteration 49/1000 | Loss: 0.00001617
Iteration 50/1000 | Loss: 0.00001617
Iteration 51/1000 | Loss: 0.00001616
Iteration 52/1000 | Loss: 0.00001616
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001615
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001615
Iteration 59/1000 | Loss: 0.00001615
Iteration 60/1000 | Loss: 0.00001615
Iteration 61/1000 | Loss: 0.00001615
Iteration 62/1000 | Loss: 0.00001614
Iteration 63/1000 | Loss: 0.00001614
Iteration 64/1000 | Loss: 0.00001614
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001613
Iteration 67/1000 | Loss: 0.00001613
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001613
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001611
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001611
Iteration 92/1000 | Loss: 0.00001611
Iteration 93/1000 | Loss: 0.00001611
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001611
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001610
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001610
Iteration 108/1000 | Loss: 0.00001610
Iteration 109/1000 | Loss: 0.00001610
Iteration 110/1000 | Loss: 0.00001610
Iteration 111/1000 | Loss: 0.00001610
Iteration 112/1000 | Loss: 0.00001610
Iteration 113/1000 | Loss: 0.00001610
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001609
Iteration 116/1000 | Loss: 0.00001609
Iteration 117/1000 | Loss: 0.00001609
Iteration 118/1000 | Loss: 0.00001609
Iteration 119/1000 | Loss: 0.00001609
Iteration 120/1000 | Loss: 0.00001609
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001608
Iteration 128/1000 | Loss: 0.00001608
Iteration 129/1000 | Loss: 0.00001608
Iteration 130/1000 | Loss: 0.00001607
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001607
Iteration 133/1000 | Loss: 0.00001607
Iteration 134/1000 | Loss: 0.00001606
Iteration 135/1000 | Loss: 0.00001606
Iteration 136/1000 | Loss: 0.00001606
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001606
Iteration 139/1000 | Loss: 0.00001606
Iteration 140/1000 | Loss: 0.00001606
Iteration 141/1000 | Loss: 0.00001606
Iteration 142/1000 | Loss: 0.00001606
Iteration 143/1000 | Loss: 0.00001606
Iteration 144/1000 | Loss: 0.00001606
Iteration 145/1000 | Loss: 0.00001606
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.6056535969255492e-05, 1.6056535969255492e-05, 1.6056535969255492e-05, 1.6056535969255492e-05, 1.6056535969255492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6056535969255492e-05

Optimization complete. Final v2v error: 3.511441707611084 mm

Highest mean error: 3.7539029121398926 mm for frame 54

Lowest mean error: 3.1286027431488037 mm for frame 129

Saving results

Total time: 105.53548216819763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458323
Iteration 2/25 | Loss: 0.00100382
Iteration 3/25 | Loss: 0.00089580
Iteration 4/25 | Loss: 0.00087398
Iteration 5/25 | Loss: 0.00087022
Iteration 6/25 | Loss: 0.00086959
Iteration 7/25 | Loss: 0.00086959
Iteration 8/25 | Loss: 0.00086959
Iteration 9/25 | Loss: 0.00086959
Iteration 10/25 | Loss: 0.00086959
Iteration 11/25 | Loss: 0.00086959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008695948054082692, 0.0008695948054082692, 0.0008695948054082692, 0.0008695948054082692, 0.0008695948054082692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008695948054082692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40155947
Iteration 2/25 | Loss: 0.00029552
Iteration 3/25 | Loss: 0.00029551
Iteration 4/25 | Loss: 0.00029551
Iteration 5/25 | Loss: 0.00029551
Iteration 6/25 | Loss: 0.00029551
Iteration 7/25 | Loss: 0.00029551
Iteration 8/25 | Loss: 0.00029551
Iteration 9/25 | Loss: 0.00029551
Iteration 10/25 | Loss: 0.00029551
Iteration 11/25 | Loss: 0.00029551
Iteration 12/25 | Loss: 0.00029551
Iteration 13/25 | Loss: 0.00029551
Iteration 14/25 | Loss: 0.00029551
Iteration 15/25 | Loss: 0.00029551
Iteration 16/25 | Loss: 0.00029551
Iteration 17/25 | Loss: 0.00029551
Iteration 18/25 | Loss: 0.00029551
Iteration 19/25 | Loss: 0.00029551
Iteration 20/25 | Loss: 0.00029551
Iteration 21/25 | Loss: 0.00029551
Iteration 22/25 | Loss: 0.00029551
Iteration 23/25 | Loss: 0.00029551
Iteration 24/25 | Loss: 0.00029551
Iteration 25/25 | Loss: 0.00029551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029551
Iteration 2/1000 | Loss: 0.00003537
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001818
Iteration 5/1000 | Loss: 0.00001719
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001531
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001527
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001520
Iteration 31/1000 | Loss: 0.00001520
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001517
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001513
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001512
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001508
Iteration 83/1000 | Loss: 0.00001508
Iteration 84/1000 | Loss: 0.00001508
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001508
Iteration 89/1000 | Loss: 0.00001508
Iteration 90/1000 | Loss: 0.00001508
Iteration 91/1000 | Loss: 0.00001508
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001507
Iteration 94/1000 | Loss: 0.00001507
Iteration 95/1000 | Loss: 0.00001507
Iteration 96/1000 | Loss: 0.00001507
Iteration 97/1000 | Loss: 0.00001507
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001505
Iteration 117/1000 | Loss: 0.00001505
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001504
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001504
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001504
Iteration 134/1000 | Loss: 0.00001504
Iteration 135/1000 | Loss: 0.00001504
Iteration 136/1000 | Loss: 0.00001504
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001503
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001501
Iteration 153/1000 | Loss: 0.00001501
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001501
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001500
Iteration 163/1000 | Loss: 0.00001499
Iteration 164/1000 | Loss: 0.00001499
Iteration 165/1000 | Loss: 0.00001499
Iteration 166/1000 | Loss: 0.00001499
Iteration 167/1000 | Loss: 0.00001499
Iteration 168/1000 | Loss: 0.00001499
Iteration 169/1000 | Loss: 0.00001499
Iteration 170/1000 | Loss: 0.00001499
Iteration 171/1000 | Loss: 0.00001499
Iteration 172/1000 | Loss: 0.00001499
Iteration 173/1000 | Loss: 0.00001499
Iteration 174/1000 | Loss: 0.00001498
Iteration 175/1000 | Loss: 0.00001498
Iteration 176/1000 | Loss: 0.00001498
Iteration 177/1000 | Loss: 0.00001498
Iteration 178/1000 | Loss: 0.00001498
Iteration 179/1000 | Loss: 0.00001498
Iteration 180/1000 | Loss: 0.00001498
Iteration 181/1000 | Loss: 0.00001498
Iteration 182/1000 | Loss: 0.00001498
Iteration 183/1000 | Loss: 0.00001498
Iteration 184/1000 | Loss: 0.00001498
Iteration 185/1000 | Loss: 0.00001498
Iteration 186/1000 | Loss: 0.00001497
Iteration 187/1000 | Loss: 0.00001497
Iteration 188/1000 | Loss: 0.00001497
Iteration 189/1000 | Loss: 0.00001497
Iteration 190/1000 | Loss: 0.00001497
Iteration 191/1000 | Loss: 0.00001497
Iteration 192/1000 | Loss: 0.00001497
Iteration 193/1000 | Loss: 0.00001497
Iteration 194/1000 | Loss: 0.00001497
Iteration 195/1000 | Loss: 0.00001497
Iteration 196/1000 | Loss: 0.00001497
Iteration 197/1000 | Loss: 0.00001497
Iteration 198/1000 | Loss: 0.00001497
Iteration 199/1000 | Loss: 0.00001497
Iteration 200/1000 | Loss: 0.00001497
Iteration 201/1000 | Loss: 0.00001497
Iteration 202/1000 | Loss: 0.00001497
Iteration 203/1000 | Loss: 0.00001497
Iteration 204/1000 | Loss: 0.00001497
Iteration 205/1000 | Loss: 0.00001497
Iteration 206/1000 | Loss: 0.00001497
Iteration 207/1000 | Loss: 0.00001497
Iteration 208/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.496960612712428e-05, 1.496960612712428e-05, 1.496960612712428e-05, 1.496960612712428e-05, 1.496960612712428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.496960612712428e-05

Optimization complete. Final v2v error: 3.2960290908813477 mm

Highest mean error: 3.7312417030334473 mm for frame 119

Lowest mean error: 2.9393184185028076 mm for frame 0

Saving results

Total time: 85.88480758666992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861324
Iteration 2/25 | Loss: 0.00132651
Iteration 3/25 | Loss: 0.00102300
Iteration 4/25 | Loss: 0.00096833
Iteration 5/25 | Loss: 0.00096475
Iteration 6/25 | Loss: 0.00093708
Iteration 7/25 | Loss: 0.00092371
Iteration 8/25 | Loss: 0.00092046
Iteration 9/25 | Loss: 0.00091914
Iteration 10/25 | Loss: 0.00091865
Iteration 11/25 | Loss: 0.00092749
Iteration 12/25 | Loss: 0.00092053
Iteration 13/25 | Loss: 0.00091519
Iteration 14/25 | Loss: 0.00091192
Iteration 15/25 | Loss: 0.00091036
Iteration 16/25 | Loss: 0.00090990
Iteration 17/25 | Loss: 0.00091302
Iteration 18/25 | Loss: 0.00090888
Iteration 19/25 | Loss: 0.00090686
Iteration 20/25 | Loss: 0.00090598
Iteration 21/25 | Loss: 0.00090581
Iteration 22/25 | Loss: 0.00090580
Iteration 23/25 | Loss: 0.00090580
Iteration 24/25 | Loss: 0.00090579
Iteration 25/25 | Loss: 0.00090578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32652497
Iteration 2/25 | Loss: 0.00029110
Iteration 3/25 | Loss: 0.00029106
Iteration 4/25 | Loss: 0.00029106
Iteration 5/25 | Loss: 0.00029106
Iteration 6/25 | Loss: 0.00029106
Iteration 7/25 | Loss: 0.00029106
Iteration 8/25 | Loss: 0.00029106
Iteration 9/25 | Loss: 0.00029106
Iteration 10/25 | Loss: 0.00029106
Iteration 11/25 | Loss: 0.00029105
Iteration 12/25 | Loss: 0.00029105
Iteration 13/25 | Loss: 0.00029105
Iteration 14/25 | Loss: 0.00029105
Iteration 15/25 | Loss: 0.00029105
Iteration 16/25 | Loss: 0.00029105
Iteration 17/25 | Loss: 0.00029105
Iteration 18/25 | Loss: 0.00029105
Iteration 19/25 | Loss: 0.00029105
Iteration 20/25 | Loss: 0.00029105
Iteration 21/25 | Loss: 0.00029105
Iteration 22/25 | Loss: 0.00029105
Iteration 23/25 | Loss: 0.00029105
Iteration 24/25 | Loss: 0.00029105
Iteration 25/25 | Loss: 0.00029105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029105
Iteration 2/1000 | Loss: 0.00003865
Iteration 3/1000 | Loss: 0.00002342
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001700
Iteration 6/1000 | Loss: 0.00001609
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001414
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001399
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001382
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00001381
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001379
Iteration 49/1000 | Loss: 0.00001379
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001374
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001374
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001374
Iteration 103/1000 | Loss: 0.00001374
Iteration 104/1000 | Loss: 0.00001374
Iteration 105/1000 | Loss: 0.00001374
Iteration 106/1000 | Loss: 0.00001374
Iteration 107/1000 | Loss: 0.00001374
Iteration 108/1000 | Loss: 0.00001374
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.3742376722802874e-05, 1.3742376722802874e-05, 1.3742376722802874e-05, 1.3742376722802874e-05, 1.3742376722802874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3742376722802874e-05

Optimization complete. Final v2v error: 3.1871895790100098 mm

Highest mean error: 3.5885848999023438 mm for frame 75

Lowest mean error: 2.802971363067627 mm for frame 46

Saving results

Total time: 144.4180018901825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358986
Iteration 2/25 | Loss: 0.00085460
Iteration 3/25 | Loss: 0.00077222
Iteration 4/25 | Loss: 0.00076418
Iteration 5/25 | Loss: 0.00076150
Iteration 6/25 | Loss: 0.00076098
Iteration 7/25 | Loss: 0.00076098
Iteration 8/25 | Loss: 0.00076098
Iteration 9/25 | Loss: 0.00076098
Iteration 10/25 | Loss: 0.00076098
Iteration 11/25 | Loss: 0.00076098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007609849562868476, 0.0007609849562868476, 0.0007609849562868476, 0.0007609849562868476, 0.0007609849562868476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007609849562868476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49462557
Iteration 2/25 | Loss: 0.00032074
Iteration 3/25 | Loss: 0.00032074
Iteration 4/25 | Loss: 0.00032074
Iteration 5/25 | Loss: 0.00032074
Iteration 6/25 | Loss: 0.00032074
Iteration 7/25 | Loss: 0.00032074
Iteration 8/25 | Loss: 0.00032074
Iteration 9/25 | Loss: 0.00032074
Iteration 10/25 | Loss: 0.00032074
Iteration 11/25 | Loss: 0.00032074
Iteration 12/25 | Loss: 0.00032074
Iteration 13/25 | Loss: 0.00032074
Iteration 14/25 | Loss: 0.00032074
Iteration 15/25 | Loss: 0.00032074
Iteration 16/25 | Loss: 0.00032074
Iteration 17/25 | Loss: 0.00032074
Iteration 18/25 | Loss: 0.00032074
Iteration 19/25 | Loss: 0.00032074
Iteration 20/25 | Loss: 0.00032074
Iteration 21/25 | Loss: 0.00032074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000320735591230914, 0.000320735591230914, 0.000320735591230914, 0.000320735591230914, 0.000320735591230914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000320735591230914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032074
Iteration 2/1000 | Loss: 0.00001746
Iteration 3/1000 | Loss: 0.00000867
Iteration 4/1000 | Loss: 0.00000775
Iteration 5/1000 | Loss: 0.00000708
Iteration 6/1000 | Loss: 0.00000697
Iteration 7/1000 | Loss: 0.00000691
Iteration 8/1000 | Loss: 0.00000670
Iteration 9/1000 | Loss: 0.00000665
Iteration 10/1000 | Loss: 0.00000665
Iteration 11/1000 | Loss: 0.00000659
Iteration 12/1000 | Loss: 0.00000655
Iteration 13/1000 | Loss: 0.00000654
Iteration 14/1000 | Loss: 0.00000654
Iteration 15/1000 | Loss: 0.00000653
Iteration 16/1000 | Loss: 0.00000653
Iteration 17/1000 | Loss: 0.00000651
Iteration 18/1000 | Loss: 0.00000650
Iteration 19/1000 | Loss: 0.00000646
Iteration 20/1000 | Loss: 0.00000644
Iteration 21/1000 | Loss: 0.00000640
Iteration 22/1000 | Loss: 0.00000639
Iteration 23/1000 | Loss: 0.00000638
Iteration 24/1000 | Loss: 0.00000638
Iteration 25/1000 | Loss: 0.00000638
Iteration 26/1000 | Loss: 0.00000638
Iteration 27/1000 | Loss: 0.00000638
Iteration 28/1000 | Loss: 0.00000637
Iteration 29/1000 | Loss: 0.00000637
Iteration 30/1000 | Loss: 0.00000637
Iteration 31/1000 | Loss: 0.00000637
Iteration 32/1000 | Loss: 0.00000637
Iteration 33/1000 | Loss: 0.00000635
Iteration 34/1000 | Loss: 0.00000634
Iteration 35/1000 | Loss: 0.00000634
Iteration 36/1000 | Loss: 0.00000634
Iteration 37/1000 | Loss: 0.00000633
Iteration 38/1000 | Loss: 0.00000633
Iteration 39/1000 | Loss: 0.00000633
Iteration 40/1000 | Loss: 0.00000632
Iteration 41/1000 | Loss: 0.00000632
Iteration 42/1000 | Loss: 0.00000632
Iteration 43/1000 | Loss: 0.00000632
Iteration 44/1000 | Loss: 0.00000631
Iteration 45/1000 | Loss: 0.00000630
Iteration 46/1000 | Loss: 0.00000630
Iteration 47/1000 | Loss: 0.00000628
Iteration 48/1000 | Loss: 0.00000628
Iteration 49/1000 | Loss: 0.00000628
Iteration 50/1000 | Loss: 0.00000628
Iteration 51/1000 | Loss: 0.00000628
Iteration 52/1000 | Loss: 0.00000628
Iteration 53/1000 | Loss: 0.00000628
Iteration 54/1000 | Loss: 0.00000628
Iteration 55/1000 | Loss: 0.00000628
Iteration 56/1000 | Loss: 0.00000628
Iteration 57/1000 | Loss: 0.00000628
Iteration 58/1000 | Loss: 0.00000628
Iteration 59/1000 | Loss: 0.00000628
Iteration 60/1000 | Loss: 0.00000627
Iteration 61/1000 | Loss: 0.00000627
Iteration 62/1000 | Loss: 0.00000627
Iteration 63/1000 | Loss: 0.00000627
Iteration 64/1000 | Loss: 0.00000627
Iteration 65/1000 | Loss: 0.00000626
Iteration 66/1000 | Loss: 0.00000626
Iteration 67/1000 | Loss: 0.00000625
Iteration 68/1000 | Loss: 0.00000625
Iteration 69/1000 | Loss: 0.00000624
Iteration 70/1000 | Loss: 0.00000624
Iteration 71/1000 | Loss: 0.00000624
Iteration 72/1000 | Loss: 0.00000623
Iteration 73/1000 | Loss: 0.00000623
Iteration 74/1000 | Loss: 0.00000623
Iteration 75/1000 | Loss: 0.00000622
Iteration 76/1000 | Loss: 0.00000622
Iteration 77/1000 | Loss: 0.00000622
Iteration 78/1000 | Loss: 0.00000622
Iteration 79/1000 | Loss: 0.00000622
Iteration 80/1000 | Loss: 0.00000621
Iteration 81/1000 | Loss: 0.00000621
Iteration 82/1000 | Loss: 0.00000621
Iteration 83/1000 | Loss: 0.00000621
Iteration 84/1000 | Loss: 0.00000621
Iteration 85/1000 | Loss: 0.00000620
Iteration 86/1000 | Loss: 0.00000620
Iteration 87/1000 | Loss: 0.00000620
Iteration 88/1000 | Loss: 0.00000620
Iteration 89/1000 | Loss: 0.00000619
Iteration 90/1000 | Loss: 0.00000619
Iteration 91/1000 | Loss: 0.00000619
Iteration 92/1000 | Loss: 0.00000619
Iteration 93/1000 | Loss: 0.00000619
Iteration 94/1000 | Loss: 0.00000619
Iteration 95/1000 | Loss: 0.00000619
Iteration 96/1000 | Loss: 0.00000619
Iteration 97/1000 | Loss: 0.00000619
Iteration 98/1000 | Loss: 0.00000619
Iteration 99/1000 | Loss: 0.00000619
Iteration 100/1000 | Loss: 0.00000619
Iteration 101/1000 | Loss: 0.00000618
Iteration 102/1000 | Loss: 0.00000618
Iteration 103/1000 | Loss: 0.00000618
Iteration 104/1000 | Loss: 0.00000618
Iteration 105/1000 | Loss: 0.00000618
Iteration 106/1000 | Loss: 0.00000618
Iteration 107/1000 | Loss: 0.00000618
Iteration 108/1000 | Loss: 0.00000618
Iteration 109/1000 | Loss: 0.00000618
Iteration 110/1000 | Loss: 0.00000618
Iteration 111/1000 | Loss: 0.00000618
Iteration 112/1000 | Loss: 0.00000618
Iteration 113/1000 | Loss: 0.00000618
Iteration 114/1000 | Loss: 0.00000618
Iteration 115/1000 | Loss: 0.00000618
Iteration 116/1000 | Loss: 0.00000618
Iteration 117/1000 | Loss: 0.00000618
Iteration 118/1000 | Loss: 0.00000618
Iteration 119/1000 | Loss: 0.00000618
Iteration 120/1000 | Loss: 0.00000617
Iteration 121/1000 | Loss: 0.00000617
Iteration 122/1000 | Loss: 0.00000617
Iteration 123/1000 | Loss: 0.00000617
Iteration 124/1000 | Loss: 0.00000617
Iteration 125/1000 | Loss: 0.00000617
Iteration 126/1000 | Loss: 0.00000617
Iteration 127/1000 | Loss: 0.00000617
Iteration 128/1000 | Loss: 0.00000617
Iteration 129/1000 | Loss: 0.00000617
Iteration 130/1000 | Loss: 0.00000617
Iteration 131/1000 | Loss: 0.00000617
Iteration 132/1000 | Loss: 0.00000617
Iteration 133/1000 | Loss: 0.00000617
Iteration 134/1000 | Loss: 0.00000616
Iteration 135/1000 | Loss: 0.00000616
Iteration 136/1000 | Loss: 0.00000616
Iteration 137/1000 | Loss: 0.00000616
Iteration 138/1000 | Loss: 0.00000616
Iteration 139/1000 | Loss: 0.00000616
Iteration 140/1000 | Loss: 0.00000616
Iteration 141/1000 | Loss: 0.00000616
Iteration 142/1000 | Loss: 0.00000616
Iteration 143/1000 | Loss: 0.00000616
Iteration 144/1000 | Loss: 0.00000616
Iteration 145/1000 | Loss: 0.00000616
Iteration 146/1000 | Loss: 0.00000616
Iteration 147/1000 | Loss: 0.00000616
Iteration 148/1000 | Loss: 0.00000616
Iteration 149/1000 | Loss: 0.00000616
Iteration 150/1000 | Loss: 0.00000616
Iteration 151/1000 | Loss: 0.00000616
Iteration 152/1000 | Loss: 0.00000616
Iteration 153/1000 | Loss: 0.00000616
Iteration 154/1000 | Loss: 0.00000616
Iteration 155/1000 | Loss: 0.00000616
Iteration 156/1000 | Loss: 0.00000616
Iteration 157/1000 | Loss: 0.00000616
Iteration 158/1000 | Loss: 0.00000616
Iteration 159/1000 | Loss: 0.00000616
Iteration 160/1000 | Loss: 0.00000616
Iteration 161/1000 | Loss: 0.00000616
Iteration 162/1000 | Loss: 0.00000616
Iteration 163/1000 | Loss: 0.00000616
Iteration 164/1000 | Loss: 0.00000616
Iteration 165/1000 | Loss: 0.00000616
Iteration 166/1000 | Loss: 0.00000616
Iteration 167/1000 | Loss: 0.00000616
Iteration 168/1000 | Loss: 0.00000616
Iteration 169/1000 | Loss: 0.00000616
Iteration 170/1000 | Loss: 0.00000616
Iteration 171/1000 | Loss: 0.00000616
Iteration 172/1000 | Loss: 0.00000616
Iteration 173/1000 | Loss: 0.00000616
Iteration 174/1000 | Loss: 0.00000616
Iteration 175/1000 | Loss: 0.00000616
Iteration 176/1000 | Loss: 0.00000616
Iteration 177/1000 | Loss: 0.00000616
Iteration 178/1000 | Loss: 0.00000616
Iteration 179/1000 | Loss: 0.00000616
Iteration 180/1000 | Loss: 0.00000616
Iteration 181/1000 | Loss: 0.00000616
Iteration 182/1000 | Loss: 0.00000616
Iteration 183/1000 | Loss: 0.00000616
Iteration 184/1000 | Loss: 0.00000616
Iteration 185/1000 | Loss: 0.00000616
Iteration 186/1000 | Loss: 0.00000616
Iteration 187/1000 | Loss: 0.00000616
Iteration 188/1000 | Loss: 0.00000616
Iteration 189/1000 | Loss: 0.00000616
Iteration 190/1000 | Loss: 0.00000616
Iteration 191/1000 | Loss: 0.00000616
Iteration 192/1000 | Loss: 0.00000616
Iteration 193/1000 | Loss: 0.00000616
Iteration 194/1000 | Loss: 0.00000616
Iteration 195/1000 | Loss: 0.00000616
Iteration 196/1000 | Loss: 0.00000616
Iteration 197/1000 | Loss: 0.00000616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [6.1599134824064095e-06, 6.1599134824064095e-06, 6.1599134824064095e-06, 6.1599134824064095e-06, 6.1599134824064095e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.1599134824064095e-06

Optimization complete. Final v2v error: 2.1223325729370117 mm

Highest mean error: 2.315735340118408 mm for frame 131

Lowest mean error: 1.9201027154922485 mm for frame 155

Saving results

Total time: 74.04001259803772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474461
Iteration 2/25 | Loss: 0.00118850
Iteration 3/25 | Loss: 0.00099006
Iteration 4/25 | Loss: 0.00095439
Iteration 5/25 | Loss: 0.00094327
Iteration 6/25 | Loss: 0.00094061
Iteration 7/25 | Loss: 0.00094039
Iteration 8/25 | Loss: 0.00094039
Iteration 9/25 | Loss: 0.00094039
Iteration 10/25 | Loss: 0.00094039
Iteration 11/25 | Loss: 0.00094039
Iteration 12/25 | Loss: 0.00094039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009403896983712912, 0.0009403896983712912, 0.0009403896983712912, 0.0009403896983712912, 0.0009403896983712912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009403896983712912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27840233
Iteration 2/25 | Loss: 0.00045990
Iteration 3/25 | Loss: 0.00045990
Iteration 4/25 | Loss: 0.00045990
Iteration 5/25 | Loss: 0.00045990
Iteration 6/25 | Loss: 0.00045990
Iteration 7/25 | Loss: 0.00045990
Iteration 8/25 | Loss: 0.00045990
Iteration 9/25 | Loss: 0.00045990
Iteration 10/25 | Loss: 0.00045990
Iteration 11/25 | Loss: 0.00045990
Iteration 12/25 | Loss: 0.00045990
Iteration 13/25 | Loss: 0.00045990
Iteration 14/25 | Loss: 0.00045990
Iteration 15/25 | Loss: 0.00045990
Iteration 16/25 | Loss: 0.00045990
Iteration 17/25 | Loss: 0.00045990
Iteration 18/25 | Loss: 0.00045990
Iteration 19/25 | Loss: 0.00045990
Iteration 20/25 | Loss: 0.00045990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004598986415658146, 0.0004598986415658146, 0.0004598986415658146, 0.0004598986415658146, 0.0004598986415658146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004598986415658146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045990
Iteration 2/1000 | Loss: 0.00004149
Iteration 3/1000 | Loss: 0.00002361
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001799
Iteration 6/1000 | Loss: 0.00001695
Iteration 7/1000 | Loss: 0.00001633
Iteration 8/1000 | Loss: 0.00001584
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001465
Iteration 25/1000 | Loss: 0.00001464
Iteration 26/1000 | Loss: 0.00001464
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001463
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001462
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001458
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001457
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001455
Iteration 62/1000 | Loss: 0.00001455
Iteration 63/1000 | Loss: 0.00001455
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001453
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001453
Iteration 74/1000 | Loss: 0.00001453
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001452
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001450
Iteration 101/1000 | Loss: 0.00001450
Iteration 102/1000 | Loss: 0.00001450
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001449
Iteration 105/1000 | Loss: 0.00001449
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001449
Iteration 108/1000 | Loss: 0.00001449
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001448
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001447
Iteration 135/1000 | Loss: 0.00001447
Iteration 136/1000 | Loss: 0.00001447
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Iteration 139/1000 | Loss: 0.00001447
Iteration 140/1000 | Loss: 0.00001447
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.4474775525741279e-05, 1.4474775525741279e-05, 1.4474775525741279e-05, 1.4474775525741279e-05, 1.4474775525741279e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4474775525741279e-05

Optimization complete. Final v2v error: 3.2442328929901123 mm

Highest mean error: 4.142178535461426 mm for frame 66

Lowest mean error: 2.7187411785125732 mm for frame 91

Saving results

Total time: 111.72399473190308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805177
Iteration 2/25 | Loss: 0.00156010
Iteration 3/25 | Loss: 0.00107368
Iteration 4/25 | Loss: 0.00102629
Iteration 5/25 | Loss: 0.00102126
Iteration 6/25 | Loss: 0.00101911
Iteration 7/25 | Loss: 0.00101839
Iteration 8/25 | Loss: 0.00101839
Iteration 9/25 | Loss: 0.00101839
Iteration 10/25 | Loss: 0.00101839
Iteration 11/25 | Loss: 0.00101839
Iteration 12/25 | Loss: 0.00101839
Iteration 13/25 | Loss: 0.00101839
Iteration 14/25 | Loss: 0.00101839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00101838621776551, 0.00101838621776551, 0.00101838621776551, 0.00101838621776551, 0.00101838621776551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00101838621776551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29635239
Iteration 2/25 | Loss: 0.00044079
Iteration 3/25 | Loss: 0.00044079
Iteration 4/25 | Loss: 0.00044079
Iteration 5/25 | Loss: 0.00044079
Iteration 6/25 | Loss: 0.00044079
Iteration 7/25 | Loss: 0.00044079
Iteration 8/25 | Loss: 0.00044079
Iteration 9/25 | Loss: 0.00044079
Iteration 10/25 | Loss: 0.00044079
Iteration 11/25 | Loss: 0.00044079
Iteration 12/25 | Loss: 0.00044079
Iteration 13/25 | Loss: 0.00044079
Iteration 14/25 | Loss: 0.00044079
Iteration 15/25 | Loss: 0.00044079
Iteration 16/25 | Loss: 0.00044079
Iteration 17/25 | Loss: 0.00044079
Iteration 18/25 | Loss: 0.00044079
Iteration 19/25 | Loss: 0.00044079
Iteration 20/25 | Loss: 0.00044079
Iteration 21/25 | Loss: 0.00044079
Iteration 22/25 | Loss: 0.00044079
Iteration 23/25 | Loss: 0.00044079
Iteration 24/25 | Loss: 0.00044079
Iteration 25/25 | Loss: 0.00044079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044079
Iteration 2/1000 | Loss: 0.00006054
Iteration 3/1000 | Loss: 0.00003167
Iteration 4/1000 | Loss: 0.00002733
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002414
Iteration 7/1000 | Loss: 0.00002318
Iteration 8/1000 | Loss: 0.00002249
Iteration 9/1000 | Loss: 0.00002192
Iteration 10/1000 | Loss: 0.00002152
Iteration 11/1000 | Loss: 0.00002129
Iteration 12/1000 | Loss: 0.00002109
Iteration 13/1000 | Loss: 0.00002101
Iteration 14/1000 | Loss: 0.00002089
Iteration 15/1000 | Loss: 0.00002085
Iteration 16/1000 | Loss: 0.00002085
Iteration 17/1000 | Loss: 0.00002083
Iteration 18/1000 | Loss: 0.00002083
Iteration 19/1000 | Loss: 0.00002083
Iteration 20/1000 | Loss: 0.00002082
Iteration 21/1000 | Loss: 0.00002082
Iteration 22/1000 | Loss: 0.00002082
Iteration 23/1000 | Loss: 0.00002082
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002081
Iteration 29/1000 | Loss: 0.00002081
Iteration 30/1000 | Loss: 0.00002081
Iteration 31/1000 | Loss: 0.00002081
Iteration 32/1000 | Loss: 0.00002081
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002080
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002080
Iteration 38/1000 | Loss: 0.00002079
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002078
Iteration 43/1000 | Loss: 0.00002078
Iteration 44/1000 | Loss: 0.00002078
Iteration 45/1000 | Loss: 0.00002077
Iteration 46/1000 | Loss: 0.00002077
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002074
Iteration 55/1000 | Loss: 0.00002074
Iteration 56/1000 | Loss: 0.00002074
Iteration 57/1000 | Loss: 0.00002074
Iteration 58/1000 | Loss: 0.00002073
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002071
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002070
Iteration 68/1000 | Loss: 0.00002070
Iteration 69/1000 | Loss: 0.00002070
Iteration 70/1000 | Loss: 0.00002070
Iteration 71/1000 | Loss: 0.00002070
Iteration 72/1000 | Loss: 0.00002070
Iteration 73/1000 | Loss: 0.00002070
Iteration 74/1000 | Loss: 0.00002070
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002069
Iteration 77/1000 | Loss: 0.00002069
Iteration 78/1000 | Loss: 0.00002069
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002068
Iteration 82/1000 | Loss: 0.00002067
Iteration 83/1000 | Loss: 0.00002067
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002066
Iteration 86/1000 | Loss: 0.00002066
Iteration 87/1000 | Loss: 0.00002066
Iteration 88/1000 | Loss: 0.00002066
Iteration 89/1000 | Loss: 0.00002066
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002065
Iteration 92/1000 | Loss: 0.00002065
Iteration 93/1000 | Loss: 0.00002065
Iteration 94/1000 | Loss: 0.00002064
Iteration 95/1000 | Loss: 0.00002064
Iteration 96/1000 | Loss: 0.00002064
Iteration 97/1000 | Loss: 0.00002064
Iteration 98/1000 | Loss: 0.00002064
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002063
Iteration 105/1000 | Loss: 0.00002063
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00002062
Iteration 115/1000 | Loss: 0.00002061
Iteration 116/1000 | Loss: 0.00002061
Iteration 117/1000 | Loss: 0.00002061
Iteration 118/1000 | Loss: 0.00002061
Iteration 119/1000 | Loss: 0.00002061
Iteration 120/1000 | Loss: 0.00002061
Iteration 121/1000 | Loss: 0.00002060
Iteration 122/1000 | Loss: 0.00002060
Iteration 123/1000 | Loss: 0.00002060
Iteration 124/1000 | Loss: 0.00002060
Iteration 125/1000 | Loss: 0.00002060
Iteration 126/1000 | Loss: 0.00002060
Iteration 127/1000 | Loss: 0.00002060
Iteration 128/1000 | Loss: 0.00002060
Iteration 129/1000 | Loss: 0.00002060
Iteration 130/1000 | Loss: 0.00002060
Iteration 131/1000 | Loss: 0.00002060
Iteration 132/1000 | Loss: 0.00002060
Iteration 133/1000 | Loss: 0.00002060
Iteration 134/1000 | Loss: 0.00002060
Iteration 135/1000 | Loss: 0.00002060
Iteration 136/1000 | Loss: 0.00002060
Iteration 137/1000 | Loss: 0.00002060
Iteration 138/1000 | Loss: 0.00002059
Iteration 139/1000 | Loss: 0.00002059
Iteration 140/1000 | Loss: 0.00002059
Iteration 141/1000 | Loss: 0.00002059
Iteration 142/1000 | Loss: 0.00002059
Iteration 143/1000 | Loss: 0.00002059
Iteration 144/1000 | Loss: 0.00002059
Iteration 145/1000 | Loss: 0.00002059
Iteration 146/1000 | Loss: 0.00002059
Iteration 147/1000 | Loss: 0.00002059
Iteration 148/1000 | Loss: 0.00002059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.0594638044713065e-05, 2.0594638044713065e-05, 2.0594638044713065e-05, 2.0594638044713065e-05, 2.0594638044713065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0594638044713065e-05

Optimization complete. Final v2v error: 3.8707375526428223 mm

Highest mean error: 4.58241081237793 mm for frame 47

Lowest mean error: 3.127955675125122 mm for frame 184

Saving results

Total time: 96.78311848640442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090396
Iteration 2/25 | Loss: 0.00258267
Iteration 3/25 | Loss: 0.00203365
Iteration 4/25 | Loss: 0.00192569
Iteration 5/25 | Loss: 0.00192147
Iteration 6/25 | Loss: 0.00176941
Iteration 7/25 | Loss: 0.00163874
Iteration 8/25 | Loss: 0.00154199
Iteration 9/25 | Loss: 0.00146971
Iteration 10/25 | Loss: 0.00139379
Iteration 11/25 | Loss: 0.00136011
Iteration 12/25 | Loss: 0.00132938
Iteration 13/25 | Loss: 0.00131254
Iteration 14/25 | Loss: 0.00128294
Iteration 15/25 | Loss: 0.00127207
Iteration 16/25 | Loss: 0.00127470
Iteration 17/25 | Loss: 0.00127216
Iteration 18/25 | Loss: 0.00126205
Iteration 19/25 | Loss: 0.00126203
Iteration 20/25 | Loss: 0.00125750
Iteration 21/25 | Loss: 0.00125547
Iteration 22/25 | Loss: 0.00125772
Iteration 23/25 | Loss: 0.00125431
Iteration 24/25 | Loss: 0.00125232
Iteration 25/25 | Loss: 0.00125191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33494234
Iteration 2/25 | Loss: 0.00472460
Iteration 3/25 | Loss: 0.00472460
Iteration 4/25 | Loss: 0.00472460
Iteration 5/25 | Loss: 0.00472460
Iteration 6/25 | Loss: 0.00472460
Iteration 7/25 | Loss: 0.00472460
Iteration 8/25 | Loss: 0.00472460
Iteration 9/25 | Loss: 0.00472460
Iteration 10/25 | Loss: 0.00472460
Iteration 11/25 | Loss: 0.00472460
Iteration 12/25 | Loss: 0.00472460
Iteration 13/25 | Loss: 0.00472460
Iteration 14/25 | Loss: 0.00472460
Iteration 15/25 | Loss: 0.00472460
Iteration 16/25 | Loss: 0.00472460
Iteration 17/25 | Loss: 0.00472460
Iteration 18/25 | Loss: 0.00472460
Iteration 19/25 | Loss: 0.00472460
Iteration 20/25 | Loss: 0.00472460
Iteration 21/25 | Loss: 0.00472460
Iteration 22/25 | Loss: 0.00472460
Iteration 23/25 | Loss: 0.00472460
Iteration 24/25 | Loss: 0.00472460
Iteration 25/25 | Loss: 0.00472460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00472460
Iteration 2/1000 | Loss: 0.00064036
Iteration 3/1000 | Loss: 0.00043101
Iteration 4/1000 | Loss: 0.00035324
Iteration 5/1000 | Loss: 0.00030714
Iteration 6/1000 | Loss: 0.00028135
Iteration 7/1000 | Loss: 0.00025485
Iteration 8/1000 | Loss: 0.00024307
Iteration 9/1000 | Loss: 0.00023809
Iteration 10/1000 | Loss: 0.00023191
Iteration 11/1000 | Loss: 0.00022897
Iteration 12/1000 | Loss: 0.00022732
Iteration 13/1000 | Loss: 0.00022640
Iteration 14/1000 | Loss: 0.00022546
Iteration 15/1000 | Loss: 0.00022474
Iteration 16/1000 | Loss: 0.00022415
Iteration 17/1000 | Loss: 0.00022377
Iteration 18/1000 | Loss: 0.00022347
Iteration 19/1000 | Loss: 0.00022330
Iteration 20/1000 | Loss: 0.00022322
Iteration 21/1000 | Loss: 0.00022318
Iteration 22/1000 | Loss: 0.00022318
Iteration 23/1000 | Loss: 0.00022316
Iteration 24/1000 | Loss: 0.00022316
Iteration 25/1000 | Loss: 0.00022316
Iteration 26/1000 | Loss: 0.00022316
Iteration 27/1000 | Loss: 0.00022316
Iteration 28/1000 | Loss: 0.00022316
Iteration 29/1000 | Loss: 0.00022316
Iteration 30/1000 | Loss: 0.00022314
Iteration 31/1000 | Loss: 0.00022314
Iteration 32/1000 | Loss: 0.00022313
Iteration 33/1000 | Loss: 0.00022313
Iteration 34/1000 | Loss: 0.00022313
Iteration 35/1000 | Loss: 0.00022313
Iteration 36/1000 | Loss: 0.00022313
Iteration 37/1000 | Loss: 0.00022313
Iteration 38/1000 | Loss: 0.00022312
Iteration 39/1000 | Loss: 0.00022312
Iteration 40/1000 | Loss: 0.00022308
Iteration 41/1000 | Loss: 0.00022308
Iteration 42/1000 | Loss: 0.00022308
Iteration 43/1000 | Loss: 0.00022308
Iteration 44/1000 | Loss: 0.00022307
Iteration 45/1000 | Loss: 0.00022307
Iteration 46/1000 | Loss: 0.00022307
Iteration 47/1000 | Loss: 0.00022307
Iteration 48/1000 | Loss: 0.00022307
Iteration 49/1000 | Loss: 0.00022307
Iteration 50/1000 | Loss: 0.00022307
Iteration 51/1000 | Loss: 0.00022307
Iteration 52/1000 | Loss: 0.00022305
Iteration 53/1000 | Loss: 0.00022305
Iteration 54/1000 | Loss: 0.00022305
Iteration 55/1000 | Loss: 0.00022305
Iteration 56/1000 | Loss: 0.00022305
Iteration 57/1000 | Loss: 0.00022305
Iteration 58/1000 | Loss: 0.00022305
Iteration 59/1000 | Loss: 0.00022304
Iteration 60/1000 | Loss: 0.00022304
Iteration 61/1000 | Loss: 0.00022304
Iteration 62/1000 | Loss: 0.00022304
Iteration 63/1000 | Loss: 0.00022304
Iteration 64/1000 | Loss: 0.00022304
Iteration 65/1000 | Loss: 0.00022304
Iteration 66/1000 | Loss: 0.00022304
Iteration 67/1000 | Loss: 0.00022304
Iteration 68/1000 | Loss: 0.00022304
Iteration 69/1000 | Loss: 0.00022304
Iteration 70/1000 | Loss: 0.00022304
Iteration 71/1000 | Loss: 0.00022304
Iteration 72/1000 | Loss: 0.00022304
Iteration 73/1000 | Loss: 0.00022304
Iteration 74/1000 | Loss: 0.00022304
Iteration 75/1000 | Loss: 0.00022304
Iteration 76/1000 | Loss: 0.00022304
Iteration 77/1000 | Loss: 0.00022304
Iteration 78/1000 | Loss: 0.00022304
Iteration 79/1000 | Loss: 0.00022304
Iteration 80/1000 | Loss: 0.00022304
Iteration 81/1000 | Loss: 0.00022304
Iteration 82/1000 | Loss: 0.00022304
Iteration 83/1000 | Loss: 0.00022304
Iteration 84/1000 | Loss: 0.00022304
Iteration 85/1000 | Loss: 0.00022304
Iteration 86/1000 | Loss: 0.00022304
Iteration 87/1000 | Loss: 0.00022304
Iteration 88/1000 | Loss: 0.00022304
Iteration 89/1000 | Loss: 0.00022304
Iteration 90/1000 | Loss: 0.00022304
Iteration 91/1000 | Loss: 0.00022304
Iteration 92/1000 | Loss: 0.00022304
Iteration 93/1000 | Loss: 0.00022304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [0.00022303618607111275, 0.00022303618607111275, 0.00022303618607111275, 0.00022303618607111275, 0.00022303618607111275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022303618607111275

Optimization complete. Final v2v error: 7.002356052398682 mm

Highest mean error: 12.210661888122559 mm for frame 57

Lowest mean error: 3.615973472595215 mm for frame 68

Saving results

Total time: 232.09615659713745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085323
Iteration 2/25 | Loss: 0.01085323
Iteration 3/25 | Loss: 0.00212416
Iteration 4/25 | Loss: 0.00115598
Iteration 5/25 | Loss: 0.00102115
Iteration 6/25 | Loss: 0.00099290
Iteration 7/25 | Loss: 0.00096306
Iteration 8/25 | Loss: 0.00092094
Iteration 9/25 | Loss: 0.00088837
Iteration 10/25 | Loss: 0.00086915
Iteration 11/25 | Loss: 0.00086634
Iteration 12/25 | Loss: 0.00086181
Iteration 13/25 | Loss: 0.00086585
Iteration 14/25 | Loss: 0.00085284
Iteration 15/25 | Loss: 0.00085103
Iteration 16/25 | Loss: 0.00085299
Iteration 17/25 | Loss: 0.00085061
Iteration 18/25 | Loss: 0.00085173
Iteration 19/25 | Loss: 0.00085133
Iteration 20/25 | Loss: 0.00085046
Iteration 21/25 | Loss: 0.00085046
Iteration 22/25 | Loss: 0.00085046
Iteration 23/25 | Loss: 0.00085046
Iteration 24/25 | Loss: 0.00085046
Iteration 25/25 | Loss: 0.00085046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48739731
Iteration 2/25 | Loss: 0.00058431
Iteration 3/25 | Loss: 0.00058431
Iteration 4/25 | Loss: 0.00058431
Iteration 5/25 | Loss: 0.00057800
Iteration 6/25 | Loss: 0.00057800
Iteration 7/25 | Loss: 0.00057800
Iteration 8/25 | Loss: 0.00057800
Iteration 9/25 | Loss: 0.00057800
Iteration 10/25 | Loss: 0.00057800
Iteration 11/25 | Loss: 0.00057800
Iteration 12/25 | Loss: 0.00057800
Iteration 13/25 | Loss: 0.00057800
Iteration 14/25 | Loss: 0.00057800
Iteration 15/25 | Loss: 0.00057800
Iteration 16/25 | Loss: 0.00057800
Iteration 17/25 | Loss: 0.00057800
Iteration 18/25 | Loss: 0.00057800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005780014907941222, 0.0005780014907941222, 0.0005780014907941222, 0.0005780014907941222, 0.0005780014907941222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005780014907941222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057800
Iteration 2/1000 | Loss: 0.00020364
Iteration 3/1000 | Loss: 0.00002839
Iteration 4/1000 | Loss: 0.00019783
Iteration 5/1000 | Loss: 0.00006769
Iteration 6/1000 | Loss: 0.00006811
Iteration 7/1000 | Loss: 0.00007103
Iteration 8/1000 | Loss: 0.00009815
Iteration 9/1000 | Loss: 0.00002853
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00091658
Iteration 12/1000 | Loss: 0.00035309
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00007516
Iteration 15/1000 | Loss: 0.00003149
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001145
Iteration 18/1000 | Loss: 0.00001109
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001098
Iteration 22/1000 | Loss: 0.00001096
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001085
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001074
Iteration 39/1000 | Loss: 0.00001073
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001072
Iteration 45/1000 | Loss: 0.00001072
Iteration 46/1000 | Loss: 0.00003192
Iteration 47/1000 | Loss: 0.00003192
Iteration 48/1000 | Loss: 0.00014809
Iteration 49/1000 | Loss: 0.00003769
Iteration 50/1000 | Loss: 0.00011620
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001054
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001054
Iteration 58/1000 | Loss: 0.00001054
Iteration 59/1000 | Loss: 0.00001054
Iteration 60/1000 | Loss: 0.00001054
Iteration 61/1000 | Loss: 0.00001053
Iteration 62/1000 | Loss: 0.00001053
Iteration 63/1000 | Loss: 0.00001053
Iteration 64/1000 | Loss: 0.00001053
Iteration 65/1000 | Loss: 0.00001053
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00003530
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001050
Iteration 76/1000 | Loss: 0.00001050
Iteration 77/1000 | Loss: 0.00001049
Iteration 78/1000 | Loss: 0.00001049
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001049
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001049
Iteration 86/1000 | Loss: 0.00001049
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001048
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001047
Iteration 104/1000 | Loss: 0.00001047
Iteration 105/1000 | Loss: 0.00001047
Iteration 106/1000 | Loss: 0.00001047
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001046
Iteration 118/1000 | Loss: 0.00001046
Iteration 119/1000 | Loss: 0.00001045
Iteration 120/1000 | Loss: 0.00001045
Iteration 121/1000 | Loss: 0.00001045
Iteration 122/1000 | Loss: 0.00001045
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001045
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00002824
Iteration 127/1000 | Loss: 0.00002824
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001043
Iteration 133/1000 | Loss: 0.00001043
Iteration 134/1000 | Loss: 0.00001043
Iteration 135/1000 | Loss: 0.00001043
Iteration 136/1000 | Loss: 0.00001043
Iteration 137/1000 | Loss: 0.00001043
Iteration 138/1000 | Loss: 0.00001043
Iteration 139/1000 | Loss: 0.00001043
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001042
Iteration 142/1000 | Loss: 0.00001042
Iteration 143/1000 | Loss: 0.00001041
Iteration 144/1000 | Loss: 0.00001041
Iteration 145/1000 | Loss: 0.00001041
Iteration 146/1000 | Loss: 0.00001041
Iteration 147/1000 | Loss: 0.00001041
Iteration 148/1000 | Loss: 0.00001041
Iteration 149/1000 | Loss: 0.00001041
Iteration 150/1000 | Loss: 0.00001041
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001041
Iteration 160/1000 | Loss: 0.00001041
Iteration 161/1000 | Loss: 0.00001041
Iteration 162/1000 | Loss: 0.00001041
Iteration 163/1000 | Loss: 0.00001041
Iteration 164/1000 | Loss: 0.00001041
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001041
Iteration 167/1000 | Loss: 0.00001041
Iteration 168/1000 | Loss: 0.00001041
Iteration 169/1000 | Loss: 0.00001041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.0406571163912304e-05, 1.0406571163912304e-05, 1.0406571163912304e-05, 1.0406571163912304e-05, 1.0406571163912304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0406571163912304e-05

Optimization complete. Final v2v error: 2.6532034873962402 mm

Highest mean error: 8.916497230529785 mm for frame 67

Lowest mean error: 2.3422908782958984 mm for frame 46

Saving results

Total time: 190.96005249023438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419599
Iteration 2/25 | Loss: 0.00091903
Iteration 3/25 | Loss: 0.00081350
Iteration 4/25 | Loss: 0.00080276
Iteration 5/25 | Loss: 0.00080009
Iteration 6/25 | Loss: 0.00080006
Iteration 7/25 | Loss: 0.00080006
Iteration 8/25 | Loss: 0.00080006
Iteration 9/25 | Loss: 0.00080006
Iteration 10/25 | Loss: 0.00080004
Iteration 11/25 | Loss: 0.00080004
Iteration 12/25 | Loss: 0.00080004
Iteration 13/25 | Loss: 0.00080004
Iteration 14/25 | Loss: 0.00080004
Iteration 15/25 | Loss: 0.00080004
Iteration 16/25 | Loss: 0.00080004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008000360685400665, 0.0008000360685400665, 0.0008000360685400665, 0.0008000360685400665, 0.0008000360685400665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008000360685400665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53121614
Iteration 2/25 | Loss: 0.00031080
Iteration 3/25 | Loss: 0.00031080
Iteration 4/25 | Loss: 0.00031080
Iteration 5/25 | Loss: 0.00031080
Iteration 6/25 | Loss: 0.00031080
Iteration 7/25 | Loss: 0.00031080
Iteration 8/25 | Loss: 0.00031080
Iteration 9/25 | Loss: 0.00031080
Iteration 10/25 | Loss: 0.00031080
Iteration 11/25 | Loss: 0.00031080
Iteration 12/25 | Loss: 0.00031080
Iteration 13/25 | Loss: 0.00031080
Iteration 14/25 | Loss: 0.00031080
Iteration 15/25 | Loss: 0.00031080
Iteration 16/25 | Loss: 0.00031080
Iteration 17/25 | Loss: 0.00031080
Iteration 18/25 | Loss: 0.00031080
Iteration 19/25 | Loss: 0.00031080
Iteration 20/25 | Loss: 0.00031080
Iteration 21/25 | Loss: 0.00031080
Iteration 22/25 | Loss: 0.00031080
Iteration 23/25 | Loss: 0.00031080
Iteration 24/25 | Loss: 0.00031080
Iteration 25/25 | Loss: 0.00031080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031080
Iteration 2/1000 | Loss: 0.00001655
Iteration 3/1000 | Loss: 0.00001128
Iteration 4/1000 | Loss: 0.00001015
Iteration 5/1000 | Loss: 0.00000959
Iteration 6/1000 | Loss: 0.00000925
Iteration 7/1000 | Loss: 0.00000907
Iteration 8/1000 | Loss: 0.00000896
Iteration 9/1000 | Loss: 0.00000895
Iteration 10/1000 | Loss: 0.00000894
Iteration 11/1000 | Loss: 0.00000894
Iteration 12/1000 | Loss: 0.00000893
Iteration 13/1000 | Loss: 0.00000885
Iteration 14/1000 | Loss: 0.00000882
Iteration 15/1000 | Loss: 0.00000881
Iteration 16/1000 | Loss: 0.00000881
Iteration 17/1000 | Loss: 0.00000881
Iteration 18/1000 | Loss: 0.00000881
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000881
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000878
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000877
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000877
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000876
Iteration 35/1000 | Loss: 0.00000876
Iteration 36/1000 | Loss: 0.00000876
Iteration 37/1000 | Loss: 0.00000876
Iteration 38/1000 | Loss: 0.00000876
Iteration 39/1000 | Loss: 0.00000876
Iteration 40/1000 | Loss: 0.00000875
Iteration 41/1000 | Loss: 0.00000875
Iteration 42/1000 | Loss: 0.00000874
Iteration 43/1000 | Loss: 0.00000874
Iteration 44/1000 | Loss: 0.00000874
Iteration 45/1000 | Loss: 0.00000874
Iteration 46/1000 | Loss: 0.00000874
Iteration 47/1000 | Loss: 0.00000874
Iteration 48/1000 | Loss: 0.00000874
Iteration 49/1000 | Loss: 0.00000874
Iteration 50/1000 | Loss: 0.00000874
Iteration 51/1000 | Loss: 0.00000874
Iteration 52/1000 | Loss: 0.00000874
Iteration 53/1000 | Loss: 0.00000874
Iteration 54/1000 | Loss: 0.00000873
Iteration 55/1000 | Loss: 0.00000873
Iteration 56/1000 | Loss: 0.00000873
Iteration 57/1000 | Loss: 0.00000873
Iteration 58/1000 | Loss: 0.00000873
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000873
Iteration 63/1000 | Loss: 0.00000873
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000872
Iteration 66/1000 | Loss: 0.00000872
Iteration 67/1000 | Loss: 0.00000872
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000872
Iteration 70/1000 | Loss: 0.00000872
Iteration 71/1000 | Loss: 0.00000872
Iteration 72/1000 | Loss: 0.00000872
Iteration 73/1000 | Loss: 0.00000872
Iteration 74/1000 | Loss: 0.00000872
Iteration 75/1000 | Loss: 0.00000872
Iteration 76/1000 | Loss: 0.00000872
Iteration 77/1000 | Loss: 0.00000872
Iteration 78/1000 | Loss: 0.00000872
Iteration 79/1000 | Loss: 0.00000872
Iteration 80/1000 | Loss: 0.00000871
Iteration 81/1000 | Loss: 0.00000871
Iteration 82/1000 | Loss: 0.00000871
Iteration 83/1000 | Loss: 0.00000871
Iteration 84/1000 | Loss: 0.00000871
Iteration 85/1000 | Loss: 0.00000871
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000871
Iteration 91/1000 | Loss: 0.00000871
Iteration 92/1000 | Loss: 0.00000870
Iteration 93/1000 | Loss: 0.00000870
Iteration 94/1000 | Loss: 0.00000870
Iteration 95/1000 | Loss: 0.00000870
Iteration 96/1000 | Loss: 0.00000870
Iteration 97/1000 | Loss: 0.00000870
Iteration 98/1000 | Loss: 0.00000870
Iteration 99/1000 | Loss: 0.00000870
Iteration 100/1000 | Loss: 0.00000870
Iteration 101/1000 | Loss: 0.00000869
Iteration 102/1000 | Loss: 0.00000869
Iteration 103/1000 | Loss: 0.00000869
Iteration 104/1000 | Loss: 0.00000869
Iteration 105/1000 | Loss: 0.00000869
Iteration 106/1000 | Loss: 0.00000869
Iteration 107/1000 | Loss: 0.00000869
Iteration 108/1000 | Loss: 0.00000869
Iteration 109/1000 | Loss: 0.00000869
Iteration 110/1000 | Loss: 0.00000869
Iteration 111/1000 | Loss: 0.00000869
Iteration 112/1000 | Loss: 0.00000869
Iteration 113/1000 | Loss: 0.00000869
Iteration 114/1000 | Loss: 0.00000869
Iteration 115/1000 | Loss: 0.00000869
Iteration 116/1000 | Loss: 0.00000869
Iteration 117/1000 | Loss: 0.00000869
Iteration 118/1000 | Loss: 0.00000869
Iteration 119/1000 | Loss: 0.00000869
Iteration 120/1000 | Loss: 0.00000869
Iteration 121/1000 | Loss: 0.00000869
Iteration 122/1000 | Loss: 0.00000869
Iteration 123/1000 | Loss: 0.00000869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [8.689908099768218e-06, 8.689908099768218e-06, 8.689908099768218e-06, 8.689908099768218e-06, 8.689908099768218e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.689908099768218e-06

Optimization complete. Final v2v error: 2.521397113800049 mm

Highest mean error: 2.7719006538391113 mm for frame 219

Lowest mean error: 2.2384321689605713 mm for frame 0

Saving results

Total time: 84.33763933181763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109787
Iteration 2/25 | Loss: 0.00267834
Iteration 3/25 | Loss: 0.00226152
Iteration 4/25 | Loss: 0.00210068
Iteration 5/25 | Loss: 0.00157202
Iteration 6/25 | Loss: 0.00141996
Iteration 7/25 | Loss: 0.00141014
Iteration 8/25 | Loss: 0.00128065
Iteration 9/25 | Loss: 0.00112675
Iteration 10/25 | Loss: 0.00108173
Iteration 11/25 | Loss: 0.00107746
Iteration 12/25 | Loss: 0.00105875
Iteration 13/25 | Loss: 0.00103866
Iteration 14/25 | Loss: 0.00102661
Iteration 15/25 | Loss: 0.00102205
Iteration 16/25 | Loss: 0.00101642
Iteration 17/25 | Loss: 0.00102441
Iteration 18/25 | Loss: 0.00103417
Iteration 19/25 | Loss: 0.00102777
Iteration 20/25 | Loss: 0.00102727
Iteration 21/25 | Loss: 0.00100228
Iteration 22/25 | Loss: 0.00099312
Iteration 23/25 | Loss: 0.00098456
Iteration 24/25 | Loss: 0.00099990
Iteration 25/25 | Loss: 0.00100341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46455514
Iteration 2/25 | Loss: 0.00164650
Iteration 3/25 | Loss: 0.00117274
Iteration 4/25 | Loss: 0.00117274
Iteration 5/25 | Loss: 0.00117274
Iteration 6/25 | Loss: 0.00117274
Iteration 7/25 | Loss: 0.00117274
Iteration 8/25 | Loss: 0.00117274
Iteration 9/25 | Loss: 0.00117274
Iteration 10/25 | Loss: 0.00117274
Iteration 11/25 | Loss: 0.00117274
Iteration 12/25 | Loss: 0.00117274
Iteration 13/25 | Loss: 0.00117274
Iteration 14/25 | Loss: 0.00117274
Iteration 15/25 | Loss: 0.00117274
Iteration 16/25 | Loss: 0.00117274
Iteration 17/25 | Loss: 0.00117274
Iteration 18/25 | Loss: 0.00117274
Iteration 19/25 | Loss: 0.00117274
Iteration 20/25 | Loss: 0.00117274
Iteration 21/25 | Loss: 0.00117274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011727351229637861, 0.0011727351229637861, 0.0011727351229637861, 0.0011727351229637861, 0.0011727351229637861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011727351229637861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117274
Iteration 2/1000 | Loss: 0.00384262
Iteration 3/1000 | Loss: 0.00314585
Iteration 4/1000 | Loss: 0.00067696
Iteration 5/1000 | Loss: 0.00098377
Iteration 6/1000 | Loss: 0.00079192
Iteration 7/1000 | Loss: 0.00058636
Iteration 8/1000 | Loss: 0.00046653
Iteration 9/1000 | Loss: 0.00035933
Iteration 10/1000 | Loss: 0.00043918
Iteration 11/1000 | Loss: 0.00189021
Iteration 12/1000 | Loss: 0.00061571
Iteration 13/1000 | Loss: 0.00029766
Iteration 14/1000 | Loss: 0.00062685
Iteration 15/1000 | Loss: 0.00045998
Iteration 16/1000 | Loss: 0.00061017
Iteration 17/1000 | Loss: 0.00151816
Iteration 18/1000 | Loss: 0.00059435
Iteration 19/1000 | Loss: 0.00036999
Iteration 20/1000 | Loss: 0.00047296
Iteration 21/1000 | Loss: 0.00030908
Iteration 22/1000 | Loss: 0.00049468
Iteration 23/1000 | Loss: 0.00104822
Iteration 24/1000 | Loss: 0.00043974
Iteration 25/1000 | Loss: 0.00031858
Iteration 26/1000 | Loss: 0.00026893
Iteration 27/1000 | Loss: 0.00024344
Iteration 28/1000 | Loss: 0.00036388
Iteration 29/1000 | Loss: 0.00094397
Iteration 30/1000 | Loss: 0.00054528
Iteration 31/1000 | Loss: 0.00030660
Iteration 32/1000 | Loss: 0.00037731
Iteration 33/1000 | Loss: 0.00033251
Iteration 34/1000 | Loss: 0.00110860
Iteration 35/1000 | Loss: 0.00188745
Iteration 36/1000 | Loss: 0.00077580
Iteration 37/1000 | Loss: 0.00178454
Iteration 38/1000 | Loss: 0.00113295
Iteration 39/1000 | Loss: 0.00139890
Iteration 40/1000 | Loss: 0.00083790
Iteration 41/1000 | Loss: 0.00062341
Iteration 42/1000 | Loss: 0.00037410
Iteration 43/1000 | Loss: 0.00027571
Iteration 44/1000 | Loss: 0.00035354
Iteration 45/1000 | Loss: 0.00025590
Iteration 46/1000 | Loss: 0.00027685
Iteration 47/1000 | Loss: 0.00021663
Iteration 48/1000 | Loss: 0.00044066
Iteration 49/1000 | Loss: 0.00027566
Iteration 50/1000 | Loss: 0.00012256
Iteration 51/1000 | Loss: 0.00013898
Iteration 52/1000 | Loss: 0.00005522
Iteration 53/1000 | Loss: 0.00007240
Iteration 54/1000 | Loss: 0.00067246
Iteration 55/1000 | Loss: 0.00007115
Iteration 56/1000 | Loss: 0.00005615
Iteration 57/1000 | Loss: 0.00005711
Iteration 58/1000 | Loss: 0.00005651
Iteration 59/1000 | Loss: 0.00130771
Iteration 60/1000 | Loss: 0.00101049
Iteration 61/1000 | Loss: 0.00059872
Iteration 62/1000 | Loss: 0.00008064
Iteration 63/1000 | Loss: 0.00007614
Iteration 64/1000 | Loss: 0.00041874
Iteration 65/1000 | Loss: 0.00080052
Iteration 66/1000 | Loss: 0.00010072
Iteration 67/1000 | Loss: 0.00016288
Iteration 68/1000 | Loss: 0.00016418
Iteration 69/1000 | Loss: 0.00010373
Iteration 70/1000 | Loss: 0.00013667
Iteration 71/1000 | Loss: 0.00053176
Iteration 72/1000 | Loss: 0.00037322
Iteration 73/1000 | Loss: 0.00007971
Iteration 74/1000 | Loss: 0.00074461
Iteration 75/1000 | Loss: 0.00015861
Iteration 76/1000 | Loss: 0.00065818
Iteration 77/1000 | Loss: 0.00117566
Iteration 78/1000 | Loss: 0.00012871
Iteration 79/1000 | Loss: 0.00005870
Iteration 80/1000 | Loss: 0.00013752
Iteration 81/1000 | Loss: 0.00013964
Iteration 82/1000 | Loss: 0.00006769
Iteration 83/1000 | Loss: 0.00005509
Iteration 84/1000 | Loss: 0.00005373
Iteration 85/1000 | Loss: 0.00017003
Iteration 86/1000 | Loss: 0.00026424
Iteration 87/1000 | Loss: 0.00018939
Iteration 88/1000 | Loss: 0.00003630
Iteration 89/1000 | Loss: 0.00003312
Iteration 90/1000 | Loss: 0.00036379
Iteration 91/1000 | Loss: 0.00087998
Iteration 92/1000 | Loss: 0.00031247
Iteration 93/1000 | Loss: 0.00018285
Iteration 94/1000 | Loss: 0.00032197
Iteration 95/1000 | Loss: 0.00003615
Iteration 96/1000 | Loss: 0.00021951
Iteration 97/1000 | Loss: 0.00017117
Iteration 98/1000 | Loss: 0.00020069
Iteration 99/1000 | Loss: 0.00060206
Iteration 100/1000 | Loss: 0.00004064
Iteration 101/1000 | Loss: 0.00003591
Iteration 102/1000 | Loss: 0.00062393
Iteration 103/1000 | Loss: 0.00039338
Iteration 104/1000 | Loss: 0.00086405
Iteration 105/1000 | Loss: 0.00003731
Iteration 106/1000 | Loss: 0.00002971
Iteration 107/1000 | Loss: 0.00014204
Iteration 108/1000 | Loss: 0.00002708
Iteration 109/1000 | Loss: 0.00028339
Iteration 110/1000 | Loss: 0.00002623
Iteration 111/1000 | Loss: 0.00002489
Iteration 112/1000 | Loss: 0.00017861
Iteration 113/1000 | Loss: 0.00003287
Iteration 114/1000 | Loss: 0.00002752
Iteration 115/1000 | Loss: 0.00022857
Iteration 116/1000 | Loss: 0.00019773
Iteration 117/1000 | Loss: 0.00037600
Iteration 118/1000 | Loss: 0.00030552
Iteration 119/1000 | Loss: 0.00002484
Iteration 120/1000 | Loss: 0.00002371
Iteration 121/1000 | Loss: 0.00023379
Iteration 122/1000 | Loss: 0.00016528
Iteration 123/1000 | Loss: 0.00032782
Iteration 124/1000 | Loss: 0.00048585
Iteration 125/1000 | Loss: 0.00008452
Iteration 126/1000 | Loss: 0.00021587
Iteration 127/1000 | Loss: 0.00074794
Iteration 128/1000 | Loss: 0.00014739
Iteration 129/1000 | Loss: 0.00023790
Iteration 130/1000 | Loss: 0.00004935
Iteration 131/1000 | Loss: 0.00012936
Iteration 132/1000 | Loss: 0.00005631
Iteration 133/1000 | Loss: 0.00005796
Iteration 134/1000 | Loss: 0.00055353
Iteration 135/1000 | Loss: 0.00008603
Iteration 136/1000 | Loss: 0.00002560
Iteration 137/1000 | Loss: 0.00002381
Iteration 138/1000 | Loss: 0.00002636
Iteration 139/1000 | Loss: 0.00062158
Iteration 140/1000 | Loss: 0.00007864
Iteration 141/1000 | Loss: 0.00008481
Iteration 142/1000 | Loss: 0.00002391
Iteration 143/1000 | Loss: 0.00017664
Iteration 144/1000 | Loss: 0.00003100
Iteration 145/1000 | Loss: 0.00004646
Iteration 146/1000 | Loss: 0.00002140
Iteration 147/1000 | Loss: 0.00002071
Iteration 148/1000 | Loss: 0.00002009
Iteration 149/1000 | Loss: 0.00001962
Iteration 150/1000 | Loss: 0.00056754
Iteration 151/1000 | Loss: 0.00020071
Iteration 152/1000 | Loss: 0.00002988
Iteration 153/1000 | Loss: 0.00001981
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00016630
Iteration 156/1000 | Loss: 0.00027300
Iteration 157/1000 | Loss: 0.00003106
Iteration 158/1000 | Loss: 0.00002909
Iteration 159/1000 | Loss: 0.00002029
Iteration 160/1000 | Loss: 0.00021494
Iteration 161/1000 | Loss: 0.00002481
Iteration 162/1000 | Loss: 0.00026184
Iteration 163/1000 | Loss: 0.00002366
Iteration 164/1000 | Loss: 0.00010520
Iteration 165/1000 | Loss: 0.00002018
Iteration 166/1000 | Loss: 0.00001941
Iteration 167/1000 | Loss: 0.00001912
Iteration 168/1000 | Loss: 0.00001892
Iteration 169/1000 | Loss: 0.00001886
Iteration 170/1000 | Loss: 0.00001886
Iteration 171/1000 | Loss: 0.00001886
Iteration 172/1000 | Loss: 0.00001886
Iteration 173/1000 | Loss: 0.00001886
Iteration 174/1000 | Loss: 0.00001886
Iteration 175/1000 | Loss: 0.00001886
Iteration 176/1000 | Loss: 0.00001885
Iteration 177/1000 | Loss: 0.00001885
Iteration 178/1000 | Loss: 0.00001885
Iteration 179/1000 | Loss: 0.00001885
Iteration 180/1000 | Loss: 0.00001885
Iteration 181/1000 | Loss: 0.00001884
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001883
Iteration 185/1000 | Loss: 0.00001882
Iteration 186/1000 | Loss: 0.00001882
Iteration 187/1000 | Loss: 0.00001881
Iteration 188/1000 | Loss: 0.00001880
Iteration 189/1000 | Loss: 0.00001880
Iteration 190/1000 | Loss: 0.00001878
Iteration 191/1000 | Loss: 0.00001877
Iteration 192/1000 | Loss: 0.00001877
Iteration 193/1000 | Loss: 0.00001876
Iteration 194/1000 | Loss: 0.00001876
Iteration 195/1000 | Loss: 0.00001876
Iteration 196/1000 | Loss: 0.00001876
Iteration 197/1000 | Loss: 0.00001875
Iteration 198/1000 | Loss: 0.00001875
Iteration 199/1000 | Loss: 0.00001875
Iteration 200/1000 | Loss: 0.00001875
Iteration 201/1000 | Loss: 0.00001874
Iteration 202/1000 | Loss: 0.00001874
Iteration 203/1000 | Loss: 0.00001874
Iteration 204/1000 | Loss: 0.00001873
Iteration 205/1000 | Loss: 0.00001873
Iteration 206/1000 | Loss: 0.00001873
Iteration 207/1000 | Loss: 0.00001872
Iteration 208/1000 | Loss: 0.00001872
Iteration 209/1000 | Loss: 0.00001872
Iteration 210/1000 | Loss: 0.00001872
Iteration 211/1000 | Loss: 0.00001872
Iteration 212/1000 | Loss: 0.00001872
Iteration 213/1000 | Loss: 0.00001872
Iteration 214/1000 | Loss: 0.00001871
Iteration 215/1000 | Loss: 0.00001871
Iteration 216/1000 | Loss: 0.00001871
Iteration 217/1000 | Loss: 0.00001870
Iteration 218/1000 | Loss: 0.00001870
Iteration 219/1000 | Loss: 0.00001870
Iteration 220/1000 | Loss: 0.00001869
Iteration 221/1000 | Loss: 0.00001869
Iteration 222/1000 | Loss: 0.00001869
Iteration 223/1000 | Loss: 0.00001868
Iteration 224/1000 | Loss: 0.00001868
Iteration 225/1000 | Loss: 0.00001868
Iteration 226/1000 | Loss: 0.00001868
Iteration 227/1000 | Loss: 0.00001868
Iteration 228/1000 | Loss: 0.00001868
Iteration 229/1000 | Loss: 0.00001868
Iteration 230/1000 | Loss: 0.00001867
Iteration 231/1000 | Loss: 0.00001867
Iteration 232/1000 | Loss: 0.00001865
Iteration 233/1000 | Loss: 0.00001865
Iteration 234/1000 | Loss: 0.00001864
Iteration 235/1000 | Loss: 0.00001864
Iteration 236/1000 | Loss: 0.00001864
Iteration 237/1000 | Loss: 0.00001864
Iteration 238/1000 | Loss: 0.00001863
Iteration 239/1000 | Loss: 0.00001863
Iteration 240/1000 | Loss: 0.00001863
Iteration 241/1000 | Loss: 0.00001863
Iteration 242/1000 | Loss: 0.00001863
Iteration 243/1000 | Loss: 0.00001863
Iteration 244/1000 | Loss: 0.00001863
Iteration 245/1000 | Loss: 0.00001862
Iteration 246/1000 | Loss: 0.00001862
Iteration 247/1000 | Loss: 0.00001862
Iteration 248/1000 | Loss: 0.00001862
Iteration 249/1000 | Loss: 0.00001862
Iteration 250/1000 | Loss: 0.00001862
Iteration 251/1000 | Loss: 0.00001862
Iteration 252/1000 | Loss: 0.00001861
Iteration 253/1000 | Loss: 0.00001861
Iteration 254/1000 | Loss: 0.00001861
Iteration 255/1000 | Loss: 0.00001861
Iteration 256/1000 | Loss: 0.00001861
Iteration 257/1000 | Loss: 0.00001861
Iteration 258/1000 | Loss: 0.00001860
Iteration 259/1000 | Loss: 0.00001860
Iteration 260/1000 | Loss: 0.00001860
Iteration 261/1000 | Loss: 0.00001860
Iteration 262/1000 | Loss: 0.00001860
Iteration 263/1000 | Loss: 0.00001860
Iteration 264/1000 | Loss: 0.00001860
Iteration 265/1000 | Loss: 0.00001860
Iteration 266/1000 | Loss: 0.00001860
Iteration 267/1000 | Loss: 0.00001860
Iteration 268/1000 | Loss: 0.00001860
Iteration 269/1000 | Loss: 0.00001859
Iteration 270/1000 | Loss: 0.00001859
Iteration 271/1000 | Loss: 0.00001859
Iteration 272/1000 | Loss: 0.00001859
Iteration 273/1000 | Loss: 0.00001859
Iteration 274/1000 | Loss: 0.00001859
Iteration 275/1000 | Loss: 0.00001859
Iteration 276/1000 | Loss: 0.00001859
Iteration 277/1000 | Loss: 0.00001859
Iteration 278/1000 | Loss: 0.00001859
Iteration 279/1000 | Loss: 0.00001859
Iteration 280/1000 | Loss: 0.00001859
Iteration 281/1000 | Loss: 0.00001859
Iteration 282/1000 | Loss: 0.00001859
Iteration 283/1000 | Loss: 0.00001858
Iteration 284/1000 | Loss: 0.00001858
Iteration 285/1000 | Loss: 0.00001858
Iteration 286/1000 | Loss: 0.00001858
Iteration 287/1000 | Loss: 0.00001858
Iteration 288/1000 | Loss: 0.00001858
Iteration 289/1000 | Loss: 0.00001858
Iteration 290/1000 | Loss: 0.00001858
Iteration 291/1000 | Loss: 0.00001858
Iteration 292/1000 | Loss: 0.00001858
Iteration 293/1000 | Loss: 0.00001857
Iteration 294/1000 | Loss: 0.00001857
Iteration 295/1000 | Loss: 0.00001857
Iteration 296/1000 | Loss: 0.00001857
Iteration 297/1000 | Loss: 0.00001857
Iteration 298/1000 | Loss: 0.00001857
Iteration 299/1000 | Loss: 0.00001857
Iteration 300/1000 | Loss: 0.00001857
Iteration 301/1000 | Loss: 0.00001857
Iteration 302/1000 | Loss: 0.00001857
Iteration 303/1000 | Loss: 0.00001857
Iteration 304/1000 | Loss: 0.00001857
Iteration 305/1000 | Loss: 0.00001857
Iteration 306/1000 | Loss: 0.00001857
Iteration 307/1000 | Loss: 0.00001857
Iteration 308/1000 | Loss: 0.00001857
Iteration 309/1000 | Loss: 0.00001857
Iteration 310/1000 | Loss: 0.00001857
Iteration 311/1000 | Loss: 0.00001857
Iteration 312/1000 | Loss: 0.00001856
Iteration 313/1000 | Loss: 0.00001856
Iteration 314/1000 | Loss: 0.00001856
Iteration 315/1000 | Loss: 0.00001856
Iteration 316/1000 | Loss: 0.00001856
Iteration 317/1000 | Loss: 0.00001856
Iteration 318/1000 | Loss: 0.00001856
Iteration 319/1000 | Loss: 0.00001856
Iteration 320/1000 | Loss: 0.00001856
Iteration 321/1000 | Loss: 0.00001856
Iteration 322/1000 | Loss: 0.00001856
Iteration 323/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [1.8562910554464906e-05, 1.8562910554464906e-05, 1.8562910554464906e-05, 1.8562910554464906e-05, 1.8562910554464906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8562910554464906e-05

Optimization complete. Final v2v error: 3.539294719696045 mm

Highest mean error: 9.379907608032227 mm for frame 93

Lowest mean error: 3.000443935394287 mm for frame 124

Saving results

Total time: 622.0935966968536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806729
Iteration 2/25 | Loss: 0.00168124
Iteration 3/25 | Loss: 0.00113022
Iteration 4/25 | Loss: 0.00101795
Iteration 5/25 | Loss: 0.00101842
Iteration 6/25 | Loss: 0.00097864
Iteration 7/25 | Loss: 0.00096882
Iteration 8/25 | Loss: 0.00093020
Iteration 9/25 | Loss: 0.00091779
Iteration 10/25 | Loss: 0.00091369
Iteration 11/25 | Loss: 0.00091219
Iteration 12/25 | Loss: 0.00091251
Iteration 13/25 | Loss: 0.00091218
Iteration 14/25 | Loss: 0.00091167
Iteration 15/25 | Loss: 0.00091248
Iteration 16/25 | Loss: 0.00091255
Iteration 17/25 | Loss: 0.00091247
Iteration 18/25 | Loss: 0.00091249
Iteration 19/25 | Loss: 0.00091270
Iteration 20/25 | Loss: 0.00091205
Iteration 21/25 | Loss: 0.00091185
Iteration 22/25 | Loss: 0.00091238
Iteration 23/25 | Loss: 0.00091192
Iteration 24/25 | Loss: 0.00091331
Iteration 25/25 | Loss: 0.00091226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39228094
Iteration 2/25 | Loss: 0.00027595
Iteration 3/25 | Loss: 0.00027594
Iteration 4/25 | Loss: 0.00027594
Iteration 5/25 | Loss: 0.00027594
Iteration 6/25 | Loss: 0.00027593
Iteration 7/25 | Loss: 0.00027593
Iteration 8/25 | Loss: 0.00027593
Iteration 9/25 | Loss: 0.00027593
Iteration 10/25 | Loss: 0.00027593
Iteration 11/25 | Loss: 0.00027593
Iteration 12/25 | Loss: 0.00027593
Iteration 13/25 | Loss: 0.00027593
Iteration 14/25 | Loss: 0.00027593
Iteration 15/25 | Loss: 0.00027593
Iteration 16/25 | Loss: 0.00027593
Iteration 17/25 | Loss: 0.00027593
Iteration 18/25 | Loss: 0.00027593
Iteration 19/25 | Loss: 0.00027593
Iteration 20/25 | Loss: 0.00027593
Iteration 21/25 | Loss: 0.00027593
Iteration 22/25 | Loss: 0.00027593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002759330964181572, 0.0002759330964181572, 0.0002759330964181572, 0.0002759330964181572, 0.0002759330964181572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002759330964181572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027593
Iteration 2/1000 | Loss: 0.00067791
Iteration 3/1000 | Loss: 0.00014959
Iteration 4/1000 | Loss: 0.00006379
Iteration 5/1000 | Loss: 0.00005821
Iteration 6/1000 | Loss: 0.00005124
Iteration 7/1000 | Loss: 0.00003220
Iteration 8/1000 | Loss: 0.00003222
Iteration 9/1000 | Loss: 0.00003201
Iteration 10/1000 | Loss: 0.00002673
Iteration 11/1000 | Loss: 0.00002062
Iteration 12/1000 | Loss: 0.00003015
Iteration 13/1000 | Loss: 0.00002734
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002726
Iteration 16/1000 | Loss: 0.00002039
Iteration 17/1000 | Loss: 0.00002706
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00002385
Iteration 21/1000 | Loss: 0.00003508
Iteration 22/1000 | Loss: 0.00002846
Iteration 23/1000 | Loss: 0.00002710
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002151
Iteration 26/1000 | Loss: 0.00002416
Iteration 27/1000 | Loss: 0.00002281
Iteration 28/1000 | Loss: 0.00004692
Iteration 29/1000 | Loss: 0.00003276
Iteration 30/1000 | Loss: 0.00004078
Iteration 31/1000 | Loss: 0.00004555
Iteration 32/1000 | Loss: 0.00004163
Iteration 33/1000 | Loss: 0.00004688
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001431
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001313
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001307
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001306
Iteration 49/1000 | Loss: 0.00001306
Iteration 50/1000 | Loss: 0.00001306
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001296
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001275
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001274
Iteration 106/1000 | Loss: 0.00001274
Iteration 107/1000 | Loss: 0.00001274
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001274
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001273
Iteration 125/1000 | Loss: 0.00001273
Iteration 126/1000 | Loss: 0.00001273
Iteration 127/1000 | Loss: 0.00001273
Iteration 128/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.2734666597680189e-05, 1.2734666597680189e-05, 1.2734666597680189e-05, 1.2734666597680189e-05, 1.2734666597680189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2734666597680189e-05

Optimization complete. Final v2v error: 3.034428834915161 mm

Highest mean error: 4.586729526519775 mm for frame 228

Lowest mean error: 2.4739601612091064 mm for frame 78

Saving results

Total time: 330.00289845466614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397213
Iteration 2/25 | Loss: 0.00095737
Iteration 3/25 | Loss: 0.00084820
Iteration 4/25 | Loss: 0.00083751
Iteration 5/25 | Loss: 0.00083434
Iteration 6/25 | Loss: 0.00083326
Iteration 7/25 | Loss: 0.00083318
Iteration 8/25 | Loss: 0.00083318
Iteration 9/25 | Loss: 0.00083318
Iteration 10/25 | Loss: 0.00083318
Iteration 11/25 | Loss: 0.00083318
Iteration 12/25 | Loss: 0.00083318
Iteration 13/25 | Loss: 0.00083318
Iteration 14/25 | Loss: 0.00083318
Iteration 15/25 | Loss: 0.00083318
Iteration 16/25 | Loss: 0.00083318
Iteration 17/25 | Loss: 0.00083318
Iteration 18/25 | Loss: 0.00083318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008331805001944304, 0.0008331805001944304, 0.0008331805001944304, 0.0008331805001944304, 0.0008331805001944304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008331805001944304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42612481
Iteration 2/25 | Loss: 0.00038358
Iteration 3/25 | Loss: 0.00038358
Iteration 4/25 | Loss: 0.00038358
Iteration 5/25 | Loss: 0.00038358
Iteration 6/25 | Loss: 0.00038358
Iteration 7/25 | Loss: 0.00038358
Iteration 8/25 | Loss: 0.00038358
Iteration 9/25 | Loss: 0.00038358
Iteration 10/25 | Loss: 0.00038358
Iteration 11/25 | Loss: 0.00038358
Iteration 12/25 | Loss: 0.00038358
Iteration 13/25 | Loss: 0.00038358
Iteration 14/25 | Loss: 0.00038358
Iteration 15/25 | Loss: 0.00038358
Iteration 16/25 | Loss: 0.00038358
Iteration 17/25 | Loss: 0.00038358
Iteration 18/25 | Loss: 0.00038358
Iteration 19/25 | Loss: 0.00038358
Iteration 20/25 | Loss: 0.00038358
Iteration 21/25 | Loss: 0.00038358
Iteration 22/25 | Loss: 0.00038358
Iteration 23/25 | Loss: 0.00038358
Iteration 24/25 | Loss: 0.00038358
Iteration 25/25 | Loss: 0.00038358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038358
Iteration 2/1000 | Loss: 0.00002745
Iteration 3/1000 | Loss: 0.00001143
Iteration 4/1000 | Loss: 0.00000984
Iteration 5/1000 | Loss: 0.00000912
Iteration 6/1000 | Loss: 0.00000877
Iteration 7/1000 | Loss: 0.00000848
Iteration 8/1000 | Loss: 0.00000830
Iteration 9/1000 | Loss: 0.00000829
Iteration 10/1000 | Loss: 0.00000826
Iteration 11/1000 | Loss: 0.00000825
Iteration 12/1000 | Loss: 0.00000820
Iteration 13/1000 | Loss: 0.00000819
Iteration 14/1000 | Loss: 0.00000816
Iteration 15/1000 | Loss: 0.00000814
Iteration 16/1000 | Loss: 0.00000813
Iteration 17/1000 | Loss: 0.00000813
Iteration 18/1000 | Loss: 0.00000812
Iteration 19/1000 | Loss: 0.00000812
Iteration 20/1000 | Loss: 0.00000811
Iteration 21/1000 | Loss: 0.00000811
Iteration 22/1000 | Loss: 0.00000811
Iteration 23/1000 | Loss: 0.00000811
Iteration 24/1000 | Loss: 0.00000808
Iteration 25/1000 | Loss: 0.00000808
Iteration 26/1000 | Loss: 0.00000805
Iteration 27/1000 | Loss: 0.00000804
Iteration 28/1000 | Loss: 0.00000804
Iteration 29/1000 | Loss: 0.00000801
Iteration 30/1000 | Loss: 0.00000801
Iteration 31/1000 | Loss: 0.00000800
Iteration 32/1000 | Loss: 0.00000800
Iteration 33/1000 | Loss: 0.00000799
Iteration 34/1000 | Loss: 0.00000799
Iteration 35/1000 | Loss: 0.00000798
Iteration 36/1000 | Loss: 0.00000796
Iteration 37/1000 | Loss: 0.00000796
Iteration 38/1000 | Loss: 0.00000796
Iteration 39/1000 | Loss: 0.00000796
Iteration 40/1000 | Loss: 0.00000796
Iteration 41/1000 | Loss: 0.00000795
Iteration 42/1000 | Loss: 0.00000795
Iteration 43/1000 | Loss: 0.00000795
Iteration 44/1000 | Loss: 0.00000795
Iteration 45/1000 | Loss: 0.00000795
Iteration 46/1000 | Loss: 0.00000794
Iteration 47/1000 | Loss: 0.00000790
Iteration 48/1000 | Loss: 0.00000790
Iteration 49/1000 | Loss: 0.00000790
Iteration 50/1000 | Loss: 0.00000790
Iteration 51/1000 | Loss: 0.00000790
Iteration 52/1000 | Loss: 0.00000790
Iteration 53/1000 | Loss: 0.00000789
Iteration 54/1000 | Loss: 0.00000789
Iteration 55/1000 | Loss: 0.00000789
Iteration 56/1000 | Loss: 0.00000789
Iteration 57/1000 | Loss: 0.00000789
Iteration 58/1000 | Loss: 0.00000789
Iteration 59/1000 | Loss: 0.00000789
Iteration 60/1000 | Loss: 0.00000789
Iteration 61/1000 | Loss: 0.00000789
Iteration 62/1000 | Loss: 0.00000788
Iteration 63/1000 | Loss: 0.00000788
Iteration 64/1000 | Loss: 0.00000787
Iteration 65/1000 | Loss: 0.00000785
Iteration 66/1000 | Loss: 0.00000785
Iteration 67/1000 | Loss: 0.00000785
Iteration 68/1000 | Loss: 0.00000782
Iteration 69/1000 | Loss: 0.00000782
Iteration 70/1000 | Loss: 0.00000782
Iteration 71/1000 | Loss: 0.00000782
Iteration 72/1000 | Loss: 0.00000782
Iteration 73/1000 | Loss: 0.00000782
Iteration 74/1000 | Loss: 0.00000782
Iteration 75/1000 | Loss: 0.00000781
Iteration 76/1000 | Loss: 0.00000781
Iteration 77/1000 | Loss: 0.00000781
Iteration 78/1000 | Loss: 0.00000781
Iteration 79/1000 | Loss: 0.00000780
Iteration 80/1000 | Loss: 0.00000780
Iteration 81/1000 | Loss: 0.00000780
Iteration 82/1000 | Loss: 0.00000779
Iteration 83/1000 | Loss: 0.00000779
Iteration 84/1000 | Loss: 0.00000779
Iteration 85/1000 | Loss: 0.00000778
Iteration 86/1000 | Loss: 0.00000778
Iteration 87/1000 | Loss: 0.00000778
Iteration 88/1000 | Loss: 0.00000778
Iteration 89/1000 | Loss: 0.00000777
Iteration 90/1000 | Loss: 0.00000777
Iteration 91/1000 | Loss: 0.00000777
Iteration 92/1000 | Loss: 0.00000776
Iteration 93/1000 | Loss: 0.00000776
Iteration 94/1000 | Loss: 0.00000776
Iteration 95/1000 | Loss: 0.00000775
Iteration 96/1000 | Loss: 0.00000775
Iteration 97/1000 | Loss: 0.00000775
Iteration 98/1000 | Loss: 0.00000775
Iteration 99/1000 | Loss: 0.00000775
Iteration 100/1000 | Loss: 0.00000775
Iteration 101/1000 | Loss: 0.00000775
Iteration 102/1000 | Loss: 0.00000775
Iteration 103/1000 | Loss: 0.00000775
Iteration 104/1000 | Loss: 0.00000774
Iteration 105/1000 | Loss: 0.00000774
Iteration 106/1000 | Loss: 0.00000774
Iteration 107/1000 | Loss: 0.00000774
Iteration 108/1000 | Loss: 0.00000774
Iteration 109/1000 | Loss: 0.00000774
Iteration 110/1000 | Loss: 0.00000774
Iteration 111/1000 | Loss: 0.00000774
Iteration 112/1000 | Loss: 0.00000774
Iteration 113/1000 | Loss: 0.00000774
Iteration 114/1000 | Loss: 0.00000774
Iteration 115/1000 | Loss: 0.00000774
Iteration 116/1000 | Loss: 0.00000774
Iteration 117/1000 | Loss: 0.00000774
Iteration 118/1000 | Loss: 0.00000774
Iteration 119/1000 | Loss: 0.00000774
Iteration 120/1000 | Loss: 0.00000773
Iteration 121/1000 | Loss: 0.00000773
Iteration 122/1000 | Loss: 0.00000773
Iteration 123/1000 | Loss: 0.00000773
Iteration 124/1000 | Loss: 0.00000773
Iteration 125/1000 | Loss: 0.00000773
Iteration 126/1000 | Loss: 0.00000773
Iteration 127/1000 | Loss: 0.00000773
Iteration 128/1000 | Loss: 0.00000773
Iteration 129/1000 | Loss: 0.00000773
Iteration 130/1000 | Loss: 0.00000773
Iteration 131/1000 | Loss: 0.00000773
Iteration 132/1000 | Loss: 0.00000773
Iteration 133/1000 | Loss: 0.00000773
Iteration 134/1000 | Loss: 0.00000772
Iteration 135/1000 | Loss: 0.00000772
Iteration 136/1000 | Loss: 0.00000772
Iteration 137/1000 | Loss: 0.00000772
Iteration 138/1000 | Loss: 0.00000772
Iteration 139/1000 | Loss: 0.00000772
Iteration 140/1000 | Loss: 0.00000772
Iteration 141/1000 | Loss: 0.00000772
Iteration 142/1000 | Loss: 0.00000772
Iteration 143/1000 | Loss: 0.00000772
Iteration 144/1000 | Loss: 0.00000771
Iteration 145/1000 | Loss: 0.00000771
Iteration 146/1000 | Loss: 0.00000771
Iteration 147/1000 | Loss: 0.00000771
Iteration 148/1000 | Loss: 0.00000771
Iteration 149/1000 | Loss: 0.00000771
Iteration 150/1000 | Loss: 0.00000771
Iteration 151/1000 | Loss: 0.00000771
Iteration 152/1000 | Loss: 0.00000771
Iteration 153/1000 | Loss: 0.00000771
Iteration 154/1000 | Loss: 0.00000771
Iteration 155/1000 | Loss: 0.00000771
Iteration 156/1000 | Loss: 0.00000771
Iteration 157/1000 | Loss: 0.00000771
Iteration 158/1000 | Loss: 0.00000771
Iteration 159/1000 | Loss: 0.00000771
Iteration 160/1000 | Loss: 0.00000771
Iteration 161/1000 | Loss: 0.00000771
Iteration 162/1000 | Loss: 0.00000771
Iteration 163/1000 | Loss: 0.00000771
Iteration 164/1000 | Loss: 0.00000771
Iteration 165/1000 | Loss: 0.00000771
Iteration 166/1000 | Loss: 0.00000771
Iteration 167/1000 | Loss: 0.00000771
Iteration 168/1000 | Loss: 0.00000771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [7.705869393248577e-06, 7.705869393248577e-06, 7.705869393248577e-06, 7.705869393248577e-06, 7.705869393248577e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.705869393248577e-06

Optimization complete. Final v2v error: 2.4324758052825928 mm

Highest mean error: 2.5927605628967285 mm for frame 81

Lowest mean error: 2.2900493144989014 mm for frame 65

Saving results

Total time: 76.390132188797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920080
Iteration 2/25 | Loss: 0.00117926
Iteration 3/25 | Loss: 0.00092688
Iteration 4/25 | Loss: 0.00090765
Iteration 5/25 | Loss: 0.00090211
Iteration 6/25 | Loss: 0.00090037
Iteration 7/25 | Loss: 0.00090014
Iteration 8/25 | Loss: 0.00090014
Iteration 9/25 | Loss: 0.00090014
Iteration 10/25 | Loss: 0.00090014
Iteration 11/25 | Loss: 0.00090014
Iteration 12/25 | Loss: 0.00090014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009001377038657665, 0.0009001377038657665, 0.0009001377038657665, 0.0009001377038657665, 0.0009001377038657665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009001377038657665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50324547
Iteration 2/25 | Loss: 0.00041398
Iteration 3/25 | Loss: 0.00041398
Iteration 4/25 | Loss: 0.00041398
Iteration 5/25 | Loss: 0.00041398
Iteration 6/25 | Loss: 0.00041398
Iteration 7/25 | Loss: 0.00041398
Iteration 8/25 | Loss: 0.00041398
Iteration 9/25 | Loss: 0.00041398
Iteration 10/25 | Loss: 0.00041398
Iteration 11/25 | Loss: 0.00041398
Iteration 12/25 | Loss: 0.00041398
Iteration 13/25 | Loss: 0.00041398
Iteration 14/25 | Loss: 0.00041398
Iteration 15/25 | Loss: 0.00041398
Iteration 16/25 | Loss: 0.00041398
Iteration 17/25 | Loss: 0.00041398
Iteration 18/25 | Loss: 0.00041398
Iteration 19/25 | Loss: 0.00041398
Iteration 20/25 | Loss: 0.00041398
Iteration 21/25 | Loss: 0.00041398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004139753000345081, 0.0004139753000345081, 0.0004139753000345081, 0.0004139753000345081, 0.0004139753000345081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004139753000345081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041398
Iteration 2/1000 | Loss: 0.00004494
Iteration 3/1000 | Loss: 0.00002416
Iteration 4/1000 | Loss: 0.00002097
Iteration 5/1000 | Loss: 0.00001979
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001837
Iteration 9/1000 | Loss: 0.00001807
Iteration 10/1000 | Loss: 0.00001784
Iteration 11/1000 | Loss: 0.00001782
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001758
Iteration 17/1000 | Loss: 0.00001754
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001753
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001749
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001749
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00001745
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00001744
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001727
Iteration 106/1000 | Loss: 0.00001727
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001724
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001724
Iteration 128/1000 | Loss: 0.00001724
Iteration 129/1000 | Loss: 0.00001724
Iteration 130/1000 | Loss: 0.00001724
Iteration 131/1000 | Loss: 0.00001724
Iteration 132/1000 | Loss: 0.00001724
Iteration 133/1000 | Loss: 0.00001724
Iteration 134/1000 | Loss: 0.00001724
Iteration 135/1000 | Loss: 0.00001724
Iteration 136/1000 | Loss: 0.00001724
Iteration 137/1000 | Loss: 0.00001724
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001723
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.7234280676348135e-05, 1.7234280676348135e-05, 1.7234280676348135e-05, 1.7234280676348135e-05, 1.7234280676348135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7234280676348135e-05

Optimization complete. Final v2v error: 3.3585124015808105 mm

Highest mean error: 4.9073309898376465 mm for frame 56

Lowest mean error: 2.7700626850128174 mm for frame 104

Saving results

Total time: 82.36152601242065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817332
Iteration 2/25 | Loss: 0.00124530
Iteration 3/25 | Loss: 0.00087447
Iteration 4/25 | Loss: 0.00083416
Iteration 5/25 | Loss: 0.00083003
Iteration 6/25 | Loss: 0.00082955
Iteration 7/25 | Loss: 0.00082955
Iteration 8/25 | Loss: 0.00082955
Iteration 9/25 | Loss: 0.00082955
Iteration 10/25 | Loss: 0.00082955
Iteration 11/25 | Loss: 0.00082955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008295545703731477, 0.0008295545703731477, 0.0008295545703731477, 0.0008295545703731477, 0.0008295545703731477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008295545703731477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41413474
Iteration 2/25 | Loss: 0.00029529
Iteration 3/25 | Loss: 0.00029529
Iteration 4/25 | Loss: 0.00029529
Iteration 5/25 | Loss: 0.00029529
Iteration 6/25 | Loss: 0.00029529
Iteration 7/25 | Loss: 0.00029529
Iteration 8/25 | Loss: 0.00029529
Iteration 9/25 | Loss: 0.00029529
Iteration 10/25 | Loss: 0.00029529
Iteration 11/25 | Loss: 0.00029529
Iteration 12/25 | Loss: 0.00029529
Iteration 13/25 | Loss: 0.00029529
Iteration 14/25 | Loss: 0.00029529
Iteration 15/25 | Loss: 0.00029529
Iteration 16/25 | Loss: 0.00029529
Iteration 17/25 | Loss: 0.00029529
Iteration 18/25 | Loss: 0.00029529
Iteration 19/25 | Loss: 0.00029529
Iteration 20/25 | Loss: 0.00029529
Iteration 21/25 | Loss: 0.00029529
Iteration 22/25 | Loss: 0.00029529
Iteration 23/25 | Loss: 0.00029529
Iteration 24/25 | Loss: 0.00029529
Iteration 25/25 | Loss: 0.00029529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029529
Iteration 2/1000 | Loss: 0.00002357
Iteration 3/1000 | Loss: 0.00001203
Iteration 4/1000 | Loss: 0.00001004
Iteration 5/1000 | Loss: 0.00000944
Iteration 6/1000 | Loss: 0.00000887
Iteration 7/1000 | Loss: 0.00000860
Iteration 8/1000 | Loss: 0.00000842
Iteration 9/1000 | Loss: 0.00000837
Iteration 10/1000 | Loss: 0.00000835
Iteration 11/1000 | Loss: 0.00000834
Iteration 12/1000 | Loss: 0.00000833
Iteration 13/1000 | Loss: 0.00000833
Iteration 14/1000 | Loss: 0.00000832
Iteration 15/1000 | Loss: 0.00000831
Iteration 16/1000 | Loss: 0.00000831
Iteration 17/1000 | Loss: 0.00000820
Iteration 18/1000 | Loss: 0.00000815
Iteration 19/1000 | Loss: 0.00000814
Iteration 20/1000 | Loss: 0.00000813
Iteration 21/1000 | Loss: 0.00000812
Iteration 22/1000 | Loss: 0.00000811
Iteration 23/1000 | Loss: 0.00000811
Iteration 24/1000 | Loss: 0.00000810
Iteration 25/1000 | Loss: 0.00000809
Iteration 26/1000 | Loss: 0.00000809
Iteration 27/1000 | Loss: 0.00000808
Iteration 28/1000 | Loss: 0.00000808
Iteration 29/1000 | Loss: 0.00000807
Iteration 30/1000 | Loss: 0.00000807
Iteration 31/1000 | Loss: 0.00000806
Iteration 32/1000 | Loss: 0.00000806
Iteration 33/1000 | Loss: 0.00000805
Iteration 34/1000 | Loss: 0.00000805
Iteration 35/1000 | Loss: 0.00000804
Iteration 36/1000 | Loss: 0.00000804
Iteration 37/1000 | Loss: 0.00000803
Iteration 38/1000 | Loss: 0.00000803
Iteration 39/1000 | Loss: 0.00000803
Iteration 40/1000 | Loss: 0.00000803
Iteration 41/1000 | Loss: 0.00000803
Iteration 42/1000 | Loss: 0.00000803
Iteration 43/1000 | Loss: 0.00000803
Iteration 44/1000 | Loss: 0.00000803
Iteration 45/1000 | Loss: 0.00000803
Iteration 46/1000 | Loss: 0.00000803
Iteration 47/1000 | Loss: 0.00000803
Iteration 48/1000 | Loss: 0.00000803
Iteration 49/1000 | Loss: 0.00000803
Iteration 50/1000 | Loss: 0.00000802
Iteration 51/1000 | Loss: 0.00000802
Iteration 52/1000 | Loss: 0.00000802
Iteration 53/1000 | Loss: 0.00000802
Iteration 54/1000 | Loss: 0.00000802
Iteration 55/1000 | Loss: 0.00000802
Iteration 56/1000 | Loss: 0.00000802
Iteration 57/1000 | Loss: 0.00000801
Iteration 58/1000 | Loss: 0.00000801
Iteration 59/1000 | Loss: 0.00000799
Iteration 60/1000 | Loss: 0.00000799
Iteration 61/1000 | Loss: 0.00000798
Iteration 62/1000 | Loss: 0.00000798
Iteration 63/1000 | Loss: 0.00000797
Iteration 64/1000 | Loss: 0.00000796
Iteration 65/1000 | Loss: 0.00000795
Iteration 66/1000 | Loss: 0.00000795
Iteration 67/1000 | Loss: 0.00000794
Iteration 68/1000 | Loss: 0.00000794
Iteration 69/1000 | Loss: 0.00000793
Iteration 70/1000 | Loss: 0.00000793
Iteration 71/1000 | Loss: 0.00000793
Iteration 72/1000 | Loss: 0.00000792
Iteration 73/1000 | Loss: 0.00000792
Iteration 74/1000 | Loss: 0.00000792
Iteration 75/1000 | Loss: 0.00000792
Iteration 76/1000 | Loss: 0.00000792
Iteration 77/1000 | Loss: 0.00000791
Iteration 78/1000 | Loss: 0.00000791
Iteration 79/1000 | Loss: 0.00000791
Iteration 80/1000 | Loss: 0.00000791
Iteration 81/1000 | Loss: 0.00000791
Iteration 82/1000 | Loss: 0.00000791
Iteration 83/1000 | Loss: 0.00000791
Iteration 84/1000 | Loss: 0.00000791
Iteration 85/1000 | Loss: 0.00000791
Iteration 86/1000 | Loss: 0.00000790
Iteration 87/1000 | Loss: 0.00000790
Iteration 88/1000 | Loss: 0.00000790
Iteration 89/1000 | Loss: 0.00000790
Iteration 90/1000 | Loss: 0.00000789
Iteration 91/1000 | Loss: 0.00000789
Iteration 92/1000 | Loss: 0.00000789
Iteration 93/1000 | Loss: 0.00000789
Iteration 94/1000 | Loss: 0.00000789
Iteration 95/1000 | Loss: 0.00000788
Iteration 96/1000 | Loss: 0.00000788
Iteration 97/1000 | Loss: 0.00000787
Iteration 98/1000 | Loss: 0.00000787
Iteration 99/1000 | Loss: 0.00000787
Iteration 100/1000 | Loss: 0.00000787
Iteration 101/1000 | Loss: 0.00000786
Iteration 102/1000 | Loss: 0.00000786
Iteration 103/1000 | Loss: 0.00000786
Iteration 104/1000 | Loss: 0.00000785
Iteration 105/1000 | Loss: 0.00000785
Iteration 106/1000 | Loss: 0.00000785
Iteration 107/1000 | Loss: 0.00000785
Iteration 108/1000 | Loss: 0.00000785
Iteration 109/1000 | Loss: 0.00000785
Iteration 110/1000 | Loss: 0.00000785
Iteration 111/1000 | Loss: 0.00000785
Iteration 112/1000 | Loss: 0.00000784
Iteration 113/1000 | Loss: 0.00000784
Iteration 114/1000 | Loss: 0.00000784
Iteration 115/1000 | Loss: 0.00000784
Iteration 116/1000 | Loss: 0.00000783
Iteration 117/1000 | Loss: 0.00000783
Iteration 118/1000 | Loss: 0.00000783
Iteration 119/1000 | Loss: 0.00000783
Iteration 120/1000 | Loss: 0.00000783
Iteration 121/1000 | Loss: 0.00000783
Iteration 122/1000 | Loss: 0.00000783
Iteration 123/1000 | Loss: 0.00000783
Iteration 124/1000 | Loss: 0.00000782
Iteration 125/1000 | Loss: 0.00000782
Iteration 126/1000 | Loss: 0.00000782
Iteration 127/1000 | Loss: 0.00000782
Iteration 128/1000 | Loss: 0.00000782
Iteration 129/1000 | Loss: 0.00000782
Iteration 130/1000 | Loss: 0.00000781
Iteration 131/1000 | Loss: 0.00000781
Iteration 132/1000 | Loss: 0.00000781
Iteration 133/1000 | Loss: 0.00000780
Iteration 134/1000 | Loss: 0.00000780
Iteration 135/1000 | Loss: 0.00000780
Iteration 136/1000 | Loss: 0.00000780
Iteration 137/1000 | Loss: 0.00000779
Iteration 138/1000 | Loss: 0.00000779
Iteration 139/1000 | Loss: 0.00000779
Iteration 140/1000 | Loss: 0.00000779
Iteration 141/1000 | Loss: 0.00000778
Iteration 142/1000 | Loss: 0.00000778
Iteration 143/1000 | Loss: 0.00000778
Iteration 144/1000 | Loss: 0.00000778
Iteration 145/1000 | Loss: 0.00000778
Iteration 146/1000 | Loss: 0.00000777
Iteration 147/1000 | Loss: 0.00000777
Iteration 148/1000 | Loss: 0.00000777
Iteration 149/1000 | Loss: 0.00000777
Iteration 150/1000 | Loss: 0.00000777
Iteration 151/1000 | Loss: 0.00000776
Iteration 152/1000 | Loss: 0.00000776
Iteration 153/1000 | Loss: 0.00000776
Iteration 154/1000 | Loss: 0.00000776
Iteration 155/1000 | Loss: 0.00000776
Iteration 156/1000 | Loss: 0.00000775
Iteration 157/1000 | Loss: 0.00000775
Iteration 158/1000 | Loss: 0.00000775
Iteration 159/1000 | Loss: 0.00000775
Iteration 160/1000 | Loss: 0.00000775
Iteration 161/1000 | Loss: 0.00000775
Iteration 162/1000 | Loss: 0.00000775
Iteration 163/1000 | Loss: 0.00000775
Iteration 164/1000 | Loss: 0.00000775
Iteration 165/1000 | Loss: 0.00000775
Iteration 166/1000 | Loss: 0.00000775
Iteration 167/1000 | Loss: 0.00000775
Iteration 168/1000 | Loss: 0.00000775
Iteration 169/1000 | Loss: 0.00000775
Iteration 170/1000 | Loss: 0.00000775
Iteration 171/1000 | Loss: 0.00000775
Iteration 172/1000 | Loss: 0.00000775
Iteration 173/1000 | Loss: 0.00000775
Iteration 174/1000 | Loss: 0.00000775
Iteration 175/1000 | Loss: 0.00000775
Iteration 176/1000 | Loss: 0.00000775
Iteration 177/1000 | Loss: 0.00000775
Iteration 178/1000 | Loss: 0.00000775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [7.746092705929186e-06, 7.746092705929186e-06, 7.746092705929186e-06, 7.746092705929186e-06, 7.746092705929186e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.746092705929186e-06

Optimization complete. Final v2v error: 2.370504140853882 mm

Highest mean error: 2.7879114151000977 mm for frame 122

Lowest mean error: 2.0163192749023438 mm for frame 0

Saving results

Total time: 109.29525089263916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075950
Iteration 2/25 | Loss: 0.00238361
Iteration 3/25 | Loss: 0.00186455
Iteration 4/25 | Loss: 0.00180315
Iteration 5/25 | Loss: 0.00168319
Iteration 6/25 | Loss: 0.00147914
Iteration 7/25 | Loss: 0.00138055
Iteration 8/25 | Loss: 0.00133950
Iteration 9/25 | Loss: 0.00129829
Iteration 10/25 | Loss: 0.00127277
Iteration 11/25 | Loss: 0.00123400
Iteration 12/25 | Loss: 0.00122396
Iteration 13/25 | Loss: 0.00120239
Iteration 14/25 | Loss: 0.00117947
Iteration 15/25 | Loss: 0.00116820
Iteration 16/25 | Loss: 0.00115920
Iteration 17/25 | Loss: 0.00115775
Iteration 18/25 | Loss: 0.00115245
Iteration 19/25 | Loss: 0.00115324
Iteration 20/25 | Loss: 0.00115182
Iteration 21/25 | Loss: 0.00114694
Iteration 22/25 | Loss: 0.00114358
Iteration 23/25 | Loss: 0.00114554
Iteration 24/25 | Loss: 0.00114200
Iteration 25/25 | Loss: 0.00114073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35179090
Iteration 2/25 | Loss: 0.00345000
Iteration 3/25 | Loss: 0.00345000
Iteration 4/25 | Loss: 0.00345000
Iteration 5/25 | Loss: 0.00345000
Iteration 6/25 | Loss: 0.00344999
Iteration 7/25 | Loss: 0.00344999
Iteration 8/25 | Loss: 0.00344999
Iteration 9/25 | Loss: 0.00344999
Iteration 10/25 | Loss: 0.00344999
Iteration 11/25 | Loss: 0.00344999
Iteration 12/25 | Loss: 0.00344999
Iteration 13/25 | Loss: 0.00344999
Iteration 14/25 | Loss: 0.00344999
Iteration 15/25 | Loss: 0.00344999
Iteration 16/25 | Loss: 0.00344999
Iteration 17/25 | Loss: 0.00344999
Iteration 18/25 | Loss: 0.00344999
Iteration 19/25 | Loss: 0.00344999
Iteration 20/25 | Loss: 0.00344999
Iteration 21/25 | Loss: 0.00344999
Iteration 22/25 | Loss: 0.00344999
Iteration 23/25 | Loss: 0.00344999
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0034499946050345898, 0.0034499946050345898, 0.0034499946050345898, 0.0034499946050345898, 0.0034499946050345898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034499946050345898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344999
Iteration 2/1000 | Loss: 0.00059291
Iteration 3/1000 | Loss: 0.00028190
Iteration 4/1000 | Loss: 0.00021107
Iteration 5/1000 | Loss: 0.00016898
Iteration 6/1000 | Loss: 0.00012418
Iteration 7/1000 | Loss: 0.00007819
Iteration 8/1000 | Loss: 0.00005734
Iteration 9/1000 | Loss: 0.00004389
Iteration 10/1000 | Loss: 0.00003488
Iteration 11/1000 | Loss: 0.00002906
Iteration 12/1000 | Loss: 0.00002489
Iteration 13/1000 | Loss: 0.00002239
Iteration 14/1000 | Loss: 0.00002081
Iteration 15/1000 | Loss: 0.00001966
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001788
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00001674
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001614
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001604
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001603
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001598
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001597
Iteration 58/1000 | Loss: 0.00001597
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001595
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001594
Iteration 65/1000 | Loss: 0.00001594
Iteration 66/1000 | Loss: 0.00001593
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001593
Iteration 74/1000 | Loss: 0.00001593
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00001593
Iteration 77/1000 | Loss: 0.00001592
Iteration 78/1000 | Loss: 0.00001592
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001591
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001590
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001589
Iteration 88/1000 | Loss: 0.00001589
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001588
Iteration 95/1000 | Loss: 0.00001588
Iteration 96/1000 | Loss: 0.00001588
Iteration 97/1000 | Loss: 0.00001588
Iteration 98/1000 | Loss: 0.00001588
Iteration 99/1000 | Loss: 0.00001588
Iteration 100/1000 | Loss: 0.00001588
Iteration 101/1000 | Loss: 0.00001588
Iteration 102/1000 | Loss: 0.00001588
Iteration 103/1000 | Loss: 0.00001588
Iteration 104/1000 | Loss: 0.00001588
Iteration 105/1000 | Loss: 0.00001588
Iteration 106/1000 | Loss: 0.00001588
Iteration 107/1000 | Loss: 0.00001588
Iteration 108/1000 | Loss: 0.00001587
Iteration 109/1000 | Loss: 0.00001587
Iteration 110/1000 | Loss: 0.00001587
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001586
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001583
Iteration 129/1000 | Loss: 0.00001583
Iteration 130/1000 | Loss: 0.00001583
Iteration 131/1000 | Loss: 0.00001583
Iteration 132/1000 | Loss: 0.00001583
Iteration 133/1000 | Loss: 0.00001583
Iteration 134/1000 | Loss: 0.00001583
Iteration 135/1000 | Loss: 0.00001583
Iteration 136/1000 | Loss: 0.00001582
Iteration 137/1000 | Loss: 0.00001582
Iteration 138/1000 | Loss: 0.00001582
Iteration 139/1000 | Loss: 0.00001582
Iteration 140/1000 | Loss: 0.00001582
Iteration 141/1000 | Loss: 0.00001582
Iteration 142/1000 | Loss: 0.00001582
Iteration 143/1000 | Loss: 0.00001581
Iteration 144/1000 | Loss: 0.00001581
Iteration 145/1000 | Loss: 0.00001581
Iteration 146/1000 | Loss: 0.00001581
Iteration 147/1000 | Loss: 0.00001581
Iteration 148/1000 | Loss: 0.00001581
Iteration 149/1000 | Loss: 0.00001581
Iteration 150/1000 | Loss: 0.00001581
Iteration 151/1000 | Loss: 0.00001581
Iteration 152/1000 | Loss: 0.00001581
Iteration 153/1000 | Loss: 0.00001581
Iteration 154/1000 | Loss: 0.00001581
Iteration 155/1000 | Loss: 0.00001581
Iteration 156/1000 | Loss: 0.00001581
Iteration 157/1000 | Loss: 0.00001581
Iteration 158/1000 | Loss: 0.00001581
Iteration 159/1000 | Loss: 0.00001581
Iteration 160/1000 | Loss: 0.00001580
Iteration 161/1000 | Loss: 0.00001580
Iteration 162/1000 | Loss: 0.00001580
Iteration 163/1000 | Loss: 0.00001580
Iteration 164/1000 | Loss: 0.00001580
Iteration 165/1000 | Loss: 0.00001580
Iteration 166/1000 | Loss: 0.00001580
Iteration 167/1000 | Loss: 0.00001580
Iteration 168/1000 | Loss: 0.00001579
Iteration 169/1000 | Loss: 0.00001579
Iteration 170/1000 | Loss: 0.00001579
Iteration 171/1000 | Loss: 0.00001579
Iteration 172/1000 | Loss: 0.00001579
Iteration 173/1000 | Loss: 0.00001579
Iteration 174/1000 | Loss: 0.00001579
Iteration 175/1000 | Loss: 0.00001579
Iteration 176/1000 | Loss: 0.00001579
Iteration 177/1000 | Loss: 0.00001579
Iteration 178/1000 | Loss: 0.00001579
Iteration 179/1000 | Loss: 0.00001579
Iteration 180/1000 | Loss: 0.00001578
Iteration 181/1000 | Loss: 0.00001578
Iteration 182/1000 | Loss: 0.00001578
Iteration 183/1000 | Loss: 0.00001578
Iteration 184/1000 | Loss: 0.00001578
Iteration 185/1000 | Loss: 0.00001578
Iteration 186/1000 | Loss: 0.00001578
Iteration 187/1000 | Loss: 0.00001578
Iteration 188/1000 | Loss: 0.00001577
Iteration 189/1000 | Loss: 0.00001577
Iteration 190/1000 | Loss: 0.00001577
Iteration 191/1000 | Loss: 0.00001577
Iteration 192/1000 | Loss: 0.00001577
Iteration 193/1000 | Loss: 0.00001577
Iteration 194/1000 | Loss: 0.00001577
Iteration 195/1000 | Loss: 0.00001577
Iteration 196/1000 | Loss: 0.00001577
Iteration 197/1000 | Loss: 0.00001577
Iteration 198/1000 | Loss: 0.00001577
Iteration 199/1000 | Loss: 0.00001577
Iteration 200/1000 | Loss: 0.00001577
Iteration 201/1000 | Loss: 0.00001577
Iteration 202/1000 | Loss: 0.00001577
Iteration 203/1000 | Loss: 0.00001577
Iteration 204/1000 | Loss: 0.00001577
Iteration 205/1000 | Loss: 0.00001577
Iteration 206/1000 | Loss: 0.00001576
Iteration 207/1000 | Loss: 0.00001576
Iteration 208/1000 | Loss: 0.00001576
Iteration 209/1000 | Loss: 0.00001576
Iteration 210/1000 | Loss: 0.00001576
Iteration 211/1000 | Loss: 0.00001576
Iteration 212/1000 | Loss: 0.00001576
Iteration 213/1000 | Loss: 0.00001576
Iteration 214/1000 | Loss: 0.00001576
Iteration 215/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.5762891052872874e-05, 1.5762891052872874e-05, 1.5762891052872874e-05, 1.5762891052872874e-05, 1.5762891052872874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5762891052872874e-05

Optimization complete. Final v2v error: 3.115682601928711 mm

Highest mean error: 12.165586471557617 mm for frame 62

Lowest mean error: 2.8176372051239014 mm for frame 1

Saving results

Total time: 284.0254282951355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377386
Iteration 2/25 | Loss: 0.00096522
Iteration 3/25 | Loss: 0.00086486
Iteration 4/25 | Loss: 0.00085135
Iteration 5/25 | Loss: 0.00084673
Iteration 6/25 | Loss: 0.00084648
Iteration 7/25 | Loss: 0.00084648
Iteration 8/25 | Loss: 0.00084648
Iteration 9/25 | Loss: 0.00084648
Iteration 10/25 | Loss: 0.00084648
Iteration 11/25 | Loss: 0.00084648
Iteration 12/25 | Loss: 0.00084648
Iteration 13/25 | Loss: 0.00084648
Iteration 14/25 | Loss: 0.00084648
Iteration 15/25 | Loss: 0.00084648
Iteration 16/25 | Loss: 0.00084648
Iteration 17/25 | Loss: 0.00084648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008464812417514622, 0.0008464812417514622, 0.0008464812417514622, 0.0008464812417514622, 0.0008464812417514622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008464812417514622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58395946
Iteration 2/25 | Loss: 0.00031053
Iteration 3/25 | Loss: 0.00031048
Iteration 4/25 | Loss: 0.00031048
Iteration 5/25 | Loss: 0.00031048
Iteration 6/25 | Loss: 0.00031048
Iteration 7/25 | Loss: 0.00031048
Iteration 8/25 | Loss: 0.00031048
Iteration 9/25 | Loss: 0.00031048
Iteration 10/25 | Loss: 0.00031048
Iteration 11/25 | Loss: 0.00031048
Iteration 12/25 | Loss: 0.00031048
Iteration 13/25 | Loss: 0.00031048
Iteration 14/25 | Loss: 0.00031048
Iteration 15/25 | Loss: 0.00031048
Iteration 16/25 | Loss: 0.00031048
Iteration 17/25 | Loss: 0.00031048
Iteration 18/25 | Loss: 0.00031048
Iteration 19/25 | Loss: 0.00031048
Iteration 20/25 | Loss: 0.00031048
Iteration 21/25 | Loss: 0.00031048
Iteration 22/25 | Loss: 0.00031048
Iteration 23/25 | Loss: 0.00031048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00031048027449287474, 0.00031048027449287474, 0.00031048027449287474, 0.00031048027449287474, 0.00031048027449287474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031048027449287474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031048
Iteration 2/1000 | Loss: 0.00002442
Iteration 3/1000 | Loss: 0.00001365
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001093
Iteration 6/1000 | Loss: 0.00001039
Iteration 7/1000 | Loss: 0.00001001
Iteration 8/1000 | Loss: 0.00000986
Iteration 9/1000 | Loss: 0.00000981
Iteration 10/1000 | Loss: 0.00000977
Iteration 11/1000 | Loss: 0.00000974
Iteration 12/1000 | Loss: 0.00000958
Iteration 13/1000 | Loss: 0.00000958
Iteration 14/1000 | Loss: 0.00000958
Iteration 15/1000 | Loss: 0.00000958
Iteration 16/1000 | Loss: 0.00000958
Iteration 17/1000 | Loss: 0.00000958
Iteration 18/1000 | Loss: 0.00000958
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000954
Iteration 21/1000 | Loss: 0.00000953
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000953
Iteration 24/1000 | Loss: 0.00000953
Iteration 25/1000 | Loss: 0.00000953
Iteration 26/1000 | Loss: 0.00000953
Iteration 27/1000 | Loss: 0.00000953
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000951
Iteration 30/1000 | Loss: 0.00000950
Iteration 31/1000 | Loss: 0.00000950
Iteration 32/1000 | Loss: 0.00000949
Iteration 33/1000 | Loss: 0.00000949
Iteration 34/1000 | Loss: 0.00000949
Iteration 35/1000 | Loss: 0.00000949
Iteration 36/1000 | Loss: 0.00000949
Iteration 37/1000 | Loss: 0.00000948
Iteration 38/1000 | Loss: 0.00000948
Iteration 39/1000 | Loss: 0.00000948
Iteration 40/1000 | Loss: 0.00000948
Iteration 41/1000 | Loss: 0.00000948
Iteration 42/1000 | Loss: 0.00000948
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000947
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000946
Iteration 47/1000 | Loss: 0.00000946
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000945
Iteration 50/1000 | Loss: 0.00000945
Iteration 51/1000 | Loss: 0.00000945
Iteration 52/1000 | Loss: 0.00000945
Iteration 53/1000 | Loss: 0.00000945
Iteration 54/1000 | Loss: 0.00000944
Iteration 55/1000 | Loss: 0.00000944
Iteration 56/1000 | Loss: 0.00000944
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000943
Iteration 59/1000 | Loss: 0.00000941
Iteration 60/1000 | Loss: 0.00000941
Iteration 61/1000 | Loss: 0.00000941
Iteration 62/1000 | Loss: 0.00000940
Iteration 63/1000 | Loss: 0.00000940
Iteration 64/1000 | Loss: 0.00000940
Iteration 65/1000 | Loss: 0.00000939
Iteration 66/1000 | Loss: 0.00000938
Iteration 67/1000 | Loss: 0.00000938
Iteration 68/1000 | Loss: 0.00000937
Iteration 69/1000 | Loss: 0.00000936
Iteration 70/1000 | Loss: 0.00000936
Iteration 71/1000 | Loss: 0.00000935
Iteration 72/1000 | Loss: 0.00000935
Iteration 73/1000 | Loss: 0.00000934
Iteration 74/1000 | Loss: 0.00000934
Iteration 75/1000 | Loss: 0.00000934
Iteration 76/1000 | Loss: 0.00000933
Iteration 77/1000 | Loss: 0.00000933
Iteration 78/1000 | Loss: 0.00000933
Iteration 79/1000 | Loss: 0.00000933
Iteration 80/1000 | Loss: 0.00000932
Iteration 81/1000 | Loss: 0.00000932
Iteration 82/1000 | Loss: 0.00000932
Iteration 83/1000 | Loss: 0.00000932
Iteration 84/1000 | Loss: 0.00000932
Iteration 85/1000 | Loss: 0.00000932
Iteration 86/1000 | Loss: 0.00000931
Iteration 87/1000 | Loss: 0.00000931
Iteration 88/1000 | Loss: 0.00000931
Iteration 89/1000 | Loss: 0.00000931
Iteration 90/1000 | Loss: 0.00000930
Iteration 91/1000 | Loss: 0.00000930
Iteration 92/1000 | Loss: 0.00000928
Iteration 93/1000 | Loss: 0.00000928
Iteration 94/1000 | Loss: 0.00000928
Iteration 95/1000 | Loss: 0.00000928
Iteration 96/1000 | Loss: 0.00000928
Iteration 97/1000 | Loss: 0.00000927
Iteration 98/1000 | Loss: 0.00000927
Iteration 99/1000 | Loss: 0.00000927
Iteration 100/1000 | Loss: 0.00000927
Iteration 101/1000 | Loss: 0.00000927
Iteration 102/1000 | Loss: 0.00000927
Iteration 103/1000 | Loss: 0.00000927
Iteration 104/1000 | Loss: 0.00000926
Iteration 105/1000 | Loss: 0.00000926
Iteration 106/1000 | Loss: 0.00000926
Iteration 107/1000 | Loss: 0.00000925
Iteration 108/1000 | Loss: 0.00000924
Iteration 109/1000 | Loss: 0.00000924
Iteration 110/1000 | Loss: 0.00000924
Iteration 111/1000 | Loss: 0.00000924
Iteration 112/1000 | Loss: 0.00000924
Iteration 113/1000 | Loss: 0.00000923
Iteration 114/1000 | Loss: 0.00000923
Iteration 115/1000 | Loss: 0.00000923
Iteration 116/1000 | Loss: 0.00000923
Iteration 117/1000 | Loss: 0.00000923
Iteration 118/1000 | Loss: 0.00000923
Iteration 119/1000 | Loss: 0.00000923
Iteration 120/1000 | Loss: 0.00000922
Iteration 121/1000 | Loss: 0.00000922
Iteration 122/1000 | Loss: 0.00000921
Iteration 123/1000 | Loss: 0.00000921
Iteration 124/1000 | Loss: 0.00000921
Iteration 125/1000 | Loss: 0.00000920
Iteration 126/1000 | Loss: 0.00000920
Iteration 127/1000 | Loss: 0.00000920
Iteration 128/1000 | Loss: 0.00000920
Iteration 129/1000 | Loss: 0.00000920
Iteration 130/1000 | Loss: 0.00000920
Iteration 131/1000 | Loss: 0.00000920
Iteration 132/1000 | Loss: 0.00000919
Iteration 133/1000 | Loss: 0.00000919
Iteration 134/1000 | Loss: 0.00000919
Iteration 135/1000 | Loss: 0.00000919
Iteration 136/1000 | Loss: 0.00000919
Iteration 137/1000 | Loss: 0.00000919
Iteration 138/1000 | Loss: 0.00000918
Iteration 139/1000 | Loss: 0.00000918
Iteration 140/1000 | Loss: 0.00000918
Iteration 141/1000 | Loss: 0.00000918
Iteration 142/1000 | Loss: 0.00000918
Iteration 143/1000 | Loss: 0.00000918
Iteration 144/1000 | Loss: 0.00000918
Iteration 145/1000 | Loss: 0.00000918
Iteration 146/1000 | Loss: 0.00000918
Iteration 147/1000 | Loss: 0.00000918
Iteration 148/1000 | Loss: 0.00000918
Iteration 149/1000 | Loss: 0.00000918
Iteration 150/1000 | Loss: 0.00000917
Iteration 151/1000 | Loss: 0.00000917
Iteration 152/1000 | Loss: 0.00000917
Iteration 153/1000 | Loss: 0.00000917
Iteration 154/1000 | Loss: 0.00000917
Iteration 155/1000 | Loss: 0.00000917
Iteration 156/1000 | Loss: 0.00000917
Iteration 157/1000 | Loss: 0.00000917
Iteration 158/1000 | Loss: 0.00000917
Iteration 159/1000 | Loss: 0.00000917
Iteration 160/1000 | Loss: 0.00000917
Iteration 161/1000 | Loss: 0.00000917
Iteration 162/1000 | Loss: 0.00000917
Iteration 163/1000 | Loss: 0.00000917
Iteration 164/1000 | Loss: 0.00000917
Iteration 165/1000 | Loss: 0.00000917
Iteration 166/1000 | Loss: 0.00000917
Iteration 167/1000 | Loss: 0.00000917
Iteration 168/1000 | Loss: 0.00000917
Iteration 169/1000 | Loss: 0.00000917
Iteration 170/1000 | Loss: 0.00000917
Iteration 171/1000 | Loss: 0.00000917
Iteration 172/1000 | Loss: 0.00000917
Iteration 173/1000 | Loss: 0.00000917
Iteration 174/1000 | Loss: 0.00000917
Iteration 175/1000 | Loss: 0.00000917
Iteration 176/1000 | Loss: 0.00000917
Iteration 177/1000 | Loss: 0.00000917
Iteration 178/1000 | Loss: 0.00000917
Iteration 179/1000 | Loss: 0.00000917
Iteration 180/1000 | Loss: 0.00000917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [9.166650670522358e-06, 9.166650670522358e-06, 9.166650670522358e-06, 9.166650670522358e-06, 9.166650670522358e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.166650670522358e-06

Optimization complete. Final v2v error: 2.5803732872009277 mm

Highest mean error: 3.1155946254730225 mm for frame 128

Lowest mean error: 2.225064277648926 mm for frame 86

Saving results

Total time: 108.71467614173889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995425
Iteration 2/25 | Loss: 0.00183902
Iteration 3/25 | Loss: 0.00177463
Iteration 4/25 | Loss: 0.00133908
Iteration 5/25 | Loss: 0.00136824
Iteration 6/25 | Loss: 0.00114792
Iteration 7/25 | Loss: 0.00113559
Iteration 8/25 | Loss: 0.00113288
Iteration 9/25 | Loss: 0.00113094
Iteration 10/25 | Loss: 0.00113063
Iteration 11/25 | Loss: 0.00113062
Iteration 12/25 | Loss: 0.00113062
Iteration 13/25 | Loss: 0.00113062
Iteration 14/25 | Loss: 0.00113062
Iteration 15/25 | Loss: 0.00113062
Iteration 16/25 | Loss: 0.00113062
Iteration 17/25 | Loss: 0.00113062
Iteration 18/25 | Loss: 0.00113062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001130619435571134, 0.001130619435571134, 0.001130619435571134, 0.001130619435571134, 0.001130619435571134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001130619435571134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30803907
Iteration 2/25 | Loss: 0.00044156
Iteration 3/25 | Loss: 0.00044156
Iteration 4/25 | Loss: 0.00044156
Iteration 5/25 | Loss: 0.00044156
Iteration 6/25 | Loss: 0.00044156
Iteration 7/25 | Loss: 0.00044156
Iteration 8/25 | Loss: 0.00044156
Iteration 9/25 | Loss: 0.00044156
Iteration 10/25 | Loss: 0.00044156
Iteration 11/25 | Loss: 0.00044156
Iteration 12/25 | Loss: 0.00044156
Iteration 13/25 | Loss: 0.00044156
Iteration 14/25 | Loss: 0.00044156
Iteration 15/25 | Loss: 0.00044156
Iteration 16/25 | Loss: 0.00044156
Iteration 17/25 | Loss: 0.00044156
Iteration 18/25 | Loss: 0.00044156
Iteration 19/25 | Loss: 0.00044156
Iteration 20/25 | Loss: 0.00044156
Iteration 21/25 | Loss: 0.00044156
Iteration 22/25 | Loss: 0.00044156
Iteration 23/25 | Loss: 0.00044156
Iteration 24/25 | Loss: 0.00044156
Iteration 25/25 | Loss: 0.00044156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044156
Iteration 2/1000 | Loss: 0.00006775
Iteration 3/1000 | Loss: 0.00045651
Iteration 4/1000 | Loss: 0.00006126
Iteration 5/1000 | Loss: 0.00004009
Iteration 6/1000 | Loss: 0.00003375
Iteration 7/1000 | Loss: 0.00003170
Iteration 8/1000 | Loss: 0.00003041
Iteration 9/1000 | Loss: 0.00002939
Iteration 10/1000 | Loss: 0.00002886
Iteration 11/1000 | Loss: 0.00002839
Iteration 12/1000 | Loss: 0.00002808
Iteration 13/1000 | Loss: 0.00002787
Iteration 14/1000 | Loss: 0.00002776
Iteration 15/1000 | Loss: 0.00002775
Iteration 16/1000 | Loss: 0.00002769
Iteration 17/1000 | Loss: 0.00032224
Iteration 18/1000 | Loss: 0.00173081
Iteration 19/1000 | Loss: 0.00003059
Iteration 20/1000 | Loss: 0.00002779
Iteration 21/1000 | Loss: 0.00002761
Iteration 22/1000 | Loss: 0.00002754
Iteration 23/1000 | Loss: 0.00002751
Iteration 24/1000 | Loss: 0.00002750
Iteration 25/1000 | Loss: 0.00002749
Iteration 26/1000 | Loss: 0.00002748
Iteration 27/1000 | Loss: 0.00002748
Iteration 28/1000 | Loss: 0.00002748
Iteration 29/1000 | Loss: 0.00002748
Iteration 30/1000 | Loss: 0.00002748
Iteration 31/1000 | Loss: 0.00002747
Iteration 32/1000 | Loss: 0.00002747
Iteration 33/1000 | Loss: 0.00002747
Iteration 34/1000 | Loss: 0.00002747
Iteration 35/1000 | Loss: 0.00002747
Iteration 36/1000 | Loss: 0.00002747
Iteration 37/1000 | Loss: 0.00002746
Iteration 38/1000 | Loss: 0.00002746
Iteration 39/1000 | Loss: 0.00002746
Iteration 40/1000 | Loss: 0.00002746
Iteration 41/1000 | Loss: 0.00002746
Iteration 42/1000 | Loss: 0.00002746
Iteration 43/1000 | Loss: 0.00002746
Iteration 44/1000 | Loss: 0.00002746
Iteration 45/1000 | Loss: 0.00002746
Iteration 46/1000 | Loss: 0.00002745
Iteration 47/1000 | Loss: 0.00002745
Iteration 48/1000 | Loss: 0.00002745
Iteration 49/1000 | Loss: 0.00002745
Iteration 50/1000 | Loss: 0.00002745
Iteration 51/1000 | Loss: 0.00002745
Iteration 52/1000 | Loss: 0.00002745
Iteration 53/1000 | Loss: 0.00002745
Iteration 54/1000 | Loss: 0.00002745
Iteration 55/1000 | Loss: 0.00002744
Iteration 56/1000 | Loss: 0.00002744
Iteration 57/1000 | Loss: 0.00002744
Iteration 58/1000 | Loss: 0.00002744
Iteration 59/1000 | Loss: 0.00002743
Iteration 60/1000 | Loss: 0.00002743
Iteration 61/1000 | Loss: 0.00002743
Iteration 62/1000 | Loss: 0.00002743
Iteration 63/1000 | Loss: 0.00002743
Iteration 64/1000 | Loss: 0.00002743
Iteration 65/1000 | Loss: 0.00002743
Iteration 66/1000 | Loss: 0.00002743
Iteration 67/1000 | Loss: 0.00002743
Iteration 68/1000 | Loss: 0.00002743
Iteration 69/1000 | Loss: 0.00002742
Iteration 70/1000 | Loss: 0.00002742
Iteration 71/1000 | Loss: 0.00002742
Iteration 72/1000 | Loss: 0.00002742
Iteration 73/1000 | Loss: 0.00002742
Iteration 74/1000 | Loss: 0.00002742
Iteration 75/1000 | Loss: 0.00002742
Iteration 76/1000 | Loss: 0.00002742
Iteration 77/1000 | Loss: 0.00002742
Iteration 78/1000 | Loss: 0.00002742
Iteration 79/1000 | Loss: 0.00002742
Iteration 80/1000 | Loss: 0.00002742
Iteration 81/1000 | Loss: 0.00002741
Iteration 82/1000 | Loss: 0.00002741
Iteration 83/1000 | Loss: 0.00002741
Iteration 84/1000 | Loss: 0.00002741
Iteration 85/1000 | Loss: 0.00002741
Iteration 86/1000 | Loss: 0.00002740
Iteration 87/1000 | Loss: 0.00002740
Iteration 88/1000 | Loss: 0.00002740
Iteration 89/1000 | Loss: 0.00002740
Iteration 90/1000 | Loss: 0.00002740
Iteration 91/1000 | Loss: 0.00002739
Iteration 92/1000 | Loss: 0.00002739
Iteration 93/1000 | Loss: 0.00002739
Iteration 94/1000 | Loss: 0.00002739
Iteration 95/1000 | Loss: 0.00002739
Iteration 96/1000 | Loss: 0.00002739
Iteration 97/1000 | Loss: 0.00002739
Iteration 98/1000 | Loss: 0.00002739
Iteration 99/1000 | Loss: 0.00002739
Iteration 100/1000 | Loss: 0.00002738
Iteration 101/1000 | Loss: 0.00032977
Iteration 102/1000 | Loss: 0.00003886
Iteration 103/1000 | Loss: 0.00003051
Iteration 104/1000 | Loss: 0.00002782
Iteration 105/1000 | Loss: 0.00002747
Iteration 106/1000 | Loss: 0.00002741
Iteration 107/1000 | Loss: 0.00002740
Iteration 108/1000 | Loss: 0.00002740
Iteration 109/1000 | Loss: 0.00002739
Iteration 110/1000 | Loss: 0.00002739
Iteration 111/1000 | Loss: 0.00002738
Iteration 112/1000 | Loss: 0.00002738
Iteration 113/1000 | Loss: 0.00002738
Iteration 114/1000 | Loss: 0.00002738
Iteration 115/1000 | Loss: 0.00002738
Iteration 116/1000 | Loss: 0.00002738
Iteration 117/1000 | Loss: 0.00002738
Iteration 118/1000 | Loss: 0.00002738
Iteration 119/1000 | Loss: 0.00002738
Iteration 120/1000 | Loss: 0.00002737
Iteration 121/1000 | Loss: 0.00002737
Iteration 122/1000 | Loss: 0.00002737
Iteration 123/1000 | Loss: 0.00002737
Iteration 124/1000 | Loss: 0.00002737
Iteration 125/1000 | Loss: 0.00002737
Iteration 126/1000 | Loss: 0.00002736
Iteration 127/1000 | Loss: 0.00002736
Iteration 128/1000 | Loss: 0.00002736
Iteration 129/1000 | Loss: 0.00002736
Iteration 130/1000 | Loss: 0.00002735
Iteration 131/1000 | Loss: 0.00002735
Iteration 132/1000 | Loss: 0.00002735
Iteration 133/1000 | Loss: 0.00002735
Iteration 134/1000 | Loss: 0.00002735
Iteration 135/1000 | Loss: 0.00002735
Iteration 136/1000 | Loss: 0.00002735
Iteration 137/1000 | Loss: 0.00002735
Iteration 138/1000 | Loss: 0.00002734
Iteration 139/1000 | Loss: 0.00002734
Iteration 140/1000 | Loss: 0.00002734
Iteration 141/1000 | Loss: 0.00002734
Iteration 142/1000 | Loss: 0.00002734
Iteration 143/1000 | Loss: 0.00002734
Iteration 144/1000 | Loss: 0.00002734
Iteration 145/1000 | Loss: 0.00002734
Iteration 146/1000 | Loss: 0.00002734
Iteration 147/1000 | Loss: 0.00002734
Iteration 148/1000 | Loss: 0.00002734
Iteration 149/1000 | Loss: 0.00002734
Iteration 150/1000 | Loss: 0.00002734
Iteration 151/1000 | Loss: 0.00002734
Iteration 152/1000 | Loss: 0.00002734
Iteration 153/1000 | Loss: 0.00002734
Iteration 154/1000 | Loss: 0.00002734
Iteration 155/1000 | Loss: 0.00002734
Iteration 156/1000 | Loss: 0.00002734
Iteration 157/1000 | Loss: 0.00002734
Iteration 158/1000 | Loss: 0.00002734
Iteration 159/1000 | Loss: 0.00002734
Iteration 160/1000 | Loss: 0.00002734
Iteration 161/1000 | Loss: 0.00002734
Iteration 162/1000 | Loss: 0.00002734
Iteration 163/1000 | Loss: 0.00002734
Iteration 164/1000 | Loss: 0.00002734
Iteration 165/1000 | Loss: 0.00002734
Iteration 166/1000 | Loss: 0.00002734
Iteration 167/1000 | Loss: 0.00002734
Iteration 168/1000 | Loss: 0.00002734
Iteration 169/1000 | Loss: 0.00002734
Iteration 170/1000 | Loss: 0.00002734
Iteration 171/1000 | Loss: 0.00002734
Iteration 172/1000 | Loss: 0.00002734
Iteration 173/1000 | Loss: 0.00002734
Iteration 174/1000 | Loss: 0.00002734
Iteration 175/1000 | Loss: 0.00002734
Iteration 176/1000 | Loss: 0.00002734
Iteration 177/1000 | Loss: 0.00002734
Iteration 178/1000 | Loss: 0.00002734
Iteration 179/1000 | Loss: 0.00002734
Iteration 180/1000 | Loss: 0.00002734
Iteration 181/1000 | Loss: 0.00002734
Iteration 182/1000 | Loss: 0.00002734
Iteration 183/1000 | Loss: 0.00002734
Iteration 184/1000 | Loss: 0.00002734
Iteration 185/1000 | Loss: 0.00002734
Iteration 186/1000 | Loss: 0.00002734
Iteration 187/1000 | Loss: 0.00002734
Iteration 188/1000 | Loss: 0.00002734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.7337846404407173e-05, 2.7337846404407173e-05, 2.7337846404407173e-05, 2.7337846404407173e-05, 2.7337846404407173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7337846404407173e-05

Optimization complete. Final v2v error: 4.445963382720947 mm

Highest mean error: 5.07784366607666 mm for frame 0

Lowest mean error: 4.103210926055908 mm for frame 203

Saving results

Total time: 180.27746868133545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880323
Iteration 2/25 | Loss: 0.00121587
Iteration 3/25 | Loss: 0.00095253
Iteration 4/25 | Loss: 0.00092285
Iteration 5/25 | Loss: 0.00091743
Iteration 6/25 | Loss: 0.00091648
Iteration 7/25 | Loss: 0.00091629
Iteration 8/25 | Loss: 0.00091629
Iteration 9/25 | Loss: 0.00091629
Iteration 10/25 | Loss: 0.00091629
Iteration 11/25 | Loss: 0.00091629
Iteration 12/25 | Loss: 0.00091629
Iteration 13/25 | Loss: 0.00091629
Iteration 14/25 | Loss: 0.00091629
Iteration 15/25 | Loss: 0.00091629
Iteration 16/25 | Loss: 0.00091629
Iteration 17/25 | Loss: 0.00091629
Iteration 18/25 | Loss: 0.00091629
Iteration 19/25 | Loss: 0.00091629
Iteration 20/25 | Loss: 0.00091629
Iteration 21/25 | Loss: 0.00091629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009162859059870243, 0.0009162859059870243, 0.0009162859059870243, 0.0009162859059870243, 0.0009162859059870243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009162859059870243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27240670
Iteration 2/25 | Loss: 0.00031356
Iteration 3/25 | Loss: 0.00031351
Iteration 4/25 | Loss: 0.00031351
Iteration 5/25 | Loss: 0.00031351
Iteration 6/25 | Loss: 0.00031351
Iteration 7/25 | Loss: 0.00031351
Iteration 8/25 | Loss: 0.00031351
Iteration 9/25 | Loss: 0.00031351
Iteration 10/25 | Loss: 0.00031351
Iteration 11/25 | Loss: 0.00031351
Iteration 12/25 | Loss: 0.00031351
Iteration 13/25 | Loss: 0.00031351
Iteration 14/25 | Loss: 0.00031351
Iteration 15/25 | Loss: 0.00031351
Iteration 16/25 | Loss: 0.00031351
Iteration 17/25 | Loss: 0.00031351
Iteration 18/25 | Loss: 0.00031351
Iteration 19/25 | Loss: 0.00031351
Iteration 20/25 | Loss: 0.00031351
Iteration 21/25 | Loss: 0.00031351
Iteration 22/25 | Loss: 0.00031351
Iteration 23/25 | Loss: 0.00031351
Iteration 24/25 | Loss: 0.00031351
Iteration 25/25 | Loss: 0.00031351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031351
Iteration 2/1000 | Loss: 0.00003768
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00001976
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001709
Iteration 7/1000 | Loss: 0.00001654
Iteration 8/1000 | Loss: 0.00001609
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001514
Iteration 15/1000 | Loss: 0.00001508
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001491
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001490
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001486
Iteration 34/1000 | Loss: 0.00001485
Iteration 35/1000 | Loss: 0.00001485
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001473
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001467
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001466
Iteration 69/1000 | Loss: 0.00001466
Iteration 70/1000 | Loss: 0.00001466
Iteration 71/1000 | Loss: 0.00001466
Iteration 72/1000 | Loss: 0.00001466
Iteration 73/1000 | Loss: 0.00001466
Iteration 74/1000 | Loss: 0.00001466
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001465
Iteration 77/1000 | Loss: 0.00001465
Iteration 78/1000 | Loss: 0.00001465
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001465
Iteration 82/1000 | Loss: 0.00001465
Iteration 83/1000 | Loss: 0.00001465
Iteration 84/1000 | Loss: 0.00001465
Iteration 85/1000 | Loss: 0.00001464
Iteration 86/1000 | Loss: 0.00001464
Iteration 87/1000 | Loss: 0.00001464
Iteration 88/1000 | Loss: 0.00001464
Iteration 89/1000 | Loss: 0.00001464
Iteration 90/1000 | Loss: 0.00001464
Iteration 91/1000 | Loss: 0.00001464
Iteration 92/1000 | Loss: 0.00001464
Iteration 93/1000 | Loss: 0.00001464
Iteration 94/1000 | Loss: 0.00001464
Iteration 95/1000 | Loss: 0.00001464
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001464
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001463
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001462
Iteration 103/1000 | Loss: 0.00001462
Iteration 104/1000 | Loss: 0.00001462
Iteration 105/1000 | Loss: 0.00001462
Iteration 106/1000 | Loss: 0.00001462
Iteration 107/1000 | Loss: 0.00001462
Iteration 108/1000 | Loss: 0.00001462
Iteration 109/1000 | Loss: 0.00001462
Iteration 110/1000 | Loss: 0.00001462
Iteration 111/1000 | Loss: 0.00001462
Iteration 112/1000 | Loss: 0.00001462
Iteration 113/1000 | Loss: 0.00001462
Iteration 114/1000 | Loss: 0.00001462
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001462
Iteration 117/1000 | Loss: 0.00001462
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001462
Iteration 120/1000 | Loss: 0.00001462
Iteration 121/1000 | Loss: 0.00001462
Iteration 122/1000 | Loss: 0.00001462
Iteration 123/1000 | Loss: 0.00001462
Iteration 124/1000 | Loss: 0.00001462
Iteration 125/1000 | Loss: 0.00001462
Iteration 126/1000 | Loss: 0.00001462
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001462
Iteration 130/1000 | Loss: 0.00001462
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001462
Iteration 140/1000 | Loss: 0.00001462
Iteration 141/1000 | Loss: 0.00001462
Iteration 142/1000 | Loss: 0.00001462
Iteration 143/1000 | Loss: 0.00001462
Iteration 144/1000 | Loss: 0.00001462
Iteration 145/1000 | Loss: 0.00001462
Iteration 146/1000 | Loss: 0.00001462
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Iteration 159/1000 | Loss: 0.00001462
Iteration 160/1000 | Loss: 0.00001462
Iteration 161/1000 | Loss: 0.00001462
Iteration 162/1000 | Loss: 0.00001462
Iteration 163/1000 | Loss: 0.00001462
Iteration 164/1000 | Loss: 0.00001462
Iteration 165/1000 | Loss: 0.00001462
Iteration 166/1000 | Loss: 0.00001462
Iteration 167/1000 | Loss: 0.00001462
Iteration 168/1000 | Loss: 0.00001462
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001462
Iteration 181/1000 | Loss: 0.00001462
Iteration 182/1000 | Loss: 0.00001462
Iteration 183/1000 | Loss: 0.00001462
Iteration 184/1000 | Loss: 0.00001462
Iteration 185/1000 | Loss: 0.00001462
Iteration 186/1000 | Loss: 0.00001462
Iteration 187/1000 | Loss: 0.00001462
Iteration 188/1000 | Loss: 0.00001462
Iteration 189/1000 | Loss: 0.00001462
Iteration 190/1000 | Loss: 0.00001462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.461612009734381e-05, 1.461612009734381e-05, 1.461612009734381e-05, 1.461612009734381e-05, 1.461612009734381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.461612009734381e-05

Optimization complete. Final v2v error: 3.2543795108795166 mm

Highest mean error: 3.504912853240967 mm for frame 71

Lowest mean error: 2.881040573120117 mm for frame 58

Saving results

Total time: 84.00544786453247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820603
Iteration 2/25 | Loss: 0.00140066
Iteration 3/25 | Loss: 0.00103852
Iteration 4/25 | Loss: 0.00100771
Iteration 5/25 | Loss: 0.00100498
Iteration 6/25 | Loss: 0.00100498
Iteration 7/25 | Loss: 0.00100498
Iteration 8/25 | Loss: 0.00100498
Iteration 9/25 | Loss: 0.00100498
Iteration 10/25 | Loss: 0.00100498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010049823904410005, 0.0010049823904410005, 0.0010049823904410005, 0.0010049823904410005, 0.0010049823904410005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010049823904410005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49639904
Iteration 2/25 | Loss: 0.00035662
Iteration 3/25 | Loss: 0.00035659
Iteration 4/25 | Loss: 0.00035659
Iteration 5/25 | Loss: 0.00035659
Iteration 6/25 | Loss: 0.00035659
Iteration 7/25 | Loss: 0.00035659
Iteration 8/25 | Loss: 0.00035659
Iteration 9/25 | Loss: 0.00035659
Iteration 10/25 | Loss: 0.00035659
Iteration 11/25 | Loss: 0.00035659
Iteration 12/25 | Loss: 0.00035659
Iteration 13/25 | Loss: 0.00035659
Iteration 14/25 | Loss: 0.00035659
Iteration 15/25 | Loss: 0.00035659
Iteration 16/25 | Loss: 0.00035659
Iteration 17/25 | Loss: 0.00035659
Iteration 18/25 | Loss: 0.00035659
Iteration 19/25 | Loss: 0.00035659
Iteration 20/25 | Loss: 0.00035659
Iteration 21/25 | Loss: 0.00035659
Iteration 22/25 | Loss: 0.00035659
Iteration 23/25 | Loss: 0.00035659
Iteration 24/25 | Loss: 0.00035659
Iteration 25/25 | Loss: 0.00035659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035659
Iteration 2/1000 | Loss: 0.00003578
Iteration 3/1000 | Loss: 0.00002354
Iteration 4/1000 | Loss: 0.00002141
Iteration 5/1000 | Loss: 0.00002043
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001943
Iteration 8/1000 | Loss: 0.00001901
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001864
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001844
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001841
Iteration 16/1000 | Loss: 0.00001841
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001838
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001837
Iteration 22/1000 | Loss: 0.00001835
Iteration 23/1000 | Loss: 0.00001835
Iteration 24/1000 | Loss: 0.00001835
Iteration 25/1000 | Loss: 0.00001834
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001834
Iteration 29/1000 | Loss: 0.00001834
Iteration 30/1000 | Loss: 0.00001834
Iteration 31/1000 | Loss: 0.00001834
Iteration 32/1000 | Loss: 0.00001834
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001833
Iteration 40/1000 | Loss: 0.00001832
Iteration 41/1000 | Loss: 0.00001832
Iteration 42/1000 | Loss: 0.00001831
Iteration 43/1000 | Loss: 0.00001831
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001830
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001827
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001826
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001825
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001824
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001823
Iteration 61/1000 | Loss: 0.00001823
Iteration 62/1000 | Loss: 0.00001823
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001822
Iteration 66/1000 | Loss: 0.00001822
Iteration 67/1000 | Loss: 0.00001822
Iteration 68/1000 | Loss: 0.00001822
Iteration 69/1000 | Loss: 0.00001822
Iteration 70/1000 | Loss: 0.00001822
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001821
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001820
Iteration 76/1000 | Loss: 0.00001820
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001819
Iteration 79/1000 | Loss: 0.00001819
Iteration 80/1000 | Loss: 0.00001818
Iteration 81/1000 | Loss: 0.00001818
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001817
Iteration 90/1000 | Loss: 0.00001817
Iteration 91/1000 | Loss: 0.00001817
Iteration 92/1000 | Loss: 0.00001817
Iteration 93/1000 | Loss: 0.00001817
Iteration 94/1000 | Loss: 0.00001817
Iteration 95/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.8167989765061066e-05, 1.8167989765061066e-05, 1.8167989765061066e-05, 1.8167989765061066e-05, 1.8167989765061066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8167989765061066e-05

Optimization complete. Final v2v error: 3.711083173751831 mm

Highest mean error: 4.225072383880615 mm for frame 173

Lowest mean error: 3.2060155868530273 mm for frame 201

Saving results

Total time: 92.45950770378113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816653
Iteration 2/25 | Loss: 0.00126389
Iteration 3/25 | Loss: 0.00098245
Iteration 4/25 | Loss: 0.00093749
Iteration 5/25 | Loss: 0.00093637
Iteration 6/25 | Loss: 0.00092450
Iteration 7/25 | Loss: 0.00091576
Iteration 8/25 | Loss: 0.00091150
Iteration 9/25 | Loss: 0.00091181
Iteration 10/25 | Loss: 0.00091059
Iteration 11/25 | Loss: 0.00090881
Iteration 12/25 | Loss: 0.00090488
Iteration 13/25 | Loss: 0.00090455
Iteration 14/25 | Loss: 0.00090767
Iteration 15/25 | Loss: 0.00090728
Iteration 16/25 | Loss: 0.00091208
Iteration 17/25 | Loss: 0.00091151
Iteration 18/25 | Loss: 0.00091209
Iteration 19/25 | Loss: 0.00091066
Iteration 20/25 | Loss: 0.00090430
Iteration 21/25 | Loss: 0.00090422
Iteration 22/25 | Loss: 0.00090422
Iteration 23/25 | Loss: 0.00090421
Iteration 24/25 | Loss: 0.00090421
Iteration 25/25 | Loss: 0.00090421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93993425
Iteration 2/25 | Loss: 0.00045887
Iteration 3/25 | Loss: 0.00045517
Iteration 4/25 | Loss: 0.00045517
Iteration 5/25 | Loss: 0.00045517
Iteration 6/25 | Loss: 0.00045517
Iteration 7/25 | Loss: 0.00045517
Iteration 8/25 | Loss: 0.00045517
Iteration 9/25 | Loss: 0.00045517
Iteration 10/25 | Loss: 0.00045517
Iteration 11/25 | Loss: 0.00045517
Iteration 12/25 | Loss: 0.00045517
Iteration 13/25 | Loss: 0.00045517
Iteration 14/25 | Loss: 0.00045517
Iteration 15/25 | Loss: 0.00045517
Iteration 16/25 | Loss: 0.00045517
Iteration 17/25 | Loss: 0.00045517
Iteration 18/25 | Loss: 0.00045517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00045517049147747457, 0.00045517049147747457, 0.00045517049147747457, 0.00045517049147747457, 0.00045517049147747457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045517049147747457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045517
Iteration 2/1000 | Loss: 0.00015831
Iteration 3/1000 | Loss: 0.00039794
Iteration 4/1000 | Loss: 0.00045581
Iteration 5/1000 | Loss: 0.00023543
Iteration 6/1000 | Loss: 0.00023472
Iteration 7/1000 | Loss: 0.00012776
Iteration 8/1000 | Loss: 0.00010102
Iteration 9/1000 | Loss: 0.00004713
Iteration 10/1000 | Loss: 0.00012010
Iteration 11/1000 | Loss: 0.00013119
Iteration 12/1000 | Loss: 0.00013166
Iteration 13/1000 | Loss: 0.00008041
Iteration 14/1000 | Loss: 0.00006780
Iteration 15/1000 | Loss: 0.00016320
Iteration 16/1000 | Loss: 0.00010640
Iteration 17/1000 | Loss: 0.00004839
Iteration 18/1000 | Loss: 0.00011428
Iteration 19/1000 | Loss: 0.00018851
Iteration 20/1000 | Loss: 0.00012355
Iteration 21/1000 | Loss: 0.00016103
Iteration 22/1000 | Loss: 0.00002633
Iteration 23/1000 | Loss: 0.00012610
Iteration 24/1000 | Loss: 0.00006737
Iteration 25/1000 | Loss: 0.00011375
Iteration 26/1000 | Loss: 0.00004728
Iteration 27/1000 | Loss: 0.00009242
Iteration 28/1000 | Loss: 0.00008287
Iteration 29/1000 | Loss: 0.00011633
Iteration 30/1000 | Loss: 0.00012225
Iteration 31/1000 | Loss: 0.00011938
Iteration 32/1000 | Loss: 0.00016135
Iteration 33/1000 | Loss: 0.00002060
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00005409
Iteration 36/1000 | Loss: 0.00005340
Iteration 37/1000 | Loss: 0.00004972
Iteration 38/1000 | Loss: 0.00016927
Iteration 39/1000 | Loss: 0.00017242
Iteration 40/1000 | Loss: 0.00014754
Iteration 41/1000 | Loss: 0.00002880
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00012106
Iteration 46/1000 | Loss: 0.00002515
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00013033
Iteration 51/1000 | Loss: 0.00014290
Iteration 52/1000 | Loss: 0.00008916
Iteration 53/1000 | Loss: 0.00014339
Iteration 54/1000 | Loss: 0.00008055
Iteration 55/1000 | Loss: 0.00008150
Iteration 56/1000 | Loss: 0.00005057
Iteration 57/1000 | Loss: 0.00002067
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00007160
Iteration 60/1000 | Loss: 0.00006521
Iteration 61/1000 | Loss: 0.00006621
Iteration 62/1000 | Loss: 0.00006039
Iteration 63/1000 | Loss: 0.00005859
Iteration 64/1000 | Loss: 0.00005758
Iteration 65/1000 | Loss: 0.00005931
Iteration 66/1000 | Loss: 0.00018499
Iteration 67/1000 | Loss: 0.00006825
Iteration 68/1000 | Loss: 0.00016840
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00016152
Iteration 73/1000 | Loss: 0.00011722
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001177
Iteration 119/1000 | Loss: 0.00001177
Iteration 120/1000 | Loss: 0.00001177
Iteration 121/1000 | Loss: 0.00001177
Iteration 122/1000 | Loss: 0.00001177
Iteration 123/1000 | Loss: 0.00001177
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001175
Iteration 137/1000 | Loss: 0.00001175
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001175
Iteration 142/1000 | Loss: 0.00001175
Iteration 143/1000 | Loss: 0.00001175
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001175
Iteration 147/1000 | Loss: 0.00001175
Iteration 148/1000 | Loss: 0.00001175
Iteration 149/1000 | Loss: 0.00001175
Iteration 150/1000 | Loss: 0.00001175
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001175
Iteration 157/1000 | Loss: 0.00001175
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001175
Iteration 166/1000 | Loss: 0.00001175
Iteration 167/1000 | Loss: 0.00001175
Iteration 168/1000 | Loss: 0.00001175
Iteration 169/1000 | Loss: 0.00001175
Iteration 170/1000 | Loss: 0.00001175
Iteration 171/1000 | Loss: 0.00001175
Iteration 172/1000 | Loss: 0.00001175
Iteration 173/1000 | Loss: 0.00001175
Iteration 174/1000 | Loss: 0.00001175
Iteration 175/1000 | Loss: 0.00001175
Iteration 176/1000 | Loss: 0.00001175
Iteration 177/1000 | Loss: 0.00001175
Iteration 178/1000 | Loss: 0.00001175
Iteration 179/1000 | Loss: 0.00001175
Iteration 180/1000 | Loss: 0.00001175
Iteration 181/1000 | Loss: 0.00001175
Iteration 182/1000 | Loss: 0.00001175
Iteration 183/1000 | Loss: 0.00001175
Iteration 184/1000 | Loss: 0.00001175
Iteration 185/1000 | Loss: 0.00001175
Iteration 186/1000 | Loss: 0.00001175
Iteration 187/1000 | Loss: 0.00001175
Iteration 188/1000 | Loss: 0.00001175
Iteration 189/1000 | Loss: 0.00001175
Iteration 190/1000 | Loss: 0.00001175
Iteration 191/1000 | Loss: 0.00001175
Iteration 192/1000 | Loss: 0.00001175
Iteration 193/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.1752799764508381e-05, 1.1752799764508381e-05, 1.1752799764508381e-05, 1.1752799764508381e-05, 1.1752799764508381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1752799764508381e-05

Optimization complete. Final v2v error: 2.953364372253418 mm

Highest mean error: 4.3550028800964355 mm for frame 39

Lowest mean error: 2.489900588989258 mm for frame 190

Saving results

Total time: 467.00550413131714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908162
Iteration 2/25 | Loss: 0.00135687
Iteration 3/25 | Loss: 0.00115628
Iteration 4/25 | Loss: 0.00109889
Iteration 5/25 | Loss: 0.00108974
Iteration 6/25 | Loss: 0.00108673
Iteration 7/25 | Loss: 0.00108579
Iteration 8/25 | Loss: 0.00108579
Iteration 9/25 | Loss: 0.00108579
Iteration 10/25 | Loss: 0.00108579
Iteration 11/25 | Loss: 0.00108579
Iteration 12/25 | Loss: 0.00108579
Iteration 13/25 | Loss: 0.00108579
Iteration 14/25 | Loss: 0.00108579
Iteration 15/25 | Loss: 0.00108579
Iteration 16/25 | Loss: 0.00108579
Iteration 17/25 | Loss: 0.00108579
Iteration 18/25 | Loss: 0.00108579
Iteration 19/25 | Loss: 0.00108579
Iteration 20/25 | Loss: 0.00108579
Iteration 21/25 | Loss: 0.00108579
Iteration 22/25 | Loss: 0.00108579
Iteration 23/25 | Loss: 0.00108579
Iteration 24/25 | Loss: 0.00108579
Iteration 25/25 | Loss: 0.00108579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38663244
Iteration 2/25 | Loss: 0.00072651
Iteration 3/25 | Loss: 0.00072651
Iteration 4/25 | Loss: 0.00072651
Iteration 5/25 | Loss: 0.00072651
Iteration 6/25 | Loss: 0.00072651
Iteration 7/25 | Loss: 0.00072651
Iteration 8/25 | Loss: 0.00072651
Iteration 9/25 | Loss: 0.00072651
Iteration 10/25 | Loss: 0.00072651
Iteration 11/25 | Loss: 0.00072651
Iteration 12/25 | Loss: 0.00072651
Iteration 13/25 | Loss: 0.00072651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000726506463252008, 0.000726506463252008, 0.000726506463252008, 0.000726506463252008, 0.000726506463252008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000726506463252008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072651
Iteration 2/1000 | Loss: 0.00010447
Iteration 3/1000 | Loss: 0.00005322
Iteration 4/1000 | Loss: 0.00003552
Iteration 5/1000 | Loss: 0.00003164
Iteration 6/1000 | Loss: 0.00002949
Iteration 7/1000 | Loss: 0.00002817
Iteration 8/1000 | Loss: 0.00002734
Iteration 9/1000 | Loss: 0.00002663
Iteration 10/1000 | Loss: 0.00002622
Iteration 11/1000 | Loss: 0.00002573
Iteration 12/1000 | Loss: 0.00002538
Iteration 13/1000 | Loss: 0.00002512
Iteration 14/1000 | Loss: 0.00002501
Iteration 15/1000 | Loss: 0.00002498
Iteration 16/1000 | Loss: 0.00002489
Iteration 17/1000 | Loss: 0.00002480
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002470
Iteration 20/1000 | Loss: 0.00002469
Iteration 21/1000 | Loss: 0.00002468
Iteration 22/1000 | Loss: 0.00002468
Iteration 23/1000 | Loss: 0.00002463
Iteration 24/1000 | Loss: 0.00002462
Iteration 25/1000 | Loss: 0.00002462
Iteration 26/1000 | Loss: 0.00002461
Iteration 27/1000 | Loss: 0.00002460
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00002459
Iteration 30/1000 | Loss: 0.00002458
Iteration 31/1000 | Loss: 0.00002458
Iteration 32/1000 | Loss: 0.00002458
Iteration 33/1000 | Loss: 0.00002457
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002457
Iteration 36/1000 | Loss: 0.00002457
Iteration 37/1000 | Loss: 0.00002457
Iteration 38/1000 | Loss: 0.00002456
Iteration 39/1000 | Loss: 0.00002456
Iteration 40/1000 | Loss: 0.00002456
Iteration 41/1000 | Loss: 0.00002456
Iteration 42/1000 | Loss: 0.00002455
Iteration 43/1000 | Loss: 0.00002455
Iteration 44/1000 | Loss: 0.00002455
Iteration 45/1000 | Loss: 0.00002455
Iteration 46/1000 | Loss: 0.00002454
Iteration 47/1000 | Loss: 0.00002454
Iteration 48/1000 | Loss: 0.00002454
Iteration 49/1000 | Loss: 0.00002454
Iteration 50/1000 | Loss: 0.00002454
Iteration 51/1000 | Loss: 0.00002454
Iteration 52/1000 | Loss: 0.00002454
Iteration 53/1000 | Loss: 0.00002454
Iteration 54/1000 | Loss: 0.00002453
Iteration 55/1000 | Loss: 0.00002452
Iteration 56/1000 | Loss: 0.00002452
Iteration 57/1000 | Loss: 0.00002451
Iteration 58/1000 | Loss: 0.00002451
Iteration 59/1000 | Loss: 0.00002451
Iteration 60/1000 | Loss: 0.00002451
Iteration 61/1000 | Loss: 0.00002451
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00002450
Iteration 65/1000 | Loss: 0.00002449
Iteration 66/1000 | Loss: 0.00002449
Iteration 67/1000 | Loss: 0.00002449
Iteration 68/1000 | Loss: 0.00002448
Iteration 69/1000 | Loss: 0.00002448
Iteration 70/1000 | Loss: 0.00002448
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002447
Iteration 76/1000 | Loss: 0.00002447
Iteration 77/1000 | Loss: 0.00002447
Iteration 78/1000 | Loss: 0.00002447
Iteration 79/1000 | Loss: 0.00002447
Iteration 80/1000 | Loss: 0.00002447
Iteration 81/1000 | Loss: 0.00002447
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00002447
Iteration 84/1000 | Loss: 0.00002447
Iteration 85/1000 | Loss: 0.00002447
Iteration 86/1000 | Loss: 0.00002447
Iteration 87/1000 | Loss: 0.00002447
Iteration 88/1000 | Loss: 0.00002447
Iteration 89/1000 | Loss: 0.00002447
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002445
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002445
Iteration 98/1000 | Loss: 0.00002445
Iteration 99/1000 | Loss: 0.00002444
Iteration 100/1000 | Loss: 0.00002444
Iteration 101/1000 | Loss: 0.00002444
Iteration 102/1000 | Loss: 0.00002444
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002444
Iteration 106/1000 | Loss: 0.00002444
Iteration 107/1000 | Loss: 0.00002444
Iteration 108/1000 | Loss: 0.00002444
Iteration 109/1000 | Loss: 0.00002444
Iteration 110/1000 | Loss: 0.00002444
Iteration 111/1000 | Loss: 0.00002444
Iteration 112/1000 | Loss: 0.00002443
Iteration 113/1000 | Loss: 0.00002443
Iteration 114/1000 | Loss: 0.00002443
Iteration 115/1000 | Loss: 0.00002443
Iteration 116/1000 | Loss: 0.00002443
Iteration 117/1000 | Loss: 0.00002443
Iteration 118/1000 | Loss: 0.00002443
Iteration 119/1000 | Loss: 0.00002443
Iteration 120/1000 | Loss: 0.00002442
Iteration 121/1000 | Loss: 0.00002442
Iteration 122/1000 | Loss: 0.00002442
Iteration 123/1000 | Loss: 0.00002442
Iteration 124/1000 | Loss: 0.00002442
Iteration 125/1000 | Loss: 0.00002442
Iteration 126/1000 | Loss: 0.00002442
Iteration 127/1000 | Loss: 0.00002442
Iteration 128/1000 | Loss: 0.00002442
Iteration 129/1000 | Loss: 0.00002442
Iteration 130/1000 | Loss: 0.00002442
Iteration 131/1000 | Loss: 0.00002442
Iteration 132/1000 | Loss: 0.00002442
Iteration 133/1000 | Loss: 0.00002442
Iteration 134/1000 | Loss: 0.00002442
Iteration 135/1000 | Loss: 0.00002442
Iteration 136/1000 | Loss: 0.00002442
Iteration 137/1000 | Loss: 0.00002441
Iteration 138/1000 | Loss: 0.00002441
Iteration 139/1000 | Loss: 0.00002441
Iteration 140/1000 | Loss: 0.00002441
Iteration 141/1000 | Loss: 0.00002441
Iteration 142/1000 | Loss: 0.00002441
Iteration 143/1000 | Loss: 0.00002441
Iteration 144/1000 | Loss: 0.00002441
Iteration 145/1000 | Loss: 0.00002440
Iteration 146/1000 | Loss: 0.00002440
Iteration 147/1000 | Loss: 0.00002440
Iteration 148/1000 | Loss: 0.00002440
Iteration 149/1000 | Loss: 0.00002440
Iteration 150/1000 | Loss: 0.00002440
Iteration 151/1000 | Loss: 0.00002440
Iteration 152/1000 | Loss: 0.00002440
Iteration 153/1000 | Loss: 0.00002440
Iteration 154/1000 | Loss: 0.00002439
Iteration 155/1000 | Loss: 0.00002439
Iteration 156/1000 | Loss: 0.00002439
Iteration 157/1000 | Loss: 0.00002439
Iteration 158/1000 | Loss: 0.00002439
Iteration 159/1000 | Loss: 0.00002439
Iteration 160/1000 | Loss: 0.00002439
Iteration 161/1000 | Loss: 0.00002439
Iteration 162/1000 | Loss: 0.00002439
Iteration 163/1000 | Loss: 0.00002439
Iteration 164/1000 | Loss: 0.00002439
Iteration 165/1000 | Loss: 0.00002438
Iteration 166/1000 | Loss: 0.00002438
Iteration 167/1000 | Loss: 0.00002438
Iteration 168/1000 | Loss: 0.00002438
Iteration 169/1000 | Loss: 0.00002438
Iteration 170/1000 | Loss: 0.00002438
Iteration 171/1000 | Loss: 0.00002438
Iteration 172/1000 | Loss: 0.00002438
Iteration 173/1000 | Loss: 0.00002438
Iteration 174/1000 | Loss: 0.00002438
Iteration 175/1000 | Loss: 0.00002438
Iteration 176/1000 | Loss: 0.00002438
Iteration 177/1000 | Loss: 0.00002438
Iteration 178/1000 | Loss: 0.00002438
Iteration 179/1000 | Loss: 0.00002438
Iteration 180/1000 | Loss: 0.00002437
Iteration 181/1000 | Loss: 0.00002437
Iteration 182/1000 | Loss: 0.00002437
Iteration 183/1000 | Loss: 0.00002437
Iteration 184/1000 | Loss: 0.00002437
Iteration 185/1000 | Loss: 0.00002437
Iteration 186/1000 | Loss: 0.00002437
Iteration 187/1000 | Loss: 0.00002437
Iteration 188/1000 | Loss: 0.00002437
Iteration 189/1000 | Loss: 0.00002437
Iteration 190/1000 | Loss: 0.00002437
Iteration 191/1000 | Loss: 0.00002437
Iteration 192/1000 | Loss: 0.00002437
Iteration 193/1000 | Loss: 0.00002437
Iteration 194/1000 | Loss: 0.00002437
Iteration 195/1000 | Loss: 0.00002437
Iteration 196/1000 | Loss: 0.00002437
Iteration 197/1000 | Loss: 0.00002437
Iteration 198/1000 | Loss: 0.00002437
Iteration 199/1000 | Loss: 0.00002437
Iteration 200/1000 | Loss: 0.00002437
Iteration 201/1000 | Loss: 0.00002437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.4367993319174275e-05, 2.4367993319174275e-05, 2.4367993319174275e-05, 2.4367993319174275e-05, 2.4367993319174275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4367993319174275e-05

Optimization complete. Final v2v error: 4.15545654296875 mm

Highest mean error: 4.461842060089111 mm for frame 74

Lowest mean error: 3.6409106254577637 mm for frame 11

Saving results

Total time: 95.4767816066742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964862
Iteration 2/25 | Loss: 0.00223259
Iteration 3/25 | Loss: 0.00115480
Iteration 4/25 | Loss: 0.00112405
Iteration 5/25 | Loss: 0.00111604
Iteration 6/25 | Loss: 0.00111274
Iteration 7/25 | Loss: 0.00111247
Iteration 8/25 | Loss: 0.00111247
Iteration 9/25 | Loss: 0.00111247
Iteration 10/25 | Loss: 0.00111247
Iteration 11/25 | Loss: 0.00111247
Iteration 12/25 | Loss: 0.00111247
Iteration 13/25 | Loss: 0.00111247
Iteration 14/25 | Loss: 0.00111247
Iteration 15/25 | Loss: 0.00111247
Iteration 16/25 | Loss: 0.00111247
Iteration 17/25 | Loss: 0.00111247
Iteration 18/25 | Loss: 0.00111247
Iteration 19/25 | Loss: 0.00111247
Iteration 20/25 | Loss: 0.00111247
Iteration 21/25 | Loss: 0.00111247
Iteration 22/25 | Loss: 0.00111247
Iteration 23/25 | Loss: 0.00111247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011124670272693038, 0.0011124670272693038, 0.0011124670272693038, 0.0011124670272693038, 0.0011124670272693038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011124670272693038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56030369
Iteration 2/25 | Loss: 0.00044873
Iteration 3/25 | Loss: 0.00044873
Iteration 4/25 | Loss: 0.00044873
Iteration 5/25 | Loss: 0.00044873
Iteration 6/25 | Loss: 0.00044873
Iteration 7/25 | Loss: 0.00044873
Iteration 8/25 | Loss: 0.00044873
Iteration 9/25 | Loss: 0.00044873
Iteration 10/25 | Loss: 0.00044873
Iteration 11/25 | Loss: 0.00044873
Iteration 12/25 | Loss: 0.00044873
Iteration 13/25 | Loss: 0.00044873
Iteration 14/25 | Loss: 0.00044873
Iteration 15/25 | Loss: 0.00044873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00044873254955746233, 0.00044873254955746233, 0.00044873254955746233, 0.00044873254955746233, 0.00044873254955746233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044873254955746233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044873
Iteration 2/1000 | Loss: 0.00007802
Iteration 3/1000 | Loss: 0.00006009
Iteration 4/1000 | Loss: 0.00005307
Iteration 5/1000 | Loss: 0.00005117
Iteration 6/1000 | Loss: 0.00005026
Iteration 7/1000 | Loss: 0.00004902
Iteration 8/1000 | Loss: 0.00004799
Iteration 9/1000 | Loss: 0.00004705
Iteration 10/1000 | Loss: 0.00004621
Iteration 11/1000 | Loss: 0.00004552
Iteration 12/1000 | Loss: 0.00004484
Iteration 13/1000 | Loss: 0.00004440
Iteration 14/1000 | Loss: 0.00004393
Iteration 15/1000 | Loss: 0.00004341
Iteration 16/1000 | Loss: 0.00004288
Iteration 17/1000 | Loss: 0.00004251
Iteration 18/1000 | Loss: 0.00004219
Iteration 19/1000 | Loss: 0.00004194
Iteration 20/1000 | Loss: 0.00004169
Iteration 21/1000 | Loss: 0.00004154
Iteration 22/1000 | Loss: 0.00004141
Iteration 23/1000 | Loss: 0.00004129
Iteration 24/1000 | Loss: 0.00004124
Iteration 25/1000 | Loss: 0.00004118
Iteration 26/1000 | Loss: 0.00004107
Iteration 27/1000 | Loss: 0.00004104
Iteration 28/1000 | Loss: 0.00004103
Iteration 29/1000 | Loss: 0.00004102
Iteration 30/1000 | Loss: 0.00004101
Iteration 31/1000 | Loss: 0.00004101
Iteration 32/1000 | Loss: 0.00004099
Iteration 33/1000 | Loss: 0.00004099
Iteration 34/1000 | Loss: 0.00004097
Iteration 35/1000 | Loss: 0.00004095
Iteration 36/1000 | Loss: 0.00004095
Iteration 37/1000 | Loss: 0.00004095
Iteration 38/1000 | Loss: 0.00004094
Iteration 39/1000 | Loss: 0.00004094
Iteration 40/1000 | Loss: 0.00004094
Iteration 41/1000 | Loss: 0.00004094
Iteration 42/1000 | Loss: 0.00004094
Iteration 43/1000 | Loss: 0.00004094
Iteration 44/1000 | Loss: 0.00004094
Iteration 45/1000 | Loss: 0.00004094
Iteration 46/1000 | Loss: 0.00004094
Iteration 47/1000 | Loss: 0.00004093
Iteration 48/1000 | Loss: 0.00004093
Iteration 49/1000 | Loss: 0.00004093
Iteration 50/1000 | Loss: 0.00004093
Iteration 51/1000 | Loss: 0.00004093
Iteration 52/1000 | Loss: 0.00004093
Iteration 53/1000 | Loss: 0.00004092
Iteration 54/1000 | Loss: 0.00004092
Iteration 55/1000 | Loss: 0.00004092
Iteration 56/1000 | Loss: 0.00004092
Iteration 57/1000 | Loss: 0.00004092
Iteration 58/1000 | Loss: 0.00004092
Iteration 59/1000 | Loss: 0.00004092
Iteration 60/1000 | Loss: 0.00004092
Iteration 61/1000 | Loss: 0.00004091
Iteration 62/1000 | Loss: 0.00004091
Iteration 63/1000 | Loss: 0.00004091
Iteration 64/1000 | Loss: 0.00004091
Iteration 65/1000 | Loss: 0.00004091
Iteration 66/1000 | Loss: 0.00004091
Iteration 67/1000 | Loss: 0.00004091
Iteration 68/1000 | Loss: 0.00004091
Iteration 69/1000 | Loss: 0.00004091
Iteration 70/1000 | Loss: 0.00004091
Iteration 71/1000 | Loss: 0.00004091
Iteration 72/1000 | Loss: 0.00004091
Iteration 73/1000 | Loss: 0.00004091
Iteration 74/1000 | Loss: 0.00004091
Iteration 75/1000 | Loss: 0.00004091
Iteration 76/1000 | Loss: 0.00004091
Iteration 77/1000 | Loss: 0.00004091
Iteration 78/1000 | Loss: 0.00004091
Iteration 79/1000 | Loss: 0.00004091
Iteration 80/1000 | Loss: 0.00004091
Iteration 81/1000 | Loss: 0.00004091
Iteration 82/1000 | Loss: 0.00004091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [4.090717629878782e-05, 4.090717629878782e-05, 4.090717629878782e-05, 4.090717629878782e-05, 4.090717629878782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.090717629878782e-05

Optimization complete. Final v2v error: 5.112600803375244 mm

Highest mean error: 5.653773784637451 mm for frame 218

Lowest mean error: 4.738890647888184 mm for frame 147

Saving results

Total time: 145.98832893371582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154636
Iteration 2/25 | Loss: 0.01154636
Iteration 3/25 | Loss: 0.00277766
Iteration 4/25 | Loss: 0.00196100
Iteration 5/25 | Loss: 0.00189430
Iteration 6/25 | Loss: 0.00140509
Iteration 7/25 | Loss: 0.00119467
Iteration 8/25 | Loss: 0.00112388
Iteration 9/25 | Loss: 0.00114839
Iteration 10/25 | Loss: 0.00113799
Iteration 11/25 | Loss: 0.00107400
Iteration 12/25 | Loss: 0.00103419
Iteration 13/25 | Loss: 0.00101775
Iteration 14/25 | Loss: 0.00102747
Iteration 15/25 | Loss: 0.00100345
Iteration 16/25 | Loss: 0.00100311
Iteration 17/25 | Loss: 0.00100306
Iteration 18/25 | Loss: 0.00100306
Iteration 19/25 | Loss: 0.00100306
Iteration 20/25 | Loss: 0.00100306
Iteration 21/25 | Loss: 0.00100306
Iteration 22/25 | Loss: 0.00100305
Iteration 23/25 | Loss: 0.00100305
Iteration 24/25 | Loss: 0.00100305
Iteration 25/25 | Loss: 0.00100305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68143439
Iteration 2/25 | Loss: 0.00047880
Iteration 3/25 | Loss: 0.00047880
Iteration 4/25 | Loss: 0.00047880
Iteration 5/25 | Loss: 0.00047880
Iteration 6/25 | Loss: 0.00047880
Iteration 7/25 | Loss: 0.00047880
Iteration 8/25 | Loss: 0.00047880
Iteration 9/25 | Loss: 0.00047880
Iteration 10/25 | Loss: 0.00047880
Iteration 11/25 | Loss: 0.00047880
Iteration 12/25 | Loss: 0.00047880
Iteration 13/25 | Loss: 0.00047880
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0004787963116541505, 0.0004787963116541505, 0.0004787963116541505, 0.0004787963116541505, 0.0004787963116541505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004787963116541505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047880
Iteration 2/1000 | Loss: 0.00009673
Iteration 3/1000 | Loss: 0.00007721
Iteration 4/1000 | Loss: 0.00005113
Iteration 5/1000 | Loss: 0.00004223
Iteration 6/1000 | Loss: 0.00003351
Iteration 7/1000 | Loss: 0.00003255
Iteration 8/1000 | Loss: 0.00002297
Iteration 9/1000 | Loss: 0.00004657
Iteration 10/1000 | Loss: 0.00003218
Iteration 11/1000 | Loss: 0.00003432
Iteration 12/1000 | Loss: 0.00002132
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00004205
Iteration 16/1000 | Loss: 0.00002060
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002055
Iteration 20/1000 | Loss: 0.00005276
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002016
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002016
Iteration 27/1000 | Loss: 0.00002015
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00004777
Iteration 31/1000 | Loss: 0.00002007
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002000
Iteration 34/1000 | Loss: 0.00002000
Iteration 35/1000 | Loss: 0.00002000
Iteration 36/1000 | Loss: 0.00002000
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00001999
Iteration 39/1000 | Loss: 0.00001999
Iteration 40/1000 | Loss: 0.00001998
Iteration 41/1000 | Loss: 0.00001996
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001994
Iteration 47/1000 | Loss: 0.00001994
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001992
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001991
Iteration 55/1000 | Loss: 0.00001991
Iteration 56/1000 | Loss: 0.00001991
Iteration 57/1000 | Loss: 0.00001991
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001991
Iteration 62/1000 | Loss: 0.00001991
Iteration 63/1000 | Loss: 0.00001990
Iteration 64/1000 | Loss: 0.00001990
Iteration 65/1000 | Loss: 0.00001989
Iteration 66/1000 | Loss: 0.00001989
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001982
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001980
Iteration 74/1000 | Loss: 0.00001980
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001979
Iteration 81/1000 | Loss: 0.00001979
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001979
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001978
Iteration 89/1000 | Loss: 0.00001978
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001976
Iteration 95/1000 | Loss: 0.00001975
Iteration 96/1000 | Loss: 0.00001975
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001975
Iteration 99/1000 | Loss: 0.00001975
Iteration 100/1000 | Loss: 0.00001974
Iteration 101/1000 | Loss: 0.00001974
Iteration 102/1000 | Loss: 0.00001974
Iteration 103/1000 | Loss: 0.00001974
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001973
Iteration 109/1000 | Loss: 0.00001973
Iteration 110/1000 | Loss: 0.00001973
Iteration 111/1000 | Loss: 0.00001973
Iteration 112/1000 | Loss: 0.00001973
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001973
Iteration 118/1000 | Loss: 0.00001973
Iteration 119/1000 | Loss: 0.00001973
Iteration 120/1000 | Loss: 0.00001973
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001971
Iteration 131/1000 | Loss: 0.00001971
Iteration 132/1000 | Loss: 0.00001971
Iteration 133/1000 | Loss: 0.00001971
Iteration 134/1000 | Loss: 0.00001971
Iteration 135/1000 | Loss: 0.00001971
Iteration 136/1000 | Loss: 0.00001971
Iteration 137/1000 | Loss: 0.00001971
Iteration 138/1000 | Loss: 0.00001971
Iteration 139/1000 | Loss: 0.00001970
Iteration 140/1000 | Loss: 0.00001970
Iteration 141/1000 | Loss: 0.00001970
Iteration 142/1000 | Loss: 0.00001970
Iteration 143/1000 | Loss: 0.00001969
Iteration 144/1000 | Loss: 0.00001969
Iteration 145/1000 | Loss: 0.00001969
Iteration 146/1000 | Loss: 0.00001969
Iteration 147/1000 | Loss: 0.00001969
Iteration 148/1000 | Loss: 0.00001969
Iteration 149/1000 | Loss: 0.00001969
Iteration 150/1000 | Loss: 0.00001969
Iteration 151/1000 | Loss: 0.00001969
Iteration 152/1000 | Loss: 0.00001969
Iteration 153/1000 | Loss: 0.00001969
Iteration 154/1000 | Loss: 0.00001969
Iteration 155/1000 | Loss: 0.00001969
Iteration 156/1000 | Loss: 0.00001968
Iteration 157/1000 | Loss: 0.00001968
Iteration 158/1000 | Loss: 0.00001968
Iteration 159/1000 | Loss: 0.00001968
Iteration 160/1000 | Loss: 0.00001968
Iteration 161/1000 | Loss: 0.00001968
Iteration 162/1000 | Loss: 0.00001968
Iteration 163/1000 | Loss: 0.00001967
Iteration 164/1000 | Loss: 0.00001967
Iteration 165/1000 | Loss: 0.00001967
Iteration 166/1000 | Loss: 0.00001967
Iteration 167/1000 | Loss: 0.00001967
Iteration 168/1000 | Loss: 0.00001967
Iteration 169/1000 | Loss: 0.00001967
Iteration 170/1000 | Loss: 0.00001967
Iteration 171/1000 | Loss: 0.00001967
Iteration 172/1000 | Loss: 0.00001967
Iteration 173/1000 | Loss: 0.00001967
Iteration 174/1000 | Loss: 0.00001967
Iteration 175/1000 | Loss: 0.00001967
Iteration 176/1000 | Loss: 0.00001967
Iteration 177/1000 | Loss: 0.00001967
Iteration 178/1000 | Loss: 0.00001967
Iteration 179/1000 | Loss: 0.00001967
Iteration 180/1000 | Loss: 0.00001967
Iteration 181/1000 | Loss: 0.00001967
Iteration 182/1000 | Loss: 0.00001967
Iteration 183/1000 | Loss: 0.00001967
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001967
Iteration 186/1000 | Loss: 0.00001967
Iteration 187/1000 | Loss: 0.00001967
Iteration 188/1000 | Loss: 0.00001967
Iteration 189/1000 | Loss: 0.00001967
Iteration 190/1000 | Loss: 0.00001967
Iteration 191/1000 | Loss: 0.00001967
Iteration 192/1000 | Loss: 0.00001967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.9673620045068674e-05, 1.9673620045068674e-05, 1.9673620045068674e-05, 1.9673620045068674e-05, 1.9673620045068674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9673620045068674e-05

Optimization complete. Final v2v error: 3.6684117317199707 mm

Highest mean error: 4.369783401489258 mm for frame 29

Lowest mean error: 3.185275077819824 mm for frame 148

Saving results

Total time: 152.91206884384155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843941
Iteration 2/25 | Loss: 0.00120243
Iteration 3/25 | Loss: 0.00105511
Iteration 4/25 | Loss: 0.00102251
Iteration 5/25 | Loss: 0.00101410
Iteration 6/25 | Loss: 0.00101290
Iteration 7/25 | Loss: 0.00101290
Iteration 8/25 | Loss: 0.00101290
Iteration 9/25 | Loss: 0.00101290
Iteration 10/25 | Loss: 0.00101290
Iteration 11/25 | Loss: 0.00101290
Iteration 12/25 | Loss: 0.00101290
Iteration 13/25 | Loss: 0.00101290
Iteration 14/25 | Loss: 0.00101290
Iteration 15/25 | Loss: 0.00101290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001012897351756692, 0.001012897351756692, 0.001012897351756692, 0.001012897351756692, 0.001012897351756692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001012897351756692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05357718
Iteration 2/25 | Loss: 0.00044534
Iteration 3/25 | Loss: 0.00044534
Iteration 4/25 | Loss: 0.00044534
Iteration 5/25 | Loss: 0.00044534
Iteration 6/25 | Loss: 0.00044534
Iteration 7/25 | Loss: 0.00044534
Iteration 8/25 | Loss: 0.00044534
Iteration 9/25 | Loss: 0.00044534
Iteration 10/25 | Loss: 0.00044534
Iteration 11/25 | Loss: 0.00044534
Iteration 12/25 | Loss: 0.00044534
Iteration 13/25 | Loss: 0.00044534
Iteration 14/25 | Loss: 0.00044534
Iteration 15/25 | Loss: 0.00044534
Iteration 16/25 | Loss: 0.00044534
Iteration 17/25 | Loss: 0.00044534
Iteration 18/25 | Loss: 0.00044534
Iteration 19/25 | Loss: 0.00044534
Iteration 20/25 | Loss: 0.00044534
Iteration 21/25 | Loss: 0.00044534
Iteration 22/25 | Loss: 0.00044534
Iteration 23/25 | Loss: 0.00044534
Iteration 24/25 | Loss: 0.00044534
Iteration 25/25 | Loss: 0.00044534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044534
Iteration 2/1000 | Loss: 0.00006057
Iteration 3/1000 | Loss: 0.00003254
Iteration 4/1000 | Loss: 0.00003038
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002879
Iteration 7/1000 | Loss: 0.00002845
Iteration 8/1000 | Loss: 0.00002800
Iteration 9/1000 | Loss: 0.00002769
Iteration 10/1000 | Loss: 0.00002766
Iteration 11/1000 | Loss: 0.00002756
Iteration 12/1000 | Loss: 0.00002752
Iteration 13/1000 | Loss: 0.00002747
Iteration 14/1000 | Loss: 0.00002746
Iteration 15/1000 | Loss: 0.00002746
Iteration 16/1000 | Loss: 0.00002742
Iteration 17/1000 | Loss: 0.00002742
Iteration 18/1000 | Loss: 0.00002742
Iteration 19/1000 | Loss: 0.00002742
Iteration 20/1000 | Loss: 0.00002742
Iteration 21/1000 | Loss: 0.00002742
Iteration 22/1000 | Loss: 0.00002742
Iteration 23/1000 | Loss: 0.00002742
Iteration 24/1000 | Loss: 0.00002742
Iteration 25/1000 | Loss: 0.00002742
Iteration 26/1000 | Loss: 0.00002742
Iteration 27/1000 | Loss: 0.00002741
Iteration 28/1000 | Loss: 0.00002740
Iteration 29/1000 | Loss: 0.00002740
Iteration 30/1000 | Loss: 0.00002740
Iteration 31/1000 | Loss: 0.00002740
Iteration 32/1000 | Loss: 0.00002740
Iteration 33/1000 | Loss: 0.00002739
Iteration 34/1000 | Loss: 0.00002739
Iteration 35/1000 | Loss: 0.00002739
Iteration 36/1000 | Loss: 0.00002738
Iteration 37/1000 | Loss: 0.00002738
Iteration 38/1000 | Loss: 0.00002738
Iteration 39/1000 | Loss: 0.00002738
Iteration 40/1000 | Loss: 0.00002738
Iteration 41/1000 | Loss: 0.00002738
Iteration 42/1000 | Loss: 0.00002737
Iteration 43/1000 | Loss: 0.00002737
Iteration 44/1000 | Loss: 0.00002737
Iteration 45/1000 | Loss: 0.00002737
Iteration 46/1000 | Loss: 0.00002737
Iteration 47/1000 | Loss: 0.00002737
Iteration 48/1000 | Loss: 0.00002737
Iteration 49/1000 | Loss: 0.00002736
Iteration 50/1000 | Loss: 0.00002736
Iteration 51/1000 | Loss: 0.00002736
Iteration 52/1000 | Loss: 0.00002736
Iteration 53/1000 | Loss: 0.00002736
Iteration 54/1000 | Loss: 0.00002736
Iteration 55/1000 | Loss: 0.00002736
Iteration 56/1000 | Loss: 0.00002736
Iteration 57/1000 | Loss: 0.00002736
Iteration 58/1000 | Loss: 0.00002736
Iteration 59/1000 | Loss: 0.00002735
Iteration 60/1000 | Loss: 0.00002735
Iteration 61/1000 | Loss: 0.00002735
Iteration 62/1000 | Loss: 0.00002735
Iteration 63/1000 | Loss: 0.00002735
Iteration 64/1000 | Loss: 0.00002735
Iteration 65/1000 | Loss: 0.00002734
Iteration 66/1000 | Loss: 0.00002734
Iteration 67/1000 | Loss: 0.00002734
Iteration 68/1000 | Loss: 0.00002734
Iteration 69/1000 | Loss: 0.00002734
Iteration 70/1000 | Loss: 0.00002734
Iteration 71/1000 | Loss: 0.00002734
Iteration 72/1000 | Loss: 0.00002734
Iteration 73/1000 | Loss: 0.00002734
Iteration 74/1000 | Loss: 0.00002734
Iteration 75/1000 | Loss: 0.00002734
Iteration 76/1000 | Loss: 0.00002734
Iteration 77/1000 | Loss: 0.00002734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.7336616767570376e-05, 2.7336616767570376e-05, 2.7336616767570376e-05, 2.7336616767570376e-05, 2.7336616767570376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7336616767570376e-05

Optimization complete. Final v2v error: 4.273767471313477 mm

Highest mean error: 4.618139266967773 mm for frame 141

Lowest mean error: 3.6174473762512207 mm for frame 128

Saving results

Total time: 91.19524502754211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_nl_5323/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_nl_5323/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113300
Iteration 2/25 | Loss: 0.00279002
Iteration 3/25 | Loss: 0.00184311
Iteration 4/25 | Loss: 0.00152983
Iteration 5/25 | Loss: 0.00149247
Iteration 6/25 | Loss: 0.00140641
Iteration 7/25 | Loss: 0.00109640
Iteration 8/25 | Loss: 0.00097778
Iteration 9/25 | Loss: 0.00091274
Iteration 10/25 | Loss: 0.00088216
Iteration 11/25 | Loss: 0.00086723
Iteration 12/25 | Loss: 0.00085695
Iteration 13/25 | Loss: 0.00085371
Iteration 14/25 | Loss: 0.00085283
Iteration 15/25 | Loss: 0.00085249
Iteration 16/25 | Loss: 0.00085223
Iteration 17/25 | Loss: 0.00085185
Iteration 18/25 | Loss: 0.00085446
Iteration 19/25 | Loss: 0.00084863
Iteration 20/25 | Loss: 0.00084766
Iteration 21/25 | Loss: 0.00084728
Iteration 22/25 | Loss: 0.00084719
Iteration 23/25 | Loss: 0.00084718
Iteration 24/25 | Loss: 0.00084718
Iteration 25/25 | Loss: 0.00084718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36523020
Iteration 2/25 | Loss: 0.00038558
Iteration 3/25 | Loss: 0.00038558
Iteration 4/25 | Loss: 0.00038558
Iteration 5/25 | Loss: 0.00038558
Iteration 6/25 | Loss: 0.00038558
Iteration 7/25 | Loss: 0.00038558
Iteration 8/25 | Loss: 0.00038558
Iteration 9/25 | Loss: 0.00038558
Iteration 10/25 | Loss: 0.00038558
Iteration 11/25 | Loss: 0.00038558
Iteration 12/25 | Loss: 0.00038558
Iteration 13/25 | Loss: 0.00038558
Iteration 14/25 | Loss: 0.00038558
Iteration 15/25 | Loss: 0.00038558
Iteration 16/25 | Loss: 0.00038558
Iteration 17/25 | Loss: 0.00038558
Iteration 18/25 | Loss: 0.00038558
Iteration 19/25 | Loss: 0.00038558
Iteration 20/25 | Loss: 0.00038558
Iteration 21/25 | Loss: 0.00038558
Iteration 22/25 | Loss: 0.00038558
Iteration 23/25 | Loss: 0.00038558
Iteration 24/25 | Loss: 0.00038558
Iteration 25/25 | Loss: 0.00038558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038558
Iteration 2/1000 | Loss: 0.00005601
Iteration 3/1000 | Loss: 0.00003286
Iteration 4/1000 | Loss: 0.00002828
Iteration 5/1000 | Loss: 0.00002592
Iteration 6/1000 | Loss: 0.00002495
Iteration 7/1000 | Loss: 0.00002390
Iteration 8/1000 | Loss: 0.00002343
Iteration 9/1000 | Loss: 0.00204718
Iteration 10/1000 | Loss: 0.00002192
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001350
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001327
Iteration 26/1000 | Loss: 0.00001326
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001295
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001291
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001291
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001289
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001288
Iteration 109/1000 | Loss: 0.00001288
Iteration 110/1000 | Loss: 0.00001288
Iteration 111/1000 | Loss: 0.00001288
Iteration 112/1000 | Loss: 0.00001288
Iteration 113/1000 | Loss: 0.00001288
Iteration 114/1000 | Loss: 0.00001288
Iteration 115/1000 | Loss: 0.00001288
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001287
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001287
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001287
Iteration 126/1000 | Loss: 0.00001287
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001286
Iteration 129/1000 | Loss: 0.00001286
Iteration 130/1000 | Loss: 0.00001286
Iteration 131/1000 | Loss: 0.00001286
Iteration 132/1000 | Loss: 0.00001286
Iteration 133/1000 | Loss: 0.00001286
Iteration 134/1000 | Loss: 0.00001286
Iteration 135/1000 | Loss: 0.00001286
Iteration 136/1000 | Loss: 0.00001286
Iteration 137/1000 | Loss: 0.00001286
Iteration 138/1000 | Loss: 0.00001286
Iteration 139/1000 | Loss: 0.00001286
Iteration 140/1000 | Loss: 0.00001286
Iteration 141/1000 | Loss: 0.00001286
Iteration 142/1000 | Loss: 0.00001286
Iteration 143/1000 | Loss: 0.00001285
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001285
Iteration 147/1000 | Loss: 0.00001285
Iteration 148/1000 | Loss: 0.00001285
Iteration 149/1000 | Loss: 0.00001285
Iteration 150/1000 | Loss: 0.00001285
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001284
Iteration 154/1000 | Loss: 0.00001284
Iteration 155/1000 | Loss: 0.00001284
Iteration 156/1000 | Loss: 0.00001284
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001284
Iteration 160/1000 | Loss: 0.00001284
Iteration 161/1000 | Loss: 0.00001284
Iteration 162/1000 | Loss: 0.00001284
Iteration 163/1000 | Loss: 0.00001284
Iteration 164/1000 | Loss: 0.00001284
Iteration 165/1000 | Loss: 0.00001284
Iteration 166/1000 | Loss: 0.00001284
Iteration 167/1000 | Loss: 0.00001284
Iteration 168/1000 | Loss: 0.00001284
Iteration 169/1000 | Loss: 0.00001284
Iteration 170/1000 | Loss: 0.00001284
Iteration 171/1000 | Loss: 0.00001284
Iteration 172/1000 | Loss: 0.00001284
Iteration 173/1000 | Loss: 0.00001284
Iteration 174/1000 | Loss: 0.00001284
Iteration 175/1000 | Loss: 0.00001284
Iteration 176/1000 | Loss: 0.00001284
Iteration 177/1000 | Loss: 0.00001284
Iteration 178/1000 | Loss: 0.00001284
Iteration 179/1000 | Loss: 0.00001284
Iteration 180/1000 | Loss: 0.00001284
Iteration 181/1000 | Loss: 0.00001284
Iteration 182/1000 | Loss: 0.00001284
Iteration 183/1000 | Loss: 0.00001284
Iteration 184/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.2838594557251781e-05, 1.2838594557251781e-05, 1.2838594557251781e-05, 1.2838594557251781e-05, 1.2838594557251781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2838594557251781e-05

Optimization complete. Final v2v error: 3.044201374053955 mm

Highest mean error: 3.492645502090454 mm for frame 142

Lowest mean error: 2.6776959896087646 mm for frame 1

Saving results

Total time: 189.8356363773346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408335
Iteration 2/25 | Loss: 0.00115716
Iteration 3/25 | Loss: 0.00101350
Iteration 4/25 | Loss: 0.00097895
Iteration 5/25 | Loss: 0.00096672
Iteration 6/25 | Loss: 0.00096429
Iteration 7/25 | Loss: 0.00096341
Iteration 8/25 | Loss: 0.00096317
Iteration 9/25 | Loss: 0.00096317
Iteration 10/25 | Loss: 0.00096317
Iteration 11/25 | Loss: 0.00096317
Iteration 12/25 | Loss: 0.00096317
Iteration 13/25 | Loss: 0.00096317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009631686261855066, 0.0009631686261855066, 0.0009631686261855066, 0.0009631686261855066, 0.0009631686261855066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009631686261855066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50414312
Iteration 2/25 | Loss: 0.00122938
Iteration 3/25 | Loss: 0.00122938
Iteration 4/25 | Loss: 0.00122937
Iteration 5/25 | Loss: 0.00122937
Iteration 6/25 | Loss: 0.00122937
Iteration 7/25 | Loss: 0.00122937
Iteration 8/25 | Loss: 0.00122937
Iteration 9/25 | Loss: 0.00122937
Iteration 10/25 | Loss: 0.00122937
Iteration 11/25 | Loss: 0.00122937
Iteration 12/25 | Loss: 0.00122937
Iteration 13/25 | Loss: 0.00122937
Iteration 14/25 | Loss: 0.00122937
Iteration 15/25 | Loss: 0.00122937
Iteration 16/25 | Loss: 0.00122937
Iteration 17/25 | Loss: 0.00122937
Iteration 18/25 | Loss: 0.00122937
Iteration 19/25 | Loss: 0.00122937
Iteration 20/25 | Loss: 0.00122937
Iteration 21/25 | Loss: 0.00122937
Iteration 22/25 | Loss: 0.00122937
Iteration 23/25 | Loss: 0.00122937
Iteration 24/25 | Loss: 0.00122937
Iteration 25/25 | Loss: 0.00122937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122937
Iteration 2/1000 | Loss: 0.00009140
Iteration 3/1000 | Loss: 0.00006078
Iteration 4/1000 | Loss: 0.00004523
Iteration 5/1000 | Loss: 0.00004051
Iteration 6/1000 | Loss: 0.00003830
Iteration 7/1000 | Loss: 0.00003667
Iteration 8/1000 | Loss: 0.00003558
Iteration 9/1000 | Loss: 0.00003469
Iteration 10/1000 | Loss: 0.00003383
Iteration 11/1000 | Loss: 0.00003329
Iteration 12/1000 | Loss: 0.00003300
Iteration 13/1000 | Loss: 0.00003279
Iteration 14/1000 | Loss: 0.00003264
Iteration 15/1000 | Loss: 0.00003259
Iteration 16/1000 | Loss: 0.00003254
Iteration 17/1000 | Loss: 0.00003249
Iteration 18/1000 | Loss: 0.00003249
Iteration 19/1000 | Loss: 0.00003249
Iteration 20/1000 | Loss: 0.00003248
Iteration 21/1000 | Loss: 0.00003248
Iteration 22/1000 | Loss: 0.00003248
Iteration 23/1000 | Loss: 0.00003246
Iteration 24/1000 | Loss: 0.00003244
Iteration 25/1000 | Loss: 0.00003244
Iteration 26/1000 | Loss: 0.00003243
Iteration 27/1000 | Loss: 0.00003243
Iteration 28/1000 | Loss: 0.00003242
Iteration 29/1000 | Loss: 0.00003242
Iteration 30/1000 | Loss: 0.00003242
Iteration 31/1000 | Loss: 0.00003242
Iteration 32/1000 | Loss: 0.00003242
Iteration 33/1000 | Loss: 0.00003241
Iteration 34/1000 | Loss: 0.00003241
Iteration 35/1000 | Loss: 0.00003240
Iteration 36/1000 | Loss: 0.00003239
Iteration 37/1000 | Loss: 0.00003239
Iteration 38/1000 | Loss: 0.00003239
Iteration 39/1000 | Loss: 0.00003239
Iteration 40/1000 | Loss: 0.00003238
Iteration 41/1000 | Loss: 0.00003238
Iteration 42/1000 | Loss: 0.00003237
Iteration 43/1000 | Loss: 0.00003237
Iteration 44/1000 | Loss: 0.00003237
Iteration 45/1000 | Loss: 0.00003236
Iteration 46/1000 | Loss: 0.00003236
Iteration 47/1000 | Loss: 0.00003235
Iteration 48/1000 | Loss: 0.00003235
Iteration 49/1000 | Loss: 0.00003234
Iteration 50/1000 | Loss: 0.00003234
Iteration 51/1000 | Loss: 0.00003234
Iteration 52/1000 | Loss: 0.00003233
Iteration 53/1000 | Loss: 0.00003233
Iteration 54/1000 | Loss: 0.00003233
Iteration 55/1000 | Loss: 0.00003233
Iteration 56/1000 | Loss: 0.00003232
Iteration 57/1000 | Loss: 0.00003232
Iteration 58/1000 | Loss: 0.00003232
Iteration 59/1000 | Loss: 0.00003231
Iteration 60/1000 | Loss: 0.00003231
Iteration 61/1000 | Loss: 0.00003231
Iteration 62/1000 | Loss: 0.00003230
Iteration 63/1000 | Loss: 0.00003230
Iteration 64/1000 | Loss: 0.00003229
Iteration 65/1000 | Loss: 0.00003229
Iteration 66/1000 | Loss: 0.00003228
Iteration 67/1000 | Loss: 0.00003228
Iteration 68/1000 | Loss: 0.00003228
Iteration 69/1000 | Loss: 0.00003227
Iteration 70/1000 | Loss: 0.00003227
Iteration 71/1000 | Loss: 0.00003227
Iteration 72/1000 | Loss: 0.00003226
Iteration 73/1000 | Loss: 0.00003226
Iteration 74/1000 | Loss: 0.00003226
Iteration 75/1000 | Loss: 0.00003225
Iteration 76/1000 | Loss: 0.00003225
Iteration 77/1000 | Loss: 0.00003224
Iteration 78/1000 | Loss: 0.00003224
Iteration 79/1000 | Loss: 0.00003224
Iteration 80/1000 | Loss: 0.00003224
Iteration 81/1000 | Loss: 0.00003224
Iteration 82/1000 | Loss: 0.00003224
Iteration 83/1000 | Loss: 0.00003224
Iteration 84/1000 | Loss: 0.00003223
Iteration 85/1000 | Loss: 0.00003223
Iteration 86/1000 | Loss: 0.00003223
Iteration 87/1000 | Loss: 0.00003223
Iteration 88/1000 | Loss: 0.00003223
Iteration 89/1000 | Loss: 0.00003223
Iteration 90/1000 | Loss: 0.00003222
Iteration 91/1000 | Loss: 0.00003222
Iteration 92/1000 | Loss: 0.00003222
Iteration 93/1000 | Loss: 0.00003222
Iteration 94/1000 | Loss: 0.00003221
Iteration 95/1000 | Loss: 0.00003221
Iteration 96/1000 | Loss: 0.00003221
Iteration 97/1000 | Loss: 0.00003221
Iteration 98/1000 | Loss: 0.00003220
Iteration 99/1000 | Loss: 0.00003220
Iteration 100/1000 | Loss: 0.00003220
Iteration 101/1000 | Loss: 0.00003220
Iteration 102/1000 | Loss: 0.00003220
Iteration 103/1000 | Loss: 0.00003220
Iteration 104/1000 | Loss: 0.00003220
Iteration 105/1000 | Loss: 0.00003219
Iteration 106/1000 | Loss: 0.00003219
Iteration 107/1000 | Loss: 0.00003219
Iteration 108/1000 | Loss: 0.00003219
Iteration 109/1000 | Loss: 0.00003219
Iteration 110/1000 | Loss: 0.00003219
Iteration 111/1000 | Loss: 0.00003218
Iteration 112/1000 | Loss: 0.00003218
Iteration 113/1000 | Loss: 0.00003218
Iteration 114/1000 | Loss: 0.00003218
Iteration 115/1000 | Loss: 0.00003218
Iteration 116/1000 | Loss: 0.00003217
Iteration 117/1000 | Loss: 0.00003217
Iteration 118/1000 | Loss: 0.00003217
Iteration 119/1000 | Loss: 0.00003217
Iteration 120/1000 | Loss: 0.00003217
Iteration 121/1000 | Loss: 0.00003217
Iteration 122/1000 | Loss: 0.00003216
Iteration 123/1000 | Loss: 0.00003216
Iteration 124/1000 | Loss: 0.00003216
Iteration 125/1000 | Loss: 0.00003216
Iteration 126/1000 | Loss: 0.00003216
Iteration 127/1000 | Loss: 0.00003216
Iteration 128/1000 | Loss: 0.00003216
Iteration 129/1000 | Loss: 0.00003215
Iteration 130/1000 | Loss: 0.00003215
Iteration 131/1000 | Loss: 0.00003215
Iteration 132/1000 | Loss: 0.00003215
Iteration 133/1000 | Loss: 0.00003215
Iteration 134/1000 | Loss: 0.00003215
Iteration 135/1000 | Loss: 0.00003215
Iteration 136/1000 | Loss: 0.00003214
Iteration 137/1000 | Loss: 0.00003214
Iteration 138/1000 | Loss: 0.00003214
Iteration 139/1000 | Loss: 0.00003214
Iteration 140/1000 | Loss: 0.00003214
Iteration 141/1000 | Loss: 0.00003214
Iteration 142/1000 | Loss: 0.00003214
Iteration 143/1000 | Loss: 0.00003214
Iteration 144/1000 | Loss: 0.00003214
Iteration 145/1000 | Loss: 0.00003214
Iteration 146/1000 | Loss: 0.00003214
Iteration 147/1000 | Loss: 0.00003214
Iteration 148/1000 | Loss: 0.00003214
Iteration 149/1000 | Loss: 0.00003214
Iteration 150/1000 | Loss: 0.00003214
Iteration 151/1000 | Loss: 0.00003214
Iteration 152/1000 | Loss: 0.00003214
Iteration 153/1000 | Loss: 0.00003214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.214496609871276e-05, 3.214496609871276e-05, 3.214496609871276e-05, 3.214496609871276e-05, 3.214496609871276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.214496609871276e-05

Optimization complete. Final v2v error: 4.695094108581543 mm

Highest mean error: 5.290434837341309 mm for frame 81

Lowest mean error: 3.8007514476776123 mm for frame 70

Saving results

Total time: 97.4864661693573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459358
Iteration 2/25 | Loss: 0.00102791
Iteration 3/25 | Loss: 0.00092154
Iteration 4/25 | Loss: 0.00089438
Iteration 5/25 | Loss: 0.00088403
Iteration 6/25 | Loss: 0.00088172
Iteration 7/25 | Loss: 0.00088133
Iteration 8/25 | Loss: 0.00088133
Iteration 9/25 | Loss: 0.00088133
Iteration 10/25 | Loss: 0.00088133
Iteration 11/25 | Loss: 0.00088133
Iteration 12/25 | Loss: 0.00088133
Iteration 13/25 | Loss: 0.00088133
Iteration 14/25 | Loss: 0.00088133
Iteration 15/25 | Loss: 0.00088133
Iteration 16/25 | Loss: 0.00088133
Iteration 17/25 | Loss: 0.00088133
Iteration 18/25 | Loss: 0.00088133
Iteration 19/25 | Loss: 0.00088133
Iteration 20/25 | Loss: 0.00088133
Iteration 21/25 | Loss: 0.00088133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008813308086246252, 0.0008813308086246252, 0.0008813308086246252, 0.0008813308086246252, 0.0008813308086246252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008813308086246252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53194618
Iteration 2/25 | Loss: 0.00093759
Iteration 3/25 | Loss: 0.00093759
Iteration 4/25 | Loss: 0.00093759
Iteration 5/25 | Loss: 0.00093759
Iteration 6/25 | Loss: 0.00093759
Iteration 7/25 | Loss: 0.00093759
Iteration 8/25 | Loss: 0.00093759
Iteration 9/25 | Loss: 0.00093759
Iteration 10/25 | Loss: 0.00093759
Iteration 11/25 | Loss: 0.00093759
Iteration 12/25 | Loss: 0.00093759
Iteration 13/25 | Loss: 0.00093759
Iteration 14/25 | Loss: 0.00093759
Iteration 15/25 | Loss: 0.00093759
Iteration 16/25 | Loss: 0.00093759
Iteration 17/25 | Loss: 0.00093759
Iteration 18/25 | Loss: 0.00093759
Iteration 19/25 | Loss: 0.00093759
Iteration 20/25 | Loss: 0.00093759
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000937586824875325, 0.000937586824875325, 0.000937586824875325, 0.000937586824875325, 0.000937586824875325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000937586824875325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093759
Iteration 2/1000 | Loss: 0.00003246
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00002199
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00002053
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00001987
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001984
Iteration 13/1000 | Loss: 0.00001983
Iteration 14/1000 | Loss: 0.00001983
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001983
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001980
Iteration 20/1000 | Loss: 0.00001980
Iteration 21/1000 | Loss: 0.00001980
Iteration 22/1000 | Loss: 0.00001980
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001978
Iteration 28/1000 | Loss: 0.00001977
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001976
Iteration 32/1000 | Loss: 0.00001976
Iteration 33/1000 | Loss: 0.00001975
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001969
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001968
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001966
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001963
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001963
Iteration 62/1000 | Loss: 0.00001962
Iteration 63/1000 | Loss: 0.00001962
Iteration 64/1000 | Loss: 0.00001962
Iteration 65/1000 | Loss: 0.00001962
Iteration 66/1000 | Loss: 0.00001962
Iteration 67/1000 | Loss: 0.00001962
Iteration 68/1000 | Loss: 0.00001962
Iteration 69/1000 | Loss: 0.00001962
Iteration 70/1000 | Loss: 0.00001962
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001961
Iteration 76/1000 | Loss: 0.00001961
Iteration 77/1000 | Loss: 0.00001961
Iteration 78/1000 | Loss: 0.00001961
Iteration 79/1000 | Loss: 0.00001960
Iteration 80/1000 | Loss: 0.00001960
Iteration 81/1000 | Loss: 0.00001960
Iteration 82/1000 | Loss: 0.00001960
Iteration 83/1000 | Loss: 0.00001960
Iteration 84/1000 | Loss: 0.00001959
Iteration 85/1000 | Loss: 0.00001959
Iteration 86/1000 | Loss: 0.00001959
Iteration 87/1000 | Loss: 0.00001959
Iteration 88/1000 | Loss: 0.00001958
Iteration 89/1000 | Loss: 0.00001958
Iteration 90/1000 | Loss: 0.00001958
Iteration 91/1000 | Loss: 0.00001958
Iteration 92/1000 | Loss: 0.00001958
Iteration 93/1000 | Loss: 0.00001958
Iteration 94/1000 | Loss: 0.00001958
Iteration 95/1000 | Loss: 0.00001958
Iteration 96/1000 | Loss: 0.00001958
Iteration 97/1000 | Loss: 0.00001958
Iteration 98/1000 | Loss: 0.00001958
Iteration 99/1000 | Loss: 0.00001958
Iteration 100/1000 | Loss: 0.00001957
Iteration 101/1000 | Loss: 0.00001957
Iteration 102/1000 | Loss: 0.00001957
Iteration 103/1000 | Loss: 0.00001956
Iteration 104/1000 | Loss: 0.00001956
Iteration 105/1000 | Loss: 0.00001956
Iteration 106/1000 | Loss: 0.00001956
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00001956
Iteration 109/1000 | Loss: 0.00001956
Iteration 110/1000 | Loss: 0.00001956
Iteration 111/1000 | Loss: 0.00001956
Iteration 112/1000 | Loss: 0.00001955
Iteration 113/1000 | Loss: 0.00001955
Iteration 114/1000 | Loss: 0.00001955
Iteration 115/1000 | Loss: 0.00001955
Iteration 116/1000 | Loss: 0.00001954
Iteration 117/1000 | Loss: 0.00001954
Iteration 118/1000 | Loss: 0.00001954
Iteration 119/1000 | Loss: 0.00001954
Iteration 120/1000 | Loss: 0.00001954
Iteration 121/1000 | Loss: 0.00001954
Iteration 122/1000 | Loss: 0.00001953
Iteration 123/1000 | Loss: 0.00001953
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001952
Iteration 129/1000 | Loss: 0.00001952
Iteration 130/1000 | Loss: 0.00001952
Iteration 131/1000 | Loss: 0.00001952
Iteration 132/1000 | Loss: 0.00001952
Iteration 133/1000 | Loss: 0.00001952
Iteration 134/1000 | Loss: 0.00001952
Iteration 135/1000 | Loss: 0.00001952
Iteration 136/1000 | Loss: 0.00001952
Iteration 137/1000 | Loss: 0.00001952
Iteration 138/1000 | Loss: 0.00001952
Iteration 139/1000 | Loss: 0.00001952
Iteration 140/1000 | Loss: 0.00001952
Iteration 141/1000 | Loss: 0.00001952
Iteration 142/1000 | Loss: 0.00001952
Iteration 143/1000 | Loss: 0.00001952
Iteration 144/1000 | Loss: 0.00001952
Iteration 145/1000 | Loss: 0.00001952
Iteration 146/1000 | Loss: 0.00001952
Iteration 147/1000 | Loss: 0.00001952
Iteration 148/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.9516033717081882e-05, 1.9516033717081882e-05, 1.9516033717081882e-05, 1.9516033717081882e-05, 1.9516033717081882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9516033717081882e-05

Optimization complete. Final v2v error: 3.8096208572387695 mm

Highest mean error: 4.022952556610107 mm for frame 184

Lowest mean error: 3.5279898643493652 mm for frame 107

Saving results

Total time: 87.03145217895508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01149620
Iteration 2/25 | Loss: 0.00215489
Iteration 3/25 | Loss: 0.00160916
Iteration 4/25 | Loss: 0.00126375
Iteration 5/25 | Loss: 0.00112934
Iteration 6/25 | Loss: 0.00120588
Iteration 7/25 | Loss: 0.00106748
Iteration 8/25 | Loss: 0.00108656
Iteration 9/25 | Loss: 0.00101682
Iteration 10/25 | Loss: 0.00101244
Iteration 11/25 | Loss: 0.00099107
Iteration 12/25 | Loss: 0.00098505
Iteration 13/25 | Loss: 0.00098222
Iteration 14/25 | Loss: 0.00098142
Iteration 15/25 | Loss: 0.00098087
Iteration 16/25 | Loss: 0.00098070
Iteration 17/25 | Loss: 0.00098063
Iteration 18/25 | Loss: 0.00098055
Iteration 19/25 | Loss: 0.00098052
Iteration 20/25 | Loss: 0.00098052
Iteration 21/25 | Loss: 0.00098052
Iteration 22/25 | Loss: 0.00098051
Iteration 23/25 | Loss: 0.00098051
Iteration 24/25 | Loss: 0.00098051
Iteration 25/25 | Loss: 0.00098051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50157905
Iteration 2/25 | Loss: 0.00087257
Iteration 3/25 | Loss: 0.00087257
Iteration 4/25 | Loss: 0.00087257
Iteration 5/25 | Loss: 0.00087257
Iteration 6/25 | Loss: 0.00087257
Iteration 7/25 | Loss: 0.00087256
Iteration 8/25 | Loss: 0.00087256
Iteration 9/25 | Loss: 0.00087256
Iteration 10/25 | Loss: 0.00087256
Iteration 11/25 | Loss: 0.00087256
Iteration 12/25 | Loss: 0.00087256
Iteration 13/25 | Loss: 0.00087256
Iteration 14/25 | Loss: 0.00087256
Iteration 15/25 | Loss: 0.00087256
Iteration 16/25 | Loss: 0.00087256
Iteration 17/25 | Loss: 0.00087256
Iteration 18/25 | Loss: 0.00087256
Iteration 19/25 | Loss: 0.00087256
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008725644438527524, 0.0008725644438527524, 0.0008725644438527524, 0.0008725644438527524, 0.0008725644438527524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008725644438527524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087256
Iteration 2/1000 | Loss: 0.00007142
Iteration 3/1000 | Loss: 0.00004344
Iteration 4/1000 | Loss: 0.00004064
Iteration 5/1000 | Loss: 0.00003808
Iteration 6/1000 | Loss: 0.00003689
Iteration 7/1000 | Loss: 0.00003566
Iteration 8/1000 | Loss: 0.00003524
Iteration 9/1000 | Loss: 0.00003503
Iteration 10/1000 | Loss: 0.00003488
Iteration 11/1000 | Loss: 0.00003487
Iteration 12/1000 | Loss: 0.00003487
Iteration 13/1000 | Loss: 0.00003487
Iteration 14/1000 | Loss: 0.00003487
Iteration 15/1000 | Loss: 0.00003486
Iteration 16/1000 | Loss: 0.00003486
Iteration 17/1000 | Loss: 0.00003486
Iteration 18/1000 | Loss: 0.00003486
Iteration 19/1000 | Loss: 0.00003486
Iteration 20/1000 | Loss: 0.00003486
Iteration 21/1000 | Loss: 0.00003486
Iteration 22/1000 | Loss: 0.00003486
Iteration 23/1000 | Loss: 0.00003486
Iteration 24/1000 | Loss: 0.00003486
Iteration 25/1000 | Loss: 0.00003485
Iteration 26/1000 | Loss: 0.00003485
Iteration 27/1000 | Loss: 0.00003483
Iteration 28/1000 | Loss: 0.00003482
Iteration 29/1000 | Loss: 0.00003482
Iteration 30/1000 | Loss: 0.00003481
Iteration 31/1000 | Loss: 0.00003480
Iteration 32/1000 | Loss: 0.00003480
Iteration 33/1000 | Loss: 0.00003479
Iteration 34/1000 | Loss: 0.00003479
Iteration 35/1000 | Loss: 0.00003479
Iteration 36/1000 | Loss: 0.00003479
Iteration 37/1000 | Loss: 0.00003478
Iteration 38/1000 | Loss: 0.00003478
Iteration 39/1000 | Loss: 0.00003478
Iteration 40/1000 | Loss: 0.00003478
Iteration 41/1000 | Loss: 0.00003478
Iteration 42/1000 | Loss: 0.00003478
Iteration 43/1000 | Loss: 0.00003477
Iteration 44/1000 | Loss: 0.00003477
Iteration 45/1000 | Loss: 0.00003477
Iteration 46/1000 | Loss: 0.00003477
Iteration 47/1000 | Loss: 0.00003477
Iteration 48/1000 | Loss: 0.00003477
Iteration 49/1000 | Loss: 0.00003477
Iteration 50/1000 | Loss: 0.00003476
Iteration 51/1000 | Loss: 0.00003476
Iteration 52/1000 | Loss: 0.00003476
Iteration 53/1000 | Loss: 0.00003476
Iteration 54/1000 | Loss: 0.00003476
Iteration 55/1000 | Loss: 0.00003476
Iteration 56/1000 | Loss: 0.00003475
Iteration 57/1000 | Loss: 0.00003475
Iteration 58/1000 | Loss: 0.00003475
Iteration 59/1000 | Loss: 0.00003475
Iteration 60/1000 | Loss: 0.00003474
Iteration 61/1000 | Loss: 0.00003473
Iteration 62/1000 | Loss: 0.00003473
Iteration 63/1000 | Loss: 0.00003473
Iteration 64/1000 | Loss: 0.00003473
Iteration 65/1000 | Loss: 0.00003473
Iteration 66/1000 | Loss: 0.00003473
Iteration 67/1000 | Loss: 0.00003472
Iteration 68/1000 | Loss: 0.00003472
Iteration 69/1000 | Loss: 0.00003472
Iteration 70/1000 | Loss: 0.00003472
Iteration 71/1000 | Loss: 0.00003472
Iteration 72/1000 | Loss: 0.00003472
Iteration 73/1000 | Loss: 0.00003471
Iteration 74/1000 | Loss: 0.00003470
Iteration 75/1000 | Loss: 0.00003469
Iteration 76/1000 | Loss: 0.00003468
Iteration 77/1000 | Loss: 0.00003468
Iteration 78/1000 | Loss: 0.00003468
Iteration 79/1000 | Loss: 0.00003467
Iteration 80/1000 | Loss: 0.00003467
Iteration 81/1000 | Loss: 0.00003466
Iteration 82/1000 | Loss: 0.00003465
Iteration 83/1000 | Loss: 0.00003465
Iteration 84/1000 | Loss: 0.00003465
Iteration 85/1000 | Loss: 0.00003465
Iteration 86/1000 | Loss: 0.00003464
Iteration 87/1000 | Loss: 0.00003464
Iteration 88/1000 | Loss: 0.00003464
Iteration 89/1000 | Loss: 0.00003463
Iteration 90/1000 | Loss: 0.00003463
Iteration 91/1000 | Loss: 0.00003463
Iteration 92/1000 | Loss: 0.00003463
Iteration 93/1000 | Loss: 0.00003463
Iteration 94/1000 | Loss: 0.00003462
Iteration 95/1000 | Loss: 0.00003462
Iteration 96/1000 | Loss: 0.00003462
Iteration 97/1000 | Loss: 0.00003462
Iteration 98/1000 | Loss: 0.00003462
Iteration 99/1000 | Loss: 0.00003462
Iteration 100/1000 | Loss: 0.00003462
Iteration 101/1000 | Loss: 0.00003462
Iteration 102/1000 | Loss: 0.00003462
Iteration 103/1000 | Loss: 0.00003461
Iteration 104/1000 | Loss: 0.00003461
Iteration 105/1000 | Loss: 0.00003461
Iteration 106/1000 | Loss: 0.00003461
Iteration 107/1000 | Loss: 0.00003461
Iteration 108/1000 | Loss: 0.00003461
Iteration 109/1000 | Loss: 0.00003460
Iteration 110/1000 | Loss: 0.00003460
Iteration 111/1000 | Loss: 0.00003460
Iteration 112/1000 | Loss: 0.00003460
Iteration 113/1000 | Loss: 0.00003460
Iteration 114/1000 | Loss: 0.00003460
Iteration 115/1000 | Loss: 0.00003460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.4602315281517804e-05, 3.4602315281517804e-05, 3.4602315281517804e-05, 3.4602315281517804e-05, 3.4602315281517804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4602315281517804e-05

Optimization complete. Final v2v error: 4.876358985900879 mm

Highest mean error: 5.070672988891602 mm for frame 239

Lowest mean error: 4.684516429901123 mm for frame 173

Saving results

Total time: 172.37705898284912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01157229
Iteration 2/25 | Loss: 0.01157229
Iteration 3/25 | Loss: 0.00645358
Iteration 4/25 | Loss: 0.00394830
Iteration 5/25 | Loss: 0.00346411
Iteration 6/25 | Loss: 0.00242924
Iteration 7/25 | Loss: 0.00219741
Iteration 8/25 | Loss: 0.00210936
Iteration 9/25 | Loss: 0.00206821
Iteration 10/25 | Loss: 0.00201572
Iteration 11/25 | Loss: 0.00198849
Iteration 12/25 | Loss: 0.00194641
Iteration 13/25 | Loss: 0.00188740
Iteration 14/25 | Loss: 0.00179852
Iteration 15/25 | Loss: 0.00177480
Iteration 16/25 | Loss: 0.00178533
Iteration 17/25 | Loss: 0.00171210
Iteration 18/25 | Loss: 0.00160423
Iteration 19/25 | Loss: 0.00147588
Iteration 20/25 | Loss: 0.00143874
Iteration 21/25 | Loss: 0.00148345
Iteration 22/25 | Loss: 0.00140874
Iteration 23/25 | Loss: 0.00135178
Iteration 24/25 | Loss: 0.00131623
Iteration 25/25 | Loss: 0.00130553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75330329
Iteration 2/25 | Loss: 0.00448665
Iteration 3/25 | Loss: 0.00360578
Iteration 4/25 | Loss: 0.00360578
Iteration 5/25 | Loss: 0.00360578
Iteration 6/25 | Loss: 0.00360578
Iteration 7/25 | Loss: 0.00360578
Iteration 8/25 | Loss: 0.00360578
Iteration 9/25 | Loss: 0.00360578
Iteration 10/25 | Loss: 0.00360578
Iteration 11/25 | Loss: 0.00360578
Iteration 12/25 | Loss: 0.00360578
Iteration 13/25 | Loss: 0.00360578
Iteration 14/25 | Loss: 0.00360578
Iteration 15/25 | Loss: 0.00360578
Iteration 16/25 | Loss: 0.00360578
Iteration 17/25 | Loss: 0.00360578
Iteration 18/25 | Loss: 0.00360578
Iteration 19/25 | Loss: 0.00360578
Iteration 20/25 | Loss: 0.00360578
Iteration 21/25 | Loss: 0.00360578
Iteration 22/25 | Loss: 0.00360578
Iteration 23/25 | Loss: 0.00360578
Iteration 24/25 | Loss: 0.00360578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003605778096243739, 0.003605778096243739, 0.003605778096243739, 0.003605778096243739, 0.003605778096243739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003605778096243739

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360578
Iteration 2/1000 | Loss: 0.00307927
Iteration 3/1000 | Loss: 0.00325150
Iteration 4/1000 | Loss: 0.00196654
Iteration 5/1000 | Loss: 0.00107392
Iteration 6/1000 | Loss: 0.00190902
Iteration 7/1000 | Loss: 0.00282107
Iteration 8/1000 | Loss: 0.00318256
Iteration 9/1000 | Loss: 0.00075071
Iteration 10/1000 | Loss: 0.00076671
Iteration 11/1000 | Loss: 0.00264344
Iteration 12/1000 | Loss: 0.00200473
Iteration 13/1000 | Loss: 0.00056039
Iteration 14/1000 | Loss: 0.00111560
Iteration 15/1000 | Loss: 0.00042488
Iteration 16/1000 | Loss: 0.00049512
Iteration 17/1000 | Loss: 0.00079386
Iteration 18/1000 | Loss: 0.00058101
Iteration 19/1000 | Loss: 0.00060400
Iteration 20/1000 | Loss: 0.00048274
Iteration 21/1000 | Loss: 0.00110942
Iteration 22/1000 | Loss: 0.00050052
Iteration 23/1000 | Loss: 0.00035642
Iteration 24/1000 | Loss: 0.00020405
Iteration 25/1000 | Loss: 0.00053806
Iteration 26/1000 | Loss: 0.00207804
Iteration 27/1000 | Loss: 0.00031690
Iteration 28/1000 | Loss: 0.00041295
Iteration 29/1000 | Loss: 0.00034682
Iteration 30/1000 | Loss: 0.00028559
Iteration 31/1000 | Loss: 0.00026765
Iteration 32/1000 | Loss: 0.00021058
Iteration 33/1000 | Loss: 0.00023320
Iteration 34/1000 | Loss: 0.00035895
Iteration 35/1000 | Loss: 0.00033798
Iteration 36/1000 | Loss: 0.00042795
Iteration 37/1000 | Loss: 0.00043745
Iteration 38/1000 | Loss: 0.00022887
Iteration 39/1000 | Loss: 0.00034704
Iteration 40/1000 | Loss: 0.00012589
Iteration 41/1000 | Loss: 0.00010262
Iteration 42/1000 | Loss: 0.00025324
Iteration 43/1000 | Loss: 0.00057005
Iteration 44/1000 | Loss: 0.00038946
Iteration 45/1000 | Loss: 0.00057798
Iteration 46/1000 | Loss: 0.00058158
Iteration 47/1000 | Loss: 0.00023122
Iteration 48/1000 | Loss: 0.00027580
Iteration 49/1000 | Loss: 0.00040218
Iteration 50/1000 | Loss: 0.00043437
Iteration 51/1000 | Loss: 0.00008944
Iteration 52/1000 | Loss: 0.00045524
Iteration 53/1000 | Loss: 0.00030722
Iteration 54/1000 | Loss: 0.00047125
Iteration 55/1000 | Loss: 0.00027926
Iteration 56/1000 | Loss: 0.00076998
Iteration 57/1000 | Loss: 0.00025898
Iteration 58/1000 | Loss: 0.00075765
Iteration 59/1000 | Loss: 0.00038608
Iteration 60/1000 | Loss: 0.00036907
Iteration 61/1000 | Loss: 0.00037497
Iteration 62/1000 | Loss: 0.00010580
Iteration 63/1000 | Loss: 0.00019621
Iteration 64/1000 | Loss: 0.00005623
Iteration 65/1000 | Loss: 0.00005169
Iteration 66/1000 | Loss: 0.00004749
Iteration 67/1000 | Loss: 0.00004750
Iteration 68/1000 | Loss: 0.00004510
Iteration 69/1000 | Loss: 0.00004265
Iteration 70/1000 | Loss: 0.00038372
Iteration 71/1000 | Loss: 0.00036632
Iteration 72/1000 | Loss: 0.00007533
Iteration 73/1000 | Loss: 0.00016998
Iteration 74/1000 | Loss: 0.00038219
Iteration 75/1000 | Loss: 0.00018768
Iteration 76/1000 | Loss: 0.00016267
Iteration 77/1000 | Loss: 0.00004428
Iteration 78/1000 | Loss: 0.00004113
Iteration 79/1000 | Loss: 0.00021452
Iteration 80/1000 | Loss: 0.00004378
Iteration 81/1000 | Loss: 0.00004195
Iteration 82/1000 | Loss: 0.00003976
Iteration 83/1000 | Loss: 0.00004534
Iteration 84/1000 | Loss: 0.00004539
Iteration 85/1000 | Loss: 0.00004475
Iteration 86/1000 | Loss: 0.00004158
Iteration 87/1000 | Loss: 0.00004356
Iteration 88/1000 | Loss: 0.00003920
Iteration 89/1000 | Loss: 0.00004583
Iteration 90/1000 | Loss: 0.00004272
Iteration 91/1000 | Loss: 0.00004502
Iteration 92/1000 | Loss: 0.00004262
Iteration 93/1000 | Loss: 0.00003916
Iteration 94/1000 | Loss: 0.00003796
Iteration 95/1000 | Loss: 0.00003734
Iteration 96/1000 | Loss: 0.00003668
Iteration 97/1000 | Loss: 0.00003637
Iteration 98/1000 | Loss: 0.00003615
Iteration 99/1000 | Loss: 0.00003595
Iteration 100/1000 | Loss: 0.00003570
Iteration 101/1000 | Loss: 0.00003564
Iteration 102/1000 | Loss: 0.00003558
Iteration 103/1000 | Loss: 0.00003558
Iteration 104/1000 | Loss: 0.00003556
Iteration 105/1000 | Loss: 0.00003555
Iteration 106/1000 | Loss: 0.00003554
Iteration 107/1000 | Loss: 0.00003554
Iteration 108/1000 | Loss: 0.00003554
Iteration 109/1000 | Loss: 0.00003554
Iteration 110/1000 | Loss: 0.00003554
Iteration 111/1000 | Loss: 0.00003553
Iteration 112/1000 | Loss: 0.00003553
Iteration 113/1000 | Loss: 0.00003553
Iteration 114/1000 | Loss: 0.00003553
Iteration 115/1000 | Loss: 0.00003553
Iteration 116/1000 | Loss: 0.00003553
Iteration 117/1000 | Loss: 0.00003553
Iteration 118/1000 | Loss: 0.00003553
Iteration 119/1000 | Loss: 0.00003550
Iteration 120/1000 | Loss: 0.00003549
Iteration 121/1000 | Loss: 0.00003549
Iteration 122/1000 | Loss: 0.00003549
Iteration 123/1000 | Loss: 0.00003548
Iteration 124/1000 | Loss: 0.00003548
Iteration 125/1000 | Loss: 0.00003548
Iteration 126/1000 | Loss: 0.00003547
Iteration 127/1000 | Loss: 0.00003547
Iteration 128/1000 | Loss: 0.00003547
Iteration 129/1000 | Loss: 0.00003547
Iteration 130/1000 | Loss: 0.00003547
Iteration 131/1000 | Loss: 0.00003546
Iteration 132/1000 | Loss: 0.00003546
Iteration 133/1000 | Loss: 0.00003546
Iteration 134/1000 | Loss: 0.00003546
Iteration 135/1000 | Loss: 0.00003546
Iteration 136/1000 | Loss: 0.00003546
Iteration 137/1000 | Loss: 0.00003546
Iteration 138/1000 | Loss: 0.00003546
Iteration 139/1000 | Loss: 0.00003546
Iteration 140/1000 | Loss: 0.00003546
Iteration 141/1000 | Loss: 0.00003546
Iteration 142/1000 | Loss: 0.00003546
Iteration 143/1000 | Loss: 0.00003546
Iteration 144/1000 | Loss: 0.00003546
Iteration 145/1000 | Loss: 0.00003546
Iteration 146/1000 | Loss: 0.00003546
Iteration 147/1000 | Loss: 0.00003546
Iteration 148/1000 | Loss: 0.00003546
Iteration 149/1000 | Loss: 0.00003546
Iteration 150/1000 | Loss: 0.00003546
Iteration 151/1000 | Loss: 0.00003546
Iteration 152/1000 | Loss: 0.00003546
Iteration 153/1000 | Loss: 0.00003546
Iteration 154/1000 | Loss: 0.00003546
Iteration 155/1000 | Loss: 0.00003546
Iteration 156/1000 | Loss: 0.00003546
Iteration 157/1000 | Loss: 0.00003546
Iteration 158/1000 | Loss: 0.00003546
Iteration 159/1000 | Loss: 0.00003546
Iteration 160/1000 | Loss: 0.00003546
Iteration 161/1000 | Loss: 0.00003546
Iteration 162/1000 | Loss: 0.00003546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.5460732760839164e-05, 3.5460732760839164e-05, 3.5460732760839164e-05, 3.5460732760839164e-05, 3.5460732760839164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5460732760839164e-05

Optimization complete. Final v2v error: 4.6946868896484375 mm

Highest mean error: 13.478177070617676 mm for frame 32

Lowest mean error: 4.3290581703186035 mm for frame 71

Saving results

Total time: 603.1470663547516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01168504
Iteration 2/25 | Loss: 0.00330295
Iteration 3/25 | Loss: 0.00237528
Iteration 4/25 | Loss: 0.00206263
Iteration 5/25 | Loss: 0.00189774
Iteration 6/25 | Loss: 0.00158093
Iteration 7/25 | Loss: 0.00140699
Iteration 8/25 | Loss: 0.00135076
Iteration 9/25 | Loss: 0.00134266
Iteration 10/25 | Loss: 0.00119232
Iteration 11/25 | Loss: 0.00114657
Iteration 12/25 | Loss: 0.00113429
Iteration 13/25 | Loss: 0.00112787
Iteration 14/25 | Loss: 0.00111500
Iteration 15/25 | Loss: 0.00110417
Iteration 16/25 | Loss: 0.00109146
Iteration 17/25 | Loss: 0.00108423
Iteration 18/25 | Loss: 0.00107849
Iteration 19/25 | Loss: 0.00107616
Iteration 20/25 | Loss: 0.00107519
Iteration 21/25 | Loss: 0.00107484
Iteration 22/25 | Loss: 0.00107463
Iteration 23/25 | Loss: 0.00107455
Iteration 24/25 | Loss: 0.00107501
Iteration 25/25 | Loss: 0.00107474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62336171
Iteration 2/25 | Loss: 0.00121135
Iteration 3/25 | Loss: 0.00121135
Iteration 4/25 | Loss: 0.00121135
Iteration 5/25 | Loss: 0.00121135
Iteration 6/25 | Loss: 0.00121135
Iteration 7/25 | Loss: 0.00121135
Iteration 8/25 | Loss: 0.00121135
Iteration 9/25 | Loss: 0.00121135
Iteration 10/25 | Loss: 0.00121135
Iteration 11/25 | Loss: 0.00121135
Iteration 12/25 | Loss: 0.00121135
Iteration 13/25 | Loss: 0.00121135
Iteration 14/25 | Loss: 0.00121135
Iteration 15/25 | Loss: 0.00121135
Iteration 16/25 | Loss: 0.00121135
Iteration 17/25 | Loss: 0.00121135
Iteration 18/25 | Loss: 0.00121135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001211345661431551, 0.001211345661431551, 0.001211345661431551, 0.001211345661431551, 0.001211345661431551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001211345661431551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121135
Iteration 2/1000 | Loss: 0.00017153
Iteration 3/1000 | Loss: 0.00011741
Iteration 4/1000 | Loss: 0.00009588
Iteration 5/1000 | Loss: 0.00007764
Iteration 6/1000 | Loss: 0.00351491
Iteration 7/1000 | Loss: 0.00064221
Iteration 8/1000 | Loss: 0.00123934
Iteration 9/1000 | Loss: 0.00008062
Iteration 10/1000 | Loss: 0.00007123
Iteration 11/1000 | Loss: 0.00006739
Iteration 12/1000 | Loss: 0.00028411
Iteration 13/1000 | Loss: 0.00072816
Iteration 14/1000 | Loss: 0.00052130
Iteration 15/1000 | Loss: 0.00011149
Iteration 16/1000 | Loss: 0.00036826
Iteration 17/1000 | Loss: 0.00030541
Iteration 18/1000 | Loss: 0.00007705
Iteration 19/1000 | Loss: 0.00006285
Iteration 20/1000 | Loss: 0.00005402
Iteration 21/1000 | Loss: 0.00004763
Iteration 22/1000 | Loss: 0.00004502
Iteration 23/1000 | Loss: 0.00004378
Iteration 24/1000 | Loss: 0.00004315
Iteration 25/1000 | Loss: 0.00004257
Iteration 26/1000 | Loss: 0.00004222
Iteration 27/1000 | Loss: 0.00004203
Iteration 28/1000 | Loss: 0.00004192
Iteration 29/1000 | Loss: 0.00004178
Iteration 30/1000 | Loss: 0.00004174
Iteration 31/1000 | Loss: 0.00004174
Iteration 32/1000 | Loss: 0.00004173
Iteration 33/1000 | Loss: 0.00004169
Iteration 34/1000 | Loss: 0.00004164
Iteration 35/1000 | Loss: 0.00004164
Iteration 36/1000 | Loss: 0.00004164
Iteration 37/1000 | Loss: 0.00004163
Iteration 38/1000 | Loss: 0.00004161
Iteration 39/1000 | Loss: 0.00004160
Iteration 40/1000 | Loss: 0.00004159
Iteration 41/1000 | Loss: 0.00004159
Iteration 42/1000 | Loss: 0.00004158
Iteration 43/1000 | Loss: 0.00004158
Iteration 44/1000 | Loss: 0.00004157
Iteration 45/1000 | Loss: 0.00004156
Iteration 46/1000 | Loss: 0.00004153
Iteration 47/1000 | Loss: 0.00004149
Iteration 48/1000 | Loss: 0.00004149
Iteration 49/1000 | Loss: 0.00004149
Iteration 50/1000 | Loss: 0.00004148
Iteration 51/1000 | Loss: 0.00004148
Iteration 52/1000 | Loss: 0.00004148
Iteration 53/1000 | Loss: 0.00004147
Iteration 54/1000 | Loss: 0.00004147
Iteration 55/1000 | Loss: 0.00004143
Iteration 56/1000 | Loss: 0.00004143
Iteration 57/1000 | Loss: 0.00004142
Iteration 58/1000 | Loss: 0.00004141
Iteration 59/1000 | Loss: 0.00004140
Iteration 60/1000 | Loss: 0.00004139
Iteration 61/1000 | Loss: 0.00004138
Iteration 62/1000 | Loss: 0.00004138
Iteration 63/1000 | Loss: 0.00004135
Iteration 64/1000 | Loss: 0.00004133
Iteration 65/1000 | Loss: 0.00004133
Iteration 66/1000 | Loss: 0.00004128
Iteration 67/1000 | Loss: 0.00004127
Iteration 68/1000 | Loss: 0.00004125
Iteration 69/1000 | Loss: 0.00004125
Iteration 70/1000 | Loss: 0.00004124
Iteration 71/1000 | Loss: 0.00004124
Iteration 72/1000 | Loss: 0.00004123
Iteration 73/1000 | Loss: 0.00004122
Iteration 74/1000 | Loss: 0.00004122
Iteration 75/1000 | Loss: 0.00004121
Iteration 76/1000 | Loss: 0.00004121
Iteration 77/1000 | Loss: 0.00004119
Iteration 78/1000 | Loss: 0.00004119
Iteration 79/1000 | Loss: 0.00004119
Iteration 80/1000 | Loss: 0.00004119
Iteration 81/1000 | Loss: 0.00004119
Iteration 82/1000 | Loss: 0.00004119
Iteration 83/1000 | Loss: 0.00004118
Iteration 84/1000 | Loss: 0.00004118
Iteration 85/1000 | Loss: 0.00004118
Iteration 86/1000 | Loss: 0.00004118
Iteration 87/1000 | Loss: 0.00004118
Iteration 88/1000 | Loss: 0.00004116
Iteration 89/1000 | Loss: 0.00004115
Iteration 90/1000 | Loss: 0.00004115
Iteration 91/1000 | Loss: 0.00004115
Iteration 92/1000 | Loss: 0.00004115
Iteration 93/1000 | Loss: 0.00004115
Iteration 94/1000 | Loss: 0.00004115
Iteration 95/1000 | Loss: 0.00004115
Iteration 96/1000 | Loss: 0.00004115
Iteration 97/1000 | Loss: 0.00004115
Iteration 98/1000 | Loss: 0.00004114
Iteration 99/1000 | Loss: 0.00004114
Iteration 100/1000 | Loss: 0.00004108
Iteration 101/1000 | Loss: 0.00004108
Iteration 102/1000 | Loss: 0.00004107
Iteration 103/1000 | Loss: 0.00004106
Iteration 104/1000 | Loss: 0.00004106
Iteration 105/1000 | Loss: 0.00004105
Iteration 106/1000 | Loss: 0.00004105
Iteration 107/1000 | Loss: 0.00004104
Iteration 108/1000 | Loss: 0.00004104
Iteration 109/1000 | Loss: 0.00004104
Iteration 110/1000 | Loss: 0.00004104
Iteration 111/1000 | Loss: 0.00004103
Iteration 112/1000 | Loss: 0.00004101
Iteration 113/1000 | Loss: 0.00004101
Iteration 114/1000 | Loss: 0.00004101
Iteration 115/1000 | Loss: 0.00004101
Iteration 116/1000 | Loss: 0.00004101
Iteration 117/1000 | Loss: 0.00004101
Iteration 118/1000 | Loss: 0.00004099
Iteration 119/1000 | Loss: 0.00004099
Iteration 120/1000 | Loss: 0.00004099
Iteration 121/1000 | Loss: 0.00004099
Iteration 122/1000 | Loss: 0.00004098
Iteration 123/1000 | Loss: 0.00004098
Iteration 124/1000 | Loss: 0.00004098
Iteration 125/1000 | Loss: 0.00004097
Iteration 126/1000 | Loss: 0.00004097
Iteration 127/1000 | Loss: 0.00004097
Iteration 128/1000 | Loss: 0.00004096
Iteration 129/1000 | Loss: 0.00004096
Iteration 130/1000 | Loss: 0.00004096
Iteration 131/1000 | Loss: 0.00004095
Iteration 132/1000 | Loss: 0.00004095
Iteration 133/1000 | Loss: 0.00004095
Iteration 134/1000 | Loss: 0.00004094
Iteration 135/1000 | Loss: 0.00004094
Iteration 136/1000 | Loss: 0.00004094
Iteration 137/1000 | Loss: 0.00004094
Iteration 138/1000 | Loss: 0.00004094
Iteration 139/1000 | Loss: 0.00004094
Iteration 140/1000 | Loss: 0.00004094
Iteration 141/1000 | Loss: 0.00004094
Iteration 142/1000 | Loss: 0.00004094
Iteration 143/1000 | Loss: 0.00004094
Iteration 144/1000 | Loss: 0.00004094
Iteration 145/1000 | Loss: 0.00004093
Iteration 146/1000 | Loss: 0.00004093
Iteration 147/1000 | Loss: 0.00004093
Iteration 148/1000 | Loss: 0.00004093
Iteration 149/1000 | Loss: 0.00004093
Iteration 150/1000 | Loss: 0.00004092
Iteration 151/1000 | Loss: 0.00004092
Iteration 152/1000 | Loss: 0.00004092
Iteration 153/1000 | Loss: 0.00004092
Iteration 154/1000 | Loss: 0.00004092
Iteration 155/1000 | Loss: 0.00004092
Iteration 156/1000 | Loss: 0.00004091
Iteration 157/1000 | Loss: 0.00004091
Iteration 158/1000 | Loss: 0.00004091
Iteration 159/1000 | Loss: 0.00004091
Iteration 160/1000 | Loss: 0.00004090
Iteration 161/1000 | Loss: 0.00004090
Iteration 162/1000 | Loss: 0.00004090
Iteration 163/1000 | Loss: 0.00004090
Iteration 164/1000 | Loss: 0.00004090
Iteration 165/1000 | Loss: 0.00004089
Iteration 166/1000 | Loss: 0.00004089
Iteration 167/1000 | Loss: 0.00004089
Iteration 168/1000 | Loss: 0.00004089
Iteration 169/1000 | Loss: 0.00004088
Iteration 170/1000 | Loss: 0.00004088
Iteration 171/1000 | Loss: 0.00004088
Iteration 172/1000 | Loss: 0.00004088
Iteration 173/1000 | Loss: 0.00004088
Iteration 174/1000 | Loss: 0.00004088
Iteration 175/1000 | Loss: 0.00004088
Iteration 176/1000 | Loss: 0.00004088
Iteration 177/1000 | Loss: 0.00004088
Iteration 178/1000 | Loss: 0.00004088
Iteration 179/1000 | Loss: 0.00004088
Iteration 180/1000 | Loss: 0.00004088
Iteration 181/1000 | Loss: 0.00004088
Iteration 182/1000 | Loss: 0.00004088
Iteration 183/1000 | Loss: 0.00004088
Iteration 184/1000 | Loss: 0.00004087
Iteration 185/1000 | Loss: 0.00004087
Iteration 186/1000 | Loss: 0.00004087
Iteration 187/1000 | Loss: 0.00004087
Iteration 188/1000 | Loss: 0.00004087
Iteration 189/1000 | Loss: 0.00004087
Iteration 190/1000 | Loss: 0.00004087
Iteration 191/1000 | Loss: 0.00004087
Iteration 192/1000 | Loss: 0.00004087
Iteration 193/1000 | Loss: 0.00004087
Iteration 194/1000 | Loss: 0.00004086
Iteration 195/1000 | Loss: 0.00004086
Iteration 196/1000 | Loss: 0.00004086
Iteration 197/1000 | Loss: 0.00004086
Iteration 198/1000 | Loss: 0.00004086
Iteration 199/1000 | Loss: 0.00004086
Iteration 200/1000 | Loss: 0.00004086
Iteration 201/1000 | Loss: 0.00004086
Iteration 202/1000 | Loss: 0.00004086
Iteration 203/1000 | Loss: 0.00004086
Iteration 204/1000 | Loss: 0.00004086
Iteration 205/1000 | Loss: 0.00004086
Iteration 206/1000 | Loss: 0.00004086
Iteration 207/1000 | Loss: 0.00004085
Iteration 208/1000 | Loss: 0.00004085
Iteration 209/1000 | Loss: 0.00004085
Iteration 210/1000 | Loss: 0.00004085
Iteration 211/1000 | Loss: 0.00004085
Iteration 212/1000 | Loss: 0.00004085
Iteration 213/1000 | Loss: 0.00004084
Iteration 214/1000 | Loss: 0.00004084
Iteration 215/1000 | Loss: 0.00004084
Iteration 216/1000 | Loss: 0.00004084
Iteration 217/1000 | Loss: 0.00004084
Iteration 218/1000 | Loss: 0.00004084
Iteration 219/1000 | Loss: 0.00004084
Iteration 220/1000 | Loss: 0.00004084
Iteration 221/1000 | Loss: 0.00004084
Iteration 222/1000 | Loss: 0.00004084
Iteration 223/1000 | Loss: 0.00004083
Iteration 224/1000 | Loss: 0.00004083
Iteration 225/1000 | Loss: 0.00004083
Iteration 226/1000 | Loss: 0.00004083
Iteration 227/1000 | Loss: 0.00004083
Iteration 228/1000 | Loss: 0.00004083
Iteration 229/1000 | Loss: 0.00004083
Iteration 230/1000 | Loss: 0.00004083
Iteration 231/1000 | Loss: 0.00004083
Iteration 232/1000 | Loss: 0.00004083
Iteration 233/1000 | Loss: 0.00004083
Iteration 234/1000 | Loss: 0.00004083
Iteration 235/1000 | Loss: 0.00004083
Iteration 236/1000 | Loss: 0.00004083
Iteration 237/1000 | Loss: 0.00004083
Iteration 238/1000 | Loss: 0.00004083
Iteration 239/1000 | Loss: 0.00004083
Iteration 240/1000 | Loss: 0.00004083
Iteration 241/1000 | Loss: 0.00004083
Iteration 242/1000 | Loss: 0.00004083
Iteration 243/1000 | Loss: 0.00004083
Iteration 244/1000 | Loss: 0.00004082
Iteration 245/1000 | Loss: 0.00004082
Iteration 246/1000 | Loss: 0.00004082
Iteration 247/1000 | Loss: 0.00004082
Iteration 248/1000 | Loss: 0.00004082
Iteration 249/1000 | Loss: 0.00004082
Iteration 250/1000 | Loss: 0.00004082
Iteration 251/1000 | Loss: 0.00004082
Iteration 252/1000 | Loss: 0.00004082
Iteration 253/1000 | Loss: 0.00004082
Iteration 254/1000 | Loss: 0.00004082
Iteration 255/1000 | Loss: 0.00004082
Iteration 256/1000 | Loss: 0.00004082
Iteration 257/1000 | Loss: 0.00004082
Iteration 258/1000 | Loss: 0.00004082
Iteration 259/1000 | Loss: 0.00004082
Iteration 260/1000 | Loss: 0.00004082
Iteration 261/1000 | Loss: 0.00004082
Iteration 262/1000 | Loss: 0.00004081
Iteration 263/1000 | Loss: 0.00004081
Iteration 264/1000 | Loss: 0.00004081
Iteration 265/1000 | Loss: 0.00004081
Iteration 266/1000 | Loss: 0.00004081
Iteration 267/1000 | Loss: 0.00004081
Iteration 268/1000 | Loss: 0.00004081
Iteration 269/1000 | Loss: 0.00004081
Iteration 270/1000 | Loss: 0.00004080
Iteration 271/1000 | Loss: 0.00004080
Iteration 272/1000 | Loss: 0.00004080
Iteration 273/1000 | Loss: 0.00004080
Iteration 274/1000 | Loss: 0.00004080
Iteration 275/1000 | Loss: 0.00004080
Iteration 276/1000 | Loss: 0.00004080
Iteration 277/1000 | Loss: 0.00004080
Iteration 278/1000 | Loss: 0.00004080
Iteration 279/1000 | Loss: 0.00004079
Iteration 280/1000 | Loss: 0.00004079
Iteration 281/1000 | Loss: 0.00004079
Iteration 282/1000 | Loss: 0.00004079
Iteration 283/1000 | Loss: 0.00004079
Iteration 284/1000 | Loss: 0.00004079
Iteration 285/1000 | Loss: 0.00004079
Iteration 286/1000 | Loss: 0.00004079
Iteration 287/1000 | Loss: 0.00004079
Iteration 288/1000 | Loss: 0.00004079
Iteration 289/1000 | Loss: 0.00004078
Iteration 290/1000 | Loss: 0.00004078
Iteration 291/1000 | Loss: 0.00004078
Iteration 292/1000 | Loss: 0.00004078
Iteration 293/1000 | Loss: 0.00004078
Iteration 294/1000 | Loss: 0.00004078
Iteration 295/1000 | Loss: 0.00004078
Iteration 296/1000 | Loss: 0.00004078
Iteration 297/1000 | Loss: 0.00004078
Iteration 298/1000 | Loss: 0.00004078
Iteration 299/1000 | Loss: 0.00004078
Iteration 300/1000 | Loss: 0.00004078
Iteration 301/1000 | Loss: 0.00004078
Iteration 302/1000 | Loss: 0.00004078
Iteration 303/1000 | Loss: 0.00004078
Iteration 304/1000 | Loss: 0.00004078
Iteration 305/1000 | Loss: 0.00004078
Iteration 306/1000 | Loss: 0.00004078
Iteration 307/1000 | Loss: 0.00004078
Iteration 308/1000 | Loss: 0.00004078
Iteration 309/1000 | Loss: 0.00004078
Iteration 310/1000 | Loss: 0.00004078
Iteration 311/1000 | Loss: 0.00004078
Iteration 312/1000 | Loss: 0.00004078
Iteration 313/1000 | Loss: 0.00004078
Iteration 314/1000 | Loss: 0.00004078
Iteration 315/1000 | Loss: 0.00004078
Iteration 316/1000 | Loss: 0.00004078
Iteration 317/1000 | Loss: 0.00004078
Iteration 318/1000 | Loss: 0.00004078
Iteration 319/1000 | Loss: 0.00004078
Iteration 320/1000 | Loss: 0.00004078
Iteration 321/1000 | Loss: 0.00004078
Iteration 322/1000 | Loss: 0.00004078
Iteration 323/1000 | Loss: 0.00004078
Iteration 324/1000 | Loss: 0.00004078
Iteration 325/1000 | Loss: 0.00004078
Iteration 326/1000 | Loss: 0.00004078
Iteration 327/1000 | Loss: 0.00004078
Iteration 328/1000 | Loss: 0.00004078
Iteration 329/1000 | Loss: 0.00004078
Iteration 330/1000 | Loss: 0.00004078
Iteration 331/1000 | Loss: 0.00004078
Iteration 332/1000 | Loss: 0.00004078
Iteration 333/1000 | Loss: 0.00004078
Iteration 334/1000 | Loss: 0.00004078
Iteration 335/1000 | Loss: 0.00004078
Iteration 336/1000 | Loss: 0.00004078
Iteration 337/1000 | Loss: 0.00004078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [4.0775023080641404e-05, 4.0775023080641404e-05, 4.0775023080641404e-05, 4.0775023080641404e-05, 4.0775023080641404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0775023080641404e-05

Optimization complete. Final v2v error: 5.310403823852539 mm

Highest mean error: 9.937349319458008 mm for frame 123

Lowest mean error: 4.895736217498779 mm for frame 86

Saving results

Total time: 264.7961916923523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105688
Iteration 2/25 | Loss: 0.00324944
Iteration 3/25 | Loss: 0.00186621
Iteration 4/25 | Loss: 0.00158866
Iteration 5/25 | Loss: 0.00160791
Iteration 6/25 | Loss: 0.00133285
Iteration 7/25 | Loss: 0.00120184
Iteration 8/25 | Loss: 0.00109950
Iteration 9/25 | Loss: 0.00103607
Iteration 10/25 | Loss: 0.00099959
Iteration 11/25 | Loss: 0.00093529
Iteration 12/25 | Loss: 0.00091495
Iteration 13/25 | Loss: 0.00091140
Iteration 14/25 | Loss: 0.00090629
Iteration 15/25 | Loss: 0.00089360
Iteration 16/25 | Loss: 0.00089233
Iteration 17/25 | Loss: 0.00089126
Iteration 18/25 | Loss: 0.00088853
Iteration 19/25 | Loss: 0.00088853
Iteration 20/25 | Loss: 0.00088792
Iteration 21/25 | Loss: 0.00088954
Iteration 22/25 | Loss: 0.00088881
Iteration 23/25 | Loss: 0.00088816
Iteration 24/25 | Loss: 0.00088571
Iteration 25/25 | Loss: 0.00088557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52217376
Iteration 2/25 | Loss: 0.00113094
Iteration 3/25 | Loss: 0.00101745
Iteration 4/25 | Loss: 0.00101745
Iteration 5/25 | Loss: 0.00101745
Iteration 6/25 | Loss: 0.00101745
Iteration 7/25 | Loss: 0.00101745
Iteration 8/25 | Loss: 0.00101745
Iteration 9/25 | Loss: 0.00101745
Iteration 10/25 | Loss: 0.00101745
Iteration 11/25 | Loss: 0.00101745
Iteration 12/25 | Loss: 0.00101745
Iteration 13/25 | Loss: 0.00101745
Iteration 14/25 | Loss: 0.00101745
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001017447910271585, 0.001017447910271585, 0.001017447910271585, 0.001017447910271585, 0.001017447910271585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001017447910271585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101745
Iteration 2/1000 | Loss: 0.00037548
Iteration 3/1000 | Loss: 0.00007346
Iteration 4/1000 | Loss: 0.00086643
Iteration 5/1000 | Loss: 0.00006726
Iteration 6/1000 | Loss: 0.00008400
Iteration 7/1000 | Loss: 0.00005568
Iteration 8/1000 | Loss: 0.00007139
Iteration 9/1000 | Loss: 0.00062713
Iteration 10/1000 | Loss: 0.00007123
Iteration 11/1000 | Loss: 0.00005866
Iteration 12/1000 | Loss: 0.00006130
Iteration 13/1000 | Loss: 0.00007356
Iteration 14/1000 | Loss: 0.00005528
Iteration 15/1000 | Loss: 0.00007565
Iteration 16/1000 | Loss: 0.00006183
Iteration 17/1000 | Loss: 0.00005460
Iteration 18/1000 | Loss: 0.00009539
Iteration 19/1000 | Loss: 0.00008148
Iteration 20/1000 | Loss: 0.00005889
Iteration 21/1000 | Loss: 0.00010114
Iteration 22/1000 | Loss: 0.00005379
Iteration 23/1000 | Loss: 0.00009080
Iteration 24/1000 | Loss: 0.00006205
Iteration 25/1000 | Loss: 0.00006608
Iteration 26/1000 | Loss: 0.00007940
Iteration 27/1000 | Loss: 0.00005455
Iteration 28/1000 | Loss: 0.00009162
Iteration 29/1000 | Loss: 0.00005889
Iteration 30/1000 | Loss: 0.00007400
Iteration 31/1000 | Loss: 0.00005864
Iteration 32/1000 | Loss: 0.00011309
Iteration 33/1000 | Loss: 0.00008252
Iteration 34/1000 | Loss: 0.00006555
Iteration 35/1000 | Loss: 0.00002777
Iteration 36/1000 | Loss: 0.00009161
Iteration 37/1000 | Loss: 0.00002778
Iteration 38/1000 | Loss: 0.00004267
Iteration 39/1000 | Loss: 0.00002593
Iteration 40/1000 | Loss: 0.00005261
Iteration 41/1000 | Loss: 0.00005372
Iteration 42/1000 | Loss: 0.00064193
Iteration 43/1000 | Loss: 0.00009154
Iteration 44/1000 | Loss: 0.00002801
Iteration 45/1000 | Loss: 0.00006566
Iteration 46/1000 | Loss: 0.00003448
Iteration 47/1000 | Loss: 0.00002708
Iteration 48/1000 | Loss: 0.00002571
Iteration 49/1000 | Loss: 0.00002522
Iteration 50/1000 | Loss: 0.00002637
Iteration 51/1000 | Loss: 0.00003971
Iteration 52/1000 | Loss: 0.00004194
Iteration 53/1000 | Loss: 0.00002500
Iteration 54/1000 | Loss: 0.00002491
Iteration 55/1000 | Loss: 0.00002488
Iteration 56/1000 | Loss: 0.00003774
Iteration 57/1000 | Loss: 0.00004024
Iteration 58/1000 | Loss: 0.00002649
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00002476
Iteration 61/1000 | Loss: 0.00002476
Iteration 62/1000 | Loss: 0.00002476
Iteration 63/1000 | Loss: 0.00002476
Iteration 64/1000 | Loss: 0.00002475
Iteration 65/1000 | Loss: 0.00002475
Iteration 66/1000 | Loss: 0.00002475
Iteration 67/1000 | Loss: 0.00003324
Iteration 68/1000 | Loss: 0.00005351
Iteration 69/1000 | Loss: 0.00004498
Iteration 70/1000 | Loss: 0.00002475
Iteration 71/1000 | Loss: 0.00002470
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002470
Iteration 80/1000 | Loss: 0.00002470
Iteration 81/1000 | Loss: 0.00002470
Iteration 82/1000 | Loss: 0.00002470
Iteration 83/1000 | Loss: 0.00002470
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002469
Iteration 87/1000 | Loss: 0.00002469
Iteration 88/1000 | Loss: 0.00002469
Iteration 89/1000 | Loss: 0.00002469
Iteration 90/1000 | Loss: 0.00002469
Iteration 91/1000 | Loss: 0.00002469
Iteration 92/1000 | Loss: 0.00002469
Iteration 93/1000 | Loss: 0.00002469
Iteration 94/1000 | Loss: 0.00002469
Iteration 95/1000 | Loss: 0.00002469
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002974
Iteration 98/1000 | Loss: 0.00002470
Iteration 99/1000 | Loss: 0.00002470
Iteration 100/1000 | Loss: 0.00002470
Iteration 101/1000 | Loss: 0.00002469
Iteration 102/1000 | Loss: 0.00002469
Iteration 103/1000 | Loss: 0.00002469
Iteration 104/1000 | Loss: 0.00002469
Iteration 105/1000 | Loss: 0.00002469
Iteration 106/1000 | Loss: 0.00002469
Iteration 107/1000 | Loss: 0.00002469
Iteration 108/1000 | Loss: 0.00002468
Iteration 109/1000 | Loss: 0.00002468
Iteration 110/1000 | Loss: 0.00002468
Iteration 111/1000 | Loss: 0.00002468
Iteration 112/1000 | Loss: 0.00002468
Iteration 113/1000 | Loss: 0.00002467
Iteration 114/1000 | Loss: 0.00002467
Iteration 115/1000 | Loss: 0.00002467
Iteration 116/1000 | Loss: 0.00002467
Iteration 117/1000 | Loss: 0.00002467
Iteration 118/1000 | Loss: 0.00002467
Iteration 119/1000 | Loss: 0.00002467
Iteration 120/1000 | Loss: 0.00002467
Iteration 121/1000 | Loss: 0.00002467
Iteration 122/1000 | Loss: 0.00002467
Iteration 123/1000 | Loss: 0.00002467
Iteration 124/1000 | Loss: 0.00002467
Iteration 125/1000 | Loss: 0.00002467
Iteration 126/1000 | Loss: 0.00002467
Iteration 127/1000 | Loss: 0.00002467
Iteration 128/1000 | Loss: 0.00002467
Iteration 129/1000 | Loss: 0.00002467
Iteration 130/1000 | Loss: 0.00002467
Iteration 131/1000 | Loss: 0.00002466
Iteration 132/1000 | Loss: 0.00002843
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002466
Iteration 135/1000 | Loss: 0.00002530
Iteration 136/1000 | Loss: 0.00002465
Iteration 137/1000 | Loss: 0.00004124
Iteration 138/1000 | Loss: 0.00002728
Iteration 139/1000 | Loss: 0.00002466
Iteration 140/1000 | Loss: 0.00002466
Iteration 141/1000 | Loss: 0.00002466
Iteration 142/1000 | Loss: 0.00002719
Iteration 143/1000 | Loss: 0.00002598
Iteration 144/1000 | Loss: 0.00002913
Iteration 145/1000 | Loss: 0.00009201
Iteration 146/1000 | Loss: 0.00004098
Iteration 147/1000 | Loss: 0.00003142
Iteration 148/1000 | Loss: 0.00002563
Iteration 149/1000 | Loss: 0.00002471
Iteration 150/1000 | Loss: 0.00002471
Iteration 151/1000 | Loss: 0.00002471
Iteration 152/1000 | Loss: 0.00002980
Iteration 153/1000 | Loss: 0.00002842
Iteration 154/1000 | Loss: 0.00002539
Iteration 155/1000 | Loss: 0.00002502
Iteration 156/1000 | Loss: 0.00002476
Iteration 157/1000 | Loss: 0.00002476
Iteration 158/1000 | Loss: 0.00002466
Iteration 159/1000 | Loss: 0.00002466
Iteration 160/1000 | Loss: 0.00002466
Iteration 161/1000 | Loss: 0.00002465
Iteration 162/1000 | Loss: 0.00002465
Iteration 163/1000 | Loss: 0.00002465
Iteration 164/1000 | Loss: 0.00002465
Iteration 165/1000 | Loss: 0.00002465
Iteration 166/1000 | Loss: 0.00002465
Iteration 167/1000 | Loss: 0.00002465
Iteration 168/1000 | Loss: 0.00002465
Iteration 169/1000 | Loss: 0.00002465
Iteration 170/1000 | Loss: 0.00002465
Iteration 171/1000 | Loss: 0.00002465
Iteration 172/1000 | Loss: 0.00002465
Iteration 173/1000 | Loss: 0.00002465
Iteration 174/1000 | Loss: 0.00002465
Iteration 175/1000 | Loss: 0.00002465
Iteration 176/1000 | Loss: 0.00002465
Iteration 177/1000 | Loss: 0.00002465
Iteration 178/1000 | Loss: 0.00002465
Iteration 179/1000 | Loss: 0.00002465
Iteration 180/1000 | Loss: 0.00002465
Iteration 181/1000 | Loss: 0.00002465
Iteration 182/1000 | Loss: 0.00002465
Iteration 183/1000 | Loss: 0.00002465
Iteration 184/1000 | Loss: 0.00002465
Iteration 185/1000 | Loss: 0.00002465
Iteration 186/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.4653189029777423e-05, 2.4653189029777423e-05, 2.4653189029777423e-05, 2.4653189029777423e-05, 2.4653189029777423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4653189029777423e-05

Optimization complete. Final v2v error: 3.98544979095459 mm

Highest mean error: 10.066046714782715 mm for frame 121

Lowest mean error: 3.488953113555908 mm for frame 43

Saving results

Total time: 480.58609795570374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894920
Iteration 2/25 | Loss: 0.00192100
Iteration 3/25 | Loss: 0.00106502
Iteration 4/25 | Loss: 0.00094074
Iteration 5/25 | Loss: 0.00092192
Iteration 6/25 | Loss: 0.00091775
Iteration 7/25 | Loss: 0.00091732
Iteration 8/25 | Loss: 0.00091729
Iteration 9/25 | Loss: 0.00091729
Iteration 10/25 | Loss: 0.00091729
Iteration 11/25 | Loss: 0.00091729
Iteration 12/25 | Loss: 0.00091729
Iteration 13/25 | Loss: 0.00091729
Iteration 14/25 | Loss: 0.00091729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009172902209684253, 0.0009172902209684253, 0.0009172902209684253, 0.0009172902209684253, 0.0009172902209684253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009172902209684253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90045083
Iteration 2/25 | Loss: 0.00117602
Iteration 3/25 | Loss: 0.00117602
Iteration 4/25 | Loss: 0.00117602
Iteration 5/25 | Loss: 0.00117602
Iteration 6/25 | Loss: 0.00117602
Iteration 7/25 | Loss: 0.00117602
Iteration 8/25 | Loss: 0.00117601
Iteration 9/25 | Loss: 0.00117601
Iteration 10/25 | Loss: 0.00117601
Iteration 11/25 | Loss: 0.00117601
Iteration 12/25 | Loss: 0.00117601
Iteration 13/25 | Loss: 0.00117601
Iteration 14/25 | Loss: 0.00117601
Iteration 15/25 | Loss: 0.00117601
Iteration 16/25 | Loss: 0.00117601
Iteration 17/25 | Loss: 0.00117601
Iteration 18/25 | Loss: 0.00117601
Iteration 19/25 | Loss: 0.00117601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001176013727672398, 0.001176013727672398, 0.001176013727672398, 0.001176013727672398, 0.001176013727672398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001176013727672398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117601
Iteration 2/1000 | Loss: 0.00003097
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001913
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001722
Iteration 9/1000 | Loss: 0.00001718
Iteration 10/1000 | Loss: 0.00001714
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001710
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001679
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001679
Iteration 24/1000 | Loss: 0.00001679
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001674
Iteration 30/1000 | Loss: 0.00001674
Iteration 31/1000 | Loss: 0.00001673
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001672
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001670
Iteration 36/1000 | Loss: 0.00001670
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001668
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001665
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001665
Iteration 49/1000 | Loss: 0.00001665
Iteration 50/1000 | Loss: 0.00001665
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001664
Iteration 56/1000 | Loss: 0.00001664
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001662
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001661
Iteration 63/1000 | Loss: 0.00001661
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001661
Iteration 66/1000 | Loss: 0.00001661
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001660
Iteration 69/1000 | Loss: 0.00001659
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Iteration 103/1000 | Loss: 0.00001656
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001656
Iteration 114/1000 | Loss: 0.00001656
Iteration 115/1000 | Loss: 0.00001656
Iteration 116/1000 | Loss: 0.00001656
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001656
Iteration 119/1000 | Loss: 0.00001656
Iteration 120/1000 | Loss: 0.00001656
Iteration 121/1000 | Loss: 0.00001656
Iteration 122/1000 | Loss: 0.00001656
Iteration 123/1000 | Loss: 0.00001656
Iteration 124/1000 | Loss: 0.00001656
Iteration 125/1000 | Loss: 0.00001656
Iteration 126/1000 | Loss: 0.00001656
Iteration 127/1000 | Loss: 0.00001656
Iteration 128/1000 | Loss: 0.00001656
Iteration 129/1000 | Loss: 0.00001656
Iteration 130/1000 | Loss: 0.00001656
Iteration 131/1000 | Loss: 0.00001656
Iteration 132/1000 | Loss: 0.00001656
Iteration 133/1000 | Loss: 0.00001656
Iteration 134/1000 | Loss: 0.00001656
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.6561125448788516e-05, 1.6561125448788516e-05, 1.6561125448788516e-05, 1.6561125448788516e-05, 1.6561125448788516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6561125448788516e-05

Optimization complete. Final v2v error: 3.556195020675659 mm

Highest mean error: 3.7715518474578857 mm for frame 169

Lowest mean error: 3.2635397911071777 mm for frame 238

Saving results

Total time: 105.57736825942993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102407
Iteration 2/25 | Loss: 0.01102407
Iteration 3/25 | Loss: 0.01102406
Iteration 4/25 | Loss: 0.00316521
Iteration 5/25 | Loss: 0.00207578
Iteration 6/25 | Loss: 0.00187151
Iteration 7/25 | Loss: 0.00175104
Iteration 8/25 | Loss: 0.00156617
Iteration 9/25 | Loss: 0.00151105
Iteration 10/25 | Loss: 0.00141809
Iteration 11/25 | Loss: 0.00135731
Iteration 12/25 | Loss: 0.00133730
Iteration 13/25 | Loss: 0.00130380
Iteration 14/25 | Loss: 0.00130534
Iteration 15/25 | Loss: 0.00128006
Iteration 16/25 | Loss: 0.00127178
Iteration 17/25 | Loss: 0.00125307
Iteration 18/25 | Loss: 0.00124439
Iteration 19/25 | Loss: 0.00122906
Iteration 20/25 | Loss: 0.00122150
Iteration 21/25 | Loss: 0.00121211
Iteration 22/25 | Loss: 0.00120562
Iteration 23/25 | Loss: 0.00120456
Iteration 24/25 | Loss: 0.00120241
Iteration 25/25 | Loss: 0.00120251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55698955
Iteration 2/25 | Loss: 0.00500021
Iteration 3/25 | Loss: 0.00448797
Iteration 4/25 | Loss: 0.00440317
Iteration 5/25 | Loss: 0.00440316
Iteration 6/25 | Loss: 0.00440316
Iteration 7/25 | Loss: 0.00440316
Iteration 8/25 | Loss: 0.00440316
Iteration 9/25 | Loss: 0.00440316
Iteration 10/25 | Loss: 0.00440316
Iteration 11/25 | Loss: 0.00440316
Iteration 12/25 | Loss: 0.00440316
Iteration 13/25 | Loss: 0.00440316
Iteration 14/25 | Loss: 0.00440316
Iteration 15/25 | Loss: 0.00440316
Iteration 16/25 | Loss: 0.00440316
Iteration 17/25 | Loss: 0.00440316
Iteration 18/25 | Loss: 0.00440316
Iteration 19/25 | Loss: 0.00440316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004403161816298962, 0.004403161816298962, 0.004403161816298962, 0.004403161816298962, 0.004403161816298962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004403161816298962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00440316
Iteration 2/1000 | Loss: 0.00179669
Iteration 3/1000 | Loss: 0.00193716
Iteration 4/1000 | Loss: 0.00184061
Iteration 5/1000 | Loss: 0.00062348
Iteration 6/1000 | Loss: 0.00072606
Iteration 7/1000 | Loss: 0.00097088
Iteration 8/1000 | Loss: 0.00031577
Iteration 9/1000 | Loss: 0.00035681
Iteration 10/1000 | Loss: 0.00029923
Iteration 11/1000 | Loss: 0.00042207
Iteration 12/1000 | Loss: 0.00037416
Iteration 13/1000 | Loss: 0.00028262
Iteration 14/1000 | Loss: 0.00021000
Iteration 15/1000 | Loss: 0.00039292
Iteration 16/1000 | Loss: 0.00040149
Iteration 17/1000 | Loss: 0.00052085
Iteration 18/1000 | Loss: 0.00029991
Iteration 19/1000 | Loss: 0.00088967
Iteration 20/1000 | Loss: 0.00033083
Iteration 21/1000 | Loss: 0.00070032
Iteration 22/1000 | Loss: 0.00020688
Iteration 23/1000 | Loss: 0.00021929
Iteration 24/1000 | Loss: 0.00062236
Iteration 25/1000 | Loss: 0.00026647
Iteration 26/1000 | Loss: 0.00022549
Iteration 27/1000 | Loss: 0.00073907
Iteration 28/1000 | Loss: 0.00076229
Iteration 29/1000 | Loss: 0.00070141
Iteration 30/1000 | Loss: 0.00127698
Iteration 31/1000 | Loss: 0.00178610
Iteration 32/1000 | Loss: 0.00213630
Iteration 33/1000 | Loss: 0.00041889
Iteration 34/1000 | Loss: 0.00117233
Iteration 35/1000 | Loss: 0.00074252
Iteration 36/1000 | Loss: 0.00096717
Iteration 37/1000 | Loss: 0.00020221
Iteration 38/1000 | Loss: 0.00022271
Iteration 39/1000 | Loss: 0.00160931
Iteration 40/1000 | Loss: 0.00077738
Iteration 41/1000 | Loss: 0.00063729
Iteration 42/1000 | Loss: 0.00017390
Iteration 43/1000 | Loss: 0.00061962
Iteration 44/1000 | Loss: 0.00041143
Iteration 45/1000 | Loss: 0.00125660
Iteration 46/1000 | Loss: 0.00106801
Iteration 47/1000 | Loss: 0.00017031
Iteration 48/1000 | Loss: 0.00017384
Iteration 49/1000 | Loss: 0.00049383
Iteration 50/1000 | Loss: 0.00089869
Iteration 51/1000 | Loss: 0.00081140
Iteration 52/1000 | Loss: 0.00031776
Iteration 53/1000 | Loss: 0.00101042
Iteration 54/1000 | Loss: 0.00043301
Iteration 55/1000 | Loss: 0.00067711
Iteration 56/1000 | Loss: 0.00077787
Iteration 57/1000 | Loss: 0.00068823
Iteration 58/1000 | Loss: 0.00031950
Iteration 59/1000 | Loss: 0.00130413
Iteration 60/1000 | Loss: 0.00068021
Iteration 61/1000 | Loss: 0.00052924
Iteration 62/1000 | Loss: 0.00016632
Iteration 63/1000 | Loss: 0.00128265
Iteration 64/1000 | Loss: 0.00045453
Iteration 65/1000 | Loss: 0.00119824
Iteration 66/1000 | Loss: 0.00035677
Iteration 67/1000 | Loss: 0.00121209
Iteration 68/1000 | Loss: 0.00050348
Iteration 69/1000 | Loss: 0.00060145
Iteration 70/1000 | Loss: 0.00093486
Iteration 71/1000 | Loss: 0.00040558
Iteration 72/1000 | Loss: 0.00045161
Iteration 73/1000 | Loss: 0.00061190
Iteration 74/1000 | Loss: 0.00094053
Iteration 75/1000 | Loss: 0.00114756
Iteration 76/1000 | Loss: 0.00079156
Iteration 77/1000 | Loss: 0.00210602
Iteration 78/1000 | Loss: 0.00091961
Iteration 79/1000 | Loss: 0.00062393
Iteration 80/1000 | Loss: 0.00132416
Iteration 81/1000 | Loss: 0.00072915
Iteration 82/1000 | Loss: 0.00074580
Iteration 83/1000 | Loss: 0.00063336
Iteration 84/1000 | Loss: 0.00025715
Iteration 85/1000 | Loss: 0.00024300
Iteration 86/1000 | Loss: 0.00016192
Iteration 87/1000 | Loss: 0.00105068
Iteration 88/1000 | Loss: 0.00075876
Iteration 89/1000 | Loss: 0.00034213
Iteration 90/1000 | Loss: 0.00014188
Iteration 91/1000 | Loss: 0.00015622
Iteration 92/1000 | Loss: 0.00019302
Iteration 93/1000 | Loss: 0.00016723
Iteration 94/1000 | Loss: 0.00013360
Iteration 95/1000 | Loss: 0.00015500
Iteration 96/1000 | Loss: 0.00015873
Iteration 97/1000 | Loss: 0.00014901
Iteration 98/1000 | Loss: 0.00019188
Iteration 99/1000 | Loss: 0.00124421
Iteration 100/1000 | Loss: 0.00038633
Iteration 101/1000 | Loss: 0.00021980
Iteration 102/1000 | Loss: 0.00036787
Iteration 103/1000 | Loss: 0.00019746
Iteration 104/1000 | Loss: 0.00015408
Iteration 105/1000 | Loss: 0.00012167
Iteration 106/1000 | Loss: 0.00012451
Iteration 107/1000 | Loss: 0.00011703
Iteration 108/1000 | Loss: 0.00011600
Iteration 109/1000 | Loss: 0.00013905
Iteration 110/1000 | Loss: 0.00085143
Iteration 111/1000 | Loss: 0.00028052
Iteration 112/1000 | Loss: 0.00077730
Iteration 113/1000 | Loss: 0.00016605
Iteration 114/1000 | Loss: 0.00032202
Iteration 115/1000 | Loss: 0.00012184
Iteration 116/1000 | Loss: 0.00016287
Iteration 117/1000 | Loss: 0.00009209
Iteration 118/1000 | Loss: 0.00010373
Iteration 119/1000 | Loss: 0.00010812
Iteration 120/1000 | Loss: 0.00015716
Iteration 121/1000 | Loss: 0.00016080
Iteration 122/1000 | Loss: 0.00151262
Iteration 123/1000 | Loss: 0.00051798
Iteration 124/1000 | Loss: 0.00137137
Iteration 125/1000 | Loss: 0.00036082
Iteration 126/1000 | Loss: 0.00016207
Iteration 127/1000 | Loss: 0.00038923
Iteration 128/1000 | Loss: 0.00029089
Iteration 129/1000 | Loss: 0.00013864
Iteration 130/1000 | Loss: 0.00014405
Iteration 131/1000 | Loss: 0.00069572
Iteration 132/1000 | Loss: 0.00067263
Iteration 133/1000 | Loss: 0.00034939
Iteration 134/1000 | Loss: 0.00032768
Iteration 135/1000 | Loss: 0.00020921
Iteration 136/1000 | Loss: 0.00027517
Iteration 137/1000 | Loss: 0.00033603
Iteration 138/1000 | Loss: 0.00031513
Iteration 139/1000 | Loss: 0.00014859
Iteration 140/1000 | Loss: 0.00023054
Iteration 141/1000 | Loss: 0.00026994
Iteration 142/1000 | Loss: 0.00043911
Iteration 143/1000 | Loss: 0.00027319
Iteration 144/1000 | Loss: 0.00021553
Iteration 145/1000 | Loss: 0.00053601
Iteration 146/1000 | Loss: 0.00069051
Iteration 147/1000 | Loss: 0.00028017
Iteration 148/1000 | Loss: 0.00026306
Iteration 149/1000 | Loss: 0.00072299
Iteration 150/1000 | Loss: 0.00040814
Iteration 151/1000 | Loss: 0.00028900
Iteration 152/1000 | Loss: 0.00078729
Iteration 153/1000 | Loss: 0.00095996
Iteration 154/1000 | Loss: 0.00052676
Iteration 155/1000 | Loss: 0.00060193
Iteration 156/1000 | Loss: 0.00162491
Iteration 157/1000 | Loss: 0.00160705
Iteration 158/1000 | Loss: 0.00072465
Iteration 159/1000 | Loss: 0.00170407
Iteration 160/1000 | Loss: 0.00086928
Iteration 161/1000 | Loss: 0.00051796
Iteration 162/1000 | Loss: 0.00063195
Iteration 163/1000 | Loss: 0.00012790
Iteration 164/1000 | Loss: 0.00022822
Iteration 165/1000 | Loss: 0.00029458
Iteration 166/1000 | Loss: 0.00023907
Iteration 167/1000 | Loss: 0.00066720
Iteration 168/1000 | Loss: 0.00042666
Iteration 169/1000 | Loss: 0.00063047
Iteration 170/1000 | Loss: 0.00017669
Iteration 171/1000 | Loss: 0.00025855
Iteration 172/1000 | Loss: 0.00021156
Iteration 173/1000 | Loss: 0.00015076
Iteration 174/1000 | Loss: 0.00013363
Iteration 175/1000 | Loss: 0.00080701
Iteration 176/1000 | Loss: 0.00027290
Iteration 177/1000 | Loss: 0.00038081
Iteration 178/1000 | Loss: 0.00035085
Iteration 179/1000 | Loss: 0.00014828
Iteration 180/1000 | Loss: 0.00051515
Iteration 181/1000 | Loss: 0.00016320
Iteration 182/1000 | Loss: 0.00050320
Iteration 183/1000 | Loss: 0.00037027
Iteration 184/1000 | Loss: 0.00074742
Iteration 185/1000 | Loss: 0.00013507
Iteration 186/1000 | Loss: 0.00020627
Iteration 187/1000 | Loss: 0.00015622
Iteration 188/1000 | Loss: 0.00014624
Iteration 189/1000 | Loss: 0.00021790
Iteration 190/1000 | Loss: 0.00072554
Iteration 191/1000 | Loss: 0.00042770
Iteration 192/1000 | Loss: 0.00020122
Iteration 193/1000 | Loss: 0.00056815
Iteration 194/1000 | Loss: 0.00035472
Iteration 195/1000 | Loss: 0.00012169
Iteration 196/1000 | Loss: 0.00040457
Iteration 197/1000 | Loss: 0.00025143
Iteration 198/1000 | Loss: 0.00014141
Iteration 199/1000 | Loss: 0.00111327
Iteration 200/1000 | Loss: 0.00029106
Iteration 201/1000 | Loss: 0.00025649
Iteration 202/1000 | Loss: 0.00017952
Iteration 203/1000 | Loss: 0.00020371
Iteration 204/1000 | Loss: 0.00032453
Iteration 205/1000 | Loss: 0.00015645
Iteration 206/1000 | Loss: 0.00013967
Iteration 207/1000 | Loss: 0.00014844
Iteration 208/1000 | Loss: 0.00017202
Iteration 209/1000 | Loss: 0.00014417
Iteration 210/1000 | Loss: 0.00019070
Iteration 211/1000 | Loss: 0.00010352
Iteration 212/1000 | Loss: 0.00014403
Iteration 213/1000 | Loss: 0.00013802
Iteration 214/1000 | Loss: 0.00079644
Iteration 215/1000 | Loss: 0.00028017
Iteration 216/1000 | Loss: 0.00014000
Iteration 217/1000 | Loss: 0.00009709
Iteration 218/1000 | Loss: 0.00081997
Iteration 219/1000 | Loss: 0.00029936
Iteration 220/1000 | Loss: 0.00016142
Iteration 221/1000 | Loss: 0.00011730
Iteration 222/1000 | Loss: 0.00014235
Iteration 223/1000 | Loss: 0.00014845
Iteration 224/1000 | Loss: 0.00013906
Iteration 225/1000 | Loss: 0.00012176
Iteration 226/1000 | Loss: 0.00019323
Iteration 227/1000 | Loss: 0.00015073
Iteration 228/1000 | Loss: 0.00012668
Iteration 229/1000 | Loss: 0.00013273
Iteration 230/1000 | Loss: 0.00015364
Iteration 231/1000 | Loss: 0.00015891
Iteration 232/1000 | Loss: 0.00020376
Iteration 233/1000 | Loss: 0.00013223
Iteration 234/1000 | Loss: 0.00012462
Iteration 235/1000 | Loss: 0.00014443
Iteration 236/1000 | Loss: 0.00017031
Iteration 237/1000 | Loss: 0.00016908
Iteration 238/1000 | Loss: 0.00029600
Iteration 239/1000 | Loss: 0.00010398
Iteration 240/1000 | Loss: 0.00011658
Iteration 241/1000 | Loss: 0.00014041
Iteration 242/1000 | Loss: 0.00014512
Iteration 243/1000 | Loss: 0.00014520
Iteration 244/1000 | Loss: 0.00013649
Iteration 245/1000 | Loss: 0.00014576
Iteration 246/1000 | Loss: 0.00014086
Iteration 247/1000 | Loss: 0.00015783
Iteration 248/1000 | Loss: 0.00015037
Iteration 249/1000 | Loss: 0.00015961
Iteration 250/1000 | Loss: 0.00014742
Iteration 251/1000 | Loss: 0.00009627
Iteration 252/1000 | Loss: 0.00038708
Iteration 253/1000 | Loss: 0.00027672
Iteration 254/1000 | Loss: 0.00019855
Iteration 255/1000 | Loss: 0.00010510
Iteration 256/1000 | Loss: 0.00013279
Iteration 257/1000 | Loss: 0.00009621
Iteration 258/1000 | Loss: 0.00009688
Iteration 259/1000 | Loss: 0.00012104
Iteration 260/1000 | Loss: 0.00012461
Iteration 261/1000 | Loss: 0.00013173
Iteration 262/1000 | Loss: 0.00011806
Iteration 263/1000 | Loss: 0.00014061
Iteration 264/1000 | Loss: 0.00012566
Iteration 265/1000 | Loss: 0.00013328
Iteration 266/1000 | Loss: 0.00012736
Iteration 267/1000 | Loss: 0.00013617
Iteration 268/1000 | Loss: 0.00012531
Iteration 269/1000 | Loss: 0.00015596
Iteration 270/1000 | Loss: 0.00012823
Iteration 271/1000 | Loss: 0.00028593
Iteration 272/1000 | Loss: 0.00013697
Iteration 273/1000 | Loss: 0.00012380
Iteration 274/1000 | Loss: 0.00012682
Iteration 275/1000 | Loss: 0.00014278
Iteration 276/1000 | Loss: 0.00013258
Iteration 277/1000 | Loss: 0.00012996
Iteration 278/1000 | Loss: 0.00015538
Iteration 279/1000 | Loss: 0.00013031
Iteration 280/1000 | Loss: 0.00015228
Iteration 281/1000 | Loss: 0.00013126
Iteration 282/1000 | Loss: 0.00012788
Iteration 283/1000 | Loss: 0.00010084
Iteration 284/1000 | Loss: 0.00012192
Iteration 285/1000 | Loss: 0.00012887
Iteration 286/1000 | Loss: 0.00011618
Iteration 287/1000 | Loss: 0.00013142
Iteration 288/1000 | Loss: 0.00011955
Iteration 289/1000 | Loss: 0.00011544
Iteration 290/1000 | Loss: 0.00011702
Iteration 291/1000 | Loss: 0.00018095
Iteration 292/1000 | Loss: 0.00089713
Iteration 293/1000 | Loss: 0.00014701
Iteration 294/1000 | Loss: 0.00007620
Iteration 295/1000 | Loss: 0.00009718
Iteration 296/1000 | Loss: 0.00006373
Iteration 297/1000 | Loss: 0.00006901
Iteration 298/1000 | Loss: 0.00006284
Iteration 299/1000 | Loss: 0.00010261
Iteration 300/1000 | Loss: 0.00036828
Iteration 301/1000 | Loss: 0.00009383
Iteration 302/1000 | Loss: 0.00009376
Iteration 303/1000 | Loss: 0.00009709
Iteration 304/1000 | Loss: 0.00008783
Iteration 305/1000 | Loss: 0.00008267
Iteration 306/1000 | Loss: 0.00010349
Iteration 307/1000 | Loss: 0.00008409
Iteration 308/1000 | Loss: 0.00008052
Iteration 309/1000 | Loss: 0.00006760
Iteration 310/1000 | Loss: 0.00005225
Iteration 311/1000 | Loss: 0.00005608
Iteration 312/1000 | Loss: 0.00005892
Iteration 313/1000 | Loss: 0.00005500
Iteration 314/1000 | Loss: 0.00005513
Iteration 315/1000 | Loss: 0.00005723
Iteration 316/1000 | Loss: 0.00005999
Iteration 317/1000 | Loss: 0.00005824
Iteration 318/1000 | Loss: 0.00005713
Iteration 319/1000 | Loss: 0.00006132
Iteration 320/1000 | Loss: 0.00005777
Iteration 321/1000 | Loss: 0.00005798
Iteration 322/1000 | Loss: 0.00006763
Iteration 323/1000 | Loss: 0.00005992
Iteration 324/1000 | Loss: 0.00006225
Iteration 325/1000 | Loss: 0.00005801
Iteration 326/1000 | Loss: 0.00005595
Iteration 327/1000 | Loss: 0.00005740
Iteration 328/1000 | Loss: 0.00006609
Iteration 329/1000 | Loss: 0.00006314
Iteration 330/1000 | Loss: 0.00007007
Iteration 331/1000 | Loss: 0.00005918
Iteration 332/1000 | Loss: 0.00005425
Iteration 333/1000 | Loss: 0.00004989
Iteration 334/1000 | Loss: 0.00005502
Iteration 335/1000 | Loss: 0.00005828
Iteration 336/1000 | Loss: 0.00005353
Iteration 337/1000 | Loss: 0.00006553
Iteration 338/1000 | Loss: 0.00005514
Iteration 339/1000 | Loss: 0.00005508
Iteration 340/1000 | Loss: 0.00004614
Iteration 341/1000 | Loss: 0.00006665
Iteration 342/1000 | Loss: 0.00005754
Iteration 343/1000 | Loss: 0.00007183
Iteration 344/1000 | Loss: 0.00008646
Iteration 345/1000 | Loss: 0.00006476
Iteration 346/1000 | Loss: 0.00006209
Iteration 347/1000 | Loss: 0.00008913
Iteration 348/1000 | Loss: 0.00005712
Iteration 349/1000 | Loss: 0.00006367
Iteration 350/1000 | Loss: 0.00006104
Iteration 351/1000 | Loss: 0.00010317
Iteration 352/1000 | Loss: 0.00006943
Iteration 353/1000 | Loss: 0.00010686
Iteration 354/1000 | Loss: 0.00007077
Iteration 355/1000 | Loss: 0.00006254
Iteration 356/1000 | Loss: 0.00005723
Iteration 357/1000 | Loss: 0.00005753
Iteration 358/1000 | Loss: 0.00005732
Iteration 359/1000 | Loss: 0.00006534
Iteration 360/1000 | Loss: 0.00005781
Iteration 361/1000 | Loss: 0.00005839
Iteration 362/1000 | Loss: 0.00005566
Iteration 363/1000 | Loss: 0.00005373
Iteration 364/1000 | Loss: 0.00006134
Iteration 365/1000 | Loss: 0.00005964
Iteration 366/1000 | Loss: 0.00005836
Iteration 367/1000 | Loss: 0.00005845
Iteration 368/1000 | Loss: 0.00005585
Iteration 369/1000 | Loss: 0.00005550
Iteration 370/1000 | Loss: 0.00005685
Iteration 371/1000 | Loss: 0.00005269
Iteration 372/1000 | Loss: 0.00005443
Iteration 373/1000 | Loss: 0.00006559
Iteration 374/1000 | Loss: 0.00019106
Iteration 375/1000 | Loss: 0.00007423
Iteration 376/1000 | Loss: 0.00004843
Iteration 377/1000 | Loss: 0.00006315
Iteration 378/1000 | Loss: 0.00006483
Iteration 379/1000 | Loss: 0.00006258
Iteration 380/1000 | Loss: 0.00007136
Iteration 381/1000 | Loss: 0.00006614
Iteration 382/1000 | Loss: 0.00007945
Iteration 383/1000 | Loss: 0.00007658
Iteration 384/1000 | Loss: 0.00009180
Iteration 385/1000 | Loss: 0.00005391
Iteration 386/1000 | Loss: 0.00005991
Iteration 387/1000 | Loss: 0.00005603
Iteration 388/1000 | Loss: 0.00006683
Iteration 389/1000 | Loss: 0.00006378
Iteration 390/1000 | Loss: 0.00006884
Iteration 391/1000 | Loss: 0.00006540
Iteration 392/1000 | Loss: 0.00006371
Iteration 393/1000 | Loss: 0.00008048
Iteration 394/1000 | Loss: 0.00005638
Iteration 395/1000 | Loss: 0.00005828
Iteration 396/1000 | Loss: 0.00006561
Iteration 397/1000 | Loss: 0.00006922
Iteration 398/1000 | Loss: 0.00005875
Iteration 399/1000 | Loss: 0.00005927
Iteration 400/1000 | Loss: 0.00007029
Iteration 401/1000 | Loss: 0.00006972
Iteration 402/1000 | Loss: 0.00006485
Iteration 403/1000 | Loss: 0.00005220
Iteration 404/1000 | Loss: 0.00006619
Iteration 405/1000 | Loss: 0.00005484
Iteration 406/1000 | Loss: 0.00005529
Iteration 407/1000 | Loss: 0.00004352
Iteration 408/1000 | Loss: 0.00006929
Iteration 409/1000 | Loss: 0.00005421
Iteration 410/1000 | Loss: 0.00005128
Iteration 411/1000 | Loss: 0.00006428
Iteration 412/1000 | Loss: 0.00005629
Iteration 413/1000 | Loss: 0.00005186
Iteration 414/1000 | Loss: 0.00005569
Iteration 415/1000 | Loss: 0.00005385
Iteration 416/1000 | Loss: 0.00004199
Iteration 417/1000 | Loss: 0.00005710
Iteration 418/1000 | Loss: 0.00005843
Iteration 419/1000 | Loss: 0.00006155
Iteration 420/1000 | Loss: 0.00004640
Iteration 421/1000 | Loss: 0.00004203
Iteration 422/1000 | Loss: 0.00004028
Iteration 423/1000 | Loss: 0.00004543
Iteration 424/1000 | Loss: 0.00003894
Iteration 425/1000 | Loss: 0.00004843
Iteration 426/1000 | Loss: 0.00003835
Iteration 427/1000 | Loss: 0.00003833
Iteration 428/1000 | Loss: 0.00003815
Iteration 429/1000 | Loss: 0.00004009
Iteration 430/1000 | Loss: 0.00004009
Iteration 431/1000 | Loss: 0.00003820
Iteration 432/1000 | Loss: 0.00003951
Iteration 433/1000 | Loss: 0.00003795
Iteration 434/1000 | Loss: 0.00003793
Iteration 435/1000 | Loss: 0.00003793
Iteration 436/1000 | Loss: 0.00003793
Iteration 437/1000 | Loss: 0.00003793
Iteration 438/1000 | Loss: 0.00003793
Iteration 439/1000 | Loss: 0.00003793
Iteration 440/1000 | Loss: 0.00003793
Iteration 441/1000 | Loss: 0.00003793
Iteration 442/1000 | Loss: 0.00003792
Iteration 443/1000 | Loss: 0.00003792
Iteration 444/1000 | Loss: 0.00003805
Iteration 445/1000 | Loss: 0.00003790
Iteration 446/1000 | Loss: 0.00003789
Iteration 447/1000 | Loss: 0.00003789
Iteration 448/1000 | Loss: 0.00003789
Iteration 449/1000 | Loss: 0.00003788
Iteration 450/1000 | Loss: 0.00003788
Iteration 451/1000 | Loss: 0.00003787
Iteration 452/1000 | Loss: 0.00003787
Iteration 453/1000 | Loss: 0.00003787
Iteration 454/1000 | Loss: 0.00003786
Iteration 455/1000 | Loss: 0.00003786
Iteration 456/1000 | Loss: 0.00003786
Iteration 457/1000 | Loss: 0.00003786
Iteration 458/1000 | Loss: 0.00003885
Iteration 459/1000 | Loss: 0.00003780
Iteration 460/1000 | Loss: 0.00003780
Iteration 461/1000 | Loss: 0.00003780
Iteration 462/1000 | Loss: 0.00003779
Iteration 463/1000 | Loss: 0.00004102
Iteration 464/1000 | Loss: 0.00003779
Iteration 465/1000 | Loss: 0.00003778
Iteration 466/1000 | Loss: 0.00003778
Iteration 467/1000 | Loss: 0.00003777
Iteration 468/1000 | Loss: 0.00003777
Iteration 469/1000 | Loss: 0.00003777
Iteration 470/1000 | Loss: 0.00003777
Iteration 471/1000 | Loss: 0.00003777
Iteration 472/1000 | Loss: 0.00003776
Iteration 473/1000 | Loss: 0.00003807
Iteration 474/1000 | Loss: 0.00003775
Iteration 475/1000 | Loss: 0.00003775
Iteration 476/1000 | Loss: 0.00003775
Iteration 477/1000 | Loss: 0.00003775
Iteration 478/1000 | Loss: 0.00003774
Iteration 479/1000 | Loss: 0.00003774
Iteration 480/1000 | Loss: 0.00003774
Iteration 481/1000 | Loss: 0.00003774
Iteration 482/1000 | Loss: 0.00003774
Iteration 483/1000 | Loss: 0.00003774
Iteration 484/1000 | Loss: 0.00003774
Iteration 485/1000 | Loss: 0.00003774
Iteration 486/1000 | Loss: 0.00003774
Iteration 487/1000 | Loss: 0.00003774
Iteration 488/1000 | Loss: 0.00003774
Iteration 489/1000 | Loss: 0.00003774
Iteration 490/1000 | Loss: 0.00003774
Iteration 491/1000 | Loss: 0.00003774
Iteration 492/1000 | Loss: 0.00003774
Iteration 493/1000 | Loss: 0.00003773
Iteration 494/1000 | Loss: 0.00003773
Iteration 495/1000 | Loss: 0.00003773
Iteration 496/1000 | Loss: 0.00003773
Iteration 497/1000 | Loss: 0.00003773
Iteration 498/1000 | Loss: 0.00003773
Iteration 499/1000 | Loss: 0.00003789
Iteration 500/1000 | Loss: 0.00003772
Iteration 501/1000 | Loss: 0.00003772
Iteration 502/1000 | Loss: 0.00003772
Iteration 503/1000 | Loss: 0.00003772
Iteration 504/1000 | Loss: 0.00003772
Iteration 505/1000 | Loss: 0.00003772
Iteration 506/1000 | Loss: 0.00003772
Iteration 507/1000 | Loss: 0.00003772
Iteration 508/1000 | Loss: 0.00003772
Iteration 509/1000 | Loss: 0.00003772
Iteration 510/1000 | Loss: 0.00003772
Iteration 511/1000 | Loss: 0.00003772
Iteration 512/1000 | Loss: 0.00003772
Iteration 513/1000 | Loss: 0.00003772
Iteration 514/1000 | Loss: 0.00003772
Iteration 515/1000 | Loss: 0.00003772
Iteration 516/1000 | Loss: 0.00003772
Iteration 517/1000 | Loss: 0.00003772
Iteration 518/1000 | Loss: 0.00003772
Iteration 519/1000 | Loss: 0.00003772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 519. Stopping optimization.
Last 5 losses: [3.7716956285294145e-05, 3.7716956285294145e-05, 3.7716956285294145e-05, 3.7716956285294145e-05, 3.7716956285294145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7716956285294145e-05

Optimization complete. Final v2v error: 3.893562078475952 mm

Highest mean error: 12.989047050476074 mm for frame 158

Lowest mean error: 2.9703781604766846 mm for frame 116

Saving results

Total time: 2042.6218135356903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068163
Iteration 2/25 | Loss: 0.00161983
Iteration 3/25 | Loss: 0.00115705
Iteration 4/25 | Loss: 0.00110387
Iteration 5/25 | Loss: 0.00104851
Iteration 6/25 | Loss: 0.00104267
Iteration 7/25 | Loss: 0.00098014
Iteration 8/25 | Loss: 0.00098397
Iteration 9/25 | Loss: 0.00094850
Iteration 10/25 | Loss: 0.00094043
Iteration 11/25 | Loss: 0.00093425
Iteration 12/25 | Loss: 0.00094216
Iteration 13/25 | Loss: 0.00092915
Iteration 14/25 | Loss: 0.00091425
Iteration 15/25 | Loss: 0.00091112
Iteration 16/25 | Loss: 0.00090675
Iteration 17/25 | Loss: 0.00089827
Iteration 18/25 | Loss: 0.00090181
Iteration 19/25 | Loss: 0.00089953
Iteration 20/25 | Loss: 0.00090150
Iteration 21/25 | Loss: 0.00089476
Iteration 22/25 | Loss: 0.00089708
Iteration 23/25 | Loss: 0.00089321
Iteration 24/25 | Loss: 0.00089361
Iteration 25/25 | Loss: 0.00089073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63693559
Iteration 2/25 | Loss: 0.00125627
Iteration 3/25 | Loss: 0.00112412
Iteration 4/25 | Loss: 0.00112412
Iteration 5/25 | Loss: 0.00112412
Iteration 6/25 | Loss: 0.00112412
Iteration 7/25 | Loss: 0.00112412
Iteration 8/25 | Loss: 0.00112412
Iteration 9/25 | Loss: 0.00112412
Iteration 10/25 | Loss: 0.00112412
Iteration 11/25 | Loss: 0.00112412
Iteration 12/25 | Loss: 0.00112412
Iteration 13/25 | Loss: 0.00112412
Iteration 14/25 | Loss: 0.00112412
Iteration 15/25 | Loss: 0.00112412
Iteration 16/25 | Loss: 0.00112412
Iteration 17/25 | Loss: 0.00112412
Iteration 18/25 | Loss: 0.00112412
Iteration 19/25 | Loss: 0.00112412
Iteration 20/25 | Loss: 0.00112412
Iteration 21/25 | Loss: 0.00112412
Iteration 22/25 | Loss: 0.00112412
Iteration 23/25 | Loss: 0.00112412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011241198517382145, 0.0011241198517382145, 0.0011241198517382145, 0.0011241198517382145, 0.0011241198517382145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011241198517382145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112412
Iteration 2/1000 | Loss: 0.00015599
Iteration 3/1000 | Loss: 0.00009892
Iteration 4/1000 | Loss: 0.00002577
Iteration 5/1000 | Loss: 0.00007127
Iteration 6/1000 | Loss: 0.00027677
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00003404
Iteration 9/1000 | Loss: 0.00002021
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00005001
Iteration 12/1000 | Loss: 0.00088360
Iteration 13/1000 | Loss: 0.00004125
Iteration 14/1000 | Loss: 0.00076646
Iteration 15/1000 | Loss: 0.00010952
Iteration 16/1000 | Loss: 0.00002156
Iteration 17/1000 | Loss: 0.00001908
Iteration 18/1000 | Loss: 0.00042636
Iteration 19/1000 | Loss: 0.00106532
Iteration 20/1000 | Loss: 0.00098709
Iteration 21/1000 | Loss: 0.00004530
Iteration 22/1000 | Loss: 0.00002395
Iteration 23/1000 | Loss: 0.00093641
Iteration 24/1000 | Loss: 0.00006345
Iteration 25/1000 | Loss: 0.00006856
Iteration 26/1000 | Loss: 0.00109932
Iteration 27/1000 | Loss: 0.00039104
Iteration 28/1000 | Loss: 0.00007539
Iteration 29/1000 | Loss: 0.00004850
Iteration 30/1000 | Loss: 0.00002131
Iteration 31/1000 | Loss: 0.00001975
Iteration 32/1000 | Loss: 0.00001794
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00008251
Iteration 35/1000 | Loss: 0.00007935
Iteration 36/1000 | Loss: 0.00001984
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001619
Iteration 39/1000 | Loss: 0.00003121
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00002427
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001554
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001548
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001541
Iteration 93/1000 | Loss: 0.00001541
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001540
Iteration 96/1000 | Loss: 0.00001540
Iteration 97/1000 | Loss: 0.00001540
Iteration 98/1000 | Loss: 0.00001540
Iteration 99/1000 | Loss: 0.00001540
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001539
Iteration 104/1000 | Loss: 0.00001539
Iteration 105/1000 | Loss: 0.00001539
Iteration 106/1000 | Loss: 0.00001539
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001537
Iteration 117/1000 | Loss: 0.00001537
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001537
Iteration 120/1000 | Loss: 0.00001537
Iteration 121/1000 | Loss: 0.00001537
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001536
Iteration 129/1000 | Loss: 0.00001536
Iteration 130/1000 | Loss: 0.00001536
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001536
Iteration 133/1000 | Loss: 0.00001536
Iteration 134/1000 | Loss: 0.00001535
Iteration 135/1000 | Loss: 0.00001535
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001535
Iteration 140/1000 | Loss: 0.00001535
Iteration 141/1000 | Loss: 0.00001535
Iteration 142/1000 | Loss: 0.00001535
Iteration 143/1000 | Loss: 0.00001535
Iteration 144/1000 | Loss: 0.00001535
Iteration 145/1000 | Loss: 0.00001535
Iteration 146/1000 | Loss: 0.00001535
Iteration 147/1000 | Loss: 0.00001535
Iteration 148/1000 | Loss: 0.00001535
Iteration 149/1000 | Loss: 0.00001535
Iteration 150/1000 | Loss: 0.00001535
Iteration 151/1000 | Loss: 0.00001535
Iteration 152/1000 | Loss: 0.00001535
Iteration 153/1000 | Loss: 0.00001535
Iteration 154/1000 | Loss: 0.00001535
Iteration 155/1000 | Loss: 0.00001535
Iteration 156/1000 | Loss: 0.00001535
Iteration 157/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5353347407653928e-05, 1.5353347407653928e-05, 1.5353347407653928e-05, 1.5353347407653928e-05, 1.5353347407653928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5353347407653928e-05

Optimization complete. Final v2v error: 3.393596649169922 mm

Highest mean error: 4.37864351272583 mm for frame 49

Lowest mean error: 2.9812569618225098 mm for frame 17

Saving results

Total time: 273.50074315071106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801138
Iteration 2/25 | Loss: 0.00135394
Iteration 3/25 | Loss: 0.00099312
Iteration 4/25 | Loss: 0.00095091
Iteration 5/25 | Loss: 0.00093902
Iteration 6/25 | Loss: 0.00093619
Iteration 7/25 | Loss: 0.00093581
Iteration 8/25 | Loss: 0.00093559
Iteration 9/25 | Loss: 0.00093559
Iteration 10/25 | Loss: 0.00093559
Iteration 11/25 | Loss: 0.00093559
Iteration 12/25 | Loss: 0.00093559
Iteration 13/25 | Loss: 0.00093559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000935590360313654, 0.000935590360313654, 0.000935590360313654, 0.000935590360313654, 0.000935590360313654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000935590360313654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70880461
Iteration 2/25 | Loss: 0.00101344
Iteration 3/25 | Loss: 0.00101342
Iteration 4/25 | Loss: 0.00101342
Iteration 5/25 | Loss: 0.00101342
Iteration 6/25 | Loss: 0.00101342
Iteration 7/25 | Loss: 0.00101342
Iteration 8/25 | Loss: 0.00101342
Iteration 9/25 | Loss: 0.00101342
Iteration 10/25 | Loss: 0.00101342
Iteration 11/25 | Loss: 0.00101342
Iteration 12/25 | Loss: 0.00101342
Iteration 13/25 | Loss: 0.00101342
Iteration 14/25 | Loss: 0.00101342
Iteration 15/25 | Loss: 0.00101342
Iteration 16/25 | Loss: 0.00101342
Iteration 17/25 | Loss: 0.00101342
Iteration 18/25 | Loss: 0.00101342
Iteration 19/25 | Loss: 0.00101342
Iteration 20/25 | Loss: 0.00101342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010134204057976604, 0.0010134204057976604, 0.0010134204057976604, 0.0010134204057976604, 0.0010134204057976604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010134204057976604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101342
Iteration 2/1000 | Loss: 0.00004569
Iteration 3/1000 | Loss: 0.00003159
Iteration 4/1000 | Loss: 0.00002931
Iteration 5/1000 | Loss: 0.00002805
Iteration 6/1000 | Loss: 0.00002728
Iteration 7/1000 | Loss: 0.00002676
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002619
Iteration 10/1000 | Loss: 0.00002603
Iteration 11/1000 | Loss: 0.00002595
Iteration 12/1000 | Loss: 0.00002593
Iteration 13/1000 | Loss: 0.00002589
Iteration 14/1000 | Loss: 0.00002589
Iteration 15/1000 | Loss: 0.00002585
Iteration 16/1000 | Loss: 0.00002585
Iteration 17/1000 | Loss: 0.00002583
Iteration 18/1000 | Loss: 0.00002582
Iteration 19/1000 | Loss: 0.00002580
Iteration 20/1000 | Loss: 0.00002577
Iteration 21/1000 | Loss: 0.00002577
Iteration 22/1000 | Loss: 0.00002574
Iteration 23/1000 | Loss: 0.00002574
Iteration 24/1000 | Loss: 0.00002571
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002571
Iteration 27/1000 | Loss: 0.00002571
Iteration 28/1000 | Loss: 0.00002570
Iteration 29/1000 | Loss: 0.00002570
Iteration 30/1000 | Loss: 0.00002570
Iteration 31/1000 | Loss: 0.00002569
Iteration 32/1000 | Loss: 0.00002569
Iteration 33/1000 | Loss: 0.00002569
Iteration 34/1000 | Loss: 0.00002569
Iteration 35/1000 | Loss: 0.00002568
Iteration 36/1000 | Loss: 0.00002568
Iteration 37/1000 | Loss: 0.00002568
Iteration 38/1000 | Loss: 0.00002567
Iteration 39/1000 | Loss: 0.00002567
Iteration 40/1000 | Loss: 0.00002567
Iteration 41/1000 | Loss: 0.00002567
Iteration 42/1000 | Loss: 0.00002567
Iteration 43/1000 | Loss: 0.00002566
Iteration 44/1000 | Loss: 0.00002566
Iteration 45/1000 | Loss: 0.00002566
Iteration 46/1000 | Loss: 0.00002566
Iteration 47/1000 | Loss: 0.00002565
Iteration 48/1000 | Loss: 0.00002565
Iteration 49/1000 | Loss: 0.00002565
Iteration 50/1000 | Loss: 0.00002565
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002565
Iteration 53/1000 | Loss: 0.00002565
Iteration 54/1000 | Loss: 0.00002564
Iteration 55/1000 | Loss: 0.00002564
Iteration 56/1000 | Loss: 0.00002564
Iteration 57/1000 | Loss: 0.00002564
Iteration 58/1000 | Loss: 0.00002563
Iteration 59/1000 | Loss: 0.00002563
Iteration 60/1000 | Loss: 0.00002563
Iteration 61/1000 | Loss: 0.00002563
Iteration 62/1000 | Loss: 0.00002562
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002562
Iteration 65/1000 | Loss: 0.00002562
Iteration 66/1000 | Loss: 0.00002562
Iteration 67/1000 | Loss: 0.00002562
Iteration 68/1000 | Loss: 0.00002562
Iteration 69/1000 | Loss: 0.00002562
Iteration 70/1000 | Loss: 0.00002562
Iteration 71/1000 | Loss: 0.00002562
Iteration 72/1000 | Loss: 0.00002562
Iteration 73/1000 | Loss: 0.00002561
Iteration 74/1000 | Loss: 0.00002561
Iteration 75/1000 | Loss: 0.00002561
Iteration 76/1000 | Loss: 0.00002561
Iteration 77/1000 | Loss: 0.00002561
Iteration 78/1000 | Loss: 0.00002561
Iteration 79/1000 | Loss: 0.00002561
Iteration 80/1000 | Loss: 0.00002561
Iteration 81/1000 | Loss: 0.00002561
Iteration 82/1000 | Loss: 0.00002561
Iteration 83/1000 | Loss: 0.00002561
Iteration 84/1000 | Loss: 0.00002561
Iteration 85/1000 | Loss: 0.00002561
Iteration 86/1000 | Loss: 0.00002561
Iteration 87/1000 | Loss: 0.00002561
Iteration 88/1000 | Loss: 0.00002561
Iteration 89/1000 | Loss: 0.00002561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.5610292141209356e-05, 2.5610292141209356e-05, 2.5610292141209356e-05, 2.5610292141209356e-05, 2.5610292141209356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5610292141209356e-05

Optimization complete. Final v2v error: 4.3691630363464355 mm

Highest mean error: 4.9958648681640625 mm for frame 187

Lowest mean error: 3.593926429748535 mm for frame 17

Saving results

Total time: 100.6067476272583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476965
Iteration 2/25 | Loss: 0.00112095
Iteration 3/25 | Loss: 0.00091689
Iteration 4/25 | Loss: 0.00087859
Iteration 5/25 | Loss: 0.00087127
Iteration 6/25 | Loss: 0.00086948
Iteration 7/25 | Loss: 0.00086907
Iteration 8/25 | Loss: 0.00086904
Iteration 9/25 | Loss: 0.00086904
Iteration 10/25 | Loss: 0.00086904
Iteration 11/25 | Loss: 0.00086904
Iteration 12/25 | Loss: 0.00086904
Iteration 13/25 | Loss: 0.00086904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008690449176356196, 0.0008690449176356196, 0.0008690449176356196, 0.0008690449176356196, 0.0008690449176356196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008690449176356196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67598164
Iteration 2/25 | Loss: 0.00108498
Iteration 3/25 | Loss: 0.00108496
Iteration 4/25 | Loss: 0.00108496
Iteration 5/25 | Loss: 0.00108496
Iteration 6/25 | Loss: 0.00108496
Iteration 7/25 | Loss: 0.00108496
Iteration 8/25 | Loss: 0.00108496
Iteration 9/25 | Loss: 0.00108496
Iteration 10/25 | Loss: 0.00108496
Iteration 11/25 | Loss: 0.00108495
Iteration 12/25 | Loss: 0.00108495
Iteration 13/25 | Loss: 0.00108495
Iteration 14/25 | Loss: 0.00108495
Iteration 15/25 | Loss: 0.00108495
Iteration 16/25 | Loss: 0.00108495
Iteration 17/25 | Loss: 0.00108495
Iteration 18/25 | Loss: 0.00108495
Iteration 19/25 | Loss: 0.00108495
Iteration 20/25 | Loss: 0.00108495
Iteration 21/25 | Loss: 0.00108496
Iteration 22/25 | Loss: 0.00108496
Iteration 23/25 | Loss: 0.00108496
Iteration 24/25 | Loss: 0.00108496
Iteration 25/25 | Loss: 0.00108496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108496
Iteration 2/1000 | Loss: 0.00003321
Iteration 3/1000 | Loss: 0.00002293
Iteration 4/1000 | Loss: 0.00002088
Iteration 5/1000 | Loss: 0.00001956
Iteration 6/1000 | Loss: 0.00001917
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001833
Iteration 11/1000 | Loss: 0.00001832
Iteration 12/1000 | Loss: 0.00001831
Iteration 13/1000 | Loss: 0.00001831
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001826
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001823
Iteration 20/1000 | Loss: 0.00001823
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001820
Iteration 28/1000 | Loss: 0.00001820
Iteration 29/1000 | Loss: 0.00001820
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001820
Iteration 32/1000 | Loss: 0.00001820
Iteration 33/1000 | Loss: 0.00001819
Iteration 34/1000 | Loss: 0.00001819
Iteration 35/1000 | Loss: 0.00001819
Iteration 36/1000 | Loss: 0.00001819
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001818
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001818
Iteration 47/1000 | Loss: 0.00001817
Iteration 48/1000 | Loss: 0.00001817
Iteration 49/1000 | Loss: 0.00001817
Iteration 50/1000 | Loss: 0.00001817
Iteration 51/1000 | Loss: 0.00001817
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00001816
Iteration 54/1000 | Loss: 0.00001816
Iteration 55/1000 | Loss: 0.00001816
Iteration 56/1000 | Loss: 0.00001816
Iteration 57/1000 | Loss: 0.00001816
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00001816
Iteration 65/1000 | Loss: 0.00001816
Iteration 66/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.8155220459448174e-05, 1.8155220459448174e-05, 1.8155220459448174e-05, 1.8155220459448174e-05, 1.8155220459448174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8155220459448174e-05

Optimization complete. Final v2v error: 3.6246297359466553 mm

Highest mean error: 4.2537007331848145 mm for frame 148

Lowest mean error: 3.225599765777588 mm for frame 5

Saving results

Total time: 71.4871916770935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914473
Iteration 2/25 | Loss: 0.00137803
Iteration 3/25 | Loss: 0.00102442
Iteration 4/25 | Loss: 0.00095585
Iteration 5/25 | Loss: 0.00094257
Iteration 6/25 | Loss: 0.00093845
Iteration 7/25 | Loss: 0.00093779
Iteration 8/25 | Loss: 0.00093779
Iteration 9/25 | Loss: 0.00093779
Iteration 10/25 | Loss: 0.00093779
Iteration 11/25 | Loss: 0.00093779
Iteration 12/25 | Loss: 0.00093779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009377859532833099, 0.0009377859532833099, 0.0009377859532833099, 0.0009377859532833099, 0.0009377859532833099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009377859532833099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53482258
Iteration 2/25 | Loss: 0.00098910
Iteration 3/25 | Loss: 0.00098909
Iteration 4/25 | Loss: 0.00098909
Iteration 5/25 | Loss: 0.00098909
Iteration 6/25 | Loss: 0.00098909
Iteration 7/25 | Loss: 0.00098909
Iteration 8/25 | Loss: 0.00098909
Iteration 9/25 | Loss: 0.00098909
Iteration 10/25 | Loss: 0.00098909
Iteration 11/25 | Loss: 0.00098909
Iteration 12/25 | Loss: 0.00098909
Iteration 13/25 | Loss: 0.00098909
Iteration 14/25 | Loss: 0.00098909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009890854125842452, 0.0009890854125842452, 0.0009890854125842452, 0.0009890854125842452, 0.0009890854125842452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009890854125842452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098909
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00002133
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001916
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001856
Iteration 9/1000 | Loss: 0.00001849
Iteration 10/1000 | Loss: 0.00001845
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001836
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001831
Iteration 18/1000 | Loss: 0.00001831
Iteration 19/1000 | Loss: 0.00001830
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001828
Iteration 25/1000 | Loss: 0.00001827
Iteration 26/1000 | Loss: 0.00001827
Iteration 27/1000 | Loss: 0.00001827
Iteration 28/1000 | Loss: 0.00001827
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001826
Iteration 32/1000 | Loss: 0.00001826
Iteration 33/1000 | Loss: 0.00001825
Iteration 34/1000 | Loss: 0.00001825
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001825
Iteration 38/1000 | Loss: 0.00001825
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001825
Iteration 42/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 42. Stopping optimization.
Last 5 losses: [1.825309664127417e-05, 1.825309664127417e-05, 1.825309664127417e-05, 1.825309664127417e-05, 1.825309664127417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.825309664127417e-05

Optimization complete. Final v2v error: 3.6776130199432373 mm

Highest mean error: 3.9755241870880127 mm for frame 230

Lowest mean error: 3.442814588546753 mm for frame 217

Saving results

Total time: 84.8550968170166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410745
Iteration 2/25 | Loss: 0.00113055
Iteration 3/25 | Loss: 0.00094592
Iteration 4/25 | Loss: 0.00091734
Iteration 5/25 | Loss: 0.00090937
Iteration 6/25 | Loss: 0.00090614
Iteration 7/25 | Loss: 0.00090500
Iteration 8/25 | Loss: 0.00090480
Iteration 9/25 | Loss: 0.00090480
Iteration 10/25 | Loss: 0.00090480
Iteration 11/25 | Loss: 0.00090480
Iteration 12/25 | Loss: 0.00090480
Iteration 13/25 | Loss: 0.00090480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009047954808920622, 0.0009047954808920622, 0.0009047954808920622, 0.0009047954808920622, 0.0009047954808920622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009047954808920622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41508055
Iteration 2/25 | Loss: 0.00097876
Iteration 3/25 | Loss: 0.00097874
Iteration 4/25 | Loss: 0.00097874
Iteration 5/25 | Loss: 0.00097874
Iteration 6/25 | Loss: 0.00097874
Iteration 7/25 | Loss: 0.00097874
Iteration 8/25 | Loss: 0.00097874
Iteration 9/25 | Loss: 0.00097874
Iteration 10/25 | Loss: 0.00097874
Iteration 11/25 | Loss: 0.00097874
Iteration 12/25 | Loss: 0.00097874
Iteration 13/25 | Loss: 0.00097874
Iteration 14/25 | Loss: 0.00097874
Iteration 15/25 | Loss: 0.00097874
Iteration 16/25 | Loss: 0.00097874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000978737836703658, 0.000978737836703658, 0.000978737836703658, 0.000978737836703658, 0.000978737836703658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978737836703658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097874
Iteration 2/1000 | Loss: 0.00005958
Iteration 3/1000 | Loss: 0.00003331
Iteration 4/1000 | Loss: 0.00002571
Iteration 5/1000 | Loss: 0.00002310
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002046
Iteration 10/1000 | Loss: 0.00002019
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001976
Iteration 15/1000 | Loss: 0.00001976
Iteration 16/1000 | Loss: 0.00001975
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001958
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001944
Iteration 39/1000 | Loss: 0.00001943
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001940
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001939
Iteration 48/1000 | Loss: 0.00001939
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001931
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001931
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001931
Iteration 81/1000 | Loss: 0.00001931
Iteration 82/1000 | Loss: 0.00001931
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00001931
Iteration 86/1000 | Loss: 0.00001931
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00001931
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.9309110939502716e-05, 1.9309110939502716e-05, 1.9309110939502716e-05, 1.9309110939502716e-05, 1.9309110939502716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9309110939502716e-05

Optimization complete. Final v2v error: 3.661813735961914 mm

Highest mean error: 5.866539001464844 mm for frame 85

Lowest mean error: 3.0781662464141846 mm for frame 0

Saving results

Total time: 96.52898454666138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439516
Iteration 2/25 | Loss: 0.00102119
Iteration 3/25 | Loss: 0.00094101
Iteration 4/25 | Loss: 0.00092194
Iteration 5/25 | Loss: 0.00091626
Iteration 6/25 | Loss: 0.00091542
Iteration 7/25 | Loss: 0.00091542
Iteration 8/25 | Loss: 0.00091542
Iteration 9/25 | Loss: 0.00091542
Iteration 10/25 | Loss: 0.00091542
Iteration 11/25 | Loss: 0.00091542
Iteration 12/25 | Loss: 0.00091542
Iteration 13/25 | Loss: 0.00091542
Iteration 14/25 | Loss: 0.00091542
Iteration 15/25 | Loss: 0.00091542
Iteration 16/25 | Loss: 0.00091542
Iteration 17/25 | Loss: 0.00091542
Iteration 18/25 | Loss: 0.00091542
Iteration 19/25 | Loss: 0.00091542
Iteration 20/25 | Loss: 0.00091542
Iteration 21/25 | Loss: 0.00091542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009154155850410461, 0.0009154155850410461, 0.0009154155850410461, 0.0009154155850410461, 0.0009154155850410461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009154155850410461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61729646
Iteration 2/25 | Loss: 0.00104642
Iteration 3/25 | Loss: 0.00104642
Iteration 4/25 | Loss: 0.00104642
Iteration 5/25 | Loss: 0.00104642
Iteration 6/25 | Loss: 0.00104642
Iteration 7/25 | Loss: 0.00104642
Iteration 8/25 | Loss: 0.00104642
Iteration 9/25 | Loss: 0.00104642
Iteration 10/25 | Loss: 0.00104642
Iteration 11/25 | Loss: 0.00104642
Iteration 12/25 | Loss: 0.00104642
Iteration 13/25 | Loss: 0.00104642
Iteration 14/25 | Loss: 0.00104642
Iteration 15/25 | Loss: 0.00104642
Iteration 16/25 | Loss: 0.00104642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010464162332937121, 0.0010464162332937121, 0.0010464162332937121, 0.0010464162332937121, 0.0010464162332937121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010464162332937121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104642
Iteration 2/1000 | Loss: 0.00003759
Iteration 3/1000 | Loss: 0.00003032
Iteration 4/1000 | Loss: 0.00002804
Iteration 5/1000 | Loss: 0.00002600
Iteration 6/1000 | Loss: 0.00002484
Iteration 7/1000 | Loss: 0.00002422
Iteration 8/1000 | Loss: 0.00002400
Iteration 9/1000 | Loss: 0.00002388
Iteration 10/1000 | Loss: 0.00002382
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002381
Iteration 13/1000 | Loss: 0.00002376
Iteration 14/1000 | Loss: 0.00002375
Iteration 15/1000 | Loss: 0.00002373
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002373
Iteration 18/1000 | Loss: 0.00002373
Iteration 19/1000 | Loss: 0.00002372
Iteration 20/1000 | Loss: 0.00002371
Iteration 21/1000 | Loss: 0.00002368
Iteration 22/1000 | Loss: 0.00002368
Iteration 23/1000 | Loss: 0.00002368
Iteration 24/1000 | Loss: 0.00002368
Iteration 25/1000 | Loss: 0.00002368
Iteration 26/1000 | Loss: 0.00002368
Iteration 27/1000 | Loss: 0.00002368
Iteration 28/1000 | Loss: 0.00002367
Iteration 29/1000 | Loss: 0.00002366
Iteration 30/1000 | Loss: 0.00002365
Iteration 31/1000 | Loss: 0.00002365
Iteration 32/1000 | Loss: 0.00002364
Iteration 33/1000 | Loss: 0.00002364
Iteration 34/1000 | Loss: 0.00002364
Iteration 35/1000 | Loss: 0.00002363
Iteration 36/1000 | Loss: 0.00002363
Iteration 37/1000 | Loss: 0.00002363
Iteration 38/1000 | Loss: 0.00002363
Iteration 39/1000 | Loss: 0.00002362
Iteration 40/1000 | Loss: 0.00002361
Iteration 41/1000 | Loss: 0.00002361
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002360
Iteration 44/1000 | Loss: 0.00002360
Iteration 45/1000 | Loss: 0.00002360
Iteration 46/1000 | Loss: 0.00002359
Iteration 47/1000 | Loss: 0.00002359
Iteration 48/1000 | Loss: 0.00002358
Iteration 49/1000 | Loss: 0.00002358
Iteration 50/1000 | Loss: 0.00002358
Iteration 51/1000 | Loss: 0.00002357
Iteration 52/1000 | Loss: 0.00002357
Iteration 53/1000 | Loss: 0.00002357
Iteration 54/1000 | Loss: 0.00002357
Iteration 55/1000 | Loss: 0.00002357
Iteration 56/1000 | Loss: 0.00002356
Iteration 57/1000 | Loss: 0.00002356
Iteration 58/1000 | Loss: 0.00002356
Iteration 59/1000 | Loss: 0.00002356
Iteration 60/1000 | Loss: 0.00002356
Iteration 61/1000 | Loss: 0.00002356
Iteration 62/1000 | Loss: 0.00002355
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002354
Iteration 66/1000 | Loss: 0.00002353
Iteration 67/1000 | Loss: 0.00002353
Iteration 68/1000 | Loss: 0.00002353
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002352
Iteration 73/1000 | Loss: 0.00002351
Iteration 74/1000 | Loss: 0.00002351
Iteration 75/1000 | Loss: 0.00002350
Iteration 76/1000 | Loss: 0.00002350
Iteration 77/1000 | Loss: 0.00002350
Iteration 78/1000 | Loss: 0.00002350
Iteration 79/1000 | Loss: 0.00002349
Iteration 80/1000 | Loss: 0.00002349
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002349
Iteration 84/1000 | Loss: 0.00002349
Iteration 85/1000 | Loss: 0.00002349
Iteration 86/1000 | Loss: 0.00002349
Iteration 87/1000 | Loss: 0.00002349
Iteration 88/1000 | Loss: 0.00002349
Iteration 89/1000 | Loss: 0.00002349
Iteration 90/1000 | Loss: 0.00002349
Iteration 91/1000 | Loss: 0.00002349
Iteration 92/1000 | Loss: 0.00002349
Iteration 93/1000 | Loss: 0.00002349
Iteration 94/1000 | Loss: 0.00002349
Iteration 95/1000 | Loss: 0.00002349
Iteration 96/1000 | Loss: 0.00002349
Iteration 97/1000 | Loss: 0.00002349
Iteration 98/1000 | Loss: 0.00002349
Iteration 99/1000 | Loss: 0.00002349
Iteration 100/1000 | Loss: 0.00002349
Iteration 101/1000 | Loss: 0.00002349
Iteration 102/1000 | Loss: 0.00002349
Iteration 103/1000 | Loss: 0.00002349
Iteration 104/1000 | Loss: 0.00002349
Iteration 105/1000 | Loss: 0.00002349
Iteration 106/1000 | Loss: 0.00002349
Iteration 107/1000 | Loss: 0.00002349
Iteration 108/1000 | Loss: 0.00002349
Iteration 109/1000 | Loss: 0.00002349
Iteration 110/1000 | Loss: 0.00002349
Iteration 111/1000 | Loss: 0.00002349
Iteration 112/1000 | Loss: 0.00002349
Iteration 113/1000 | Loss: 0.00002349
Iteration 114/1000 | Loss: 0.00002349
Iteration 115/1000 | Loss: 0.00002349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.349133137613535e-05, 2.349133137613535e-05, 2.349133137613535e-05, 2.349133137613535e-05, 2.349133137613535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.349133137613535e-05

Optimization complete. Final v2v error: 4.094931602478027 mm

Highest mean error: 4.472931861877441 mm for frame 71

Lowest mean error: 3.8594350814819336 mm for frame 183

Saving results

Total time: 99.81739640235901
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793600
Iteration 2/25 | Loss: 0.00206875
Iteration 3/25 | Loss: 0.00153093
Iteration 4/25 | Loss: 0.00148430
Iteration 5/25 | Loss: 0.00135058
Iteration 6/25 | Loss: 0.00126749
Iteration 7/25 | Loss: 0.00120090
Iteration 8/25 | Loss: 0.00118978
Iteration 9/25 | Loss: 0.00118762
Iteration 10/25 | Loss: 0.00118678
Iteration 11/25 | Loss: 0.00118643
Iteration 12/25 | Loss: 0.00118645
Iteration 13/25 | Loss: 0.00118657
Iteration 14/25 | Loss: 0.00118672
Iteration 15/25 | Loss: 0.00118658
Iteration 16/25 | Loss: 0.00118650
Iteration 17/25 | Loss: 0.00118664
Iteration 18/25 | Loss: 0.00118644
Iteration 19/25 | Loss: 0.00118650
Iteration 20/25 | Loss: 0.00118636
Iteration 21/25 | Loss: 0.00118620
Iteration 22/25 | Loss: 0.00118667
Iteration 23/25 | Loss: 0.00118613
Iteration 24/25 | Loss: 0.00118541
Iteration 25/25 | Loss: 0.00118491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52113473
Iteration 2/25 | Loss: 0.00153414
Iteration 3/25 | Loss: 0.00153412
Iteration 4/25 | Loss: 0.00153412
Iteration 5/25 | Loss: 0.00153412
Iteration 6/25 | Loss: 0.00153412
Iteration 7/25 | Loss: 0.00153412
Iteration 8/25 | Loss: 0.00153412
Iteration 9/25 | Loss: 0.00153412
Iteration 10/25 | Loss: 0.00153412
Iteration 11/25 | Loss: 0.00153412
Iteration 12/25 | Loss: 0.00153412
Iteration 13/25 | Loss: 0.00153412
Iteration 14/25 | Loss: 0.00153412
Iteration 15/25 | Loss: 0.00153412
Iteration 16/25 | Loss: 0.00153412
Iteration 17/25 | Loss: 0.00153412
Iteration 18/25 | Loss: 0.00153412
Iteration 19/25 | Loss: 0.00153412
Iteration 20/25 | Loss: 0.00153412
Iteration 21/25 | Loss: 0.00153412
Iteration 22/25 | Loss: 0.00153412
Iteration 23/25 | Loss: 0.00153412
Iteration 24/25 | Loss: 0.00153412
Iteration 25/25 | Loss: 0.00153412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153412
Iteration 2/1000 | Loss: 0.00014289
Iteration 3/1000 | Loss: 0.00008054
Iteration 4/1000 | Loss: 0.00006806
Iteration 5/1000 | Loss: 0.00006319
Iteration 6/1000 | Loss: 0.00006056
Iteration 7/1000 | Loss: 0.00005935
Iteration 8/1000 | Loss: 0.00005857
Iteration 9/1000 | Loss: 0.00005801
Iteration 10/1000 | Loss: 0.00005759
Iteration 11/1000 | Loss: 0.00005734
Iteration 12/1000 | Loss: 0.00005709
Iteration 13/1000 | Loss: 0.00005692
Iteration 14/1000 | Loss: 0.00005677
Iteration 15/1000 | Loss: 0.00005669
Iteration 16/1000 | Loss: 0.00005667
Iteration 17/1000 | Loss: 0.00005652
Iteration 18/1000 | Loss: 0.00005646
Iteration 19/1000 | Loss: 0.00005645
Iteration 20/1000 | Loss: 0.00005640
Iteration 21/1000 | Loss: 0.00005640
Iteration 22/1000 | Loss: 0.00005639
Iteration 23/1000 | Loss: 0.00005639
Iteration 24/1000 | Loss: 0.00005638
Iteration 25/1000 | Loss: 0.00005636
Iteration 26/1000 | Loss: 0.00005636
Iteration 27/1000 | Loss: 0.00005636
Iteration 28/1000 | Loss: 0.00005636
Iteration 29/1000 | Loss: 0.00005635
Iteration 30/1000 | Loss: 0.00005635
Iteration 31/1000 | Loss: 0.00005635
Iteration 32/1000 | Loss: 0.00005635
Iteration 33/1000 | Loss: 0.00005635
Iteration 34/1000 | Loss: 0.00005634
Iteration 35/1000 | Loss: 0.00005634
Iteration 36/1000 | Loss: 0.00005634
Iteration 37/1000 | Loss: 0.00005634
Iteration 38/1000 | Loss: 0.00005634
Iteration 39/1000 | Loss: 0.00005634
Iteration 40/1000 | Loss: 0.00005634
Iteration 41/1000 | Loss: 0.00005634
Iteration 42/1000 | Loss: 0.00005634
Iteration 43/1000 | Loss: 0.00005634
Iteration 44/1000 | Loss: 0.00005634
Iteration 45/1000 | Loss: 0.00005634
Iteration 46/1000 | Loss: 0.00005634
Iteration 47/1000 | Loss: 0.00005634
Iteration 48/1000 | Loss: 0.00005634
Iteration 49/1000 | Loss: 0.00005634
Iteration 50/1000 | Loss: 0.00005634
Iteration 51/1000 | Loss: 0.00005634
Iteration 52/1000 | Loss: 0.00005634
Iteration 53/1000 | Loss: 0.00005634
Iteration 54/1000 | Loss: 0.00005634
Iteration 55/1000 | Loss: 0.00005634
Iteration 56/1000 | Loss: 0.00005634
Iteration 57/1000 | Loss: 0.00005634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [5.633526961901225e-05, 5.633526961901225e-05, 5.633526961901225e-05, 5.633526961901225e-05, 5.633526961901225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.633526961901225e-05

Optimization complete. Final v2v error: 6.163033962249756 mm

Highest mean error: 6.755436897277832 mm for frame 42

Lowest mean error: 5.514430999755859 mm for frame 227

Saving results

Total time: 225.67126965522766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666539
Iteration 2/25 | Loss: 0.00104431
Iteration 3/25 | Loss: 0.00093057
Iteration 4/25 | Loss: 0.00090764
Iteration 5/25 | Loss: 0.00090064
Iteration 6/25 | Loss: 0.00089885
Iteration 7/25 | Loss: 0.00089867
Iteration 8/25 | Loss: 0.00089867
Iteration 9/25 | Loss: 0.00089867
Iteration 10/25 | Loss: 0.00089867
Iteration 11/25 | Loss: 0.00089867
Iteration 12/25 | Loss: 0.00089867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008986718603409827, 0.0008986718603409827, 0.0008986718603409827, 0.0008986718603409827, 0.0008986718603409827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008986718603409827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56647336
Iteration 2/25 | Loss: 0.00094630
Iteration 3/25 | Loss: 0.00094630
Iteration 4/25 | Loss: 0.00094630
Iteration 5/25 | Loss: 0.00094630
Iteration 6/25 | Loss: 0.00094630
Iteration 7/25 | Loss: 0.00094630
Iteration 8/25 | Loss: 0.00094630
Iteration 9/25 | Loss: 0.00094630
Iteration 10/25 | Loss: 0.00094630
Iteration 11/25 | Loss: 0.00094630
Iteration 12/25 | Loss: 0.00094630
Iteration 13/25 | Loss: 0.00094630
Iteration 14/25 | Loss: 0.00094630
Iteration 15/25 | Loss: 0.00094630
Iteration 16/25 | Loss: 0.00094630
Iteration 17/25 | Loss: 0.00094630
Iteration 18/25 | Loss: 0.00094630
Iteration 19/25 | Loss: 0.00094630
Iteration 20/25 | Loss: 0.00094630
Iteration 21/25 | Loss: 0.00094630
Iteration 22/25 | Loss: 0.00094630
Iteration 23/25 | Loss: 0.00094630
Iteration 24/25 | Loss: 0.00094630
Iteration 25/25 | Loss: 0.00094630

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094630
Iteration 2/1000 | Loss: 0.00003064
Iteration 3/1000 | Loss: 0.00002253
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001993
Iteration 7/1000 | Loss: 0.00001957
Iteration 8/1000 | Loss: 0.00001940
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001925
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001925
Iteration 14/1000 | Loss: 0.00001924
Iteration 15/1000 | Loss: 0.00001924
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001922
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00001921
Iteration 23/1000 | Loss: 0.00001921
Iteration 24/1000 | Loss: 0.00001920
Iteration 25/1000 | Loss: 0.00001919
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001914
Iteration 28/1000 | Loss: 0.00001914
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001907
Iteration 43/1000 | Loss: 0.00001907
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001906
Iteration 46/1000 | Loss: 0.00001906
Iteration 47/1000 | Loss: 0.00001906
Iteration 48/1000 | Loss: 0.00001906
Iteration 49/1000 | Loss: 0.00001906
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001905
Iteration 55/1000 | Loss: 0.00001905
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001904
Iteration 58/1000 | Loss: 0.00001904
Iteration 59/1000 | Loss: 0.00001904
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001904
Iteration 62/1000 | Loss: 0.00001904
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001904
Iteration 68/1000 | Loss: 0.00001904
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001904
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.9036006051464938e-05, 1.9036006051464938e-05, 1.9036006051464938e-05, 1.9036006051464938e-05, 1.9036006051464938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9036006051464938e-05

Optimization complete. Final v2v error: 3.7943239212036133 mm

Highest mean error: 4.500716686248779 mm for frame 160

Lowest mean error: 3.201948881149292 mm for frame 66

Saving results

Total time: 83.68153095245361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816479
Iteration 2/25 | Loss: 0.00113493
Iteration 3/25 | Loss: 0.00101816
Iteration 4/25 | Loss: 0.00098154
Iteration 5/25 | Loss: 0.00096979
Iteration 6/25 | Loss: 0.00096664
Iteration 7/25 | Loss: 0.00096605
Iteration 8/25 | Loss: 0.00096605
Iteration 9/25 | Loss: 0.00096605
Iteration 10/25 | Loss: 0.00096605
Iteration 11/25 | Loss: 0.00096605
Iteration 12/25 | Loss: 0.00096605
Iteration 13/25 | Loss: 0.00096605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009660510113462806, 0.0009660510113462806, 0.0009660510113462806, 0.0009660510113462806, 0.0009660510113462806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009660510113462806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39238811
Iteration 2/25 | Loss: 0.00117209
Iteration 3/25 | Loss: 0.00117202
Iteration 4/25 | Loss: 0.00117202
Iteration 5/25 | Loss: 0.00117202
Iteration 6/25 | Loss: 0.00117201
Iteration 7/25 | Loss: 0.00117201
Iteration 8/25 | Loss: 0.00117201
Iteration 9/25 | Loss: 0.00117201
Iteration 10/25 | Loss: 0.00117201
Iteration 11/25 | Loss: 0.00117201
Iteration 12/25 | Loss: 0.00117201
Iteration 13/25 | Loss: 0.00117201
Iteration 14/25 | Loss: 0.00117201
Iteration 15/25 | Loss: 0.00117201
Iteration 16/25 | Loss: 0.00117201
Iteration 17/25 | Loss: 0.00117201
Iteration 18/25 | Loss: 0.00117201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011720139300450683, 0.0011720139300450683, 0.0011720139300450683, 0.0011720139300450683, 0.0011720139300450683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011720139300450683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117201
Iteration 2/1000 | Loss: 0.00006266
Iteration 3/1000 | Loss: 0.00003232
Iteration 4/1000 | Loss: 0.00002794
Iteration 5/1000 | Loss: 0.00002586
Iteration 6/1000 | Loss: 0.00002453
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002274
Iteration 9/1000 | Loss: 0.00002226
Iteration 10/1000 | Loss: 0.00002205
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002201
Iteration 13/1000 | Loss: 0.00002200
Iteration 14/1000 | Loss: 0.00002186
Iteration 15/1000 | Loss: 0.00002181
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002178
Iteration 18/1000 | Loss: 0.00002177
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002176
Iteration 22/1000 | Loss: 0.00002171
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002167
Iteration 30/1000 | Loss: 0.00002166
Iteration 31/1000 | Loss: 0.00002166
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002165
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002165
Iteration 41/1000 | Loss: 0.00002165
Iteration 42/1000 | Loss: 0.00002165
Iteration 43/1000 | Loss: 0.00002165
Iteration 44/1000 | Loss: 0.00002165
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002164
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002163
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002162
Iteration 51/1000 | Loss: 0.00002162
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002161
Iteration 54/1000 | Loss: 0.00002161
Iteration 55/1000 | Loss: 0.00002161
Iteration 56/1000 | Loss: 0.00002161
Iteration 57/1000 | Loss: 0.00002161
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002160
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002159
Iteration 64/1000 | Loss: 0.00002159
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002159
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002159
Iteration 72/1000 | Loss: 0.00002159
Iteration 73/1000 | Loss: 0.00002159
Iteration 74/1000 | Loss: 0.00002159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [2.1586749426205643e-05, 2.1586749426205643e-05, 2.1586749426205643e-05, 2.1586749426205643e-05, 2.1586749426205643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1586749426205643e-05

Optimization complete. Final v2v error: 3.96235990524292 mm

Highest mean error: 5.293572902679443 mm for frame 3

Lowest mean error: 3.6043219566345215 mm for frame 102

Saving results

Total time: 98.18881487846375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057757
Iteration 2/25 | Loss: 0.00175822
Iteration 3/25 | Loss: 0.00115329
Iteration 4/25 | Loss: 0.00100440
Iteration 5/25 | Loss: 0.00097595
Iteration 6/25 | Loss: 0.00095624
Iteration 7/25 | Loss: 0.00094449
Iteration 8/25 | Loss: 0.00094277
Iteration 9/25 | Loss: 0.00094187
Iteration 10/25 | Loss: 0.00094033
Iteration 11/25 | Loss: 0.00093937
Iteration 12/25 | Loss: 0.00093879
Iteration 13/25 | Loss: 0.00093830
Iteration 14/25 | Loss: 0.00093806
Iteration 15/25 | Loss: 0.00093787
Iteration 16/25 | Loss: 0.00093773
Iteration 17/25 | Loss: 0.00093766
Iteration 18/25 | Loss: 0.00093765
Iteration 19/25 | Loss: 0.00093764
Iteration 20/25 | Loss: 0.00093764
Iteration 21/25 | Loss: 0.00093764
Iteration 22/25 | Loss: 0.00093764
Iteration 23/25 | Loss: 0.00093764
Iteration 24/25 | Loss: 0.00093764
Iteration 25/25 | Loss: 0.00093764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70446634
Iteration 2/25 | Loss: 0.00142879
Iteration 3/25 | Loss: 0.00142879
Iteration 4/25 | Loss: 0.00142879
Iteration 5/25 | Loss: 0.00142879
Iteration 6/25 | Loss: 0.00142879
Iteration 7/25 | Loss: 0.00142879
Iteration 8/25 | Loss: 0.00142879
Iteration 9/25 | Loss: 0.00142878
Iteration 10/25 | Loss: 0.00142878
Iteration 11/25 | Loss: 0.00142878
Iteration 12/25 | Loss: 0.00142878
Iteration 13/25 | Loss: 0.00142878
Iteration 14/25 | Loss: 0.00142878
Iteration 15/25 | Loss: 0.00142878
Iteration 16/25 | Loss: 0.00142878
Iteration 17/25 | Loss: 0.00142878
Iteration 18/25 | Loss: 0.00142878
Iteration 19/25 | Loss: 0.00142878
Iteration 20/25 | Loss: 0.00142878
Iteration 21/25 | Loss: 0.00142878
Iteration 22/25 | Loss: 0.00142878
Iteration 23/25 | Loss: 0.00142878
Iteration 24/25 | Loss: 0.00142878
Iteration 25/25 | Loss: 0.00142878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014287844533100724, 0.0014287844533100724, 0.0014287844533100724, 0.0014287844533100724, 0.0014287844533100724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014287844533100724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142878
Iteration 2/1000 | Loss: 0.00014435
Iteration 3/1000 | Loss: 0.00004791
Iteration 4/1000 | Loss: 0.00002998
Iteration 5/1000 | Loss: 0.00002532
Iteration 6/1000 | Loss: 0.00002337
Iteration 7/1000 | Loss: 0.00002205
Iteration 8/1000 | Loss: 0.00002132
Iteration 9/1000 | Loss: 0.00002082
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002026
Iteration 12/1000 | Loss: 0.00002020
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00002005
Iteration 16/1000 | Loss: 0.00001989
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001980
Iteration 22/1000 | Loss: 0.00001980
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001978
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001974
Iteration 33/1000 | Loss: 0.00001974
Iteration 34/1000 | Loss: 0.00001973
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001973
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001972
Iteration 39/1000 | Loss: 0.00001972
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001971
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001970
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001964
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001964
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001963
Iteration 62/1000 | Loss: 0.00001962
Iteration 63/1000 | Loss: 0.00001962
Iteration 64/1000 | Loss: 0.00001962
Iteration 65/1000 | Loss: 0.00001962
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001959
Iteration 73/1000 | Loss: 0.00001959
Iteration 74/1000 | Loss: 0.00001959
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001958
Iteration 88/1000 | Loss: 0.00001958
Iteration 89/1000 | Loss: 0.00001958
Iteration 90/1000 | Loss: 0.00001958
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001957
Iteration 94/1000 | Loss: 0.00001957
Iteration 95/1000 | Loss: 0.00001957
Iteration 96/1000 | Loss: 0.00001957
Iteration 97/1000 | Loss: 0.00001957
Iteration 98/1000 | Loss: 0.00001957
Iteration 99/1000 | Loss: 0.00001957
Iteration 100/1000 | Loss: 0.00001957
Iteration 101/1000 | Loss: 0.00001957
Iteration 102/1000 | Loss: 0.00001957
Iteration 103/1000 | Loss: 0.00001957
Iteration 104/1000 | Loss: 0.00001957
Iteration 105/1000 | Loss: 0.00001957
Iteration 106/1000 | Loss: 0.00001957
Iteration 107/1000 | Loss: 0.00001957
Iteration 108/1000 | Loss: 0.00001957
Iteration 109/1000 | Loss: 0.00001957
Iteration 110/1000 | Loss: 0.00001957
Iteration 111/1000 | Loss: 0.00001957
Iteration 112/1000 | Loss: 0.00001957
Iteration 113/1000 | Loss: 0.00001957
Iteration 114/1000 | Loss: 0.00001957
Iteration 115/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.9574146790546365e-05, 1.9574146790546365e-05, 1.9574146790546365e-05, 1.9574146790546365e-05, 1.9574146790546365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9574146790546365e-05

Optimization complete. Final v2v error: 3.813789129257202 mm

Highest mean error: 4.858700752258301 mm for frame 5

Lowest mean error: 3.0188090801239014 mm for frame 127

Saving results

Total time: 177.54060244560242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403447
Iteration 2/25 | Loss: 0.00098295
Iteration 3/25 | Loss: 0.00084877
Iteration 4/25 | Loss: 0.00082981
Iteration 5/25 | Loss: 0.00082467
Iteration 6/25 | Loss: 0.00082306
Iteration 7/25 | Loss: 0.00082269
Iteration 8/25 | Loss: 0.00082269
Iteration 9/25 | Loss: 0.00082269
Iteration 10/25 | Loss: 0.00082269
Iteration 11/25 | Loss: 0.00082269
Iteration 12/25 | Loss: 0.00082269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008226886857300997, 0.0008226886857300997, 0.0008226886857300997, 0.0008226886857300997, 0.0008226886857300997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008226886857300997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53127396
Iteration 2/25 | Loss: 0.00090253
Iteration 3/25 | Loss: 0.00090253
Iteration 4/25 | Loss: 0.00090253
Iteration 5/25 | Loss: 0.00090253
Iteration 6/25 | Loss: 0.00090253
Iteration 7/25 | Loss: 0.00090253
Iteration 8/25 | Loss: 0.00090253
Iteration 9/25 | Loss: 0.00090253
Iteration 10/25 | Loss: 0.00090253
Iteration 11/25 | Loss: 0.00090253
Iteration 12/25 | Loss: 0.00090253
Iteration 13/25 | Loss: 0.00090253
Iteration 14/25 | Loss: 0.00090253
Iteration 15/25 | Loss: 0.00090253
Iteration 16/25 | Loss: 0.00090253
Iteration 17/25 | Loss: 0.00090253
Iteration 18/25 | Loss: 0.00090253
Iteration 19/25 | Loss: 0.00090253
Iteration 20/25 | Loss: 0.00090253
Iteration 21/25 | Loss: 0.00090253
Iteration 22/25 | Loss: 0.00090253
Iteration 23/25 | Loss: 0.00090253
Iteration 24/25 | Loss: 0.00090253
Iteration 25/25 | Loss: 0.00090253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090253
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001712
Iteration 4/1000 | Loss: 0.00001572
Iteration 5/1000 | Loss: 0.00001487
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001410
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001379
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001379
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001376
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001372
Iteration 54/1000 | Loss: 0.00001372
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001366
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001365
Iteration 74/1000 | Loss: 0.00001364
Iteration 75/1000 | Loss: 0.00001364
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001363
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001363
Iteration 84/1000 | Loss: 0.00001363
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001362
Iteration 88/1000 | Loss: 0.00001362
Iteration 89/1000 | Loss: 0.00001362
Iteration 90/1000 | Loss: 0.00001362
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001357
Iteration 111/1000 | Loss: 0.00001357
Iteration 112/1000 | Loss: 0.00001357
Iteration 113/1000 | Loss: 0.00001357
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001356
Iteration 119/1000 | Loss: 0.00001356
Iteration 120/1000 | Loss: 0.00001356
Iteration 121/1000 | Loss: 0.00001356
Iteration 122/1000 | Loss: 0.00001356
Iteration 123/1000 | Loss: 0.00001356
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001356
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.3564747860073112e-05, 1.3564747860073112e-05, 1.3564747860073112e-05, 1.3564747860073112e-05, 1.3564747860073112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3564747860073112e-05

Optimization complete. Final v2v error: 3.1599321365356445 mm

Highest mean error: 4.281064510345459 mm for frame 70

Lowest mean error: 2.772902727127075 mm for frame 175

Saving results

Total time: 79.21150922775269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106468
Iteration 2/25 | Loss: 0.00365175
Iteration 3/25 | Loss: 0.00270457
Iteration 4/25 | Loss: 0.00198311
Iteration 5/25 | Loss: 0.00155203
Iteration 6/25 | Loss: 0.00129688
Iteration 7/25 | Loss: 0.00106389
Iteration 8/25 | Loss: 0.00095789
Iteration 9/25 | Loss: 0.00093341
Iteration 10/25 | Loss: 0.00092336
Iteration 11/25 | Loss: 0.00091991
Iteration 12/25 | Loss: 0.00091206
Iteration 13/25 | Loss: 0.00090818
Iteration 14/25 | Loss: 0.00090721
Iteration 15/25 | Loss: 0.00091228
Iteration 16/25 | Loss: 0.00091252
Iteration 17/25 | Loss: 0.00090465
Iteration 18/25 | Loss: 0.00090312
Iteration 19/25 | Loss: 0.00090410
Iteration 20/25 | Loss: 0.00090322
Iteration 21/25 | Loss: 0.00090302
Iteration 22/25 | Loss: 0.00090289
Iteration 23/25 | Loss: 0.00090412
Iteration 24/25 | Loss: 0.00090317
Iteration 25/25 | Loss: 0.00090406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51755166
Iteration 2/25 | Loss: 0.00127516
Iteration 3/25 | Loss: 0.00127516
Iteration 4/25 | Loss: 0.00127516
Iteration 5/25 | Loss: 0.00127516
Iteration 6/25 | Loss: 0.00127516
Iteration 7/25 | Loss: 0.00127516
Iteration 8/25 | Loss: 0.00127516
Iteration 9/25 | Loss: 0.00127516
Iteration 10/25 | Loss: 0.00127516
Iteration 11/25 | Loss: 0.00127516
Iteration 12/25 | Loss: 0.00127516
Iteration 13/25 | Loss: 0.00127516
Iteration 14/25 | Loss: 0.00127516
Iteration 15/25 | Loss: 0.00127516
Iteration 16/25 | Loss: 0.00127516
Iteration 17/25 | Loss: 0.00127516
Iteration 18/25 | Loss: 0.00127516
Iteration 19/25 | Loss: 0.00127516
Iteration 20/25 | Loss: 0.00127516
Iteration 21/25 | Loss: 0.00127516
Iteration 22/25 | Loss: 0.00127516
Iteration 23/25 | Loss: 0.00127516
Iteration 24/25 | Loss: 0.00127516
Iteration 25/25 | Loss: 0.00127516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127516
Iteration 2/1000 | Loss: 0.00012485
Iteration 3/1000 | Loss: 0.00008399
Iteration 4/1000 | Loss: 0.00006956
Iteration 5/1000 | Loss: 0.00006040
Iteration 6/1000 | Loss: 0.00005464
Iteration 7/1000 | Loss: 0.00005519
Iteration 8/1000 | Loss: 0.00005015
Iteration 9/1000 | Loss: 0.00222247
Iteration 10/1000 | Loss: 0.00013473
Iteration 11/1000 | Loss: 0.00005570
Iteration 12/1000 | Loss: 0.00003991
Iteration 13/1000 | Loss: 0.00003706
Iteration 14/1000 | Loss: 0.00003030
Iteration 15/1000 | Loss: 0.00002820
Iteration 16/1000 | Loss: 0.00002637
Iteration 17/1000 | Loss: 0.00002578
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002473
Iteration 20/1000 | Loss: 0.00002451
Iteration 21/1000 | Loss: 0.00002617
Iteration 22/1000 | Loss: 0.00002476
Iteration 23/1000 | Loss: 0.00002622
Iteration 24/1000 | Loss: 0.00002449
Iteration 25/1000 | Loss: 0.00002577
Iteration 26/1000 | Loss: 0.00002407
Iteration 27/1000 | Loss: 0.00002414
Iteration 28/1000 | Loss: 0.00002414
Iteration 29/1000 | Loss: 0.00002571
Iteration 30/1000 | Loss: 0.00002397
Iteration 31/1000 | Loss: 0.00002395
Iteration 32/1000 | Loss: 0.00002422
Iteration 33/1000 | Loss: 0.00002582
Iteration 34/1000 | Loss: 0.00002392
Iteration 35/1000 | Loss: 0.00002392
Iteration 36/1000 | Loss: 0.00002392
Iteration 37/1000 | Loss: 0.00002392
Iteration 38/1000 | Loss: 0.00002392
Iteration 39/1000 | Loss: 0.00002391
Iteration 40/1000 | Loss: 0.00002391
Iteration 41/1000 | Loss: 0.00002391
Iteration 42/1000 | Loss: 0.00002390
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002530
Iteration 45/1000 | Loss: 0.00002391
Iteration 46/1000 | Loss: 0.00002449
Iteration 47/1000 | Loss: 0.00002483
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002388
Iteration 50/1000 | Loss: 0.00002388
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002387
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00002387
Iteration 58/1000 | Loss: 0.00002387
Iteration 59/1000 | Loss: 0.00002387
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002387
Iteration 65/1000 | Loss: 0.00002387
Iteration 66/1000 | Loss: 0.00002387
Iteration 67/1000 | Loss: 0.00002387
Iteration 68/1000 | Loss: 0.00002387
Iteration 69/1000 | Loss: 0.00002387
Iteration 70/1000 | Loss: 0.00002387
Iteration 71/1000 | Loss: 0.00002387
Iteration 72/1000 | Loss: 0.00002387
Iteration 73/1000 | Loss: 0.00002387
Iteration 74/1000 | Loss: 0.00002387
Iteration 75/1000 | Loss: 0.00002387
Iteration 76/1000 | Loss: 0.00002429
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002429
Iteration 79/1000 | Loss: 0.00002429
Iteration 80/1000 | Loss: 0.00002429
Iteration 81/1000 | Loss: 0.00002429
Iteration 82/1000 | Loss: 0.00002429
Iteration 83/1000 | Loss: 0.00002429
Iteration 84/1000 | Loss: 0.00002429
Iteration 85/1000 | Loss: 0.00002429
Iteration 86/1000 | Loss: 0.00002429
Iteration 87/1000 | Loss: 0.00002429
Iteration 88/1000 | Loss: 0.00002429
Iteration 89/1000 | Loss: 0.00002429
Iteration 90/1000 | Loss: 0.00002429
Iteration 91/1000 | Loss: 0.00002429
Iteration 92/1000 | Loss: 0.00002429
Iteration 93/1000 | Loss: 0.00002429
Iteration 94/1000 | Loss: 0.00002429
Iteration 95/1000 | Loss: 0.00002429
Iteration 96/1000 | Loss: 0.00002429
Iteration 97/1000 | Loss: 0.00002429
Iteration 98/1000 | Loss: 0.00002429
Iteration 99/1000 | Loss: 0.00002429
Iteration 100/1000 | Loss: 0.00002429
Iteration 101/1000 | Loss: 0.00002429
Iteration 102/1000 | Loss: 0.00002429
Iteration 103/1000 | Loss: 0.00002429
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00002429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.4285431209136732e-05, 2.4285431209136732e-05, 2.4285431209136732e-05, 2.4285431209136732e-05, 2.4285431209136732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4285431209136732e-05

Optimization complete. Final v2v error: 4.118527889251709 mm

Highest mean error: 11.794193267822266 mm for frame 32

Lowest mean error: 3.7973854541778564 mm for frame 53

Saving results

Total time: 231.98733568191528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869503
Iteration 2/25 | Loss: 0.00262775
Iteration 3/25 | Loss: 0.00184648
Iteration 4/25 | Loss: 0.00165551
Iteration 5/25 | Loss: 0.00158365
Iteration 6/25 | Loss: 0.00167454
Iteration 7/25 | Loss: 0.00148332
Iteration 8/25 | Loss: 0.00141140
Iteration 9/25 | Loss: 0.00143027
Iteration 10/25 | Loss: 0.00135334
Iteration 11/25 | Loss: 0.00133739
Iteration 12/25 | Loss: 0.00132174
Iteration 13/25 | Loss: 0.00130166
Iteration 14/25 | Loss: 0.00130123
Iteration 15/25 | Loss: 0.00129159
Iteration 16/25 | Loss: 0.00127830
Iteration 17/25 | Loss: 0.00129790
Iteration 18/25 | Loss: 0.00128878
Iteration 19/25 | Loss: 0.00127571
Iteration 20/25 | Loss: 0.00127238
Iteration 21/25 | Loss: 0.00127283
Iteration 22/25 | Loss: 0.00126566
Iteration 23/25 | Loss: 0.00127435
Iteration 24/25 | Loss: 0.00127419
Iteration 25/25 | Loss: 0.00127502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96967089
Iteration 2/25 | Loss: 0.00368604
Iteration 3/25 | Loss: 0.00319131
Iteration 4/25 | Loss: 0.00319131
Iteration 5/25 | Loss: 0.00319131
Iteration 6/25 | Loss: 0.00319131
Iteration 7/25 | Loss: 0.00319131
Iteration 8/25 | Loss: 0.00319130
Iteration 9/25 | Loss: 0.00319130
Iteration 10/25 | Loss: 0.00319130
Iteration 11/25 | Loss: 0.00319131
Iteration 12/25 | Loss: 0.00319130
Iteration 13/25 | Loss: 0.00319130
Iteration 14/25 | Loss: 0.00319130
Iteration 15/25 | Loss: 0.00319130
Iteration 16/25 | Loss: 0.00319130
Iteration 17/25 | Loss: 0.00319130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003191304625943303, 0.003191304625943303, 0.003191304625943303, 0.003191304625943303, 0.003191304625943303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003191304625943303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319130
Iteration 2/1000 | Loss: 0.00075353
Iteration 3/1000 | Loss: 0.00087618
Iteration 4/1000 | Loss: 0.00250626
Iteration 5/1000 | Loss: 0.00266727
Iteration 6/1000 | Loss: 0.00284313
Iteration 7/1000 | Loss: 0.00024749
Iteration 8/1000 | Loss: 0.00022032
Iteration 9/1000 | Loss: 0.00294914
Iteration 10/1000 | Loss: 0.00021480
Iteration 11/1000 | Loss: 0.00121095
Iteration 12/1000 | Loss: 0.01812757
Iteration 13/1000 | Loss: 0.00550399
Iteration 14/1000 | Loss: 0.00522049
Iteration 15/1000 | Loss: 0.00050531
Iteration 16/1000 | Loss: 0.00073668
Iteration 17/1000 | Loss: 0.00040358
Iteration 18/1000 | Loss: 0.00093028
Iteration 19/1000 | Loss: 0.00034160
Iteration 20/1000 | Loss: 0.00064421
Iteration 21/1000 | Loss: 0.00061182
Iteration 22/1000 | Loss: 0.00150581
Iteration 23/1000 | Loss: 0.00046551
Iteration 24/1000 | Loss: 0.00010127
Iteration 25/1000 | Loss: 0.00029543
Iteration 26/1000 | Loss: 0.00073201
Iteration 27/1000 | Loss: 0.00038406
Iteration 28/1000 | Loss: 0.00004712
Iteration 29/1000 | Loss: 0.00020345
Iteration 30/1000 | Loss: 0.00004801
Iteration 31/1000 | Loss: 0.00031310
Iteration 32/1000 | Loss: 0.00004031
Iteration 33/1000 | Loss: 0.00003203
Iteration 34/1000 | Loss: 0.00002889
Iteration 35/1000 | Loss: 0.00028699
Iteration 36/1000 | Loss: 0.00002621
Iteration 37/1000 | Loss: 0.00007382
Iteration 38/1000 | Loss: 0.00002443
Iteration 39/1000 | Loss: 0.00002393
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002319
Iteration 42/1000 | Loss: 0.00002299
Iteration 43/1000 | Loss: 0.00002294
Iteration 44/1000 | Loss: 0.00002287
Iteration 45/1000 | Loss: 0.00002284
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002278
Iteration 48/1000 | Loss: 0.00002274
Iteration 49/1000 | Loss: 0.00002273
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002270
Iteration 52/1000 | Loss: 0.00002270
Iteration 53/1000 | Loss: 0.00002269
Iteration 54/1000 | Loss: 0.00002269
Iteration 55/1000 | Loss: 0.00002269
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002268
Iteration 58/1000 | Loss: 0.00002268
Iteration 59/1000 | Loss: 0.00002267
Iteration 60/1000 | Loss: 0.00002267
Iteration 61/1000 | Loss: 0.00002266
Iteration 62/1000 | Loss: 0.00002266
Iteration 63/1000 | Loss: 0.00002266
Iteration 64/1000 | Loss: 0.00002263
Iteration 65/1000 | Loss: 0.00002263
Iteration 66/1000 | Loss: 0.00002263
Iteration 67/1000 | Loss: 0.00002263
Iteration 68/1000 | Loss: 0.00002263
Iteration 69/1000 | Loss: 0.00002263
Iteration 70/1000 | Loss: 0.00002263
Iteration 71/1000 | Loss: 0.00002262
Iteration 72/1000 | Loss: 0.00002261
Iteration 73/1000 | Loss: 0.00002261
Iteration 74/1000 | Loss: 0.00002261
Iteration 75/1000 | Loss: 0.00002261
Iteration 76/1000 | Loss: 0.00002261
Iteration 77/1000 | Loss: 0.00002261
Iteration 78/1000 | Loss: 0.00002260
Iteration 79/1000 | Loss: 0.00002260
Iteration 80/1000 | Loss: 0.00002260
Iteration 81/1000 | Loss: 0.00002260
Iteration 82/1000 | Loss: 0.00002260
Iteration 83/1000 | Loss: 0.00002259
Iteration 84/1000 | Loss: 0.00002259
Iteration 85/1000 | Loss: 0.00002259
Iteration 86/1000 | Loss: 0.00002259
Iteration 87/1000 | Loss: 0.00002259
Iteration 88/1000 | Loss: 0.00002259
Iteration 89/1000 | Loss: 0.00002259
Iteration 90/1000 | Loss: 0.00002259
Iteration 91/1000 | Loss: 0.00002259
Iteration 92/1000 | Loss: 0.00002258
Iteration 93/1000 | Loss: 0.00002258
Iteration 94/1000 | Loss: 0.00002258
Iteration 95/1000 | Loss: 0.00002257
Iteration 96/1000 | Loss: 0.00002257
Iteration 97/1000 | Loss: 0.00002257
Iteration 98/1000 | Loss: 0.00002257
Iteration 99/1000 | Loss: 0.00002257
Iteration 100/1000 | Loss: 0.00002257
Iteration 101/1000 | Loss: 0.00002257
Iteration 102/1000 | Loss: 0.00002257
Iteration 103/1000 | Loss: 0.00002257
Iteration 104/1000 | Loss: 0.00002257
Iteration 105/1000 | Loss: 0.00002257
Iteration 106/1000 | Loss: 0.00002257
Iteration 107/1000 | Loss: 0.00002257
Iteration 108/1000 | Loss: 0.00002257
Iteration 109/1000 | Loss: 0.00002257
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002256
Iteration 114/1000 | Loss: 0.00002256
Iteration 115/1000 | Loss: 0.00002256
Iteration 116/1000 | Loss: 0.00002256
Iteration 117/1000 | Loss: 0.00002256
Iteration 118/1000 | Loss: 0.00002256
Iteration 119/1000 | Loss: 0.00002256
Iteration 120/1000 | Loss: 0.00002256
Iteration 121/1000 | Loss: 0.00002256
Iteration 122/1000 | Loss: 0.00002255
Iteration 123/1000 | Loss: 0.00002255
Iteration 124/1000 | Loss: 0.00002255
Iteration 125/1000 | Loss: 0.00002255
Iteration 126/1000 | Loss: 0.00002255
Iteration 127/1000 | Loss: 0.00002255
Iteration 128/1000 | Loss: 0.00002255
Iteration 129/1000 | Loss: 0.00002255
Iteration 130/1000 | Loss: 0.00002255
Iteration 131/1000 | Loss: 0.00002255
Iteration 132/1000 | Loss: 0.00002255
Iteration 133/1000 | Loss: 0.00002255
Iteration 134/1000 | Loss: 0.00002255
Iteration 135/1000 | Loss: 0.00002255
Iteration 136/1000 | Loss: 0.00002255
Iteration 137/1000 | Loss: 0.00002255
Iteration 138/1000 | Loss: 0.00002254
Iteration 139/1000 | Loss: 0.00002254
Iteration 140/1000 | Loss: 0.00002254
Iteration 141/1000 | Loss: 0.00002254
Iteration 142/1000 | Loss: 0.00002254
Iteration 143/1000 | Loss: 0.00002254
Iteration 144/1000 | Loss: 0.00002254
Iteration 145/1000 | Loss: 0.00002254
Iteration 146/1000 | Loss: 0.00002254
Iteration 147/1000 | Loss: 0.00002254
Iteration 148/1000 | Loss: 0.00002254
Iteration 149/1000 | Loss: 0.00002254
Iteration 150/1000 | Loss: 0.00002254
Iteration 151/1000 | Loss: 0.00002254
Iteration 152/1000 | Loss: 0.00002254
Iteration 153/1000 | Loss: 0.00002254
Iteration 154/1000 | Loss: 0.00002254
Iteration 155/1000 | Loss: 0.00002254
Iteration 156/1000 | Loss: 0.00002254
Iteration 157/1000 | Loss: 0.00002254
Iteration 158/1000 | Loss: 0.00002254
Iteration 159/1000 | Loss: 0.00002254
Iteration 160/1000 | Loss: 0.00002254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.253640559501946e-05, 2.253640559501946e-05, 2.253640559501946e-05, 2.253640559501946e-05, 2.253640559501946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.253640559501946e-05

Optimization complete. Final v2v error: 4.067183971405029 mm

Highest mean error: 4.367988109588623 mm for frame 22

Lowest mean error: 3.820026397705078 mm for frame 95

Saving results

Total time: 260.545517206192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556774
Iteration 2/25 | Loss: 0.00133555
Iteration 3/25 | Loss: 0.00102849
Iteration 4/25 | Loss: 0.00098412
Iteration 5/25 | Loss: 0.00097006
Iteration 6/25 | Loss: 0.00096675
Iteration 7/25 | Loss: 0.00096613
Iteration 8/25 | Loss: 0.00096613
Iteration 9/25 | Loss: 0.00096613
Iteration 10/25 | Loss: 0.00096613
Iteration 11/25 | Loss: 0.00096613
Iteration 12/25 | Loss: 0.00096613
Iteration 13/25 | Loss: 0.00096613
Iteration 14/25 | Loss: 0.00096613
Iteration 15/25 | Loss: 0.00096613
Iteration 16/25 | Loss: 0.00096613
Iteration 17/25 | Loss: 0.00096613
Iteration 18/25 | Loss: 0.00096613
Iteration 19/25 | Loss: 0.00096613
Iteration 20/25 | Loss: 0.00096613
Iteration 21/25 | Loss: 0.00096613
Iteration 22/25 | Loss: 0.00096613
Iteration 23/25 | Loss: 0.00096613
Iteration 24/25 | Loss: 0.00096613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000966133491601795, 0.000966133491601795, 0.000966133491601795, 0.000966133491601795, 0.000966133491601795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000966133491601795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49720919
Iteration 2/25 | Loss: 0.00100653
Iteration 3/25 | Loss: 0.00100650
Iteration 4/25 | Loss: 0.00100650
Iteration 5/25 | Loss: 0.00100650
Iteration 6/25 | Loss: 0.00100650
Iteration 7/25 | Loss: 0.00100650
Iteration 8/25 | Loss: 0.00100650
Iteration 9/25 | Loss: 0.00100650
Iteration 10/25 | Loss: 0.00100650
Iteration 11/25 | Loss: 0.00100650
Iteration 12/25 | Loss: 0.00100650
Iteration 13/25 | Loss: 0.00100650
Iteration 14/25 | Loss: 0.00100650
Iteration 15/25 | Loss: 0.00100650
Iteration 16/25 | Loss: 0.00100650
Iteration 17/25 | Loss: 0.00100650
Iteration 18/25 | Loss: 0.00100650
Iteration 19/25 | Loss: 0.00100650
Iteration 20/25 | Loss: 0.00100650
Iteration 21/25 | Loss: 0.00100650
Iteration 22/25 | Loss: 0.00100650
Iteration 23/25 | Loss: 0.00100650
Iteration 24/25 | Loss: 0.00100650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010065014939755201, 0.0010065014939755201, 0.0010065014939755201, 0.0010065014939755201, 0.0010065014939755201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010065014939755201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100650
Iteration 2/1000 | Loss: 0.00006002
Iteration 3/1000 | Loss: 0.00004202
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003381
Iteration 6/1000 | Loss: 0.00003226
Iteration 7/1000 | Loss: 0.00003106
Iteration 8/1000 | Loss: 0.00003015
Iteration 9/1000 | Loss: 0.00002966
Iteration 10/1000 | Loss: 0.00002946
Iteration 11/1000 | Loss: 0.00002927
Iteration 12/1000 | Loss: 0.00002922
Iteration 13/1000 | Loss: 0.00002917
Iteration 14/1000 | Loss: 0.00002917
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002914
Iteration 17/1000 | Loss: 0.00002913
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002912
Iteration 20/1000 | Loss: 0.00002911
Iteration 21/1000 | Loss: 0.00002911
Iteration 22/1000 | Loss: 0.00002911
Iteration 23/1000 | Loss: 0.00002911
Iteration 24/1000 | Loss: 0.00002911
Iteration 25/1000 | Loss: 0.00002908
Iteration 26/1000 | Loss: 0.00002906
Iteration 27/1000 | Loss: 0.00002906
Iteration 28/1000 | Loss: 0.00002905
Iteration 29/1000 | Loss: 0.00002904
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002903
Iteration 32/1000 | Loss: 0.00002903
Iteration 33/1000 | Loss: 0.00002903
Iteration 34/1000 | Loss: 0.00002902
Iteration 35/1000 | Loss: 0.00002902
Iteration 36/1000 | Loss: 0.00002902
Iteration 37/1000 | Loss: 0.00002902
Iteration 38/1000 | Loss: 0.00002902
Iteration 39/1000 | Loss: 0.00002901
Iteration 40/1000 | Loss: 0.00002901
Iteration 41/1000 | Loss: 0.00002901
Iteration 42/1000 | Loss: 0.00002901
Iteration 43/1000 | Loss: 0.00002901
Iteration 44/1000 | Loss: 0.00002901
Iteration 45/1000 | Loss: 0.00002901
Iteration 46/1000 | Loss: 0.00002901
Iteration 47/1000 | Loss: 0.00002901
Iteration 48/1000 | Loss: 0.00002901
Iteration 49/1000 | Loss: 0.00002901
Iteration 50/1000 | Loss: 0.00002900
Iteration 51/1000 | Loss: 0.00002900
Iteration 52/1000 | Loss: 0.00002900
Iteration 53/1000 | Loss: 0.00002900
Iteration 54/1000 | Loss: 0.00002899
Iteration 55/1000 | Loss: 0.00002899
Iteration 56/1000 | Loss: 0.00002899
Iteration 57/1000 | Loss: 0.00002898
Iteration 58/1000 | Loss: 0.00002898
Iteration 59/1000 | Loss: 0.00002898
Iteration 60/1000 | Loss: 0.00002898
Iteration 61/1000 | Loss: 0.00002897
Iteration 62/1000 | Loss: 0.00002897
Iteration 63/1000 | Loss: 0.00002897
Iteration 64/1000 | Loss: 0.00002897
Iteration 65/1000 | Loss: 0.00002897
Iteration 66/1000 | Loss: 0.00002897
Iteration 67/1000 | Loss: 0.00002897
Iteration 68/1000 | Loss: 0.00002897
Iteration 69/1000 | Loss: 0.00002897
Iteration 70/1000 | Loss: 0.00002896
Iteration 71/1000 | Loss: 0.00002896
Iteration 72/1000 | Loss: 0.00002896
Iteration 73/1000 | Loss: 0.00002896
Iteration 74/1000 | Loss: 0.00002896
Iteration 75/1000 | Loss: 0.00002896
Iteration 76/1000 | Loss: 0.00002896
Iteration 77/1000 | Loss: 0.00002896
Iteration 78/1000 | Loss: 0.00002896
Iteration 79/1000 | Loss: 0.00002896
Iteration 80/1000 | Loss: 0.00002896
Iteration 81/1000 | Loss: 0.00002896
Iteration 82/1000 | Loss: 0.00002896
Iteration 83/1000 | Loss: 0.00002896
Iteration 84/1000 | Loss: 0.00002896
Iteration 85/1000 | Loss: 0.00002896
Iteration 86/1000 | Loss: 0.00002895
Iteration 87/1000 | Loss: 0.00002895
Iteration 88/1000 | Loss: 0.00002895
Iteration 89/1000 | Loss: 0.00002895
Iteration 90/1000 | Loss: 0.00002895
Iteration 91/1000 | Loss: 0.00002895
Iteration 92/1000 | Loss: 0.00002895
Iteration 93/1000 | Loss: 0.00002895
Iteration 94/1000 | Loss: 0.00002895
Iteration 95/1000 | Loss: 0.00002895
Iteration 96/1000 | Loss: 0.00002895
Iteration 97/1000 | Loss: 0.00002895
Iteration 98/1000 | Loss: 0.00002895
Iteration 99/1000 | Loss: 0.00002895
Iteration 100/1000 | Loss: 0.00002895
Iteration 101/1000 | Loss: 0.00002895
Iteration 102/1000 | Loss: 0.00002895
Iteration 103/1000 | Loss: 0.00002895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.8953923902008682e-05, 2.8953923902008682e-05, 2.8953923902008682e-05, 2.8953923902008682e-05, 2.8953923902008682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8953923902008682e-05

Optimization complete. Final v2v error: 4.584521293640137 mm

Highest mean error: 4.940435886383057 mm for frame 54

Lowest mean error: 3.9744935035705566 mm for frame 105

Saving results

Total time: 78.87470579147339
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676954
Iteration 2/25 | Loss: 0.00150111
Iteration 3/25 | Loss: 0.00104157
Iteration 4/25 | Loss: 0.00097779
Iteration 5/25 | Loss: 0.00096374
Iteration 6/25 | Loss: 0.00096037
Iteration 7/25 | Loss: 0.00095951
Iteration 8/25 | Loss: 0.00095932
Iteration 9/25 | Loss: 0.00095932
Iteration 10/25 | Loss: 0.00095932
Iteration 11/25 | Loss: 0.00095932
Iteration 12/25 | Loss: 0.00095932
Iteration 13/25 | Loss: 0.00095932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009593217400833964, 0.0009593217400833964, 0.0009593217400833964, 0.0009593217400833964, 0.0009593217400833964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009593217400833964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31578684
Iteration 2/25 | Loss: 0.00110889
Iteration 3/25 | Loss: 0.00110886
Iteration 4/25 | Loss: 0.00110886
Iteration 5/25 | Loss: 0.00110885
Iteration 6/25 | Loss: 0.00110885
Iteration 7/25 | Loss: 0.00110885
Iteration 8/25 | Loss: 0.00110885
Iteration 9/25 | Loss: 0.00110885
Iteration 10/25 | Loss: 0.00110885
Iteration 11/25 | Loss: 0.00110885
Iteration 12/25 | Loss: 0.00110885
Iteration 13/25 | Loss: 0.00110885
Iteration 14/25 | Loss: 0.00110885
Iteration 15/25 | Loss: 0.00110885
Iteration 16/25 | Loss: 0.00110885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011088527971878648, 0.0011088527971878648, 0.0011088527971878648, 0.0011088527971878648, 0.0011088527971878648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011088527971878648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110885
Iteration 2/1000 | Loss: 0.00003691
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00002119
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001725
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001705
Iteration 14/1000 | Loss: 0.00001702
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001691
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001691
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001689
Iteration 27/1000 | Loss: 0.00001689
Iteration 28/1000 | Loss: 0.00001689
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001683
Iteration 55/1000 | Loss: 0.00001683
Iteration 56/1000 | Loss: 0.00001683
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001682
Iteration 60/1000 | Loss: 0.00001682
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001681
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001678
Iteration 76/1000 | Loss: 0.00001678
Iteration 77/1000 | Loss: 0.00001678
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001671
Iteration 111/1000 | Loss: 0.00001671
Iteration 112/1000 | Loss: 0.00001671
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001670
Iteration 115/1000 | Loss: 0.00001670
Iteration 116/1000 | Loss: 0.00001670
Iteration 117/1000 | Loss: 0.00001670
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001669
Iteration 121/1000 | Loss: 0.00001669
Iteration 122/1000 | Loss: 0.00001669
Iteration 123/1000 | Loss: 0.00001669
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001668
Iteration 130/1000 | Loss: 0.00001668
Iteration 131/1000 | Loss: 0.00001668
Iteration 132/1000 | Loss: 0.00001668
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001668
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001667
Iteration 139/1000 | Loss: 0.00001667
Iteration 140/1000 | Loss: 0.00001667
Iteration 141/1000 | Loss: 0.00001667
Iteration 142/1000 | Loss: 0.00001667
Iteration 143/1000 | Loss: 0.00001667
Iteration 144/1000 | Loss: 0.00001667
Iteration 145/1000 | Loss: 0.00001667
Iteration 146/1000 | Loss: 0.00001666
Iteration 147/1000 | Loss: 0.00001666
Iteration 148/1000 | Loss: 0.00001666
Iteration 149/1000 | Loss: 0.00001666
Iteration 150/1000 | Loss: 0.00001666
Iteration 151/1000 | Loss: 0.00001666
Iteration 152/1000 | Loss: 0.00001666
Iteration 153/1000 | Loss: 0.00001666
Iteration 154/1000 | Loss: 0.00001666
Iteration 155/1000 | Loss: 0.00001666
Iteration 156/1000 | Loss: 0.00001666
Iteration 157/1000 | Loss: 0.00001666
Iteration 158/1000 | Loss: 0.00001666
Iteration 159/1000 | Loss: 0.00001666
Iteration 160/1000 | Loss: 0.00001666
Iteration 161/1000 | Loss: 0.00001666
Iteration 162/1000 | Loss: 0.00001666
Iteration 163/1000 | Loss: 0.00001666
Iteration 164/1000 | Loss: 0.00001666
Iteration 165/1000 | Loss: 0.00001666
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001666
Iteration 168/1000 | Loss: 0.00001666
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001666
Iteration 171/1000 | Loss: 0.00001666
Iteration 172/1000 | Loss: 0.00001666
Iteration 173/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.6660624169162475e-05, 1.6660624169162475e-05, 1.6660624169162475e-05, 1.6660624169162475e-05, 1.6660624169162475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6660624169162475e-05

Optimization complete. Final v2v error: 3.498734712600708 mm

Highest mean error: 4.023609161376953 mm for frame 51

Lowest mean error: 3.271496295928955 mm for frame 109

Saving results

Total time: 92.0871651172638
