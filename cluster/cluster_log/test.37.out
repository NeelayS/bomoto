Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=37, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2072-2127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564935
Iteration 2/25 | Loss: 0.00131773
Iteration 3/25 | Loss: 0.00125390
Iteration 4/25 | Loss: 0.00124367
Iteration 5/25 | Loss: 0.00123925
Iteration 6/25 | Loss: 0.00123845
Iteration 7/25 | Loss: 0.00123845
Iteration 8/25 | Loss: 0.00123845
Iteration 9/25 | Loss: 0.00123845
Iteration 10/25 | Loss: 0.00123845
Iteration 11/25 | Loss: 0.00123845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012384463334456086, 0.0012384463334456086, 0.0012384463334456086, 0.0012384463334456086, 0.0012384463334456086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012384463334456086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69559073
Iteration 2/25 | Loss: 0.00108586
Iteration 3/25 | Loss: 0.00108586
Iteration 4/25 | Loss: 0.00108586
Iteration 5/25 | Loss: 0.00108586
Iteration 6/25 | Loss: 0.00108585
Iteration 7/25 | Loss: 0.00108585
Iteration 8/25 | Loss: 0.00108585
Iteration 9/25 | Loss: 0.00108585
Iteration 10/25 | Loss: 0.00108585
Iteration 11/25 | Loss: 0.00108585
Iteration 12/25 | Loss: 0.00108585
Iteration 13/25 | Loss: 0.00108585
Iteration 14/25 | Loss: 0.00108585
Iteration 15/25 | Loss: 0.00108585
Iteration 16/25 | Loss: 0.00108585
Iteration 17/25 | Loss: 0.00108585
Iteration 18/25 | Loss: 0.00108585
Iteration 19/25 | Loss: 0.00108585
Iteration 20/25 | Loss: 0.00108585
Iteration 21/25 | Loss: 0.00108585
Iteration 22/25 | Loss: 0.00108585
Iteration 23/25 | Loss: 0.00108585
Iteration 24/25 | Loss: 0.00108585
Iteration 25/25 | Loss: 0.00108585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108585
Iteration 2/1000 | Loss: 0.00002146
Iteration 3/1000 | Loss: 0.00001580
Iteration 4/1000 | Loss: 0.00001426
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001182
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001161
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001153
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001139
Iteration 46/1000 | Loss: 0.00001139
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001135
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001134
Iteration 58/1000 | Loss: 0.00001133
Iteration 59/1000 | Loss: 0.00001133
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001133
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001130
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001125
Iteration 83/1000 | Loss: 0.00001125
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001123
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001122
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001122
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001120
Iteration 99/1000 | Loss: 0.00001120
Iteration 100/1000 | Loss: 0.00001120
Iteration 101/1000 | Loss: 0.00001120
Iteration 102/1000 | Loss: 0.00001120
Iteration 103/1000 | Loss: 0.00001120
Iteration 104/1000 | Loss: 0.00001120
Iteration 105/1000 | Loss: 0.00001120
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001119
Iteration 110/1000 | Loss: 0.00001119
Iteration 111/1000 | Loss: 0.00001119
Iteration 112/1000 | Loss: 0.00001119
Iteration 113/1000 | Loss: 0.00001119
Iteration 114/1000 | Loss: 0.00001119
Iteration 115/1000 | Loss: 0.00001119
Iteration 116/1000 | Loss: 0.00001118
Iteration 117/1000 | Loss: 0.00001118
Iteration 118/1000 | Loss: 0.00001118
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001117
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001116
Iteration 127/1000 | Loss: 0.00001116
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001115
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001113
Iteration 138/1000 | Loss: 0.00001113
Iteration 139/1000 | Loss: 0.00001113
Iteration 140/1000 | Loss: 0.00001113
Iteration 141/1000 | Loss: 0.00001113
Iteration 142/1000 | Loss: 0.00001113
Iteration 143/1000 | Loss: 0.00001113
Iteration 144/1000 | Loss: 0.00001113
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001112
Iteration 148/1000 | Loss: 0.00001112
Iteration 149/1000 | Loss: 0.00001112
Iteration 150/1000 | Loss: 0.00001112
Iteration 151/1000 | Loss: 0.00001112
Iteration 152/1000 | Loss: 0.00001112
Iteration 153/1000 | Loss: 0.00001112
Iteration 154/1000 | Loss: 0.00001112
Iteration 155/1000 | Loss: 0.00001112
Iteration 156/1000 | Loss: 0.00001111
Iteration 157/1000 | Loss: 0.00001111
Iteration 158/1000 | Loss: 0.00001111
Iteration 159/1000 | Loss: 0.00001111
Iteration 160/1000 | Loss: 0.00001111
Iteration 161/1000 | Loss: 0.00001111
Iteration 162/1000 | Loss: 0.00001111
Iteration 163/1000 | Loss: 0.00001111
Iteration 164/1000 | Loss: 0.00001111
Iteration 165/1000 | Loss: 0.00001111
Iteration 166/1000 | Loss: 0.00001111
Iteration 167/1000 | Loss: 0.00001111
Iteration 168/1000 | Loss: 0.00001111
Iteration 169/1000 | Loss: 0.00001111
Iteration 170/1000 | Loss: 0.00001111
Iteration 171/1000 | Loss: 0.00001111
Iteration 172/1000 | Loss: 0.00001111
Iteration 173/1000 | Loss: 0.00001111
Iteration 174/1000 | Loss: 0.00001111
Iteration 175/1000 | Loss: 0.00001110
Iteration 176/1000 | Loss: 0.00001110
Iteration 177/1000 | Loss: 0.00001110
Iteration 178/1000 | Loss: 0.00001110
Iteration 179/1000 | Loss: 0.00001110
Iteration 180/1000 | Loss: 0.00001110
Iteration 181/1000 | Loss: 0.00001110
Iteration 182/1000 | Loss: 0.00001110
Iteration 183/1000 | Loss: 0.00001110
Iteration 184/1000 | Loss: 0.00001110
Iteration 185/1000 | Loss: 0.00001110
Iteration 186/1000 | Loss: 0.00001110
Iteration 187/1000 | Loss: 0.00001110
Iteration 188/1000 | Loss: 0.00001110
Iteration 189/1000 | Loss: 0.00001110
Iteration 190/1000 | Loss: 0.00001110
Iteration 191/1000 | Loss: 0.00001110
Iteration 192/1000 | Loss: 0.00001110
Iteration 193/1000 | Loss: 0.00001110
Iteration 194/1000 | Loss: 0.00001110
Iteration 195/1000 | Loss: 0.00001110
Iteration 196/1000 | Loss: 0.00001110
Iteration 197/1000 | Loss: 0.00001109
Iteration 198/1000 | Loss: 0.00001109
Iteration 199/1000 | Loss: 0.00001109
Iteration 200/1000 | Loss: 0.00001109
Iteration 201/1000 | Loss: 0.00001109
Iteration 202/1000 | Loss: 0.00001109
Iteration 203/1000 | Loss: 0.00001109
Iteration 204/1000 | Loss: 0.00001109
Iteration 205/1000 | Loss: 0.00001109
Iteration 206/1000 | Loss: 0.00001109
Iteration 207/1000 | Loss: 0.00001109
Iteration 208/1000 | Loss: 0.00001109
Iteration 209/1000 | Loss: 0.00001108
Iteration 210/1000 | Loss: 0.00001108
Iteration 211/1000 | Loss: 0.00001108
Iteration 212/1000 | Loss: 0.00001108
Iteration 213/1000 | Loss: 0.00001108
Iteration 214/1000 | Loss: 0.00001108
Iteration 215/1000 | Loss: 0.00001108
Iteration 216/1000 | Loss: 0.00001108
Iteration 217/1000 | Loss: 0.00001108
Iteration 218/1000 | Loss: 0.00001108
Iteration 219/1000 | Loss: 0.00001108
Iteration 220/1000 | Loss: 0.00001108
Iteration 221/1000 | Loss: 0.00001108
Iteration 222/1000 | Loss: 0.00001108
Iteration 223/1000 | Loss: 0.00001108
Iteration 224/1000 | Loss: 0.00001108
Iteration 225/1000 | Loss: 0.00001108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1078703209932428e-05, 1.1078703209932428e-05, 1.1078703209932428e-05, 1.1078703209932428e-05, 1.1078703209932428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1078703209932428e-05

Optimization complete. Final v2v error: 2.8617966175079346 mm

Highest mean error: 3.1083121299743652 mm for frame 58

Lowest mean error: 2.7047715187072754 mm for frame 124

Saving results

Total time: 43.25034427642822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390556
Iteration 2/25 | Loss: 0.00129127
Iteration 3/25 | Loss: 0.00122927
Iteration 4/25 | Loss: 0.00122101
Iteration 5/25 | Loss: 0.00121835
Iteration 6/25 | Loss: 0.00121815
Iteration 7/25 | Loss: 0.00121815
Iteration 8/25 | Loss: 0.00121815
Iteration 9/25 | Loss: 0.00121815
Iteration 10/25 | Loss: 0.00121815
Iteration 11/25 | Loss: 0.00121815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012181535130366683, 0.0012181535130366683, 0.0012181535130366683, 0.0012181535130366683, 0.0012181535130366683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012181535130366683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.03865337
Iteration 2/25 | Loss: 0.00101191
Iteration 3/25 | Loss: 0.00101191
Iteration 4/25 | Loss: 0.00101191
Iteration 5/25 | Loss: 0.00101191
Iteration 6/25 | Loss: 0.00101191
Iteration 7/25 | Loss: 0.00101190
Iteration 8/25 | Loss: 0.00101190
Iteration 9/25 | Loss: 0.00101190
Iteration 10/25 | Loss: 0.00101190
Iteration 11/25 | Loss: 0.00101190
Iteration 12/25 | Loss: 0.00101190
Iteration 13/25 | Loss: 0.00101190
Iteration 14/25 | Loss: 0.00101190
Iteration 15/25 | Loss: 0.00101190
Iteration 16/25 | Loss: 0.00101190
Iteration 17/25 | Loss: 0.00101190
Iteration 18/25 | Loss: 0.00101190
Iteration 19/25 | Loss: 0.00101190
Iteration 20/25 | Loss: 0.00101190
Iteration 21/25 | Loss: 0.00101190
Iteration 22/25 | Loss: 0.00101190
Iteration 23/25 | Loss: 0.00101190
Iteration 24/25 | Loss: 0.00101190
Iteration 25/25 | Loss: 0.00101190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101190
Iteration 2/1000 | Loss: 0.00001953
Iteration 3/1000 | Loss: 0.00001450
Iteration 4/1000 | Loss: 0.00001327
Iteration 5/1000 | Loss: 0.00001259
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001183
Iteration 8/1000 | Loss: 0.00001169
Iteration 9/1000 | Loss: 0.00001168
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001100
Iteration 13/1000 | Loss: 0.00001100
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001099
Iteration 16/1000 | Loss: 0.00001089
Iteration 17/1000 | Loss: 0.00001087
Iteration 18/1000 | Loss: 0.00001087
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001082
Iteration 22/1000 | Loss: 0.00001080
Iteration 23/1000 | Loss: 0.00001073
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001069
Iteration 26/1000 | Loss: 0.00001068
Iteration 27/1000 | Loss: 0.00001067
Iteration 28/1000 | Loss: 0.00001062
Iteration 29/1000 | Loss: 0.00001062
Iteration 30/1000 | Loss: 0.00001060
Iteration 31/1000 | Loss: 0.00001058
Iteration 32/1000 | Loss: 0.00001057
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001056
Iteration 35/1000 | Loss: 0.00001056
Iteration 36/1000 | Loss: 0.00001055
Iteration 37/1000 | Loss: 0.00001055
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001054
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001036
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001035
Iteration 52/1000 | Loss: 0.00001035
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001034
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001034
Iteration 59/1000 | Loss: 0.00001034
Iteration 60/1000 | Loss: 0.00001034
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001034
Iteration 63/1000 | Loss: 0.00001034
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001033
Iteration 67/1000 | Loss: 0.00001033
Iteration 68/1000 | Loss: 0.00001033
Iteration 69/1000 | Loss: 0.00001032
Iteration 70/1000 | Loss: 0.00001032
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001032
Iteration 73/1000 | Loss: 0.00001032
Iteration 74/1000 | Loss: 0.00001032
Iteration 75/1000 | Loss: 0.00001032
Iteration 76/1000 | Loss: 0.00001031
Iteration 77/1000 | Loss: 0.00001031
Iteration 78/1000 | Loss: 0.00001031
Iteration 79/1000 | Loss: 0.00001031
Iteration 80/1000 | Loss: 0.00001031
Iteration 81/1000 | Loss: 0.00001031
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001028
Iteration 85/1000 | Loss: 0.00001027
Iteration 86/1000 | Loss: 0.00001027
Iteration 87/1000 | Loss: 0.00001027
Iteration 88/1000 | Loss: 0.00001027
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001027
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001027
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001026
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001024
Iteration 106/1000 | Loss: 0.00001024
Iteration 107/1000 | Loss: 0.00001024
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001023
Iteration 110/1000 | Loss: 0.00001023
Iteration 111/1000 | Loss: 0.00001023
Iteration 112/1000 | Loss: 0.00001022
Iteration 113/1000 | Loss: 0.00001022
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001021
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001020
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001020
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001018
Iteration 141/1000 | Loss: 0.00001018
Iteration 142/1000 | Loss: 0.00001018
Iteration 143/1000 | Loss: 0.00001018
Iteration 144/1000 | Loss: 0.00001018
Iteration 145/1000 | Loss: 0.00001018
Iteration 146/1000 | Loss: 0.00001018
Iteration 147/1000 | Loss: 0.00001018
Iteration 148/1000 | Loss: 0.00001018
Iteration 149/1000 | Loss: 0.00001018
Iteration 150/1000 | Loss: 0.00001018
Iteration 151/1000 | Loss: 0.00001018
Iteration 152/1000 | Loss: 0.00001018
Iteration 153/1000 | Loss: 0.00001018
Iteration 154/1000 | Loss: 0.00001018
Iteration 155/1000 | Loss: 0.00001018
Iteration 156/1000 | Loss: 0.00001018
Iteration 157/1000 | Loss: 0.00001018
Iteration 158/1000 | Loss: 0.00001018
Iteration 159/1000 | Loss: 0.00001018
Iteration 160/1000 | Loss: 0.00001018
Iteration 161/1000 | Loss: 0.00001018
Iteration 162/1000 | Loss: 0.00001018
Iteration 163/1000 | Loss: 0.00001018
Iteration 164/1000 | Loss: 0.00001018
Iteration 165/1000 | Loss: 0.00001018
Iteration 166/1000 | Loss: 0.00001018
Iteration 167/1000 | Loss: 0.00001018
Iteration 168/1000 | Loss: 0.00001018
Iteration 169/1000 | Loss: 0.00001018
Iteration 170/1000 | Loss: 0.00001018
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001018
Iteration 174/1000 | Loss: 0.00001018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.017969407257624e-05, 1.017969407257624e-05, 1.017969407257624e-05, 1.017969407257624e-05, 1.017969407257624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.017969407257624e-05

Optimization complete. Final v2v error: 2.772824764251709 mm

Highest mean error: 2.9428048133850098 mm for frame 116

Lowest mean error: 2.675595998764038 mm for frame 0

Saving results

Total time: 38.422483682632446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00285307
Iteration 2/25 | Loss: 0.00146511
Iteration 3/25 | Loss: 0.00130443
Iteration 4/25 | Loss: 0.00126772
Iteration 5/25 | Loss: 0.00126095
Iteration 6/25 | Loss: 0.00125952
Iteration 7/25 | Loss: 0.00125928
Iteration 8/25 | Loss: 0.00125928
Iteration 9/25 | Loss: 0.00125928
Iteration 10/25 | Loss: 0.00125928
Iteration 11/25 | Loss: 0.00125928
Iteration 12/25 | Loss: 0.00125928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012592785060405731, 0.0012592785060405731, 0.0012592785060405731, 0.0012592785060405731, 0.0012592785060405731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012592785060405731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33657146
Iteration 2/25 | Loss: 0.00131269
Iteration 3/25 | Loss: 0.00131268
Iteration 4/25 | Loss: 0.00131268
Iteration 5/25 | Loss: 0.00131268
Iteration 6/25 | Loss: 0.00131268
Iteration 7/25 | Loss: 0.00131268
Iteration 8/25 | Loss: 0.00131268
Iteration 9/25 | Loss: 0.00131268
Iteration 10/25 | Loss: 0.00131268
Iteration 11/25 | Loss: 0.00131268
Iteration 12/25 | Loss: 0.00131268
Iteration 13/25 | Loss: 0.00131268
Iteration 14/25 | Loss: 0.00131268
Iteration 15/25 | Loss: 0.00131268
Iteration 16/25 | Loss: 0.00131268
Iteration 17/25 | Loss: 0.00131268
Iteration 18/25 | Loss: 0.00131268
Iteration 19/25 | Loss: 0.00131268
Iteration 20/25 | Loss: 0.00131268
Iteration 21/25 | Loss: 0.00131268
Iteration 22/25 | Loss: 0.00131268
Iteration 23/25 | Loss: 0.00131268
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001312681706622243, 0.001312681706622243, 0.001312681706622243, 0.001312681706622243, 0.001312681706622243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001312681706622243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131268
Iteration 2/1000 | Loss: 0.00004954
Iteration 3/1000 | Loss: 0.00003123
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002127
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001827
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001731
Iteration 15/1000 | Loss: 0.00001730
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001717
Iteration 21/1000 | Loss: 0.00001716
Iteration 22/1000 | Loss: 0.00001716
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001712
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001711
Iteration 31/1000 | Loss: 0.00001711
Iteration 32/1000 | Loss: 0.00001711
Iteration 33/1000 | Loss: 0.00001710
Iteration 34/1000 | Loss: 0.00001710
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001700
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001698
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001697
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001695
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001691
Iteration 112/1000 | Loss: 0.00001691
Iteration 113/1000 | Loss: 0.00001691
Iteration 114/1000 | Loss: 0.00001691
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001690
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001689
Iteration 133/1000 | Loss: 0.00001689
Iteration 134/1000 | Loss: 0.00001689
Iteration 135/1000 | Loss: 0.00001689
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001689
Iteration 138/1000 | Loss: 0.00001689
Iteration 139/1000 | Loss: 0.00001689
Iteration 140/1000 | Loss: 0.00001689
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001688
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.6884790966287255e-05, 1.6884790966287255e-05, 1.6884790966287255e-05, 1.6884790966287255e-05, 1.6884790966287255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6884790966287255e-05

Optimization complete. Final v2v error: 3.450418472290039 mm

Highest mean error: 3.9153852462768555 mm for frame 87

Lowest mean error: 3.1406662464141846 mm for frame 1

Saving results

Total time: 39.27226686477661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910758
Iteration 2/25 | Loss: 0.00151453
Iteration 3/25 | Loss: 0.00141171
Iteration 4/25 | Loss: 0.00139941
Iteration 5/25 | Loss: 0.00139567
Iteration 6/25 | Loss: 0.00139567
Iteration 7/25 | Loss: 0.00139567
Iteration 8/25 | Loss: 0.00139567
Iteration 9/25 | Loss: 0.00139567
Iteration 10/25 | Loss: 0.00139567
Iteration 11/25 | Loss: 0.00139567
Iteration 12/25 | Loss: 0.00139567
Iteration 13/25 | Loss: 0.00139567
Iteration 14/25 | Loss: 0.00139567
Iteration 15/25 | Loss: 0.00139567
Iteration 16/25 | Loss: 0.00139567
Iteration 17/25 | Loss: 0.00139567
Iteration 18/25 | Loss: 0.00139567
Iteration 19/25 | Loss: 0.00139567
Iteration 20/25 | Loss: 0.00139567
Iteration 21/25 | Loss: 0.00139567
Iteration 22/25 | Loss: 0.00139567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013956696493551135, 0.0013956696493551135, 0.0013956696493551135, 0.0013956696493551135, 0.0013956696493551135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013956696493551135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98156786
Iteration 2/25 | Loss: 0.00119932
Iteration 3/25 | Loss: 0.00119932
Iteration 4/25 | Loss: 0.00119932
Iteration 5/25 | Loss: 0.00119932
Iteration 6/25 | Loss: 0.00119932
Iteration 7/25 | Loss: 0.00119932
Iteration 8/25 | Loss: 0.00119932
Iteration 9/25 | Loss: 0.00119932
Iteration 10/25 | Loss: 0.00119931
Iteration 11/25 | Loss: 0.00119931
Iteration 12/25 | Loss: 0.00119931
Iteration 13/25 | Loss: 0.00119931
Iteration 14/25 | Loss: 0.00119931
Iteration 15/25 | Loss: 0.00119931
Iteration 16/25 | Loss: 0.00119931
Iteration 17/25 | Loss: 0.00119932
Iteration 18/25 | Loss: 0.00119932
Iteration 19/25 | Loss: 0.00119931
Iteration 20/25 | Loss: 0.00119931
Iteration 21/25 | Loss: 0.00119931
Iteration 22/25 | Loss: 0.00119931
Iteration 23/25 | Loss: 0.00119931
Iteration 24/25 | Loss: 0.00119931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011993148364126682, 0.0011993148364126682, 0.0011993148364126682, 0.0011993148364126682, 0.0011993148364126682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011993148364126682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119931
Iteration 2/1000 | Loss: 0.00006682
Iteration 3/1000 | Loss: 0.00003805
Iteration 4/1000 | Loss: 0.00002860
Iteration 5/1000 | Loss: 0.00002653
Iteration 6/1000 | Loss: 0.00002537
Iteration 7/1000 | Loss: 0.00002482
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002390
Iteration 10/1000 | Loss: 0.00002358
Iteration 11/1000 | Loss: 0.00002327
Iteration 12/1000 | Loss: 0.00002302
Iteration 13/1000 | Loss: 0.00002283
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002265
Iteration 16/1000 | Loss: 0.00002256
Iteration 17/1000 | Loss: 0.00002255
Iteration 18/1000 | Loss: 0.00002250
Iteration 19/1000 | Loss: 0.00002250
Iteration 20/1000 | Loss: 0.00002246
Iteration 21/1000 | Loss: 0.00002245
Iteration 22/1000 | Loss: 0.00002244
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002241
Iteration 26/1000 | Loss: 0.00002241
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002237
Iteration 29/1000 | Loss: 0.00002237
Iteration 30/1000 | Loss: 0.00002236
Iteration 31/1000 | Loss: 0.00002233
Iteration 32/1000 | Loss: 0.00002233
Iteration 33/1000 | Loss: 0.00002232
Iteration 34/1000 | Loss: 0.00002230
Iteration 35/1000 | Loss: 0.00002229
Iteration 36/1000 | Loss: 0.00002229
Iteration 37/1000 | Loss: 0.00002228
Iteration 38/1000 | Loss: 0.00002227
Iteration 39/1000 | Loss: 0.00002226
Iteration 40/1000 | Loss: 0.00002226
Iteration 41/1000 | Loss: 0.00002226
Iteration 42/1000 | Loss: 0.00002226
Iteration 43/1000 | Loss: 0.00002225
Iteration 44/1000 | Loss: 0.00002225
Iteration 45/1000 | Loss: 0.00002225
Iteration 46/1000 | Loss: 0.00002223
Iteration 47/1000 | Loss: 0.00002223
Iteration 48/1000 | Loss: 0.00002223
Iteration 49/1000 | Loss: 0.00002222
Iteration 50/1000 | Loss: 0.00002222
Iteration 51/1000 | Loss: 0.00002222
Iteration 52/1000 | Loss: 0.00002222
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002217
Iteration 55/1000 | Loss: 0.00002217
Iteration 56/1000 | Loss: 0.00002215
Iteration 57/1000 | Loss: 0.00002214
Iteration 58/1000 | Loss: 0.00002214
Iteration 59/1000 | Loss: 0.00002212
Iteration 60/1000 | Loss: 0.00002212
Iteration 61/1000 | Loss: 0.00002212
Iteration 62/1000 | Loss: 0.00002212
Iteration 63/1000 | Loss: 0.00002212
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002211
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002210
Iteration 69/1000 | Loss: 0.00002210
Iteration 70/1000 | Loss: 0.00002210
Iteration 71/1000 | Loss: 0.00002210
Iteration 72/1000 | Loss: 0.00002210
Iteration 73/1000 | Loss: 0.00002209
Iteration 74/1000 | Loss: 0.00002209
Iteration 75/1000 | Loss: 0.00002209
Iteration 76/1000 | Loss: 0.00002209
Iteration 77/1000 | Loss: 0.00002209
Iteration 78/1000 | Loss: 0.00002209
Iteration 79/1000 | Loss: 0.00002209
Iteration 80/1000 | Loss: 0.00002209
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002208
Iteration 83/1000 | Loss: 0.00002208
Iteration 84/1000 | Loss: 0.00002207
Iteration 85/1000 | Loss: 0.00002207
Iteration 86/1000 | Loss: 0.00002207
Iteration 87/1000 | Loss: 0.00002207
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002206
Iteration 90/1000 | Loss: 0.00002206
Iteration 91/1000 | Loss: 0.00002206
Iteration 92/1000 | Loss: 0.00002206
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002205
Iteration 96/1000 | Loss: 0.00002205
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002204
Iteration 99/1000 | Loss: 0.00002204
Iteration 100/1000 | Loss: 0.00002204
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002203
Iteration 104/1000 | Loss: 0.00002203
Iteration 105/1000 | Loss: 0.00002203
Iteration 106/1000 | Loss: 0.00002203
Iteration 107/1000 | Loss: 0.00002203
Iteration 108/1000 | Loss: 0.00002203
Iteration 109/1000 | Loss: 0.00002202
Iteration 110/1000 | Loss: 0.00002202
Iteration 111/1000 | Loss: 0.00002202
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002201
Iteration 120/1000 | Loss: 0.00002201
Iteration 121/1000 | Loss: 0.00002201
Iteration 122/1000 | Loss: 0.00002201
Iteration 123/1000 | Loss: 0.00002201
Iteration 124/1000 | Loss: 0.00002201
Iteration 125/1000 | Loss: 0.00002201
Iteration 126/1000 | Loss: 0.00002200
Iteration 127/1000 | Loss: 0.00002200
Iteration 128/1000 | Loss: 0.00002200
Iteration 129/1000 | Loss: 0.00002200
Iteration 130/1000 | Loss: 0.00002200
Iteration 131/1000 | Loss: 0.00002200
Iteration 132/1000 | Loss: 0.00002200
Iteration 133/1000 | Loss: 0.00002200
Iteration 134/1000 | Loss: 0.00002200
Iteration 135/1000 | Loss: 0.00002199
Iteration 136/1000 | Loss: 0.00002199
Iteration 137/1000 | Loss: 0.00002199
Iteration 138/1000 | Loss: 0.00002199
Iteration 139/1000 | Loss: 0.00002199
Iteration 140/1000 | Loss: 0.00002199
Iteration 141/1000 | Loss: 0.00002199
Iteration 142/1000 | Loss: 0.00002199
Iteration 143/1000 | Loss: 0.00002198
Iteration 144/1000 | Loss: 0.00002198
Iteration 145/1000 | Loss: 0.00002198
Iteration 146/1000 | Loss: 0.00002198
Iteration 147/1000 | Loss: 0.00002198
Iteration 148/1000 | Loss: 0.00002198
Iteration 149/1000 | Loss: 0.00002198
Iteration 150/1000 | Loss: 0.00002198
Iteration 151/1000 | Loss: 0.00002198
Iteration 152/1000 | Loss: 0.00002198
Iteration 153/1000 | Loss: 0.00002198
Iteration 154/1000 | Loss: 0.00002197
Iteration 155/1000 | Loss: 0.00002197
Iteration 156/1000 | Loss: 0.00002197
Iteration 157/1000 | Loss: 0.00002197
Iteration 158/1000 | Loss: 0.00002197
Iteration 159/1000 | Loss: 0.00002197
Iteration 160/1000 | Loss: 0.00002197
Iteration 161/1000 | Loss: 0.00002197
Iteration 162/1000 | Loss: 0.00002197
Iteration 163/1000 | Loss: 0.00002197
Iteration 164/1000 | Loss: 0.00002197
Iteration 165/1000 | Loss: 0.00002197
Iteration 166/1000 | Loss: 0.00002197
Iteration 167/1000 | Loss: 0.00002196
Iteration 168/1000 | Loss: 0.00002196
Iteration 169/1000 | Loss: 0.00002196
Iteration 170/1000 | Loss: 0.00002196
Iteration 171/1000 | Loss: 0.00002196
Iteration 172/1000 | Loss: 0.00002196
Iteration 173/1000 | Loss: 0.00002196
Iteration 174/1000 | Loss: 0.00002195
Iteration 175/1000 | Loss: 0.00002195
Iteration 176/1000 | Loss: 0.00002195
Iteration 177/1000 | Loss: 0.00002195
Iteration 178/1000 | Loss: 0.00002195
Iteration 179/1000 | Loss: 0.00002195
Iteration 180/1000 | Loss: 0.00002195
Iteration 181/1000 | Loss: 0.00002195
Iteration 182/1000 | Loss: 0.00002195
Iteration 183/1000 | Loss: 0.00002195
Iteration 184/1000 | Loss: 0.00002194
Iteration 185/1000 | Loss: 0.00002194
Iteration 186/1000 | Loss: 0.00002194
Iteration 187/1000 | Loss: 0.00002194
Iteration 188/1000 | Loss: 0.00002194
Iteration 189/1000 | Loss: 0.00002194
Iteration 190/1000 | Loss: 0.00002194
Iteration 191/1000 | Loss: 0.00002194
Iteration 192/1000 | Loss: 0.00002194
Iteration 193/1000 | Loss: 0.00002194
Iteration 194/1000 | Loss: 0.00002194
Iteration 195/1000 | Loss: 0.00002194
Iteration 196/1000 | Loss: 0.00002193
Iteration 197/1000 | Loss: 0.00002193
Iteration 198/1000 | Loss: 0.00002193
Iteration 199/1000 | Loss: 0.00002193
Iteration 200/1000 | Loss: 0.00002193
Iteration 201/1000 | Loss: 0.00002193
Iteration 202/1000 | Loss: 0.00002193
Iteration 203/1000 | Loss: 0.00002193
Iteration 204/1000 | Loss: 0.00002193
Iteration 205/1000 | Loss: 0.00002192
Iteration 206/1000 | Loss: 0.00002192
Iteration 207/1000 | Loss: 0.00002192
Iteration 208/1000 | Loss: 0.00002192
Iteration 209/1000 | Loss: 0.00002192
Iteration 210/1000 | Loss: 0.00002192
Iteration 211/1000 | Loss: 0.00002192
Iteration 212/1000 | Loss: 0.00002192
Iteration 213/1000 | Loss: 0.00002191
Iteration 214/1000 | Loss: 0.00002191
Iteration 215/1000 | Loss: 0.00002191
Iteration 216/1000 | Loss: 0.00002191
Iteration 217/1000 | Loss: 0.00002191
Iteration 218/1000 | Loss: 0.00002191
Iteration 219/1000 | Loss: 0.00002190
Iteration 220/1000 | Loss: 0.00002190
Iteration 221/1000 | Loss: 0.00002190
Iteration 222/1000 | Loss: 0.00002190
Iteration 223/1000 | Loss: 0.00002190
Iteration 224/1000 | Loss: 0.00002190
Iteration 225/1000 | Loss: 0.00002190
Iteration 226/1000 | Loss: 0.00002190
Iteration 227/1000 | Loss: 0.00002190
Iteration 228/1000 | Loss: 0.00002190
Iteration 229/1000 | Loss: 0.00002190
Iteration 230/1000 | Loss: 0.00002190
Iteration 231/1000 | Loss: 0.00002190
Iteration 232/1000 | Loss: 0.00002190
Iteration 233/1000 | Loss: 0.00002190
Iteration 234/1000 | Loss: 0.00002190
Iteration 235/1000 | Loss: 0.00002190
Iteration 236/1000 | Loss: 0.00002190
Iteration 237/1000 | Loss: 0.00002190
Iteration 238/1000 | Loss: 0.00002190
Iteration 239/1000 | Loss: 0.00002190
Iteration 240/1000 | Loss: 0.00002190
Iteration 241/1000 | Loss: 0.00002190
Iteration 242/1000 | Loss: 0.00002190
Iteration 243/1000 | Loss: 0.00002190
Iteration 244/1000 | Loss: 0.00002190
Iteration 245/1000 | Loss: 0.00002190
Iteration 246/1000 | Loss: 0.00002190
Iteration 247/1000 | Loss: 0.00002190
Iteration 248/1000 | Loss: 0.00002190
Iteration 249/1000 | Loss: 0.00002190
Iteration 250/1000 | Loss: 0.00002190
Iteration 251/1000 | Loss: 0.00002190
Iteration 252/1000 | Loss: 0.00002190
Iteration 253/1000 | Loss: 0.00002190
Iteration 254/1000 | Loss: 0.00002190
Iteration 255/1000 | Loss: 0.00002190
Iteration 256/1000 | Loss: 0.00002190
Iteration 257/1000 | Loss: 0.00002190
Iteration 258/1000 | Loss: 0.00002190
Iteration 259/1000 | Loss: 0.00002190
Iteration 260/1000 | Loss: 0.00002190
Iteration 261/1000 | Loss: 0.00002190
Iteration 262/1000 | Loss: 0.00002190
Iteration 263/1000 | Loss: 0.00002190
Iteration 264/1000 | Loss: 0.00002190
Iteration 265/1000 | Loss: 0.00002190
Iteration 266/1000 | Loss: 0.00002190
Iteration 267/1000 | Loss: 0.00002190
Iteration 268/1000 | Loss: 0.00002190
Iteration 269/1000 | Loss: 0.00002190
Iteration 270/1000 | Loss: 0.00002190
Iteration 271/1000 | Loss: 0.00002190
Iteration 272/1000 | Loss: 0.00002190
Iteration 273/1000 | Loss: 0.00002190
Iteration 274/1000 | Loss: 0.00002190
Iteration 275/1000 | Loss: 0.00002190
Iteration 276/1000 | Loss: 0.00002190
Iteration 277/1000 | Loss: 0.00002190
Iteration 278/1000 | Loss: 0.00002190
Iteration 279/1000 | Loss: 0.00002190
Iteration 280/1000 | Loss: 0.00002190
Iteration 281/1000 | Loss: 0.00002190
Iteration 282/1000 | Loss: 0.00002190
Iteration 283/1000 | Loss: 0.00002190
Iteration 284/1000 | Loss: 0.00002190
Iteration 285/1000 | Loss: 0.00002190
Iteration 286/1000 | Loss: 0.00002190
Iteration 287/1000 | Loss: 0.00002190
Iteration 288/1000 | Loss: 0.00002190
Iteration 289/1000 | Loss: 0.00002190
Iteration 290/1000 | Loss: 0.00002190
Iteration 291/1000 | Loss: 0.00002190
Iteration 292/1000 | Loss: 0.00002190
Iteration 293/1000 | Loss: 0.00002190
Iteration 294/1000 | Loss: 0.00002190
Iteration 295/1000 | Loss: 0.00002190
Iteration 296/1000 | Loss: 0.00002190
Iteration 297/1000 | Loss: 0.00002190
Iteration 298/1000 | Loss: 0.00002190
Iteration 299/1000 | Loss: 0.00002190
Iteration 300/1000 | Loss: 0.00002190
Iteration 301/1000 | Loss: 0.00002190
Iteration 302/1000 | Loss: 0.00002190
Iteration 303/1000 | Loss: 0.00002190
Iteration 304/1000 | Loss: 0.00002190
Iteration 305/1000 | Loss: 0.00002190
Iteration 306/1000 | Loss: 0.00002190
Iteration 307/1000 | Loss: 0.00002190
Iteration 308/1000 | Loss: 0.00002190
Iteration 309/1000 | Loss: 0.00002190
Iteration 310/1000 | Loss: 0.00002190
Iteration 311/1000 | Loss: 0.00002190
Iteration 312/1000 | Loss: 0.00002190
Iteration 313/1000 | Loss: 0.00002190
Iteration 314/1000 | Loss: 0.00002190
Iteration 315/1000 | Loss: 0.00002190
Iteration 316/1000 | Loss: 0.00002190
Iteration 317/1000 | Loss: 0.00002190
Iteration 318/1000 | Loss: 0.00002190
Iteration 319/1000 | Loss: 0.00002190
Iteration 320/1000 | Loss: 0.00002190
Iteration 321/1000 | Loss: 0.00002190
Iteration 322/1000 | Loss: 0.00002190
Iteration 323/1000 | Loss: 0.00002190
Iteration 324/1000 | Loss: 0.00002190
Iteration 325/1000 | Loss: 0.00002190
Iteration 326/1000 | Loss: 0.00002190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 326. Stopping optimization.
Last 5 losses: [2.189523547713179e-05, 2.189523547713179e-05, 2.189523547713179e-05, 2.189523547713179e-05, 2.189523547713179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.189523547713179e-05

Optimization complete. Final v2v error: 3.862581491470337 mm

Highest mean error: 4.54938268661499 mm for frame 81

Lowest mean error: 3.378185749053955 mm for frame 0

Saving results

Total time: 50.23805046081543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814692
Iteration 2/25 | Loss: 0.00135850
Iteration 3/25 | Loss: 0.00125810
Iteration 4/25 | Loss: 0.00124959
Iteration 5/25 | Loss: 0.00124803
Iteration 6/25 | Loss: 0.00124799
Iteration 7/25 | Loss: 0.00124799
Iteration 8/25 | Loss: 0.00124799
Iteration 9/25 | Loss: 0.00124799
Iteration 10/25 | Loss: 0.00124799
Iteration 11/25 | Loss: 0.00124799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012479913420975208, 0.0012479913420975208, 0.0012479913420975208, 0.0012479913420975208, 0.0012479913420975208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012479913420975208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34051287
Iteration 2/25 | Loss: 0.00093666
Iteration 3/25 | Loss: 0.00093665
Iteration 4/25 | Loss: 0.00093665
Iteration 5/25 | Loss: 0.00093664
Iteration 6/25 | Loss: 0.00093664
Iteration 7/25 | Loss: 0.00093664
Iteration 8/25 | Loss: 0.00093664
Iteration 9/25 | Loss: 0.00093664
Iteration 10/25 | Loss: 0.00093664
Iteration 11/25 | Loss: 0.00093664
Iteration 12/25 | Loss: 0.00093664
Iteration 13/25 | Loss: 0.00093664
Iteration 14/25 | Loss: 0.00093664
Iteration 15/25 | Loss: 0.00093664
Iteration 16/25 | Loss: 0.00093664
Iteration 17/25 | Loss: 0.00093664
Iteration 18/25 | Loss: 0.00093664
Iteration 19/25 | Loss: 0.00093664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009366418817080557, 0.0009366418817080557, 0.0009366418817080557, 0.0009366418817080557, 0.0009366418817080557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009366418817080557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093664
Iteration 2/1000 | Loss: 0.00002489
Iteration 3/1000 | Loss: 0.00001796
Iteration 4/1000 | Loss: 0.00001624
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001237
Iteration 16/1000 | Loss: 0.00001237
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001184
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001184
Iteration 62/1000 | Loss: 0.00001184
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001181
Iteration 70/1000 | Loss: 0.00001181
Iteration 71/1000 | Loss: 0.00001181
Iteration 72/1000 | Loss: 0.00001181
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001178
Iteration 98/1000 | Loss: 0.00001178
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001177
Iteration 110/1000 | Loss: 0.00001177
Iteration 111/1000 | Loss: 0.00001177
Iteration 112/1000 | Loss: 0.00001177
Iteration 113/1000 | Loss: 0.00001177
Iteration 114/1000 | Loss: 0.00001177
Iteration 115/1000 | Loss: 0.00001177
Iteration 116/1000 | Loss: 0.00001176
Iteration 117/1000 | Loss: 0.00001176
Iteration 118/1000 | Loss: 0.00001176
Iteration 119/1000 | Loss: 0.00001176
Iteration 120/1000 | Loss: 0.00001176
Iteration 121/1000 | Loss: 0.00001176
Iteration 122/1000 | Loss: 0.00001176
Iteration 123/1000 | Loss: 0.00001176
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001176
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001175
Iteration 132/1000 | Loss: 0.00001175
Iteration 133/1000 | Loss: 0.00001175
Iteration 134/1000 | Loss: 0.00001175
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001175
Iteration 137/1000 | Loss: 0.00001174
Iteration 138/1000 | Loss: 0.00001174
Iteration 139/1000 | Loss: 0.00001174
Iteration 140/1000 | Loss: 0.00001174
Iteration 141/1000 | Loss: 0.00001174
Iteration 142/1000 | Loss: 0.00001174
Iteration 143/1000 | Loss: 0.00001174
Iteration 144/1000 | Loss: 0.00001174
Iteration 145/1000 | Loss: 0.00001174
Iteration 146/1000 | Loss: 0.00001174
Iteration 147/1000 | Loss: 0.00001174
Iteration 148/1000 | Loss: 0.00001174
Iteration 149/1000 | Loss: 0.00001173
Iteration 150/1000 | Loss: 0.00001173
Iteration 151/1000 | Loss: 0.00001173
Iteration 152/1000 | Loss: 0.00001173
Iteration 153/1000 | Loss: 0.00001173
Iteration 154/1000 | Loss: 0.00001173
Iteration 155/1000 | Loss: 0.00001173
Iteration 156/1000 | Loss: 0.00001173
Iteration 157/1000 | Loss: 0.00001173
Iteration 158/1000 | Loss: 0.00001173
Iteration 159/1000 | Loss: 0.00001173
Iteration 160/1000 | Loss: 0.00001173
Iteration 161/1000 | Loss: 0.00001173
Iteration 162/1000 | Loss: 0.00001173
Iteration 163/1000 | Loss: 0.00001173
Iteration 164/1000 | Loss: 0.00001172
Iteration 165/1000 | Loss: 0.00001172
Iteration 166/1000 | Loss: 0.00001172
Iteration 167/1000 | Loss: 0.00001172
Iteration 168/1000 | Loss: 0.00001172
Iteration 169/1000 | Loss: 0.00001171
Iteration 170/1000 | Loss: 0.00001171
Iteration 171/1000 | Loss: 0.00001171
Iteration 172/1000 | Loss: 0.00001171
Iteration 173/1000 | Loss: 0.00001171
Iteration 174/1000 | Loss: 0.00001171
Iteration 175/1000 | Loss: 0.00001171
Iteration 176/1000 | Loss: 0.00001171
Iteration 177/1000 | Loss: 0.00001171
Iteration 178/1000 | Loss: 0.00001170
Iteration 179/1000 | Loss: 0.00001170
Iteration 180/1000 | Loss: 0.00001170
Iteration 181/1000 | Loss: 0.00001170
Iteration 182/1000 | Loss: 0.00001170
Iteration 183/1000 | Loss: 0.00001170
Iteration 184/1000 | Loss: 0.00001170
Iteration 185/1000 | Loss: 0.00001170
Iteration 186/1000 | Loss: 0.00001170
Iteration 187/1000 | Loss: 0.00001170
Iteration 188/1000 | Loss: 0.00001170
Iteration 189/1000 | Loss: 0.00001170
Iteration 190/1000 | Loss: 0.00001170
Iteration 191/1000 | Loss: 0.00001170
Iteration 192/1000 | Loss: 0.00001170
Iteration 193/1000 | Loss: 0.00001170
Iteration 194/1000 | Loss: 0.00001170
Iteration 195/1000 | Loss: 0.00001170
Iteration 196/1000 | Loss: 0.00001170
Iteration 197/1000 | Loss: 0.00001170
Iteration 198/1000 | Loss: 0.00001170
Iteration 199/1000 | Loss: 0.00001170
Iteration 200/1000 | Loss: 0.00001170
Iteration 201/1000 | Loss: 0.00001170
Iteration 202/1000 | Loss: 0.00001170
Iteration 203/1000 | Loss: 0.00001170
Iteration 204/1000 | Loss: 0.00001170
Iteration 205/1000 | Loss: 0.00001170
Iteration 206/1000 | Loss: 0.00001170
Iteration 207/1000 | Loss: 0.00001170
Iteration 208/1000 | Loss: 0.00001170
Iteration 209/1000 | Loss: 0.00001170
Iteration 210/1000 | Loss: 0.00001170
Iteration 211/1000 | Loss: 0.00001170
Iteration 212/1000 | Loss: 0.00001170
Iteration 213/1000 | Loss: 0.00001170
Iteration 214/1000 | Loss: 0.00001170
Iteration 215/1000 | Loss: 0.00001170
Iteration 216/1000 | Loss: 0.00001170
Iteration 217/1000 | Loss: 0.00001170
Iteration 218/1000 | Loss: 0.00001170
Iteration 219/1000 | Loss: 0.00001170
Iteration 220/1000 | Loss: 0.00001170
Iteration 221/1000 | Loss: 0.00001170
Iteration 222/1000 | Loss: 0.00001170
Iteration 223/1000 | Loss: 0.00001170
Iteration 224/1000 | Loss: 0.00001170
Iteration 225/1000 | Loss: 0.00001170
Iteration 226/1000 | Loss: 0.00001170
Iteration 227/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.1696261026372667e-05, 1.1696261026372667e-05, 1.1696261026372667e-05, 1.1696261026372667e-05, 1.1696261026372667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1696261026372667e-05

Optimization complete. Final v2v error: 2.9283597469329834 mm

Highest mean error: 3.076019048690796 mm for frame 31

Lowest mean error: 2.8296496868133545 mm for frame 157

Saving results

Total time: 43.99234175682068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387382
Iteration 2/25 | Loss: 0.00131724
Iteration 3/25 | Loss: 0.00124358
Iteration 4/25 | Loss: 0.00123483
Iteration 5/25 | Loss: 0.00123206
Iteration 6/25 | Loss: 0.00123131
Iteration 7/25 | Loss: 0.00123126
Iteration 8/25 | Loss: 0.00123126
Iteration 9/25 | Loss: 0.00123126
Iteration 10/25 | Loss: 0.00123126
Iteration 11/25 | Loss: 0.00123126
Iteration 12/25 | Loss: 0.00123126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001231255941092968, 0.001231255941092968, 0.001231255941092968, 0.001231255941092968, 0.001231255941092968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001231255941092968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67206323
Iteration 2/25 | Loss: 0.00108894
Iteration 3/25 | Loss: 0.00108893
Iteration 4/25 | Loss: 0.00108893
Iteration 5/25 | Loss: 0.00108893
Iteration 6/25 | Loss: 0.00108893
Iteration 7/25 | Loss: 0.00108893
Iteration 8/25 | Loss: 0.00108893
Iteration 9/25 | Loss: 0.00108893
Iteration 10/25 | Loss: 0.00108893
Iteration 11/25 | Loss: 0.00108893
Iteration 12/25 | Loss: 0.00108893
Iteration 13/25 | Loss: 0.00108893
Iteration 14/25 | Loss: 0.00108893
Iteration 15/25 | Loss: 0.00108893
Iteration 16/25 | Loss: 0.00108893
Iteration 17/25 | Loss: 0.00108893
Iteration 18/25 | Loss: 0.00108893
Iteration 19/25 | Loss: 0.00108893
Iteration 20/25 | Loss: 0.00108893
Iteration 21/25 | Loss: 0.00108893
Iteration 22/25 | Loss: 0.00108893
Iteration 23/25 | Loss: 0.00108893
Iteration 24/25 | Loss: 0.00108893
Iteration 25/25 | Loss: 0.00108893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010889284312725067, 0.0010889284312725067, 0.0010889284312725067, 0.0010889284312725067, 0.0010889284312725067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010889284312725067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108893
Iteration 2/1000 | Loss: 0.00002392
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001243
Iteration 6/1000 | Loss: 0.00001180
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001051
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001043
Iteration 16/1000 | Loss: 0.00001037
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001036
Iteration 19/1000 | Loss: 0.00001035
Iteration 20/1000 | Loss: 0.00001035
Iteration 21/1000 | Loss: 0.00001035
Iteration 22/1000 | Loss: 0.00001034
Iteration 23/1000 | Loss: 0.00001033
Iteration 24/1000 | Loss: 0.00001031
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001028
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001022
Iteration 31/1000 | Loss: 0.00001021
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001018
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001015
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001012
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001008
Iteration 50/1000 | Loss: 0.00001008
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001008
Iteration 53/1000 | Loss: 0.00001007
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001006
Iteration 56/1000 | Loss: 0.00001006
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001003
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001002
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001001
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000999
Iteration 84/1000 | Loss: 0.00000999
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000999
Iteration 87/1000 | Loss: 0.00000999
Iteration 88/1000 | Loss: 0.00000999
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000998
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000995
Iteration 112/1000 | Loss: 0.00000995
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000994
Iteration 117/1000 | Loss: 0.00000994
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000992
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000991
Iteration 126/1000 | Loss: 0.00000991
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000991
Iteration 129/1000 | Loss: 0.00000991
Iteration 130/1000 | Loss: 0.00000991
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000989
Iteration 137/1000 | Loss: 0.00000988
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000988
Iteration 140/1000 | Loss: 0.00000988
Iteration 141/1000 | Loss: 0.00000988
Iteration 142/1000 | Loss: 0.00000988
Iteration 143/1000 | Loss: 0.00000988
Iteration 144/1000 | Loss: 0.00000988
Iteration 145/1000 | Loss: 0.00000988
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000987
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000986
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000986
Iteration 156/1000 | Loss: 0.00000986
Iteration 157/1000 | Loss: 0.00000986
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000984
Iteration 163/1000 | Loss: 0.00000984
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000984
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000984
Iteration 169/1000 | Loss: 0.00000984
Iteration 170/1000 | Loss: 0.00000984
Iteration 171/1000 | Loss: 0.00000984
Iteration 172/1000 | Loss: 0.00000983
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000983
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000983
Iteration 182/1000 | Loss: 0.00000983
Iteration 183/1000 | Loss: 0.00000983
Iteration 184/1000 | Loss: 0.00000983
Iteration 185/1000 | Loss: 0.00000983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [9.83405152510386e-06, 9.83405152510386e-06, 9.83405152510386e-06, 9.83405152510386e-06, 9.83405152510386e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.83405152510386e-06

Optimization complete. Final v2v error: 2.7007384300231934 mm

Highest mean error: 3.13612961769104 mm for frame 66

Lowest mean error: 2.5987250804901123 mm for frame 107

Saving results

Total time: 38.49376702308655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826229
Iteration 2/25 | Loss: 0.00144494
Iteration 3/25 | Loss: 0.00126186
Iteration 4/25 | Loss: 0.00124386
Iteration 5/25 | Loss: 0.00123912
Iteration 6/25 | Loss: 0.00123871
Iteration 7/25 | Loss: 0.00123871
Iteration 8/25 | Loss: 0.00123871
Iteration 9/25 | Loss: 0.00123871
Iteration 10/25 | Loss: 0.00123871
Iteration 11/25 | Loss: 0.00123871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001238707802258432, 0.001238707802258432, 0.001238707802258432, 0.001238707802258432, 0.001238707802258432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238707802258432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82103825
Iteration 2/25 | Loss: 0.00105909
Iteration 3/25 | Loss: 0.00105909
Iteration 4/25 | Loss: 0.00105909
Iteration 5/25 | Loss: 0.00105909
Iteration 6/25 | Loss: 0.00105909
Iteration 7/25 | Loss: 0.00105909
Iteration 8/25 | Loss: 0.00105909
Iteration 9/25 | Loss: 0.00105909
Iteration 10/25 | Loss: 0.00105909
Iteration 11/25 | Loss: 0.00105909
Iteration 12/25 | Loss: 0.00105909
Iteration 13/25 | Loss: 0.00105909
Iteration 14/25 | Loss: 0.00105909
Iteration 15/25 | Loss: 0.00105909
Iteration 16/25 | Loss: 0.00105909
Iteration 17/25 | Loss: 0.00105909
Iteration 18/25 | Loss: 0.00105909
Iteration 19/25 | Loss: 0.00105909
Iteration 20/25 | Loss: 0.00105909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010590885067358613, 0.0010590885067358613, 0.0010590885067358613, 0.0010590885067358613, 0.0010590885067358613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010590885067358613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105909
Iteration 2/1000 | Loss: 0.00002266
Iteration 3/1000 | Loss: 0.00001691
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001364
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001328
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001279
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001225
Iteration 66/1000 | Loss: 0.00001225
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001224
Iteration 69/1000 | Loss: 0.00001224
Iteration 70/1000 | Loss: 0.00001224
Iteration 71/1000 | Loss: 0.00001224
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001219
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001213
Iteration 109/1000 | Loss: 0.00001213
Iteration 110/1000 | Loss: 0.00001213
Iteration 111/1000 | Loss: 0.00001213
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001213
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001212
Iteration 118/1000 | Loss: 0.00001212
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001212
Iteration 121/1000 | Loss: 0.00001212
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001211
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001211
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.2106323993066326e-05, 1.2106323993066326e-05, 1.2106323993066326e-05, 1.2106323993066326e-05, 1.2106323993066326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2106323993066326e-05

Optimization complete. Final v2v error: 3.000073194503784 mm

Highest mean error: 3.252171039581299 mm for frame 110

Lowest mean error: 2.7266685962677 mm for frame 1

Saving results

Total time: 42.44117259979248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894359
Iteration 2/25 | Loss: 0.00169138
Iteration 3/25 | Loss: 0.00139824
Iteration 4/25 | Loss: 0.00137413
Iteration 5/25 | Loss: 0.00136715
Iteration 6/25 | Loss: 0.00136513
Iteration 7/25 | Loss: 0.00136500
Iteration 8/25 | Loss: 0.00136500
Iteration 9/25 | Loss: 0.00136500
Iteration 10/25 | Loss: 0.00136500
Iteration 11/25 | Loss: 0.00136500
Iteration 12/25 | Loss: 0.00136500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001364999101497233, 0.001364999101497233, 0.001364999101497233, 0.001364999101497233, 0.001364999101497233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001364999101497233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07307279
Iteration 2/25 | Loss: 0.00098567
Iteration 3/25 | Loss: 0.00098565
Iteration 4/25 | Loss: 0.00098565
Iteration 5/25 | Loss: 0.00098565
Iteration 6/25 | Loss: 0.00098565
Iteration 7/25 | Loss: 0.00098565
Iteration 8/25 | Loss: 0.00098565
Iteration 9/25 | Loss: 0.00098565
Iteration 10/25 | Loss: 0.00098565
Iteration 11/25 | Loss: 0.00098565
Iteration 12/25 | Loss: 0.00098565
Iteration 13/25 | Loss: 0.00098565
Iteration 14/25 | Loss: 0.00098565
Iteration 15/25 | Loss: 0.00098565
Iteration 16/25 | Loss: 0.00098565
Iteration 17/25 | Loss: 0.00098565
Iteration 18/25 | Loss: 0.00098565
Iteration 19/25 | Loss: 0.00098565
Iteration 20/25 | Loss: 0.00098565
Iteration 21/25 | Loss: 0.00098565
Iteration 22/25 | Loss: 0.00098565
Iteration 23/25 | Loss: 0.00098565
Iteration 24/25 | Loss: 0.00098565
Iteration 25/25 | Loss: 0.00098565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098565
Iteration 2/1000 | Loss: 0.00004735
Iteration 3/1000 | Loss: 0.00003602
Iteration 4/1000 | Loss: 0.00002999
Iteration 5/1000 | Loss: 0.00002820
Iteration 6/1000 | Loss: 0.00002710
Iteration 7/1000 | Loss: 0.00002632
Iteration 8/1000 | Loss: 0.00002558
Iteration 9/1000 | Loss: 0.00002504
Iteration 10/1000 | Loss: 0.00002473
Iteration 11/1000 | Loss: 0.00002442
Iteration 12/1000 | Loss: 0.00002414
Iteration 13/1000 | Loss: 0.00002392
Iteration 14/1000 | Loss: 0.00002372
Iteration 15/1000 | Loss: 0.00002352
Iteration 16/1000 | Loss: 0.00002330
Iteration 17/1000 | Loss: 0.00002314
Iteration 18/1000 | Loss: 0.00002296
Iteration 19/1000 | Loss: 0.00002288
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002277
Iteration 22/1000 | Loss: 0.00002277
Iteration 23/1000 | Loss: 0.00002277
Iteration 24/1000 | Loss: 0.00002275
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002271
Iteration 29/1000 | Loss: 0.00002265
Iteration 30/1000 | Loss: 0.00002261
Iteration 31/1000 | Loss: 0.00002260
Iteration 32/1000 | Loss: 0.00002260
Iteration 33/1000 | Loss: 0.00002259
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00002257
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002255
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002255
Iteration 44/1000 | Loss: 0.00002255
Iteration 45/1000 | Loss: 0.00002255
Iteration 46/1000 | Loss: 0.00002255
Iteration 47/1000 | Loss: 0.00002255
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002255
Iteration 51/1000 | Loss: 0.00002255
Iteration 52/1000 | Loss: 0.00002255
Iteration 53/1000 | Loss: 0.00002254
Iteration 54/1000 | Loss: 0.00002254
Iteration 55/1000 | Loss: 0.00002254
Iteration 56/1000 | Loss: 0.00002254
Iteration 57/1000 | Loss: 0.00002253
Iteration 58/1000 | Loss: 0.00002253
Iteration 59/1000 | Loss: 0.00002253
Iteration 60/1000 | Loss: 0.00002253
Iteration 61/1000 | Loss: 0.00002252
Iteration 62/1000 | Loss: 0.00002252
Iteration 63/1000 | Loss: 0.00002252
Iteration 64/1000 | Loss: 0.00002252
Iteration 65/1000 | Loss: 0.00002252
Iteration 66/1000 | Loss: 0.00002252
Iteration 67/1000 | Loss: 0.00002252
Iteration 68/1000 | Loss: 0.00002252
Iteration 69/1000 | Loss: 0.00002251
Iteration 70/1000 | Loss: 0.00002251
Iteration 71/1000 | Loss: 0.00002251
Iteration 72/1000 | Loss: 0.00002251
Iteration 73/1000 | Loss: 0.00002251
Iteration 74/1000 | Loss: 0.00002250
Iteration 75/1000 | Loss: 0.00002250
Iteration 76/1000 | Loss: 0.00002250
Iteration 77/1000 | Loss: 0.00002250
Iteration 78/1000 | Loss: 0.00002250
Iteration 79/1000 | Loss: 0.00002250
Iteration 80/1000 | Loss: 0.00002250
Iteration 81/1000 | Loss: 0.00002250
Iteration 82/1000 | Loss: 0.00002250
Iteration 83/1000 | Loss: 0.00002250
Iteration 84/1000 | Loss: 0.00002250
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002250
Iteration 87/1000 | Loss: 0.00002250
Iteration 88/1000 | Loss: 0.00002250
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002250
Iteration 91/1000 | Loss: 0.00002250
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002250
Iteration 94/1000 | Loss: 0.00002250
Iteration 95/1000 | Loss: 0.00002250
Iteration 96/1000 | Loss: 0.00002250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.2497502868645824e-05, 2.2497502868645824e-05, 2.2497502868645824e-05, 2.2497502868645824e-05, 2.2497502868645824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2497502868645824e-05

Optimization complete. Final v2v error: 3.9396843910217285 mm

Highest mean error: 5.193874835968018 mm for frame 103

Lowest mean error: 3.15783953666687 mm for frame 122

Saving results

Total time: 44.59164237976074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764136
Iteration 2/25 | Loss: 0.00155696
Iteration 3/25 | Loss: 0.00135960
Iteration 4/25 | Loss: 0.00133728
Iteration 5/25 | Loss: 0.00133234
Iteration 6/25 | Loss: 0.00133217
Iteration 7/25 | Loss: 0.00133217
Iteration 8/25 | Loss: 0.00133217
Iteration 9/25 | Loss: 0.00133217
Iteration 10/25 | Loss: 0.00133217
Iteration 11/25 | Loss: 0.00133217
Iteration 12/25 | Loss: 0.00133217
Iteration 13/25 | Loss: 0.00133217
Iteration 14/25 | Loss: 0.00133217
Iteration 15/25 | Loss: 0.00133217
Iteration 16/25 | Loss: 0.00133217
Iteration 17/25 | Loss: 0.00133217
Iteration 18/25 | Loss: 0.00133217
Iteration 19/25 | Loss: 0.00133217
Iteration 20/25 | Loss: 0.00133217
Iteration 21/25 | Loss: 0.00133217
Iteration 22/25 | Loss: 0.00133217
Iteration 23/25 | Loss: 0.00133217
Iteration 24/25 | Loss: 0.00133217
Iteration 25/25 | Loss: 0.00133217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15071106
Iteration 2/25 | Loss: 0.00100097
Iteration 3/25 | Loss: 0.00100096
Iteration 4/25 | Loss: 0.00100096
Iteration 5/25 | Loss: 0.00100096
Iteration 6/25 | Loss: 0.00100096
Iteration 7/25 | Loss: 0.00100096
Iteration 8/25 | Loss: 0.00100096
Iteration 9/25 | Loss: 0.00100096
Iteration 10/25 | Loss: 0.00100096
Iteration 11/25 | Loss: 0.00100096
Iteration 12/25 | Loss: 0.00100096
Iteration 13/25 | Loss: 0.00100096
Iteration 14/25 | Loss: 0.00100096
Iteration 15/25 | Loss: 0.00100096
Iteration 16/25 | Loss: 0.00100096
Iteration 17/25 | Loss: 0.00100096
Iteration 18/25 | Loss: 0.00100096
Iteration 19/25 | Loss: 0.00100096
Iteration 20/25 | Loss: 0.00100096
Iteration 21/25 | Loss: 0.00100096
Iteration 22/25 | Loss: 0.00100096
Iteration 23/25 | Loss: 0.00100096
Iteration 24/25 | Loss: 0.00100096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001000962103717029, 0.001000962103717029, 0.001000962103717029, 0.001000962103717029, 0.001000962103717029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001000962103717029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100096
Iteration 2/1000 | Loss: 0.00004538
Iteration 3/1000 | Loss: 0.00002792
Iteration 4/1000 | Loss: 0.00002242
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001976
Iteration 8/1000 | Loss: 0.00001937
Iteration 9/1000 | Loss: 0.00001909
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001864
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001841
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001823
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001820
Iteration 18/1000 | Loss: 0.00001820
Iteration 19/1000 | Loss: 0.00001819
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001815
Iteration 25/1000 | Loss: 0.00001814
Iteration 26/1000 | Loss: 0.00001814
Iteration 27/1000 | Loss: 0.00001814
Iteration 28/1000 | Loss: 0.00001814
Iteration 29/1000 | Loss: 0.00001813
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001808
Iteration 37/1000 | Loss: 0.00001803
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001797
Iteration 42/1000 | Loss: 0.00001796
Iteration 43/1000 | Loss: 0.00001796
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001793
Iteration 48/1000 | Loss: 0.00001792
Iteration 49/1000 | Loss: 0.00001792
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001791
Iteration 52/1000 | Loss: 0.00001791
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001788
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001788
Iteration 65/1000 | Loss: 0.00001788
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001786
Iteration 71/1000 | Loss: 0.00001786
Iteration 72/1000 | Loss: 0.00001786
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001785
Iteration 75/1000 | Loss: 0.00001785
Iteration 76/1000 | Loss: 0.00001785
Iteration 77/1000 | Loss: 0.00001785
Iteration 78/1000 | Loss: 0.00001785
Iteration 79/1000 | Loss: 0.00001785
Iteration 80/1000 | Loss: 0.00001785
Iteration 81/1000 | Loss: 0.00001785
Iteration 82/1000 | Loss: 0.00001784
Iteration 83/1000 | Loss: 0.00001784
Iteration 84/1000 | Loss: 0.00001784
Iteration 85/1000 | Loss: 0.00001784
Iteration 86/1000 | Loss: 0.00001784
Iteration 87/1000 | Loss: 0.00001783
Iteration 88/1000 | Loss: 0.00001783
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001781
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001780
Iteration 95/1000 | Loss: 0.00001780
Iteration 96/1000 | Loss: 0.00001780
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001779
Iteration 99/1000 | Loss: 0.00001779
Iteration 100/1000 | Loss: 0.00001779
Iteration 101/1000 | Loss: 0.00001779
Iteration 102/1000 | Loss: 0.00001779
Iteration 103/1000 | Loss: 0.00001779
Iteration 104/1000 | Loss: 0.00001779
Iteration 105/1000 | Loss: 0.00001779
Iteration 106/1000 | Loss: 0.00001779
Iteration 107/1000 | Loss: 0.00001779
Iteration 108/1000 | Loss: 0.00001779
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001778
Iteration 112/1000 | Loss: 0.00001778
Iteration 113/1000 | Loss: 0.00001777
Iteration 114/1000 | Loss: 0.00001777
Iteration 115/1000 | Loss: 0.00001777
Iteration 116/1000 | Loss: 0.00001776
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001776
Iteration 119/1000 | Loss: 0.00001776
Iteration 120/1000 | Loss: 0.00001776
Iteration 121/1000 | Loss: 0.00001776
Iteration 122/1000 | Loss: 0.00001775
Iteration 123/1000 | Loss: 0.00001775
Iteration 124/1000 | Loss: 0.00001775
Iteration 125/1000 | Loss: 0.00001774
Iteration 126/1000 | Loss: 0.00001774
Iteration 127/1000 | Loss: 0.00001774
Iteration 128/1000 | Loss: 0.00001773
Iteration 129/1000 | Loss: 0.00001773
Iteration 130/1000 | Loss: 0.00001773
Iteration 131/1000 | Loss: 0.00001773
Iteration 132/1000 | Loss: 0.00001772
Iteration 133/1000 | Loss: 0.00001772
Iteration 134/1000 | Loss: 0.00001772
Iteration 135/1000 | Loss: 0.00001772
Iteration 136/1000 | Loss: 0.00001772
Iteration 137/1000 | Loss: 0.00001772
Iteration 138/1000 | Loss: 0.00001772
Iteration 139/1000 | Loss: 0.00001771
Iteration 140/1000 | Loss: 0.00001771
Iteration 141/1000 | Loss: 0.00001771
Iteration 142/1000 | Loss: 0.00001771
Iteration 143/1000 | Loss: 0.00001771
Iteration 144/1000 | Loss: 0.00001770
Iteration 145/1000 | Loss: 0.00001770
Iteration 146/1000 | Loss: 0.00001770
Iteration 147/1000 | Loss: 0.00001769
Iteration 148/1000 | Loss: 0.00001769
Iteration 149/1000 | Loss: 0.00001769
Iteration 150/1000 | Loss: 0.00001769
Iteration 151/1000 | Loss: 0.00001769
Iteration 152/1000 | Loss: 0.00001768
Iteration 153/1000 | Loss: 0.00001768
Iteration 154/1000 | Loss: 0.00001767
Iteration 155/1000 | Loss: 0.00001767
Iteration 156/1000 | Loss: 0.00001767
Iteration 157/1000 | Loss: 0.00001767
Iteration 158/1000 | Loss: 0.00001766
Iteration 159/1000 | Loss: 0.00001766
Iteration 160/1000 | Loss: 0.00001766
Iteration 161/1000 | Loss: 0.00001766
Iteration 162/1000 | Loss: 0.00001766
Iteration 163/1000 | Loss: 0.00001766
Iteration 164/1000 | Loss: 0.00001766
Iteration 165/1000 | Loss: 0.00001766
Iteration 166/1000 | Loss: 0.00001766
Iteration 167/1000 | Loss: 0.00001766
Iteration 168/1000 | Loss: 0.00001765
Iteration 169/1000 | Loss: 0.00001765
Iteration 170/1000 | Loss: 0.00001765
Iteration 171/1000 | Loss: 0.00001765
Iteration 172/1000 | Loss: 0.00001765
Iteration 173/1000 | Loss: 0.00001765
Iteration 174/1000 | Loss: 0.00001764
Iteration 175/1000 | Loss: 0.00001764
Iteration 176/1000 | Loss: 0.00001763
Iteration 177/1000 | Loss: 0.00001763
Iteration 178/1000 | Loss: 0.00001763
Iteration 179/1000 | Loss: 0.00001763
Iteration 180/1000 | Loss: 0.00001763
Iteration 181/1000 | Loss: 0.00001763
Iteration 182/1000 | Loss: 0.00001763
Iteration 183/1000 | Loss: 0.00001762
Iteration 184/1000 | Loss: 0.00001762
Iteration 185/1000 | Loss: 0.00001762
Iteration 186/1000 | Loss: 0.00001762
Iteration 187/1000 | Loss: 0.00001761
Iteration 188/1000 | Loss: 0.00001761
Iteration 189/1000 | Loss: 0.00001760
Iteration 190/1000 | Loss: 0.00001760
Iteration 191/1000 | Loss: 0.00001760
Iteration 192/1000 | Loss: 0.00001760
Iteration 193/1000 | Loss: 0.00001760
Iteration 194/1000 | Loss: 0.00001760
Iteration 195/1000 | Loss: 0.00001760
Iteration 196/1000 | Loss: 0.00001760
Iteration 197/1000 | Loss: 0.00001760
Iteration 198/1000 | Loss: 0.00001759
Iteration 199/1000 | Loss: 0.00001759
Iteration 200/1000 | Loss: 0.00001759
Iteration 201/1000 | Loss: 0.00001759
Iteration 202/1000 | Loss: 0.00001759
Iteration 203/1000 | Loss: 0.00001759
Iteration 204/1000 | Loss: 0.00001759
Iteration 205/1000 | Loss: 0.00001759
Iteration 206/1000 | Loss: 0.00001759
Iteration 207/1000 | Loss: 0.00001759
Iteration 208/1000 | Loss: 0.00001759
Iteration 209/1000 | Loss: 0.00001759
Iteration 210/1000 | Loss: 0.00001759
Iteration 211/1000 | Loss: 0.00001759
Iteration 212/1000 | Loss: 0.00001758
Iteration 213/1000 | Loss: 0.00001758
Iteration 214/1000 | Loss: 0.00001758
Iteration 215/1000 | Loss: 0.00001758
Iteration 216/1000 | Loss: 0.00001758
Iteration 217/1000 | Loss: 0.00001758
Iteration 218/1000 | Loss: 0.00001758
Iteration 219/1000 | Loss: 0.00001758
Iteration 220/1000 | Loss: 0.00001758
Iteration 221/1000 | Loss: 0.00001758
Iteration 222/1000 | Loss: 0.00001758
Iteration 223/1000 | Loss: 0.00001758
Iteration 224/1000 | Loss: 0.00001758
Iteration 225/1000 | Loss: 0.00001758
Iteration 226/1000 | Loss: 0.00001758
Iteration 227/1000 | Loss: 0.00001758
Iteration 228/1000 | Loss: 0.00001758
Iteration 229/1000 | Loss: 0.00001758
Iteration 230/1000 | Loss: 0.00001758
Iteration 231/1000 | Loss: 0.00001758
Iteration 232/1000 | Loss: 0.00001758
Iteration 233/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.757886820996646e-05, 1.757886820996646e-05, 1.757886820996646e-05, 1.757886820996646e-05, 1.757886820996646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.757886820996646e-05

Optimization complete. Final v2v error: 3.5160961151123047 mm

Highest mean error: 4.546940803527832 mm for frame 210

Lowest mean error: 2.935621976852417 mm for frame 144

Saving results

Total time: 51.47238826751709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489428
Iteration 2/25 | Loss: 0.00139221
Iteration 3/25 | Loss: 0.00130798
Iteration 4/25 | Loss: 0.00129428
Iteration 5/25 | Loss: 0.00128962
Iteration 6/25 | Loss: 0.00128942
Iteration 7/25 | Loss: 0.00128942
Iteration 8/25 | Loss: 0.00128942
Iteration 9/25 | Loss: 0.00128942
Iteration 10/25 | Loss: 0.00128942
Iteration 11/25 | Loss: 0.00128906
Iteration 12/25 | Loss: 0.00128906
Iteration 13/25 | Loss: 0.00128906
Iteration 14/25 | Loss: 0.00128906
Iteration 15/25 | Loss: 0.00128906
Iteration 16/25 | Loss: 0.00128906
Iteration 17/25 | Loss: 0.00128906
Iteration 18/25 | Loss: 0.00128906
Iteration 19/25 | Loss: 0.00128906
Iteration 20/25 | Loss: 0.00128906
Iteration 21/25 | Loss: 0.00128906
Iteration 22/25 | Loss: 0.00128906
Iteration 23/25 | Loss: 0.00128906
Iteration 24/25 | Loss: 0.00128906
Iteration 25/25 | Loss: 0.00128906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53519511
Iteration 2/25 | Loss: 0.00110368
Iteration 3/25 | Loss: 0.00110367
Iteration 4/25 | Loss: 0.00110367
Iteration 5/25 | Loss: 0.00110367
Iteration 6/25 | Loss: 0.00110366
Iteration 7/25 | Loss: 0.00110366
Iteration 8/25 | Loss: 0.00110366
Iteration 9/25 | Loss: 0.00110366
Iteration 10/25 | Loss: 0.00110366
Iteration 11/25 | Loss: 0.00110366
Iteration 12/25 | Loss: 0.00110366
Iteration 13/25 | Loss: 0.00110366
Iteration 14/25 | Loss: 0.00110366
Iteration 15/25 | Loss: 0.00110366
Iteration 16/25 | Loss: 0.00110366
Iteration 17/25 | Loss: 0.00110366
Iteration 18/25 | Loss: 0.00110366
Iteration 19/25 | Loss: 0.00110366
Iteration 20/25 | Loss: 0.00110366
Iteration 21/25 | Loss: 0.00110366
Iteration 22/25 | Loss: 0.00110366
Iteration 23/25 | Loss: 0.00110366
Iteration 24/25 | Loss: 0.00110366
Iteration 25/25 | Loss: 0.00110366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110366
Iteration 2/1000 | Loss: 0.00003535
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00002021
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00001665
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001635
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001616
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001609
Iteration 22/1000 | Loss: 0.00001608
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00001604
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001589
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001587
Iteration 48/1000 | Loss: 0.00001587
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001586
Iteration 52/1000 | Loss: 0.00001586
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001585
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001581
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001580
Iteration 64/1000 | Loss: 0.00001580
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001579
Iteration 67/1000 | Loss: 0.00001579
Iteration 68/1000 | Loss: 0.00001579
Iteration 69/1000 | Loss: 0.00001579
Iteration 70/1000 | Loss: 0.00001579
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001578
Iteration 73/1000 | Loss: 0.00001578
Iteration 74/1000 | Loss: 0.00001577
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00001576
Iteration 77/1000 | Loss: 0.00001575
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001570
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001569
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001568
Iteration 87/1000 | Loss: 0.00001567
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001566
Iteration 95/1000 | Loss: 0.00001565
Iteration 96/1000 | Loss: 0.00001565
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001565
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Iteration 108/1000 | Loss: 0.00001563
Iteration 109/1000 | Loss: 0.00001563
Iteration 110/1000 | Loss: 0.00001563
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001563
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001561
Iteration 117/1000 | Loss: 0.00001561
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001561
Iteration 121/1000 | Loss: 0.00001561
Iteration 122/1000 | Loss: 0.00001560
Iteration 123/1000 | Loss: 0.00001560
Iteration 124/1000 | Loss: 0.00001560
Iteration 125/1000 | Loss: 0.00001560
Iteration 126/1000 | Loss: 0.00001560
Iteration 127/1000 | Loss: 0.00001560
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001559
Iteration 136/1000 | Loss: 0.00001559
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001559
Iteration 140/1000 | Loss: 0.00001559
Iteration 141/1000 | Loss: 0.00001559
Iteration 142/1000 | Loss: 0.00001559
Iteration 143/1000 | Loss: 0.00001559
Iteration 144/1000 | Loss: 0.00001559
Iteration 145/1000 | Loss: 0.00001559
Iteration 146/1000 | Loss: 0.00001559
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001558
Iteration 150/1000 | Loss: 0.00001558
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001558
Iteration 153/1000 | Loss: 0.00001558
Iteration 154/1000 | Loss: 0.00001558
Iteration 155/1000 | Loss: 0.00001558
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001557
Iteration 158/1000 | Loss: 0.00001557
Iteration 159/1000 | Loss: 0.00001557
Iteration 160/1000 | Loss: 0.00001557
Iteration 161/1000 | Loss: 0.00001557
Iteration 162/1000 | Loss: 0.00001557
Iteration 163/1000 | Loss: 0.00001557
Iteration 164/1000 | Loss: 0.00001557
Iteration 165/1000 | Loss: 0.00001557
Iteration 166/1000 | Loss: 0.00001557
Iteration 167/1000 | Loss: 0.00001557
Iteration 168/1000 | Loss: 0.00001557
Iteration 169/1000 | Loss: 0.00001557
Iteration 170/1000 | Loss: 0.00001557
Iteration 171/1000 | Loss: 0.00001557
Iteration 172/1000 | Loss: 0.00001557
Iteration 173/1000 | Loss: 0.00001557
Iteration 174/1000 | Loss: 0.00001557
Iteration 175/1000 | Loss: 0.00001557
Iteration 176/1000 | Loss: 0.00001557
Iteration 177/1000 | Loss: 0.00001557
Iteration 178/1000 | Loss: 0.00001557
Iteration 179/1000 | Loss: 0.00001557
Iteration 180/1000 | Loss: 0.00001557
Iteration 181/1000 | Loss: 0.00001557
Iteration 182/1000 | Loss: 0.00001557
Iteration 183/1000 | Loss: 0.00001557
Iteration 184/1000 | Loss: 0.00001557
Iteration 185/1000 | Loss: 0.00001557
Iteration 186/1000 | Loss: 0.00001557
Iteration 187/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.5568051821901463e-05, 1.5568051821901463e-05, 1.5568051821901463e-05, 1.5568051821901463e-05, 1.5568051821901463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5568051821901463e-05

Optimization complete. Final v2v error: 3.3765571117401123 mm

Highest mean error: 4.112241268157959 mm for frame 228

Lowest mean error: 3.082064151763916 mm for frame 191

Saving results

Total time: 49.18228578567505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777677
Iteration 2/25 | Loss: 0.00144424
Iteration 3/25 | Loss: 0.00136504
Iteration 4/25 | Loss: 0.00135424
Iteration 5/25 | Loss: 0.00135211
Iteration 6/25 | Loss: 0.00135211
Iteration 7/25 | Loss: 0.00135211
Iteration 8/25 | Loss: 0.00135211
Iteration 9/25 | Loss: 0.00135211
Iteration 10/25 | Loss: 0.00135211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001352105988189578, 0.001352105988189578, 0.001352105988189578, 0.001352105988189578, 0.001352105988189578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001352105988189578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.57238770
Iteration 2/25 | Loss: 0.00093129
Iteration 3/25 | Loss: 0.00093129
Iteration 4/25 | Loss: 0.00093128
Iteration 5/25 | Loss: 0.00093128
Iteration 6/25 | Loss: 0.00093128
Iteration 7/25 | Loss: 0.00093128
Iteration 8/25 | Loss: 0.00093128
Iteration 9/25 | Loss: 0.00093128
Iteration 10/25 | Loss: 0.00093128
Iteration 11/25 | Loss: 0.00093128
Iteration 12/25 | Loss: 0.00093128
Iteration 13/25 | Loss: 0.00093128
Iteration 14/25 | Loss: 0.00093128
Iteration 15/25 | Loss: 0.00093128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009312789770774543, 0.0009312789770774543, 0.0009312789770774543, 0.0009312789770774543, 0.0009312789770774543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009312789770774543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093128
Iteration 2/1000 | Loss: 0.00003642
Iteration 3/1000 | Loss: 0.00002717
Iteration 4/1000 | Loss: 0.00002473
Iteration 5/1000 | Loss: 0.00002375
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002188
Iteration 9/1000 | Loss: 0.00002149
Iteration 10/1000 | Loss: 0.00002109
Iteration 11/1000 | Loss: 0.00002086
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00002027
Iteration 15/1000 | Loss: 0.00002027
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002019
Iteration 19/1000 | Loss: 0.00002012
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002012
Iteration 22/1000 | Loss: 0.00002011
Iteration 23/1000 | Loss: 0.00002011
Iteration 24/1000 | Loss: 0.00002011
Iteration 25/1000 | Loss: 0.00002010
Iteration 26/1000 | Loss: 0.00002010
Iteration 27/1000 | Loss: 0.00002009
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00002008
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002007
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00002005
Iteration 37/1000 | Loss: 0.00002004
Iteration 38/1000 | Loss: 0.00002004
Iteration 39/1000 | Loss: 0.00002004
Iteration 40/1000 | Loss: 0.00002004
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00002003
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00002001
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002001
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00002000
Iteration 55/1000 | Loss: 0.00002000
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001996
Iteration 62/1000 | Loss: 0.00001996
Iteration 63/1000 | Loss: 0.00001995
Iteration 64/1000 | Loss: 0.00001994
Iteration 65/1000 | Loss: 0.00001994
Iteration 66/1000 | Loss: 0.00001991
Iteration 67/1000 | Loss: 0.00001991
Iteration 68/1000 | Loss: 0.00001991
Iteration 69/1000 | Loss: 0.00001991
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001990
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001989
Iteration 79/1000 | Loss: 0.00001989
Iteration 80/1000 | Loss: 0.00001988
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001988
Iteration 90/1000 | Loss: 0.00001987
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001986
Iteration 94/1000 | Loss: 0.00001986
Iteration 95/1000 | Loss: 0.00001986
Iteration 96/1000 | Loss: 0.00001986
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00001985
Iteration 99/1000 | Loss: 0.00001985
Iteration 100/1000 | Loss: 0.00001985
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001981
Iteration 107/1000 | Loss: 0.00001980
Iteration 108/1000 | Loss: 0.00001980
Iteration 109/1000 | Loss: 0.00001980
Iteration 110/1000 | Loss: 0.00001979
Iteration 111/1000 | Loss: 0.00001979
Iteration 112/1000 | Loss: 0.00001979
Iteration 113/1000 | Loss: 0.00001978
Iteration 114/1000 | Loss: 0.00001978
Iteration 115/1000 | Loss: 0.00001978
Iteration 116/1000 | Loss: 0.00001978
Iteration 117/1000 | Loss: 0.00001978
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001977
Iteration 124/1000 | Loss: 0.00001977
Iteration 125/1000 | Loss: 0.00001976
Iteration 126/1000 | Loss: 0.00001976
Iteration 127/1000 | Loss: 0.00001976
Iteration 128/1000 | Loss: 0.00001976
Iteration 129/1000 | Loss: 0.00001976
Iteration 130/1000 | Loss: 0.00001976
Iteration 131/1000 | Loss: 0.00001976
Iteration 132/1000 | Loss: 0.00001976
Iteration 133/1000 | Loss: 0.00001976
Iteration 134/1000 | Loss: 0.00001976
Iteration 135/1000 | Loss: 0.00001976
Iteration 136/1000 | Loss: 0.00001976
Iteration 137/1000 | Loss: 0.00001975
Iteration 138/1000 | Loss: 0.00001975
Iteration 139/1000 | Loss: 0.00001975
Iteration 140/1000 | Loss: 0.00001975
Iteration 141/1000 | Loss: 0.00001974
Iteration 142/1000 | Loss: 0.00001974
Iteration 143/1000 | Loss: 0.00001974
Iteration 144/1000 | Loss: 0.00001974
Iteration 145/1000 | Loss: 0.00001974
Iteration 146/1000 | Loss: 0.00001974
Iteration 147/1000 | Loss: 0.00001974
Iteration 148/1000 | Loss: 0.00001973
Iteration 149/1000 | Loss: 0.00001973
Iteration 150/1000 | Loss: 0.00001973
Iteration 151/1000 | Loss: 0.00001973
Iteration 152/1000 | Loss: 0.00001973
Iteration 153/1000 | Loss: 0.00001972
Iteration 154/1000 | Loss: 0.00001972
Iteration 155/1000 | Loss: 0.00001972
Iteration 156/1000 | Loss: 0.00001972
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00001972
Iteration 159/1000 | Loss: 0.00001972
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001971
Iteration 162/1000 | Loss: 0.00001971
Iteration 163/1000 | Loss: 0.00001971
Iteration 164/1000 | Loss: 0.00001971
Iteration 165/1000 | Loss: 0.00001970
Iteration 166/1000 | Loss: 0.00001970
Iteration 167/1000 | Loss: 0.00001970
Iteration 168/1000 | Loss: 0.00001970
Iteration 169/1000 | Loss: 0.00001970
Iteration 170/1000 | Loss: 0.00001970
Iteration 171/1000 | Loss: 0.00001970
Iteration 172/1000 | Loss: 0.00001970
Iteration 173/1000 | Loss: 0.00001970
Iteration 174/1000 | Loss: 0.00001970
Iteration 175/1000 | Loss: 0.00001970
Iteration 176/1000 | Loss: 0.00001970
Iteration 177/1000 | Loss: 0.00001970
Iteration 178/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.969803088286426e-05, 1.969803088286426e-05, 1.969803088286426e-05, 1.969803088286426e-05, 1.969803088286426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.969803088286426e-05

Optimization complete. Final v2v error: 3.702730655670166 mm

Highest mean error: 4.147900104522705 mm for frame 60

Lowest mean error: 3.3558616638183594 mm for frame 208

Saving results

Total time: 43.15610671043396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039607
Iteration 2/25 | Loss: 0.00296329
Iteration 3/25 | Loss: 0.00211974
Iteration 4/25 | Loss: 0.00202663
Iteration 5/25 | Loss: 0.00189815
Iteration 6/25 | Loss: 0.00186380
Iteration 7/25 | Loss: 0.00191017
Iteration 8/25 | Loss: 0.00179843
Iteration 9/25 | Loss: 0.00171560
Iteration 10/25 | Loss: 0.00170879
Iteration 11/25 | Loss: 0.00167091
Iteration 12/25 | Loss: 0.00165877
Iteration 13/25 | Loss: 0.00165486
Iteration 14/25 | Loss: 0.00162902
Iteration 15/25 | Loss: 0.00162182
Iteration 16/25 | Loss: 0.00161936
Iteration 17/25 | Loss: 0.00163448
Iteration 18/25 | Loss: 0.00161290
Iteration 19/25 | Loss: 0.00160675
Iteration 20/25 | Loss: 0.00161658
Iteration 21/25 | Loss: 0.00162399
Iteration 22/25 | Loss: 0.00161098
Iteration 23/25 | Loss: 0.00159785
Iteration 24/25 | Loss: 0.00159359
Iteration 25/25 | Loss: 0.00159207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08830869
Iteration 2/25 | Loss: 0.00237108
Iteration 3/25 | Loss: 0.00237107
Iteration 4/25 | Loss: 0.00235526
Iteration 5/25 | Loss: 0.00235523
Iteration 6/25 | Loss: 0.00235523
Iteration 7/25 | Loss: 0.00235523
Iteration 8/25 | Loss: 0.00235523
Iteration 9/25 | Loss: 0.00235523
Iteration 10/25 | Loss: 0.00235523
Iteration 11/25 | Loss: 0.00235523
Iteration 12/25 | Loss: 0.00235523
Iteration 13/25 | Loss: 0.00235523
Iteration 14/25 | Loss: 0.00235523
Iteration 15/25 | Loss: 0.00235523
Iteration 16/25 | Loss: 0.00235523
Iteration 17/25 | Loss: 0.00235523
Iteration 18/25 | Loss: 0.00235523
Iteration 19/25 | Loss: 0.00235523
Iteration 20/25 | Loss: 0.00235523
Iteration 21/25 | Loss: 0.00235523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023552256170660257, 0.0023552256170660257, 0.0023552256170660257, 0.0023552256170660257, 0.0023552256170660257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023552256170660257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235523
Iteration 2/1000 | Loss: 0.00109802
Iteration 3/1000 | Loss: 0.00037141
Iteration 4/1000 | Loss: 0.00017305
Iteration 5/1000 | Loss: 0.00020773
Iteration 6/1000 | Loss: 0.00013854
Iteration 7/1000 | Loss: 0.00042002
Iteration 8/1000 | Loss: 0.00019861
Iteration 9/1000 | Loss: 0.00013900
Iteration 10/1000 | Loss: 0.00025743
Iteration 11/1000 | Loss: 0.00072337
Iteration 12/1000 | Loss: 0.00012357
Iteration 13/1000 | Loss: 0.00011948
Iteration 14/1000 | Loss: 0.00037188
Iteration 15/1000 | Loss: 0.00018520
Iteration 16/1000 | Loss: 0.00011637
Iteration 17/1000 | Loss: 0.00021698
Iteration 18/1000 | Loss: 0.00022579
Iteration 19/1000 | Loss: 0.00019113
Iteration 20/1000 | Loss: 0.00016416
Iteration 21/1000 | Loss: 0.00011508
Iteration 22/1000 | Loss: 0.00019330
Iteration 23/1000 | Loss: 0.00015526
Iteration 24/1000 | Loss: 0.00010774
Iteration 25/1000 | Loss: 0.00010538
Iteration 26/1000 | Loss: 0.00010347
Iteration 27/1000 | Loss: 0.00018814
Iteration 28/1000 | Loss: 0.00010193
Iteration 29/1000 | Loss: 0.00010050
Iteration 30/1000 | Loss: 0.00012953
Iteration 31/1000 | Loss: 0.00016747
Iteration 32/1000 | Loss: 0.00009711
Iteration 33/1000 | Loss: 0.00011797
Iteration 34/1000 | Loss: 0.00012466
Iteration 35/1000 | Loss: 0.00009526
Iteration 36/1000 | Loss: 0.00012847
Iteration 37/1000 | Loss: 0.00009781
Iteration 38/1000 | Loss: 0.00009400
Iteration 39/1000 | Loss: 0.00048657
Iteration 40/1000 | Loss: 0.00015286
Iteration 41/1000 | Loss: 0.00011546
Iteration 42/1000 | Loss: 0.00009377
Iteration 43/1000 | Loss: 0.00009256
Iteration 44/1000 | Loss: 0.00009976
Iteration 45/1000 | Loss: 0.00009152
Iteration 46/1000 | Loss: 0.00009123
Iteration 47/1000 | Loss: 0.00009094
Iteration 48/1000 | Loss: 0.00011563
Iteration 49/1000 | Loss: 0.00009080
Iteration 50/1000 | Loss: 0.00009067
Iteration 51/1000 | Loss: 0.00009064
Iteration 52/1000 | Loss: 0.00009064
Iteration 53/1000 | Loss: 0.00009047
Iteration 54/1000 | Loss: 0.00009045
Iteration 55/1000 | Loss: 0.00009041
Iteration 56/1000 | Loss: 0.00009040
Iteration 57/1000 | Loss: 0.00009037
Iteration 58/1000 | Loss: 0.00013976
Iteration 59/1000 | Loss: 0.00012376
Iteration 60/1000 | Loss: 0.00013698
Iteration 61/1000 | Loss: 0.00012672
Iteration 62/1000 | Loss: 0.00009394
Iteration 63/1000 | Loss: 0.00009576
Iteration 64/1000 | Loss: 0.00012265
Iteration 65/1000 | Loss: 0.00009366
Iteration 66/1000 | Loss: 0.00011968
Iteration 67/1000 | Loss: 0.00009756
Iteration 68/1000 | Loss: 0.00009330
Iteration 69/1000 | Loss: 0.00010044
Iteration 70/1000 | Loss: 0.00009236
Iteration 71/1000 | Loss: 0.00011586
Iteration 72/1000 | Loss: 0.00009195
Iteration 73/1000 | Loss: 0.00011506
Iteration 74/1000 | Loss: 0.00009609
Iteration 75/1000 | Loss: 0.00009734
Iteration 76/1000 | Loss: 0.00011732
Iteration 77/1000 | Loss: 0.00009778
Iteration 78/1000 | Loss: 0.00011424
Iteration 79/1000 | Loss: 0.00010276
Iteration 80/1000 | Loss: 0.00009042
Iteration 81/1000 | Loss: 0.00009002
Iteration 82/1000 | Loss: 0.00008994
Iteration 83/1000 | Loss: 0.00010118
Iteration 84/1000 | Loss: 0.00008977
Iteration 85/1000 | Loss: 0.00008969
Iteration 86/1000 | Loss: 0.00008969
Iteration 87/1000 | Loss: 0.00008967
Iteration 88/1000 | Loss: 0.00008966
Iteration 89/1000 | Loss: 0.00008966
Iteration 90/1000 | Loss: 0.00008966
Iteration 91/1000 | Loss: 0.00008962
Iteration 92/1000 | Loss: 0.00008961
Iteration 93/1000 | Loss: 0.00008955
Iteration 94/1000 | Loss: 0.00008952
Iteration 95/1000 | Loss: 0.00008952
Iteration 96/1000 | Loss: 0.00008952
Iteration 97/1000 | Loss: 0.00008952
Iteration 98/1000 | Loss: 0.00008952
Iteration 99/1000 | Loss: 0.00008951
Iteration 100/1000 | Loss: 0.00008951
Iteration 101/1000 | Loss: 0.00008951
Iteration 102/1000 | Loss: 0.00008951
Iteration 103/1000 | Loss: 0.00008951
Iteration 104/1000 | Loss: 0.00008951
Iteration 105/1000 | Loss: 0.00008951
Iteration 106/1000 | Loss: 0.00008950
Iteration 107/1000 | Loss: 0.00008950
Iteration 108/1000 | Loss: 0.00008949
Iteration 109/1000 | Loss: 0.00008949
Iteration 110/1000 | Loss: 0.00008949
Iteration 111/1000 | Loss: 0.00008949
Iteration 112/1000 | Loss: 0.00008949
Iteration 113/1000 | Loss: 0.00008948
Iteration 114/1000 | Loss: 0.00008948
Iteration 115/1000 | Loss: 0.00008948
Iteration 116/1000 | Loss: 0.00008948
Iteration 117/1000 | Loss: 0.00008947
Iteration 118/1000 | Loss: 0.00008947
Iteration 119/1000 | Loss: 0.00008947
Iteration 120/1000 | Loss: 0.00008947
Iteration 121/1000 | Loss: 0.00008945
Iteration 122/1000 | Loss: 0.00008939
Iteration 123/1000 | Loss: 0.00008930
Iteration 124/1000 | Loss: 0.00008918
Iteration 125/1000 | Loss: 0.00008918
Iteration 126/1000 | Loss: 0.00008918
Iteration 127/1000 | Loss: 0.00008918
Iteration 128/1000 | Loss: 0.00008918
Iteration 129/1000 | Loss: 0.00008918
Iteration 130/1000 | Loss: 0.00008918
Iteration 131/1000 | Loss: 0.00008918
Iteration 132/1000 | Loss: 0.00008918
Iteration 133/1000 | Loss: 0.00008918
Iteration 134/1000 | Loss: 0.00008917
Iteration 135/1000 | Loss: 0.00008917
Iteration 136/1000 | Loss: 0.00008917
Iteration 137/1000 | Loss: 0.00008917
Iteration 138/1000 | Loss: 0.00008906
Iteration 139/1000 | Loss: 0.00011115
Iteration 140/1000 | Loss: 0.00008893
Iteration 141/1000 | Loss: 0.00008861
Iteration 142/1000 | Loss: 0.00008833
Iteration 143/1000 | Loss: 0.00010431
Iteration 144/1000 | Loss: 0.00011404
Iteration 145/1000 | Loss: 0.00009719
Iteration 146/1000 | Loss: 0.00010943
Iteration 147/1000 | Loss: 0.00008789
Iteration 148/1000 | Loss: 0.00008751
Iteration 149/1000 | Loss: 0.00062720
Iteration 150/1000 | Loss: 0.00018451
Iteration 151/1000 | Loss: 0.00013519
Iteration 152/1000 | Loss: 0.00010375
Iteration 153/1000 | Loss: 0.00009734
Iteration 154/1000 | Loss: 0.00009222
Iteration 155/1000 | Loss: 0.00009163
Iteration 156/1000 | Loss: 0.00012775
Iteration 157/1000 | Loss: 0.00009114
Iteration 158/1000 | Loss: 0.00009063
Iteration 159/1000 | Loss: 0.00010622
Iteration 160/1000 | Loss: 0.00011618
Iteration 161/1000 | Loss: 0.00008970
Iteration 162/1000 | Loss: 0.00008938
Iteration 163/1000 | Loss: 0.00008903
Iteration 164/1000 | Loss: 0.00011062
Iteration 165/1000 | Loss: 0.00008862
Iteration 166/1000 | Loss: 0.00010818
Iteration 167/1000 | Loss: 0.00008787
Iteration 168/1000 | Loss: 0.00008708
Iteration 169/1000 | Loss: 0.00015736
Iteration 170/1000 | Loss: 0.00027675
Iteration 171/1000 | Loss: 0.00058001
Iteration 172/1000 | Loss: 0.00012079
Iteration 173/1000 | Loss: 0.00039874
Iteration 174/1000 | Loss: 0.00033391
Iteration 175/1000 | Loss: 0.00036610
Iteration 176/1000 | Loss: 0.00014413
Iteration 177/1000 | Loss: 0.00016187
Iteration 178/1000 | Loss: 0.00008770
Iteration 179/1000 | Loss: 0.00013988
Iteration 180/1000 | Loss: 0.00008389
Iteration 181/1000 | Loss: 0.00013608
Iteration 182/1000 | Loss: 0.00007795
Iteration 183/1000 | Loss: 0.00011989
Iteration 184/1000 | Loss: 0.00041101
Iteration 185/1000 | Loss: 0.00033415
Iteration 186/1000 | Loss: 0.00012489
Iteration 187/1000 | Loss: 0.00007357
Iteration 188/1000 | Loss: 0.00041879
Iteration 189/1000 | Loss: 0.00010424
Iteration 190/1000 | Loss: 0.00007830
Iteration 191/1000 | Loss: 0.00043673
Iteration 192/1000 | Loss: 0.00163722
Iteration 193/1000 | Loss: 0.00054803
Iteration 194/1000 | Loss: 0.00018001
Iteration 195/1000 | Loss: 0.00010916
Iteration 196/1000 | Loss: 0.00008856
Iteration 197/1000 | Loss: 0.00008292
Iteration 198/1000 | Loss: 0.00009500
Iteration 199/1000 | Loss: 0.00007841
Iteration 200/1000 | Loss: 0.00008955
Iteration 201/1000 | Loss: 0.00079603
Iteration 202/1000 | Loss: 0.00087768
Iteration 203/1000 | Loss: 0.00038243
Iteration 204/1000 | Loss: 0.00013097
Iteration 205/1000 | Loss: 0.00010028
Iteration 206/1000 | Loss: 0.00009027
Iteration 207/1000 | Loss: 0.00011986
Iteration 208/1000 | Loss: 0.00008448
Iteration 209/1000 | Loss: 0.00008172
Iteration 210/1000 | Loss: 0.00007950
Iteration 211/1000 | Loss: 0.00017713
Iteration 212/1000 | Loss: 0.00011546
Iteration 213/1000 | Loss: 0.00010732
Iteration 214/1000 | Loss: 0.00046445
Iteration 215/1000 | Loss: 0.00011431
Iteration 216/1000 | Loss: 0.00009532
Iteration 217/1000 | Loss: 0.00007901
Iteration 218/1000 | Loss: 0.00007760
Iteration 219/1000 | Loss: 0.00007597
Iteration 220/1000 | Loss: 0.00010589
Iteration 221/1000 | Loss: 0.00007325
Iteration 222/1000 | Loss: 0.00042807
Iteration 223/1000 | Loss: 0.00029964
Iteration 224/1000 | Loss: 0.00007652
Iteration 225/1000 | Loss: 0.00010415
Iteration 226/1000 | Loss: 0.00006943
Iteration 227/1000 | Loss: 0.00014305
Iteration 228/1000 | Loss: 0.00006900
Iteration 229/1000 | Loss: 0.00006711
Iteration 230/1000 | Loss: 0.00010476
Iteration 231/1000 | Loss: 0.00007539
Iteration 232/1000 | Loss: 0.00006683
Iteration 233/1000 | Loss: 0.00006647
Iteration 234/1000 | Loss: 0.00006304
Iteration 235/1000 | Loss: 0.00105303
Iteration 236/1000 | Loss: 0.00010599
Iteration 237/1000 | Loss: 0.00008167
Iteration 238/1000 | Loss: 0.00007284
Iteration 239/1000 | Loss: 0.00007074
Iteration 240/1000 | Loss: 0.00041928
Iteration 241/1000 | Loss: 0.00008687
Iteration 242/1000 | Loss: 0.00007346
Iteration 243/1000 | Loss: 0.00007052
Iteration 244/1000 | Loss: 0.00006897
Iteration 245/1000 | Loss: 0.00041024
Iteration 246/1000 | Loss: 0.00081269
Iteration 247/1000 | Loss: 0.00010592
Iteration 248/1000 | Loss: 0.00007715
Iteration 249/1000 | Loss: 0.00007201
Iteration 250/1000 | Loss: 0.00007019
Iteration 251/1000 | Loss: 0.00006872
Iteration 252/1000 | Loss: 0.00008111
Iteration 253/1000 | Loss: 0.00059294
Iteration 254/1000 | Loss: 0.00010297
Iteration 255/1000 | Loss: 0.00007371
Iteration 256/1000 | Loss: 0.00007054
Iteration 257/1000 | Loss: 0.00006917
Iteration 258/1000 | Loss: 0.00008328
Iteration 259/1000 | Loss: 0.00006718
Iteration 260/1000 | Loss: 0.00006638
Iteration 261/1000 | Loss: 0.00006541
Iteration 262/1000 | Loss: 0.00040735
Iteration 263/1000 | Loss: 0.00009523
Iteration 264/1000 | Loss: 0.00006483
Iteration 265/1000 | Loss: 0.00011723
Iteration 266/1000 | Loss: 0.00006007
Iteration 267/1000 | Loss: 0.00005903
Iteration 268/1000 | Loss: 0.00005748
Iteration 269/1000 | Loss: 0.00040471
Iteration 270/1000 | Loss: 0.00007149
Iteration 271/1000 | Loss: 0.00042267
Iteration 272/1000 | Loss: 0.00007463
Iteration 273/1000 | Loss: 0.00006487
Iteration 274/1000 | Loss: 0.00006201
Iteration 275/1000 | Loss: 0.00006074
Iteration 276/1000 | Loss: 0.00040373
Iteration 277/1000 | Loss: 0.00115791
Iteration 278/1000 | Loss: 0.00012667
Iteration 279/1000 | Loss: 0.00008078
Iteration 280/1000 | Loss: 0.00006990
Iteration 281/1000 | Loss: 0.00006733
Iteration 282/1000 | Loss: 0.00007468
Iteration 283/1000 | Loss: 0.00006454
Iteration 284/1000 | Loss: 0.00007400
Iteration 285/1000 | Loss: 0.00007510
Iteration 286/1000 | Loss: 0.00006167
Iteration 287/1000 | Loss: 0.00041605
Iteration 288/1000 | Loss: 0.00007631
Iteration 289/1000 | Loss: 0.00006496
Iteration 290/1000 | Loss: 0.00008817
Iteration 291/1000 | Loss: 0.00006273
Iteration 292/1000 | Loss: 0.00007614
Iteration 293/1000 | Loss: 0.00006515
Iteration 294/1000 | Loss: 0.00006094
Iteration 295/1000 | Loss: 0.00007749
Iteration 296/1000 | Loss: 0.00017009
Iteration 297/1000 | Loss: 0.00006902
Iteration 298/1000 | Loss: 0.00007833
Iteration 299/1000 | Loss: 0.00005806
Iteration 300/1000 | Loss: 0.00045776
Iteration 301/1000 | Loss: 0.00007757
Iteration 302/1000 | Loss: 0.00006165
Iteration 303/1000 | Loss: 0.00005868
Iteration 304/1000 | Loss: 0.00007574
Iteration 305/1000 | Loss: 0.00037801
Iteration 306/1000 | Loss: 0.00006995
Iteration 307/1000 | Loss: 0.00006076
Iteration 308/1000 | Loss: 0.00005777
Iteration 309/1000 | Loss: 0.00072514
Iteration 310/1000 | Loss: 0.00008583
Iteration 311/1000 | Loss: 0.00040410
Iteration 312/1000 | Loss: 0.00007726
Iteration 313/1000 | Loss: 0.00006887
Iteration 314/1000 | Loss: 0.00006594
Iteration 315/1000 | Loss: 0.00041712
Iteration 316/1000 | Loss: 0.00007858
Iteration 317/1000 | Loss: 0.00008113
Iteration 318/1000 | Loss: 0.00006596
Iteration 319/1000 | Loss: 0.00007504
Iteration 320/1000 | Loss: 0.00006426
Iteration 321/1000 | Loss: 0.00006263
Iteration 322/1000 | Loss: 0.00007065
Iteration 323/1000 | Loss: 0.00006092
Iteration 324/1000 | Loss: 0.00006003
Iteration 325/1000 | Loss: 0.00043958
Iteration 326/1000 | Loss: 0.00014017
Iteration 327/1000 | Loss: 0.00006800
Iteration 328/1000 | Loss: 0.00005804
Iteration 329/1000 | Loss: 0.00005668
Iteration 330/1000 | Loss: 0.00005559
Iteration 331/1000 | Loss: 0.00009600
Iteration 332/1000 | Loss: 0.00006309
Iteration 333/1000 | Loss: 0.00005395
Iteration 334/1000 | Loss: 0.00005266
Iteration 335/1000 | Loss: 0.00039422
Iteration 336/1000 | Loss: 0.00006830
Iteration 337/1000 | Loss: 0.00005852
Iteration 338/1000 | Loss: 0.00005505
Iteration 339/1000 | Loss: 0.00005392
Iteration 340/1000 | Loss: 0.00075252
Iteration 341/1000 | Loss: 0.00009488
Iteration 342/1000 | Loss: 0.00015226
Iteration 343/1000 | Loss: 0.00044194
Iteration 344/1000 | Loss: 0.00007428
Iteration 345/1000 | Loss: 0.00006456
Iteration 346/1000 | Loss: 0.00006237
Iteration 347/1000 | Loss: 0.00006148
Iteration 348/1000 | Loss: 0.00039925
Iteration 349/1000 | Loss: 0.00007618
Iteration 350/1000 | Loss: 0.00006494
Iteration 351/1000 | Loss: 0.00009275
Iteration 352/1000 | Loss: 0.00006217
Iteration 353/1000 | Loss: 0.00006151
Iteration 354/1000 | Loss: 0.00006024
Iteration 355/1000 | Loss: 0.00005909
Iteration 356/1000 | Loss: 0.00005831
Iteration 357/1000 | Loss: 0.00005745
Iteration 358/1000 | Loss: 0.00037749
Iteration 359/1000 | Loss: 0.00008070
Iteration 360/1000 | Loss: 0.00007393
Iteration 361/1000 | Loss: 0.00006042
Iteration 362/1000 | Loss: 0.00005943
Iteration 363/1000 | Loss: 0.00007134
Iteration 364/1000 | Loss: 0.00005809
Iteration 365/1000 | Loss: 0.00005711
Iteration 366/1000 | Loss: 0.00005625
Iteration 367/1000 | Loss: 0.00006858
Iteration 368/1000 | Loss: 0.00005344
Iteration 369/1000 | Loss: 0.00069817
Iteration 370/1000 | Loss: 0.00047512
Iteration 371/1000 | Loss: 0.00008139
Iteration 372/1000 | Loss: 0.00005962
Iteration 373/1000 | Loss: 0.00005390
Iteration 374/1000 | Loss: 0.00006962
Iteration 375/1000 | Loss: 0.00005115
Iteration 376/1000 | Loss: 0.00040204
Iteration 377/1000 | Loss: 0.00008341
Iteration 378/1000 | Loss: 0.00005655
Iteration 379/1000 | Loss: 0.00041081
Iteration 380/1000 | Loss: 0.00006955
Iteration 381/1000 | Loss: 0.00005843
Iteration 382/1000 | Loss: 0.00005603
Iteration 383/1000 | Loss: 0.00005494
Iteration 384/1000 | Loss: 0.00005409
Iteration 385/1000 | Loss: 0.00006668
Iteration 386/1000 | Loss: 0.00005413
Iteration 387/1000 | Loss: 0.00005235
Iteration 388/1000 | Loss: 0.00039614
Iteration 389/1000 | Loss: 0.00006801
Iteration 390/1000 | Loss: 0.00005882
Iteration 391/1000 | Loss: 0.00005582
Iteration 392/1000 | Loss: 0.00005549
Iteration 393/1000 | Loss: 0.00005460
Iteration 394/1000 | Loss: 0.00005360
Iteration 395/1000 | Loss: 0.00005296
Iteration 396/1000 | Loss: 0.00005261
Iteration 397/1000 | Loss: 0.00006762
Iteration 398/1000 | Loss: 0.00005207
Iteration 399/1000 | Loss: 0.00005151
Iteration 400/1000 | Loss: 0.00005090
Iteration 401/1000 | Loss: 0.00008140
Iteration 402/1000 | Loss: 0.00005011
Iteration 403/1000 | Loss: 0.00004940
Iteration 404/1000 | Loss: 0.00038439
Iteration 405/1000 | Loss: 0.00006204
Iteration 406/1000 | Loss: 0.00006912
Iteration 407/1000 | Loss: 0.00005223
Iteration 408/1000 | Loss: 0.00005117
Iteration 409/1000 | Loss: 0.00005020
Iteration 410/1000 | Loss: 0.00041261
Iteration 411/1000 | Loss: 0.00075680
Iteration 412/1000 | Loss: 0.00007988
Iteration 413/1000 | Loss: 0.00006324
Iteration 414/1000 | Loss: 0.00010721
Iteration 415/1000 | Loss: 0.00005158
Iteration 416/1000 | Loss: 0.00005207
Iteration 417/1000 | Loss: 0.00005183
Iteration 418/1000 | Loss: 0.00004922
Iteration 419/1000 | Loss: 0.00004864
Iteration 420/1000 | Loss: 0.00041878
Iteration 421/1000 | Loss: 0.00040403
Iteration 422/1000 | Loss: 0.00253638
Iteration 423/1000 | Loss: 0.00135316
Iteration 424/1000 | Loss: 0.00240023
Iteration 425/1000 | Loss: 0.00109116
Iteration 426/1000 | Loss: 0.00015278
Iteration 427/1000 | Loss: 0.00011458
Iteration 428/1000 | Loss: 0.00006074
Iteration 429/1000 | Loss: 0.00041001
Iteration 430/1000 | Loss: 0.00004813
Iteration 431/1000 | Loss: 0.00004923
Iteration 432/1000 | Loss: 0.00004906
Iteration 433/1000 | Loss: 0.00004641
Iteration 434/1000 | Loss: 0.00004602
Iteration 435/1000 | Loss: 0.00004583
Iteration 436/1000 | Loss: 0.00004556
Iteration 437/1000 | Loss: 0.00006360
Iteration 438/1000 | Loss: 0.00004500
Iteration 439/1000 | Loss: 0.00007675
Iteration 440/1000 | Loss: 0.00004437
Iteration 441/1000 | Loss: 0.00004375
Iteration 442/1000 | Loss: 0.00035852
Iteration 443/1000 | Loss: 0.00022566
Iteration 444/1000 | Loss: 0.00004456
Iteration 445/1000 | Loss: 0.00034764
Iteration 446/1000 | Loss: 0.00044412
Iteration 447/1000 | Loss: 0.00007666
Iteration 448/1000 | Loss: 0.00005460
Iteration 449/1000 | Loss: 0.00004783
Iteration 450/1000 | Loss: 0.00004613
Iteration 451/1000 | Loss: 0.00007950
Iteration 452/1000 | Loss: 0.00004883
Iteration 453/1000 | Loss: 0.00006515
Iteration 454/1000 | Loss: 0.00004478
Iteration 455/1000 | Loss: 0.00006247
Iteration 456/1000 | Loss: 0.00004699
Iteration 457/1000 | Loss: 0.00004421
Iteration 458/1000 | Loss: 0.00004391
Iteration 459/1000 | Loss: 0.00008069
Iteration 460/1000 | Loss: 0.00007318
Iteration 461/1000 | Loss: 0.00007847
Iteration 462/1000 | Loss: 0.00005011
Iteration 463/1000 | Loss: 0.00004744
Iteration 464/1000 | Loss: 0.00004949
Iteration 465/1000 | Loss: 0.00007072
Iteration 466/1000 | Loss: 0.00004492
Iteration 467/1000 | Loss: 0.00004448
Iteration 468/1000 | Loss: 0.00005455
Iteration 469/1000 | Loss: 0.00004391
Iteration 470/1000 | Loss: 0.00007725
Iteration 471/1000 | Loss: 0.00004984
Iteration 472/1000 | Loss: 0.00008609
Iteration 473/1000 | Loss: 0.00006978
Iteration 474/1000 | Loss: 0.00008970
Iteration 475/1000 | Loss: 0.00007302
Iteration 476/1000 | Loss: 0.00004320
Iteration 477/1000 | Loss: 0.00004306
Iteration 478/1000 | Loss: 0.00004296
Iteration 479/1000 | Loss: 0.00004799
Iteration 480/1000 | Loss: 0.00004289
Iteration 481/1000 | Loss: 0.00004285
Iteration 482/1000 | Loss: 0.00004275
Iteration 483/1000 | Loss: 0.00004271
Iteration 484/1000 | Loss: 0.00004258
Iteration 485/1000 | Loss: 0.00004239
Iteration 486/1000 | Loss: 0.00006377
Iteration 487/1000 | Loss: 0.00039528
Iteration 488/1000 | Loss: 0.00017165
Iteration 489/1000 | Loss: 0.00004275
Iteration 490/1000 | Loss: 0.00004217
Iteration 491/1000 | Loss: 0.00004178
Iteration 492/1000 | Loss: 0.00038111
Iteration 493/1000 | Loss: 0.00011162
Iteration 494/1000 | Loss: 0.00033460
Iteration 495/1000 | Loss: 0.00009140
Iteration 496/1000 | Loss: 0.00004277
Iteration 497/1000 | Loss: 0.00004164
Iteration 498/1000 | Loss: 0.00004145
Iteration 499/1000 | Loss: 0.00038182
Iteration 500/1000 | Loss: 0.00011426
Iteration 501/1000 | Loss: 0.00005264
Iteration 502/1000 | Loss: 0.00004594
Iteration 503/1000 | Loss: 0.00004457
Iteration 504/1000 | Loss: 0.00005058
Iteration 505/1000 | Loss: 0.00004289
Iteration 506/1000 | Loss: 0.00004217
Iteration 507/1000 | Loss: 0.00004172
Iteration 508/1000 | Loss: 0.00004122
Iteration 509/1000 | Loss: 0.00005644
Iteration 510/1000 | Loss: 0.00004073
Iteration 511/1000 | Loss: 0.00004046
Iteration 512/1000 | Loss: 0.00005289
Iteration 513/1000 | Loss: 0.00004035
Iteration 514/1000 | Loss: 0.00004035
Iteration 515/1000 | Loss: 0.00004035
Iteration 516/1000 | Loss: 0.00004035
Iteration 517/1000 | Loss: 0.00004034
Iteration 518/1000 | Loss: 0.00004034
Iteration 519/1000 | Loss: 0.00004031
Iteration 520/1000 | Loss: 0.00004031
Iteration 521/1000 | Loss: 0.00004031
Iteration 522/1000 | Loss: 0.00004030
Iteration 523/1000 | Loss: 0.00004030
Iteration 524/1000 | Loss: 0.00004029
Iteration 525/1000 | Loss: 0.00004029
Iteration 526/1000 | Loss: 0.00004028
Iteration 527/1000 | Loss: 0.00004028
Iteration 528/1000 | Loss: 0.00004028
Iteration 529/1000 | Loss: 0.00004028
Iteration 530/1000 | Loss: 0.00004027
Iteration 531/1000 | Loss: 0.00004027
Iteration 532/1000 | Loss: 0.00004027
Iteration 533/1000 | Loss: 0.00004027
Iteration 534/1000 | Loss: 0.00004027
Iteration 535/1000 | Loss: 0.00004027
Iteration 536/1000 | Loss: 0.00004027
Iteration 537/1000 | Loss: 0.00004027
Iteration 538/1000 | Loss: 0.00004026
Iteration 539/1000 | Loss: 0.00004026
Iteration 540/1000 | Loss: 0.00004026
Iteration 541/1000 | Loss: 0.00004025
Iteration 542/1000 | Loss: 0.00004025
Iteration 543/1000 | Loss: 0.00004025
Iteration 544/1000 | Loss: 0.00004025
Iteration 545/1000 | Loss: 0.00004025
Iteration 546/1000 | Loss: 0.00004025
Iteration 547/1000 | Loss: 0.00004025
Iteration 548/1000 | Loss: 0.00004025
Iteration 549/1000 | Loss: 0.00004025
Iteration 550/1000 | Loss: 0.00004025
Iteration 551/1000 | Loss: 0.00004024
Iteration 552/1000 | Loss: 0.00004024
Iteration 553/1000 | Loss: 0.00004024
Iteration 554/1000 | Loss: 0.00005596
Iteration 555/1000 | Loss: 0.00005596
Iteration 556/1000 | Loss: 0.00004194
Iteration 557/1000 | Loss: 0.00004023
Iteration 558/1000 | Loss: 0.00004318
Iteration 559/1000 | Loss: 0.00004023
Iteration 560/1000 | Loss: 0.00004022
Iteration 561/1000 | Loss: 0.00004021
Iteration 562/1000 | Loss: 0.00004020
Iteration 563/1000 | Loss: 0.00004020
Iteration 564/1000 | Loss: 0.00004020
Iteration 565/1000 | Loss: 0.00004019
Iteration 566/1000 | Loss: 0.00004019
Iteration 567/1000 | Loss: 0.00004019
Iteration 568/1000 | Loss: 0.00004019
Iteration 569/1000 | Loss: 0.00004019
Iteration 570/1000 | Loss: 0.00004019
Iteration 571/1000 | Loss: 0.00004019
Iteration 572/1000 | Loss: 0.00004018
Iteration 573/1000 | Loss: 0.00004018
Iteration 574/1000 | Loss: 0.00004018
Iteration 575/1000 | Loss: 0.00004018
Iteration 576/1000 | Loss: 0.00004018
Iteration 577/1000 | Loss: 0.00004018
Iteration 578/1000 | Loss: 0.00004018
Iteration 579/1000 | Loss: 0.00004018
Iteration 580/1000 | Loss: 0.00004018
Iteration 581/1000 | Loss: 0.00004018
Iteration 582/1000 | Loss: 0.00004018
Iteration 583/1000 | Loss: 0.00004018
Iteration 584/1000 | Loss: 0.00004018
Iteration 585/1000 | Loss: 0.00004018
Iteration 586/1000 | Loss: 0.00004018
Iteration 587/1000 | Loss: 0.00004018
Iteration 588/1000 | Loss: 0.00004018
Iteration 589/1000 | Loss: 0.00004018
Iteration 590/1000 | Loss: 0.00004018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 590. Stopping optimization.
Last 5 losses: [4.0180366340791807e-05, 4.0180366340791807e-05, 4.0180366340791807e-05, 4.0180366340791807e-05, 4.0180366340791807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0180366340791807e-05

Optimization complete. Final v2v error: 4.666112899780273 mm

Highest mean error: 6.505321025848389 mm for frame 35

Lowest mean error: 3.471944570541382 mm for frame 1

Saving results

Total time: 681.7427577972412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800416
Iteration 2/25 | Loss: 0.00141661
Iteration 3/25 | Loss: 0.00130395
Iteration 4/25 | Loss: 0.00127976
Iteration 5/25 | Loss: 0.00127135
Iteration 6/25 | Loss: 0.00126914
Iteration 7/25 | Loss: 0.00126879
Iteration 8/25 | Loss: 0.00126879
Iteration 9/25 | Loss: 0.00126879
Iteration 10/25 | Loss: 0.00126879
Iteration 11/25 | Loss: 0.00126879
Iteration 12/25 | Loss: 0.00126879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001268788706511259, 0.001268788706511259, 0.001268788706511259, 0.001268788706511259, 0.001268788706511259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268788706511259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33769298
Iteration 2/25 | Loss: 0.00144364
Iteration 3/25 | Loss: 0.00144364
Iteration 4/25 | Loss: 0.00144364
Iteration 5/25 | Loss: 0.00144364
Iteration 6/25 | Loss: 0.00144364
Iteration 7/25 | Loss: 0.00144364
Iteration 8/25 | Loss: 0.00144364
Iteration 9/25 | Loss: 0.00144364
Iteration 10/25 | Loss: 0.00144364
Iteration 11/25 | Loss: 0.00144364
Iteration 12/25 | Loss: 0.00144364
Iteration 13/25 | Loss: 0.00144364
Iteration 14/25 | Loss: 0.00144364
Iteration 15/25 | Loss: 0.00144364
Iteration 16/25 | Loss: 0.00144364
Iteration 17/25 | Loss: 0.00144364
Iteration 18/25 | Loss: 0.00144364
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001443641260266304, 0.001443641260266304, 0.001443641260266304, 0.001443641260266304, 0.001443641260266304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001443641260266304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144364
Iteration 2/1000 | Loss: 0.00004099
Iteration 3/1000 | Loss: 0.00003092
Iteration 4/1000 | Loss: 0.00002671
Iteration 5/1000 | Loss: 0.00002567
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002331
Iteration 9/1000 | Loss: 0.00002289
Iteration 10/1000 | Loss: 0.00002258
Iteration 11/1000 | Loss: 0.00002225
Iteration 12/1000 | Loss: 0.00002207
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002170
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002155
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002151
Iteration 20/1000 | Loss: 0.00002150
Iteration 21/1000 | Loss: 0.00002150
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002149
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002148
Iteration 26/1000 | Loss: 0.00002143
Iteration 27/1000 | Loss: 0.00002143
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002141
Iteration 30/1000 | Loss: 0.00002141
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002140
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002139
Iteration 35/1000 | Loss: 0.00002139
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002137
Iteration 39/1000 | Loss: 0.00002137
Iteration 40/1000 | Loss: 0.00002137
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002136
Iteration 46/1000 | Loss: 0.00002135
Iteration 47/1000 | Loss: 0.00002135
Iteration 48/1000 | Loss: 0.00002135
Iteration 49/1000 | Loss: 0.00002135
Iteration 50/1000 | Loss: 0.00002135
Iteration 51/1000 | Loss: 0.00002134
Iteration 52/1000 | Loss: 0.00002134
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002132
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002131
Iteration 64/1000 | Loss: 0.00002131
Iteration 65/1000 | Loss: 0.00002131
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002130
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002127
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002126
Iteration 92/1000 | Loss: 0.00002126
Iteration 93/1000 | Loss: 0.00002126
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002125
Iteration 104/1000 | Loss: 0.00002125
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002124
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002123
Iteration 117/1000 | Loss: 0.00002123
Iteration 118/1000 | Loss: 0.00002123
Iteration 119/1000 | Loss: 0.00002123
Iteration 120/1000 | Loss: 0.00002123
Iteration 121/1000 | Loss: 0.00002123
Iteration 122/1000 | Loss: 0.00002123
Iteration 123/1000 | Loss: 0.00002123
Iteration 124/1000 | Loss: 0.00002123
Iteration 125/1000 | Loss: 0.00002123
Iteration 126/1000 | Loss: 0.00002123
Iteration 127/1000 | Loss: 0.00002123
Iteration 128/1000 | Loss: 0.00002123
Iteration 129/1000 | Loss: 0.00002123
Iteration 130/1000 | Loss: 0.00002123
Iteration 131/1000 | Loss: 0.00002123
Iteration 132/1000 | Loss: 0.00002122
Iteration 133/1000 | Loss: 0.00002122
Iteration 134/1000 | Loss: 0.00002122
Iteration 135/1000 | Loss: 0.00002122
Iteration 136/1000 | Loss: 0.00002122
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002122
Iteration 139/1000 | Loss: 0.00002122
Iteration 140/1000 | Loss: 0.00002122
Iteration 141/1000 | Loss: 0.00002122
Iteration 142/1000 | Loss: 0.00002122
Iteration 143/1000 | Loss: 0.00002122
Iteration 144/1000 | Loss: 0.00002122
Iteration 145/1000 | Loss: 0.00002122
Iteration 146/1000 | Loss: 0.00002122
Iteration 147/1000 | Loss: 0.00002122
Iteration 148/1000 | Loss: 0.00002122
Iteration 149/1000 | Loss: 0.00002122
Iteration 150/1000 | Loss: 0.00002122
Iteration 151/1000 | Loss: 0.00002122
Iteration 152/1000 | Loss: 0.00002122
Iteration 153/1000 | Loss: 0.00002122
Iteration 154/1000 | Loss: 0.00002122
Iteration 155/1000 | Loss: 0.00002122
Iteration 156/1000 | Loss: 0.00002122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.121586658176966e-05, 2.121586658176966e-05, 2.121586658176966e-05, 2.121586658176966e-05, 2.121586658176966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.121586658176966e-05

Optimization complete. Final v2v error: 3.890814781188965 mm

Highest mean error: 4.66499137878418 mm for frame 60

Lowest mean error: 3.6089813709259033 mm for frame 34

Saving results

Total time: 40.493133783340454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419881
Iteration 2/25 | Loss: 0.00134838
Iteration 3/25 | Loss: 0.00128493
Iteration 4/25 | Loss: 0.00127538
Iteration 5/25 | Loss: 0.00127237
Iteration 6/25 | Loss: 0.00127216
Iteration 7/25 | Loss: 0.00127216
Iteration 8/25 | Loss: 0.00127216
Iteration 9/25 | Loss: 0.00127216
Iteration 10/25 | Loss: 0.00127216
Iteration 11/25 | Loss: 0.00127216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00127216090913862, 0.00127216090913862, 0.00127216090913862, 0.00127216090913862, 0.00127216090913862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127216090913862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39308751
Iteration 2/25 | Loss: 0.00104267
Iteration 3/25 | Loss: 0.00104267
Iteration 4/25 | Loss: 0.00104267
Iteration 5/25 | Loss: 0.00104267
Iteration 6/25 | Loss: 0.00104267
Iteration 7/25 | Loss: 0.00104267
Iteration 8/25 | Loss: 0.00104267
Iteration 9/25 | Loss: 0.00104267
Iteration 10/25 | Loss: 0.00104267
Iteration 11/25 | Loss: 0.00104267
Iteration 12/25 | Loss: 0.00104267
Iteration 13/25 | Loss: 0.00104267
Iteration 14/25 | Loss: 0.00104267
Iteration 15/25 | Loss: 0.00104267
Iteration 16/25 | Loss: 0.00104267
Iteration 17/25 | Loss: 0.00104267
Iteration 18/25 | Loss: 0.00104267
Iteration 19/25 | Loss: 0.00104267
Iteration 20/25 | Loss: 0.00104267
Iteration 21/25 | Loss: 0.00104267
Iteration 22/25 | Loss: 0.00104267
Iteration 23/25 | Loss: 0.00104267
Iteration 24/25 | Loss: 0.00104267
Iteration 25/25 | Loss: 0.00104267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010426660301163793, 0.0010426660301163793, 0.0010426660301163793, 0.0010426660301163793, 0.0010426660301163793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010426660301163793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104267
Iteration 2/1000 | Loss: 0.00003398
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002104
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001906
Iteration 9/1000 | Loss: 0.00001878
Iteration 10/1000 | Loss: 0.00001845
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001795
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001758
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001750
Iteration 19/1000 | Loss: 0.00001746
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001719
Iteration 43/1000 | Loss: 0.00001719
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001713
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001710
Iteration 66/1000 | Loss: 0.00001710
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001707
Iteration 78/1000 | Loss: 0.00001706
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001702
Iteration 99/1000 | Loss: 0.00001702
Iteration 100/1000 | Loss: 0.00001702
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001701
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001700
Iteration 115/1000 | Loss: 0.00001700
Iteration 116/1000 | Loss: 0.00001700
Iteration 117/1000 | Loss: 0.00001700
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00001699
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001699
Iteration 125/1000 | Loss: 0.00001699
Iteration 126/1000 | Loss: 0.00001699
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001699
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001699
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001698
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001698
Iteration 156/1000 | Loss: 0.00001698
Iteration 157/1000 | Loss: 0.00001698
Iteration 158/1000 | Loss: 0.00001698
Iteration 159/1000 | Loss: 0.00001698
Iteration 160/1000 | Loss: 0.00001698
Iteration 161/1000 | Loss: 0.00001698
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001697
Iteration 164/1000 | Loss: 0.00001697
Iteration 165/1000 | Loss: 0.00001697
Iteration 166/1000 | Loss: 0.00001697
Iteration 167/1000 | Loss: 0.00001697
Iteration 168/1000 | Loss: 0.00001697
Iteration 169/1000 | Loss: 0.00001697
Iteration 170/1000 | Loss: 0.00001697
Iteration 171/1000 | Loss: 0.00001697
Iteration 172/1000 | Loss: 0.00001697
Iteration 173/1000 | Loss: 0.00001697
Iteration 174/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.697243715170771e-05, 1.697243715170771e-05, 1.697243715170771e-05, 1.697243715170771e-05, 1.697243715170771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.697243715170771e-05

Optimization complete. Final v2v error: 3.529050588607788 mm

Highest mean error: 3.940918445587158 mm for frame 24

Lowest mean error: 3.3105902671813965 mm for frame 46

Saving results

Total time: 40.86220717430115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876058
Iteration 2/25 | Loss: 0.00199361
Iteration 3/25 | Loss: 0.00160553
Iteration 4/25 | Loss: 0.00152848
Iteration 5/25 | Loss: 0.00148153
Iteration 6/25 | Loss: 0.00141088
Iteration 7/25 | Loss: 0.00138864
Iteration 8/25 | Loss: 0.00138397
Iteration 9/25 | Loss: 0.00138061
Iteration 10/25 | Loss: 0.00138340
Iteration 11/25 | Loss: 0.00137929
Iteration 12/25 | Loss: 0.00137738
Iteration 13/25 | Loss: 0.00137693
Iteration 14/25 | Loss: 0.00137680
Iteration 15/25 | Loss: 0.00137679
Iteration 16/25 | Loss: 0.00137678
Iteration 17/25 | Loss: 0.00137678
Iteration 18/25 | Loss: 0.00137678
Iteration 19/25 | Loss: 0.00137678
Iteration 20/25 | Loss: 0.00137678
Iteration 21/25 | Loss: 0.00137678
Iteration 22/25 | Loss: 0.00137678
Iteration 23/25 | Loss: 0.00137678
Iteration 24/25 | Loss: 0.00137678
Iteration 25/25 | Loss: 0.00137677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57868338
Iteration 2/25 | Loss: 0.00094396
Iteration 3/25 | Loss: 0.00094396
Iteration 4/25 | Loss: 0.00094396
Iteration 5/25 | Loss: 0.00094396
Iteration 6/25 | Loss: 0.00094395
Iteration 7/25 | Loss: 0.00094395
Iteration 8/25 | Loss: 0.00094395
Iteration 9/25 | Loss: 0.00094395
Iteration 10/25 | Loss: 0.00094395
Iteration 11/25 | Loss: 0.00094395
Iteration 12/25 | Loss: 0.00094395
Iteration 13/25 | Loss: 0.00094395
Iteration 14/25 | Loss: 0.00094395
Iteration 15/25 | Loss: 0.00094395
Iteration 16/25 | Loss: 0.00094395
Iteration 17/25 | Loss: 0.00094395
Iteration 18/25 | Loss: 0.00094395
Iteration 19/25 | Loss: 0.00094395
Iteration 20/25 | Loss: 0.00094395
Iteration 21/25 | Loss: 0.00094395
Iteration 22/25 | Loss: 0.00094395
Iteration 23/25 | Loss: 0.00094395
Iteration 24/25 | Loss: 0.00094395
Iteration 25/25 | Loss: 0.00094395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094395
Iteration 2/1000 | Loss: 0.00004655
Iteration 3/1000 | Loss: 0.00003297
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002806
Iteration 6/1000 | Loss: 0.00002734
Iteration 7/1000 | Loss: 0.00002685
Iteration 8/1000 | Loss: 0.00002651
Iteration 9/1000 | Loss: 0.00002618
Iteration 10/1000 | Loss: 0.00002590
Iteration 11/1000 | Loss: 0.00002570
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002549
Iteration 15/1000 | Loss: 0.00002548
Iteration 16/1000 | Loss: 0.00002548
Iteration 17/1000 | Loss: 0.00002548
Iteration 18/1000 | Loss: 0.00002547
Iteration 19/1000 | Loss: 0.00002547
Iteration 20/1000 | Loss: 0.00002545
Iteration 21/1000 | Loss: 0.00002543
Iteration 22/1000 | Loss: 0.00002542
Iteration 23/1000 | Loss: 0.00002541
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002536
Iteration 27/1000 | Loss: 0.00002536
Iteration 28/1000 | Loss: 0.00002535
Iteration 29/1000 | Loss: 0.00002527
Iteration 30/1000 | Loss: 0.00002527
Iteration 31/1000 | Loss: 0.00002518
Iteration 32/1000 | Loss: 0.00002513
Iteration 33/1000 | Loss: 0.00002512
Iteration 34/1000 | Loss: 0.00002511
Iteration 35/1000 | Loss: 0.00002511
Iteration 36/1000 | Loss: 0.00002511
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002510
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002509
Iteration 43/1000 | Loss: 0.00002508
Iteration 44/1000 | Loss: 0.00002508
Iteration 45/1000 | Loss: 0.00002508
Iteration 46/1000 | Loss: 0.00002508
Iteration 47/1000 | Loss: 0.00002508
Iteration 48/1000 | Loss: 0.00002508
Iteration 49/1000 | Loss: 0.00002508
Iteration 50/1000 | Loss: 0.00002507
Iteration 51/1000 | Loss: 0.00002507
Iteration 52/1000 | Loss: 0.00002507
Iteration 53/1000 | Loss: 0.00002507
Iteration 54/1000 | Loss: 0.00002507
Iteration 55/1000 | Loss: 0.00002507
Iteration 56/1000 | Loss: 0.00002506
Iteration 57/1000 | Loss: 0.00002506
Iteration 58/1000 | Loss: 0.00002505
Iteration 59/1000 | Loss: 0.00002505
Iteration 60/1000 | Loss: 0.00002505
Iteration 61/1000 | Loss: 0.00002505
Iteration 62/1000 | Loss: 0.00002504
Iteration 63/1000 | Loss: 0.00002504
Iteration 64/1000 | Loss: 0.00002504
Iteration 65/1000 | Loss: 0.00002504
Iteration 66/1000 | Loss: 0.00002503
Iteration 67/1000 | Loss: 0.00002503
Iteration 68/1000 | Loss: 0.00002503
Iteration 69/1000 | Loss: 0.00002502
Iteration 70/1000 | Loss: 0.00002502
Iteration 71/1000 | Loss: 0.00002501
Iteration 72/1000 | Loss: 0.00002501
Iteration 73/1000 | Loss: 0.00002500
Iteration 74/1000 | Loss: 0.00002500
Iteration 75/1000 | Loss: 0.00002500
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002499
Iteration 78/1000 | Loss: 0.00002499
Iteration 79/1000 | Loss: 0.00002499
Iteration 80/1000 | Loss: 0.00002499
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002497
Iteration 85/1000 | Loss: 0.00002497
Iteration 86/1000 | Loss: 0.00002497
Iteration 87/1000 | Loss: 0.00002497
Iteration 88/1000 | Loss: 0.00002496
Iteration 89/1000 | Loss: 0.00002496
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002494
Iteration 95/1000 | Loss: 0.00002494
Iteration 96/1000 | Loss: 0.00002494
Iteration 97/1000 | Loss: 0.00002494
Iteration 98/1000 | Loss: 0.00002493
Iteration 99/1000 | Loss: 0.00002493
Iteration 100/1000 | Loss: 0.00002493
Iteration 101/1000 | Loss: 0.00002493
Iteration 102/1000 | Loss: 0.00002492
Iteration 103/1000 | Loss: 0.00002492
Iteration 104/1000 | Loss: 0.00002492
Iteration 105/1000 | Loss: 0.00002492
Iteration 106/1000 | Loss: 0.00002491
Iteration 107/1000 | Loss: 0.00002491
Iteration 108/1000 | Loss: 0.00002491
Iteration 109/1000 | Loss: 0.00002491
Iteration 110/1000 | Loss: 0.00002491
Iteration 111/1000 | Loss: 0.00002491
Iteration 112/1000 | Loss: 0.00002490
Iteration 113/1000 | Loss: 0.00002490
Iteration 114/1000 | Loss: 0.00002490
Iteration 115/1000 | Loss: 0.00002490
Iteration 116/1000 | Loss: 0.00002490
Iteration 117/1000 | Loss: 0.00002489
Iteration 118/1000 | Loss: 0.00002489
Iteration 119/1000 | Loss: 0.00002489
Iteration 120/1000 | Loss: 0.00002489
Iteration 121/1000 | Loss: 0.00002488
Iteration 122/1000 | Loss: 0.00002488
Iteration 123/1000 | Loss: 0.00002488
Iteration 124/1000 | Loss: 0.00002488
Iteration 125/1000 | Loss: 0.00002488
Iteration 126/1000 | Loss: 0.00002488
Iteration 127/1000 | Loss: 0.00002487
Iteration 128/1000 | Loss: 0.00002487
Iteration 129/1000 | Loss: 0.00002487
Iteration 130/1000 | Loss: 0.00002487
Iteration 131/1000 | Loss: 0.00002487
Iteration 132/1000 | Loss: 0.00002487
Iteration 133/1000 | Loss: 0.00002486
Iteration 134/1000 | Loss: 0.00002486
Iteration 135/1000 | Loss: 0.00002486
Iteration 136/1000 | Loss: 0.00002486
Iteration 137/1000 | Loss: 0.00002486
Iteration 138/1000 | Loss: 0.00002486
Iteration 139/1000 | Loss: 0.00002486
Iteration 140/1000 | Loss: 0.00002486
Iteration 141/1000 | Loss: 0.00002486
Iteration 142/1000 | Loss: 0.00002486
Iteration 143/1000 | Loss: 0.00002486
Iteration 144/1000 | Loss: 0.00002486
Iteration 145/1000 | Loss: 0.00002485
Iteration 146/1000 | Loss: 0.00002485
Iteration 147/1000 | Loss: 0.00002485
Iteration 148/1000 | Loss: 0.00002485
Iteration 149/1000 | Loss: 0.00002485
Iteration 150/1000 | Loss: 0.00002485
Iteration 151/1000 | Loss: 0.00002484
Iteration 152/1000 | Loss: 0.00002484
Iteration 153/1000 | Loss: 0.00002484
Iteration 154/1000 | Loss: 0.00002484
Iteration 155/1000 | Loss: 0.00002484
Iteration 156/1000 | Loss: 0.00002484
Iteration 157/1000 | Loss: 0.00002484
Iteration 158/1000 | Loss: 0.00002484
Iteration 159/1000 | Loss: 0.00002484
Iteration 160/1000 | Loss: 0.00002484
Iteration 161/1000 | Loss: 0.00002484
Iteration 162/1000 | Loss: 0.00002484
Iteration 163/1000 | Loss: 0.00002484
Iteration 164/1000 | Loss: 0.00002483
Iteration 165/1000 | Loss: 0.00002483
Iteration 166/1000 | Loss: 0.00002483
Iteration 167/1000 | Loss: 0.00002483
Iteration 168/1000 | Loss: 0.00002483
Iteration 169/1000 | Loss: 0.00002483
Iteration 170/1000 | Loss: 0.00002483
Iteration 171/1000 | Loss: 0.00002483
Iteration 172/1000 | Loss: 0.00002483
Iteration 173/1000 | Loss: 0.00002483
Iteration 174/1000 | Loss: 0.00002483
Iteration 175/1000 | Loss: 0.00002483
Iteration 176/1000 | Loss: 0.00002483
Iteration 177/1000 | Loss: 0.00002483
Iteration 178/1000 | Loss: 0.00002483
Iteration 179/1000 | Loss: 0.00002483
Iteration 180/1000 | Loss: 0.00002483
Iteration 181/1000 | Loss: 0.00002483
Iteration 182/1000 | Loss: 0.00002483
Iteration 183/1000 | Loss: 0.00002483
Iteration 184/1000 | Loss: 0.00002483
Iteration 185/1000 | Loss: 0.00002483
Iteration 186/1000 | Loss: 0.00002483
Iteration 187/1000 | Loss: 0.00002483
Iteration 188/1000 | Loss: 0.00002483
Iteration 189/1000 | Loss: 0.00002483
Iteration 190/1000 | Loss: 0.00002483
Iteration 191/1000 | Loss: 0.00002483
Iteration 192/1000 | Loss: 0.00002483
Iteration 193/1000 | Loss: 0.00002483
Iteration 194/1000 | Loss: 0.00002483
Iteration 195/1000 | Loss: 0.00002483
Iteration 196/1000 | Loss: 0.00002483
Iteration 197/1000 | Loss: 0.00002483
Iteration 198/1000 | Loss: 0.00002483
Iteration 199/1000 | Loss: 0.00002483
Iteration 200/1000 | Loss: 0.00002483
Iteration 201/1000 | Loss: 0.00002483
Iteration 202/1000 | Loss: 0.00002483
Iteration 203/1000 | Loss: 0.00002483
Iteration 204/1000 | Loss: 0.00002483
Iteration 205/1000 | Loss: 0.00002483
Iteration 206/1000 | Loss: 0.00002483
Iteration 207/1000 | Loss: 0.00002483
Iteration 208/1000 | Loss: 0.00002483
Iteration 209/1000 | Loss: 0.00002483
Iteration 210/1000 | Loss: 0.00002483
Iteration 211/1000 | Loss: 0.00002483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.4829167159623466e-05, 2.4829167159623466e-05, 2.4829167159623466e-05, 2.4829167159623466e-05, 2.4829167159623466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4829167159623466e-05

Optimization complete. Final v2v error: 4.055284023284912 mm

Highest mean error: 5.196299076080322 mm for frame 90

Lowest mean error: 3.0770280361175537 mm for frame 5

Saving results

Total time: 56.525436878204346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00295750
Iteration 2/25 | Loss: 0.00133520
Iteration 3/25 | Loss: 0.00125910
Iteration 4/25 | Loss: 0.00123702
Iteration 5/25 | Loss: 0.00122773
Iteration 6/25 | Loss: 0.00122571
Iteration 7/25 | Loss: 0.00122486
Iteration 8/25 | Loss: 0.00122486
Iteration 9/25 | Loss: 0.00122486
Iteration 10/25 | Loss: 0.00122486
Iteration 11/25 | Loss: 0.00122486
Iteration 12/25 | Loss: 0.00122486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001224864274263382, 0.001224864274263382, 0.001224864274263382, 0.001224864274263382, 0.001224864274263382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001224864274263382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21725667
Iteration 2/25 | Loss: 0.00141363
Iteration 3/25 | Loss: 0.00141363
Iteration 4/25 | Loss: 0.00141363
Iteration 5/25 | Loss: 0.00141363
Iteration 6/25 | Loss: 0.00141363
Iteration 7/25 | Loss: 0.00141363
Iteration 8/25 | Loss: 0.00141363
Iteration 9/25 | Loss: 0.00141363
Iteration 10/25 | Loss: 0.00141363
Iteration 11/25 | Loss: 0.00141363
Iteration 12/25 | Loss: 0.00141363
Iteration 13/25 | Loss: 0.00141363
Iteration 14/25 | Loss: 0.00141363
Iteration 15/25 | Loss: 0.00141363
Iteration 16/25 | Loss: 0.00141363
Iteration 17/25 | Loss: 0.00141363
Iteration 18/25 | Loss: 0.00141363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001413630205206573, 0.001413630205206573, 0.001413630205206573, 0.001413630205206573, 0.001413630205206573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001413630205206573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141363
Iteration 2/1000 | Loss: 0.00004117
Iteration 3/1000 | Loss: 0.00002532
Iteration 4/1000 | Loss: 0.00002118
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001731
Iteration 17/1000 | Loss: 0.00001731
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001730
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001729
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001708
Iteration 25/1000 | Loss: 0.00001708
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001707
Iteration 28/1000 | Loss: 0.00001707
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001701
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001700
Iteration 35/1000 | Loss: 0.00001700
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001699
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00001698
Iteration 40/1000 | Loss: 0.00001697
Iteration 41/1000 | Loss: 0.00001697
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001696
Iteration 44/1000 | Loss: 0.00001696
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001694
Iteration 49/1000 | Loss: 0.00001693
Iteration 50/1000 | Loss: 0.00001693
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001687
Iteration 69/1000 | Loss: 0.00001687
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001683
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001682
Iteration 86/1000 | Loss: 0.00001682
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001681
Iteration 97/1000 | Loss: 0.00001681
Iteration 98/1000 | Loss: 0.00001681
Iteration 99/1000 | Loss: 0.00001681
Iteration 100/1000 | Loss: 0.00001681
Iteration 101/1000 | Loss: 0.00001681
Iteration 102/1000 | Loss: 0.00001681
Iteration 103/1000 | Loss: 0.00001681
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6810945453471504e-05, 1.6810945453471504e-05, 1.6810945453471504e-05, 1.6810945453471504e-05, 1.6810945453471504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6810945453471504e-05

Optimization complete. Final v2v error: 3.4984583854675293 mm

Highest mean error: 3.9477128982543945 mm for frame 136

Lowest mean error: 3.242790937423706 mm for frame 87

Saving results

Total time: 37.04713726043701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558934
Iteration 2/25 | Loss: 0.00152033
Iteration 3/25 | Loss: 0.00137724
Iteration 4/25 | Loss: 0.00136079
Iteration 5/25 | Loss: 0.00135568
Iteration 6/25 | Loss: 0.00135433
Iteration 7/25 | Loss: 0.00135433
Iteration 8/25 | Loss: 0.00135433
Iteration 9/25 | Loss: 0.00135433
Iteration 10/25 | Loss: 0.00135433
Iteration 11/25 | Loss: 0.00135433
Iteration 12/25 | Loss: 0.00135433
Iteration 13/25 | Loss: 0.00135433
Iteration 14/25 | Loss: 0.00135433
Iteration 15/25 | Loss: 0.00135433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001354331150650978, 0.001354331150650978, 0.001354331150650978, 0.001354331150650978, 0.001354331150650978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001354331150650978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56202209
Iteration 2/25 | Loss: 0.00123349
Iteration 3/25 | Loss: 0.00123348
Iteration 4/25 | Loss: 0.00123348
Iteration 5/25 | Loss: 0.00123348
Iteration 6/25 | Loss: 0.00123348
Iteration 7/25 | Loss: 0.00123348
Iteration 8/25 | Loss: 0.00123348
Iteration 9/25 | Loss: 0.00123348
Iteration 10/25 | Loss: 0.00123348
Iteration 11/25 | Loss: 0.00123348
Iteration 12/25 | Loss: 0.00123348
Iteration 13/25 | Loss: 0.00123348
Iteration 14/25 | Loss: 0.00123348
Iteration 15/25 | Loss: 0.00123348
Iteration 16/25 | Loss: 0.00123348
Iteration 17/25 | Loss: 0.00123348
Iteration 18/25 | Loss: 0.00123348
Iteration 19/25 | Loss: 0.00123348
Iteration 20/25 | Loss: 0.00123348
Iteration 21/25 | Loss: 0.00123348
Iteration 22/25 | Loss: 0.00123348
Iteration 23/25 | Loss: 0.00123348
Iteration 24/25 | Loss: 0.00123348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012334800558164716, 0.0012334800558164716, 0.0012334800558164716, 0.0012334800558164716, 0.0012334800558164716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012334800558164716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123348
Iteration 2/1000 | Loss: 0.00003345
Iteration 3/1000 | Loss: 0.00002473
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00002174
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00002026
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00002008
Iteration 12/1000 | Loss: 0.00002001
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001998
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001974
Iteration 20/1000 | Loss: 0.00001968
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001963
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001960
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001949
Iteration 37/1000 | Loss: 0.00001947
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001942
Iteration 51/1000 | Loss: 0.00001942
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001941
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001938
Iteration 64/1000 | Loss: 0.00001938
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001937
Iteration 67/1000 | Loss: 0.00001937
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001937
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.9371898815734312e-05, 1.9371898815734312e-05, 1.9371898815734312e-05, 1.9371898815734312e-05, 1.9371898815734312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9371898815734312e-05

Optimization complete. Final v2v error: 3.6990857124328613 mm

Highest mean error: 4.355024814605713 mm for frame 104

Lowest mean error: 3.258164405822754 mm for frame 41

Saving results

Total time: 37.20271134376526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426877
Iteration 2/25 | Loss: 0.00146664
Iteration 3/25 | Loss: 0.00128100
Iteration 4/25 | Loss: 0.00126029
Iteration 5/25 | Loss: 0.00125725
Iteration 6/25 | Loss: 0.00125656
Iteration 7/25 | Loss: 0.00125656
Iteration 8/25 | Loss: 0.00125656
Iteration 9/25 | Loss: 0.00125656
Iteration 10/25 | Loss: 0.00125656
Iteration 11/25 | Loss: 0.00125656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012565609067678452, 0.0012565609067678452, 0.0012565609067678452, 0.0012565609067678452, 0.0012565609067678452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012565609067678452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35716939
Iteration 2/25 | Loss: 0.00104242
Iteration 3/25 | Loss: 0.00104242
Iteration 4/25 | Loss: 0.00104242
Iteration 5/25 | Loss: 0.00104242
Iteration 6/25 | Loss: 0.00104241
Iteration 7/25 | Loss: 0.00104241
Iteration 8/25 | Loss: 0.00104241
Iteration 9/25 | Loss: 0.00104241
Iteration 10/25 | Loss: 0.00104241
Iteration 11/25 | Loss: 0.00104241
Iteration 12/25 | Loss: 0.00104241
Iteration 13/25 | Loss: 0.00104241
Iteration 14/25 | Loss: 0.00104241
Iteration 15/25 | Loss: 0.00104241
Iteration 16/25 | Loss: 0.00104241
Iteration 17/25 | Loss: 0.00104241
Iteration 18/25 | Loss: 0.00104241
Iteration 19/25 | Loss: 0.00104241
Iteration 20/25 | Loss: 0.00104241
Iteration 21/25 | Loss: 0.00104241
Iteration 22/25 | Loss: 0.00104241
Iteration 23/25 | Loss: 0.00104241
Iteration 24/25 | Loss: 0.00104241
Iteration 25/25 | Loss: 0.00104241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104241
Iteration 2/1000 | Loss: 0.00002849
Iteration 3/1000 | Loss: 0.00001816
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001238
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001208
Iteration 44/1000 | Loss: 0.00001207
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001206
Iteration 48/1000 | Loss: 0.00001206
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001186
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001173
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001172
Iteration 114/1000 | Loss: 0.00001172
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001171
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001168
Iteration 127/1000 | Loss: 0.00001168
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001167
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001164
Iteration 153/1000 | Loss: 0.00001164
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001164
Iteration 159/1000 | Loss: 0.00001164
Iteration 160/1000 | Loss: 0.00001164
Iteration 161/1000 | Loss: 0.00001164
Iteration 162/1000 | Loss: 0.00001164
Iteration 163/1000 | Loss: 0.00001164
Iteration 164/1000 | Loss: 0.00001164
Iteration 165/1000 | Loss: 0.00001164
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001162
Iteration 171/1000 | Loss: 0.00001162
Iteration 172/1000 | Loss: 0.00001162
Iteration 173/1000 | Loss: 0.00001162
Iteration 174/1000 | Loss: 0.00001162
Iteration 175/1000 | Loss: 0.00001162
Iteration 176/1000 | Loss: 0.00001162
Iteration 177/1000 | Loss: 0.00001162
Iteration 178/1000 | Loss: 0.00001161
Iteration 179/1000 | Loss: 0.00001161
Iteration 180/1000 | Loss: 0.00001161
Iteration 181/1000 | Loss: 0.00001161
Iteration 182/1000 | Loss: 0.00001161
Iteration 183/1000 | Loss: 0.00001161
Iteration 184/1000 | Loss: 0.00001161
Iteration 185/1000 | Loss: 0.00001161
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001161
Iteration 191/1000 | Loss: 0.00001161
Iteration 192/1000 | Loss: 0.00001161
Iteration 193/1000 | Loss: 0.00001160
Iteration 194/1000 | Loss: 0.00001160
Iteration 195/1000 | Loss: 0.00001160
Iteration 196/1000 | Loss: 0.00001160
Iteration 197/1000 | Loss: 0.00001160
Iteration 198/1000 | Loss: 0.00001160
Iteration 199/1000 | Loss: 0.00001160
Iteration 200/1000 | Loss: 0.00001160
Iteration 201/1000 | Loss: 0.00001160
Iteration 202/1000 | Loss: 0.00001160
Iteration 203/1000 | Loss: 0.00001160
Iteration 204/1000 | Loss: 0.00001160
Iteration 205/1000 | Loss: 0.00001160
Iteration 206/1000 | Loss: 0.00001160
Iteration 207/1000 | Loss: 0.00001160
Iteration 208/1000 | Loss: 0.00001160
Iteration 209/1000 | Loss: 0.00001160
Iteration 210/1000 | Loss: 0.00001160
Iteration 211/1000 | Loss: 0.00001160
Iteration 212/1000 | Loss: 0.00001160
Iteration 213/1000 | Loss: 0.00001160
Iteration 214/1000 | Loss: 0.00001160
Iteration 215/1000 | Loss: 0.00001160
Iteration 216/1000 | Loss: 0.00001160
Iteration 217/1000 | Loss: 0.00001160
Iteration 218/1000 | Loss: 0.00001160
Iteration 219/1000 | Loss: 0.00001160
Iteration 220/1000 | Loss: 0.00001160
Iteration 221/1000 | Loss: 0.00001160
Iteration 222/1000 | Loss: 0.00001160
Iteration 223/1000 | Loss: 0.00001160
Iteration 224/1000 | Loss: 0.00001160
Iteration 225/1000 | Loss: 0.00001160
Iteration 226/1000 | Loss: 0.00001160
Iteration 227/1000 | Loss: 0.00001160
Iteration 228/1000 | Loss: 0.00001160
Iteration 229/1000 | Loss: 0.00001160
Iteration 230/1000 | Loss: 0.00001160
Iteration 231/1000 | Loss: 0.00001160
Iteration 232/1000 | Loss: 0.00001160
Iteration 233/1000 | Loss: 0.00001160
Iteration 234/1000 | Loss: 0.00001160
Iteration 235/1000 | Loss: 0.00001160
Iteration 236/1000 | Loss: 0.00001160
Iteration 237/1000 | Loss: 0.00001160
Iteration 238/1000 | Loss: 0.00001160
Iteration 239/1000 | Loss: 0.00001160
Iteration 240/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.1595173418754712e-05, 1.1595173418754712e-05, 1.1595173418754712e-05, 1.1595173418754712e-05, 1.1595173418754712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1595173418754712e-05

Optimization complete. Final v2v error: 2.918889045715332 mm

Highest mean error: 3.5090694427490234 mm for frame 90

Lowest mean error: 2.737718105316162 mm for frame 216

Saving results

Total time: 49.146326780319214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899486
Iteration 2/25 | Loss: 0.00141791
Iteration 3/25 | Loss: 0.00132324
Iteration 4/25 | Loss: 0.00131015
Iteration 5/25 | Loss: 0.00130714
Iteration 6/25 | Loss: 0.00130687
Iteration 7/25 | Loss: 0.00130687
Iteration 8/25 | Loss: 0.00130687
Iteration 9/25 | Loss: 0.00130687
Iteration 10/25 | Loss: 0.00130687
Iteration 11/25 | Loss: 0.00130687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013068713014945388, 0.0013068713014945388, 0.0013068713014945388, 0.0013068713014945388, 0.0013068713014945388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013068713014945388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.38004971
Iteration 2/25 | Loss: 0.00148445
Iteration 3/25 | Loss: 0.00148441
Iteration 4/25 | Loss: 0.00148441
Iteration 5/25 | Loss: 0.00148441
Iteration 6/25 | Loss: 0.00148441
Iteration 7/25 | Loss: 0.00148441
Iteration 8/25 | Loss: 0.00148441
Iteration 9/25 | Loss: 0.00148441
Iteration 10/25 | Loss: 0.00148441
Iteration 11/25 | Loss: 0.00148441
Iteration 12/25 | Loss: 0.00148441
Iteration 13/25 | Loss: 0.00148441
Iteration 14/25 | Loss: 0.00148441
Iteration 15/25 | Loss: 0.00148441
Iteration 16/25 | Loss: 0.00148441
Iteration 17/25 | Loss: 0.00148441
Iteration 18/25 | Loss: 0.00148441
Iteration 19/25 | Loss: 0.00148441
Iteration 20/25 | Loss: 0.00148441
Iteration 21/25 | Loss: 0.00148441
Iteration 22/25 | Loss: 0.00148441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014844060642644763, 0.0014844060642644763, 0.0014844060642644763, 0.0014844060642644763, 0.0014844060642644763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014844060642644763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148441
Iteration 2/1000 | Loss: 0.00004071
Iteration 3/1000 | Loss: 0.00002824
Iteration 4/1000 | Loss: 0.00002297
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002095
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001948
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001874
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001816
Iteration 16/1000 | Loss: 0.00001814
Iteration 17/1000 | Loss: 0.00001810
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001806
Iteration 20/1000 | Loss: 0.00001806
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001805
Iteration 25/1000 | Loss: 0.00001805
Iteration 26/1000 | Loss: 0.00001805
Iteration 27/1000 | Loss: 0.00001804
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001803
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00001803
Iteration 32/1000 | Loss: 0.00001802
Iteration 33/1000 | Loss: 0.00001802
Iteration 34/1000 | Loss: 0.00001802
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001800
Iteration 40/1000 | Loss: 0.00001800
Iteration 41/1000 | Loss: 0.00001800
Iteration 42/1000 | Loss: 0.00001799
Iteration 43/1000 | Loss: 0.00001799
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001798
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001798
Iteration 51/1000 | Loss: 0.00001798
Iteration 52/1000 | Loss: 0.00001798
Iteration 53/1000 | Loss: 0.00001798
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001798
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00001797
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001797
Iteration 61/1000 | Loss: 0.00001797
Iteration 62/1000 | Loss: 0.00001797
Iteration 63/1000 | Loss: 0.00001797
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001796
Iteration 72/1000 | Loss: 0.00001796
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001795
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001794
Iteration 84/1000 | Loss: 0.00001794
Iteration 85/1000 | Loss: 0.00001794
Iteration 86/1000 | Loss: 0.00001793
Iteration 87/1000 | Loss: 0.00001793
Iteration 88/1000 | Loss: 0.00001793
Iteration 89/1000 | Loss: 0.00001793
Iteration 90/1000 | Loss: 0.00001793
Iteration 91/1000 | Loss: 0.00001793
Iteration 92/1000 | Loss: 0.00001793
Iteration 93/1000 | Loss: 0.00001793
Iteration 94/1000 | Loss: 0.00001793
Iteration 95/1000 | Loss: 0.00001793
Iteration 96/1000 | Loss: 0.00001793
Iteration 97/1000 | Loss: 0.00001793
Iteration 98/1000 | Loss: 0.00001792
Iteration 99/1000 | Loss: 0.00001792
Iteration 100/1000 | Loss: 0.00001792
Iteration 101/1000 | Loss: 0.00001792
Iteration 102/1000 | Loss: 0.00001792
Iteration 103/1000 | Loss: 0.00001792
Iteration 104/1000 | Loss: 0.00001792
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001792
Iteration 107/1000 | Loss: 0.00001792
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001791
Iteration 111/1000 | Loss: 0.00001791
Iteration 112/1000 | Loss: 0.00001791
Iteration 113/1000 | Loss: 0.00001791
Iteration 114/1000 | Loss: 0.00001791
Iteration 115/1000 | Loss: 0.00001791
Iteration 116/1000 | Loss: 0.00001791
Iteration 117/1000 | Loss: 0.00001791
Iteration 118/1000 | Loss: 0.00001791
Iteration 119/1000 | Loss: 0.00001791
Iteration 120/1000 | Loss: 0.00001790
Iteration 121/1000 | Loss: 0.00001790
Iteration 122/1000 | Loss: 0.00001790
Iteration 123/1000 | Loss: 0.00001790
Iteration 124/1000 | Loss: 0.00001790
Iteration 125/1000 | Loss: 0.00001790
Iteration 126/1000 | Loss: 0.00001790
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001790
Iteration 129/1000 | Loss: 0.00001789
Iteration 130/1000 | Loss: 0.00001789
Iteration 131/1000 | Loss: 0.00001789
Iteration 132/1000 | Loss: 0.00001789
Iteration 133/1000 | Loss: 0.00001789
Iteration 134/1000 | Loss: 0.00001788
Iteration 135/1000 | Loss: 0.00001788
Iteration 136/1000 | Loss: 0.00001788
Iteration 137/1000 | Loss: 0.00001788
Iteration 138/1000 | Loss: 0.00001788
Iteration 139/1000 | Loss: 0.00001788
Iteration 140/1000 | Loss: 0.00001788
Iteration 141/1000 | Loss: 0.00001788
Iteration 142/1000 | Loss: 0.00001788
Iteration 143/1000 | Loss: 0.00001788
Iteration 144/1000 | Loss: 0.00001788
Iteration 145/1000 | Loss: 0.00001788
Iteration 146/1000 | Loss: 0.00001787
Iteration 147/1000 | Loss: 0.00001787
Iteration 148/1000 | Loss: 0.00001787
Iteration 149/1000 | Loss: 0.00001787
Iteration 150/1000 | Loss: 0.00001787
Iteration 151/1000 | Loss: 0.00001787
Iteration 152/1000 | Loss: 0.00001787
Iteration 153/1000 | Loss: 0.00001787
Iteration 154/1000 | Loss: 0.00001787
Iteration 155/1000 | Loss: 0.00001787
Iteration 156/1000 | Loss: 0.00001787
Iteration 157/1000 | Loss: 0.00001787
Iteration 158/1000 | Loss: 0.00001787
Iteration 159/1000 | Loss: 0.00001787
Iteration 160/1000 | Loss: 0.00001787
Iteration 161/1000 | Loss: 0.00001787
Iteration 162/1000 | Loss: 0.00001787
Iteration 163/1000 | Loss: 0.00001787
Iteration 164/1000 | Loss: 0.00001787
Iteration 165/1000 | Loss: 0.00001786
Iteration 166/1000 | Loss: 0.00001786
Iteration 167/1000 | Loss: 0.00001786
Iteration 168/1000 | Loss: 0.00001786
Iteration 169/1000 | Loss: 0.00001786
Iteration 170/1000 | Loss: 0.00001786
Iteration 171/1000 | Loss: 0.00001786
Iteration 172/1000 | Loss: 0.00001786
Iteration 173/1000 | Loss: 0.00001786
Iteration 174/1000 | Loss: 0.00001786
Iteration 175/1000 | Loss: 0.00001786
Iteration 176/1000 | Loss: 0.00001786
Iteration 177/1000 | Loss: 0.00001785
Iteration 178/1000 | Loss: 0.00001785
Iteration 179/1000 | Loss: 0.00001785
Iteration 180/1000 | Loss: 0.00001785
Iteration 181/1000 | Loss: 0.00001785
Iteration 182/1000 | Loss: 0.00001785
Iteration 183/1000 | Loss: 0.00001785
Iteration 184/1000 | Loss: 0.00001785
Iteration 185/1000 | Loss: 0.00001785
Iteration 186/1000 | Loss: 0.00001785
Iteration 187/1000 | Loss: 0.00001785
Iteration 188/1000 | Loss: 0.00001785
Iteration 189/1000 | Loss: 0.00001785
Iteration 190/1000 | Loss: 0.00001785
Iteration 191/1000 | Loss: 0.00001785
Iteration 192/1000 | Loss: 0.00001785
Iteration 193/1000 | Loss: 0.00001785
Iteration 194/1000 | Loss: 0.00001785
Iteration 195/1000 | Loss: 0.00001785
Iteration 196/1000 | Loss: 0.00001785
Iteration 197/1000 | Loss: 0.00001785
Iteration 198/1000 | Loss: 0.00001784
Iteration 199/1000 | Loss: 0.00001784
Iteration 200/1000 | Loss: 0.00001784
Iteration 201/1000 | Loss: 0.00001784
Iteration 202/1000 | Loss: 0.00001784
Iteration 203/1000 | Loss: 0.00001784
Iteration 204/1000 | Loss: 0.00001784
Iteration 205/1000 | Loss: 0.00001784
Iteration 206/1000 | Loss: 0.00001784
Iteration 207/1000 | Loss: 0.00001784
Iteration 208/1000 | Loss: 0.00001783
Iteration 209/1000 | Loss: 0.00001783
Iteration 210/1000 | Loss: 0.00001783
Iteration 211/1000 | Loss: 0.00001783
Iteration 212/1000 | Loss: 0.00001783
Iteration 213/1000 | Loss: 0.00001783
Iteration 214/1000 | Loss: 0.00001783
Iteration 215/1000 | Loss: 0.00001783
Iteration 216/1000 | Loss: 0.00001783
Iteration 217/1000 | Loss: 0.00001783
Iteration 218/1000 | Loss: 0.00001783
Iteration 219/1000 | Loss: 0.00001783
Iteration 220/1000 | Loss: 0.00001783
Iteration 221/1000 | Loss: 0.00001782
Iteration 222/1000 | Loss: 0.00001782
Iteration 223/1000 | Loss: 0.00001782
Iteration 224/1000 | Loss: 0.00001782
Iteration 225/1000 | Loss: 0.00001782
Iteration 226/1000 | Loss: 0.00001782
Iteration 227/1000 | Loss: 0.00001782
Iteration 228/1000 | Loss: 0.00001782
Iteration 229/1000 | Loss: 0.00001782
Iteration 230/1000 | Loss: 0.00001782
Iteration 231/1000 | Loss: 0.00001782
Iteration 232/1000 | Loss: 0.00001782
Iteration 233/1000 | Loss: 0.00001782
Iteration 234/1000 | Loss: 0.00001782
Iteration 235/1000 | Loss: 0.00001782
Iteration 236/1000 | Loss: 0.00001782
Iteration 237/1000 | Loss: 0.00001781
Iteration 238/1000 | Loss: 0.00001781
Iteration 239/1000 | Loss: 0.00001781
Iteration 240/1000 | Loss: 0.00001781
Iteration 241/1000 | Loss: 0.00001781
Iteration 242/1000 | Loss: 0.00001781
Iteration 243/1000 | Loss: 0.00001781
Iteration 244/1000 | Loss: 0.00001781
Iteration 245/1000 | Loss: 0.00001781
Iteration 246/1000 | Loss: 0.00001781
Iteration 247/1000 | Loss: 0.00001781
Iteration 248/1000 | Loss: 0.00001781
Iteration 249/1000 | Loss: 0.00001780
Iteration 250/1000 | Loss: 0.00001780
Iteration 251/1000 | Loss: 0.00001780
Iteration 252/1000 | Loss: 0.00001780
Iteration 253/1000 | Loss: 0.00001780
Iteration 254/1000 | Loss: 0.00001780
Iteration 255/1000 | Loss: 0.00001780
Iteration 256/1000 | Loss: 0.00001780
Iteration 257/1000 | Loss: 0.00001780
Iteration 258/1000 | Loss: 0.00001780
Iteration 259/1000 | Loss: 0.00001780
Iteration 260/1000 | Loss: 0.00001780
Iteration 261/1000 | Loss: 0.00001780
Iteration 262/1000 | Loss: 0.00001780
Iteration 263/1000 | Loss: 0.00001780
Iteration 264/1000 | Loss: 0.00001780
Iteration 265/1000 | Loss: 0.00001780
Iteration 266/1000 | Loss: 0.00001780
Iteration 267/1000 | Loss: 0.00001780
Iteration 268/1000 | Loss: 0.00001780
Iteration 269/1000 | Loss: 0.00001780
Iteration 270/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.7800493878894486e-05, 1.7800493878894486e-05, 1.7800493878894486e-05, 1.7800493878894486e-05, 1.7800493878894486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7800493878894486e-05

Optimization complete. Final v2v error: 3.604738712310791 mm

Highest mean error: 4.0956645011901855 mm for frame 65

Lowest mean error: 3.2306997776031494 mm for frame 47

Saving results

Total time: 43.26421356201172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038591
Iteration 2/25 | Loss: 0.00182407
Iteration 3/25 | Loss: 0.00154479
Iteration 4/25 | Loss: 0.00145917
Iteration 5/25 | Loss: 0.00139135
Iteration 6/25 | Loss: 0.00133933
Iteration 7/25 | Loss: 0.00133734
Iteration 8/25 | Loss: 0.00131451
Iteration 9/25 | Loss: 0.00131215
Iteration 10/25 | Loss: 0.00131068
Iteration 11/25 | Loss: 0.00131926
Iteration 12/25 | Loss: 0.00131126
Iteration 13/25 | Loss: 0.00130921
Iteration 14/25 | Loss: 0.00130895
Iteration 15/25 | Loss: 0.00130770
Iteration 16/25 | Loss: 0.00130735
Iteration 17/25 | Loss: 0.00130726
Iteration 18/25 | Loss: 0.00130725
Iteration 19/25 | Loss: 0.00130725
Iteration 20/25 | Loss: 0.00130725
Iteration 21/25 | Loss: 0.00130725
Iteration 22/25 | Loss: 0.00130725
Iteration 23/25 | Loss: 0.00130724
Iteration 24/25 | Loss: 0.00130724
Iteration 25/25 | Loss: 0.00130724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44115210
Iteration 2/25 | Loss: 0.00153226
Iteration 3/25 | Loss: 0.00120863
Iteration 4/25 | Loss: 0.00120863
Iteration 5/25 | Loss: 0.00120863
Iteration 6/25 | Loss: 0.00120863
Iteration 7/25 | Loss: 0.00120863
Iteration 8/25 | Loss: 0.00120863
Iteration 9/25 | Loss: 0.00120863
Iteration 10/25 | Loss: 0.00120863
Iteration 11/25 | Loss: 0.00120863
Iteration 12/25 | Loss: 0.00120863
Iteration 13/25 | Loss: 0.00120863
Iteration 14/25 | Loss: 0.00120863
Iteration 15/25 | Loss: 0.00120863
Iteration 16/25 | Loss: 0.00120863
Iteration 17/25 | Loss: 0.00120863
Iteration 18/25 | Loss: 0.00120863
Iteration 19/25 | Loss: 0.00120863
Iteration 20/25 | Loss: 0.00120863
Iteration 21/25 | Loss: 0.00120863
Iteration 22/25 | Loss: 0.00120863
Iteration 23/25 | Loss: 0.00120863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012086285278201103, 0.0012086285278201103, 0.0012086285278201103, 0.0012086285278201103, 0.0012086285278201103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012086285278201103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120863
Iteration 2/1000 | Loss: 0.00035155
Iteration 3/1000 | Loss: 0.00016441
Iteration 4/1000 | Loss: 0.00009578
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00024399
Iteration 7/1000 | Loss: 0.00059927
Iteration 8/1000 | Loss: 0.00151363
Iteration 9/1000 | Loss: 0.00005654
Iteration 10/1000 | Loss: 0.00018754
Iteration 11/1000 | Loss: 0.00025492
Iteration 12/1000 | Loss: 0.00010719
Iteration 13/1000 | Loss: 0.00006279
Iteration 14/1000 | Loss: 0.00002307
Iteration 15/1000 | Loss: 0.00002969
Iteration 16/1000 | Loss: 0.00002228
Iteration 17/1000 | Loss: 0.00024210
Iteration 18/1000 | Loss: 0.00002201
Iteration 19/1000 | Loss: 0.00002147
Iteration 20/1000 | Loss: 0.00002109
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002032
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00084501
Iteration 25/1000 | Loss: 0.00071981
Iteration 26/1000 | Loss: 0.00061940
Iteration 27/1000 | Loss: 0.00085836
Iteration 28/1000 | Loss: 0.00014950
Iteration 29/1000 | Loss: 0.00026935
Iteration 30/1000 | Loss: 0.00015266
Iteration 31/1000 | Loss: 0.00018356
Iteration 32/1000 | Loss: 0.00004690
Iteration 33/1000 | Loss: 0.00019800
Iteration 34/1000 | Loss: 0.00003318
Iteration 35/1000 | Loss: 0.00001942
Iteration 36/1000 | Loss: 0.00008699
Iteration 37/1000 | Loss: 0.00001830
Iteration 38/1000 | Loss: 0.00002450
Iteration 39/1000 | Loss: 0.00001986
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001474
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001440
Iteration 55/1000 | Loss: 0.00001440
Iteration 56/1000 | Loss: 0.00001440
Iteration 57/1000 | Loss: 0.00001440
Iteration 58/1000 | Loss: 0.00001440
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001429
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001424
Iteration 109/1000 | Loss: 0.00001424
Iteration 110/1000 | Loss: 0.00001424
Iteration 111/1000 | Loss: 0.00001424
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001424
Iteration 114/1000 | Loss: 0.00001424
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001423
Iteration 125/1000 | Loss: 0.00001423
Iteration 126/1000 | Loss: 0.00001423
Iteration 127/1000 | Loss: 0.00001423
Iteration 128/1000 | Loss: 0.00001423
Iteration 129/1000 | Loss: 0.00001423
Iteration 130/1000 | Loss: 0.00001423
Iteration 131/1000 | Loss: 0.00001423
Iteration 132/1000 | Loss: 0.00001423
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001423
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001422
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001422
Iteration 142/1000 | Loss: 0.00001422
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001421
Iteration 150/1000 | Loss: 0.00001421
Iteration 151/1000 | Loss: 0.00001421
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001421
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001421
Iteration 166/1000 | Loss: 0.00001421
Iteration 167/1000 | Loss: 0.00001421
Iteration 168/1000 | Loss: 0.00001421
Iteration 169/1000 | Loss: 0.00001421
Iteration 170/1000 | Loss: 0.00001421
Iteration 171/1000 | Loss: 0.00001421
Iteration 172/1000 | Loss: 0.00001421
Iteration 173/1000 | Loss: 0.00001421
Iteration 174/1000 | Loss: 0.00001421
Iteration 175/1000 | Loss: 0.00001421
Iteration 176/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4208234460966196e-05, 1.4208234460966196e-05, 1.4208234460966196e-05, 1.4208234460966196e-05, 1.4208234460966196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4208234460966196e-05

Optimization complete. Final v2v error: 3.20633602142334 mm

Highest mean error: 3.5475146770477295 mm for frame 93

Lowest mean error: 3.0017309188842773 mm for frame 10

Saving results

Total time: 97.66984724998474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842328
Iteration 2/25 | Loss: 0.00136188
Iteration 3/25 | Loss: 0.00126908
Iteration 4/25 | Loss: 0.00125817
Iteration 5/25 | Loss: 0.00125515
Iteration 6/25 | Loss: 0.00125515
Iteration 7/25 | Loss: 0.00125515
Iteration 8/25 | Loss: 0.00125515
Iteration 9/25 | Loss: 0.00125515
Iteration 10/25 | Loss: 0.00125515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012551526306197047, 0.0012551526306197047, 0.0012551526306197047, 0.0012551526306197047, 0.0012551526306197047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012551526306197047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.89189577
Iteration 2/25 | Loss: 0.00103253
Iteration 3/25 | Loss: 0.00103253
Iteration 4/25 | Loss: 0.00103253
Iteration 5/25 | Loss: 0.00103253
Iteration 6/25 | Loss: 0.00103253
Iteration 7/25 | Loss: 0.00103253
Iteration 8/25 | Loss: 0.00103253
Iteration 9/25 | Loss: 0.00103253
Iteration 10/25 | Loss: 0.00103253
Iteration 11/25 | Loss: 0.00103253
Iteration 12/25 | Loss: 0.00103253
Iteration 13/25 | Loss: 0.00103253
Iteration 14/25 | Loss: 0.00103253
Iteration 15/25 | Loss: 0.00103253
Iteration 16/25 | Loss: 0.00103253
Iteration 17/25 | Loss: 0.00103253
Iteration 18/25 | Loss: 0.00103253
Iteration 19/25 | Loss: 0.00103253
Iteration 20/25 | Loss: 0.00103253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010325289331376553, 0.0010325289331376553, 0.0010325289331376553, 0.0010325289331376553, 0.0010325289331376553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010325289331376553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103253
Iteration 2/1000 | Loss: 0.00002217
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001353
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001217
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001183
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001180
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001172
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001161
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001147
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001140
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001139
Iteration 46/1000 | Loss: 0.00001139
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001135
Iteration 54/1000 | Loss: 0.00001135
Iteration 55/1000 | Loss: 0.00001135
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001134
Iteration 58/1000 | Loss: 0.00001134
Iteration 59/1000 | Loss: 0.00001134
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001125
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001122
Iteration 91/1000 | Loss: 0.00001122
Iteration 92/1000 | Loss: 0.00001122
Iteration 93/1000 | Loss: 0.00001122
Iteration 94/1000 | Loss: 0.00001121
Iteration 95/1000 | Loss: 0.00001121
Iteration 96/1000 | Loss: 0.00001121
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001120
Iteration 99/1000 | Loss: 0.00001120
Iteration 100/1000 | Loss: 0.00001120
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001118
Iteration 113/1000 | Loss: 0.00001118
Iteration 114/1000 | Loss: 0.00001118
Iteration 115/1000 | Loss: 0.00001118
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001117
Iteration 124/1000 | Loss: 0.00001117
Iteration 125/1000 | Loss: 0.00001117
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001117
Iteration 128/1000 | Loss: 0.00001117
Iteration 129/1000 | Loss: 0.00001117
Iteration 130/1000 | Loss: 0.00001117
Iteration 131/1000 | Loss: 0.00001117
Iteration 132/1000 | Loss: 0.00001117
Iteration 133/1000 | Loss: 0.00001117
Iteration 134/1000 | Loss: 0.00001117
Iteration 135/1000 | Loss: 0.00001117
Iteration 136/1000 | Loss: 0.00001117
Iteration 137/1000 | Loss: 0.00001117
Iteration 138/1000 | Loss: 0.00001117
Iteration 139/1000 | Loss: 0.00001117
Iteration 140/1000 | Loss: 0.00001117
Iteration 141/1000 | Loss: 0.00001117
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001117
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001117
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.116520979849156e-05, 1.116520979849156e-05, 1.116520979849156e-05, 1.116520979849156e-05, 1.116520979849156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.116520979849156e-05

Optimization complete. Final v2v error: 2.8834707736968994 mm

Highest mean error: 3.1299378871917725 mm for frame 239

Lowest mean error: 2.70192289352417 mm for frame 131

Saving results

Total time: 42.57727861404419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965337
Iteration 2/25 | Loss: 0.00218702
Iteration 3/25 | Loss: 0.00178785
Iteration 4/25 | Loss: 0.00160782
Iteration 5/25 | Loss: 0.00171769
Iteration 6/25 | Loss: 0.00150853
Iteration 7/25 | Loss: 0.00151025
Iteration 8/25 | Loss: 0.00143676
Iteration 9/25 | Loss: 0.00147923
Iteration 10/25 | Loss: 0.00137685
Iteration 11/25 | Loss: 0.00136466
Iteration 12/25 | Loss: 0.00136164
Iteration 13/25 | Loss: 0.00135883
Iteration 14/25 | Loss: 0.00136555
Iteration 15/25 | Loss: 0.00135988
Iteration 16/25 | Loss: 0.00135450
Iteration 17/25 | Loss: 0.00135160
Iteration 18/25 | Loss: 0.00135123
Iteration 19/25 | Loss: 0.00135118
Iteration 20/25 | Loss: 0.00135117
Iteration 21/25 | Loss: 0.00135117
Iteration 22/25 | Loss: 0.00135117
Iteration 23/25 | Loss: 0.00135117
Iteration 24/25 | Loss: 0.00135117
Iteration 25/25 | Loss: 0.00135117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78697312
Iteration 2/25 | Loss: 0.00174875
Iteration 3/25 | Loss: 0.00174875
Iteration 4/25 | Loss: 0.00174875
Iteration 5/25 | Loss: 0.00174875
Iteration 6/25 | Loss: 0.00174875
Iteration 7/25 | Loss: 0.00174875
Iteration 8/25 | Loss: 0.00174875
Iteration 9/25 | Loss: 0.00174875
Iteration 10/25 | Loss: 0.00174875
Iteration 11/25 | Loss: 0.00174875
Iteration 12/25 | Loss: 0.00174875
Iteration 13/25 | Loss: 0.00174875
Iteration 14/25 | Loss: 0.00174875
Iteration 15/25 | Loss: 0.00174875
Iteration 16/25 | Loss: 0.00174875
Iteration 17/25 | Loss: 0.00174875
Iteration 18/25 | Loss: 0.00174875
Iteration 19/25 | Loss: 0.00174875
Iteration 20/25 | Loss: 0.00174875
Iteration 21/25 | Loss: 0.00174875
Iteration 22/25 | Loss: 0.00174875
Iteration 23/25 | Loss: 0.00174875
Iteration 24/25 | Loss: 0.00174875
Iteration 25/25 | Loss: 0.00174875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001748747774399817, 0.001748747774399817, 0.001748747774399817, 0.001748747774399817, 0.001748747774399817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001748747774399817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174875
Iteration 2/1000 | Loss: 0.00014136
Iteration 3/1000 | Loss: 0.00008986
Iteration 4/1000 | Loss: 0.00006553
Iteration 5/1000 | Loss: 0.00005659
Iteration 6/1000 | Loss: 0.00005235
Iteration 7/1000 | Loss: 0.00004961
Iteration 8/1000 | Loss: 0.00020065
Iteration 9/1000 | Loss: 0.00047682
Iteration 10/1000 | Loss: 0.00006183
Iteration 11/1000 | Loss: 0.00004468
Iteration 12/1000 | Loss: 0.00003821
Iteration 13/1000 | Loss: 0.00003284
Iteration 14/1000 | Loss: 0.00002857
Iteration 15/1000 | Loss: 0.00002586
Iteration 16/1000 | Loss: 0.00002400
Iteration 17/1000 | Loss: 0.00002272
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00025708
Iteration 20/1000 | Loss: 0.00091533
Iteration 21/1000 | Loss: 0.00006302
Iteration 22/1000 | Loss: 0.00003408
Iteration 23/1000 | Loss: 0.00002426
Iteration 24/1000 | Loss: 0.00002152
Iteration 25/1000 | Loss: 0.00027065
Iteration 26/1000 | Loss: 0.00010450
Iteration 27/1000 | Loss: 0.00025459
Iteration 28/1000 | Loss: 0.00020922
Iteration 29/1000 | Loss: 0.00003687
Iteration 30/1000 | Loss: 0.00002667
Iteration 31/1000 | Loss: 0.00002230
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001623
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001623
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001618
Iteration 53/1000 | Loss: 0.00001618
Iteration 54/1000 | Loss: 0.00001618
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001617
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001609
Iteration 93/1000 | Loss: 0.00001609
Iteration 94/1000 | Loss: 0.00001609
Iteration 95/1000 | Loss: 0.00001609
Iteration 96/1000 | Loss: 0.00001608
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001608
Iteration 99/1000 | Loss: 0.00001608
Iteration 100/1000 | Loss: 0.00001607
Iteration 101/1000 | Loss: 0.00001607
Iteration 102/1000 | Loss: 0.00001607
Iteration 103/1000 | Loss: 0.00001607
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001605
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001603
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001602
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001602
Iteration 144/1000 | Loss: 0.00001602
Iteration 145/1000 | Loss: 0.00001602
Iteration 146/1000 | Loss: 0.00001602
Iteration 147/1000 | Loss: 0.00001602
Iteration 148/1000 | Loss: 0.00001602
Iteration 149/1000 | Loss: 0.00001602
Iteration 150/1000 | Loss: 0.00001602
Iteration 151/1000 | Loss: 0.00001602
Iteration 152/1000 | Loss: 0.00001602
Iteration 153/1000 | Loss: 0.00001602
Iteration 154/1000 | Loss: 0.00001602
Iteration 155/1000 | Loss: 0.00001602
Iteration 156/1000 | Loss: 0.00001602
Iteration 157/1000 | Loss: 0.00001602
Iteration 158/1000 | Loss: 0.00001602
Iteration 159/1000 | Loss: 0.00001602
Iteration 160/1000 | Loss: 0.00001602
Iteration 161/1000 | Loss: 0.00001602
Iteration 162/1000 | Loss: 0.00001602
Iteration 163/1000 | Loss: 0.00001602
Iteration 164/1000 | Loss: 0.00001602
Iteration 165/1000 | Loss: 0.00001602
Iteration 166/1000 | Loss: 0.00001602
Iteration 167/1000 | Loss: 0.00001602
Iteration 168/1000 | Loss: 0.00001602
Iteration 169/1000 | Loss: 0.00001602
Iteration 170/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.602413976797834e-05, 1.602413976797834e-05, 1.602413976797834e-05, 1.602413976797834e-05, 1.602413976797834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.602413976797834e-05

Optimization complete. Final v2v error: 3.3639309406280518 mm

Highest mean error: 4.568755626678467 mm for frame 69

Lowest mean error: 2.980043411254883 mm for frame 108

Saving results

Total time: 93.05686593055725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391607
Iteration 2/25 | Loss: 0.00134930
Iteration 3/25 | Loss: 0.00127179
Iteration 4/25 | Loss: 0.00126515
Iteration 5/25 | Loss: 0.00126213
Iteration 6/25 | Loss: 0.00126213
Iteration 7/25 | Loss: 0.00126213
Iteration 8/25 | Loss: 0.00126213
Iteration 9/25 | Loss: 0.00126213
Iteration 10/25 | Loss: 0.00126213
Iteration 11/25 | Loss: 0.00126213
Iteration 12/25 | Loss: 0.00126213
Iteration 13/25 | Loss: 0.00126213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001262132660485804, 0.001262132660485804, 0.001262132660485804, 0.001262132660485804, 0.001262132660485804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001262132660485804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48058701
Iteration 2/25 | Loss: 0.00094825
Iteration 3/25 | Loss: 0.00094825
Iteration 4/25 | Loss: 0.00094825
Iteration 5/25 | Loss: 0.00094825
Iteration 6/25 | Loss: 0.00094825
Iteration 7/25 | Loss: 0.00094825
Iteration 8/25 | Loss: 0.00094825
Iteration 9/25 | Loss: 0.00094824
Iteration 10/25 | Loss: 0.00094824
Iteration 11/25 | Loss: 0.00094824
Iteration 12/25 | Loss: 0.00094824
Iteration 13/25 | Loss: 0.00094824
Iteration 14/25 | Loss: 0.00094824
Iteration 15/25 | Loss: 0.00094824
Iteration 16/25 | Loss: 0.00094824
Iteration 17/25 | Loss: 0.00094824
Iteration 18/25 | Loss: 0.00094824
Iteration 19/25 | Loss: 0.00094824
Iteration 20/25 | Loss: 0.00094824
Iteration 21/25 | Loss: 0.00094824
Iteration 22/25 | Loss: 0.00094824
Iteration 23/25 | Loss: 0.00094824
Iteration 24/25 | Loss: 0.00094824
Iteration 25/25 | Loss: 0.00094824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009482447640039027, 0.0009482447640039027, 0.0009482447640039027, 0.0009482447640039027, 0.0009482447640039027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009482447640039027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094824
Iteration 2/1000 | Loss: 0.00002595
Iteration 3/1000 | Loss: 0.00001746
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001438
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001369
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001256
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001249
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001243
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001242
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001238
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001237
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.2267664715182036e-05, 1.2267664715182036e-05, 1.2267664715182036e-05, 1.2267664715182036e-05, 1.2267664715182036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2267664715182036e-05

Optimization complete. Final v2v error: 3.0105037689208984 mm

Highest mean error: 3.1994736194610596 mm for frame 177

Lowest mean error: 2.8859946727752686 mm for frame 213

Saving results

Total time: 40.173596143722534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423572
Iteration 2/25 | Loss: 0.00136651
Iteration 3/25 | Loss: 0.00129070
Iteration 4/25 | Loss: 0.00128008
Iteration 5/25 | Loss: 0.00127626
Iteration 6/25 | Loss: 0.00127626
Iteration 7/25 | Loss: 0.00127626
Iteration 8/25 | Loss: 0.00127626
Iteration 9/25 | Loss: 0.00127626
Iteration 10/25 | Loss: 0.00127626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012762579135596752, 0.0012762579135596752, 0.0012762579135596752, 0.0012762579135596752, 0.0012762579135596752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012762579135596752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39469087
Iteration 2/25 | Loss: 0.00125136
Iteration 3/25 | Loss: 0.00125136
Iteration 4/25 | Loss: 0.00125136
Iteration 5/25 | Loss: 0.00125136
Iteration 6/25 | Loss: 0.00125136
Iteration 7/25 | Loss: 0.00125136
Iteration 8/25 | Loss: 0.00125136
Iteration 9/25 | Loss: 0.00125136
Iteration 10/25 | Loss: 0.00125136
Iteration 11/25 | Loss: 0.00125136
Iteration 12/25 | Loss: 0.00125136
Iteration 13/25 | Loss: 0.00125136
Iteration 14/25 | Loss: 0.00125136
Iteration 15/25 | Loss: 0.00125136
Iteration 16/25 | Loss: 0.00125136
Iteration 17/25 | Loss: 0.00125136
Iteration 18/25 | Loss: 0.00125136
Iteration 19/25 | Loss: 0.00125136
Iteration 20/25 | Loss: 0.00125136
Iteration 21/25 | Loss: 0.00125136
Iteration 22/25 | Loss: 0.00125136
Iteration 23/25 | Loss: 0.00125136
Iteration 24/25 | Loss: 0.00125136
Iteration 25/25 | Loss: 0.00125136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125136
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00001810
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001390
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001368
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001352
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001348
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001322
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001321
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001320
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.3165629752620589e-05, 1.3165629752620589e-05, 1.3165629752620589e-05, 1.3165629752620589e-05, 1.3165629752620589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3165629752620589e-05

Optimization complete. Final v2v error: 3.050111770629883 mm

Highest mean error: 3.194742441177368 mm for frame 167

Lowest mean error: 2.8884875774383545 mm for frame 50

Saving results

Total time: 36.22875428199768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761158
Iteration 2/25 | Loss: 0.00150475
Iteration 3/25 | Loss: 0.00128972
Iteration 4/25 | Loss: 0.00126542
Iteration 5/25 | Loss: 0.00125913
Iteration 6/25 | Loss: 0.00125754
Iteration 7/25 | Loss: 0.00125706
Iteration 8/25 | Loss: 0.00125686
Iteration 9/25 | Loss: 0.00125686
Iteration 10/25 | Loss: 0.00125681
Iteration 11/25 | Loss: 0.00125671
Iteration 12/25 | Loss: 0.00125671
Iteration 13/25 | Loss: 0.00125671
Iteration 14/25 | Loss: 0.00125671
Iteration 15/25 | Loss: 0.00125671
Iteration 16/25 | Loss: 0.00125671
Iteration 17/25 | Loss: 0.00125671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012567066587507725, 0.0012567066587507725, 0.0012567066587507725, 0.0012567066587507725, 0.0012567066587507725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012567066587507725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67351866
Iteration 2/25 | Loss: 0.00103751
Iteration 3/25 | Loss: 0.00103749
Iteration 4/25 | Loss: 0.00103749
Iteration 5/25 | Loss: 0.00103749
Iteration 6/25 | Loss: 0.00103749
Iteration 7/25 | Loss: 0.00103749
Iteration 8/25 | Loss: 0.00103749
Iteration 9/25 | Loss: 0.00103749
Iteration 10/25 | Loss: 0.00103749
Iteration 11/25 | Loss: 0.00103749
Iteration 12/25 | Loss: 0.00103749
Iteration 13/25 | Loss: 0.00103749
Iteration 14/25 | Loss: 0.00103749
Iteration 15/25 | Loss: 0.00103749
Iteration 16/25 | Loss: 0.00103749
Iteration 17/25 | Loss: 0.00103749
Iteration 18/25 | Loss: 0.00103749
Iteration 19/25 | Loss: 0.00103749
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010374916018918157, 0.0010374916018918157, 0.0010374916018918157, 0.0010374916018918157, 0.0010374916018918157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010374916018918157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103749
Iteration 2/1000 | Loss: 0.00002392
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001597
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001437
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001409
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001374
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001349
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Iteration 64/1000 | Loss: 0.00001349
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001349
Iteration 67/1000 | Loss: 0.00001348
Iteration 68/1000 | Loss: 0.00001348
Iteration 69/1000 | Loss: 0.00001348
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001347
Iteration 72/1000 | Loss: 0.00001347
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001347
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001342
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001341
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001340
Iteration 137/1000 | Loss: 0.00001340
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00001339
Iteration 149/1000 | Loss: 0.00001339
Iteration 150/1000 | Loss: 0.00001339
Iteration 151/1000 | Loss: 0.00001339
Iteration 152/1000 | Loss: 0.00001339
Iteration 153/1000 | Loss: 0.00001339
Iteration 154/1000 | Loss: 0.00001338
Iteration 155/1000 | Loss: 0.00001338
Iteration 156/1000 | Loss: 0.00001338
Iteration 157/1000 | Loss: 0.00001338
Iteration 158/1000 | Loss: 0.00001337
Iteration 159/1000 | Loss: 0.00001337
Iteration 160/1000 | Loss: 0.00001337
Iteration 161/1000 | Loss: 0.00001337
Iteration 162/1000 | Loss: 0.00001337
Iteration 163/1000 | Loss: 0.00001337
Iteration 164/1000 | Loss: 0.00001337
Iteration 165/1000 | Loss: 0.00001337
Iteration 166/1000 | Loss: 0.00001337
Iteration 167/1000 | Loss: 0.00001336
Iteration 168/1000 | Loss: 0.00001336
Iteration 169/1000 | Loss: 0.00001336
Iteration 170/1000 | Loss: 0.00001336
Iteration 171/1000 | Loss: 0.00001336
Iteration 172/1000 | Loss: 0.00001336
Iteration 173/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.3364638107304927e-05, 1.3364638107304927e-05, 1.3364638107304927e-05, 1.3364638107304927e-05, 1.3364638107304927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3364638107304927e-05

Optimization complete. Final v2v error: 3.106189250946045 mm

Highest mean error: 3.4507336616516113 mm for frame 138

Lowest mean error: 2.8587169647216797 mm for frame 56

Saving results

Total time: 54.95721244812012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386734
Iteration 2/25 | Loss: 0.00140030
Iteration 3/25 | Loss: 0.00127578
Iteration 4/25 | Loss: 0.00125333
Iteration 5/25 | Loss: 0.00124686
Iteration 6/25 | Loss: 0.00124543
Iteration 7/25 | Loss: 0.00124515
Iteration 8/25 | Loss: 0.00124515
Iteration 9/25 | Loss: 0.00124515
Iteration 10/25 | Loss: 0.00124515
Iteration 11/25 | Loss: 0.00124515
Iteration 12/25 | Loss: 0.00124515
Iteration 13/25 | Loss: 0.00124515
Iteration 14/25 | Loss: 0.00124515
Iteration 15/25 | Loss: 0.00124515
Iteration 16/25 | Loss: 0.00124515
Iteration 17/25 | Loss: 0.00124515
Iteration 18/25 | Loss: 0.00124515
Iteration 19/25 | Loss: 0.00124515
Iteration 20/25 | Loss: 0.00124515
Iteration 21/25 | Loss: 0.00124515
Iteration 22/25 | Loss: 0.00124515
Iteration 23/25 | Loss: 0.00124515
Iteration 24/25 | Loss: 0.00124515
Iteration 25/25 | Loss: 0.00124515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31333697
Iteration 2/25 | Loss: 0.00107511
Iteration 3/25 | Loss: 0.00107511
Iteration 4/25 | Loss: 0.00107511
Iteration 5/25 | Loss: 0.00107511
Iteration 6/25 | Loss: 0.00107511
Iteration 7/25 | Loss: 0.00107511
Iteration 8/25 | Loss: 0.00107511
Iteration 9/25 | Loss: 0.00107511
Iteration 10/25 | Loss: 0.00107511
Iteration 11/25 | Loss: 0.00107511
Iteration 12/25 | Loss: 0.00107511
Iteration 13/25 | Loss: 0.00107511
Iteration 14/25 | Loss: 0.00107511
Iteration 15/25 | Loss: 0.00107511
Iteration 16/25 | Loss: 0.00107511
Iteration 17/25 | Loss: 0.00107511
Iteration 18/25 | Loss: 0.00107511
Iteration 19/25 | Loss: 0.00107511
Iteration 20/25 | Loss: 0.00107511
Iteration 21/25 | Loss: 0.00107511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010751115623861551, 0.0010751115623861551, 0.0010751115623861551, 0.0010751115623861551, 0.0010751115623861551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010751115623861551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107511
Iteration 2/1000 | Loss: 0.00005128
Iteration 3/1000 | Loss: 0.00003240
Iteration 4/1000 | Loss: 0.00002686
Iteration 5/1000 | Loss: 0.00002444
Iteration 6/1000 | Loss: 0.00002270
Iteration 7/1000 | Loss: 0.00002103
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00001973
Iteration 10/1000 | Loss: 0.00001931
Iteration 11/1000 | Loss: 0.00001902
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001800
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001795
Iteration 23/1000 | Loss: 0.00001795
Iteration 24/1000 | Loss: 0.00001795
Iteration 25/1000 | Loss: 0.00001795
Iteration 26/1000 | Loss: 0.00001795
Iteration 27/1000 | Loss: 0.00001794
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001786
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001783
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001782
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001781
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001781
Iteration 69/1000 | Loss: 0.00001781
Iteration 70/1000 | Loss: 0.00001781
Iteration 71/1000 | Loss: 0.00001781
Iteration 72/1000 | Loss: 0.00001781
Iteration 73/1000 | Loss: 0.00001781
Iteration 74/1000 | Loss: 0.00001781
Iteration 75/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.7814238162827678e-05, 1.7814238162827678e-05, 1.7814238162827678e-05, 1.7814238162827678e-05, 1.7814238162827678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7814238162827678e-05

Optimization complete. Final v2v error: 3.587521553039551 mm

Highest mean error: 4.275784492492676 mm for frame 73

Lowest mean error: 3.0899012088775635 mm for frame 83

Saving results

Total time: 37.024258613586426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979942
Iteration 2/25 | Loss: 0.00350274
Iteration 3/25 | Loss: 0.00232097
Iteration 4/25 | Loss: 0.00212333
Iteration 5/25 | Loss: 0.00200345
Iteration 6/25 | Loss: 0.00209198
Iteration 7/25 | Loss: 0.00197536
Iteration 8/25 | Loss: 0.00184463
Iteration 9/25 | Loss: 0.00175465
Iteration 10/25 | Loss: 0.00173623
Iteration 11/25 | Loss: 0.00168219
Iteration 12/25 | Loss: 0.00167072
Iteration 13/25 | Loss: 0.00163767
Iteration 14/25 | Loss: 0.00162099
Iteration 15/25 | Loss: 0.00161917
Iteration 16/25 | Loss: 0.00160220
Iteration 17/25 | Loss: 0.00160276
Iteration 18/25 | Loss: 0.00158540
Iteration 19/25 | Loss: 0.00158082
Iteration 20/25 | Loss: 0.00158421
Iteration 21/25 | Loss: 0.00157402
Iteration 22/25 | Loss: 0.00157598
Iteration 23/25 | Loss: 0.00157067
Iteration 24/25 | Loss: 0.00156814
Iteration 25/25 | Loss: 0.00156667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34717762
Iteration 2/25 | Loss: 0.00596088
Iteration 3/25 | Loss: 0.00391716
Iteration 4/25 | Loss: 0.00391715
Iteration 5/25 | Loss: 0.00391715
Iteration 6/25 | Loss: 0.00391715
Iteration 7/25 | Loss: 0.00391715
Iteration 8/25 | Loss: 0.00391715
Iteration 9/25 | Loss: 0.00391715
Iteration 10/25 | Loss: 0.00391715
Iteration 11/25 | Loss: 0.00391715
Iteration 12/25 | Loss: 0.00391715
Iteration 13/25 | Loss: 0.00391715
Iteration 14/25 | Loss: 0.00391715
Iteration 15/25 | Loss: 0.00391715
Iteration 16/25 | Loss: 0.00391715
Iteration 17/25 | Loss: 0.00391715
Iteration 18/25 | Loss: 0.00391715
Iteration 19/25 | Loss: 0.00391715
Iteration 20/25 | Loss: 0.00391715
Iteration 21/25 | Loss: 0.00391715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003917145077139139, 0.003917145077139139, 0.003917145077139139, 0.003917145077139139, 0.003917145077139139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003917145077139139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00391715
Iteration 2/1000 | Loss: 0.00322879
Iteration 3/1000 | Loss: 0.00063448
Iteration 4/1000 | Loss: 0.00041746
Iteration 5/1000 | Loss: 0.00049221
Iteration 6/1000 | Loss: 0.00032950
Iteration 7/1000 | Loss: 0.00056489
Iteration 8/1000 | Loss: 0.00043156
Iteration 9/1000 | Loss: 0.00045974
Iteration 10/1000 | Loss: 0.00032944
Iteration 11/1000 | Loss: 0.00070718
Iteration 12/1000 | Loss: 0.00023191
Iteration 13/1000 | Loss: 0.00047114
Iteration 14/1000 | Loss: 0.00109596
Iteration 15/1000 | Loss: 0.00122566
Iteration 16/1000 | Loss: 0.00134350
Iteration 17/1000 | Loss: 0.00133292
Iteration 18/1000 | Loss: 0.00239956
Iteration 19/1000 | Loss: 0.00047818
Iteration 20/1000 | Loss: 0.00069186
Iteration 21/1000 | Loss: 0.00049765
Iteration 22/1000 | Loss: 0.00077993
Iteration 23/1000 | Loss: 0.00070839
Iteration 24/1000 | Loss: 0.00203712
Iteration 25/1000 | Loss: 0.00018494
Iteration 26/1000 | Loss: 0.00018212
Iteration 27/1000 | Loss: 0.00018803
Iteration 28/1000 | Loss: 0.00016400
Iteration 29/1000 | Loss: 0.00029100
Iteration 30/1000 | Loss: 0.00017921
Iteration 31/1000 | Loss: 0.00048328
Iteration 32/1000 | Loss: 0.00040376
Iteration 33/1000 | Loss: 0.00017384
Iteration 34/1000 | Loss: 0.00015510
Iteration 35/1000 | Loss: 0.00024147
Iteration 36/1000 | Loss: 0.00015102
Iteration 37/1000 | Loss: 0.00025559
Iteration 38/1000 | Loss: 0.00083686
Iteration 39/1000 | Loss: 0.00103764
Iteration 40/1000 | Loss: 0.00042705
Iteration 41/1000 | Loss: 0.00057877
Iteration 42/1000 | Loss: 0.00016259
Iteration 43/1000 | Loss: 0.00049245
Iteration 44/1000 | Loss: 0.00028033
Iteration 45/1000 | Loss: 0.00029451
Iteration 46/1000 | Loss: 0.00016747
Iteration 47/1000 | Loss: 0.00014723
Iteration 48/1000 | Loss: 0.00020102
Iteration 49/1000 | Loss: 0.00014362
Iteration 50/1000 | Loss: 0.00014517
Iteration 51/1000 | Loss: 0.00014865
Iteration 52/1000 | Loss: 0.00018412
Iteration 53/1000 | Loss: 0.00014525
Iteration 54/1000 | Loss: 0.00072828
Iteration 55/1000 | Loss: 0.00046349
Iteration 56/1000 | Loss: 0.00013625
Iteration 57/1000 | Loss: 0.00061633
Iteration 58/1000 | Loss: 0.00035197
Iteration 59/1000 | Loss: 0.00039618
Iteration 60/1000 | Loss: 0.00015736
Iteration 61/1000 | Loss: 0.00013822
Iteration 62/1000 | Loss: 0.00013013
Iteration 63/1000 | Loss: 0.00014278
Iteration 64/1000 | Loss: 0.00017229
Iteration 65/1000 | Loss: 0.00012999
Iteration 66/1000 | Loss: 0.00012425
Iteration 67/1000 | Loss: 0.00087335
Iteration 68/1000 | Loss: 0.00055675
Iteration 69/1000 | Loss: 0.00043970
Iteration 70/1000 | Loss: 0.00027366
Iteration 71/1000 | Loss: 0.00012974
Iteration 72/1000 | Loss: 0.00012710
Iteration 73/1000 | Loss: 0.00013417
Iteration 74/1000 | Loss: 0.00049237
Iteration 75/1000 | Loss: 0.00016871
Iteration 76/1000 | Loss: 0.00012909
Iteration 77/1000 | Loss: 0.00052174
Iteration 78/1000 | Loss: 0.00020218
Iteration 79/1000 | Loss: 0.00013328
Iteration 80/1000 | Loss: 0.00013245
Iteration 81/1000 | Loss: 0.00012128
Iteration 82/1000 | Loss: 0.00013433
Iteration 83/1000 | Loss: 0.00056383
Iteration 84/1000 | Loss: 0.00025950
Iteration 85/1000 | Loss: 0.00022410
Iteration 86/1000 | Loss: 0.00013699
Iteration 87/1000 | Loss: 0.00014055
Iteration 88/1000 | Loss: 0.00012565
Iteration 89/1000 | Loss: 0.00012084
Iteration 90/1000 | Loss: 0.00011784
Iteration 91/1000 | Loss: 0.00038639
Iteration 92/1000 | Loss: 0.00014798
Iteration 93/1000 | Loss: 0.00011624
Iteration 94/1000 | Loss: 0.00042060
Iteration 95/1000 | Loss: 0.00016683
Iteration 96/1000 | Loss: 0.00012597
Iteration 97/1000 | Loss: 0.00012393
Iteration 98/1000 | Loss: 0.00026185
Iteration 99/1000 | Loss: 0.00016226
Iteration 100/1000 | Loss: 0.00022319
Iteration 101/1000 | Loss: 0.00014970
Iteration 102/1000 | Loss: 0.00020871
Iteration 103/1000 | Loss: 0.00054830
Iteration 104/1000 | Loss: 0.00052198
Iteration 105/1000 | Loss: 0.00080180
Iteration 106/1000 | Loss: 0.00082054
Iteration 107/1000 | Loss: 0.00032204
Iteration 108/1000 | Loss: 0.00023135
Iteration 109/1000 | Loss: 0.00048144
Iteration 110/1000 | Loss: 0.00056330
Iteration 111/1000 | Loss: 0.00080816
Iteration 112/1000 | Loss: 0.00055192
Iteration 113/1000 | Loss: 0.00012014
Iteration 114/1000 | Loss: 0.00016340
Iteration 115/1000 | Loss: 0.00011568
Iteration 116/1000 | Loss: 0.00013503
Iteration 117/1000 | Loss: 0.00011926
Iteration 118/1000 | Loss: 0.00011285
Iteration 119/1000 | Loss: 0.00011006
Iteration 120/1000 | Loss: 0.00010789
Iteration 121/1000 | Loss: 0.00010626
Iteration 122/1000 | Loss: 0.00010527
Iteration 123/1000 | Loss: 0.00017825
Iteration 124/1000 | Loss: 0.00011008
Iteration 125/1000 | Loss: 0.00010673
Iteration 126/1000 | Loss: 0.00010487
Iteration 127/1000 | Loss: 0.00010308
Iteration 128/1000 | Loss: 0.00010200
Iteration 129/1000 | Loss: 0.00010136
Iteration 130/1000 | Loss: 0.00010084
Iteration 131/1000 | Loss: 0.00010033
Iteration 132/1000 | Loss: 0.00043609
Iteration 133/1000 | Loss: 0.00031320
Iteration 134/1000 | Loss: 0.00021394
Iteration 135/1000 | Loss: 0.00010604
Iteration 136/1000 | Loss: 0.00010266
Iteration 137/1000 | Loss: 0.00009984
Iteration 138/1000 | Loss: 0.00009830
Iteration 139/1000 | Loss: 0.00009713
Iteration 140/1000 | Loss: 0.00009635
Iteration 141/1000 | Loss: 0.00009587
Iteration 142/1000 | Loss: 0.00009545
Iteration 143/1000 | Loss: 0.00009503
Iteration 144/1000 | Loss: 0.00009468
Iteration 145/1000 | Loss: 0.00009445
Iteration 146/1000 | Loss: 0.00009409
Iteration 147/1000 | Loss: 0.00009382
Iteration 148/1000 | Loss: 0.00009372
Iteration 149/1000 | Loss: 0.00009351
Iteration 150/1000 | Loss: 0.00016189
Iteration 151/1000 | Loss: 0.00010022
Iteration 152/1000 | Loss: 0.00009607
Iteration 153/1000 | Loss: 0.00009476
Iteration 154/1000 | Loss: 0.00009319
Iteration 155/1000 | Loss: 0.00009242
Iteration 156/1000 | Loss: 0.00009183
Iteration 157/1000 | Loss: 0.00009160
Iteration 158/1000 | Loss: 0.00009140
Iteration 159/1000 | Loss: 0.00009124
Iteration 160/1000 | Loss: 0.00009119
Iteration 161/1000 | Loss: 0.00009105
Iteration 162/1000 | Loss: 0.00009103
Iteration 163/1000 | Loss: 0.00009102
Iteration 164/1000 | Loss: 0.00009102
Iteration 165/1000 | Loss: 0.00009102
Iteration 166/1000 | Loss: 0.00009101
Iteration 167/1000 | Loss: 0.00009101
Iteration 168/1000 | Loss: 0.00009099
Iteration 169/1000 | Loss: 0.00009099
Iteration 170/1000 | Loss: 0.00018246
Iteration 171/1000 | Loss: 0.00009914
Iteration 172/1000 | Loss: 0.00027090
Iteration 173/1000 | Loss: 0.00063251
Iteration 174/1000 | Loss: 0.00015942
Iteration 175/1000 | Loss: 0.00021423
Iteration 176/1000 | Loss: 0.00023116
Iteration 177/1000 | Loss: 0.00009695
Iteration 178/1000 | Loss: 0.00009295
Iteration 179/1000 | Loss: 0.00009024
Iteration 180/1000 | Loss: 0.00008829
Iteration 181/1000 | Loss: 0.00008684
Iteration 182/1000 | Loss: 0.00008551
Iteration 183/1000 | Loss: 0.00008453
Iteration 184/1000 | Loss: 0.00021190
Iteration 185/1000 | Loss: 0.00008804
Iteration 186/1000 | Loss: 0.00029602
Iteration 187/1000 | Loss: 0.00027393
Iteration 188/1000 | Loss: 0.00051634
Iteration 189/1000 | Loss: 0.00032721
Iteration 190/1000 | Loss: 0.00008834
Iteration 191/1000 | Loss: 0.00008908
Iteration 192/1000 | Loss: 0.00009541
Iteration 193/1000 | Loss: 0.00009016
Iteration 194/1000 | Loss: 0.00008132
Iteration 195/1000 | Loss: 0.00008060
Iteration 196/1000 | Loss: 0.00031739
Iteration 197/1000 | Loss: 0.00036255
Iteration 198/1000 | Loss: 0.00023868
Iteration 199/1000 | Loss: 0.00008667
Iteration 200/1000 | Loss: 0.00026932
Iteration 201/1000 | Loss: 0.00028241
Iteration 202/1000 | Loss: 0.00008061
Iteration 203/1000 | Loss: 0.00008015
Iteration 204/1000 | Loss: 0.00007597
Iteration 205/1000 | Loss: 0.00048644
Iteration 206/1000 | Loss: 0.00044309
Iteration 207/1000 | Loss: 0.00033169
Iteration 208/1000 | Loss: 0.00075558
Iteration 209/1000 | Loss: 0.00015855
Iteration 210/1000 | Loss: 0.00027289
Iteration 211/1000 | Loss: 0.00021385
Iteration 212/1000 | Loss: 0.00032145
Iteration 213/1000 | Loss: 0.00008318
Iteration 214/1000 | Loss: 0.00010410
Iteration 215/1000 | Loss: 0.00010509
Iteration 216/1000 | Loss: 0.00014753
Iteration 217/1000 | Loss: 0.00015100
Iteration 218/1000 | Loss: 0.00010882
Iteration 219/1000 | Loss: 0.00008004
Iteration 220/1000 | Loss: 0.00007448
Iteration 221/1000 | Loss: 0.00030811
Iteration 222/1000 | Loss: 0.00028548
Iteration 223/1000 | Loss: 0.00050589
Iteration 224/1000 | Loss: 0.00157213
Iteration 225/1000 | Loss: 0.00125689
Iteration 226/1000 | Loss: 0.00169360
Iteration 227/1000 | Loss: 0.00079740
Iteration 228/1000 | Loss: 0.00043624
Iteration 229/1000 | Loss: 0.00076513
Iteration 230/1000 | Loss: 0.00057377
Iteration 231/1000 | Loss: 0.00055434
Iteration 232/1000 | Loss: 0.00009385
Iteration 233/1000 | Loss: 0.00036362
Iteration 234/1000 | Loss: 0.00028857
Iteration 235/1000 | Loss: 0.00010650
Iteration 236/1000 | Loss: 0.00008359
Iteration 237/1000 | Loss: 0.00028897
Iteration 238/1000 | Loss: 0.00035347
Iteration 239/1000 | Loss: 0.00036806
Iteration 240/1000 | Loss: 0.00016985
Iteration 241/1000 | Loss: 0.00056605
Iteration 242/1000 | Loss: 0.00025529
Iteration 243/1000 | Loss: 0.00009181
Iteration 244/1000 | Loss: 0.00040959
Iteration 245/1000 | Loss: 0.00020182
Iteration 246/1000 | Loss: 0.00009665
Iteration 247/1000 | Loss: 0.00014716
Iteration 248/1000 | Loss: 0.00063142
Iteration 249/1000 | Loss: 0.00039861
Iteration 250/1000 | Loss: 0.00037082
Iteration 251/1000 | Loss: 0.00015807
Iteration 252/1000 | Loss: 0.00009963
Iteration 253/1000 | Loss: 0.00008938
Iteration 254/1000 | Loss: 0.00016765
Iteration 255/1000 | Loss: 0.00029749
Iteration 256/1000 | Loss: 0.00056402
Iteration 257/1000 | Loss: 0.00029527
Iteration 258/1000 | Loss: 0.00009870
Iteration 259/1000 | Loss: 0.00011104
Iteration 260/1000 | Loss: 0.00013560
Iteration 261/1000 | Loss: 0.00051482
Iteration 262/1000 | Loss: 0.00034895
Iteration 263/1000 | Loss: 0.00009258
Iteration 264/1000 | Loss: 0.00009657
Iteration 265/1000 | Loss: 0.00007977
Iteration 266/1000 | Loss: 0.00008044
Iteration 267/1000 | Loss: 0.00008107
Iteration 268/1000 | Loss: 0.00049181
Iteration 269/1000 | Loss: 0.00039360
Iteration 270/1000 | Loss: 0.00050055
Iteration 271/1000 | Loss: 0.00029317
Iteration 272/1000 | Loss: 0.00033454
Iteration 273/1000 | Loss: 0.00018381
Iteration 274/1000 | Loss: 0.00013229
Iteration 275/1000 | Loss: 0.00010680
Iteration 276/1000 | Loss: 0.00008814
Iteration 277/1000 | Loss: 0.00007677
Iteration 278/1000 | Loss: 0.00009237
Iteration 279/1000 | Loss: 0.00028484
Iteration 280/1000 | Loss: 0.00008110
Iteration 281/1000 | Loss: 0.00020014
Iteration 282/1000 | Loss: 0.00007530
Iteration 283/1000 | Loss: 0.00007084
Iteration 284/1000 | Loss: 0.00026622
Iteration 285/1000 | Loss: 0.00028020
Iteration 286/1000 | Loss: 0.00065612
Iteration 287/1000 | Loss: 0.00182817
Iteration 288/1000 | Loss: 0.00157009
Iteration 289/1000 | Loss: 0.00019517
Iteration 290/1000 | Loss: 0.00009920
Iteration 291/1000 | Loss: 0.00013156
Iteration 292/1000 | Loss: 0.00040720
Iteration 293/1000 | Loss: 0.00010072
Iteration 294/1000 | Loss: 0.00013817
Iteration 295/1000 | Loss: 0.00034995
Iteration 296/1000 | Loss: 0.00033339
Iteration 297/1000 | Loss: 0.00020755
Iteration 298/1000 | Loss: 0.00018881
Iteration 299/1000 | Loss: 0.00019552
Iteration 300/1000 | Loss: 0.00007714
Iteration 301/1000 | Loss: 0.00009403
Iteration 302/1000 | Loss: 0.00007083
Iteration 303/1000 | Loss: 0.00007937
Iteration 304/1000 | Loss: 0.00006978
Iteration 305/1000 | Loss: 0.00007530
Iteration 306/1000 | Loss: 0.00029387
Iteration 307/1000 | Loss: 0.00007482
Iteration 308/1000 | Loss: 0.00009021
Iteration 309/1000 | Loss: 0.00008662
Iteration 310/1000 | Loss: 0.00006897
Iteration 311/1000 | Loss: 0.00007989
Iteration 312/1000 | Loss: 0.00044664
Iteration 313/1000 | Loss: 0.00037226
Iteration 314/1000 | Loss: 0.00010209
Iteration 315/1000 | Loss: 0.00016112
Iteration 316/1000 | Loss: 0.00015483
Iteration 317/1000 | Loss: 0.00012367
Iteration 318/1000 | Loss: 0.00006606
Iteration 319/1000 | Loss: 0.00006473
Iteration 320/1000 | Loss: 0.00013733
Iteration 321/1000 | Loss: 0.00006441
Iteration 322/1000 | Loss: 0.00006334
Iteration 323/1000 | Loss: 0.00014294
Iteration 324/1000 | Loss: 0.00006275
Iteration 325/1000 | Loss: 0.00034149
Iteration 326/1000 | Loss: 0.00027032
Iteration 327/1000 | Loss: 0.00014333
Iteration 328/1000 | Loss: 0.00006327
Iteration 329/1000 | Loss: 0.00056592
Iteration 330/1000 | Loss: 0.00035493
Iteration 331/1000 | Loss: 0.00038098
Iteration 332/1000 | Loss: 0.00021554
Iteration 333/1000 | Loss: 0.00081882
Iteration 334/1000 | Loss: 0.00024112
Iteration 335/1000 | Loss: 0.00024947
Iteration 336/1000 | Loss: 0.00007691
Iteration 337/1000 | Loss: 0.00008350
Iteration 338/1000 | Loss: 0.00009166
Iteration 339/1000 | Loss: 0.00006416
Iteration 340/1000 | Loss: 0.00007025
Iteration 341/1000 | Loss: 0.00048639
Iteration 342/1000 | Loss: 0.00035363
Iteration 343/1000 | Loss: 0.00006278
Iteration 344/1000 | Loss: 0.00053365
Iteration 345/1000 | Loss: 0.00038552
Iteration 346/1000 | Loss: 0.00093019
Iteration 347/1000 | Loss: 0.00035822
Iteration 348/1000 | Loss: 0.00046985
Iteration 349/1000 | Loss: 0.00022427
Iteration 350/1000 | Loss: 0.00014105
Iteration 351/1000 | Loss: 0.00015529
Iteration 352/1000 | Loss: 0.00006994
Iteration 353/1000 | Loss: 0.00006607
Iteration 354/1000 | Loss: 0.00006320
Iteration 355/1000 | Loss: 0.00012435
Iteration 356/1000 | Loss: 0.00026064
Iteration 357/1000 | Loss: 0.00022291
Iteration 358/1000 | Loss: 0.00006128
Iteration 359/1000 | Loss: 0.00010089
Iteration 360/1000 | Loss: 0.00006812
Iteration 361/1000 | Loss: 0.00036448
Iteration 362/1000 | Loss: 0.00014312
Iteration 363/1000 | Loss: 0.00009091
Iteration 364/1000 | Loss: 0.00007276
Iteration 365/1000 | Loss: 0.00007116
Iteration 366/1000 | Loss: 0.00005891
Iteration 367/1000 | Loss: 0.00007873
Iteration 368/1000 | Loss: 0.00005697
Iteration 369/1000 | Loss: 0.00008431
Iteration 370/1000 | Loss: 0.00026771
Iteration 371/1000 | Loss: 0.00022590
Iteration 372/1000 | Loss: 0.00006271
Iteration 373/1000 | Loss: 0.00025737
Iteration 374/1000 | Loss: 0.00021072
Iteration 375/1000 | Loss: 0.00017889
Iteration 376/1000 | Loss: 0.00041989
Iteration 377/1000 | Loss: 0.00054819
Iteration 378/1000 | Loss: 0.00013033
Iteration 379/1000 | Loss: 0.00006956
Iteration 380/1000 | Loss: 0.00005887
Iteration 381/1000 | Loss: 0.00005813
Iteration 382/1000 | Loss: 0.00012288
Iteration 383/1000 | Loss: 0.00018734
Iteration 384/1000 | Loss: 0.00011857
Iteration 385/1000 | Loss: 0.00014707
Iteration 386/1000 | Loss: 0.00021382
Iteration 387/1000 | Loss: 0.00006465
Iteration 388/1000 | Loss: 0.00014395
Iteration 389/1000 | Loss: 0.00050515
Iteration 390/1000 | Loss: 0.00031955
Iteration 391/1000 | Loss: 0.00034360
Iteration 392/1000 | Loss: 0.00028306
Iteration 393/1000 | Loss: 0.00033035
Iteration 394/1000 | Loss: 0.00008412
Iteration 395/1000 | Loss: 0.00007310
Iteration 396/1000 | Loss: 0.00014291
Iteration 397/1000 | Loss: 0.00005838
Iteration 398/1000 | Loss: 0.00012084
Iteration 399/1000 | Loss: 0.00005467
Iteration 400/1000 | Loss: 0.00012508
Iteration 401/1000 | Loss: 0.00005225
Iteration 402/1000 | Loss: 0.00005155
Iteration 403/1000 | Loss: 0.00005081
Iteration 404/1000 | Loss: 0.00005021
Iteration 405/1000 | Loss: 0.00009525
Iteration 406/1000 | Loss: 0.00005371
Iteration 407/1000 | Loss: 0.00004929
Iteration 408/1000 | Loss: 0.00013318
Iteration 409/1000 | Loss: 0.00057517
Iteration 410/1000 | Loss: 0.00014124
Iteration 411/1000 | Loss: 0.00010410
Iteration 412/1000 | Loss: 0.00026900
Iteration 413/1000 | Loss: 0.00005752
Iteration 414/1000 | Loss: 0.00004922
Iteration 415/1000 | Loss: 0.00004685
Iteration 416/1000 | Loss: 0.00004586
Iteration 417/1000 | Loss: 0.00004500
Iteration 418/1000 | Loss: 0.00004464
Iteration 419/1000 | Loss: 0.00004436
Iteration 420/1000 | Loss: 0.00006679
Iteration 421/1000 | Loss: 0.00004646
Iteration 422/1000 | Loss: 0.00004386
Iteration 423/1000 | Loss: 0.00024952
Iteration 424/1000 | Loss: 0.00019093
Iteration 425/1000 | Loss: 0.00005523
Iteration 426/1000 | Loss: 0.00004462
Iteration 427/1000 | Loss: 0.00025267
Iteration 428/1000 | Loss: 0.00020030
Iteration 429/1000 | Loss: 0.00046150
Iteration 430/1000 | Loss: 0.00033162
Iteration 431/1000 | Loss: 0.00005367
Iteration 432/1000 | Loss: 0.00004384
Iteration 433/1000 | Loss: 0.00047731
Iteration 434/1000 | Loss: 0.00008991
Iteration 435/1000 | Loss: 0.00006810
Iteration 436/1000 | Loss: 0.00007177
Iteration 437/1000 | Loss: 0.00004878
Iteration 438/1000 | Loss: 0.00005890
Iteration 439/1000 | Loss: 0.00004487
Iteration 440/1000 | Loss: 0.00006638
Iteration 441/1000 | Loss: 0.00004338
Iteration 442/1000 | Loss: 0.00006851
Iteration 443/1000 | Loss: 0.00004243
Iteration 444/1000 | Loss: 0.00004216
Iteration 445/1000 | Loss: 0.00006874
Iteration 446/1000 | Loss: 0.00009394
Iteration 447/1000 | Loss: 0.00006494
Iteration 448/1000 | Loss: 0.00004197
Iteration 449/1000 | Loss: 0.00004177
Iteration 450/1000 | Loss: 0.00004177
Iteration 451/1000 | Loss: 0.00004176
Iteration 452/1000 | Loss: 0.00004176
Iteration 453/1000 | Loss: 0.00004175
Iteration 454/1000 | Loss: 0.00004172
Iteration 455/1000 | Loss: 0.00004171
Iteration 456/1000 | Loss: 0.00004170
Iteration 457/1000 | Loss: 0.00004170
Iteration 458/1000 | Loss: 0.00004169
Iteration 459/1000 | Loss: 0.00004168
Iteration 460/1000 | Loss: 0.00004160
Iteration 461/1000 | Loss: 0.00004158
Iteration 462/1000 | Loss: 0.00010484
Iteration 463/1000 | Loss: 0.00004954
Iteration 464/1000 | Loss: 0.00004401
Iteration 465/1000 | Loss: 0.00004450
Iteration 466/1000 | Loss: 0.00004189
Iteration 467/1000 | Loss: 0.00005282
Iteration 468/1000 | Loss: 0.00004799
Iteration 469/1000 | Loss: 0.00027449
Iteration 470/1000 | Loss: 0.00018387
Iteration 471/1000 | Loss: 0.00025294
Iteration 472/1000 | Loss: 0.00013620
Iteration 473/1000 | Loss: 0.00004765
Iteration 474/1000 | Loss: 0.00005854
Iteration 475/1000 | Loss: 0.00005488
Iteration 476/1000 | Loss: 0.00004196
Iteration 477/1000 | Loss: 0.00004033
Iteration 478/1000 | Loss: 0.00004008
Iteration 479/1000 | Loss: 0.00005839
Iteration 480/1000 | Loss: 0.00004249
Iteration 481/1000 | Loss: 0.00003986
Iteration 482/1000 | Loss: 0.00003985
Iteration 483/1000 | Loss: 0.00003985
Iteration 484/1000 | Loss: 0.00003981
Iteration 485/1000 | Loss: 0.00003975
Iteration 486/1000 | Loss: 0.00004466
Iteration 487/1000 | Loss: 0.00004112
Iteration 488/1000 | Loss: 0.00003972
Iteration 489/1000 | Loss: 0.00003972
Iteration 490/1000 | Loss: 0.00003971
Iteration 491/1000 | Loss: 0.00003971
Iteration 492/1000 | Loss: 0.00003971
Iteration 493/1000 | Loss: 0.00003971
Iteration 494/1000 | Loss: 0.00003971
Iteration 495/1000 | Loss: 0.00003967
Iteration 496/1000 | Loss: 0.00003967
Iteration 497/1000 | Loss: 0.00003966
Iteration 498/1000 | Loss: 0.00003966
Iteration 499/1000 | Loss: 0.00003965
Iteration 500/1000 | Loss: 0.00003964
Iteration 501/1000 | Loss: 0.00003963
Iteration 502/1000 | Loss: 0.00003963
Iteration 503/1000 | Loss: 0.00003962
Iteration 504/1000 | Loss: 0.00003961
Iteration 505/1000 | Loss: 0.00004402
Iteration 506/1000 | Loss: 0.00003993
Iteration 507/1000 | Loss: 0.00003959
Iteration 508/1000 | Loss: 0.00003959
Iteration 509/1000 | Loss: 0.00004008
Iteration 510/1000 | Loss: 0.00003954
Iteration 511/1000 | Loss: 0.00003954
Iteration 512/1000 | Loss: 0.00003953
Iteration 513/1000 | Loss: 0.00003953
Iteration 514/1000 | Loss: 0.00004005
Iteration 515/1000 | Loss: 0.00003948
Iteration 516/1000 | Loss: 0.00003938
Iteration 517/1000 | Loss: 0.00003935
Iteration 518/1000 | Loss: 0.00003934
Iteration 519/1000 | Loss: 0.00003934
Iteration 520/1000 | Loss: 0.00003933
Iteration 521/1000 | Loss: 0.00003933
Iteration 522/1000 | Loss: 0.00004203
Iteration 523/1000 | Loss: 0.00003927
Iteration 524/1000 | Loss: 0.00003926
Iteration 525/1000 | Loss: 0.00023312
Iteration 526/1000 | Loss: 0.00020291
Iteration 527/1000 | Loss: 0.00004574
Iteration 528/1000 | Loss: 0.00003950
Iteration 529/1000 | Loss: 0.00003940
Iteration 530/1000 | Loss: 0.00003930
Iteration 531/1000 | Loss: 0.00003927
Iteration 532/1000 | Loss: 0.00003925
Iteration 533/1000 | Loss: 0.00003924
Iteration 534/1000 | Loss: 0.00003924
Iteration 535/1000 | Loss: 0.00003924
Iteration 536/1000 | Loss: 0.00003923
Iteration 537/1000 | Loss: 0.00003920
Iteration 538/1000 | Loss: 0.00003920
Iteration 539/1000 | Loss: 0.00003919
Iteration 540/1000 | Loss: 0.00024171
Iteration 541/1000 | Loss: 0.00014046
Iteration 542/1000 | Loss: 0.00003957
Iteration 543/1000 | Loss: 0.00003926
Iteration 544/1000 | Loss: 0.00024082
Iteration 545/1000 | Loss: 0.00012569
Iteration 546/1000 | Loss: 0.00003958
Iteration 547/1000 | Loss: 0.00003927
Iteration 548/1000 | Loss: 0.00003917
Iteration 549/1000 | Loss: 0.00023962
Iteration 550/1000 | Loss: 0.00011078
Iteration 551/1000 | Loss: 0.00006229
Iteration 552/1000 | Loss: 0.00018696
Iteration 553/1000 | Loss: 0.00004996
Iteration 554/1000 | Loss: 0.00003973
Iteration 555/1000 | Loss: 0.00003941
Iteration 556/1000 | Loss: 0.00003932
Iteration 557/1000 | Loss: 0.00024408
Iteration 558/1000 | Loss: 0.00010894
Iteration 559/1000 | Loss: 0.00006988
Iteration 560/1000 | Loss: 0.00004524
Iteration 561/1000 | Loss: 0.00004113
Iteration 562/1000 | Loss: 0.00004004
Iteration 563/1000 | Loss: 0.00003963
Iteration 564/1000 | Loss: 0.00003949
Iteration 565/1000 | Loss: 0.00003945
Iteration 566/1000 | Loss: 0.00003943
Iteration 567/1000 | Loss: 0.00003943
Iteration 568/1000 | Loss: 0.00003943
Iteration 569/1000 | Loss: 0.00003943
Iteration 570/1000 | Loss: 0.00003943
Iteration 571/1000 | Loss: 0.00003943
Iteration 572/1000 | Loss: 0.00003943
Iteration 573/1000 | Loss: 0.00003943
Iteration 574/1000 | Loss: 0.00003943
Iteration 575/1000 | Loss: 0.00003942
Iteration 576/1000 | Loss: 0.00003942
Iteration 577/1000 | Loss: 0.00003942
Iteration 578/1000 | Loss: 0.00003938
Iteration 579/1000 | Loss: 0.00003937
Iteration 580/1000 | Loss: 0.00003937
Iteration 581/1000 | Loss: 0.00003936
Iteration 582/1000 | Loss: 0.00003932
Iteration 583/1000 | Loss: 0.00003928
Iteration 584/1000 | Loss: 0.00024958
Iteration 585/1000 | Loss: 0.00007189
Iteration 586/1000 | Loss: 0.00004942
Iteration 587/1000 | Loss: 0.00004135
Iteration 588/1000 | Loss: 0.00004009
Iteration 589/1000 | Loss: 0.00027222
Iteration 590/1000 | Loss: 0.00006250
Iteration 591/1000 | Loss: 0.00032919
Iteration 592/1000 | Loss: 0.00012404
Iteration 593/1000 | Loss: 0.00019227
Iteration 594/1000 | Loss: 0.00004183
Iteration 595/1000 | Loss: 0.00003987
Iteration 596/1000 | Loss: 0.00003953
Iteration 597/1000 | Loss: 0.00003952
Iteration 598/1000 | Loss: 0.00003927
Iteration 599/1000 | Loss: 0.00024723
Iteration 600/1000 | Loss: 0.00005475
Iteration 601/1000 | Loss: 0.00004136
Iteration 602/1000 | Loss: 0.00007507
Iteration 603/1000 | Loss: 0.00003954
Iteration 604/1000 | Loss: 0.00003917
Iteration 605/1000 | Loss: 0.00030489
Iteration 606/1000 | Loss: 0.00005623
Iteration 607/1000 | Loss: 0.00003990
Iteration 608/1000 | Loss: 0.00003923
Iteration 609/1000 | Loss: 0.00032099
Iteration 610/1000 | Loss: 0.00035565
Iteration 611/1000 | Loss: 0.00006101
Iteration 612/1000 | Loss: 0.00003955
Iteration 613/1000 | Loss: 0.00027455
Iteration 614/1000 | Loss: 0.00014418
Iteration 615/1000 | Loss: 0.00005281
Iteration 616/1000 | Loss: 0.00004582
Iteration 617/1000 | Loss: 0.00004404
Iteration 618/1000 | Loss: 0.00005290
Iteration 619/1000 | Loss: 0.00025950
Iteration 620/1000 | Loss: 0.00005867
Iteration 621/1000 | Loss: 0.00004827
Iteration 622/1000 | Loss: 0.00005532
Iteration 623/1000 | Loss: 0.00004203
Iteration 624/1000 | Loss: 0.00005829
Iteration 625/1000 | Loss: 0.00004120
Iteration 626/1000 | Loss: 0.00008178
Iteration 627/1000 | Loss: 0.00008197
Iteration 628/1000 | Loss: 0.00004379
Iteration 629/1000 | Loss: 0.00008510
Iteration 630/1000 | Loss: 0.00004749
Iteration 631/1000 | Loss: 0.00004550
Iteration 632/1000 | Loss: 0.00004053
Iteration 633/1000 | Loss: 0.00004045
Iteration 634/1000 | Loss: 0.00004042
Iteration 635/1000 | Loss: 0.00004042
Iteration 636/1000 | Loss: 0.00004041
Iteration 637/1000 | Loss: 0.00004041
Iteration 638/1000 | Loss: 0.00004027
Iteration 639/1000 | Loss: 0.00004025
Iteration 640/1000 | Loss: 0.00004024
Iteration 641/1000 | Loss: 0.00004019
Iteration 642/1000 | Loss: 0.00004016
Iteration 643/1000 | Loss: 0.00004012
Iteration 644/1000 | Loss: 0.00004010
Iteration 645/1000 | Loss: 0.00004010
Iteration 646/1000 | Loss: 0.00004010
Iteration 647/1000 | Loss: 0.00004010
Iteration 648/1000 | Loss: 0.00004009
Iteration 649/1000 | Loss: 0.00004009
Iteration 650/1000 | Loss: 0.00004009
Iteration 651/1000 | Loss: 0.00004009
Iteration 652/1000 | Loss: 0.00004007
Iteration 653/1000 | Loss: 0.00004005
Iteration 654/1000 | Loss: 0.00004001
Iteration 655/1000 | Loss: 0.00004000
Iteration 656/1000 | Loss: 0.00004000
Iteration 657/1000 | Loss: 0.00003999
Iteration 658/1000 | Loss: 0.00003999
Iteration 659/1000 | Loss: 0.00003999
Iteration 660/1000 | Loss: 0.00003999
Iteration 661/1000 | Loss: 0.00003999
Iteration 662/1000 | Loss: 0.00003999
Iteration 663/1000 | Loss: 0.00007913
Iteration 664/1000 | Loss: 0.00004003
Iteration 665/1000 | Loss: 0.00003995
Iteration 666/1000 | Loss: 0.00003995
Iteration 667/1000 | Loss: 0.00003994
Iteration 668/1000 | Loss: 0.00003994
Iteration 669/1000 | Loss: 0.00003993
Iteration 670/1000 | Loss: 0.00003993
Iteration 671/1000 | Loss: 0.00003993
Iteration 672/1000 | Loss: 0.00003993
Iteration 673/1000 | Loss: 0.00003992
Iteration 674/1000 | Loss: 0.00003992
Iteration 675/1000 | Loss: 0.00003992
Iteration 676/1000 | Loss: 0.00003992
Iteration 677/1000 | Loss: 0.00003992
Iteration 678/1000 | Loss: 0.00003992
Iteration 679/1000 | Loss: 0.00003992
Iteration 680/1000 | Loss: 0.00003992
Iteration 681/1000 | Loss: 0.00003992
Iteration 682/1000 | Loss: 0.00003992
Iteration 683/1000 | Loss: 0.00003992
Iteration 684/1000 | Loss: 0.00003992
Iteration 685/1000 | Loss: 0.00003992
Iteration 686/1000 | Loss: 0.00003992
Iteration 687/1000 | Loss: 0.00003992
Iteration 688/1000 | Loss: 0.00003992
Iteration 689/1000 | Loss: 0.00003992
Iteration 690/1000 | Loss: 0.00003992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 690. Stopping optimization.
Last 5 losses: [3.991901394329034e-05, 3.991901394329034e-05, 3.991901394329034e-05, 3.991901394329034e-05, 3.991901394329034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.991901394329034e-05

Optimization complete. Final v2v error: 3.7512450218200684 mm

Highest mean error: 10.947580337524414 mm for frame 17

Lowest mean error: 2.839589834213257 mm for frame 70

Saving results

Total time: 919.5908358097076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865773
Iteration 2/25 | Loss: 0.00133526
Iteration 3/25 | Loss: 0.00127144
Iteration 4/25 | Loss: 0.00126104
Iteration 5/25 | Loss: 0.00125854
Iteration 6/25 | Loss: 0.00125829
Iteration 7/25 | Loss: 0.00125829
Iteration 8/25 | Loss: 0.00125829
Iteration 9/25 | Loss: 0.00125829
Iteration 10/25 | Loss: 0.00125829
Iteration 11/25 | Loss: 0.00125829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001258293865248561, 0.001258293865248561, 0.001258293865248561, 0.001258293865248561, 0.001258293865248561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001258293865248561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41885924
Iteration 2/25 | Loss: 0.00100677
Iteration 3/25 | Loss: 0.00100677
Iteration 4/25 | Loss: 0.00100677
Iteration 5/25 | Loss: 0.00100677
Iteration 6/25 | Loss: 0.00100677
Iteration 7/25 | Loss: 0.00100676
Iteration 8/25 | Loss: 0.00100676
Iteration 9/25 | Loss: 0.00100676
Iteration 10/25 | Loss: 0.00100676
Iteration 11/25 | Loss: 0.00100676
Iteration 12/25 | Loss: 0.00100676
Iteration 13/25 | Loss: 0.00100676
Iteration 14/25 | Loss: 0.00100676
Iteration 15/25 | Loss: 0.00100676
Iteration 16/25 | Loss: 0.00100676
Iteration 17/25 | Loss: 0.00100676
Iteration 18/25 | Loss: 0.00100676
Iteration 19/25 | Loss: 0.00100676
Iteration 20/25 | Loss: 0.00100676
Iteration 21/25 | Loss: 0.00100676
Iteration 22/25 | Loss: 0.00100676
Iteration 23/25 | Loss: 0.00100676
Iteration 24/25 | Loss: 0.00100676
Iteration 25/25 | Loss: 0.00100676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100676
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001807
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001653
Iteration 7/1000 | Loss: 0.00001613
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001551
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001463
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00001443
Iteration 22/1000 | Loss: 0.00001442
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001439
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001434
Iteration 34/1000 | Loss: 0.00001434
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001432
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001431
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001429
Iteration 42/1000 | Loss: 0.00001427
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001423
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001417
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001411
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001408
Iteration 94/1000 | Loss: 0.00001408
Iteration 95/1000 | Loss: 0.00001408
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001407
Iteration 98/1000 | Loss: 0.00001407
Iteration 99/1000 | Loss: 0.00001407
Iteration 100/1000 | Loss: 0.00001407
Iteration 101/1000 | Loss: 0.00001407
Iteration 102/1000 | Loss: 0.00001407
Iteration 103/1000 | Loss: 0.00001407
Iteration 104/1000 | Loss: 0.00001407
Iteration 105/1000 | Loss: 0.00001407
Iteration 106/1000 | Loss: 0.00001407
Iteration 107/1000 | Loss: 0.00001407
Iteration 108/1000 | Loss: 0.00001407
Iteration 109/1000 | Loss: 0.00001407
Iteration 110/1000 | Loss: 0.00001407
Iteration 111/1000 | Loss: 0.00001407
Iteration 112/1000 | Loss: 0.00001407
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00001406
Iteration 115/1000 | Loss: 0.00001406
Iteration 116/1000 | Loss: 0.00001406
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001405
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001405
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001405
Iteration 133/1000 | Loss: 0.00001405
Iteration 134/1000 | Loss: 0.00001405
Iteration 135/1000 | Loss: 0.00001405
Iteration 136/1000 | Loss: 0.00001405
Iteration 137/1000 | Loss: 0.00001405
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001405
Iteration 141/1000 | Loss: 0.00001405
Iteration 142/1000 | Loss: 0.00001405
Iteration 143/1000 | Loss: 0.00001405
Iteration 144/1000 | Loss: 0.00001405
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001404
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001403
Iteration 160/1000 | Loss: 0.00001403
Iteration 161/1000 | Loss: 0.00001403
Iteration 162/1000 | Loss: 0.00001403
Iteration 163/1000 | Loss: 0.00001403
Iteration 164/1000 | Loss: 0.00001403
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001403
Iteration 168/1000 | Loss: 0.00001403
Iteration 169/1000 | Loss: 0.00001403
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001403
Iteration 176/1000 | Loss: 0.00001403
Iteration 177/1000 | Loss: 0.00001403
Iteration 178/1000 | Loss: 0.00001403
Iteration 179/1000 | Loss: 0.00001403
Iteration 180/1000 | Loss: 0.00001403
Iteration 181/1000 | Loss: 0.00001403
Iteration 182/1000 | Loss: 0.00001403
Iteration 183/1000 | Loss: 0.00001403
Iteration 184/1000 | Loss: 0.00001403
Iteration 185/1000 | Loss: 0.00001403
Iteration 186/1000 | Loss: 0.00001403
Iteration 187/1000 | Loss: 0.00001403
Iteration 188/1000 | Loss: 0.00001403
Iteration 189/1000 | Loss: 0.00001403
Iteration 190/1000 | Loss: 0.00001403
Iteration 191/1000 | Loss: 0.00001403
Iteration 192/1000 | Loss: 0.00001403
Iteration 193/1000 | Loss: 0.00001403
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001403
Iteration 199/1000 | Loss: 0.00001403
Iteration 200/1000 | Loss: 0.00001403
Iteration 201/1000 | Loss: 0.00001403
Iteration 202/1000 | Loss: 0.00001403
Iteration 203/1000 | Loss: 0.00001403
Iteration 204/1000 | Loss: 0.00001403
Iteration 205/1000 | Loss: 0.00001403
Iteration 206/1000 | Loss: 0.00001403
Iteration 207/1000 | Loss: 0.00001403
Iteration 208/1000 | Loss: 0.00001403
Iteration 209/1000 | Loss: 0.00001403
Iteration 210/1000 | Loss: 0.00001403
Iteration 211/1000 | Loss: 0.00001403
Iteration 212/1000 | Loss: 0.00001403
Iteration 213/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.403014175593853e-05, 1.403014175593853e-05, 1.403014175593853e-05, 1.403014175593853e-05, 1.403014175593853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.403014175593853e-05

Optimization complete. Final v2v error: 3.190803050994873 mm

Highest mean error: 3.420809268951416 mm for frame 84

Lowest mean error: 2.9711456298828125 mm for frame 60

Saving results

Total time: 41.23425650596619
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961159
Iteration 2/25 | Loss: 0.00185569
Iteration 3/25 | Loss: 0.00150918
Iteration 4/25 | Loss: 0.00146966
Iteration 5/25 | Loss: 0.00146322
Iteration 6/25 | Loss: 0.00146239
Iteration 7/25 | Loss: 0.00146239
Iteration 8/25 | Loss: 0.00146239
Iteration 9/25 | Loss: 0.00146239
Iteration 10/25 | Loss: 0.00146239
Iteration 11/25 | Loss: 0.00146239
Iteration 12/25 | Loss: 0.00146239
Iteration 13/25 | Loss: 0.00146239
Iteration 14/25 | Loss: 0.00146239
Iteration 15/25 | Loss: 0.00146239
Iteration 16/25 | Loss: 0.00146239
Iteration 17/25 | Loss: 0.00146239
Iteration 18/25 | Loss: 0.00146239
Iteration 19/25 | Loss: 0.00146239
Iteration 20/25 | Loss: 0.00146239
Iteration 21/25 | Loss: 0.00146239
Iteration 22/25 | Loss: 0.00146239
Iteration 23/25 | Loss: 0.00146239
Iteration 24/25 | Loss: 0.00146239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001462393207475543, 0.001462393207475543, 0.001462393207475543, 0.001462393207475543, 0.001462393207475543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001462393207475543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01256394
Iteration 2/25 | Loss: 0.00120923
Iteration 3/25 | Loss: 0.00120918
Iteration 4/25 | Loss: 0.00120918
Iteration 5/25 | Loss: 0.00120918
Iteration 6/25 | Loss: 0.00120918
Iteration 7/25 | Loss: 0.00120918
Iteration 8/25 | Loss: 0.00120918
Iteration 9/25 | Loss: 0.00120918
Iteration 10/25 | Loss: 0.00120918
Iteration 11/25 | Loss: 0.00120918
Iteration 12/25 | Loss: 0.00120918
Iteration 13/25 | Loss: 0.00120918
Iteration 14/25 | Loss: 0.00120918
Iteration 15/25 | Loss: 0.00120918
Iteration 16/25 | Loss: 0.00120918
Iteration 17/25 | Loss: 0.00120918
Iteration 18/25 | Loss: 0.00120918
Iteration 19/25 | Loss: 0.00120918
Iteration 20/25 | Loss: 0.00120918
Iteration 21/25 | Loss: 0.00120918
Iteration 22/25 | Loss: 0.00120918
Iteration 23/25 | Loss: 0.00120918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012091760290786624, 0.0012091760290786624, 0.0012091760290786624, 0.0012091760290786624, 0.0012091760290786624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012091760290786624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120918
Iteration 2/1000 | Loss: 0.00007452
Iteration 3/1000 | Loss: 0.00004382
Iteration 4/1000 | Loss: 0.00003670
Iteration 5/1000 | Loss: 0.00003395
Iteration 6/1000 | Loss: 0.00003246
Iteration 7/1000 | Loss: 0.00003154
Iteration 8/1000 | Loss: 0.00003068
Iteration 9/1000 | Loss: 0.00003017
Iteration 10/1000 | Loss: 0.00002981
Iteration 11/1000 | Loss: 0.00002953
Iteration 12/1000 | Loss: 0.00002929
Iteration 13/1000 | Loss: 0.00002929
Iteration 14/1000 | Loss: 0.00002904
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00002866
Iteration 17/1000 | Loss: 0.00002852
Iteration 18/1000 | Loss: 0.00002846
Iteration 19/1000 | Loss: 0.00002843
Iteration 20/1000 | Loss: 0.00002837
Iteration 21/1000 | Loss: 0.00002837
Iteration 22/1000 | Loss: 0.00002835
Iteration 23/1000 | Loss: 0.00002834
Iteration 24/1000 | Loss: 0.00002834
Iteration 25/1000 | Loss: 0.00002834
Iteration 26/1000 | Loss: 0.00002833
Iteration 27/1000 | Loss: 0.00002831
Iteration 28/1000 | Loss: 0.00002831
Iteration 29/1000 | Loss: 0.00002829
Iteration 30/1000 | Loss: 0.00002829
Iteration 31/1000 | Loss: 0.00002829
Iteration 32/1000 | Loss: 0.00002829
Iteration 33/1000 | Loss: 0.00002829
Iteration 34/1000 | Loss: 0.00002829
Iteration 35/1000 | Loss: 0.00002829
Iteration 36/1000 | Loss: 0.00002829
Iteration 37/1000 | Loss: 0.00002829
Iteration 38/1000 | Loss: 0.00002829
Iteration 39/1000 | Loss: 0.00002828
Iteration 40/1000 | Loss: 0.00002828
Iteration 41/1000 | Loss: 0.00002827
Iteration 42/1000 | Loss: 0.00002827
Iteration 43/1000 | Loss: 0.00002827
Iteration 44/1000 | Loss: 0.00002826
Iteration 45/1000 | Loss: 0.00002826
Iteration 46/1000 | Loss: 0.00002826
Iteration 47/1000 | Loss: 0.00002825
Iteration 48/1000 | Loss: 0.00002825
Iteration 49/1000 | Loss: 0.00002824
Iteration 50/1000 | Loss: 0.00002824
Iteration 51/1000 | Loss: 0.00002824
Iteration 52/1000 | Loss: 0.00002823
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00002822
Iteration 55/1000 | Loss: 0.00002822
Iteration 56/1000 | Loss: 0.00002822
Iteration 57/1000 | Loss: 0.00002822
Iteration 58/1000 | Loss: 0.00002822
Iteration 59/1000 | Loss: 0.00002822
Iteration 60/1000 | Loss: 0.00002822
Iteration 61/1000 | Loss: 0.00002821
Iteration 62/1000 | Loss: 0.00002821
Iteration 63/1000 | Loss: 0.00002821
Iteration 64/1000 | Loss: 0.00002821
Iteration 65/1000 | Loss: 0.00002821
Iteration 66/1000 | Loss: 0.00002821
Iteration 67/1000 | Loss: 0.00002821
Iteration 68/1000 | Loss: 0.00002821
Iteration 69/1000 | Loss: 0.00002820
Iteration 70/1000 | Loss: 0.00002820
Iteration 71/1000 | Loss: 0.00002819
Iteration 72/1000 | Loss: 0.00002819
Iteration 73/1000 | Loss: 0.00002819
Iteration 74/1000 | Loss: 0.00002819
Iteration 75/1000 | Loss: 0.00002818
Iteration 76/1000 | Loss: 0.00002818
Iteration 77/1000 | Loss: 0.00002818
Iteration 78/1000 | Loss: 0.00002818
Iteration 79/1000 | Loss: 0.00002818
Iteration 80/1000 | Loss: 0.00002818
Iteration 81/1000 | Loss: 0.00002818
Iteration 82/1000 | Loss: 0.00002817
Iteration 83/1000 | Loss: 0.00002817
Iteration 84/1000 | Loss: 0.00002817
Iteration 85/1000 | Loss: 0.00002816
Iteration 86/1000 | Loss: 0.00002816
Iteration 87/1000 | Loss: 0.00002816
Iteration 88/1000 | Loss: 0.00002816
Iteration 89/1000 | Loss: 0.00002816
Iteration 90/1000 | Loss: 0.00002816
Iteration 91/1000 | Loss: 0.00002815
Iteration 92/1000 | Loss: 0.00002815
Iteration 93/1000 | Loss: 0.00002815
Iteration 94/1000 | Loss: 0.00002815
Iteration 95/1000 | Loss: 0.00002815
Iteration 96/1000 | Loss: 0.00002815
Iteration 97/1000 | Loss: 0.00002815
Iteration 98/1000 | Loss: 0.00002815
Iteration 99/1000 | Loss: 0.00002814
Iteration 100/1000 | Loss: 0.00002814
Iteration 101/1000 | Loss: 0.00002814
Iteration 102/1000 | Loss: 0.00002814
Iteration 103/1000 | Loss: 0.00002814
Iteration 104/1000 | Loss: 0.00002814
Iteration 105/1000 | Loss: 0.00002814
Iteration 106/1000 | Loss: 0.00002813
Iteration 107/1000 | Loss: 0.00002813
Iteration 108/1000 | Loss: 0.00002813
Iteration 109/1000 | Loss: 0.00002813
Iteration 110/1000 | Loss: 0.00002813
Iteration 111/1000 | Loss: 0.00002812
Iteration 112/1000 | Loss: 0.00002812
Iteration 113/1000 | Loss: 0.00002812
Iteration 114/1000 | Loss: 0.00002812
Iteration 115/1000 | Loss: 0.00002811
Iteration 116/1000 | Loss: 0.00002811
Iteration 117/1000 | Loss: 0.00002811
Iteration 118/1000 | Loss: 0.00002811
Iteration 119/1000 | Loss: 0.00002811
Iteration 120/1000 | Loss: 0.00002811
Iteration 121/1000 | Loss: 0.00002811
Iteration 122/1000 | Loss: 0.00002811
Iteration 123/1000 | Loss: 0.00002811
Iteration 124/1000 | Loss: 0.00002811
Iteration 125/1000 | Loss: 0.00002811
Iteration 126/1000 | Loss: 0.00002811
Iteration 127/1000 | Loss: 0.00002811
Iteration 128/1000 | Loss: 0.00002811
Iteration 129/1000 | Loss: 0.00002811
Iteration 130/1000 | Loss: 0.00002811
Iteration 131/1000 | Loss: 0.00002811
Iteration 132/1000 | Loss: 0.00002811
Iteration 133/1000 | Loss: 0.00002811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.8105701858294196e-05, 2.8105701858294196e-05, 2.8105701858294196e-05, 2.8105701858294196e-05, 2.8105701858294196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8105701858294196e-05

Optimization complete. Final v2v error: 4.323637962341309 mm

Highest mean error: 5.056268692016602 mm for frame 84

Lowest mean error: 3.483520984649658 mm for frame 26

Saving results

Total time: 45.74982523918152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835338
Iteration 2/25 | Loss: 0.00146809
Iteration 3/25 | Loss: 0.00131612
Iteration 4/25 | Loss: 0.00130036
Iteration 5/25 | Loss: 0.00129600
Iteration 6/25 | Loss: 0.00129561
Iteration 7/25 | Loss: 0.00129561
Iteration 8/25 | Loss: 0.00129561
Iteration 9/25 | Loss: 0.00129561
Iteration 10/25 | Loss: 0.00129561
Iteration 11/25 | Loss: 0.00129561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001295612077228725, 0.001295612077228725, 0.001295612077228725, 0.001295612077228725, 0.001295612077228725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001295612077228725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90092713
Iteration 2/25 | Loss: 0.00066290
Iteration 3/25 | Loss: 0.00066290
Iteration 4/25 | Loss: 0.00066290
Iteration 5/25 | Loss: 0.00066289
Iteration 6/25 | Loss: 0.00066289
Iteration 7/25 | Loss: 0.00066289
Iteration 8/25 | Loss: 0.00066289
Iteration 9/25 | Loss: 0.00066289
Iteration 10/25 | Loss: 0.00066289
Iteration 11/25 | Loss: 0.00066289
Iteration 12/25 | Loss: 0.00066289
Iteration 13/25 | Loss: 0.00066289
Iteration 14/25 | Loss: 0.00066289
Iteration 15/25 | Loss: 0.00066289
Iteration 16/25 | Loss: 0.00066289
Iteration 17/25 | Loss: 0.00066289
Iteration 18/25 | Loss: 0.00066289
Iteration 19/25 | Loss: 0.00066289
Iteration 20/25 | Loss: 0.00066289
Iteration 21/25 | Loss: 0.00066289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006628927076235414, 0.0006628927076235414, 0.0006628927076235414, 0.0006628927076235414, 0.0006628927076235414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006628927076235414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066289
Iteration 2/1000 | Loss: 0.00003987
Iteration 3/1000 | Loss: 0.00003273
Iteration 4/1000 | Loss: 0.00003012
Iteration 5/1000 | Loss: 0.00002917
Iteration 6/1000 | Loss: 0.00002819
Iteration 7/1000 | Loss: 0.00002760
Iteration 8/1000 | Loss: 0.00002726
Iteration 9/1000 | Loss: 0.00002701
Iteration 10/1000 | Loss: 0.00002677
Iteration 11/1000 | Loss: 0.00002673
Iteration 12/1000 | Loss: 0.00002657
Iteration 13/1000 | Loss: 0.00002655
Iteration 14/1000 | Loss: 0.00002640
Iteration 15/1000 | Loss: 0.00002639
Iteration 16/1000 | Loss: 0.00002638
Iteration 17/1000 | Loss: 0.00002638
Iteration 18/1000 | Loss: 0.00002638
Iteration 19/1000 | Loss: 0.00002638
Iteration 20/1000 | Loss: 0.00002638
Iteration 21/1000 | Loss: 0.00002638
Iteration 22/1000 | Loss: 0.00002638
Iteration 23/1000 | Loss: 0.00002638
Iteration 24/1000 | Loss: 0.00002638
Iteration 25/1000 | Loss: 0.00002637
Iteration 26/1000 | Loss: 0.00002637
Iteration 27/1000 | Loss: 0.00002637
Iteration 28/1000 | Loss: 0.00002637
Iteration 29/1000 | Loss: 0.00002637
Iteration 30/1000 | Loss: 0.00002637
Iteration 31/1000 | Loss: 0.00002637
Iteration 32/1000 | Loss: 0.00002636
Iteration 33/1000 | Loss: 0.00002636
Iteration 34/1000 | Loss: 0.00002636
Iteration 35/1000 | Loss: 0.00002633
Iteration 36/1000 | Loss: 0.00002632
Iteration 37/1000 | Loss: 0.00002631
Iteration 38/1000 | Loss: 0.00002630
Iteration 39/1000 | Loss: 0.00002629
Iteration 40/1000 | Loss: 0.00002629
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002629
Iteration 43/1000 | Loss: 0.00002629
Iteration 44/1000 | Loss: 0.00002629
Iteration 45/1000 | Loss: 0.00002629
Iteration 46/1000 | Loss: 0.00002629
Iteration 47/1000 | Loss: 0.00002629
Iteration 48/1000 | Loss: 0.00002628
Iteration 49/1000 | Loss: 0.00002625
Iteration 50/1000 | Loss: 0.00002625
Iteration 51/1000 | Loss: 0.00002625
Iteration 52/1000 | Loss: 0.00002625
Iteration 53/1000 | Loss: 0.00002625
Iteration 54/1000 | Loss: 0.00002625
Iteration 55/1000 | Loss: 0.00002625
Iteration 56/1000 | Loss: 0.00002625
Iteration 57/1000 | Loss: 0.00002624
Iteration 58/1000 | Loss: 0.00002624
Iteration 59/1000 | Loss: 0.00002624
Iteration 60/1000 | Loss: 0.00002624
Iteration 61/1000 | Loss: 0.00002624
Iteration 62/1000 | Loss: 0.00002624
Iteration 63/1000 | Loss: 0.00002624
Iteration 64/1000 | Loss: 0.00002624
Iteration 65/1000 | Loss: 0.00002624
Iteration 66/1000 | Loss: 0.00002624
Iteration 67/1000 | Loss: 0.00002624
Iteration 68/1000 | Loss: 0.00002624
Iteration 69/1000 | Loss: 0.00002624
Iteration 70/1000 | Loss: 0.00002624
Iteration 71/1000 | Loss: 0.00002624
Iteration 72/1000 | Loss: 0.00002624
Iteration 73/1000 | Loss: 0.00002624
Iteration 74/1000 | Loss: 0.00002624
Iteration 75/1000 | Loss: 0.00002624
Iteration 76/1000 | Loss: 0.00002624
Iteration 77/1000 | Loss: 0.00002624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.624096850922797e-05, 2.624096850922797e-05, 2.624096850922797e-05, 2.624096850922797e-05, 2.624096850922797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.624096850922797e-05

Optimization complete. Final v2v error: 4.329487323760986 mm

Highest mean error: 4.415059566497803 mm for frame 28

Lowest mean error: 4.2415242195129395 mm for frame 111

Saving results

Total time: 29.25805354118347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393454
Iteration 2/25 | Loss: 0.00135702
Iteration 3/25 | Loss: 0.00126206
Iteration 4/25 | Loss: 0.00125425
Iteration 5/25 | Loss: 0.00125166
Iteration 6/25 | Loss: 0.00125166
Iteration 7/25 | Loss: 0.00125166
Iteration 8/25 | Loss: 0.00125166
Iteration 9/25 | Loss: 0.00125166
Iteration 10/25 | Loss: 0.00125166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012516636634245515, 0.0012516636634245515, 0.0012516636634245515, 0.0012516636634245515, 0.0012516636634245515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012516636634245515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63183141
Iteration 2/25 | Loss: 0.00100028
Iteration 3/25 | Loss: 0.00100027
Iteration 4/25 | Loss: 0.00100027
Iteration 5/25 | Loss: 0.00100027
Iteration 6/25 | Loss: 0.00100027
Iteration 7/25 | Loss: 0.00100027
Iteration 8/25 | Loss: 0.00100027
Iteration 9/25 | Loss: 0.00100027
Iteration 10/25 | Loss: 0.00100027
Iteration 11/25 | Loss: 0.00100027
Iteration 12/25 | Loss: 0.00100027
Iteration 13/25 | Loss: 0.00100027
Iteration 14/25 | Loss: 0.00100027
Iteration 15/25 | Loss: 0.00100027
Iteration 16/25 | Loss: 0.00100027
Iteration 17/25 | Loss: 0.00100027
Iteration 18/25 | Loss: 0.00100027
Iteration 19/25 | Loss: 0.00100027
Iteration 20/25 | Loss: 0.00100027
Iteration 21/25 | Loss: 0.00100027
Iteration 22/25 | Loss: 0.00100027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010002690833061934, 0.0010002690833061934, 0.0010002690833061934, 0.0010002690833061934, 0.0010002690833061934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010002690833061934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100027
Iteration 2/1000 | Loss: 0.00002851
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001363
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001166
Iteration 10/1000 | Loss: 0.00001144
Iteration 11/1000 | Loss: 0.00001128
Iteration 12/1000 | Loss: 0.00001123
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001113
Iteration 16/1000 | Loss: 0.00001113
Iteration 17/1000 | Loss: 0.00001112
Iteration 18/1000 | Loss: 0.00001111
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001109
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001105
Iteration 31/1000 | Loss: 0.00001105
Iteration 32/1000 | Loss: 0.00001105
Iteration 33/1000 | Loss: 0.00001105
Iteration 34/1000 | Loss: 0.00001104
Iteration 35/1000 | Loss: 0.00001104
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001100
Iteration 40/1000 | Loss: 0.00001100
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001099
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001097
Iteration 47/1000 | Loss: 0.00001096
Iteration 48/1000 | Loss: 0.00001096
Iteration 49/1000 | Loss: 0.00001096
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001095
Iteration 52/1000 | Loss: 0.00001095
Iteration 53/1000 | Loss: 0.00001095
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001088
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001085
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001080
Iteration 80/1000 | Loss: 0.00001080
Iteration 81/1000 | Loss: 0.00001080
Iteration 82/1000 | Loss: 0.00001080
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001080
Iteration 85/1000 | Loss: 0.00001079
Iteration 86/1000 | Loss: 0.00001079
Iteration 87/1000 | Loss: 0.00001079
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001078
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001077
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001073
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001068
Iteration 137/1000 | Loss: 0.00001068
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001068
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001067
Iteration 145/1000 | Loss: 0.00001067
Iteration 146/1000 | Loss: 0.00001067
Iteration 147/1000 | Loss: 0.00001067
Iteration 148/1000 | Loss: 0.00001067
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001065
Iteration 153/1000 | Loss: 0.00001065
Iteration 154/1000 | Loss: 0.00001065
Iteration 155/1000 | Loss: 0.00001065
Iteration 156/1000 | Loss: 0.00001065
Iteration 157/1000 | Loss: 0.00001064
Iteration 158/1000 | Loss: 0.00001064
Iteration 159/1000 | Loss: 0.00001064
Iteration 160/1000 | Loss: 0.00001064
Iteration 161/1000 | Loss: 0.00001064
Iteration 162/1000 | Loss: 0.00001064
Iteration 163/1000 | Loss: 0.00001063
Iteration 164/1000 | Loss: 0.00001063
Iteration 165/1000 | Loss: 0.00001063
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001062
Iteration 169/1000 | Loss: 0.00001062
Iteration 170/1000 | Loss: 0.00001062
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001060
Iteration 178/1000 | Loss: 0.00001060
Iteration 179/1000 | Loss: 0.00001060
Iteration 180/1000 | Loss: 0.00001060
Iteration 181/1000 | Loss: 0.00001060
Iteration 182/1000 | Loss: 0.00001060
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001059
Iteration 185/1000 | Loss: 0.00001059
Iteration 186/1000 | Loss: 0.00001059
Iteration 187/1000 | Loss: 0.00001059
Iteration 188/1000 | Loss: 0.00001059
Iteration 189/1000 | Loss: 0.00001059
Iteration 190/1000 | Loss: 0.00001059
Iteration 191/1000 | Loss: 0.00001059
Iteration 192/1000 | Loss: 0.00001059
Iteration 193/1000 | Loss: 0.00001059
Iteration 194/1000 | Loss: 0.00001059
Iteration 195/1000 | Loss: 0.00001059
Iteration 196/1000 | Loss: 0.00001058
Iteration 197/1000 | Loss: 0.00001058
Iteration 198/1000 | Loss: 0.00001058
Iteration 199/1000 | Loss: 0.00001058
Iteration 200/1000 | Loss: 0.00001058
Iteration 201/1000 | Loss: 0.00001058
Iteration 202/1000 | Loss: 0.00001057
Iteration 203/1000 | Loss: 0.00001057
Iteration 204/1000 | Loss: 0.00001057
Iteration 205/1000 | Loss: 0.00001057
Iteration 206/1000 | Loss: 0.00001057
Iteration 207/1000 | Loss: 0.00001057
Iteration 208/1000 | Loss: 0.00001057
Iteration 209/1000 | Loss: 0.00001057
Iteration 210/1000 | Loss: 0.00001057
Iteration 211/1000 | Loss: 0.00001057
Iteration 212/1000 | Loss: 0.00001057
Iteration 213/1000 | Loss: 0.00001057
Iteration 214/1000 | Loss: 0.00001057
Iteration 215/1000 | Loss: 0.00001057
Iteration 216/1000 | Loss: 0.00001057
Iteration 217/1000 | Loss: 0.00001057
Iteration 218/1000 | Loss: 0.00001057
Iteration 219/1000 | Loss: 0.00001057
Iteration 220/1000 | Loss: 0.00001057
Iteration 221/1000 | Loss: 0.00001057
Iteration 222/1000 | Loss: 0.00001057
Iteration 223/1000 | Loss: 0.00001057
Iteration 224/1000 | Loss: 0.00001057
Iteration 225/1000 | Loss: 0.00001057
Iteration 226/1000 | Loss: 0.00001057
Iteration 227/1000 | Loss: 0.00001057
Iteration 228/1000 | Loss: 0.00001057
Iteration 229/1000 | Loss: 0.00001057
Iteration 230/1000 | Loss: 0.00001057
Iteration 231/1000 | Loss: 0.00001057
Iteration 232/1000 | Loss: 0.00001057
Iteration 233/1000 | Loss: 0.00001057
Iteration 234/1000 | Loss: 0.00001057
Iteration 235/1000 | Loss: 0.00001057
Iteration 236/1000 | Loss: 0.00001057
Iteration 237/1000 | Loss: 0.00001057
Iteration 238/1000 | Loss: 0.00001057
Iteration 239/1000 | Loss: 0.00001057
Iteration 240/1000 | Loss: 0.00001057
Iteration 241/1000 | Loss: 0.00001057
Iteration 242/1000 | Loss: 0.00001057
Iteration 243/1000 | Loss: 0.00001057
Iteration 244/1000 | Loss: 0.00001057
Iteration 245/1000 | Loss: 0.00001057
Iteration 246/1000 | Loss: 0.00001057
Iteration 247/1000 | Loss: 0.00001057
Iteration 248/1000 | Loss: 0.00001057
Iteration 249/1000 | Loss: 0.00001057
Iteration 250/1000 | Loss: 0.00001057
Iteration 251/1000 | Loss: 0.00001057
Iteration 252/1000 | Loss: 0.00001057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.0569362530077342e-05, 1.0569362530077342e-05, 1.0569362530077342e-05, 1.0569362530077342e-05, 1.0569362530077342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0569362530077342e-05

Optimization complete. Final v2v error: 2.785642623901367 mm

Highest mean error: 2.927983045578003 mm for frame 106

Lowest mean error: 2.672638177871704 mm for frame 198

Saving results

Total time: 48.95010232925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012287
Iteration 2/25 | Loss: 0.01012287
Iteration 3/25 | Loss: 0.01012287
Iteration 4/25 | Loss: 0.00189052
Iteration 5/25 | Loss: 0.00137241
Iteration 6/25 | Loss: 0.00131830
Iteration 7/25 | Loss: 0.00131030
Iteration 8/25 | Loss: 0.00129917
Iteration 9/25 | Loss: 0.00129644
Iteration 10/25 | Loss: 0.00126986
Iteration 11/25 | Loss: 0.00126205
Iteration 12/25 | Loss: 0.00125851
Iteration 13/25 | Loss: 0.00125878
Iteration 14/25 | Loss: 0.00125769
Iteration 15/25 | Loss: 0.00125759
Iteration 16/25 | Loss: 0.00125786
Iteration 17/25 | Loss: 0.00125840
Iteration 18/25 | Loss: 0.00125743
Iteration 19/25 | Loss: 0.00125650
Iteration 20/25 | Loss: 0.00125634
Iteration 21/25 | Loss: 0.00125632
Iteration 22/25 | Loss: 0.00125632
Iteration 23/25 | Loss: 0.00125631
Iteration 24/25 | Loss: 0.00125630
Iteration 25/25 | Loss: 0.00125630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37056077
Iteration 2/25 | Loss: 0.00112711
Iteration 3/25 | Loss: 0.00112710
Iteration 4/25 | Loss: 0.00112710
Iteration 5/25 | Loss: 0.00112710
Iteration 6/25 | Loss: 0.00112710
Iteration 7/25 | Loss: 0.00112710
Iteration 8/25 | Loss: 0.00112710
Iteration 9/25 | Loss: 0.00112710
Iteration 10/25 | Loss: 0.00112710
Iteration 11/25 | Loss: 0.00112710
Iteration 12/25 | Loss: 0.00112710
Iteration 13/25 | Loss: 0.00112710
Iteration 14/25 | Loss: 0.00112710
Iteration 15/25 | Loss: 0.00112710
Iteration 16/25 | Loss: 0.00112710
Iteration 17/25 | Loss: 0.00112710
Iteration 18/25 | Loss: 0.00112710
Iteration 19/25 | Loss: 0.00112710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011271038092672825, 0.0011271038092672825, 0.0011271038092672825, 0.0011271038092672825, 0.0011271038092672825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011271038092672825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112710
Iteration 2/1000 | Loss: 0.00003051
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001807
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00006270
Iteration 7/1000 | Loss: 0.00023229
Iteration 8/1000 | Loss: 0.00034664
Iteration 9/1000 | Loss: 0.00003193
Iteration 10/1000 | Loss: 0.00008454
Iteration 11/1000 | Loss: 0.00003094
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00007486
Iteration 14/1000 | Loss: 0.00020410
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00003050
Iteration 17/1000 | Loss: 0.00006892
Iteration 18/1000 | Loss: 0.00002439
Iteration 19/1000 | Loss: 0.00004112
Iteration 20/1000 | Loss: 0.00001609
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001438
Iteration 28/1000 | Loss: 0.00001437
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001426
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00005667
Iteration 34/1000 | Loss: 0.00005330
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00028075
Iteration 37/1000 | Loss: 0.00002504
Iteration 38/1000 | Loss: 0.00008681
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00006049
Iteration 41/1000 | Loss: 0.00002434
Iteration 42/1000 | Loss: 0.00009993
Iteration 43/1000 | Loss: 0.00002458
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00016591
Iteration 46/1000 | Loss: 0.00002994
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00005177
Iteration 50/1000 | Loss: 0.00011170
Iteration 51/1000 | Loss: 0.00003457
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00002523
Iteration 54/1000 | Loss: 0.00001401
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00002191
Iteration 72/1000 | Loss: 0.00014734
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00003818
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001607
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001388
Iteration 91/1000 | Loss: 0.00001388
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00002929
Iteration 110/1000 | Loss: 0.00010784
Iteration 111/1000 | Loss: 0.00003181
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00006196
Iteration 114/1000 | Loss: 0.00002520
Iteration 115/1000 | Loss: 0.00001676
Iteration 116/1000 | Loss: 0.00004046
Iteration 117/1000 | Loss: 0.00002448
Iteration 118/1000 | Loss: 0.00001679
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001646
Iteration 121/1000 | Loss: 0.00003832
Iteration 122/1000 | Loss: 0.00002648
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00003553
Iteration 126/1000 | Loss: 0.00003034
Iteration 127/1000 | Loss: 0.00003064
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001384
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00003046
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00003166
Iteration 153/1000 | Loss: 0.00001818
Iteration 154/1000 | Loss: 0.00001954
Iteration 155/1000 | Loss: 0.00006203
Iteration 156/1000 | Loss: 0.00021843
Iteration 157/1000 | Loss: 0.00001582
Iteration 158/1000 | Loss: 0.00004787
Iteration 159/1000 | Loss: 0.00001793
Iteration 160/1000 | Loss: 0.00001435
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001383
Iteration 169/1000 | Loss: 0.00001383
Iteration 170/1000 | Loss: 0.00001383
Iteration 171/1000 | Loss: 0.00001383
Iteration 172/1000 | Loss: 0.00001383
Iteration 173/1000 | Loss: 0.00001383
Iteration 174/1000 | Loss: 0.00001383
Iteration 175/1000 | Loss: 0.00001382
Iteration 176/1000 | Loss: 0.00001382
Iteration 177/1000 | Loss: 0.00001382
Iteration 178/1000 | Loss: 0.00001382
Iteration 179/1000 | Loss: 0.00001829
Iteration 180/1000 | Loss: 0.00001829
Iteration 181/1000 | Loss: 0.00002815
Iteration 182/1000 | Loss: 0.00001437
Iteration 183/1000 | Loss: 0.00002165
Iteration 184/1000 | Loss: 0.00005454
Iteration 185/1000 | Loss: 0.00002114
Iteration 186/1000 | Loss: 0.00001810
Iteration 187/1000 | Loss: 0.00001598
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001381
Iteration 193/1000 | Loss: 0.00001381
Iteration 194/1000 | Loss: 0.00001381
Iteration 195/1000 | Loss: 0.00001381
Iteration 196/1000 | Loss: 0.00001381
Iteration 197/1000 | Loss: 0.00001827
Iteration 198/1000 | Loss: 0.00001566
Iteration 199/1000 | Loss: 0.00001382
Iteration 200/1000 | Loss: 0.00001382
Iteration 201/1000 | Loss: 0.00001381
Iteration 202/1000 | Loss: 0.00001381
Iteration 203/1000 | Loss: 0.00001381
Iteration 204/1000 | Loss: 0.00001381
Iteration 205/1000 | Loss: 0.00001381
Iteration 206/1000 | Loss: 0.00001381
Iteration 207/1000 | Loss: 0.00001381
Iteration 208/1000 | Loss: 0.00001381
Iteration 209/1000 | Loss: 0.00001381
Iteration 210/1000 | Loss: 0.00001381
Iteration 211/1000 | Loss: 0.00001381
Iteration 212/1000 | Loss: 0.00001381
Iteration 213/1000 | Loss: 0.00001381
Iteration 214/1000 | Loss: 0.00001381
Iteration 215/1000 | Loss: 0.00001381
Iteration 216/1000 | Loss: 0.00001381
Iteration 217/1000 | Loss: 0.00001381
Iteration 218/1000 | Loss: 0.00001381
Iteration 219/1000 | Loss: 0.00001381
Iteration 220/1000 | Loss: 0.00001381
Iteration 221/1000 | Loss: 0.00001381
Iteration 222/1000 | Loss: 0.00001381
Iteration 223/1000 | Loss: 0.00001381
Iteration 224/1000 | Loss: 0.00001381
Iteration 225/1000 | Loss: 0.00001381
Iteration 226/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.3812168617732823e-05, 1.3812168617732823e-05, 1.3812168617732823e-05, 1.3812168617732823e-05, 1.3812168617732823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3812168617732823e-05

Optimization complete. Final v2v error: 3.1841859817504883 mm

Highest mean error: 3.7470409870147705 mm for frame 11

Lowest mean error: 2.8428330421447754 mm for frame 38

Saving results

Total time: 175.75910019874573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495648
Iteration 2/25 | Loss: 0.00153370
Iteration 3/25 | Loss: 0.00132276
Iteration 4/25 | Loss: 0.00129885
Iteration 5/25 | Loss: 0.00129479
Iteration 6/25 | Loss: 0.00129368
Iteration 7/25 | Loss: 0.00129368
Iteration 8/25 | Loss: 0.00129368
Iteration 9/25 | Loss: 0.00129368
Iteration 10/25 | Loss: 0.00129368
Iteration 11/25 | Loss: 0.00129368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001293684123083949, 0.001293684123083949, 0.001293684123083949, 0.001293684123083949, 0.001293684123083949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001293684123083949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37327623
Iteration 2/25 | Loss: 0.00095311
Iteration 3/25 | Loss: 0.00095311
Iteration 4/25 | Loss: 0.00095311
Iteration 5/25 | Loss: 0.00095311
Iteration 6/25 | Loss: 0.00095311
Iteration 7/25 | Loss: 0.00095311
Iteration 8/25 | Loss: 0.00095311
Iteration 9/25 | Loss: 0.00095311
Iteration 10/25 | Loss: 0.00095311
Iteration 11/25 | Loss: 0.00095311
Iteration 12/25 | Loss: 0.00095311
Iteration 13/25 | Loss: 0.00095311
Iteration 14/25 | Loss: 0.00095311
Iteration 15/25 | Loss: 0.00095311
Iteration 16/25 | Loss: 0.00095311
Iteration 17/25 | Loss: 0.00095311
Iteration 18/25 | Loss: 0.00095311
Iteration 19/25 | Loss: 0.00095311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009531051036901772, 0.0009531051036901772, 0.0009531051036901772, 0.0009531051036901772, 0.0009531051036901772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009531051036901772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095311
Iteration 2/1000 | Loss: 0.00003741
Iteration 3/1000 | Loss: 0.00002309
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001926
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001783
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001686
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001658
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001637
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001632
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001628
Iteration 26/1000 | Loss: 0.00001628
Iteration 27/1000 | Loss: 0.00001628
Iteration 28/1000 | Loss: 0.00001628
Iteration 29/1000 | Loss: 0.00001627
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001625
Iteration 34/1000 | Loss: 0.00001625
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001622
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001620
Iteration 46/1000 | Loss: 0.00001620
Iteration 47/1000 | Loss: 0.00001619
Iteration 48/1000 | Loss: 0.00001619
Iteration 49/1000 | Loss: 0.00001618
Iteration 50/1000 | Loss: 0.00001618
Iteration 51/1000 | Loss: 0.00001617
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001617
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001616
Iteration 56/1000 | Loss: 0.00001616
Iteration 57/1000 | Loss: 0.00001615
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001614
Iteration 60/1000 | Loss: 0.00001614
Iteration 61/1000 | Loss: 0.00001614
Iteration 62/1000 | Loss: 0.00001614
Iteration 63/1000 | Loss: 0.00001614
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00001613
Iteration 66/1000 | Loss: 0.00001613
Iteration 67/1000 | Loss: 0.00001613
Iteration 68/1000 | Loss: 0.00001612
Iteration 69/1000 | Loss: 0.00001612
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001611
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001611
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001610
Iteration 81/1000 | Loss: 0.00001610
Iteration 82/1000 | Loss: 0.00001610
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001609
Iteration 85/1000 | Loss: 0.00001609
Iteration 86/1000 | Loss: 0.00001609
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001608
Iteration 90/1000 | Loss: 0.00001607
Iteration 91/1000 | Loss: 0.00001607
Iteration 92/1000 | Loss: 0.00001607
Iteration 93/1000 | Loss: 0.00001607
Iteration 94/1000 | Loss: 0.00001607
Iteration 95/1000 | Loss: 0.00001607
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001606
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001605
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001603
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001601
Iteration 115/1000 | Loss: 0.00001601
Iteration 116/1000 | Loss: 0.00001601
Iteration 117/1000 | Loss: 0.00001600
Iteration 118/1000 | Loss: 0.00001600
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001599
Iteration 124/1000 | Loss: 0.00001599
Iteration 125/1000 | Loss: 0.00001599
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001596
Iteration 138/1000 | Loss: 0.00001596
Iteration 139/1000 | Loss: 0.00001596
Iteration 140/1000 | Loss: 0.00001596
Iteration 141/1000 | Loss: 0.00001596
Iteration 142/1000 | Loss: 0.00001596
Iteration 143/1000 | Loss: 0.00001596
Iteration 144/1000 | Loss: 0.00001595
Iteration 145/1000 | Loss: 0.00001595
Iteration 146/1000 | Loss: 0.00001595
Iteration 147/1000 | Loss: 0.00001595
Iteration 148/1000 | Loss: 0.00001595
Iteration 149/1000 | Loss: 0.00001595
Iteration 150/1000 | Loss: 0.00001595
Iteration 151/1000 | Loss: 0.00001594
Iteration 152/1000 | Loss: 0.00001594
Iteration 153/1000 | Loss: 0.00001594
Iteration 154/1000 | Loss: 0.00001594
Iteration 155/1000 | Loss: 0.00001593
Iteration 156/1000 | Loss: 0.00001593
Iteration 157/1000 | Loss: 0.00001593
Iteration 158/1000 | Loss: 0.00001593
Iteration 159/1000 | Loss: 0.00001592
Iteration 160/1000 | Loss: 0.00001592
Iteration 161/1000 | Loss: 0.00001592
Iteration 162/1000 | Loss: 0.00001591
Iteration 163/1000 | Loss: 0.00001591
Iteration 164/1000 | Loss: 0.00001591
Iteration 165/1000 | Loss: 0.00001590
Iteration 166/1000 | Loss: 0.00001590
Iteration 167/1000 | Loss: 0.00001590
Iteration 168/1000 | Loss: 0.00001590
Iteration 169/1000 | Loss: 0.00001590
Iteration 170/1000 | Loss: 0.00001590
Iteration 171/1000 | Loss: 0.00001590
Iteration 172/1000 | Loss: 0.00001589
Iteration 173/1000 | Loss: 0.00001589
Iteration 174/1000 | Loss: 0.00001589
Iteration 175/1000 | Loss: 0.00001589
Iteration 176/1000 | Loss: 0.00001588
Iteration 177/1000 | Loss: 0.00001588
Iteration 178/1000 | Loss: 0.00001588
Iteration 179/1000 | Loss: 0.00001588
Iteration 180/1000 | Loss: 0.00001588
Iteration 181/1000 | Loss: 0.00001588
Iteration 182/1000 | Loss: 0.00001588
Iteration 183/1000 | Loss: 0.00001588
Iteration 184/1000 | Loss: 0.00001588
Iteration 185/1000 | Loss: 0.00001588
Iteration 186/1000 | Loss: 0.00001588
Iteration 187/1000 | Loss: 0.00001588
Iteration 188/1000 | Loss: 0.00001587
Iteration 189/1000 | Loss: 0.00001587
Iteration 190/1000 | Loss: 0.00001587
Iteration 191/1000 | Loss: 0.00001587
Iteration 192/1000 | Loss: 0.00001587
Iteration 193/1000 | Loss: 0.00001587
Iteration 194/1000 | Loss: 0.00001587
Iteration 195/1000 | Loss: 0.00001587
Iteration 196/1000 | Loss: 0.00001587
Iteration 197/1000 | Loss: 0.00001587
Iteration 198/1000 | Loss: 0.00001587
Iteration 199/1000 | Loss: 0.00001586
Iteration 200/1000 | Loss: 0.00001586
Iteration 201/1000 | Loss: 0.00001586
Iteration 202/1000 | Loss: 0.00001586
Iteration 203/1000 | Loss: 0.00001586
Iteration 204/1000 | Loss: 0.00001586
Iteration 205/1000 | Loss: 0.00001586
Iteration 206/1000 | Loss: 0.00001586
Iteration 207/1000 | Loss: 0.00001586
Iteration 208/1000 | Loss: 0.00001586
Iteration 209/1000 | Loss: 0.00001586
Iteration 210/1000 | Loss: 0.00001586
Iteration 211/1000 | Loss: 0.00001586
Iteration 212/1000 | Loss: 0.00001586
Iteration 213/1000 | Loss: 0.00001586
Iteration 214/1000 | Loss: 0.00001586
Iteration 215/1000 | Loss: 0.00001586
Iteration 216/1000 | Loss: 0.00001585
Iteration 217/1000 | Loss: 0.00001585
Iteration 218/1000 | Loss: 0.00001585
Iteration 219/1000 | Loss: 0.00001585
Iteration 220/1000 | Loss: 0.00001585
Iteration 221/1000 | Loss: 0.00001585
Iteration 222/1000 | Loss: 0.00001585
Iteration 223/1000 | Loss: 0.00001585
Iteration 224/1000 | Loss: 0.00001585
Iteration 225/1000 | Loss: 0.00001585
Iteration 226/1000 | Loss: 0.00001585
Iteration 227/1000 | Loss: 0.00001585
Iteration 228/1000 | Loss: 0.00001585
Iteration 229/1000 | Loss: 0.00001585
Iteration 230/1000 | Loss: 0.00001585
Iteration 231/1000 | Loss: 0.00001585
Iteration 232/1000 | Loss: 0.00001585
Iteration 233/1000 | Loss: 0.00001585
Iteration 234/1000 | Loss: 0.00001585
Iteration 235/1000 | Loss: 0.00001585
Iteration 236/1000 | Loss: 0.00001585
Iteration 237/1000 | Loss: 0.00001585
Iteration 238/1000 | Loss: 0.00001585
Iteration 239/1000 | Loss: 0.00001585
Iteration 240/1000 | Loss: 0.00001585
Iteration 241/1000 | Loss: 0.00001585
Iteration 242/1000 | Loss: 0.00001585
Iteration 243/1000 | Loss: 0.00001585
Iteration 244/1000 | Loss: 0.00001585
Iteration 245/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.5852832802920602e-05, 1.5852832802920602e-05, 1.5852832802920602e-05, 1.5852832802920602e-05, 1.5852832802920602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5852832802920602e-05

Optimization complete. Final v2v error: 3.338043212890625 mm

Highest mean error: 4.376777172088623 mm for frame 92

Lowest mean error: 2.998399257659912 mm for frame 16

Saving results

Total time: 45.80170297622681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690344
Iteration 2/25 | Loss: 0.00159140
Iteration 3/25 | Loss: 0.00145199
Iteration 4/25 | Loss: 0.00142932
Iteration 5/25 | Loss: 0.00142301
Iteration 6/25 | Loss: 0.00142237
Iteration 7/25 | Loss: 0.00142237
Iteration 8/25 | Loss: 0.00142237
Iteration 9/25 | Loss: 0.00142237
Iteration 10/25 | Loss: 0.00142237
Iteration 11/25 | Loss: 0.00142237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001422371482476592, 0.001422371482476592, 0.001422371482476592, 0.001422371482476592, 0.001422371482476592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001422371482476592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58413887
Iteration 2/25 | Loss: 0.00146534
Iteration 3/25 | Loss: 0.00146534
Iteration 4/25 | Loss: 0.00146534
Iteration 5/25 | Loss: 0.00146534
Iteration 6/25 | Loss: 0.00146534
Iteration 7/25 | Loss: 0.00146534
Iteration 8/25 | Loss: 0.00146534
Iteration 9/25 | Loss: 0.00146534
Iteration 10/25 | Loss: 0.00146534
Iteration 11/25 | Loss: 0.00146534
Iteration 12/25 | Loss: 0.00146534
Iteration 13/25 | Loss: 0.00146534
Iteration 14/25 | Loss: 0.00146534
Iteration 15/25 | Loss: 0.00146534
Iteration 16/25 | Loss: 0.00146534
Iteration 17/25 | Loss: 0.00146534
Iteration 18/25 | Loss: 0.00146534
Iteration 19/25 | Loss: 0.00146534
Iteration 20/25 | Loss: 0.00146534
Iteration 21/25 | Loss: 0.00146534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014653382822871208, 0.0014653382822871208, 0.0014653382822871208, 0.0014653382822871208, 0.0014653382822871208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014653382822871208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146534
Iteration 2/1000 | Loss: 0.00008882
Iteration 3/1000 | Loss: 0.00004980
Iteration 4/1000 | Loss: 0.00004042
Iteration 5/1000 | Loss: 0.00003704
Iteration 6/1000 | Loss: 0.00003537
Iteration 7/1000 | Loss: 0.00003426
Iteration 8/1000 | Loss: 0.00003323
Iteration 9/1000 | Loss: 0.00003251
Iteration 10/1000 | Loss: 0.00003207
Iteration 11/1000 | Loss: 0.00003167
Iteration 12/1000 | Loss: 0.00003136
Iteration 13/1000 | Loss: 0.00003113
Iteration 14/1000 | Loss: 0.00003086
Iteration 15/1000 | Loss: 0.00003071
Iteration 16/1000 | Loss: 0.00003058
Iteration 17/1000 | Loss: 0.00003054
Iteration 18/1000 | Loss: 0.00003052
Iteration 19/1000 | Loss: 0.00003046
Iteration 20/1000 | Loss: 0.00003043
Iteration 21/1000 | Loss: 0.00003043
Iteration 22/1000 | Loss: 0.00003042
Iteration 23/1000 | Loss: 0.00003042
Iteration 24/1000 | Loss: 0.00003034
Iteration 25/1000 | Loss: 0.00003030
Iteration 26/1000 | Loss: 0.00003030
Iteration 27/1000 | Loss: 0.00003029
Iteration 28/1000 | Loss: 0.00003029
Iteration 29/1000 | Loss: 0.00003028
Iteration 30/1000 | Loss: 0.00003028
Iteration 31/1000 | Loss: 0.00003028
Iteration 32/1000 | Loss: 0.00003028
Iteration 33/1000 | Loss: 0.00003027
Iteration 34/1000 | Loss: 0.00003027
Iteration 35/1000 | Loss: 0.00003027
Iteration 36/1000 | Loss: 0.00003025
Iteration 37/1000 | Loss: 0.00003025
Iteration 38/1000 | Loss: 0.00003025
Iteration 39/1000 | Loss: 0.00003025
Iteration 40/1000 | Loss: 0.00003025
Iteration 41/1000 | Loss: 0.00003024
Iteration 42/1000 | Loss: 0.00003023
Iteration 43/1000 | Loss: 0.00003023
Iteration 44/1000 | Loss: 0.00003023
Iteration 45/1000 | Loss: 0.00003022
Iteration 46/1000 | Loss: 0.00003022
Iteration 47/1000 | Loss: 0.00003022
Iteration 48/1000 | Loss: 0.00003022
Iteration 49/1000 | Loss: 0.00003021
Iteration 50/1000 | Loss: 0.00003021
Iteration 51/1000 | Loss: 0.00003020
Iteration 52/1000 | Loss: 0.00003020
Iteration 53/1000 | Loss: 0.00003020
Iteration 54/1000 | Loss: 0.00003019
Iteration 55/1000 | Loss: 0.00003019
Iteration 56/1000 | Loss: 0.00003019
Iteration 57/1000 | Loss: 0.00003018
Iteration 58/1000 | Loss: 0.00003018
Iteration 59/1000 | Loss: 0.00003017
Iteration 60/1000 | Loss: 0.00003017
Iteration 61/1000 | Loss: 0.00003017
Iteration 62/1000 | Loss: 0.00003017
Iteration 63/1000 | Loss: 0.00003017
Iteration 64/1000 | Loss: 0.00003017
Iteration 65/1000 | Loss: 0.00003016
Iteration 66/1000 | Loss: 0.00003016
Iteration 67/1000 | Loss: 0.00003016
Iteration 68/1000 | Loss: 0.00003016
Iteration 69/1000 | Loss: 0.00003015
Iteration 70/1000 | Loss: 0.00003014
Iteration 71/1000 | Loss: 0.00003014
Iteration 72/1000 | Loss: 0.00003014
Iteration 73/1000 | Loss: 0.00003013
Iteration 74/1000 | Loss: 0.00003013
Iteration 75/1000 | Loss: 0.00003013
Iteration 76/1000 | Loss: 0.00003012
Iteration 77/1000 | Loss: 0.00003012
Iteration 78/1000 | Loss: 0.00003012
Iteration 79/1000 | Loss: 0.00003011
Iteration 80/1000 | Loss: 0.00003011
Iteration 81/1000 | Loss: 0.00003011
Iteration 82/1000 | Loss: 0.00003011
Iteration 83/1000 | Loss: 0.00003010
Iteration 84/1000 | Loss: 0.00003010
Iteration 85/1000 | Loss: 0.00003009
Iteration 86/1000 | Loss: 0.00003009
Iteration 87/1000 | Loss: 0.00003009
Iteration 88/1000 | Loss: 0.00003009
Iteration 89/1000 | Loss: 0.00003009
Iteration 90/1000 | Loss: 0.00003008
Iteration 91/1000 | Loss: 0.00003008
Iteration 92/1000 | Loss: 0.00003008
Iteration 93/1000 | Loss: 0.00003007
Iteration 94/1000 | Loss: 0.00003007
Iteration 95/1000 | Loss: 0.00003007
Iteration 96/1000 | Loss: 0.00003007
Iteration 97/1000 | Loss: 0.00003007
Iteration 98/1000 | Loss: 0.00003007
Iteration 99/1000 | Loss: 0.00003006
Iteration 100/1000 | Loss: 0.00003006
Iteration 101/1000 | Loss: 0.00003006
Iteration 102/1000 | Loss: 0.00003006
Iteration 103/1000 | Loss: 0.00003006
Iteration 104/1000 | Loss: 0.00003006
Iteration 105/1000 | Loss: 0.00003006
Iteration 106/1000 | Loss: 0.00003006
Iteration 107/1000 | Loss: 0.00003006
Iteration 108/1000 | Loss: 0.00003005
Iteration 109/1000 | Loss: 0.00003005
Iteration 110/1000 | Loss: 0.00003005
Iteration 111/1000 | Loss: 0.00003005
Iteration 112/1000 | Loss: 0.00003005
Iteration 113/1000 | Loss: 0.00003005
Iteration 114/1000 | Loss: 0.00003004
Iteration 115/1000 | Loss: 0.00003004
Iteration 116/1000 | Loss: 0.00003004
Iteration 117/1000 | Loss: 0.00003004
Iteration 118/1000 | Loss: 0.00003004
Iteration 119/1000 | Loss: 0.00003003
Iteration 120/1000 | Loss: 0.00003003
Iteration 121/1000 | Loss: 0.00003003
Iteration 122/1000 | Loss: 0.00003003
Iteration 123/1000 | Loss: 0.00003003
Iteration 124/1000 | Loss: 0.00003002
Iteration 125/1000 | Loss: 0.00003002
Iteration 126/1000 | Loss: 0.00003002
Iteration 127/1000 | Loss: 0.00003002
Iteration 128/1000 | Loss: 0.00003001
Iteration 129/1000 | Loss: 0.00003001
Iteration 130/1000 | Loss: 0.00003001
Iteration 131/1000 | Loss: 0.00003001
Iteration 132/1000 | Loss: 0.00003001
Iteration 133/1000 | Loss: 0.00003000
Iteration 134/1000 | Loss: 0.00003000
Iteration 135/1000 | Loss: 0.00003000
Iteration 136/1000 | Loss: 0.00003000
Iteration 137/1000 | Loss: 0.00003000
Iteration 138/1000 | Loss: 0.00002999
Iteration 139/1000 | Loss: 0.00002999
Iteration 140/1000 | Loss: 0.00002999
Iteration 141/1000 | Loss: 0.00002999
Iteration 142/1000 | Loss: 0.00002998
Iteration 143/1000 | Loss: 0.00002998
Iteration 144/1000 | Loss: 0.00002998
Iteration 145/1000 | Loss: 0.00002998
Iteration 146/1000 | Loss: 0.00002998
Iteration 147/1000 | Loss: 0.00002998
Iteration 148/1000 | Loss: 0.00002998
Iteration 149/1000 | Loss: 0.00002997
Iteration 150/1000 | Loss: 0.00002997
Iteration 151/1000 | Loss: 0.00002997
Iteration 152/1000 | Loss: 0.00002997
Iteration 153/1000 | Loss: 0.00002997
Iteration 154/1000 | Loss: 0.00002997
Iteration 155/1000 | Loss: 0.00002997
Iteration 156/1000 | Loss: 0.00002997
Iteration 157/1000 | Loss: 0.00002997
Iteration 158/1000 | Loss: 0.00002997
Iteration 159/1000 | Loss: 0.00002997
Iteration 160/1000 | Loss: 0.00002997
Iteration 161/1000 | Loss: 0.00002997
Iteration 162/1000 | Loss: 0.00002996
Iteration 163/1000 | Loss: 0.00002996
Iteration 164/1000 | Loss: 0.00002996
Iteration 165/1000 | Loss: 0.00002996
Iteration 166/1000 | Loss: 0.00002996
Iteration 167/1000 | Loss: 0.00002996
Iteration 168/1000 | Loss: 0.00002996
Iteration 169/1000 | Loss: 0.00002996
Iteration 170/1000 | Loss: 0.00002996
Iteration 171/1000 | Loss: 0.00002996
Iteration 172/1000 | Loss: 0.00002996
Iteration 173/1000 | Loss: 0.00002995
Iteration 174/1000 | Loss: 0.00002995
Iteration 175/1000 | Loss: 0.00002995
Iteration 176/1000 | Loss: 0.00002995
Iteration 177/1000 | Loss: 0.00002995
Iteration 178/1000 | Loss: 0.00002995
Iteration 179/1000 | Loss: 0.00002995
Iteration 180/1000 | Loss: 0.00002995
Iteration 181/1000 | Loss: 0.00002995
Iteration 182/1000 | Loss: 0.00002995
Iteration 183/1000 | Loss: 0.00002995
Iteration 184/1000 | Loss: 0.00002995
Iteration 185/1000 | Loss: 0.00002995
Iteration 186/1000 | Loss: 0.00002994
Iteration 187/1000 | Loss: 0.00002994
Iteration 188/1000 | Loss: 0.00002994
Iteration 189/1000 | Loss: 0.00002994
Iteration 190/1000 | Loss: 0.00002994
Iteration 191/1000 | Loss: 0.00002994
Iteration 192/1000 | Loss: 0.00002994
Iteration 193/1000 | Loss: 0.00002994
Iteration 194/1000 | Loss: 0.00002994
Iteration 195/1000 | Loss: 0.00002994
Iteration 196/1000 | Loss: 0.00002994
Iteration 197/1000 | Loss: 0.00002994
Iteration 198/1000 | Loss: 0.00002994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.9940056265331805e-05, 2.9940056265331805e-05, 2.9940056265331805e-05, 2.9940056265331805e-05, 2.9940056265331805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9940056265331805e-05

Optimization complete. Final v2v error: 4.402194499969482 mm

Highest mean error: 6.102360248565674 mm for frame 153

Lowest mean error: 2.945297956466675 mm for frame 233

Saving results

Total time: 53.16149306297302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00579767
Iteration 2/25 | Loss: 0.00165089
Iteration 3/25 | Loss: 0.00134780
Iteration 4/25 | Loss: 0.00131527
Iteration 5/25 | Loss: 0.00131167
Iteration 6/25 | Loss: 0.00131107
Iteration 7/25 | Loss: 0.00131107
Iteration 8/25 | Loss: 0.00131107
Iteration 9/25 | Loss: 0.00131107
Iteration 10/25 | Loss: 0.00131107
Iteration 11/25 | Loss: 0.00131107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013110664440318942, 0.0013110664440318942, 0.0013110664440318942, 0.0013110664440318942, 0.0013110664440318942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013110664440318942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30842292
Iteration 2/25 | Loss: 0.00086721
Iteration 3/25 | Loss: 0.00086717
Iteration 4/25 | Loss: 0.00086717
Iteration 5/25 | Loss: 0.00086717
Iteration 6/25 | Loss: 0.00086717
Iteration 7/25 | Loss: 0.00086717
Iteration 8/25 | Loss: 0.00086716
Iteration 9/25 | Loss: 0.00086716
Iteration 10/25 | Loss: 0.00086716
Iteration 11/25 | Loss: 0.00086716
Iteration 12/25 | Loss: 0.00086716
Iteration 13/25 | Loss: 0.00086716
Iteration 14/25 | Loss: 0.00086716
Iteration 15/25 | Loss: 0.00086716
Iteration 16/25 | Loss: 0.00086716
Iteration 17/25 | Loss: 0.00086716
Iteration 18/25 | Loss: 0.00086716
Iteration 19/25 | Loss: 0.00086716
Iteration 20/25 | Loss: 0.00086716
Iteration 21/25 | Loss: 0.00086716
Iteration 22/25 | Loss: 0.00086716
Iteration 23/25 | Loss: 0.00086716
Iteration 24/25 | Loss: 0.00086716
Iteration 25/25 | Loss: 0.00086716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086716
Iteration 2/1000 | Loss: 0.00003957
Iteration 3/1000 | Loss: 0.00002916
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002142
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001950
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001829
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001788
Iteration 16/1000 | Loss: 0.00001779
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001746
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001744
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001743
Iteration 33/1000 | Loss: 0.00001743
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001741
Iteration 38/1000 | Loss: 0.00001740
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001739
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001738
Iteration 44/1000 | Loss: 0.00001737
Iteration 45/1000 | Loss: 0.00001737
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001735
Iteration 48/1000 | Loss: 0.00001735
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001733
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001729
Iteration 101/1000 | Loss: 0.00001729
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001728
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001728
Iteration 113/1000 | Loss: 0.00001728
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001727
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001727
Iteration 126/1000 | Loss: 0.00001727
Iteration 127/1000 | Loss: 0.00001727
Iteration 128/1000 | Loss: 0.00001727
Iteration 129/1000 | Loss: 0.00001727
Iteration 130/1000 | Loss: 0.00001727
Iteration 131/1000 | Loss: 0.00001726
Iteration 132/1000 | Loss: 0.00001726
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00001726
Iteration 135/1000 | Loss: 0.00001726
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001726
Iteration 140/1000 | Loss: 0.00001726
Iteration 141/1000 | Loss: 0.00001726
Iteration 142/1000 | Loss: 0.00001725
Iteration 143/1000 | Loss: 0.00001725
Iteration 144/1000 | Loss: 0.00001725
Iteration 145/1000 | Loss: 0.00001725
Iteration 146/1000 | Loss: 0.00001725
Iteration 147/1000 | Loss: 0.00001725
Iteration 148/1000 | Loss: 0.00001725
Iteration 149/1000 | Loss: 0.00001725
Iteration 150/1000 | Loss: 0.00001725
Iteration 151/1000 | Loss: 0.00001725
Iteration 152/1000 | Loss: 0.00001724
Iteration 153/1000 | Loss: 0.00001724
Iteration 154/1000 | Loss: 0.00001724
Iteration 155/1000 | Loss: 0.00001724
Iteration 156/1000 | Loss: 0.00001724
Iteration 157/1000 | Loss: 0.00001724
Iteration 158/1000 | Loss: 0.00001724
Iteration 159/1000 | Loss: 0.00001724
Iteration 160/1000 | Loss: 0.00001724
Iteration 161/1000 | Loss: 0.00001724
Iteration 162/1000 | Loss: 0.00001724
Iteration 163/1000 | Loss: 0.00001724
Iteration 164/1000 | Loss: 0.00001723
Iteration 165/1000 | Loss: 0.00001723
Iteration 166/1000 | Loss: 0.00001723
Iteration 167/1000 | Loss: 0.00001723
Iteration 168/1000 | Loss: 0.00001723
Iteration 169/1000 | Loss: 0.00001723
Iteration 170/1000 | Loss: 0.00001723
Iteration 171/1000 | Loss: 0.00001723
Iteration 172/1000 | Loss: 0.00001723
Iteration 173/1000 | Loss: 0.00001723
Iteration 174/1000 | Loss: 0.00001723
Iteration 175/1000 | Loss: 0.00001723
Iteration 176/1000 | Loss: 0.00001723
Iteration 177/1000 | Loss: 0.00001723
Iteration 178/1000 | Loss: 0.00001722
Iteration 179/1000 | Loss: 0.00001722
Iteration 180/1000 | Loss: 0.00001722
Iteration 181/1000 | Loss: 0.00001722
Iteration 182/1000 | Loss: 0.00001722
Iteration 183/1000 | Loss: 0.00001722
Iteration 184/1000 | Loss: 0.00001722
Iteration 185/1000 | Loss: 0.00001722
Iteration 186/1000 | Loss: 0.00001722
Iteration 187/1000 | Loss: 0.00001722
Iteration 188/1000 | Loss: 0.00001721
Iteration 189/1000 | Loss: 0.00001721
Iteration 190/1000 | Loss: 0.00001721
Iteration 191/1000 | Loss: 0.00001721
Iteration 192/1000 | Loss: 0.00001721
Iteration 193/1000 | Loss: 0.00001721
Iteration 194/1000 | Loss: 0.00001721
Iteration 195/1000 | Loss: 0.00001721
Iteration 196/1000 | Loss: 0.00001721
Iteration 197/1000 | Loss: 0.00001721
Iteration 198/1000 | Loss: 0.00001721
Iteration 199/1000 | Loss: 0.00001721
Iteration 200/1000 | Loss: 0.00001721
Iteration 201/1000 | Loss: 0.00001721
Iteration 202/1000 | Loss: 0.00001721
Iteration 203/1000 | Loss: 0.00001721
Iteration 204/1000 | Loss: 0.00001721
Iteration 205/1000 | Loss: 0.00001721
Iteration 206/1000 | Loss: 0.00001721
Iteration 207/1000 | Loss: 0.00001721
Iteration 208/1000 | Loss: 0.00001721
Iteration 209/1000 | Loss: 0.00001721
Iteration 210/1000 | Loss: 0.00001721
Iteration 211/1000 | Loss: 0.00001721
Iteration 212/1000 | Loss: 0.00001721
Iteration 213/1000 | Loss: 0.00001721
Iteration 214/1000 | Loss: 0.00001721
Iteration 215/1000 | Loss: 0.00001721
Iteration 216/1000 | Loss: 0.00001721
Iteration 217/1000 | Loss: 0.00001721
Iteration 218/1000 | Loss: 0.00001721
Iteration 219/1000 | Loss: 0.00001721
Iteration 220/1000 | Loss: 0.00001721
Iteration 221/1000 | Loss: 0.00001721
Iteration 222/1000 | Loss: 0.00001721
Iteration 223/1000 | Loss: 0.00001721
Iteration 224/1000 | Loss: 0.00001721
Iteration 225/1000 | Loss: 0.00001721
Iteration 226/1000 | Loss: 0.00001721
Iteration 227/1000 | Loss: 0.00001721
Iteration 228/1000 | Loss: 0.00001721
Iteration 229/1000 | Loss: 0.00001721
Iteration 230/1000 | Loss: 0.00001721
Iteration 231/1000 | Loss: 0.00001721
Iteration 232/1000 | Loss: 0.00001721
Iteration 233/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.7210164514835924e-05, 1.7210164514835924e-05, 1.7210164514835924e-05, 1.7210164514835924e-05, 1.7210164514835924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7210164514835924e-05

Optimization complete. Final v2v error: 3.4961225986480713 mm

Highest mean error: 3.7746379375457764 mm for frame 66

Lowest mean error: 3.1531550884246826 mm for frame 16

Saving results

Total time: 44.15469288825989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964289
Iteration 2/25 | Loss: 0.00193167
Iteration 3/25 | Loss: 0.00162713
Iteration 4/25 | Loss: 0.00155825
Iteration 5/25 | Loss: 0.00154564
Iteration 6/25 | Loss: 0.00154258
Iteration 7/25 | Loss: 0.00154258
Iteration 8/25 | Loss: 0.00154258
Iteration 9/25 | Loss: 0.00154258
Iteration 10/25 | Loss: 0.00154258
Iteration 11/25 | Loss: 0.00154258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015425813617184758, 0.0015425813617184758, 0.0015425813617184758, 0.0015425813617184758, 0.0015425813617184758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015425813617184758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32419729
Iteration 2/25 | Loss: 0.00085553
Iteration 3/25 | Loss: 0.00085553
Iteration 4/25 | Loss: 0.00085553
Iteration 5/25 | Loss: 0.00085553
Iteration 6/25 | Loss: 0.00085553
Iteration 7/25 | Loss: 0.00085553
Iteration 8/25 | Loss: 0.00085553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0008555310778319836, 0.0008555310778319836, 0.0008555310778319836, 0.0008555310778319836, 0.0008555310778319836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008555310778319836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085553
Iteration 2/1000 | Loss: 0.00010266
Iteration 3/1000 | Loss: 0.00091420
Iteration 4/1000 | Loss: 0.00037884
Iteration 5/1000 | Loss: 0.00010310
Iteration 6/1000 | Loss: 0.00009633
Iteration 7/1000 | Loss: 0.00008978
Iteration 8/1000 | Loss: 0.00008571
Iteration 9/1000 | Loss: 0.00036001
Iteration 10/1000 | Loss: 0.00036352
Iteration 11/1000 | Loss: 0.00024503
Iteration 12/1000 | Loss: 0.00034775
Iteration 13/1000 | Loss: 0.00018655
Iteration 14/1000 | Loss: 0.00035439
Iteration 15/1000 | Loss: 0.00019141
Iteration 16/1000 | Loss: 0.00011042
Iteration 17/1000 | Loss: 0.00009662
Iteration 18/1000 | Loss: 0.00008221
Iteration 19/1000 | Loss: 0.00033828
Iteration 20/1000 | Loss: 0.00008381
Iteration 21/1000 | Loss: 0.00037103
Iteration 22/1000 | Loss: 0.00012749
Iteration 23/1000 | Loss: 0.00008534
Iteration 24/1000 | Loss: 0.00008159
Iteration 25/1000 | Loss: 0.00007986
Iteration 26/1000 | Loss: 0.00007838
Iteration 27/1000 | Loss: 0.00007670
Iteration 28/1000 | Loss: 0.00008225
Iteration 29/1000 | Loss: 0.00007639
Iteration 30/1000 | Loss: 0.00007504
Iteration 31/1000 | Loss: 0.00007460
Iteration 32/1000 | Loss: 0.00007409
Iteration 33/1000 | Loss: 0.00035077
Iteration 34/1000 | Loss: 0.00009651
Iteration 35/1000 | Loss: 0.00008585
Iteration 36/1000 | Loss: 0.00008602
Iteration 37/1000 | Loss: 0.00007745
Iteration 38/1000 | Loss: 0.00007623
Iteration 39/1000 | Loss: 0.00007521
Iteration 40/1000 | Loss: 0.00007457
Iteration 41/1000 | Loss: 0.00007408
Iteration 42/1000 | Loss: 0.00034849
Iteration 43/1000 | Loss: 0.00029593
Iteration 44/1000 | Loss: 0.00036289
Iteration 45/1000 | Loss: 0.00027858
Iteration 46/1000 | Loss: 0.00035993
Iteration 47/1000 | Loss: 0.00022739
Iteration 48/1000 | Loss: 0.00021201
Iteration 49/1000 | Loss: 0.00016374
Iteration 50/1000 | Loss: 0.00022234
Iteration 51/1000 | Loss: 0.00021384
Iteration 52/1000 | Loss: 0.00009611
Iteration 53/1000 | Loss: 0.00007829
Iteration 54/1000 | Loss: 0.00025919
Iteration 55/1000 | Loss: 0.00007439
Iteration 56/1000 | Loss: 0.00018042
Iteration 57/1000 | Loss: 0.00007388
Iteration 58/1000 | Loss: 0.00007261
Iteration 59/1000 | Loss: 0.00007225
Iteration 60/1000 | Loss: 0.00007193
Iteration 61/1000 | Loss: 0.00007182
Iteration 62/1000 | Loss: 0.00007181
Iteration 63/1000 | Loss: 0.00007180
Iteration 64/1000 | Loss: 0.00007179
Iteration 65/1000 | Loss: 0.00007179
Iteration 66/1000 | Loss: 0.00007179
Iteration 67/1000 | Loss: 0.00007178
Iteration 68/1000 | Loss: 0.00007177
Iteration 69/1000 | Loss: 0.00007177
Iteration 70/1000 | Loss: 0.00007177
Iteration 71/1000 | Loss: 0.00007177
Iteration 72/1000 | Loss: 0.00007177
Iteration 73/1000 | Loss: 0.00007177
Iteration 74/1000 | Loss: 0.00007177
Iteration 75/1000 | Loss: 0.00007176
Iteration 76/1000 | Loss: 0.00007176
Iteration 77/1000 | Loss: 0.00007176
Iteration 78/1000 | Loss: 0.00007176
Iteration 79/1000 | Loss: 0.00007176
Iteration 80/1000 | Loss: 0.00007176
Iteration 81/1000 | Loss: 0.00007175
Iteration 82/1000 | Loss: 0.00007175
Iteration 83/1000 | Loss: 0.00007174
Iteration 84/1000 | Loss: 0.00007174
Iteration 85/1000 | Loss: 0.00007174
Iteration 86/1000 | Loss: 0.00007174
Iteration 87/1000 | Loss: 0.00007174
Iteration 88/1000 | Loss: 0.00007173
Iteration 89/1000 | Loss: 0.00007173
Iteration 90/1000 | Loss: 0.00007173
Iteration 91/1000 | Loss: 0.00007173
Iteration 92/1000 | Loss: 0.00007173
Iteration 93/1000 | Loss: 0.00007173
Iteration 94/1000 | Loss: 0.00007173
Iteration 95/1000 | Loss: 0.00007173
Iteration 96/1000 | Loss: 0.00007173
Iteration 97/1000 | Loss: 0.00007173
Iteration 98/1000 | Loss: 0.00007173
Iteration 99/1000 | Loss: 0.00007173
Iteration 100/1000 | Loss: 0.00007172
Iteration 101/1000 | Loss: 0.00007172
Iteration 102/1000 | Loss: 0.00007172
Iteration 103/1000 | Loss: 0.00007171
Iteration 104/1000 | Loss: 0.00007171
Iteration 105/1000 | Loss: 0.00007171
Iteration 106/1000 | Loss: 0.00007171
Iteration 107/1000 | Loss: 0.00007171
Iteration 108/1000 | Loss: 0.00007170
Iteration 109/1000 | Loss: 0.00007170
Iteration 110/1000 | Loss: 0.00007170
Iteration 111/1000 | Loss: 0.00007170
Iteration 112/1000 | Loss: 0.00007170
Iteration 113/1000 | Loss: 0.00007170
Iteration 114/1000 | Loss: 0.00007170
Iteration 115/1000 | Loss: 0.00007170
Iteration 116/1000 | Loss: 0.00007170
Iteration 117/1000 | Loss: 0.00007169
Iteration 118/1000 | Loss: 0.00007169
Iteration 119/1000 | Loss: 0.00007169
Iteration 120/1000 | Loss: 0.00007169
Iteration 121/1000 | Loss: 0.00007169
Iteration 122/1000 | Loss: 0.00007169
Iteration 123/1000 | Loss: 0.00007169
Iteration 124/1000 | Loss: 0.00007169
Iteration 125/1000 | Loss: 0.00007169
Iteration 126/1000 | Loss: 0.00007169
Iteration 127/1000 | Loss: 0.00007169
Iteration 128/1000 | Loss: 0.00007169
Iteration 129/1000 | Loss: 0.00007169
Iteration 130/1000 | Loss: 0.00007168
Iteration 131/1000 | Loss: 0.00007168
Iteration 132/1000 | Loss: 0.00007168
Iteration 133/1000 | Loss: 0.00007168
Iteration 134/1000 | Loss: 0.00007168
Iteration 135/1000 | Loss: 0.00007168
Iteration 136/1000 | Loss: 0.00007168
Iteration 137/1000 | Loss: 0.00007168
Iteration 138/1000 | Loss: 0.00007168
Iteration 139/1000 | Loss: 0.00007168
Iteration 140/1000 | Loss: 0.00007167
Iteration 141/1000 | Loss: 0.00007167
Iteration 142/1000 | Loss: 0.00007167
Iteration 143/1000 | Loss: 0.00007167
Iteration 144/1000 | Loss: 0.00007167
Iteration 145/1000 | Loss: 0.00007167
Iteration 146/1000 | Loss: 0.00007167
Iteration 147/1000 | Loss: 0.00007167
Iteration 148/1000 | Loss: 0.00007167
Iteration 149/1000 | Loss: 0.00007167
Iteration 150/1000 | Loss: 0.00007167
Iteration 151/1000 | Loss: 0.00007167
Iteration 152/1000 | Loss: 0.00007166
Iteration 153/1000 | Loss: 0.00007166
Iteration 154/1000 | Loss: 0.00007166
Iteration 155/1000 | Loss: 0.00007166
Iteration 156/1000 | Loss: 0.00007166
Iteration 157/1000 | Loss: 0.00007166
Iteration 158/1000 | Loss: 0.00007166
Iteration 159/1000 | Loss: 0.00007166
Iteration 160/1000 | Loss: 0.00007166
Iteration 161/1000 | Loss: 0.00007165
Iteration 162/1000 | Loss: 0.00007165
Iteration 163/1000 | Loss: 0.00007165
Iteration 164/1000 | Loss: 0.00007165
Iteration 165/1000 | Loss: 0.00007165
Iteration 166/1000 | Loss: 0.00007164
Iteration 167/1000 | Loss: 0.00007164
Iteration 168/1000 | Loss: 0.00007164
Iteration 169/1000 | Loss: 0.00007164
Iteration 170/1000 | Loss: 0.00007164
Iteration 171/1000 | Loss: 0.00007164
Iteration 172/1000 | Loss: 0.00007164
Iteration 173/1000 | Loss: 0.00007164
Iteration 174/1000 | Loss: 0.00007164
Iteration 175/1000 | Loss: 0.00007163
Iteration 176/1000 | Loss: 0.00007163
Iteration 177/1000 | Loss: 0.00007163
Iteration 178/1000 | Loss: 0.00007163
Iteration 179/1000 | Loss: 0.00007162
Iteration 180/1000 | Loss: 0.00007162
Iteration 181/1000 | Loss: 0.00007162
Iteration 182/1000 | Loss: 0.00007162
Iteration 183/1000 | Loss: 0.00007162
Iteration 184/1000 | Loss: 0.00007162
Iteration 185/1000 | Loss: 0.00007162
Iteration 186/1000 | Loss: 0.00007161
Iteration 187/1000 | Loss: 0.00007161
Iteration 188/1000 | Loss: 0.00007161
Iteration 189/1000 | Loss: 0.00007161
Iteration 190/1000 | Loss: 0.00007160
Iteration 191/1000 | Loss: 0.00007160
Iteration 192/1000 | Loss: 0.00007160
Iteration 193/1000 | Loss: 0.00007160
Iteration 194/1000 | Loss: 0.00007160
Iteration 195/1000 | Loss: 0.00007160
Iteration 196/1000 | Loss: 0.00007160
Iteration 197/1000 | Loss: 0.00007160
Iteration 198/1000 | Loss: 0.00007160
Iteration 199/1000 | Loss: 0.00007160
Iteration 200/1000 | Loss: 0.00007160
Iteration 201/1000 | Loss: 0.00007160
Iteration 202/1000 | Loss: 0.00007160
Iteration 203/1000 | Loss: 0.00007160
Iteration 204/1000 | Loss: 0.00007160
Iteration 205/1000 | Loss: 0.00007159
Iteration 206/1000 | Loss: 0.00007159
Iteration 207/1000 | Loss: 0.00007159
Iteration 208/1000 | Loss: 0.00007159
Iteration 209/1000 | Loss: 0.00007159
Iteration 210/1000 | Loss: 0.00007159
Iteration 211/1000 | Loss: 0.00007158
Iteration 212/1000 | Loss: 0.00007158
Iteration 213/1000 | Loss: 0.00007158
Iteration 214/1000 | Loss: 0.00007158
Iteration 215/1000 | Loss: 0.00007158
Iteration 216/1000 | Loss: 0.00007158
Iteration 217/1000 | Loss: 0.00007158
Iteration 218/1000 | Loss: 0.00007158
Iteration 219/1000 | Loss: 0.00007158
Iteration 220/1000 | Loss: 0.00007158
Iteration 221/1000 | Loss: 0.00007158
Iteration 222/1000 | Loss: 0.00007158
Iteration 223/1000 | Loss: 0.00007158
Iteration 224/1000 | Loss: 0.00007158
Iteration 225/1000 | Loss: 0.00007158
Iteration 226/1000 | Loss: 0.00007157
Iteration 227/1000 | Loss: 0.00007157
Iteration 228/1000 | Loss: 0.00007157
Iteration 229/1000 | Loss: 0.00007157
Iteration 230/1000 | Loss: 0.00007157
Iteration 231/1000 | Loss: 0.00007157
Iteration 232/1000 | Loss: 0.00007157
Iteration 233/1000 | Loss: 0.00007157
Iteration 234/1000 | Loss: 0.00007156
Iteration 235/1000 | Loss: 0.00007156
Iteration 236/1000 | Loss: 0.00007156
Iteration 237/1000 | Loss: 0.00007156
Iteration 238/1000 | Loss: 0.00007156
Iteration 239/1000 | Loss: 0.00007156
Iteration 240/1000 | Loss: 0.00007156
Iteration 241/1000 | Loss: 0.00007156
Iteration 242/1000 | Loss: 0.00007156
Iteration 243/1000 | Loss: 0.00007155
Iteration 244/1000 | Loss: 0.00007155
Iteration 245/1000 | Loss: 0.00007155
Iteration 246/1000 | Loss: 0.00007155
Iteration 247/1000 | Loss: 0.00007155
Iteration 248/1000 | Loss: 0.00007155
Iteration 249/1000 | Loss: 0.00007155
Iteration 250/1000 | Loss: 0.00007155
Iteration 251/1000 | Loss: 0.00007155
Iteration 252/1000 | Loss: 0.00007155
Iteration 253/1000 | Loss: 0.00007155
Iteration 254/1000 | Loss: 0.00007155
Iteration 255/1000 | Loss: 0.00007155
Iteration 256/1000 | Loss: 0.00007155
Iteration 257/1000 | Loss: 0.00007155
Iteration 258/1000 | Loss: 0.00007155
Iteration 259/1000 | Loss: 0.00007155
Iteration 260/1000 | Loss: 0.00007155
Iteration 261/1000 | Loss: 0.00007155
Iteration 262/1000 | Loss: 0.00007155
Iteration 263/1000 | Loss: 0.00007155
Iteration 264/1000 | Loss: 0.00007155
Iteration 265/1000 | Loss: 0.00007155
Iteration 266/1000 | Loss: 0.00007155
Iteration 267/1000 | Loss: 0.00007155
Iteration 268/1000 | Loss: 0.00007155
Iteration 269/1000 | Loss: 0.00007155
Iteration 270/1000 | Loss: 0.00007155
Iteration 271/1000 | Loss: 0.00007155
Iteration 272/1000 | Loss: 0.00007155
Iteration 273/1000 | Loss: 0.00007155
Iteration 274/1000 | Loss: 0.00007155
Iteration 275/1000 | Loss: 0.00007155
Iteration 276/1000 | Loss: 0.00007155
Iteration 277/1000 | Loss: 0.00007155
Iteration 278/1000 | Loss: 0.00007155
Iteration 279/1000 | Loss: 0.00007155
Iteration 280/1000 | Loss: 0.00007155
Iteration 281/1000 | Loss: 0.00007155
Iteration 282/1000 | Loss: 0.00007155
Iteration 283/1000 | Loss: 0.00007155
Iteration 284/1000 | Loss: 0.00007155
Iteration 285/1000 | Loss: 0.00007155
Iteration 286/1000 | Loss: 0.00007155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [7.154793274821714e-05, 7.154793274821714e-05, 7.154793274821714e-05, 7.154793274821714e-05, 7.154793274821714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.154793274821714e-05

Optimization complete. Final v2v error: 5.421052932739258 mm

Highest mean error: 5.5618062019348145 mm for frame 78

Lowest mean error: 3.3891403675079346 mm for frame 5

Saving results

Total time: 107.11543869972229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003107
Iteration 2/25 | Loss: 0.00186286
Iteration 3/25 | Loss: 0.00167310
Iteration 4/25 | Loss: 0.00154732
Iteration 5/25 | Loss: 0.00155128
Iteration 6/25 | Loss: 0.00140122
Iteration 7/25 | Loss: 0.00137480
Iteration 8/25 | Loss: 0.00134515
Iteration 9/25 | Loss: 0.00133270
Iteration 10/25 | Loss: 0.00131740
Iteration 11/25 | Loss: 0.00131327
Iteration 12/25 | Loss: 0.00130743
Iteration 13/25 | Loss: 0.00130256
Iteration 14/25 | Loss: 0.00130534
Iteration 15/25 | Loss: 0.00130149
Iteration 16/25 | Loss: 0.00129545
Iteration 17/25 | Loss: 0.00129296
Iteration 18/25 | Loss: 0.00129203
Iteration 19/25 | Loss: 0.00129199
Iteration 20/25 | Loss: 0.00128974
Iteration 21/25 | Loss: 0.00129089
Iteration 22/25 | Loss: 0.00128811
Iteration 23/25 | Loss: 0.00128703
Iteration 24/25 | Loss: 0.00128672
Iteration 25/25 | Loss: 0.00128662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41540778
Iteration 2/25 | Loss: 0.00122638
Iteration 3/25 | Loss: 0.00111950
Iteration 4/25 | Loss: 0.00111949
Iteration 5/25 | Loss: 0.00111949
Iteration 6/25 | Loss: 0.00111949
Iteration 7/25 | Loss: 0.00111949
Iteration 8/25 | Loss: 0.00111949
Iteration 9/25 | Loss: 0.00111949
Iteration 10/25 | Loss: 0.00111949
Iteration 11/25 | Loss: 0.00111949
Iteration 12/25 | Loss: 0.00111949
Iteration 13/25 | Loss: 0.00111949
Iteration 14/25 | Loss: 0.00111949
Iteration 15/25 | Loss: 0.00111949
Iteration 16/25 | Loss: 0.00111949
Iteration 17/25 | Loss: 0.00111949
Iteration 18/25 | Loss: 0.00111949
Iteration 19/25 | Loss: 0.00111949
Iteration 20/25 | Loss: 0.00111949
Iteration 21/25 | Loss: 0.00111949
Iteration 22/25 | Loss: 0.00111949
Iteration 23/25 | Loss: 0.00111949
Iteration 24/25 | Loss: 0.00111949
Iteration 25/25 | Loss: 0.00111949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111949
Iteration 2/1000 | Loss: 0.00066219
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001612
Iteration 6/1000 | Loss: 0.00001545
Iteration 7/1000 | Loss: 0.00015897
Iteration 8/1000 | Loss: 0.00042818
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001381
Iteration 23/1000 | Loss: 0.00001379
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001372
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001363
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001360
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001348
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001345
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001343
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001340
Iteration 115/1000 | Loss: 0.00001340
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001340
Iteration 119/1000 | Loss: 0.00001340
Iteration 120/1000 | Loss: 0.00001340
Iteration 121/1000 | Loss: 0.00001340
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001340
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001339
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001338
Iteration 136/1000 | Loss: 0.00001338
Iteration 137/1000 | Loss: 0.00001338
Iteration 138/1000 | Loss: 0.00001338
Iteration 139/1000 | Loss: 0.00001338
Iteration 140/1000 | Loss: 0.00001338
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001337
Iteration 144/1000 | Loss: 0.00001337
Iteration 145/1000 | Loss: 0.00001337
Iteration 146/1000 | Loss: 0.00001337
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001335
Iteration 152/1000 | Loss: 0.00001335
Iteration 153/1000 | Loss: 0.00001335
Iteration 154/1000 | Loss: 0.00001335
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001334
Iteration 157/1000 | Loss: 0.00001334
Iteration 158/1000 | Loss: 0.00001334
Iteration 159/1000 | Loss: 0.00001333
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001333
Iteration 162/1000 | Loss: 0.00001333
Iteration 163/1000 | Loss: 0.00001333
Iteration 164/1000 | Loss: 0.00001333
Iteration 165/1000 | Loss: 0.00001333
Iteration 166/1000 | Loss: 0.00001333
Iteration 167/1000 | Loss: 0.00001333
Iteration 168/1000 | Loss: 0.00001333
Iteration 169/1000 | Loss: 0.00001333
Iteration 170/1000 | Loss: 0.00001333
Iteration 171/1000 | Loss: 0.00001333
Iteration 172/1000 | Loss: 0.00001333
Iteration 173/1000 | Loss: 0.00001333
Iteration 174/1000 | Loss: 0.00001333
Iteration 175/1000 | Loss: 0.00001333
Iteration 176/1000 | Loss: 0.00001333
Iteration 177/1000 | Loss: 0.00001333
Iteration 178/1000 | Loss: 0.00001333
Iteration 179/1000 | Loss: 0.00001333
Iteration 180/1000 | Loss: 0.00001333
Iteration 181/1000 | Loss: 0.00001333
Iteration 182/1000 | Loss: 0.00001333
Iteration 183/1000 | Loss: 0.00001333
Iteration 184/1000 | Loss: 0.00001333
Iteration 185/1000 | Loss: 0.00001333
Iteration 186/1000 | Loss: 0.00001333
Iteration 187/1000 | Loss: 0.00001333
Iteration 188/1000 | Loss: 0.00001333
Iteration 189/1000 | Loss: 0.00001333
Iteration 190/1000 | Loss: 0.00001333
Iteration 191/1000 | Loss: 0.00001333
Iteration 192/1000 | Loss: 0.00001333
Iteration 193/1000 | Loss: 0.00001333
Iteration 194/1000 | Loss: 0.00001333
Iteration 195/1000 | Loss: 0.00001333
Iteration 196/1000 | Loss: 0.00001333
Iteration 197/1000 | Loss: 0.00001333
Iteration 198/1000 | Loss: 0.00001333
Iteration 199/1000 | Loss: 0.00001333
Iteration 200/1000 | Loss: 0.00001333
Iteration 201/1000 | Loss: 0.00001333
Iteration 202/1000 | Loss: 0.00001333
Iteration 203/1000 | Loss: 0.00001333
Iteration 204/1000 | Loss: 0.00001333
Iteration 205/1000 | Loss: 0.00001333
Iteration 206/1000 | Loss: 0.00001333
Iteration 207/1000 | Loss: 0.00001333
Iteration 208/1000 | Loss: 0.00001333
Iteration 209/1000 | Loss: 0.00001333
Iteration 210/1000 | Loss: 0.00001333
Iteration 211/1000 | Loss: 0.00001333
Iteration 212/1000 | Loss: 0.00001333
Iteration 213/1000 | Loss: 0.00001333
Iteration 214/1000 | Loss: 0.00001333
Iteration 215/1000 | Loss: 0.00001333
Iteration 216/1000 | Loss: 0.00001333
Iteration 217/1000 | Loss: 0.00001333
Iteration 218/1000 | Loss: 0.00001333
Iteration 219/1000 | Loss: 0.00001333
Iteration 220/1000 | Loss: 0.00001333
Iteration 221/1000 | Loss: 0.00001333
Iteration 222/1000 | Loss: 0.00001333
Iteration 223/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.3330359251995105e-05, 1.3330359251995105e-05, 1.3330359251995105e-05, 1.3330359251995105e-05, 1.3330359251995105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3330359251995105e-05

Optimization complete. Final v2v error: 3.0974671840667725 mm

Highest mean error: 3.476733684539795 mm for frame 49

Lowest mean error: 2.8257343769073486 mm for frame 25

Saving results

Total time: 79.1554548740387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784559
Iteration 2/25 | Loss: 0.00166830
Iteration 3/25 | Loss: 0.00140205
Iteration 4/25 | Loss: 0.00137109
Iteration 5/25 | Loss: 0.00136330
Iteration 6/25 | Loss: 0.00136053
Iteration 7/25 | Loss: 0.00136021
Iteration 8/25 | Loss: 0.00136021
Iteration 9/25 | Loss: 0.00136021
Iteration 10/25 | Loss: 0.00136021
Iteration 11/25 | Loss: 0.00136021
Iteration 12/25 | Loss: 0.00136021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00136020767968148, 0.00136020767968148, 0.00136020767968148, 0.00136020767968148, 0.00136020767968148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00136020767968148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13401496
Iteration 2/25 | Loss: 0.00124713
Iteration 3/25 | Loss: 0.00124712
Iteration 4/25 | Loss: 0.00124712
Iteration 5/25 | Loss: 0.00124712
Iteration 6/25 | Loss: 0.00124712
Iteration 7/25 | Loss: 0.00124712
Iteration 8/25 | Loss: 0.00124712
Iteration 9/25 | Loss: 0.00124712
Iteration 10/25 | Loss: 0.00124712
Iteration 11/25 | Loss: 0.00124712
Iteration 12/25 | Loss: 0.00124712
Iteration 13/25 | Loss: 0.00124712
Iteration 14/25 | Loss: 0.00124712
Iteration 15/25 | Loss: 0.00124712
Iteration 16/25 | Loss: 0.00124712
Iteration 17/25 | Loss: 0.00124712
Iteration 18/25 | Loss: 0.00124712
Iteration 19/25 | Loss: 0.00124712
Iteration 20/25 | Loss: 0.00124712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012471212539821863, 0.0012471212539821863, 0.0012471212539821863, 0.0012471212539821863, 0.0012471212539821863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012471212539821863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124712
Iteration 2/1000 | Loss: 0.00009776
Iteration 3/1000 | Loss: 0.00006088
Iteration 4/1000 | Loss: 0.00004743
Iteration 5/1000 | Loss: 0.00004263
Iteration 6/1000 | Loss: 0.00004096
Iteration 7/1000 | Loss: 0.00003905
Iteration 8/1000 | Loss: 0.00003814
Iteration 9/1000 | Loss: 0.00003728
Iteration 10/1000 | Loss: 0.00003665
Iteration 11/1000 | Loss: 0.00003629
Iteration 12/1000 | Loss: 0.00003583
Iteration 13/1000 | Loss: 0.00003540
Iteration 14/1000 | Loss: 0.00003520
Iteration 15/1000 | Loss: 0.00003506
Iteration 16/1000 | Loss: 0.00003481
Iteration 17/1000 | Loss: 0.00003459
Iteration 18/1000 | Loss: 0.00003444
Iteration 19/1000 | Loss: 0.00003440
Iteration 20/1000 | Loss: 0.00003432
Iteration 21/1000 | Loss: 0.00003420
Iteration 22/1000 | Loss: 0.00003415
Iteration 23/1000 | Loss: 0.00003414
Iteration 24/1000 | Loss: 0.00003413
Iteration 25/1000 | Loss: 0.00003413
Iteration 26/1000 | Loss: 0.00003411
Iteration 27/1000 | Loss: 0.00003411
Iteration 28/1000 | Loss: 0.00003410
Iteration 29/1000 | Loss: 0.00003410
Iteration 30/1000 | Loss: 0.00003409
Iteration 31/1000 | Loss: 0.00003408
Iteration 32/1000 | Loss: 0.00003407
Iteration 33/1000 | Loss: 0.00003407
Iteration 34/1000 | Loss: 0.00003407
Iteration 35/1000 | Loss: 0.00003407
Iteration 36/1000 | Loss: 0.00003407
Iteration 37/1000 | Loss: 0.00003406
Iteration 38/1000 | Loss: 0.00003406
Iteration 39/1000 | Loss: 0.00003406
Iteration 40/1000 | Loss: 0.00003405
Iteration 41/1000 | Loss: 0.00003405
Iteration 42/1000 | Loss: 0.00003405
Iteration 43/1000 | Loss: 0.00003404
Iteration 44/1000 | Loss: 0.00003404
Iteration 45/1000 | Loss: 0.00003403
Iteration 46/1000 | Loss: 0.00003402
Iteration 47/1000 | Loss: 0.00003402
Iteration 48/1000 | Loss: 0.00003402
Iteration 49/1000 | Loss: 0.00003402
Iteration 50/1000 | Loss: 0.00003402
Iteration 51/1000 | Loss: 0.00003402
Iteration 52/1000 | Loss: 0.00003402
Iteration 53/1000 | Loss: 0.00003402
Iteration 54/1000 | Loss: 0.00003402
Iteration 55/1000 | Loss: 0.00003402
Iteration 56/1000 | Loss: 0.00003401
Iteration 57/1000 | Loss: 0.00003401
Iteration 58/1000 | Loss: 0.00003401
Iteration 59/1000 | Loss: 0.00003398
Iteration 60/1000 | Loss: 0.00003398
Iteration 61/1000 | Loss: 0.00003398
Iteration 62/1000 | Loss: 0.00003397
Iteration 63/1000 | Loss: 0.00003397
Iteration 64/1000 | Loss: 0.00003397
Iteration 65/1000 | Loss: 0.00003396
Iteration 66/1000 | Loss: 0.00003395
Iteration 67/1000 | Loss: 0.00003395
Iteration 68/1000 | Loss: 0.00003395
Iteration 69/1000 | Loss: 0.00003394
Iteration 70/1000 | Loss: 0.00003393
Iteration 71/1000 | Loss: 0.00003393
Iteration 72/1000 | Loss: 0.00003392
Iteration 73/1000 | Loss: 0.00003392
Iteration 74/1000 | Loss: 0.00003392
Iteration 75/1000 | Loss: 0.00003392
Iteration 76/1000 | Loss: 0.00003392
Iteration 77/1000 | Loss: 0.00003391
Iteration 78/1000 | Loss: 0.00003391
Iteration 79/1000 | Loss: 0.00003391
Iteration 80/1000 | Loss: 0.00003391
Iteration 81/1000 | Loss: 0.00003391
Iteration 82/1000 | Loss: 0.00003390
Iteration 83/1000 | Loss: 0.00003390
Iteration 84/1000 | Loss: 0.00003389
Iteration 85/1000 | Loss: 0.00003389
Iteration 86/1000 | Loss: 0.00003389
Iteration 87/1000 | Loss: 0.00003389
Iteration 88/1000 | Loss: 0.00003389
Iteration 89/1000 | Loss: 0.00003389
Iteration 90/1000 | Loss: 0.00003389
Iteration 91/1000 | Loss: 0.00003389
Iteration 92/1000 | Loss: 0.00003388
Iteration 93/1000 | Loss: 0.00003388
Iteration 94/1000 | Loss: 0.00003388
Iteration 95/1000 | Loss: 0.00003388
Iteration 96/1000 | Loss: 0.00003387
Iteration 97/1000 | Loss: 0.00003387
Iteration 98/1000 | Loss: 0.00003387
Iteration 99/1000 | Loss: 0.00003387
Iteration 100/1000 | Loss: 0.00003386
Iteration 101/1000 | Loss: 0.00003386
Iteration 102/1000 | Loss: 0.00003386
Iteration 103/1000 | Loss: 0.00003386
Iteration 104/1000 | Loss: 0.00003386
Iteration 105/1000 | Loss: 0.00003386
Iteration 106/1000 | Loss: 0.00003386
Iteration 107/1000 | Loss: 0.00003386
Iteration 108/1000 | Loss: 0.00003386
Iteration 109/1000 | Loss: 0.00003385
Iteration 110/1000 | Loss: 0.00003385
Iteration 111/1000 | Loss: 0.00003385
Iteration 112/1000 | Loss: 0.00003385
Iteration 113/1000 | Loss: 0.00003385
Iteration 114/1000 | Loss: 0.00003385
Iteration 115/1000 | Loss: 0.00003385
Iteration 116/1000 | Loss: 0.00003385
Iteration 117/1000 | Loss: 0.00003385
Iteration 118/1000 | Loss: 0.00003385
Iteration 119/1000 | Loss: 0.00003385
Iteration 120/1000 | Loss: 0.00003385
Iteration 121/1000 | Loss: 0.00003385
Iteration 122/1000 | Loss: 0.00003385
Iteration 123/1000 | Loss: 0.00003385
Iteration 124/1000 | Loss: 0.00003385
Iteration 125/1000 | Loss: 0.00003385
Iteration 126/1000 | Loss: 0.00003385
Iteration 127/1000 | Loss: 0.00003385
Iteration 128/1000 | Loss: 0.00003385
Iteration 129/1000 | Loss: 0.00003385
Iteration 130/1000 | Loss: 0.00003385
Iteration 131/1000 | Loss: 0.00003385
Iteration 132/1000 | Loss: 0.00003385
Iteration 133/1000 | Loss: 0.00003385
Iteration 134/1000 | Loss: 0.00003385
Iteration 135/1000 | Loss: 0.00003385
Iteration 136/1000 | Loss: 0.00003385
Iteration 137/1000 | Loss: 0.00003385
Iteration 138/1000 | Loss: 0.00003385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [3.384927549632266e-05, 3.384927549632266e-05, 3.384927549632266e-05, 3.384927549632266e-05, 3.384927549632266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.384927549632266e-05

Optimization complete. Final v2v error: 4.710808753967285 mm

Highest mean error: 5.582276821136475 mm for frame 81

Lowest mean error: 3.94621205329895 mm for frame 12

Saving results

Total time: 48.15851974487305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473082
Iteration 2/25 | Loss: 0.00153564
Iteration 3/25 | Loss: 0.00133667
Iteration 4/25 | Loss: 0.00131834
Iteration 5/25 | Loss: 0.00131507
Iteration 6/25 | Loss: 0.00131403
Iteration 7/25 | Loss: 0.00131403
Iteration 8/25 | Loss: 0.00131403
Iteration 9/25 | Loss: 0.00131403
Iteration 10/25 | Loss: 0.00131403
Iteration 11/25 | Loss: 0.00131403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013140321243554354, 0.0013140321243554354, 0.0013140321243554354, 0.0013140321243554354, 0.0013140321243554354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013140321243554354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40922427
Iteration 2/25 | Loss: 0.00099091
Iteration 3/25 | Loss: 0.00099089
Iteration 4/25 | Loss: 0.00099089
Iteration 5/25 | Loss: 0.00099089
Iteration 6/25 | Loss: 0.00099089
Iteration 7/25 | Loss: 0.00099089
Iteration 8/25 | Loss: 0.00099088
Iteration 9/25 | Loss: 0.00099088
Iteration 10/25 | Loss: 0.00099088
Iteration 11/25 | Loss: 0.00099088
Iteration 12/25 | Loss: 0.00099088
Iteration 13/25 | Loss: 0.00099088
Iteration 14/25 | Loss: 0.00099088
Iteration 15/25 | Loss: 0.00099088
Iteration 16/25 | Loss: 0.00099088
Iteration 17/25 | Loss: 0.00099088
Iteration 18/25 | Loss: 0.00099088
Iteration 19/25 | Loss: 0.00099088
Iteration 20/25 | Loss: 0.00099088
Iteration 21/25 | Loss: 0.00099088
Iteration 22/25 | Loss: 0.00099088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009908839128911495, 0.0009908839128911495, 0.0009908839128911495, 0.0009908839128911495, 0.0009908839128911495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009908839128911495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099088
Iteration 2/1000 | Loss: 0.00003165
Iteration 3/1000 | Loss: 0.00002216
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001792
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001690
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001641
Iteration 18/1000 | Loss: 0.00001637
Iteration 19/1000 | Loss: 0.00001637
Iteration 20/1000 | Loss: 0.00001635
Iteration 21/1000 | Loss: 0.00001633
Iteration 22/1000 | Loss: 0.00001632
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001628
Iteration 29/1000 | Loss: 0.00001627
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001625
Iteration 33/1000 | Loss: 0.00001625
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001624
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001619
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001606
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001602
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001597
Iteration 73/1000 | Loss: 0.00001597
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001594
Iteration 80/1000 | Loss: 0.00001594
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001593
Iteration 84/1000 | Loss: 0.00001592
Iteration 85/1000 | Loss: 0.00001592
Iteration 86/1000 | Loss: 0.00001592
Iteration 87/1000 | Loss: 0.00001592
Iteration 88/1000 | Loss: 0.00001592
Iteration 89/1000 | Loss: 0.00001591
Iteration 90/1000 | Loss: 0.00001591
Iteration 91/1000 | Loss: 0.00001591
Iteration 92/1000 | Loss: 0.00001591
Iteration 93/1000 | Loss: 0.00001590
Iteration 94/1000 | Loss: 0.00001590
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001590
Iteration 97/1000 | Loss: 0.00001590
Iteration 98/1000 | Loss: 0.00001590
Iteration 99/1000 | Loss: 0.00001590
Iteration 100/1000 | Loss: 0.00001590
Iteration 101/1000 | Loss: 0.00001590
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001590
Iteration 105/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.5899957361398265e-05, 1.5899957361398265e-05, 1.5899957361398265e-05, 1.5899957361398265e-05, 1.5899957361398265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5899957361398265e-05

Optimization complete. Final v2v error: 3.311417818069458 mm

Highest mean error: 3.97094988822937 mm for frame 70

Lowest mean error: 2.743313789367676 mm for frame 146

Saving results

Total time: 38.17207312583923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018199
Iteration 2/25 | Loss: 0.00180612
Iteration 3/25 | Loss: 0.00152867
Iteration 4/25 | Loss: 0.00163485
Iteration 5/25 | Loss: 0.00153052
Iteration 6/25 | Loss: 0.00149283
Iteration 7/25 | Loss: 0.00143637
Iteration 8/25 | Loss: 0.00142259
Iteration 9/25 | Loss: 0.00141652
Iteration 10/25 | Loss: 0.00146652
Iteration 11/25 | Loss: 0.00141746
Iteration 12/25 | Loss: 0.00138508
Iteration 13/25 | Loss: 0.00138274
Iteration 14/25 | Loss: 0.00140432
Iteration 15/25 | Loss: 0.00143854
Iteration 16/25 | Loss: 0.00139607
Iteration 17/25 | Loss: 0.00135083
Iteration 18/25 | Loss: 0.00132043
Iteration 19/25 | Loss: 0.00134077
Iteration 20/25 | Loss: 0.00131150
Iteration 21/25 | Loss: 0.00130984
Iteration 22/25 | Loss: 0.00130977
Iteration 23/25 | Loss: 0.00130977
Iteration 24/25 | Loss: 0.00130977
Iteration 25/25 | Loss: 0.00130977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.59443521
Iteration 2/25 | Loss: 0.00086646
Iteration 3/25 | Loss: 0.00086646
Iteration 4/25 | Loss: 0.00086646
Iteration 5/25 | Loss: 0.00086646
Iteration 6/25 | Loss: 0.00086646
Iteration 7/25 | Loss: 0.00086646
Iteration 8/25 | Loss: 0.00086646
Iteration 9/25 | Loss: 0.00086646
Iteration 10/25 | Loss: 0.00086646
Iteration 11/25 | Loss: 0.00086646
Iteration 12/25 | Loss: 0.00086646
Iteration 13/25 | Loss: 0.00086646
Iteration 14/25 | Loss: 0.00086646
Iteration 15/25 | Loss: 0.00086646
Iteration 16/25 | Loss: 0.00086646
Iteration 17/25 | Loss: 0.00086646
Iteration 18/25 | Loss: 0.00086646
Iteration 19/25 | Loss: 0.00086646
Iteration 20/25 | Loss: 0.00086646
Iteration 21/25 | Loss: 0.00086646
Iteration 22/25 | Loss: 0.00086646
Iteration 23/25 | Loss: 0.00086646
Iteration 24/25 | Loss: 0.00086646
Iteration 25/25 | Loss: 0.00086646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086646
Iteration 2/1000 | Loss: 0.00003380
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001988
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001789
Iteration 7/1000 | Loss: 0.00001739
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001675
Iteration 10/1000 | Loss: 0.00001646
Iteration 11/1000 | Loss: 0.00001645
Iteration 12/1000 | Loss: 0.00036059
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001661
Iteration 15/1000 | Loss: 0.00001585
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001446
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001438
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001426
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001412
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001408
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001402
Iteration 46/1000 | Loss: 0.00001402
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001394
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001391
Iteration 58/1000 | Loss: 0.00001391
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001390
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001389
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001389
Iteration 70/1000 | Loss: 0.00001389
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001388
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001385
Iteration 97/1000 | Loss: 0.00001385
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001384
Iteration 107/1000 | Loss: 0.00001384
Iteration 108/1000 | Loss: 0.00001384
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001383
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001383
Iteration 118/1000 | Loss: 0.00001383
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001379
Iteration 171/1000 | Loss: 0.00001379
Iteration 172/1000 | Loss: 0.00001379
Iteration 173/1000 | Loss: 0.00001379
Iteration 174/1000 | Loss: 0.00001379
Iteration 175/1000 | Loss: 0.00001379
Iteration 176/1000 | Loss: 0.00001379
Iteration 177/1000 | Loss: 0.00001379
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001379
Iteration 180/1000 | Loss: 0.00001379
Iteration 181/1000 | Loss: 0.00001379
Iteration 182/1000 | Loss: 0.00001379
Iteration 183/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.3792731806461234e-05, 1.3792731806461234e-05, 1.3792731806461234e-05, 1.3792731806461234e-05, 1.3792731806461234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3792731806461234e-05

Optimization complete. Final v2v error: 3.1481282711029053 mm

Highest mean error: 4.394410610198975 mm for frame 1

Lowest mean error: 2.868680715560913 mm for frame 100

Saving results

Total time: 76.81396746635437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985937
Iteration 2/25 | Loss: 0.00173878
Iteration 3/25 | Loss: 0.00145976
Iteration 4/25 | Loss: 0.00140427
Iteration 5/25 | Loss: 0.00139276
Iteration 6/25 | Loss: 0.00134125
Iteration 7/25 | Loss: 0.00132143
Iteration 8/25 | Loss: 0.00130876
Iteration 9/25 | Loss: 0.00130114
Iteration 10/25 | Loss: 0.00130330
Iteration 11/25 | Loss: 0.00128917
Iteration 12/25 | Loss: 0.00127973
Iteration 13/25 | Loss: 0.00127818
Iteration 14/25 | Loss: 0.00127779
Iteration 15/25 | Loss: 0.00127768
Iteration 16/25 | Loss: 0.00127765
Iteration 17/25 | Loss: 0.00127765
Iteration 18/25 | Loss: 0.00127765
Iteration 19/25 | Loss: 0.00127762
Iteration 20/25 | Loss: 0.00127762
Iteration 21/25 | Loss: 0.00127762
Iteration 22/25 | Loss: 0.00127762
Iteration 23/25 | Loss: 0.00127762
Iteration 24/25 | Loss: 0.00127762
Iteration 25/25 | Loss: 0.00127762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43950891
Iteration 2/25 | Loss: 0.00110271
Iteration 3/25 | Loss: 0.00101603
Iteration 4/25 | Loss: 0.00101602
Iteration 5/25 | Loss: 0.00101602
Iteration 6/25 | Loss: 0.00101602
Iteration 7/25 | Loss: 0.00101602
Iteration 8/25 | Loss: 0.00101602
Iteration 9/25 | Loss: 0.00101602
Iteration 10/25 | Loss: 0.00101602
Iteration 11/25 | Loss: 0.00101602
Iteration 12/25 | Loss: 0.00101602
Iteration 13/25 | Loss: 0.00101602
Iteration 14/25 | Loss: 0.00101602
Iteration 15/25 | Loss: 0.00101602
Iteration 16/25 | Loss: 0.00101602
Iteration 17/25 | Loss: 0.00101602
Iteration 18/25 | Loss: 0.00101602
Iteration 19/25 | Loss: 0.00101602
Iteration 20/25 | Loss: 0.00101602
Iteration 21/25 | Loss: 0.00101602
Iteration 22/25 | Loss: 0.00101602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010160185629501939, 0.0010160185629501939, 0.0010160185629501939, 0.0010160185629501939, 0.0010160185629501939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010160185629501939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101602
Iteration 2/1000 | Loss: 0.00008890
Iteration 3/1000 | Loss: 0.00002483
Iteration 4/1000 | Loss: 0.00001841
Iteration 5/1000 | Loss: 0.00002562
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001656
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00041376
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001524
Iteration 16/1000 | Loss: 0.00001438
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001289
Iteration 24/1000 | Loss: 0.00001283
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001261
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001244
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.2315520507399924e-05, 1.2315520507399924e-05, 1.2315520507399924e-05, 1.2315520507399924e-05, 1.2315520507399924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2315520507399924e-05

Optimization complete. Final v2v error: 2.9847302436828613 mm

Highest mean error: 4.317816257476807 mm for frame 66

Lowest mean error: 2.760118246078491 mm for frame 16

Saving results

Total time: 66.38928413391113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434523
Iteration 2/25 | Loss: 0.00133241
Iteration 3/25 | Loss: 0.00126337
Iteration 4/25 | Loss: 0.00125680
Iteration 5/25 | Loss: 0.00125473
Iteration 6/25 | Loss: 0.00125431
Iteration 7/25 | Loss: 0.00125431
Iteration 8/25 | Loss: 0.00125431
Iteration 9/25 | Loss: 0.00125431
Iteration 10/25 | Loss: 0.00125431
Iteration 11/25 | Loss: 0.00125431
Iteration 12/25 | Loss: 0.00125431
Iteration 13/25 | Loss: 0.00125431
Iteration 14/25 | Loss: 0.00125431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012543054763227701, 0.0012543054763227701, 0.0012543054763227701, 0.0012543054763227701, 0.0012543054763227701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012543054763227701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.49972296
Iteration 2/25 | Loss: 0.00103323
Iteration 3/25 | Loss: 0.00103322
Iteration 4/25 | Loss: 0.00103322
Iteration 5/25 | Loss: 0.00103322
Iteration 6/25 | Loss: 0.00103322
Iteration 7/25 | Loss: 0.00103322
Iteration 8/25 | Loss: 0.00103321
Iteration 9/25 | Loss: 0.00103321
Iteration 10/25 | Loss: 0.00103321
Iteration 11/25 | Loss: 0.00103321
Iteration 12/25 | Loss: 0.00103321
Iteration 13/25 | Loss: 0.00103321
Iteration 14/25 | Loss: 0.00103321
Iteration 15/25 | Loss: 0.00103321
Iteration 16/25 | Loss: 0.00103321
Iteration 17/25 | Loss: 0.00103321
Iteration 18/25 | Loss: 0.00103321
Iteration 19/25 | Loss: 0.00103321
Iteration 20/25 | Loss: 0.00103321
Iteration 21/25 | Loss: 0.00103321
Iteration 22/25 | Loss: 0.00103321
Iteration 23/25 | Loss: 0.00103321
Iteration 24/25 | Loss: 0.00103321
Iteration 25/25 | Loss: 0.00103321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103321
Iteration 2/1000 | Loss: 0.00002417
Iteration 3/1000 | Loss: 0.00001843
Iteration 4/1000 | Loss: 0.00001701
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001533
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001343
Iteration 22/1000 | Loss: 0.00001342
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001336
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001317
Iteration 41/1000 | Loss: 0.00001317
Iteration 42/1000 | Loss: 0.00001316
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001314
Iteration 47/1000 | Loss: 0.00001314
Iteration 48/1000 | Loss: 0.00001314
Iteration 49/1000 | Loss: 0.00001313
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001302
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001294
Iteration 87/1000 | Loss: 0.00001294
Iteration 88/1000 | Loss: 0.00001294
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001294
Iteration 91/1000 | Loss: 0.00001293
Iteration 92/1000 | Loss: 0.00001293
Iteration 93/1000 | Loss: 0.00001293
Iteration 94/1000 | Loss: 0.00001293
Iteration 95/1000 | Loss: 0.00001293
Iteration 96/1000 | Loss: 0.00001293
Iteration 97/1000 | Loss: 0.00001293
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001292
Iteration 101/1000 | Loss: 0.00001292
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001290
Iteration 131/1000 | Loss: 0.00001290
Iteration 132/1000 | Loss: 0.00001290
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.2879080713901203e-05, 1.2879080713901203e-05, 1.2879080713901203e-05, 1.2879080713901203e-05, 1.2879080713901203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2879080713901203e-05

Optimization complete. Final v2v error: 3.0660898685455322 mm

Highest mean error: 3.782116651535034 mm for frame 89

Lowest mean error: 2.75693678855896 mm for frame 37

Saving results

Total time: 40.20106554031372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466840
Iteration 2/25 | Loss: 0.00145881
Iteration 3/25 | Loss: 0.00129032
Iteration 4/25 | Loss: 0.00127617
Iteration 5/25 | Loss: 0.00127389
Iteration 6/25 | Loss: 0.00127327
Iteration 7/25 | Loss: 0.00127327
Iteration 8/25 | Loss: 0.00127327
Iteration 9/25 | Loss: 0.00127327
Iteration 10/25 | Loss: 0.00127327
Iteration 11/25 | Loss: 0.00127327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001273270114324987, 0.001273270114324987, 0.001273270114324987, 0.001273270114324987, 0.001273270114324987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273270114324987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35687101
Iteration 2/25 | Loss: 0.00098785
Iteration 3/25 | Loss: 0.00098785
Iteration 4/25 | Loss: 0.00098785
Iteration 5/25 | Loss: 0.00098785
Iteration 6/25 | Loss: 0.00098785
Iteration 7/25 | Loss: 0.00098785
Iteration 8/25 | Loss: 0.00098784
Iteration 9/25 | Loss: 0.00098784
Iteration 10/25 | Loss: 0.00098784
Iteration 11/25 | Loss: 0.00098784
Iteration 12/25 | Loss: 0.00098784
Iteration 13/25 | Loss: 0.00098784
Iteration 14/25 | Loss: 0.00098784
Iteration 15/25 | Loss: 0.00098784
Iteration 16/25 | Loss: 0.00098784
Iteration 17/25 | Loss: 0.00098784
Iteration 18/25 | Loss: 0.00098784
Iteration 19/25 | Loss: 0.00098784
Iteration 20/25 | Loss: 0.00098784
Iteration 21/25 | Loss: 0.00098784
Iteration 22/25 | Loss: 0.00098784
Iteration 23/25 | Loss: 0.00098784
Iteration 24/25 | Loss: 0.00098784
Iteration 25/25 | Loss: 0.00098784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009878437267616391, 0.0009878437267616391, 0.0009878437267616391, 0.0009878437267616391, 0.0009878437267616391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009878437267616391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098784
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001647
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001411
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001348
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001302
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001296
Iteration 15/1000 | Loss: 0.00001295
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001257
Iteration 48/1000 | Loss: 0.00001256
Iteration 49/1000 | Loss: 0.00001256
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001254
Iteration 55/1000 | Loss: 0.00001254
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001249
Iteration 65/1000 | Loss: 0.00001249
Iteration 66/1000 | Loss: 0.00001248
Iteration 67/1000 | Loss: 0.00001248
Iteration 68/1000 | Loss: 0.00001248
Iteration 69/1000 | Loss: 0.00001247
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001247
Iteration 72/1000 | Loss: 0.00001247
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001246
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001244
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001242
Iteration 87/1000 | Loss: 0.00001242
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001240
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001236
Iteration 122/1000 | Loss: 0.00001235
Iteration 123/1000 | Loss: 0.00001235
Iteration 124/1000 | Loss: 0.00001235
Iteration 125/1000 | Loss: 0.00001234
Iteration 126/1000 | Loss: 0.00001234
Iteration 127/1000 | Loss: 0.00001234
Iteration 128/1000 | Loss: 0.00001234
Iteration 129/1000 | Loss: 0.00001233
Iteration 130/1000 | Loss: 0.00001233
Iteration 131/1000 | Loss: 0.00001233
Iteration 132/1000 | Loss: 0.00001233
Iteration 133/1000 | Loss: 0.00001233
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Iteration 137/1000 | Loss: 0.00001232
Iteration 138/1000 | Loss: 0.00001232
Iteration 139/1000 | Loss: 0.00001231
Iteration 140/1000 | Loss: 0.00001231
Iteration 141/1000 | Loss: 0.00001231
Iteration 142/1000 | Loss: 0.00001231
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001230
Iteration 146/1000 | Loss: 0.00001230
Iteration 147/1000 | Loss: 0.00001230
Iteration 148/1000 | Loss: 0.00001230
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001229
Iteration 151/1000 | Loss: 0.00001229
Iteration 152/1000 | Loss: 0.00001229
Iteration 153/1000 | Loss: 0.00001229
Iteration 154/1000 | Loss: 0.00001229
Iteration 155/1000 | Loss: 0.00001229
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001228
Iteration 163/1000 | Loss: 0.00001228
Iteration 164/1000 | Loss: 0.00001228
Iteration 165/1000 | Loss: 0.00001228
Iteration 166/1000 | Loss: 0.00001228
Iteration 167/1000 | Loss: 0.00001228
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001227
Iteration 170/1000 | Loss: 0.00001227
Iteration 171/1000 | Loss: 0.00001227
Iteration 172/1000 | Loss: 0.00001227
Iteration 173/1000 | Loss: 0.00001227
Iteration 174/1000 | Loss: 0.00001227
Iteration 175/1000 | Loss: 0.00001227
Iteration 176/1000 | Loss: 0.00001227
Iteration 177/1000 | Loss: 0.00001227
Iteration 178/1000 | Loss: 0.00001227
Iteration 179/1000 | Loss: 0.00001226
Iteration 180/1000 | Loss: 0.00001226
Iteration 181/1000 | Loss: 0.00001226
Iteration 182/1000 | Loss: 0.00001225
Iteration 183/1000 | Loss: 0.00001225
Iteration 184/1000 | Loss: 0.00001225
Iteration 185/1000 | Loss: 0.00001225
Iteration 186/1000 | Loss: 0.00001225
Iteration 187/1000 | Loss: 0.00001225
Iteration 188/1000 | Loss: 0.00001225
Iteration 189/1000 | Loss: 0.00001225
Iteration 190/1000 | Loss: 0.00001225
Iteration 191/1000 | Loss: 0.00001225
Iteration 192/1000 | Loss: 0.00001225
Iteration 193/1000 | Loss: 0.00001225
Iteration 194/1000 | Loss: 0.00001225
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001224
Iteration 199/1000 | Loss: 0.00001224
Iteration 200/1000 | Loss: 0.00001224
Iteration 201/1000 | Loss: 0.00001223
Iteration 202/1000 | Loss: 0.00001223
Iteration 203/1000 | Loss: 0.00001223
Iteration 204/1000 | Loss: 0.00001223
Iteration 205/1000 | Loss: 0.00001223
Iteration 206/1000 | Loss: 0.00001223
Iteration 207/1000 | Loss: 0.00001223
Iteration 208/1000 | Loss: 0.00001223
Iteration 209/1000 | Loss: 0.00001223
Iteration 210/1000 | Loss: 0.00001223
Iteration 211/1000 | Loss: 0.00001223
Iteration 212/1000 | Loss: 0.00001223
Iteration 213/1000 | Loss: 0.00001223
Iteration 214/1000 | Loss: 0.00001223
Iteration 215/1000 | Loss: 0.00001223
Iteration 216/1000 | Loss: 0.00001223
Iteration 217/1000 | Loss: 0.00001223
Iteration 218/1000 | Loss: 0.00001223
Iteration 219/1000 | Loss: 0.00001223
Iteration 220/1000 | Loss: 0.00001223
Iteration 221/1000 | Loss: 0.00001223
Iteration 222/1000 | Loss: 0.00001223
Iteration 223/1000 | Loss: 0.00001223
Iteration 224/1000 | Loss: 0.00001223
Iteration 225/1000 | Loss: 0.00001223
Iteration 226/1000 | Loss: 0.00001223
Iteration 227/1000 | Loss: 0.00001223
Iteration 228/1000 | Loss: 0.00001223
Iteration 229/1000 | Loss: 0.00001223
Iteration 230/1000 | Loss: 0.00001223
Iteration 231/1000 | Loss: 0.00001223
Iteration 232/1000 | Loss: 0.00001223
Iteration 233/1000 | Loss: 0.00001223
Iteration 234/1000 | Loss: 0.00001223
Iteration 235/1000 | Loss: 0.00001223
Iteration 236/1000 | Loss: 0.00001223
Iteration 237/1000 | Loss: 0.00001223
Iteration 238/1000 | Loss: 0.00001223
Iteration 239/1000 | Loss: 0.00001223
Iteration 240/1000 | Loss: 0.00001223
Iteration 241/1000 | Loss: 0.00001223
Iteration 242/1000 | Loss: 0.00001223
Iteration 243/1000 | Loss: 0.00001223
Iteration 244/1000 | Loss: 0.00001223
Iteration 245/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.2229519597894978e-05, 1.2229519597894978e-05, 1.2229519597894978e-05, 1.2229519597894978e-05, 1.2229519597894978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2229519597894978e-05

Optimization complete. Final v2v error: 2.9567742347717285 mm

Highest mean error: 3.6628191471099854 mm for frame 80

Lowest mean error: 2.6671903133392334 mm for frame 158

Saving results

Total time: 42.358882188797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958288
Iteration 2/25 | Loss: 0.00250256
Iteration 3/25 | Loss: 0.00217516
Iteration 4/25 | Loss: 0.00212599
Iteration 5/25 | Loss: 0.00204897
Iteration 6/25 | Loss: 0.00203868
Iteration 7/25 | Loss: 0.00202982
Iteration 8/25 | Loss: 0.00200979
Iteration 9/25 | Loss: 0.00200970
Iteration 10/25 | Loss: 0.00199594
Iteration 11/25 | Loss: 0.00199362
Iteration 12/25 | Loss: 0.00197562
Iteration 13/25 | Loss: 0.00198157
Iteration 14/25 | Loss: 0.00197085
Iteration 15/25 | Loss: 0.00196958
Iteration 16/25 | Loss: 0.00196919
Iteration 17/25 | Loss: 0.00196876
Iteration 18/25 | Loss: 0.00196786
Iteration 19/25 | Loss: 0.00197183
Iteration 20/25 | Loss: 0.00196603
Iteration 21/25 | Loss: 0.00196525
Iteration 22/25 | Loss: 0.00196507
Iteration 23/25 | Loss: 0.00196498
Iteration 24/25 | Loss: 0.00196498
Iteration 25/25 | Loss: 0.00196498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33384418
Iteration 2/25 | Loss: 0.00552937
Iteration 3/25 | Loss: 0.00552936
Iteration 4/25 | Loss: 0.00552936
Iteration 5/25 | Loss: 0.00552936
Iteration 6/25 | Loss: 0.00552936
Iteration 7/25 | Loss: 0.00552936
Iteration 8/25 | Loss: 0.00552936
Iteration 9/25 | Loss: 0.00552936
Iteration 10/25 | Loss: 0.00552936
Iteration 11/25 | Loss: 0.00552936
Iteration 12/25 | Loss: 0.00552936
Iteration 13/25 | Loss: 0.00552936
Iteration 14/25 | Loss: 0.00552936
Iteration 15/25 | Loss: 0.00552936
Iteration 16/25 | Loss: 0.00552936
Iteration 17/25 | Loss: 0.00552936
Iteration 18/25 | Loss: 0.00552936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005529359448701143, 0.005529359448701143, 0.005529359448701143, 0.005529359448701143, 0.005529359448701143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005529359448701143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00552936
Iteration 2/1000 | Loss: 0.00092021
Iteration 3/1000 | Loss: 0.00073984
Iteration 4/1000 | Loss: 0.00051763
Iteration 5/1000 | Loss: 0.00124824
Iteration 6/1000 | Loss: 0.00664969
Iteration 7/1000 | Loss: 0.00614067
Iteration 8/1000 | Loss: 0.00065328
Iteration 9/1000 | Loss: 0.00051754
Iteration 10/1000 | Loss: 0.00061449
Iteration 11/1000 | Loss: 0.00057086
Iteration 12/1000 | Loss: 0.00069133
Iteration 13/1000 | Loss: 0.00096612
Iteration 14/1000 | Loss: 0.00157434
Iteration 15/1000 | Loss: 0.00122305
Iteration 16/1000 | Loss: 0.00093373
Iteration 17/1000 | Loss: 0.00061901
Iteration 18/1000 | Loss: 0.00044071
Iteration 19/1000 | Loss: 0.00082191
Iteration 20/1000 | Loss: 0.00049780
Iteration 21/1000 | Loss: 0.00097603
Iteration 22/1000 | Loss: 0.00312028
Iteration 23/1000 | Loss: 0.02185290
Iteration 24/1000 | Loss: 0.01806096
Iteration 25/1000 | Loss: 0.00626573
Iteration 26/1000 | Loss: 0.00105002
Iteration 27/1000 | Loss: 0.00327208
Iteration 28/1000 | Loss: 0.00077874
Iteration 29/1000 | Loss: 0.00033555
Iteration 30/1000 | Loss: 0.00246360
Iteration 31/1000 | Loss: 0.00538479
Iteration 32/1000 | Loss: 0.00332382
Iteration 33/1000 | Loss: 0.00305077
Iteration 34/1000 | Loss: 0.00275502
Iteration 35/1000 | Loss: 0.00327743
Iteration 36/1000 | Loss: 0.00229405
Iteration 37/1000 | Loss: 0.00190420
Iteration 38/1000 | Loss: 0.00092437
Iteration 39/1000 | Loss: 0.00101272
Iteration 40/1000 | Loss: 0.00099789
Iteration 41/1000 | Loss: 0.00021911
Iteration 42/1000 | Loss: 0.00079078
Iteration 43/1000 | Loss: 0.00061698
Iteration 44/1000 | Loss: 0.00028172
Iteration 45/1000 | Loss: 0.00061324
Iteration 46/1000 | Loss: 0.00105693
Iteration 47/1000 | Loss: 0.00052483
Iteration 48/1000 | Loss: 0.00031897
Iteration 49/1000 | Loss: 0.00099072
Iteration 50/1000 | Loss: 0.00034991
Iteration 51/1000 | Loss: 0.00068787
Iteration 52/1000 | Loss: 0.00051070
Iteration 53/1000 | Loss: 0.00040679
Iteration 54/1000 | Loss: 0.00035988
Iteration 55/1000 | Loss: 0.00091581
Iteration 56/1000 | Loss: 0.00008616
Iteration 57/1000 | Loss: 0.00033729
Iteration 58/1000 | Loss: 0.00130159
Iteration 59/1000 | Loss: 0.00191649
Iteration 60/1000 | Loss: 0.00053504
Iteration 61/1000 | Loss: 0.00239256
Iteration 62/1000 | Loss: 0.00139993
Iteration 63/1000 | Loss: 0.00046821
Iteration 64/1000 | Loss: 0.00037281
Iteration 65/1000 | Loss: 0.00089161
Iteration 66/1000 | Loss: 0.00315348
Iteration 67/1000 | Loss: 0.00034102
Iteration 68/1000 | Loss: 0.00028467
Iteration 69/1000 | Loss: 0.00006479
Iteration 70/1000 | Loss: 0.00040268
Iteration 71/1000 | Loss: 0.00152913
Iteration 72/1000 | Loss: 0.00046746
Iteration 73/1000 | Loss: 0.00047725
Iteration 74/1000 | Loss: 0.00005709
Iteration 75/1000 | Loss: 0.00048686
Iteration 76/1000 | Loss: 0.00026660
Iteration 77/1000 | Loss: 0.00128297
Iteration 78/1000 | Loss: 0.00177119
Iteration 79/1000 | Loss: 0.00091187
Iteration 80/1000 | Loss: 0.00216478
Iteration 81/1000 | Loss: 0.00063546
Iteration 82/1000 | Loss: 0.00006251
Iteration 83/1000 | Loss: 0.00038688
Iteration 84/1000 | Loss: 0.00061177
Iteration 85/1000 | Loss: 0.00039681
Iteration 86/1000 | Loss: 0.00057698
Iteration 87/1000 | Loss: 0.00052364
Iteration 88/1000 | Loss: 0.00055982
Iteration 89/1000 | Loss: 0.00032510
Iteration 90/1000 | Loss: 0.00050315
Iteration 91/1000 | Loss: 0.00065910
Iteration 92/1000 | Loss: 0.00006747
Iteration 93/1000 | Loss: 0.00059652
Iteration 94/1000 | Loss: 0.00055798
Iteration 95/1000 | Loss: 0.00026790
Iteration 96/1000 | Loss: 0.00022895
Iteration 97/1000 | Loss: 0.00079734
Iteration 98/1000 | Loss: 0.00072617
Iteration 99/1000 | Loss: 0.00087126
Iteration 100/1000 | Loss: 0.00029786
Iteration 101/1000 | Loss: 0.00022353
Iteration 102/1000 | Loss: 0.00027082
Iteration 103/1000 | Loss: 0.00087225
Iteration 104/1000 | Loss: 0.00048186
Iteration 105/1000 | Loss: 0.00043369
Iteration 106/1000 | Loss: 0.00048003
Iteration 107/1000 | Loss: 0.00098182
Iteration 108/1000 | Loss: 0.00049069
Iteration 109/1000 | Loss: 0.00031203
Iteration 110/1000 | Loss: 0.00095126
Iteration 111/1000 | Loss: 0.00061729
Iteration 112/1000 | Loss: 0.00017670
Iteration 113/1000 | Loss: 0.00039023
Iteration 114/1000 | Loss: 0.00025176
Iteration 115/1000 | Loss: 0.00027062
Iteration 116/1000 | Loss: 0.00010857
Iteration 117/1000 | Loss: 0.00006106
Iteration 118/1000 | Loss: 0.00005986
Iteration 119/1000 | Loss: 0.00020137
Iteration 120/1000 | Loss: 0.00005245
Iteration 121/1000 | Loss: 0.00009845
Iteration 122/1000 | Loss: 0.00011071
Iteration 123/1000 | Loss: 0.00007638
Iteration 124/1000 | Loss: 0.00015646
Iteration 125/1000 | Loss: 0.00005146
Iteration 126/1000 | Loss: 0.00009656
Iteration 127/1000 | Loss: 0.00019589
Iteration 128/1000 | Loss: 0.00004014
Iteration 129/1000 | Loss: 0.00011619
Iteration 130/1000 | Loss: 0.00003348
Iteration 131/1000 | Loss: 0.00003190
Iteration 132/1000 | Loss: 0.00024449
Iteration 133/1000 | Loss: 0.00005092
Iteration 134/1000 | Loss: 0.00002712
Iteration 135/1000 | Loss: 0.00002609
Iteration 136/1000 | Loss: 0.00002432
Iteration 137/1000 | Loss: 0.00014705
Iteration 138/1000 | Loss: 0.00002320
Iteration 139/1000 | Loss: 0.00002239
Iteration 140/1000 | Loss: 0.00002162
Iteration 141/1000 | Loss: 0.00002104
Iteration 142/1000 | Loss: 0.00009767
Iteration 143/1000 | Loss: 0.00014486
Iteration 144/1000 | Loss: 0.00009985
Iteration 145/1000 | Loss: 0.00005748
Iteration 146/1000 | Loss: 0.00005621
Iteration 147/1000 | Loss: 0.00002065
Iteration 148/1000 | Loss: 0.00003260
Iteration 149/1000 | Loss: 0.00002623
Iteration 150/1000 | Loss: 0.00002685
Iteration 151/1000 | Loss: 0.00002298
Iteration 152/1000 | Loss: 0.00002167
Iteration 153/1000 | Loss: 0.00002121
Iteration 154/1000 | Loss: 0.00002098
Iteration 155/1000 | Loss: 0.00002092
Iteration 156/1000 | Loss: 0.00002092
Iteration 157/1000 | Loss: 0.00002092
Iteration 158/1000 | Loss: 0.00002091
Iteration 159/1000 | Loss: 0.00015224
Iteration 160/1000 | Loss: 0.00015224
Iteration 161/1000 | Loss: 0.00103830
Iteration 162/1000 | Loss: 0.00274558
Iteration 163/1000 | Loss: 0.00364109
Iteration 164/1000 | Loss: 0.00144238
Iteration 165/1000 | Loss: 0.00002797
Iteration 166/1000 | Loss: 0.00002243
Iteration 167/1000 | Loss: 0.00002122
Iteration 168/1000 | Loss: 0.00006332
Iteration 169/1000 | Loss: 0.00002071
Iteration 170/1000 | Loss: 0.00002070
Iteration 171/1000 | Loss: 0.00002069
Iteration 172/1000 | Loss: 0.00002068
Iteration 173/1000 | Loss: 0.00002068
Iteration 174/1000 | Loss: 0.00002060
Iteration 175/1000 | Loss: 0.00002058
Iteration 176/1000 | Loss: 0.00002058
Iteration 177/1000 | Loss: 0.00002058
Iteration 178/1000 | Loss: 0.00002058
Iteration 179/1000 | Loss: 0.00002058
Iteration 180/1000 | Loss: 0.00002057
Iteration 181/1000 | Loss: 0.00002057
Iteration 182/1000 | Loss: 0.00002057
Iteration 183/1000 | Loss: 0.00002057
Iteration 184/1000 | Loss: 0.00002057
Iteration 185/1000 | Loss: 0.00002057
Iteration 186/1000 | Loss: 0.00002057
Iteration 187/1000 | Loss: 0.00002057
Iteration 188/1000 | Loss: 0.00002057
Iteration 189/1000 | Loss: 0.00002056
Iteration 190/1000 | Loss: 0.00002056
Iteration 191/1000 | Loss: 0.00002056
Iteration 192/1000 | Loss: 0.00002055
Iteration 193/1000 | Loss: 0.00002055
Iteration 194/1000 | Loss: 0.00002055
Iteration 195/1000 | Loss: 0.00002055
Iteration 196/1000 | Loss: 0.00002055
Iteration 197/1000 | Loss: 0.00002055
Iteration 198/1000 | Loss: 0.00002054
Iteration 199/1000 | Loss: 0.00002054
Iteration 200/1000 | Loss: 0.00002054
Iteration 201/1000 | Loss: 0.00002054
Iteration 202/1000 | Loss: 0.00002054
Iteration 203/1000 | Loss: 0.00002053
Iteration 204/1000 | Loss: 0.00002053
Iteration 205/1000 | Loss: 0.00002052
Iteration 206/1000 | Loss: 0.00002052
Iteration 207/1000 | Loss: 0.00002051
Iteration 208/1000 | Loss: 0.00002051
Iteration 209/1000 | Loss: 0.00002051
Iteration 210/1000 | Loss: 0.00002050
Iteration 211/1000 | Loss: 0.00002050
Iteration 212/1000 | Loss: 0.00002049
Iteration 213/1000 | Loss: 0.00002049
Iteration 214/1000 | Loss: 0.00017671
Iteration 215/1000 | Loss: 0.00003176
Iteration 216/1000 | Loss: 0.00002760
Iteration 217/1000 | Loss: 0.00002050
Iteration 218/1000 | Loss: 0.00002048
Iteration 219/1000 | Loss: 0.00002047
Iteration 220/1000 | Loss: 0.00002046
Iteration 221/1000 | Loss: 0.00002046
Iteration 222/1000 | Loss: 0.00002043
Iteration 223/1000 | Loss: 0.00002043
Iteration 224/1000 | Loss: 0.00002042
Iteration 225/1000 | Loss: 0.00002042
Iteration 226/1000 | Loss: 0.00002042
Iteration 227/1000 | Loss: 0.00002042
Iteration 228/1000 | Loss: 0.00002041
Iteration 229/1000 | Loss: 0.00002041
Iteration 230/1000 | Loss: 0.00002041
Iteration 231/1000 | Loss: 0.00002041
Iteration 232/1000 | Loss: 0.00002040
Iteration 233/1000 | Loss: 0.00002040
Iteration 234/1000 | Loss: 0.00002040
Iteration 235/1000 | Loss: 0.00002040
Iteration 236/1000 | Loss: 0.00002040
Iteration 237/1000 | Loss: 0.00002040
Iteration 238/1000 | Loss: 0.00002040
Iteration 239/1000 | Loss: 0.00002040
Iteration 240/1000 | Loss: 0.00002040
Iteration 241/1000 | Loss: 0.00002040
Iteration 242/1000 | Loss: 0.00002040
Iteration 243/1000 | Loss: 0.00002040
Iteration 244/1000 | Loss: 0.00002040
Iteration 245/1000 | Loss: 0.00002040
Iteration 246/1000 | Loss: 0.00002040
Iteration 247/1000 | Loss: 0.00002040
Iteration 248/1000 | Loss: 0.00002040
Iteration 249/1000 | Loss: 0.00002040
Iteration 250/1000 | Loss: 0.00002040
Iteration 251/1000 | Loss: 0.00002040
Iteration 252/1000 | Loss: 0.00002040
Iteration 253/1000 | Loss: 0.00002040
Iteration 254/1000 | Loss: 0.00002040
Iteration 255/1000 | Loss: 0.00002040
Iteration 256/1000 | Loss: 0.00002040
Iteration 257/1000 | Loss: 0.00002040
Iteration 258/1000 | Loss: 0.00002040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [2.0399435015860945e-05, 2.0399435015860945e-05, 2.0399435015860945e-05, 2.0399435015860945e-05, 2.0399435015860945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0399435015860945e-05

Optimization complete. Final v2v error: 3.730087995529175 mm

Highest mean error: 12.437202453613281 mm for frame 164

Lowest mean error: 3.4986753463745117 mm for frame 112

Saving results

Total time: 280.265985250473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928574
Iteration 2/25 | Loss: 0.00452378
Iteration 3/25 | Loss: 0.00377928
Iteration 4/25 | Loss: 0.00307597
Iteration 5/25 | Loss: 0.00250326
Iteration 6/25 | Loss: 0.00200300
Iteration 7/25 | Loss: 0.00196857
Iteration 8/25 | Loss: 0.00176710
Iteration 9/25 | Loss: 0.00171541
Iteration 10/25 | Loss: 0.00172423
Iteration 11/25 | Loss: 0.00171575
Iteration 12/25 | Loss: 0.00159957
Iteration 13/25 | Loss: 0.00159014
Iteration 14/25 | Loss: 0.00157555
Iteration 15/25 | Loss: 0.00157676
Iteration 16/25 | Loss: 0.00156013
Iteration 17/25 | Loss: 0.00154321
Iteration 18/25 | Loss: 0.00153155
Iteration 19/25 | Loss: 0.00155175
Iteration 20/25 | Loss: 0.00151699
Iteration 21/25 | Loss: 0.00150584
Iteration 22/25 | Loss: 0.00150622
Iteration 23/25 | Loss: 0.00150472
Iteration 24/25 | Loss: 0.00150487
Iteration 25/25 | Loss: 0.00150467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33120525
Iteration 2/25 | Loss: 0.00159391
Iteration 3/25 | Loss: 0.00159391
Iteration 4/25 | Loss: 0.00159391
Iteration 5/25 | Loss: 0.00159391
Iteration 6/25 | Loss: 0.00159391
Iteration 7/25 | Loss: 0.00159391
Iteration 8/25 | Loss: 0.00159391
Iteration 9/25 | Loss: 0.00159391
Iteration 10/25 | Loss: 0.00159391
Iteration 11/25 | Loss: 0.00159391
Iteration 12/25 | Loss: 0.00159391
Iteration 13/25 | Loss: 0.00159391
Iteration 14/25 | Loss: 0.00159391
Iteration 15/25 | Loss: 0.00159391
Iteration 16/25 | Loss: 0.00159391
Iteration 17/25 | Loss: 0.00159391
Iteration 18/25 | Loss: 0.00159391
Iteration 19/25 | Loss: 0.00159391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001593909109942615, 0.001593909109942615, 0.001593909109942615, 0.001593909109942615, 0.001593909109942615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001593909109942615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159391
Iteration 2/1000 | Loss: 0.00030804
Iteration 3/1000 | Loss: 0.00015690
Iteration 4/1000 | Loss: 0.00012889
Iteration 5/1000 | Loss: 0.00014555
Iteration 6/1000 | Loss: 0.00011396
Iteration 7/1000 | Loss: 0.00024103
Iteration 8/1000 | Loss: 0.00178984
Iteration 9/1000 | Loss: 0.00192600
Iteration 10/1000 | Loss: 0.00074694
Iteration 11/1000 | Loss: 0.00028454
Iteration 12/1000 | Loss: 0.00009727
Iteration 13/1000 | Loss: 0.00006609
Iteration 14/1000 | Loss: 0.00005218
Iteration 15/1000 | Loss: 0.00004568
Iteration 16/1000 | Loss: 0.00003950
Iteration 17/1000 | Loss: 0.00003509
Iteration 18/1000 | Loss: 0.00003173
Iteration 19/1000 | Loss: 0.00002934
Iteration 20/1000 | Loss: 0.00002739
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00002486
Iteration 23/1000 | Loss: 0.00002398
Iteration 24/1000 | Loss: 0.00002341
Iteration 25/1000 | Loss: 0.00002306
Iteration 26/1000 | Loss: 0.00002280
Iteration 27/1000 | Loss: 0.00002257
Iteration 28/1000 | Loss: 0.00002251
Iteration 29/1000 | Loss: 0.00002250
Iteration 30/1000 | Loss: 0.00002245
Iteration 31/1000 | Loss: 0.00002240
Iteration 32/1000 | Loss: 0.00002239
Iteration 33/1000 | Loss: 0.00002236
Iteration 34/1000 | Loss: 0.00002233
Iteration 35/1000 | Loss: 0.00002232
Iteration 36/1000 | Loss: 0.00002230
Iteration 37/1000 | Loss: 0.00002230
Iteration 38/1000 | Loss: 0.00002230
Iteration 39/1000 | Loss: 0.00002230
Iteration 40/1000 | Loss: 0.00002230
Iteration 41/1000 | Loss: 0.00002230
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002230
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002229
Iteration 46/1000 | Loss: 0.00002229
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00002228
Iteration 49/1000 | Loss: 0.00002225
Iteration 50/1000 | Loss: 0.00002225
Iteration 51/1000 | Loss: 0.00002223
Iteration 52/1000 | Loss: 0.00002223
Iteration 53/1000 | Loss: 0.00002222
Iteration 54/1000 | Loss: 0.00002222
Iteration 55/1000 | Loss: 0.00002222
Iteration 56/1000 | Loss: 0.00002222
Iteration 57/1000 | Loss: 0.00002222
Iteration 58/1000 | Loss: 0.00002222
Iteration 59/1000 | Loss: 0.00002222
Iteration 60/1000 | Loss: 0.00002221
Iteration 61/1000 | Loss: 0.00002221
Iteration 62/1000 | Loss: 0.00002221
Iteration 63/1000 | Loss: 0.00002221
Iteration 64/1000 | Loss: 0.00002221
Iteration 65/1000 | Loss: 0.00002221
Iteration 66/1000 | Loss: 0.00002221
Iteration 67/1000 | Loss: 0.00002221
Iteration 68/1000 | Loss: 0.00002220
Iteration 69/1000 | Loss: 0.00002220
Iteration 70/1000 | Loss: 0.00002220
Iteration 71/1000 | Loss: 0.00002219
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002219
Iteration 74/1000 | Loss: 0.00002219
Iteration 75/1000 | Loss: 0.00002219
Iteration 76/1000 | Loss: 0.00002219
Iteration 77/1000 | Loss: 0.00002219
Iteration 78/1000 | Loss: 0.00002219
Iteration 79/1000 | Loss: 0.00002219
Iteration 80/1000 | Loss: 0.00002219
Iteration 81/1000 | Loss: 0.00002218
Iteration 82/1000 | Loss: 0.00002218
Iteration 83/1000 | Loss: 0.00002218
Iteration 84/1000 | Loss: 0.00002218
Iteration 85/1000 | Loss: 0.00002218
Iteration 86/1000 | Loss: 0.00002218
Iteration 87/1000 | Loss: 0.00002218
Iteration 88/1000 | Loss: 0.00002218
Iteration 89/1000 | Loss: 0.00002218
Iteration 90/1000 | Loss: 0.00002218
Iteration 91/1000 | Loss: 0.00002218
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002218
Iteration 94/1000 | Loss: 0.00002218
Iteration 95/1000 | Loss: 0.00002218
Iteration 96/1000 | Loss: 0.00002218
Iteration 97/1000 | Loss: 0.00002218
Iteration 98/1000 | Loss: 0.00002218
Iteration 99/1000 | Loss: 0.00002218
Iteration 100/1000 | Loss: 0.00002218
Iteration 101/1000 | Loss: 0.00002218
Iteration 102/1000 | Loss: 0.00002218
Iteration 103/1000 | Loss: 0.00002218
Iteration 104/1000 | Loss: 0.00002218
Iteration 105/1000 | Loss: 0.00002218
Iteration 106/1000 | Loss: 0.00002218
Iteration 107/1000 | Loss: 0.00002218
Iteration 108/1000 | Loss: 0.00002218
Iteration 109/1000 | Loss: 0.00002218
Iteration 110/1000 | Loss: 0.00002218
Iteration 111/1000 | Loss: 0.00002218
Iteration 112/1000 | Loss: 0.00002218
Iteration 113/1000 | Loss: 0.00002218
Iteration 114/1000 | Loss: 0.00002218
Iteration 115/1000 | Loss: 0.00002218
Iteration 116/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.2182259272085503e-05, 2.2182259272085503e-05, 2.2182259272085503e-05, 2.2182259272085503e-05, 2.2182259272085503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2182259272085503e-05

Optimization complete. Final v2v error: 4.010155200958252 mm

Highest mean error: 4.964971542358398 mm for frame 162

Lowest mean error: 3.463829278945923 mm for frame 27

Saving results

Total time: 97.4406144618988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540886
Iteration 2/25 | Loss: 0.00139236
Iteration 3/25 | Loss: 0.00132061
Iteration 4/25 | Loss: 0.00130731
Iteration 5/25 | Loss: 0.00130255
Iteration 6/25 | Loss: 0.00130255
Iteration 7/25 | Loss: 0.00130255
Iteration 8/25 | Loss: 0.00130255
Iteration 9/25 | Loss: 0.00130255
Iteration 10/25 | Loss: 0.00130255
Iteration 11/25 | Loss: 0.00130255
Iteration 12/25 | Loss: 0.00130255
Iteration 13/25 | Loss: 0.00130255
Iteration 14/25 | Loss: 0.00130255
Iteration 15/25 | Loss: 0.00130255
Iteration 16/25 | Loss: 0.00130255
Iteration 17/25 | Loss: 0.00130255
Iteration 18/25 | Loss: 0.00130255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013025468215346336, 0.0013025468215346336, 0.0013025468215346336, 0.0013025468215346336, 0.0013025468215346336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013025468215346336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75011724
Iteration 2/25 | Loss: 0.00091177
Iteration 3/25 | Loss: 0.00091177
Iteration 4/25 | Loss: 0.00091177
Iteration 5/25 | Loss: 0.00091177
Iteration 6/25 | Loss: 0.00091177
Iteration 7/25 | Loss: 0.00091177
Iteration 8/25 | Loss: 0.00091177
Iteration 9/25 | Loss: 0.00091177
Iteration 10/25 | Loss: 0.00091177
Iteration 11/25 | Loss: 0.00091177
Iteration 12/25 | Loss: 0.00091177
Iteration 13/25 | Loss: 0.00091177
Iteration 14/25 | Loss: 0.00091177
Iteration 15/25 | Loss: 0.00091177
Iteration 16/25 | Loss: 0.00091177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009117682348005474, 0.0009117682348005474, 0.0009117682348005474, 0.0009117682348005474, 0.0009117682348005474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009117682348005474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091177
Iteration 2/1000 | Loss: 0.00003401
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002311
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002204
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002114
Iteration 10/1000 | Loss: 0.00002082
Iteration 11/1000 | Loss: 0.00002044
Iteration 12/1000 | Loss: 0.00002016
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001965
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001905
Iteration 22/1000 | Loss: 0.00001905
Iteration 23/1000 | Loss: 0.00001905
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001899
Iteration 27/1000 | Loss: 0.00001899
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001897
Iteration 31/1000 | Loss: 0.00001897
Iteration 32/1000 | Loss: 0.00001897
Iteration 33/1000 | Loss: 0.00001897
Iteration 34/1000 | Loss: 0.00001896
Iteration 35/1000 | Loss: 0.00001896
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001896
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001895
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001893
Iteration 45/1000 | Loss: 0.00001893
Iteration 46/1000 | Loss: 0.00001893
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001892
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001892
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001892
Iteration 53/1000 | Loss: 0.00001892
Iteration 54/1000 | Loss: 0.00001892
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001891
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001881
Iteration 78/1000 | Loss: 0.00001880
Iteration 79/1000 | Loss: 0.00001880
Iteration 80/1000 | Loss: 0.00001880
Iteration 81/1000 | Loss: 0.00001880
Iteration 82/1000 | Loss: 0.00001880
Iteration 83/1000 | Loss: 0.00001880
Iteration 84/1000 | Loss: 0.00001880
Iteration 85/1000 | Loss: 0.00001880
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001879
Iteration 88/1000 | Loss: 0.00001879
Iteration 89/1000 | Loss: 0.00001879
Iteration 90/1000 | Loss: 0.00001879
Iteration 91/1000 | Loss: 0.00001879
Iteration 92/1000 | Loss: 0.00001879
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001878
Iteration 95/1000 | Loss: 0.00001878
Iteration 96/1000 | Loss: 0.00001878
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001877
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001876
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001875
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001875
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001875
Iteration 122/1000 | Loss: 0.00001875
Iteration 123/1000 | Loss: 0.00001875
Iteration 124/1000 | Loss: 0.00001875
Iteration 125/1000 | Loss: 0.00001875
Iteration 126/1000 | Loss: 0.00001875
Iteration 127/1000 | Loss: 0.00001875
Iteration 128/1000 | Loss: 0.00001875
Iteration 129/1000 | Loss: 0.00001875
Iteration 130/1000 | Loss: 0.00001875
Iteration 131/1000 | Loss: 0.00001875
Iteration 132/1000 | Loss: 0.00001875
Iteration 133/1000 | Loss: 0.00001875
Iteration 134/1000 | Loss: 0.00001875
Iteration 135/1000 | Loss: 0.00001875
Iteration 136/1000 | Loss: 0.00001875
Iteration 137/1000 | Loss: 0.00001875
Iteration 138/1000 | Loss: 0.00001875
Iteration 139/1000 | Loss: 0.00001875
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001875
Iteration 142/1000 | Loss: 0.00001875
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001875
Iteration 145/1000 | Loss: 0.00001875
Iteration 146/1000 | Loss: 0.00001875
Iteration 147/1000 | Loss: 0.00001875
Iteration 148/1000 | Loss: 0.00001875
Iteration 149/1000 | Loss: 0.00001875
Iteration 150/1000 | Loss: 0.00001875
Iteration 151/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.87504556379281e-05, 1.87504556379281e-05, 1.87504556379281e-05, 1.87504556379281e-05, 1.87504556379281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.87504556379281e-05

Optimization complete. Final v2v error: 3.668272018432617 mm

Highest mean error: 3.716853380203247 mm for frame 162

Lowest mean error: 3.5853023529052734 mm for frame 14

Saving results

Total time: 43.43666100502014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897708
Iteration 2/25 | Loss: 0.00197005
Iteration 3/25 | Loss: 0.00161042
Iteration 4/25 | Loss: 0.00154458
Iteration 5/25 | Loss: 0.00152266
Iteration 6/25 | Loss: 0.00145933
Iteration 7/25 | Loss: 0.00142472
Iteration 8/25 | Loss: 0.00140791
Iteration 9/25 | Loss: 0.00140297
Iteration 10/25 | Loss: 0.00141805
Iteration 11/25 | Loss: 0.00142243
Iteration 12/25 | Loss: 0.00139925
Iteration 13/25 | Loss: 0.00140127
Iteration 14/25 | Loss: 0.00137989
Iteration 15/25 | Loss: 0.00137391
Iteration 16/25 | Loss: 0.00137300
Iteration 17/25 | Loss: 0.00137266
Iteration 18/25 | Loss: 0.00137256
Iteration 19/25 | Loss: 0.00137256
Iteration 20/25 | Loss: 0.00137256
Iteration 21/25 | Loss: 0.00137256
Iteration 22/25 | Loss: 0.00137256
Iteration 23/25 | Loss: 0.00137256
Iteration 24/25 | Loss: 0.00137256
Iteration 25/25 | Loss: 0.00137256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34380603
Iteration 2/25 | Loss: 0.00116752
Iteration 3/25 | Loss: 0.00116498
Iteration 4/25 | Loss: 0.00116498
Iteration 5/25 | Loss: 0.00116498
Iteration 6/25 | Loss: 0.00116498
Iteration 7/25 | Loss: 0.00116498
Iteration 8/25 | Loss: 0.00116498
Iteration 9/25 | Loss: 0.00116498
Iteration 10/25 | Loss: 0.00116498
Iteration 11/25 | Loss: 0.00116498
Iteration 12/25 | Loss: 0.00116498
Iteration 13/25 | Loss: 0.00116498
Iteration 14/25 | Loss: 0.00116498
Iteration 15/25 | Loss: 0.00116498
Iteration 16/25 | Loss: 0.00116498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011649783700704575, 0.0011649783700704575, 0.0011649783700704575, 0.0011649783700704575, 0.0011649783700704575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011649783700704575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116498
Iteration 2/1000 | Loss: 0.00025866
Iteration 3/1000 | Loss: 0.00034935
Iteration 4/1000 | Loss: 0.00019929
Iteration 5/1000 | Loss: 0.00014089
Iteration 6/1000 | Loss: 0.00010719
Iteration 7/1000 | Loss: 0.00008306
Iteration 8/1000 | Loss: 0.00007159
Iteration 9/1000 | Loss: 0.00006561
Iteration 10/1000 | Loss: 0.00012347
Iteration 11/1000 | Loss: 0.00015215
Iteration 12/1000 | Loss: 0.00020299
Iteration 13/1000 | Loss: 0.00010346
Iteration 14/1000 | Loss: 0.00009240
Iteration 15/1000 | Loss: 0.00015568
Iteration 16/1000 | Loss: 0.00009367
Iteration 17/1000 | Loss: 0.00011898
Iteration 18/1000 | Loss: 0.00007947
Iteration 19/1000 | Loss: 0.00011034
Iteration 20/1000 | Loss: 0.00006755
Iteration 21/1000 | Loss: 0.00006184
Iteration 22/1000 | Loss: 0.00005473
Iteration 23/1000 | Loss: 0.00005032
Iteration 24/1000 | Loss: 0.00004764
Iteration 25/1000 | Loss: 0.00004449
Iteration 26/1000 | Loss: 0.00009152
Iteration 27/1000 | Loss: 0.00007022
Iteration 28/1000 | Loss: 0.00004999
Iteration 29/1000 | Loss: 0.00004666
Iteration 30/1000 | Loss: 0.00005120
Iteration 31/1000 | Loss: 0.00005692
Iteration 32/1000 | Loss: 0.00004114
Iteration 33/1000 | Loss: 0.00004430
Iteration 34/1000 | Loss: 0.00004237
Iteration 35/1000 | Loss: 0.00004009
Iteration 36/1000 | Loss: 0.00005339
Iteration 37/1000 | Loss: 0.00005316
Iteration 38/1000 | Loss: 0.00003976
Iteration 39/1000 | Loss: 0.00004680
Iteration 40/1000 | Loss: 0.00003817
Iteration 41/1000 | Loss: 0.00005261
Iteration 42/1000 | Loss: 0.00004696
Iteration 43/1000 | Loss: 0.00004491
Iteration 44/1000 | Loss: 0.00007639
Iteration 45/1000 | Loss: 0.00007268
Iteration 46/1000 | Loss: 0.00007676
Iteration 47/1000 | Loss: 0.00007870
Iteration 48/1000 | Loss: 0.00008322
Iteration 49/1000 | Loss: 0.00008612
Iteration 50/1000 | Loss: 0.00008524
Iteration 51/1000 | Loss: 0.00005184
Iteration 52/1000 | Loss: 0.00014642
Iteration 53/1000 | Loss: 0.00005193
Iteration 54/1000 | Loss: 0.00004612
Iteration 55/1000 | Loss: 0.00004087
Iteration 56/1000 | Loss: 0.00003822
Iteration 57/1000 | Loss: 0.00004704
Iteration 58/1000 | Loss: 0.00003685
Iteration 59/1000 | Loss: 0.00003798
Iteration 60/1000 | Loss: 0.00004361
Iteration 61/1000 | Loss: 0.00003703
Iteration 62/1000 | Loss: 0.00004932
Iteration 63/1000 | Loss: 0.00003439
Iteration 64/1000 | Loss: 0.00006418
Iteration 65/1000 | Loss: 0.00006156
Iteration 66/1000 | Loss: 0.00003978
Iteration 67/1000 | Loss: 0.00003909
Iteration 68/1000 | Loss: 0.00004446
Iteration 69/1000 | Loss: 0.00004152
Iteration 70/1000 | Loss: 0.00014361
Iteration 71/1000 | Loss: 0.00004646
Iteration 72/1000 | Loss: 0.00003804
Iteration 73/1000 | Loss: 0.00003382
Iteration 74/1000 | Loss: 0.00004093
Iteration 75/1000 | Loss: 0.00003898
Iteration 76/1000 | Loss: 0.00003943
Iteration 77/1000 | Loss: 0.00003360
Iteration 78/1000 | Loss: 0.00014818
Iteration 79/1000 | Loss: 0.00004066
Iteration 80/1000 | Loss: 0.00003989
Iteration 81/1000 | Loss: 0.00003093
Iteration 82/1000 | Loss: 0.00013753
Iteration 83/1000 | Loss: 0.00007838
Iteration 84/1000 | Loss: 0.00006808
Iteration 85/1000 | Loss: 0.00006662
Iteration 86/1000 | Loss: 0.00005705
Iteration 87/1000 | Loss: 0.00004901
Iteration 88/1000 | Loss: 0.00005213
Iteration 89/1000 | Loss: 0.00004786
Iteration 90/1000 | Loss: 0.00005874
Iteration 91/1000 | Loss: 0.00004945
Iteration 92/1000 | Loss: 0.00005735
Iteration 93/1000 | Loss: 0.00004841
Iteration 94/1000 | Loss: 0.00004344
Iteration 95/1000 | Loss: 0.00004254
Iteration 96/1000 | Loss: 0.00004892
Iteration 97/1000 | Loss: 0.00008619
Iteration 98/1000 | Loss: 0.00012704
Iteration 99/1000 | Loss: 0.00008489
Iteration 100/1000 | Loss: 0.00005465
Iteration 101/1000 | Loss: 0.00003497
Iteration 102/1000 | Loss: 0.00004685
Iteration 103/1000 | Loss: 0.00008400
Iteration 104/1000 | Loss: 0.00006729
Iteration 105/1000 | Loss: 0.00007333
Iteration 106/1000 | Loss: 0.00005750
Iteration 107/1000 | Loss: 0.00005985
Iteration 108/1000 | Loss: 0.00005861
Iteration 109/1000 | Loss: 0.00005508
Iteration 110/1000 | Loss: 0.00004509
Iteration 111/1000 | Loss: 0.00004094
Iteration 112/1000 | Loss: 0.00003964
Iteration 113/1000 | Loss: 0.00004895
Iteration 114/1000 | Loss: 0.00004392
Iteration 115/1000 | Loss: 0.00005610
Iteration 116/1000 | Loss: 0.00005681
Iteration 117/1000 | Loss: 0.00004052
Iteration 118/1000 | Loss: 0.00003475
Iteration 119/1000 | Loss: 0.00004330
Iteration 120/1000 | Loss: 0.00004702
Iteration 121/1000 | Loss: 0.00004294
Iteration 122/1000 | Loss: 0.00003511
Iteration 123/1000 | Loss: 0.00003325
Iteration 124/1000 | Loss: 0.00003322
Iteration 125/1000 | Loss: 0.00003924
Iteration 126/1000 | Loss: 0.00003362
Iteration 127/1000 | Loss: 0.00003264
Iteration 128/1000 | Loss: 0.00004227
Iteration 129/1000 | Loss: 0.00003544
Iteration 130/1000 | Loss: 0.00003651
Iteration 131/1000 | Loss: 0.00003984
Iteration 132/1000 | Loss: 0.00003261
Iteration 133/1000 | Loss: 0.00002985
Iteration 134/1000 | Loss: 0.00003648
Iteration 135/1000 | Loss: 0.00005991
Iteration 136/1000 | Loss: 0.00005345
Iteration 137/1000 | Loss: 0.00003617
Iteration 138/1000 | Loss: 0.00003049
Iteration 139/1000 | Loss: 0.00003175
Iteration 140/1000 | Loss: 0.00003380
Iteration 141/1000 | Loss: 0.00003215
Iteration 142/1000 | Loss: 0.00003016
Iteration 143/1000 | Loss: 0.00002989
Iteration 144/1000 | Loss: 0.00002964
Iteration 145/1000 | Loss: 0.00003045
Iteration 146/1000 | Loss: 0.00003045
Iteration 147/1000 | Loss: 0.00002939
Iteration 148/1000 | Loss: 0.00002980
Iteration 149/1000 | Loss: 0.00002916
Iteration 150/1000 | Loss: 0.00002984
Iteration 151/1000 | Loss: 0.00002925
Iteration 152/1000 | Loss: 0.00002925
Iteration 153/1000 | Loss: 0.00003403
Iteration 154/1000 | Loss: 0.00006307
Iteration 155/1000 | Loss: 0.00005078
Iteration 156/1000 | Loss: 0.00009538
Iteration 157/1000 | Loss: 0.00004673
Iteration 158/1000 | Loss: 0.00006376
Iteration 159/1000 | Loss: 0.00003141
Iteration 160/1000 | Loss: 0.00002986
Iteration 161/1000 | Loss: 0.00004301
Iteration 162/1000 | Loss: 0.00003733
Iteration 163/1000 | Loss: 0.00003157
Iteration 164/1000 | Loss: 0.00002944
Iteration 165/1000 | Loss: 0.00002829
Iteration 166/1000 | Loss: 0.00002784
Iteration 167/1000 | Loss: 0.00005041
Iteration 168/1000 | Loss: 0.00004063
Iteration 169/1000 | Loss: 0.00002750
Iteration 170/1000 | Loss: 0.00002725
Iteration 171/1000 | Loss: 0.00005195
Iteration 172/1000 | Loss: 0.00003638
Iteration 173/1000 | Loss: 0.00003380
Iteration 174/1000 | Loss: 0.00003159
Iteration 175/1000 | Loss: 0.00003082
Iteration 176/1000 | Loss: 0.00003042
Iteration 177/1000 | Loss: 0.00002986
Iteration 178/1000 | Loss: 0.00002919
Iteration 179/1000 | Loss: 0.00002882
Iteration 180/1000 | Loss: 0.00002849
Iteration 181/1000 | Loss: 0.00002809
Iteration 182/1000 | Loss: 0.00002781
Iteration 183/1000 | Loss: 0.00008432
Iteration 184/1000 | Loss: 0.00006099
Iteration 185/1000 | Loss: 0.00004306
Iteration 186/1000 | Loss: 0.00005386
Iteration 187/1000 | Loss: 0.00005087
Iteration 188/1000 | Loss: 0.00002740
Iteration 189/1000 | Loss: 0.00002709
Iteration 190/1000 | Loss: 0.00002703
Iteration 191/1000 | Loss: 0.00005772
Iteration 192/1000 | Loss: 0.00005162
Iteration 193/1000 | Loss: 0.00002817
Iteration 194/1000 | Loss: 0.00002696
Iteration 195/1000 | Loss: 0.00002670
Iteration 196/1000 | Loss: 0.00002669
Iteration 197/1000 | Loss: 0.00002669
Iteration 198/1000 | Loss: 0.00002668
Iteration 199/1000 | Loss: 0.00002668
Iteration 200/1000 | Loss: 0.00002668
Iteration 201/1000 | Loss: 0.00005512
Iteration 202/1000 | Loss: 0.00004741
Iteration 203/1000 | Loss: 0.00005503
Iteration 204/1000 | Loss: 0.00004696
Iteration 205/1000 | Loss: 0.00002702
Iteration 206/1000 | Loss: 0.00002659
Iteration 207/1000 | Loss: 0.00002655
Iteration 208/1000 | Loss: 0.00002651
Iteration 209/1000 | Loss: 0.00002651
Iteration 210/1000 | Loss: 0.00002647
Iteration 211/1000 | Loss: 0.00002647
Iteration 212/1000 | Loss: 0.00002646
Iteration 213/1000 | Loss: 0.00002646
Iteration 214/1000 | Loss: 0.00002645
Iteration 215/1000 | Loss: 0.00002645
Iteration 216/1000 | Loss: 0.00002644
Iteration 217/1000 | Loss: 0.00005613
Iteration 218/1000 | Loss: 0.00005274
Iteration 219/1000 | Loss: 0.00003575
Iteration 220/1000 | Loss: 0.00002746
Iteration 221/1000 | Loss: 0.00002683
Iteration 222/1000 | Loss: 0.00002644
Iteration 223/1000 | Loss: 0.00002641
Iteration 224/1000 | Loss: 0.00002638
Iteration 225/1000 | Loss: 0.00002638
Iteration 226/1000 | Loss: 0.00002638
Iteration 227/1000 | Loss: 0.00002638
Iteration 228/1000 | Loss: 0.00002638
Iteration 229/1000 | Loss: 0.00002638
Iteration 230/1000 | Loss: 0.00002638
Iteration 231/1000 | Loss: 0.00002638
Iteration 232/1000 | Loss: 0.00002638
Iteration 233/1000 | Loss: 0.00002638
Iteration 234/1000 | Loss: 0.00002637
Iteration 235/1000 | Loss: 0.00002637
Iteration 236/1000 | Loss: 0.00005445
Iteration 237/1000 | Loss: 0.00004875
Iteration 238/1000 | Loss: 0.00008300
Iteration 239/1000 | Loss: 0.00005698
Iteration 240/1000 | Loss: 0.00003525
Iteration 241/1000 | Loss: 0.00005464
Iteration 242/1000 | Loss: 0.00004951
Iteration 243/1000 | Loss: 0.00008325
Iteration 244/1000 | Loss: 0.00006463
Iteration 245/1000 | Loss: 0.00004979
Iteration 246/1000 | Loss: 0.00004099
Iteration 247/1000 | Loss: 0.00003474
Iteration 248/1000 | Loss: 0.00003236
Iteration 249/1000 | Loss: 0.00003348
Iteration 250/1000 | Loss: 0.00003209
Iteration 251/1000 | Loss: 0.00002874
Iteration 252/1000 | Loss: 0.00002722
Iteration 253/1000 | Loss: 0.00002676
Iteration 254/1000 | Loss: 0.00002662
Iteration 255/1000 | Loss: 0.00002662
Iteration 256/1000 | Loss: 0.00002659
Iteration 257/1000 | Loss: 0.00002659
Iteration 258/1000 | Loss: 0.00002658
Iteration 259/1000 | Loss: 0.00002645
Iteration 260/1000 | Loss: 0.00002645
Iteration 261/1000 | Loss: 0.00002637
Iteration 262/1000 | Loss: 0.00002637
Iteration 263/1000 | Loss: 0.00002637
Iteration 264/1000 | Loss: 0.00002636
Iteration 265/1000 | Loss: 0.00002636
Iteration 266/1000 | Loss: 0.00002636
Iteration 267/1000 | Loss: 0.00002636
Iteration 268/1000 | Loss: 0.00002636
Iteration 269/1000 | Loss: 0.00002636
Iteration 270/1000 | Loss: 0.00002636
Iteration 271/1000 | Loss: 0.00002636
Iteration 272/1000 | Loss: 0.00002635
Iteration 273/1000 | Loss: 0.00002635
Iteration 274/1000 | Loss: 0.00002635
Iteration 275/1000 | Loss: 0.00002634
Iteration 276/1000 | Loss: 0.00002634
Iteration 277/1000 | Loss: 0.00002633
Iteration 278/1000 | Loss: 0.00002633
Iteration 279/1000 | Loss: 0.00002633
Iteration 280/1000 | Loss: 0.00002633
Iteration 281/1000 | Loss: 0.00002633
Iteration 282/1000 | Loss: 0.00002632
Iteration 283/1000 | Loss: 0.00002632
Iteration 284/1000 | Loss: 0.00002632
Iteration 285/1000 | Loss: 0.00002632
Iteration 286/1000 | Loss: 0.00002632
Iteration 287/1000 | Loss: 0.00002632
Iteration 288/1000 | Loss: 0.00002632
Iteration 289/1000 | Loss: 0.00002631
Iteration 290/1000 | Loss: 0.00002631
Iteration 291/1000 | Loss: 0.00002631
Iteration 292/1000 | Loss: 0.00002631
Iteration 293/1000 | Loss: 0.00002631
Iteration 294/1000 | Loss: 0.00002631
Iteration 295/1000 | Loss: 0.00002630
Iteration 296/1000 | Loss: 0.00002630
Iteration 297/1000 | Loss: 0.00002630
Iteration 298/1000 | Loss: 0.00002629
Iteration 299/1000 | Loss: 0.00002629
Iteration 300/1000 | Loss: 0.00002628
Iteration 301/1000 | Loss: 0.00002628
Iteration 302/1000 | Loss: 0.00002628
Iteration 303/1000 | Loss: 0.00002628
Iteration 304/1000 | Loss: 0.00002628
Iteration 305/1000 | Loss: 0.00002628
Iteration 306/1000 | Loss: 0.00002627
Iteration 307/1000 | Loss: 0.00002627
Iteration 308/1000 | Loss: 0.00002627
Iteration 309/1000 | Loss: 0.00002627
Iteration 310/1000 | Loss: 0.00002627
Iteration 311/1000 | Loss: 0.00002627
Iteration 312/1000 | Loss: 0.00002627
Iteration 313/1000 | Loss: 0.00002627
Iteration 314/1000 | Loss: 0.00002627
Iteration 315/1000 | Loss: 0.00002626
Iteration 316/1000 | Loss: 0.00002626
Iteration 317/1000 | Loss: 0.00002626
Iteration 318/1000 | Loss: 0.00002625
Iteration 319/1000 | Loss: 0.00002625
Iteration 320/1000 | Loss: 0.00002625
Iteration 321/1000 | Loss: 0.00002624
Iteration 322/1000 | Loss: 0.00002624
Iteration 323/1000 | Loss: 0.00002624
Iteration 324/1000 | Loss: 0.00002624
Iteration 325/1000 | Loss: 0.00002623
Iteration 326/1000 | Loss: 0.00002623
Iteration 327/1000 | Loss: 0.00002623
Iteration 328/1000 | Loss: 0.00002623
Iteration 329/1000 | Loss: 0.00002623
Iteration 330/1000 | Loss: 0.00002622
Iteration 331/1000 | Loss: 0.00002622
Iteration 332/1000 | Loss: 0.00002622
Iteration 333/1000 | Loss: 0.00002622
Iteration 334/1000 | Loss: 0.00002622
Iteration 335/1000 | Loss: 0.00002622
Iteration 336/1000 | Loss: 0.00002622
Iteration 337/1000 | Loss: 0.00002622
Iteration 338/1000 | Loss: 0.00002622
Iteration 339/1000 | Loss: 0.00002622
Iteration 340/1000 | Loss: 0.00002622
Iteration 341/1000 | Loss: 0.00002621
Iteration 342/1000 | Loss: 0.00002621
Iteration 343/1000 | Loss: 0.00002621
Iteration 344/1000 | Loss: 0.00002621
Iteration 345/1000 | Loss: 0.00002621
Iteration 346/1000 | Loss: 0.00002621
Iteration 347/1000 | Loss: 0.00002621
Iteration 348/1000 | Loss: 0.00002621
Iteration 349/1000 | Loss: 0.00002621
Iteration 350/1000 | Loss: 0.00002621
Iteration 351/1000 | Loss: 0.00002621
Iteration 352/1000 | Loss: 0.00002621
Iteration 353/1000 | Loss: 0.00002621
Iteration 354/1000 | Loss: 0.00002621
Iteration 355/1000 | Loss: 0.00002621
Iteration 356/1000 | Loss: 0.00002621
Iteration 357/1000 | Loss: 0.00002621
Iteration 358/1000 | Loss: 0.00002621
Iteration 359/1000 | Loss: 0.00002621
Iteration 360/1000 | Loss: 0.00002621
Iteration 361/1000 | Loss: 0.00002621
Iteration 362/1000 | Loss: 0.00002621
Iteration 363/1000 | Loss: 0.00002621
Iteration 364/1000 | Loss: 0.00002620
Iteration 365/1000 | Loss: 0.00002620
Iteration 366/1000 | Loss: 0.00002620
Iteration 367/1000 | Loss: 0.00002620
Iteration 368/1000 | Loss: 0.00002620
Iteration 369/1000 | Loss: 0.00002620
Iteration 370/1000 | Loss: 0.00002620
Iteration 371/1000 | Loss: 0.00002620
Iteration 372/1000 | Loss: 0.00002620
Iteration 373/1000 | Loss: 0.00002620
Iteration 374/1000 | Loss: 0.00002620
Iteration 375/1000 | Loss: 0.00002620
Iteration 376/1000 | Loss: 0.00002620
Iteration 377/1000 | Loss: 0.00002620
Iteration 378/1000 | Loss: 0.00002620
Iteration 379/1000 | Loss: 0.00002620
Iteration 380/1000 | Loss: 0.00002620
Iteration 381/1000 | Loss: 0.00002620
Iteration 382/1000 | Loss: 0.00002620
Iteration 383/1000 | Loss: 0.00002620
Iteration 384/1000 | Loss: 0.00002620
Iteration 385/1000 | Loss: 0.00002620
Iteration 386/1000 | Loss: 0.00002620
Iteration 387/1000 | Loss: 0.00002620
Iteration 388/1000 | Loss: 0.00002620
Iteration 389/1000 | Loss: 0.00002620
Iteration 390/1000 | Loss: 0.00002620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 390. Stopping optimization.
Last 5 losses: [2.6200979846180417e-05, 2.6200979846180417e-05, 2.6200979846180417e-05, 2.6200979846180417e-05, 2.6200979846180417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6200979846180417e-05

Optimization complete. Final v2v error: 4.100866317749023 mm

Highest mean error: 8.08560562133789 mm for frame 113

Lowest mean error: 3.1518452167510986 mm for frame 74

Saving results

Total time: 367.0138611793518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377699
Iteration 2/25 | Loss: 0.00129816
Iteration 3/25 | Loss: 0.00123794
Iteration 4/25 | Loss: 0.00123055
Iteration 5/25 | Loss: 0.00122823
Iteration 6/25 | Loss: 0.00122797
Iteration 7/25 | Loss: 0.00122797
Iteration 8/25 | Loss: 0.00122797
Iteration 9/25 | Loss: 0.00122797
Iteration 10/25 | Loss: 0.00122797
Iteration 11/25 | Loss: 0.00122797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012279724469408393, 0.0012279724469408393, 0.0012279724469408393, 0.0012279724469408393, 0.0012279724469408393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012279724469408393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43298459
Iteration 2/25 | Loss: 0.00106842
Iteration 3/25 | Loss: 0.00106842
Iteration 4/25 | Loss: 0.00106841
Iteration 5/25 | Loss: 0.00106841
Iteration 6/25 | Loss: 0.00106841
Iteration 7/25 | Loss: 0.00106841
Iteration 8/25 | Loss: 0.00106841
Iteration 9/25 | Loss: 0.00106841
Iteration 10/25 | Loss: 0.00106841
Iteration 11/25 | Loss: 0.00106841
Iteration 12/25 | Loss: 0.00106841
Iteration 13/25 | Loss: 0.00106841
Iteration 14/25 | Loss: 0.00106841
Iteration 15/25 | Loss: 0.00106841
Iteration 16/25 | Loss: 0.00106841
Iteration 17/25 | Loss: 0.00106841
Iteration 18/25 | Loss: 0.00106841
Iteration 19/25 | Loss: 0.00106841
Iteration 20/25 | Loss: 0.00106841
Iteration 21/25 | Loss: 0.00106841
Iteration 22/25 | Loss: 0.00106841
Iteration 23/25 | Loss: 0.00106841
Iteration 24/25 | Loss: 0.00106841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010684110457077622, 0.0010684110457077622, 0.0010684110457077622, 0.0010684110457077622, 0.0010684110457077622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010684110457077622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106841
Iteration 2/1000 | Loss: 0.00002796
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001291
Iteration 6/1000 | Loss: 0.00001231
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001121
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001096
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001074
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001073
Iteration 17/1000 | Loss: 0.00001065
Iteration 18/1000 | Loss: 0.00001062
Iteration 19/1000 | Loss: 0.00001057
Iteration 20/1000 | Loss: 0.00001057
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001053
Iteration 23/1000 | Loss: 0.00001052
Iteration 24/1000 | Loss: 0.00001051
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001051
Iteration 27/1000 | Loss: 0.00001051
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001049
Iteration 32/1000 | Loss: 0.00001049
Iteration 33/1000 | Loss: 0.00001049
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001043
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001038
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001036
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001035
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001033
Iteration 46/1000 | Loss: 0.00001032
Iteration 47/1000 | Loss: 0.00001032
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001030
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001029
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001028
Iteration 58/1000 | Loss: 0.00001028
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001027
Iteration 61/1000 | Loss: 0.00001027
Iteration 62/1000 | Loss: 0.00001027
Iteration 63/1000 | Loss: 0.00001027
Iteration 64/1000 | Loss: 0.00001026
Iteration 65/1000 | Loss: 0.00001026
Iteration 66/1000 | Loss: 0.00001023
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001022
Iteration 69/1000 | Loss: 0.00001022
Iteration 70/1000 | Loss: 0.00001021
Iteration 71/1000 | Loss: 0.00001017
Iteration 72/1000 | Loss: 0.00001017
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001013
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001012
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001011
Iteration 87/1000 | Loss: 0.00001011
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001010
Iteration 90/1000 | Loss: 0.00001010
Iteration 91/1000 | Loss: 0.00001010
Iteration 92/1000 | Loss: 0.00001010
Iteration 93/1000 | Loss: 0.00001010
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001008
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001007
Iteration 100/1000 | Loss: 0.00001007
Iteration 101/1000 | Loss: 0.00001006
Iteration 102/1000 | Loss: 0.00001006
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001005
Iteration 105/1000 | Loss: 0.00001005
Iteration 106/1000 | Loss: 0.00001005
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001005
Iteration 109/1000 | Loss: 0.00001005
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001004
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001002
Iteration 117/1000 | Loss: 0.00001002
Iteration 118/1000 | Loss: 0.00001002
Iteration 119/1000 | Loss: 0.00001002
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001001
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001000
Iteration 128/1000 | Loss: 0.00001000
Iteration 129/1000 | Loss: 0.00001000
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00000999
Iteration 142/1000 | Loss: 0.00000999
Iteration 143/1000 | Loss: 0.00000998
Iteration 144/1000 | Loss: 0.00000998
Iteration 145/1000 | Loss: 0.00000998
Iteration 146/1000 | Loss: 0.00000998
Iteration 147/1000 | Loss: 0.00000998
Iteration 148/1000 | Loss: 0.00000998
Iteration 149/1000 | Loss: 0.00000998
Iteration 150/1000 | Loss: 0.00000998
Iteration 151/1000 | Loss: 0.00000998
Iteration 152/1000 | Loss: 0.00000998
Iteration 153/1000 | Loss: 0.00000998
Iteration 154/1000 | Loss: 0.00000998
Iteration 155/1000 | Loss: 0.00000998
Iteration 156/1000 | Loss: 0.00000998
Iteration 157/1000 | Loss: 0.00000997
Iteration 158/1000 | Loss: 0.00000997
Iteration 159/1000 | Loss: 0.00000997
Iteration 160/1000 | Loss: 0.00000997
Iteration 161/1000 | Loss: 0.00000997
Iteration 162/1000 | Loss: 0.00000997
Iteration 163/1000 | Loss: 0.00000997
Iteration 164/1000 | Loss: 0.00000997
Iteration 165/1000 | Loss: 0.00000997
Iteration 166/1000 | Loss: 0.00000997
Iteration 167/1000 | Loss: 0.00000997
Iteration 168/1000 | Loss: 0.00000997
Iteration 169/1000 | Loss: 0.00000996
Iteration 170/1000 | Loss: 0.00000996
Iteration 171/1000 | Loss: 0.00000996
Iteration 172/1000 | Loss: 0.00000996
Iteration 173/1000 | Loss: 0.00000996
Iteration 174/1000 | Loss: 0.00000996
Iteration 175/1000 | Loss: 0.00000996
Iteration 176/1000 | Loss: 0.00000996
Iteration 177/1000 | Loss: 0.00000996
Iteration 178/1000 | Loss: 0.00000996
Iteration 179/1000 | Loss: 0.00000996
Iteration 180/1000 | Loss: 0.00000996
Iteration 181/1000 | Loss: 0.00000996
Iteration 182/1000 | Loss: 0.00000996
Iteration 183/1000 | Loss: 0.00000996
Iteration 184/1000 | Loss: 0.00000996
Iteration 185/1000 | Loss: 0.00000996
Iteration 186/1000 | Loss: 0.00000995
Iteration 187/1000 | Loss: 0.00000995
Iteration 188/1000 | Loss: 0.00000995
Iteration 189/1000 | Loss: 0.00000995
Iteration 190/1000 | Loss: 0.00000995
Iteration 191/1000 | Loss: 0.00000995
Iteration 192/1000 | Loss: 0.00000995
Iteration 193/1000 | Loss: 0.00000995
Iteration 194/1000 | Loss: 0.00000995
Iteration 195/1000 | Loss: 0.00000995
Iteration 196/1000 | Loss: 0.00000995
Iteration 197/1000 | Loss: 0.00000995
Iteration 198/1000 | Loss: 0.00000994
Iteration 199/1000 | Loss: 0.00000994
Iteration 200/1000 | Loss: 0.00000994
Iteration 201/1000 | Loss: 0.00000994
Iteration 202/1000 | Loss: 0.00000993
Iteration 203/1000 | Loss: 0.00000993
Iteration 204/1000 | Loss: 0.00000993
Iteration 205/1000 | Loss: 0.00000993
Iteration 206/1000 | Loss: 0.00000993
Iteration 207/1000 | Loss: 0.00000993
Iteration 208/1000 | Loss: 0.00000993
Iteration 209/1000 | Loss: 0.00000993
Iteration 210/1000 | Loss: 0.00000993
Iteration 211/1000 | Loss: 0.00000993
Iteration 212/1000 | Loss: 0.00000993
Iteration 213/1000 | Loss: 0.00000993
Iteration 214/1000 | Loss: 0.00000993
Iteration 215/1000 | Loss: 0.00000993
Iteration 216/1000 | Loss: 0.00000992
Iteration 217/1000 | Loss: 0.00000992
Iteration 218/1000 | Loss: 0.00000992
Iteration 219/1000 | Loss: 0.00000992
Iteration 220/1000 | Loss: 0.00000992
Iteration 221/1000 | Loss: 0.00000992
Iteration 222/1000 | Loss: 0.00000992
Iteration 223/1000 | Loss: 0.00000992
Iteration 224/1000 | Loss: 0.00000992
Iteration 225/1000 | Loss: 0.00000991
Iteration 226/1000 | Loss: 0.00000991
Iteration 227/1000 | Loss: 0.00000991
Iteration 228/1000 | Loss: 0.00000991
Iteration 229/1000 | Loss: 0.00000991
Iteration 230/1000 | Loss: 0.00000991
Iteration 231/1000 | Loss: 0.00000991
Iteration 232/1000 | Loss: 0.00000991
Iteration 233/1000 | Loss: 0.00000991
Iteration 234/1000 | Loss: 0.00000991
Iteration 235/1000 | Loss: 0.00000991
Iteration 236/1000 | Loss: 0.00000990
Iteration 237/1000 | Loss: 0.00000990
Iteration 238/1000 | Loss: 0.00000990
Iteration 239/1000 | Loss: 0.00000990
Iteration 240/1000 | Loss: 0.00000990
Iteration 241/1000 | Loss: 0.00000990
Iteration 242/1000 | Loss: 0.00000990
Iteration 243/1000 | Loss: 0.00000990
Iteration 244/1000 | Loss: 0.00000990
Iteration 245/1000 | Loss: 0.00000990
Iteration 246/1000 | Loss: 0.00000990
Iteration 247/1000 | Loss: 0.00000990
Iteration 248/1000 | Loss: 0.00000990
Iteration 249/1000 | Loss: 0.00000990
Iteration 250/1000 | Loss: 0.00000990
Iteration 251/1000 | Loss: 0.00000990
Iteration 252/1000 | Loss: 0.00000990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [9.896972187561914e-06, 9.896972187561914e-06, 9.896972187561914e-06, 9.896972187561914e-06, 9.896972187561914e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.896972187561914e-06

Optimization complete. Final v2v error: 2.7043914794921875 mm

Highest mean error: 3.19214129447937 mm for frame 98

Lowest mean error: 2.5857982635498047 mm for frame 45

Saving results

Total time: 45.41329646110535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038121
Iteration 2/25 | Loss: 0.01038121
Iteration 3/25 | Loss: 0.00250610
Iteration 4/25 | Loss: 0.00170558
Iteration 5/25 | Loss: 0.00149416
Iteration 6/25 | Loss: 0.00143656
Iteration 7/25 | Loss: 0.00142617
Iteration 8/25 | Loss: 0.00131927
Iteration 9/25 | Loss: 0.00129201
Iteration 10/25 | Loss: 0.00126167
Iteration 11/25 | Loss: 0.00125711
Iteration 12/25 | Loss: 0.00124724
Iteration 13/25 | Loss: 0.00124622
Iteration 14/25 | Loss: 0.00124599
Iteration 15/25 | Loss: 0.00124591
Iteration 16/25 | Loss: 0.00124591
Iteration 17/25 | Loss: 0.00124591
Iteration 18/25 | Loss: 0.00124591
Iteration 19/25 | Loss: 0.00124591
Iteration 20/25 | Loss: 0.00124590
Iteration 21/25 | Loss: 0.00124590
Iteration 22/25 | Loss: 0.00124590
Iteration 23/25 | Loss: 0.00124590
Iteration 24/25 | Loss: 0.00124590
Iteration 25/25 | Loss: 0.00124590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32032454
Iteration 2/25 | Loss: 0.00108546
Iteration 3/25 | Loss: 0.00108546
Iteration 4/25 | Loss: 0.00108546
Iteration 5/25 | Loss: 0.00108546
Iteration 6/25 | Loss: 0.00108546
Iteration 7/25 | Loss: 0.00108546
Iteration 8/25 | Loss: 0.00108546
Iteration 9/25 | Loss: 0.00108546
Iteration 10/25 | Loss: 0.00108546
Iteration 11/25 | Loss: 0.00108546
Iteration 12/25 | Loss: 0.00108546
Iteration 13/25 | Loss: 0.00108546
Iteration 14/25 | Loss: 0.00108546
Iteration 15/25 | Loss: 0.00108546
Iteration 16/25 | Loss: 0.00108546
Iteration 17/25 | Loss: 0.00108546
Iteration 18/25 | Loss: 0.00108546
Iteration 19/25 | Loss: 0.00108546
Iteration 20/25 | Loss: 0.00108546
Iteration 21/25 | Loss: 0.00108546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001085457974113524, 0.001085457974113524, 0.001085457974113524, 0.001085457974113524, 0.001085457974113524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001085457974113524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108546
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00002000
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001720
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001495
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001438
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001432
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001430
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001422
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001380
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001379
Iteration 69/1000 | Loss: 0.00001379
Iteration 70/1000 | Loss: 0.00001379
Iteration 71/1000 | Loss: 0.00001379
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001377
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001376
Iteration 80/1000 | Loss: 0.00001376
Iteration 81/1000 | Loss: 0.00001376
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001375
Iteration 84/1000 | Loss: 0.00001375
Iteration 85/1000 | Loss: 0.00001375
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Iteration 152/1000 | Loss: 0.00001373
Iteration 153/1000 | Loss: 0.00001373
Iteration 154/1000 | Loss: 0.00001373
Iteration 155/1000 | Loss: 0.00001373
Iteration 156/1000 | Loss: 0.00001373
Iteration 157/1000 | Loss: 0.00001373
Iteration 158/1000 | Loss: 0.00001373
Iteration 159/1000 | Loss: 0.00001373
Iteration 160/1000 | Loss: 0.00001373
Iteration 161/1000 | Loss: 0.00001373
Iteration 162/1000 | Loss: 0.00001373
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001373
Iteration 166/1000 | Loss: 0.00001373
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001373
Iteration 169/1000 | Loss: 0.00001373
Iteration 170/1000 | Loss: 0.00001373
Iteration 171/1000 | Loss: 0.00001373
Iteration 172/1000 | Loss: 0.00001373
Iteration 173/1000 | Loss: 0.00001373
Iteration 174/1000 | Loss: 0.00001373
Iteration 175/1000 | Loss: 0.00001373
Iteration 176/1000 | Loss: 0.00001373
Iteration 177/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3725829376198817e-05, 1.3725829376198817e-05, 1.3725829376198817e-05, 1.3725829376198817e-05, 1.3725829376198817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3725829376198817e-05

Optimization complete. Final v2v error: 3.1646342277526855 mm

Highest mean error: 3.301254987716675 mm for frame 65

Lowest mean error: 3.0383243560791016 mm for frame 134

Saving results

Total time: 55.267138719558716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722608
Iteration 2/25 | Loss: 0.00141261
Iteration 3/25 | Loss: 0.00130679
Iteration 4/25 | Loss: 0.00126757
Iteration 5/25 | Loss: 0.00126961
Iteration 6/25 | Loss: 0.00124697
Iteration 7/25 | Loss: 0.00124392
Iteration 8/25 | Loss: 0.00123929
Iteration 9/25 | Loss: 0.00123872
Iteration 10/25 | Loss: 0.00123850
Iteration 11/25 | Loss: 0.00123839
Iteration 12/25 | Loss: 0.00123839
Iteration 13/25 | Loss: 0.00123839
Iteration 14/25 | Loss: 0.00123838
Iteration 15/25 | Loss: 0.00123838
Iteration 16/25 | Loss: 0.00123838
Iteration 17/25 | Loss: 0.00123838
Iteration 18/25 | Loss: 0.00123838
Iteration 19/25 | Loss: 0.00123838
Iteration 20/25 | Loss: 0.00123838
Iteration 21/25 | Loss: 0.00123838
Iteration 22/25 | Loss: 0.00123838
Iteration 23/25 | Loss: 0.00123838
Iteration 24/25 | Loss: 0.00123838
Iteration 25/25 | Loss: 0.00123837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69894290
Iteration 2/25 | Loss: 0.00149081
Iteration 3/25 | Loss: 0.00149077
Iteration 4/25 | Loss: 0.00149077
Iteration 5/25 | Loss: 0.00149077
Iteration 6/25 | Loss: 0.00149077
Iteration 7/25 | Loss: 0.00149077
Iteration 8/25 | Loss: 0.00149077
Iteration 9/25 | Loss: 0.00149077
Iteration 10/25 | Loss: 0.00149077
Iteration 11/25 | Loss: 0.00149077
Iteration 12/25 | Loss: 0.00149077
Iteration 13/25 | Loss: 0.00149077
Iteration 14/25 | Loss: 0.00149077
Iteration 15/25 | Loss: 0.00149077
Iteration 16/25 | Loss: 0.00149077
Iteration 17/25 | Loss: 0.00149077
Iteration 18/25 | Loss: 0.00149077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014907678123563528, 0.0014907678123563528, 0.0014907678123563528, 0.0014907678123563528, 0.0014907678123563528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014907678123563528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149077
Iteration 2/1000 | Loss: 0.00004101
Iteration 3/1000 | Loss: 0.00018613
Iteration 4/1000 | Loss: 0.00024214
Iteration 5/1000 | Loss: 0.00024120
Iteration 6/1000 | Loss: 0.00004561
Iteration 7/1000 | Loss: 0.00002343
Iteration 8/1000 | Loss: 0.00012601
Iteration 9/1000 | Loss: 0.00019777
Iteration 10/1000 | Loss: 0.00006548
Iteration 11/1000 | Loss: 0.00014320
Iteration 12/1000 | Loss: 0.00014448
Iteration 13/1000 | Loss: 0.00013171
Iteration 14/1000 | Loss: 0.00013496
Iteration 15/1000 | Loss: 0.00012874
Iteration 16/1000 | Loss: 0.00013234
Iteration 17/1000 | Loss: 0.00009632
Iteration 18/1000 | Loss: 0.00012548
Iteration 19/1000 | Loss: 0.00022262
Iteration 20/1000 | Loss: 0.00019959
Iteration 21/1000 | Loss: 0.00014538
Iteration 22/1000 | Loss: 0.00019824
Iteration 23/1000 | Loss: 0.00025784
Iteration 24/1000 | Loss: 0.00002725
Iteration 25/1000 | Loss: 0.00025800
Iteration 26/1000 | Loss: 0.00025968
Iteration 27/1000 | Loss: 0.00009017
Iteration 28/1000 | Loss: 0.00011575
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00016212
Iteration 31/1000 | Loss: 0.00012877
Iteration 32/1000 | Loss: 0.00016916
Iteration 33/1000 | Loss: 0.00021409
Iteration 34/1000 | Loss: 0.00012248
Iteration 35/1000 | Loss: 0.00013185
Iteration 36/1000 | Loss: 0.00011102
Iteration 37/1000 | Loss: 0.00012964
Iteration 38/1000 | Loss: 0.00008416
Iteration 39/1000 | Loss: 0.00015914
Iteration 40/1000 | Loss: 0.00017047
Iteration 41/1000 | Loss: 0.00004172
Iteration 42/1000 | Loss: 0.00002126
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001526
Iteration 50/1000 | Loss: 0.00001520
Iteration 51/1000 | Loss: 0.00001510
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001473
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001471
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001470
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001464
Iteration 82/1000 | Loss: 0.00001464
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001462
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001461
Iteration 87/1000 | Loss: 0.00001461
Iteration 88/1000 | Loss: 0.00001460
Iteration 89/1000 | Loss: 0.00001460
Iteration 90/1000 | Loss: 0.00001460
Iteration 91/1000 | Loss: 0.00001460
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001459
Iteration 94/1000 | Loss: 0.00001459
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001455
Iteration 109/1000 | Loss: 0.00001455
Iteration 110/1000 | Loss: 0.00001455
Iteration 111/1000 | Loss: 0.00001455
Iteration 112/1000 | Loss: 0.00001455
Iteration 113/1000 | Loss: 0.00001455
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001455
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Iteration 138/1000 | Loss: 0.00001455
Iteration 139/1000 | Loss: 0.00001455
Iteration 140/1000 | Loss: 0.00001455
Iteration 141/1000 | Loss: 0.00001455
Iteration 142/1000 | Loss: 0.00001455
Iteration 143/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4547248611052055e-05, 1.4547248611052055e-05, 1.4547248611052055e-05, 1.4547248611052055e-05, 1.4547248611052055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4547248611052055e-05

Optimization complete. Final v2v error: 3.2748188972473145 mm

Highest mean error: 3.8716073036193848 mm for frame 20

Lowest mean error: 2.9681873321533203 mm for frame 128

Saving results

Total time: 98.78911972045898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401673
Iteration 2/25 | Loss: 0.00128750
Iteration 3/25 | Loss: 0.00123109
Iteration 4/25 | Loss: 0.00122362
Iteration 5/25 | Loss: 0.00122172
Iteration 6/25 | Loss: 0.00122172
Iteration 7/25 | Loss: 0.00122172
Iteration 8/25 | Loss: 0.00122172
Iteration 9/25 | Loss: 0.00122172
Iteration 10/25 | Loss: 0.00122172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012217175681144, 0.0012217175681144, 0.0012217175681144, 0.0012217175681144, 0.0012217175681144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012217175681144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96733785
Iteration 2/25 | Loss: 0.00098347
Iteration 3/25 | Loss: 0.00098346
Iteration 4/25 | Loss: 0.00098346
Iteration 5/25 | Loss: 0.00098345
Iteration 6/25 | Loss: 0.00098345
Iteration 7/25 | Loss: 0.00098345
Iteration 8/25 | Loss: 0.00098345
Iteration 9/25 | Loss: 0.00098345
Iteration 10/25 | Loss: 0.00098345
Iteration 11/25 | Loss: 0.00098345
Iteration 12/25 | Loss: 0.00098345
Iteration 13/25 | Loss: 0.00098345
Iteration 14/25 | Loss: 0.00098345
Iteration 15/25 | Loss: 0.00098345
Iteration 16/25 | Loss: 0.00098345
Iteration 17/25 | Loss: 0.00098345
Iteration 18/25 | Loss: 0.00098345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000983452657237649, 0.000983452657237649, 0.000983452657237649, 0.000983452657237649, 0.000983452657237649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983452657237649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098345
Iteration 2/1000 | Loss: 0.00002003
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001278
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001235
Iteration 10/1000 | Loss: 0.00001210
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001183
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001163
Iteration 19/1000 | Loss: 0.00001159
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001154
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001153
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001140
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001138
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001133
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001130
Iteration 52/1000 | Loss: 0.00001130
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001127
Iteration 69/1000 | Loss: 0.00001127
Iteration 70/1000 | Loss: 0.00001127
Iteration 71/1000 | Loss: 0.00001127
Iteration 72/1000 | Loss: 0.00001127
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001125
Iteration 79/1000 | Loss: 0.00001125
Iteration 80/1000 | Loss: 0.00001125
Iteration 81/1000 | Loss: 0.00001124
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001123
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001122
Iteration 88/1000 | Loss: 0.00001122
Iteration 89/1000 | Loss: 0.00001122
Iteration 90/1000 | Loss: 0.00001122
Iteration 91/1000 | Loss: 0.00001122
Iteration 92/1000 | Loss: 0.00001121
Iteration 93/1000 | Loss: 0.00001121
Iteration 94/1000 | Loss: 0.00001119
Iteration 95/1000 | Loss: 0.00001119
Iteration 96/1000 | Loss: 0.00001119
Iteration 97/1000 | Loss: 0.00001119
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001114
Iteration 108/1000 | Loss: 0.00001114
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001113
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001112
Iteration 116/1000 | Loss: 0.00001112
Iteration 117/1000 | Loss: 0.00001112
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001111
Iteration 122/1000 | Loss: 0.00001111
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001104
Iteration 161/1000 | Loss: 0.00001104
Iteration 162/1000 | Loss: 0.00001104
Iteration 163/1000 | Loss: 0.00001104
Iteration 164/1000 | Loss: 0.00001104
Iteration 165/1000 | Loss: 0.00001104
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001103
Iteration 168/1000 | Loss: 0.00001103
Iteration 169/1000 | Loss: 0.00001103
Iteration 170/1000 | Loss: 0.00001103
Iteration 171/1000 | Loss: 0.00001103
Iteration 172/1000 | Loss: 0.00001103
Iteration 173/1000 | Loss: 0.00001103
Iteration 174/1000 | Loss: 0.00001103
Iteration 175/1000 | Loss: 0.00001103
Iteration 176/1000 | Loss: 0.00001103
Iteration 177/1000 | Loss: 0.00001103
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001103
Iteration 181/1000 | Loss: 0.00001103
Iteration 182/1000 | Loss: 0.00001103
Iteration 183/1000 | Loss: 0.00001103
Iteration 184/1000 | Loss: 0.00001103
Iteration 185/1000 | Loss: 0.00001103
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001102
Iteration 188/1000 | Loss: 0.00001102
Iteration 189/1000 | Loss: 0.00001102
Iteration 190/1000 | Loss: 0.00001102
Iteration 191/1000 | Loss: 0.00001102
Iteration 192/1000 | Loss: 0.00001102
Iteration 193/1000 | Loss: 0.00001102
Iteration 194/1000 | Loss: 0.00001102
Iteration 195/1000 | Loss: 0.00001102
Iteration 196/1000 | Loss: 0.00001102
Iteration 197/1000 | Loss: 0.00001102
Iteration 198/1000 | Loss: 0.00001102
Iteration 199/1000 | Loss: 0.00001102
Iteration 200/1000 | Loss: 0.00001102
Iteration 201/1000 | Loss: 0.00001102
Iteration 202/1000 | Loss: 0.00001102
Iteration 203/1000 | Loss: 0.00001102
Iteration 204/1000 | Loss: 0.00001102
Iteration 205/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1024213563359808e-05, 1.1024213563359808e-05, 1.1024213563359808e-05, 1.1024213563359808e-05, 1.1024213563359808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1024213563359808e-05

Optimization complete. Final v2v error: 2.8592331409454346 mm

Highest mean error: 3.172410249710083 mm for frame 81

Lowest mean error: 2.7397806644439697 mm for frame 142

Saving results

Total time: 40.7366201877594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796864
Iteration 2/25 | Loss: 0.00161785
Iteration 3/25 | Loss: 0.00144405
Iteration 4/25 | Loss: 0.00136806
Iteration 5/25 | Loss: 0.00135960
Iteration 6/25 | Loss: 0.00135679
Iteration 7/25 | Loss: 0.00135615
Iteration 8/25 | Loss: 0.00135607
Iteration 9/25 | Loss: 0.00135607
Iteration 10/25 | Loss: 0.00135607
Iteration 11/25 | Loss: 0.00135607
Iteration 12/25 | Loss: 0.00135607
Iteration 13/25 | Loss: 0.00135607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013560678344219923, 0.0013560678344219923, 0.0013560678344219923, 0.0013560678344219923, 0.0013560678344219923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013560678344219923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.12582874
Iteration 2/25 | Loss: 0.00142198
Iteration 3/25 | Loss: 0.00142198
Iteration 4/25 | Loss: 0.00134771
Iteration 5/25 | Loss: 0.00134771
Iteration 6/25 | Loss: 0.00134771
Iteration 7/25 | Loss: 0.00134771
Iteration 8/25 | Loss: 0.00134771
Iteration 9/25 | Loss: 0.00134771
Iteration 10/25 | Loss: 0.00134771
Iteration 11/25 | Loss: 0.00134771
Iteration 12/25 | Loss: 0.00134771
Iteration 13/25 | Loss: 0.00134771
Iteration 14/25 | Loss: 0.00134771
Iteration 15/25 | Loss: 0.00134771
Iteration 16/25 | Loss: 0.00134771
Iteration 17/25 | Loss: 0.00134771
Iteration 18/25 | Loss: 0.00134771
Iteration 19/25 | Loss: 0.00134771
Iteration 20/25 | Loss: 0.00134771
Iteration 21/25 | Loss: 0.00134771
Iteration 22/25 | Loss: 0.00134771
Iteration 23/25 | Loss: 0.00134771
Iteration 24/25 | Loss: 0.00134771
Iteration 25/25 | Loss: 0.00134771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134771
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00011846
Iteration 4/1000 | Loss: 0.00004331
Iteration 5/1000 | Loss: 0.00002754
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00021651
Iteration 9/1000 | Loss: 0.00012150
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002299
Iteration 12/1000 | Loss: 0.00002277
Iteration 13/1000 | Loss: 0.00022214
Iteration 14/1000 | Loss: 0.00013816
Iteration 15/1000 | Loss: 0.00015338
Iteration 16/1000 | Loss: 0.00002309
Iteration 17/1000 | Loss: 0.00002255
Iteration 18/1000 | Loss: 0.00021361
Iteration 19/1000 | Loss: 0.00010010
Iteration 20/1000 | Loss: 0.00004032
Iteration 21/1000 | Loss: 0.00002243
Iteration 22/1000 | Loss: 0.00002224
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00002193
Iteration 26/1000 | Loss: 0.00002186
Iteration 27/1000 | Loss: 0.00013097
Iteration 28/1000 | Loss: 0.00002300
Iteration 29/1000 | Loss: 0.00002182
Iteration 30/1000 | Loss: 0.00002176
Iteration 31/1000 | Loss: 0.00002176
Iteration 32/1000 | Loss: 0.00002176
Iteration 33/1000 | Loss: 0.00002176
Iteration 34/1000 | Loss: 0.00002176
Iteration 35/1000 | Loss: 0.00002176
Iteration 36/1000 | Loss: 0.00002176
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002172
Iteration 39/1000 | Loss: 0.00002171
Iteration 40/1000 | Loss: 0.00002171
Iteration 41/1000 | Loss: 0.00002170
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002168
Iteration 50/1000 | Loss: 0.00002168
Iteration 51/1000 | Loss: 0.00002168
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002167
Iteration 54/1000 | Loss: 0.00002166
Iteration 55/1000 | Loss: 0.00002166
Iteration 56/1000 | Loss: 0.00002165
Iteration 57/1000 | Loss: 0.00002165
Iteration 58/1000 | Loss: 0.00002164
Iteration 59/1000 | Loss: 0.00002164
Iteration 60/1000 | Loss: 0.00002163
Iteration 61/1000 | Loss: 0.00002163
Iteration 62/1000 | Loss: 0.00002162
Iteration 63/1000 | Loss: 0.00002162
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002157
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002156
Iteration 71/1000 | Loss: 0.00002155
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002155
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002152
Iteration 81/1000 | Loss: 0.00002151
Iteration 82/1000 | Loss: 0.00002151
Iteration 83/1000 | Loss: 0.00002151
Iteration 84/1000 | Loss: 0.00002150
Iteration 85/1000 | Loss: 0.00002150
Iteration 86/1000 | Loss: 0.00002150
Iteration 87/1000 | Loss: 0.00002150
Iteration 88/1000 | Loss: 0.00002149
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002149
Iteration 92/1000 | Loss: 0.00002149
Iteration 93/1000 | Loss: 0.00002149
Iteration 94/1000 | Loss: 0.00002149
Iteration 95/1000 | Loss: 0.00002149
Iteration 96/1000 | Loss: 0.00002149
Iteration 97/1000 | Loss: 0.00002149
Iteration 98/1000 | Loss: 0.00002149
Iteration 99/1000 | Loss: 0.00002149
Iteration 100/1000 | Loss: 0.00002149
Iteration 101/1000 | Loss: 0.00022965
Iteration 102/1000 | Loss: 0.00002579
Iteration 103/1000 | Loss: 0.00003750
Iteration 104/1000 | Loss: 0.00002183
Iteration 105/1000 | Loss: 0.00002156
Iteration 106/1000 | Loss: 0.00002143
Iteration 107/1000 | Loss: 0.00002142
Iteration 108/1000 | Loss: 0.00002141
Iteration 109/1000 | Loss: 0.00002141
Iteration 110/1000 | Loss: 0.00002141
Iteration 111/1000 | Loss: 0.00002141
Iteration 112/1000 | Loss: 0.00002141
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00002139
Iteration 118/1000 | Loss: 0.00002139
Iteration 119/1000 | Loss: 0.00002139
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002137
Iteration 124/1000 | Loss: 0.00002137
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002136
Iteration 127/1000 | Loss: 0.00002136
Iteration 128/1000 | Loss: 0.00002136
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002136
Iteration 131/1000 | Loss: 0.00002136
Iteration 132/1000 | Loss: 0.00002136
Iteration 133/1000 | Loss: 0.00002136
Iteration 134/1000 | Loss: 0.00002136
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.135683098458685e-05, 2.135683098458685e-05, 2.135683098458685e-05, 2.135683098458685e-05, 2.135683098458685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.135683098458685e-05

Optimization complete. Final v2v error: 3.8831076622009277 mm

Highest mean error: 4.650997638702393 mm for frame 133

Lowest mean error: 3.291184902191162 mm for frame 177

Saving results

Total time: 74.05107998847961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967197
Iteration 2/25 | Loss: 0.00235653
Iteration 3/25 | Loss: 0.00189793
Iteration 4/25 | Loss: 0.00180137
Iteration 5/25 | Loss: 0.00182825
Iteration 6/25 | Loss: 0.00177515
Iteration 7/25 | Loss: 0.00173319
Iteration 8/25 | Loss: 0.00173864
Iteration 9/25 | Loss: 0.00169152
Iteration 10/25 | Loss: 0.00162319
Iteration 11/25 | Loss: 0.00157352
Iteration 12/25 | Loss: 0.00154028
Iteration 13/25 | Loss: 0.00152487
Iteration 14/25 | Loss: 0.00151936
Iteration 15/25 | Loss: 0.00151137
Iteration 16/25 | Loss: 0.00147283
Iteration 17/25 | Loss: 0.00144861
Iteration 18/25 | Loss: 0.00144645
Iteration 19/25 | Loss: 0.00144922
Iteration 20/25 | Loss: 0.00144941
Iteration 21/25 | Loss: 0.00145389
Iteration 22/25 | Loss: 0.00144416
Iteration 23/25 | Loss: 0.00143733
Iteration 24/25 | Loss: 0.00145056
Iteration 25/25 | Loss: 0.00144965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39388049
Iteration 2/25 | Loss: 0.00626537
Iteration 3/25 | Loss: 0.00325489
Iteration 4/25 | Loss: 0.00325489
Iteration 5/25 | Loss: 0.00325489
Iteration 6/25 | Loss: 0.00325489
Iteration 7/25 | Loss: 0.00325489
Iteration 8/25 | Loss: 0.00325489
Iteration 9/25 | Loss: 0.00325489
Iteration 10/25 | Loss: 0.00325489
Iteration 11/25 | Loss: 0.00325489
Iteration 12/25 | Loss: 0.00325489
Iteration 13/25 | Loss: 0.00325489
Iteration 14/25 | Loss: 0.00325489
Iteration 15/25 | Loss: 0.00325489
Iteration 16/25 | Loss: 0.00325489
Iteration 17/25 | Loss: 0.00325489
Iteration 18/25 | Loss: 0.00325489
Iteration 19/25 | Loss: 0.00325489
Iteration 20/25 | Loss: 0.00325489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003254887880757451, 0.003254887880757451, 0.003254887880757451, 0.003254887880757451, 0.003254887880757451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003254887880757451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325489
Iteration 2/1000 | Loss: 0.00222419
Iteration 3/1000 | Loss: 0.00154880
Iteration 4/1000 | Loss: 0.00093142
Iteration 5/1000 | Loss: 0.00059962
Iteration 6/1000 | Loss: 0.00143049
Iteration 7/1000 | Loss: 0.00102764
Iteration 8/1000 | Loss: 0.00132125
Iteration 9/1000 | Loss: 0.00055467
Iteration 10/1000 | Loss: 0.00122289
Iteration 11/1000 | Loss: 0.00172526
Iteration 12/1000 | Loss: 0.00242041
Iteration 13/1000 | Loss: 0.00140661
Iteration 14/1000 | Loss: 0.00125938
Iteration 15/1000 | Loss: 0.00132274
Iteration 16/1000 | Loss: 0.00180041
Iteration 17/1000 | Loss: 0.00151835
Iteration 18/1000 | Loss: 0.00072576
Iteration 19/1000 | Loss: 0.00031881
Iteration 20/1000 | Loss: 0.00042339
Iteration 21/1000 | Loss: 0.00065088
Iteration 22/1000 | Loss: 0.00080336
Iteration 23/1000 | Loss: 0.00198006
Iteration 24/1000 | Loss: 0.00076819
Iteration 25/1000 | Loss: 0.00068086
Iteration 26/1000 | Loss: 0.00084307
Iteration 27/1000 | Loss: 0.00083253
Iteration 28/1000 | Loss: 0.00045886
Iteration 29/1000 | Loss: 0.00079093
Iteration 30/1000 | Loss: 0.00065214
Iteration 31/1000 | Loss: 0.00091982
Iteration 32/1000 | Loss: 0.00076425
Iteration 33/1000 | Loss: 0.00052971
Iteration 34/1000 | Loss: 0.00055213
Iteration 35/1000 | Loss: 0.00054338
Iteration 36/1000 | Loss: 0.00057730
Iteration 37/1000 | Loss: 0.00037393
Iteration 38/1000 | Loss: 0.00039863
Iteration 39/1000 | Loss: 0.00052793
Iteration 40/1000 | Loss: 0.00044819
Iteration 41/1000 | Loss: 0.00056925
Iteration 42/1000 | Loss: 0.00057169
Iteration 43/1000 | Loss: 0.00048973
Iteration 44/1000 | Loss: 0.00046895
Iteration 45/1000 | Loss: 0.00058024
Iteration 46/1000 | Loss: 0.00049492
Iteration 47/1000 | Loss: 0.00044798
Iteration 48/1000 | Loss: 0.00053108
Iteration 49/1000 | Loss: 0.00091422
Iteration 50/1000 | Loss: 0.00065180
Iteration 51/1000 | Loss: 0.00069610
Iteration 52/1000 | Loss: 0.00094221
Iteration 53/1000 | Loss: 0.00197463
Iteration 54/1000 | Loss: 0.00172179
Iteration 55/1000 | Loss: 0.00247058
Iteration 56/1000 | Loss: 0.00136748
Iteration 57/1000 | Loss: 0.00138293
Iteration 58/1000 | Loss: 0.00166817
Iteration 59/1000 | Loss: 0.00064870
Iteration 60/1000 | Loss: 0.00075770
Iteration 61/1000 | Loss: 0.00052214
Iteration 62/1000 | Loss: 0.00095925
Iteration 63/1000 | Loss: 0.00062043
Iteration 64/1000 | Loss: 0.00064630
Iteration 65/1000 | Loss: 0.00052948
Iteration 66/1000 | Loss: 0.00059656
Iteration 67/1000 | Loss: 0.00065772
Iteration 68/1000 | Loss: 0.00054226
Iteration 69/1000 | Loss: 0.00111996
Iteration 70/1000 | Loss: 0.00057081
Iteration 71/1000 | Loss: 0.00061001
Iteration 72/1000 | Loss: 0.00048603
Iteration 73/1000 | Loss: 0.00066238
Iteration 74/1000 | Loss: 0.00054822
Iteration 75/1000 | Loss: 0.00055075
Iteration 76/1000 | Loss: 0.00058152
Iteration 77/1000 | Loss: 0.00060992
Iteration 78/1000 | Loss: 0.00056502
Iteration 79/1000 | Loss: 0.00042355
Iteration 80/1000 | Loss: 0.00063271
Iteration 81/1000 | Loss: 0.00044185
Iteration 82/1000 | Loss: 0.00048391
Iteration 83/1000 | Loss: 0.00023619
Iteration 84/1000 | Loss: 0.00027659
Iteration 85/1000 | Loss: 0.00026725
Iteration 86/1000 | Loss: 0.00024794
Iteration 87/1000 | Loss: 0.00020350
Iteration 88/1000 | Loss: 0.00048673
Iteration 89/1000 | Loss: 0.00041116
Iteration 90/1000 | Loss: 0.00053850
Iteration 91/1000 | Loss: 0.00048168
Iteration 92/1000 | Loss: 0.00020956
Iteration 93/1000 | Loss: 0.00020376
Iteration 94/1000 | Loss: 0.00040517
Iteration 95/1000 | Loss: 0.00014308
Iteration 96/1000 | Loss: 0.00010181
Iteration 97/1000 | Loss: 0.00009062
Iteration 98/1000 | Loss: 0.00015281
Iteration 99/1000 | Loss: 0.00011223
Iteration 100/1000 | Loss: 0.00008155
Iteration 101/1000 | Loss: 0.00007784
Iteration 102/1000 | Loss: 0.00007592
Iteration 103/1000 | Loss: 0.00165983
Iteration 104/1000 | Loss: 0.00109603
Iteration 105/1000 | Loss: 0.00115279
Iteration 106/1000 | Loss: 0.00375453
Iteration 107/1000 | Loss: 0.00116995
Iteration 108/1000 | Loss: 0.00124641
Iteration 109/1000 | Loss: 0.00230199
Iteration 110/1000 | Loss: 0.00152228
Iteration 111/1000 | Loss: 0.00376192
Iteration 112/1000 | Loss: 0.00199689
Iteration 113/1000 | Loss: 0.00293247
Iteration 114/1000 | Loss: 0.00117684
Iteration 115/1000 | Loss: 0.00050317
Iteration 116/1000 | Loss: 0.00028952
Iteration 117/1000 | Loss: 0.00009719
Iteration 118/1000 | Loss: 0.00149142
Iteration 119/1000 | Loss: 0.00100391
Iteration 120/1000 | Loss: 0.00140893
Iteration 121/1000 | Loss: 0.00133227
Iteration 122/1000 | Loss: 0.00151956
Iteration 123/1000 | Loss: 0.00180219
Iteration 124/1000 | Loss: 0.00113205
Iteration 125/1000 | Loss: 0.00089022
Iteration 126/1000 | Loss: 0.00018775
Iteration 127/1000 | Loss: 0.00022057
Iteration 128/1000 | Loss: 0.00008311
Iteration 129/1000 | Loss: 0.00017968
Iteration 130/1000 | Loss: 0.00059329
Iteration 131/1000 | Loss: 0.00010689
Iteration 132/1000 | Loss: 0.00006901
Iteration 133/1000 | Loss: 0.00181957
Iteration 134/1000 | Loss: 0.00322764
Iteration 135/1000 | Loss: 0.00142428
Iteration 136/1000 | Loss: 0.00194740
Iteration 137/1000 | Loss: 0.00049457
Iteration 138/1000 | Loss: 0.00010590
Iteration 139/1000 | Loss: 0.00008043
Iteration 140/1000 | Loss: 0.00046348
Iteration 141/1000 | Loss: 0.00058922
Iteration 142/1000 | Loss: 0.00079460
Iteration 143/1000 | Loss: 0.00058240
Iteration 144/1000 | Loss: 0.00035348
Iteration 145/1000 | Loss: 0.00080463
Iteration 146/1000 | Loss: 0.00057352
Iteration 147/1000 | Loss: 0.00080619
Iteration 148/1000 | Loss: 0.00062674
Iteration 149/1000 | Loss: 0.00033562
Iteration 150/1000 | Loss: 0.00099575
Iteration 151/1000 | Loss: 0.00053262
Iteration 152/1000 | Loss: 0.00048619
Iteration 153/1000 | Loss: 0.00031119
Iteration 154/1000 | Loss: 0.00046447
Iteration 155/1000 | Loss: 0.00030233
Iteration 156/1000 | Loss: 0.00033754
Iteration 157/1000 | Loss: 0.00085815
Iteration 158/1000 | Loss: 0.00006315
Iteration 159/1000 | Loss: 0.00019343
Iteration 160/1000 | Loss: 0.00070555
Iteration 161/1000 | Loss: 0.00056888
Iteration 162/1000 | Loss: 0.00052481
Iteration 163/1000 | Loss: 0.00024346
Iteration 164/1000 | Loss: 0.00029070
Iteration 165/1000 | Loss: 0.00032171
Iteration 166/1000 | Loss: 0.00014666
Iteration 167/1000 | Loss: 0.00005359
Iteration 168/1000 | Loss: 0.00005134
Iteration 169/1000 | Loss: 0.00087362
Iteration 170/1000 | Loss: 0.00044148
Iteration 171/1000 | Loss: 0.00058339
Iteration 172/1000 | Loss: 0.00039259
Iteration 173/1000 | Loss: 0.00050543
Iteration 174/1000 | Loss: 0.00025012
Iteration 175/1000 | Loss: 0.00004774
Iteration 176/1000 | Loss: 0.00008453
Iteration 177/1000 | Loss: 0.00076714
Iteration 178/1000 | Loss: 0.00046087
Iteration 179/1000 | Loss: 0.00070243
Iteration 180/1000 | Loss: 0.00044342
Iteration 181/1000 | Loss: 0.00060113
Iteration 182/1000 | Loss: 0.00022971
Iteration 183/1000 | Loss: 0.00006015
Iteration 184/1000 | Loss: 0.00026571
Iteration 185/1000 | Loss: 0.00005732
Iteration 186/1000 | Loss: 0.00006960
Iteration 187/1000 | Loss: 0.00094544
Iteration 188/1000 | Loss: 0.00039171
Iteration 189/1000 | Loss: 0.00054017
Iteration 190/1000 | Loss: 0.00021971
Iteration 191/1000 | Loss: 0.00083695
Iteration 192/1000 | Loss: 0.00131868
Iteration 193/1000 | Loss: 0.00085298
Iteration 194/1000 | Loss: 0.00095300
Iteration 195/1000 | Loss: 0.00005821
Iteration 196/1000 | Loss: 0.00004497
Iteration 197/1000 | Loss: 0.00020275
Iteration 198/1000 | Loss: 0.00134446
Iteration 199/1000 | Loss: 0.00014907
Iteration 200/1000 | Loss: 0.00028940
Iteration 201/1000 | Loss: 0.00058665
Iteration 202/1000 | Loss: 0.00027783
Iteration 203/1000 | Loss: 0.00036416
Iteration 204/1000 | Loss: 0.00067706
Iteration 205/1000 | Loss: 0.00005549
Iteration 206/1000 | Loss: 0.00063534
Iteration 207/1000 | Loss: 0.00003827
Iteration 208/1000 | Loss: 0.00016501
Iteration 209/1000 | Loss: 0.00244568
Iteration 210/1000 | Loss: 0.00190887
Iteration 211/1000 | Loss: 0.00285643
Iteration 212/1000 | Loss: 0.00214001
Iteration 213/1000 | Loss: 0.00089985
Iteration 214/1000 | Loss: 0.00153845
Iteration 215/1000 | Loss: 0.00079410
Iteration 216/1000 | Loss: 0.00093263
Iteration 217/1000 | Loss: 0.00008894
Iteration 218/1000 | Loss: 0.00053602
Iteration 219/1000 | Loss: 0.00028175
Iteration 220/1000 | Loss: 0.00130498
Iteration 221/1000 | Loss: 0.00044741
Iteration 222/1000 | Loss: 0.00025346
Iteration 223/1000 | Loss: 0.00027397
Iteration 224/1000 | Loss: 0.00026318
Iteration 225/1000 | Loss: 0.00027069
Iteration 226/1000 | Loss: 0.00037591
Iteration 227/1000 | Loss: 0.00015916
Iteration 228/1000 | Loss: 0.00003680
Iteration 229/1000 | Loss: 0.00064083
Iteration 230/1000 | Loss: 0.00036441
Iteration 231/1000 | Loss: 0.00007135
Iteration 232/1000 | Loss: 0.00024465
Iteration 233/1000 | Loss: 0.00003208
Iteration 234/1000 | Loss: 0.00003075
Iteration 235/1000 | Loss: 0.00025621
Iteration 236/1000 | Loss: 0.00125240
Iteration 237/1000 | Loss: 0.00005196
Iteration 238/1000 | Loss: 0.00003019
Iteration 239/1000 | Loss: 0.00002875
Iteration 240/1000 | Loss: 0.00035065
Iteration 241/1000 | Loss: 0.00007499
Iteration 242/1000 | Loss: 0.00103200
Iteration 243/1000 | Loss: 0.00084929
Iteration 244/1000 | Loss: 0.00059314
Iteration 245/1000 | Loss: 0.00051502
Iteration 246/1000 | Loss: 0.00068892
Iteration 247/1000 | Loss: 0.00038867
Iteration 248/1000 | Loss: 0.00039825
Iteration 249/1000 | Loss: 0.00009473
Iteration 250/1000 | Loss: 0.00003340
Iteration 251/1000 | Loss: 0.00010994
Iteration 252/1000 | Loss: 0.00100068
Iteration 253/1000 | Loss: 0.00087379
Iteration 254/1000 | Loss: 0.00003537
Iteration 255/1000 | Loss: 0.00003008
Iteration 256/1000 | Loss: 0.00113855
Iteration 257/1000 | Loss: 0.00012985
Iteration 258/1000 | Loss: 0.00010730
Iteration 259/1000 | Loss: 0.00014916
Iteration 260/1000 | Loss: 0.00003503
Iteration 261/1000 | Loss: 0.00002812
Iteration 262/1000 | Loss: 0.00002495
Iteration 263/1000 | Loss: 0.00002286
Iteration 264/1000 | Loss: 0.00002180
Iteration 265/1000 | Loss: 0.00002048
Iteration 266/1000 | Loss: 0.00001924
Iteration 267/1000 | Loss: 0.00001846
Iteration 268/1000 | Loss: 0.00001805
Iteration 269/1000 | Loss: 0.00001780
Iteration 270/1000 | Loss: 0.00001768
Iteration 271/1000 | Loss: 0.00001767
Iteration 272/1000 | Loss: 0.00001762
Iteration 273/1000 | Loss: 0.00001758
Iteration 274/1000 | Loss: 0.00001757
Iteration 275/1000 | Loss: 0.00001736
Iteration 276/1000 | Loss: 0.00001723
Iteration 277/1000 | Loss: 0.00001715
Iteration 278/1000 | Loss: 0.00001711
Iteration 279/1000 | Loss: 0.00001711
Iteration 280/1000 | Loss: 0.00001709
Iteration 281/1000 | Loss: 0.00001709
Iteration 282/1000 | Loss: 0.00001709
Iteration 283/1000 | Loss: 0.00001708
Iteration 284/1000 | Loss: 0.00001708
Iteration 285/1000 | Loss: 0.00001706
Iteration 286/1000 | Loss: 0.00001706
Iteration 287/1000 | Loss: 0.00001704
Iteration 288/1000 | Loss: 0.00001703
Iteration 289/1000 | Loss: 0.00001703
Iteration 290/1000 | Loss: 0.00001702
Iteration 291/1000 | Loss: 0.00001702
Iteration 292/1000 | Loss: 0.00001702
Iteration 293/1000 | Loss: 0.00001701
Iteration 294/1000 | Loss: 0.00001701
Iteration 295/1000 | Loss: 0.00001700
Iteration 296/1000 | Loss: 0.00001700
Iteration 297/1000 | Loss: 0.00001700
Iteration 298/1000 | Loss: 0.00001699
Iteration 299/1000 | Loss: 0.00001699
Iteration 300/1000 | Loss: 0.00001699
Iteration 301/1000 | Loss: 0.00001698
Iteration 302/1000 | Loss: 0.00001698
Iteration 303/1000 | Loss: 0.00092655
Iteration 304/1000 | Loss: 0.00037795
Iteration 305/1000 | Loss: 0.00112185
Iteration 306/1000 | Loss: 0.00033376
Iteration 307/1000 | Loss: 0.00001780
Iteration 308/1000 | Loss: 0.00012081
Iteration 309/1000 | Loss: 0.00035811
Iteration 310/1000 | Loss: 0.00027966
Iteration 311/1000 | Loss: 0.00001956
Iteration 312/1000 | Loss: 0.00001576
Iteration 313/1000 | Loss: 0.00001500
Iteration 314/1000 | Loss: 0.00012218
Iteration 315/1000 | Loss: 0.00014442
Iteration 316/1000 | Loss: 0.00001841
Iteration 317/1000 | Loss: 0.00010031
Iteration 318/1000 | Loss: 0.00001777
Iteration 319/1000 | Loss: 0.00001603
Iteration 320/1000 | Loss: 0.00005858
Iteration 321/1000 | Loss: 0.00001429
Iteration 322/1000 | Loss: 0.00001419
Iteration 323/1000 | Loss: 0.00001417
Iteration 324/1000 | Loss: 0.00001409
Iteration 325/1000 | Loss: 0.00001398
Iteration 326/1000 | Loss: 0.00001381
Iteration 327/1000 | Loss: 0.00001378
Iteration 328/1000 | Loss: 0.00001376
Iteration 329/1000 | Loss: 0.00001375
Iteration 330/1000 | Loss: 0.00001374
Iteration 331/1000 | Loss: 0.00001374
Iteration 332/1000 | Loss: 0.00001373
Iteration 333/1000 | Loss: 0.00001372
Iteration 334/1000 | Loss: 0.00001369
Iteration 335/1000 | Loss: 0.00001368
Iteration 336/1000 | Loss: 0.00001368
Iteration 337/1000 | Loss: 0.00001367
Iteration 338/1000 | Loss: 0.00001366
Iteration 339/1000 | Loss: 0.00001363
Iteration 340/1000 | Loss: 0.00032346
Iteration 341/1000 | Loss: 0.00001388
Iteration 342/1000 | Loss: 0.00001356
Iteration 343/1000 | Loss: 0.00001356
Iteration 344/1000 | Loss: 0.00001356
Iteration 345/1000 | Loss: 0.00001355
Iteration 346/1000 | Loss: 0.00001355
Iteration 347/1000 | Loss: 0.00001355
Iteration 348/1000 | Loss: 0.00001355
Iteration 349/1000 | Loss: 0.00001355
Iteration 350/1000 | Loss: 0.00001355
Iteration 351/1000 | Loss: 0.00001355
Iteration 352/1000 | Loss: 0.00001355
Iteration 353/1000 | Loss: 0.00001355
Iteration 354/1000 | Loss: 0.00001355
Iteration 355/1000 | Loss: 0.00001355
Iteration 356/1000 | Loss: 0.00001354
Iteration 357/1000 | Loss: 0.00001354
Iteration 358/1000 | Loss: 0.00001354
Iteration 359/1000 | Loss: 0.00001354
Iteration 360/1000 | Loss: 0.00001354
Iteration 361/1000 | Loss: 0.00001354
Iteration 362/1000 | Loss: 0.00001354
Iteration 363/1000 | Loss: 0.00001354
Iteration 364/1000 | Loss: 0.00001352
Iteration 365/1000 | Loss: 0.00001352
Iteration 366/1000 | Loss: 0.00001352
Iteration 367/1000 | Loss: 0.00001352
Iteration 368/1000 | Loss: 0.00001352
Iteration 369/1000 | Loss: 0.00001352
Iteration 370/1000 | Loss: 0.00001352
Iteration 371/1000 | Loss: 0.00001352
Iteration 372/1000 | Loss: 0.00001352
Iteration 373/1000 | Loss: 0.00001351
Iteration 374/1000 | Loss: 0.00001351
Iteration 375/1000 | Loss: 0.00001351
Iteration 376/1000 | Loss: 0.00001350
Iteration 377/1000 | Loss: 0.00001350
Iteration 378/1000 | Loss: 0.00001350
Iteration 379/1000 | Loss: 0.00001350
Iteration 380/1000 | Loss: 0.00001350
Iteration 381/1000 | Loss: 0.00001350
Iteration 382/1000 | Loss: 0.00001350
Iteration 383/1000 | Loss: 0.00001350
Iteration 384/1000 | Loss: 0.00001350
Iteration 385/1000 | Loss: 0.00001350
Iteration 386/1000 | Loss: 0.00001349
Iteration 387/1000 | Loss: 0.00001349
Iteration 388/1000 | Loss: 0.00001349
Iteration 389/1000 | Loss: 0.00001349
Iteration 390/1000 | Loss: 0.00001348
Iteration 391/1000 | Loss: 0.00001348
Iteration 392/1000 | Loss: 0.00001348
Iteration 393/1000 | Loss: 0.00001348
Iteration 394/1000 | Loss: 0.00001347
Iteration 395/1000 | Loss: 0.00001347
Iteration 396/1000 | Loss: 0.00001347
Iteration 397/1000 | Loss: 0.00001347
Iteration 398/1000 | Loss: 0.00001347
Iteration 399/1000 | Loss: 0.00001347
Iteration 400/1000 | Loss: 0.00001347
Iteration 401/1000 | Loss: 0.00001347
Iteration 402/1000 | Loss: 0.00001347
Iteration 403/1000 | Loss: 0.00001347
Iteration 404/1000 | Loss: 0.00001347
Iteration 405/1000 | Loss: 0.00001347
Iteration 406/1000 | Loss: 0.00001347
Iteration 407/1000 | Loss: 0.00001347
Iteration 408/1000 | Loss: 0.00001347
Iteration 409/1000 | Loss: 0.00001347
Iteration 410/1000 | Loss: 0.00001347
Iteration 411/1000 | Loss: 0.00001347
Iteration 412/1000 | Loss: 0.00001347
Iteration 413/1000 | Loss: 0.00001346
Iteration 414/1000 | Loss: 0.00001346
Iteration 415/1000 | Loss: 0.00001346
Iteration 416/1000 | Loss: 0.00001346
Iteration 417/1000 | Loss: 0.00001346
Iteration 418/1000 | Loss: 0.00001346
Iteration 419/1000 | Loss: 0.00001346
Iteration 420/1000 | Loss: 0.00001346
Iteration 421/1000 | Loss: 0.00001346
Iteration 422/1000 | Loss: 0.00001346
Iteration 423/1000 | Loss: 0.00001346
Iteration 424/1000 | Loss: 0.00001346
Iteration 425/1000 | Loss: 0.00001346
Iteration 426/1000 | Loss: 0.00001346
Iteration 427/1000 | Loss: 0.00001346
Iteration 428/1000 | Loss: 0.00001346
Iteration 429/1000 | Loss: 0.00001346
Iteration 430/1000 | Loss: 0.00001346
Iteration 431/1000 | Loss: 0.00001346
Iteration 432/1000 | Loss: 0.00001346
Iteration 433/1000 | Loss: 0.00001346
Iteration 434/1000 | Loss: 0.00001346
Iteration 435/1000 | Loss: 0.00001346
Iteration 436/1000 | Loss: 0.00001346
Iteration 437/1000 | Loss: 0.00001346
Iteration 438/1000 | Loss: 0.00001346
Iteration 439/1000 | Loss: 0.00001346
Iteration 440/1000 | Loss: 0.00001346
Iteration 441/1000 | Loss: 0.00001346
Iteration 442/1000 | Loss: 0.00001346
Iteration 443/1000 | Loss: 0.00001346
Iteration 444/1000 | Loss: 0.00001346
Iteration 445/1000 | Loss: 0.00001346
Iteration 446/1000 | Loss: 0.00001346
Iteration 447/1000 | Loss: 0.00001346
Iteration 448/1000 | Loss: 0.00001346
Iteration 449/1000 | Loss: 0.00001346
Iteration 450/1000 | Loss: 0.00001346
Iteration 451/1000 | Loss: 0.00001346
Iteration 452/1000 | Loss: 0.00001346
Iteration 453/1000 | Loss: 0.00001346
Iteration 454/1000 | Loss: 0.00001346
Iteration 455/1000 | Loss: 0.00001346
Iteration 456/1000 | Loss: 0.00001346
Iteration 457/1000 | Loss: 0.00001346
Iteration 458/1000 | Loss: 0.00001346
Iteration 459/1000 | Loss: 0.00001346
Iteration 460/1000 | Loss: 0.00001346
Iteration 461/1000 | Loss: 0.00001346
Iteration 462/1000 | Loss: 0.00001346
Iteration 463/1000 | Loss: 0.00001346
Iteration 464/1000 | Loss: 0.00001346
Iteration 465/1000 | Loss: 0.00001346
Iteration 466/1000 | Loss: 0.00001346
Iteration 467/1000 | Loss: 0.00001346
Iteration 468/1000 | Loss: 0.00001346
Iteration 469/1000 | Loss: 0.00001346
Iteration 470/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 470. Stopping optimization.
Last 5 losses: [1.3455194675771054e-05, 1.3455194675771054e-05, 1.3455194675771054e-05, 1.3455194675771054e-05, 1.3455194675771054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3455194675771054e-05

Optimization complete. Final v2v error: 3.105379581451416 mm

Highest mean error: 4.504487991333008 mm for frame 62

Lowest mean error: 2.6772074699401855 mm for frame 0

Saving results

Total time: 462.7105059623718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01157474
Iteration 2/25 | Loss: 0.00297557
Iteration 3/25 | Loss: 0.00231497
Iteration 4/25 | Loss: 0.00218684
Iteration 5/25 | Loss: 0.00216946
Iteration 6/25 | Loss: 0.00200596
Iteration 7/25 | Loss: 0.00189347
Iteration 8/25 | Loss: 0.00185339
Iteration 9/25 | Loss: 0.00184456
Iteration 10/25 | Loss: 0.00183525
Iteration 11/25 | Loss: 0.00182689
Iteration 12/25 | Loss: 0.00181940
Iteration 13/25 | Loss: 0.00182400
Iteration 14/25 | Loss: 0.00182956
Iteration 15/25 | Loss: 0.00182638
Iteration 16/25 | Loss: 0.00182368
Iteration 17/25 | Loss: 0.00181700
Iteration 18/25 | Loss: 0.00181678
Iteration 19/25 | Loss: 0.00182341
Iteration 20/25 | Loss: 0.00181941
Iteration 21/25 | Loss: 0.00182137
Iteration 22/25 | Loss: 0.00182246
Iteration 23/25 | Loss: 0.00182233
Iteration 24/25 | Loss: 0.00182097
Iteration 25/25 | Loss: 0.00181882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53518450
Iteration 2/25 | Loss: 0.00170131
Iteration 3/25 | Loss: 0.00165693
Iteration 4/25 | Loss: 0.00165692
Iteration 5/25 | Loss: 0.00165692
Iteration 6/25 | Loss: 0.00165692
Iteration 7/25 | Loss: 0.00165692
Iteration 8/25 | Loss: 0.00165692
Iteration 9/25 | Loss: 0.00165692
Iteration 10/25 | Loss: 0.00165692
Iteration 11/25 | Loss: 0.00165692
Iteration 12/25 | Loss: 0.00165692
Iteration 13/25 | Loss: 0.00165692
Iteration 14/25 | Loss: 0.00165692
Iteration 15/25 | Loss: 0.00165692
Iteration 16/25 | Loss: 0.00165692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016569209983572364, 0.0016569209983572364, 0.0016569209983572364, 0.0016569209983572364, 0.0016569209983572364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016569209983572364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165692
Iteration 2/1000 | Loss: 0.00035506
Iteration 3/1000 | Loss: 0.00039655
Iteration 4/1000 | Loss: 0.00025457
Iteration 5/1000 | Loss: 0.00011912
Iteration 6/1000 | Loss: 0.00016910
Iteration 7/1000 | Loss: 0.00040692
Iteration 8/1000 | Loss: 0.00031917
Iteration 9/1000 | Loss: 0.00008862
Iteration 10/1000 | Loss: 0.00019795
Iteration 11/1000 | Loss: 0.00036197
Iteration 12/1000 | Loss: 0.00030728
Iteration 13/1000 | Loss: 0.00021943
Iteration 14/1000 | Loss: 0.00018525
Iteration 15/1000 | Loss: 0.00007970
Iteration 16/1000 | Loss: 0.00007918
Iteration 17/1000 | Loss: 0.00017012
Iteration 18/1000 | Loss: 0.00024619
Iteration 19/1000 | Loss: 0.00008767
Iteration 20/1000 | Loss: 0.00007353
Iteration 21/1000 | Loss: 0.00008616
Iteration 22/1000 | Loss: 0.00019213
Iteration 23/1000 | Loss: 0.00022833
Iteration 24/1000 | Loss: 0.00007279
Iteration 25/1000 | Loss: 0.00008631
Iteration 26/1000 | Loss: 0.00006639
Iteration 27/1000 | Loss: 0.00005473
Iteration 28/1000 | Loss: 0.00018353
Iteration 29/1000 | Loss: 0.00019240
Iteration 30/1000 | Loss: 0.00007081
Iteration 31/1000 | Loss: 0.00006244
Iteration 32/1000 | Loss: 0.00017426
Iteration 33/1000 | Loss: 0.00012171
Iteration 34/1000 | Loss: 0.00011357
Iteration 35/1000 | Loss: 0.00014153
Iteration 36/1000 | Loss: 0.00007539
Iteration 37/1000 | Loss: 0.00014806
Iteration 38/1000 | Loss: 0.00015776
Iteration 39/1000 | Loss: 0.00012083
Iteration 40/1000 | Loss: 0.00013911
Iteration 41/1000 | Loss: 0.00013632
Iteration 42/1000 | Loss: 0.00014799
Iteration 43/1000 | Loss: 0.00016172
Iteration 44/1000 | Loss: 0.00015166
Iteration 45/1000 | Loss: 0.00012898
Iteration 46/1000 | Loss: 0.00014835
Iteration 47/1000 | Loss: 0.00013719
Iteration 48/1000 | Loss: 0.00016416
Iteration 49/1000 | Loss: 0.00014471
Iteration 50/1000 | Loss: 0.00006110
Iteration 51/1000 | Loss: 0.00006055
Iteration 52/1000 | Loss: 0.00008494
Iteration 53/1000 | Loss: 0.00008317
Iteration 54/1000 | Loss: 0.00005189
Iteration 55/1000 | Loss: 0.00007718
Iteration 56/1000 | Loss: 0.00005001
Iteration 57/1000 | Loss: 0.00005500
Iteration 58/1000 | Loss: 0.00005462
Iteration 59/1000 | Loss: 0.00004795
Iteration 60/1000 | Loss: 0.00005350
Iteration 61/1000 | Loss: 0.00005479
Iteration 62/1000 | Loss: 0.00006114
Iteration 63/1000 | Loss: 0.00005775
Iteration 64/1000 | Loss: 0.00005678
Iteration 65/1000 | Loss: 0.00006588
Iteration 66/1000 | Loss: 0.00006442
Iteration 67/1000 | Loss: 0.00004799
Iteration 68/1000 | Loss: 0.00004319
Iteration 69/1000 | Loss: 0.00004230
Iteration 70/1000 | Loss: 0.00004128
Iteration 71/1000 | Loss: 0.00004957
Iteration 72/1000 | Loss: 0.00004035
Iteration 73/1000 | Loss: 0.00005624
Iteration 74/1000 | Loss: 0.00004000
Iteration 75/1000 | Loss: 0.00005793
Iteration 76/1000 | Loss: 0.00004329
Iteration 77/1000 | Loss: 0.00004052
Iteration 78/1000 | Loss: 0.00003955
Iteration 79/1000 | Loss: 0.00003916
Iteration 80/1000 | Loss: 0.00039986
Iteration 81/1000 | Loss: 0.00049195
Iteration 82/1000 | Loss: 0.00005465
Iteration 83/1000 | Loss: 0.00004443
Iteration 84/1000 | Loss: 0.00004009
Iteration 85/1000 | Loss: 0.00006521
Iteration 86/1000 | Loss: 0.00003616
Iteration 87/1000 | Loss: 0.00005679
Iteration 88/1000 | Loss: 0.00003834
Iteration 89/1000 | Loss: 0.00003400
Iteration 90/1000 | Loss: 0.00003684
Iteration 91/1000 | Loss: 0.00003347
Iteration 92/1000 | Loss: 0.00003330
Iteration 93/1000 | Loss: 0.00003312
Iteration 94/1000 | Loss: 0.00003300
Iteration 95/1000 | Loss: 0.00003293
Iteration 96/1000 | Loss: 0.00003606
Iteration 97/1000 | Loss: 0.00003304
Iteration 98/1000 | Loss: 0.00003357
Iteration 99/1000 | Loss: 0.00003276
Iteration 100/1000 | Loss: 0.00003276
Iteration 101/1000 | Loss: 0.00003276
Iteration 102/1000 | Loss: 0.00003276
Iteration 103/1000 | Loss: 0.00003276
Iteration 104/1000 | Loss: 0.00006147
Iteration 105/1000 | Loss: 0.00003284
Iteration 106/1000 | Loss: 0.00003273
Iteration 107/1000 | Loss: 0.00003272
Iteration 108/1000 | Loss: 0.00003271
Iteration 109/1000 | Loss: 0.00003270
Iteration 110/1000 | Loss: 0.00003270
Iteration 111/1000 | Loss: 0.00003270
Iteration 112/1000 | Loss: 0.00003269
Iteration 113/1000 | Loss: 0.00003269
Iteration 114/1000 | Loss: 0.00003269
Iteration 115/1000 | Loss: 0.00003269
Iteration 116/1000 | Loss: 0.00004798
Iteration 117/1000 | Loss: 0.00003272
Iteration 118/1000 | Loss: 0.00003270
Iteration 119/1000 | Loss: 0.00003748
Iteration 120/1000 | Loss: 0.00003267
Iteration 121/1000 | Loss: 0.00003267
Iteration 122/1000 | Loss: 0.00003267
Iteration 123/1000 | Loss: 0.00003267
Iteration 124/1000 | Loss: 0.00003267
Iteration 125/1000 | Loss: 0.00003267
Iteration 126/1000 | Loss: 0.00003267
Iteration 127/1000 | Loss: 0.00003267
Iteration 128/1000 | Loss: 0.00003267
Iteration 129/1000 | Loss: 0.00003267
Iteration 130/1000 | Loss: 0.00003267
Iteration 131/1000 | Loss: 0.00003267
Iteration 132/1000 | Loss: 0.00003267
Iteration 133/1000 | Loss: 0.00003267
Iteration 134/1000 | Loss: 0.00003267
Iteration 135/1000 | Loss: 0.00003267
Iteration 136/1000 | Loss: 0.00003267
Iteration 137/1000 | Loss: 0.00003267
Iteration 138/1000 | Loss: 0.00003267
Iteration 139/1000 | Loss: 0.00003267
Iteration 140/1000 | Loss: 0.00003267
Iteration 141/1000 | Loss: 0.00003267
Iteration 142/1000 | Loss: 0.00003267
Iteration 143/1000 | Loss: 0.00003267
Iteration 144/1000 | Loss: 0.00003267
Iteration 145/1000 | Loss: 0.00003267
Iteration 146/1000 | Loss: 0.00003267
Iteration 147/1000 | Loss: 0.00003267
Iteration 148/1000 | Loss: 0.00003267
Iteration 149/1000 | Loss: 0.00003267
Iteration 150/1000 | Loss: 0.00003267
Iteration 151/1000 | Loss: 0.00003267
Iteration 152/1000 | Loss: 0.00003267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.26691624650266e-05, 3.26691624650266e-05, 3.26691624650266e-05, 3.26691624650266e-05, 3.26691624650266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.26691624650266e-05

Optimization complete. Final v2v error: 4.8779072761535645 mm

Highest mean error: 12.001527786254883 mm for frame 179

Lowest mean error: 4.5348381996154785 mm for frame 213

Saving results

Total time: 213.35710763931274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527824
Iteration 2/25 | Loss: 0.00203591
Iteration 3/25 | Loss: 0.00189928
Iteration 4/25 | Loss: 0.00187865
Iteration 5/25 | Loss: 0.00187459
Iteration 6/25 | Loss: 0.00187322
Iteration 7/25 | Loss: 0.00187300
Iteration 8/25 | Loss: 0.00187300
Iteration 9/25 | Loss: 0.00187300
Iteration 10/25 | Loss: 0.00187300
Iteration 11/25 | Loss: 0.00187300
Iteration 12/25 | Loss: 0.00187300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018730033189058304, 0.0018730033189058304, 0.0018730033189058304, 0.0018730033189058304, 0.0018730033189058304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018730033189058304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32657540
Iteration 2/25 | Loss: 0.00137355
Iteration 3/25 | Loss: 0.00137354
Iteration 4/25 | Loss: 0.00137354
Iteration 5/25 | Loss: 0.00137354
Iteration 6/25 | Loss: 0.00137354
Iteration 7/25 | Loss: 0.00137354
Iteration 8/25 | Loss: 0.00137354
Iteration 9/25 | Loss: 0.00137354
Iteration 10/25 | Loss: 0.00137354
Iteration 11/25 | Loss: 0.00137354
Iteration 12/25 | Loss: 0.00137354
Iteration 13/25 | Loss: 0.00137354
Iteration 14/25 | Loss: 0.00137354
Iteration 15/25 | Loss: 0.00137354
Iteration 16/25 | Loss: 0.00137354
Iteration 17/25 | Loss: 0.00137354
Iteration 18/25 | Loss: 0.00137354
Iteration 19/25 | Loss: 0.00137354
Iteration 20/25 | Loss: 0.00137354
Iteration 21/25 | Loss: 0.00137354
Iteration 22/25 | Loss: 0.00137354
Iteration 23/25 | Loss: 0.00137354
Iteration 24/25 | Loss: 0.00137354
Iteration 25/25 | Loss: 0.00137354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137354
Iteration 2/1000 | Loss: 0.00008624
Iteration 3/1000 | Loss: 0.00005516
Iteration 4/1000 | Loss: 0.00004837
Iteration 5/1000 | Loss: 0.00004581
Iteration 6/1000 | Loss: 0.00004428
Iteration 7/1000 | Loss: 0.00004315
Iteration 8/1000 | Loss: 0.00004213
Iteration 9/1000 | Loss: 0.00004161
Iteration 10/1000 | Loss: 0.00004111
Iteration 11/1000 | Loss: 0.00004069
Iteration 12/1000 | Loss: 0.00004040
Iteration 13/1000 | Loss: 0.00004007
Iteration 14/1000 | Loss: 0.00003995
Iteration 15/1000 | Loss: 0.00003994
Iteration 16/1000 | Loss: 0.00003993
Iteration 17/1000 | Loss: 0.00003993
Iteration 18/1000 | Loss: 0.00003993
Iteration 19/1000 | Loss: 0.00003992
Iteration 20/1000 | Loss: 0.00003992
Iteration 21/1000 | Loss: 0.00003992
Iteration 22/1000 | Loss: 0.00003990
Iteration 23/1000 | Loss: 0.00003990
Iteration 24/1000 | Loss: 0.00003988
Iteration 25/1000 | Loss: 0.00003988
Iteration 26/1000 | Loss: 0.00003988
Iteration 27/1000 | Loss: 0.00003988
Iteration 28/1000 | Loss: 0.00003988
Iteration 29/1000 | Loss: 0.00003988
Iteration 30/1000 | Loss: 0.00003988
Iteration 31/1000 | Loss: 0.00003987
Iteration 32/1000 | Loss: 0.00003987
Iteration 33/1000 | Loss: 0.00003984
Iteration 34/1000 | Loss: 0.00003982
Iteration 35/1000 | Loss: 0.00003980
Iteration 36/1000 | Loss: 0.00003979
Iteration 37/1000 | Loss: 0.00003978
Iteration 38/1000 | Loss: 0.00003978
Iteration 39/1000 | Loss: 0.00003975
Iteration 40/1000 | Loss: 0.00003974
Iteration 41/1000 | Loss: 0.00003974
Iteration 42/1000 | Loss: 0.00003973
Iteration 43/1000 | Loss: 0.00003973
Iteration 44/1000 | Loss: 0.00003973
Iteration 45/1000 | Loss: 0.00003972
Iteration 46/1000 | Loss: 0.00003972
Iteration 47/1000 | Loss: 0.00003972
Iteration 48/1000 | Loss: 0.00003972
Iteration 49/1000 | Loss: 0.00003971
Iteration 50/1000 | Loss: 0.00003971
Iteration 51/1000 | Loss: 0.00003971
Iteration 52/1000 | Loss: 0.00003970
Iteration 53/1000 | Loss: 0.00003970
Iteration 54/1000 | Loss: 0.00003970
Iteration 55/1000 | Loss: 0.00003970
Iteration 56/1000 | Loss: 0.00003970
Iteration 57/1000 | Loss: 0.00003969
Iteration 58/1000 | Loss: 0.00003969
Iteration 59/1000 | Loss: 0.00003969
Iteration 60/1000 | Loss: 0.00003968
Iteration 61/1000 | Loss: 0.00003967
Iteration 62/1000 | Loss: 0.00003967
Iteration 63/1000 | Loss: 0.00003966
Iteration 64/1000 | Loss: 0.00003966
Iteration 65/1000 | Loss: 0.00003965
Iteration 66/1000 | Loss: 0.00003965
Iteration 67/1000 | Loss: 0.00003965
Iteration 68/1000 | Loss: 0.00003964
Iteration 69/1000 | Loss: 0.00003964
Iteration 70/1000 | Loss: 0.00003964
Iteration 71/1000 | Loss: 0.00003963
Iteration 72/1000 | Loss: 0.00003963
Iteration 73/1000 | Loss: 0.00003963
Iteration 74/1000 | Loss: 0.00003962
Iteration 75/1000 | Loss: 0.00003962
Iteration 76/1000 | Loss: 0.00003962
Iteration 77/1000 | Loss: 0.00003962
Iteration 78/1000 | Loss: 0.00003961
Iteration 79/1000 | Loss: 0.00003961
Iteration 80/1000 | Loss: 0.00003960
Iteration 81/1000 | Loss: 0.00003960
Iteration 82/1000 | Loss: 0.00003960
Iteration 83/1000 | Loss: 0.00003960
Iteration 84/1000 | Loss: 0.00003960
Iteration 85/1000 | Loss: 0.00003959
Iteration 86/1000 | Loss: 0.00003959
Iteration 87/1000 | Loss: 0.00003959
Iteration 88/1000 | Loss: 0.00003959
Iteration 89/1000 | Loss: 0.00003959
Iteration 90/1000 | Loss: 0.00003959
Iteration 91/1000 | Loss: 0.00003959
Iteration 92/1000 | Loss: 0.00003958
Iteration 93/1000 | Loss: 0.00003958
Iteration 94/1000 | Loss: 0.00003958
Iteration 95/1000 | Loss: 0.00003958
Iteration 96/1000 | Loss: 0.00003958
Iteration 97/1000 | Loss: 0.00003958
Iteration 98/1000 | Loss: 0.00003958
Iteration 99/1000 | Loss: 0.00003958
Iteration 100/1000 | Loss: 0.00003958
Iteration 101/1000 | Loss: 0.00003957
Iteration 102/1000 | Loss: 0.00003957
Iteration 103/1000 | Loss: 0.00003957
Iteration 104/1000 | Loss: 0.00003957
Iteration 105/1000 | Loss: 0.00003957
Iteration 106/1000 | Loss: 0.00003956
Iteration 107/1000 | Loss: 0.00003956
Iteration 108/1000 | Loss: 0.00003956
Iteration 109/1000 | Loss: 0.00003956
Iteration 110/1000 | Loss: 0.00003956
Iteration 111/1000 | Loss: 0.00003956
Iteration 112/1000 | Loss: 0.00003956
Iteration 113/1000 | Loss: 0.00003956
Iteration 114/1000 | Loss: 0.00003956
Iteration 115/1000 | Loss: 0.00003955
Iteration 116/1000 | Loss: 0.00003955
Iteration 117/1000 | Loss: 0.00003955
Iteration 118/1000 | Loss: 0.00003955
Iteration 119/1000 | Loss: 0.00003955
Iteration 120/1000 | Loss: 0.00003955
Iteration 121/1000 | Loss: 0.00003955
Iteration 122/1000 | Loss: 0.00003955
Iteration 123/1000 | Loss: 0.00003955
Iteration 124/1000 | Loss: 0.00003955
Iteration 125/1000 | Loss: 0.00003955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.955299689550884e-05, 3.955299689550884e-05, 3.955299689550884e-05, 3.955299689550884e-05, 3.955299689550884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.955299689550884e-05

Optimization complete. Final v2v error: 5.335708141326904 mm

Highest mean error: 5.784239292144775 mm for frame 39

Lowest mean error: 4.986869812011719 mm for frame 83

Saving results

Total time: 37.74843120574951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_50_us_1638/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_50_us_1638/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591537
Iteration 2/25 | Loss: 0.00200588
Iteration 3/25 | Loss: 0.00191007
Iteration 4/25 | Loss: 0.00189109
Iteration 5/25 | Loss: 0.00188706
Iteration 6/25 | Loss: 0.00188706
Iteration 7/25 | Loss: 0.00188706
Iteration 8/25 | Loss: 0.00188706
Iteration 9/25 | Loss: 0.00188706
Iteration 10/25 | Loss: 0.00188706
Iteration 11/25 | Loss: 0.00188706
Iteration 12/25 | Loss: 0.00188706
Iteration 13/25 | Loss: 0.00188706
Iteration 14/25 | Loss: 0.00188706
Iteration 15/25 | Loss: 0.00188706
Iteration 16/25 | Loss: 0.00188706
Iteration 17/25 | Loss: 0.00188706
Iteration 18/25 | Loss: 0.00188706
Iteration 19/25 | Loss: 0.00188706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018870639614760876, 0.0018870639614760876, 0.0018870639614760876, 0.0018870639614760876, 0.0018870639614760876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018870639614760876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42867184
Iteration 2/25 | Loss: 0.00134834
Iteration 3/25 | Loss: 0.00134830
Iteration 4/25 | Loss: 0.00134830
Iteration 5/25 | Loss: 0.00134830
Iteration 6/25 | Loss: 0.00134830
Iteration 7/25 | Loss: 0.00134830
Iteration 8/25 | Loss: 0.00134830
Iteration 9/25 | Loss: 0.00134830
Iteration 10/25 | Loss: 0.00134830
Iteration 11/25 | Loss: 0.00134830
Iteration 12/25 | Loss: 0.00134830
Iteration 13/25 | Loss: 0.00134830
Iteration 14/25 | Loss: 0.00134830
Iteration 15/25 | Loss: 0.00134830
Iteration 16/25 | Loss: 0.00134830
Iteration 17/25 | Loss: 0.00134830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001348301419056952, 0.001348301419056952, 0.001348301419056952, 0.001348301419056952, 0.001348301419056952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001348301419056952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134830
Iteration 2/1000 | Loss: 0.00009681
Iteration 3/1000 | Loss: 0.00006282
Iteration 4/1000 | Loss: 0.00005388
Iteration 5/1000 | Loss: 0.00005130
Iteration 6/1000 | Loss: 0.00004997
Iteration 7/1000 | Loss: 0.00004820
Iteration 8/1000 | Loss: 0.00004694
Iteration 9/1000 | Loss: 0.00004584
Iteration 10/1000 | Loss: 0.00004486
Iteration 11/1000 | Loss: 0.00004416
Iteration 12/1000 | Loss: 0.00004353
Iteration 13/1000 | Loss: 0.00004305
Iteration 14/1000 | Loss: 0.00004279
Iteration 15/1000 | Loss: 0.00004249
Iteration 16/1000 | Loss: 0.00004248
Iteration 17/1000 | Loss: 0.00004245
Iteration 18/1000 | Loss: 0.00004237
Iteration 19/1000 | Loss: 0.00004224
Iteration 20/1000 | Loss: 0.00004217
Iteration 21/1000 | Loss: 0.00004214
Iteration 22/1000 | Loss: 0.00004214
Iteration 23/1000 | Loss: 0.00004212
Iteration 24/1000 | Loss: 0.00004212
Iteration 25/1000 | Loss: 0.00004211
Iteration 26/1000 | Loss: 0.00004210
Iteration 27/1000 | Loss: 0.00004209
Iteration 28/1000 | Loss: 0.00004209
Iteration 29/1000 | Loss: 0.00004209
Iteration 30/1000 | Loss: 0.00004208
Iteration 31/1000 | Loss: 0.00004208
Iteration 32/1000 | Loss: 0.00004208
Iteration 33/1000 | Loss: 0.00004207
Iteration 34/1000 | Loss: 0.00004207
Iteration 35/1000 | Loss: 0.00004206
Iteration 36/1000 | Loss: 0.00004205
Iteration 37/1000 | Loss: 0.00004204
Iteration 38/1000 | Loss: 0.00004203
Iteration 39/1000 | Loss: 0.00004203
Iteration 40/1000 | Loss: 0.00004203
Iteration 41/1000 | Loss: 0.00004203
Iteration 42/1000 | Loss: 0.00004203
Iteration 43/1000 | Loss: 0.00004203
Iteration 44/1000 | Loss: 0.00004203
Iteration 45/1000 | Loss: 0.00004203
Iteration 46/1000 | Loss: 0.00004203
Iteration 47/1000 | Loss: 0.00004202
Iteration 48/1000 | Loss: 0.00004202
Iteration 49/1000 | Loss: 0.00004202
Iteration 50/1000 | Loss: 0.00004202
Iteration 51/1000 | Loss: 0.00004202
Iteration 52/1000 | Loss: 0.00004202
Iteration 53/1000 | Loss: 0.00004202
Iteration 54/1000 | Loss: 0.00004201
Iteration 55/1000 | Loss: 0.00004200
Iteration 56/1000 | Loss: 0.00004200
Iteration 57/1000 | Loss: 0.00004199
Iteration 58/1000 | Loss: 0.00004199
Iteration 59/1000 | Loss: 0.00004199
Iteration 60/1000 | Loss: 0.00004199
Iteration 61/1000 | Loss: 0.00004199
Iteration 62/1000 | Loss: 0.00004198
Iteration 63/1000 | Loss: 0.00004198
Iteration 64/1000 | Loss: 0.00004198
Iteration 65/1000 | Loss: 0.00004198
Iteration 66/1000 | Loss: 0.00004197
Iteration 67/1000 | Loss: 0.00004197
Iteration 68/1000 | Loss: 0.00004197
Iteration 69/1000 | Loss: 0.00004196
Iteration 70/1000 | Loss: 0.00004196
Iteration 71/1000 | Loss: 0.00004196
Iteration 72/1000 | Loss: 0.00004195
Iteration 73/1000 | Loss: 0.00004195
Iteration 74/1000 | Loss: 0.00004195
Iteration 75/1000 | Loss: 0.00004195
Iteration 76/1000 | Loss: 0.00004195
Iteration 77/1000 | Loss: 0.00004194
Iteration 78/1000 | Loss: 0.00004194
Iteration 79/1000 | Loss: 0.00004194
Iteration 80/1000 | Loss: 0.00004194
Iteration 81/1000 | Loss: 0.00004194
Iteration 82/1000 | Loss: 0.00004194
Iteration 83/1000 | Loss: 0.00004194
Iteration 84/1000 | Loss: 0.00004194
Iteration 85/1000 | Loss: 0.00004194
Iteration 86/1000 | Loss: 0.00004194
Iteration 87/1000 | Loss: 0.00004194
Iteration 88/1000 | Loss: 0.00004194
Iteration 89/1000 | Loss: 0.00004194
Iteration 90/1000 | Loss: 0.00004194
Iteration 91/1000 | Loss: 0.00004194
Iteration 92/1000 | Loss: 0.00004194
Iteration 93/1000 | Loss: 0.00004194
Iteration 94/1000 | Loss: 0.00004194
Iteration 95/1000 | Loss: 0.00004194
Iteration 96/1000 | Loss: 0.00004194
Iteration 97/1000 | Loss: 0.00004194
Iteration 98/1000 | Loss: 0.00004194
Iteration 99/1000 | Loss: 0.00004194
Iteration 100/1000 | Loss: 0.00004194
Iteration 101/1000 | Loss: 0.00004194
Iteration 102/1000 | Loss: 0.00004194
Iteration 103/1000 | Loss: 0.00004194
Iteration 104/1000 | Loss: 0.00004194
Iteration 105/1000 | Loss: 0.00004194
Iteration 106/1000 | Loss: 0.00004194
Iteration 107/1000 | Loss: 0.00004194
Iteration 108/1000 | Loss: 0.00004194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [4.193875065539032e-05, 4.193875065539032e-05, 4.193875065539032e-05, 4.193875065539032e-05, 4.193875065539032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.193875065539032e-05

Optimization complete. Final v2v error: 5.489642143249512 mm

Highest mean error: 6.204051971435547 mm for frame 160

Lowest mean error: 4.952360153198242 mm for frame 222

Saving results

Total time: 42.9755277633667
