Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=80, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4480-4535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048849
Iteration 2/25 | Loss: 0.00220055
Iteration 3/25 | Loss: 0.00169623
Iteration 4/25 | Loss: 0.00156662
Iteration 5/25 | Loss: 0.00152080
Iteration 6/25 | Loss: 0.00131165
Iteration 7/25 | Loss: 0.00118910
Iteration 8/25 | Loss: 0.00115903
Iteration 9/25 | Loss: 0.00110859
Iteration 10/25 | Loss: 0.00109900
Iteration 11/25 | Loss: 0.00109880
Iteration 12/25 | Loss: 0.00109555
Iteration 13/25 | Loss: 0.00109775
Iteration 14/25 | Loss: 0.00109481
Iteration 15/25 | Loss: 0.00109759
Iteration 16/25 | Loss: 0.00109628
Iteration 17/25 | Loss: 0.00109444
Iteration 18/25 | Loss: 0.00109444
Iteration 19/25 | Loss: 0.00109443
Iteration 20/25 | Loss: 0.00109443
Iteration 21/25 | Loss: 0.00109443
Iteration 22/25 | Loss: 0.00109443
Iteration 23/25 | Loss: 0.00109443
Iteration 24/25 | Loss: 0.00109443
Iteration 25/25 | Loss: 0.00109443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29995251
Iteration 2/25 | Loss: 0.00096841
Iteration 3/25 | Loss: 0.00084426
Iteration 4/25 | Loss: 0.00084426
Iteration 5/25 | Loss: 0.00084426
Iteration 6/25 | Loss: 0.00084426
Iteration 7/25 | Loss: 0.00084426
Iteration 8/25 | Loss: 0.00084426
Iteration 9/25 | Loss: 0.00084426
Iteration 10/25 | Loss: 0.00084426
Iteration 11/25 | Loss: 0.00084426
Iteration 12/25 | Loss: 0.00084426
Iteration 13/25 | Loss: 0.00084426
Iteration 14/25 | Loss: 0.00084426
Iteration 15/25 | Loss: 0.00084426
Iteration 16/25 | Loss: 0.00084426
Iteration 17/25 | Loss: 0.00084426
Iteration 18/25 | Loss: 0.00084426
Iteration 19/25 | Loss: 0.00084426
Iteration 20/25 | Loss: 0.00084426
Iteration 21/25 | Loss: 0.00084426
Iteration 22/25 | Loss: 0.00084426
Iteration 23/25 | Loss: 0.00084426
Iteration 24/25 | Loss: 0.00084426
Iteration 25/25 | Loss: 0.00084426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084426
Iteration 2/1000 | Loss: 0.00007266
Iteration 3/1000 | Loss: 0.00003769
Iteration 4/1000 | Loss: 0.00005059
Iteration 5/1000 | Loss: 0.00003641
Iteration 6/1000 | Loss: 0.00003624
Iteration 7/1000 | Loss: 0.00002797
Iteration 8/1000 | Loss: 0.00006907
Iteration 9/1000 | Loss: 0.00002686
Iteration 10/1000 | Loss: 0.00007524
Iteration 11/1000 | Loss: 0.00012581
Iteration 12/1000 | Loss: 0.00002563
Iteration 13/1000 | Loss: 0.00002506
Iteration 14/1000 | Loss: 0.00002464
Iteration 15/1000 | Loss: 0.00005102
Iteration 16/1000 | Loss: 0.00002410
Iteration 17/1000 | Loss: 0.00002388
Iteration 18/1000 | Loss: 0.00002377
Iteration 19/1000 | Loss: 0.00002376
Iteration 20/1000 | Loss: 0.00002376
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00004875
Iteration 23/1000 | Loss: 0.00002360
Iteration 24/1000 | Loss: 0.00228271
Iteration 25/1000 | Loss: 0.00173677
Iteration 26/1000 | Loss: 0.00012874
Iteration 27/1000 | Loss: 0.00003673
Iteration 28/1000 | Loss: 0.00053416
Iteration 29/1000 | Loss: 0.00025840
Iteration 30/1000 | Loss: 0.00050401
Iteration 31/1000 | Loss: 0.00029346
Iteration 32/1000 | Loss: 0.00103222
Iteration 33/1000 | Loss: 0.00064269
Iteration 34/1000 | Loss: 0.00002856
Iteration 35/1000 | Loss: 0.00002444
Iteration 36/1000 | Loss: 0.00002406
Iteration 37/1000 | Loss: 0.00002370
Iteration 38/1000 | Loss: 0.00062226
Iteration 39/1000 | Loss: 0.00161576
Iteration 40/1000 | Loss: 0.00305861
Iteration 41/1000 | Loss: 0.00184957
Iteration 42/1000 | Loss: 0.00090459
Iteration 43/1000 | Loss: 0.00061045
Iteration 44/1000 | Loss: 0.00025207
Iteration 45/1000 | Loss: 0.00006577
Iteration 46/1000 | Loss: 0.00117691
Iteration 47/1000 | Loss: 0.00016016
Iteration 48/1000 | Loss: 0.00049629
Iteration 49/1000 | Loss: 0.00006140
Iteration 50/1000 | Loss: 0.00004068
Iteration 51/1000 | Loss: 0.00004586
Iteration 52/1000 | Loss: 0.00006386
Iteration 53/1000 | Loss: 0.00003985
Iteration 54/1000 | Loss: 0.00004190
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00064749
Iteration 57/1000 | Loss: 0.00004906
Iteration 58/1000 | Loss: 0.00006757
Iteration 59/1000 | Loss: 0.00002974
Iteration 60/1000 | Loss: 0.00002588
Iteration 61/1000 | Loss: 0.00002508
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00000962
Iteration 64/1000 | Loss: 0.00000909
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000837
Iteration 67/1000 | Loss: 0.00001043
Iteration 68/1000 | Loss: 0.00000813
Iteration 69/1000 | Loss: 0.00000805
Iteration 70/1000 | Loss: 0.00000799
Iteration 71/1000 | Loss: 0.00000798
Iteration 72/1000 | Loss: 0.00000795
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000793
Iteration 75/1000 | Loss: 0.00000792
Iteration 76/1000 | Loss: 0.00000787
Iteration 77/1000 | Loss: 0.00000787
Iteration 78/1000 | Loss: 0.00000786
Iteration 79/1000 | Loss: 0.00000780
Iteration 80/1000 | Loss: 0.00000780
Iteration 81/1000 | Loss: 0.00000780
Iteration 82/1000 | Loss: 0.00000780
Iteration 83/1000 | Loss: 0.00000780
Iteration 84/1000 | Loss: 0.00000780
Iteration 85/1000 | Loss: 0.00000780
Iteration 86/1000 | Loss: 0.00000780
Iteration 87/1000 | Loss: 0.00000779
Iteration 88/1000 | Loss: 0.00000779
Iteration 89/1000 | Loss: 0.00000777
Iteration 90/1000 | Loss: 0.00000776
Iteration 91/1000 | Loss: 0.00000776
Iteration 92/1000 | Loss: 0.00000775
Iteration 93/1000 | Loss: 0.00000775
Iteration 94/1000 | Loss: 0.00000775
Iteration 95/1000 | Loss: 0.00000775
Iteration 96/1000 | Loss: 0.00000775
Iteration 97/1000 | Loss: 0.00000775
Iteration 98/1000 | Loss: 0.00000775
Iteration 99/1000 | Loss: 0.00000775
Iteration 100/1000 | Loss: 0.00000774
Iteration 101/1000 | Loss: 0.00000774
Iteration 102/1000 | Loss: 0.00000773
Iteration 103/1000 | Loss: 0.00000773
Iteration 104/1000 | Loss: 0.00000773
Iteration 105/1000 | Loss: 0.00000773
Iteration 106/1000 | Loss: 0.00000772
Iteration 107/1000 | Loss: 0.00000772
Iteration 108/1000 | Loss: 0.00000772
Iteration 109/1000 | Loss: 0.00000772
Iteration 110/1000 | Loss: 0.00000771
Iteration 111/1000 | Loss: 0.00000771
Iteration 112/1000 | Loss: 0.00000771
Iteration 113/1000 | Loss: 0.00000771
Iteration 114/1000 | Loss: 0.00000771
Iteration 115/1000 | Loss: 0.00000771
Iteration 116/1000 | Loss: 0.00000771
Iteration 117/1000 | Loss: 0.00000771
Iteration 118/1000 | Loss: 0.00000770
Iteration 119/1000 | Loss: 0.00000770
Iteration 120/1000 | Loss: 0.00000770
Iteration 121/1000 | Loss: 0.00000770
Iteration 122/1000 | Loss: 0.00000770
Iteration 123/1000 | Loss: 0.00000770
Iteration 124/1000 | Loss: 0.00000770
Iteration 125/1000 | Loss: 0.00000770
Iteration 126/1000 | Loss: 0.00000770
Iteration 127/1000 | Loss: 0.00000769
Iteration 128/1000 | Loss: 0.00000769
Iteration 129/1000 | Loss: 0.00000769
Iteration 130/1000 | Loss: 0.00000769
Iteration 131/1000 | Loss: 0.00000769
Iteration 132/1000 | Loss: 0.00000769
Iteration 133/1000 | Loss: 0.00000768
Iteration 134/1000 | Loss: 0.00000768
Iteration 135/1000 | Loss: 0.00000768
Iteration 136/1000 | Loss: 0.00000768
Iteration 137/1000 | Loss: 0.00000768
Iteration 138/1000 | Loss: 0.00000768
Iteration 139/1000 | Loss: 0.00000768
Iteration 140/1000 | Loss: 0.00000768
Iteration 141/1000 | Loss: 0.00000768
Iteration 142/1000 | Loss: 0.00000768
Iteration 143/1000 | Loss: 0.00000768
Iteration 144/1000 | Loss: 0.00000768
Iteration 145/1000 | Loss: 0.00000768
Iteration 146/1000 | Loss: 0.00000768
Iteration 147/1000 | Loss: 0.00000768
Iteration 148/1000 | Loss: 0.00000767
Iteration 149/1000 | Loss: 0.00000767
Iteration 150/1000 | Loss: 0.00000767
Iteration 151/1000 | Loss: 0.00000767
Iteration 152/1000 | Loss: 0.00000767
Iteration 153/1000 | Loss: 0.00000767
Iteration 154/1000 | Loss: 0.00000767
Iteration 155/1000 | Loss: 0.00000767
Iteration 156/1000 | Loss: 0.00000767
Iteration 157/1000 | Loss: 0.00000767
Iteration 158/1000 | Loss: 0.00000767
Iteration 159/1000 | Loss: 0.00000767
Iteration 160/1000 | Loss: 0.00000767
Iteration 161/1000 | Loss: 0.00000767
Iteration 162/1000 | Loss: 0.00000767
Iteration 163/1000 | Loss: 0.00000767
Iteration 164/1000 | Loss: 0.00000767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [7.667141289857682e-06, 7.667141289857682e-06, 7.667141289857682e-06, 7.667141289857682e-06, 7.667141289857682e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.667141289857682e-06

Optimization complete. Final v2v error: 2.3757894039154053 mm

Highest mean error: 3.185580015182495 mm for frame 78

Lowest mean error: 2.293980836868286 mm for frame 33

Saving results

Total time: 129.81212329864502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514685
Iteration 2/25 | Loss: 0.00125373
Iteration 3/25 | Loss: 0.00115968
Iteration 4/25 | Loss: 0.00115513
Iteration 5/25 | Loss: 0.00115513
Iteration 6/25 | Loss: 0.00115513
Iteration 7/25 | Loss: 0.00115513
Iteration 8/25 | Loss: 0.00115513
Iteration 9/25 | Loss: 0.00115513
Iteration 10/25 | Loss: 0.00115513
Iteration 11/25 | Loss: 0.00115513
Iteration 12/25 | Loss: 0.00115513
Iteration 13/25 | Loss: 0.00115513
Iteration 14/25 | Loss: 0.00115513
Iteration 15/25 | Loss: 0.00115513
Iteration 16/25 | Loss: 0.00115513
Iteration 17/25 | Loss: 0.00115513
Iteration 18/25 | Loss: 0.00115513
Iteration 19/25 | Loss: 0.00115513
Iteration 20/25 | Loss: 0.00115513
Iteration 21/25 | Loss: 0.00115513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011551313800737262, 0.0011551313800737262, 0.0011551313800737262, 0.0011551313800737262, 0.0011551313800737262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011551313800737262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81406587
Iteration 2/25 | Loss: 0.00056125
Iteration 3/25 | Loss: 0.00056124
Iteration 4/25 | Loss: 0.00056124
Iteration 5/25 | Loss: 0.00056124
Iteration 6/25 | Loss: 0.00056124
Iteration 7/25 | Loss: 0.00056124
Iteration 8/25 | Loss: 0.00056124
Iteration 9/25 | Loss: 0.00056124
Iteration 10/25 | Loss: 0.00056124
Iteration 11/25 | Loss: 0.00056124
Iteration 12/25 | Loss: 0.00056124
Iteration 13/25 | Loss: 0.00056124
Iteration 14/25 | Loss: 0.00056124
Iteration 15/25 | Loss: 0.00056124
Iteration 16/25 | Loss: 0.00056124
Iteration 17/25 | Loss: 0.00056124
Iteration 18/25 | Loss: 0.00056124
Iteration 19/25 | Loss: 0.00056124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005612398381344974, 0.0005612398381344974, 0.0005612398381344974, 0.0005612398381344974, 0.0005612398381344974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005612398381344974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056124
Iteration 2/1000 | Loss: 0.00002097
Iteration 3/1000 | Loss: 0.00001590
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001431
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001392
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001292
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001264
Iteration 25/1000 | Loss: 0.00001261
Iteration 26/1000 | Loss: 0.00001261
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001238
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001225
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001225
Iteration 155/1000 | Loss: 0.00001225
Iteration 156/1000 | Loss: 0.00001225
Iteration 157/1000 | Loss: 0.00001225
Iteration 158/1000 | Loss: 0.00001225
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.2246725418663118e-05, 1.2246725418663118e-05, 1.2246725418663118e-05, 1.2246725418663118e-05, 1.2246725418663118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2246725418663118e-05

Optimization complete. Final v2v error: 2.9536118507385254 mm

Highest mean error: 2.982445478439331 mm for frame 0

Lowest mean error: 2.920901298522949 mm for frame 244

Saving results

Total time: 40.6236937046051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080940
Iteration 2/25 | Loss: 0.00246274
Iteration 3/25 | Loss: 0.00156050
Iteration 4/25 | Loss: 0.00129467
Iteration 5/25 | Loss: 0.00125076
Iteration 6/25 | Loss: 0.00125095
Iteration 7/25 | Loss: 0.00123609
Iteration 8/25 | Loss: 0.00122629
Iteration 9/25 | Loss: 0.00122384
Iteration 10/25 | Loss: 0.00121591
Iteration 11/25 | Loss: 0.00121441
Iteration 12/25 | Loss: 0.00120939
Iteration 13/25 | Loss: 0.00121093
Iteration 14/25 | Loss: 0.00120643
Iteration 15/25 | Loss: 0.00120491
Iteration 16/25 | Loss: 0.00120823
Iteration 17/25 | Loss: 0.00120464
Iteration 18/25 | Loss: 0.00120367
Iteration 19/25 | Loss: 0.00120288
Iteration 20/25 | Loss: 0.00120089
Iteration 21/25 | Loss: 0.00120089
Iteration 22/25 | Loss: 0.00120098
Iteration 23/25 | Loss: 0.00120092
Iteration 24/25 | Loss: 0.00120107
Iteration 25/25 | Loss: 0.00120094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39418352
Iteration 2/25 | Loss: 0.00116713
Iteration 3/25 | Loss: 0.00116713
Iteration 4/25 | Loss: 0.00116713
Iteration 5/25 | Loss: 0.00116713
Iteration 6/25 | Loss: 0.00116713
Iteration 7/25 | Loss: 0.00116713
Iteration 8/25 | Loss: 0.00116712
Iteration 9/25 | Loss: 0.00116712
Iteration 10/25 | Loss: 0.00116712
Iteration 11/25 | Loss: 0.00116712
Iteration 12/25 | Loss: 0.00116712
Iteration 13/25 | Loss: 0.00116712
Iteration 14/25 | Loss: 0.00116712
Iteration 15/25 | Loss: 0.00116712
Iteration 16/25 | Loss: 0.00116712
Iteration 17/25 | Loss: 0.00116712
Iteration 18/25 | Loss: 0.00116712
Iteration 19/25 | Loss: 0.00116712
Iteration 20/25 | Loss: 0.00116712
Iteration 21/25 | Loss: 0.00116712
Iteration 22/25 | Loss: 0.00116712
Iteration 23/25 | Loss: 0.00116712
Iteration 24/25 | Loss: 0.00116712
Iteration 25/25 | Loss: 0.00116712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116712
Iteration 2/1000 | Loss: 0.00005055
Iteration 3/1000 | Loss: 0.00003176
Iteration 4/1000 | Loss: 0.00004500
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002242
Iteration 7/1000 | Loss: 0.00002550
Iteration 8/1000 | Loss: 0.00004654
Iteration 9/1000 | Loss: 0.00023250
Iteration 10/1000 | Loss: 0.00003917
Iteration 11/1000 | Loss: 0.00002830
Iteration 12/1000 | Loss: 0.00003342
Iteration 13/1000 | Loss: 0.00008558
Iteration 14/1000 | Loss: 0.00002834
Iteration 15/1000 | Loss: 0.00002683
Iteration 16/1000 | Loss: 0.00002164
Iteration 17/1000 | Loss: 0.00001738
Iteration 18/1000 | Loss: 0.00005880
Iteration 19/1000 | Loss: 0.00003043
Iteration 20/1000 | Loss: 0.00002801
Iteration 21/1000 | Loss: 0.00003753
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00004445
Iteration 25/1000 | Loss: 0.00001704
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00008033
Iteration 30/1000 | Loss: 0.00002517
Iteration 31/1000 | Loss: 0.00003964
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00002165
Iteration 34/1000 | Loss: 0.00001628
Iteration 35/1000 | Loss: 0.00001628
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001628
Iteration 40/1000 | Loss: 0.00001628
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001627
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001626
Iteration 51/1000 | Loss: 0.00001626
Iteration 52/1000 | Loss: 0.00001626
Iteration 53/1000 | Loss: 0.00001626
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001622
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001652
Iteration 73/1000 | Loss: 0.00001889
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001615
Iteration 96/1000 | Loss: 0.00001614
Iteration 97/1000 | Loss: 0.00001614
Iteration 98/1000 | Loss: 0.00001614
Iteration 99/1000 | Loss: 0.00001614
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001614
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001611
Iteration 122/1000 | Loss: 0.00001611
Iteration 123/1000 | Loss: 0.00001611
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.6102685549412854e-05, 1.6102685549412854e-05, 1.6102685549412854e-05, 1.6102685549412854e-05, 1.6102685549412854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6102685549412854e-05

Optimization complete. Final v2v error: 3.3644425868988037 mm

Highest mean error: 9.482463836669922 mm for frame 9

Lowest mean error: 2.878514289855957 mm for frame 48

Saving results

Total time: 98.98822736740112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899791
Iteration 2/25 | Loss: 0.00150140
Iteration 3/25 | Loss: 0.00129722
Iteration 4/25 | Loss: 0.00128492
Iteration 5/25 | Loss: 0.00127994
Iteration 6/25 | Loss: 0.00127152
Iteration 7/25 | Loss: 0.00127012
Iteration 8/25 | Loss: 0.00126592
Iteration 9/25 | Loss: 0.00125919
Iteration 10/25 | Loss: 0.00124525
Iteration 11/25 | Loss: 0.00124402
Iteration 12/25 | Loss: 0.00124438
Iteration 13/25 | Loss: 0.00124559
Iteration 14/25 | Loss: 0.00123668
Iteration 15/25 | Loss: 0.00123600
Iteration 16/25 | Loss: 0.00124129
Iteration 17/25 | Loss: 0.00123391
Iteration 18/25 | Loss: 0.00122894
Iteration 19/25 | Loss: 0.00122659
Iteration 20/25 | Loss: 0.00122923
Iteration 21/25 | Loss: 0.00122886
Iteration 22/25 | Loss: 0.00122584
Iteration 23/25 | Loss: 0.00123023
Iteration 24/25 | Loss: 0.00122497
Iteration 25/25 | Loss: 0.00122482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62963593
Iteration 2/25 | Loss: 0.00085093
Iteration 3/25 | Loss: 0.00085093
Iteration 4/25 | Loss: 0.00085093
Iteration 5/25 | Loss: 0.00085093
Iteration 6/25 | Loss: 0.00085093
Iteration 7/25 | Loss: 0.00085092
Iteration 8/25 | Loss: 0.00085092
Iteration 9/25 | Loss: 0.00085092
Iteration 10/25 | Loss: 0.00085092
Iteration 11/25 | Loss: 0.00085092
Iteration 12/25 | Loss: 0.00085092
Iteration 13/25 | Loss: 0.00085092
Iteration 14/25 | Loss: 0.00085092
Iteration 15/25 | Loss: 0.00085092
Iteration 16/25 | Loss: 0.00085092
Iteration 17/25 | Loss: 0.00085092
Iteration 18/25 | Loss: 0.00085092
Iteration 19/25 | Loss: 0.00085092
Iteration 20/25 | Loss: 0.00085092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008509233593940735, 0.0008509233593940735, 0.0008509233593940735, 0.0008509233593940735, 0.0008509233593940735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008509233593940735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085092
Iteration 2/1000 | Loss: 0.00008686
Iteration 3/1000 | Loss: 0.00005732
Iteration 4/1000 | Loss: 0.00005049
Iteration 5/1000 | Loss: 0.00004878
Iteration 6/1000 | Loss: 0.00004657
Iteration 7/1000 | Loss: 0.00004624
Iteration 8/1000 | Loss: 0.00033825
Iteration 9/1000 | Loss: 0.00004362
Iteration 10/1000 | Loss: 0.00016598
Iteration 11/1000 | Loss: 0.00004318
Iteration 12/1000 | Loss: 0.00004147
Iteration 13/1000 | Loss: 0.00004002
Iteration 14/1000 | Loss: 0.00004258
Iteration 15/1000 | Loss: 0.00004056
Iteration 16/1000 | Loss: 0.00031352
Iteration 17/1000 | Loss: 0.00004069
Iteration 18/1000 | Loss: 0.00163281
Iteration 19/1000 | Loss: 0.00014340
Iteration 20/1000 | Loss: 0.00005290
Iteration 21/1000 | Loss: 0.00006080
Iteration 22/1000 | Loss: 0.00007513
Iteration 23/1000 | Loss: 0.00003964
Iteration 24/1000 | Loss: 0.00003887
Iteration 25/1000 | Loss: 0.00003846
Iteration 26/1000 | Loss: 0.00221817
Iteration 27/1000 | Loss: 0.00260145
Iteration 28/1000 | Loss: 0.00103555
Iteration 29/1000 | Loss: 0.00030580
Iteration 30/1000 | Loss: 0.00005620
Iteration 31/1000 | Loss: 0.00040718
Iteration 32/1000 | Loss: 0.00096137
Iteration 33/1000 | Loss: 0.00060665
Iteration 34/1000 | Loss: 0.00036832
Iteration 35/1000 | Loss: 0.00004625
Iteration 36/1000 | Loss: 0.00051230
Iteration 37/1000 | Loss: 0.00011886
Iteration 38/1000 | Loss: 0.00003698
Iteration 39/1000 | Loss: 0.00003776
Iteration 40/1000 | Loss: 0.00047689
Iteration 41/1000 | Loss: 0.00012042
Iteration 42/1000 | Loss: 0.00066480
Iteration 43/1000 | Loss: 0.00041543
Iteration 44/1000 | Loss: 0.00043904
Iteration 45/1000 | Loss: 0.00011113
Iteration 46/1000 | Loss: 0.00003353
Iteration 47/1000 | Loss: 0.00003487
Iteration 48/1000 | Loss: 0.00003007
Iteration 49/1000 | Loss: 0.00022191
Iteration 50/1000 | Loss: 0.00003524
Iteration 51/1000 | Loss: 0.00011412
Iteration 52/1000 | Loss: 0.00002755
Iteration 53/1000 | Loss: 0.00002526
Iteration 54/1000 | Loss: 0.00002634
Iteration 55/1000 | Loss: 0.00002928
Iteration 56/1000 | Loss: 0.00002736
Iteration 57/1000 | Loss: 0.00002792
Iteration 58/1000 | Loss: 0.00002695
Iteration 59/1000 | Loss: 0.00002810
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002545
Iteration 62/1000 | Loss: 0.00002529
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002594
Iteration 65/1000 | Loss: 0.00002530
Iteration 66/1000 | Loss: 0.00002512
Iteration 67/1000 | Loss: 0.00009840
Iteration 68/1000 | Loss: 0.00002748
Iteration 69/1000 | Loss: 0.00002579
Iteration 70/1000 | Loss: 0.00002530
Iteration 71/1000 | Loss: 0.00015279
Iteration 72/1000 | Loss: 0.00002860
Iteration 73/1000 | Loss: 0.00003314
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002437
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002389
Iteration 78/1000 | Loss: 0.00002832
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002755
Iteration 81/1000 | Loss: 0.00033714
Iteration 82/1000 | Loss: 0.00055632
Iteration 83/1000 | Loss: 0.00004119
Iteration 84/1000 | Loss: 0.00012325
Iteration 85/1000 | Loss: 0.00002876
Iteration 86/1000 | Loss: 0.00002757
Iteration 87/1000 | Loss: 0.00058419
Iteration 88/1000 | Loss: 0.00007448
Iteration 89/1000 | Loss: 0.00013373
Iteration 90/1000 | Loss: 0.00039743
Iteration 91/1000 | Loss: 0.00011177
Iteration 92/1000 | Loss: 0.00025959
Iteration 93/1000 | Loss: 0.00020726
Iteration 94/1000 | Loss: 0.00012171
Iteration 95/1000 | Loss: 0.00009412
Iteration 96/1000 | Loss: 0.00006399
Iteration 97/1000 | Loss: 0.00004657
Iteration 98/1000 | Loss: 0.00002882
Iteration 99/1000 | Loss: 0.00002750
Iteration 100/1000 | Loss: 0.00002656
Iteration 101/1000 | Loss: 0.00002528
Iteration 102/1000 | Loss: 0.00002445
Iteration 103/1000 | Loss: 0.00002365
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002224
Iteration 106/1000 | Loss: 0.00002205
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002171
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002170
Iteration 116/1000 | Loss: 0.00002169
Iteration 117/1000 | Loss: 0.00002169
Iteration 118/1000 | Loss: 0.00002169
Iteration 119/1000 | Loss: 0.00002168
Iteration 120/1000 | Loss: 0.00002168
Iteration 121/1000 | Loss: 0.00002168
Iteration 122/1000 | Loss: 0.00002168
Iteration 123/1000 | Loss: 0.00002168
Iteration 124/1000 | Loss: 0.00002168
Iteration 125/1000 | Loss: 0.00002168
Iteration 126/1000 | Loss: 0.00002168
Iteration 127/1000 | Loss: 0.00002168
Iteration 128/1000 | Loss: 0.00002168
Iteration 129/1000 | Loss: 0.00002168
Iteration 130/1000 | Loss: 0.00002168
Iteration 131/1000 | Loss: 0.00002167
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002167
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00002167
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002167
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002166
Iteration 144/1000 | Loss: 0.00002165
Iteration 145/1000 | Loss: 0.00002165
Iteration 146/1000 | Loss: 0.00002165
Iteration 147/1000 | Loss: 0.00002165
Iteration 148/1000 | Loss: 0.00002165
Iteration 149/1000 | Loss: 0.00002164
Iteration 150/1000 | Loss: 0.00002164
Iteration 151/1000 | Loss: 0.00002164
Iteration 152/1000 | Loss: 0.00002164
Iteration 153/1000 | Loss: 0.00002162
Iteration 154/1000 | Loss: 0.00002162
Iteration 155/1000 | Loss: 0.00002162
Iteration 156/1000 | Loss: 0.00002162
Iteration 157/1000 | Loss: 0.00002162
Iteration 158/1000 | Loss: 0.00002162
Iteration 159/1000 | Loss: 0.00002162
Iteration 160/1000 | Loss: 0.00002162
Iteration 161/1000 | Loss: 0.00002162
Iteration 162/1000 | Loss: 0.00002162
Iteration 163/1000 | Loss: 0.00002162
Iteration 164/1000 | Loss: 0.00002162
Iteration 165/1000 | Loss: 0.00002162
Iteration 166/1000 | Loss: 0.00002183
Iteration 167/1000 | Loss: 0.00002183
Iteration 168/1000 | Loss: 0.00002182
Iteration 169/1000 | Loss: 0.00002182
Iteration 170/1000 | Loss: 0.00002182
Iteration 171/1000 | Loss: 0.00002182
Iteration 172/1000 | Loss: 0.00002179
Iteration 173/1000 | Loss: 0.00002173
Iteration 174/1000 | Loss: 0.00002166
Iteration 175/1000 | Loss: 0.00002157
Iteration 176/1000 | Loss: 0.00002157
Iteration 177/1000 | Loss: 0.00002156
Iteration 178/1000 | Loss: 0.00002156
Iteration 179/1000 | Loss: 0.00002156
Iteration 180/1000 | Loss: 0.00002156
Iteration 181/1000 | Loss: 0.00002156
Iteration 182/1000 | Loss: 0.00002156
Iteration 183/1000 | Loss: 0.00002156
Iteration 184/1000 | Loss: 0.00002156
Iteration 185/1000 | Loss: 0.00002156
Iteration 186/1000 | Loss: 0.00002156
Iteration 187/1000 | Loss: 0.00002155
Iteration 188/1000 | Loss: 0.00002155
Iteration 189/1000 | Loss: 0.00002155
Iteration 190/1000 | Loss: 0.00002155
Iteration 191/1000 | Loss: 0.00002155
Iteration 192/1000 | Loss: 0.00002154
Iteration 193/1000 | Loss: 0.00002154
Iteration 194/1000 | Loss: 0.00002154
Iteration 195/1000 | Loss: 0.00002153
Iteration 196/1000 | Loss: 0.00002153
Iteration 197/1000 | Loss: 0.00002153
Iteration 198/1000 | Loss: 0.00002153
Iteration 199/1000 | Loss: 0.00002153
Iteration 200/1000 | Loss: 0.00002153
Iteration 201/1000 | Loss: 0.00002153
Iteration 202/1000 | Loss: 0.00002152
Iteration 203/1000 | Loss: 0.00002152
Iteration 204/1000 | Loss: 0.00002152
Iteration 205/1000 | Loss: 0.00002152
Iteration 206/1000 | Loss: 0.00002152
Iteration 207/1000 | Loss: 0.00002152
Iteration 208/1000 | Loss: 0.00002152
Iteration 209/1000 | Loss: 0.00002152
Iteration 210/1000 | Loss: 0.00002152
Iteration 211/1000 | Loss: 0.00002152
Iteration 212/1000 | Loss: 0.00002152
Iteration 213/1000 | Loss: 0.00002152
Iteration 214/1000 | Loss: 0.00002152
Iteration 215/1000 | Loss: 0.00002152
Iteration 216/1000 | Loss: 0.00002152
Iteration 217/1000 | Loss: 0.00002152
Iteration 218/1000 | Loss: 0.00002152
Iteration 219/1000 | Loss: 0.00002152
Iteration 220/1000 | Loss: 0.00002151
Iteration 221/1000 | Loss: 0.00002151
Iteration 222/1000 | Loss: 0.00002151
Iteration 223/1000 | Loss: 0.00002151
Iteration 224/1000 | Loss: 0.00002151
Iteration 225/1000 | Loss: 0.00002151
Iteration 226/1000 | Loss: 0.00002151
Iteration 227/1000 | Loss: 0.00002151
Iteration 228/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.151422631868627e-05, 2.151422631868627e-05, 2.151422631868627e-05, 2.151422631868627e-05, 2.151422631868627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.151422631868627e-05

Optimization complete. Final v2v error: 3.600644111633301 mm

Highest mean error: 12.520224571228027 mm for frame 41

Lowest mean error: 2.8637478351593018 mm for frame 140

Saving results

Total time: 238.93783020973206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983638
Iteration 2/25 | Loss: 0.00983638
Iteration 3/25 | Loss: 0.00983637
Iteration 4/25 | Loss: 0.00983637
Iteration 5/25 | Loss: 0.00983637
Iteration 6/25 | Loss: 0.00277995
Iteration 7/25 | Loss: 0.00177596
Iteration 8/25 | Loss: 0.00157796
Iteration 9/25 | Loss: 0.00160739
Iteration 10/25 | Loss: 0.00152222
Iteration 11/25 | Loss: 0.00144862
Iteration 12/25 | Loss: 0.00131728
Iteration 13/25 | Loss: 0.00131574
Iteration 14/25 | Loss: 0.00129382
Iteration 15/25 | Loss: 0.00124950
Iteration 16/25 | Loss: 0.00123600
Iteration 17/25 | Loss: 0.00123503
Iteration 18/25 | Loss: 0.00121625
Iteration 19/25 | Loss: 0.00120322
Iteration 20/25 | Loss: 0.00119734
Iteration 21/25 | Loss: 0.00118406
Iteration 22/25 | Loss: 0.00118422
Iteration 23/25 | Loss: 0.00117933
Iteration 24/25 | Loss: 0.00117504
Iteration 25/25 | Loss: 0.00117333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33000135
Iteration 2/25 | Loss: 0.00116639
Iteration 3/25 | Loss: 0.00107726
Iteration 4/25 | Loss: 0.00107726
Iteration 5/25 | Loss: 0.00107726
Iteration 6/25 | Loss: 0.00107726
Iteration 7/25 | Loss: 0.00107725
Iteration 8/25 | Loss: 0.00107725
Iteration 9/25 | Loss: 0.00107725
Iteration 10/25 | Loss: 0.00107725
Iteration 11/25 | Loss: 0.00107725
Iteration 12/25 | Loss: 0.00107725
Iteration 13/25 | Loss: 0.00107725
Iteration 14/25 | Loss: 0.00107725
Iteration 15/25 | Loss: 0.00107725
Iteration 16/25 | Loss: 0.00107725
Iteration 17/25 | Loss: 0.00107725
Iteration 18/25 | Loss: 0.00107725
Iteration 19/25 | Loss: 0.00107725
Iteration 20/25 | Loss: 0.00107725
Iteration 21/25 | Loss: 0.00107725
Iteration 22/25 | Loss: 0.00107725
Iteration 23/25 | Loss: 0.00107725
Iteration 24/25 | Loss: 0.00107725
Iteration 25/25 | Loss: 0.00107725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107725
Iteration 2/1000 | Loss: 0.00082366
Iteration 3/1000 | Loss: 0.00010948
Iteration 4/1000 | Loss: 0.00031799
Iteration 5/1000 | Loss: 0.00030076
Iteration 6/1000 | Loss: 0.00030850
Iteration 7/1000 | Loss: 0.00018325
Iteration 8/1000 | Loss: 0.00006862
Iteration 9/1000 | Loss: 0.00006238
Iteration 10/1000 | Loss: 0.00013844
Iteration 11/1000 | Loss: 0.00040890
Iteration 12/1000 | Loss: 0.00022925
Iteration 13/1000 | Loss: 0.00017458
Iteration 14/1000 | Loss: 0.00022229
Iteration 15/1000 | Loss: 0.00018272
Iteration 16/1000 | Loss: 0.00018053
Iteration 17/1000 | Loss: 0.00020525
Iteration 18/1000 | Loss: 0.00013149
Iteration 19/1000 | Loss: 0.00031264
Iteration 20/1000 | Loss: 0.00023809
Iteration 21/1000 | Loss: 0.00030321
Iteration 22/1000 | Loss: 0.00027048
Iteration 23/1000 | Loss: 0.00030342
Iteration 24/1000 | Loss: 0.00017597
Iteration 25/1000 | Loss: 0.00007323
Iteration 26/1000 | Loss: 0.00013777
Iteration 27/1000 | Loss: 0.00009911
Iteration 28/1000 | Loss: 0.00014831
Iteration 29/1000 | Loss: 0.00029650
Iteration 30/1000 | Loss: 0.00036695
Iteration 31/1000 | Loss: 0.00020669
Iteration 32/1000 | Loss: 0.00056470
Iteration 33/1000 | Loss: 0.00022838
Iteration 34/1000 | Loss: 0.00028785
Iteration 35/1000 | Loss: 0.00031306
Iteration 36/1000 | Loss: 0.00011405
Iteration 37/1000 | Loss: 0.00008931
Iteration 38/1000 | Loss: 0.00009810
Iteration 39/1000 | Loss: 0.00019641
Iteration 40/1000 | Loss: 0.00027785
Iteration 41/1000 | Loss: 0.00005418
Iteration 42/1000 | Loss: 0.00007744
Iteration 43/1000 | Loss: 0.00012219
Iteration 44/1000 | Loss: 0.00015313
Iteration 45/1000 | Loss: 0.00012896
Iteration 46/1000 | Loss: 0.00004284
Iteration 47/1000 | Loss: 0.00005909
Iteration 48/1000 | Loss: 0.00013683
Iteration 49/1000 | Loss: 0.00008869
Iteration 50/1000 | Loss: 0.00014166
Iteration 51/1000 | Loss: 0.00009000
Iteration 52/1000 | Loss: 0.00004446
Iteration 53/1000 | Loss: 0.00003810
Iteration 54/1000 | Loss: 0.00003626
Iteration 55/1000 | Loss: 0.00003419
Iteration 56/1000 | Loss: 0.00007059
Iteration 57/1000 | Loss: 0.00026259
Iteration 58/1000 | Loss: 0.00059768
Iteration 59/1000 | Loss: 0.00024265
Iteration 60/1000 | Loss: 0.00005442
Iteration 61/1000 | Loss: 0.00010985
Iteration 62/1000 | Loss: 0.00004581
Iteration 63/1000 | Loss: 0.00009896
Iteration 64/1000 | Loss: 0.00028149
Iteration 65/1000 | Loss: 0.00010801
Iteration 66/1000 | Loss: 0.00011107
Iteration 67/1000 | Loss: 0.00009468
Iteration 68/1000 | Loss: 0.00006958
Iteration 69/1000 | Loss: 0.00034600
Iteration 70/1000 | Loss: 0.00099125
Iteration 71/1000 | Loss: 0.00113390
Iteration 72/1000 | Loss: 0.00039839
Iteration 73/1000 | Loss: 0.00004647
Iteration 74/1000 | Loss: 0.00003465
Iteration 75/1000 | Loss: 0.00009841
Iteration 76/1000 | Loss: 0.00015087
Iteration 77/1000 | Loss: 0.00017157
Iteration 78/1000 | Loss: 0.00023363
Iteration 79/1000 | Loss: 0.00027800
Iteration 80/1000 | Loss: 0.00029207
Iteration 81/1000 | Loss: 0.00018040
Iteration 82/1000 | Loss: 0.00014337
Iteration 83/1000 | Loss: 0.00061153
Iteration 84/1000 | Loss: 0.00040455
Iteration 85/1000 | Loss: 0.00042884
Iteration 86/1000 | Loss: 0.00022845
Iteration 87/1000 | Loss: 0.00009516
Iteration 88/1000 | Loss: 0.00002931
Iteration 89/1000 | Loss: 0.00003279
Iteration 90/1000 | Loss: 0.00011677
Iteration 91/1000 | Loss: 0.00009547
Iteration 92/1000 | Loss: 0.00003732
Iteration 93/1000 | Loss: 0.00012129
Iteration 94/1000 | Loss: 0.00011878
Iteration 95/1000 | Loss: 0.00002010
Iteration 96/1000 | Loss: 0.00016743
Iteration 97/1000 | Loss: 0.00004131
Iteration 98/1000 | Loss: 0.00002665
Iteration 99/1000 | Loss: 0.00012246
Iteration 100/1000 | Loss: 0.00003718
Iteration 101/1000 | Loss: 0.00015551
Iteration 102/1000 | Loss: 0.00003047
Iteration 103/1000 | Loss: 0.00003103
Iteration 104/1000 | Loss: 0.00002106
Iteration 105/1000 | Loss: 0.00002079
Iteration 106/1000 | Loss: 0.00002301
Iteration 107/1000 | Loss: 0.00002882
Iteration 108/1000 | Loss: 0.00002195
Iteration 109/1000 | Loss: 0.00001930
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001477
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001404
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001363
Iteration 118/1000 | Loss: 0.00001360
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001338
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001336
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001335
Iteration 135/1000 | Loss: 0.00001335
Iteration 136/1000 | Loss: 0.00001334
Iteration 137/1000 | Loss: 0.00001334
Iteration 138/1000 | Loss: 0.00001333
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001318
Iteration 142/1000 | Loss: 0.00001318
Iteration 143/1000 | Loss: 0.00001318
Iteration 144/1000 | Loss: 0.00001318
Iteration 145/1000 | Loss: 0.00001318
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001310
Iteration 173/1000 | Loss: 0.00001310
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001307
Iteration 186/1000 | Loss: 0.00001307
Iteration 187/1000 | Loss: 0.00001307
Iteration 188/1000 | Loss: 0.00001306
Iteration 189/1000 | Loss: 0.00001306
Iteration 190/1000 | Loss: 0.00001306
Iteration 191/1000 | Loss: 0.00001306
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001305
Iteration 195/1000 | Loss: 0.00001305
Iteration 196/1000 | Loss: 0.00001305
Iteration 197/1000 | Loss: 0.00001304
Iteration 198/1000 | Loss: 0.00001304
Iteration 199/1000 | Loss: 0.00001304
Iteration 200/1000 | Loss: 0.00001304
Iteration 201/1000 | Loss: 0.00001304
Iteration 202/1000 | Loss: 0.00001304
Iteration 203/1000 | Loss: 0.00001303
Iteration 204/1000 | Loss: 0.00001303
Iteration 205/1000 | Loss: 0.00001303
Iteration 206/1000 | Loss: 0.00001303
Iteration 207/1000 | Loss: 0.00001303
Iteration 208/1000 | Loss: 0.00001303
Iteration 209/1000 | Loss: 0.00001303
Iteration 210/1000 | Loss: 0.00001303
Iteration 211/1000 | Loss: 0.00001303
Iteration 212/1000 | Loss: 0.00001303
Iteration 213/1000 | Loss: 0.00001303
Iteration 214/1000 | Loss: 0.00001303
Iteration 215/1000 | Loss: 0.00001302
Iteration 216/1000 | Loss: 0.00001302
Iteration 217/1000 | Loss: 0.00001302
Iteration 218/1000 | Loss: 0.00001302
Iteration 219/1000 | Loss: 0.00001302
Iteration 220/1000 | Loss: 0.00001302
Iteration 221/1000 | Loss: 0.00001301
Iteration 222/1000 | Loss: 0.00001301
Iteration 223/1000 | Loss: 0.00001301
Iteration 224/1000 | Loss: 0.00001301
Iteration 225/1000 | Loss: 0.00001301
Iteration 226/1000 | Loss: 0.00001301
Iteration 227/1000 | Loss: 0.00001301
Iteration 228/1000 | Loss: 0.00001300
Iteration 229/1000 | Loss: 0.00001300
Iteration 230/1000 | Loss: 0.00001300
Iteration 231/1000 | Loss: 0.00001300
Iteration 232/1000 | Loss: 0.00001300
Iteration 233/1000 | Loss: 0.00001300
Iteration 234/1000 | Loss: 0.00001300
Iteration 235/1000 | Loss: 0.00001299
Iteration 236/1000 | Loss: 0.00001299
Iteration 237/1000 | Loss: 0.00001299
Iteration 238/1000 | Loss: 0.00001299
Iteration 239/1000 | Loss: 0.00001299
Iteration 240/1000 | Loss: 0.00001299
Iteration 241/1000 | Loss: 0.00001299
Iteration 242/1000 | Loss: 0.00001299
Iteration 243/1000 | Loss: 0.00001299
Iteration 244/1000 | Loss: 0.00001299
Iteration 245/1000 | Loss: 0.00001299
Iteration 246/1000 | Loss: 0.00001299
Iteration 247/1000 | Loss: 0.00001299
Iteration 248/1000 | Loss: 0.00001299
Iteration 249/1000 | Loss: 0.00001299
Iteration 250/1000 | Loss: 0.00001299
Iteration 251/1000 | Loss: 0.00001299
Iteration 252/1000 | Loss: 0.00001299
Iteration 253/1000 | Loss: 0.00001299
Iteration 254/1000 | Loss: 0.00001299
Iteration 255/1000 | Loss: 0.00001299
Iteration 256/1000 | Loss: 0.00001299
Iteration 257/1000 | Loss: 0.00001299
Iteration 258/1000 | Loss: 0.00001298
Iteration 259/1000 | Loss: 0.00001298
Iteration 260/1000 | Loss: 0.00001298
Iteration 261/1000 | Loss: 0.00001298
Iteration 262/1000 | Loss: 0.00001298
Iteration 263/1000 | Loss: 0.00001298
Iteration 264/1000 | Loss: 0.00001298
Iteration 265/1000 | Loss: 0.00001298
Iteration 266/1000 | Loss: 0.00001298
Iteration 267/1000 | Loss: 0.00001298
Iteration 268/1000 | Loss: 0.00001298
Iteration 269/1000 | Loss: 0.00001298
Iteration 270/1000 | Loss: 0.00001298
Iteration 271/1000 | Loss: 0.00001298
Iteration 272/1000 | Loss: 0.00001298
Iteration 273/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [1.2984838576812763e-05, 1.2984838576812763e-05, 1.2984838576812763e-05, 1.2984838576812763e-05, 1.2984838576812763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2984838576812763e-05

Optimization complete. Final v2v error: 2.9471895694732666 mm

Highest mean error: 4.735983371734619 mm for frame 99

Lowest mean error: 2.690152645111084 mm for frame 85

Saving results

Total time: 249.35248684883118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795105
Iteration 2/25 | Loss: 0.00117501
Iteration 3/25 | Loss: 0.00107177
Iteration 4/25 | Loss: 0.00105274
Iteration 5/25 | Loss: 0.00104449
Iteration 6/25 | Loss: 0.00104249
Iteration 7/25 | Loss: 0.00104249
Iteration 8/25 | Loss: 0.00104249
Iteration 9/25 | Loss: 0.00104249
Iteration 10/25 | Loss: 0.00104249
Iteration 11/25 | Loss: 0.00104249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00104249338619411, 0.00104249338619411, 0.00104249338619411, 0.00104249338619411, 0.00104249338619411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00104249338619411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32989275
Iteration 2/25 | Loss: 0.00104442
Iteration 3/25 | Loss: 0.00104442
Iteration 4/25 | Loss: 0.00104441
Iteration 5/25 | Loss: 0.00104441
Iteration 6/25 | Loss: 0.00104441
Iteration 7/25 | Loss: 0.00104441
Iteration 8/25 | Loss: 0.00104441
Iteration 9/25 | Loss: 0.00104441
Iteration 10/25 | Loss: 0.00104441
Iteration 11/25 | Loss: 0.00104441
Iteration 12/25 | Loss: 0.00104441
Iteration 13/25 | Loss: 0.00104441
Iteration 14/25 | Loss: 0.00104441
Iteration 15/25 | Loss: 0.00104441
Iteration 16/25 | Loss: 0.00104441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010444123763591051, 0.0010444123763591051, 0.0010444123763591051, 0.0010444123763591051, 0.0010444123763591051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010444123763591051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104441
Iteration 2/1000 | Loss: 0.00004265
Iteration 3/1000 | Loss: 0.00002666
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001314
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001251
Iteration 20/1000 | Loss: 0.00001246
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001237
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001229
Iteration 34/1000 | Loss: 0.00001228
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001227
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001213
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001209
Iteration 113/1000 | Loss: 0.00001209
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001207
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2054902072122786e-05, 1.2054902072122786e-05, 1.2054902072122786e-05, 1.2054902072122786e-05, 1.2054902072122786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2054902072122786e-05

Optimization complete. Final v2v error: 2.8819193840026855 mm

Highest mean error: 4.779013633728027 mm for frame 25

Lowest mean error: 2.2596187591552734 mm for frame 78

Saving results

Total time: 42.355496406555176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830915
Iteration 2/25 | Loss: 0.00151696
Iteration 3/25 | Loss: 0.00121727
Iteration 4/25 | Loss: 0.00118654
Iteration 5/25 | Loss: 0.00116727
Iteration 6/25 | Loss: 0.00115901
Iteration 7/25 | Loss: 0.00114469
Iteration 8/25 | Loss: 0.00114114
Iteration 9/25 | Loss: 0.00113928
Iteration 10/25 | Loss: 0.00113903
Iteration 11/25 | Loss: 0.00113732
Iteration 12/25 | Loss: 0.00113647
Iteration 13/25 | Loss: 0.00113696
Iteration 14/25 | Loss: 0.00113536
Iteration 15/25 | Loss: 0.00113447
Iteration 16/25 | Loss: 0.00113416
Iteration 17/25 | Loss: 0.00113409
Iteration 18/25 | Loss: 0.00113409
Iteration 19/25 | Loss: 0.00113409
Iteration 20/25 | Loss: 0.00113408
Iteration 21/25 | Loss: 0.00113408
Iteration 22/25 | Loss: 0.00113408
Iteration 23/25 | Loss: 0.00113408
Iteration 24/25 | Loss: 0.00113408
Iteration 25/25 | Loss: 0.00113408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83879161
Iteration 2/25 | Loss: 0.00078920
Iteration 3/25 | Loss: 0.00078919
Iteration 4/25 | Loss: 0.00078919
Iteration 5/25 | Loss: 0.00078919
Iteration 6/25 | Loss: 0.00078919
Iteration 7/25 | Loss: 0.00078919
Iteration 8/25 | Loss: 0.00078919
Iteration 9/25 | Loss: 0.00078919
Iteration 10/25 | Loss: 0.00078919
Iteration 11/25 | Loss: 0.00078919
Iteration 12/25 | Loss: 0.00078919
Iteration 13/25 | Loss: 0.00078919
Iteration 14/25 | Loss: 0.00078919
Iteration 15/25 | Loss: 0.00078919
Iteration 16/25 | Loss: 0.00078919
Iteration 17/25 | Loss: 0.00078919
Iteration 18/25 | Loss: 0.00078919
Iteration 19/25 | Loss: 0.00078919
Iteration 20/25 | Loss: 0.00078919
Iteration 21/25 | Loss: 0.00078919
Iteration 22/25 | Loss: 0.00078919
Iteration 23/25 | Loss: 0.00078919
Iteration 24/25 | Loss: 0.00078919
Iteration 25/25 | Loss: 0.00078919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078919
Iteration 2/1000 | Loss: 0.00002059
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00013227
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001459
Iteration 9/1000 | Loss: 0.00001437
Iteration 10/1000 | Loss: 0.00001416
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001393
Iteration 14/1000 | Loss: 0.00006176
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00007361
Iteration 60/1000 | Loss: 0.00010046
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00004516
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001337
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00002458
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001339
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001784
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001334
Iteration 94/1000 | Loss: 0.00001334
Iteration 95/1000 | Loss: 0.00001334
Iteration 96/1000 | Loss: 0.00001334
Iteration 97/1000 | Loss: 0.00001334
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001333
Iteration 105/1000 | Loss: 0.00001333
Iteration 106/1000 | Loss: 0.00001333
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001331
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001326
Iteration 135/1000 | Loss: 0.00001326
Iteration 136/1000 | Loss: 0.00001326
Iteration 137/1000 | Loss: 0.00001326
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001325
Iteration 140/1000 | Loss: 0.00001325
Iteration 141/1000 | Loss: 0.00001325
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.3247490642243065e-05, 1.3247490642243065e-05, 1.3247490642243065e-05, 1.3247490642243065e-05, 1.3247490642243065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3247490642243065e-05

Optimization complete. Final v2v error: 3.0809199810028076 mm

Highest mean error: 3.5017576217651367 mm for frame 55

Lowest mean error: 2.7269504070281982 mm for frame 239

Saving results

Total time: 80.65133452415466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455368
Iteration 2/25 | Loss: 0.00115901
Iteration 3/25 | Loss: 0.00110076
Iteration 4/25 | Loss: 0.00109476
Iteration 5/25 | Loss: 0.00109310
Iteration 6/25 | Loss: 0.00109282
Iteration 7/25 | Loss: 0.00109282
Iteration 8/25 | Loss: 0.00109282
Iteration 9/25 | Loss: 0.00109282
Iteration 10/25 | Loss: 0.00109282
Iteration 11/25 | Loss: 0.00109282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010928174015134573, 0.0010928174015134573, 0.0010928174015134573, 0.0010928174015134573, 0.0010928174015134573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010928174015134573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.01988983
Iteration 2/25 | Loss: 0.00083356
Iteration 3/25 | Loss: 0.00083356
Iteration 4/25 | Loss: 0.00083355
Iteration 5/25 | Loss: 0.00083355
Iteration 6/25 | Loss: 0.00083355
Iteration 7/25 | Loss: 0.00083355
Iteration 8/25 | Loss: 0.00083355
Iteration 9/25 | Loss: 0.00083355
Iteration 10/25 | Loss: 0.00083355
Iteration 11/25 | Loss: 0.00083355
Iteration 12/25 | Loss: 0.00083355
Iteration 13/25 | Loss: 0.00083355
Iteration 14/25 | Loss: 0.00083355
Iteration 15/25 | Loss: 0.00083355
Iteration 16/25 | Loss: 0.00083355
Iteration 17/25 | Loss: 0.00083355
Iteration 18/25 | Loss: 0.00083355
Iteration 19/25 | Loss: 0.00083355
Iteration 20/25 | Loss: 0.00083355
Iteration 21/25 | Loss: 0.00083355
Iteration 22/25 | Loss: 0.00083355
Iteration 23/25 | Loss: 0.00083355
Iteration 24/25 | Loss: 0.00083355
Iteration 25/25 | Loss: 0.00083355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083355
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001508
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001214
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001207
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001202
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001199
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001193
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001188
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.1833177268272266e-05, 1.1833177268272266e-05, 1.1833177268272266e-05, 1.1833177268272266e-05, 1.1833177268272266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1833177268272266e-05

Optimization complete. Final v2v error: 2.9231514930725098 mm

Highest mean error: 3.289490222930908 mm for frame 114

Lowest mean error: 2.6263647079467773 mm for frame 125

Saving results

Total time: 29.043553113937378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368962
Iteration 2/25 | Loss: 0.00122752
Iteration 3/25 | Loss: 0.00109207
Iteration 4/25 | Loss: 0.00107455
Iteration 5/25 | Loss: 0.00106993
Iteration 6/25 | Loss: 0.00106943
Iteration 7/25 | Loss: 0.00106943
Iteration 8/25 | Loss: 0.00106943
Iteration 9/25 | Loss: 0.00106943
Iteration 10/25 | Loss: 0.00106943
Iteration 11/25 | Loss: 0.00106943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010694349184632301, 0.0010694349184632301, 0.0010694349184632301, 0.0010694349184632301, 0.0010694349184632301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010694349184632301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32815814
Iteration 2/25 | Loss: 0.00072857
Iteration 3/25 | Loss: 0.00072857
Iteration 4/25 | Loss: 0.00072856
Iteration 5/25 | Loss: 0.00072856
Iteration 6/25 | Loss: 0.00072856
Iteration 7/25 | Loss: 0.00072856
Iteration 8/25 | Loss: 0.00072856
Iteration 9/25 | Loss: 0.00072856
Iteration 10/25 | Loss: 0.00072856
Iteration 11/25 | Loss: 0.00072856
Iteration 12/25 | Loss: 0.00072856
Iteration 13/25 | Loss: 0.00072856
Iteration 14/25 | Loss: 0.00072856
Iteration 15/25 | Loss: 0.00072856
Iteration 16/25 | Loss: 0.00072856
Iteration 17/25 | Loss: 0.00072856
Iteration 18/25 | Loss: 0.00072856
Iteration 19/25 | Loss: 0.00072856
Iteration 20/25 | Loss: 0.00072856
Iteration 21/25 | Loss: 0.00072856
Iteration 22/25 | Loss: 0.00072856
Iteration 23/25 | Loss: 0.00072856
Iteration 24/25 | Loss: 0.00072856
Iteration 25/25 | Loss: 0.00072856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072856
Iteration 2/1000 | Loss: 0.00002685
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001563
Iteration 6/1000 | Loss: 0.00001487
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001385
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001361
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001346
Iteration 23/1000 | Loss: 0.00001346
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001342
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001329
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001328
Iteration 37/1000 | Loss: 0.00001328
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001327
Iteration 40/1000 | Loss: 0.00001326
Iteration 41/1000 | Loss: 0.00001326
Iteration 42/1000 | Loss: 0.00001325
Iteration 43/1000 | Loss: 0.00001325
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001324
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001323
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001316
Iteration 53/1000 | Loss: 0.00001315
Iteration 54/1000 | Loss: 0.00001314
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001313
Iteration 57/1000 | Loss: 0.00001313
Iteration 58/1000 | Loss: 0.00001313
Iteration 59/1000 | Loss: 0.00001313
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001312
Iteration 66/1000 | Loss: 0.00001312
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001311
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001309
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001302
Iteration 94/1000 | Loss: 0.00001302
Iteration 95/1000 | Loss: 0.00001302
Iteration 96/1000 | Loss: 0.00001302
Iteration 97/1000 | Loss: 0.00001302
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001301
Iteration 102/1000 | Loss: 0.00001301
Iteration 103/1000 | Loss: 0.00001301
Iteration 104/1000 | Loss: 0.00001300
Iteration 105/1000 | Loss: 0.00001300
Iteration 106/1000 | Loss: 0.00001300
Iteration 107/1000 | Loss: 0.00001299
Iteration 108/1000 | Loss: 0.00001299
Iteration 109/1000 | Loss: 0.00001299
Iteration 110/1000 | Loss: 0.00001299
Iteration 111/1000 | Loss: 0.00001298
Iteration 112/1000 | Loss: 0.00001298
Iteration 113/1000 | Loss: 0.00001298
Iteration 114/1000 | Loss: 0.00001298
Iteration 115/1000 | Loss: 0.00001298
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001297
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001296
Iteration 127/1000 | Loss: 0.00001296
Iteration 128/1000 | Loss: 0.00001296
Iteration 129/1000 | Loss: 0.00001296
Iteration 130/1000 | Loss: 0.00001296
Iteration 131/1000 | Loss: 0.00001296
Iteration 132/1000 | Loss: 0.00001296
Iteration 133/1000 | Loss: 0.00001296
Iteration 134/1000 | Loss: 0.00001296
Iteration 135/1000 | Loss: 0.00001296
Iteration 136/1000 | Loss: 0.00001296
Iteration 137/1000 | Loss: 0.00001296
Iteration 138/1000 | Loss: 0.00001296
Iteration 139/1000 | Loss: 0.00001296
Iteration 140/1000 | Loss: 0.00001296
Iteration 141/1000 | Loss: 0.00001296
Iteration 142/1000 | Loss: 0.00001296
Iteration 143/1000 | Loss: 0.00001296
Iteration 144/1000 | Loss: 0.00001296
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001296
Iteration 147/1000 | Loss: 0.00001296
Iteration 148/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2964884263055865e-05, 1.2964884263055865e-05, 1.2964884263055865e-05, 1.2964884263055865e-05, 1.2964884263055865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2964884263055865e-05

Optimization complete. Final v2v error: 3.047785758972168 mm

Highest mean error: 3.2699637413024902 mm for frame 144

Lowest mean error: 2.5742878913879395 mm for frame 3

Saving results

Total time: 41.58721709251404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00702428
Iteration 2/25 | Loss: 0.00125790
Iteration 3/25 | Loss: 0.00113768
Iteration 4/25 | Loss: 0.00110170
Iteration 5/25 | Loss: 0.00109191
Iteration 6/25 | Loss: 0.00108842
Iteration 7/25 | Loss: 0.00108672
Iteration 8/25 | Loss: 0.00108644
Iteration 9/25 | Loss: 0.00108621
Iteration 10/25 | Loss: 0.00108853
Iteration 11/25 | Loss: 0.00108894
Iteration 12/25 | Loss: 0.00108554
Iteration 13/25 | Loss: 0.00108540
Iteration 14/25 | Loss: 0.00108533
Iteration 15/25 | Loss: 0.00108533
Iteration 16/25 | Loss: 0.00108533
Iteration 17/25 | Loss: 0.00108533
Iteration 18/25 | Loss: 0.00108533
Iteration 19/25 | Loss: 0.00108533
Iteration 20/25 | Loss: 0.00108533
Iteration 21/25 | Loss: 0.00108533
Iteration 22/25 | Loss: 0.00108533
Iteration 23/25 | Loss: 0.00108533
Iteration 24/25 | Loss: 0.00108533
Iteration 25/25 | Loss: 0.00108532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56487203
Iteration 2/25 | Loss: 0.00083690
Iteration 3/25 | Loss: 0.00083690
Iteration 4/25 | Loss: 0.00083690
Iteration 5/25 | Loss: 0.00083690
Iteration 6/25 | Loss: 0.00083690
Iteration 7/25 | Loss: 0.00083690
Iteration 8/25 | Loss: 0.00083690
Iteration 9/25 | Loss: 0.00083690
Iteration 10/25 | Loss: 0.00083690
Iteration 11/25 | Loss: 0.00083690
Iteration 12/25 | Loss: 0.00083690
Iteration 13/25 | Loss: 0.00083690
Iteration 14/25 | Loss: 0.00083690
Iteration 15/25 | Loss: 0.00083690
Iteration 16/25 | Loss: 0.00083690
Iteration 17/25 | Loss: 0.00083690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008368986891582608, 0.0008368986891582608, 0.0008368986891582608, 0.0008368986891582608, 0.0008368986891582608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008368986891582608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083690
Iteration 2/1000 | Loss: 0.00001841
Iteration 3/1000 | Loss: 0.00001542
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001381
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00025990
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001176
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001125
Iteration 14/1000 | Loss: 0.00001124
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001112
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001111
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001106
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001106
Iteration 42/1000 | Loss: 0.00001106
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001106
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001102
Iteration 51/1000 | Loss: 0.00001102
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001091
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001086
Iteration 70/1000 | Loss: 0.00001086
Iteration 71/1000 | Loss: 0.00001086
Iteration 72/1000 | Loss: 0.00001086
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001085
Iteration 79/1000 | Loss: 0.00001085
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001083
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001081
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001081
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001078
Iteration 104/1000 | Loss: 0.00001078
Iteration 105/1000 | Loss: 0.00001078
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001075
Iteration 112/1000 | Loss: 0.00001075
Iteration 113/1000 | Loss: 0.00001075
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001073
Iteration 121/1000 | Loss: 0.00001073
Iteration 122/1000 | Loss: 0.00001073
Iteration 123/1000 | Loss: 0.00001073
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001072
Iteration 128/1000 | Loss: 0.00001072
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001072
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001070
Iteration 142/1000 | Loss: 0.00001070
Iteration 143/1000 | Loss: 0.00001070
Iteration 144/1000 | Loss: 0.00001070
Iteration 145/1000 | Loss: 0.00001070
Iteration 146/1000 | Loss: 0.00001070
Iteration 147/1000 | Loss: 0.00001070
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001070
Iteration 154/1000 | Loss: 0.00001070
Iteration 155/1000 | Loss: 0.00001070
Iteration 156/1000 | Loss: 0.00001070
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001070
Iteration 160/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.0699451195250731e-05, 1.0699451195250731e-05, 1.0699451195250731e-05, 1.0699451195250731e-05, 1.0699451195250731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0699451195250731e-05

Optimization complete. Final v2v error: 2.803692579269409 mm

Highest mean error: 3.660017728805542 mm for frame 140

Lowest mean error: 2.5339667797088623 mm for frame 5

Saving results

Total time: 62.77763819694519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781975
Iteration 2/25 | Loss: 0.00110924
Iteration 3/25 | Loss: 0.00105312
Iteration 4/25 | Loss: 0.00104507
Iteration 5/25 | Loss: 0.00104288
Iteration 6/25 | Loss: 0.00104246
Iteration 7/25 | Loss: 0.00104246
Iteration 8/25 | Loss: 0.00104246
Iteration 9/25 | Loss: 0.00104246
Iteration 10/25 | Loss: 0.00104246
Iteration 11/25 | Loss: 0.00104246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001042459742166102, 0.001042459742166102, 0.001042459742166102, 0.001042459742166102, 0.001042459742166102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001042459742166102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40184307
Iteration 2/25 | Loss: 0.00081742
Iteration 3/25 | Loss: 0.00081741
Iteration 4/25 | Loss: 0.00081741
Iteration 5/25 | Loss: 0.00081741
Iteration 6/25 | Loss: 0.00081741
Iteration 7/25 | Loss: 0.00081741
Iteration 8/25 | Loss: 0.00081741
Iteration 9/25 | Loss: 0.00081741
Iteration 10/25 | Loss: 0.00081741
Iteration 11/25 | Loss: 0.00081741
Iteration 12/25 | Loss: 0.00081741
Iteration 13/25 | Loss: 0.00081741
Iteration 14/25 | Loss: 0.00081741
Iteration 15/25 | Loss: 0.00081741
Iteration 16/25 | Loss: 0.00081741
Iteration 17/25 | Loss: 0.00081741
Iteration 18/25 | Loss: 0.00081741
Iteration 19/25 | Loss: 0.00081741
Iteration 20/25 | Loss: 0.00081741
Iteration 21/25 | Loss: 0.00081741
Iteration 22/25 | Loss: 0.00081741
Iteration 23/25 | Loss: 0.00081741
Iteration 24/25 | Loss: 0.00081741
Iteration 25/25 | Loss: 0.00081741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081741
Iteration 2/1000 | Loss: 0.00002161
Iteration 3/1000 | Loss: 0.00001391
Iteration 4/1000 | Loss: 0.00001195
Iteration 5/1000 | Loss: 0.00001102
Iteration 6/1000 | Loss: 0.00001052
Iteration 7/1000 | Loss: 0.00001006
Iteration 8/1000 | Loss: 0.00000984
Iteration 9/1000 | Loss: 0.00000966
Iteration 10/1000 | Loss: 0.00000956
Iteration 11/1000 | Loss: 0.00000941
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000927
Iteration 14/1000 | Loss: 0.00000927
Iteration 15/1000 | Loss: 0.00000926
Iteration 16/1000 | Loss: 0.00000925
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000925
Iteration 19/1000 | Loss: 0.00000920
Iteration 20/1000 | Loss: 0.00000919
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000917
Iteration 23/1000 | Loss: 0.00000916
Iteration 24/1000 | Loss: 0.00000916
Iteration 25/1000 | Loss: 0.00000912
Iteration 26/1000 | Loss: 0.00000909
Iteration 27/1000 | Loss: 0.00000909
Iteration 28/1000 | Loss: 0.00000908
Iteration 29/1000 | Loss: 0.00000908
Iteration 30/1000 | Loss: 0.00000908
Iteration 31/1000 | Loss: 0.00000908
Iteration 32/1000 | Loss: 0.00000908
Iteration 33/1000 | Loss: 0.00000908
Iteration 34/1000 | Loss: 0.00000907
Iteration 35/1000 | Loss: 0.00000907
Iteration 36/1000 | Loss: 0.00000907
Iteration 37/1000 | Loss: 0.00000906
Iteration 38/1000 | Loss: 0.00000906
Iteration 39/1000 | Loss: 0.00000905
Iteration 40/1000 | Loss: 0.00000905
Iteration 41/1000 | Loss: 0.00000905
Iteration 42/1000 | Loss: 0.00000905
Iteration 43/1000 | Loss: 0.00000905
Iteration 44/1000 | Loss: 0.00000905
Iteration 45/1000 | Loss: 0.00000905
Iteration 46/1000 | Loss: 0.00000904
Iteration 47/1000 | Loss: 0.00000904
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000904
Iteration 50/1000 | Loss: 0.00000904
Iteration 51/1000 | Loss: 0.00000904
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000903
Iteration 56/1000 | Loss: 0.00000903
Iteration 57/1000 | Loss: 0.00000903
Iteration 58/1000 | Loss: 0.00000903
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000902
Iteration 61/1000 | Loss: 0.00000902
Iteration 62/1000 | Loss: 0.00000902
Iteration 63/1000 | Loss: 0.00000902
Iteration 64/1000 | Loss: 0.00000902
Iteration 65/1000 | Loss: 0.00000901
Iteration 66/1000 | Loss: 0.00000901
Iteration 67/1000 | Loss: 0.00000900
Iteration 68/1000 | Loss: 0.00000900
Iteration 69/1000 | Loss: 0.00000899
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000898
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000898
Iteration 74/1000 | Loss: 0.00000898
Iteration 75/1000 | Loss: 0.00000897
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000897
Iteration 78/1000 | Loss: 0.00000897
Iteration 79/1000 | Loss: 0.00000897
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000897
Iteration 82/1000 | Loss: 0.00000896
Iteration 83/1000 | Loss: 0.00000895
Iteration 84/1000 | Loss: 0.00000895
Iteration 85/1000 | Loss: 0.00000895
Iteration 86/1000 | Loss: 0.00000895
Iteration 87/1000 | Loss: 0.00000895
Iteration 88/1000 | Loss: 0.00000895
Iteration 89/1000 | Loss: 0.00000895
Iteration 90/1000 | Loss: 0.00000895
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000894
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000893
Iteration 101/1000 | Loss: 0.00000893
Iteration 102/1000 | Loss: 0.00000893
Iteration 103/1000 | Loss: 0.00000893
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000892
Iteration 106/1000 | Loss: 0.00000892
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000891
Iteration 109/1000 | Loss: 0.00000891
Iteration 110/1000 | Loss: 0.00000891
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000890
Iteration 113/1000 | Loss: 0.00000890
Iteration 114/1000 | Loss: 0.00000890
Iteration 115/1000 | Loss: 0.00000889
Iteration 116/1000 | Loss: 0.00000889
Iteration 117/1000 | Loss: 0.00000889
Iteration 118/1000 | Loss: 0.00000888
Iteration 119/1000 | Loss: 0.00000888
Iteration 120/1000 | Loss: 0.00000888
Iteration 121/1000 | Loss: 0.00000887
Iteration 122/1000 | Loss: 0.00000887
Iteration 123/1000 | Loss: 0.00000887
Iteration 124/1000 | Loss: 0.00000887
Iteration 125/1000 | Loss: 0.00000887
Iteration 126/1000 | Loss: 0.00000886
Iteration 127/1000 | Loss: 0.00000886
Iteration 128/1000 | Loss: 0.00000886
Iteration 129/1000 | Loss: 0.00000885
Iteration 130/1000 | Loss: 0.00000885
Iteration 131/1000 | Loss: 0.00000885
Iteration 132/1000 | Loss: 0.00000885
Iteration 133/1000 | Loss: 0.00000885
Iteration 134/1000 | Loss: 0.00000885
Iteration 135/1000 | Loss: 0.00000885
Iteration 136/1000 | Loss: 0.00000885
Iteration 137/1000 | Loss: 0.00000884
Iteration 138/1000 | Loss: 0.00000884
Iteration 139/1000 | Loss: 0.00000884
Iteration 140/1000 | Loss: 0.00000884
Iteration 141/1000 | Loss: 0.00000884
Iteration 142/1000 | Loss: 0.00000884
Iteration 143/1000 | Loss: 0.00000884
Iteration 144/1000 | Loss: 0.00000884
Iteration 145/1000 | Loss: 0.00000883
Iteration 146/1000 | Loss: 0.00000883
Iteration 147/1000 | Loss: 0.00000883
Iteration 148/1000 | Loss: 0.00000883
Iteration 149/1000 | Loss: 0.00000883
Iteration 150/1000 | Loss: 0.00000883
Iteration 151/1000 | Loss: 0.00000883
Iteration 152/1000 | Loss: 0.00000883
Iteration 153/1000 | Loss: 0.00000883
Iteration 154/1000 | Loss: 0.00000883
Iteration 155/1000 | Loss: 0.00000883
Iteration 156/1000 | Loss: 0.00000883
Iteration 157/1000 | Loss: 0.00000883
Iteration 158/1000 | Loss: 0.00000883
Iteration 159/1000 | Loss: 0.00000883
Iteration 160/1000 | Loss: 0.00000882
Iteration 161/1000 | Loss: 0.00000882
Iteration 162/1000 | Loss: 0.00000882
Iteration 163/1000 | Loss: 0.00000882
Iteration 164/1000 | Loss: 0.00000881
Iteration 165/1000 | Loss: 0.00000881
Iteration 166/1000 | Loss: 0.00000881
Iteration 167/1000 | Loss: 0.00000881
Iteration 168/1000 | Loss: 0.00000881
Iteration 169/1000 | Loss: 0.00000881
Iteration 170/1000 | Loss: 0.00000881
Iteration 171/1000 | Loss: 0.00000881
Iteration 172/1000 | Loss: 0.00000881
Iteration 173/1000 | Loss: 0.00000881
Iteration 174/1000 | Loss: 0.00000881
Iteration 175/1000 | Loss: 0.00000881
Iteration 176/1000 | Loss: 0.00000881
Iteration 177/1000 | Loss: 0.00000881
Iteration 178/1000 | Loss: 0.00000881
Iteration 179/1000 | Loss: 0.00000881
Iteration 180/1000 | Loss: 0.00000881
Iteration 181/1000 | Loss: 0.00000881
Iteration 182/1000 | Loss: 0.00000881
Iteration 183/1000 | Loss: 0.00000881
Iteration 184/1000 | Loss: 0.00000881
Iteration 185/1000 | Loss: 0.00000881
Iteration 186/1000 | Loss: 0.00000881
Iteration 187/1000 | Loss: 0.00000881
Iteration 188/1000 | Loss: 0.00000881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [8.808282473182771e-06, 8.808282473182771e-06, 8.808282473182771e-06, 8.808282473182771e-06, 8.808282473182771e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.808282473182771e-06

Optimization complete. Final v2v error: 2.573197364807129 mm

Highest mean error: 2.7827179431915283 mm for frame 84

Lowest mean error: 2.4423162937164307 mm for frame 40

Saving results

Total time: 37.50210475921631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737369
Iteration 2/25 | Loss: 0.00218354
Iteration 3/25 | Loss: 0.00165534
Iteration 4/25 | Loss: 0.00158631
Iteration 5/25 | Loss: 0.00150062
Iteration 6/25 | Loss: 0.00147910
Iteration 7/25 | Loss: 0.00143914
Iteration 8/25 | Loss: 0.00144021
Iteration 9/25 | Loss: 0.00166815
Iteration 10/25 | Loss: 0.00164887
Iteration 11/25 | Loss: 0.00137062
Iteration 12/25 | Loss: 0.00131888
Iteration 13/25 | Loss: 0.00121699
Iteration 14/25 | Loss: 0.00115325
Iteration 15/25 | Loss: 0.00115283
Iteration 16/25 | Loss: 0.00115206
Iteration 17/25 | Loss: 0.00114519
Iteration 18/25 | Loss: 0.00114830
Iteration 19/25 | Loss: 0.00114921
Iteration 20/25 | Loss: 0.00114770
Iteration 21/25 | Loss: 0.00114066
Iteration 22/25 | Loss: 0.00114030
Iteration 23/25 | Loss: 0.00114029
Iteration 24/25 | Loss: 0.00114029
Iteration 25/25 | Loss: 0.00114029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42080140
Iteration 2/25 | Loss: 0.00127802
Iteration 3/25 | Loss: 0.00105468
Iteration 4/25 | Loss: 0.00105394
Iteration 5/25 | Loss: 0.00105394
Iteration 6/25 | Loss: 0.00105394
Iteration 7/25 | Loss: 0.00105394
Iteration 8/25 | Loss: 0.00105394
Iteration 9/25 | Loss: 0.00105394
Iteration 10/25 | Loss: 0.00105394
Iteration 11/25 | Loss: 0.00105394
Iteration 12/25 | Loss: 0.00105394
Iteration 13/25 | Loss: 0.00105394
Iteration 14/25 | Loss: 0.00105394
Iteration 15/25 | Loss: 0.00105394
Iteration 16/25 | Loss: 0.00105394
Iteration 17/25 | Loss: 0.00105394
Iteration 18/25 | Loss: 0.00105394
Iteration 19/25 | Loss: 0.00105394
Iteration 20/25 | Loss: 0.00105394
Iteration 21/25 | Loss: 0.00105394
Iteration 22/25 | Loss: 0.00105394
Iteration 23/25 | Loss: 0.00105394
Iteration 24/25 | Loss: 0.00105394
Iteration 25/25 | Loss: 0.00105394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105394
Iteration 2/1000 | Loss: 0.00006223
Iteration 3/1000 | Loss: 0.00003873
Iteration 4/1000 | Loss: 0.00003130
Iteration 5/1000 | Loss: 0.00002878
Iteration 6/1000 | Loss: 0.00002693
Iteration 7/1000 | Loss: 0.00002568
Iteration 8/1000 | Loss: 0.00002484
Iteration 9/1000 | Loss: 0.00002422
Iteration 10/1000 | Loss: 0.00002370
Iteration 11/1000 | Loss: 0.00002334
Iteration 12/1000 | Loss: 0.00002304
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002264
Iteration 15/1000 | Loss: 0.00002258
Iteration 16/1000 | Loss: 0.00002248
Iteration 17/1000 | Loss: 0.00002238
Iteration 18/1000 | Loss: 0.00002225
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002215
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002211
Iteration 23/1000 | Loss: 0.00002211
Iteration 24/1000 | Loss: 0.00002211
Iteration 25/1000 | Loss: 0.00002211
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002209
Iteration 29/1000 | Loss: 0.00002206
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002194
Iteration 33/1000 | Loss: 0.00002194
Iteration 34/1000 | Loss: 0.00002191
Iteration 35/1000 | Loss: 0.00002191
Iteration 36/1000 | Loss: 0.00002190
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002188
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002187
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002184
Iteration 49/1000 | Loss: 0.00002183
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002182
Iteration 52/1000 | Loss: 0.00002182
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002178
Iteration 60/1000 | Loss: 0.00002177
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002174
Iteration 72/1000 | Loss: 0.00002174
Iteration 73/1000 | Loss: 0.00002174
Iteration 74/1000 | Loss: 0.00002174
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002172
Iteration 77/1000 | Loss: 0.00002172
Iteration 78/1000 | Loss: 0.00002171
Iteration 79/1000 | Loss: 0.00002171
Iteration 80/1000 | Loss: 0.00002171
Iteration 81/1000 | Loss: 0.00002171
Iteration 82/1000 | Loss: 0.00002171
Iteration 83/1000 | Loss: 0.00002170
Iteration 84/1000 | Loss: 0.00002170
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002169
Iteration 87/1000 | Loss: 0.00002168
Iteration 88/1000 | Loss: 0.00002167
Iteration 89/1000 | Loss: 0.00002167
Iteration 90/1000 | Loss: 0.00002166
Iteration 91/1000 | Loss: 0.00002166
Iteration 92/1000 | Loss: 0.00002165
Iteration 93/1000 | Loss: 0.00002165
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002164
Iteration 96/1000 | Loss: 0.00002164
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00002163
Iteration 100/1000 | Loss: 0.00002163
Iteration 101/1000 | Loss: 0.00002162
Iteration 102/1000 | Loss: 0.00002162
Iteration 103/1000 | Loss: 0.00002161
Iteration 104/1000 | Loss: 0.00002161
Iteration 105/1000 | Loss: 0.00002161
Iteration 106/1000 | Loss: 0.00002160
Iteration 107/1000 | Loss: 0.00002160
Iteration 108/1000 | Loss: 0.00002160
Iteration 109/1000 | Loss: 0.00002159
Iteration 110/1000 | Loss: 0.00002159
Iteration 111/1000 | Loss: 0.00002159
Iteration 112/1000 | Loss: 0.00002159
Iteration 113/1000 | Loss: 0.00002159
Iteration 114/1000 | Loss: 0.00002159
Iteration 115/1000 | Loss: 0.00002158
Iteration 116/1000 | Loss: 0.00002158
Iteration 117/1000 | Loss: 0.00002158
Iteration 118/1000 | Loss: 0.00002158
Iteration 119/1000 | Loss: 0.00002158
Iteration 120/1000 | Loss: 0.00002157
Iteration 121/1000 | Loss: 0.00002157
Iteration 122/1000 | Loss: 0.00002157
Iteration 123/1000 | Loss: 0.00002157
Iteration 124/1000 | Loss: 0.00002157
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00002156
Iteration 129/1000 | Loss: 0.00002155
Iteration 130/1000 | Loss: 0.00002155
Iteration 131/1000 | Loss: 0.00002155
Iteration 132/1000 | Loss: 0.00002155
Iteration 133/1000 | Loss: 0.00002155
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002155
Iteration 136/1000 | Loss: 0.00002155
Iteration 137/1000 | Loss: 0.00002155
Iteration 138/1000 | Loss: 0.00002154
Iteration 139/1000 | Loss: 0.00002154
Iteration 140/1000 | Loss: 0.00002154
Iteration 141/1000 | Loss: 0.00002154
Iteration 142/1000 | Loss: 0.00002154
Iteration 143/1000 | Loss: 0.00002153
Iteration 144/1000 | Loss: 0.00002153
Iteration 145/1000 | Loss: 0.00002153
Iteration 146/1000 | Loss: 0.00002153
Iteration 147/1000 | Loss: 0.00002153
Iteration 148/1000 | Loss: 0.00002153
Iteration 149/1000 | Loss: 0.00002153
Iteration 150/1000 | Loss: 0.00002153
Iteration 151/1000 | Loss: 0.00002153
Iteration 152/1000 | Loss: 0.00002153
Iteration 153/1000 | Loss: 0.00002153
Iteration 154/1000 | Loss: 0.00002152
Iteration 155/1000 | Loss: 0.00002152
Iteration 156/1000 | Loss: 0.00002152
Iteration 157/1000 | Loss: 0.00002152
Iteration 158/1000 | Loss: 0.00002152
Iteration 159/1000 | Loss: 0.00002152
Iteration 160/1000 | Loss: 0.00002151
Iteration 161/1000 | Loss: 0.00002151
Iteration 162/1000 | Loss: 0.00002151
Iteration 163/1000 | Loss: 0.00002151
Iteration 164/1000 | Loss: 0.00002151
Iteration 165/1000 | Loss: 0.00002151
Iteration 166/1000 | Loss: 0.00002151
Iteration 167/1000 | Loss: 0.00002151
Iteration 168/1000 | Loss: 0.00002151
Iteration 169/1000 | Loss: 0.00002151
Iteration 170/1000 | Loss: 0.00002151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.150997897842899e-05, 2.150997897842899e-05, 2.150997897842899e-05, 2.150997897842899e-05, 2.150997897842899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.150997897842899e-05

Optimization complete. Final v2v error: 3.7064151763916016 mm

Highest mean error: 6.045987129211426 mm for frame 158

Lowest mean error: 2.72082257270813 mm for frame 203

Saving results

Total time: 95.53782296180725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792327
Iteration 2/25 | Loss: 0.00146233
Iteration 3/25 | Loss: 0.00124795
Iteration 4/25 | Loss: 0.00122796
Iteration 5/25 | Loss: 0.00121368
Iteration 6/25 | Loss: 0.00119711
Iteration 7/25 | Loss: 0.00120072
Iteration 8/25 | Loss: 0.00118674
Iteration 9/25 | Loss: 0.00118498
Iteration 10/25 | Loss: 0.00118404
Iteration 11/25 | Loss: 0.00118368
Iteration 12/25 | Loss: 0.00118356
Iteration 13/25 | Loss: 0.00118356
Iteration 14/25 | Loss: 0.00118355
Iteration 15/25 | Loss: 0.00118355
Iteration 16/25 | Loss: 0.00118355
Iteration 17/25 | Loss: 0.00118355
Iteration 18/25 | Loss: 0.00118355
Iteration 19/25 | Loss: 0.00118355
Iteration 20/25 | Loss: 0.00118354
Iteration 21/25 | Loss: 0.00118354
Iteration 22/25 | Loss: 0.00118354
Iteration 23/25 | Loss: 0.00118354
Iteration 24/25 | Loss: 0.00118354
Iteration 25/25 | Loss: 0.00118354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58987832
Iteration 2/25 | Loss: 0.00071275
Iteration 3/25 | Loss: 0.00071274
Iteration 4/25 | Loss: 0.00071274
Iteration 5/25 | Loss: 0.00071274
Iteration 6/25 | Loss: 0.00071274
Iteration 7/25 | Loss: 0.00071274
Iteration 8/25 | Loss: 0.00071274
Iteration 9/25 | Loss: 0.00071274
Iteration 10/25 | Loss: 0.00071274
Iteration 11/25 | Loss: 0.00071274
Iteration 12/25 | Loss: 0.00071274
Iteration 13/25 | Loss: 0.00071274
Iteration 14/25 | Loss: 0.00071274
Iteration 15/25 | Loss: 0.00071274
Iteration 16/25 | Loss: 0.00071274
Iteration 17/25 | Loss: 0.00071274
Iteration 18/25 | Loss: 0.00071274
Iteration 19/25 | Loss: 0.00071274
Iteration 20/25 | Loss: 0.00071274
Iteration 21/25 | Loss: 0.00071274
Iteration 22/25 | Loss: 0.00071274
Iteration 23/25 | Loss: 0.00071274
Iteration 24/25 | Loss: 0.00071274
Iteration 25/25 | Loss: 0.00071274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071274
Iteration 2/1000 | Loss: 0.00004382
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001826
Iteration 8/1000 | Loss: 0.00001800
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001721
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001719
Iteration 20/1000 | Loss: 0.00001713
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001709
Iteration 26/1000 | Loss: 0.00001709
Iteration 27/1000 | Loss: 0.00001708
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00001707
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00001706
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001705
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001699
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001699
Iteration 79/1000 | Loss: 0.00001699
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001698
Iteration 84/1000 | Loss: 0.00001698
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001693
Iteration 122/1000 | Loss: 0.00001693
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001693
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001693
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001693
Iteration 135/1000 | Loss: 0.00001692
Iteration 136/1000 | Loss: 0.00001692
Iteration 137/1000 | Loss: 0.00001692
Iteration 138/1000 | Loss: 0.00001692
Iteration 139/1000 | Loss: 0.00001692
Iteration 140/1000 | Loss: 0.00001692
Iteration 141/1000 | Loss: 0.00001692
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001692
Iteration 144/1000 | Loss: 0.00001692
Iteration 145/1000 | Loss: 0.00001692
Iteration 146/1000 | Loss: 0.00001692
Iteration 147/1000 | Loss: 0.00001692
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001691
Iteration 155/1000 | Loss: 0.00001691
Iteration 156/1000 | Loss: 0.00001691
Iteration 157/1000 | Loss: 0.00001691
Iteration 158/1000 | Loss: 0.00001691
Iteration 159/1000 | Loss: 0.00001691
Iteration 160/1000 | Loss: 0.00001691
Iteration 161/1000 | Loss: 0.00001691
Iteration 162/1000 | Loss: 0.00001691
Iteration 163/1000 | Loss: 0.00001691
Iteration 164/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.690986391622573e-05, 1.690986391622573e-05, 1.690986391622573e-05, 1.690986391622573e-05, 1.690986391622573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.690986391622573e-05

Optimization complete. Final v2v error: 3.507887363433838 mm

Highest mean error: 4.1329450607299805 mm for frame 41

Lowest mean error: 3.1164042949676514 mm for frame 182

Saving results

Total time: 55.423789739608765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432288
Iteration 2/25 | Loss: 0.00130681
Iteration 3/25 | Loss: 0.00118806
Iteration 4/25 | Loss: 0.00117297
Iteration 5/25 | Loss: 0.00116927
Iteration 6/25 | Loss: 0.00116833
Iteration 7/25 | Loss: 0.00116833
Iteration 8/25 | Loss: 0.00116833
Iteration 9/25 | Loss: 0.00116833
Iteration 10/25 | Loss: 0.00116833
Iteration 11/25 | Loss: 0.00116833
Iteration 12/25 | Loss: 0.00116833
Iteration 13/25 | Loss: 0.00116833
Iteration 14/25 | Loss: 0.00116833
Iteration 15/25 | Loss: 0.00116833
Iteration 16/25 | Loss: 0.00116833
Iteration 17/25 | Loss: 0.00116833
Iteration 18/25 | Loss: 0.00116833
Iteration 19/25 | Loss: 0.00116833
Iteration 20/25 | Loss: 0.00116833
Iteration 21/25 | Loss: 0.00116833
Iteration 22/25 | Loss: 0.00116833
Iteration 23/25 | Loss: 0.00116833
Iteration 24/25 | Loss: 0.00116833
Iteration 25/25 | Loss: 0.00116833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97197390
Iteration 2/25 | Loss: 0.00071722
Iteration 3/25 | Loss: 0.00071719
Iteration 4/25 | Loss: 0.00071718
Iteration 5/25 | Loss: 0.00071718
Iteration 6/25 | Loss: 0.00071718
Iteration 7/25 | Loss: 0.00071718
Iteration 8/25 | Loss: 0.00071718
Iteration 9/25 | Loss: 0.00071718
Iteration 10/25 | Loss: 0.00071718
Iteration 11/25 | Loss: 0.00071718
Iteration 12/25 | Loss: 0.00071718
Iteration 13/25 | Loss: 0.00071718
Iteration 14/25 | Loss: 0.00071718
Iteration 15/25 | Loss: 0.00071718
Iteration 16/25 | Loss: 0.00071718
Iteration 17/25 | Loss: 0.00071718
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007171822944656014, 0.0007171822944656014, 0.0007171822944656014, 0.0007171822944656014, 0.0007171822944656014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007171822944656014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071718
Iteration 2/1000 | Loss: 0.00004293
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002455
Iteration 5/1000 | Loss: 0.00002301
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00002179
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002083
Iteration 10/1000 | Loss: 0.00002051
Iteration 11/1000 | Loss: 0.00002029
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002007
Iteration 14/1000 | Loss: 0.00002003
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001984
Iteration 19/1000 | Loss: 0.00001984
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001981
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001980
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001977
Iteration 28/1000 | Loss: 0.00001977
Iteration 29/1000 | Loss: 0.00001976
Iteration 30/1000 | Loss: 0.00001973
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001970
Iteration 37/1000 | Loss: 0.00001970
Iteration 38/1000 | Loss: 0.00001968
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001968
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001964
Iteration 55/1000 | Loss: 0.00001964
Iteration 56/1000 | Loss: 0.00001964
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001963
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001962
Iteration 62/1000 | Loss: 0.00001962
Iteration 63/1000 | Loss: 0.00001962
Iteration 64/1000 | Loss: 0.00001962
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001961
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001959
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001955
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00001955
Iteration 104/1000 | Loss: 0.00001955
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001954
Iteration 111/1000 | Loss: 0.00001954
Iteration 112/1000 | Loss: 0.00001954
Iteration 113/1000 | Loss: 0.00001954
Iteration 114/1000 | Loss: 0.00001954
Iteration 115/1000 | Loss: 0.00001954
Iteration 116/1000 | Loss: 0.00001954
Iteration 117/1000 | Loss: 0.00001954
Iteration 118/1000 | Loss: 0.00001953
Iteration 119/1000 | Loss: 0.00001953
Iteration 120/1000 | Loss: 0.00001953
Iteration 121/1000 | Loss: 0.00001953
Iteration 122/1000 | Loss: 0.00001953
Iteration 123/1000 | Loss: 0.00001953
Iteration 124/1000 | Loss: 0.00001953
Iteration 125/1000 | Loss: 0.00001953
Iteration 126/1000 | Loss: 0.00001953
Iteration 127/1000 | Loss: 0.00001953
Iteration 128/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.9527013137121685e-05, 1.9527013137121685e-05, 1.9527013137121685e-05, 1.9527013137121685e-05, 1.9527013137121685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9527013137121685e-05

Optimization complete. Final v2v error: 3.746211051940918 mm

Highest mean error: 4.248724460601807 mm for frame 59

Lowest mean error: 3.299316167831421 mm for frame 3

Saving results

Total time: 38.03472876548767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846768
Iteration 2/25 | Loss: 0.00115531
Iteration 3/25 | Loss: 0.00107309
Iteration 4/25 | Loss: 0.00106569
Iteration 5/25 | Loss: 0.00106370
Iteration 6/25 | Loss: 0.00106356
Iteration 7/25 | Loss: 0.00106356
Iteration 8/25 | Loss: 0.00106356
Iteration 9/25 | Loss: 0.00106356
Iteration 10/25 | Loss: 0.00106356
Iteration 11/25 | Loss: 0.00106356
Iteration 12/25 | Loss: 0.00106356
Iteration 13/25 | Loss: 0.00106356
Iteration 14/25 | Loss: 0.00106356
Iteration 15/25 | Loss: 0.00106356
Iteration 16/25 | Loss: 0.00106356
Iteration 17/25 | Loss: 0.00106356
Iteration 18/25 | Loss: 0.00106356
Iteration 19/25 | Loss: 0.00106356
Iteration 20/25 | Loss: 0.00106356
Iteration 21/25 | Loss: 0.00106356
Iteration 22/25 | Loss: 0.00106356
Iteration 23/25 | Loss: 0.00106356
Iteration 24/25 | Loss: 0.00106356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010635590879246593, 0.0010635590879246593, 0.0010635590879246593, 0.0010635590879246593, 0.0010635590879246593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010635590879246593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67970037
Iteration 2/25 | Loss: 0.00085999
Iteration 3/25 | Loss: 0.00085999
Iteration 4/25 | Loss: 0.00085999
Iteration 5/25 | Loss: 0.00085999
Iteration 6/25 | Loss: 0.00085999
Iteration 7/25 | Loss: 0.00085999
Iteration 8/25 | Loss: 0.00085999
Iteration 9/25 | Loss: 0.00085999
Iteration 10/25 | Loss: 0.00085999
Iteration 11/25 | Loss: 0.00085999
Iteration 12/25 | Loss: 0.00085999
Iteration 13/25 | Loss: 0.00085999
Iteration 14/25 | Loss: 0.00085999
Iteration 15/25 | Loss: 0.00085999
Iteration 16/25 | Loss: 0.00085999
Iteration 17/25 | Loss: 0.00085999
Iteration 18/25 | Loss: 0.00085999
Iteration 19/25 | Loss: 0.00085999
Iteration 20/25 | Loss: 0.00085999
Iteration 21/25 | Loss: 0.00085999
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008599917637184262, 0.0008599917637184262, 0.0008599917637184262, 0.0008599917637184262, 0.0008599917637184262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008599917637184262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085999
Iteration 2/1000 | Loss: 0.00002086
Iteration 3/1000 | Loss: 0.00001397
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001071
Iteration 6/1000 | Loss: 0.00001027
Iteration 7/1000 | Loss: 0.00000987
Iteration 8/1000 | Loss: 0.00000956
Iteration 9/1000 | Loss: 0.00000939
Iteration 10/1000 | Loss: 0.00000935
Iteration 11/1000 | Loss: 0.00000930
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000905
Iteration 15/1000 | Loss: 0.00000903
Iteration 16/1000 | Loss: 0.00000902
Iteration 17/1000 | Loss: 0.00000902
Iteration 18/1000 | Loss: 0.00000899
Iteration 19/1000 | Loss: 0.00000898
Iteration 20/1000 | Loss: 0.00000898
Iteration 21/1000 | Loss: 0.00000896
Iteration 22/1000 | Loss: 0.00000894
Iteration 23/1000 | Loss: 0.00000892
Iteration 24/1000 | Loss: 0.00000891
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000889
Iteration 27/1000 | Loss: 0.00000886
Iteration 28/1000 | Loss: 0.00000885
Iteration 29/1000 | Loss: 0.00000883
Iteration 30/1000 | Loss: 0.00000882
Iteration 31/1000 | Loss: 0.00000882
Iteration 32/1000 | Loss: 0.00000882
Iteration 33/1000 | Loss: 0.00000881
Iteration 34/1000 | Loss: 0.00000881
Iteration 35/1000 | Loss: 0.00000881
Iteration 36/1000 | Loss: 0.00000881
Iteration 37/1000 | Loss: 0.00000881
Iteration 38/1000 | Loss: 0.00000881
Iteration 39/1000 | Loss: 0.00000880
Iteration 40/1000 | Loss: 0.00000880
Iteration 41/1000 | Loss: 0.00000880
Iteration 42/1000 | Loss: 0.00000879
Iteration 43/1000 | Loss: 0.00000879
Iteration 44/1000 | Loss: 0.00000878
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000877
Iteration 47/1000 | Loss: 0.00000876
Iteration 48/1000 | Loss: 0.00000876
Iteration 49/1000 | Loss: 0.00000876
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000876
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000875
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000874
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000870
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000869
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000868
Iteration 75/1000 | Loss: 0.00000868
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000867
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000866
Iteration 81/1000 | Loss: 0.00000866
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000865
Iteration 90/1000 | Loss: 0.00000865
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000864
Iteration 94/1000 | Loss: 0.00000864
Iteration 95/1000 | Loss: 0.00000863
Iteration 96/1000 | Loss: 0.00000863
Iteration 97/1000 | Loss: 0.00000863
Iteration 98/1000 | Loss: 0.00000863
Iteration 99/1000 | Loss: 0.00000863
Iteration 100/1000 | Loss: 0.00000863
Iteration 101/1000 | Loss: 0.00000862
Iteration 102/1000 | Loss: 0.00000862
Iteration 103/1000 | Loss: 0.00000862
Iteration 104/1000 | Loss: 0.00000862
Iteration 105/1000 | Loss: 0.00000862
Iteration 106/1000 | Loss: 0.00000862
Iteration 107/1000 | Loss: 0.00000862
Iteration 108/1000 | Loss: 0.00000862
Iteration 109/1000 | Loss: 0.00000862
Iteration 110/1000 | Loss: 0.00000862
Iteration 111/1000 | Loss: 0.00000861
Iteration 112/1000 | Loss: 0.00000861
Iteration 113/1000 | Loss: 0.00000861
Iteration 114/1000 | Loss: 0.00000860
Iteration 115/1000 | Loss: 0.00000860
Iteration 116/1000 | Loss: 0.00000860
Iteration 117/1000 | Loss: 0.00000860
Iteration 118/1000 | Loss: 0.00000860
Iteration 119/1000 | Loss: 0.00000859
Iteration 120/1000 | Loss: 0.00000859
Iteration 121/1000 | Loss: 0.00000859
Iteration 122/1000 | Loss: 0.00000859
Iteration 123/1000 | Loss: 0.00000859
Iteration 124/1000 | Loss: 0.00000859
Iteration 125/1000 | Loss: 0.00000859
Iteration 126/1000 | Loss: 0.00000859
Iteration 127/1000 | Loss: 0.00000859
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000858
Iteration 132/1000 | Loss: 0.00000858
Iteration 133/1000 | Loss: 0.00000857
Iteration 134/1000 | Loss: 0.00000857
Iteration 135/1000 | Loss: 0.00000857
Iteration 136/1000 | Loss: 0.00000857
Iteration 137/1000 | Loss: 0.00000857
Iteration 138/1000 | Loss: 0.00000857
Iteration 139/1000 | Loss: 0.00000856
Iteration 140/1000 | Loss: 0.00000856
Iteration 141/1000 | Loss: 0.00000856
Iteration 142/1000 | Loss: 0.00000856
Iteration 143/1000 | Loss: 0.00000856
Iteration 144/1000 | Loss: 0.00000856
Iteration 145/1000 | Loss: 0.00000855
Iteration 146/1000 | Loss: 0.00000855
Iteration 147/1000 | Loss: 0.00000855
Iteration 148/1000 | Loss: 0.00000855
Iteration 149/1000 | Loss: 0.00000855
Iteration 150/1000 | Loss: 0.00000855
Iteration 151/1000 | Loss: 0.00000854
Iteration 152/1000 | Loss: 0.00000854
Iteration 153/1000 | Loss: 0.00000854
Iteration 154/1000 | Loss: 0.00000854
Iteration 155/1000 | Loss: 0.00000854
Iteration 156/1000 | Loss: 0.00000853
Iteration 157/1000 | Loss: 0.00000853
Iteration 158/1000 | Loss: 0.00000853
Iteration 159/1000 | Loss: 0.00000853
Iteration 160/1000 | Loss: 0.00000853
Iteration 161/1000 | Loss: 0.00000853
Iteration 162/1000 | Loss: 0.00000853
Iteration 163/1000 | Loss: 0.00000853
Iteration 164/1000 | Loss: 0.00000852
Iteration 165/1000 | Loss: 0.00000852
Iteration 166/1000 | Loss: 0.00000852
Iteration 167/1000 | Loss: 0.00000852
Iteration 168/1000 | Loss: 0.00000852
Iteration 169/1000 | Loss: 0.00000852
Iteration 170/1000 | Loss: 0.00000852
Iteration 171/1000 | Loss: 0.00000852
Iteration 172/1000 | Loss: 0.00000852
Iteration 173/1000 | Loss: 0.00000852
Iteration 174/1000 | Loss: 0.00000852
Iteration 175/1000 | Loss: 0.00000852
Iteration 176/1000 | Loss: 0.00000852
Iteration 177/1000 | Loss: 0.00000852
Iteration 178/1000 | Loss: 0.00000852
Iteration 179/1000 | Loss: 0.00000852
Iteration 180/1000 | Loss: 0.00000852
Iteration 181/1000 | Loss: 0.00000851
Iteration 182/1000 | Loss: 0.00000851
Iteration 183/1000 | Loss: 0.00000851
Iteration 184/1000 | Loss: 0.00000851
Iteration 185/1000 | Loss: 0.00000851
Iteration 186/1000 | Loss: 0.00000850
Iteration 187/1000 | Loss: 0.00000850
Iteration 188/1000 | Loss: 0.00000850
Iteration 189/1000 | Loss: 0.00000850
Iteration 190/1000 | Loss: 0.00000850
Iteration 191/1000 | Loss: 0.00000850
Iteration 192/1000 | Loss: 0.00000850
Iteration 193/1000 | Loss: 0.00000850
Iteration 194/1000 | Loss: 0.00000850
Iteration 195/1000 | Loss: 0.00000849
Iteration 196/1000 | Loss: 0.00000849
Iteration 197/1000 | Loss: 0.00000849
Iteration 198/1000 | Loss: 0.00000849
Iteration 199/1000 | Loss: 0.00000849
Iteration 200/1000 | Loss: 0.00000849
Iteration 201/1000 | Loss: 0.00000849
Iteration 202/1000 | Loss: 0.00000849
Iteration 203/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [8.49374737299513e-06, 8.49374737299513e-06, 8.49374737299513e-06, 8.49374737299513e-06, 8.49374737299513e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.49374737299513e-06

Optimization complete. Final v2v error: 2.4975831508636475 mm

Highest mean error: 2.876694679260254 mm for frame 57

Lowest mean error: 2.304379463195801 mm for frame 129

Saving results

Total time: 38.94716548919678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431206
Iteration 2/25 | Loss: 0.00124923
Iteration 3/25 | Loss: 0.00116108
Iteration 4/25 | Loss: 0.00114474
Iteration 5/25 | Loss: 0.00113846
Iteration 6/25 | Loss: 0.00113705
Iteration 7/25 | Loss: 0.00113705
Iteration 8/25 | Loss: 0.00113705
Iteration 9/25 | Loss: 0.00113705
Iteration 10/25 | Loss: 0.00113705
Iteration 11/25 | Loss: 0.00113705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001137048122473061, 0.001137048122473061, 0.001137048122473061, 0.001137048122473061, 0.001137048122473061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001137048122473061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76661551
Iteration 2/25 | Loss: 0.00094013
Iteration 3/25 | Loss: 0.00094013
Iteration 4/25 | Loss: 0.00094013
Iteration 5/25 | Loss: 0.00094013
Iteration 6/25 | Loss: 0.00094012
Iteration 7/25 | Loss: 0.00094012
Iteration 8/25 | Loss: 0.00094012
Iteration 9/25 | Loss: 0.00094012
Iteration 10/25 | Loss: 0.00094012
Iteration 11/25 | Loss: 0.00094012
Iteration 12/25 | Loss: 0.00094012
Iteration 13/25 | Loss: 0.00094012
Iteration 14/25 | Loss: 0.00094012
Iteration 15/25 | Loss: 0.00094012
Iteration 16/25 | Loss: 0.00094012
Iteration 17/25 | Loss: 0.00094012
Iteration 18/25 | Loss: 0.00094012
Iteration 19/25 | Loss: 0.00094012
Iteration 20/25 | Loss: 0.00094012
Iteration 21/25 | Loss: 0.00094012
Iteration 22/25 | Loss: 0.00094012
Iteration 23/25 | Loss: 0.00094012
Iteration 24/25 | Loss: 0.00094012
Iteration 25/25 | Loss: 0.00094012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094012
Iteration 2/1000 | Loss: 0.00002495
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001772
Iteration 5/1000 | Loss: 0.00001722
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001593
Iteration 14/1000 | Loss: 0.00001591
Iteration 15/1000 | Loss: 0.00001590
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001581
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001568
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001566
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001565
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001553
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001551
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001550
Iteration 110/1000 | Loss: 0.00001550
Iteration 111/1000 | Loss: 0.00001550
Iteration 112/1000 | Loss: 0.00001550
Iteration 113/1000 | Loss: 0.00001550
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.549338958284352e-05, 1.549338958284352e-05, 1.549338958284352e-05, 1.549338958284352e-05, 1.549338958284352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.549338958284352e-05

Optimization complete. Final v2v error: 3.3326330184936523 mm

Highest mean error: 3.9301445484161377 mm for frame 157

Lowest mean error: 3.239729166030884 mm for frame 69

Saving results

Total time: 38.13512659072876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688044
Iteration 2/25 | Loss: 0.00143386
Iteration 3/25 | Loss: 0.00121593
Iteration 4/25 | Loss: 0.00119345
Iteration 5/25 | Loss: 0.00117147
Iteration 6/25 | Loss: 0.00118054
Iteration 7/25 | Loss: 0.00115862
Iteration 8/25 | Loss: 0.00116055
Iteration 9/25 | Loss: 0.00115999
Iteration 10/25 | Loss: 0.00115754
Iteration 11/25 | Loss: 0.00115768
Iteration 12/25 | Loss: 0.00115703
Iteration 13/25 | Loss: 0.00115625
Iteration 14/25 | Loss: 0.00115578
Iteration 15/25 | Loss: 0.00115742
Iteration 16/25 | Loss: 0.00115767
Iteration 17/25 | Loss: 0.00115671
Iteration 18/25 | Loss: 0.00115606
Iteration 19/25 | Loss: 0.00115484
Iteration 20/25 | Loss: 0.00115432
Iteration 21/25 | Loss: 0.00115423
Iteration 22/25 | Loss: 0.00115423
Iteration 23/25 | Loss: 0.00115423
Iteration 24/25 | Loss: 0.00115423
Iteration 25/25 | Loss: 0.00115423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46139205
Iteration 2/25 | Loss: 0.00106703
Iteration 3/25 | Loss: 0.00106702
Iteration 4/25 | Loss: 0.00106702
Iteration 5/25 | Loss: 0.00106702
Iteration 6/25 | Loss: 0.00106702
Iteration 7/25 | Loss: 0.00106702
Iteration 8/25 | Loss: 0.00106702
Iteration 9/25 | Loss: 0.00106702
Iteration 10/25 | Loss: 0.00106702
Iteration 11/25 | Loss: 0.00106702
Iteration 12/25 | Loss: 0.00106702
Iteration 13/25 | Loss: 0.00106702
Iteration 14/25 | Loss: 0.00106702
Iteration 15/25 | Loss: 0.00106702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010670189512893558, 0.0010670189512893558, 0.0010670189512893558, 0.0010670189512893558, 0.0010670189512893558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670189512893558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106702
Iteration 2/1000 | Loss: 0.00004431
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00002040
Iteration 7/1000 | Loss: 0.00001982
Iteration 8/1000 | Loss: 0.00001942
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001867
Iteration 12/1000 | Loss: 0.00001852
Iteration 13/1000 | Loss: 0.00001848
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001839
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001832
Iteration 20/1000 | Loss: 0.00001832
Iteration 21/1000 | Loss: 0.00001831
Iteration 22/1000 | Loss: 0.00001831
Iteration 23/1000 | Loss: 0.00001830
Iteration 24/1000 | Loss: 0.00001830
Iteration 25/1000 | Loss: 0.00001829
Iteration 26/1000 | Loss: 0.00001829
Iteration 27/1000 | Loss: 0.00001829
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001828
Iteration 30/1000 | Loss: 0.00001827
Iteration 31/1000 | Loss: 0.00001827
Iteration 32/1000 | Loss: 0.00001826
Iteration 33/1000 | Loss: 0.00001826
Iteration 34/1000 | Loss: 0.00001826
Iteration 35/1000 | Loss: 0.00001826
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001826
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001825
Iteration 40/1000 | Loss: 0.00001825
Iteration 41/1000 | Loss: 0.00001824
Iteration 42/1000 | Loss: 0.00001824
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001824
Iteration 45/1000 | Loss: 0.00001824
Iteration 46/1000 | Loss: 0.00001823
Iteration 47/1000 | Loss: 0.00001823
Iteration 48/1000 | Loss: 0.00001823
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001821
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001820
Iteration 63/1000 | Loss: 0.00001820
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001819
Iteration 68/1000 | Loss: 0.00001819
Iteration 69/1000 | Loss: 0.00001819
Iteration 70/1000 | Loss: 0.00001819
Iteration 71/1000 | Loss: 0.00001819
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001816
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001816
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001816
Iteration 96/1000 | Loss: 0.00001816
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001815
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001815
Iteration 102/1000 | Loss: 0.00001815
Iteration 103/1000 | Loss: 0.00001815
Iteration 104/1000 | Loss: 0.00001815
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001814
Iteration 108/1000 | Loss: 0.00001814
Iteration 109/1000 | Loss: 0.00001814
Iteration 110/1000 | Loss: 0.00001814
Iteration 111/1000 | Loss: 0.00001814
Iteration 112/1000 | Loss: 0.00001814
Iteration 113/1000 | Loss: 0.00001814
Iteration 114/1000 | Loss: 0.00001814
Iteration 115/1000 | Loss: 0.00001814
Iteration 116/1000 | Loss: 0.00001814
Iteration 117/1000 | Loss: 0.00001814
Iteration 118/1000 | Loss: 0.00001814
Iteration 119/1000 | Loss: 0.00001813
Iteration 120/1000 | Loss: 0.00001813
Iteration 121/1000 | Loss: 0.00001813
Iteration 122/1000 | Loss: 0.00001813
Iteration 123/1000 | Loss: 0.00001813
Iteration 124/1000 | Loss: 0.00001813
Iteration 125/1000 | Loss: 0.00001812
Iteration 126/1000 | Loss: 0.00001812
Iteration 127/1000 | Loss: 0.00001812
Iteration 128/1000 | Loss: 0.00001812
Iteration 129/1000 | Loss: 0.00001812
Iteration 130/1000 | Loss: 0.00001812
Iteration 131/1000 | Loss: 0.00001812
Iteration 132/1000 | Loss: 0.00001812
Iteration 133/1000 | Loss: 0.00001811
Iteration 134/1000 | Loss: 0.00001811
Iteration 135/1000 | Loss: 0.00001811
Iteration 136/1000 | Loss: 0.00001811
Iteration 137/1000 | Loss: 0.00001811
Iteration 138/1000 | Loss: 0.00001811
Iteration 139/1000 | Loss: 0.00001811
Iteration 140/1000 | Loss: 0.00001811
Iteration 141/1000 | Loss: 0.00001810
Iteration 142/1000 | Loss: 0.00001810
Iteration 143/1000 | Loss: 0.00001810
Iteration 144/1000 | Loss: 0.00001810
Iteration 145/1000 | Loss: 0.00001810
Iteration 146/1000 | Loss: 0.00001810
Iteration 147/1000 | Loss: 0.00001809
Iteration 148/1000 | Loss: 0.00001809
Iteration 149/1000 | Loss: 0.00001809
Iteration 150/1000 | Loss: 0.00001809
Iteration 151/1000 | Loss: 0.00001808
Iteration 152/1000 | Loss: 0.00001808
Iteration 153/1000 | Loss: 0.00001808
Iteration 154/1000 | Loss: 0.00001808
Iteration 155/1000 | Loss: 0.00001807
Iteration 156/1000 | Loss: 0.00001807
Iteration 157/1000 | Loss: 0.00001807
Iteration 158/1000 | Loss: 0.00001807
Iteration 159/1000 | Loss: 0.00001807
Iteration 160/1000 | Loss: 0.00001806
Iteration 161/1000 | Loss: 0.00001806
Iteration 162/1000 | Loss: 0.00001806
Iteration 163/1000 | Loss: 0.00001806
Iteration 164/1000 | Loss: 0.00001806
Iteration 165/1000 | Loss: 0.00001806
Iteration 166/1000 | Loss: 0.00001806
Iteration 167/1000 | Loss: 0.00001806
Iteration 168/1000 | Loss: 0.00001805
Iteration 169/1000 | Loss: 0.00001805
Iteration 170/1000 | Loss: 0.00001805
Iteration 171/1000 | Loss: 0.00001805
Iteration 172/1000 | Loss: 0.00001805
Iteration 173/1000 | Loss: 0.00001805
Iteration 174/1000 | Loss: 0.00001804
Iteration 175/1000 | Loss: 0.00001804
Iteration 176/1000 | Loss: 0.00001804
Iteration 177/1000 | Loss: 0.00001804
Iteration 178/1000 | Loss: 0.00001804
Iteration 179/1000 | Loss: 0.00001804
Iteration 180/1000 | Loss: 0.00001804
Iteration 181/1000 | Loss: 0.00001804
Iteration 182/1000 | Loss: 0.00001804
Iteration 183/1000 | Loss: 0.00001804
Iteration 184/1000 | Loss: 0.00001804
Iteration 185/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.803552186174784e-05, 1.803552186174784e-05, 1.803552186174784e-05, 1.803552186174784e-05, 1.803552186174784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.803552186174784e-05

Optimization complete. Final v2v error: 3.559969663619995 mm

Highest mean error: 4.238964080810547 mm for frame 83

Lowest mean error: 3.0924177169799805 mm for frame 6

Saving results

Total time: 76.85499238967896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742046
Iteration 2/25 | Loss: 0.00150037
Iteration 3/25 | Loss: 0.00120466
Iteration 4/25 | Loss: 0.00117769
Iteration 5/25 | Loss: 0.00117176
Iteration 6/25 | Loss: 0.00117872
Iteration 7/25 | Loss: 0.00114759
Iteration 8/25 | Loss: 0.00114275
Iteration 9/25 | Loss: 0.00114205
Iteration 10/25 | Loss: 0.00114191
Iteration 11/25 | Loss: 0.00114186
Iteration 12/25 | Loss: 0.00114186
Iteration 13/25 | Loss: 0.00114185
Iteration 14/25 | Loss: 0.00114185
Iteration 15/25 | Loss: 0.00114185
Iteration 16/25 | Loss: 0.00114185
Iteration 17/25 | Loss: 0.00114185
Iteration 18/25 | Loss: 0.00114185
Iteration 19/25 | Loss: 0.00114185
Iteration 20/25 | Loss: 0.00114185
Iteration 21/25 | Loss: 0.00114185
Iteration 22/25 | Loss: 0.00114185
Iteration 23/25 | Loss: 0.00114185
Iteration 24/25 | Loss: 0.00114185
Iteration 25/25 | Loss: 0.00114185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.48657799
Iteration 2/25 | Loss: 0.00076348
Iteration 3/25 | Loss: 0.00076335
Iteration 4/25 | Loss: 0.00076335
Iteration 5/25 | Loss: 0.00076335
Iteration 6/25 | Loss: 0.00076335
Iteration 7/25 | Loss: 0.00076335
Iteration 8/25 | Loss: 0.00076335
Iteration 9/25 | Loss: 0.00076335
Iteration 10/25 | Loss: 0.00076335
Iteration 11/25 | Loss: 0.00076335
Iteration 12/25 | Loss: 0.00076335
Iteration 13/25 | Loss: 0.00076335
Iteration 14/25 | Loss: 0.00076335
Iteration 15/25 | Loss: 0.00076335
Iteration 16/25 | Loss: 0.00076335
Iteration 17/25 | Loss: 0.00076335
Iteration 18/25 | Loss: 0.00076335
Iteration 19/25 | Loss: 0.00076335
Iteration 20/25 | Loss: 0.00076335
Iteration 21/25 | Loss: 0.00076335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007633488276042044, 0.0007633488276042044, 0.0007633488276042044, 0.0007633488276042044, 0.0007633488276042044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007633488276042044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076335
Iteration 2/1000 | Loss: 0.00002920
Iteration 3/1000 | Loss: 0.00001894
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001555
Iteration 6/1000 | Loss: 0.00001487
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001404
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001351
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001322
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001307
Iteration 26/1000 | Loss: 0.00001307
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001307
Iteration 29/1000 | Loss: 0.00001307
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001297
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001296
Iteration 56/1000 | Loss: 0.00001296
Iteration 57/1000 | Loss: 0.00001295
Iteration 58/1000 | Loss: 0.00001295
Iteration 59/1000 | Loss: 0.00001294
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001293
Iteration 63/1000 | Loss: 0.00001293
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001292
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001290
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001287
Iteration 86/1000 | Loss: 0.00001287
Iteration 87/1000 | Loss: 0.00001287
Iteration 88/1000 | Loss: 0.00001287
Iteration 89/1000 | Loss: 0.00001287
Iteration 90/1000 | Loss: 0.00001287
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001286
Iteration 94/1000 | Loss: 0.00001286
Iteration 95/1000 | Loss: 0.00001286
Iteration 96/1000 | Loss: 0.00001286
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.282507128053112e-05, 1.282507128053112e-05, 1.282507128053112e-05, 1.282507128053112e-05, 1.282507128053112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.282507128053112e-05

Optimization complete. Final v2v error: 3.054494619369507 mm

Highest mean error: 3.8235738277435303 mm for frame 26

Lowest mean error: 2.6199331283569336 mm for frame 10

Saving results

Total time: 51.96892595291138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459439
Iteration 2/25 | Loss: 0.00116999
Iteration 3/25 | Loss: 0.00109864
Iteration 4/25 | Loss: 0.00109382
Iteration 5/25 | Loss: 0.00109286
Iteration 6/25 | Loss: 0.00109286
Iteration 7/25 | Loss: 0.00109286
Iteration 8/25 | Loss: 0.00109286
Iteration 9/25 | Loss: 0.00109286
Iteration 10/25 | Loss: 0.00109286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010928574483841658, 0.0010928574483841658, 0.0010928574483841658, 0.0010928574483841658, 0.0010928574483841658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010928574483841658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35122144
Iteration 2/25 | Loss: 0.00091502
Iteration 3/25 | Loss: 0.00091500
Iteration 4/25 | Loss: 0.00091500
Iteration 5/25 | Loss: 0.00091500
Iteration 6/25 | Loss: 0.00091500
Iteration 7/25 | Loss: 0.00091500
Iteration 8/25 | Loss: 0.00091500
Iteration 9/25 | Loss: 0.00091500
Iteration 10/25 | Loss: 0.00091500
Iteration 11/25 | Loss: 0.00091500
Iteration 12/25 | Loss: 0.00091500
Iteration 13/25 | Loss: 0.00091500
Iteration 14/25 | Loss: 0.00091500
Iteration 15/25 | Loss: 0.00091500
Iteration 16/25 | Loss: 0.00091500
Iteration 17/25 | Loss: 0.00091500
Iteration 18/25 | Loss: 0.00091500
Iteration 19/25 | Loss: 0.00091500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009150010882876813, 0.0009150010882876813, 0.0009150010882876813, 0.0009150010882876813, 0.0009150010882876813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009150010882876813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091500
Iteration 2/1000 | Loss: 0.00002270
Iteration 3/1000 | Loss: 0.00001405
Iteration 4/1000 | Loss: 0.00001209
Iteration 5/1000 | Loss: 0.00001126
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001055
Iteration 8/1000 | Loss: 0.00001038
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001034
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001032
Iteration 13/1000 | Loss: 0.00001030
Iteration 14/1000 | Loss: 0.00001030
Iteration 15/1000 | Loss: 0.00001029
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001023
Iteration 18/1000 | Loss: 0.00001022
Iteration 19/1000 | Loss: 0.00001021
Iteration 20/1000 | Loss: 0.00001016
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001007
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00001001
Iteration 28/1000 | Loss: 0.00000993
Iteration 29/1000 | Loss: 0.00000993
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000986
Iteration 33/1000 | Loss: 0.00000985
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000984
Iteration 36/1000 | Loss: 0.00000984
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000977
Iteration 51/1000 | Loss: 0.00000976
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000974
Iteration 58/1000 | Loss: 0.00000974
Iteration 59/1000 | Loss: 0.00000973
Iteration 60/1000 | Loss: 0.00000973
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000973
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000972
Iteration 65/1000 | Loss: 0.00000972
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000971
Iteration 68/1000 | Loss: 0.00000971
Iteration 69/1000 | Loss: 0.00000971
Iteration 70/1000 | Loss: 0.00000970
Iteration 71/1000 | Loss: 0.00000970
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000969
Iteration 75/1000 | Loss: 0.00000969
Iteration 76/1000 | Loss: 0.00000969
Iteration 77/1000 | Loss: 0.00000968
Iteration 78/1000 | Loss: 0.00000968
Iteration 79/1000 | Loss: 0.00000968
Iteration 80/1000 | Loss: 0.00000968
Iteration 81/1000 | Loss: 0.00000968
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000967
Iteration 86/1000 | Loss: 0.00000967
Iteration 87/1000 | Loss: 0.00000967
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000967
Iteration 90/1000 | Loss: 0.00000967
Iteration 91/1000 | Loss: 0.00000966
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000964
Iteration 98/1000 | Loss: 0.00000964
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000964
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000964
Iteration 108/1000 | Loss: 0.00000963
Iteration 109/1000 | Loss: 0.00000963
Iteration 110/1000 | Loss: 0.00000963
Iteration 111/1000 | Loss: 0.00000963
Iteration 112/1000 | Loss: 0.00000963
Iteration 113/1000 | Loss: 0.00000963
Iteration 114/1000 | Loss: 0.00000963
Iteration 115/1000 | Loss: 0.00000963
Iteration 116/1000 | Loss: 0.00000963
Iteration 117/1000 | Loss: 0.00000963
Iteration 118/1000 | Loss: 0.00000962
Iteration 119/1000 | Loss: 0.00000962
Iteration 120/1000 | Loss: 0.00000962
Iteration 121/1000 | Loss: 0.00000962
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Iteration 130/1000 | Loss: 0.00000961
Iteration 131/1000 | Loss: 0.00000961
Iteration 132/1000 | Loss: 0.00000960
Iteration 133/1000 | Loss: 0.00000960
Iteration 134/1000 | Loss: 0.00000960
Iteration 135/1000 | Loss: 0.00000960
Iteration 136/1000 | Loss: 0.00000960
Iteration 137/1000 | Loss: 0.00000960
Iteration 138/1000 | Loss: 0.00000960
Iteration 139/1000 | Loss: 0.00000960
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000958
Iteration 145/1000 | Loss: 0.00000958
Iteration 146/1000 | Loss: 0.00000958
Iteration 147/1000 | Loss: 0.00000958
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000958
Iteration 151/1000 | Loss: 0.00000958
Iteration 152/1000 | Loss: 0.00000958
Iteration 153/1000 | Loss: 0.00000958
Iteration 154/1000 | Loss: 0.00000958
Iteration 155/1000 | Loss: 0.00000958
Iteration 156/1000 | Loss: 0.00000957
Iteration 157/1000 | Loss: 0.00000957
Iteration 158/1000 | Loss: 0.00000956
Iteration 159/1000 | Loss: 0.00000956
Iteration 160/1000 | Loss: 0.00000956
Iteration 161/1000 | Loss: 0.00000956
Iteration 162/1000 | Loss: 0.00000955
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000955
Iteration 168/1000 | Loss: 0.00000955
Iteration 169/1000 | Loss: 0.00000955
Iteration 170/1000 | Loss: 0.00000955
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000953
Iteration 178/1000 | Loss: 0.00000953
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000953
Iteration 186/1000 | Loss: 0.00000953
Iteration 187/1000 | Loss: 0.00000953
Iteration 188/1000 | Loss: 0.00000953
Iteration 189/1000 | Loss: 0.00000952
Iteration 190/1000 | Loss: 0.00000952
Iteration 191/1000 | Loss: 0.00000952
Iteration 192/1000 | Loss: 0.00000952
Iteration 193/1000 | Loss: 0.00000952
Iteration 194/1000 | Loss: 0.00000952
Iteration 195/1000 | Loss: 0.00000952
Iteration 196/1000 | Loss: 0.00000952
Iteration 197/1000 | Loss: 0.00000952
Iteration 198/1000 | Loss: 0.00000951
Iteration 199/1000 | Loss: 0.00000951
Iteration 200/1000 | Loss: 0.00000951
Iteration 201/1000 | Loss: 0.00000951
Iteration 202/1000 | Loss: 0.00000951
Iteration 203/1000 | Loss: 0.00000951
Iteration 204/1000 | Loss: 0.00000951
Iteration 205/1000 | Loss: 0.00000951
Iteration 206/1000 | Loss: 0.00000950
Iteration 207/1000 | Loss: 0.00000950
Iteration 208/1000 | Loss: 0.00000950
Iteration 209/1000 | Loss: 0.00000950
Iteration 210/1000 | Loss: 0.00000950
Iteration 211/1000 | Loss: 0.00000950
Iteration 212/1000 | Loss: 0.00000950
Iteration 213/1000 | Loss: 0.00000949
Iteration 214/1000 | Loss: 0.00000949
Iteration 215/1000 | Loss: 0.00000949
Iteration 216/1000 | Loss: 0.00000949
Iteration 217/1000 | Loss: 0.00000949
Iteration 218/1000 | Loss: 0.00000949
Iteration 219/1000 | Loss: 0.00000949
Iteration 220/1000 | Loss: 0.00000949
Iteration 221/1000 | Loss: 0.00000949
Iteration 222/1000 | Loss: 0.00000949
Iteration 223/1000 | Loss: 0.00000949
Iteration 224/1000 | Loss: 0.00000948
Iteration 225/1000 | Loss: 0.00000948
Iteration 226/1000 | Loss: 0.00000948
Iteration 227/1000 | Loss: 0.00000948
Iteration 228/1000 | Loss: 0.00000948
Iteration 229/1000 | Loss: 0.00000948
Iteration 230/1000 | Loss: 0.00000948
Iteration 231/1000 | Loss: 0.00000948
Iteration 232/1000 | Loss: 0.00000948
Iteration 233/1000 | Loss: 0.00000948
Iteration 234/1000 | Loss: 0.00000948
Iteration 235/1000 | Loss: 0.00000948
Iteration 236/1000 | Loss: 0.00000948
Iteration 237/1000 | Loss: 0.00000947
Iteration 238/1000 | Loss: 0.00000947
Iteration 239/1000 | Loss: 0.00000947
Iteration 240/1000 | Loss: 0.00000947
Iteration 241/1000 | Loss: 0.00000947
Iteration 242/1000 | Loss: 0.00000947
Iteration 243/1000 | Loss: 0.00000947
Iteration 244/1000 | Loss: 0.00000947
Iteration 245/1000 | Loss: 0.00000947
Iteration 246/1000 | Loss: 0.00000947
Iteration 247/1000 | Loss: 0.00000947
Iteration 248/1000 | Loss: 0.00000947
Iteration 249/1000 | Loss: 0.00000947
Iteration 250/1000 | Loss: 0.00000947
Iteration 251/1000 | Loss: 0.00000947
Iteration 252/1000 | Loss: 0.00000947
Iteration 253/1000 | Loss: 0.00000946
Iteration 254/1000 | Loss: 0.00000946
Iteration 255/1000 | Loss: 0.00000946
Iteration 256/1000 | Loss: 0.00000946
Iteration 257/1000 | Loss: 0.00000946
Iteration 258/1000 | Loss: 0.00000946
Iteration 259/1000 | Loss: 0.00000946
Iteration 260/1000 | Loss: 0.00000946
Iteration 261/1000 | Loss: 0.00000946
Iteration 262/1000 | Loss: 0.00000946
Iteration 263/1000 | Loss: 0.00000946
Iteration 264/1000 | Loss: 0.00000946
Iteration 265/1000 | Loss: 0.00000946
Iteration 266/1000 | Loss: 0.00000946
Iteration 267/1000 | Loss: 0.00000946
Iteration 268/1000 | Loss: 0.00000945
Iteration 269/1000 | Loss: 0.00000945
Iteration 270/1000 | Loss: 0.00000945
Iteration 271/1000 | Loss: 0.00000945
Iteration 272/1000 | Loss: 0.00000945
Iteration 273/1000 | Loss: 0.00000945
Iteration 274/1000 | Loss: 0.00000945
Iteration 275/1000 | Loss: 0.00000945
Iteration 276/1000 | Loss: 0.00000945
Iteration 277/1000 | Loss: 0.00000945
Iteration 278/1000 | Loss: 0.00000945
Iteration 279/1000 | Loss: 0.00000945
Iteration 280/1000 | Loss: 0.00000945
Iteration 281/1000 | Loss: 0.00000945
Iteration 282/1000 | Loss: 0.00000945
Iteration 283/1000 | Loss: 0.00000944
Iteration 284/1000 | Loss: 0.00000944
Iteration 285/1000 | Loss: 0.00000944
Iteration 286/1000 | Loss: 0.00000944
Iteration 287/1000 | Loss: 0.00000944
Iteration 288/1000 | Loss: 0.00000944
Iteration 289/1000 | Loss: 0.00000944
Iteration 290/1000 | Loss: 0.00000944
Iteration 291/1000 | Loss: 0.00000944
Iteration 292/1000 | Loss: 0.00000944
Iteration 293/1000 | Loss: 0.00000943
Iteration 294/1000 | Loss: 0.00000943
Iteration 295/1000 | Loss: 0.00000943
Iteration 296/1000 | Loss: 0.00000943
Iteration 297/1000 | Loss: 0.00000943
Iteration 298/1000 | Loss: 0.00000943
Iteration 299/1000 | Loss: 0.00000943
Iteration 300/1000 | Loss: 0.00000943
Iteration 301/1000 | Loss: 0.00000943
Iteration 302/1000 | Loss: 0.00000943
Iteration 303/1000 | Loss: 0.00000943
Iteration 304/1000 | Loss: 0.00000943
Iteration 305/1000 | Loss: 0.00000943
Iteration 306/1000 | Loss: 0.00000943
Iteration 307/1000 | Loss: 0.00000943
Iteration 308/1000 | Loss: 0.00000943
Iteration 309/1000 | Loss: 0.00000943
Iteration 310/1000 | Loss: 0.00000943
Iteration 311/1000 | Loss: 0.00000942
Iteration 312/1000 | Loss: 0.00000942
Iteration 313/1000 | Loss: 0.00000942
Iteration 314/1000 | Loss: 0.00000942
Iteration 315/1000 | Loss: 0.00000942
Iteration 316/1000 | Loss: 0.00000942
Iteration 317/1000 | Loss: 0.00000942
Iteration 318/1000 | Loss: 0.00000941
Iteration 319/1000 | Loss: 0.00000941
Iteration 320/1000 | Loss: 0.00000941
Iteration 321/1000 | Loss: 0.00000941
Iteration 322/1000 | Loss: 0.00000941
Iteration 323/1000 | Loss: 0.00000941
Iteration 324/1000 | Loss: 0.00000941
Iteration 325/1000 | Loss: 0.00000941
Iteration 326/1000 | Loss: 0.00000941
Iteration 327/1000 | Loss: 0.00000941
Iteration 328/1000 | Loss: 0.00000941
Iteration 329/1000 | Loss: 0.00000941
Iteration 330/1000 | Loss: 0.00000940
Iteration 331/1000 | Loss: 0.00000940
Iteration 332/1000 | Loss: 0.00000940
Iteration 333/1000 | Loss: 0.00000940
Iteration 334/1000 | Loss: 0.00000940
Iteration 335/1000 | Loss: 0.00000940
Iteration 336/1000 | Loss: 0.00000939
Iteration 337/1000 | Loss: 0.00000939
Iteration 338/1000 | Loss: 0.00000939
Iteration 339/1000 | Loss: 0.00000939
Iteration 340/1000 | Loss: 0.00000939
Iteration 341/1000 | Loss: 0.00000939
Iteration 342/1000 | Loss: 0.00000939
Iteration 343/1000 | Loss: 0.00000939
Iteration 344/1000 | Loss: 0.00000939
Iteration 345/1000 | Loss: 0.00000939
Iteration 346/1000 | Loss: 0.00000939
Iteration 347/1000 | Loss: 0.00000939
Iteration 348/1000 | Loss: 0.00000939
Iteration 349/1000 | Loss: 0.00000939
Iteration 350/1000 | Loss: 0.00000939
Iteration 351/1000 | Loss: 0.00000939
Iteration 352/1000 | Loss: 0.00000939
Iteration 353/1000 | Loss: 0.00000939
Iteration 354/1000 | Loss: 0.00000939
Iteration 355/1000 | Loss: 0.00000938
Iteration 356/1000 | Loss: 0.00000938
Iteration 357/1000 | Loss: 0.00000938
Iteration 358/1000 | Loss: 0.00000938
Iteration 359/1000 | Loss: 0.00000938
Iteration 360/1000 | Loss: 0.00000938
Iteration 361/1000 | Loss: 0.00000938
Iteration 362/1000 | Loss: 0.00000938
Iteration 363/1000 | Loss: 0.00000938
Iteration 364/1000 | Loss: 0.00000938
Iteration 365/1000 | Loss: 0.00000938
Iteration 366/1000 | Loss: 0.00000938
Iteration 367/1000 | Loss: 0.00000938
Iteration 368/1000 | Loss: 0.00000938
Iteration 369/1000 | Loss: 0.00000938
Iteration 370/1000 | Loss: 0.00000938
Iteration 371/1000 | Loss: 0.00000938
Iteration 372/1000 | Loss: 0.00000938
Iteration 373/1000 | Loss: 0.00000938
Iteration 374/1000 | Loss: 0.00000938
Iteration 375/1000 | Loss: 0.00000938
Iteration 376/1000 | Loss: 0.00000938
Iteration 377/1000 | Loss: 0.00000938
Iteration 378/1000 | Loss: 0.00000937
Iteration 379/1000 | Loss: 0.00000937
Iteration 380/1000 | Loss: 0.00000937
Iteration 381/1000 | Loss: 0.00000937
Iteration 382/1000 | Loss: 0.00000937
Iteration 383/1000 | Loss: 0.00000937
Iteration 384/1000 | Loss: 0.00000937
Iteration 385/1000 | Loss: 0.00000937
Iteration 386/1000 | Loss: 0.00000937
Iteration 387/1000 | Loss: 0.00000937
Iteration 388/1000 | Loss: 0.00000937
Iteration 389/1000 | Loss: 0.00000937
Iteration 390/1000 | Loss: 0.00000937
Iteration 391/1000 | Loss: 0.00000937
Iteration 392/1000 | Loss: 0.00000937
Iteration 393/1000 | Loss: 0.00000937
Iteration 394/1000 | Loss: 0.00000937
Iteration 395/1000 | Loss: 0.00000937
Iteration 396/1000 | Loss: 0.00000937
Iteration 397/1000 | Loss: 0.00000937
Iteration 398/1000 | Loss: 0.00000936
Iteration 399/1000 | Loss: 0.00000936
Iteration 400/1000 | Loss: 0.00000936
Iteration 401/1000 | Loss: 0.00000936
Iteration 402/1000 | Loss: 0.00000936
Iteration 403/1000 | Loss: 0.00000936
Iteration 404/1000 | Loss: 0.00000936
Iteration 405/1000 | Loss: 0.00000936
Iteration 406/1000 | Loss: 0.00000936
Iteration 407/1000 | Loss: 0.00000936
Iteration 408/1000 | Loss: 0.00000936
Iteration 409/1000 | Loss: 0.00000936
Iteration 410/1000 | Loss: 0.00000936
Iteration 411/1000 | Loss: 0.00000936
Iteration 412/1000 | Loss: 0.00000936
Iteration 413/1000 | Loss: 0.00000936
Iteration 414/1000 | Loss: 0.00000936
Iteration 415/1000 | Loss: 0.00000936
Iteration 416/1000 | Loss: 0.00000936
Iteration 417/1000 | Loss: 0.00000936
Iteration 418/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 418. Stopping optimization.
Last 5 losses: [9.364536708744708e-06, 9.364536708744708e-06, 9.364536708744708e-06, 9.364536708744708e-06, 9.364536708744708e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.364536708744708e-06

Optimization complete. Final v2v error: 2.505270004272461 mm

Highest mean error: 3.083437442779541 mm for frame 70

Lowest mean error: 2.309724807739258 mm for frame 114

Saving results

Total time: 47.457910776138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967250
Iteration 2/25 | Loss: 0.00180720
Iteration 3/25 | Loss: 0.00135049
Iteration 4/25 | Loss: 0.00132795
Iteration 5/25 | Loss: 0.00132060
Iteration 6/25 | Loss: 0.00131888
Iteration 7/25 | Loss: 0.00131888
Iteration 8/25 | Loss: 0.00131888
Iteration 9/25 | Loss: 0.00131888
Iteration 10/25 | Loss: 0.00131888
Iteration 11/25 | Loss: 0.00131888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001318883616477251, 0.001318883616477251, 0.001318883616477251, 0.001318883616477251, 0.001318883616477251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001318883616477251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78411144
Iteration 2/25 | Loss: 0.00109833
Iteration 3/25 | Loss: 0.00109832
Iteration 4/25 | Loss: 0.00109832
Iteration 5/25 | Loss: 0.00109832
Iteration 6/25 | Loss: 0.00109832
Iteration 7/25 | Loss: 0.00109832
Iteration 8/25 | Loss: 0.00109832
Iteration 9/25 | Loss: 0.00109832
Iteration 10/25 | Loss: 0.00109832
Iteration 11/25 | Loss: 0.00109832
Iteration 12/25 | Loss: 0.00109832
Iteration 13/25 | Loss: 0.00109832
Iteration 14/25 | Loss: 0.00109832
Iteration 15/25 | Loss: 0.00109832
Iteration 16/25 | Loss: 0.00109832
Iteration 17/25 | Loss: 0.00109832
Iteration 18/25 | Loss: 0.00109832
Iteration 19/25 | Loss: 0.00109832
Iteration 20/25 | Loss: 0.00109832
Iteration 21/25 | Loss: 0.00109832
Iteration 22/25 | Loss: 0.00109832
Iteration 23/25 | Loss: 0.00109832
Iteration 24/25 | Loss: 0.00109832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010983203537762165, 0.0010983203537762165, 0.0010983203537762165, 0.0010983203537762165, 0.0010983203537762165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010983203537762165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109832
Iteration 2/1000 | Loss: 0.00008466
Iteration 3/1000 | Loss: 0.00004959
Iteration 4/1000 | Loss: 0.00003847
Iteration 5/1000 | Loss: 0.00003578
Iteration 6/1000 | Loss: 0.00003434
Iteration 7/1000 | Loss: 0.00003372
Iteration 8/1000 | Loss: 0.00003303
Iteration 9/1000 | Loss: 0.00003249
Iteration 10/1000 | Loss: 0.00003202
Iteration 11/1000 | Loss: 0.00003168
Iteration 12/1000 | Loss: 0.00003143
Iteration 13/1000 | Loss: 0.00003111
Iteration 14/1000 | Loss: 0.00003081
Iteration 15/1000 | Loss: 0.00003065
Iteration 16/1000 | Loss: 0.00003056
Iteration 17/1000 | Loss: 0.00003035
Iteration 18/1000 | Loss: 0.00003014
Iteration 19/1000 | Loss: 0.00003002
Iteration 20/1000 | Loss: 0.00002994
Iteration 21/1000 | Loss: 0.00002984
Iteration 22/1000 | Loss: 0.00002979
Iteration 23/1000 | Loss: 0.00002975
Iteration 24/1000 | Loss: 0.00002968
Iteration 25/1000 | Loss: 0.00002965
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002964
Iteration 28/1000 | Loss: 0.00002963
Iteration 29/1000 | Loss: 0.00002961
Iteration 30/1000 | Loss: 0.00002960
Iteration 31/1000 | Loss: 0.00002960
Iteration 32/1000 | Loss: 0.00002960
Iteration 33/1000 | Loss: 0.00002960
Iteration 34/1000 | Loss: 0.00002960
Iteration 35/1000 | Loss: 0.00002960
Iteration 36/1000 | Loss: 0.00002960
Iteration 37/1000 | Loss: 0.00002960
Iteration 38/1000 | Loss: 0.00002960
Iteration 39/1000 | Loss: 0.00002960
Iteration 40/1000 | Loss: 0.00002960
Iteration 41/1000 | Loss: 0.00002959
Iteration 42/1000 | Loss: 0.00002959
Iteration 43/1000 | Loss: 0.00002959
Iteration 44/1000 | Loss: 0.00002959
Iteration 45/1000 | Loss: 0.00002959
Iteration 46/1000 | Loss: 0.00002959
Iteration 47/1000 | Loss: 0.00002959
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002959
Iteration 50/1000 | Loss: 0.00002959
Iteration 51/1000 | Loss: 0.00002959
Iteration 52/1000 | Loss: 0.00002959
Iteration 53/1000 | Loss: 0.00002959
Iteration 54/1000 | Loss: 0.00002958
Iteration 55/1000 | Loss: 0.00002958
Iteration 56/1000 | Loss: 0.00002958
Iteration 57/1000 | Loss: 0.00002958
Iteration 58/1000 | Loss: 0.00002957
Iteration 59/1000 | Loss: 0.00002957
Iteration 60/1000 | Loss: 0.00002957
Iteration 61/1000 | Loss: 0.00002956
Iteration 62/1000 | Loss: 0.00002956
Iteration 63/1000 | Loss: 0.00002956
Iteration 64/1000 | Loss: 0.00002955
Iteration 65/1000 | Loss: 0.00002955
Iteration 66/1000 | Loss: 0.00002954
Iteration 67/1000 | Loss: 0.00002954
Iteration 68/1000 | Loss: 0.00002954
Iteration 69/1000 | Loss: 0.00002954
Iteration 70/1000 | Loss: 0.00002954
Iteration 71/1000 | Loss: 0.00002954
Iteration 72/1000 | Loss: 0.00002953
Iteration 73/1000 | Loss: 0.00002953
Iteration 74/1000 | Loss: 0.00002953
Iteration 75/1000 | Loss: 0.00002953
Iteration 76/1000 | Loss: 0.00002952
Iteration 77/1000 | Loss: 0.00002952
Iteration 78/1000 | Loss: 0.00002952
Iteration 79/1000 | Loss: 0.00002952
Iteration 80/1000 | Loss: 0.00002951
Iteration 81/1000 | Loss: 0.00002951
Iteration 82/1000 | Loss: 0.00002951
Iteration 83/1000 | Loss: 0.00002951
Iteration 84/1000 | Loss: 0.00002951
Iteration 85/1000 | Loss: 0.00002951
Iteration 86/1000 | Loss: 0.00002951
Iteration 87/1000 | Loss: 0.00002951
Iteration 88/1000 | Loss: 0.00002951
Iteration 89/1000 | Loss: 0.00002951
Iteration 90/1000 | Loss: 0.00002951
Iteration 91/1000 | Loss: 0.00002951
Iteration 92/1000 | Loss: 0.00002951
Iteration 93/1000 | Loss: 0.00002951
Iteration 94/1000 | Loss: 0.00002951
Iteration 95/1000 | Loss: 0.00002950
Iteration 96/1000 | Loss: 0.00002950
Iteration 97/1000 | Loss: 0.00002950
Iteration 98/1000 | Loss: 0.00002950
Iteration 99/1000 | Loss: 0.00002950
Iteration 100/1000 | Loss: 0.00002949
Iteration 101/1000 | Loss: 0.00002949
Iteration 102/1000 | Loss: 0.00002949
Iteration 103/1000 | Loss: 0.00002949
Iteration 104/1000 | Loss: 0.00002949
Iteration 105/1000 | Loss: 0.00002949
Iteration 106/1000 | Loss: 0.00002949
Iteration 107/1000 | Loss: 0.00002948
Iteration 108/1000 | Loss: 0.00002948
Iteration 109/1000 | Loss: 0.00002948
Iteration 110/1000 | Loss: 0.00002948
Iteration 111/1000 | Loss: 0.00002948
Iteration 112/1000 | Loss: 0.00002948
Iteration 113/1000 | Loss: 0.00002948
Iteration 114/1000 | Loss: 0.00002948
Iteration 115/1000 | Loss: 0.00002948
Iteration 116/1000 | Loss: 0.00002948
Iteration 117/1000 | Loss: 0.00002948
Iteration 118/1000 | Loss: 0.00002948
Iteration 119/1000 | Loss: 0.00002948
Iteration 120/1000 | Loss: 0.00002948
Iteration 121/1000 | Loss: 0.00002948
Iteration 122/1000 | Loss: 0.00002948
Iteration 123/1000 | Loss: 0.00002947
Iteration 124/1000 | Loss: 0.00002947
Iteration 125/1000 | Loss: 0.00002947
Iteration 126/1000 | Loss: 0.00002947
Iteration 127/1000 | Loss: 0.00002946
Iteration 128/1000 | Loss: 0.00002946
Iteration 129/1000 | Loss: 0.00002946
Iteration 130/1000 | Loss: 0.00002946
Iteration 131/1000 | Loss: 0.00002946
Iteration 132/1000 | Loss: 0.00002946
Iteration 133/1000 | Loss: 0.00002945
Iteration 134/1000 | Loss: 0.00002945
Iteration 135/1000 | Loss: 0.00002945
Iteration 136/1000 | Loss: 0.00002945
Iteration 137/1000 | Loss: 0.00002945
Iteration 138/1000 | Loss: 0.00002945
Iteration 139/1000 | Loss: 0.00002945
Iteration 140/1000 | Loss: 0.00002945
Iteration 141/1000 | Loss: 0.00002945
Iteration 142/1000 | Loss: 0.00002945
Iteration 143/1000 | Loss: 0.00002945
Iteration 144/1000 | Loss: 0.00002944
Iteration 145/1000 | Loss: 0.00002944
Iteration 146/1000 | Loss: 0.00002944
Iteration 147/1000 | Loss: 0.00002944
Iteration 148/1000 | Loss: 0.00002944
Iteration 149/1000 | Loss: 0.00002944
Iteration 150/1000 | Loss: 0.00002944
Iteration 151/1000 | Loss: 0.00002943
Iteration 152/1000 | Loss: 0.00002943
Iteration 153/1000 | Loss: 0.00002943
Iteration 154/1000 | Loss: 0.00002943
Iteration 155/1000 | Loss: 0.00002943
Iteration 156/1000 | Loss: 0.00002943
Iteration 157/1000 | Loss: 0.00002943
Iteration 158/1000 | Loss: 0.00002943
Iteration 159/1000 | Loss: 0.00002943
Iteration 160/1000 | Loss: 0.00002943
Iteration 161/1000 | Loss: 0.00002943
Iteration 162/1000 | Loss: 0.00002943
Iteration 163/1000 | Loss: 0.00002943
Iteration 164/1000 | Loss: 0.00002943
Iteration 165/1000 | Loss: 0.00002943
Iteration 166/1000 | Loss: 0.00002943
Iteration 167/1000 | Loss: 0.00002943
Iteration 168/1000 | Loss: 0.00002942
Iteration 169/1000 | Loss: 0.00002942
Iteration 170/1000 | Loss: 0.00002942
Iteration 171/1000 | Loss: 0.00002942
Iteration 172/1000 | Loss: 0.00002942
Iteration 173/1000 | Loss: 0.00002942
Iteration 174/1000 | Loss: 0.00002942
Iteration 175/1000 | Loss: 0.00002942
Iteration 176/1000 | Loss: 0.00002942
Iteration 177/1000 | Loss: 0.00002942
Iteration 178/1000 | Loss: 0.00002942
Iteration 179/1000 | Loss: 0.00002942
Iteration 180/1000 | Loss: 0.00002942
Iteration 181/1000 | Loss: 0.00002942
Iteration 182/1000 | Loss: 0.00002942
Iteration 183/1000 | Loss: 0.00002942
Iteration 184/1000 | Loss: 0.00002942
Iteration 185/1000 | Loss: 0.00002942
Iteration 186/1000 | Loss: 0.00002941
Iteration 187/1000 | Loss: 0.00002941
Iteration 188/1000 | Loss: 0.00002941
Iteration 189/1000 | Loss: 0.00002941
Iteration 190/1000 | Loss: 0.00002941
Iteration 191/1000 | Loss: 0.00002941
Iteration 192/1000 | Loss: 0.00002941
Iteration 193/1000 | Loss: 0.00002941
Iteration 194/1000 | Loss: 0.00002941
Iteration 195/1000 | Loss: 0.00002941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.9413411539280787e-05, 2.9413411539280787e-05, 2.9413411539280787e-05, 2.9413411539280787e-05, 2.9413411539280787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9413411539280787e-05

Optimization complete. Final v2v error: 4.352782249450684 mm

Highest mean error: 5.515229225158691 mm for frame 105

Lowest mean error: 3.294858694076538 mm for frame 33

Saving results

Total time: 52.48179507255554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382869
Iteration 2/25 | Loss: 0.00109340
Iteration 3/25 | Loss: 0.00104979
Iteration 4/25 | Loss: 0.00104403
Iteration 5/25 | Loss: 0.00104226
Iteration 6/25 | Loss: 0.00104225
Iteration 7/25 | Loss: 0.00104225
Iteration 8/25 | Loss: 0.00104225
Iteration 9/25 | Loss: 0.00104225
Iteration 10/25 | Loss: 0.00104225
Iteration 11/25 | Loss: 0.00104226
Iteration 12/25 | Loss: 0.00104226
Iteration 13/25 | Loss: 0.00104226
Iteration 14/25 | Loss: 0.00104226
Iteration 15/25 | Loss: 0.00104226
Iteration 16/25 | Loss: 0.00104226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010422550840303302, 0.0010422550840303302, 0.0010422550840303302, 0.0010422550840303302, 0.0010422550840303302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010422550840303302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34514403
Iteration 2/25 | Loss: 0.00071919
Iteration 3/25 | Loss: 0.00071919
Iteration 4/25 | Loss: 0.00071918
Iteration 5/25 | Loss: 0.00071918
Iteration 6/25 | Loss: 0.00071918
Iteration 7/25 | Loss: 0.00071918
Iteration 8/25 | Loss: 0.00071918
Iteration 9/25 | Loss: 0.00071918
Iteration 10/25 | Loss: 0.00071918
Iteration 11/25 | Loss: 0.00071918
Iteration 12/25 | Loss: 0.00071918
Iteration 13/25 | Loss: 0.00071918
Iteration 14/25 | Loss: 0.00071918
Iteration 15/25 | Loss: 0.00071918
Iteration 16/25 | Loss: 0.00071918
Iteration 17/25 | Loss: 0.00071918
Iteration 18/25 | Loss: 0.00071918
Iteration 19/25 | Loss: 0.00071918
Iteration 20/25 | Loss: 0.00071918
Iteration 21/25 | Loss: 0.00071918
Iteration 22/25 | Loss: 0.00071918
Iteration 23/25 | Loss: 0.00071918
Iteration 24/25 | Loss: 0.00071918
Iteration 25/25 | Loss: 0.00071918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071918
Iteration 2/1000 | Loss: 0.00002299
Iteration 3/1000 | Loss: 0.00001633
Iteration 4/1000 | Loss: 0.00001424
Iteration 5/1000 | Loss: 0.00001291
Iteration 6/1000 | Loss: 0.00001214
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001113
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001075
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001038
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001022
Iteration 17/1000 | Loss: 0.00001019
Iteration 18/1000 | Loss: 0.00001015
Iteration 19/1000 | Loss: 0.00001015
Iteration 20/1000 | Loss: 0.00001014
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001009
Iteration 24/1000 | Loss: 0.00001008
Iteration 25/1000 | Loss: 0.00001008
Iteration 26/1000 | Loss: 0.00001007
Iteration 27/1000 | Loss: 0.00001007
Iteration 28/1000 | Loss: 0.00001006
Iteration 29/1000 | Loss: 0.00001006
Iteration 30/1000 | Loss: 0.00001006
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001005
Iteration 36/1000 | Loss: 0.00001005
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001004
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001004
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001002
Iteration 44/1000 | Loss: 0.00001002
Iteration 45/1000 | Loss: 0.00001002
Iteration 46/1000 | Loss: 0.00001002
Iteration 47/1000 | Loss: 0.00001002
Iteration 48/1000 | Loss: 0.00001002
Iteration 49/1000 | Loss: 0.00001001
Iteration 50/1000 | Loss: 0.00001001
Iteration 51/1000 | Loss: 0.00001001
Iteration 52/1000 | Loss: 0.00001001
Iteration 53/1000 | Loss: 0.00001000
Iteration 54/1000 | Loss: 0.00001000
Iteration 55/1000 | Loss: 0.00000999
Iteration 56/1000 | Loss: 0.00000998
Iteration 57/1000 | Loss: 0.00000998
Iteration 58/1000 | Loss: 0.00000998
Iteration 59/1000 | Loss: 0.00000998
Iteration 60/1000 | Loss: 0.00000998
Iteration 61/1000 | Loss: 0.00000998
Iteration 62/1000 | Loss: 0.00000997
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000993
Iteration 68/1000 | Loss: 0.00000993
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000989
Iteration 75/1000 | Loss: 0.00000989
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000984
Iteration 85/1000 | Loss: 0.00000983
Iteration 86/1000 | Loss: 0.00000983
Iteration 87/1000 | Loss: 0.00000983
Iteration 88/1000 | Loss: 0.00000983
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000982
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000975
Iteration 118/1000 | Loss: 0.00000975
Iteration 119/1000 | Loss: 0.00000975
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Iteration 122/1000 | Loss: 0.00000975
Iteration 123/1000 | Loss: 0.00000974
Iteration 124/1000 | Loss: 0.00000974
Iteration 125/1000 | Loss: 0.00000973
Iteration 126/1000 | Loss: 0.00000973
Iteration 127/1000 | Loss: 0.00000973
Iteration 128/1000 | Loss: 0.00000973
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000971
Iteration 131/1000 | Loss: 0.00000971
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000970
Iteration 137/1000 | Loss: 0.00000970
Iteration 138/1000 | Loss: 0.00000970
Iteration 139/1000 | Loss: 0.00000970
Iteration 140/1000 | Loss: 0.00000969
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000968
Iteration 145/1000 | Loss: 0.00000968
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000967
Iteration 149/1000 | Loss: 0.00000967
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000967
Iteration 153/1000 | Loss: 0.00000967
Iteration 154/1000 | Loss: 0.00000967
Iteration 155/1000 | Loss: 0.00000967
Iteration 156/1000 | Loss: 0.00000966
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000966
Iteration 159/1000 | Loss: 0.00000966
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000964
Iteration 164/1000 | Loss: 0.00000964
Iteration 165/1000 | Loss: 0.00000964
Iteration 166/1000 | Loss: 0.00000964
Iteration 167/1000 | Loss: 0.00000964
Iteration 168/1000 | Loss: 0.00000963
Iteration 169/1000 | Loss: 0.00000963
Iteration 170/1000 | Loss: 0.00000963
Iteration 171/1000 | Loss: 0.00000963
Iteration 172/1000 | Loss: 0.00000963
Iteration 173/1000 | Loss: 0.00000962
Iteration 174/1000 | Loss: 0.00000962
Iteration 175/1000 | Loss: 0.00000962
Iteration 176/1000 | Loss: 0.00000962
Iteration 177/1000 | Loss: 0.00000962
Iteration 178/1000 | Loss: 0.00000962
Iteration 179/1000 | Loss: 0.00000962
Iteration 180/1000 | Loss: 0.00000962
Iteration 181/1000 | Loss: 0.00000962
Iteration 182/1000 | Loss: 0.00000962
Iteration 183/1000 | Loss: 0.00000962
Iteration 184/1000 | Loss: 0.00000962
Iteration 185/1000 | Loss: 0.00000961
Iteration 186/1000 | Loss: 0.00000961
Iteration 187/1000 | Loss: 0.00000961
Iteration 188/1000 | Loss: 0.00000961
Iteration 189/1000 | Loss: 0.00000961
Iteration 190/1000 | Loss: 0.00000961
Iteration 191/1000 | Loss: 0.00000961
Iteration 192/1000 | Loss: 0.00000961
Iteration 193/1000 | Loss: 0.00000961
Iteration 194/1000 | Loss: 0.00000961
Iteration 195/1000 | Loss: 0.00000961
Iteration 196/1000 | Loss: 0.00000961
Iteration 197/1000 | Loss: 0.00000961
Iteration 198/1000 | Loss: 0.00000961
Iteration 199/1000 | Loss: 0.00000960
Iteration 200/1000 | Loss: 0.00000960
Iteration 201/1000 | Loss: 0.00000960
Iteration 202/1000 | Loss: 0.00000960
Iteration 203/1000 | Loss: 0.00000960
Iteration 204/1000 | Loss: 0.00000960
Iteration 205/1000 | Loss: 0.00000960
Iteration 206/1000 | Loss: 0.00000960
Iteration 207/1000 | Loss: 0.00000960
Iteration 208/1000 | Loss: 0.00000960
Iteration 209/1000 | Loss: 0.00000960
Iteration 210/1000 | Loss: 0.00000960
Iteration 211/1000 | Loss: 0.00000960
Iteration 212/1000 | Loss: 0.00000960
Iteration 213/1000 | Loss: 0.00000960
Iteration 214/1000 | Loss: 0.00000960
Iteration 215/1000 | Loss: 0.00000960
Iteration 216/1000 | Loss: 0.00000960
Iteration 217/1000 | Loss: 0.00000960
Iteration 218/1000 | Loss: 0.00000960
Iteration 219/1000 | Loss: 0.00000960
Iteration 220/1000 | Loss: 0.00000960
Iteration 221/1000 | Loss: 0.00000960
Iteration 222/1000 | Loss: 0.00000960
Iteration 223/1000 | Loss: 0.00000960
Iteration 224/1000 | Loss: 0.00000960
Iteration 225/1000 | Loss: 0.00000960
Iteration 226/1000 | Loss: 0.00000960
Iteration 227/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [9.598428732715547e-06, 9.598428732715547e-06, 9.598428732715547e-06, 9.598428732715547e-06, 9.598428732715547e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.598428732715547e-06

Optimization complete. Final v2v error: 2.6570701599121094 mm

Highest mean error: 2.8016786575317383 mm for frame 152

Lowest mean error: 2.537169933319092 mm for frame 46

Saving results

Total time: 42.071128129959106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00535231
Iteration 2/25 | Loss: 0.00116601
Iteration 3/25 | Loss: 0.00109157
Iteration 4/25 | Loss: 0.00108134
Iteration 5/25 | Loss: 0.00107801
Iteration 6/25 | Loss: 0.00107759
Iteration 7/25 | Loss: 0.00107759
Iteration 8/25 | Loss: 0.00107759
Iteration 9/25 | Loss: 0.00107759
Iteration 10/25 | Loss: 0.00107759
Iteration 11/25 | Loss: 0.00107759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010775948176160455, 0.0010775948176160455, 0.0010775948176160455, 0.0010775948176160455, 0.0010775948176160455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010775948176160455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.37982607
Iteration 2/25 | Loss: 0.00076929
Iteration 3/25 | Loss: 0.00076928
Iteration 4/25 | Loss: 0.00076927
Iteration 5/25 | Loss: 0.00076927
Iteration 6/25 | Loss: 0.00076927
Iteration 7/25 | Loss: 0.00076927
Iteration 8/25 | Loss: 0.00076927
Iteration 9/25 | Loss: 0.00076927
Iteration 10/25 | Loss: 0.00076927
Iteration 11/25 | Loss: 0.00076927
Iteration 12/25 | Loss: 0.00076927
Iteration 13/25 | Loss: 0.00076927
Iteration 14/25 | Loss: 0.00076927
Iteration 15/25 | Loss: 0.00076927
Iteration 16/25 | Loss: 0.00076927
Iteration 17/25 | Loss: 0.00076927
Iteration 18/25 | Loss: 0.00076927
Iteration 19/25 | Loss: 0.00076927
Iteration 20/25 | Loss: 0.00076927
Iteration 21/25 | Loss: 0.00076927
Iteration 22/25 | Loss: 0.00076927
Iteration 23/25 | Loss: 0.00076927
Iteration 24/25 | Loss: 0.00076927
Iteration 25/25 | Loss: 0.00076927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076927
Iteration 2/1000 | Loss: 0.00001938
Iteration 3/1000 | Loss: 0.00001437
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001214
Iteration 7/1000 | Loss: 0.00001170
Iteration 8/1000 | Loss: 0.00001146
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001099
Iteration 11/1000 | Loss: 0.00001095
Iteration 12/1000 | Loss: 0.00001091
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001081
Iteration 15/1000 | Loss: 0.00001079
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001077
Iteration 18/1000 | Loss: 0.00001072
Iteration 19/1000 | Loss: 0.00001068
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001067
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001063
Iteration 26/1000 | Loss: 0.00001063
Iteration 27/1000 | Loss: 0.00001061
Iteration 28/1000 | Loss: 0.00001061
Iteration 29/1000 | Loss: 0.00001060
Iteration 30/1000 | Loss: 0.00001060
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001059
Iteration 33/1000 | Loss: 0.00001059
Iteration 34/1000 | Loss: 0.00001059
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001057
Iteration 39/1000 | Loss: 0.00001057
Iteration 40/1000 | Loss: 0.00001057
Iteration 41/1000 | Loss: 0.00001056
Iteration 42/1000 | Loss: 0.00001055
Iteration 43/1000 | Loss: 0.00001055
Iteration 44/1000 | Loss: 0.00001054
Iteration 45/1000 | Loss: 0.00001054
Iteration 46/1000 | Loss: 0.00001053
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001051
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001050
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001045
Iteration 56/1000 | Loss: 0.00001044
Iteration 57/1000 | Loss: 0.00001043
Iteration 58/1000 | Loss: 0.00001042
Iteration 59/1000 | Loss: 0.00001042
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001039
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001037
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001036
Iteration 68/1000 | Loss: 0.00001036
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001035
Iteration 72/1000 | Loss: 0.00001034
Iteration 73/1000 | Loss: 0.00001034
Iteration 74/1000 | Loss: 0.00001034
Iteration 75/1000 | Loss: 0.00001033
Iteration 76/1000 | Loss: 0.00001033
Iteration 77/1000 | Loss: 0.00001033
Iteration 78/1000 | Loss: 0.00001032
Iteration 79/1000 | Loss: 0.00001032
Iteration 80/1000 | Loss: 0.00001032
Iteration 81/1000 | Loss: 0.00001032
Iteration 82/1000 | Loss: 0.00001032
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001031
Iteration 85/1000 | Loss: 0.00001031
Iteration 86/1000 | Loss: 0.00001031
Iteration 87/1000 | Loss: 0.00001031
Iteration 88/1000 | Loss: 0.00001031
Iteration 89/1000 | Loss: 0.00001031
Iteration 90/1000 | Loss: 0.00001031
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001030
Iteration 93/1000 | Loss: 0.00001030
Iteration 94/1000 | Loss: 0.00001030
Iteration 95/1000 | Loss: 0.00001030
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001029
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001028
Iteration 101/1000 | Loss: 0.00001028
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001027
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001027
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001026
Iteration 115/1000 | Loss: 0.00001026
Iteration 116/1000 | Loss: 0.00001026
Iteration 117/1000 | Loss: 0.00001025
Iteration 118/1000 | Loss: 0.00001025
Iteration 119/1000 | Loss: 0.00001025
Iteration 120/1000 | Loss: 0.00001025
Iteration 121/1000 | Loss: 0.00001024
Iteration 122/1000 | Loss: 0.00001024
Iteration 123/1000 | Loss: 0.00001024
Iteration 124/1000 | Loss: 0.00001024
Iteration 125/1000 | Loss: 0.00001023
Iteration 126/1000 | Loss: 0.00001023
Iteration 127/1000 | Loss: 0.00001023
Iteration 128/1000 | Loss: 0.00001023
Iteration 129/1000 | Loss: 0.00001022
Iteration 130/1000 | Loss: 0.00001022
Iteration 131/1000 | Loss: 0.00001022
Iteration 132/1000 | Loss: 0.00001022
Iteration 133/1000 | Loss: 0.00001022
Iteration 134/1000 | Loss: 0.00001022
Iteration 135/1000 | Loss: 0.00001022
Iteration 136/1000 | Loss: 0.00001021
Iteration 137/1000 | Loss: 0.00001021
Iteration 138/1000 | Loss: 0.00001021
Iteration 139/1000 | Loss: 0.00001021
Iteration 140/1000 | Loss: 0.00001021
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001020
Iteration 144/1000 | Loss: 0.00001020
Iteration 145/1000 | Loss: 0.00001020
Iteration 146/1000 | Loss: 0.00001020
Iteration 147/1000 | Loss: 0.00001020
Iteration 148/1000 | Loss: 0.00001020
Iteration 149/1000 | Loss: 0.00001020
Iteration 150/1000 | Loss: 0.00001020
Iteration 151/1000 | Loss: 0.00001020
Iteration 152/1000 | Loss: 0.00001020
Iteration 153/1000 | Loss: 0.00001020
Iteration 154/1000 | Loss: 0.00001020
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001018
Iteration 163/1000 | Loss: 0.00001018
Iteration 164/1000 | Loss: 0.00001018
Iteration 165/1000 | Loss: 0.00001018
Iteration 166/1000 | Loss: 0.00001018
Iteration 167/1000 | Loss: 0.00001018
Iteration 168/1000 | Loss: 0.00001018
Iteration 169/1000 | Loss: 0.00001017
Iteration 170/1000 | Loss: 0.00001017
Iteration 171/1000 | Loss: 0.00001017
Iteration 172/1000 | Loss: 0.00001017
Iteration 173/1000 | Loss: 0.00001017
Iteration 174/1000 | Loss: 0.00001017
Iteration 175/1000 | Loss: 0.00001017
Iteration 176/1000 | Loss: 0.00001017
Iteration 177/1000 | Loss: 0.00001017
Iteration 178/1000 | Loss: 0.00001017
Iteration 179/1000 | Loss: 0.00001017
Iteration 180/1000 | Loss: 0.00001017
Iteration 181/1000 | Loss: 0.00001017
Iteration 182/1000 | Loss: 0.00001017
Iteration 183/1000 | Loss: 0.00001016
Iteration 184/1000 | Loss: 0.00001016
Iteration 185/1000 | Loss: 0.00001016
Iteration 186/1000 | Loss: 0.00001016
Iteration 187/1000 | Loss: 0.00001016
Iteration 188/1000 | Loss: 0.00001016
Iteration 189/1000 | Loss: 0.00001016
Iteration 190/1000 | Loss: 0.00001016
Iteration 191/1000 | Loss: 0.00001016
Iteration 192/1000 | Loss: 0.00001016
Iteration 193/1000 | Loss: 0.00001016
Iteration 194/1000 | Loss: 0.00001016
Iteration 195/1000 | Loss: 0.00001015
Iteration 196/1000 | Loss: 0.00001015
Iteration 197/1000 | Loss: 0.00001015
Iteration 198/1000 | Loss: 0.00001015
Iteration 199/1000 | Loss: 0.00001015
Iteration 200/1000 | Loss: 0.00001015
Iteration 201/1000 | Loss: 0.00001015
Iteration 202/1000 | Loss: 0.00001015
Iteration 203/1000 | Loss: 0.00001015
Iteration 204/1000 | Loss: 0.00001015
Iteration 205/1000 | Loss: 0.00001015
Iteration 206/1000 | Loss: 0.00001015
Iteration 207/1000 | Loss: 0.00001015
Iteration 208/1000 | Loss: 0.00001015
Iteration 209/1000 | Loss: 0.00001015
Iteration 210/1000 | Loss: 0.00001015
Iteration 211/1000 | Loss: 0.00001015
Iteration 212/1000 | Loss: 0.00001015
Iteration 213/1000 | Loss: 0.00001015
Iteration 214/1000 | Loss: 0.00001015
Iteration 215/1000 | Loss: 0.00001015
Iteration 216/1000 | Loss: 0.00001015
Iteration 217/1000 | Loss: 0.00001015
Iteration 218/1000 | Loss: 0.00001015
Iteration 219/1000 | Loss: 0.00001015
Iteration 220/1000 | Loss: 0.00001014
Iteration 221/1000 | Loss: 0.00001014
Iteration 222/1000 | Loss: 0.00001014
Iteration 223/1000 | Loss: 0.00001014
Iteration 224/1000 | Loss: 0.00001014
Iteration 225/1000 | Loss: 0.00001014
Iteration 226/1000 | Loss: 0.00001014
Iteration 227/1000 | Loss: 0.00001014
Iteration 228/1000 | Loss: 0.00001014
Iteration 229/1000 | Loss: 0.00001014
Iteration 230/1000 | Loss: 0.00001014
Iteration 231/1000 | Loss: 0.00001014
Iteration 232/1000 | Loss: 0.00001014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.0144365660380572e-05, 1.0144365660380572e-05, 1.0144365660380572e-05, 1.0144365660380572e-05, 1.0144365660380572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0144365660380572e-05

Optimization complete. Final v2v error: 2.7096564769744873 mm

Highest mean error: 3.5297341346740723 mm for frame 61

Lowest mean error: 2.438141345977783 mm for frame 134

Saving results

Total time: 43.84326958656311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462766
Iteration 2/25 | Loss: 0.00124727
Iteration 3/25 | Loss: 0.00112132
Iteration 4/25 | Loss: 0.00110827
Iteration 5/25 | Loss: 0.00110540
Iteration 6/25 | Loss: 0.00110495
Iteration 7/25 | Loss: 0.00110495
Iteration 8/25 | Loss: 0.00110495
Iteration 9/25 | Loss: 0.00110495
Iteration 10/25 | Loss: 0.00110495
Iteration 11/25 | Loss: 0.00110495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011049527674913406, 0.0011049527674913406, 0.0011049527674913406, 0.0011049527674913406, 0.0011049527674913406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011049527674913406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.12366724
Iteration 2/25 | Loss: 0.00067763
Iteration 3/25 | Loss: 0.00067762
Iteration 4/25 | Loss: 0.00067762
Iteration 5/25 | Loss: 0.00067762
Iteration 6/25 | Loss: 0.00067762
Iteration 7/25 | Loss: 0.00067762
Iteration 8/25 | Loss: 0.00067762
Iteration 9/25 | Loss: 0.00067762
Iteration 10/25 | Loss: 0.00067762
Iteration 11/25 | Loss: 0.00067762
Iteration 12/25 | Loss: 0.00067762
Iteration 13/25 | Loss: 0.00067762
Iteration 14/25 | Loss: 0.00067762
Iteration 15/25 | Loss: 0.00067762
Iteration 16/25 | Loss: 0.00067762
Iteration 17/25 | Loss: 0.00067762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000677616277243942, 0.000677616277243942, 0.000677616277243942, 0.000677616277243942, 0.000677616277243942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000677616277243942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067762
Iteration 2/1000 | Loss: 0.00003184
Iteration 3/1000 | Loss: 0.00002017
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001331
Iteration 8/1000 | Loss: 0.00001302
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001198
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001188
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001186
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001186
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001186
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.1861530765600037e-05, 1.1861530765600037e-05, 1.1861530765600037e-05, 1.1861530765600037e-05, 1.1861530765600037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1861530765600037e-05

Optimization complete. Final v2v error: 2.9522159099578857 mm

Highest mean error: 3.214012861251831 mm for frame 108

Lowest mean error: 2.7324912548065186 mm for frame 136

Saving results

Total time: 31.697299480438232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372873
Iteration 2/25 | Loss: 0.00117652
Iteration 3/25 | Loss: 0.00109465
Iteration 4/25 | Loss: 0.00108686
Iteration 5/25 | Loss: 0.00108264
Iteration 6/25 | Loss: 0.00108264
Iteration 7/25 | Loss: 0.00108264
Iteration 8/25 | Loss: 0.00108264
Iteration 9/25 | Loss: 0.00108264
Iteration 10/25 | Loss: 0.00108264
Iteration 11/25 | Loss: 0.00108264
Iteration 12/25 | Loss: 0.00108264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010826400248333812, 0.0010826400248333812, 0.0010826400248333812, 0.0010826400248333812, 0.0010826400248333812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010826400248333812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50043929
Iteration 2/25 | Loss: 0.00077311
Iteration 3/25 | Loss: 0.00077310
Iteration 4/25 | Loss: 0.00077310
Iteration 5/25 | Loss: 0.00077310
Iteration 6/25 | Loss: 0.00077310
Iteration 7/25 | Loss: 0.00077310
Iteration 8/25 | Loss: 0.00077310
Iteration 9/25 | Loss: 0.00077310
Iteration 10/25 | Loss: 0.00077310
Iteration 11/25 | Loss: 0.00077310
Iteration 12/25 | Loss: 0.00077310
Iteration 13/25 | Loss: 0.00077310
Iteration 14/25 | Loss: 0.00077310
Iteration 15/25 | Loss: 0.00077310
Iteration 16/25 | Loss: 0.00077310
Iteration 17/25 | Loss: 0.00077310
Iteration 18/25 | Loss: 0.00077310
Iteration 19/25 | Loss: 0.00077310
Iteration 20/25 | Loss: 0.00077310
Iteration 21/25 | Loss: 0.00077310
Iteration 22/25 | Loss: 0.00077310
Iteration 23/25 | Loss: 0.00077310
Iteration 24/25 | Loss: 0.00077310
Iteration 25/25 | Loss: 0.00077310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007730959332548082, 0.0007730959332548082, 0.0007730959332548082, 0.0007730959332548082, 0.0007730959332548082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007730959332548082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077310
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00001444
Iteration 4/1000 | Loss: 0.00001303
Iteration 5/1000 | Loss: 0.00001203
Iteration 6/1000 | Loss: 0.00001142
Iteration 7/1000 | Loss: 0.00001114
Iteration 8/1000 | Loss: 0.00001093
Iteration 9/1000 | Loss: 0.00001066
Iteration 10/1000 | Loss: 0.00001051
Iteration 11/1000 | Loss: 0.00001039
Iteration 12/1000 | Loss: 0.00001037
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001006
Iteration 26/1000 | Loss: 0.00001004
Iteration 27/1000 | Loss: 0.00001004
Iteration 28/1000 | Loss: 0.00001003
Iteration 29/1000 | Loss: 0.00001003
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001002
Iteration 32/1000 | Loss: 0.00001002
Iteration 33/1000 | Loss: 0.00001001
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000999
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000994
Iteration 56/1000 | Loss: 0.00000994
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000994
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000993
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000993
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000991
Iteration 75/1000 | Loss: 0.00000991
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000991
Iteration 81/1000 | Loss: 0.00000991
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000991
Iteration 86/1000 | Loss: 0.00000991
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000991
Iteration 89/1000 | Loss: 0.00000991
Iteration 90/1000 | Loss: 0.00000991
Iteration 91/1000 | Loss: 0.00000991
Iteration 92/1000 | Loss: 0.00000991
Iteration 93/1000 | Loss: 0.00000991
Iteration 94/1000 | Loss: 0.00000991
Iteration 95/1000 | Loss: 0.00000991
Iteration 96/1000 | Loss: 0.00000991
Iteration 97/1000 | Loss: 0.00000991
Iteration 98/1000 | Loss: 0.00000991
Iteration 99/1000 | Loss: 0.00000991
Iteration 100/1000 | Loss: 0.00000991
Iteration 101/1000 | Loss: 0.00000991
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [9.913523172144778e-06, 9.913523172144778e-06, 9.913523172144778e-06, 9.913523172144778e-06, 9.913523172144778e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.913523172144778e-06

Optimization complete. Final v2v error: 2.7052714824676514 mm

Highest mean error: 3.1573429107666016 mm for frame 193

Lowest mean error: 2.354707956314087 mm for frame 100

Saving results

Total time: 35.63436818122864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997925
Iteration 2/25 | Loss: 0.00157336
Iteration 3/25 | Loss: 0.00127135
Iteration 4/25 | Loss: 0.00124595
Iteration 5/25 | Loss: 0.00123794
Iteration 6/25 | Loss: 0.00123578
Iteration 7/25 | Loss: 0.00123578
Iteration 8/25 | Loss: 0.00123578
Iteration 9/25 | Loss: 0.00123578
Iteration 10/25 | Loss: 0.00123578
Iteration 11/25 | Loss: 0.00123578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012357820523902774, 0.0012357820523902774, 0.0012357820523902774, 0.0012357820523902774, 0.0012357820523902774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012357820523902774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02169478
Iteration 2/25 | Loss: 0.00108077
Iteration 3/25 | Loss: 0.00108077
Iteration 4/25 | Loss: 0.00108077
Iteration 5/25 | Loss: 0.00108077
Iteration 6/25 | Loss: 0.00108077
Iteration 7/25 | Loss: 0.00108077
Iteration 8/25 | Loss: 0.00108077
Iteration 9/25 | Loss: 0.00108077
Iteration 10/25 | Loss: 0.00108077
Iteration 11/25 | Loss: 0.00108077
Iteration 12/25 | Loss: 0.00108077
Iteration 13/25 | Loss: 0.00108077
Iteration 14/25 | Loss: 0.00108077
Iteration 15/25 | Loss: 0.00108077
Iteration 16/25 | Loss: 0.00108077
Iteration 17/25 | Loss: 0.00108077
Iteration 18/25 | Loss: 0.00108077
Iteration 19/25 | Loss: 0.00108077
Iteration 20/25 | Loss: 0.00108077
Iteration 21/25 | Loss: 0.00108077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010807706275954843, 0.0010807706275954843, 0.0010807706275954843, 0.0010807706275954843, 0.0010807706275954843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010807706275954843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108077
Iteration 2/1000 | Loss: 0.00006920
Iteration 3/1000 | Loss: 0.00003728
Iteration 4/1000 | Loss: 0.00002861
Iteration 5/1000 | Loss: 0.00002705
Iteration 6/1000 | Loss: 0.00002592
Iteration 7/1000 | Loss: 0.00002526
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002421
Iteration 10/1000 | Loss: 0.00002388
Iteration 11/1000 | Loss: 0.00002362
Iteration 12/1000 | Loss: 0.00002340
Iteration 13/1000 | Loss: 0.00002323
Iteration 14/1000 | Loss: 0.00002308
Iteration 15/1000 | Loss: 0.00002303
Iteration 16/1000 | Loss: 0.00002301
Iteration 17/1000 | Loss: 0.00002297
Iteration 18/1000 | Loss: 0.00002293
Iteration 19/1000 | Loss: 0.00002292
Iteration 20/1000 | Loss: 0.00002292
Iteration 21/1000 | Loss: 0.00002291
Iteration 22/1000 | Loss: 0.00002290
Iteration 23/1000 | Loss: 0.00002290
Iteration 24/1000 | Loss: 0.00002290
Iteration 25/1000 | Loss: 0.00002290
Iteration 26/1000 | Loss: 0.00002290
Iteration 27/1000 | Loss: 0.00002288
Iteration 28/1000 | Loss: 0.00002287
Iteration 29/1000 | Loss: 0.00002287
Iteration 30/1000 | Loss: 0.00002286
Iteration 31/1000 | Loss: 0.00002286
Iteration 32/1000 | Loss: 0.00002285
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002284
Iteration 36/1000 | Loss: 0.00002283
Iteration 37/1000 | Loss: 0.00002280
Iteration 38/1000 | Loss: 0.00002277
Iteration 39/1000 | Loss: 0.00002271
Iteration 40/1000 | Loss: 0.00002271
Iteration 41/1000 | Loss: 0.00002271
Iteration 42/1000 | Loss: 0.00002270
Iteration 43/1000 | Loss: 0.00002269
Iteration 44/1000 | Loss: 0.00002269
Iteration 45/1000 | Loss: 0.00002266
Iteration 46/1000 | Loss: 0.00002266
Iteration 47/1000 | Loss: 0.00002266
Iteration 48/1000 | Loss: 0.00002266
Iteration 49/1000 | Loss: 0.00002265
Iteration 50/1000 | Loss: 0.00002264
Iteration 51/1000 | Loss: 0.00002264
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002263
Iteration 54/1000 | Loss: 0.00002263
Iteration 55/1000 | Loss: 0.00002262
Iteration 56/1000 | Loss: 0.00002262
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002258
Iteration 59/1000 | Loss: 0.00002257
Iteration 60/1000 | Loss: 0.00002257
Iteration 61/1000 | Loss: 0.00002256
Iteration 62/1000 | Loss: 0.00002255
Iteration 63/1000 | Loss: 0.00002255
Iteration 64/1000 | Loss: 0.00002255
Iteration 65/1000 | Loss: 0.00002255
Iteration 66/1000 | Loss: 0.00002255
Iteration 67/1000 | Loss: 0.00002255
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002255
Iteration 72/1000 | Loss: 0.00002254
Iteration 73/1000 | Loss: 0.00002254
Iteration 74/1000 | Loss: 0.00002254
Iteration 75/1000 | Loss: 0.00002252
Iteration 76/1000 | Loss: 0.00002252
Iteration 77/1000 | Loss: 0.00002251
Iteration 78/1000 | Loss: 0.00002251
Iteration 79/1000 | Loss: 0.00002251
Iteration 80/1000 | Loss: 0.00002251
Iteration 81/1000 | Loss: 0.00002250
Iteration 82/1000 | Loss: 0.00002250
Iteration 83/1000 | Loss: 0.00002249
Iteration 84/1000 | Loss: 0.00002249
Iteration 85/1000 | Loss: 0.00002249
Iteration 86/1000 | Loss: 0.00002249
Iteration 87/1000 | Loss: 0.00002249
Iteration 88/1000 | Loss: 0.00002249
Iteration 89/1000 | Loss: 0.00002248
Iteration 90/1000 | Loss: 0.00002248
Iteration 91/1000 | Loss: 0.00002248
Iteration 92/1000 | Loss: 0.00002248
Iteration 93/1000 | Loss: 0.00002248
Iteration 94/1000 | Loss: 0.00002248
Iteration 95/1000 | Loss: 0.00002248
Iteration 96/1000 | Loss: 0.00002248
Iteration 97/1000 | Loss: 0.00002248
Iteration 98/1000 | Loss: 0.00002248
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00002247
Iteration 101/1000 | Loss: 0.00002247
Iteration 102/1000 | Loss: 0.00002247
Iteration 103/1000 | Loss: 0.00002247
Iteration 104/1000 | Loss: 0.00002247
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002246
Iteration 111/1000 | Loss: 0.00002246
Iteration 112/1000 | Loss: 0.00002245
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002245
Iteration 117/1000 | Loss: 0.00002245
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002245
Iteration 120/1000 | Loss: 0.00002245
Iteration 121/1000 | Loss: 0.00002245
Iteration 122/1000 | Loss: 0.00002244
Iteration 123/1000 | Loss: 0.00002244
Iteration 124/1000 | Loss: 0.00002244
Iteration 125/1000 | Loss: 0.00002244
Iteration 126/1000 | Loss: 0.00002244
Iteration 127/1000 | Loss: 0.00002244
Iteration 128/1000 | Loss: 0.00002244
Iteration 129/1000 | Loss: 0.00002244
Iteration 130/1000 | Loss: 0.00002244
Iteration 131/1000 | Loss: 0.00002243
Iteration 132/1000 | Loss: 0.00002243
Iteration 133/1000 | Loss: 0.00002243
Iteration 134/1000 | Loss: 0.00002243
Iteration 135/1000 | Loss: 0.00002243
Iteration 136/1000 | Loss: 0.00002243
Iteration 137/1000 | Loss: 0.00002242
Iteration 138/1000 | Loss: 0.00002242
Iteration 139/1000 | Loss: 0.00002242
Iteration 140/1000 | Loss: 0.00002242
Iteration 141/1000 | Loss: 0.00002242
Iteration 142/1000 | Loss: 0.00002241
Iteration 143/1000 | Loss: 0.00002241
Iteration 144/1000 | Loss: 0.00002241
Iteration 145/1000 | Loss: 0.00002241
Iteration 146/1000 | Loss: 0.00002241
Iteration 147/1000 | Loss: 0.00002241
Iteration 148/1000 | Loss: 0.00002240
Iteration 149/1000 | Loss: 0.00002240
Iteration 150/1000 | Loss: 0.00002240
Iteration 151/1000 | Loss: 0.00002240
Iteration 152/1000 | Loss: 0.00002240
Iteration 153/1000 | Loss: 0.00002240
Iteration 154/1000 | Loss: 0.00002240
Iteration 155/1000 | Loss: 0.00002240
Iteration 156/1000 | Loss: 0.00002240
Iteration 157/1000 | Loss: 0.00002240
Iteration 158/1000 | Loss: 0.00002240
Iteration 159/1000 | Loss: 0.00002240
Iteration 160/1000 | Loss: 0.00002240
Iteration 161/1000 | Loss: 0.00002239
Iteration 162/1000 | Loss: 0.00002239
Iteration 163/1000 | Loss: 0.00002239
Iteration 164/1000 | Loss: 0.00002239
Iteration 165/1000 | Loss: 0.00002239
Iteration 166/1000 | Loss: 0.00002239
Iteration 167/1000 | Loss: 0.00002239
Iteration 168/1000 | Loss: 0.00002239
Iteration 169/1000 | Loss: 0.00002239
Iteration 170/1000 | Loss: 0.00002239
Iteration 171/1000 | Loss: 0.00002239
Iteration 172/1000 | Loss: 0.00002239
Iteration 173/1000 | Loss: 0.00002239
Iteration 174/1000 | Loss: 0.00002239
Iteration 175/1000 | Loss: 0.00002239
Iteration 176/1000 | Loss: 0.00002239
Iteration 177/1000 | Loss: 0.00002239
Iteration 178/1000 | Loss: 0.00002239
Iteration 179/1000 | Loss: 0.00002239
Iteration 180/1000 | Loss: 0.00002239
Iteration 181/1000 | Loss: 0.00002239
Iteration 182/1000 | Loss: 0.00002239
Iteration 183/1000 | Loss: 0.00002239
Iteration 184/1000 | Loss: 0.00002239
Iteration 185/1000 | Loss: 0.00002239
Iteration 186/1000 | Loss: 0.00002239
Iteration 187/1000 | Loss: 0.00002239
Iteration 188/1000 | Loss: 0.00002239
Iteration 189/1000 | Loss: 0.00002239
Iteration 190/1000 | Loss: 0.00002239
Iteration 191/1000 | Loss: 0.00002239
Iteration 192/1000 | Loss: 0.00002239
Iteration 193/1000 | Loss: 0.00002239
Iteration 194/1000 | Loss: 0.00002239
Iteration 195/1000 | Loss: 0.00002239
Iteration 196/1000 | Loss: 0.00002239
Iteration 197/1000 | Loss: 0.00002239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.2387785065802746e-05, 2.2387785065802746e-05, 2.2387785065802746e-05, 2.2387785065802746e-05, 2.2387785065802746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2387785065802746e-05

Optimization complete. Final v2v error: 3.89253306388855 mm

Highest mean error: 4.791755676269531 mm for frame 76

Lowest mean error: 3.247403860092163 mm for frame 7

Saving results

Total time: 47.1073112487793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432112
Iteration 2/25 | Loss: 0.00139874
Iteration 3/25 | Loss: 0.00118388
Iteration 4/25 | Loss: 0.00116392
Iteration 5/25 | Loss: 0.00116184
Iteration 6/25 | Loss: 0.00116184
Iteration 7/25 | Loss: 0.00116184
Iteration 8/25 | Loss: 0.00116184
Iteration 9/25 | Loss: 0.00116184
Iteration 10/25 | Loss: 0.00116184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011618437711149454, 0.0011618437711149454, 0.0011618437711149454, 0.0011618437711149454, 0.0011618437711149454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011618437711149454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32683289
Iteration 2/25 | Loss: 0.00081187
Iteration 3/25 | Loss: 0.00081186
Iteration 4/25 | Loss: 0.00081186
Iteration 5/25 | Loss: 0.00081186
Iteration 6/25 | Loss: 0.00081186
Iteration 7/25 | Loss: 0.00081186
Iteration 8/25 | Loss: 0.00081186
Iteration 9/25 | Loss: 0.00081186
Iteration 10/25 | Loss: 0.00081186
Iteration 11/25 | Loss: 0.00081186
Iteration 12/25 | Loss: 0.00081186
Iteration 13/25 | Loss: 0.00081186
Iteration 14/25 | Loss: 0.00081186
Iteration 15/25 | Loss: 0.00081186
Iteration 16/25 | Loss: 0.00081186
Iteration 17/25 | Loss: 0.00081186
Iteration 18/25 | Loss: 0.00081186
Iteration 19/25 | Loss: 0.00081186
Iteration 20/25 | Loss: 0.00081186
Iteration 21/25 | Loss: 0.00081186
Iteration 22/25 | Loss: 0.00081186
Iteration 23/25 | Loss: 0.00081186
Iteration 24/25 | Loss: 0.00081186
Iteration 25/25 | Loss: 0.00081186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081186
Iteration 2/1000 | Loss: 0.00003163
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001293
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001232
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001216
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001191
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001186
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001179
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001176
Iteration 139/1000 | Loss: 0.00001176
Iteration 140/1000 | Loss: 0.00001176
Iteration 141/1000 | Loss: 0.00001176
Iteration 142/1000 | Loss: 0.00001176
Iteration 143/1000 | Loss: 0.00001176
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001175
Iteration 147/1000 | Loss: 0.00001175
Iteration 148/1000 | Loss: 0.00001175
Iteration 149/1000 | Loss: 0.00001175
Iteration 150/1000 | Loss: 0.00001175
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001175
Iteration 157/1000 | Loss: 0.00001174
Iteration 158/1000 | Loss: 0.00001174
Iteration 159/1000 | Loss: 0.00001174
Iteration 160/1000 | Loss: 0.00001174
Iteration 161/1000 | Loss: 0.00001174
Iteration 162/1000 | Loss: 0.00001174
Iteration 163/1000 | Loss: 0.00001174
Iteration 164/1000 | Loss: 0.00001174
Iteration 165/1000 | Loss: 0.00001174
Iteration 166/1000 | Loss: 0.00001174
Iteration 167/1000 | Loss: 0.00001174
Iteration 168/1000 | Loss: 0.00001174
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001173
Iteration 172/1000 | Loss: 0.00001173
Iteration 173/1000 | Loss: 0.00001173
Iteration 174/1000 | Loss: 0.00001173
Iteration 175/1000 | Loss: 0.00001173
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Iteration 180/1000 | Loss: 0.00001172
Iteration 181/1000 | Loss: 0.00001172
Iteration 182/1000 | Loss: 0.00001172
Iteration 183/1000 | Loss: 0.00001172
Iteration 184/1000 | Loss: 0.00001172
Iteration 185/1000 | Loss: 0.00001172
Iteration 186/1000 | Loss: 0.00001172
Iteration 187/1000 | Loss: 0.00001172
Iteration 188/1000 | Loss: 0.00001172
Iteration 189/1000 | Loss: 0.00001172
Iteration 190/1000 | Loss: 0.00001172
Iteration 191/1000 | Loss: 0.00001172
Iteration 192/1000 | Loss: 0.00001172
Iteration 193/1000 | Loss: 0.00001172
Iteration 194/1000 | Loss: 0.00001172
Iteration 195/1000 | Loss: 0.00001171
Iteration 196/1000 | Loss: 0.00001171
Iteration 197/1000 | Loss: 0.00001171
Iteration 198/1000 | Loss: 0.00001171
Iteration 199/1000 | Loss: 0.00001171
Iteration 200/1000 | Loss: 0.00001171
Iteration 201/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.1714885658875573e-05, 1.1714885658875573e-05, 1.1714885658875573e-05, 1.1714885658875573e-05, 1.1714885658875573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1714885658875573e-05

Optimization complete. Final v2v error: 2.893157958984375 mm

Highest mean error: 4.113466739654541 mm for frame 235

Lowest mean error: 2.5191476345062256 mm for frame 201

Saving results

Total time: 47.37306523323059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005752
Iteration 2/25 | Loss: 0.00259140
Iteration 3/25 | Loss: 0.00149644
Iteration 4/25 | Loss: 0.00136771
Iteration 5/25 | Loss: 0.00118923
Iteration 6/25 | Loss: 0.00116798
Iteration 7/25 | Loss: 0.00116456
Iteration 8/25 | Loss: 0.00116676
Iteration 9/25 | Loss: 0.00116357
Iteration 10/25 | Loss: 0.00115687
Iteration 11/25 | Loss: 0.00115759
Iteration 12/25 | Loss: 0.00115205
Iteration 13/25 | Loss: 0.00115217
Iteration 14/25 | Loss: 0.00114708
Iteration 15/25 | Loss: 0.00114737
Iteration 16/25 | Loss: 0.00114171
Iteration 17/25 | Loss: 0.00114138
Iteration 18/25 | Loss: 0.00114840
Iteration 19/25 | Loss: 0.00113538
Iteration 20/25 | Loss: 0.00113137
Iteration 21/25 | Loss: 0.00112636
Iteration 22/25 | Loss: 0.00112314
Iteration 23/25 | Loss: 0.00112338
Iteration 24/25 | Loss: 0.00112247
Iteration 25/25 | Loss: 0.00112226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30827844
Iteration 2/25 | Loss: 0.00121127
Iteration 3/25 | Loss: 0.00121112
Iteration 4/25 | Loss: 0.00121112
Iteration 5/25 | Loss: 0.00121112
Iteration 6/25 | Loss: 0.00121112
Iteration 7/25 | Loss: 0.00121112
Iteration 8/25 | Loss: 0.00121112
Iteration 9/25 | Loss: 0.00121112
Iteration 10/25 | Loss: 0.00121112
Iteration 11/25 | Loss: 0.00121112
Iteration 12/25 | Loss: 0.00121112
Iteration 13/25 | Loss: 0.00121112
Iteration 14/25 | Loss: 0.00121112
Iteration 15/25 | Loss: 0.00121112
Iteration 16/25 | Loss: 0.00121112
Iteration 17/25 | Loss: 0.00121112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012111201649531722, 0.0012111201649531722, 0.0012111201649531722, 0.0012111201649531722, 0.0012111201649531722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012111201649531722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121112
Iteration 2/1000 | Loss: 0.00006810
Iteration 3/1000 | Loss: 0.00004635
Iteration 4/1000 | Loss: 0.00013782
Iteration 5/1000 | Loss: 0.00020503
Iteration 6/1000 | Loss: 0.00069639
Iteration 7/1000 | Loss: 0.00029053
Iteration 8/1000 | Loss: 0.00028363
Iteration 9/1000 | Loss: 0.00053298
Iteration 10/1000 | Loss: 0.00107995
Iteration 11/1000 | Loss: 0.00011428
Iteration 12/1000 | Loss: 0.00040040
Iteration 13/1000 | Loss: 0.00030804
Iteration 14/1000 | Loss: 0.00003591
Iteration 15/1000 | Loss: 0.00085968
Iteration 16/1000 | Loss: 0.00019127
Iteration 17/1000 | Loss: 0.00249824
Iteration 18/1000 | Loss: 0.00015419
Iteration 19/1000 | Loss: 0.00014639
Iteration 20/1000 | Loss: 0.00003646
Iteration 21/1000 | Loss: 0.00003058
Iteration 22/1000 | Loss: 0.00002675
Iteration 23/1000 | Loss: 0.00002369
Iteration 24/1000 | Loss: 0.00007525
Iteration 25/1000 | Loss: 0.00003303
Iteration 26/1000 | Loss: 0.00002879
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00002858
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001605
Iteration 31/1000 | Loss: 0.00006141
Iteration 32/1000 | Loss: 0.00002079
Iteration 33/1000 | Loss: 0.00002354
Iteration 34/1000 | Loss: 0.00001534
Iteration 35/1000 | Loss: 0.00002775
Iteration 36/1000 | Loss: 0.00001874
Iteration 37/1000 | Loss: 0.00002804
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001662
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001476
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001475
Iteration 48/1000 | Loss: 0.00001475
Iteration 49/1000 | Loss: 0.00001475
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001474
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001467
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001467
Iteration 69/1000 | Loss: 0.00001467
Iteration 70/1000 | Loss: 0.00001467
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001466
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001466
Iteration 80/1000 | Loss: 0.00001466
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001466
Iteration 83/1000 | Loss: 0.00001464
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001464
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001460
Iteration 90/1000 | Loss: 0.00001460
Iteration 91/1000 | Loss: 0.00001460
Iteration 92/1000 | Loss: 0.00001460
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00001459
Iteration 95/1000 | Loss: 0.00001459
Iteration 96/1000 | Loss: 0.00001459
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001455
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001453
Iteration 115/1000 | Loss: 0.00001453
Iteration 116/1000 | Loss: 0.00001452
Iteration 117/1000 | Loss: 0.00001452
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001450
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Iteration 139/1000 | Loss: 0.00001447
Iteration 140/1000 | Loss: 0.00001447
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00002731
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001443
Iteration 145/1000 | Loss: 0.00001443
Iteration 146/1000 | Loss: 0.00001443
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001442
Iteration 152/1000 | Loss: 0.00001442
Iteration 153/1000 | Loss: 0.00001442
Iteration 154/1000 | Loss: 0.00001442
Iteration 155/1000 | Loss: 0.00001442
Iteration 156/1000 | Loss: 0.00001442
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001442
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4422077583731152e-05, 1.4422077583731152e-05, 1.4422077583731152e-05, 1.4422077583731152e-05, 1.4422077583731152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4422077583731152e-05

Optimization complete. Final v2v error: 3.2298314571380615 mm

Highest mean error: 4.709029674530029 mm for frame 218

Lowest mean error: 2.816749095916748 mm for frame 176

Saving results

Total time: 122.8894567489624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026124
Iteration 2/25 | Loss: 0.00251302
Iteration 3/25 | Loss: 0.00190405
Iteration 4/25 | Loss: 0.00180067
Iteration 5/25 | Loss: 0.00192974
Iteration 6/25 | Loss: 0.00194060
Iteration 7/25 | Loss: 0.00186086
Iteration 8/25 | Loss: 0.00178235
Iteration 9/25 | Loss: 0.00176536
Iteration 10/25 | Loss: 0.00170944
Iteration 11/25 | Loss: 0.00154579
Iteration 12/25 | Loss: 0.00136534
Iteration 13/25 | Loss: 0.00124688
Iteration 14/25 | Loss: 0.00119706
Iteration 15/25 | Loss: 0.00117704
Iteration 16/25 | Loss: 0.00116086
Iteration 17/25 | Loss: 0.00114421
Iteration 18/25 | Loss: 0.00112562
Iteration 19/25 | Loss: 0.00111990
Iteration 20/25 | Loss: 0.00111493
Iteration 21/25 | Loss: 0.00111690
Iteration 22/25 | Loss: 0.00111379
Iteration 23/25 | Loss: 0.00111338
Iteration 24/25 | Loss: 0.00111072
Iteration 25/25 | Loss: 0.00111249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58792436
Iteration 2/25 | Loss: 0.00068203
Iteration 3/25 | Loss: 0.00068202
Iteration 4/25 | Loss: 0.00068202
Iteration 5/25 | Loss: 0.00068202
Iteration 6/25 | Loss: 0.00068202
Iteration 7/25 | Loss: 0.00068202
Iteration 8/25 | Loss: 0.00068202
Iteration 9/25 | Loss: 0.00068202
Iteration 10/25 | Loss: 0.00068202
Iteration 11/25 | Loss: 0.00068202
Iteration 12/25 | Loss: 0.00068202
Iteration 13/25 | Loss: 0.00068202
Iteration 14/25 | Loss: 0.00068202
Iteration 15/25 | Loss: 0.00068202
Iteration 16/25 | Loss: 0.00068202
Iteration 17/25 | Loss: 0.00068202
Iteration 18/25 | Loss: 0.00068202
Iteration 19/25 | Loss: 0.00068202
Iteration 20/25 | Loss: 0.00068202
Iteration 21/25 | Loss: 0.00068202
Iteration 22/25 | Loss: 0.00068202
Iteration 23/25 | Loss: 0.00068202
Iteration 24/25 | Loss: 0.00068202
Iteration 25/25 | Loss: 0.00068202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068202
Iteration 2/1000 | Loss: 0.00003404
Iteration 3/1000 | Loss: 0.00002524
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00001946
Iteration 6/1000 | Loss: 0.00002439
Iteration 7/1000 | Loss: 0.00001758
Iteration 8/1000 | Loss: 0.00002342
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00002228
Iteration 16/1000 | Loss: 0.00015312
Iteration 17/1000 | Loss: 0.00001546
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00005528
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00002392
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001337
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00002131
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001311
Iteration 55/1000 | Loss: 0.00001311
Iteration 56/1000 | Loss: 0.00001311
Iteration 57/1000 | Loss: 0.00001311
Iteration 58/1000 | Loss: 0.00001311
Iteration 59/1000 | Loss: 0.00001311
Iteration 60/1000 | Loss: 0.00001311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.3110260624671355e-05, 1.3110260624671355e-05, 1.3110260624671355e-05, 1.3110260624671355e-05, 1.3110260624671355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3110260624671355e-05

Optimization complete. Final v2v error: 3.0836703777313232 mm

Highest mean error: 5.480568885803223 mm for frame 111

Lowest mean error: 2.929091215133667 mm for frame 202

Saving results

Total time: 98.09664177894592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900061
Iteration 2/25 | Loss: 0.00146505
Iteration 3/25 | Loss: 0.00126331
Iteration 4/25 | Loss: 0.00124821
Iteration 5/25 | Loss: 0.00124821
Iteration 6/25 | Loss: 0.00124821
Iteration 7/25 | Loss: 0.00124821
Iteration 8/25 | Loss: 0.00124821
Iteration 9/25 | Loss: 0.00124821
Iteration 10/25 | Loss: 0.00124821
Iteration 11/25 | Loss: 0.00124821
Iteration 12/25 | Loss: 0.00124821
Iteration 13/25 | Loss: 0.00124821
Iteration 14/25 | Loss: 0.00124821
Iteration 15/25 | Loss: 0.00124821
Iteration 16/25 | Loss: 0.00124821
Iteration 17/25 | Loss: 0.00124821
Iteration 18/25 | Loss: 0.00124821
Iteration 19/25 | Loss: 0.00124821
Iteration 20/25 | Loss: 0.00124821
Iteration 21/25 | Loss: 0.00124821
Iteration 22/25 | Loss: 0.00124821
Iteration 23/25 | Loss: 0.00124821
Iteration 24/25 | Loss: 0.00124821
Iteration 25/25 | Loss: 0.00124821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25220883
Iteration 2/25 | Loss: 0.00062908
Iteration 3/25 | Loss: 0.00062901
Iteration 4/25 | Loss: 0.00062901
Iteration 5/25 | Loss: 0.00062901
Iteration 6/25 | Loss: 0.00062901
Iteration 7/25 | Loss: 0.00062901
Iteration 8/25 | Loss: 0.00062901
Iteration 9/25 | Loss: 0.00062901
Iteration 10/25 | Loss: 0.00062901
Iteration 11/25 | Loss: 0.00062901
Iteration 12/25 | Loss: 0.00062901
Iteration 13/25 | Loss: 0.00062901
Iteration 14/25 | Loss: 0.00062901
Iteration 15/25 | Loss: 0.00062901
Iteration 16/25 | Loss: 0.00062901
Iteration 17/25 | Loss: 0.00062901
Iteration 18/25 | Loss: 0.00062901
Iteration 19/25 | Loss: 0.00062901
Iteration 20/25 | Loss: 0.00062901
Iteration 21/25 | Loss: 0.00062901
Iteration 22/25 | Loss: 0.00062901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006290050805546343, 0.0006290050805546343, 0.0006290050805546343, 0.0006290050805546343, 0.0006290050805546343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006290050805546343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062901
Iteration 2/1000 | Loss: 0.00003237
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00002079
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001942
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001877
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001828
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001786
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001771
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001742
Iteration 40/1000 | Loss: 0.00001742
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001741
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001738
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001737
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001736
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001728
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001728
Iteration 113/1000 | Loss: 0.00001728
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.727779999782797e-05, 1.727779999782797e-05, 1.727779999782797e-05, 1.727779999782797e-05, 1.727779999782797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.727779999782797e-05

Optimization complete. Final v2v error: 3.47910213470459 mm

Highest mean error: 3.645291328430176 mm for frame 35

Lowest mean error: 3.0511131286621094 mm for frame 239

Saving results

Total time: 37.97006845474243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419928
Iteration 2/25 | Loss: 0.00119303
Iteration 3/25 | Loss: 0.00110722
Iteration 4/25 | Loss: 0.00109216
Iteration 5/25 | Loss: 0.00108831
Iteration 6/25 | Loss: 0.00108780
Iteration 7/25 | Loss: 0.00108780
Iteration 8/25 | Loss: 0.00108780
Iteration 9/25 | Loss: 0.00108780
Iteration 10/25 | Loss: 0.00108780
Iteration 11/25 | Loss: 0.00108780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010878047905862331, 0.0010878047905862331, 0.0010878047905862331, 0.0010878047905862331, 0.0010878047905862331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010878047905862331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44631529
Iteration 2/25 | Loss: 0.00076959
Iteration 3/25 | Loss: 0.00076959
Iteration 4/25 | Loss: 0.00076958
Iteration 5/25 | Loss: 0.00076958
Iteration 6/25 | Loss: 0.00076958
Iteration 7/25 | Loss: 0.00076958
Iteration 8/25 | Loss: 0.00076958
Iteration 9/25 | Loss: 0.00076958
Iteration 10/25 | Loss: 0.00076958
Iteration 11/25 | Loss: 0.00076958
Iteration 12/25 | Loss: 0.00076958
Iteration 13/25 | Loss: 0.00076958
Iteration 14/25 | Loss: 0.00076958
Iteration 15/25 | Loss: 0.00076958
Iteration 16/25 | Loss: 0.00076958
Iteration 17/25 | Loss: 0.00076958
Iteration 18/25 | Loss: 0.00076958
Iteration 19/25 | Loss: 0.00076958
Iteration 20/25 | Loss: 0.00076958
Iteration 21/25 | Loss: 0.00076958
Iteration 22/25 | Loss: 0.00076958
Iteration 23/25 | Loss: 0.00076958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007695832755416632, 0.0007695832755416632, 0.0007695832755416632, 0.0007695832755416632, 0.0007695832755416632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007695832755416632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076958
Iteration 2/1000 | Loss: 0.00002767
Iteration 3/1000 | Loss: 0.00001735
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001362
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001320
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001279
Iteration 15/1000 | Loss: 0.00001279
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001240
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001239
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001236
Iteration 101/1000 | Loss: 0.00001236
Iteration 102/1000 | Loss: 0.00001236
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001228
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001228
Iteration 154/1000 | Loss: 0.00001228
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001227
Iteration 158/1000 | Loss: 0.00001227
Iteration 159/1000 | Loss: 0.00001227
Iteration 160/1000 | Loss: 0.00001227
Iteration 161/1000 | Loss: 0.00001227
Iteration 162/1000 | Loss: 0.00001227
Iteration 163/1000 | Loss: 0.00001227
Iteration 164/1000 | Loss: 0.00001227
Iteration 165/1000 | Loss: 0.00001227
Iteration 166/1000 | Loss: 0.00001227
Iteration 167/1000 | Loss: 0.00001227
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001227
Iteration 170/1000 | Loss: 0.00001227
Iteration 171/1000 | Loss: 0.00001227
Iteration 172/1000 | Loss: 0.00001227
Iteration 173/1000 | Loss: 0.00001227
Iteration 174/1000 | Loss: 0.00001227
Iteration 175/1000 | Loss: 0.00001227
Iteration 176/1000 | Loss: 0.00001227
Iteration 177/1000 | Loss: 0.00001227
Iteration 178/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.2272840649529826e-05, 1.2272840649529826e-05, 1.2272840649529826e-05, 1.2272840649529826e-05, 1.2272840649529826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2272840649529826e-05

Optimization complete. Final v2v error: 3.009012460708618 mm

Highest mean error: 3.5052599906921387 mm for frame 120

Lowest mean error: 2.8734703063964844 mm for frame 41

Saving results

Total time: 40.48299479484558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822353
Iteration 2/25 | Loss: 0.00119348
Iteration 3/25 | Loss: 0.00107324
Iteration 4/25 | Loss: 0.00105515
Iteration 5/25 | Loss: 0.00105319
Iteration 6/25 | Loss: 0.00105319
Iteration 7/25 | Loss: 0.00105319
Iteration 8/25 | Loss: 0.00105319
Iteration 9/25 | Loss: 0.00105319
Iteration 10/25 | Loss: 0.00105319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010531864827498794, 0.0010531864827498794, 0.0010531864827498794, 0.0010531864827498794, 0.0010531864827498794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010531864827498794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34956896
Iteration 2/25 | Loss: 0.00073832
Iteration 3/25 | Loss: 0.00073832
Iteration 4/25 | Loss: 0.00073832
Iteration 5/25 | Loss: 0.00073832
Iteration 6/25 | Loss: 0.00073832
Iteration 7/25 | Loss: 0.00073832
Iteration 8/25 | Loss: 0.00073832
Iteration 9/25 | Loss: 0.00073832
Iteration 10/25 | Loss: 0.00073831
Iteration 11/25 | Loss: 0.00073831
Iteration 12/25 | Loss: 0.00073831
Iteration 13/25 | Loss: 0.00073831
Iteration 14/25 | Loss: 0.00073831
Iteration 15/25 | Loss: 0.00073831
Iteration 16/25 | Loss: 0.00073831
Iteration 17/25 | Loss: 0.00073831
Iteration 18/25 | Loss: 0.00073831
Iteration 19/25 | Loss: 0.00073831
Iteration 20/25 | Loss: 0.00073831
Iteration 21/25 | Loss: 0.00073831
Iteration 22/25 | Loss: 0.00073831
Iteration 23/25 | Loss: 0.00073831
Iteration 24/25 | Loss: 0.00073831
Iteration 25/25 | Loss: 0.00073831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073831
Iteration 2/1000 | Loss: 0.00001747
Iteration 3/1000 | Loss: 0.00001271
Iteration 4/1000 | Loss: 0.00001161
Iteration 5/1000 | Loss: 0.00001080
Iteration 6/1000 | Loss: 0.00001025
Iteration 7/1000 | Loss: 0.00000998
Iteration 8/1000 | Loss: 0.00000977
Iteration 9/1000 | Loss: 0.00000950
Iteration 10/1000 | Loss: 0.00000939
Iteration 11/1000 | Loss: 0.00000939
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000922
Iteration 16/1000 | Loss: 0.00000920
Iteration 17/1000 | Loss: 0.00000920
Iteration 18/1000 | Loss: 0.00000920
Iteration 19/1000 | Loss: 0.00000919
Iteration 20/1000 | Loss: 0.00000919
Iteration 21/1000 | Loss: 0.00000919
Iteration 22/1000 | Loss: 0.00000919
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000918
Iteration 25/1000 | Loss: 0.00000917
Iteration 26/1000 | Loss: 0.00000916
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000916
Iteration 29/1000 | Loss: 0.00000916
Iteration 30/1000 | Loss: 0.00000916
Iteration 31/1000 | Loss: 0.00000916
Iteration 32/1000 | Loss: 0.00000916
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000916
Iteration 36/1000 | Loss: 0.00000916
Iteration 37/1000 | Loss: 0.00000916
Iteration 38/1000 | Loss: 0.00000916
Iteration 39/1000 | Loss: 0.00000916
Iteration 40/1000 | Loss: 0.00000916
Iteration 41/1000 | Loss: 0.00000916
Iteration 42/1000 | Loss: 0.00000916
Iteration 43/1000 | Loss: 0.00000916
Iteration 44/1000 | Loss: 0.00000916
Iteration 45/1000 | Loss: 0.00000916
Iteration 46/1000 | Loss: 0.00000916
Iteration 47/1000 | Loss: 0.00000916
Iteration 48/1000 | Loss: 0.00000916
Iteration 49/1000 | Loss: 0.00000916
Iteration 50/1000 | Loss: 0.00000916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [9.16301451070467e-06, 9.16301451070467e-06, 9.16301451070467e-06, 9.16301451070467e-06, 9.16301451070467e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.16301451070467e-06

Optimization complete. Final v2v error: 2.6230812072753906 mm

Highest mean error: 2.8480942249298096 mm for frame 66

Lowest mean error: 2.4605772495269775 mm for frame 241

Saving results

Total time: 28.54306435585022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532756
Iteration 2/25 | Loss: 0.00116921
Iteration 3/25 | Loss: 0.00109877
Iteration 4/25 | Loss: 0.00108869
Iteration 5/25 | Loss: 0.00108550
Iteration 6/25 | Loss: 0.00108480
Iteration 7/25 | Loss: 0.00108480
Iteration 8/25 | Loss: 0.00108480
Iteration 9/25 | Loss: 0.00108480
Iteration 10/25 | Loss: 0.00108480
Iteration 11/25 | Loss: 0.00108480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010847985977306962, 0.0010847985977306962, 0.0010847985977306962, 0.0010847985977306962, 0.0010847985977306962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010847985977306962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11963463
Iteration 2/25 | Loss: 0.00079257
Iteration 3/25 | Loss: 0.00079256
Iteration 4/25 | Loss: 0.00079256
Iteration 5/25 | Loss: 0.00079256
Iteration 6/25 | Loss: 0.00079256
Iteration 7/25 | Loss: 0.00079256
Iteration 8/25 | Loss: 0.00079256
Iteration 9/25 | Loss: 0.00079256
Iteration 10/25 | Loss: 0.00079256
Iteration 11/25 | Loss: 0.00079256
Iteration 12/25 | Loss: 0.00079256
Iteration 13/25 | Loss: 0.00079256
Iteration 14/25 | Loss: 0.00079256
Iteration 15/25 | Loss: 0.00079256
Iteration 16/25 | Loss: 0.00079256
Iteration 17/25 | Loss: 0.00079256
Iteration 18/25 | Loss: 0.00079256
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007925584795884788, 0.0007925584795884788, 0.0007925584795884788, 0.0007925584795884788, 0.0007925584795884788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007925584795884788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079256
Iteration 2/1000 | Loss: 0.00002435
Iteration 3/1000 | Loss: 0.00001730
Iteration 4/1000 | Loss: 0.00001407
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00001279
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001123
Iteration 15/1000 | Loss: 0.00001122
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001096
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001085
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001074
Iteration 72/1000 | Loss: 0.00001074
Iteration 73/1000 | Loss: 0.00001074
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001073
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001070
Iteration 82/1000 | Loss: 0.00001070
Iteration 83/1000 | Loss: 0.00001069
Iteration 84/1000 | Loss: 0.00001069
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001068
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001067
Iteration 95/1000 | Loss: 0.00001067
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001066
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001065
Iteration 108/1000 | Loss: 0.00001065
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001064
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001064
Iteration 121/1000 | Loss: 0.00001064
Iteration 122/1000 | Loss: 0.00001064
Iteration 123/1000 | Loss: 0.00001064
Iteration 124/1000 | Loss: 0.00001064
Iteration 125/1000 | Loss: 0.00001064
Iteration 126/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.0644576832419261e-05, 1.0644576832419261e-05, 1.0644576832419261e-05, 1.0644576832419261e-05, 1.0644576832419261e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0644576832419261e-05

Optimization complete. Final v2v error: 2.7894973754882812 mm

Highest mean error: 3.374990940093994 mm for frame 59

Lowest mean error: 2.487751007080078 mm for frame 41

Saving results

Total time: 36.81412124633789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975926
Iteration 2/25 | Loss: 0.00165163
Iteration 3/25 | Loss: 0.00133470
Iteration 4/25 | Loss: 0.00131274
Iteration 5/25 | Loss: 0.00130556
Iteration 6/25 | Loss: 0.00130397
Iteration 7/25 | Loss: 0.00130397
Iteration 8/25 | Loss: 0.00130397
Iteration 9/25 | Loss: 0.00130397
Iteration 10/25 | Loss: 0.00130397
Iteration 11/25 | Loss: 0.00130397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013039734913036227, 0.0013039734913036227, 0.0013039734913036227, 0.0013039734913036227, 0.0013039734913036227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013039734913036227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52652961
Iteration 2/25 | Loss: 0.00097466
Iteration 3/25 | Loss: 0.00097466
Iteration 4/25 | Loss: 0.00097466
Iteration 5/25 | Loss: 0.00097466
Iteration 6/25 | Loss: 0.00097466
Iteration 7/25 | Loss: 0.00097465
Iteration 8/25 | Loss: 0.00097465
Iteration 9/25 | Loss: 0.00097465
Iteration 10/25 | Loss: 0.00097465
Iteration 11/25 | Loss: 0.00097465
Iteration 12/25 | Loss: 0.00097465
Iteration 13/25 | Loss: 0.00097465
Iteration 14/25 | Loss: 0.00097465
Iteration 15/25 | Loss: 0.00097465
Iteration 16/25 | Loss: 0.00097465
Iteration 17/25 | Loss: 0.00097465
Iteration 18/25 | Loss: 0.00097465
Iteration 19/25 | Loss: 0.00097465
Iteration 20/25 | Loss: 0.00097465
Iteration 21/25 | Loss: 0.00097465
Iteration 22/25 | Loss: 0.00097465
Iteration 23/25 | Loss: 0.00097465
Iteration 24/25 | Loss: 0.00097465
Iteration 25/25 | Loss: 0.00097465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097465
Iteration 2/1000 | Loss: 0.00007064
Iteration 3/1000 | Loss: 0.00004773
Iteration 4/1000 | Loss: 0.00003962
Iteration 5/1000 | Loss: 0.00003697
Iteration 6/1000 | Loss: 0.00003520
Iteration 7/1000 | Loss: 0.00003420
Iteration 8/1000 | Loss: 0.00003345
Iteration 9/1000 | Loss: 0.00003281
Iteration 10/1000 | Loss: 0.00003244
Iteration 11/1000 | Loss: 0.00003212
Iteration 12/1000 | Loss: 0.00003168
Iteration 13/1000 | Loss: 0.00003129
Iteration 14/1000 | Loss: 0.00003101
Iteration 15/1000 | Loss: 0.00003067
Iteration 16/1000 | Loss: 0.00003034
Iteration 17/1000 | Loss: 0.00003007
Iteration 18/1000 | Loss: 0.00002985
Iteration 19/1000 | Loss: 0.00002966
Iteration 20/1000 | Loss: 0.00002952
Iteration 21/1000 | Loss: 0.00002951
Iteration 22/1000 | Loss: 0.00002945
Iteration 23/1000 | Loss: 0.00002936
Iteration 24/1000 | Loss: 0.00002925
Iteration 25/1000 | Loss: 0.00002917
Iteration 26/1000 | Loss: 0.00002914
Iteration 27/1000 | Loss: 0.00002908
Iteration 28/1000 | Loss: 0.00002908
Iteration 29/1000 | Loss: 0.00002907
Iteration 30/1000 | Loss: 0.00002907
Iteration 31/1000 | Loss: 0.00002906
Iteration 32/1000 | Loss: 0.00002906
Iteration 33/1000 | Loss: 0.00002906
Iteration 34/1000 | Loss: 0.00002906
Iteration 35/1000 | Loss: 0.00002906
Iteration 36/1000 | Loss: 0.00002906
Iteration 37/1000 | Loss: 0.00002905
Iteration 38/1000 | Loss: 0.00002905
Iteration 39/1000 | Loss: 0.00002905
Iteration 40/1000 | Loss: 0.00002905
Iteration 41/1000 | Loss: 0.00002905
Iteration 42/1000 | Loss: 0.00002905
Iteration 43/1000 | Loss: 0.00002905
Iteration 44/1000 | Loss: 0.00002904
Iteration 45/1000 | Loss: 0.00002902
Iteration 46/1000 | Loss: 0.00002902
Iteration 47/1000 | Loss: 0.00002901
Iteration 48/1000 | Loss: 0.00002901
Iteration 49/1000 | Loss: 0.00002901
Iteration 50/1000 | Loss: 0.00002901
Iteration 51/1000 | Loss: 0.00002901
Iteration 52/1000 | Loss: 0.00002901
Iteration 53/1000 | Loss: 0.00002901
Iteration 54/1000 | Loss: 0.00002900
Iteration 55/1000 | Loss: 0.00002898
Iteration 56/1000 | Loss: 0.00002898
Iteration 57/1000 | Loss: 0.00002898
Iteration 58/1000 | Loss: 0.00002898
Iteration 59/1000 | Loss: 0.00002898
Iteration 60/1000 | Loss: 0.00002898
Iteration 61/1000 | Loss: 0.00002898
Iteration 62/1000 | Loss: 0.00002898
Iteration 63/1000 | Loss: 0.00002898
Iteration 64/1000 | Loss: 0.00002898
Iteration 65/1000 | Loss: 0.00002898
Iteration 66/1000 | Loss: 0.00002898
Iteration 67/1000 | Loss: 0.00002896
Iteration 68/1000 | Loss: 0.00002896
Iteration 69/1000 | Loss: 0.00002896
Iteration 70/1000 | Loss: 0.00002896
Iteration 71/1000 | Loss: 0.00002896
Iteration 72/1000 | Loss: 0.00002896
Iteration 73/1000 | Loss: 0.00002896
Iteration 74/1000 | Loss: 0.00002896
Iteration 75/1000 | Loss: 0.00002896
Iteration 76/1000 | Loss: 0.00002896
Iteration 77/1000 | Loss: 0.00002896
Iteration 78/1000 | Loss: 0.00002895
Iteration 79/1000 | Loss: 0.00002895
Iteration 80/1000 | Loss: 0.00002895
Iteration 81/1000 | Loss: 0.00002894
Iteration 82/1000 | Loss: 0.00002894
Iteration 83/1000 | Loss: 0.00002894
Iteration 84/1000 | Loss: 0.00002894
Iteration 85/1000 | Loss: 0.00002894
Iteration 86/1000 | Loss: 0.00002894
Iteration 87/1000 | Loss: 0.00002894
Iteration 88/1000 | Loss: 0.00002894
Iteration 89/1000 | Loss: 0.00002894
Iteration 90/1000 | Loss: 0.00002894
Iteration 91/1000 | Loss: 0.00002894
Iteration 92/1000 | Loss: 0.00002893
Iteration 93/1000 | Loss: 0.00002893
Iteration 94/1000 | Loss: 0.00002893
Iteration 95/1000 | Loss: 0.00002893
Iteration 96/1000 | Loss: 0.00002893
Iteration 97/1000 | Loss: 0.00002893
Iteration 98/1000 | Loss: 0.00002893
Iteration 99/1000 | Loss: 0.00002893
Iteration 100/1000 | Loss: 0.00002893
Iteration 101/1000 | Loss: 0.00002892
Iteration 102/1000 | Loss: 0.00002892
Iteration 103/1000 | Loss: 0.00002892
Iteration 104/1000 | Loss: 0.00002892
Iteration 105/1000 | Loss: 0.00002892
Iteration 106/1000 | Loss: 0.00002892
Iteration 107/1000 | Loss: 0.00002892
Iteration 108/1000 | Loss: 0.00002892
Iteration 109/1000 | Loss: 0.00002892
Iteration 110/1000 | Loss: 0.00002892
Iteration 111/1000 | Loss: 0.00002892
Iteration 112/1000 | Loss: 0.00002892
Iteration 113/1000 | Loss: 0.00002892
Iteration 114/1000 | Loss: 0.00002892
Iteration 115/1000 | Loss: 0.00002892
Iteration 116/1000 | Loss: 0.00002892
Iteration 117/1000 | Loss: 0.00002892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.8922408091602847e-05, 2.8922408091602847e-05, 2.8922408091602847e-05, 2.8922408091602847e-05, 2.8922408091602847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8922408091602847e-05

Optimization complete. Final v2v error: 4.555771827697754 mm

Highest mean error: 4.7878851890563965 mm for frame 75

Lowest mean error: 4.039183616638184 mm for frame 39

Saving results

Total time: 49.08282828330994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987806
Iteration 2/25 | Loss: 0.00217803
Iteration 3/25 | Loss: 0.00168667
Iteration 4/25 | Loss: 0.00162416
Iteration 5/25 | Loss: 0.00163104
Iteration 6/25 | Loss: 0.00144045
Iteration 7/25 | Loss: 0.00130406
Iteration 8/25 | Loss: 0.00128056
Iteration 9/25 | Loss: 0.00127385
Iteration 10/25 | Loss: 0.00125080
Iteration 11/25 | Loss: 0.00123832
Iteration 12/25 | Loss: 0.00123985
Iteration 13/25 | Loss: 0.00124740
Iteration 14/25 | Loss: 0.00125803
Iteration 15/25 | Loss: 0.00125674
Iteration 16/25 | Loss: 0.00125155
Iteration 17/25 | Loss: 0.00123683
Iteration 18/25 | Loss: 0.00121999
Iteration 19/25 | Loss: 0.00122129
Iteration 20/25 | Loss: 0.00121649
Iteration 21/25 | Loss: 0.00122052
Iteration 22/25 | Loss: 0.00122825
Iteration 23/25 | Loss: 0.00122752
Iteration 24/25 | Loss: 0.00123199
Iteration 25/25 | Loss: 0.00122406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31158209
Iteration 2/25 | Loss: 0.00160702
Iteration 3/25 | Loss: 0.00160702
Iteration 4/25 | Loss: 0.00160701
Iteration 5/25 | Loss: 0.00160701
Iteration 6/25 | Loss: 0.00160701
Iteration 7/25 | Loss: 0.00160701
Iteration 8/25 | Loss: 0.00160701
Iteration 9/25 | Loss: 0.00160701
Iteration 10/25 | Loss: 0.00160701
Iteration 11/25 | Loss: 0.00160701
Iteration 12/25 | Loss: 0.00160701
Iteration 13/25 | Loss: 0.00160701
Iteration 14/25 | Loss: 0.00160701
Iteration 15/25 | Loss: 0.00160701
Iteration 16/25 | Loss: 0.00160701
Iteration 17/25 | Loss: 0.00160701
Iteration 18/25 | Loss: 0.00160701
Iteration 19/25 | Loss: 0.00160701
Iteration 20/25 | Loss: 0.00160701
Iteration 21/25 | Loss: 0.00160701
Iteration 22/25 | Loss: 0.00160701
Iteration 23/25 | Loss: 0.00160701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016070123529061675, 0.0016070123529061675, 0.0016070123529061675, 0.0016070123529061675, 0.0016070123529061675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016070123529061675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160701
Iteration 2/1000 | Loss: 0.00015389
Iteration 3/1000 | Loss: 0.00027938
Iteration 4/1000 | Loss: 0.00030619
Iteration 5/1000 | Loss: 0.00017559
Iteration 6/1000 | Loss: 0.00014451
Iteration 7/1000 | Loss: 0.00028456
Iteration 8/1000 | Loss: 0.00022160
Iteration 9/1000 | Loss: 0.00023601
Iteration 10/1000 | Loss: 0.00017134
Iteration 11/1000 | Loss: 0.00006511
Iteration 12/1000 | Loss: 0.00006104
Iteration 13/1000 | Loss: 0.00005785
Iteration 14/1000 | Loss: 0.00032273
Iteration 15/1000 | Loss: 0.00056645
Iteration 16/1000 | Loss: 0.00007419
Iteration 17/1000 | Loss: 0.00006408
Iteration 18/1000 | Loss: 0.00007567
Iteration 19/1000 | Loss: 0.00005401
Iteration 20/1000 | Loss: 0.00005156
Iteration 21/1000 | Loss: 0.00004980
Iteration 22/1000 | Loss: 0.00022545
Iteration 23/1000 | Loss: 0.00005308
Iteration 24/1000 | Loss: 0.00006615
Iteration 25/1000 | Loss: 0.00004864
Iteration 26/1000 | Loss: 0.00004378
Iteration 27/1000 | Loss: 0.00006048
Iteration 28/1000 | Loss: 0.00016322
Iteration 29/1000 | Loss: 0.00004098
Iteration 30/1000 | Loss: 0.00004027
Iteration 31/1000 | Loss: 0.00003958
Iteration 32/1000 | Loss: 0.00023074
Iteration 33/1000 | Loss: 0.00079619
Iteration 34/1000 | Loss: 0.00005063
Iteration 35/1000 | Loss: 0.00004821
Iteration 36/1000 | Loss: 0.00004096
Iteration 37/1000 | Loss: 0.00067044
Iteration 38/1000 | Loss: 0.00058608
Iteration 39/1000 | Loss: 0.00288451
Iteration 40/1000 | Loss: 0.00096375
Iteration 41/1000 | Loss: 0.00145975
Iteration 42/1000 | Loss: 0.00077707
Iteration 43/1000 | Loss: 0.00068034
Iteration 44/1000 | Loss: 0.00029432
Iteration 45/1000 | Loss: 0.00005476
Iteration 46/1000 | Loss: 0.00004475
Iteration 47/1000 | Loss: 0.00003951
Iteration 48/1000 | Loss: 0.00003568
Iteration 49/1000 | Loss: 0.00028590
Iteration 50/1000 | Loss: 0.00005664
Iteration 51/1000 | Loss: 0.00003866
Iteration 52/1000 | Loss: 0.00005022
Iteration 53/1000 | Loss: 0.00004684
Iteration 54/1000 | Loss: 0.00004567
Iteration 55/1000 | Loss: 0.00003690
Iteration 56/1000 | Loss: 0.00003891
Iteration 57/1000 | Loss: 0.00003206
Iteration 58/1000 | Loss: 0.00004664
Iteration 59/1000 | Loss: 0.00003029
Iteration 60/1000 | Loss: 0.00002929
Iteration 61/1000 | Loss: 0.00003692
Iteration 62/1000 | Loss: 0.00002830
Iteration 63/1000 | Loss: 0.00049771
Iteration 64/1000 | Loss: 0.00003791
Iteration 65/1000 | Loss: 0.00002739
Iteration 66/1000 | Loss: 0.00002567
Iteration 67/1000 | Loss: 0.00007380
Iteration 68/1000 | Loss: 0.00003244
Iteration 69/1000 | Loss: 0.00003779
Iteration 70/1000 | Loss: 0.00002635
Iteration 71/1000 | Loss: 0.00002316
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002254
Iteration 74/1000 | Loss: 0.00002237
Iteration 75/1000 | Loss: 0.00002219
Iteration 76/1000 | Loss: 0.00002219
Iteration 77/1000 | Loss: 0.00002219
Iteration 78/1000 | Loss: 0.00002218
Iteration 79/1000 | Loss: 0.00002216
Iteration 80/1000 | Loss: 0.00002212
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002201
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00004132
Iteration 86/1000 | Loss: 0.00002201
Iteration 87/1000 | Loss: 0.00002194
Iteration 88/1000 | Loss: 0.00002194
Iteration 89/1000 | Loss: 0.00002194
Iteration 90/1000 | Loss: 0.00002194
Iteration 91/1000 | Loss: 0.00002194
Iteration 92/1000 | Loss: 0.00002194
Iteration 93/1000 | Loss: 0.00002194
Iteration 94/1000 | Loss: 0.00002194
Iteration 95/1000 | Loss: 0.00002194
Iteration 96/1000 | Loss: 0.00002194
Iteration 97/1000 | Loss: 0.00002193
Iteration 98/1000 | Loss: 0.00002193
Iteration 99/1000 | Loss: 0.00002193
Iteration 100/1000 | Loss: 0.00002193
Iteration 101/1000 | Loss: 0.00002192
Iteration 102/1000 | Loss: 0.00002192
Iteration 103/1000 | Loss: 0.00002192
Iteration 104/1000 | Loss: 0.00002819
Iteration 105/1000 | Loss: 0.00002196
Iteration 106/1000 | Loss: 0.00002189
Iteration 107/1000 | Loss: 0.00002188
Iteration 108/1000 | Loss: 0.00002188
Iteration 109/1000 | Loss: 0.00002188
Iteration 110/1000 | Loss: 0.00002187
Iteration 111/1000 | Loss: 0.00002187
Iteration 112/1000 | Loss: 0.00002187
Iteration 113/1000 | Loss: 0.00002186
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002185
Iteration 116/1000 | Loss: 0.00002185
Iteration 117/1000 | Loss: 0.00002185
Iteration 118/1000 | Loss: 0.00002185
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002184
Iteration 121/1000 | Loss: 0.00002184
Iteration 122/1000 | Loss: 0.00002184
Iteration 123/1000 | Loss: 0.00002183
Iteration 124/1000 | Loss: 0.00002183
Iteration 125/1000 | Loss: 0.00002182
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002181
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002245
Iteration 133/1000 | Loss: 0.00002178
Iteration 134/1000 | Loss: 0.00002178
Iteration 135/1000 | Loss: 0.00002178
Iteration 136/1000 | Loss: 0.00002178
Iteration 137/1000 | Loss: 0.00002178
Iteration 138/1000 | Loss: 0.00002178
Iteration 139/1000 | Loss: 0.00002178
Iteration 140/1000 | Loss: 0.00002178
Iteration 141/1000 | Loss: 0.00002178
Iteration 142/1000 | Loss: 0.00002178
Iteration 143/1000 | Loss: 0.00002177
Iteration 144/1000 | Loss: 0.00002177
Iteration 145/1000 | Loss: 0.00002177
Iteration 146/1000 | Loss: 0.00002177
Iteration 147/1000 | Loss: 0.00002177
Iteration 148/1000 | Loss: 0.00002177
Iteration 149/1000 | Loss: 0.00002176
Iteration 150/1000 | Loss: 0.00002176
Iteration 151/1000 | Loss: 0.00002176
Iteration 152/1000 | Loss: 0.00002176
Iteration 153/1000 | Loss: 0.00002176
Iteration 154/1000 | Loss: 0.00002176
Iteration 155/1000 | Loss: 0.00002176
Iteration 156/1000 | Loss: 0.00002176
Iteration 157/1000 | Loss: 0.00002176
Iteration 158/1000 | Loss: 0.00002176
Iteration 159/1000 | Loss: 0.00002176
Iteration 160/1000 | Loss: 0.00002176
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002175
Iteration 167/1000 | Loss: 0.00002175
Iteration 168/1000 | Loss: 0.00002175
Iteration 169/1000 | Loss: 0.00002175
Iteration 170/1000 | Loss: 0.00002175
Iteration 171/1000 | Loss: 0.00002175
Iteration 172/1000 | Loss: 0.00002175
Iteration 173/1000 | Loss: 0.00002175
Iteration 174/1000 | Loss: 0.00002175
Iteration 175/1000 | Loss: 0.00002175
Iteration 176/1000 | Loss: 0.00002175
Iteration 177/1000 | Loss: 0.00002175
Iteration 178/1000 | Loss: 0.00002175
Iteration 179/1000 | Loss: 0.00002175
Iteration 180/1000 | Loss: 0.00002175
Iteration 181/1000 | Loss: 0.00002175
Iteration 182/1000 | Loss: 0.00002175
Iteration 183/1000 | Loss: 0.00002174
Iteration 184/1000 | Loss: 0.00002174
Iteration 185/1000 | Loss: 0.00002174
Iteration 186/1000 | Loss: 0.00002174
Iteration 187/1000 | Loss: 0.00002173
Iteration 188/1000 | Loss: 0.00002173
Iteration 189/1000 | Loss: 0.00002172
Iteration 190/1000 | Loss: 0.00002171
Iteration 191/1000 | Loss: 0.00002171
Iteration 192/1000 | Loss: 0.00002171
Iteration 193/1000 | Loss: 0.00002170
Iteration 194/1000 | Loss: 0.00002170
Iteration 195/1000 | Loss: 0.00002170
Iteration 196/1000 | Loss: 0.00002170
Iteration 197/1000 | Loss: 0.00002170
Iteration 198/1000 | Loss: 0.00002169
Iteration 199/1000 | Loss: 0.00002169
Iteration 200/1000 | Loss: 0.00002169
Iteration 201/1000 | Loss: 0.00002169
Iteration 202/1000 | Loss: 0.00002169
Iteration 203/1000 | Loss: 0.00002607
Iteration 204/1000 | Loss: 0.00002168
Iteration 205/1000 | Loss: 0.00002167
Iteration 206/1000 | Loss: 0.00002166
Iteration 207/1000 | Loss: 0.00002166
Iteration 208/1000 | Loss: 0.00002166
Iteration 209/1000 | Loss: 0.00002166
Iteration 210/1000 | Loss: 0.00002166
Iteration 211/1000 | Loss: 0.00002166
Iteration 212/1000 | Loss: 0.00002166
Iteration 213/1000 | Loss: 0.00002166
Iteration 214/1000 | Loss: 0.00002165
Iteration 215/1000 | Loss: 0.00002165
Iteration 216/1000 | Loss: 0.00002165
Iteration 217/1000 | Loss: 0.00002165
Iteration 218/1000 | Loss: 0.00002165
Iteration 219/1000 | Loss: 0.00002165
Iteration 220/1000 | Loss: 0.00002165
Iteration 221/1000 | Loss: 0.00002165
Iteration 222/1000 | Loss: 0.00002165
Iteration 223/1000 | Loss: 0.00002165
Iteration 224/1000 | Loss: 0.00002165
Iteration 225/1000 | Loss: 0.00002165
Iteration 226/1000 | Loss: 0.00002165
Iteration 227/1000 | Loss: 0.00002165
Iteration 228/1000 | Loss: 0.00002165
Iteration 229/1000 | Loss: 0.00002165
Iteration 230/1000 | Loss: 0.00002165
Iteration 231/1000 | Loss: 0.00002165
Iteration 232/1000 | Loss: 0.00002165
Iteration 233/1000 | Loss: 0.00002165
Iteration 234/1000 | Loss: 0.00002165
Iteration 235/1000 | Loss: 0.00002165
Iteration 236/1000 | Loss: 0.00002165
Iteration 237/1000 | Loss: 0.00002165
Iteration 238/1000 | Loss: 0.00002165
Iteration 239/1000 | Loss: 0.00002165
Iteration 240/1000 | Loss: 0.00002165
Iteration 241/1000 | Loss: 0.00002165
Iteration 242/1000 | Loss: 0.00002165
Iteration 243/1000 | Loss: 0.00002165
Iteration 244/1000 | Loss: 0.00002165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.165346268157009e-05, 2.165346268157009e-05, 2.165346268157009e-05, 2.165346268157009e-05, 2.165346268157009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.165346268157009e-05

Optimization complete. Final v2v error: 3.6541261672973633 mm

Highest mean error: 5.433838844299316 mm for frame 113

Lowest mean error: 2.4382753372192383 mm for frame 30

Saving results

Total time: 168.44385027885437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00706516
Iteration 2/25 | Loss: 0.00160088
Iteration 3/25 | Loss: 0.00124563
Iteration 4/25 | Loss: 0.00120688
Iteration 5/25 | Loss: 0.00119259
Iteration 6/25 | Loss: 0.00119313
Iteration 7/25 | Loss: 0.00118640
Iteration 8/25 | Loss: 0.00118057
Iteration 9/25 | Loss: 0.00117999
Iteration 10/25 | Loss: 0.00117788
Iteration 11/25 | Loss: 0.00117720
Iteration 12/25 | Loss: 0.00117554
Iteration 13/25 | Loss: 0.00117529
Iteration 14/25 | Loss: 0.00117510
Iteration 15/25 | Loss: 0.00117568
Iteration 16/25 | Loss: 0.00117444
Iteration 17/25 | Loss: 0.00117402
Iteration 18/25 | Loss: 0.00117395
Iteration 19/25 | Loss: 0.00117394
Iteration 20/25 | Loss: 0.00117394
Iteration 21/25 | Loss: 0.00117393
Iteration 22/25 | Loss: 0.00117393
Iteration 23/25 | Loss: 0.00117393
Iteration 24/25 | Loss: 0.00117393
Iteration 25/25 | Loss: 0.00117393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33862281
Iteration 2/25 | Loss: 0.00057885
Iteration 3/25 | Loss: 0.00057882
Iteration 4/25 | Loss: 0.00057882
Iteration 5/25 | Loss: 0.00057882
Iteration 6/25 | Loss: 0.00057882
Iteration 7/25 | Loss: 0.00057882
Iteration 8/25 | Loss: 0.00057881
Iteration 9/25 | Loss: 0.00057881
Iteration 10/25 | Loss: 0.00057881
Iteration 11/25 | Loss: 0.00057881
Iteration 12/25 | Loss: 0.00057881
Iteration 13/25 | Loss: 0.00057881
Iteration 14/25 | Loss: 0.00057881
Iteration 15/25 | Loss: 0.00057881
Iteration 16/25 | Loss: 0.00057881
Iteration 17/25 | Loss: 0.00057881
Iteration 18/25 | Loss: 0.00057881
Iteration 19/25 | Loss: 0.00057881
Iteration 20/25 | Loss: 0.00057881
Iteration 21/25 | Loss: 0.00057881
Iteration 22/25 | Loss: 0.00057881
Iteration 23/25 | Loss: 0.00057881
Iteration 24/25 | Loss: 0.00057881
Iteration 25/25 | Loss: 0.00057881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057881
Iteration 2/1000 | Loss: 0.00003600
Iteration 3/1000 | Loss: 0.00002234
Iteration 4/1000 | Loss: 0.00002030
Iteration 5/1000 | Loss: 0.00001940
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001812
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001724
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001717
Iteration 22/1000 | Loss: 0.00001717
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001715
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001708
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001706
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001705
Iteration 36/1000 | Loss: 0.00001705
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001704
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001702
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001701
Iteration 45/1000 | Loss: 0.00001701
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001698
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001698
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001697
Iteration 75/1000 | Loss: 0.00001696
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001696
Iteration 84/1000 | Loss: 0.00001696
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001695
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001695
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001695
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001695
Iteration 118/1000 | Loss: 0.00001695
Iteration 119/1000 | Loss: 0.00001695
Iteration 120/1000 | Loss: 0.00001695
Iteration 121/1000 | Loss: 0.00001695
Iteration 122/1000 | Loss: 0.00001695
Iteration 123/1000 | Loss: 0.00001695
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001695
Iteration 126/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.6948497432167642e-05, 1.6948497432167642e-05, 1.6948497432167642e-05, 1.6948497432167642e-05, 1.6948497432167642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6948497432167642e-05

Optimization complete. Final v2v error: 3.36984920501709 mm

Highest mean error: 3.8161582946777344 mm for frame 118

Lowest mean error: 2.7284672260284424 mm for frame 15

Saving results

Total time: 64.31787252426147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030718
Iteration 2/25 | Loss: 0.00251430
Iteration 3/25 | Loss: 0.00170536
Iteration 4/25 | Loss: 0.00147902
Iteration 5/25 | Loss: 0.00151810
Iteration 6/25 | Loss: 0.00139790
Iteration 7/25 | Loss: 0.00128968
Iteration 8/25 | Loss: 0.00125580
Iteration 9/25 | Loss: 0.00122024
Iteration 10/25 | Loss: 0.00117465
Iteration 11/25 | Loss: 0.00116719
Iteration 12/25 | Loss: 0.00112852
Iteration 13/25 | Loss: 0.00110332
Iteration 14/25 | Loss: 0.00110876
Iteration 15/25 | Loss: 0.00109465
Iteration 16/25 | Loss: 0.00109553
Iteration 17/25 | Loss: 0.00108002
Iteration 18/25 | Loss: 0.00107286
Iteration 19/25 | Loss: 0.00107508
Iteration 20/25 | Loss: 0.00107507
Iteration 21/25 | Loss: 0.00108068
Iteration 22/25 | Loss: 0.00107761
Iteration 23/25 | Loss: 0.00107436
Iteration 24/25 | Loss: 0.00107439
Iteration 25/25 | Loss: 0.00107239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92390060
Iteration 2/25 | Loss: 0.00088338
Iteration 3/25 | Loss: 0.00075541
Iteration 4/25 | Loss: 0.00075541
Iteration 5/25 | Loss: 0.00075541
Iteration 6/25 | Loss: 0.00075541
Iteration 7/25 | Loss: 0.00075540
Iteration 8/25 | Loss: 0.00075540
Iteration 9/25 | Loss: 0.00075540
Iteration 10/25 | Loss: 0.00075540
Iteration 11/25 | Loss: 0.00075540
Iteration 12/25 | Loss: 0.00075540
Iteration 13/25 | Loss: 0.00075540
Iteration 14/25 | Loss: 0.00075540
Iteration 15/25 | Loss: 0.00075540
Iteration 16/25 | Loss: 0.00075540
Iteration 17/25 | Loss: 0.00075540
Iteration 18/25 | Loss: 0.00075540
Iteration 19/25 | Loss: 0.00075540
Iteration 20/25 | Loss: 0.00075540
Iteration 21/25 | Loss: 0.00075540
Iteration 22/25 | Loss: 0.00075540
Iteration 23/25 | Loss: 0.00075540
Iteration 24/25 | Loss: 0.00075540
Iteration 25/25 | Loss: 0.00075540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075540
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00051715
Iteration 4/1000 | Loss: 0.00209361
Iteration 5/1000 | Loss: 0.00007004
Iteration 6/1000 | Loss: 0.00072798
Iteration 7/1000 | Loss: 0.00016842
Iteration 8/1000 | Loss: 0.00004406
Iteration 9/1000 | Loss: 0.00005455
Iteration 10/1000 | Loss: 0.00001445
Iteration 11/1000 | Loss: 0.00007619
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00006944
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00002148
Iteration 16/1000 | Loss: 0.00001349
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001317
Iteration 25/1000 | Loss: 0.00001316
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001315
Iteration 29/1000 | Loss: 0.00001314
Iteration 30/1000 | Loss: 0.00001840
Iteration 31/1000 | Loss: 0.00001840
Iteration 32/1000 | Loss: 0.00016678
Iteration 33/1000 | Loss: 0.00001998
Iteration 34/1000 | Loss: 0.00002684
Iteration 35/1000 | Loss: 0.00005992
Iteration 36/1000 | Loss: 0.00001380
Iteration 37/1000 | Loss: 0.00004618
Iteration 38/1000 | Loss: 0.00001660
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001284
Iteration 45/1000 | Loss: 0.00001284
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001284
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001283
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001283
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001282
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001281
Iteration 71/1000 | Loss: 0.00001281
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.2807140592485666e-05, 1.2807140592485666e-05, 1.2807140592485666e-05, 1.2807140592485666e-05, 1.2807140592485666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2807140592485666e-05

Optimization complete. Final v2v error: 3.044492483139038 mm

Highest mean error: 3.6792397499084473 mm for frame 141

Lowest mean error: 2.7786788940429688 mm for frame 70

Saving results

Total time: 86.56292057037354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564422
Iteration 2/25 | Loss: 0.00115275
Iteration 3/25 | Loss: 0.00108423
Iteration 4/25 | Loss: 0.00107283
Iteration 5/25 | Loss: 0.00106816
Iteration 6/25 | Loss: 0.00106709
Iteration 7/25 | Loss: 0.00106709
Iteration 8/25 | Loss: 0.00106709
Iteration 9/25 | Loss: 0.00106709
Iteration 10/25 | Loss: 0.00106709
Iteration 11/25 | Loss: 0.00106709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010670925257727504, 0.0010670925257727504, 0.0010670925257727504, 0.0010670925257727504, 0.0010670925257727504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010670925257727504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70226932
Iteration 2/25 | Loss: 0.00085392
Iteration 3/25 | Loss: 0.00085392
Iteration 4/25 | Loss: 0.00085392
Iteration 5/25 | Loss: 0.00085392
Iteration 6/25 | Loss: 0.00085391
Iteration 7/25 | Loss: 0.00085391
Iteration 8/25 | Loss: 0.00085391
Iteration 9/25 | Loss: 0.00085391
Iteration 10/25 | Loss: 0.00085391
Iteration 11/25 | Loss: 0.00085391
Iteration 12/25 | Loss: 0.00085391
Iteration 13/25 | Loss: 0.00085391
Iteration 14/25 | Loss: 0.00085391
Iteration 15/25 | Loss: 0.00085391
Iteration 16/25 | Loss: 0.00085391
Iteration 17/25 | Loss: 0.00085391
Iteration 18/25 | Loss: 0.00085391
Iteration 19/25 | Loss: 0.00085391
Iteration 20/25 | Loss: 0.00085391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000853913021273911, 0.000853913021273911, 0.000853913021273911, 0.000853913021273911, 0.000853913021273911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000853913021273911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085391
Iteration 2/1000 | Loss: 0.00002120
Iteration 3/1000 | Loss: 0.00001423
Iteration 4/1000 | Loss: 0.00001212
Iteration 5/1000 | Loss: 0.00001154
Iteration 6/1000 | Loss: 0.00001101
Iteration 7/1000 | Loss: 0.00001055
Iteration 8/1000 | Loss: 0.00001037
Iteration 9/1000 | Loss: 0.00001024
Iteration 10/1000 | Loss: 0.00001022
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001005
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000993
Iteration 15/1000 | Loss: 0.00000984
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000981
Iteration 18/1000 | Loss: 0.00000972
Iteration 19/1000 | Loss: 0.00000967
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000965
Iteration 22/1000 | Loss: 0.00000964
Iteration 23/1000 | Loss: 0.00000963
Iteration 24/1000 | Loss: 0.00000960
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000959
Iteration 28/1000 | Loss: 0.00000959
Iteration 29/1000 | Loss: 0.00000958
Iteration 30/1000 | Loss: 0.00000958
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000956
Iteration 33/1000 | Loss: 0.00000956
Iteration 34/1000 | Loss: 0.00000954
Iteration 35/1000 | Loss: 0.00000954
Iteration 36/1000 | Loss: 0.00000953
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000952
Iteration 39/1000 | Loss: 0.00000952
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000950
Iteration 43/1000 | Loss: 0.00000949
Iteration 44/1000 | Loss: 0.00000948
Iteration 45/1000 | Loss: 0.00000948
Iteration 46/1000 | Loss: 0.00000948
Iteration 47/1000 | Loss: 0.00000948
Iteration 48/1000 | Loss: 0.00000948
Iteration 49/1000 | Loss: 0.00000947
Iteration 50/1000 | Loss: 0.00000947
Iteration 51/1000 | Loss: 0.00000947
Iteration 52/1000 | Loss: 0.00000946
Iteration 53/1000 | Loss: 0.00000946
Iteration 54/1000 | Loss: 0.00000945
Iteration 55/1000 | Loss: 0.00000945
Iteration 56/1000 | Loss: 0.00000944
Iteration 57/1000 | Loss: 0.00000944
Iteration 58/1000 | Loss: 0.00000943
Iteration 59/1000 | Loss: 0.00000943
Iteration 60/1000 | Loss: 0.00000943
Iteration 61/1000 | Loss: 0.00000942
Iteration 62/1000 | Loss: 0.00000942
Iteration 63/1000 | Loss: 0.00000942
Iteration 64/1000 | Loss: 0.00000941
Iteration 65/1000 | Loss: 0.00000940
Iteration 66/1000 | Loss: 0.00000939
Iteration 67/1000 | Loss: 0.00000939
Iteration 68/1000 | Loss: 0.00000938
Iteration 69/1000 | Loss: 0.00000938
Iteration 70/1000 | Loss: 0.00000938
Iteration 71/1000 | Loss: 0.00000938
Iteration 72/1000 | Loss: 0.00000938
Iteration 73/1000 | Loss: 0.00000937
Iteration 74/1000 | Loss: 0.00000936
Iteration 75/1000 | Loss: 0.00000936
Iteration 76/1000 | Loss: 0.00000936
Iteration 77/1000 | Loss: 0.00000936
Iteration 78/1000 | Loss: 0.00000935
Iteration 79/1000 | Loss: 0.00000935
Iteration 80/1000 | Loss: 0.00000935
Iteration 81/1000 | Loss: 0.00000935
Iteration 82/1000 | Loss: 0.00000935
Iteration 83/1000 | Loss: 0.00000935
Iteration 84/1000 | Loss: 0.00000935
Iteration 85/1000 | Loss: 0.00000935
Iteration 86/1000 | Loss: 0.00000935
Iteration 87/1000 | Loss: 0.00000935
Iteration 88/1000 | Loss: 0.00000935
Iteration 89/1000 | Loss: 0.00000935
Iteration 90/1000 | Loss: 0.00000935
Iteration 91/1000 | Loss: 0.00000934
Iteration 92/1000 | Loss: 0.00000934
Iteration 93/1000 | Loss: 0.00000934
Iteration 94/1000 | Loss: 0.00000933
Iteration 95/1000 | Loss: 0.00000933
Iteration 96/1000 | Loss: 0.00000933
Iteration 97/1000 | Loss: 0.00000933
Iteration 98/1000 | Loss: 0.00000933
Iteration 99/1000 | Loss: 0.00000933
Iteration 100/1000 | Loss: 0.00000933
Iteration 101/1000 | Loss: 0.00000932
Iteration 102/1000 | Loss: 0.00000932
Iteration 103/1000 | Loss: 0.00000931
Iteration 104/1000 | Loss: 0.00000931
Iteration 105/1000 | Loss: 0.00000931
Iteration 106/1000 | Loss: 0.00000931
Iteration 107/1000 | Loss: 0.00000931
Iteration 108/1000 | Loss: 0.00000930
Iteration 109/1000 | Loss: 0.00000930
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000929
Iteration 112/1000 | Loss: 0.00000929
Iteration 113/1000 | Loss: 0.00000929
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000929
Iteration 116/1000 | Loss: 0.00000928
Iteration 117/1000 | Loss: 0.00000928
Iteration 118/1000 | Loss: 0.00000928
Iteration 119/1000 | Loss: 0.00000927
Iteration 120/1000 | Loss: 0.00000927
Iteration 121/1000 | Loss: 0.00000927
Iteration 122/1000 | Loss: 0.00000927
Iteration 123/1000 | Loss: 0.00000926
Iteration 124/1000 | Loss: 0.00000926
Iteration 125/1000 | Loss: 0.00000926
Iteration 126/1000 | Loss: 0.00000926
Iteration 127/1000 | Loss: 0.00000926
Iteration 128/1000 | Loss: 0.00000926
Iteration 129/1000 | Loss: 0.00000926
Iteration 130/1000 | Loss: 0.00000926
Iteration 131/1000 | Loss: 0.00000926
Iteration 132/1000 | Loss: 0.00000926
Iteration 133/1000 | Loss: 0.00000925
Iteration 134/1000 | Loss: 0.00000925
Iteration 135/1000 | Loss: 0.00000925
Iteration 136/1000 | Loss: 0.00000925
Iteration 137/1000 | Loss: 0.00000925
Iteration 138/1000 | Loss: 0.00000925
Iteration 139/1000 | Loss: 0.00000925
Iteration 140/1000 | Loss: 0.00000925
Iteration 141/1000 | Loss: 0.00000925
Iteration 142/1000 | Loss: 0.00000925
Iteration 143/1000 | Loss: 0.00000925
Iteration 144/1000 | Loss: 0.00000925
Iteration 145/1000 | Loss: 0.00000925
Iteration 146/1000 | Loss: 0.00000925
Iteration 147/1000 | Loss: 0.00000925
Iteration 148/1000 | Loss: 0.00000925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [9.246733498002868e-06, 9.246733498002868e-06, 9.246733498002868e-06, 9.246733498002868e-06, 9.246733498002868e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.246733498002868e-06

Optimization complete. Final v2v error: 2.6154675483703613 mm

Highest mean error: 2.814117193222046 mm for frame 105

Lowest mean error: 2.46173357963562 mm for frame 125

Saving results

Total time: 36.47139358520508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808006
Iteration 2/25 | Loss: 0.00112080
Iteration 3/25 | Loss: 0.00105830
Iteration 4/25 | Loss: 0.00104266
Iteration 5/25 | Loss: 0.00103953
Iteration 6/25 | Loss: 0.00103934
Iteration 7/25 | Loss: 0.00103934
Iteration 8/25 | Loss: 0.00103934
Iteration 9/25 | Loss: 0.00103934
Iteration 10/25 | Loss: 0.00103934
Iteration 11/25 | Loss: 0.00103934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001039335154928267, 0.001039335154928267, 0.001039335154928267, 0.001039335154928267, 0.001039335154928267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001039335154928267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.56465626
Iteration 2/25 | Loss: 0.00118155
Iteration 3/25 | Loss: 0.00118149
Iteration 4/25 | Loss: 0.00118149
Iteration 5/25 | Loss: 0.00118149
Iteration 6/25 | Loss: 0.00118149
Iteration 7/25 | Loss: 0.00118149
Iteration 8/25 | Loss: 0.00118149
Iteration 9/25 | Loss: 0.00118149
Iteration 10/25 | Loss: 0.00118149
Iteration 11/25 | Loss: 0.00118149
Iteration 12/25 | Loss: 0.00118149
Iteration 13/25 | Loss: 0.00118149
Iteration 14/25 | Loss: 0.00118149
Iteration 15/25 | Loss: 0.00118149
Iteration 16/25 | Loss: 0.00118149
Iteration 17/25 | Loss: 0.00118149
Iteration 18/25 | Loss: 0.00118149
Iteration 19/25 | Loss: 0.00118149
Iteration 20/25 | Loss: 0.00118149
Iteration 21/25 | Loss: 0.00118149
Iteration 22/25 | Loss: 0.00118149
Iteration 23/25 | Loss: 0.00118149
Iteration 24/25 | Loss: 0.00118149
Iteration 25/25 | Loss: 0.00118149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118149
Iteration 2/1000 | Loss: 0.00002944
Iteration 3/1000 | Loss: 0.00001871
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001210
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001158
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001132
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001123
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001119
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001111
Iteration 56/1000 | Loss: 0.00001111
Iteration 57/1000 | Loss: 0.00001111
Iteration 58/1000 | Loss: 0.00001111
Iteration 59/1000 | Loss: 0.00001111
Iteration 60/1000 | Loss: 0.00001111
Iteration 61/1000 | Loss: 0.00001110
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001110
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001107
Iteration 70/1000 | Loss: 0.00001107
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001104
Iteration 74/1000 | Loss: 0.00001104
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001101
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001100
Iteration 95/1000 | Loss: 0.00001100
Iteration 96/1000 | Loss: 0.00001100
Iteration 97/1000 | Loss: 0.00001100
Iteration 98/1000 | Loss: 0.00001100
Iteration 99/1000 | Loss: 0.00001100
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001099
Iteration 102/1000 | Loss: 0.00001099
Iteration 103/1000 | Loss: 0.00001099
Iteration 104/1000 | Loss: 0.00001099
Iteration 105/1000 | Loss: 0.00001098
Iteration 106/1000 | Loss: 0.00001098
Iteration 107/1000 | Loss: 0.00001098
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001097
Iteration 114/1000 | Loss: 0.00001097
Iteration 115/1000 | Loss: 0.00001097
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001097
Iteration 118/1000 | Loss: 0.00001097
Iteration 119/1000 | Loss: 0.00001097
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001097
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001097
Iteration 128/1000 | Loss: 0.00001097
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001096
Iteration 134/1000 | Loss: 0.00001096
Iteration 135/1000 | Loss: 0.00001096
Iteration 136/1000 | Loss: 0.00001096
Iteration 137/1000 | Loss: 0.00001096
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001096
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001096
Iteration 142/1000 | Loss: 0.00001096
Iteration 143/1000 | Loss: 0.00001096
Iteration 144/1000 | Loss: 0.00001096
Iteration 145/1000 | Loss: 0.00001096
Iteration 146/1000 | Loss: 0.00001095
Iteration 147/1000 | Loss: 0.00001095
Iteration 148/1000 | Loss: 0.00001095
Iteration 149/1000 | Loss: 0.00001095
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001095
Iteration 152/1000 | Loss: 0.00001095
Iteration 153/1000 | Loss: 0.00001095
Iteration 154/1000 | Loss: 0.00001095
Iteration 155/1000 | Loss: 0.00001095
Iteration 156/1000 | Loss: 0.00001095
Iteration 157/1000 | Loss: 0.00001095
Iteration 158/1000 | Loss: 0.00001095
Iteration 159/1000 | Loss: 0.00001095
Iteration 160/1000 | Loss: 0.00001095
Iteration 161/1000 | Loss: 0.00001095
Iteration 162/1000 | Loss: 0.00001095
Iteration 163/1000 | Loss: 0.00001095
Iteration 164/1000 | Loss: 0.00001095
Iteration 165/1000 | Loss: 0.00001095
Iteration 166/1000 | Loss: 0.00001095
Iteration 167/1000 | Loss: 0.00001095
Iteration 168/1000 | Loss: 0.00001095
Iteration 169/1000 | Loss: 0.00001095
Iteration 170/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.0949677744065411e-05, 1.0949677744065411e-05, 1.0949677744065411e-05, 1.0949677744065411e-05, 1.0949677744065411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0949677744065411e-05

Optimization complete. Final v2v error: 2.8420650959014893 mm

Highest mean error: 3.2100162506103516 mm for frame 28

Lowest mean error: 2.5184993743896484 mm for frame 235

Saving results

Total time: 43.1027729511261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518863
Iteration 2/25 | Loss: 0.00122995
Iteration 3/25 | Loss: 0.00112323
Iteration 4/25 | Loss: 0.00110968
Iteration 5/25 | Loss: 0.00110689
Iteration 6/25 | Loss: 0.00110689
Iteration 7/25 | Loss: 0.00110689
Iteration 8/25 | Loss: 0.00110689
Iteration 9/25 | Loss: 0.00110689
Iteration 10/25 | Loss: 0.00110689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011068879393860698, 0.0011068879393860698, 0.0011068879393860698, 0.0011068879393860698, 0.0011068879393860698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011068879393860698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76723504
Iteration 2/25 | Loss: 0.00084596
Iteration 3/25 | Loss: 0.00084595
Iteration 4/25 | Loss: 0.00084595
Iteration 5/25 | Loss: 0.00084595
Iteration 6/25 | Loss: 0.00084595
Iteration 7/25 | Loss: 0.00084595
Iteration 8/25 | Loss: 0.00084595
Iteration 9/25 | Loss: 0.00084595
Iteration 10/25 | Loss: 0.00084594
Iteration 11/25 | Loss: 0.00084594
Iteration 12/25 | Loss: 0.00084594
Iteration 13/25 | Loss: 0.00084594
Iteration 14/25 | Loss: 0.00084594
Iteration 15/25 | Loss: 0.00084594
Iteration 16/25 | Loss: 0.00084594
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008459447999484837, 0.0008459447999484837, 0.0008459447999484837, 0.0008459447999484837, 0.0008459447999484837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008459447999484837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084594
Iteration 2/1000 | Loss: 0.00002306
Iteration 3/1000 | Loss: 0.00001735
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001507
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001304
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001296
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001289
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00001283
Iteration 37/1000 | Loss: 0.00001282
Iteration 38/1000 | Loss: 0.00001282
Iteration 39/1000 | Loss: 0.00001282
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001281
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001276
Iteration 56/1000 | Loss: 0.00001276
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001275
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001274
Iteration 72/1000 | Loss: 0.00001274
Iteration 73/1000 | Loss: 0.00001273
Iteration 74/1000 | Loss: 0.00001273
Iteration 75/1000 | Loss: 0.00001273
Iteration 76/1000 | Loss: 0.00001273
Iteration 77/1000 | Loss: 0.00001273
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001273
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001272
Iteration 89/1000 | Loss: 0.00001272
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001270
Iteration 101/1000 | Loss: 0.00001270
Iteration 102/1000 | Loss: 0.00001270
Iteration 103/1000 | Loss: 0.00001270
Iteration 104/1000 | Loss: 0.00001270
Iteration 105/1000 | Loss: 0.00001270
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001270
Iteration 109/1000 | Loss: 0.00001270
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001270
Iteration 116/1000 | Loss: 0.00001270
Iteration 117/1000 | Loss: 0.00001270
Iteration 118/1000 | Loss: 0.00001270
Iteration 119/1000 | Loss: 0.00001270
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001270
Iteration 122/1000 | Loss: 0.00001270
Iteration 123/1000 | Loss: 0.00001270
Iteration 124/1000 | Loss: 0.00001270
Iteration 125/1000 | Loss: 0.00001270
Iteration 126/1000 | Loss: 0.00001270
Iteration 127/1000 | Loss: 0.00001270
Iteration 128/1000 | Loss: 0.00001270
Iteration 129/1000 | Loss: 0.00001270
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.2701547348115128e-05, 1.2701547348115128e-05, 1.2701547348115128e-05, 1.2701547348115128e-05, 1.2701547348115128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2701547348115128e-05

Optimization complete. Final v2v error: 3.018789291381836 mm

Highest mean error: 3.308232307434082 mm for frame 126

Lowest mean error: 2.794545888900757 mm for frame 256

Saving results

Total time: 38.75291085243225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461223
Iteration 2/25 | Loss: 0.00122171
Iteration 3/25 | Loss: 0.00111560
Iteration 4/25 | Loss: 0.00110161
Iteration 5/25 | Loss: 0.00109860
Iteration 6/25 | Loss: 0.00109860
Iteration 7/25 | Loss: 0.00109860
Iteration 8/25 | Loss: 0.00109860
Iteration 9/25 | Loss: 0.00109860
Iteration 10/25 | Loss: 0.00109860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001098604523576796, 0.001098604523576796, 0.001098604523576796, 0.001098604523576796, 0.001098604523576796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098604523576796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87390614
Iteration 2/25 | Loss: 0.00075411
Iteration 3/25 | Loss: 0.00075411
Iteration 4/25 | Loss: 0.00075411
Iteration 5/25 | Loss: 0.00075411
Iteration 6/25 | Loss: 0.00075411
Iteration 7/25 | Loss: 0.00075411
Iteration 8/25 | Loss: 0.00075411
Iteration 9/25 | Loss: 0.00075411
Iteration 10/25 | Loss: 0.00075411
Iteration 11/25 | Loss: 0.00075411
Iteration 12/25 | Loss: 0.00075411
Iteration 13/25 | Loss: 0.00075411
Iteration 14/25 | Loss: 0.00075411
Iteration 15/25 | Loss: 0.00075411
Iteration 16/25 | Loss: 0.00075411
Iteration 17/25 | Loss: 0.00075411
Iteration 18/25 | Loss: 0.00075411
Iteration 19/25 | Loss: 0.00075411
Iteration 20/25 | Loss: 0.00075411
Iteration 21/25 | Loss: 0.00075411
Iteration 22/25 | Loss: 0.00075411
Iteration 23/25 | Loss: 0.00075411
Iteration 24/25 | Loss: 0.00075411
Iteration 25/25 | Loss: 0.00075411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075411
Iteration 2/1000 | Loss: 0.00002213
Iteration 3/1000 | Loss: 0.00001529
Iteration 4/1000 | Loss: 0.00001428
Iteration 5/1000 | Loss: 0.00001344
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001239
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001158
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001132
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001131
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001125
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001122
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001118
Iteration 66/1000 | Loss: 0.00001118
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001105
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001103
Iteration 119/1000 | Loss: 0.00001103
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001103
Iteration 125/1000 | Loss: 0.00001103
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001102
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001101
Iteration 140/1000 | Loss: 0.00001101
Iteration 141/1000 | Loss: 0.00001101
Iteration 142/1000 | Loss: 0.00001101
Iteration 143/1000 | Loss: 0.00001101
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001100
Iteration 147/1000 | Loss: 0.00001100
Iteration 148/1000 | Loss: 0.00001100
Iteration 149/1000 | Loss: 0.00001100
Iteration 150/1000 | Loss: 0.00001100
Iteration 151/1000 | Loss: 0.00001100
Iteration 152/1000 | Loss: 0.00001100
Iteration 153/1000 | Loss: 0.00001100
Iteration 154/1000 | Loss: 0.00001100
Iteration 155/1000 | Loss: 0.00001100
Iteration 156/1000 | Loss: 0.00001100
Iteration 157/1000 | Loss: 0.00001100
Iteration 158/1000 | Loss: 0.00001100
Iteration 159/1000 | Loss: 0.00001100
Iteration 160/1000 | Loss: 0.00001100
Iteration 161/1000 | Loss: 0.00001100
Iteration 162/1000 | Loss: 0.00001100
Iteration 163/1000 | Loss: 0.00001100
Iteration 164/1000 | Loss: 0.00001100
Iteration 165/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.0997627214237582e-05, 1.0997627214237582e-05, 1.0997627214237582e-05, 1.0997627214237582e-05, 1.0997627214237582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0997627214237582e-05

Optimization complete. Final v2v error: 2.8365399837493896 mm

Highest mean error: 3.0698676109313965 mm for frame 122

Lowest mean error: 2.67085862159729 mm for frame 161

Saving results

Total time: 43.46867370605469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009124
Iteration 2/25 | Loss: 0.00381429
Iteration 3/25 | Loss: 0.00196652
Iteration 4/25 | Loss: 0.00180457
Iteration 5/25 | Loss: 0.00199271
Iteration 6/25 | Loss: 0.00165549
Iteration 7/25 | Loss: 0.00144798
Iteration 8/25 | Loss: 0.00122822
Iteration 9/25 | Loss: 0.00117861
Iteration 10/25 | Loss: 0.00117222
Iteration 11/25 | Loss: 0.00113908
Iteration 12/25 | Loss: 0.00113221
Iteration 13/25 | Loss: 0.00112901
Iteration 14/25 | Loss: 0.00114288
Iteration 15/25 | Loss: 0.00113723
Iteration 16/25 | Loss: 0.00114366
Iteration 17/25 | Loss: 0.00112295
Iteration 18/25 | Loss: 0.00112182
Iteration 19/25 | Loss: 0.00112767
Iteration 20/25 | Loss: 0.00111816
Iteration 21/25 | Loss: 0.00111762
Iteration 22/25 | Loss: 0.00111892
Iteration 23/25 | Loss: 0.00111798
Iteration 24/25 | Loss: 0.00111802
Iteration 25/25 | Loss: 0.00111683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37122583
Iteration 2/25 | Loss: 0.00125959
Iteration 3/25 | Loss: 0.00086910
Iteration 4/25 | Loss: 0.00086910
Iteration 5/25 | Loss: 0.00086909
Iteration 6/25 | Loss: 0.00086909
Iteration 7/25 | Loss: 0.00086909
Iteration 8/25 | Loss: 0.00086909
Iteration 9/25 | Loss: 0.00086909
Iteration 10/25 | Loss: 0.00086909
Iteration 11/25 | Loss: 0.00086909
Iteration 12/25 | Loss: 0.00086909
Iteration 13/25 | Loss: 0.00086909
Iteration 14/25 | Loss: 0.00086909
Iteration 15/25 | Loss: 0.00086909
Iteration 16/25 | Loss: 0.00086909
Iteration 17/25 | Loss: 0.00086909
Iteration 18/25 | Loss: 0.00086909
Iteration 19/25 | Loss: 0.00086909
Iteration 20/25 | Loss: 0.00086909
Iteration 21/25 | Loss: 0.00086909
Iteration 22/25 | Loss: 0.00086909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008690928807482123, 0.0008690928807482123, 0.0008690928807482123, 0.0008690928807482123, 0.0008690928807482123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008690928807482123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086909
Iteration 2/1000 | Loss: 0.00017685
Iteration 3/1000 | Loss: 0.00042921
Iteration 4/1000 | Loss: 0.00020284
Iteration 5/1000 | Loss: 0.00004085
Iteration 6/1000 | Loss: 0.00003847
Iteration 7/1000 | Loss: 0.00005612
Iteration 8/1000 | Loss: 0.00004796
Iteration 9/1000 | Loss: 0.00003825
Iteration 10/1000 | Loss: 0.00004129
Iteration 11/1000 | Loss: 0.00003392
Iteration 12/1000 | Loss: 0.00004195
Iteration 13/1000 | Loss: 0.00004767
Iteration 14/1000 | Loss: 0.00004559
Iteration 15/1000 | Loss: 0.00004212
Iteration 16/1000 | Loss: 0.00003626
Iteration 17/1000 | Loss: 0.00005831
Iteration 18/1000 | Loss: 0.00004206
Iteration 19/1000 | Loss: 0.00004136
Iteration 20/1000 | Loss: 0.00003948
Iteration 21/1000 | Loss: 0.00004363
Iteration 22/1000 | Loss: 0.00004495
Iteration 23/1000 | Loss: 0.00006354
Iteration 24/1000 | Loss: 0.00003952
Iteration 25/1000 | Loss: 0.00047809
Iteration 26/1000 | Loss: 0.00089302
Iteration 27/1000 | Loss: 0.00003020
Iteration 28/1000 | Loss: 0.00002433
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001101
Iteration 54/1000 | Loss: 0.00001101
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001097
Iteration 61/1000 | Loss: 0.00001097
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001092
Iteration 82/1000 | Loss: 0.00001092
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001091
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001087
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001087
Iteration 148/1000 | Loss: 0.00001087
Iteration 149/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.0865501280932222e-05, 1.0865501280932222e-05, 1.0865501280932222e-05, 1.0865501280932222e-05, 1.0865501280932222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0865501280932222e-05

Optimization complete. Final v2v error: 2.8159565925598145 mm

Highest mean error: 3.4720702171325684 mm for frame 74

Lowest mean error: 2.572780132293701 mm for frame 111

Saving results

Total time: 110.04835200309753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383446
Iteration 2/25 | Loss: 0.00120247
Iteration 3/25 | Loss: 0.00110297
Iteration 4/25 | Loss: 0.00108240
Iteration 5/25 | Loss: 0.00107438
Iteration 6/25 | Loss: 0.00107223
Iteration 7/25 | Loss: 0.00107163
Iteration 8/25 | Loss: 0.00107153
Iteration 9/25 | Loss: 0.00107153
Iteration 10/25 | Loss: 0.00107153
Iteration 11/25 | Loss: 0.00107153
Iteration 12/25 | Loss: 0.00107153
Iteration 13/25 | Loss: 0.00107153
Iteration 14/25 | Loss: 0.00107153
Iteration 15/25 | Loss: 0.00107153
Iteration 16/25 | Loss: 0.00107153
Iteration 17/25 | Loss: 0.00107153
Iteration 18/25 | Loss: 0.00107153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010715347016230226, 0.0010715347016230226, 0.0010715347016230226, 0.0010715347016230226, 0.0010715347016230226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010715347016230226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41774476
Iteration 2/25 | Loss: 0.00104498
Iteration 3/25 | Loss: 0.00104498
Iteration 4/25 | Loss: 0.00104498
Iteration 5/25 | Loss: 0.00104498
Iteration 6/25 | Loss: 0.00104498
Iteration 7/25 | Loss: 0.00104498
Iteration 8/25 | Loss: 0.00104498
Iteration 9/25 | Loss: 0.00104498
Iteration 10/25 | Loss: 0.00104498
Iteration 11/25 | Loss: 0.00104498
Iteration 12/25 | Loss: 0.00104498
Iteration 13/25 | Loss: 0.00104498
Iteration 14/25 | Loss: 0.00104498
Iteration 15/25 | Loss: 0.00104498
Iteration 16/25 | Loss: 0.00104498
Iteration 17/25 | Loss: 0.00104498
Iteration 18/25 | Loss: 0.00104498
Iteration 19/25 | Loss: 0.00104498
Iteration 20/25 | Loss: 0.00104498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010449809487909079, 0.0010449809487909079, 0.0010449809487909079, 0.0010449809487909079, 0.0010449809487909079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010449809487909079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104498
Iteration 2/1000 | Loss: 0.00005384
Iteration 3/1000 | Loss: 0.00003639
Iteration 4/1000 | Loss: 0.00002741
Iteration 5/1000 | Loss: 0.00002389
Iteration 6/1000 | Loss: 0.00002234
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00001898
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001826
Iteration 13/1000 | Loss: 0.00001806
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001766
Iteration 17/1000 | Loss: 0.00001765
Iteration 18/1000 | Loss: 0.00001764
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001749
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001744
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001741
Iteration 34/1000 | Loss: 0.00001741
Iteration 35/1000 | Loss: 0.00001740
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001739
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001737
Iteration 44/1000 | Loss: 0.00001737
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001735
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001734
Iteration 53/1000 | Loss: 0.00001734
Iteration 54/1000 | Loss: 0.00001734
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001733
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001728
Iteration 95/1000 | Loss: 0.00001728
Iteration 96/1000 | Loss: 0.00001728
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001727
Iteration 106/1000 | Loss: 0.00001727
Iteration 107/1000 | Loss: 0.00001727
Iteration 108/1000 | Loss: 0.00001727
Iteration 109/1000 | Loss: 0.00001727
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001726
Iteration 114/1000 | Loss: 0.00001726
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001725
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001725
Iteration 123/1000 | Loss: 0.00001725
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001724
Iteration 128/1000 | Loss: 0.00001724
Iteration 129/1000 | Loss: 0.00001724
Iteration 130/1000 | Loss: 0.00001724
Iteration 131/1000 | Loss: 0.00001724
Iteration 132/1000 | Loss: 0.00001723
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001722
Iteration 142/1000 | Loss: 0.00001722
Iteration 143/1000 | Loss: 0.00001722
Iteration 144/1000 | Loss: 0.00001722
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001722
Iteration 156/1000 | Loss: 0.00001722
Iteration 157/1000 | Loss: 0.00001722
Iteration 158/1000 | Loss: 0.00001722
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Iteration 165/1000 | Loss: 0.00001722
Iteration 166/1000 | Loss: 0.00001722
Iteration 167/1000 | Loss: 0.00001722
Iteration 168/1000 | Loss: 0.00001722
Iteration 169/1000 | Loss: 0.00001722
Iteration 170/1000 | Loss: 0.00001722
Iteration 171/1000 | Loss: 0.00001722
Iteration 172/1000 | Loss: 0.00001722
Iteration 173/1000 | Loss: 0.00001722
Iteration 174/1000 | Loss: 0.00001722
Iteration 175/1000 | Loss: 0.00001722
Iteration 176/1000 | Loss: 0.00001722
Iteration 177/1000 | Loss: 0.00001722
Iteration 178/1000 | Loss: 0.00001722
Iteration 179/1000 | Loss: 0.00001722
Iteration 180/1000 | Loss: 0.00001722
Iteration 181/1000 | Loss: 0.00001722
Iteration 182/1000 | Loss: 0.00001722
Iteration 183/1000 | Loss: 0.00001722
Iteration 184/1000 | Loss: 0.00001722
Iteration 185/1000 | Loss: 0.00001722
Iteration 186/1000 | Loss: 0.00001722
Iteration 187/1000 | Loss: 0.00001722
Iteration 188/1000 | Loss: 0.00001722
Iteration 189/1000 | Loss: 0.00001722
Iteration 190/1000 | Loss: 0.00001722
Iteration 191/1000 | Loss: 0.00001722
Iteration 192/1000 | Loss: 0.00001722
Iteration 193/1000 | Loss: 0.00001722
Iteration 194/1000 | Loss: 0.00001722
Iteration 195/1000 | Loss: 0.00001722
Iteration 196/1000 | Loss: 0.00001722
Iteration 197/1000 | Loss: 0.00001722
Iteration 198/1000 | Loss: 0.00001722
Iteration 199/1000 | Loss: 0.00001722
Iteration 200/1000 | Loss: 0.00001722
Iteration 201/1000 | Loss: 0.00001722
Iteration 202/1000 | Loss: 0.00001722
Iteration 203/1000 | Loss: 0.00001722
Iteration 204/1000 | Loss: 0.00001722
Iteration 205/1000 | Loss: 0.00001722
Iteration 206/1000 | Loss: 0.00001722
Iteration 207/1000 | Loss: 0.00001722
Iteration 208/1000 | Loss: 0.00001722
Iteration 209/1000 | Loss: 0.00001722
Iteration 210/1000 | Loss: 0.00001722
Iteration 211/1000 | Loss: 0.00001722
Iteration 212/1000 | Loss: 0.00001722
Iteration 213/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.7216116248164326e-05, 1.7216116248164326e-05, 1.7216116248164326e-05, 1.7216116248164326e-05, 1.7216116248164326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7216116248164326e-05

Optimization complete. Final v2v error: 3.3642077445983887 mm

Highest mean error: 4.760885238647461 mm for frame 105

Lowest mean error: 2.2930240631103516 mm for frame 64

Saving results

Total time: 45.33990263938904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971134
Iteration 2/25 | Loss: 0.00169972
Iteration 3/25 | Loss: 0.00126297
Iteration 4/25 | Loss: 0.00121287
Iteration 5/25 | Loss: 0.00122792
Iteration 6/25 | Loss: 0.00122351
Iteration 7/25 | Loss: 0.00116073
Iteration 8/25 | Loss: 0.00116024
Iteration 9/25 | Loss: 0.00113932
Iteration 10/25 | Loss: 0.00113100
Iteration 11/25 | Loss: 0.00112352
Iteration 12/25 | Loss: 0.00111661
Iteration 13/25 | Loss: 0.00111584
Iteration 14/25 | Loss: 0.00111407
Iteration 15/25 | Loss: 0.00110789
Iteration 16/25 | Loss: 0.00110648
Iteration 17/25 | Loss: 0.00110766
Iteration 18/25 | Loss: 0.00110301
Iteration 19/25 | Loss: 0.00110261
Iteration 20/25 | Loss: 0.00110303
Iteration 21/25 | Loss: 0.00110227
Iteration 22/25 | Loss: 0.00110222
Iteration 23/25 | Loss: 0.00110222
Iteration 24/25 | Loss: 0.00110222
Iteration 25/25 | Loss: 0.00110222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45796859
Iteration 2/25 | Loss: 0.00111756
Iteration 3/25 | Loss: 0.00111756
Iteration 4/25 | Loss: 0.00109671
Iteration 5/25 | Loss: 0.00109671
Iteration 6/25 | Loss: 0.00109670
Iteration 7/25 | Loss: 0.00109670
Iteration 8/25 | Loss: 0.00109670
Iteration 9/25 | Loss: 0.00109670
Iteration 10/25 | Loss: 0.00109670
Iteration 11/25 | Loss: 0.00109670
Iteration 12/25 | Loss: 0.00109670
Iteration 13/25 | Loss: 0.00109670
Iteration 14/25 | Loss: 0.00109670
Iteration 15/25 | Loss: 0.00109670
Iteration 16/25 | Loss: 0.00109670
Iteration 17/25 | Loss: 0.00109670
Iteration 18/25 | Loss: 0.00109670
Iteration 19/25 | Loss: 0.00109670
Iteration 20/25 | Loss: 0.00109670
Iteration 21/25 | Loss: 0.00109670
Iteration 22/25 | Loss: 0.00109670
Iteration 23/25 | Loss: 0.00109670
Iteration 24/25 | Loss: 0.00109670
Iteration 25/25 | Loss: 0.00109670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109670
Iteration 2/1000 | Loss: 0.00005409
Iteration 3/1000 | Loss: 0.00006755
Iteration 4/1000 | Loss: 0.00002710
Iteration 5/1000 | Loss: 0.00004337
Iteration 6/1000 | Loss: 0.00013128
Iteration 7/1000 | Loss: 0.00004518
Iteration 8/1000 | Loss: 0.00008840
Iteration 9/1000 | Loss: 0.00006052
Iteration 10/1000 | Loss: 0.00020086
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002800
Iteration 13/1000 | Loss: 0.00008417
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00005560
Iteration 17/1000 | Loss: 0.00001906
Iteration 18/1000 | Loss: 0.00004138
Iteration 19/1000 | Loss: 0.00001895
Iteration 20/1000 | Loss: 0.00012946
Iteration 21/1000 | Loss: 0.00013018
Iteration 22/1000 | Loss: 0.00004515
Iteration 23/1000 | Loss: 0.00006460
Iteration 24/1000 | Loss: 0.00023274
Iteration 25/1000 | Loss: 0.00011047
Iteration 26/1000 | Loss: 0.00006518
Iteration 27/1000 | Loss: 0.00005152
Iteration 28/1000 | Loss: 0.00005262
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00004244
Iteration 32/1000 | Loss: 0.00001425
Iteration 33/1000 | Loss: 0.00003196
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00003992
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00004390
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00002501
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00003250
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001233
Iteration 45/1000 | Loss: 0.00005171
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00003655
Iteration 48/1000 | Loss: 0.00001225
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001187
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001187
Iteration 91/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.1871401511598378e-05, 1.1871401511598378e-05, 1.1871401511598378e-05, 1.1871401511598378e-05, 1.1871401511598378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1871401511598378e-05

Optimization complete. Final v2v error: 2.922956705093384 mm

Highest mean error: 4.663276672363281 mm for frame 58

Lowest mean error: 2.300781011581421 mm for frame 97

Saving results

Total time: 114.41843247413635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404775
Iteration 2/25 | Loss: 0.00114653
Iteration 3/25 | Loss: 0.00110036
Iteration 4/25 | Loss: 0.00109294
Iteration 5/25 | Loss: 0.00109022
Iteration 6/25 | Loss: 0.00108979
Iteration 7/25 | Loss: 0.00108979
Iteration 8/25 | Loss: 0.00108979
Iteration 9/25 | Loss: 0.00108979
Iteration 10/25 | Loss: 0.00108979
Iteration 11/25 | Loss: 0.00108979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010897930478677154, 0.0010897930478677154, 0.0010897930478677154, 0.0010897930478677154, 0.0010897930478677154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010897930478677154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39191997
Iteration 2/25 | Loss: 0.00106454
Iteration 3/25 | Loss: 0.00106454
Iteration 4/25 | Loss: 0.00106454
Iteration 5/25 | Loss: 0.00106454
Iteration 6/25 | Loss: 0.00106454
Iteration 7/25 | Loss: 0.00106454
Iteration 8/25 | Loss: 0.00106454
Iteration 9/25 | Loss: 0.00106454
Iteration 10/25 | Loss: 0.00106454
Iteration 11/25 | Loss: 0.00106454
Iteration 12/25 | Loss: 0.00106454
Iteration 13/25 | Loss: 0.00106454
Iteration 14/25 | Loss: 0.00106454
Iteration 15/25 | Loss: 0.00106454
Iteration 16/25 | Loss: 0.00106454
Iteration 17/25 | Loss: 0.00106454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001064539304934442, 0.001064539304934442, 0.001064539304934442, 0.001064539304934442, 0.001064539304934442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001064539304934442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106454
Iteration 2/1000 | Loss: 0.00002363
Iteration 3/1000 | Loss: 0.00001538
Iteration 4/1000 | Loss: 0.00001323
Iteration 5/1000 | Loss: 0.00001259
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001201
Iteration 8/1000 | Loss: 0.00001178
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001134
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001113
Iteration 25/1000 | Loss: 0.00001113
Iteration 26/1000 | Loss: 0.00001109
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001102
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001102
Iteration 35/1000 | Loss: 0.00001102
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001100
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001099
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001098
Iteration 53/1000 | Loss: 0.00001098
Iteration 54/1000 | Loss: 0.00001098
Iteration 55/1000 | Loss: 0.00001098
Iteration 56/1000 | Loss: 0.00001097
Iteration 57/1000 | Loss: 0.00001097
Iteration 58/1000 | Loss: 0.00001097
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001095
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001092
Iteration 73/1000 | Loss: 0.00001092
Iteration 74/1000 | Loss: 0.00001092
Iteration 75/1000 | Loss: 0.00001091
Iteration 76/1000 | Loss: 0.00001091
Iteration 77/1000 | Loss: 0.00001091
Iteration 78/1000 | Loss: 0.00001090
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001090
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001087
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001087
Iteration 95/1000 | Loss: 0.00001087
Iteration 96/1000 | Loss: 0.00001087
Iteration 97/1000 | Loss: 0.00001087
Iteration 98/1000 | Loss: 0.00001086
Iteration 99/1000 | Loss: 0.00001086
Iteration 100/1000 | Loss: 0.00001086
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001085
Iteration 103/1000 | Loss: 0.00001085
Iteration 104/1000 | Loss: 0.00001085
Iteration 105/1000 | Loss: 0.00001085
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001085
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001084
Iteration 120/1000 | Loss: 0.00001083
Iteration 121/1000 | Loss: 0.00001083
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001082
Iteration 124/1000 | Loss: 0.00001082
Iteration 125/1000 | Loss: 0.00001082
Iteration 126/1000 | Loss: 0.00001082
Iteration 127/1000 | Loss: 0.00001082
Iteration 128/1000 | Loss: 0.00001082
Iteration 129/1000 | Loss: 0.00001082
Iteration 130/1000 | Loss: 0.00001082
Iteration 131/1000 | Loss: 0.00001081
Iteration 132/1000 | Loss: 0.00001081
Iteration 133/1000 | Loss: 0.00001081
Iteration 134/1000 | Loss: 0.00001081
Iteration 135/1000 | Loss: 0.00001081
Iteration 136/1000 | Loss: 0.00001081
Iteration 137/1000 | Loss: 0.00001081
Iteration 138/1000 | Loss: 0.00001081
Iteration 139/1000 | Loss: 0.00001081
Iteration 140/1000 | Loss: 0.00001080
Iteration 141/1000 | Loss: 0.00001080
Iteration 142/1000 | Loss: 0.00001080
Iteration 143/1000 | Loss: 0.00001080
Iteration 144/1000 | Loss: 0.00001079
Iteration 145/1000 | Loss: 0.00001079
Iteration 146/1000 | Loss: 0.00001079
Iteration 147/1000 | Loss: 0.00001079
Iteration 148/1000 | Loss: 0.00001079
Iteration 149/1000 | Loss: 0.00001078
Iteration 150/1000 | Loss: 0.00001078
Iteration 151/1000 | Loss: 0.00001078
Iteration 152/1000 | Loss: 0.00001078
Iteration 153/1000 | Loss: 0.00001078
Iteration 154/1000 | Loss: 0.00001078
Iteration 155/1000 | Loss: 0.00001078
Iteration 156/1000 | Loss: 0.00001078
Iteration 157/1000 | Loss: 0.00001078
Iteration 158/1000 | Loss: 0.00001078
Iteration 159/1000 | Loss: 0.00001078
Iteration 160/1000 | Loss: 0.00001078
Iteration 161/1000 | Loss: 0.00001078
Iteration 162/1000 | Loss: 0.00001078
Iteration 163/1000 | Loss: 0.00001078
Iteration 164/1000 | Loss: 0.00001078
Iteration 165/1000 | Loss: 0.00001078
Iteration 166/1000 | Loss: 0.00001078
Iteration 167/1000 | Loss: 0.00001078
Iteration 168/1000 | Loss: 0.00001078
Iteration 169/1000 | Loss: 0.00001078
Iteration 170/1000 | Loss: 0.00001078
Iteration 171/1000 | Loss: 0.00001078
Iteration 172/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.0781317541841418e-05, 1.0781317541841418e-05, 1.0781317541841418e-05, 1.0781317541841418e-05, 1.0781317541841418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0781317541841418e-05

Optimization complete. Final v2v error: 2.7839279174804688 mm

Highest mean error: 2.8846817016601562 mm for frame 9

Lowest mean error: 2.6700713634490967 mm for frame 103

Saving results

Total time: 35.96995544433594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496654
Iteration 2/25 | Loss: 0.00126719
Iteration 3/25 | Loss: 0.00116426
Iteration 4/25 | Loss: 0.00114826
Iteration 5/25 | Loss: 0.00114382
Iteration 6/25 | Loss: 0.00114212
Iteration 7/25 | Loss: 0.00114415
Iteration 8/25 | Loss: 0.00114283
Iteration 9/25 | Loss: 0.00114071
Iteration 10/25 | Loss: 0.00113992
Iteration 11/25 | Loss: 0.00113816
Iteration 12/25 | Loss: 0.00113777
Iteration 13/25 | Loss: 0.00113771
Iteration 14/25 | Loss: 0.00113770
Iteration 15/25 | Loss: 0.00113770
Iteration 16/25 | Loss: 0.00113770
Iteration 17/25 | Loss: 0.00113770
Iteration 18/25 | Loss: 0.00113770
Iteration 19/25 | Loss: 0.00113770
Iteration 20/25 | Loss: 0.00113770
Iteration 21/25 | Loss: 0.00113770
Iteration 22/25 | Loss: 0.00113770
Iteration 23/25 | Loss: 0.00113770
Iteration 24/25 | Loss: 0.00113769
Iteration 25/25 | Loss: 0.00113769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38824928
Iteration 2/25 | Loss: 0.00115828
Iteration 3/25 | Loss: 0.00115828
Iteration 4/25 | Loss: 0.00115828
Iteration 5/25 | Loss: 0.00115828
Iteration 6/25 | Loss: 0.00115827
Iteration 7/25 | Loss: 0.00115827
Iteration 8/25 | Loss: 0.00115827
Iteration 9/25 | Loss: 0.00115827
Iteration 10/25 | Loss: 0.00115827
Iteration 11/25 | Loss: 0.00115827
Iteration 12/25 | Loss: 0.00115827
Iteration 13/25 | Loss: 0.00115827
Iteration 14/25 | Loss: 0.00115827
Iteration 15/25 | Loss: 0.00115827
Iteration 16/25 | Loss: 0.00115827
Iteration 17/25 | Loss: 0.00115827
Iteration 18/25 | Loss: 0.00115827
Iteration 19/25 | Loss: 0.00115827
Iteration 20/25 | Loss: 0.00115827
Iteration 21/25 | Loss: 0.00115827
Iteration 22/25 | Loss: 0.00115827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011582730803638697, 0.0011582730803638697, 0.0011582730803638697, 0.0011582730803638697, 0.0011582730803638697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011582730803638697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115827
Iteration 2/1000 | Loss: 0.00004961
Iteration 3/1000 | Loss: 0.00003369
Iteration 4/1000 | Loss: 0.00002506
Iteration 5/1000 | Loss: 0.00002287
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002110
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001965
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001932
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001929
Iteration 21/1000 | Loss: 0.00001929
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001928
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001925
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001922
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001920
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001919
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001918
Iteration 50/1000 | Loss: 0.00001918
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001917
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001914
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001912
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001911
Iteration 75/1000 | Loss: 0.00001911
Iteration 76/1000 | Loss: 0.00001911
Iteration 77/1000 | Loss: 0.00001911
Iteration 78/1000 | Loss: 0.00001911
Iteration 79/1000 | Loss: 0.00001911
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001910
Iteration 84/1000 | Loss: 0.00001910
Iteration 85/1000 | Loss: 0.00001910
Iteration 86/1000 | Loss: 0.00001909
Iteration 87/1000 | Loss: 0.00001909
Iteration 88/1000 | Loss: 0.00001909
Iteration 89/1000 | Loss: 0.00001909
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001908
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001908
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001907
Iteration 110/1000 | Loss: 0.00001907
Iteration 111/1000 | Loss: 0.00001907
Iteration 112/1000 | Loss: 0.00001907
Iteration 113/1000 | Loss: 0.00001907
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001906
Iteration 119/1000 | Loss: 0.00001906
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001905
Iteration 129/1000 | Loss: 0.00001905
Iteration 130/1000 | Loss: 0.00001905
Iteration 131/1000 | Loss: 0.00001905
Iteration 132/1000 | Loss: 0.00001905
Iteration 133/1000 | Loss: 0.00001905
Iteration 134/1000 | Loss: 0.00001905
Iteration 135/1000 | Loss: 0.00001905
Iteration 136/1000 | Loss: 0.00001905
Iteration 137/1000 | Loss: 0.00001905
Iteration 138/1000 | Loss: 0.00001905
Iteration 139/1000 | Loss: 0.00001905
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001904
Iteration 142/1000 | Loss: 0.00001904
Iteration 143/1000 | Loss: 0.00001904
Iteration 144/1000 | Loss: 0.00001904
Iteration 145/1000 | Loss: 0.00001904
Iteration 146/1000 | Loss: 0.00001904
Iteration 147/1000 | Loss: 0.00001904
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001903
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001903
Iteration 153/1000 | Loss: 0.00001903
Iteration 154/1000 | Loss: 0.00001903
Iteration 155/1000 | Loss: 0.00001903
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001901
Iteration 162/1000 | Loss: 0.00001901
Iteration 163/1000 | Loss: 0.00001901
Iteration 164/1000 | Loss: 0.00001901
Iteration 165/1000 | Loss: 0.00001901
Iteration 166/1000 | Loss: 0.00001901
Iteration 167/1000 | Loss: 0.00001901
Iteration 168/1000 | Loss: 0.00001901
Iteration 169/1000 | Loss: 0.00001901
Iteration 170/1000 | Loss: 0.00001901
Iteration 171/1000 | Loss: 0.00001901
Iteration 172/1000 | Loss: 0.00001900
Iteration 173/1000 | Loss: 0.00001900
Iteration 174/1000 | Loss: 0.00001900
Iteration 175/1000 | Loss: 0.00001900
Iteration 176/1000 | Loss: 0.00001900
Iteration 177/1000 | Loss: 0.00001900
Iteration 178/1000 | Loss: 0.00001900
Iteration 179/1000 | Loss: 0.00001900
Iteration 180/1000 | Loss: 0.00001900
Iteration 181/1000 | Loss: 0.00001899
Iteration 182/1000 | Loss: 0.00001899
Iteration 183/1000 | Loss: 0.00001899
Iteration 184/1000 | Loss: 0.00001899
Iteration 185/1000 | Loss: 0.00001899
Iteration 186/1000 | Loss: 0.00001899
Iteration 187/1000 | Loss: 0.00001899
Iteration 188/1000 | Loss: 0.00001899
Iteration 189/1000 | Loss: 0.00001899
Iteration 190/1000 | Loss: 0.00001899
Iteration 191/1000 | Loss: 0.00001899
Iteration 192/1000 | Loss: 0.00001898
Iteration 193/1000 | Loss: 0.00001898
Iteration 194/1000 | Loss: 0.00001898
Iteration 195/1000 | Loss: 0.00001898
Iteration 196/1000 | Loss: 0.00001898
Iteration 197/1000 | Loss: 0.00001898
Iteration 198/1000 | Loss: 0.00001898
Iteration 199/1000 | Loss: 0.00001898
Iteration 200/1000 | Loss: 0.00001898
Iteration 201/1000 | Loss: 0.00001898
Iteration 202/1000 | Loss: 0.00001898
Iteration 203/1000 | Loss: 0.00001898
Iteration 204/1000 | Loss: 0.00001898
Iteration 205/1000 | Loss: 0.00001897
Iteration 206/1000 | Loss: 0.00001897
Iteration 207/1000 | Loss: 0.00001897
Iteration 208/1000 | Loss: 0.00001897
Iteration 209/1000 | Loss: 0.00001897
Iteration 210/1000 | Loss: 0.00001897
Iteration 211/1000 | Loss: 0.00001897
Iteration 212/1000 | Loss: 0.00001897
Iteration 213/1000 | Loss: 0.00001897
Iteration 214/1000 | Loss: 0.00001897
Iteration 215/1000 | Loss: 0.00001897
Iteration 216/1000 | Loss: 0.00001897
Iteration 217/1000 | Loss: 0.00001897
Iteration 218/1000 | Loss: 0.00001896
Iteration 219/1000 | Loss: 0.00001896
Iteration 220/1000 | Loss: 0.00001896
Iteration 221/1000 | Loss: 0.00001896
Iteration 222/1000 | Loss: 0.00001896
Iteration 223/1000 | Loss: 0.00001896
Iteration 224/1000 | Loss: 0.00001895
Iteration 225/1000 | Loss: 0.00001895
Iteration 226/1000 | Loss: 0.00001895
Iteration 227/1000 | Loss: 0.00001895
Iteration 228/1000 | Loss: 0.00001895
Iteration 229/1000 | Loss: 0.00001895
Iteration 230/1000 | Loss: 0.00001895
Iteration 231/1000 | Loss: 0.00001895
Iteration 232/1000 | Loss: 0.00001895
Iteration 233/1000 | Loss: 0.00001895
Iteration 234/1000 | Loss: 0.00001895
Iteration 235/1000 | Loss: 0.00001895
Iteration 236/1000 | Loss: 0.00001895
Iteration 237/1000 | Loss: 0.00001895
Iteration 238/1000 | Loss: 0.00001895
Iteration 239/1000 | Loss: 0.00001895
Iteration 240/1000 | Loss: 0.00001895
Iteration 241/1000 | Loss: 0.00001895
Iteration 242/1000 | Loss: 0.00001895
Iteration 243/1000 | Loss: 0.00001895
Iteration 244/1000 | Loss: 0.00001895
Iteration 245/1000 | Loss: 0.00001895
Iteration 246/1000 | Loss: 0.00001894
Iteration 247/1000 | Loss: 0.00001894
Iteration 248/1000 | Loss: 0.00001894
Iteration 249/1000 | Loss: 0.00001894
Iteration 250/1000 | Loss: 0.00001894
Iteration 251/1000 | Loss: 0.00001894
Iteration 252/1000 | Loss: 0.00001894
Iteration 253/1000 | Loss: 0.00001894
Iteration 254/1000 | Loss: 0.00001894
Iteration 255/1000 | Loss: 0.00001894
Iteration 256/1000 | Loss: 0.00001894
Iteration 257/1000 | Loss: 0.00001894
Iteration 258/1000 | Loss: 0.00001894
Iteration 259/1000 | Loss: 0.00001894
Iteration 260/1000 | Loss: 0.00001894
Iteration 261/1000 | Loss: 0.00001894
Iteration 262/1000 | Loss: 0.00001894
Iteration 263/1000 | Loss: 0.00001893
Iteration 264/1000 | Loss: 0.00001893
Iteration 265/1000 | Loss: 0.00001893
Iteration 266/1000 | Loss: 0.00001893
Iteration 267/1000 | Loss: 0.00001893
Iteration 268/1000 | Loss: 0.00001893
Iteration 269/1000 | Loss: 0.00001893
Iteration 270/1000 | Loss: 0.00001893
Iteration 271/1000 | Loss: 0.00001893
Iteration 272/1000 | Loss: 0.00001893
Iteration 273/1000 | Loss: 0.00001893
Iteration 274/1000 | Loss: 0.00001893
Iteration 275/1000 | Loss: 0.00001892
Iteration 276/1000 | Loss: 0.00001892
Iteration 277/1000 | Loss: 0.00001892
Iteration 278/1000 | Loss: 0.00001892
Iteration 279/1000 | Loss: 0.00001892
Iteration 280/1000 | Loss: 0.00001892
Iteration 281/1000 | Loss: 0.00001892
Iteration 282/1000 | Loss: 0.00001892
Iteration 283/1000 | Loss: 0.00001892
Iteration 284/1000 | Loss: 0.00001892
Iteration 285/1000 | Loss: 0.00001892
Iteration 286/1000 | Loss: 0.00001892
Iteration 287/1000 | Loss: 0.00001892
Iteration 288/1000 | Loss: 0.00001892
Iteration 289/1000 | Loss: 0.00001892
Iteration 290/1000 | Loss: 0.00001892
Iteration 291/1000 | Loss: 0.00001892
Iteration 292/1000 | Loss: 0.00001892
Iteration 293/1000 | Loss: 0.00001892
Iteration 294/1000 | Loss: 0.00001892
Iteration 295/1000 | Loss: 0.00001892
Iteration 296/1000 | Loss: 0.00001892
Iteration 297/1000 | Loss: 0.00001892
Iteration 298/1000 | Loss: 0.00001892
Iteration 299/1000 | Loss: 0.00001892
Iteration 300/1000 | Loss: 0.00001892
Iteration 301/1000 | Loss: 0.00001892
Iteration 302/1000 | Loss: 0.00001892
Iteration 303/1000 | Loss: 0.00001892
Iteration 304/1000 | Loss: 0.00001892
Iteration 305/1000 | Loss: 0.00001892
Iteration 306/1000 | Loss: 0.00001892
Iteration 307/1000 | Loss: 0.00001892
Iteration 308/1000 | Loss: 0.00001892
Iteration 309/1000 | Loss: 0.00001892
Iteration 310/1000 | Loss: 0.00001892
Iteration 311/1000 | Loss: 0.00001892
Iteration 312/1000 | Loss: 0.00001892
Iteration 313/1000 | Loss: 0.00001892
Iteration 314/1000 | Loss: 0.00001892
Iteration 315/1000 | Loss: 0.00001892
Iteration 316/1000 | Loss: 0.00001892
Iteration 317/1000 | Loss: 0.00001892
Iteration 318/1000 | Loss: 0.00001892
Iteration 319/1000 | Loss: 0.00001892
Iteration 320/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [1.891543979581911e-05, 1.891543979581911e-05, 1.891543979581911e-05, 1.891543979581911e-05, 1.891543979581911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.891543979581911e-05

Optimization complete. Final v2v error: 3.519202470779419 mm

Highest mean error: 4.757311820983887 mm for frame 94

Lowest mean error: 2.5353400707244873 mm for frame 49

Saving results

Total time: 59.1667218208313
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809110
Iteration 2/25 | Loss: 0.00115838
Iteration 3/25 | Loss: 0.00106823
Iteration 4/25 | Loss: 0.00106322
Iteration 5/25 | Loss: 0.00106204
Iteration 6/25 | Loss: 0.00106204
Iteration 7/25 | Loss: 0.00106204
Iteration 8/25 | Loss: 0.00106204
Iteration 9/25 | Loss: 0.00106204
Iteration 10/25 | Loss: 0.00106204
Iteration 11/25 | Loss: 0.00106204
Iteration 12/25 | Loss: 0.00106204
Iteration 13/25 | Loss: 0.00106204
Iteration 14/25 | Loss: 0.00106204
Iteration 15/25 | Loss: 0.00106204
Iteration 16/25 | Loss: 0.00106204
Iteration 17/25 | Loss: 0.00106204
Iteration 18/25 | Loss: 0.00106204
Iteration 19/25 | Loss: 0.00106204
Iteration 20/25 | Loss: 0.00106204
Iteration 21/25 | Loss: 0.00106204
Iteration 22/25 | Loss: 0.00106204
Iteration 23/25 | Loss: 0.00106204
Iteration 24/25 | Loss: 0.00106204
Iteration 25/25 | Loss: 0.00106204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35721922
Iteration 2/25 | Loss: 0.00076509
Iteration 3/25 | Loss: 0.00076509
Iteration 4/25 | Loss: 0.00076509
Iteration 5/25 | Loss: 0.00076509
Iteration 6/25 | Loss: 0.00076509
Iteration 7/25 | Loss: 0.00076509
Iteration 8/25 | Loss: 0.00076509
Iteration 9/25 | Loss: 0.00076509
Iteration 10/25 | Loss: 0.00076509
Iteration 11/25 | Loss: 0.00076509
Iteration 12/25 | Loss: 0.00076509
Iteration 13/25 | Loss: 0.00076509
Iteration 14/25 | Loss: 0.00076509
Iteration 15/25 | Loss: 0.00076508
Iteration 16/25 | Loss: 0.00076508
Iteration 17/25 | Loss: 0.00076508
Iteration 18/25 | Loss: 0.00076508
Iteration 19/25 | Loss: 0.00076508
Iteration 20/25 | Loss: 0.00076508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007650849875062704, 0.0007650849875062704, 0.0007650849875062704, 0.0007650849875062704, 0.0007650849875062704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007650849875062704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076508
Iteration 2/1000 | Loss: 0.00002037
Iteration 3/1000 | Loss: 0.00001257
Iteration 4/1000 | Loss: 0.00001141
Iteration 5/1000 | Loss: 0.00001062
Iteration 6/1000 | Loss: 0.00001010
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000970
Iteration 9/1000 | Loss: 0.00000960
Iteration 10/1000 | Loss: 0.00000946
Iteration 11/1000 | Loss: 0.00000931
Iteration 12/1000 | Loss: 0.00000926
Iteration 13/1000 | Loss: 0.00000924
Iteration 14/1000 | Loss: 0.00000924
Iteration 15/1000 | Loss: 0.00000923
Iteration 16/1000 | Loss: 0.00000922
Iteration 17/1000 | Loss: 0.00000922
Iteration 18/1000 | Loss: 0.00000921
Iteration 19/1000 | Loss: 0.00000921
Iteration 20/1000 | Loss: 0.00000920
Iteration 21/1000 | Loss: 0.00000920
Iteration 22/1000 | Loss: 0.00000919
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000914
Iteration 27/1000 | Loss: 0.00000913
Iteration 28/1000 | Loss: 0.00000912
Iteration 29/1000 | Loss: 0.00000910
Iteration 30/1000 | Loss: 0.00000909
Iteration 31/1000 | Loss: 0.00000909
Iteration 32/1000 | Loss: 0.00000908
Iteration 33/1000 | Loss: 0.00000908
Iteration 34/1000 | Loss: 0.00000908
Iteration 35/1000 | Loss: 0.00000908
Iteration 36/1000 | Loss: 0.00000906
Iteration 37/1000 | Loss: 0.00000905
Iteration 38/1000 | Loss: 0.00000903
Iteration 39/1000 | Loss: 0.00000901
Iteration 40/1000 | Loss: 0.00000900
Iteration 41/1000 | Loss: 0.00000899
Iteration 42/1000 | Loss: 0.00000899
Iteration 43/1000 | Loss: 0.00000899
Iteration 44/1000 | Loss: 0.00000898
Iteration 45/1000 | Loss: 0.00000898
Iteration 46/1000 | Loss: 0.00000898
Iteration 47/1000 | Loss: 0.00000897
Iteration 48/1000 | Loss: 0.00000897
Iteration 49/1000 | Loss: 0.00000897
Iteration 50/1000 | Loss: 0.00000897
Iteration 51/1000 | Loss: 0.00000896
Iteration 52/1000 | Loss: 0.00000896
Iteration 53/1000 | Loss: 0.00000895
Iteration 54/1000 | Loss: 0.00000895
Iteration 55/1000 | Loss: 0.00000894
Iteration 56/1000 | Loss: 0.00000894
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000891
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000891
Iteration 61/1000 | Loss: 0.00000891
Iteration 62/1000 | Loss: 0.00000891
Iteration 63/1000 | Loss: 0.00000891
Iteration 64/1000 | Loss: 0.00000891
Iteration 65/1000 | Loss: 0.00000891
Iteration 66/1000 | Loss: 0.00000890
Iteration 67/1000 | Loss: 0.00000890
Iteration 68/1000 | Loss: 0.00000890
Iteration 69/1000 | Loss: 0.00000889
Iteration 70/1000 | Loss: 0.00000889
Iteration 71/1000 | Loss: 0.00000889
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000888
Iteration 74/1000 | Loss: 0.00000888
Iteration 75/1000 | Loss: 0.00000888
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000888
Iteration 78/1000 | Loss: 0.00000887
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000887
Iteration 81/1000 | Loss: 0.00000886
Iteration 82/1000 | Loss: 0.00000885
Iteration 83/1000 | Loss: 0.00000885
Iteration 84/1000 | Loss: 0.00000884
Iteration 85/1000 | Loss: 0.00000884
Iteration 86/1000 | Loss: 0.00000883
Iteration 87/1000 | Loss: 0.00000883
Iteration 88/1000 | Loss: 0.00000882
Iteration 89/1000 | Loss: 0.00000882
Iteration 90/1000 | Loss: 0.00000881
Iteration 91/1000 | Loss: 0.00000880
Iteration 92/1000 | Loss: 0.00000880
Iteration 93/1000 | Loss: 0.00000880
Iteration 94/1000 | Loss: 0.00000879
Iteration 95/1000 | Loss: 0.00000879
Iteration 96/1000 | Loss: 0.00000879
Iteration 97/1000 | Loss: 0.00000879
Iteration 98/1000 | Loss: 0.00000879
Iteration 99/1000 | Loss: 0.00000879
Iteration 100/1000 | Loss: 0.00000879
Iteration 101/1000 | Loss: 0.00000879
Iteration 102/1000 | Loss: 0.00000878
Iteration 103/1000 | Loss: 0.00000878
Iteration 104/1000 | Loss: 0.00000877
Iteration 105/1000 | Loss: 0.00000877
Iteration 106/1000 | Loss: 0.00000877
Iteration 107/1000 | Loss: 0.00000876
Iteration 108/1000 | Loss: 0.00000876
Iteration 109/1000 | Loss: 0.00000876
Iteration 110/1000 | Loss: 0.00000876
Iteration 111/1000 | Loss: 0.00000876
Iteration 112/1000 | Loss: 0.00000876
Iteration 113/1000 | Loss: 0.00000875
Iteration 114/1000 | Loss: 0.00000875
Iteration 115/1000 | Loss: 0.00000875
Iteration 116/1000 | Loss: 0.00000874
Iteration 117/1000 | Loss: 0.00000874
Iteration 118/1000 | Loss: 0.00000874
Iteration 119/1000 | Loss: 0.00000874
Iteration 120/1000 | Loss: 0.00000874
Iteration 121/1000 | Loss: 0.00000873
Iteration 122/1000 | Loss: 0.00000873
Iteration 123/1000 | Loss: 0.00000873
Iteration 124/1000 | Loss: 0.00000873
Iteration 125/1000 | Loss: 0.00000873
Iteration 126/1000 | Loss: 0.00000873
Iteration 127/1000 | Loss: 0.00000872
Iteration 128/1000 | Loss: 0.00000872
Iteration 129/1000 | Loss: 0.00000872
Iteration 130/1000 | Loss: 0.00000872
Iteration 131/1000 | Loss: 0.00000872
Iteration 132/1000 | Loss: 0.00000872
Iteration 133/1000 | Loss: 0.00000872
Iteration 134/1000 | Loss: 0.00000872
Iteration 135/1000 | Loss: 0.00000872
Iteration 136/1000 | Loss: 0.00000872
Iteration 137/1000 | Loss: 0.00000872
Iteration 138/1000 | Loss: 0.00000871
Iteration 139/1000 | Loss: 0.00000871
Iteration 140/1000 | Loss: 0.00000870
Iteration 141/1000 | Loss: 0.00000870
Iteration 142/1000 | Loss: 0.00000870
Iteration 143/1000 | Loss: 0.00000870
Iteration 144/1000 | Loss: 0.00000869
Iteration 145/1000 | Loss: 0.00000869
Iteration 146/1000 | Loss: 0.00000869
Iteration 147/1000 | Loss: 0.00000869
Iteration 148/1000 | Loss: 0.00000869
Iteration 149/1000 | Loss: 0.00000869
Iteration 150/1000 | Loss: 0.00000869
Iteration 151/1000 | Loss: 0.00000869
Iteration 152/1000 | Loss: 0.00000869
Iteration 153/1000 | Loss: 0.00000869
Iteration 154/1000 | Loss: 0.00000869
Iteration 155/1000 | Loss: 0.00000869
Iteration 156/1000 | Loss: 0.00000869
Iteration 157/1000 | Loss: 0.00000869
Iteration 158/1000 | Loss: 0.00000869
Iteration 159/1000 | Loss: 0.00000868
Iteration 160/1000 | Loss: 0.00000868
Iteration 161/1000 | Loss: 0.00000868
Iteration 162/1000 | Loss: 0.00000868
Iteration 163/1000 | Loss: 0.00000868
Iteration 164/1000 | Loss: 0.00000868
Iteration 165/1000 | Loss: 0.00000868
Iteration 166/1000 | Loss: 0.00000868
Iteration 167/1000 | Loss: 0.00000867
Iteration 168/1000 | Loss: 0.00000867
Iteration 169/1000 | Loss: 0.00000867
Iteration 170/1000 | Loss: 0.00000867
Iteration 171/1000 | Loss: 0.00000867
Iteration 172/1000 | Loss: 0.00000867
Iteration 173/1000 | Loss: 0.00000867
Iteration 174/1000 | Loss: 0.00000867
Iteration 175/1000 | Loss: 0.00000867
Iteration 176/1000 | Loss: 0.00000867
Iteration 177/1000 | Loss: 0.00000867
Iteration 178/1000 | Loss: 0.00000867
Iteration 179/1000 | Loss: 0.00000867
Iteration 180/1000 | Loss: 0.00000867
Iteration 181/1000 | Loss: 0.00000867
Iteration 182/1000 | Loss: 0.00000867
Iteration 183/1000 | Loss: 0.00000867
Iteration 184/1000 | Loss: 0.00000867
Iteration 185/1000 | Loss: 0.00000867
Iteration 186/1000 | Loss: 0.00000867
Iteration 187/1000 | Loss: 0.00000867
Iteration 188/1000 | Loss: 0.00000867
Iteration 189/1000 | Loss: 0.00000867
Iteration 190/1000 | Loss: 0.00000867
Iteration 191/1000 | Loss: 0.00000867
Iteration 192/1000 | Loss: 0.00000867
Iteration 193/1000 | Loss: 0.00000867
Iteration 194/1000 | Loss: 0.00000867
Iteration 195/1000 | Loss: 0.00000867
Iteration 196/1000 | Loss: 0.00000867
Iteration 197/1000 | Loss: 0.00000867
Iteration 198/1000 | Loss: 0.00000867
Iteration 199/1000 | Loss: 0.00000867
Iteration 200/1000 | Loss: 0.00000867
Iteration 201/1000 | Loss: 0.00000867
Iteration 202/1000 | Loss: 0.00000867
Iteration 203/1000 | Loss: 0.00000867
Iteration 204/1000 | Loss: 0.00000867
Iteration 205/1000 | Loss: 0.00000867
Iteration 206/1000 | Loss: 0.00000867
Iteration 207/1000 | Loss: 0.00000867
Iteration 208/1000 | Loss: 0.00000867
Iteration 209/1000 | Loss: 0.00000867
Iteration 210/1000 | Loss: 0.00000867
Iteration 211/1000 | Loss: 0.00000867
Iteration 212/1000 | Loss: 0.00000867
Iteration 213/1000 | Loss: 0.00000867
Iteration 214/1000 | Loss: 0.00000867
Iteration 215/1000 | Loss: 0.00000867
Iteration 216/1000 | Loss: 0.00000867
Iteration 217/1000 | Loss: 0.00000867
Iteration 218/1000 | Loss: 0.00000867
Iteration 219/1000 | Loss: 0.00000867
Iteration 220/1000 | Loss: 0.00000867
Iteration 221/1000 | Loss: 0.00000867
Iteration 222/1000 | Loss: 0.00000867
Iteration 223/1000 | Loss: 0.00000867
Iteration 224/1000 | Loss: 0.00000867
Iteration 225/1000 | Loss: 0.00000867
Iteration 226/1000 | Loss: 0.00000867
Iteration 227/1000 | Loss: 0.00000867
Iteration 228/1000 | Loss: 0.00000867
Iteration 229/1000 | Loss: 0.00000867
Iteration 230/1000 | Loss: 0.00000867
Iteration 231/1000 | Loss: 0.00000867
Iteration 232/1000 | Loss: 0.00000867
Iteration 233/1000 | Loss: 0.00000867
Iteration 234/1000 | Loss: 0.00000867
Iteration 235/1000 | Loss: 0.00000867
Iteration 236/1000 | Loss: 0.00000867
Iteration 237/1000 | Loss: 0.00000867
Iteration 238/1000 | Loss: 0.00000867
Iteration 239/1000 | Loss: 0.00000867
Iteration 240/1000 | Loss: 0.00000867
Iteration 241/1000 | Loss: 0.00000867
Iteration 242/1000 | Loss: 0.00000867
Iteration 243/1000 | Loss: 0.00000867
Iteration 244/1000 | Loss: 0.00000867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [8.671680916449986e-06, 8.671680916449986e-06, 8.671680916449986e-06, 8.671680916449986e-06, 8.671680916449986e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.671680916449986e-06

Optimization complete. Final v2v error: 2.5138049125671387 mm

Highest mean error: 2.652909517288208 mm for frame 57

Lowest mean error: 2.3923885822296143 mm for frame 19

Saving results

Total time: 37.61703109741211
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974891
Iteration 2/25 | Loss: 0.00153913
Iteration 3/25 | Loss: 0.00132262
Iteration 4/25 | Loss: 0.00129937
Iteration 5/25 | Loss: 0.00129373
Iteration 6/25 | Loss: 0.00129338
Iteration 7/25 | Loss: 0.00129338
Iteration 8/25 | Loss: 0.00129338
Iteration 9/25 | Loss: 0.00129338
Iteration 10/25 | Loss: 0.00129338
Iteration 11/25 | Loss: 0.00129338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012933752732351422, 0.0012933752732351422, 0.0012933752732351422, 0.0012933752732351422, 0.0012933752732351422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012933752732351422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60940862
Iteration 2/25 | Loss: 0.00091282
Iteration 3/25 | Loss: 0.00091280
Iteration 4/25 | Loss: 0.00091280
Iteration 5/25 | Loss: 0.00091280
Iteration 6/25 | Loss: 0.00091280
Iteration 7/25 | Loss: 0.00091280
Iteration 8/25 | Loss: 0.00091280
Iteration 9/25 | Loss: 0.00091280
Iteration 10/25 | Loss: 0.00091280
Iteration 11/25 | Loss: 0.00091280
Iteration 12/25 | Loss: 0.00091280
Iteration 13/25 | Loss: 0.00091280
Iteration 14/25 | Loss: 0.00091280
Iteration 15/25 | Loss: 0.00091280
Iteration 16/25 | Loss: 0.00091280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009127954836003482, 0.0009127954836003482, 0.0009127954836003482, 0.0009127954836003482, 0.0009127954836003482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009127954836003482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091280
Iteration 2/1000 | Loss: 0.00007229
Iteration 3/1000 | Loss: 0.00004327
Iteration 4/1000 | Loss: 0.00003823
Iteration 5/1000 | Loss: 0.00003629
Iteration 6/1000 | Loss: 0.00003520
Iteration 7/1000 | Loss: 0.00003438
Iteration 8/1000 | Loss: 0.00003365
Iteration 9/1000 | Loss: 0.00003322
Iteration 10/1000 | Loss: 0.00003285
Iteration 11/1000 | Loss: 0.00003254
Iteration 12/1000 | Loss: 0.00003234
Iteration 13/1000 | Loss: 0.00003218
Iteration 14/1000 | Loss: 0.00003209
Iteration 15/1000 | Loss: 0.00003205
Iteration 16/1000 | Loss: 0.00003204
Iteration 17/1000 | Loss: 0.00003199
Iteration 18/1000 | Loss: 0.00003194
Iteration 19/1000 | Loss: 0.00003193
Iteration 20/1000 | Loss: 0.00003191
Iteration 21/1000 | Loss: 0.00003190
Iteration 22/1000 | Loss: 0.00003189
Iteration 23/1000 | Loss: 0.00003188
Iteration 24/1000 | Loss: 0.00003187
Iteration 25/1000 | Loss: 0.00003187
Iteration 26/1000 | Loss: 0.00003187
Iteration 27/1000 | Loss: 0.00003187
Iteration 28/1000 | Loss: 0.00003187
Iteration 29/1000 | Loss: 0.00003187
Iteration 30/1000 | Loss: 0.00003186
Iteration 31/1000 | Loss: 0.00003186
Iteration 32/1000 | Loss: 0.00003180
Iteration 33/1000 | Loss: 0.00003180
Iteration 34/1000 | Loss: 0.00003178
Iteration 35/1000 | Loss: 0.00003176
Iteration 36/1000 | Loss: 0.00003176
Iteration 37/1000 | Loss: 0.00003175
Iteration 38/1000 | Loss: 0.00003175
Iteration 39/1000 | Loss: 0.00003175
Iteration 40/1000 | Loss: 0.00003174
Iteration 41/1000 | Loss: 0.00003173
Iteration 42/1000 | Loss: 0.00003173
Iteration 43/1000 | Loss: 0.00003171
Iteration 44/1000 | Loss: 0.00003171
Iteration 45/1000 | Loss: 0.00003171
Iteration 46/1000 | Loss: 0.00003171
Iteration 47/1000 | Loss: 0.00003171
Iteration 48/1000 | Loss: 0.00003171
Iteration 49/1000 | Loss: 0.00003171
Iteration 50/1000 | Loss: 0.00003171
Iteration 51/1000 | Loss: 0.00003171
Iteration 52/1000 | Loss: 0.00003171
Iteration 53/1000 | Loss: 0.00003170
Iteration 54/1000 | Loss: 0.00003170
Iteration 55/1000 | Loss: 0.00003169
Iteration 56/1000 | Loss: 0.00003169
Iteration 57/1000 | Loss: 0.00003169
Iteration 58/1000 | Loss: 0.00003168
Iteration 59/1000 | Loss: 0.00003168
Iteration 60/1000 | Loss: 0.00003168
Iteration 61/1000 | Loss: 0.00003168
Iteration 62/1000 | Loss: 0.00003167
Iteration 63/1000 | Loss: 0.00003166
Iteration 64/1000 | Loss: 0.00003166
Iteration 65/1000 | Loss: 0.00003166
Iteration 66/1000 | Loss: 0.00003166
Iteration 67/1000 | Loss: 0.00003166
Iteration 68/1000 | Loss: 0.00003165
Iteration 69/1000 | Loss: 0.00003165
Iteration 70/1000 | Loss: 0.00003165
Iteration 71/1000 | Loss: 0.00003165
Iteration 72/1000 | Loss: 0.00003165
Iteration 73/1000 | Loss: 0.00003164
Iteration 74/1000 | Loss: 0.00003164
Iteration 75/1000 | Loss: 0.00003164
Iteration 76/1000 | Loss: 0.00003163
Iteration 77/1000 | Loss: 0.00003163
Iteration 78/1000 | Loss: 0.00003163
Iteration 79/1000 | Loss: 0.00003162
Iteration 80/1000 | Loss: 0.00003162
Iteration 81/1000 | Loss: 0.00003162
Iteration 82/1000 | Loss: 0.00003162
Iteration 83/1000 | Loss: 0.00003161
Iteration 84/1000 | Loss: 0.00003161
Iteration 85/1000 | Loss: 0.00003161
Iteration 86/1000 | Loss: 0.00003161
Iteration 87/1000 | Loss: 0.00003161
Iteration 88/1000 | Loss: 0.00003161
Iteration 89/1000 | Loss: 0.00003161
Iteration 90/1000 | Loss: 0.00003160
Iteration 91/1000 | Loss: 0.00003160
Iteration 92/1000 | Loss: 0.00003160
Iteration 93/1000 | Loss: 0.00003160
Iteration 94/1000 | Loss: 0.00003159
Iteration 95/1000 | Loss: 0.00003159
Iteration 96/1000 | Loss: 0.00003159
Iteration 97/1000 | Loss: 0.00003159
Iteration 98/1000 | Loss: 0.00003159
Iteration 99/1000 | Loss: 0.00003159
Iteration 100/1000 | Loss: 0.00003159
Iteration 101/1000 | Loss: 0.00003159
Iteration 102/1000 | Loss: 0.00003159
Iteration 103/1000 | Loss: 0.00003159
Iteration 104/1000 | Loss: 0.00003159
Iteration 105/1000 | Loss: 0.00003159
Iteration 106/1000 | Loss: 0.00003159
Iteration 107/1000 | Loss: 0.00003159
Iteration 108/1000 | Loss: 0.00003159
Iteration 109/1000 | Loss: 0.00003159
Iteration 110/1000 | Loss: 0.00003158
Iteration 111/1000 | Loss: 0.00003158
Iteration 112/1000 | Loss: 0.00003157
Iteration 113/1000 | Loss: 0.00003157
Iteration 114/1000 | Loss: 0.00003157
Iteration 115/1000 | Loss: 0.00003157
Iteration 116/1000 | Loss: 0.00003157
Iteration 117/1000 | Loss: 0.00003157
Iteration 118/1000 | Loss: 0.00003157
Iteration 119/1000 | Loss: 0.00003157
Iteration 120/1000 | Loss: 0.00003157
Iteration 121/1000 | Loss: 0.00003157
Iteration 122/1000 | Loss: 0.00003157
Iteration 123/1000 | Loss: 0.00003157
Iteration 124/1000 | Loss: 0.00003156
Iteration 125/1000 | Loss: 0.00003156
Iteration 126/1000 | Loss: 0.00003156
Iteration 127/1000 | Loss: 0.00003156
Iteration 128/1000 | Loss: 0.00003156
Iteration 129/1000 | Loss: 0.00003156
Iteration 130/1000 | Loss: 0.00003156
Iteration 131/1000 | Loss: 0.00003156
Iteration 132/1000 | Loss: 0.00003156
Iteration 133/1000 | Loss: 0.00003156
Iteration 134/1000 | Loss: 0.00003155
Iteration 135/1000 | Loss: 0.00003155
Iteration 136/1000 | Loss: 0.00003155
Iteration 137/1000 | Loss: 0.00003154
Iteration 138/1000 | Loss: 0.00003154
Iteration 139/1000 | Loss: 0.00003154
Iteration 140/1000 | Loss: 0.00003154
Iteration 141/1000 | Loss: 0.00003154
Iteration 142/1000 | Loss: 0.00003153
Iteration 143/1000 | Loss: 0.00003153
Iteration 144/1000 | Loss: 0.00003153
Iteration 145/1000 | Loss: 0.00003153
Iteration 146/1000 | Loss: 0.00003153
Iteration 147/1000 | Loss: 0.00003153
Iteration 148/1000 | Loss: 0.00003153
Iteration 149/1000 | Loss: 0.00003153
Iteration 150/1000 | Loss: 0.00003152
Iteration 151/1000 | Loss: 0.00003152
Iteration 152/1000 | Loss: 0.00003152
Iteration 153/1000 | Loss: 0.00003152
Iteration 154/1000 | Loss: 0.00003151
Iteration 155/1000 | Loss: 0.00003151
Iteration 156/1000 | Loss: 0.00003151
Iteration 157/1000 | Loss: 0.00003151
Iteration 158/1000 | Loss: 0.00003151
Iteration 159/1000 | Loss: 0.00003151
Iteration 160/1000 | Loss: 0.00003151
Iteration 161/1000 | Loss: 0.00003150
Iteration 162/1000 | Loss: 0.00003150
Iteration 163/1000 | Loss: 0.00003150
Iteration 164/1000 | Loss: 0.00003150
Iteration 165/1000 | Loss: 0.00003150
Iteration 166/1000 | Loss: 0.00003150
Iteration 167/1000 | Loss: 0.00003150
Iteration 168/1000 | Loss: 0.00003149
Iteration 169/1000 | Loss: 0.00003149
Iteration 170/1000 | Loss: 0.00003149
Iteration 171/1000 | Loss: 0.00003149
Iteration 172/1000 | Loss: 0.00003149
Iteration 173/1000 | Loss: 0.00003149
Iteration 174/1000 | Loss: 0.00003149
Iteration 175/1000 | Loss: 0.00003149
Iteration 176/1000 | Loss: 0.00003149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.149140320601873e-05, 3.149140320601873e-05, 3.149140320601873e-05, 3.149140320601873e-05, 3.149140320601873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.149140320601873e-05

Optimization complete. Final v2v error: 4.486679553985596 mm

Highest mean error: 6.0890655517578125 mm for frame 48

Lowest mean error: 3.2830679416656494 mm for frame 75

Saving results

Total time: 47.675031423568726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858620
Iteration 2/25 | Loss: 0.00137182
Iteration 3/25 | Loss: 0.00119126
Iteration 4/25 | Loss: 0.00118092
Iteration 5/25 | Loss: 0.00117813
Iteration 6/25 | Loss: 0.00117774
Iteration 7/25 | Loss: 0.00117774
Iteration 8/25 | Loss: 0.00117774
Iteration 9/25 | Loss: 0.00117774
Iteration 10/25 | Loss: 0.00117774
Iteration 11/25 | Loss: 0.00117774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001177739119157195, 0.001177739119157195, 0.001177739119157195, 0.001177739119157195, 0.001177739119157195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001177739119157195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92675889
Iteration 2/25 | Loss: 0.00050561
Iteration 3/25 | Loss: 0.00050560
Iteration 4/25 | Loss: 0.00050560
Iteration 5/25 | Loss: 0.00050560
Iteration 6/25 | Loss: 0.00050560
Iteration 7/25 | Loss: 0.00050560
Iteration 8/25 | Loss: 0.00050560
Iteration 9/25 | Loss: 0.00050560
Iteration 10/25 | Loss: 0.00050560
Iteration 11/25 | Loss: 0.00050560
Iteration 12/25 | Loss: 0.00050560
Iteration 13/25 | Loss: 0.00050560
Iteration 14/25 | Loss: 0.00050560
Iteration 15/25 | Loss: 0.00050560
Iteration 16/25 | Loss: 0.00050560
Iteration 17/25 | Loss: 0.00050560
Iteration 18/25 | Loss: 0.00050560
Iteration 19/25 | Loss: 0.00050560
Iteration 20/25 | Loss: 0.00050560
Iteration 21/25 | Loss: 0.00050560
Iteration 22/25 | Loss: 0.00050560
Iteration 23/25 | Loss: 0.00050560
Iteration 24/25 | Loss: 0.00050560
Iteration 25/25 | Loss: 0.00050560

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050560
Iteration 2/1000 | Loss: 0.00003828
Iteration 3/1000 | Loss: 0.00002999
Iteration 4/1000 | Loss: 0.00002711
Iteration 5/1000 | Loss: 0.00002626
Iteration 6/1000 | Loss: 0.00002581
Iteration 7/1000 | Loss: 0.00002518
Iteration 8/1000 | Loss: 0.00002483
Iteration 9/1000 | Loss: 0.00002451
Iteration 10/1000 | Loss: 0.00002424
Iteration 11/1000 | Loss: 0.00002405
Iteration 12/1000 | Loss: 0.00002405
Iteration 13/1000 | Loss: 0.00002405
Iteration 14/1000 | Loss: 0.00002404
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002393
Iteration 17/1000 | Loss: 0.00002393
Iteration 18/1000 | Loss: 0.00002393
Iteration 19/1000 | Loss: 0.00002392
Iteration 20/1000 | Loss: 0.00002392
Iteration 21/1000 | Loss: 0.00002392
Iteration 22/1000 | Loss: 0.00002389
Iteration 23/1000 | Loss: 0.00002389
Iteration 24/1000 | Loss: 0.00002389
Iteration 25/1000 | Loss: 0.00002389
Iteration 26/1000 | Loss: 0.00002389
Iteration 27/1000 | Loss: 0.00002389
Iteration 28/1000 | Loss: 0.00002389
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00002388
Iteration 31/1000 | Loss: 0.00002387
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002384
Iteration 34/1000 | Loss: 0.00002384
Iteration 35/1000 | Loss: 0.00002384
Iteration 36/1000 | Loss: 0.00002383
Iteration 37/1000 | Loss: 0.00002383
Iteration 38/1000 | Loss: 0.00002383
Iteration 39/1000 | Loss: 0.00002383
Iteration 40/1000 | Loss: 0.00002383
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002382
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002381
Iteration 45/1000 | Loss: 0.00002381
Iteration 46/1000 | Loss: 0.00002381
Iteration 47/1000 | Loss: 0.00002381
Iteration 48/1000 | Loss: 0.00002381
Iteration 49/1000 | Loss: 0.00002381
Iteration 50/1000 | Loss: 0.00002381
Iteration 51/1000 | Loss: 0.00002380
Iteration 52/1000 | Loss: 0.00002380
Iteration 53/1000 | Loss: 0.00002380
Iteration 54/1000 | Loss: 0.00002380
Iteration 55/1000 | Loss: 0.00002380
Iteration 56/1000 | Loss: 0.00002380
Iteration 57/1000 | Loss: 0.00002380
Iteration 58/1000 | Loss: 0.00002379
Iteration 59/1000 | Loss: 0.00002378
Iteration 60/1000 | Loss: 0.00002378
Iteration 61/1000 | Loss: 0.00002378
Iteration 62/1000 | Loss: 0.00002378
Iteration 63/1000 | Loss: 0.00002378
Iteration 64/1000 | Loss: 0.00002378
Iteration 65/1000 | Loss: 0.00002378
Iteration 66/1000 | Loss: 0.00002377
Iteration 67/1000 | Loss: 0.00002377
Iteration 68/1000 | Loss: 0.00002377
Iteration 69/1000 | Loss: 0.00002377
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002376
Iteration 72/1000 | Loss: 0.00002376
Iteration 73/1000 | Loss: 0.00002376
Iteration 74/1000 | Loss: 0.00002376
Iteration 75/1000 | Loss: 0.00002376
Iteration 76/1000 | Loss: 0.00002376
Iteration 77/1000 | Loss: 0.00002376
Iteration 78/1000 | Loss: 0.00002376
Iteration 79/1000 | Loss: 0.00002376
Iteration 80/1000 | Loss: 0.00002376
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002376
Iteration 83/1000 | Loss: 0.00002376
Iteration 84/1000 | Loss: 0.00002376
Iteration 85/1000 | Loss: 0.00002376
Iteration 86/1000 | Loss: 0.00002376
Iteration 87/1000 | Loss: 0.00002376
Iteration 88/1000 | Loss: 0.00002375
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002375
Iteration 92/1000 | Loss: 0.00002375
Iteration 93/1000 | Loss: 0.00002375
Iteration 94/1000 | Loss: 0.00002375
Iteration 95/1000 | Loss: 0.00002375
Iteration 96/1000 | Loss: 0.00002375
Iteration 97/1000 | Loss: 0.00002375
Iteration 98/1000 | Loss: 0.00002375
Iteration 99/1000 | Loss: 0.00002374
Iteration 100/1000 | Loss: 0.00002374
Iteration 101/1000 | Loss: 0.00002374
Iteration 102/1000 | Loss: 0.00002374
Iteration 103/1000 | Loss: 0.00002374
Iteration 104/1000 | Loss: 0.00002374
Iteration 105/1000 | Loss: 0.00002374
Iteration 106/1000 | Loss: 0.00002374
Iteration 107/1000 | Loss: 0.00002373
Iteration 108/1000 | Loss: 0.00002373
Iteration 109/1000 | Loss: 0.00002373
Iteration 110/1000 | Loss: 0.00002373
Iteration 111/1000 | Loss: 0.00002373
Iteration 112/1000 | Loss: 0.00002373
Iteration 113/1000 | Loss: 0.00002373
Iteration 114/1000 | Loss: 0.00002373
Iteration 115/1000 | Loss: 0.00002373
Iteration 116/1000 | Loss: 0.00002372
Iteration 117/1000 | Loss: 0.00002372
Iteration 118/1000 | Loss: 0.00002372
Iteration 119/1000 | Loss: 0.00002372
Iteration 120/1000 | Loss: 0.00002372
Iteration 121/1000 | Loss: 0.00002372
Iteration 122/1000 | Loss: 0.00002372
Iteration 123/1000 | Loss: 0.00002372
Iteration 124/1000 | Loss: 0.00002372
Iteration 125/1000 | Loss: 0.00002372
Iteration 126/1000 | Loss: 0.00002372
Iteration 127/1000 | Loss: 0.00002372
Iteration 128/1000 | Loss: 0.00002372
Iteration 129/1000 | Loss: 0.00002372
Iteration 130/1000 | Loss: 0.00002372
Iteration 131/1000 | Loss: 0.00002372
Iteration 132/1000 | Loss: 0.00002372
Iteration 133/1000 | Loss: 0.00002372
Iteration 134/1000 | Loss: 0.00002372
Iteration 135/1000 | Loss: 0.00002372
Iteration 136/1000 | Loss: 0.00002372
Iteration 137/1000 | Loss: 0.00002372
Iteration 138/1000 | Loss: 0.00002372
Iteration 139/1000 | Loss: 0.00002372
Iteration 140/1000 | Loss: 0.00002372
Iteration 141/1000 | Loss: 0.00002372
Iteration 142/1000 | Loss: 0.00002372
Iteration 143/1000 | Loss: 0.00002372
Iteration 144/1000 | Loss: 0.00002372
Iteration 145/1000 | Loss: 0.00002372
Iteration 146/1000 | Loss: 0.00002372
Iteration 147/1000 | Loss: 0.00002372
Iteration 148/1000 | Loss: 0.00002372
Iteration 149/1000 | Loss: 0.00002372
Iteration 150/1000 | Loss: 0.00002372
Iteration 151/1000 | Loss: 0.00002372
Iteration 152/1000 | Loss: 0.00002372
Iteration 153/1000 | Loss: 0.00002372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.3718639567960054e-05, 2.3718639567960054e-05, 2.3718639567960054e-05, 2.3718639567960054e-05, 2.3718639567960054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3718639567960054e-05

Optimization complete. Final v2v error: 4.155728340148926 mm

Highest mean error: 4.368065357208252 mm for frame 126

Lowest mean error: 4.008102893829346 mm for frame 0

Saving results

Total time: 33.760345458984375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418150
Iteration 2/25 | Loss: 0.00117987
Iteration 3/25 | Loss: 0.00108717
Iteration 4/25 | Loss: 0.00108000
Iteration 5/25 | Loss: 0.00107776
Iteration 6/25 | Loss: 0.00107742
Iteration 7/25 | Loss: 0.00107742
Iteration 8/25 | Loss: 0.00107742
Iteration 9/25 | Loss: 0.00107742
Iteration 10/25 | Loss: 0.00107742
Iteration 11/25 | Loss: 0.00107742
Iteration 12/25 | Loss: 0.00107742
Iteration 13/25 | Loss: 0.00107742
Iteration 14/25 | Loss: 0.00107742
Iteration 15/25 | Loss: 0.00107742
Iteration 16/25 | Loss: 0.00107742
Iteration 17/25 | Loss: 0.00107742
Iteration 18/25 | Loss: 0.00107742
Iteration 19/25 | Loss: 0.00107742
Iteration 20/25 | Loss: 0.00107742
Iteration 21/25 | Loss: 0.00107742
Iteration 22/25 | Loss: 0.00107742
Iteration 23/25 | Loss: 0.00107742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001077422988601029, 0.001077422988601029, 0.001077422988601029, 0.001077422988601029, 0.001077422988601029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001077422988601029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13342083
Iteration 2/25 | Loss: 0.00061417
Iteration 3/25 | Loss: 0.00061417
Iteration 4/25 | Loss: 0.00061417
Iteration 5/25 | Loss: 0.00061417
Iteration 6/25 | Loss: 0.00061417
Iteration 7/25 | Loss: 0.00061417
Iteration 8/25 | Loss: 0.00061417
Iteration 9/25 | Loss: 0.00061417
Iteration 10/25 | Loss: 0.00061417
Iteration 11/25 | Loss: 0.00061417
Iteration 12/25 | Loss: 0.00061417
Iteration 13/25 | Loss: 0.00061417
Iteration 14/25 | Loss: 0.00061417
Iteration 15/25 | Loss: 0.00061417
Iteration 16/25 | Loss: 0.00061417
Iteration 17/25 | Loss: 0.00061417
Iteration 18/25 | Loss: 0.00061417
Iteration 19/25 | Loss: 0.00061417
Iteration 20/25 | Loss: 0.00061417
Iteration 21/25 | Loss: 0.00061417
Iteration 22/25 | Loss: 0.00061417
Iteration 23/25 | Loss: 0.00061417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006141696940176189, 0.0006141696940176189, 0.0006141696940176189, 0.0006141696940176189, 0.0006141696940176189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006141696940176189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061417
Iteration 2/1000 | Loss: 0.00003480
Iteration 3/1000 | Loss: 0.00001899
Iteration 4/1000 | Loss: 0.00001524
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001226
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001153
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00001125
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001117
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001108
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001108
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001108
Iteration 30/1000 | Loss: 0.00001108
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001107
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001106
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001105
Iteration 43/1000 | Loss: 0.00001105
Iteration 44/1000 | Loss: 0.00001105
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001104
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001102
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001101
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.1012464710802305e-05, 1.1012464710802305e-05, 1.1012464710802305e-05, 1.1012464710802305e-05, 1.1012464710802305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1012464710802305e-05

Optimization complete. Final v2v error: 2.8716416358947754 mm

Highest mean error: 2.8979601860046387 mm for frame 64

Lowest mean error: 2.8585832118988037 mm for frame 119

Saving results

Total time: 31.075990915298462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541965
Iteration 2/25 | Loss: 0.00136481
Iteration 3/25 | Loss: 0.00116983
Iteration 4/25 | Loss: 0.00112230
Iteration 5/25 | Loss: 0.00114491
Iteration 6/25 | Loss: 0.00109381
Iteration 7/25 | Loss: 0.00109126
Iteration 8/25 | Loss: 0.00109093
Iteration 9/25 | Loss: 0.00109074
Iteration 10/25 | Loss: 0.00109061
Iteration 11/25 | Loss: 0.00109058
Iteration 12/25 | Loss: 0.00109057
Iteration 13/25 | Loss: 0.00109057
Iteration 14/25 | Loss: 0.00109057
Iteration 15/25 | Loss: 0.00109057
Iteration 16/25 | Loss: 0.00109057
Iteration 17/25 | Loss: 0.00109057
Iteration 18/25 | Loss: 0.00109057
Iteration 19/25 | Loss: 0.00109057
Iteration 20/25 | Loss: 0.00109057
Iteration 21/25 | Loss: 0.00109057
Iteration 22/25 | Loss: 0.00109057
Iteration 23/25 | Loss: 0.00109057
Iteration 24/25 | Loss: 0.00109056
Iteration 25/25 | Loss: 0.00109056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74691129
Iteration 2/25 | Loss: 0.00086179
Iteration 3/25 | Loss: 0.00086177
Iteration 4/25 | Loss: 0.00086177
Iteration 5/25 | Loss: 0.00086177
Iteration 6/25 | Loss: 0.00086177
Iteration 7/25 | Loss: 0.00086177
Iteration 8/25 | Loss: 0.00086177
Iteration 9/25 | Loss: 0.00086177
Iteration 10/25 | Loss: 0.00086177
Iteration 11/25 | Loss: 0.00086177
Iteration 12/25 | Loss: 0.00086177
Iteration 13/25 | Loss: 0.00086177
Iteration 14/25 | Loss: 0.00086177
Iteration 15/25 | Loss: 0.00086177
Iteration 16/25 | Loss: 0.00086177
Iteration 17/25 | Loss: 0.00086177
Iteration 18/25 | Loss: 0.00086177
Iteration 19/25 | Loss: 0.00086177
Iteration 20/25 | Loss: 0.00086177
Iteration 21/25 | Loss: 0.00086177
Iteration 22/25 | Loss: 0.00086177
Iteration 23/25 | Loss: 0.00086176
Iteration 24/25 | Loss: 0.00086177
Iteration 25/25 | Loss: 0.00086176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086176
Iteration 2/1000 | Loss: 0.00003117
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001749
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001595
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001560
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001556
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001554
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001550
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001546
Iteration 35/1000 | Loss: 0.00001546
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001537
Iteration 49/1000 | Loss: 0.00001536
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001521
Iteration 113/1000 | Loss: 0.00001521
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001521
Iteration 116/1000 | Loss: 0.00001521
Iteration 117/1000 | Loss: 0.00001521
Iteration 118/1000 | Loss: 0.00001521
Iteration 119/1000 | Loss: 0.00001521
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001521
Iteration 124/1000 | Loss: 0.00001521
Iteration 125/1000 | Loss: 0.00001521
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001521
Iteration 135/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.5207965589070227e-05, 1.5207965589070227e-05, 1.5207965589070227e-05, 1.5207965589070227e-05, 1.5207965589070227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5207965589070227e-05

Optimization complete. Final v2v error: 3.218669891357422 mm

Highest mean error: 4.432488918304443 mm for frame 107

Lowest mean error: 2.687166213989258 mm for frame 71

Saving results

Total time: 49.92557072639465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851573
Iteration 2/25 | Loss: 0.00157089
Iteration 3/25 | Loss: 0.00130627
Iteration 4/25 | Loss: 0.00125061
Iteration 5/25 | Loss: 0.00123592
Iteration 6/25 | Loss: 0.00123664
Iteration 7/25 | Loss: 0.00123797
Iteration 8/25 | Loss: 0.00123252
Iteration 9/25 | Loss: 0.00122949
Iteration 10/25 | Loss: 0.00122800
Iteration 11/25 | Loss: 0.00122704
Iteration 12/25 | Loss: 0.00122673
Iteration 13/25 | Loss: 0.00122658
Iteration 14/25 | Loss: 0.00122654
Iteration 15/25 | Loss: 0.00122654
Iteration 16/25 | Loss: 0.00122654
Iteration 17/25 | Loss: 0.00122653
Iteration 18/25 | Loss: 0.00122653
Iteration 19/25 | Loss: 0.00122653
Iteration 20/25 | Loss: 0.00122653
Iteration 21/25 | Loss: 0.00122653
Iteration 22/25 | Loss: 0.00122653
Iteration 23/25 | Loss: 0.00122653
Iteration 24/25 | Loss: 0.00122653
Iteration 25/25 | Loss: 0.00122653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27724874
Iteration 2/25 | Loss: 0.00126613
Iteration 3/25 | Loss: 0.00126612
Iteration 4/25 | Loss: 0.00126612
Iteration 5/25 | Loss: 0.00126612
Iteration 6/25 | Loss: 0.00126612
Iteration 7/25 | Loss: 0.00126612
Iteration 8/25 | Loss: 0.00126612
Iteration 9/25 | Loss: 0.00126612
Iteration 10/25 | Loss: 0.00126612
Iteration 11/25 | Loss: 0.00126612
Iteration 12/25 | Loss: 0.00126612
Iteration 13/25 | Loss: 0.00126612
Iteration 14/25 | Loss: 0.00126612
Iteration 15/25 | Loss: 0.00126612
Iteration 16/25 | Loss: 0.00126611
Iteration 17/25 | Loss: 0.00126611
Iteration 18/25 | Loss: 0.00126611
Iteration 19/25 | Loss: 0.00126611
Iteration 20/25 | Loss: 0.00126611
Iteration 21/25 | Loss: 0.00126611
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012661147629842162, 0.0012661147629842162, 0.0012661147629842162, 0.0012661147629842162, 0.0012661147629842162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012661147629842162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126611
Iteration 2/1000 | Loss: 0.00017338
Iteration 3/1000 | Loss: 0.00011352
Iteration 4/1000 | Loss: 0.00009492
Iteration 5/1000 | Loss: 0.00008608
Iteration 6/1000 | Loss: 0.00008043
Iteration 7/1000 | Loss: 0.00013005
Iteration 8/1000 | Loss: 0.00021016
Iteration 9/1000 | Loss: 0.00006920
Iteration 10/1000 | Loss: 0.00006763
Iteration 11/1000 | Loss: 0.00007971
Iteration 12/1000 | Loss: 0.00009897
Iteration 13/1000 | Loss: 0.00005258
Iteration 14/1000 | Loss: 0.00004765
Iteration 15/1000 | Loss: 0.00004488
Iteration 16/1000 | Loss: 0.00006053
Iteration 17/1000 | Loss: 0.00016369
Iteration 18/1000 | Loss: 0.00003973
Iteration 19/1000 | Loss: 0.00003728
Iteration 20/1000 | Loss: 0.00003557
Iteration 21/1000 | Loss: 0.00003439
Iteration 22/1000 | Loss: 0.00003359
Iteration 23/1000 | Loss: 0.00003301
Iteration 24/1000 | Loss: 0.00003252
Iteration 25/1000 | Loss: 0.00003210
Iteration 26/1000 | Loss: 0.00003172
Iteration 27/1000 | Loss: 0.00003150
Iteration 28/1000 | Loss: 0.00003128
Iteration 29/1000 | Loss: 0.00003120
Iteration 30/1000 | Loss: 0.00003118
Iteration 31/1000 | Loss: 0.00003111
Iteration 32/1000 | Loss: 0.00003111
Iteration 33/1000 | Loss: 0.00003110
Iteration 34/1000 | Loss: 0.00003109
Iteration 35/1000 | Loss: 0.00003108
Iteration 36/1000 | Loss: 0.00003107
Iteration 37/1000 | Loss: 0.00003106
Iteration 38/1000 | Loss: 0.00003106
Iteration 39/1000 | Loss: 0.00003105
Iteration 40/1000 | Loss: 0.00003105
Iteration 41/1000 | Loss: 0.00003104
Iteration 42/1000 | Loss: 0.00003104
Iteration 43/1000 | Loss: 0.00003102
Iteration 44/1000 | Loss: 0.00003102
Iteration 45/1000 | Loss: 0.00003099
Iteration 46/1000 | Loss: 0.00003098
Iteration 47/1000 | Loss: 0.00003098
Iteration 48/1000 | Loss: 0.00003097
Iteration 49/1000 | Loss: 0.00003096
Iteration 50/1000 | Loss: 0.00003096
Iteration 51/1000 | Loss: 0.00003096
Iteration 52/1000 | Loss: 0.00003096
Iteration 53/1000 | Loss: 0.00003095
Iteration 54/1000 | Loss: 0.00003094
Iteration 55/1000 | Loss: 0.00003093
Iteration 56/1000 | Loss: 0.00003093
Iteration 57/1000 | Loss: 0.00003092
Iteration 58/1000 | Loss: 0.00003092
Iteration 59/1000 | Loss: 0.00003092
Iteration 60/1000 | Loss: 0.00003091
Iteration 61/1000 | Loss: 0.00003091
Iteration 62/1000 | Loss: 0.00003087
Iteration 63/1000 | Loss: 0.00003086
Iteration 64/1000 | Loss: 0.00003080
Iteration 65/1000 | Loss: 0.00003077
Iteration 66/1000 | Loss: 0.00003077
Iteration 67/1000 | Loss: 0.00003076
Iteration 68/1000 | Loss: 0.00003070
Iteration 69/1000 | Loss: 0.00003057
Iteration 70/1000 | Loss: 0.00003050
Iteration 71/1000 | Loss: 0.00003045
Iteration 72/1000 | Loss: 0.00003043
Iteration 73/1000 | Loss: 0.00003037
Iteration 74/1000 | Loss: 0.00003037
Iteration 75/1000 | Loss: 0.00003036
Iteration 76/1000 | Loss: 0.00003036
Iteration 77/1000 | Loss: 0.00003035
Iteration 78/1000 | Loss: 0.00003035
Iteration 79/1000 | Loss: 0.00003032
Iteration 80/1000 | Loss: 0.00003031
Iteration 81/1000 | Loss: 0.00003031
Iteration 82/1000 | Loss: 0.00003030
Iteration 83/1000 | Loss: 0.00003030
Iteration 84/1000 | Loss: 0.00003029
Iteration 85/1000 | Loss: 0.00003028
Iteration 86/1000 | Loss: 0.00003028
Iteration 87/1000 | Loss: 0.00003027
Iteration 88/1000 | Loss: 0.00003027
Iteration 89/1000 | Loss: 0.00003026
Iteration 90/1000 | Loss: 0.00003023
Iteration 91/1000 | Loss: 0.00003022
Iteration 92/1000 | Loss: 0.00003022
Iteration 93/1000 | Loss: 0.00003022
Iteration 94/1000 | Loss: 0.00003021
Iteration 95/1000 | Loss: 0.00003021
Iteration 96/1000 | Loss: 0.00003020
Iteration 97/1000 | Loss: 0.00003020
Iteration 98/1000 | Loss: 0.00003019
Iteration 99/1000 | Loss: 0.00003019
Iteration 100/1000 | Loss: 0.00003019
Iteration 101/1000 | Loss: 0.00003019
Iteration 102/1000 | Loss: 0.00003019
Iteration 103/1000 | Loss: 0.00003018
Iteration 104/1000 | Loss: 0.00003018
Iteration 105/1000 | Loss: 0.00003018
Iteration 106/1000 | Loss: 0.00003017
Iteration 107/1000 | Loss: 0.00003017
Iteration 108/1000 | Loss: 0.00003017
Iteration 109/1000 | Loss: 0.00003017
Iteration 110/1000 | Loss: 0.00003016
Iteration 111/1000 | Loss: 0.00003016
Iteration 112/1000 | Loss: 0.00003016
Iteration 113/1000 | Loss: 0.00003016
Iteration 114/1000 | Loss: 0.00003016
Iteration 115/1000 | Loss: 0.00003015
Iteration 116/1000 | Loss: 0.00003015
Iteration 117/1000 | Loss: 0.00003015
Iteration 118/1000 | Loss: 0.00003015
Iteration 119/1000 | Loss: 0.00003015
Iteration 120/1000 | Loss: 0.00003015
Iteration 121/1000 | Loss: 0.00003014
Iteration 122/1000 | Loss: 0.00003014
Iteration 123/1000 | Loss: 0.00003014
Iteration 124/1000 | Loss: 0.00003014
Iteration 125/1000 | Loss: 0.00003013
Iteration 126/1000 | Loss: 0.00003013
Iteration 127/1000 | Loss: 0.00003013
Iteration 128/1000 | Loss: 0.00003013
Iteration 129/1000 | Loss: 0.00003013
Iteration 130/1000 | Loss: 0.00003013
Iteration 131/1000 | Loss: 0.00003013
Iteration 132/1000 | Loss: 0.00003012
Iteration 133/1000 | Loss: 0.00003012
Iteration 134/1000 | Loss: 0.00003012
Iteration 135/1000 | Loss: 0.00003012
Iteration 136/1000 | Loss: 0.00003012
Iteration 137/1000 | Loss: 0.00003012
Iteration 138/1000 | Loss: 0.00003012
Iteration 139/1000 | Loss: 0.00003012
Iteration 140/1000 | Loss: 0.00003012
Iteration 141/1000 | Loss: 0.00003012
Iteration 142/1000 | Loss: 0.00003011
Iteration 143/1000 | Loss: 0.00003011
Iteration 144/1000 | Loss: 0.00003011
Iteration 145/1000 | Loss: 0.00003011
Iteration 146/1000 | Loss: 0.00003011
Iteration 147/1000 | Loss: 0.00003011
Iteration 148/1000 | Loss: 0.00003011
Iteration 149/1000 | Loss: 0.00003011
Iteration 150/1000 | Loss: 0.00003011
Iteration 151/1000 | Loss: 0.00003011
Iteration 152/1000 | Loss: 0.00003011
Iteration 153/1000 | Loss: 0.00003011
Iteration 154/1000 | Loss: 0.00003011
Iteration 155/1000 | Loss: 0.00003010
Iteration 156/1000 | Loss: 0.00003010
Iteration 157/1000 | Loss: 0.00003010
Iteration 158/1000 | Loss: 0.00003010
Iteration 159/1000 | Loss: 0.00003010
Iteration 160/1000 | Loss: 0.00003010
Iteration 161/1000 | Loss: 0.00003010
Iteration 162/1000 | Loss: 0.00003010
Iteration 163/1000 | Loss: 0.00003010
Iteration 164/1000 | Loss: 0.00003010
Iteration 165/1000 | Loss: 0.00003010
Iteration 166/1000 | Loss: 0.00003010
Iteration 167/1000 | Loss: 0.00003010
Iteration 168/1000 | Loss: 0.00003010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [3.0098852221271954e-05, 3.0098852221271954e-05, 3.0098852221271954e-05, 3.0098852221271954e-05, 3.0098852221271954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0098852221271954e-05

Optimization complete. Final v2v error: 4.236298084259033 mm

Highest mean error: 11.68009090423584 mm for frame 164

Lowest mean error: 3.6484742164611816 mm for frame 115

Saving results

Total time: 94.55173087120056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787118
Iteration 2/25 | Loss: 0.00226490
Iteration 3/25 | Loss: 0.00172955
Iteration 4/25 | Loss: 0.00159213
Iteration 5/25 | Loss: 0.00147074
Iteration 6/25 | Loss: 0.00142955
Iteration 7/25 | Loss: 0.00137773
Iteration 8/25 | Loss: 0.00133966
Iteration 9/25 | Loss: 0.00132212
Iteration 10/25 | Loss: 0.00134823
Iteration 11/25 | Loss: 0.00140404
Iteration 12/25 | Loss: 0.00129826
Iteration 13/25 | Loss: 0.00129142
Iteration 14/25 | Loss: 0.00128469
Iteration 15/25 | Loss: 0.00127688
Iteration 16/25 | Loss: 0.00127559
Iteration 17/25 | Loss: 0.00127516
Iteration 18/25 | Loss: 0.00127503
Iteration 19/25 | Loss: 0.00127500
Iteration 20/25 | Loss: 0.00127499
Iteration 21/25 | Loss: 0.00127499
Iteration 22/25 | Loss: 0.00127499
Iteration 23/25 | Loss: 0.00127499
Iteration 24/25 | Loss: 0.00127499
Iteration 25/25 | Loss: 0.00127499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34530556
Iteration 2/25 | Loss: 0.00063305
Iteration 3/25 | Loss: 0.00063304
Iteration 4/25 | Loss: 0.00063303
Iteration 5/25 | Loss: 0.00063303
Iteration 6/25 | Loss: 0.00063303
Iteration 7/25 | Loss: 0.00063303
Iteration 8/25 | Loss: 0.00063303
Iteration 9/25 | Loss: 0.00063303
Iteration 10/25 | Loss: 0.00063303
Iteration 11/25 | Loss: 0.00063303
Iteration 12/25 | Loss: 0.00063303
Iteration 13/25 | Loss: 0.00063303
Iteration 14/25 | Loss: 0.00063303
Iteration 15/25 | Loss: 0.00063303
Iteration 16/25 | Loss: 0.00063303
Iteration 17/25 | Loss: 0.00063303
Iteration 18/25 | Loss: 0.00063303
Iteration 19/25 | Loss: 0.00063303
Iteration 20/25 | Loss: 0.00063303
Iteration 21/25 | Loss: 0.00063303
Iteration 22/25 | Loss: 0.00063303
Iteration 23/25 | Loss: 0.00063303
Iteration 24/25 | Loss: 0.00063303
Iteration 25/25 | Loss: 0.00063303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063303
Iteration 2/1000 | Loss: 0.00004379
Iteration 3/1000 | Loss: 0.00003020
Iteration 4/1000 | Loss: 0.00002747
Iteration 5/1000 | Loss: 0.00002647
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002544
Iteration 8/1000 | Loss: 0.00002518
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002482
Iteration 11/1000 | Loss: 0.00002470
Iteration 12/1000 | Loss: 0.00002469
Iteration 13/1000 | Loss: 0.00002469
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002454
Iteration 16/1000 | Loss: 0.00002450
Iteration 17/1000 | Loss: 0.00002450
Iteration 18/1000 | Loss: 0.00002449
Iteration 19/1000 | Loss: 0.00002448
Iteration 20/1000 | Loss: 0.00002448
Iteration 21/1000 | Loss: 0.00002448
Iteration 22/1000 | Loss: 0.00002447
Iteration 23/1000 | Loss: 0.00002447
Iteration 24/1000 | Loss: 0.00002447
Iteration 25/1000 | Loss: 0.00002446
Iteration 26/1000 | Loss: 0.00002446
Iteration 27/1000 | Loss: 0.00002446
Iteration 28/1000 | Loss: 0.00002445
Iteration 29/1000 | Loss: 0.00002445
Iteration 30/1000 | Loss: 0.00002444
Iteration 31/1000 | Loss: 0.00002443
Iteration 32/1000 | Loss: 0.00002443
Iteration 33/1000 | Loss: 0.00002443
Iteration 34/1000 | Loss: 0.00002443
Iteration 35/1000 | Loss: 0.00002443
Iteration 36/1000 | Loss: 0.00002440
Iteration 37/1000 | Loss: 0.00002431
Iteration 38/1000 | Loss: 0.00002430
Iteration 39/1000 | Loss: 0.00002430
Iteration 40/1000 | Loss: 0.00002429
Iteration 41/1000 | Loss: 0.00002429
Iteration 42/1000 | Loss: 0.00002429
Iteration 43/1000 | Loss: 0.00002429
Iteration 44/1000 | Loss: 0.00002429
Iteration 45/1000 | Loss: 0.00002428
Iteration 46/1000 | Loss: 0.00002428
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00002428
Iteration 49/1000 | Loss: 0.00002428
Iteration 50/1000 | Loss: 0.00002427
Iteration 51/1000 | Loss: 0.00002427
Iteration 52/1000 | Loss: 0.00002427
Iteration 53/1000 | Loss: 0.00002427
Iteration 54/1000 | Loss: 0.00002427
Iteration 55/1000 | Loss: 0.00002427
Iteration 56/1000 | Loss: 0.00002426
Iteration 57/1000 | Loss: 0.00002426
Iteration 58/1000 | Loss: 0.00002426
Iteration 59/1000 | Loss: 0.00002425
Iteration 60/1000 | Loss: 0.00002425
Iteration 61/1000 | Loss: 0.00002424
Iteration 62/1000 | Loss: 0.00002424
Iteration 63/1000 | Loss: 0.00002424
Iteration 64/1000 | Loss: 0.00002424
Iteration 65/1000 | Loss: 0.00002424
Iteration 66/1000 | Loss: 0.00002424
Iteration 67/1000 | Loss: 0.00002423
Iteration 68/1000 | Loss: 0.00002423
Iteration 69/1000 | Loss: 0.00002423
Iteration 70/1000 | Loss: 0.00002423
Iteration 71/1000 | Loss: 0.00002423
Iteration 72/1000 | Loss: 0.00002422
Iteration 73/1000 | Loss: 0.00002422
Iteration 74/1000 | Loss: 0.00002422
Iteration 75/1000 | Loss: 0.00002422
Iteration 76/1000 | Loss: 0.00002422
Iteration 77/1000 | Loss: 0.00002421
Iteration 78/1000 | Loss: 0.00002421
Iteration 79/1000 | Loss: 0.00002421
Iteration 80/1000 | Loss: 0.00002421
Iteration 81/1000 | Loss: 0.00002421
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002421
Iteration 84/1000 | Loss: 0.00002421
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002421
Iteration 88/1000 | Loss: 0.00002421
Iteration 89/1000 | Loss: 0.00002421
Iteration 90/1000 | Loss: 0.00002421
Iteration 91/1000 | Loss: 0.00002421
Iteration 92/1000 | Loss: 0.00002421
Iteration 93/1000 | Loss: 0.00002421
Iteration 94/1000 | Loss: 0.00002420
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00002420
Iteration 97/1000 | Loss: 0.00002420
Iteration 98/1000 | Loss: 0.00002420
Iteration 99/1000 | Loss: 0.00002420
Iteration 100/1000 | Loss: 0.00002420
Iteration 101/1000 | Loss: 0.00002420
Iteration 102/1000 | Loss: 0.00002420
Iteration 103/1000 | Loss: 0.00002420
Iteration 104/1000 | Loss: 0.00002420
Iteration 105/1000 | Loss: 0.00002420
Iteration 106/1000 | Loss: 0.00002420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.420413693471346e-05, 2.420413693471346e-05, 2.420413693471346e-05, 2.420413693471346e-05, 2.420413693471346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.420413693471346e-05

Optimization complete. Final v2v error: 4.112242698669434 mm

Highest mean error: 4.354811191558838 mm for frame 7

Lowest mean error: 3.8146767616271973 mm for frame 153

Saving results

Total time: 56.55274438858032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978838
Iteration 2/25 | Loss: 0.00159861
Iteration 3/25 | Loss: 0.00127941
Iteration 4/25 | Loss: 0.00126082
Iteration 5/25 | Loss: 0.00125395
Iteration 6/25 | Loss: 0.00125247
Iteration 7/25 | Loss: 0.00125247
Iteration 8/25 | Loss: 0.00125247
Iteration 9/25 | Loss: 0.00125247
Iteration 10/25 | Loss: 0.00125247
Iteration 11/25 | Loss: 0.00125247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012524734484031796, 0.0012524734484031796, 0.0012524734484031796, 0.0012524734484031796, 0.0012524734484031796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012524734484031796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87824655
Iteration 2/25 | Loss: 0.00099414
Iteration 3/25 | Loss: 0.00099413
Iteration 4/25 | Loss: 0.00099413
Iteration 5/25 | Loss: 0.00099413
Iteration 6/25 | Loss: 0.00099413
Iteration 7/25 | Loss: 0.00099413
Iteration 8/25 | Loss: 0.00099413
Iteration 9/25 | Loss: 0.00099413
Iteration 10/25 | Loss: 0.00099413
Iteration 11/25 | Loss: 0.00099413
Iteration 12/25 | Loss: 0.00099413
Iteration 13/25 | Loss: 0.00099413
Iteration 14/25 | Loss: 0.00099413
Iteration 15/25 | Loss: 0.00099413
Iteration 16/25 | Loss: 0.00099413
Iteration 17/25 | Loss: 0.00099413
Iteration 18/25 | Loss: 0.00099413
Iteration 19/25 | Loss: 0.00099413
Iteration 20/25 | Loss: 0.00099413
Iteration 21/25 | Loss: 0.00099413
Iteration 22/25 | Loss: 0.00099413
Iteration 23/25 | Loss: 0.00099413
Iteration 24/25 | Loss: 0.00099413
Iteration 25/25 | Loss: 0.00099413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099413
Iteration 2/1000 | Loss: 0.00006044
Iteration 3/1000 | Loss: 0.00003938
Iteration 4/1000 | Loss: 0.00003139
Iteration 5/1000 | Loss: 0.00002910
Iteration 6/1000 | Loss: 0.00002792
Iteration 7/1000 | Loss: 0.00002725
Iteration 8/1000 | Loss: 0.00002639
Iteration 9/1000 | Loss: 0.00002597
Iteration 10/1000 | Loss: 0.00002562
Iteration 11/1000 | Loss: 0.00002532
Iteration 12/1000 | Loss: 0.00002512
Iteration 13/1000 | Loss: 0.00002488
Iteration 14/1000 | Loss: 0.00002470
Iteration 15/1000 | Loss: 0.00002451
Iteration 16/1000 | Loss: 0.00002434
Iteration 17/1000 | Loss: 0.00002417
Iteration 18/1000 | Loss: 0.00002402
Iteration 19/1000 | Loss: 0.00002393
Iteration 20/1000 | Loss: 0.00002390
Iteration 21/1000 | Loss: 0.00002390
Iteration 22/1000 | Loss: 0.00002386
Iteration 23/1000 | Loss: 0.00002385
Iteration 24/1000 | Loss: 0.00002385
Iteration 25/1000 | Loss: 0.00002385
Iteration 26/1000 | Loss: 0.00002384
Iteration 27/1000 | Loss: 0.00002384
Iteration 28/1000 | Loss: 0.00002384
Iteration 29/1000 | Loss: 0.00002384
Iteration 30/1000 | Loss: 0.00002384
Iteration 31/1000 | Loss: 0.00002384
Iteration 32/1000 | Loss: 0.00002384
Iteration 33/1000 | Loss: 0.00002382
Iteration 34/1000 | Loss: 0.00002381
Iteration 35/1000 | Loss: 0.00002381
Iteration 36/1000 | Loss: 0.00002381
Iteration 37/1000 | Loss: 0.00002381
Iteration 38/1000 | Loss: 0.00002381
Iteration 39/1000 | Loss: 0.00002381
Iteration 40/1000 | Loss: 0.00002380
Iteration 41/1000 | Loss: 0.00002380
Iteration 42/1000 | Loss: 0.00002380
Iteration 43/1000 | Loss: 0.00002380
Iteration 44/1000 | Loss: 0.00002379
Iteration 45/1000 | Loss: 0.00002379
Iteration 46/1000 | Loss: 0.00002379
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00002379
Iteration 49/1000 | Loss: 0.00002378
Iteration 50/1000 | Loss: 0.00002378
Iteration 51/1000 | Loss: 0.00002378
Iteration 52/1000 | Loss: 0.00002378
Iteration 53/1000 | Loss: 0.00002374
Iteration 54/1000 | Loss: 0.00002369
Iteration 55/1000 | Loss: 0.00002369
Iteration 56/1000 | Loss: 0.00002368
Iteration 57/1000 | Loss: 0.00002366
Iteration 58/1000 | Loss: 0.00002365
Iteration 59/1000 | Loss: 0.00002365
Iteration 60/1000 | Loss: 0.00002365
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002364
Iteration 64/1000 | Loss: 0.00002363
Iteration 65/1000 | Loss: 0.00002363
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002363
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002363
Iteration 73/1000 | Loss: 0.00002362
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002362
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002361
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002359
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002359
Iteration 93/1000 | Loss: 0.00002358
Iteration 94/1000 | Loss: 0.00002358
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002358
Iteration 97/1000 | Loss: 0.00002358
Iteration 98/1000 | Loss: 0.00002358
Iteration 99/1000 | Loss: 0.00002358
Iteration 100/1000 | Loss: 0.00002358
Iteration 101/1000 | Loss: 0.00002358
Iteration 102/1000 | Loss: 0.00002357
Iteration 103/1000 | Loss: 0.00002357
Iteration 104/1000 | Loss: 0.00002357
Iteration 105/1000 | Loss: 0.00002357
Iteration 106/1000 | Loss: 0.00002357
Iteration 107/1000 | Loss: 0.00002357
Iteration 108/1000 | Loss: 0.00002357
Iteration 109/1000 | Loss: 0.00002357
Iteration 110/1000 | Loss: 0.00002357
Iteration 111/1000 | Loss: 0.00002356
Iteration 112/1000 | Loss: 0.00002356
Iteration 113/1000 | Loss: 0.00002356
Iteration 114/1000 | Loss: 0.00002356
Iteration 115/1000 | Loss: 0.00002356
Iteration 116/1000 | Loss: 0.00002355
Iteration 117/1000 | Loss: 0.00002355
Iteration 118/1000 | Loss: 0.00002355
Iteration 119/1000 | Loss: 0.00002355
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002355
Iteration 122/1000 | Loss: 0.00002355
Iteration 123/1000 | Loss: 0.00002355
Iteration 124/1000 | Loss: 0.00002355
Iteration 125/1000 | Loss: 0.00002355
Iteration 126/1000 | Loss: 0.00002355
Iteration 127/1000 | Loss: 0.00002355
Iteration 128/1000 | Loss: 0.00002355
Iteration 129/1000 | Loss: 0.00002354
Iteration 130/1000 | Loss: 0.00002354
Iteration 131/1000 | Loss: 0.00002354
Iteration 132/1000 | Loss: 0.00002354
Iteration 133/1000 | Loss: 0.00002354
Iteration 134/1000 | Loss: 0.00002354
Iteration 135/1000 | Loss: 0.00002354
Iteration 136/1000 | Loss: 0.00002354
Iteration 137/1000 | Loss: 0.00002354
Iteration 138/1000 | Loss: 0.00002354
Iteration 139/1000 | Loss: 0.00002354
Iteration 140/1000 | Loss: 0.00002354
Iteration 141/1000 | Loss: 0.00002354
Iteration 142/1000 | Loss: 0.00002354
Iteration 143/1000 | Loss: 0.00002354
Iteration 144/1000 | Loss: 0.00002354
Iteration 145/1000 | Loss: 0.00002354
Iteration 146/1000 | Loss: 0.00002354
Iteration 147/1000 | Loss: 0.00002354
Iteration 148/1000 | Loss: 0.00002354
Iteration 149/1000 | Loss: 0.00002354
Iteration 150/1000 | Loss: 0.00002354
Iteration 151/1000 | Loss: 0.00002354
Iteration 152/1000 | Loss: 0.00002354
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002354
Iteration 157/1000 | Loss: 0.00002354
Iteration 158/1000 | Loss: 0.00002354
Iteration 159/1000 | Loss: 0.00002354
Iteration 160/1000 | Loss: 0.00002354
Iteration 161/1000 | Loss: 0.00002354
Iteration 162/1000 | Loss: 0.00002354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.354168645979371e-05, 2.354168645979371e-05, 2.354168645979371e-05, 2.354168645979371e-05, 2.354168645979371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.354168645979371e-05

Optimization complete. Final v2v error: 4.0034308433532715 mm

Highest mean error: 4.9571452140808105 mm for frame 111

Lowest mean error: 3.1403913497924805 mm for frame 52

Saving results

Total time: 51.93491721153259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975206
Iteration 2/25 | Loss: 0.00233059
Iteration 3/25 | Loss: 0.00178442
Iteration 4/25 | Loss: 0.00172397
Iteration 5/25 | Loss: 0.00171123
Iteration 6/25 | Loss: 0.00171447
Iteration 7/25 | Loss: 0.00171036
Iteration 8/25 | Loss: 0.00170568
Iteration 9/25 | Loss: 0.00170296
Iteration 10/25 | Loss: 0.00170258
Iteration 11/25 | Loss: 0.00170245
Iteration 12/25 | Loss: 0.00170237
Iteration 13/25 | Loss: 0.00170237
Iteration 14/25 | Loss: 0.00170237
Iteration 15/25 | Loss: 0.00170237
Iteration 16/25 | Loss: 0.00170237
Iteration 17/25 | Loss: 0.00170237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00170237198472023, 0.00170237198472023, 0.00170237198472023, 0.00170237198472023, 0.00170237198472023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00170237198472023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31046557
Iteration 2/25 | Loss: 0.00421708
Iteration 3/25 | Loss: 0.00421708
Iteration 4/25 | Loss: 0.00419518
Iteration 5/25 | Loss: 0.00419518
Iteration 6/25 | Loss: 0.00419518
Iteration 7/25 | Loss: 0.00419518
Iteration 8/25 | Loss: 0.00419518
Iteration 9/25 | Loss: 0.00419518
Iteration 10/25 | Loss: 0.00419518
Iteration 11/25 | Loss: 0.00419517
Iteration 12/25 | Loss: 0.00419517
Iteration 13/25 | Loss: 0.00419517
Iteration 14/25 | Loss: 0.00419517
Iteration 15/25 | Loss: 0.00419517
Iteration 16/25 | Loss: 0.00419517
Iteration 17/25 | Loss: 0.00419517
Iteration 18/25 | Loss: 0.00419517
Iteration 19/25 | Loss: 0.00419517
Iteration 20/25 | Loss: 0.00419517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004195173736661673, 0.004195173736661673, 0.004195173736661673, 0.004195173736661673, 0.004195173736661673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004195173736661673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00419517
Iteration 2/1000 | Loss: 0.00059129
Iteration 3/1000 | Loss: 0.00042005
Iteration 4/1000 | Loss: 0.00036142
Iteration 5/1000 | Loss: 0.00032764
Iteration 6/1000 | Loss: 0.00030650
Iteration 7/1000 | Loss: 0.00029175
Iteration 8/1000 | Loss: 0.00027827
Iteration 9/1000 | Loss: 0.00027014
Iteration 10/1000 | Loss: 0.00025893
Iteration 11/1000 | Loss: 0.00099485
Iteration 12/1000 | Loss: 0.00957429
Iteration 13/1000 | Loss: 0.00392278
Iteration 14/1000 | Loss: 0.00040738
Iteration 15/1000 | Loss: 0.00037585
Iteration 16/1000 | Loss: 0.00043553
Iteration 17/1000 | Loss: 0.00041176
Iteration 18/1000 | Loss: 0.00014692
Iteration 19/1000 | Loss: 0.00017203
Iteration 20/1000 | Loss: 0.00051446
Iteration 21/1000 | Loss: 0.00019856
Iteration 22/1000 | Loss: 0.00006681
Iteration 23/1000 | Loss: 0.00005090
Iteration 24/1000 | Loss: 0.00004240
Iteration 25/1000 | Loss: 0.00003681
Iteration 26/1000 | Loss: 0.00003054
Iteration 27/1000 | Loss: 0.00002642
Iteration 28/1000 | Loss: 0.00002247
Iteration 29/1000 | Loss: 0.00002038
Iteration 30/1000 | Loss: 0.00001853
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001600
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001395
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001336
Iteration 40/1000 | Loss: 0.00001336
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001334
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001332
Iteration 48/1000 | Loss: 0.00001331
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001329
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001329
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001322
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001309
Iteration 109/1000 | Loss: 0.00001309
Iteration 110/1000 | Loss: 0.00001309
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001309
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001308
Iteration 124/1000 | Loss: 0.00001308
Iteration 125/1000 | Loss: 0.00001307
Iteration 126/1000 | Loss: 0.00001307
Iteration 127/1000 | Loss: 0.00001307
Iteration 128/1000 | Loss: 0.00001307
Iteration 129/1000 | Loss: 0.00001307
Iteration 130/1000 | Loss: 0.00001307
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001307
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.306603826378705e-05, 1.306603826378705e-05, 1.306603826378705e-05, 1.306603826378705e-05, 1.306603826378705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.306603826378705e-05

Optimization complete. Final v2v error: 3.0836610794067383 mm

Highest mean error: 3.5389113426208496 mm for frame 223

Lowest mean error: 2.957432746887207 mm for frame 84

Saving results

Total time: 90.88368368148804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917732
Iteration 2/25 | Loss: 0.00142689
Iteration 3/25 | Loss: 0.00131246
Iteration 4/25 | Loss: 0.00121991
Iteration 5/25 | Loss: 0.00119539
Iteration 6/25 | Loss: 0.00116858
Iteration 7/25 | Loss: 0.00116767
Iteration 8/25 | Loss: 0.00116247
Iteration 9/25 | Loss: 0.00116092
Iteration 10/25 | Loss: 0.00116050
Iteration 11/25 | Loss: 0.00115996
Iteration 12/25 | Loss: 0.00115818
Iteration 13/25 | Loss: 0.00116272
Iteration 14/25 | Loss: 0.00115998
Iteration 15/25 | Loss: 0.00115330
Iteration 16/25 | Loss: 0.00115148
Iteration 17/25 | Loss: 0.00115114
Iteration 18/25 | Loss: 0.00115089
Iteration 19/25 | Loss: 0.00115063
Iteration 20/25 | Loss: 0.00115043
Iteration 21/25 | Loss: 0.00115035
Iteration 22/25 | Loss: 0.00115034
Iteration 23/25 | Loss: 0.00115034
Iteration 24/25 | Loss: 0.00115033
Iteration 25/25 | Loss: 0.00115033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28710365
Iteration 2/25 | Loss: 0.00079311
Iteration 3/25 | Loss: 0.00079310
Iteration 4/25 | Loss: 0.00079310
Iteration 5/25 | Loss: 0.00079310
Iteration 6/25 | Loss: 0.00079310
Iteration 7/25 | Loss: 0.00079310
Iteration 8/25 | Loss: 0.00079310
Iteration 9/25 | Loss: 0.00079310
Iteration 10/25 | Loss: 0.00079310
Iteration 11/25 | Loss: 0.00079310
Iteration 12/25 | Loss: 0.00079310
Iteration 13/25 | Loss: 0.00079310
Iteration 14/25 | Loss: 0.00079310
Iteration 15/25 | Loss: 0.00079310
Iteration 16/25 | Loss: 0.00079310
Iteration 17/25 | Loss: 0.00079310
Iteration 18/25 | Loss: 0.00079310
Iteration 19/25 | Loss: 0.00079310
Iteration 20/25 | Loss: 0.00079310
Iteration 21/25 | Loss: 0.00079310
Iteration 22/25 | Loss: 0.00079310
Iteration 23/25 | Loss: 0.00079310
Iteration 24/25 | Loss: 0.00079310
Iteration 25/25 | Loss: 0.00079310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079310
Iteration 2/1000 | Loss: 0.00005414
Iteration 3/1000 | Loss: 0.00003872
Iteration 4/1000 | Loss: 0.00003214
Iteration 5/1000 | Loss: 0.00002906
Iteration 6/1000 | Loss: 0.00002711
Iteration 7/1000 | Loss: 0.00002605
Iteration 8/1000 | Loss: 0.00061054
Iteration 9/1000 | Loss: 0.00090390
Iteration 10/1000 | Loss: 0.00006605
Iteration 11/1000 | Loss: 0.00003937
Iteration 12/1000 | Loss: 0.00002723
Iteration 13/1000 | Loss: 0.00002398
Iteration 14/1000 | Loss: 0.00002302
Iteration 15/1000 | Loss: 0.00002228
Iteration 16/1000 | Loss: 0.00002170
Iteration 17/1000 | Loss: 0.00002117
Iteration 18/1000 | Loss: 0.00002080
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002014
Iteration 22/1000 | Loss: 0.00002005
Iteration 23/1000 | Loss: 0.00002003
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001995
Iteration 26/1000 | Loss: 0.00001994
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001990
Iteration 29/1000 | Loss: 0.00001989
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001976
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001967
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00001966
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001961
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001959
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001957
Iteration 54/1000 | Loss: 0.00001957
Iteration 55/1000 | Loss: 0.00001957
Iteration 56/1000 | Loss: 0.00001957
Iteration 57/1000 | Loss: 0.00001957
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001956
Iteration 63/1000 | Loss: 0.00001956
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001955
Iteration 68/1000 | Loss: 0.00001955
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001954
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00001953
Iteration 73/1000 | Loss: 0.00001953
Iteration 74/1000 | Loss: 0.00001953
Iteration 75/1000 | Loss: 0.00001953
Iteration 76/1000 | Loss: 0.00001952
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001951
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001951
Iteration 82/1000 | Loss: 0.00001950
Iteration 83/1000 | Loss: 0.00001950
Iteration 84/1000 | Loss: 0.00001950
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001947
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001946
Iteration 93/1000 | Loss: 0.00001946
Iteration 94/1000 | Loss: 0.00001946
Iteration 95/1000 | Loss: 0.00001945
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001944
Iteration 98/1000 | Loss: 0.00001944
Iteration 99/1000 | Loss: 0.00001944
Iteration 100/1000 | Loss: 0.00001944
Iteration 101/1000 | Loss: 0.00001943
Iteration 102/1000 | Loss: 0.00001943
Iteration 103/1000 | Loss: 0.00001943
Iteration 104/1000 | Loss: 0.00001942
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001942
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001939
Iteration 118/1000 | Loss: 0.00001939
Iteration 119/1000 | Loss: 0.00001939
Iteration 120/1000 | Loss: 0.00001939
Iteration 121/1000 | Loss: 0.00001939
Iteration 122/1000 | Loss: 0.00001938
Iteration 123/1000 | Loss: 0.00001938
Iteration 124/1000 | Loss: 0.00001938
Iteration 125/1000 | Loss: 0.00001938
Iteration 126/1000 | Loss: 0.00001938
Iteration 127/1000 | Loss: 0.00001938
Iteration 128/1000 | Loss: 0.00001938
Iteration 129/1000 | Loss: 0.00001938
Iteration 130/1000 | Loss: 0.00001938
Iteration 131/1000 | Loss: 0.00001937
Iteration 132/1000 | Loss: 0.00001937
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001937
Iteration 135/1000 | Loss: 0.00001936
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001936
Iteration 138/1000 | Loss: 0.00001936
Iteration 139/1000 | Loss: 0.00001936
Iteration 140/1000 | Loss: 0.00001936
Iteration 141/1000 | Loss: 0.00001936
Iteration 142/1000 | Loss: 0.00001936
Iteration 143/1000 | Loss: 0.00001936
Iteration 144/1000 | Loss: 0.00001936
Iteration 145/1000 | Loss: 0.00001935
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001935
Iteration 148/1000 | Loss: 0.00001935
Iteration 149/1000 | Loss: 0.00001934
Iteration 150/1000 | Loss: 0.00001934
Iteration 151/1000 | Loss: 0.00001934
Iteration 152/1000 | Loss: 0.00001934
Iteration 153/1000 | Loss: 0.00001934
Iteration 154/1000 | Loss: 0.00001934
Iteration 155/1000 | Loss: 0.00001934
Iteration 156/1000 | Loss: 0.00001934
Iteration 157/1000 | Loss: 0.00001934
Iteration 158/1000 | Loss: 0.00001933
Iteration 159/1000 | Loss: 0.00001933
Iteration 160/1000 | Loss: 0.00001933
Iteration 161/1000 | Loss: 0.00001932
Iteration 162/1000 | Loss: 0.00001932
Iteration 163/1000 | Loss: 0.00001932
Iteration 164/1000 | Loss: 0.00001932
Iteration 165/1000 | Loss: 0.00001932
Iteration 166/1000 | Loss: 0.00001932
Iteration 167/1000 | Loss: 0.00001932
Iteration 168/1000 | Loss: 0.00001932
Iteration 169/1000 | Loss: 0.00001932
Iteration 170/1000 | Loss: 0.00001932
Iteration 171/1000 | Loss: 0.00001932
Iteration 172/1000 | Loss: 0.00001931
Iteration 173/1000 | Loss: 0.00001931
Iteration 174/1000 | Loss: 0.00001931
Iteration 175/1000 | Loss: 0.00001931
Iteration 176/1000 | Loss: 0.00001931
Iteration 177/1000 | Loss: 0.00001931
Iteration 178/1000 | Loss: 0.00001931
Iteration 179/1000 | Loss: 0.00001931
Iteration 180/1000 | Loss: 0.00001931
Iteration 181/1000 | Loss: 0.00001931
Iteration 182/1000 | Loss: 0.00001930
Iteration 183/1000 | Loss: 0.00001930
Iteration 184/1000 | Loss: 0.00001930
Iteration 185/1000 | Loss: 0.00001930
Iteration 186/1000 | Loss: 0.00001930
Iteration 187/1000 | Loss: 0.00001930
Iteration 188/1000 | Loss: 0.00001930
Iteration 189/1000 | Loss: 0.00001930
Iteration 190/1000 | Loss: 0.00001930
Iteration 191/1000 | Loss: 0.00001930
Iteration 192/1000 | Loss: 0.00001930
Iteration 193/1000 | Loss: 0.00001930
Iteration 194/1000 | Loss: 0.00001930
Iteration 195/1000 | Loss: 0.00001930
Iteration 196/1000 | Loss: 0.00001929
Iteration 197/1000 | Loss: 0.00001929
Iteration 198/1000 | Loss: 0.00001929
Iteration 199/1000 | Loss: 0.00001929
Iteration 200/1000 | Loss: 0.00001929
Iteration 201/1000 | Loss: 0.00001929
Iteration 202/1000 | Loss: 0.00001929
Iteration 203/1000 | Loss: 0.00001929
Iteration 204/1000 | Loss: 0.00001929
Iteration 205/1000 | Loss: 0.00001929
Iteration 206/1000 | Loss: 0.00001929
Iteration 207/1000 | Loss: 0.00001929
Iteration 208/1000 | Loss: 0.00001929
Iteration 209/1000 | Loss: 0.00001929
Iteration 210/1000 | Loss: 0.00001929
Iteration 211/1000 | Loss: 0.00001929
Iteration 212/1000 | Loss: 0.00001929
Iteration 213/1000 | Loss: 0.00001929
Iteration 214/1000 | Loss: 0.00001929
Iteration 215/1000 | Loss: 0.00001929
Iteration 216/1000 | Loss: 0.00001928
Iteration 217/1000 | Loss: 0.00001928
Iteration 218/1000 | Loss: 0.00001928
Iteration 219/1000 | Loss: 0.00001928
Iteration 220/1000 | Loss: 0.00001928
Iteration 221/1000 | Loss: 0.00001928
Iteration 222/1000 | Loss: 0.00001928
Iteration 223/1000 | Loss: 0.00001928
Iteration 224/1000 | Loss: 0.00001928
Iteration 225/1000 | Loss: 0.00001928
Iteration 226/1000 | Loss: 0.00001928
Iteration 227/1000 | Loss: 0.00001928
Iteration 228/1000 | Loss: 0.00001928
Iteration 229/1000 | Loss: 0.00001928
Iteration 230/1000 | Loss: 0.00001928
Iteration 231/1000 | Loss: 0.00001928
Iteration 232/1000 | Loss: 0.00001928
Iteration 233/1000 | Loss: 0.00001928
Iteration 234/1000 | Loss: 0.00001928
Iteration 235/1000 | Loss: 0.00001928
Iteration 236/1000 | Loss: 0.00001928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.928192432387732e-05, 1.928192432387732e-05, 1.928192432387732e-05, 1.928192432387732e-05, 1.928192432387732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.928192432387732e-05

Optimization complete. Final v2v error: 3.687809705734253 mm

Highest mean error: 5.285343647003174 mm for frame 136

Lowest mean error: 3.110870361328125 mm for frame 69

Saving results

Total time: 85.27624082565308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972764
Iteration 2/25 | Loss: 0.00149455
Iteration 3/25 | Loss: 0.00120333
Iteration 4/25 | Loss: 0.00117809
Iteration 5/25 | Loss: 0.00117614
Iteration 6/25 | Loss: 0.00117614
Iteration 7/25 | Loss: 0.00117614
Iteration 8/25 | Loss: 0.00117614
Iteration 9/25 | Loss: 0.00117614
Iteration 10/25 | Loss: 0.00117614
Iteration 11/25 | Loss: 0.00117614
Iteration 12/25 | Loss: 0.00117614
Iteration 13/25 | Loss: 0.00117614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011761419009417295, 0.0011761419009417295, 0.0011761419009417295, 0.0011761419009417295, 0.0011761419009417295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011761419009417295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35088813
Iteration 2/25 | Loss: 0.00061479
Iteration 3/25 | Loss: 0.00061479
Iteration 4/25 | Loss: 0.00061479
Iteration 5/25 | Loss: 0.00061479
Iteration 6/25 | Loss: 0.00061479
Iteration 7/25 | Loss: 0.00061479
Iteration 8/25 | Loss: 0.00061479
Iteration 9/25 | Loss: 0.00061479
Iteration 10/25 | Loss: 0.00061479
Iteration 11/25 | Loss: 0.00061479
Iteration 12/25 | Loss: 0.00061479
Iteration 13/25 | Loss: 0.00061479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000614791177213192, 0.000614791177213192, 0.000614791177213192, 0.000614791177213192, 0.000614791177213192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000614791177213192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061479
Iteration 2/1000 | Loss: 0.00002711
Iteration 3/1000 | Loss: 0.00002036
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001859
Iteration 6/1000 | Loss: 0.00001812
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001727
Iteration 10/1000 | Loss: 0.00001708
Iteration 11/1000 | Loss: 0.00001686
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001660
Iteration 19/1000 | Loss: 0.00001658
Iteration 20/1000 | Loss: 0.00001657
Iteration 21/1000 | Loss: 0.00001657
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001656
Iteration 24/1000 | Loss: 0.00001656
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001653
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001651
Iteration 34/1000 | Loss: 0.00001651
Iteration 35/1000 | Loss: 0.00001651
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001650
Iteration 38/1000 | Loss: 0.00001650
Iteration 39/1000 | Loss: 0.00001647
Iteration 40/1000 | Loss: 0.00001647
Iteration 41/1000 | Loss: 0.00001647
Iteration 42/1000 | Loss: 0.00001647
Iteration 43/1000 | Loss: 0.00001647
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001646
Iteration 48/1000 | Loss: 0.00001646
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001645
Iteration 53/1000 | Loss: 0.00001645
Iteration 54/1000 | Loss: 0.00001644
Iteration 55/1000 | Loss: 0.00001644
Iteration 56/1000 | Loss: 0.00001644
Iteration 57/1000 | Loss: 0.00001644
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001642
Iteration 61/1000 | Loss: 0.00001642
Iteration 62/1000 | Loss: 0.00001642
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001639
Iteration 82/1000 | Loss: 0.00001639
Iteration 83/1000 | Loss: 0.00001639
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001638
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001637
Iteration 96/1000 | Loss: 0.00001637
Iteration 97/1000 | Loss: 0.00001637
Iteration 98/1000 | Loss: 0.00001637
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001637
Iteration 101/1000 | Loss: 0.00001637
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001635
Iteration 116/1000 | Loss: 0.00001635
Iteration 117/1000 | Loss: 0.00001635
Iteration 118/1000 | Loss: 0.00001635
Iteration 119/1000 | Loss: 0.00001635
Iteration 120/1000 | Loss: 0.00001635
Iteration 121/1000 | Loss: 0.00001635
Iteration 122/1000 | Loss: 0.00001635
Iteration 123/1000 | Loss: 0.00001635
Iteration 124/1000 | Loss: 0.00001635
Iteration 125/1000 | Loss: 0.00001635
Iteration 126/1000 | Loss: 0.00001635
Iteration 127/1000 | Loss: 0.00001635
Iteration 128/1000 | Loss: 0.00001635
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001635
Iteration 136/1000 | Loss: 0.00001635
Iteration 137/1000 | Loss: 0.00001635
Iteration 138/1000 | Loss: 0.00001635
Iteration 139/1000 | Loss: 0.00001635
Iteration 140/1000 | Loss: 0.00001635
Iteration 141/1000 | Loss: 0.00001635
Iteration 142/1000 | Loss: 0.00001635
Iteration 143/1000 | Loss: 0.00001635
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001635
Iteration 149/1000 | Loss: 0.00001635
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001635
Iteration 154/1000 | Loss: 0.00001635
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001635
Iteration 158/1000 | Loss: 0.00001635
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001635
Iteration 161/1000 | Loss: 0.00001635
Iteration 162/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.6345706171705388e-05, 1.6345706171705388e-05, 1.6345706171705388e-05, 1.6345706171705388e-05, 1.6345706171705388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6345706171705388e-05

Optimization complete. Final v2v error: 3.4174699783325195 mm

Highest mean error: 3.5794098377227783 mm for frame 112

Lowest mean error: 2.69290828704834 mm for frame 3

Saving results

Total time: 33.15749931335449
