Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=283, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15848-15903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01191497
Iteration 2/25 | Loss: 0.01191497
Iteration 3/25 | Loss: 0.01191497
Iteration 4/25 | Loss: 0.01191497
Iteration 5/25 | Loss: 0.01191497
Iteration 6/25 | Loss: 0.01191496
Iteration 7/25 | Loss: 0.01191496
Iteration 8/25 | Loss: 0.01191496
Iteration 9/25 | Loss: 0.01191496
Iteration 10/25 | Loss: 0.01191496
Iteration 11/25 | Loss: 0.01191496
Iteration 12/25 | Loss: 0.01191496
Iteration 13/25 | Loss: 0.01191496
Iteration 14/25 | Loss: 0.01191496
Iteration 15/25 | Loss: 0.01191496
Iteration 16/25 | Loss: 0.01191496
Iteration 17/25 | Loss: 0.01191496
Iteration 18/25 | Loss: 0.01191496
Iteration 19/25 | Loss: 0.01191496
Iteration 20/25 | Loss: 0.01191496
Iteration 21/25 | Loss: 0.01191496
Iteration 22/25 | Loss: 0.01191496
Iteration 23/25 | Loss: 0.01191495
Iteration 24/25 | Loss: 0.01191495
Iteration 25/25 | Loss: 0.01191495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74273646
Iteration 2/25 | Loss: 0.11605247
Iteration 3/25 | Loss: 0.11587942
Iteration 4/25 | Loss: 0.11544003
Iteration 5/25 | Loss: 0.11543342
Iteration 6/25 | Loss: 0.11543342
Iteration 7/25 | Loss: 0.11543341
Iteration 8/25 | Loss: 0.11543341
Iteration 9/25 | Loss: 0.11543341
Iteration 10/25 | Loss: 0.11543341
Iteration 11/25 | Loss: 0.11543341
Iteration 12/25 | Loss: 0.11543341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.11543340981006622, 0.11543340981006622, 0.11543340981006622, 0.11543340981006622, 0.11543340981006622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11543340981006622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11543341
Iteration 2/1000 | Loss: 0.00155233
Iteration 3/1000 | Loss: 0.00088308
Iteration 4/1000 | Loss: 0.00040746
Iteration 5/1000 | Loss: 0.00035499
Iteration 6/1000 | Loss: 0.00026545
Iteration 7/1000 | Loss: 0.00028910
Iteration 8/1000 | Loss: 0.00015436
Iteration 9/1000 | Loss: 0.00023864
Iteration 10/1000 | Loss: 0.00007044
Iteration 11/1000 | Loss: 0.00007002
Iteration 12/1000 | Loss: 0.00046348
Iteration 13/1000 | Loss: 0.00254547
Iteration 14/1000 | Loss: 0.00007296
Iteration 15/1000 | Loss: 0.00010543
Iteration 16/1000 | Loss: 0.00010192
Iteration 17/1000 | Loss: 0.00013506
Iteration 18/1000 | Loss: 0.00003515
Iteration 19/1000 | Loss: 0.00006186
Iteration 20/1000 | Loss: 0.00008395
Iteration 21/1000 | Loss: 0.00005420
Iteration 22/1000 | Loss: 0.00005137
Iteration 23/1000 | Loss: 0.00004315
Iteration 24/1000 | Loss: 0.00004561
Iteration 25/1000 | Loss: 0.00003550
Iteration 26/1000 | Loss: 0.00004600
Iteration 27/1000 | Loss: 0.00002233
Iteration 28/1000 | Loss: 0.00004257
Iteration 29/1000 | Loss: 0.00005962
Iteration 30/1000 | Loss: 0.00007168
Iteration 31/1000 | Loss: 0.00003602
Iteration 32/1000 | Loss: 0.00002050
Iteration 33/1000 | Loss: 0.00007609
Iteration 34/1000 | Loss: 0.00003637
Iteration 35/1000 | Loss: 0.00003511
Iteration 36/1000 | Loss: 0.00002397
Iteration 37/1000 | Loss: 0.00003169
Iteration 38/1000 | Loss: 0.00005161
Iteration 39/1000 | Loss: 0.00005499
Iteration 40/1000 | Loss: 0.00003871
Iteration 41/1000 | Loss: 0.00004907
Iteration 42/1000 | Loss: 0.00002995
Iteration 43/1000 | Loss: 0.00006492
Iteration 44/1000 | Loss: 0.00002385
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00003564
Iteration 49/1000 | Loss: 0.00002061
Iteration 50/1000 | Loss: 0.00002630
Iteration 51/1000 | Loss: 0.00001851
Iteration 52/1000 | Loss: 0.00003301
Iteration 53/1000 | Loss: 0.00001940
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00001832
Iteration 56/1000 | Loss: 0.00001848
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001840
Iteration 59/1000 | Loss: 0.00001840
Iteration 60/1000 | Loss: 0.00001840
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001836
Iteration 65/1000 | Loss: 0.00003022
Iteration 66/1000 | Loss: 0.00001872
Iteration 67/1000 | Loss: 0.00002946
Iteration 68/1000 | Loss: 0.00003218
Iteration 69/1000 | Loss: 0.00001848
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001810
Iteration 76/1000 | Loss: 0.00001813
Iteration 77/1000 | Loss: 0.00001813
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001808
Iteration 92/1000 | Loss: 0.00002503
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001806
Iteration 101/1000 | Loss: 0.00001806
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001806
Iteration 110/1000 | Loss: 0.00001806
Iteration 111/1000 | Loss: 0.00001806
Iteration 112/1000 | Loss: 0.00001806
Iteration 113/1000 | Loss: 0.00001806
Iteration 114/1000 | Loss: 0.00001806
Iteration 115/1000 | Loss: 0.00001806
Iteration 116/1000 | Loss: 0.00001806
Iteration 117/1000 | Loss: 0.00001806
Iteration 118/1000 | Loss: 0.00001806
Iteration 119/1000 | Loss: 0.00001806
Iteration 120/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.8060980437439866e-05, 1.8060980437439866e-05, 1.8060980437439866e-05, 1.8060980437439866e-05, 1.8060980437439866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8060980437439866e-05

Optimization complete. Final v2v error: 3.57768177986145 mm

Highest mean error: 11.831012725830078 mm for frame 214

Lowest mean error: 3.2083935737609863 mm for frame 54

Saving results

Total time: 104.22334003448486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955124
Iteration 2/25 | Loss: 0.00192700
Iteration 3/25 | Loss: 0.00156308
Iteration 4/25 | Loss: 0.00153613
Iteration 5/25 | Loss: 0.00152969
Iteration 6/25 | Loss: 0.00152866
Iteration 7/25 | Loss: 0.00152866
Iteration 8/25 | Loss: 0.00152866
Iteration 9/25 | Loss: 0.00152866
Iteration 10/25 | Loss: 0.00152866
Iteration 11/25 | Loss: 0.00152866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015286632115021348, 0.0015286632115021348, 0.0015286632115021348, 0.0015286632115021348, 0.0015286632115021348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015286632115021348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62752950
Iteration 2/25 | Loss: 0.00251205
Iteration 3/25 | Loss: 0.00251205
Iteration 4/25 | Loss: 0.00251205
Iteration 5/25 | Loss: 0.00251205
Iteration 6/25 | Loss: 0.00251205
Iteration 7/25 | Loss: 0.00251205
Iteration 8/25 | Loss: 0.00251205
Iteration 9/25 | Loss: 0.00251205
Iteration 10/25 | Loss: 0.00251205
Iteration 11/25 | Loss: 0.00251205
Iteration 12/25 | Loss: 0.00251205
Iteration 13/25 | Loss: 0.00251205
Iteration 14/25 | Loss: 0.00251205
Iteration 15/25 | Loss: 0.00251205
Iteration 16/25 | Loss: 0.00251205
Iteration 17/25 | Loss: 0.00251205
Iteration 18/25 | Loss: 0.00251205
Iteration 19/25 | Loss: 0.00251205
Iteration 20/25 | Loss: 0.00251205
Iteration 21/25 | Loss: 0.00251205
Iteration 22/25 | Loss: 0.00251205
Iteration 23/25 | Loss: 0.00251205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0025120482314378023, 0.0025120482314378023, 0.0025120482314378023, 0.0025120482314378023, 0.0025120482314378023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025120482314378023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251205
Iteration 2/1000 | Loss: 0.00004214
Iteration 3/1000 | Loss: 0.00003533
Iteration 4/1000 | Loss: 0.00003257
Iteration 5/1000 | Loss: 0.00003127
Iteration 6/1000 | Loss: 0.00003027
Iteration 7/1000 | Loss: 0.00002968
Iteration 8/1000 | Loss: 0.00002928
Iteration 9/1000 | Loss: 0.00002893
Iteration 10/1000 | Loss: 0.00002857
Iteration 11/1000 | Loss: 0.00002834
Iteration 12/1000 | Loss: 0.00002820
Iteration 13/1000 | Loss: 0.00002818
Iteration 14/1000 | Loss: 0.00002812
Iteration 15/1000 | Loss: 0.00002812
Iteration 16/1000 | Loss: 0.00002810
Iteration 17/1000 | Loss: 0.00002809
Iteration 18/1000 | Loss: 0.00002808
Iteration 19/1000 | Loss: 0.00002807
Iteration 20/1000 | Loss: 0.00002807
Iteration 21/1000 | Loss: 0.00002804
Iteration 22/1000 | Loss: 0.00002804
Iteration 23/1000 | Loss: 0.00002803
Iteration 24/1000 | Loss: 0.00002803
Iteration 25/1000 | Loss: 0.00002803
Iteration 26/1000 | Loss: 0.00002802
Iteration 27/1000 | Loss: 0.00002801
Iteration 28/1000 | Loss: 0.00002800
Iteration 29/1000 | Loss: 0.00002800
Iteration 30/1000 | Loss: 0.00002800
Iteration 31/1000 | Loss: 0.00002799
Iteration 32/1000 | Loss: 0.00002799
Iteration 33/1000 | Loss: 0.00002799
Iteration 34/1000 | Loss: 0.00002798
Iteration 35/1000 | Loss: 0.00002798
Iteration 36/1000 | Loss: 0.00002798
Iteration 37/1000 | Loss: 0.00002797
Iteration 38/1000 | Loss: 0.00002797
Iteration 39/1000 | Loss: 0.00002796
Iteration 40/1000 | Loss: 0.00002796
Iteration 41/1000 | Loss: 0.00002796
Iteration 42/1000 | Loss: 0.00002796
Iteration 43/1000 | Loss: 0.00002796
Iteration 44/1000 | Loss: 0.00002796
Iteration 45/1000 | Loss: 0.00002795
Iteration 46/1000 | Loss: 0.00002795
Iteration 47/1000 | Loss: 0.00002795
Iteration 48/1000 | Loss: 0.00002794
Iteration 49/1000 | Loss: 0.00002794
Iteration 50/1000 | Loss: 0.00002794
Iteration 51/1000 | Loss: 0.00002794
Iteration 52/1000 | Loss: 0.00002793
Iteration 53/1000 | Loss: 0.00002793
Iteration 54/1000 | Loss: 0.00002792
Iteration 55/1000 | Loss: 0.00002792
Iteration 56/1000 | Loss: 0.00002792
Iteration 57/1000 | Loss: 0.00002792
Iteration 58/1000 | Loss: 0.00002792
Iteration 59/1000 | Loss: 0.00002792
Iteration 60/1000 | Loss: 0.00002792
Iteration 61/1000 | Loss: 0.00002791
Iteration 62/1000 | Loss: 0.00002791
Iteration 63/1000 | Loss: 0.00002791
Iteration 64/1000 | Loss: 0.00002791
Iteration 65/1000 | Loss: 0.00002791
Iteration 66/1000 | Loss: 0.00002791
Iteration 67/1000 | Loss: 0.00002791
Iteration 68/1000 | Loss: 0.00002790
Iteration 69/1000 | Loss: 0.00002790
Iteration 70/1000 | Loss: 0.00002790
Iteration 71/1000 | Loss: 0.00002790
Iteration 72/1000 | Loss: 0.00002790
Iteration 73/1000 | Loss: 0.00002790
Iteration 74/1000 | Loss: 0.00002790
Iteration 75/1000 | Loss: 0.00002790
Iteration 76/1000 | Loss: 0.00002790
Iteration 77/1000 | Loss: 0.00002789
Iteration 78/1000 | Loss: 0.00002789
Iteration 79/1000 | Loss: 0.00002789
Iteration 80/1000 | Loss: 0.00002789
Iteration 81/1000 | Loss: 0.00002789
Iteration 82/1000 | Loss: 0.00002789
Iteration 83/1000 | Loss: 0.00002789
Iteration 84/1000 | Loss: 0.00002789
Iteration 85/1000 | Loss: 0.00002788
Iteration 86/1000 | Loss: 0.00002788
Iteration 87/1000 | Loss: 0.00002788
Iteration 88/1000 | Loss: 0.00002788
Iteration 89/1000 | Loss: 0.00002788
Iteration 90/1000 | Loss: 0.00002788
Iteration 91/1000 | Loss: 0.00002788
Iteration 92/1000 | Loss: 0.00002788
Iteration 93/1000 | Loss: 0.00002788
Iteration 94/1000 | Loss: 0.00002787
Iteration 95/1000 | Loss: 0.00002787
Iteration 96/1000 | Loss: 0.00002787
Iteration 97/1000 | Loss: 0.00002787
Iteration 98/1000 | Loss: 0.00002787
Iteration 99/1000 | Loss: 0.00002787
Iteration 100/1000 | Loss: 0.00002787
Iteration 101/1000 | Loss: 0.00002787
Iteration 102/1000 | Loss: 0.00002787
Iteration 103/1000 | Loss: 0.00002787
Iteration 104/1000 | Loss: 0.00002787
Iteration 105/1000 | Loss: 0.00002787
Iteration 106/1000 | Loss: 0.00002787
Iteration 107/1000 | Loss: 0.00002787
Iteration 108/1000 | Loss: 0.00002787
Iteration 109/1000 | Loss: 0.00002787
Iteration 110/1000 | Loss: 0.00002787
Iteration 111/1000 | Loss: 0.00002787
Iteration 112/1000 | Loss: 0.00002787
Iteration 113/1000 | Loss: 0.00002787
Iteration 114/1000 | Loss: 0.00002787
Iteration 115/1000 | Loss: 0.00002787
Iteration 116/1000 | Loss: 0.00002787
Iteration 117/1000 | Loss: 0.00002787
Iteration 118/1000 | Loss: 0.00002787
Iteration 119/1000 | Loss: 0.00002787
Iteration 120/1000 | Loss: 0.00002787
Iteration 121/1000 | Loss: 0.00002787
Iteration 122/1000 | Loss: 0.00002787
Iteration 123/1000 | Loss: 0.00002787
Iteration 124/1000 | Loss: 0.00002787
Iteration 125/1000 | Loss: 0.00002787
Iteration 126/1000 | Loss: 0.00002787
Iteration 127/1000 | Loss: 0.00002787
Iteration 128/1000 | Loss: 0.00002787
Iteration 129/1000 | Loss: 0.00002787
Iteration 130/1000 | Loss: 0.00002787
Iteration 131/1000 | Loss: 0.00002787
Iteration 132/1000 | Loss: 0.00002787
Iteration 133/1000 | Loss: 0.00002787
Iteration 134/1000 | Loss: 0.00002787
Iteration 135/1000 | Loss: 0.00002787
Iteration 136/1000 | Loss: 0.00002787
Iteration 137/1000 | Loss: 0.00002787
Iteration 138/1000 | Loss: 0.00002787
Iteration 139/1000 | Loss: 0.00002787
Iteration 140/1000 | Loss: 0.00002787
Iteration 141/1000 | Loss: 0.00002787
Iteration 142/1000 | Loss: 0.00002787
Iteration 143/1000 | Loss: 0.00002787
Iteration 144/1000 | Loss: 0.00002787
Iteration 145/1000 | Loss: 0.00002787
Iteration 146/1000 | Loss: 0.00002787
Iteration 147/1000 | Loss: 0.00002787
Iteration 148/1000 | Loss: 0.00002787
Iteration 149/1000 | Loss: 0.00002787
Iteration 150/1000 | Loss: 0.00002787
Iteration 151/1000 | Loss: 0.00002787
Iteration 152/1000 | Loss: 0.00002787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.7870197300217114e-05, 2.7870197300217114e-05, 2.7870197300217114e-05, 2.7870197300217114e-05, 2.7870197300217114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7870197300217114e-05

Optimization complete. Final v2v error: 4.534150123596191 mm

Highest mean error: 5.465983867645264 mm for frame 198

Lowest mean error: 3.8302783966064453 mm for frame 102

Saving results

Total time: 39.8790078163147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845666
Iteration 2/25 | Loss: 0.00123586
Iteration 3/25 | Loss: 0.00114599
Iteration 4/25 | Loss: 0.00112337
Iteration 5/25 | Loss: 0.00111777
Iteration 6/25 | Loss: 0.00111617
Iteration 7/25 | Loss: 0.00111617
Iteration 8/25 | Loss: 0.00111617
Iteration 9/25 | Loss: 0.00111617
Iteration 10/25 | Loss: 0.00111617
Iteration 11/25 | Loss: 0.00111617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011161703150719404, 0.0011161703150719404, 0.0011161703150719404, 0.0011161703150719404, 0.0011161703150719404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011161703150719404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18795180
Iteration 2/25 | Loss: 0.00235934
Iteration 3/25 | Loss: 0.00235934
Iteration 4/25 | Loss: 0.00235934
Iteration 5/25 | Loss: 0.00235934
Iteration 6/25 | Loss: 0.00235934
Iteration 7/25 | Loss: 0.00235934
Iteration 8/25 | Loss: 0.00235934
Iteration 9/25 | Loss: 0.00235934
Iteration 10/25 | Loss: 0.00235934
Iteration 11/25 | Loss: 0.00235934
Iteration 12/25 | Loss: 0.00235934
Iteration 13/25 | Loss: 0.00235934
Iteration 14/25 | Loss: 0.00235934
Iteration 15/25 | Loss: 0.00235933
Iteration 16/25 | Loss: 0.00235933
Iteration 17/25 | Loss: 0.00235933
Iteration 18/25 | Loss: 0.00235933
Iteration 19/25 | Loss: 0.00235933
Iteration 20/25 | Loss: 0.00235933
Iteration 21/25 | Loss: 0.00235933
Iteration 22/25 | Loss: 0.00235933
Iteration 23/25 | Loss: 0.00235933
Iteration 24/25 | Loss: 0.00235933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0023593346122652292, 0.0023593346122652292, 0.0023593346122652292, 0.0023593346122652292, 0.0023593346122652292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023593346122652292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235933
Iteration 2/1000 | Loss: 0.00005895
Iteration 3/1000 | Loss: 0.00002744
Iteration 4/1000 | Loss: 0.00002410
Iteration 5/1000 | Loss: 0.00002258
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001922
Iteration 9/1000 | Loss: 0.00001872
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001732
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001721
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001721
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001720
Iteration 23/1000 | Loss: 0.00001720
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001718
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001712
Iteration 50/1000 | Loss: 0.00001712
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001711
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001709
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001709
Iteration 73/1000 | Loss: 0.00001709
Iteration 74/1000 | Loss: 0.00001709
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.7084006685763597e-05, 1.7084006685763597e-05, 1.7084006685763597e-05, 1.7084006685763597e-05, 1.7084006685763597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7084006685763597e-05

Optimization complete. Final v2v error: 3.6324687004089355 mm

Highest mean error: 3.8044192790985107 mm for frame 66

Lowest mean error: 3.4374053478240967 mm for frame 83

Saving results

Total time: 32.98945331573486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069539
Iteration 2/25 | Loss: 0.00174774
Iteration 3/25 | Loss: 0.00160005
Iteration 4/25 | Loss: 0.00113619
Iteration 5/25 | Loss: 0.00113715
Iteration 6/25 | Loss: 0.00108632
Iteration 7/25 | Loss: 0.00108124
Iteration 8/25 | Loss: 0.00107967
Iteration 9/25 | Loss: 0.00108056
Iteration 10/25 | Loss: 0.00107892
Iteration 11/25 | Loss: 0.00107846
Iteration 12/25 | Loss: 0.00107841
Iteration 13/25 | Loss: 0.00107841
Iteration 14/25 | Loss: 0.00107841
Iteration 15/25 | Loss: 0.00107841
Iteration 16/25 | Loss: 0.00107841
Iteration 17/25 | Loss: 0.00107841
Iteration 18/25 | Loss: 0.00107840
Iteration 19/25 | Loss: 0.00107840
Iteration 20/25 | Loss: 0.00107840
Iteration 21/25 | Loss: 0.00107840
Iteration 22/25 | Loss: 0.00107840
Iteration 23/25 | Loss: 0.00107837
Iteration 24/25 | Loss: 0.00107836
Iteration 25/25 | Loss: 0.00107835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.72814322
Iteration 2/25 | Loss: 0.00236401
Iteration 3/25 | Loss: 0.00232263
Iteration 4/25 | Loss: 0.00232262
Iteration 5/25 | Loss: 0.00232262
Iteration 6/25 | Loss: 0.00232262
Iteration 7/25 | Loss: 0.00232262
Iteration 8/25 | Loss: 0.00232262
Iteration 9/25 | Loss: 0.00232262
Iteration 10/25 | Loss: 0.00232262
Iteration 11/25 | Loss: 0.00232262
Iteration 12/25 | Loss: 0.00232262
Iteration 13/25 | Loss: 0.00232262
Iteration 14/25 | Loss: 0.00232262
Iteration 15/25 | Loss: 0.00232262
Iteration 16/25 | Loss: 0.00232262
Iteration 17/25 | Loss: 0.00232262
Iteration 18/25 | Loss: 0.00232262
Iteration 19/25 | Loss: 0.00232262
Iteration 20/25 | Loss: 0.00232262
Iteration 21/25 | Loss: 0.00232262
Iteration 22/25 | Loss: 0.00232262
Iteration 23/25 | Loss: 0.00232262
Iteration 24/25 | Loss: 0.00232262
Iteration 25/25 | Loss: 0.00232262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232262
Iteration 2/1000 | Loss: 0.00008711
Iteration 3/1000 | Loss: 0.00004935
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00006070
Iteration 6/1000 | Loss: 0.00001876
Iteration 7/1000 | Loss: 0.00018319
Iteration 8/1000 | Loss: 0.00002284
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00003538
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001509
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001454
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001454
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001443
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001442
Iteration 34/1000 | Loss: 0.00001442
Iteration 35/1000 | Loss: 0.00001442
Iteration 36/1000 | Loss: 0.00001441
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001437
Iteration 48/1000 | Loss: 0.00001437
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001436
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001435
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001432
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001429
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001426
Iteration 119/1000 | Loss: 0.00001426
Iteration 120/1000 | Loss: 0.00001426
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001425
Iteration 126/1000 | Loss: 0.00001425
Iteration 127/1000 | Loss: 0.00001425
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001423
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001423
Iteration 137/1000 | Loss: 0.00001423
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001421
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001421
Iteration 166/1000 | Loss: 0.00001421
Iteration 167/1000 | Loss: 0.00001421
Iteration 168/1000 | Loss: 0.00001421
Iteration 169/1000 | Loss: 0.00001420
Iteration 170/1000 | Loss: 0.00001420
Iteration 171/1000 | Loss: 0.00001420
Iteration 172/1000 | Loss: 0.00001420
Iteration 173/1000 | Loss: 0.00001420
Iteration 174/1000 | Loss: 0.00001420
Iteration 175/1000 | Loss: 0.00001420
Iteration 176/1000 | Loss: 0.00001420
Iteration 177/1000 | Loss: 0.00001420
Iteration 178/1000 | Loss: 0.00001420
Iteration 179/1000 | Loss: 0.00001420
Iteration 180/1000 | Loss: 0.00001420
Iteration 181/1000 | Loss: 0.00001420
Iteration 182/1000 | Loss: 0.00001420
Iteration 183/1000 | Loss: 0.00001420
Iteration 184/1000 | Loss: 0.00001420
Iteration 185/1000 | Loss: 0.00001420
Iteration 186/1000 | Loss: 0.00001420
Iteration 187/1000 | Loss: 0.00001420
Iteration 188/1000 | Loss: 0.00001420
Iteration 189/1000 | Loss: 0.00001420
Iteration 190/1000 | Loss: 0.00001420
Iteration 191/1000 | Loss: 0.00001420
Iteration 192/1000 | Loss: 0.00001420
Iteration 193/1000 | Loss: 0.00001420
Iteration 194/1000 | Loss: 0.00001420
Iteration 195/1000 | Loss: 0.00001420
Iteration 196/1000 | Loss: 0.00001420
Iteration 197/1000 | Loss: 0.00001420
Iteration 198/1000 | Loss: 0.00001420
Iteration 199/1000 | Loss: 0.00001420
Iteration 200/1000 | Loss: 0.00001420
Iteration 201/1000 | Loss: 0.00001420
Iteration 202/1000 | Loss: 0.00001420
Iteration 203/1000 | Loss: 0.00001420
Iteration 204/1000 | Loss: 0.00001420
Iteration 205/1000 | Loss: 0.00001420
Iteration 206/1000 | Loss: 0.00001420
Iteration 207/1000 | Loss: 0.00001420
Iteration 208/1000 | Loss: 0.00001420
Iteration 209/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4203495993569959e-05, 1.4203495993569959e-05, 1.4203495993569959e-05, 1.4203495993569959e-05, 1.4203495993569959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4203495993569959e-05

Optimization complete. Final v2v error: 3.121335983276367 mm

Highest mean error: 8.559494972229004 mm for frame 23

Lowest mean error: 2.6847522258758545 mm for frame 122

Saving results

Total time: 57.69817280769348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00265464
Iteration 2/25 | Loss: 0.00130740
Iteration 3/25 | Loss: 0.00110284
Iteration 4/25 | Loss: 0.00107784
Iteration 5/25 | Loss: 0.00107419
Iteration 6/25 | Loss: 0.00107274
Iteration 7/25 | Loss: 0.00107229
Iteration 8/25 | Loss: 0.00107229
Iteration 9/25 | Loss: 0.00107229
Iteration 10/25 | Loss: 0.00107229
Iteration 11/25 | Loss: 0.00107229
Iteration 12/25 | Loss: 0.00107229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010722934966906905, 0.0010722934966906905, 0.0010722934966906905, 0.0010722934966906905, 0.0010722934966906905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010722934966906905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14079368
Iteration 2/25 | Loss: 0.00365941
Iteration 3/25 | Loss: 0.00365941
Iteration 4/25 | Loss: 0.00365941
Iteration 5/25 | Loss: 0.00365941
Iteration 6/25 | Loss: 0.00365941
Iteration 7/25 | Loss: 0.00365941
Iteration 8/25 | Loss: 0.00365941
Iteration 9/25 | Loss: 0.00365941
Iteration 10/25 | Loss: 0.00365941
Iteration 11/25 | Loss: 0.00365941
Iteration 12/25 | Loss: 0.00365941
Iteration 13/25 | Loss: 0.00365941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003659406676888466, 0.003659406676888466, 0.003659406676888466, 0.003659406676888466, 0.003659406676888466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003659406676888466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00365941
Iteration 2/1000 | Loss: 0.00005348
Iteration 3/1000 | Loss: 0.00002519
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001409
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001133
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001118
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001118
Iteration 30/1000 | Loss: 0.00001118
Iteration 31/1000 | Loss: 0.00001117
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001112
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001110
Iteration 41/1000 | Loss: 0.00001109
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001108
Iteration 44/1000 | Loss: 0.00001108
Iteration 45/1000 | Loss: 0.00001108
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001104
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001104
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001102
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001101
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001100
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001095
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001090
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001087
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001085
Iteration 142/1000 | Loss: 0.00001085
Iteration 143/1000 | Loss: 0.00001085
Iteration 144/1000 | Loss: 0.00001085
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001084
Iteration 148/1000 | Loss: 0.00001084
Iteration 149/1000 | Loss: 0.00001084
Iteration 150/1000 | Loss: 0.00001084
Iteration 151/1000 | Loss: 0.00001084
Iteration 152/1000 | Loss: 0.00001083
Iteration 153/1000 | Loss: 0.00001083
Iteration 154/1000 | Loss: 0.00001083
Iteration 155/1000 | Loss: 0.00001082
Iteration 156/1000 | Loss: 0.00001082
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001082
Iteration 159/1000 | Loss: 0.00001082
Iteration 160/1000 | Loss: 0.00001082
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001081
Iteration 163/1000 | Loss: 0.00001081
Iteration 164/1000 | Loss: 0.00001080
Iteration 165/1000 | Loss: 0.00001080
Iteration 166/1000 | Loss: 0.00001080
Iteration 167/1000 | Loss: 0.00001080
Iteration 168/1000 | Loss: 0.00001080
Iteration 169/1000 | Loss: 0.00001079
Iteration 170/1000 | Loss: 0.00001079
Iteration 171/1000 | Loss: 0.00001079
Iteration 172/1000 | Loss: 0.00001079
Iteration 173/1000 | Loss: 0.00001079
Iteration 174/1000 | Loss: 0.00001078
Iteration 175/1000 | Loss: 0.00001078
Iteration 176/1000 | Loss: 0.00001077
Iteration 177/1000 | Loss: 0.00001077
Iteration 178/1000 | Loss: 0.00001077
Iteration 179/1000 | Loss: 0.00001077
Iteration 180/1000 | Loss: 0.00001076
Iteration 181/1000 | Loss: 0.00001076
Iteration 182/1000 | Loss: 0.00001076
Iteration 183/1000 | Loss: 0.00001075
Iteration 184/1000 | Loss: 0.00001075
Iteration 185/1000 | Loss: 0.00001075
Iteration 186/1000 | Loss: 0.00001075
Iteration 187/1000 | Loss: 0.00001075
Iteration 188/1000 | Loss: 0.00001074
Iteration 189/1000 | Loss: 0.00001074
Iteration 190/1000 | Loss: 0.00001074
Iteration 191/1000 | Loss: 0.00001074
Iteration 192/1000 | Loss: 0.00001074
Iteration 193/1000 | Loss: 0.00001074
Iteration 194/1000 | Loss: 0.00001074
Iteration 195/1000 | Loss: 0.00001073
Iteration 196/1000 | Loss: 0.00001073
Iteration 197/1000 | Loss: 0.00001073
Iteration 198/1000 | Loss: 0.00001073
Iteration 199/1000 | Loss: 0.00001073
Iteration 200/1000 | Loss: 0.00001073
Iteration 201/1000 | Loss: 0.00001073
Iteration 202/1000 | Loss: 0.00001073
Iteration 203/1000 | Loss: 0.00001073
Iteration 204/1000 | Loss: 0.00001073
Iteration 205/1000 | Loss: 0.00001073
Iteration 206/1000 | Loss: 0.00001073
Iteration 207/1000 | Loss: 0.00001073
Iteration 208/1000 | Loss: 0.00001072
Iteration 209/1000 | Loss: 0.00001072
Iteration 210/1000 | Loss: 0.00001072
Iteration 211/1000 | Loss: 0.00001072
Iteration 212/1000 | Loss: 0.00001072
Iteration 213/1000 | Loss: 0.00001072
Iteration 214/1000 | Loss: 0.00001072
Iteration 215/1000 | Loss: 0.00001072
Iteration 216/1000 | Loss: 0.00001072
Iteration 217/1000 | Loss: 0.00001072
Iteration 218/1000 | Loss: 0.00001072
Iteration 219/1000 | Loss: 0.00001072
Iteration 220/1000 | Loss: 0.00001072
Iteration 221/1000 | Loss: 0.00001072
Iteration 222/1000 | Loss: 0.00001072
Iteration 223/1000 | Loss: 0.00001072
Iteration 224/1000 | Loss: 0.00001072
Iteration 225/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.0721403668867424e-05, 1.0721403668867424e-05, 1.0721403668867424e-05, 1.0721403668867424e-05, 1.0721403668867424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0721403668867424e-05

Optimization complete. Final v2v error: 2.8286752700805664 mm

Highest mean error: 3.048874855041504 mm for frame 45

Lowest mean error: 2.6025381088256836 mm for frame 12

Saving results

Total time: 53.37684106826782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400144
Iteration 2/25 | Loss: 0.00121493
Iteration 3/25 | Loss: 0.00112757
Iteration 4/25 | Loss: 0.00110343
Iteration 5/25 | Loss: 0.00109628
Iteration 6/25 | Loss: 0.00109369
Iteration 7/25 | Loss: 0.00109369
Iteration 8/25 | Loss: 0.00109369
Iteration 9/25 | Loss: 0.00109369
Iteration 10/25 | Loss: 0.00109369
Iteration 11/25 | Loss: 0.00109369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010936897015199065, 0.0010936897015199065, 0.0010936897015199065, 0.0010936897015199065, 0.0010936897015199065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010936897015199065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31795514
Iteration 2/25 | Loss: 0.00237291
Iteration 3/25 | Loss: 0.00237291
Iteration 4/25 | Loss: 0.00237291
Iteration 5/25 | Loss: 0.00237291
Iteration 6/25 | Loss: 0.00237291
Iteration 7/25 | Loss: 0.00237291
Iteration 8/25 | Loss: 0.00237291
Iteration 9/25 | Loss: 0.00237291
Iteration 10/25 | Loss: 0.00237291
Iteration 11/25 | Loss: 0.00237291
Iteration 12/25 | Loss: 0.00237291
Iteration 13/25 | Loss: 0.00237291
Iteration 14/25 | Loss: 0.00237291
Iteration 15/25 | Loss: 0.00237291
Iteration 16/25 | Loss: 0.00237291
Iteration 17/25 | Loss: 0.00237290
Iteration 18/25 | Loss: 0.00237291
Iteration 19/25 | Loss: 0.00237291
Iteration 20/25 | Loss: 0.00237291
Iteration 21/25 | Loss: 0.00237291
Iteration 22/25 | Loss: 0.00237291
Iteration 23/25 | Loss: 0.00237291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002372905146330595, 0.002372905146330595, 0.002372905146330595, 0.002372905146330595, 0.002372905146330595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002372905146330595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237291
Iteration 2/1000 | Loss: 0.00003138
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00002088
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001643
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001584
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001580
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001575
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001574
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001568
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.5622284990968183e-05, 1.5622284990968183e-05, 1.5622284990968183e-05, 1.5622284990968183e-05, 1.5622284990968183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5622284990968183e-05

Optimization complete. Final v2v error: 3.422027826309204 mm

Highest mean error: 3.7446815967559814 mm for frame 189

Lowest mean error: 3.148038625717163 mm for frame 22

Saving results

Total time: 32.18558049201965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429058
Iteration 2/25 | Loss: 0.00118648
Iteration 3/25 | Loss: 0.00109574
Iteration 4/25 | Loss: 0.00108602
Iteration 5/25 | Loss: 0.00108337
Iteration 6/25 | Loss: 0.00108269
Iteration 7/25 | Loss: 0.00108269
Iteration 8/25 | Loss: 0.00108269
Iteration 9/25 | Loss: 0.00108269
Iteration 10/25 | Loss: 0.00108269
Iteration 11/25 | Loss: 0.00108269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010826862417161465, 0.0010826862417161465, 0.0010826862417161465, 0.0010826862417161465, 0.0010826862417161465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010826862417161465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84278822
Iteration 2/25 | Loss: 0.00235278
Iteration 3/25 | Loss: 0.00235278
Iteration 4/25 | Loss: 0.00235278
Iteration 5/25 | Loss: 0.00235278
Iteration 6/25 | Loss: 0.00235278
Iteration 7/25 | Loss: 0.00235278
Iteration 8/25 | Loss: 0.00235278
Iteration 9/25 | Loss: 0.00235278
Iteration 10/25 | Loss: 0.00235278
Iteration 11/25 | Loss: 0.00235278
Iteration 12/25 | Loss: 0.00235278
Iteration 13/25 | Loss: 0.00235278
Iteration 14/25 | Loss: 0.00235278
Iteration 15/25 | Loss: 0.00235278
Iteration 16/25 | Loss: 0.00235278
Iteration 17/25 | Loss: 0.00235278
Iteration 18/25 | Loss: 0.00235278
Iteration 19/25 | Loss: 0.00235278
Iteration 20/25 | Loss: 0.00235278
Iteration 21/25 | Loss: 0.00235278
Iteration 22/25 | Loss: 0.00235278
Iteration 23/25 | Loss: 0.00235278
Iteration 24/25 | Loss: 0.00235278
Iteration 25/25 | Loss: 0.00235278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0023527767043560743, 0.0023527767043560743, 0.0023527767043560743, 0.0023527767043560743, 0.0023527767043560743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023527767043560743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235278
Iteration 2/1000 | Loss: 0.00004858
Iteration 3/1000 | Loss: 0.00002135
Iteration 4/1000 | Loss: 0.00001688
Iteration 5/1000 | Loss: 0.00001498
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001324
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001236
Iteration 46/1000 | Loss: 0.00001236
Iteration 47/1000 | Loss: 0.00001236
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001235
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001224
Iteration 122/1000 | Loss: 0.00001224
Iteration 123/1000 | Loss: 0.00001224
Iteration 124/1000 | Loss: 0.00001224
Iteration 125/1000 | Loss: 0.00001224
Iteration 126/1000 | Loss: 0.00001224
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001223
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001223
Iteration 136/1000 | Loss: 0.00001223
Iteration 137/1000 | Loss: 0.00001223
Iteration 138/1000 | Loss: 0.00001223
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001222
Iteration 144/1000 | Loss: 0.00001222
Iteration 145/1000 | Loss: 0.00001222
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001221
Iteration 179/1000 | Loss: 0.00001221
Iteration 180/1000 | Loss: 0.00001221
Iteration 181/1000 | Loss: 0.00001221
Iteration 182/1000 | Loss: 0.00001221
Iteration 183/1000 | Loss: 0.00001221
Iteration 184/1000 | Loss: 0.00001221
Iteration 185/1000 | Loss: 0.00001221
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001221
Iteration 190/1000 | Loss: 0.00001221
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001221
Iteration 193/1000 | Loss: 0.00001221
Iteration 194/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.2211629837111104e-05, 1.2211629837111104e-05, 1.2211629837111104e-05, 1.2211629837111104e-05, 1.2211629837111104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2211629837111104e-05

Optimization complete. Final v2v error: 3.0188465118408203 mm

Highest mean error: 3.221972942352295 mm for frame 118

Lowest mean error: 2.9149019718170166 mm for frame 45

Saving results

Total time: 37.442270040512085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052566
Iteration 2/25 | Loss: 0.01052566
Iteration 3/25 | Loss: 0.01052566
Iteration 4/25 | Loss: 0.00269246
Iteration 5/25 | Loss: 0.00192939
Iteration 6/25 | Loss: 0.00167911
Iteration 7/25 | Loss: 0.00153787
Iteration 8/25 | Loss: 0.00145959
Iteration 9/25 | Loss: 0.00133480
Iteration 10/25 | Loss: 0.00127785
Iteration 11/25 | Loss: 0.00126696
Iteration 12/25 | Loss: 0.00126160
Iteration 13/25 | Loss: 0.00124290
Iteration 14/25 | Loss: 0.00124043
Iteration 15/25 | Loss: 0.00123168
Iteration 16/25 | Loss: 0.00121826
Iteration 17/25 | Loss: 0.00119779
Iteration 18/25 | Loss: 0.00118916
Iteration 19/25 | Loss: 0.00118757
Iteration 20/25 | Loss: 0.00118414
Iteration 21/25 | Loss: 0.00118246
Iteration 22/25 | Loss: 0.00117495
Iteration 23/25 | Loss: 0.00116694
Iteration 24/25 | Loss: 0.00116443
Iteration 25/25 | Loss: 0.00116295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17691803
Iteration 2/25 | Loss: 0.00357811
Iteration 3/25 | Loss: 0.00328866
Iteration 4/25 | Loss: 0.00328842
Iteration 5/25 | Loss: 0.00328842
Iteration 6/25 | Loss: 0.00328842
Iteration 7/25 | Loss: 0.00328842
Iteration 8/25 | Loss: 0.00328842
Iteration 9/25 | Loss: 0.00328842
Iteration 10/25 | Loss: 0.00328842
Iteration 11/25 | Loss: 0.00328842
Iteration 12/25 | Loss: 0.00328842
Iteration 13/25 | Loss: 0.00328842
Iteration 14/25 | Loss: 0.00328842
Iteration 15/25 | Loss: 0.00328842
Iteration 16/25 | Loss: 0.00328842
Iteration 17/25 | Loss: 0.00328842
Iteration 18/25 | Loss: 0.00328842
Iteration 19/25 | Loss: 0.00328842
Iteration 20/25 | Loss: 0.00328842
Iteration 21/25 | Loss: 0.00328842
Iteration 22/25 | Loss: 0.00328842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.003288418287411332, 0.003288418287411332, 0.003288418287411332, 0.003288418287411332, 0.003288418287411332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003288418287411332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00328842
Iteration 2/1000 | Loss: 0.00137593
Iteration 3/1000 | Loss: 0.00031779
Iteration 4/1000 | Loss: 0.00019494
Iteration 5/1000 | Loss: 0.00009109
Iteration 6/1000 | Loss: 0.00025345
Iteration 7/1000 | Loss: 0.00008549
Iteration 8/1000 | Loss: 0.00023831
Iteration 9/1000 | Loss: 0.00020094
Iteration 10/1000 | Loss: 0.00021065
Iteration 11/1000 | Loss: 0.00018260
Iteration 12/1000 | Loss: 0.00015404
Iteration 13/1000 | Loss: 0.00020083
Iteration 14/1000 | Loss: 0.00020659
Iteration 15/1000 | Loss: 0.00018290
Iteration 16/1000 | Loss: 0.00019504
Iteration 17/1000 | Loss: 0.00066617
Iteration 18/1000 | Loss: 0.00022694
Iteration 19/1000 | Loss: 0.00024366
Iteration 20/1000 | Loss: 0.00008085
Iteration 21/1000 | Loss: 0.00006374
Iteration 22/1000 | Loss: 0.00005945
Iteration 23/1000 | Loss: 0.00017899
Iteration 24/1000 | Loss: 0.00030147
Iteration 25/1000 | Loss: 0.00006760
Iteration 26/1000 | Loss: 0.00005714
Iteration 27/1000 | Loss: 0.00005383
Iteration 28/1000 | Loss: 0.00021370
Iteration 29/1000 | Loss: 0.00012686
Iteration 30/1000 | Loss: 0.00025320
Iteration 31/1000 | Loss: 0.00095924
Iteration 32/1000 | Loss: 0.00064788
Iteration 33/1000 | Loss: 0.00052905
Iteration 34/1000 | Loss: 0.00058176
Iteration 35/1000 | Loss: 0.00030987
Iteration 36/1000 | Loss: 0.00020325
Iteration 37/1000 | Loss: 0.00016718
Iteration 38/1000 | Loss: 0.00007430
Iteration 39/1000 | Loss: 0.00005242
Iteration 40/1000 | Loss: 0.00004575
Iteration 41/1000 | Loss: 0.00008207
Iteration 42/1000 | Loss: 0.00011514
Iteration 43/1000 | Loss: 0.00005913
Iteration 44/1000 | Loss: 0.00003886
Iteration 45/1000 | Loss: 0.00006011
Iteration 46/1000 | Loss: 0.00003658
Iteration 47/1000 | Loss: 0.00003582
Iteration 48/1000 | Loss: 0.00003543
Iteration 49/1000 | Loss: 0.00003505
Iteration 50/1000 | Loss: 0.00003480
Iteration 51/1000 | Loss: 0.00003458
Iteration 52/1000 | Loss: 0.00013791
Iteration 53/1000 | Loss: 0.00018138
Iteration 54/1000 | Loss: 0.00039454
Iteration 55/1000 | Loss: 0.00006978
Iteration 56/1000 | Loss: 0.00004897
Iteration 57/1000 | Loss: 0.00048452
Iteration 58/1000 | Loss: 0.00003634
Iteration 59/1000 | Loss: 0.00003290
Iteration 60/1000 | Loss: 0.00003019
Iteration 61/1000 | Loss: 0.00002866
Iteration 62/1000 | Loss: 0.00002805
Iteration 63/1000 | Loss: 0.00002754
Iteration 64/1000 | Loss: 0.00002713
Iteration 65/1000 | Loss: 0.00002698
Iteration 66/1000 | Loss: 0.00002673
Iteration 67/1000 | Loss: 0.00002649
Iteration 68/1000 | Loss: 0.00010286
Iteration 69/1000 | Loss: 0.00002632
Iteration 70/1000 | Loss: 0.00002624
Iteration 71/1000 | Loss: 0.00002617
Iteration 72/1000 | Loss: 0.00002615
Iteration 73/1000 | Loss: 0.00002614
Iteration 74/1000 | Loss: 0.00002614
Iteration 75/1000 | Loss: 0.00002613
Iteration 76/1000 | Loss: 0.00002612
Iteration 77/1000 | Loss: 0.00002604
Iteration 78/1000 | Loss: 0.00002588
Iteration 79/1000 | Loss: 0.00002584
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002570
Iteration 83/1000 | Loss: 0.00002567
Iteration 84/1000 | Loss: 0.00011470
Iteration 85/1000 | Loss: 0.00019119
Iteration 86/1000 | Loss: 0.00019527
Iteration 87/1000 | Loss: 0.00003051
Iteration 88/1000 | Loss: 0.00002709
Iteration 89/1000 | Loss: 0.00017240
Iteration 90/1000 | Loss: 0.00002406
Iteration 91/1000 | Loss: 0.00002178
Iteration 92/1000 | Loss: 0.00002082
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001938
Iteration 98/1000 | Loss: 0.00001923
Iteration 99/1000 | Loss: 0.00001916
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001911
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001909
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001909
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001908
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001905
Iteration 128/1000 | Loss: 0.00001905
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001902
Iteration 143/1000 | Loss: 0.00001902
Iteration 144/1000 | Loss: 0.00001902
Iteration 145/1000 | Loss: 0.00001901
Iteration 146/1000 | Loss: 0.00001901
Iteration 147/1000 | Loss: 0.00001901
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001900
Iteration 152/1000 | Loss: 0.00001900
Iteration 153/1000 | Loss: 0.00001900
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001900
Iteration 158/1000 | Loss: 0.00001900
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00001900
Iteration 161/1000 | Loss: 0.00001900
Iteration 162/1000 | Loss: 0.00001899
Iteration 163/1000 | Loss: 0.00001899
Iteration 164/1000 | Loss: 0.00001899
Iteration 165/1000 | Loss: 0.00001899
Iteration 166/1000 | Loss: 0.00001899
Iteration 167/1000 | Loss: 0.00001899
Iteration 168/1000 | Loss: 0.00001899
Iteration 169/1000 | Loss: 0.00001899
Iteration 170/1000 | Loss: 0.00001899
Iteration 171/1000 | Loss: 0.00001898
Iteration 172/1000 | Loss: 0.00001898
Iteration 173/1000 | Loss: 0.00001898
Iteration 174/1000 | Loss: 0.00001898
Iteration 175/1000 | Loss: 0.00001898
Iteration 176/1000 | Loss: 0.00001898
Iteration 177/1000 | Loss: 0.00001898
Iteration 178/1000 | Loss: 0.00001898
Iteration 179/1000 | Loss: 0.00001898
Iteration 180/1000 | Loss: 0.00001898
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001898
Iteration 183/1000 | Loss: 0.00001898
Iteration 184/1000 | Loss: 0.00001898
Iteration 185/1000 | Loss: 0.00001898
Iteration 186/1000 | Loss: 0.00001898
Iteration 187/1000 | Loss: 0.00001898
Iteration 188/1000 | Loss: 0.00001898
Iteration 189/1000 | Loss: 0.00001898
Iteration 190/1000 | Loss: 0.00001898
Iteration 191/1000 | Loss: 0.00001898
Iteration 192/1000 | Loss: 0.00001898
Iteration 193/1000 | Loss: 0.00001898
Iteration 194/1000 | Loss: 0.00001898
Iteration 195/1000 | Loss: 0.00001898
Iteration 196/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.8980965251103044e-05, 1.8980965251103044e-05, 1.8980965251103044e-05, 1.8980965251103044e-05, 1.8980965251103044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8980965251103044e-05

Optimization complete. Final v2v error: 3.023789644241333 mm

Highest mean error: 11.572452545166016 mm for frame 136

Lowest mean error: 2.621828556060791 mm for frame 62

Saving results

Total time: 198.86404275894165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049279
Iteration 2/25 | Loss: 0.00225843
Iteration 3/25 | Loss: 0.00164207
Iteration 4/25 | Loss: 0.00151139
Iteration 5/25 | Loss: 0.00211017
Iteration 6/25 | Loss: 0.00188968
Iteration 7/25 | Loss: 0.00199925
Iteration 8/25 | Loss: 0.00230815
Iteration 9/25 | Loss: 0.00213489
Iteration 10/25 | Loss: 0.00197580
Iteration 11/25 | Loss: 0.00191189
Iteration 12/25 | Loss: 0.00195985
Iteration 13/25 | Loss: 0.00178695
Iteration 14/25 | Loss: 0.00174565
Iteration 15/25 | Loss: 0.00170497
Iteration 16/25 | Loss: 0.00166703
Iteration 17/25 | Loss: 0.00163515
Iteration 18/25 | Loss: 0.00157259
Iteration 19/25 | Loss: 0.00157222
Iteration 20/25 | Loss: 0.00156502
Iteration 21/25 | Loss: 0.00150909
Iteration 22/25 | Loss: 0.00152100
Iteration 23/25 | Loss: 0.00148684
Iteration 24/25 | Loss: 0.00148558
Iteration 25/25 | Loss: 0.00148173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14358735
Iteration 2/25 | Loss: 0.00620313
Iteration 3/25 | Loss: 0.00612419
Iteration 4/25 | Loss: 0.00612419
Iteration 5/25 | Loss: 0.00612419
Iteration 6/25 | Loss: 0.00612419
Iteration 7/25 | Loss: 0.00612419
Iteration 8/25 | Loss: 0.00612850
Iteration 9/25 | Loss: 0.00612419
Iteration 10/25 | Loss: 0.00612419
Iteration 11/25 | Loss: 0.00612419
Iteration 12/25 | Loss: 0.00612419
Iteration 13/25 | Loss: 0.00612419
Iteration 14/25 | Loss: 0.00612419
Iteration 15/25 | Loss: 0.00612419
Iteration 16/25 | Loss: 0.00612419
Iteration 17/25 | Loss: 0.00612419
Iteration 18/25 | Loss: 0.00612419
Iteration 19/25 | Loss: 0.00612419
Iteration 20/25 | Loss: 0.00612419
Iteration 21/25 | Loss: 0.00612419
Iteration 22/25 | Loss: 0.00612419
Iteration 23/25 | Loss: 0.00612419
Iteration 24/25 | Loss: 0.00612419
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.006124185398221016, 0.006124185398221016, 0.006124185398221016, 0.006124185398221016, 0.006124185398221016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006124185398221016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00612419
Iteration 2/1000 | Loss: 0.00351480
Iteration 3/1000 | Loss: 0.00290685
Iteration 4/1000 | Loss: 0.00327012
Iteration 5/1000 | Loss: 0.00277607
Iteration 6/1000 | Loss: 0.00185548
Iteration 7/1000 | Loss: 0.00198693
Iteration 8/1000 | Loss: 0.00225331
Iteration 9/1000 | Loss: 0.00328734
Iteration 10/1000 | Loss: 0.00562692
Iteration 11/1000 | Loss: 0.00533124
Iteration 12/1000 | Loss: 0.00555722
Iteration 13/1000 | Loss: 0.00494652
Iteration 14/1000 | Loss: 0.00359073
Iteration 15/1000 | Loss: 0.00512486
Iteration 16/1000 | Loss: 0.00353431
Iteration 17/1000 | Loss: 0.00287976
Iteration 18/1000 | Loss: 0.00228362
Iteration 19/1000 | Loss: 0.00230588
Iteration 20/1000 | Loss: 0.00205111
Iteration 21/1000 | Loss: 0.00180497
Iteration 22/1000 | Loss: 0.00242093
Iteration 23/1000 | Loss: 0.00301207
Iteration 24/1000 | Loss: 0.00287235
Iteration 25/1000 | Loss: 0.00267153
Iteration 26/1000 | Loss: 0.00358555
Iteration 27/1000 | Loss: 0.00341256
Iteration 28/1000 | Loss: 0.00333056
Iteration 29/1000 | Loss: 0.00447158
Iteration 30/1000 | Loss: 0.00454762
Iteration 31/1000 | Loss: 0.00320745
Iteration 32/1000 | Loss: 0.00219443
Iteration 33/1000 | Loss: 0.00166382
Iteration 34/1000 | Loss: 0.00161270
Iteration 35/1000 | Loss: 0.00153066
Iteration 36/1000 | Loss: 0.00167386
Iteration 37/1000 | Loss: 0.00137250
Iteration 38/1000 | Loss: 0.00107712
Iteration 39/1000 | Loss: 0.00170320
Iteration 40/1000 | Loss: 0.00089159
Iteration 41/1000 | Loss: 0.00075050
Iteration 42/1000 | Loss: 0.00174303
Iteration 43/1000 | Loss: 0.00200365
Iteration 44/1000 | Loss: 0.00142105
Iteration 45/1000 | Loss: 0.00179014
Iteration 46/1000 | Loss: 0.00220416
Iteration 47/1000 | Loss: 0.00160368
Iteration 48/1000 | Loss: 0.00190918
Iteration 49/1000 | Loss: 0.00166624
Iteration 50/1000 | Loss: 0.00140958
Iteration 51/1000 | Loss: 0.00096490
Iteration 52/1000 | Loss: 0.00128067
Iteration 53/1000 | Loss: 0.00101015
Iteration 54/1000 | Loss: 0.00123057
Iteration 55/1000 | Loss: 0.00067479
Iteration 56/1000 | Loss: 0.00046242
Iteration 57/1000 | Loss: 0.00090906
Iteration 58/1000 | Loss: 0.00110322
Iteration 59/1000 | Loss: 0.00117112
Iteration 60/1000 | Loss: 0.00091964
Iteration 61/1000 | Loss: 0.00076446
Iteration 62/1000 | Loss: 0.00078340
Iteration 63/1000 | Loss: 0.00093937
Iteration 64/1000 | Loss: 0.00082053
Iteration 65/1000 | Loss: 0.00104470
Iteration 66/1000 | Loss: 0.00183734
Iteration 67/1000 | Loss: 0.00139587
Iteration 68/1000 | Loss: 0.00187647
Iteration 69/1000 | Loss: 0.00190996
Iteration 70/1000 | Loss: 0.00165239
Iteration 71/1000 | Loss: 0.00109773
Iteration 72/1000 | Loss: 0.00097357
Iteration 73/1000 | Loss: 0.00092514
Iteration 74/1000 | Loss: 0.00094347
Iteration 75/1000 | Loss: 0.00088316
Iteration 76/1000 | Loss: 0.00063877
Iteration 77/1000 | Loss: 0.00033063
Iteration 78/1000 | Loss: 0.00088577
Iteration 79/1000 | Loss: 0.00080184
Iteration 80/1000 | Loss: 0.00059097
Iteration 81/1000 | Loss: 0.00093101
Iteration 82/1000 | Loss: 0.00130068
Iteration 83/1000 | Loss: 0.00171037
Iteration 84/1000 | Loss: 0.00108781
Iteration 85/1000 | Loss: 0.00079329
Iteration 86/1000 | Loss: 0.00111265
Iteration 87/1000 | Loss: 0.00075180
Iteration 88/1000 | Loss: 0.00121982
Iteration 89/1000 | Loss: 0.00148911
Iteration 90/1000 | Loss: 0.00114520
Iteration 91/1000 | Loss: 0.00061927
Iteration 92/1000 | Loss: 0.00070948
Iteration 93/1000 | Loss: 0.00069576
Iteration 94/1000 | Loss: 0.00094864
Iteration 95/1000 | Loss: 0.00101113
Iteration 96/1000 | Loss: 0.00105674
Iteration 97/1000 | Loss: 0.00058062
Iteration 98/1000 | Loss: 0.00041175
Iteration 99/1000 | Loss: 0.00063910
Iteration 100/1000 | Loss: 0.00133805
Iteration 101/1000 | Loss: 0.00095353
Iteration 102/1000 | Loss: 0.00106973
Iteration 103/1000 | Loss: 0.00084502
Iteration 104/1000 | Loss: 0.00068940
Iteration 105/1000 | Loss: 0.00030865
Iteration 106/1000 | Loss: 0.00026813
Iteration 107/1000 | Loss: 0.00020271
Iteration 108/1000 | Loss: 0.00058520
Iteration 109/1000 | Loss: 0.00100070
Iteration 110/1000 | Loss: 0.00059348
Iteration 111/1000 | Loss: 0.00037993
Iteration 112/1000 | Loss: 0.00054672
Iteration 113/1000 | Loss: 0.00045340
Iteration 114/1000 | Loss: 0.00084151
Iteration 115/1000 | Loss: 0.00065424
Iteration 116/1000 | Loss: 0.00081160
Iteration 117/1000 | Loss: 0.00080668
Iteration 118/1000 | Loss: 0.00077707
Iteration 119/1000 | Loss: 0.00096663
Iteration 120/1000 | Loss: 0.00074750
Iteration 121/1000 | Loss: 0.00022514
Iteration 122/1000 | Loss: 0.00051818
Iteration 123/1000 | Loss: 0.00082936
Iteration 124/1000 | Loss: 0.00048375
Iteration 125/1000 | Loss: 0.00034867
Iteration 126/1000 | Loss: 0.00067149
Iteration 127/1000 | Loss: 0.00052351
Iteration 128/1000 | Loss: 0.00047513
Iteration 129/1000 | Loss: 0.00048898
Iteration 130/1000 | Loss: 0.00029859
Iteration 131/1000 | Loss: 0.00053550
Iteration 132/1000 | Loss: 0.00058043
Iteration 133/1000 | Loss: 0.00049023
Iteration 134/1000 | Loss: 0.00028349
Iteration 135/1000 | Loss: 0.00066904
Iteration 136/1000 | Loss: 0.00087834
Iteration 137/1000 | Loss: 0.00063328
Iteration 138/1000 | Loss: 0.00121735
Iteration 139/1000 | Loss: 0.00101500
Iteration 140/1000 | Loss: 0.00096957
Iteration 141/1000 | Loss: 0.00081717
Iteration 142/1000 | Loss: 0.00067143
Iteration 143/1000 | Loss: 0.00030972
Iteration 144/1000 | Loss: 0.00045262
Iteration 145/1000 | Loss: 0.00049138
Iteration 146/1000 | Loss: 0.00052721
Iteration 147/1000 | Loss: 0.00037706
Iteration 148/1000 | Loss: 0.00031374
Iteration 149/1000 | Loss: 0.00052000
Iteration 150/1000 | Loss: 0.00064906
Iteration 151/1000 | Loss: 0.00085171
Iteration 152/1000 | Loss: 0.00073246
Iteration 153/1000 | Loss: 0.00030948
Iteration 154/1000 | Loss: 0.00049751
Iteration 155/1000 | Loss: 0.00067168
Iteration 156/1000 | Loss: 0.00039928
Iteration 157/1000 | Loss: 0.00029798
Iteration 158/1000 | Loss: 0.00026040
Iteration 159/1000 | Loss: 0.00033096
Iteration 160/1000 | Loss: 0.00029978
Iteration 161/1000 | Loss: 0.00031961
Iteration 162/1000 | Loss: 0.00038586
Iteration 163/1000 | Loss: 0.00038218
Iteration 164/1000 | Loss: 0.00027347
Iteration 165/1000 | Loss: 0.00025268
Iteration 166/1000 | Loss: 0.00021745
Iteration 167/1000 | Loss: 0.00036528
Iteration 168/1000 | Loss: 0.00043069
Iteration 169/1000 | Loss: 0.00038377
Iteration 170/1000 | Loss: 0.00054292
Iteration 171/1000 | Loss: 0.00043233
Iteration 172/1000 | Loss: 0.00032682
Iteration 173/1000 | Loss: 0.00034812
Iteration 174/1000 | Loss: 0.00017810
Iteration 175/1000 | Loss: 0.00027902
Iteration 176/1000 | Loss: 0.00046622
Iteration 177/1000 | Loss: 0.00028948
Iteration 178/1000 | Loss: 0.00031204
Iteration 179/1000 | Loss: 0.00034142
Iteration 180/1000 | Loss: 0.00038230
Iteration 181/1000 | Loss: 0.00083108
Iteration 182/1000 | Loss: 0.00022636
Iteration 183/1000 | Loss: 0.00008918
Iteration 184/1000 | Loss: 0.00006270
Iteration 185/1000 | Loss: 0.00010043
Iteration 186/1000 | Loss: 0.00004912
Iteration 187/1000 | Loss: 0.00007108
Iteration 188/1000 | Loss: 0.00005388
Iteration 189/1000 | Loss: 0.00021550
Iteration 190/1000 | Loss: 0.00032414
Iteration 191/1000 | Loss: 0.00030153
Iteration 192/1000 | Loss: 0.00017729
Iteration 193/1000 | Loss: 0.00022426
Iteration 194/1000 | Loss: 0.00028298
Iteration 195/1000 | Loss: 0.00012932
Iteration 196/1000 | Loss: 0.00014082
Iteration 197/1000 | Loss: 0.00005658
Iteration 198/1000 | Loss: 0.00022678
Iteration 199/1000 | Loss: 0.00025852
Iteration 200/1000 | Loss: 0.00027561
Iteration 201/1000 | Loss: 0.00032528
Iteration 202/1000 | Loss: 0.00023163
Iteration 203/1000 | Loss: 0.00017178
Iteration 204/1000 | Loss: 0.00012502
Iteration 205/1000 | Loss: 0.00006081
Iteration 206/1000 | Loss: 0.00044815
Iteration 207/1000 | Loss: 0.00020696
Iteration 208/1000 | Loss: 0.00026238
Iteration 209/1000 | Loss: 0.00033188
Iteration 210/1000 | Loss: 0.00053822
Iteration 211/1000 | Loss: 0.00023236
Iteration 212/1000 | Loss: 0.00028824
Iteration 213/1000 | Loss: 0.00056437
Iteration 214/1000 | Loss: 0.00021840
Iteration 215/1000 | Loss: 0.00034307
Iteration 216/1000 | Loss: 0.00040299
Iteration 217/1000 | Loss: 0.00034628
Iteration 218/1000 | Loss: 0.00058220
Iteration 219/1000 | Loss: 0.00011111
Iteration 220/1000 | Loss: 0.00003876
Iteration 221/1000 | Loss: 0.00003515
Iteration 222/1000 | Loss: 0.00003097
Iteration 223/1000 | Loss: 0.00002917
Iteration 224/1000 | Loss: 0.00002761
Iteration 225/1000 | Loss: 0.00021950
Iteration 226/1000 | Loss: 0.00018856
Iteration 227/1000 | Loss: 0.00021020
Iteration 228/1000 | Loss: 0.00017564
Iteration 229/1000 | Loss: 0.00003240
Iteration 230/1000 | Loss: 0.00002604
Iteration 231/1000 | Loss: 0.00002476
Iteration 232/1000 | Loss: 0.00002422
Iteration 233/1000 | Loss: 0.00002382
Iteration 234/1000 | Loss: 0.00021337
Iteration 235/1000 | Loss: 0.00014113
Iteration 236/1000 | Loss: 0.00018655
Iteration 237/1000 | Loss: 0.00004354
Iteration 238/1000 | Loss: 0.00003219
Iteration 239/1000 | Loss: 0.00002828
Iteration 240/1000 | Loss: 0.00002595
Iteration 241/1000 | Loss: 0.00002915
Iteration 242/1000 | Loss: 0.00012205
Iteration 243/1000 | Loss: 0.00004989
Iteration 244/1000 | Loss: 0.00002257
Iteration 245/1000 | Loss: 0.00002187
Iteration 246/1000 | Loss: 0.00002150
Iteration 247/1000 | Loss: 0.00002127
Iteration 248/1000 | Loss: 0.00020414
Iteration 249/1000 | Loss: 0.00018012
Iteration 250/1000 | Loss: 0.00003310
Iteration 251/1000 | Loss: 0.00002504
Iteration 252/1000 | Loss: 0.00002120
Iteration 253/1000 | Loss: 0.00018642
Iteration 254/1000 | Loss: 0.00004258
Iteration 255/1000 | Loss: 0.00002106
Iteration 256/1000 | Loss: 0.00002079
Iteration 257/1000 | Loss: 0.00019677
Iteration 258/1000 | Loss: 0.00019957
Iteration 259/1000 | Loss: 0.00017857
Iteration 260/1000 | Loss: 0.00002554
Iteration 261/1000 | Loss: 0.00019532
Iteration 262/1000 | Loss: 0.00002667
Iteration 263/1000 | Loss: 0.00002451
Iteration 264/1000 | Loss: 0.00019533
Iteration 265/1000 | Loss: 0.00013017
Iteration 266/1000 | Loss: 0.00003294
Iteration 267/1000 | Loss: 0.00027205
Iteration 268/1000 | Loss: 0.00019962
Iteration 269/1000 | Loss: 0.00004892
Iteration 270/1000 | Loss: 0.00011825
Iteration 271/1000 | Loss: 0.00017830
Iteration 272/1000 | Loss: 0.00005176
Iteration 273/1000 | Loss: 0.00004163
Iteration 274/1000 | Loss: 0.00002711
Iteration 275/1000 | Loss: 0.00002647
Iteration 276/1000 | Loss: 0.00003742
Iteration 277/1000 | Loss: 0.00018571
Iteration 278/1000 | Loss: 0.00009882
Iteration 279/1000 | Loss: 0.00002538
Iteration 280/1000 | Loss: 0.00013769
Iteration 281/1000 | Loss: 0.00003447
Iteration 282/1000 | Loss: 0.00006154
Iteration 283/1000 | Loss: 0.00008264
Iteration 284/1000 | Loss: 0.00002946
Iteration 285/1000 | Loss: 0.00018532
Iteration 286/1000 | Loss: 0.00014761
Iteration 287/1000 | Loss: 0.00002971
Iteration 288/1000 | Loss: 0.00045621
Iteration 289/1000 | Loss: 0.00031018
Iteration 290/1000 | Loss: 0.00011185
Iteration 291/1000 | Loss: 0.00019950
Iteration 292/1000 | Loss: 0.00004054
Iteration 293/1000 | Loss: 0.00002907
Iteration 294/1000 | Loss: 0.00002466
Iteration 295/1000 | Loss: 0.00002183
Iteration 296/1000 | Loss: 0.00002075
Iteration 297/1000 | Loss: 0.00001991
Iteration 298/1000 | Loss: 0.00001952
Iteration 299/1000 | Loss: 0.00001916
Iteration 300/1000 | Loss: 0.00021123
Iteration 301/1000 | Loss: 0.00002448
Iteration 302/1000 | Loss: 0.00002138
Iteration 303/1000 | Loss: 0.00008457
Iteration 304/1000 | Loss: 0.00010546
Iteration 305/1000 | Loss: 0.00009307
Iteration 306/1000 | Loss: 0.00003137
Iteration 307/1000 | Loss: 0.00010443
Iteration 308/1000 | Loss: 0.00011668
Iteration 309/1000 | Loss: 0.00008073
Iteration 310/1000 | Loss: 0.00009530
Iteration 311/1000 | Loss: 0.00003438
Iteration 312/1000 | Loss: 0.00002679
Iteration 313/1000 | Loss: 0.00013258
Iteration 314/1000 | Loss: 0.00002483
Iteration 315/1000 | Loss: 0.00002174
Iteration 316/1000 | Loss: 0.00015674
Iteration 317/1000 | Loss: 0.00011787
Iteration 318/1000 | Loss: 0.00002447
Iteration 319/1000 | Loss: 0.00002097
Iteration 320/1000 | Loss: 0.00001949
Iteration 321/1000 | Loss: 0.00001883
Iteration 322/1000 | Loss: 0.00001837
Iteration 323/1000 | Loss: 0.00001783
Iteration 324/1000 | Loss: 0.00001757
Iteration 325/1000 | Loss: 0.00001748
Iteration 326/1000 | Loss: 0.00002323
Iteration 327/1000 | Loss: 0.00001999
Iteration 328/1000 | Loss: 0.00001843
Iteration 329/1000 | Loss: 0.00001799
Iteration 330/1000 | Loss: 0.00001768
Iteration 331/1000 | Loss: 0.00001758
Iteration 332/1000 | Loss: 0.00001757
Iteration 333/1000 | Loss: 0.00001754
Iteration 334/1000 | Loss: 0.00001752
Iteration 335/1000 | Loss: 0.00001745
Iteration 336/1000 | Loss: 0.00001732
Iteration 337/1000 | Loss: 0.00001704
Iteration 338/1000 | Loss: 0.00001677
Iteration 339/1000 | Loss: 0.00001663
Iteration 340/1000 | Loss: 0.00001662
Iteration 341/1000 | Loss: 0.00001661
Iteration 342/1000 | Loss: 0.00001656
Iteration 343/1000 | Loss: 0.00001655
Iteration 344/1000 | Loss: 0.00001643
Iteration 345/1000 | Loss: 0.00001642
Iteration 346/1000 | Loss: 0.00001642
Iteration 347/1000 | Loss: 0.00001637
Iteration 348/1000 | Loss: 0.00001637
Iteration 349/1000 | Loss: 0.00001636
Iteration 350/1000 | Loss: 0.00001636
Iteration 351/1000 | Loss: 0.00001635
Iteration 352/1000 | Loss: 0.00001634
Iteration 353/1000 | Loss: 0.00001634
Iteration 354/1000 | Loss: 0.00001633
Iteration 355/1000 | Loss: 0.00001633
Iteration 356/1000 | Loss: 0.00001633
Iteration 357/1000 | Loss: 0.00001633
Iteration 358/1000 | Loss: 0.00001632
Iteration 359/1000 | Loss: 0.00001632
Iteration 360/1000 | Loss: 0.00001631
Iteration 361/1000 | Loss: 0.00001631
Iteration 362/1000 | Loss: 0.00001630
Iteration 363/1000 | Loss: 0.00001629
Iteration 364/1000 | Loss: 0.00001628
Iteration 365/1000 | Loss: 0.00001628
Iteration 366/1000 | Loss: 0.00001627
Iteration 367/1000 | Loss: 0.00001627
Iteration 368/1000 | Loss: 0.00001627
Iteration 369/1000 | Loss: 0.00001627
Iteration 370/1000 | Loss: 0.00001626
Iteration 371/1000 | Loss: 0.00001626
Iteration 372/1000 | Loss: 0.00001626
Iteration 373/1000 | Loss: 0.00001625
Iteration 374/1000 | Loss: 0.00001625
Iteration 375/1000 | Loss: 0.00001624
Iteration 376/1000 | Loss: 0.00001623
Iteration 377/1000 | Loss: 0.00001622
Iteration 378/1000 | Loss: 0.00001622
Iteration 379/1000 | Loss: 0.00001622
Iteration 380/1000 | Loss: 0.00001622
Iteration 381/1000 | Loss: 0.00001621
Iteration 382/1000 | Loss: 0.00001621
Iteration 383/1000 | Loss: 0.00001621
Iteration 384/1000 | Loss: 0.00001621
Iteration 385/1000 | Loss: 0.00001621
Iteration 386/1000 | Loss: 0.00001621
Iteration 387/1000 | Loss: 0.00001621
Iteration 388/1000 | Loss: 0.00001621
Iteration 389/1000 | Loss: 0.00001621
Iteration 390/1000 | Loss: 0.00001621
Iteration 391/1000 | Loss: 0.00001621
Iteration 392/1000 | Loss: 0.00001621
Iteration 393/1000 | Loss: 0.00001621
Iteration 394/1000 | Loss: 0.00001621
Iteration 395/1000 | Loss: 0.00001621
Iteration 396/1000 | Loss: 0.00001621
Iteration 397/1000 | Loss: 0.00001620
Iteration 398/1000 | Loss: 0.00001619
Iteration 399/1000 | Loss: 0.00001619
Iteration 400/1000 | Loss: 0.00001618
Iteration 401/1000 | Loss: 0.00001617
Iteration 402/1000 | Loss: 0.00001617
Iteration 403/1000 | Loss: 0.00001616
Iteration 404/1000 | Loss: 0.00001616
Iteration 405/1000 | Loss: 0.00001616
Iteration 406/1000 | Loss: 0.00001615
Iteration 407/1000 | Loss: 0.00001615
Iteration 408/1000 | Loss: 0.00001615
Iteration 409/1000 | Loss: 0.00001614
Iteration 410/1000 | Loss: 0.00001614
Iteration 411/1000 | Loss: 0.00001614
Iteration 412/1000 | Loss: 0.00001614
Iteration 413/1000 | Loss: 0.00001614
Iteration 414/1000 | Loss: 0.00001614
Iteration 415/1000 | Loss: 0.00001614
Iteration 416/1000 | Loss: 0.00001614
Iteration 417/1000 | Loss: 0.00001614
Iteration 418/1000 | Loss: 0.00001614
Iteration 419/1000 | Loss: 0.00001613
Iteration 420/1000 | Loss: 0.00001613
Iteration 421/1000 | Loss: 0.00001613
Iteration 422/1000 | Loss: 0.00001613
Iteration 423/1000 | Loss: 0.00001613
Iteration 424/1000 | Loss: 0.00001613
Iteration 425/1000 | Loss: 0.00001613
Iteration 426/1000 | Loss: 0.00001613
Iteration 427/1000 | Loss: 0.00001613
Iteration 428/1000 | Loss: 0.00001613
Iteration 429/1000 | Loss: 0.00001613
Iteration 430/1000 | Loss: 0.00001613
Iteration 431/1000 | Loss: 0.00001613
Iteration 432/1000 | Loss: 0.00001612
Iteration 433/1000 | Loss: 0.00001612
Iteration 434/1000 | Loss: 0.00001612
Iteration 435/1000 | Loss: 0.00001612
Iteration 436/1000 | Loss: 0.00001612
Iteration 437/1000 | Loss: 0.00001612
Iteration 438/1000 | Loss: 0.00001612
Iteration 439/1000 | Loss: 0.00001612
Iteration 440/1000 | Loss: 0.00001612
Iteration 441/1000 | Loss: 0.00001612
Iteration 442/1000 | Loss: 0.00001612
Iteration 443/1000 | Loss: 0.00001612
Iteration 444/1000 | Loss: 0.00001612
Iteration 445/1000 | Loss: 0.00001612
Iteration 446/1000 | Loss: 0.00001612
Iteration 447/1000 | Loss: 0.00001612
Iteration 448/1000 | Loss: 0.00001611
Iteration 449/1000 | Loss: 0.00001611
Iteration 450/1000 | Loss: 0.00001611
Iteration 451/1000 | Loss: 0.00001611
Iteration 452/1000 | Loss: 0.00001611
Iteration 453/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 453. Stopping optimization.
Last 5 losses: [1.6114870959427208e-05, 1.6114870959427208e-05, 1.6114870959427208e-05, 1.6114870959427208e-05, 1.6114870959427208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6114870959427208e-05

Optimization complete. Final v2v error: 2.8373241424560547 mm

Highest mean error: 8.593035697937012 mm for frame 16

Lowest mean error: 2.500502347946167 mm for frame 138

Saving results

Total time: 552.8100755214691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033213
Iteration 2/25 | Loss: 0.00322563
Iteration 3/25 | Loss: 0.00261157
Iteration 4/25 | Loss: 0.00216580
Iteration 5/25 | Loss: 0.00208016
Iteration 6/25 | Loss: 0.00207304
Iteration 7/25 | Loss: 0.00193173
Iteration 8/25 | Loss: 0.00169453
Iteration 9/25 | Loss: 0.00164976
Iteration 10/25 | Loss: 0.00162172
Iteration 11/25 | Loss: 0.00161040
Iteration 12/25 | Loss: 0.00160792
Iteration 13/25 | Loss: 0.00157623
Iteration 14/25 | Loss: 0.00156128
Iteration 15/25 | Loss: 0.00156055
Iteration 16/25 | Loss: 0.00155508
Iteration 17/25 | Loss: 0.00154787
Iteration 18/25 | Loss: 0.00154722
Iteration 19/25 | Loss: 0.00154903
Iteration 20/25 | Loss: 0.00155297
Iteration 21/25 | Loss: 0.00155372
Iteration 22/25 | Loss: 0.00154878
Iteration 23/25 | Loss: 0.00155326
Iteration 24/25 | Loss: 0.00155236
Iteration 25/25 | Loss: 0.00154967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15927112
Iteration 2/25 | Loss: 0.00645199
Iteration 3/25 | Loss: 0.00428975
Iteration 4/25 | Loss: 0.00428975
Iteration 5/25 | Loss: 0.00428975
Iteration 6/25 | Loss: 0.00428975
Iteration 7/25 | Loss: 0.00428975
Iteration 8/25 | Loss: 0.00428975
Iteration 9/25 | Loss: 0.00428975
Iteration 10/25 | Loss: 0.00428975
Iteration 11/25 | Loss: 0.00428975
Iteration 12/25 | Loss: 0.00428975
Iteration 13/25 | Loss: 0.00428975
Iteration 14/25 | Loss: 0.00428975
Iteration 15/25 | Loss: 0.00428975
Iteration 16/25 | Loss: 0.00428975
Iteration 17/25 | Loss: 0.00428975
Iteration 18/25 | Loss: 0.00428975
Iteration 19/25 | Loss: 0.00428975
Iteration 20/25 | Loss: 0.00428975
Iteration 21/25 | Loss: 0.00428975
Iteration 22/25 | Loss: 0.00428975
Iteration 23/25 | Loss: 0.00428975
Iteration 24/25 | Loss: 0.00428975
Iteration 25/25 | Loss: 0.00428975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00428975
Iteration 2/1000 | Loss: 0.00099469
Iteration 3/1000 | Loss: 0.00083725
Iteration 4/1000 | Loss: 0.00024421
Iteration 5/1000 | Loss: 0.00108299
Iteration 6/1000 | Loss: 0.00023174
Iteration 7/1000 | Loss: 0.00026818
Iteration 8/1000 | Loss: 0.00023603
Iteration 9/1000 | Loss: 0.00020911
Iteration 10/1000 | Loss: 0.00018562
Iteration 11/1000 | Loss: 0.00017906
Iteration 12/1000 | Loss: 0.00019475
Iteration 13/1000 | Loss: 0.00023746
Iteration 14/1000 | Loss: 0.00019092
Iteration 15/1000 | Loss: 0.00018702
Iteration 16/1000 | Loss: 0.00019851
Iteration 17/1000 | Loss: 0.00020670
Iteration 18/1000 | Loss: 0.00031765
Iteration 19/1000 | Loss: 0.00017693
Iteration 20/1000 | Loss: 0.00016506
Iteration 21/1000 | Loss: 0.00015979
Iteration 22/1000 | Loss: 0.00061841
Iteration 23/1000 | Loss: 0.00015495
Iteration 24/1000 | Loss: 0.00051468
Iteration 25/1000 | Loss: 0.00015945
Iteration 26/1000 | Loss: 0.00036581
Iteration 27/1000 | Loss: 0.00015418
Iteration 28/1000 | Loss: 0.00044280
Iteration 29/1000 | Loss: 0.00060440
Iteration 30/1000 | Loss: 0.00044124
Iteration 31/1000 | Loss: 0.00014997
Iteration 32/1000 | Loss: 0.00098885
Iteration 33/1000 | Loss: 0.00017993
Iteration 34/1000 | Loss: 0.00018102
Iteration 35/1000 | Loss: 0.00014282
Iteration 36/1000 | Loss: 0.00014164
Iteration 37/1000 | Loss: 0.00014092
Iteration 38/1000 | Loss: 0.00014043
Iteration 39/1000 | Loss: 0.00013994
Iteration 40/1000 | Loss: 0.00013957
Iteration 41/1000 | Loss: 0.00015569
Iteration 42/1000 | Loss: 0.00014192
Iteration 43/1000 | Loss: 0.00014042
Iteration 44/1000 | Loss: 0.00028982
Iteration 45/1000 | Loss: 0.00014782
Iteration 46/1000 | Loss: 0.00018806
Iteration 47/1000 | Loss: 0.00016541
Iteration 48/1000 | Loss: 0.00016194
Iteration 49/1000 | Loss: 0.00056590
Iteration 50/1000 | Loss: 0.00021228
Iteration 51/1000 | Loss: 0.00015458
Iteration 52/1000 | Loss: 0.00014057
Iteration 53/1000 | Loss: 0.00013877
Iteration 54/1000 | Loss: 0.00013842
Iteration 55/1000 | Loss: 0.00013824
Iteration 56/1000 | Loss: 0.00013820
Iteration 57/1000 | Loss: 0.00013819
Iteration 58/1000 | Loss: 0.00013819
Iteration 59/1000 | Loss: 0.00013819
Iteration 60/1000 | Loss: 0.00013819
Iteration 61/1000 | Loss: 0.00013819
Iteration 62/1000 | Loss: 0.00013819
Iteration 63/1000 | Loss: 0.00013819
Iteration 64/1000 | Loss: 0.00013818
Iteration 65/1000 | Loss: 0.00013818
Iteration 66/1000 | Loss: 0.00013818
Iteration 67/1000 | Loss: 0.00013816
Iteration 68/1000 | Loss: 0.00013812
Iteration 69/1000 | Loss: 0.00013811
Iteration 70/1000 | Loss: 0.00013809
Iteration 71/1000 | Loss: 0.00013809
Iteration 72/1000 | Loss: 0.00013808
Iteration 73/1000 | Loss: 0.00013806
Iteration 74/1000 | Loss: 0.00013806
Iteration 75/1000 | Loss: 0.00013805
Iteration 76/1000 | Loss: 0.00013805
Iteration 77/1000 | Loss: 0.00013805
Iteration 78/1000 | Loss: 0.00013804
Iteration 79/1000 | Loss: 0.00013804
Iteration 80/1000 | Loss: 0.00013804
Iteration 81/1000 | Loss: 0.00013804
Iteration 82/1000 | Loss: 0.00013800
Iteration 83/1000 | Loss: 0.00013799
Iteration 84/1000 | Loss: 0.00013798
Iteration 85/1000 | Loss: 0.00013786
Iteration 86/1000 | Loss: 0.00013786
Iteration 87/1000 | Loss: 0.00013784
Iteration 88/1000 | Loss: 0.00013771
Iteration 89/1000 | Loss: 0.00013766
Iteration 90/1000 | Loss: 0.00013759
Iteration 91/1000 | Loss: 0.00013758
Iteration 92/1000 | Loss: 0.00013751
Iteration 93/1000 | Loss: 0.00013746
Iteration 94/1000 | Loss: 0.00013746
Iteration 95/1000 | Loss: 0.00013746
Iteration 96/1000 | Loss: 0.00013746
Iteration 97/1000 | Loss: 0.00013746
Iteration 98/1000 | Loss: 0.00013746
Iteration 99/1000 | Loss: 0.00013746
Iteration 100/1000 | Loss: 0.00013746
Iteration 101/1000 | Loss: 0.00013745
Iteration 102/1000 | Loss: 0.00013745
Iteration 103/1000 | Loss: 0.00013745
Iteration 104/1000 | Loss: 0.00013745
Iteration 105/1000 | Loss: 0.00013745
Iteration 106/1000 | Loss: 0.00013745
Iteration 107/1000 | Loss: 0.00013745
Iteration 108/1000 | Loss: 0.00013745
Iteration 109/1000 | Loss: 0.00013745
Iteration 110/1000 | Loss: 0.00013744
Iteration 111/1000 | Loss: 0.00013744
Iteration 112/1000 | Loss: 0.00013744
Iteration 113/1000 | Loss: 0.00013743
Iteration 114/1000 | Loss: 0.00013743
Iteration 115/1000 | Loss: 0.00013743
Iteration 116/1000 | Loss: 0.00013743
Iteration 117/1000 | Loss: 0.00013743
Iteration 118/1000 | Loss: 0.00013742
Iteration 119/1000 | Loss: 0.00013742
Iteration 120/1000 | Loss: 0.00013742
Iteration 121/1000 | Loss: 0.00013742
Iteration 122/1000 | Loss: 0.00013741
Iteration 123/1000 | Loss: 0.00013741
Iteration 124/1000 | Loss: 0.00013741
Iteration 125/1000 | Loss: 0.00013741
Iteration 126/1000 | Loss: 0.00013741
Iteration 127/1000 | Loss: 0.00013741
Iteration 128/1000 | Loss: 0.00013741
Iteration 129/1000 | Loss: 0.00013740
Iteration 130/1000 | Loss: 0.00013740
Iteration 131/1000 | Loss: 0.00013740
Iteration 132/1000 | Loss: 0.00013740
Iteration 133/1000 | Loss: 0.00013740
Iteration 134/1000 | Loss: 0.00013740
Iteration 135/1000 | Loss: 0.00013740
Iteration 136/1000 | Loss: 0.00013740
Iteration 137/1000 | Loss: 0.00013740
Iteration 138/1000 | Loss: 0.00013740
Iteration 139/1000 | Loss: 0.00013740
Iteration 140/1000 | Loss: 0.00013740
Iteration 141/1000 | Loss: 0.00013739
Iteration 142/1000 | Loss: 0.00013739
Iteration 143/1000 | Loss: 0.00013739
Iteration 144/1000 | Loss: 0.00013739
Iteration 145/1000 | Loss: 0.00013739
Iteration 146/1000 | Loss: 0.00013739
Iteration 147/1000 | Loss: 0.00013739
Iteration 148/1000 | Loss: 0.00013739
Iteration 149/1000 | Loss: 0.00013739
Iteration 150/1000 | Loss: 0.00013739
Iteration 151/1000 | Loss: 0.00013739
Iteration 152/1000 | Loss: 0.00013739
Iteration 153/1000 | Loss: 0.00013739
Iteration 154/1000 | Loss: 0.00013739
Iteration 155/1000 | Loss: 0.00013739
Iteration 156/1000 | Loss: 0.00013739
Iteration 157/1000 | Loss: 0.00013738
Iteration 158/1000 | Loss: 0.00013738
Iteration 159/1000 | Loss: 0.00013738
Iteration 160/1000 | Loss: 0.00013738
Iteration 161/1000 | Loss: 0.00013738
Iteration 162/1000 | Loss: 0.00013738
Iteration 163/1000 | Loss: 0.00013738
Iteration 164/1000 | Loss: 0.00013738
Iteration 165/1000 | Loss: 0.00013738
Iteration 166/1000 | Loss: 0.00013738
Iteration 167/1000 | Loss: 0.00013738
Iteration 168/1000 | Loss: 0.00013738
Iteration 169/1000 | Loss: 0.00013738
Iteration 170/1000 | Loss: 0.00013738
Iteration 171/1000 | Loss: 0.00013738
Iteration 172/1000 | Loss: 0.00013738
Iteration 173/1000 | Loss: 0.00013738
Iteration 174/1000 | Loss: 0.00013738
Iteration 175/1000 | Loss: 0.00013738
Iteration 176/1000 | Loss: 0.00013738
Iteration 177/1000 | Loss: 0.00013738
Iteration 178/1000 | Loss: 0.00013738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [0.00013737786503043026, 0.00013737786503043026, 0.00013737786503043026, 0.00013737786503043026, 0.00013737786503043026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013737786503043026

Optimization complete. Final v2v error: 6.86993408203125 mm

Highest mean error: 11.962733268737793 mm for frame 59

Lowest mean error: 4.225940704345703 mm for frame 106

Saving results

Total time: 137.35297966003418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967588
Iteration 2/25 | Loss: 0.00163303
Iteration 3/25 | Loss: 0.00126135
Iteration 4/25 | Loss: 0.00118521
Iteration 5/25 | Loss: 0.00117573
Iteration 6/25 | Loss: 0.00117965
Iteration 7/25 | Loss: 0.00115378
Iteration 8/25 | Loss: 0.00115458
Iteration 9/25 | Loss: 0.00114792
Iteration 10/25 | Loss: 0.00115379
Iteration 11/25 | Loss: 0.00114627
Iteration 12/25 | Loss: 0.00111663
Iteration 13/25 | Loss: 0.00111058
Iteration 14/25 | Loss: 0.00111007
Iteration 15/25 | Loss: 0.00111003
Iteration 16/25 | Loss: 0.00111003
Iteration 17/25 | Loss: 0.00111003
Iteration 18/25 | Loss: 0.00111003
Iteration 19/25 | Loss: 0.00111003
Iteration 20/25 | Loss: 0.00111003
Iteration 21/25 | Loss: 0.00111003
Iteration 22/25 | Loss: 0.00111003
Iteration 23/25 | Loss: 0.00111002
Iteration 24/25 | Loss: 0.00111002
Iteration 25/25 | Loss: 0.00111002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74010038
Iteration 2/25 | Loss: 0.00212428
Iteration 3/25 | Loss: 0.00212426
Iteration 4/25 | Loss: 0.00212426
Iteration 5/25 | Loss: 0.00212426
Iteration 6/25 | Loss: 0.00212426
Iteration 7/25 | Loss: 0.00212426
Iteration 8/25 | Loss: 0.00212426
Iteration 9/25 | Loss: 0.00212426
Iteration 10/25 | Loss: 0.00212426
Iteration 11/25 | Loss: 0.00212426
Iteration 12/25 | Loss: 0.00212426
Iteration 13/25 | Loss: 0.00212426
Iteration 14/25 | Loss: 0.00212426
Iteration 15/25 | Loss: 0.00212426
Iteration 16/25 | Loss: 0.00212426
Iteration 17/25 | Loss: 0.00212426
Iteration 18/25 | Loss: 0.00212426
Iteration 19/25 | Loss: 0.00212426
Iteration 20/25 | Loss: 0.00212426
Iteration 21/25 | Loss: 0.00212426
Iteration 22/25 | Loss: 0.00212426
Iteration 23/25 | Loss: 0.00212426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0021242599468678236, 0.0021242599468678236, 0.0021242599468678236, 0.0021242599468678236, 0.0021242599468678236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021242599468678236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212426
Iteration 2/1000 | Loss: 0.00006389
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002113
Iteration 5/1000 | Loss: 0.00001861
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001662
Iteration 8/1000 | Loss: 0.00069578
Iteration 9/1000 | Loss: 0.00115114
Iteration 10/1000 | Loss: 0.00008083
Iteration 11/1000 | Loss: 0.00057194
Iteration 12/1000 | Loss: 0.00017425
Iteration 13/1000 | Loss: 0.00008266
Iteration 14/1000 | Loss: 0.00004303
Iteration 15/1000 | Loss: 0.00005026
Iteration 16/1000 | Loss: 0.00001921
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001442
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001436
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001431
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001421
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001421
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001421
Iteration 37/1000 | Loss: 0.00001421
Iteration 38/1000 | Loss: 0.00001421
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001418
Iteration 46/1000 | Loss: 0.00001418
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001407
Iteration 60/1000 | Loss: 0.00001407
Iteration 61/1000 | Loss: 0.00001407
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001406
Iteration 64/1000 | Loss: 0.00001406
Iteration 65/1000 | Loss: 0.00001406
Iteration 66/1000 | Loss: 0.00001406
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001406
Iteration 69/1000 | Loss: 0.00001406
Iteration 70/1000 | Loss: 0.00001406
Iteration 71/1000 | Loss: 0.00001406
Iteration 72/1000 | Loss: 0.00001405
Iteration 73/1000 | Loss: 0.00001405
Iteration 74/1000 | Loss: 0.00001405
Iteration 75/1000 | Loss: 0.00001405
Iteration 76/1000 | Loss: 0.00001405
Iteration 77/1000 | Loss: 0.00001404
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001402
Iteration 93/1000 | Loss: 0.00001402
Iteration 94/1000 | Loss: 0.00001402
Iteration 95/1000 | Loss: 0.00001402
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001397
Iteration 108/1000 | Loss: 0.00001396
Iteration 109/1000 | Loss: 0.00001396
Iteration 110/1000 | Loss: 0.00001396
Iteration 111/1000 | Loss: 0.00001396
Iteration 112/1000 | Loss: 0.00001396
Iteration 113/1000 | Loss: 0.00001396
Iteration 114/1000 | Loss: 0.00001395
Iteration 115/1000 | Loss: 0.00001395
Iteration 116/1000 | Loss: 0.00001395
Iteration 117/1000 | Loss: 0.00001395
Iteration 118/1000 | Loss: 0.00001395
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001393
Iteration 129/1000 | Loss: 0.00001393
Iteration 130/1000 | Loss: 0.00001393
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001393
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001391
Iteration 138/1000 | Loss: 0.00001391
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001390
Iteration 144/1000 | Loss: 0.00001390
Iteration 145/1000 | Loss: 0.00001390
Iteration 146/1000 | Loss: 0.00001390
Iteration 147/1000 | Loss: 0.00001389
Iteration 148/1000 | Loss: 0.00001389
Iteration 149/1000 | Loss: 0.00001389
Iteration 150/1000 | Loss: 0.00001388
Iteration 151/1000 | Loss: 0.00001388
Iteration 152/1000 | Loss: 0.00001388
Iteration 153/1000 | Loss: 0.00001388
Iteration 154/1000 | Loss: 0.00001388
Iteration 155/1000 | Loss: 0.00001388
Iteration 156/1000 | Loss: 0.00001388
Iteration 157/1000 | Loss: 0.00001388
Iteration 158/1000 | Loss: 0.00001388
Iteration 159/1000 | Loss: 0.00001388
Iteration 160/1000 | Loss: 0.00001387
Iteration 161/1000 | Loss: 0.00001387
Iteration 162/1000 | Loss: 0.00001387
Iteration 163/1000 | Loss: 0.00001387
Iteration 164/1000 | Loss: 0.00001387
Iteration 165/1000 | Loss: 0.00001387
Iteration 166/1000 | Loss: 0.00001387
Iteration 167/1000 | Loss: 0.00001386
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001386
Iteration 170/1000 | Loss: 0.00001386
Iteration 171/1000 | Loss: 0.00001386
Iteration 172/1000 | Loss: 0.00001386
Iteration 173/1000 | Loss: 0.00001386
Iteration 174/1000 | Loss: 0.00001386
Iteration 175/1000 | Loss: 0.00001385
Iteration 176/1000 | Loss: 0.00001385
Iteration 177/1000 | Loss: 0.00001385
Iteration 178/1000 | Loss: 0.00001385
Iteration 179/1000 | Loss: 0.00001385
Iteration 180/1000 | Loss: 0.00001385
Iteration 181/1000 | Loss: 0.00001385
Iteration 182/1000 | Loss: 0.00001385
Iteration 183/1000 | Loss: 0.00001385
Iteration 184/1000 | Loss: 0.00001385
Iteration 185/1000 | Loss: 0.00001385
Iteration 186/1000 | Loss: 0.00001385
Iteration 187/1000 | Loss: 0.00001385
Iteration 188/1000 | Loss: 0.00001385
Iteration 189/1000 | Loss: 0.00001384
Iteration 190/1000 | Loss: 0.00001384
Iteration 191/1000 | Loss: 0.00001384
Iteration 192/1000 | Loss: 0.00001384
Iteration 193/1000 | Loss: 0.00001384
Iteration 194/1000 | Loss: 0.00001384
Iteration 195/1000 | Loss: 0.00001384
Iteration 196/1000 | Loss: 0.00001384
Iteration 197/1000 | Loss: 0.00001384
Iteration 198/1000 | Loss: 0.00001384
Iteration 199/1000 | Loss: 0.00001384
Iteration 200/1000 | Loss: 0.00001384
Iteration 201/1000 | Loss: 0.00001384
Iteration 202/1000 | Loss: 0.00001384
Iteration 203/1000 | Loss: 0.00001384
Iteration 204/1000 | Loss: 0.00001383
Iteration 205/1000 | Loss: 0.00001383
Iteration 206/1000 | Loss: 0.00001383
Iteration 207/1000 | Loss: 0.00001383
Iteration 208/1000 | Loss: 0.00001383
Iteration 209/1000 | Loss: 0.00001383
Iteration 210/1000 | Loss: 0.00001383
Iteration 211/1000 | Loss: 0.00001383
Iteration 212/1000 | Loss: 0.00001383
Iteration 213/1000 | Loss: 0.00001383
Iteration 214/1000 | Loss: 0.00001383
Iteration 215/1000 | Loss: 0.00001383
Iteration 216/1000 | Loss: 0.00001383
Iteration 217/1000 | Loss: 0.00001382
Iteration 218/1000 | Loss: 0.00001382
Iteration 219/1000 | Loss: 0.00001382
Iteration 220/1000 | Loss: 0.00001382
Iteration 221/1000 | Loss: 0.00001382
Iteration 222/1000 | Loss: 0.00001382
Iteration 223/1000 | Loss: 0.00001381
Iteration 224/1000 | Loss: 0.00001381
Iteration 225/1000 | Loss: 0.00001381
Iteration 226/1000 | Loss: 0.00001381
Iteration 227/1000 | Loss: 0.00001381
Iteration 228/1000 | Loss: 0.00001381
Iteration 229/1000 | Loss: 0.00001381
Iteration 230/1000 | Loss: 0.00001381
Iteration 231/1000 | Loss: 0.00001380
Iteration 232/1000 | Loss: 0.00001380
Iteration 233/1000 | Loss: 0.00001380
Iteration 234/1000 | Loss: 0.00001380
Iteration 235/1000 | Loss: 0.00001380
Iteration 236/1000 | Loss: 0.00001380
Iteration 237/1000 | Loss: 0.00001380
Iteration 238/1000 | Loss: 0.00001380
Iteration 239/1000 | Loss: 0.00001380
Iteration 240/1000 | Loss: 0.00001380
Iteration 241/1000 | Loss: 0.00001380
Iteration 242/1000 | Loss: 0.00001380
Iteration 243/1000 | Loss: 0.00001380
Iteration 244/1000 | Loss: 0.00001380
Iteration 245/1000 | Loss: 0.00001380
Iteration 246/1000 | Loss: 0.00001380
Iteration 247/1000 | Loss: 0.00001380
Iteration 248/1000 | Loss: 0.00001380
Iteration 249/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.3798862710245885e-05, 1.3798862710245885e-05, 1.3798862710245885e-05, 1.3798862710245885e-05, 1.3798862710245885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3798862710245885e-05

Optimization complete. Final v2v error: 3.124919891357422 mm

Highest mean error: 3.9181923866271973 mm for frame 85

Lowest mean error: 2.6779496669769287 mm for frame 35

Saving results

Total time: 69.71922135353088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005717
Iteration 2/25 | Loss: 0.00371011
Iteration 3/25 | Loss: 0.00229612
Iteration 4/25 | Loss: 0.00201827
Iteration 5/25 | Loss: 0.00184751
Iteration 6/25 | Loss: 0.00178141
Iteration 7/25 | Loss: 0.00174766
Iteration 8/25 | Loss: 0.00172529
Iteration 9/25 | Loss: 0.00172335
Iteration 10/25 | Loss: 0.00170968
Iteration 11/25 | Loss: 0.00169480
Iteration 12/25 | Loss: 0.00169310
Iteration 13/25 | Loss: 0.00168194
Iteration 14/25 | Loss: 0.00168101
Iteration 15/25 | Loss: 0.00166818
Iteration 16/25 | Loss: 0.00166638
Iteration 17/25 | Loss: 0.00166066
Iteration 18/25 | Loss: 0.00165865
Iteration 19/25 | Loss: 0.00165835
Iteration 20/25 | Loss: 0.00165778
Iteration 21/25 | Loss: 0.00165872
Iteration 22/25 | Loss: 0.00165683
Iteration 23/25 | Loss: 0.00165463
Iteration 24/25 | Loss: 0.00165683
Iteration 25/25 | Loss: 0.00165418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12558746
Iteration 2/25 | Loss: 0.01021671
Iteration 3/25 | Loss: 0.00996017
Iteration 4/25 | Loss: 0.00996017
Iteration 5/25 | Loss: 0.00996017
Iteration 6/25 | Loss: 0.00996017
Iteration 7/25 | Loss: 0.00996017
Iteration 8/25 | Loss: 0.00996017
Iteration 9/25 | Loss: 0.00996017
Iteration 10/25 | Loss: 0.00996017
Iteration 11/25 | Loss: 0.00996017
Iteration 12/25 | Loss: 0.00996017
Iteration 13/25 | Loss: 0.00996017
Iteration 14/25 | Loss: 0.00996017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.009960169903934002, 0.009960169903934002, 0.009960169903934002, 0.009960169903934002, 0.009960169903934002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009960169903934002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00996017
Iteration 2/1000 | Loss: 0.00134536
Iteration 3/1000 | Loss: 0.00402059
Iteration 4/1000 | Loss: 0.00172855
Iteration 5/1000 | Loss: 0.00099593
Iteration 6/1000 | Loss: 0.00581653
Iteration 7/1000 | Loss: 0.01614659
Iteration 8/1000 | Loss: 0.01212428
Iteration 9/1000 | Loss: 0.00544504
Iteration 10/1000 | Loss: 0.00143232
Iteration 11/1000 | Loss: 0.00161943
Iteration 12/1000 | Loss: 0.00116911
Iteration 13/1000 | Loss: 0.00152317
Iteration 14/1000 | Loss: 0.00127488
Iteration 15/1000 | Loss: 0.00066811
Iteration 16/1000 | Loss: 0.00134008
Iteration 17/1000 | Loss: 0.00193045
Iteration 18/1000 | Loss: 0.00119671
Iteration 19/1000 | Loss: 0.00048503
Iteration 20/1000 | Loss: 0.00050911
Iteration 21/1000 | Loss: 0.00047898
Iteration 22/1000 | Loss: 0.00102668
Iteration 23/1000 | Loss: 0.00121407
Iteration 24/1000 | Loss: 0.00241192
Iteration 25/1000 | Loss: 0.00339608
Iteration 26/1000 | Loss: 0.00131961
Iteration 27/1000 | Loss: 0.00138158
Iteration 28/1000 | Loss: 0.00037440
Iteration 29/1000 | Loss: 0.00064619
Iteration 30/1000 | Loss: 0.00031621
Iteration 31/1000 | Loss: 0.00040058
Iteration 32/1000 | Loss: 0.00023694
Iteration 33/1000 | Loss: 0.00045227
Iteration 34/1000 | Loss: 0.00060531
Iteration 35/1000 | Loss: 0.00022220
Iteration 36/1000 | Loss: 0.00110845
Iteration 37/1000 | Loss: 0.00176113
Iteration 38/1000 | Loss: 0.00067550
Iteration 39/1000 | Loss: 0.00052405
Iteration 40/1000 | Loss: 0.00037775
Iteration 41/1000 | Loss: 0.00034634
Iteration 42/1000 | Loss: 0.00022045
Iteration 43/1000 | Loss: 0.00034132
Iteration 44/1000 | Loss: 0.00093290
Iteration 45/1000 | Loss: 0.00224897
Iteration 46/1000 | Loss: 0.00025295
Iteration 47/1000 | Loss: 0.00019988
Iteration 48/1000 | Loss: 0.00019970
Iteration 49/1000 | Loss: 0.00062941
Iteration 50/1000 | Loss: 0.00052395
Iteration 51/1000 | Loss: 0.00061050
Iteration 52/1000 | Loss: 0.00035622
Iteration 53/1000 | Loss: 0.00046688
Iteration 54/1000 | Loss: 0.00022914
Iteration 55/1000 | Loss: 0.00035444
Iteration 56/1000 | Loss: 0.00049484
Iteration 57/1000 | Loss: 0.00037694
Iteration 58/1000 | Loss: 0.00037596
Iteration 59/1000 | Loss: 0.00044941
Iteration 60/1000 | Loss: 0.00046554
Iteration 61/1000 | Loss: 0.00032256
Iteration 62/1000 | Loss: 0.00050759
Iteration 63/1000 | Loss: 0.00018671
Iteration 64/1000 | Loss: 0.00024628
Iteration 65/1000 | Loss: 0.00031084
Iteration 66/1000 | Loss: 0.00026010
Iteration 67/1000 | Loss: 0.00028059
Iteration 68/1000 | Loss: 0.00018150
Iteration 69/1000 | Loss: 0.00024974
Iteration 70/1000 | Loss: 0.00034155
Iteration 71/1000 | Loss: 0.00063330
Iteration 72/1000 | Loss: 0.00020451
Iteration 73/1000 | Loss: 0.00027470
Iteration 74/1000 | Loss: 0.00019168
Iteration 75/1000 | Loss: 0.00018877
Iteration 76/1000 | Loss: 0.00016122
Iteration 77/1000 | Loss: 0.00032857
Iteration 78/1000 | Loss: 0.00035269
Iteration 79/1000 | Loss: 0.00033074
Iteration 80/1000 | Loss: 0.00025060
Iteration 81/1000 | Loss: 0.00022971
Iteration 82/1000 | Loss: 0.00022543
Iteration 83/1000 | Loss: 0.00018803
Iteration 84/1000 | Loss: 0.00024696
Iteration 85/1000 | Loss: 0.00025201
Iteration 86/1000 | Loss: 0.00023996
Iteration 87/1000 | Loss: 0.00040468
Iteration 88/1000 | Loss: 0.00038920
Iteration 89/1000 | Loss: 0.00016868
Iteration 90/1000 | Loss: 0.00016229
Iteration 91/1000 | Loss: 0.00032560
Iteration 92/1000 | Loss: 0.00016164
Iteration 93/1000 | Loss: 0.00025803
Iteration 94/1000 | Loss: 0.00090388
Iteration 95/1000 | Loss: 0.00019727
Iteration 96/1000 | Loss: 0.00024399
Iteration 97/1000 | Loss: 0.00026753
Iteration 98/1000 | Loss: 0.00020698
Iteration 99/1000 | Loss: 0.00015395
Iteration 100/1000 | Loss: 0.00015868
Iteration 101/1000 | Loss: 0.00030395
Iteration 102/1000 | Loss: 0.00092597
Iteration 103/1000 | Loss: 0.00035960
Iteration 104/1000 | Loss: 0.00044668
Iteration 105/1000 | Loss: 0.00040763
Iteration 106/1000 | Loss: 0.00015207
Iteration 107/1000 | Loss: 0.00049796
Iteration 108/1000 | Loss: 0.00032581
Iteration 109/1000 | Loss: 0.00014671
Iteration 110/1000 | Loss: 0.00014129
Iteration 111/1000 | Loss: 0.00051739
Iteration 112/1000 | Loss: 0.00014175
Iteration 113/1000 | Loss: 0.00013652
Iteration 114/1000 | Loss: 0.00046436
Iteration 115/1000 | Loss: 0.00025022
Iteration 116/1000 | Loss: 0.00014179
Iteration 117/1000 | Loss: 0.00013639
Iteration 118/1000 | Loss: 0.00025824
Iteration 119/1000 | Loss: 0.00023427
Iteration 120/1000 | Loss: 0.00017637
Iteration 121/1000 | Loss: 0.00038527
Iteration 122/1000 | Loss: 0.00040092
Iteration 123/1000 | Loss: 0.00060197
Iteration 124/1000 | Loss: 0.00029173
Iteration 125/1000 | Loss: 0.00017864
Iteration 126/1000 | Loss: 0.00023452
Iteration 127/1000 | Loss: 0.00012909
Iteration 128/1000 | Loss: 0.00013486
Iteration 129/1000 | Loss: 0.00016061
Iteration 130/1000 | Loss: 0.00014041
Iteration 131/1000 | Loss: 0.00012691
Iteration 132/1000 | Loss: 0.00013510
Iteration 133/1000 | Loss: 0.00021429
Iteration 134/1000 | Loss: 0.00012689
Iteration 135/1000 | Loss: 0.00012204
Iteration 136/1000 | Loss: 0.00012845
Iteration 137/1000 | Loss: 0.00034523
Iteration 138/1000 | Loss: 0.00024287
Iteration 139/1000 | Loss: 0.00024989
Iteration 140/1000 | Loss: 0.00036377
Iteration 141/1000 | Loss: 0.00020756
Iteration 142/1000 | Loss: 0.00046722
Iteration 143/1000 | Loss: 0.00039968
Iteration 144/1000 | Loss: 0.00026429
Iteration 145/1000 | Loss: 0.00058499
Iteration 146/1000 | Loss: 0.00051058
Iteration 147/1000 | Loss: 0.00028687
Iteration 148/1000 | Loss: 0.00013198
Iteration 149/1000 | Loss: 0.00014986
Iteration 150/1000 | Loss: 0.00044867
Iteration 151/1000 | Loss: 0.00012411
Iteration 152/1000 | Loss: 0.00011944
Iteration 153/1000 | Loss: 0.00011668
Iteration 154/1000 | Loss: 0.00011726
Iteration 155/1000 | Loss: 0.00011435
Iteration 156/1000 | Loss: 0.00011239
Iteration 157/1000 | Loss: 0.00011159
Iteration 158/1000 | Loss: 0.00011108
Iteration 159/1000 | Loss: 0.00033786
Iteration 160/1000 | Loss: 0.00032825
Iteration 161/1000 | Loss: 0.00131237
Iteration 162/1000 | Loss: 0.00014072
Iteration 163/1000 | Loss: 0.00012922
Iteration 164/1000 | Loss: 0.00013017
Iteration 165/1000 | Loss: 0.00011823
Iteration 166/1000 | Loss: 0.00038457
Iteration 167/1000 | Loss: 0.00012854
Iteration 168/1000 | Loss: 0.00022884
Iteration 169/1000 | Loss: 0.00012304
Iteration 170/1000 | Loss: 0.00012072
Iteration 171/1000 | Loss: 0.00011874
Iteration 172/1000 | Loss: 0.00011188
Iteration 173/1000 | Loss: 0.00031666
Iteration 174/1000 | Loss: 0.00193900
Iteration 175/1000 | Loss: 0.00101549
Iteration 176/1000 | Loss: 0.00141305
Iteration 177/1000 | Loss: 0.00021293
Iteration 178/1000 | Loss: 0.00012511
Iteration 179/1000 | Loss: 0.00011434
Iteration 180/1000 | Loss: 0.00047593
Iteration 181/1000 | Loss: 0.00035115
Iteration 182/1000 | Loss: 0.00020525
Iteration 183/1000 | Loss: 0.00020423
Iteration 184/1000 | Loss: 0.00019347
Iteration 185/1000 | Loss: 0.00019049
Iteration 186/1000 | Loss: 0.00011582
Iteration 187/1000 | Loss: 0.00016284
Iteration 188/1000 | Loss: 0.00009808
Iteration 189/1000 | Loss: 0.00010821
Iteration 190/1000 | Loss: 0.00010314
Iteration 191/1000 | Loss: 0.00009445
Iteration 192/1000 | Loss: 0.00032925
Iteration 193/1000 | Loss: 0.00018630
Iteration 194/1000 | Loss: 0.00024545
Iteration 195/1000 | Loss: 0.00012766
Iteration 196/1000 | Loss: 0.00010724
Iteration 197/1000 | Loss: 0.00009482
Iteration 198/1000 | Loss: 0.00011842
Iteration 199/1000 | Loss: 0.00009274
Iteration 200/1000 | Loss: 0.00009142
Iteration 201/1000 | Loss: 0.00009207
Iteration 202/1000 | Loss: 0.00009100
Iteration 203/1000 | Loss: 0.00031575
Iteration 204/1000 | Loss: 0.00165748
Iteration 205/1000 | Loss: 0.00055816
Iteration 206/1000 | Loss: 0.00106102
Iteration 207/1000 | Loss: 0.00048534
Iteration 208/1000 | Loss: 0.00026498
Iteration 209/1000 | Loss: 0.00021806
Iteration 210/1000 | Loss: 0.00013752
Iteration 211/1000 | Loss: 0.00012702
Iteration 212/1000 | Loss: 0.00010080
Iteration 213/1000 | Loss: 0.00012746
Iteration 214/1000 | Loss: 0.00031776
Iteration 215/1000 | Loss: 0.00013036
Iteration 216/1000 | Loss: 0.00027824
Iteration 217/1000 | Loss: 0.00008746
Iteration 218/1000 | Loss: 0.00011682
Iteration 219/1000 | Loss: 0.00008388
Iteration 220/1000 | Loss: 0.00010066
Iteration 221/1000 | Loss: 0.00009218
Iteration 222/1000 | Loss: 0.00010452
Iteration 223/1000 | Loss: 0.00008118
Iteration 224/1000 | Loss: 0.00008779
Iteration 225/1000 | Loss: 0.00010342
Iteration 226/1000 | Loss: 0.00024525
Iteration 227/1000 | Loss: 0.00008257
Iteration 228/1000 | Loss: 0.00007959
Iteration 229/1000 | Loss: 0.00008650
Iteration 230/1000 | Loss: 0.00007931
Iteration 231/1000 | Loss: 0.00007930
Iteration 232/1000 | Loss: 0.00008817
Iteration 233/1000 | Loss: 0.00055975
Iteration 234/1000 | Loss: 0.00016039
Iteration 235/1000 | Loss: 0.00007919
Iteration 236/1000 | Loss: 0.00007892
Iteration 237/1000 | Loss: 0.00007891
Iteration 238/1000 | Loss: 0.00007888
Iteration 239/1000 | Loss: 0.00007887
Iteration 240/1000 | Loss: 0.00008013
Iteration 241/1000 | Loss: 0.00007882
Iteration 242/1000 | Loss: 0.00007882
Iteration 243/1000 | Loss: 0.00007882
Iteration 244/1000 | Loss: 0.00007882
Iteration 245/1000 | Loss: 0.00007882
Iteration 246/1000 | Loss: 0.00007882
Iteration 247/1000 | Loss: 0.00007882
Iteration 248/1000 | Loss: 0.00007882
Iteration 249/1000 | Loss: 0.00007882
Iteration 250/1000 | Loss: 0.00007882
Iteration 251/1000 | Loss: 0.00007882
Iteration 252/1000 | Loss: 0.00007882
Iteration 253/1000 | Loss: 0.00007881
Iteration 254/1000 | Loss: 0.00007881
Iteration 255/1000 | Loss: 0.00007881
Iteration 256/1000 | Loss: 0.00007881
Iteration 257/1000 | Loss: 0.00007881
Iteration 258/1000 | Loss: 0.00007881
Iteration 259/1000 | Loss: 0.00007881
Iteration 260/1000 | Loss: 0.00007881
Iteration 261/1000 | Loss: 0.00007881
Iteration 262/1000 | Loss: 0.00007881
Iteration 263/1000 | Loss: 0.00007881
Iteration 264/1000 | Loss: 0.00007881
Iteration 265/1000 | Loss: 0.00007881
Iteration 266/1000 | Loss: 0.00007881
Iteration 267/1000 | Loss: 0.00007881
Iteration 268/1000 | Loss: 0.00007881
Iteration 269/1000 | Loss: 0.00007881
Iteration 270/1000 | Loss: 0.00007881
Iteration 271/1000 | Loss: 0.00007881
Iteration 272/1000 | Loss: 0.00007881
Iteration 273/1000 | Loss: 0.00007881
Iteration 274/1000 | Loss: 0.00007881
Iteration 275/1000 | Loss: 0.00007881
Iteration 276/1000 | Loss: 0.00007881
Iteration 277/1000 | Loss: 0.00007881
Iteration 278/1000 | Loss: 0.00007881
Iteration 279/1000 | Loss: 0.00007881
Iteration 280/1000 | Loss: 0.00007881
Iteration 281/1000 | Loss: 0.00007881
Iteration 282/1000 | Loss: 0.00007881
Iteration 283/1000 | Loss: 0.00007881
Iteration 284/1000 | Loss: 0.00007881
Iteration 285/1000 | Loss: 0.00007881
Iteration 286/1000 | Loss: 0.00007881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [7.881425699451938e-05, 7.881425699451938e-05, 7.881425699451938e-05, 7.881425699451938e-05, 7.881425699451938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.881425699451938e-05

Optimization complete. Final v2v error: 4.583704471588135 mm

Highest mean error: 13.364036560058594 mm for frame 9

Lowest mean error: 2.9077625274658203 mm for frame 92

Saving results

Total time: 430.803049325943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605136
Iteration 2/25 | Loss: 0.00125569
Iteration 3/25 | Loss: 0.00111366
Iteration 4/25 | Loss: 0.00108867
Iteration 5/25 | Loss: 0.00108214
Iteration 6/25 | Loss: 0.00108041
Iteration 7/25 | Loss: 0.00108041
Iteration 8/25 | Loss: 0.00108041
Iteration 9/25 | Loss: 0.00108041
Iteration 10/25 | Loss: 0.00108041
Iteration 11/25 | Loss: 0.00108041
Iteration 12/25 | Loss: 0.00108041
Iteration 13/25 | Loss: 0.00108041
Iteration 14/25 | Loss: 0.00108041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00108041288331151, 0.00108041288331151, 0.00108041288331151, 0.00108041288331151, 0.00108041288331151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00108041288331151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57381523
Iteration 2/25 | Loss: 0.00237029
Iteration 3/25 | Loss: 0.00237029
Iteration 4/25 | Loss: 0.00237028
Iteration 5/25 | Loss: 0.00237028
Iteration 6/25 | Loss: 0.00237028
Iteration 7/25 | Loss: 0.00237028
Iteration 8/25 | Loss: 0.00237028
Iteration 9/25 | Loss: 0.00237028
Iteration 10/25 | Loss: 0.00237028
Iteration 11/25 | Loss: 0.00237028
Iteration 12/25 | Loss: 0.00237028
Iteration 13/25 | Loss: 0.00237028
Iteration 14/25 | Loss: 0.00237028
Iteration 15/25 | Loss: 0.00237028
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023702827747911215, 0.0023702827747911215, 0.0023702827747911215, 0.0023702827747911215, 0.0023702827747911215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023702827747911215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237028
Iteration 2/1000 | Loss: 0.00006147
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001805
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001616
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001390
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001342
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001337
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001332
Iteration 63/1000 | Loss: 0.00001332
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001331
Iteration 68/1000 | Loss: 0.00001331
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Iteration 72/1000 | Loss: 0.00001330
Iteration 73/1000 | Loss: 0.00001330
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001327
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001326
Iteration 122/1000 | Loss: 0.00001326
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001326
Iteration 133/1000 | Loss: 0.00001326
Iteration 134/1000 | Loss: 0.00001326
Iteration 135/1000 | Loss: 0.00001326
Iteration 136/1000 | Loss: 0.00001326
Iteration 137/1000 | Loss: 0.00001326
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001325
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3249020412331447e-05, 1.3249020412331447e-05, 1.3249020412331447e-05, 1.3249020412331447e-05, 1.3249020412331447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3249020412331447e-05

Optimization complete. Final v2v error: 3.074964761734009 mm

Highest mean error: 3.4257524013519287 mm for frame 169

Lowest mean error: 2.802013635635376 mm for frame 43

Saving results

Total time: 45.096543312072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394861
Iteration 2/25 | Loss: 0.00121745
Iteration 3/25 | Loss: 0.00114260
Iteration 4/25 | Loss: 0.00112449
Iteration 5/25 | Loss: 0.00111908
Iteration 6/25 | Loss: 0.00111777
Iteration 7/25 | Loss: 0.00111735
Iteration 8/25 | Loss: 0.00111735
Iteration 9/25 | Loss: 0.00111735
Iteration 10/25 | Loss: 0.00111735
Iteration 11/25 | Loss: 0.00111735
Iteration 12/25 | Loss: 0.00111735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011173501843586564, 0.0011173501843586564, 0.0011173501843586564, 0.0011173501843586564, 0.0011173501843586564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011173501843586564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15492129
Iteration 2/25 | Loss: 0.00387317
Iteration 3/25 | Loss: 0.00387317
Iteration 4/25 | Loss: 0.00387317
Iteration 5/25 | Loss: 0.00387317
Iteration 6/25 | Loss: 0.00387317
Iteration 7/25 | Loss: 0.00387317
Iteration 8/25 | Loss: 0.00387317
Iteration 9/25 | Loss: 0.00387317
Iteration 10/25 | Loss: 0.00387317
Iteration 11/25 | Loss: 0.00387317
Iteration 12/25 | Loss: 0.00387317
Iteration 13/25 | Loss: 0.00387317
Iteration 14/25 | Loss: 0.00387317
Iteration 15/25 | Loss: 0.00387317
Iteration 16/25 | Loss: 0.00387317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0038731684908270836, 0.0038731684908270836, 0.0038731684908270836, 0.0038731684908270836, 0.0038731684908270836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038731684908270836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387317
Iteration 2/1000 | Loss: 0.00006042
Iteration 3/1000 | Loss: 0.00002696
Iteration 4/1000 | Loss: 0.00002182
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001606
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001538
Iteration 22/1000 | Loss: 0.00001537
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001535
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001531
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001530
Iteration 48/1000 | Loss: 0.00001530
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001529
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001529
Iteration 55/1000 | Loss: 0.00001529
Iteration 56/1000 | Loss: 0.00001529
Iteration 57/1000 | Loss: 0.00001529
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001528
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001526
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001526
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001526
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001525
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001525
Iteration 111/1000 | Loss: 0.00001525
Iteration 112/1000 | Loss: 0.00001525
Iteration 113/1000 | Loss: 0.00001525
Iteration 114/1000 | Loss: 0.00001525
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001525
Iteration 119/1000 | Loss: 0.00001525
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001525
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00001525
Iteration 138/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.5254262507369276e-05, 1.5254262507369276e-05, 1.5254262507369276e-05, 1.5254262507369276e-05, 1.5254262507369276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5254262507369276e-05

Optimization complete. Final v2v error: 3.213331699371338 mm

Highest mean error: 3.511369466781616 mm for frame 62

Lowest mean error: 2.9730982780456543 mm for frame 18

Saving results

Total time: 37.49214053153992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761342
Iteration 2/25 | Loss: 0.00149955
Iteration 3/25 | Loss: 0.00122821
Iteration 4/25 | Loss: 0.00118433
Iteration 5/25 | Loss: 0.00115185
Iteration 6/25 | Loss: 0.00114780
Iteration 7/25 | Loss: 0.00114574
Iteration 8/25 | Loss: 0.00114343
Iteration 9/25 | Loss: 0.00114279
Iteration 10/25 | Loss: 0.00114185
Iteration 11/25 | Loss: 0.00114125
Iteration 12/25 | Loss: 0.00114103
Iteration 13/25 | Loss: 0.00114092
Iteration 14/25 | Loss: 0.00114088
Iteration 15/25 | Loss: 0.00114088
Iteration 16/25 | Loss: 0.00114088
Iteration 17/25 | Loss: 0.00114088
Iteration 18/25 | Loss: 0.00114088
Iteration 19/25 | Loss: 0.00114088
Iteration 20/25 | Loss: 0.00114088
Iteration 21/25 | Loss: 0.00114088
Iteration 22/25 | Loss: 0.00114088
Iteration 23/25 | Loss: 0.00114088
Iteration 24/25 | Loss: 0.00114088
Iteration 25/25 | Loss: 0.00114087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.77750206
Iteration 2/25 | Loss: 0.00273686
Iteration 3/25 | Loss: 0.00273681
Iteration 4/25 | Loss: 0.00273681
Iteration 5/25 | Loss: 0.00273681
Iteration 6/25 | Loss: 0.00273680
Iteration 7/25 | Loss: 0.00273680
Iteration 8/25 | Loss: 0.00273680
Iteration 9/25 | Loss: 0.00273680
Iteration 10/25 | Loss: 0.00273680
Iteration 11/25 | Loss: 0.00273680
Iteration 12/25 | Loss: 0.00273680
Iteration 13/25 | Loss: 0.00273680
Iteration 14/25 | Loss: 0.00273680
Iteration 15/25 | Loss: 0.00273680
Iteration 16/25 | Loss: 0.00273680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002736803377047181, 0.002736803377047181, 0.002736803377047181, 0.002736803377047181, 0.002736803377047181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002736803377047181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273680
Iteration 2/1000 | Loss: 0.00005973
Iteration 3/1000 | Loss: 0.00003116
Iteration 4/1000 | Loss: 0.00002451
Iteration 5/1000 | Loss: 0.00002206
Iteration 6/1000 | Loss: 0.00004739
Iteration 7/1000 | Loss: 0.00002035
Iteration 8/1000 | Loss: 0.00001959
Iteration 9/1000 | Loss: 0.00001916
Iteration 10/1000 | Loss: 0.00001884
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001754
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001715
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001712
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00001705
Iteration 36/1000 | Loss: 0.00001703
Iteration 37/1000 | Loss: 0.00001703
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001701
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001700
Iteration 56/1000 | Loss: 0.00001700
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001696
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001695
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001695
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001694
Iteration 84/1000 | Loss: 0.00001694
Iteration 85/1000 | Loss: 0.00001694
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001692
Iteration 96/1000 | Loss: 0.00001692
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001691
Iteration 103/1000 | Loss: 0.00001691
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001682
Iteration 154/1000 | Loss: 0.00001682
Iteration 155/1000 | Loss: 0.00001682
Iteration 156/1000 | Loss: 0.00001682
Iteration 157/1000 | Loss: 0.00001682
Iteration 158/1000 | Loss: 0.00001681
Iteration 159/1000 | Loss: 0.00001681
Iteration 160/1000 | Loss: 0.00001681
Iteration 161/1000 | Loss: 0.00001681
Iteration 162/1000 | Loss: 0.00001681
Iteration 163/1000 | Loss: 0.00001681
Iteration 164/1000 | Loss: 0.00001681
Iteration 165/1000 | Loss: 0.00001681
Iteration 166/1000 | Loss: 0.00001681
Iteration 167/1000 | Loss: 0.00001681
Iteration 168/1000 | Loss: 0.00001681
Iteration 169/1000 | Loss: 0.00001681
Iteration 170/1000 | Loss: 0.00001681
Iteration 171/1000 | Loss: 0.00001681
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001681
Iteration 178/1000 | Loss: 0.00001681
Iteration 179/1000 | Loss: 0.00001681
Iteration 180/1000 | Loss: 0.00001681
Iteration 181/1000 | Loss: 0.00001681
Iteration 182/1000 | Loss: 0.00001681
Iteration 183/1000 | Loss: 0.00001681
Iteration 184/1000 | Loss: 0.00001681
Iteration 185/1000 | Loss: 0.00001681
Iteration 186/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.6809328371891752e-05, 1.6809328371891752e-05, 1.6809328371891752e-05, 1.6809328371891752e-05, 1.6809328371891752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6809328371891752e-05

Optimization complete. Final v2v error: 3.3470101356506348 mm

Highest mean error: 13.002073287963867 mm for frame 158

Lowest mean error: 2.667863607406616 mm for frame 230

Saving results

Total time: 68.62220573425293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015101
Iteration 2/25 | Loss: 0.00176975
Iteration 3/25 | Loss: 0.00132986
Iteration 4/25 | Loss: 0.00130142
Iteration 5/25 | Loss: 0.00129343
Iteration 6/25 | Loss: 0.00129137
Iteration 7/25 | Loss: 0.00129115
Iteration 8/25 | Loss: 0.00129115
Iteration 9/25 | Loss: 0.00129115
Iteration 10/25 | Loss: 0.00129115
Iteration 11/25 | Loss: 0.00129115
Iteration 12/25 | Loss: 0.00129115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001291152322664857, 0.001291152322664857, 0.001291152322664857, 0.001291152322664857, 0.001291152322664857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001291152322664857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50837082
Iteration 2/25 | Loss: 0.00162907
Iteration 3/25 | Loss: 0.00162907
Iteration 4/25 | Loss: 0.00162907
Iteration 5/25 | Loss: 0.00162907
Iteration 6/25 | Loss: 0.00162907
Iteration 7/25 | Loss: 0.00162907
Iteration 8/25 | Loss: 0.00162907
Iteration 9/25 | Loss: 0.00162907
Iteration 10/25 | Loss: 0.00162907
Iteration 11/25 | Loss: 0.00162907
Iteration 12/25 | Loss: 0.00162907
Iteration 13/25 | Loss: 0.00162907
Iteration 14/25 | Loss: 0.00162907
Iteration 15/25 | Loss: 0.00162907
Iteration 16/25 | Loss: 0.00162907
Iteration 17/25 | Loss: 0.00162907
Iteration 18/25 | Loss: 0.00162907
Iteration 19/25 | Loss: 0.00162907
Iteration 20/25 | Loss: 0.00162907
Iteration 21/25 | Loss: 0.00162907
Iteration 22/25 | Loss: 0.00162907
Iteration 23/25 | Loss: 0.00162907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001629066769964993, 0.001629066769964993, 0.001629066769964993, 0.001629066769964993, 0.001629066769964993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001629066769964993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162907
Iteration 2/1000 | Loss: 0.00009708
Iteration 3/1000 | Loss: 0.00005844
Iteration 4/1000 | Loss: 0.00004642
Iteration 5/1000 | Loss: 0.00004075
Iteration 6/1000 | Loss: 0.00003826
Iteration 7/1000 | Loss: 0.00003684
Iteration 8/1000 | Loss: 0.00003577
Iteration 9/1000 | Loss: 0.00003511
Iteration 10/1000 | Loss: 0.00003471
Iteration 11/1000 | Loss: 0.00003428
Iteration 12/1000 | Loss: 0.00003403
Iteration 13/1000 | Loss: 0.00003378
Iteration 14/1000 | Loss: 0.00003357
Iteration 15/1000 | Loss: 0.00003342
Iteration 16/1000 | Loss: 0.00003326
Iteration 17/1000 | Loss: 0.00003317
Iteration 18/1000 | Loss: 0.00003303
Iteration 19/1000 | Loss: 0.00003292
Iteration 20/1000 | Loss: 0.00003292
Iteration 21/1000 | Loss: 0.00003290
Iteration 22/1000 | Loss: 0.00003290
Iteration 23/1000 | Loss: 0.00003288
Iteration 24/1000 | Loss: 0.00003287
Iteration 25/1000 | Loss: 0.00003287
Iteration 26/1000 | Loss: 0.00003287
Iteration 27/1000 | Loss: 0.00003287
Iteration 28/1000 | Loss: 0.00003287
Iteration 29/1000 | Loss: 0.00003287
Iteration 30/1000 | Loss: 0.00003287
Iteration 31/1000 | Loss: 0.00003286
Iteration 32/1000 | Loss: 0.00003286
Iteration 33/1000 | Loss: 0.00003285
Iteration 34/1000 | Loss: 0.00003284
Iteration 35/1000 | Loss: 0.00003284
Iteration 36/1000 | Loss: 0.00003281
Iteration 37/1000 | Loss: 0.00003278
Iteration 38/1000 | Loss: 0.00003278
Iteration 39/1000 | Loss: 0.00003275
Iteration 40/1000 | Loss: 0.00003275
Iteration 41/1000 | Loss: 0.00003273
Iteration 42/1000 | Loss: 0.00003273
Iteration 43/1000 | Loss: 0.00003273
Iteration 44/1000 | Loss: 0.00003272
Iteration 45/1000 | Loss: 0.00003269
Iteration 46/1000 | Loss: 0.00003269
Iteration 47/1000 | Loss: 0.00003269
Iteration 48/1000 | Loss: 0.00003268
Iteration 49/1000 | Loss: 0.00003268
Iteration 50/1000 | Loss: 0.00003268
Iteration 51/1000 | Loss: 0.00003268
Iteration 52/1000 | Loss: 0.00003268
Iteration 53/1000 | Loss: 0.00003268
Iteration 54/1000 | Loss: 0.00003268
Iteration 55/1000 | Loss: 0.00003268
Iteration 56/1000 | Loss: 0.00003268
Iteration 57/1000 | Loss: 0.00003268
Iteration 58/1000 | Loss: 0.00003268
Iteration 59/1000 | Loss: 0.00003267
Iteration 60/1000 | Loss: 0.00003267
Iteration 61/1000 | Loss: 0.00003267
Iteration 62/1000 | Loss: 0.00003267
Iteration 63/1000 | Loss: 0.00003267
Iteration 64/1000 | Loss: 0.00003267
Iteration 65/1000 | Loss: 0.00003267
Iteration 66/1000 | Loss: 0.00003267
Iteration 67/1000 | Loss: 0.00003267
Iteration 68/1000 | Loss: 0.00003265
Iteration 69/1000 | Loss: 0.00003265
Iteration 70/1000 | Loss: 0.00003265
Iteration 71/1000 | Loss: 0.00003265
Iteration 72/1000 | Loss: 0.00003265
Iteration 73/1000 | Loss: 0.00003265
Iteration 74/1000 | Loss: 0.00003265
Iteration 75/1000 | Loss: 0.00003265
Iteration 76/1000 | Loss: 0.00003264
Iteration 77/1000 | Loss: 0.00003264
Iteration 78/1000 | Loss: 0.00003263
Iteration 79/1000 | Loss: 0.00003263
Iteration 80/1000 | Loss: 0.00003262
Iteration 81/1000 | Loss: 0.00003262
Iteration 82/1000 | Loss: 0.00003262
Iteration 83/1000 | Loss: 0.00003260
Iteration 84/1000 | Loss: 0.00003260
Iteration 85/1000 | Loss: 0.00003260
Iteration 86/1000 | Loss: 0.00003260
Iteration 87/1000 | Loss: 0.00003260
Iteration 88/1000 | Loss: 0.00003260
Iteration 89/1000 | Loss: 0.00003260
Iteration 90/1000 | Loss: 0.00003260
Iteration 91/1000 | Loss: 0.00003260
Iteration 92/1000 | Loss: 0.00003259
Iteration 93/1000 | Loss: 0.00003259
Iteration 94/1000 | Loss: 0.00003258
Iteration 95/1000 | Loss: 0.00003258
Iteration 96/1000 | Loss: 0.00003258
Iteration 97/1000 | Loss: 0.00003258
Iteration 98/1000 | Loss: 0.00003258
Iteration 99/1000 | Loss: 0.00003257
Iteration 100/1000 | Loss: 0.00003257
Iteration 101/1000 | Loss: 0.00003257
Iteration 102/1000 | Loss: 0.00003257
Iteration 103/1000 | Loss: 0.00003257
Iteration 104/1000 | Loss: 0.00003257
Iteration 105/1000 | Loss: 0.00003257
Iteration 106/1000 | Loss: 0.00003257
Iteration 107/1000 | Loss: 0.00003257
Iteration 108/1000 | Loss: 0.00003256
Iteration 109/1000 | Loss: 0.00003256
Iteration 110/1000 | Loss: 0.00003256
Iteration 111/1000 | Loss: 0.00003255
Iteration 112/1000 | Loss: 0.00003255
Iteration 113/1000 | Loss: 0.00003255
Iteration 114/1000 | Loss: 0.00003255
Iteration 115/1000 | Loss: 0.00003255
Iteration 116/1000 | Loss: 0.00003255
Iteration 117/1000 | Loss: 0.00003255
Iteration 118/1000 | Loss: 0.00003255
Iteration 119/1000 | Loss: 0.00003255
Iteration 120/1000 | Loss: 0.00003254
Iteration 121/1000 | Loss: 0.00003254
Iteration 122/1000 | Loss: 0.00003254
Iteration 123/1000 | Loss: 0.00003253
Iteration 124/1000 | Loss: 0.00003253
Iteration 125/1000 | Loss: 0.00003253
Iteration 126/1000 | Loss: 0.00003253
Iteration 127/1000 | Loss: 0.00003253
Iteration 128/1000 | Loss: 0.00003253
Iteration 129/1000 | Loss: 0.00003253
Iteration 130/1000 | Loss: 0.00003252
Iteration 131/1000 | Loss: 0.00003252
Iteration 132/1000 | Loss: 0.00003252
Iteration 133/1000 | Loss: 0.00003252
Iteration 134/1000 | Loss: 0.00003252
Iteration 135/1000 | Loss: 0.00003251
Iteration 136/1000 | Loss: 0.00003251
Iteration 137/1000 | Loss: 0.00003251
Iteration 138/1000 | Loss: 0.00003251
Iteration 139/1000 | Loss: 0.00003250
Iteration 140/1000 | Loss: 0.00003250
Iteration 141/1000 | Loss: 0.00003250
Iteration 142/1000 | Loss: 0.00003250
Iteration 143/1000 | Loss: 0.00003250
Iteration 144/1000 | Loss: 0.00003249
Iteration 145/1000 | Loss: 0.00003249
Iteration 146/1000 | Loss: 0.00003249
Iteration 147/1000 | Loss: 0.00003249
Iteration 148/1000 | Loss: 0.00003249
Iteration 149/1000 | Loss: 0.00003248
Iteration 150/1000 | Loss: 0.00003248
Iteration 151/1000 | Loss: 0.00003248
Iteration 152/1000 | Loss: 0.00003248
Iteration 153/1000 | Loss: 0.00003248
Iteration 154/1000 | Loss: 0.00003248
Iteration 155/1000 | Loss: 0.00003248
Iteration 156/1000 | Loss: 0.00003248
Iteration 157/1000 | Loss: 0.00003248
Iteration 158/1000 | Loss: 0.00003248
Iteration 159/1000 | Loss: 0.00003247
Iteration 160/1000 | Loss: 0.00003247
Iteration 161/1000 | Loss: 0.00003247
Iteration 162/1000 | Loss: 0.00003247
Iteration 163/1000 | Loss: 0.00003247
Iteration 164/1000 | Loss: 0.00003247
Iteration 165/1000 | Loss: 0.00003247
Iteration 166/1000 | Loss: 0.00003247
Iteration 167/1000 | Loss: 0.00003247
Iteration 168/1000 | Loss: 0.00003247
Iteration 169/1000 | Loss: 0.00003247
Iteration 170/1000 | Loss: 0.00003247
Iteration 171/1000 | Loss: 0.00003247
Iteration 172/1000 | Loss: 0.00003246
Iteration 173/1000 | Loss: 0.00003246
Iteration 174/1000 | Loss: 0.00003246
Iteration 175/1000 | Loss: 0.00003246
Iteration 176/1000 | Loss: 0.00003246
Iteration 177/1000 | Loss: 0.00003246
Iteration 178/1000 | Loss: 0.00003246
Iteration 179/1000 | Loss: 0.00003246
Iteration 180/1000 | Loss: 0.00003246
Iteration 181/1000 | Loss: 0.00003246
Iteration 182/1000 | Loss: 0.00003245
Iteration 183/1000 | Loss: 0.00003245
Iteration 184/1000 | Loss: 0.00003245
Iteration 185/1000 | Loss: 0.00003245
Iteration 186/1000 | Loss: 0.00003245
Iteration 187/1000 | Loss: 0.00003245
Iteration 188/1000 | Loss: 0.00003245
Iteration 189/1000 | Loss: 0.00003245
Iteration 190/1000 | Loss: 0.00003245
Iteration 191/1000 | Loss: 0.00003245
Iteration 192/1000 | Loss: 0.00003244
Iteration 193/1000 | Loss: 0.00003244
Iteration 194/1000 | Loss: 0.00003244
Iteration 195/1000 | Loss: 0.00003244
Iteration 196/1000 | Loss: 0.00003244
Iteration 197/1000 | Loss: 0.00003244
Iteration 198/1000 | Loss: 0.00003244
Iteration 199/1000 | Loss: 0.00003244
Iteration 200/1000 | Loss: 0.00003244
Iteration 201/1000 | Loss: 0.00003244
Iteration 202/1000 | Loss: 0.00003244
Iteration 203/1000 | Loss: 0.00003243
Iteration 204/1000 | Loss: 0.00003243
Iteration 205/1000 | Loss: 0.00003243
Iteration 206/1000 | Loss: 0.00003243
Iteration 207/1000 | Loss: 0.00003243
Iteration 208/1000 | Loss: 0.00003243
Iteration 209/1000 | Loss: 0.00003243
Iteration 210/1000 | Loss: 0.00003243
Iteration 211/1000 | Loss: 0.00003243
Iteration 212/1000 | Loss: 0.00003243
Iteration 213/1000 | Loss: 0.00003243
Iteration 214/1000 | Loss: 0.00003243
Iteration 215/1000 | Loss: 0.00003242
Iteration 216/1000 | Loss: 0.00003242
Iteration 217/1000 | Loss: 0.00003242
Iteration 218/1000 | Loss: 0.00003242
Iteration 219/1000 | Loss: 0.00003242
Iteration 220/1000 | Loss: 0.00003242
Iteration 221/1000 | Loss: 0.00003242
Iteration 222/1000 | Loss: 0.00003242
Iteration 223/1000 | Loss: 0.00003242
Iteration 224/1000 | Loss: 0.00003242
Iteration 225/1000 | Loss: 0.00003242
Iteration 226/1000 | Loss: 0.00003242
Iteration 227/1000 | Loss: 0.00003242
Iteration 228/1000 | Loss: 0.00003242
Iteration 229/1000 | Loss: 0.00003242
Iteration 230/1000 | Loss: 0.00003242
Iteration 231/1000 | Loss: 0.00003242
Iteration 232/1000 | Loss: 0.00003242
Iteration 233/1000 | Loss: 0.00003242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [3.2423344237031415e-05, 3.2423344237031415e-05, 3.2423344237031415e-05, 3.2423344237031415e-05, 3.2423344237031415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2423344237031415e-05

Optimization complete. Final v2v error: 4.474437713623047 mm

Highest mean error: 5.534707069396973 mm for frame 40

Lowest mean error: 3.875091552734375 mm for frame 97

Saving results

Total time: 54.0668830871582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817853
Iteration 2/25 | Loss: 0.00148886
Iteration 3/25 | Loss: 0.00120986
Iteration 4/25 | Loss: 0.00119448
Iteration 5/25 | Loss: 0.00119252
Iteration 6/25 | Loss: 0.00119252
Iteration 7/25 | Loss: 0.00119252
Iteration 8/25 | Loss: 0.00119252
Iteration 9/25 | Loss: 0.00119252
Iteration 10/25 | Loss: 0.00119252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011925174621865153, 0.0011925174621865153, 0.0011925174621865153, 0.0011925174621865153, 0.0011925174621865153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011925174621865153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19153774
Iteration 2/25 | Loss: 0.00215276
Iteration 3/25 | Loss: 0.00215275
Iteration 4/25 | Loss: 0.00215275
Iteration 5/25 | Loss: 0.00215275
Iteration 6/25 | Loss: 0.00215275
Iteration 7/25 | Loss: 0.00215275
Iteration 8/25 | Loss: 0.00215275
Iteration 9/25 | Loss: 0.00215275
Iteration 10/25 | Loss: 0.00215275
Iteration 11/25 | Loss: 0.00215275
Iteration 12/25 | Loss: 0.00215275
Iteration 13/25 | Loss: 0.00215275
Iteration 14/25 | Loss: 0.00215275
Iteration 15/25 | Loss: 0.00215275
Iteration 16/25 | Loss: 0.00215275
Iteration 17/25 | Loss: 0.00215275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002152747940272093, 0.002152747940272093, 0.002152747940272093, 0.002152747940272093, 0.002152747940272093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002152747940272093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215275
Iteration 2/1000 | Loss: 0.00007986
Iteration 3/1000 | Loss: 0.00004022
Iteration 4/1000 | Loss: 0.00003482
Iteration 5/1000 | Loss: 0.00003255
Iteration 6/1000 | Loss: 0.00003122
Iteration 7/1000 | Loss: 0.00003045
Iteration 8/1000 | Loss: 0.00002988
Iteration 9/1000 | Loss: 0.00002906
Iteration 10/1000 | Loss: 0.00002861
Iteration 11/1000 | Loss: 0.00002813
Iteration 12/1000 | Loss: 0.00002789
Iteration 13/1000 | Loss: 0.00002773
Iteration 14/1000 | Loss: 0.00002766
Iteration 15/1000 | Loss: 0.00002757
Iteration 16/1000 | Loss: 0.00002751
Iteration 17/1000 | Loss: 0.00002750
Iteration 18/1000 | Loss: 0.00002750
Iteration 19/1000 | Loss: 0.00002745
Iteration 20/1000 | Loss: 0.00002745
Iteration 21/1000 | Loss: 0.00002744
Iteration 22/1000 | Loss: 0.00002743
Iteration 23/1000 | Loss: 0.00002742
Iteration 24/1000 | Loss: 0.00002742
Iteration 25/1000 | Loss: 0.00002742
Iteration 26/1000 | Loss: 0.00002742
Iteration 27/1000 | Loss: 0.00002742
Iteration 28/1000 | Loss: 0.00002741
Iteration 29/1000 | Loss: 0.00002741
Iteration 30/1000 | Loss: 0.00002741
Iteration 31/1000 | Loss: 0.00002741
Iteration 32/1000 | Loss: 0.00002741
Iteration 33/1000 | Loss: 0.00002741
Iteration 34/1000 | Loss: 0.00002741
Iteration 35/1000 | Loss: 0.00002740
Iteration 36/1000 | Loss: 0.00002740
Iteration 37/1000 | Loss: 0.00002739
Iteration 38/1000 | Loss: 0.00002739
Iteration 39/1000 | Loss: 0.00002739
Iteration 40/1000 | Loss: 0.00002739
Iteration 41/1000 | Loss: 0.00002738
Iteration 42/1000 | Loss: 0.00002738
Iteration 43/1000 | Loss: 0.00002738
Iteration 44/1000 | Loss: 0.00002737
Iteration 45/1000 | Loss: 0.00002737
Iteration 46/1000 | Loss: 0.00002737
Iteration 47/1000 | Loss: 0.00002736
Iteration 48/1000 | Loss: 0.00002736
Iteration 49/1000 | Loss: 0.00002736
Iteration 50/1000 | Loss: 0.00002736
Iteration 51/1000 | Loss: 0.00002736
Iteration 52/1000 | Loss: 0.00002735
Iteration 53/1000 | Loss: 0.00002735
Iteration 54/1000 | Loss: 0.00002735
Iteration 55/1000 | Loss: 0.00002734
Iteration 56/1000 | Loss: 0.00002734
Iteration 57/1000 | Loss: 0.00002734
Iteration 58/1000 | Loss: 0.00002733
Iteration 59/1000 | Loss: 0.00002733
Iteration 60/1000 | Loss: 0.00002733
Iteration 61/1000 | Loss: 0.00002733
Iteration 62/1000 | Loss: 0.00002733
Iteration 63/1000 | Loss: 0.00002733
Iteration 64/1000 | Loss: 0.00002733
Iteration 65/1000 | Loss: 0.00002732
Iteration 66/1000 | Loss: 0.00002732
Iteration 67/1000 | Loss: 0.00002732
Iteration 68/1000 | Loss: 0.00002732
Iteration 69/1000 | Loss: 0.00002731
Iteration 70/1000 | Loss: 0.00002731
Iteration 71/1000 | Loss: 0.00002731
Iteration 72/1000 | Loss: 0.00002730
Iteration 73/1000 | Loss: 0.00002730
Iteration 74/1000 | Loss: 0.00002730
Iteration 75/1000 | Loss: 0.00002730
Iteration 76/1000 | Loss: 0.00002730
Iteration 77/1000 | Loss: 0.00002730
Iteration 78/1000 | Loss: 0.00002729
Iteration 79/1000 | Loss: 0.00002729
Iteration 80/1000 | Loss: 0.00002729
Iteration 81/1000 | Loss: 0.00002729
Iteration 82/1000 | Loss: 0.00002728
Iteration 83/1000 | Loss: 0.00002728
Iteration 84/1000 | Loss: 0.00002727
Iteration 85/1000 | Loss: 0.00002727
Iteration 86/1000 | Loss: 0.00002727
Iteration 87/1000 | Loss: 0.00002727
Iteration 88/1000 | Loss: 0.00002726
Iteration 89/1000 | Loss: 0.00002726
Iteration 90/1000 | Loss: 0.00002726
Iteration 91/1000 | Loss: 0.00002726
Iteration 92/1000 | Loss: 0.00002726
Iteration 93/1000 | Loss: 0.00002726
Iteration 94/1000 | Loss: 0.00002726
Iteration 95/1000 | Loss: 0.00002726
Iteration 96/1000 | Loss: 0.00002726
Iteration 97/1000 | Loss: 0.00002726
Iteration 98/1000 | Loss: 0.00002726
Iteration 99/1000 | Loss: 0.00002726
Iteration 100/1000 | Loss: 0.00002726
Iteration 101/1000 | Loss: 0.00002726
Iteration 102/1000 | Loss: 0.00002726
Iteration 103/1000 | Loss: 0.00002726
Iteration 104/1000 | Loss: 0.00002726
Iteration 105/1000 | Loss: 0.00002726
Iteration 106/1000 | Loss: 0.00002726
Iteration 107/1000 | Loss: 0.00002726
Iteration 108/1000 | Loss: 0.00002726
Iteration 109/1000 | Loss: 0.00002726
Iteration 110/1000 | Loss: 0.00002726
Iteration 111/1000 | Loss: 0.00002726
Iteration 112/1000 | Loss: 0.00002726
Iteration 113/1000 | Loss: 0.00002726
Iteration 114/1000 | Loss: 0.00002726
Iteration 115/1000 | Loss: 0.00002726
Iteration 116/1000 | Loss: 0.00002726
Iteration 117/1000 | Loss: 0.00002726
Iteration 118/1000 | Loss: 0.00002726
Iteration 119/1000 | Loss: 0.00002726
Iteration 120/1000 | Loss: 0.00002726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.725724334595725e-05, 2.725724334595725e-05, 2.725724334595725e-05, 2.725724334595725e-05, 2.725724334595725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.725724334595725e-05

Optimization complete. Final v2v error: 4.508466720581055 mm

Highest mean error: 4.847917079925537 mm for frame 77

Lowest mean error: 3.950403928756714 mm for frame 239

Saving results

Total time: 40.75741195678711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064337
Iteration 2/25 | Loss: 0.01064337
Iteration 3/25 | Loss: 0.01064337
Iteration 4/25 | Loss: 0.01064337
Iteration 5/25 | Loss: 0.01064337
Iteration 6/25 | Loss: 0.01064337
Iteration 7/25 | Loss: 0.01064337
Iteration 8/25 | Loss: 0.01064336
Iteration 9/25 | Loss: 0.01064336
Iteration 10/25 | Loss: 0.01064336
Iteration 11/25 | Loss: 0.01064336
Iteration 12/25 | Loss: 0.01064336
Iteration 13/25 | Loss: 0.01064336
Iteration 14/25 | Loss: 0.01064336
Iteration 15/25 | Loss: 0.01064336
Iteration 16/25 | Loss: 0.01064335
Iteration 17/25 | Loss: 0.01064335
Iteration 18/25 | Loss: 0.01064335
Iteration 19/25 | Loss: 0.01064335
Iteration 20/25 | Loss: 0.01064335
Iteration 21/25 | Loss: 0.01064335
Iteration 22/25 | Loss: 0.01064335
Iteration 23/25 | Loss: 0.01064335
Iteration 24/25 | Loss: 0.01064335
Iteration 25/25 | Loss: 0.01064335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58658957
Iteration 2/25 | Loss: 0.14647359
Iteration 3/25 | Loss: 0.08970596
Iteration 4/25 | Loss: 0.08968614
Iteration 5/25 | Loss: 0.08968612
Iteration 6/25 | Loss: 0.08968612
Iteration 7/25 | Loss: 0.08968612
Iteration 8/25 | Loss: 0.08968612
Iteration 9/25 | Loss: 0.08968612
Iteration 10/25 | Loss: 0.08968612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.08968611806631088, 0.08968611806631088, 0.08968611806631088, 0.08968611806631088, 0.08968611806631088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08968611806631088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08968612
Iteration 2/1000 | Loss: 0.00686592
Iteration 3/1000 | Loss: 0.00304722
Iteration 4/1000 | Loss: 0.00135226
Iteration 5/1000 | Loss: 0.00081529
Iteration 6/1000 | Loss: 0.00144026
Iteration 7/1000 | Loss: 0.00097880
Iteration 8/1000 | Loss: 0.00118638
Iteration 9/1000 | Loss: 0.00036185
Iteration 10/1000 | Loss: 0.00061736
Iteration 11/1000 | Loss: 0.00106710
Iteration 12/1000 | Loss: 0.00063123
Iteration 13/1000 | Loss: 0.00097079
Iteration 14/1000 | Loss: 0.00032900
Iteration 15/1000 | Loss: 0.00015068
Iteration 16/1000 | Loss: 0.00024953
Iteration 17/1000 | Loss: 0.00045656
Iteration 18/1000 | Loss: 0.00132356
Iteration 19/1000 | Loss: 0.00592393
Iteration 20/1000 | Loss: 0.00558564
Iteration 21/1000 | Loss: 0.00304438
Iteration 22/1000 | Loss: 0.00163627
Iteration 23/1000 | Loss: 0.00021423
Iteration 24/1000 | Loss: 0.00013045
Iteration 25/1000 | Loss: 0.00006693
Iteration 26/1000 | Loss: 0.00005905
Iteration 27/1000 | Loss: 0.00041244
Iteration 28/1000 | Loss: 0.00082397
Iteration 29/1000 | Loss: 0.00009683
Iteration 30/1000 | Loss: 0.00008311
Iteration 31/1000 | Loss: 0.00006161
Iteration 32/1000 | Loss: 0.00004808
Iteration 33/1000 | Loss: 0.00037236
Iteration 34/1000 | Loss: 0.00033013
Iteration 35/1000 | Loss: 0.00005330
Iteration 36/1000 | Loss: 0.00019548
Iteration 37/1000 | Loss: 0.00013968
Iteration 38/1000 | Loss: 0.00004490
Iteration 39/1000 | Loss: 0.00021302
Iteration 40/1000 | Loss: 0.00007424
Iteration 41/1000 | Loss: 0.00004227
Iteration 42/1000 | Loss: 0.00003973
Iteration 43/1000 | Loss: 0.00003893
Iteration 44/1000 | Loss: 0.00031668
Iteration 45/1000 | Loss: 0.00016550
Iteration 46/1000 | Loss: 0.00015263
Iteration 47/1000 | Loss: 0.00008570
Iteration 48/1000 | Loss: 0.00010532
Iteration 49/1000 | Loss: 0.00004692
Iteration 50/1000 | Loss: 0.00006092
Iteration 51/1000 | Loss: 0.00004449
Iteration 52/1000 | Loss: 0.00022336
Iteration 53/1000 | Loss: 0.00006782
Iteration 54/1000 | Loss: 0.00004347
Iteration 55/1000 | Loss: 0.00003811
Iteration 56/1000 | Loss: 0.00006658
Iteration 57/1000 | Loss: 0.00003744
Iteration 58/1000 | Loss: 0.00003635
Iteration 59/1000 | Loss: 0.00003783
Iteration 60/1000 | Loss: 0.00010061
Iteration 61/1000 | Loss: 0.00004303
Iteration 62/1000 | Loss: 0.00006822
Iteration 63/1000 | Loss: 0.00003532
Iteration 64/1000 | Loss: 0.00005681
Iteration 65/1000 | Loss: 0.00004293
Iteration 66/1000 | Loss: 0.00003390
Iteration 67/1000 | Loss: 0.00005861
Iteration 68/1000 | Loss: 0.00005449
Iteration 69/1000 | Loss: 0.00003335
Iteration 70/1000 | Loss: 0.00003333
Iteration 71/1000 | Loss: 0.00006863
Iteration 72/1000 | Loss: 0.00003583
Iteration 73/1000 | Loss: 0.00003721
Iteration 74/1000 | Loss: 0.00003289
Iteration 75/1000 | Loss: 0.00007400
Iteration 76/1000 | Loss: 0.00003750
Iteration 77/1000 | Loss: 0.00003342
Iteration 78/1000 | Loss: 0.00003243
Iteration 79/1000 | Loss: 0.00003224
Iteration 80/1000 | Loss: 0.00003209
Iteration 81/1000 | Loss: 0.00003208
Iteration 82/1000 | Loss: 0.00003192
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00013925
Iteration 85/1000 | Loss: 0.00003205
Iteration 86/1000 | Loss: 0.00003163
Iteration 87/1000 | Loss: 0.00003154
Iteration 88/1000 | Loss: 0.00003154
Iteration 89/1000 | Loss: 0.00003153
Iteration 90/1000 | Loss: 0.00003152
Iteration 91/1000 | Loss: 0.00003149
Iteration 92/1000 | Loss: 0.00003149
Iteration 93/1000 | Loss: 0.00003149
Iteration 94/1000 | Loss: 0.00003149
Iteration 95/1000 | Loss: 0.00003149
Iteration 96/1000 | Loss: 0.00003149
Iteration 97/1000 | Loss: 0.00003148
Iteration 98/1000 | Loss: 0.00003148
Iteration 99/1000 | Loss: 0.00003148
Iteration 100/1000 | Loss: 0.00003147
Iteration 101/1000 | Loss: 0.00003147
Iteration 102/1000 | Loss: 0.00003146
Iteration 103/1000 | Loss: 0.00003146
Iteration 104/1000 | Loss: 0.00003146
Iteration 105/1000 | Loss: 0.00003146
Iteration 106/1000 | Loss: 0.00003146
Iteration 107/1000 | Loss: 0.00003145
Iteration 108/1000 | Loss: 0.00003145
Iteration 109/1000 | Loss: 0.00003145
Iteration 110/1000 | Loss: 0.00003145
Iteration 111/1000 | Loss: 0.00003145
Iteration 112/1000 | Loss: 0.00003145
Iteration 113/1000 | Loss: 0.00003145
Iteration 114/1000 | Loss: 0.00003145
Iteration 115/1000 | Loss: 0.00003145
Iteration 116/1000 | Loss: 0.00003145
Iteration 117/1000 | Loss: 0.00003145
Iteration 118/1000 | Loss: 0.00003144
Iteration 119/1000 | Loss: 0.00003144
Iteration 120/1000 | Loss: 0.00003143
Iteration 121/1000 | Loss: 0.00003142
Iteration 122/1000 | Loss: 0.00003141
Iteration 123/1000 | Loss: 0.00003141
Iteration 124/1000 | Loss: 0.00003140
Iteration 125/1000 | Loss: 0.00003140
Iteration 126/1000 | Loss: 0.00003140
Iteration 127/1000 | Loss: 0.00003138
Iteration 128/1000 | Loss: 0.00003137
Iteration 129/1000 | Loss: 0.00003133
Iteration 130/1000 | Loss: 0.00003132
Iteration 131/1000 | Loss: 0.00003131
Iteration 132/1000 | Loss: 0.00003131
Iteration 133/1000 | Loss: 0.00003131
Iteration 134/1000 | Loss: 0.00003131
Iteration 135/1000 | Loss: 0.00003130
Iteration 136/1000 | Loss: 0.00003130
Iteration 137/1000 | Loss: 0.00003130
Iteration 138/1000 | Loss: 0.00003130
Iteration 139/1000 | Loss: 0.00003130
Iteration 140/1000 | Loss: 0.00003130
Iteration 141/1000 | Loss: 0.00003129
Iteration 142/1000 | Loss: 0.00003129
Iteration 143/1000 | Loss: 0.00003129
Iteration 144/1000 | Loss: 0.00003128
Iteration 145/1000 | Loss: 0.00003128
Iteration 146/1000 | Loss: 0.00003128
Iteration 147/1000 | Loss: 0.00003127
Iteration 148/1000 | Loss: 0.00003127
Iteration 149/1000 | Loss: 0.00003127
Iteration 150/1000 | Loss: 0.00003127
Iteration 151/1000 | Loss: 0.00003127
Iteration 152/1000 | Loss: 0.00003127
Iteration 153/1000 | Loss: 0.00003127
Iteration 154/1000 | Loss: 0.00003126
Iteration 155/1000 | Loss: 0.00003126
Iteration 156/1000 | Loss: 0.00003126
Iteration 157/1000 | Loss: 0.00003126
Iteration 158/1000 | Loss: 0.00003125
Iteration 159/1000 | Loss: 0.00003125
Iteration 160/1000 | Loss: 0.00003124
Iteration 161/1000 | Loss: 0.00003124
Iteration 162/1000 | Loss: 0.00003124
Iteration 163/1000 | Loss: 0.00003124
Iteration 164/1000 | Loss: 0.00003124
Iteration 165/1000 | Loss: 0.00003124
Iteration 166/1000 | Loss: 0.00003124
Iteration 167/1000 | Loss: 0.00003124
Iteration 168/1000 | Loss: 0.00003124
Iteration 169/1000 | Loss: 0.00003124
Iteration 170/1000 | Loss: 0.00003124
Iteration 171/1000 | Loss: 0.00003124
Iteration 172/1000 | Loss: 0.00003123
Iteration 173/1000 | Loss: 0.00003123
Iteration 174/1000 | Loss: 0.00003122
Iteration 175/1000 | Loss: 0.00003122
Iteration 176/1000 | Loss: 0.00003122
Iteration 177/1000 | Loss: 0.00003122
Iteration 178/1000 | Loss: 0.00003121
Iteration 179/1000 | Loss: 0.00003121
Iteration 180/1000 | Loss: 0.00003121
Iteration 181/1000 | Loss: 0.00003121
Iteration 182/1000 | Loss: 0.00003121
Iteration 183/1000 | Loss: 0.00003120
Iteration 184/1000 | Loss: 0.00003120
Iteration 185/1000 | Loss: 0.00003120
Iteration 186/1000 | Loss: 0.00003120
Iteration 187/1000 | Loss: 0.00003119
Iteration 188/1000 | Loss: 0.00003119
Iteration 189/1000 | Loss: 0.00003119
Iteration 190/1000 | Loss: 0.00003119
Iteration 191/1000 | Loss: 0.00003119
Iteration 192/1000 | Loss: 0.00003119
Iteration 193/1000 | Loss: 0.00003118
Iteration 194/1000 | Loss: 0.00003118
Iteration 195/1000 | Loss: 0.00003118
Iteration 196/1000 | Loss: 0.00003118
Iteration 197/1000 | Loss: 0.00003118
Iteration 198/1000 | Loss: 0.00003118
Iteration 199/1000 | Loss: 0.00003117
Iteration 200/1000 | Loss: 0.00003117
Iteration 201/1000 | Loss: 0.00003117
Iteration 202/1000 | Loss: 0.00003117
Iteration 203/1000 | Loss: 0.00003117
Iteration 204/1000 | Loss: 0.00003117
Iteration 205/1000 | Loss: 0.00003117
Iteration 206/1000 | Loss: 0.00003117
Iteration 207/1000 | Loss: 0.00003117
Iteration 208/1000 | Loss: 0.00003117
Iteration 209/1000 | Loss: 0.00003117
Iteration 210/1000 | Loss: 0.00003117
Iteration 211/1000 | Loss: 0.00003116
Iteration 212/1000 | Loss: 0.00003116
Iteration 213/1000 | Loss: 0.00003116
Iteration 214/1000 | Loss: 0.00003116
Iteration 215/1000 | Loss: 0.00003116
Iteration 216/1000 | Loss: 0.00003116
Iteration 217/1000 | Loss: 0.00003116
Iteration 218/1000 | Loss: 0.00003116
Iteration 219/1000 | Loss: 0.00003116
Iteration 220/1000 | Loss: 0.00003115
Iteration 221/1000 | Loss: 0.00003115
Iteration 222/1000 | Loss: 0.00003115
Iteration 223/1000 | Loss: 0.00003115
Iteration 224/1000 | Loss: 0.00003115
Iteration 225/1000 | Loss: 0.00003115
Iteration 226/1000 | Loss: 0.00003115
Iteration 227/1000 | Loss: 0.00003115
Iteration 228/1000 | Loss: 0.00003115
Iteration 229/1000 | Loss: 0.00003115
Iteration 230/1000 | Loss: 0.00003115
Iteration 231/1000 | Loss: 0.00003115
Iteration 232/1000 | Loss: 0.00003115
Iteration 233/1000 | Loss: 0.00003114
Iteration 234/1000 | Loss: 0.00003114
Iteration 235/1000 | Loss: 0.00003114
Iteration 236/1000 | Loss: 0.00003114
Iteration 237/1000 | Loss: 0.00003114
Iteration 238/1000 | Loss: 0.00003114
Iteration 239/1000 | Loss: 0.00003114
Iteration 240/1000 | Loss: 0.00003114
Iteration 241/1000 | Loss: 0.00003114
Iteration 242/1000 | Loss: 0.00003114
Iteration 243/1000 | Loss: 0.00003114
Iteration 244/1000 | Loss: 0.00003114
Iteration 245/1000 | Loss: 0.00003114
Iteration 246/1000 | Loss: 0.00003114
Iteration 247/1000 | Loss: 0.00003114
Iteration 248/1000 | Loss: 0.00003114
Iteration 249/1000 | Loss: 0.00003114
Iteration 250/1000 | Loss: 0.00003114
Iteration 251/1000 | Loss: 0.00003114
Iteration 252/1000 | Loss: 0.00003114
Iteration 253/1000 | Loss: 0.00003113
Iteration 254/1000 | Loss: 0.00003113
Iteration 255/1000 | Loss: 0.00003113
Iteration 256/1000 | Loss: 0.00003113
Iteration 257/1000 | Loss: 0.00003113
Iteration 258/1000 | Loss: 0.00003113
Iteration 259/1000 | Loss: 0.00003113
Iteration 260/1000 | Loss: 0.00003113
Iteration 261/1000 | Loss: 0.00003113
Iteration 262/1000 | Loss: 0.00003113
Iteration 263/1000 | Loss: 0.00003113
Iteration 264/1000 | Loss: 0.00003113
Iteration 265/1000 | Loss: 0.00003113
Iteration 266/1000 | Loss: 0.00003113
Iteration 267/1000 | Loss: 0.00003113
Iteration 268/1000 | Loss: 0.00003113
Iteration 269/1000 | Loss: 0.00003113
Iteration 270/1000 | Loss: 0.00003113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [3.113204365945421e-05, 3.113204365945421e-05, 3.113204365945421e-05, 3.113204365945421e-05, 3.113204365945421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.113204365945421e-05

Optimization complete. Final v2v error: 4.121379375457764 mm

Highest mean error: 6.5680999755859375 mm for frame 0

Lowest mean error: 3.2927253246307373 mm for frame 90

Saving results

Total time: 159.14767360687256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808557
Iteration 2/25 | Loss: 0.00136461
Iteration 3/25 | Loss: 0.00113165
Iteration 4/25 | Loss: 0.00109473
Iteration 5/25 | Loss: 0.00108646
Iteration 6/25 | Loss: 0.00108403
Iteration 7/25 | Loss: 0.00108339
Iteration 8/25 | Loss: 0.00108338
Iteration 9/25 | Loss: 0.00108338
Iteration 10/25 | Loss: 0.00108338
Iteration 11/25 | Loss: 0.00108338
Iteration 12/25 | Loss: 0.00108338
Iteration 13/25 | Loss: 0.00108338
Iteration 14/25 | Loss: 0.00108338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001083376002497971, 0.001083376002497971, 0.001083376002497971, 0.001083376002497971, 0.001083376002497971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083376002497971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09558368
Iteration 2/25 | Loss: 0.00329599
Iteration 3/25 | Loss: 0.00329599
Iteration 4/25 | Loss: 0.00329599
Iteration 5/25 | Loss: 0.00329599
Iteration 6/25 | Loss: 0.00329599
Iteration 7/25 | Loss: 0.00329598
Iteration 8/25 | Loss: 0.00329598
Iteration 9/25 | Loss: 0.00329598
Iteration 10/25 | Loss: 0.00329598
Iteration 11/25 | Loss: 0.00329598
Iteration 12/25 | Loss: 0.00329598
Iteration 13/25 | Loss: 0.00329598
Iteration 14/25 | Loss: 0.00329598
Iteration 15/25 | Loss: 0.00329598
Iteration 16/25 | Loss: 0.00329598
Iteration 17/25 | Loss: 0.00329598
Iteration 18/25 | Loss: 0.00329598
Iteration 19/25 | Loss: 0.00329598
Iteration 20/25 | Loss: 0.00329598
Iteration 21/25 | Loss: 0.00329598
Iteration 22/25 | Loss: 0.00329598
Iteration 23/25 | Loss: 0.00329598
Iteration 24/25 | Loss: 0.00329598
Iteration 25/25 | Loss: 0.00329598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329598
Iteration 2/1000 | Loss: 0.00006166
Iteration 3/1000 | Loss: 0.00003336
Iteration 4/1000 | Loss: 0.00002324
Iteration 5/1000 | Loss: 0.00002000
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001749
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001654
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001458
Iteration 25/1000 | Loss: 0.00001457
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001436
Iteration 103/1000 | Loss: 0.00001436
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001431
Iteration 138/1000 | Loss: 0.00001431
Iteration 139/1000 | Loss: 0.00001431
Iteration 140/1000 | Loss: 0.00001431
Iteration 141/1000 | Loss: 0.00001431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4313360225060023e-05, 1.4313360225060023e-05, 1.4313360225060023e-05, 1.4313360225060023e-05, 1.4313360225060023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4313360225060023e-05

Optimization complete. Final v2v error: 3.2497849464416504 mm

Highest mean error: 3.6381990909576416 mm for frame 68

Lowest mean error: 2.7354259490966797 mm for frame 132

Saving results

Total time: 43.20984673500061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775589
Iteration 2/25 | Loss: 0.00183599
Iteration 3/25 | Loss: 0.00125007
Iteration 4/25 | Loss: 0.00113661
Iteration 5/25 | Loss: 0.00110960
Iteration 6/25 | Loss: 0.00111891
Iteration 7/25 | Loss: 0.00110915
Iteration 8/25 | Loss: 0.00109282
Iteration 9/25 | Loss: 0.00108193
Iteration 10/25 | Loss: 0.00107549
Iteration 11/25 | Loss: 0.00107363
Iteration 12/25 | Loss: 0.00107305
Iteration 13/25 | Loss: 0.00107287
Iteration 14/25 | Loss: 0.00107281
Iteration 15/25 | Loss: 0.00107281
Iteration 16/25 | Loss: 0.00107280
Iteration 17/25 | Loss: 0.00107280
Iteration 18/25 | Loss: 0.00107280
Iteration 19/25 | Loss: 0.00107280
Iteration 20/25 | Loss: 0.00107280
Iteration 21/25 | Loss: 0.00107280
Iteration 22/25 | Loss: 0.00107280
Iteration 23/25 | Loss: 0.00107280
Iteration 24/25 | Loss: 0.00107280
Iteration 25/25 | Loss: 0.00107280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010728007182478905, 0.0010728007182478905, 0.0010728007182478905, 0.0010728007182478905, 0.0010728007182478905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010728007182478905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65386581
Iteration 2/25 | Loss: 0.00231390
Iteration 3/25 | Loss: 0.00231390
Iteration 4/25 | Loss: 0.00231390
Iteration 5/25 | Loss: 0.00231390
Iteration 6/25 | Loss: 0.00231390
Iteration 7/25 | Loss: 0.00231390
Iteration 8/25 | Loss: 0.00231390
Iteration 9/25 | Loss: 0.00231390
Iteration 10/25 | Loss: 0.00231389
Iteration 11/25 | Loss: 0.00231390
Iteration 12/25 | Loss: 0.00231389
Iteration 13/25 | Loss: 0.00231389
Iteration 14/25 | Loss: 0.00231389
Iteration 15/25 | Loss: 0.00231389
Iteration 16/25 | Loss: 0.00231389
Iteration 17/25 | Loss: 0.00231389
Iteration 18/25 | Loss: 0.00231389
Iteration 19/25 | Loss: 0.00231389
Iteration 20/25 | Loss: 0.00231389
Iteration 21/25 | Loss: 0.00231389
Iteration 22/25 | Loss: 0.00231389
Iteration 23/25 | Loss: 0.00231389
Iteration 24/25 | Loss: 0.00231389
Iteration 25/25 | Loss: 0.00231389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231389
Iteration 2/1000 | Loss: 0.00002700
Iteration 3/1000 | Loss: 0.00002053
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001773
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001648
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001488
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001445
Iteration 21/1000 | Loss: 0.00001444
Iteration 22/1000 | Loss: 0.00001442
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001439
Iteration 27/1000 | Loss: 0.00001438
Iteration 28/1000 | Loss: 0.00001437
Iteration 29/1000 | Loss: 0.00001437
Iteration 30/1000 | Loss: 0.00001435
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001434
Iteration 33/1000 | Loss: 0.00001434
Iteration 34/1000 | Loss: 0.00001433
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001432
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001430
Iteration 49/1000 | Loss: 0.00001430
Iteration 50/1000 | Loss: 0.00001430
Iteration 51/1000 | Loss: 0.00001430
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001429
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001429
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001428
Iteration 68/1000 | Loss: 0.00001428
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001427
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001427
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001426
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001421
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001420
Iteration 126/1000 | Loss: 0.00001420
Iteration 127/1000 | Loss: 0.00001420
Iteration 128/1000 | Loss: 0.00001420
Iteration 129/1000 | Loss: 0.00001420
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001419
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001418
Iteration 150/1000 | Loss: 0.00001418
Iteration 151/1000 | Loss: 0.00001418
Iteration 152/1000 | Loss: 0.00001418
Iteration 153/1000 | Loss: 0.00001418
Iteration 154/1000 | Loss: 0.00001418
Iteration 155/1000 | Loss: 0.00001418
Iteration 156/1000 | Loss: 0.00001418
Iteration 157/1000 | Loss: 0.00001418
Iteration 158/1000 | Loss: 0.00001418
Iteration 159/1000 | Loss: 0.00001418
Iteration 160/1000 | Loss: 0.00001418
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001418
Iteration 182/1000 | Loss: 0.00001418
Iteration 183/1000 | Loss: 0.00001418
Iteration 184/1000 | Loss: 0.00001418
Iteration 185/1000 | Loss: 0.00001418
Iteration 186/1000 | Loss: 0.00001418
Iteration 187/1000 | Loss: 0.00001418
Iteration 188/1000 | Loss: 0.00001418
Iteration 189/1000 | Loss: 0.00001418
Iteration 190/1000 | Loss: 0.00001418
Iteration 191/1000 | Loss: 0.00001418
Iteration 192/1000 | Loss: 0.00001418
Iteration 193/1000 | Loss: 0.00001418
Iteration 194/1000 | Loss: 0.00001418
Iteration 195/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.4176948752719909e-05, 1.4176948752719909e-05, 1.4176948752719909e-05, 1.4176948752719909e-05, 1.4176948752719909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4176948752719909e-05

Optimization complete. Final v2v error: 3.20755934715271 mm

Highest mean error: 3.778923749923706 mm for frame 132

Lowest mean error: 2.823120594024658 mm for frame 0

Saving results

Total time: 58.463430404663086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013211
Iteration 2/25 | Loss: 0.00256660
Iteration 3/25 | Loss: 0.00178588
Iteration 4/25 | Loss: 0.00193852
Iteration 5/25 | Loss: 0.00178841
Iteration 6/25 | Loss: 0.00147538
Iteration 7/25 | Loss: 0.00141966
Iteration 8/25 | Loss: 0.00136429
Iteration 9/25 | Loss: 0.00133822
Iteration 10/25 | Loss: 0.00130340
Iteration 11/25 | Loss: 0.00131987
Iteration 12/25 | Loss: 0.00129743
Iteration 13/25 | Loss: 0.00129263
Iteration 14/25 | Loss: 0.00129169
Iteration 15/25 | Loss: 0.00128914
Iteration 16/25 | Loss: 0.00128842
Iteration 17/25 | Loss: 0.00128819
Iteration 18/25 | Loss: 0.00128810
Iteration 19/25 | Loss: 0.00128934
Iteration 20/25 | Loss: 0.00128773
Iteration 21/25 | Loss: 0.00128716
Iteration 22/25 | Loss: 0.00128702
Iteration 23/25 | Loss: 0.00128701
Iteration 24/25 | Loss: 0.00128701
Iteration 25/25 | Loss: 0.00128700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09845805
Iteration 2/25 | Loss: 0.00286083
Iteration 3/25 | Loss: 0.00286083
Iteration 4/25 | Loss: 0.00286083
Iteration 5/25 | Loss: 0.00286083
Iteration 6/25 | Loss: 0.00286083
Iteration 7/25 | Loss: 0.00286083
Iteration 8/25 | Loss: 0.00286083
Iteration 9/25 | Loss: 0.00286083
Iteration 10/25 | Loss: 0.00286082
Iteration 11/25 | Loss: 0.00286082
Iteration 12/25 | Loss: 0.00286082
Iteration 13/25 | Loss: 0.00286082
Iteration 14/25 | Loss: 0.00286082
Iteration 15/25 | Loss: 0.00286082
Iteration 16/25 | Loss: 0.00286082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002860824577510357, 0.002860824577510357, 0.002860824577510357, 0.002860824577510357, 0.002860824577510357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002860824577510357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286082
Iteration 2/1000 | Loss: 0.00051223
Iteration 3/1000 | Loss: 0.00009250
Iteration 4/1000 | Loss: 0.00007577
Iteration 5/1000 | Loss: 0.00006872
Iteration 6/1000 | Loss: 0.00006313
Iteration 7/1000 | Loss: 0.00006062
Iteration 8/1000 | Loss: 0.00005779
Iteration 9/1000 | Loss: 0.00005601
Iteration 10/1000 | Loss: 0.00005451
Iteration 11/1000 | Loss: 0.00005322
Iteration 12/1000 | Loss: 0.00005221
Iteration 13/1000 | Loss: 0.00005168
Iteration 14/1000 | Loss: 0.00005129
Iteration 15/1000 | Loss: 0.00005102
Iteration 16/1000 | Loss: 0.00005089
Iteration 17/1000 | Loss: 0.00005084
Iteration 18/1000 | Loss: 0.00005084
Iteration 19/1000 | Loss: 0.00005084
Iteration 20/1000 | Loss: 0.00005084
Iteration 21/1000 | Loss: 0.00005083
Iteration 22/1000 | Loss: 0.00005083
Iteration 23/1000 | Loss: 0.00005082
Iteration 24/1000 | Loss: 0.00005081
Iteration 25/1000 | Loss: 0.00005081
Iteration 26/1000 | Loss: 0.00005080
Iteration 27/1000 | Loss: 0.00005080
Iteration 28/1000 | Loss: 0.00005080
Iteration 29/1000 | Loss: 0.00005080
Iteration 30/1000 | Loss: 0.00005080
Iteration 31/1000 | Loss: 0.00005080
Iteration 32/1000 | Loss: 0.00005080
Iteration 33/1000 | Loss: 0.00005080
Iteration 34/1000 | Loss: 0.00005080
Iteration 35/1000 | Loss: 0.00005079
Iteration 36/1000 | Loss: 0.00005079
Iteration 37/1000 | Loss: 0.00005079
Iteration 38/1000 | Loss: 0.00005079
Iteration 39/1000 | Loss: 0.00005079
Iteration 40/1000 | Loss: 0.00005077
Iteration 41/1000 | Loss: 0.00005077
Iteration 42/1000 | Loss: 0.00005076
Iteration 43/1000 | Loss: 0.00005076
Iteration 44/1000 | Loss: 0.00005076
Iteration 45/1000 | Loss: 0.00005076
Iteration 46/1000 | Loss: 0.00005076
Iteration 47/1000 | Loss: 0.00005075
Iteration 48/1000 | Loss: 0.00005074
Iteration 49/1000 | Loss: 0.00005074
Iteration 50/1000 | Loss: 0.00005074
Iteration 51/1000 | Loss: 0.00005073
Iteration 52/1000 | Loss: 0.00005073
Iteration 53/1000 | Loss: 0.00005073
Iteration 54/1000 | Loss: 0.00005073
Iteration 55/1000 | Loss: 0.00005073
Iteration 56/1000 | Loss: 0.00005072
Iteration 57/1000 | Loss: 0.00005072
Iteration 58/1000 | Loss: 0.00005072
Iteration 59/1000 | Loss: 0.00005072
Iteration 60/1000 | Loss: 0.00005072
Iteration 61/1000 | Loss: 0.00005071
Iteration 62/1000 | Loss: 0.00005071
Iteration 63/1000 | Loss: 0.00005071
Iteration 64/1000 | Loss: 0.00005070
Iteration 65/1000 | Loss: 0.00005070
Iteration 66/1000 | Loss: 0.00005070
Iteration 67/1000 | Loss: 0.00005069
Iteration 68/1000 | Loss: 0.00005069
Iteration 69/1000 | Loss: 0.00005069
Iteration 70/1000 | Loss: 0.00005069
Iteration 71/1000 | Loss: 0.00005069
Iteration 72/1000 | Loss: 0.00005069
Iteration 73/1000 | Loss: 0.00005069
Iteration 74/1000 | Loss: 0.00005069
Iteration 75/1000 | Loss: 0.00005069
Iteration 76/1000 | Loss: 0.00005069
Iteration 77/1000 | Loss: 0.00005069
Iteration 78/1000 | Loss: 0.00005068
Iteration 79/1000 | Loss: 0.00005068
Iteration 80/1000 | Loss: 0.00005068
Iteration 81/1000 | Loss: 0.00005068
Iteration 82/1000 | Loss: 0.00005067
Iteration 83/1000 | Loss: 0.00005067
Iteration 84/1000 | Loss: 0.00005067
Iteration 85/1000 | Loss: 0.00005067
Iteration 86/1000 | Loss: 0.00005067
Iteration 87/1000 | Loss: 0.00005067
Iteration 88/1000 | Loss: 0.00005067
Iteration 89/1000 | Loss: 0.00005067
Iteration 90/1000 | Loss: 0.00005067
Iteration 91/1000 | Loss: 0.00005066
Iteration 92/1000 | Loss: 0.00005066
Iteration 93/1000 | Loss: 0.00005066
Iteration 94/1000 | Loss: 0.00005066
Iteration 95/1000 | Loss: 0.00005066
Iteration 96/1000 | Loss: 0.00005066
Iteration 97/1000 | Loss: 0.00005066
Iteration 98/1000 | Loss: 0.00005066
Iteration 99/1000 | Loss: 0.00005065
Iteration 100/1000 | Loss: 0.00005065
Iteration 101/1000 | Loss: 0.00005065
Iteration 102/1000 | Loss: 0.00005064
Iteration 103/1000 | Loss: 0.00005064
Iteration 104/1000 | Loss: 0.00005064
Iteration 105/1000 | Loss: 0.00005064
Iteration 106/1000 | Loss: 0.00005064
Iteration 107/1000 | Loss: 0.00005064
Iteration 108/1000 | Loss: 0.00005064
Iteration 109/1000 | Loss: 0.00005064
Iteration 110/1000 | Loss: 0.00005064
Iteration 111/1000 | Loss: 0.00005064
Iteration 112/1000 | Loss: 0.00005064
Iteration 113/1000 | Loss: 0.00005063
Iteration 114/1000 | Loss: 0.00005063
Iteration 115/1000 | Loss: 0.00005063
Iteration 116/1000 | Loss: 0.00005063
Iteration 117/1000 | Loss: 0.00005063
Iteration 118/1000 | Loss: 0.00005063
Iteration 119/1000 | Loss: 0.00005063
Iteration 120/1000 | Loss: 0.00005063
Iteration 121/1000 | Loss: 0.00005063
Iteration 122/1000 | Loss: 0.00005063
Iteration 123/1000 | Loss: 0.00005062
Iteration 124/1000 | Loss: 0.00005062
Iteration 125/1000 | Loss: 0.00005062
Iteration 126/1000 | Loss: 0.00005062
Iteration 127/1000 | Loss: 0.00005062
Iteration 128/1000 | Loss: 0.00005062
Iteration 129/1000 | Loss: 0.00005062
Iteration 130/1000 | Loss: 0.00005062
Iteration 131/1000 | Loss: 0.00005062
Iteration 132/1000 | Loss: 0.00005062
Iteration 133/1000 | Loss: 0.00005062
Iteration 134/1000 | Loss: 0.00005062
Iteration 135/1000 | Loss: 0.00005062
Iteration 136/1000 | Loss: 0.00005061
Iteration 137/1000 | Loss: 0.00005061
Iteration 138/1000 | Loss: 0.00005061
Iteration 139/1000 | Loss: 0.00005061
Iteration 140/1000 | Loss: 0.00005061
Iteration 141/1000 | Loss: 0.00005061
Iteration 142/1000 | Loss: 0.00005061
Iteration 143/1000 | Loss: 0.00005061
Iteration 144/1000 | Loss: 0.00005061
Iteration 145/1000 | Loss: 0.00005061
Iteration 146/1000 | Loss: 0.00005060
Iteration 147/1000 | Loss: 0.00005060
Iteration 148/1000 | Loss: 0.00005060
Iteration 149/1000 | Loss: 0.00005060
Iteration 150/1000 | Loss: 0.00005060
Iteration 151/1000 | Loss: 0.00005060
Iteration 152/1000 | Loss: 0.00005060
Iteration 153/1000 | Loss: 0.00005060
Iteration 154/1000 | Loss: 0.00005060
Iteration 155/1000 | Loss: 0.00005060
Iteration 156/1000 | Loss: 0.00005060
Iteration 157/1000 | Loss: 0.00005060
Iteration 158/1000 | Loss: 0.00005060
Iteration 159/1000 | Loss: 0.00005060
Iteration 160/1000 | Loss: 0.00005060
Iteration 161/1000 | Loss: 0.00005060
Iteration 162/1000 | Loss: 0.00005060
Iteration 163/1000 | Loss: 0.00005060
Iteration 164/1000 | Loss: 0.00005059
Iteration 165/1000 | Loss: 0.00005059
Iteration 166/1000 | Loss: 0.00005059
Iteration 167/1000 | Loss: 0.00005059
Iteration 168/1000 | Loss: 0.00005059
Iteration 169/1000 | Loss: 0.00005059
Iteration 170/1000 | Loss: 0.00005059
Iteration 171/1000 | Loss: 0.00005059
Iteration 172/1000 | Loss: 0.00011723
Iteration 173/1000 | Loss: 0.00083583
Iteration 174/1000 | Loss: 0.00101405
Iteration 175/1000 | Loss: 0.00013031
Iteration 176/1000 | Loss: 0.00008803
Iteration 177/1000 | Loss: 0.00006869
Iteration 178/1000 | Loss: 0.00005325
Iteration 179/1000 | Loss: 0.00004036
Iteration 180/1000 | Loss: 0.00003462
Iteration 181/1000 | Loss: 0.00003166
Iteration 182/1000 | Loss: 0.00002972
Iteration 183/1000 | Loss: 0.00002787
Iteration 184/1000 | Loss: 0.00002669
Iteration 185/1000 | Loss: 0.00002586
Iteration 186/1000 | Loss: 0.00002500
Iteration 187/1000 | Loss: 0.00002442
Iteration 188/1000 | Loss: 0.00002403
Iteration 189/1000 | Loss: 0.00002377
Iteration 190/1000 | Loss: 0.00002348
Iteration 191/1000 | Loss: 0.00002347
Iteration 192/1000 | Loss: 0.00002339
Iteration 193/1000 | Loss: 0.00002335
Iteration 194/1000 | Loss: 0.00002334
Iteration 195/1000 | Loss: 0.00002333
Iteration 196/1000 | Loss: 0.00002333
Iteration 197/1000 | Loss: 0.00002333
Iteration 198/1000 | Loss: 0.00002332
Iteration 199/1000 | Loss: 0.00002332
Iteration 200/1000 | Loss: 0.00002332
Iteration 201/1000 | Loss: 0.00002332
Iteration 202/1000 | Loss: 0.00002332
Iteration 203/1000 | Loss: 0.00002332
Iteration 204/1000 | Loss: 0.00002332
Iteration 205/1000 | Loss: 0.00002332
Iteration 206/1000 | Loss: 0.00002331
Iteration 207/1000 | Loss: 0.00002331
Iteration 208/1000 | Loss: 0.00002331
Iteration 209/1000 | Loss: 0.00002331
Iteration 210/1000 | Loss: 0.00002331
Iteration 211/1000 | Loss: 0.00002331
Iteration 212/1000 | Loss: 0.00002330
Iteration 213/1000 | Loss: 0.00002330
Iteration 214/1000 | Loss: 0.00002330
Iteration 215/1000 | Loss: 0.00002329
Iteration 216/1000 | Loss: 0.00002329
Iteration 217/1000 | Loss: 0.00002329
Iteration 218/1000 | Loss: 0.00002329
Iteration 219/1000 | Loss: 0.00002329
Iteration 220/1000 | Loss: 0.00002329
Iteration 221/1000 | Loss: 0.00002329
Iteration 222/1000 | Loss: 0.00002328
Iteration 223/1000 | Loss: 0.00002328
Iteration 224/1000 | Loss: 0.00002328
Iteration 225/1000 | Loss: 0.00002328
Iteration 226/1000 | Loss: 0.00002328
Iteration 227/1000 | Loss: 0.00002327
Iteration 228/1000 | Loss: 0.00002327
Iteration 229/1000 | Loss: 0.00002326
Iteration 230/1000 | Loss: 0.00002326
Iteration 231/1000 | Loss: 0.00002326
Iteration 232/1000 | Loss: 0.00002325
Iteration 233/1000 | Loss: 0.00002325
Iteration 234/1000 | Loss: 0.00002325
Iteration 235/1000 | Loss: 0.00002324
Iteration 236/1000 | Loss: 0.00002324
Iteration 237/1000 | Loss: 0.00002324
Iteration 238/1000 | Loss: 0.00002324
Iteration 239/1000 | Loss: 0.00002324
Iteration 240/1000 | Loss: 0.00002324
Iteration 241/1000 | Loss: 0.00002324
Iteration 242/1000 | Loss: 0.00002323
Iteration 243/1000 | Loss: 0.00002323
Iteration 244/1000 | Loss: 0.00002323
Iteration 245/1000 | Loss: 0.00002323
Iteration 246/1000 | Loss: 0.00002323
Iteration 247/1000 | Loss: 0.00002323
Iteration 248/1000 | Loss: 0.00002322
Iteration 249/1000 | Loss: 0.00002322
Iteration 250/1000 | Loss: 0.00002322
Iteration 251/1000 | Loss: 0.00002322
Iteration 252/1000 | Loss: 0.00002321
Iteration 253/1000 | Loss: 0.00002321
Iteration 254/1000 | Loss: 0.00002321
Iteration 255/1000 | Loss: 0.00002321
Iteration 256/1000 | Loss: 0.00002321
Iteration 257/1000 | Loss: 0.00002320
Iteration 258/1000 | Loss: 0.00002320
Iteration 259/1000 | Loss: 0.00002320
Iteration 260/1000 | Loss: 0.00002320
Iteration 261/1000 | Loss: 0.00002320
Iteration 262/1000 | Loss: 0.00002320
Iteration 263/1000 | Loss: 0.00002320
Iteration 264/1000 | Loss: 0.00002319
Iteration 265/1000 | Loss: 0.00002319
Iteration 266/1000 | Loss: 0.00002319
Iteration 267/1000 | Loss: 0.00002319
Iteration 268/1000 | Loss: 0.00002319
Iteration 269/1000 | Loss: 0.00002319
Iteration 270/1000 | Loss: 0.00002319
Iteration 271/1000 | Loss: 0.00002319
Iteration 272/1000 | Loss: 0.00002319
Iteration 273/1000 | Loss: 0.00002319
Iteration 274/1000 | Loss: 0.00002319
Iteration 275/1000 | Loss: 0.00002319
Iteration 276/1000 | Loss: 0.00002319
Iteration 277/1000 | Loss: 0.00002319
Iteration 278/1000 | Loss: 0.00002319
Iteration 279/1000 | Loss: 0.00002319
Iteration 280/1000 | Loss: 0.00002319
Iteration 281/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.3189486455521546e-05, 2.3189486455521546e-05, 2.3189486455521546e-05, 2.3189486455521546e-05, 2.3189486455521546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3189486455521546e-05

Optimization complete. Final v2v error: 4.011628150939941 mm

Highest mean error: 4.303283214569092 mm for frame 126

Lowest mean error: 3.597813129425049 mm for frame 32

Saving results

Total time: 121.58812642097473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00706429
Iteration 2/25 | Loss: 0.00150772
Iteration 3/25 | Loss: 0.00120916
Iteration 4/25 | Loss: 0.00116883
Iteration 5/25 | Loss: 0.00116079
Iteration 6/25 | Loss: 0.00116958
Iteration 7/25 | Loss: 0.00115368
Iteration 8/25 | Loss: 0.00115475
Iteration 9/25 | Loss: 0.00115029
Iteration 10/25 | Loss: 0.00115176
Iteration 11/25 | Loss: 0.00114449
Iteration 12/25 | Loss: 0.00113712
Iteration 13/25 | Loss: 0.00113070
Iteration 14/25 | Loss: 0.00112855
Iteration 15/25 | Loss: 0.00112978
Iteration 16/25 | Loss: 0.00112714
Iteration 17/25 | Loss: 0.00112506
Iteration 18/25 | Loss: 0.00112285
Iteration 19/25 | Loss: 0.00112204
Iteration 20/25 | Loss: 0.00112156
Iteration 21/25 | Loss: 0.00112079
Iteration 22/25 | Loss: 0.00111958
Iteration 23/25 | Loss: 0.00111881
Iteration 24/25 | Loss: 0.00111858
Iteration 25/25 | Loss: 0.00111850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83404875
Iteration 2/25 | Loss: 0.00246905
Iteration 3/25 | Loss: 0.00246904
Iteration 4/25 | Loss: 0.00246904
Iteration 5/25 | Loss: 0.00246904
Iteration 6/25 | Loss: 0.00246904
Iteration 7/25 | Loss: 0.00246904
Iteration 8/25 | Loss: 0.00246904
Iteration 9/25 | Loss: 0.00246904
Iteration 10/25 | Loss: 0.00246904
Iteration 11/25 | Loss: 0.00246904
Iteration 12/25 | Loss: 0.00246904
Iteration 13/25 | Loss: 0.00246904
Iteration 14/25 | Loss: 0.00246904
Iteration 15/25 | Loss: 0.00246904
Iteration 16/25 | Loss: 0.00246904
Iteration 17/25 | Loss: 0.00246904
Iteration 18/25 | Loss: 0.00246904
Iteration 19/25 | Loss: 0.00246904
Iteration 20/25 | Loss: 0.00246904
Iteration 21/25 | Loss: 0.00246904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024690371938049793, 0.0024690371938049793, 0.0024690371938049793, 0.0024690371938049793, 0.0024690371938049793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024690371938049793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246904
Iteration 2/1000 | Loss: 0.00005246
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001855
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001659
Iteration 7/1000 | Loss: 0.00001598
Iteration 8/1000 | Loss: 0.00001555
Iteration 9/1000 | Loss: 0.00001509
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001443
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001435
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001426
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001418
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001400
Iteration 39/1000 | Loss: 0.00001400
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001395
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001394
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001393
Iteration 81/1000 | Loss: 0.00001393
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001393
Iteration 84/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.3927767213317566e-05, 1.3927767213317566e-05, 1.3927767213317566e-05, 1.3927767213317566e-05, 1.3927767213317566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3927767213317566e-05

Optimization complete. Final v2v error: 3.219470500946045 mm

Highest mean error: 3.4992263317108154 mm for frame 76

Lowest mean error: 2.615086555480957 mm for frame 234

Saving results

Total time: 78.18697333335876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794481
Iteration 2/25 | Loss: 0.00162964
Iteration 3/25 | Loss: 0.00138685
Iteration 4/25 | Loss: 0.00137208
Iteration 5/25 | Loss: 0.00125698
Iteration 6/25 | Loss: 0.00122883
Iteration 7/25 | Loss: 0.00117461
Iteration 8/25 | Loss: 0.00115660
Iteration 9/25 | Loss: 0.00115425
Iteration 10/25 | Loss: 0.00115394
Iteration 11/25 | Loss: 0.00115379
Iteration 12/25 | Loss: 0.00115375
Iteration 13/25 | Loss: 0.00115374
Iteration 14/25 | Loss: 0.00115374
Iteration 15/25 | Loss: 0.00115374
Iteration 16/25 | Loss: 0.00115372
Iteration 17/25 | Loss: 0.00115371
Iteration 18/25 | Loss: 0.00115371
Iteration 19/25 | Loss: 0.00115371
Iteration 20/25 | Loss: 0.00115371
Iteration 21/25 | Loss: 0.00115371
Iteration 22/25 | Loss: 0.00115371
Iteration 23/25 | Loss: 0.00115371
Iteration 24/25 | Loss: 0.00115371
Iteration 25/25 | Loss: 0.00115371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29081702
Iteration 2/25 | Loss: 0.00265232
Iteration 3/25 | Loss: 0.00265230
Iteration 4/25 | Loss: 0.00265230
Iteration 5/25 | Loss: 0.00265230
Iteration 6/25 | Loss: 0.00265230
Iteration 7/25 | Loss: 0.00265230
Iteration 8/25 | Loss: 0.00265230
Iteration 9/25 | Loss: 0.00265230
Iteration 10/25 | Loss: 0.00265230
Iteration 11/25 | Loss: 0.00265230
Iteration 12/25 | Loss: 0.00265230
Iteration 13/25 | Loss: 0.00265230
Iteration 14/25 | Loss: 0.00265230
Iteration 15/25 | Loss: 0.00265230
Iteration 16/25 | Loss: 0.00265230
Iteration 17/25 | Loss: 0.00265230
Iteration 18/25 | Loss: 0.00265230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0026522979605942965, 0.0026522979605942965, 0.0026522979605942965, 0.0026522979605942965, 0.0026522979605942965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026522979605942965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265230
Iteration 2/1000 | Loss: 0.00005998
Iteration 3/1000 | Loss: 0.00156483
Iteration 4/1000 | Loss: 0.00141866
Iteration 5/1000 | Loss: 0.00160310
Iteration 6/1000 | Loss: 0.00005292
Iteration 7/1000 | Loss: 0.00033962
Iteration 8/1000 | Loss: 0.00004356
Iteration 9/1000 | Loss: 0.00003022
Iteration 10/1000 | Loss: 0.00083418
Iteration 11/1000 | Loss: 0.00003206
Iteration 12/1000 | Loss: 0.00002571
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002010
Iteration 16/1000 | Loss: 0.00001938
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001865
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001754
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001753
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001751
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001749
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001748
Iteration 54/1000 | Loss: 0.00001748
Iteration 55/1000 | Loss: 0.00001748
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001747
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001746
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001745
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001740
Iteration 86/1000 | Loss: 0.00001739
Iteration 87/1000 | Loss: 0.00001738
Iteration 88/1000 | Loss: 0.00001738
Iteration 89/1000 | Loss: 0.00001738
Iteration 90/1000 | Loss: 0.00001738
Iteration 91/1000 | Loss: 0.00001737
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001736
Iteration 94/1000 | Loss: 0.00001736
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001735
Iteration 106/1000 | Loss: 0.00001735
Iteration 107/1000 | Loss: 0.00001735
Iteration 108/1000 | Loss: 0.00001735
Iteration 109/1000 | Loss: 0.00001734
Iteration 110/1000 | Loss: 0.00001734
Iteration 111/1000 | Loss: 0.00001734
Iteration 112/1000 | Loss: 0.00001734
Iteration 113/1000 | Loss: 0.00001734
Iteration 114/1000 | Loss: 0.00001734
Iteration 115/1000 | Loss: 0.00001734
Iteration 116/1000 | Loss: 0.00001734
Iteration 117/1000 | Loss: 0.00001734
Iteration 118/1000 | Loss: 0.00001734
Iteration 119/1000 | Loss: 0.00001733
Iteration 120/1000 | Loss: 0.00001733
Iteration 121/1000 | Loss: 0.00001733
Iteration 122/1000 | Loss: 0.00001733
Iteration 123/1000 | Loss: 0.00001733
Iteration 124/1000 | Loss: 0.00001733
Iteration 125/1000 | Loss: 0.00001733
Iteration 126/1000 | Loss: 0.00001733
Iteration 127/1000 | Loss: 0.00001733
Iteration 128/1000 | Loss: 0.00001733
Iteration 129/1000 | Loss: 0.00001733
Iteration 130/1000 | Loss: 0.00001733
Iteration 131/1000 | Loss: 0.00001733
Iteration 132/1000 | Loss: 0.00001732
Iteration 133/1000 | Loss: 0.00001732
Iteration 134/1000 | Loss: 0.00001732
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001732
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001732
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Iteration 145/1000 | Loss: 0.00001731
Iteration 146/1000 | Loss: 0.00001731
Iteration 147/1000 | Loss: 0.00001731
Iteration 148/1000 | Loss: 0.00001731
Iteration 149/1000 | Loss: 0.00001731
Iteration 150/1000 | Loss: 0.00001731
Iteration 151/1000 | Loss: 0.00001730
Iteration 152/1000 | Loss: 0.00001730
Iteration 153/1000 | Loss: 0.00001730
Iteration 154/1000 | Loss: 0.00001730
Iteration 155/1000 | Loss: 0.00001730
Iteration 156/1000 | Loss: 0.00001730
Iteration 157/1000 | Loss: 0.00001730
Iteration 158/1000 | Loss: 0.00001730
Iteration 159/1000 | Loss: 0.00001730
Iteration 160/1000 | Loss: 0.00001730
Iteration 161/1000 | Loss: 0.00001730
Iteration 162/1000 | Loss: 0.00001730
Iteration 163/1000 | Loss: 0.00001730
Iteration 164/1000 | Loss: 0.00001730
Iteration 165/1000 | Loss: 0.00001729
Iteration 166/1000 | Loss: 0.00001729
Iteration 167/1000 | Loss: 0.00001729
Iteration 168/1000 | Loss: 0.00001729
Iteration 169/1000 | Loss: 0.00001729
Iteration 170/1000 | Loss: 0.00001729
Iteration 171/1000 | Loss: 0.00001729
Iteration 172/1000 | Loss: 0.00001729
Iteration 173/1000 | Loss: 0.00001729
Iteration 174/1000 | Loss: 0.00001729
Iteration 175/1000 | Loss: 0.00001728
Iteration 176/1000 | Loss: 0.00001728
Iteration 177/1000 | Loss: 0.00001728
Iteration 178/1000 | Loss: 0.00001728
Iteration 179/1000 | Loss: 0.00001728
Iteration 180/1000 | Loss: 0.00001727
Iteration 181/1000 | Loss: 0.00001727
Iteration 182/1000 | Loss: 0.00001727
Iteration 183/1000 | Loss: 0.00001727
Iteration 184/1000 | Loss: 0.00001727
Iteration 185/1000 | Loss: 0.00001727
Iteration 186/1000 | Loss: 0.00001727
Iteration 187/1000 | Loss: 0.00001727
Iteration 188/1000 | Loss: 0.00001727
Iteration 189/1000 | Loss: 0.00001727
Iteration 190/1000 | Loss: 0.00001727
Iteration 191/1000 | Loss: 0.00001727
Iteration 192/1000 | Loss: 0.00001726
Iteration 193/1000 | Loss: 0.00001726
Iteration 194/1000 | Loss: 0.00001726
Iteration 195/1000 | Loss: 0.00001726
Iteration 196/1000 | Loss: 0.00001726
Iteration 197/1000 | Loss: 0.00001726
Iteration 198/1000 | Loss: 0.00001726
Iteration 199/1000 | Loss: 0.00001726
Iteration 200/1000 | Loss: 0.00001726
Iteration 201/1000 | Loss: 0.00001726
Iteration 202/1000 | Loss: 0.00001726
Iteration 203/1000 | Loss: 0.00001726
Iteration 204/1000 | Loss: 0.00001726
Iteration 205/1000 | Loss: 0.00001726
Iteration 206/1000 | Loss: 0.00001726
Iteration 207/1000 | Loss: 0.00001726
Iteration 208/1000 | Loss: 0.00001726
Iteration 209/1000 | Loss: 0.00001726
Iteration 210/1000 | Loss: 0.00001726
Iteration 211/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.7257319996133447e-05, 1.7257319996133447e-05, 1.7257319996133447e-05, 1.7257319996133447e-05, 1.7257319996133447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7257319996133447e-05

Optimization complete. Final v2v error: 3.4624814987182617 mm

Highest mean error: 4.388202667236328 mm for frame 133

Lowest mean error: 2.732107162475586 mm for frame 167

Saving results

Total time: 76.82734394073486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042004
Iteration 2/25 | Loss: 0.00241935
Iteration 3/25 | Loss: 0.00271618
Iteration 4/25 | Loss: 0.00127625
Iteration 5/25 | Loss: 0.00122904
Iteration 6/25 | Loss: 0.00127381
Iteration 7/25 | Loss: 0.00125458
Iteration 8/25 | Loss: 0.00120913
Iteration 9/25 | Loss: 0.00117133
Iteration 10/25 | Loss: 0.00115637
Iteration 11/25 | Loss: 0.00115244
Iteration 12/25 | Loss: 0.00114417
Iteration 13/25 | Loss: 0.00114304
Iteration 14/25 | Loss: 0.00114247
Iteration 15/25 | Loss: 0.00112930
Iteration 16/25 | Loss: 0.00111057
Iteration 17/25 | Loss: 0.00110903
Iteration 18/25 | Loss: 0.00110247
Iteration 19/25 | Loss: 0.00109838
Iteration 20/25 | Loss: 0.00109318
Iteration 21/25 | Loss: 0.00109073
Iteration 22/25 | Loss: 0.00108747
Iteration 23/25 | Loss: 0.00108563
Iteration 24/25 | Loss: 0.00108495
Iteration 25/25 | Loss: 0.00108489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26535571
Iteration 2/25 | Loss: 0.00243595
Iteration 3/25 | Loss: 0.00243595
Iteration 4/25 | Loss: 0.00243595
Iteration 5/25 | Loss: 0.00243595
Iteration 6/25 | Loss: 0.00243595
Iteration 7/25 | Loss: 0.00243595
Iteration 8/25 | Loss: 0.00243595
Iteration 9/25 | Loss: 0.00243595
Iteration 10/25 | Loss: 0.00243595
Iteration 11/25 | Loss: 0.00243595
Iteration 12/25 | Loss: 0.00243595
Iteration 13/25 | Loss: 0.00243595
Iteration 14/25 | Loss: 0.00243595
Iteration 15/25 | Loss: 0.00243595
Iteration 16/25 | Loss: 0.00243595
Iteration 17/25 | Loss: 0.00243595
Iteration 18/25 | Loss: 0.00243595
Iteration 19/25 | Loss: 0.00243595
Iteration 20/25 | Loss: 0.00243595
Iteration 21/25 | Loss: 0.00243595
Iteration 22/25 | Loss: 0.00243595
Iteration 23/25 | Loss: 0.00243595
Iteration 24/25 | Loss: 0.00243595
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0024359470698982477, 0.0024359470698982477, 0.0024359470698982477, 0.0024359470698982477, 0.0024359470698982477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024359470698982477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243595
Iteration 2/1000 | Loss: 0.00048754
Iteration 3/1000 | Loss: 0.00004977
Iteration 4/1000 | Loss: 0.00003489
Iteration 5/1000 | Loss: 0.00002774
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00003192
Iteration 8/1000 | Loss: 0.00002958
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001621
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001370
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001348
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001331
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00026015
Iteration 33/1000 | Loss: 0.00080603
Iteration 34/1000 | Loss: 0.00004103
Iteration 35/1000 | Loss: 0.00002387
Iteration 36/1000 | Loss: 0.00027054
Iteration 37/1000 | Loss: 0.00020418
Iteration 38/1000 | Loss: 0.00088110
Iteration 39/1000 | Loss: 0.00018923
Iteration 40/1000 | Loss: 0.00050866
Iteration 41/1000 | Loss: 0.00003700
Iteration 42/1000 | Loss: 0.00002602
Iteration 43/1000 | Loss: 0.00002025
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001758
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00023317
Iteration 48/1000 | Loss: 0.00035360
Iteration 49/1000 | Loss: 0.00006599
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00014384
Iteration 52/1000 | Loss: 0.00032127
Iteration 53/1000 | Loss: 0.00136473
Iteration 54/1000 | Loss: 0.00016096
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00035508
Iteration 57/1000 | Loss: 0.00002671
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001251
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001245
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001236
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001235
Iteration 130/1000 | Loss: 0.00001235
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001234
Iteration 137/1000 | Loss: 0.00001234
Iteration 138/1000 | Loss: 0.00001234
Iteration 139/1000 | Loss: 0.00001234
Iteration 140/1000 | Loss: 0.00001234
Iteration 141/1000 | Loss: 0.00001234
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Iteration 149/1000 | Loss: 0.00001233
Iteration 150/1000 | Loss: 0.00001232
Iteration 151/1000 | Loss: 0.00001232
Iteration 152/1000 | Loss: 0.00001232
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001232
Iteration 155/1000 | Loss: 0.00001232
Iteration 156/1000 | Loss: 0.00001232
Iteration 157/1000 | Loss: 0.00001232
Iteration 158/1000 | Loss: 0.00001232
Iteration 159/1000 | Loss: 0.00001232
Iteration 160/1000 | Loss: 0.00001232
Iteration 161/1000 | Loss: 0.00001231
Iteration 162/1000 | Loss: 0.00001231
Iteration 163/1000 | Loss: 0.00001231
Iteration 164/1000 | Loss: 0.00001231
Iteration 165/1000 | Loss: 0.00001231
Iteration 166/1000 | Loss: 0.00001231
Iteration 167/1000 | Loss: 0.00001231
Iteration 168/1000 | Loss: 0.00001231
Iteration 169/1000 | Loss: 0.00001231
Iteration 170/1000 | Loss: 0.00001231
Iteration 171/1000 | Loss: 0.00001231
Iteration 172/1000 | Loss: 0.00001231
Iteration 173/1000 | Loss: 0.00001231
Iteration 174/1000 | Loss: 0.00001231
Iteration 175/1000 | Loss: 0.00001231
Iteration 176/1000 | Loss: 0.00001231
Iteration 177/1000 | Loss: 0.00001231
Iteration 178/1000 | Loss: 0.00001231
Iteration 179/1000 | Loss: 0.00001231
Iteration 180/1000 | Loss: 0.00001231
Iteration 181/1000 | Loss: 0.00001230
Iteration 182/1000 | Loss: 0.00001230
Iteration 183/1000 | Loss: 0.00001230
Iteration 184/1000 | Loss: 0.00001230
Iteration 185/1000 | Loss: 0.00001230
Iteration 186/1000 | Loss: 0.00001230
Iteration 187/1000 | Loss: 0.00001230
Iteration 188/1000 | Loss: 0.00001230
Iteration 189/1000 | Loss: 0.00001230
Iteration 190/1000 | Loss: 0.00001230
Iteration 191/1000 | Loss: 0.00001230
Iteration 192/1000 | Loss: 0.00001230
Iteration 193/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.2304739357205108e-05, 1.2304739357205108e-05, 1.2304739357205108e-05, 1.2304739357205108e-05, 1.2304739357205108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2304739357205108e-05

Optimization complete. Final v2v error: 2.9326283931732178 mm

Highest mean error: 3.799508810043335 mm for frame 103

Lowest mean error: 2.5844972133636475 mm for frame 125

Saving results

Total time: 127.61240983009338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935426
Iteration 2/25 | Loss: 0.00166361
Iteration 3/25 | Loss: 0.00136108
Iteration 4/25 | Loss: 0.00121551
Iteration 5/25 | Loss: 0.00121121
Iteration 6/25 | Loss: 0.00120953
Iteration 7/25 | Loss: 0.00121067
Iteration 8/25 | Loss: 0.00120440
Iteration 9/25 | Loss: 0.00118835
Iteration 10/25 | Loss: 0.00118721
Iteration 11/25 | Loss: 0.00118688
Iteration 12/25 | Loss: 0.00118676
Iteration 13/25 | Loss: 0.00118675
Iteration 14/25 | Loss: 0.00118674
Iteration 15/25 | Loss: 0.00118674
Iteration 16/25 | Loss: 0.00118674
Iteration 17/25 | Loss: 0.00118674
Iteration 18/25 | Loss: 0.00118674
Iteration 19/25 | Loss: 0.00118674
Iteration 20/25 | Loss: 0.00118674
Iteration 21/25 | Loss: 0.00118673
Iteration 22/25 | Loss: 0.00118673
Iteration 23/25 | Loss: 0.00118673
Iteration 24/25 | Loss: 0.00118673
Iteration 25/25 | Loss: 0.00118673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14577794
Iteration 2/25 | Loss: 0.00277789
Iteration 3/25 | Loss: 0.00277789
Iteration 4/25 | Loss: 0.00277789
Iteration 5/25 | Loss: 0.00277789
Iteration 6/25 | Loss: 0.00277789
Iteration 7/25 | Loss: 0.00277789
Iteration 8/25 | Loss: 0.00277789
Iteration 9/25 | Loss: 0.00277789
Iteration 10/25 | Loss: 0.00277789
Iteration 11/25 | Loss: 0.00277789
Iteration 12/25 | Loss: 0.00277789
Iteration 13/25 | Loss: 0.00277789
Iteration 14/25 | Loss: 0.00277789
Iteration 15/25 | Loss: 0.00277789
Iteration 16/25 | Loss: 0.00277789
Iteration 17/25 | Loss: 0.00277789
Iteration 18/25 | Loss: 0.00277789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0027778884395956993, 0.0027778884395956993, 0.0027778884395956993, 0.0027778884395956993, 0.0027778884395956993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027778884395956993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277789
Iteration 2/1000 | Loss: 0.00004378
Iteration 3/1000 | Loss: 0.00002795
Iteration 4/1000 | Loss: 0.00003608
Iteration 5/1000 | Loss: 0.00002680
Iteration 6/1000 | Loss: 0.00002223
Iteration 7/1000 | Loss: 0.00002871
Iteration 8/1000 | Loss: 0.00002143
Iteration 9/1000 | Loss: 0.00003606
Iteration 10/1000 | Loss: 0.00002088
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002074
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002066
Iteration 16/1000 | Loss: 0.00002051
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002045
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002044
Iteration 23/1000 | Loss: 0.00002043
Iteration 24/1000 | Loss: 0.00002043
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002035
Iteration 28/1000 | Loss: 0.00002035
Iteration 29/1000 | Loss: 0.00002035
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002034
Iteration 32/1000 | Loss: 0.00002034
Iteration 33/1000 | Loss: 0.00002033
Iteration 34/1000 | Loss: 0.00002033
Iteration 35/1000 | Loss: 0.00002033
Iteration 36/1000 | Loss: 0.00002032
Iteration 37/1000 | Loss: 0.00002032
Iteration 38/1000 | Loss: 0.00002031
Iteration 39/1000 | Loss: 0.00002031
Iteration 40/1000 | Loss: 0.00002030
Iteration 41/1000 | Loss: 0.00002030
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00003390
Iteration 44/1000 | Loss: 0.00002029
Iteration 45/1000 | Loss: 0.00002024
Iteration 46/1000 | Loss: 0.00002024
Iteration 47/1000 | Loss: 0.00002023
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002022
Iteration 52/1000 | Loss: 0.00002022
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002021
Iteration 57/1000 | Loss: 0.00002021
Iteration 58/1000 | Loss: 0.00002021
Iteration 59/1000 | Loss: 0.00002021
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002021
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002020
Iteration 66/1000 | Loss: 0.00002955
Iteration 67/1000 | Loss: 0.00002020
Iteration 68/1000 | Loss: 0.00002016
Iteration 69/1000 | Loss: 0.00002016
Iteration 70/1000 | Loss: 0.00002016
Iteration 71/1000 | Loss: 0.00002015
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002016
Iteration 74/1000 | Loss: 0.00002016
Iteration 75/1000 | Loss: 0.00002016
Iteration 76/1000 | Loss: 0.00002016
Iteration 77/1000 | Loss: 0.00002016
Iteration 78/1000 | Loss: 0.00002015
Iteration 79/1000 | Loss: 0.00002015
Iteration 80/1000 | Loss: 0.00002015
Iteration 81/1000 | Loss: 0.00002015
Iteration 82/1000 | Loss: 0.00002014
Iteration 83/1000 | Loss: 0.00002013
Iteration 84/1000 | Loss: 0.00002013
Iteration 85/1000 | Loss: 0.00002013
Iteration 86/1000 | Loss: 0.00002013
Iteration 87/1000 | Loss: 0.00002013
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002012
Iteration 90/1000 | Loss: 0.00002012
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002012
Iteration 96/1000 | Loss: 0.00002012
Iteration 97/1000 | Loss: 0.00002012
Iteration 98/1000 | Loss: 0.00002011
Iteration 99/1000 | Loss: 0.00002011
Iteration 100/1000 | Loss: 0.00002011
Iteration 101/1000 | Loss: 0.00002011
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002011
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002010
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002010
Iteration 108/1000 | Loss: 0.00002010
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002009
Iteration 112/1000 | Loss: 0.00002009
Iteration 113/1000 | Loss: 0.00002009
Iteration 114/1000 | Loss: 0.00002009
Iteration 115/1000 | Loss: 0.00002009
Iteration 116/1000 | Loss: 0.00002009
Iteration 117/1000 | Loss: 0.00002009
Iteration 118/1000 | Loss: 0.00002688
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002007
Iteration 121/1000 | Loss: 0.00002007
Iteration 122/1000 | Loss: 0.00002006
Iteration 123/1000 | Loss: 0.00002006
Iteration 124/1000 | Loss: 0.00002006
Iteration 125/1000 | Loss: 0.00002006
Iteration 126/1000 | Loss: 0.00002006
Iteration 127/1000 | Loss: 0.00002006
Iteration 128/1000 | Loss: 0.00002006
Iteration 129/1000 | Loss: 0.00002006
Iteration 130/1000 | Loss: 0.00002006
Iteration 131/1000 | Loss: 0.00002006
Iteration 132/1000 | Loss: 0.00002006
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00002006
Iteration 135/1000 | Loss: 0.00002006
Iteration 136/1000 | Loss: 0.00002006
Iteration 137/1000 | Loss: 0.00002006
Iteration 138/1000 | Loss: 0.00002006
Iteration 139/1000 | Loss: 0.00002006
Iteration 140/1000 | Loss: 0.00002006
Iteration 141/1000 | Loss: 0.00002006
Iteration 142/1000 | Loss: 0.00002006
Iteration 143/1000 | Loss: 0.00002006
Iteration 144/1000 | Loss: 0.00002006
Iteration 145/1000 | Loss: 0.00002006
Iteration 146/1000 | Loss: 0.00002006
Iteration 147/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.0061826944584027e-05, 2.0061826944584027e-05, 2.0061826944584027e-05, 2.0061826944584027e-05, 2.0061826944584027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0061826944584027e-05

Optimization complete. Final v2v error: 3.6885039806365967 mm

Highest mean error: 4.186529159545898 mm for frame 196

Lowest mean error: 3.322737455368042 mm for frame 185

Saving results

Total time: 62.40775275230408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005765
Iteration 2/25 | Loss: 0.00221240
Iteration 3/25 | Loss: 0.00206068
Iteration 4/25 | Loss: 0.00156392
Iteration 5/25 | Loss: 0.00140819
Iteration 6/25 | Loss: 0.00139099
Iteration 7/25 | Loss: 0.00143639
Iteration 8/25 | Loss: 0.00134390
Iteration 9/25 | Loss: 0.00128703
Iteration 10/25 | Loss: 0.00126341
Iteration 11/25 | Loss: 0.00119528
Iteration 12/25 | Loss: 0.00118721
Iteration 13/25 | Loss: 0.00121687
Iteration 14/25 | Loss: 0.00117124
Iteration 15/25 | Loss: 0.00116086
Iteration 16/25 | Loss: 0.00115551
Iteration 17/25 | Loss: 0.00116009
Iteration 18/25 | Loss: 0.00115184
Iteration 19/25 | Loss: 0.00114892
Iteration 20/25 | Loss: 0.00114779
Iteration 21/25 | Loss: 0.00114743
Iteration 22/25 | Loss: 0.00115336
Iteration 23/25 | Loss: 0.00115460
Iteration 24/25 | Loss: 0.00115515
Iteration 25/25 | Loss: 0.00115005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21303034
Iteration 2/25 | Loss: 0.00279968
Iteration 3/25 | Loss: 0.00279968
Iteration 4/25 | Loss: 0.00279968
Iteration 5/25 | Loss: 0.00279968
Iteration 6/25 | Loss: 0.00279968
Iteration 7/25 | Loss: 0.00279968
Iteration 8/25 | Loss: 0.00279968
Iteration 9/25 | Loss: 0.00279968
Iteration 10/25 | Loss: 0.00279968
Iteration 11/25 | Loss: 0.00279968
Iteration 12/25 | Loss: 0.00279968
Iteration 13/25 | Loss: 0.00279968
Iteration 14/25 | Loss: 0.00279968
Iteration 15/25 | Loss: 0.00279968
Iteration 16/25 | Loss: 0.00279968
Iteration 17/25 | Loss: 0.00279968
Iteration 18/25 | Loss: 0.00279968
Iteration 19/25 | Loss: 0.00279968
Iteration 20/25 | Loss: 0.00279968
Iteration 21/25 | Loss: 0.00279968
Iteration 22/25 | Loss: 0.00279968
Iteration 23/25 | Loss: 0.00279968
Iteration 24/25 | Loss: 0.00279968
Iteration 25/25 | Loss: 0.00279968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279968
Iteration 2/1000 | Loss: 0.00010909
Iteration 3/1000 | Loss: 0.00037659
Iteration 4/1000 | Loss: 0.00006815
Iteration 5/1000 | Loss: 0.00005132
Iteration 6/1000 | Loss: 0.00004355
Iteration 7/1000 | Loss: 0.00099883
Iteration 8/1000 | Loss: 0.00066496
Iteration 9/1000 | Loss: 0.00006120
Iteration 10/1000 | Loss: 0.00004195
Iteration 11/1000 | Loss: 0.00003765
Iteration 12/1000 | Loss: 0.00015022
Iteration 13/1000 | Loss: 0.00003738
Iteration 14/1000 | Loss: 0.00002733
Iteration 15/1000 | Loss: 0.00002348
Iteration 16/1000 | Loss: 0.00002112
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001520
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001514
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001513
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001510
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001506
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001505
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001500
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001497
Iteration 98/1000 | Loss: 0.00001497
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001495
Iteration 104/1000 | Loss: 0.00001495
Iteration 105/1000 | Loss: 0.00001495
Iteration 106/1000 | Loss: 0.00001495
Iteration 107/1000 | Loss: 0.00001495
Iteration 108/1000 | Loss: 0.00001494
Iteration 109/1000 | Loss: 0.00001494
Iteration 110/1000 | Loss: 0.00001494
Iteration 111/1000 | Loss: 0.00001494
Iteration 112/1000 | Loss: 0.00001494
Iteration 113/1000 | Loss: 0.00001493
Iteration 114/1000 | Loss: 0.00001493
Iteration 115/1000 | Loss: 0.00001493
Iteration 116/1000 | Loss: 0.00001492
Iteration 117/1000 | Loss: 0.00001492
Iteration 118/1000 | Loss: 0.00001492
Iteration 119/1000 | Loss: 0.00001491
Iteration 120/1000 | Loss: 0.00001491
Iteration 121/1000 | Loss: 0.00001491
Iteration 122/1000 | Loss: 0.00001491
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Iteration 129/1000 | Loss: 0.00001490
Iteration 130/1000 | Loss: 0.00001490
Iteration 131/1000 | Loss: 0.00001490
Iteration 132/1000 | Loss: 0.00001489
Iteration 133/1000 | Loss: 0.00001489
Iteration 134/1000 | Loss: 0.00001489
Iteration 135/1000 | Loss: 0.00001489
Iteration 136/1000 | Loss: 0.00001488
Iteration 137/1000 | Loss: 0.00001488
Iteration 138/1000 | Loss: 0.00001488
Iteration 139/1000 | Loss: 0.00001488
Iteration 140/1000 | Loss: 0.00001488
Iteration 141/1000 | Loss: 0.00001488
Iteration 142/1000 | Loss: 0.00001488
Iteration 143/1000 | Loss: 0.00001488
Iteration 144/1000 | Loss: 0.00001488
Iteration 145/1000 | Loss: 0.00001488
Iteration 146/1000 | Loss: 0.00001488
Iteration 147/1000 | Loss: 0.00001488
Iteration 148/1000 | Loss: 0.00001488
Iteration 149/1000 | Loss: 0.00001487
Iteration 150/1000 | Loss: 0.00001487
Iteration 151/1000 | Loss: 0.00001487
Iteration 152/1000 | Loss: 0.00001487
Iteration 153/1000 | Loss: 0.00001487
Iteration 154/1000 | Loss: 0.00001487
Iteration 155/1000 | Loss: 0.00001487
Iteration 156/1000 | Loss: 0.00001487
Iteration 157/1000 | Loss: 0.00001487
Iteration 158/1000 | Loss: 0.00001487
Iteration 159/1000 | Loss: 0.00001487
Iteration 160/1000 | Loss: 0.00001487
Iteration 161/1000 | Loss: 0.00001487
Iteration 162/1000 | Loss: 0.00001487
Iteration 163/1000 | Loss: 0.00001487
Iteration 164/1000 | Loss: 0.00001487
Iteration 165/1000 | Loss: 0.00001486
Iteration 166/1000 | Loss: 0.00001486
Iteration 167/1000 | Loss: 0.00001486
Iteration 168/1000 | Loss: 0.00001486
Iteration 169/1000 | Loss: 0.00001486
Iteration 170/1000 | Loss: 0.00001486
Iteration 171/1000 | Loss: 0.00001486
Iteration 172/1000 | Loss: 0.00001486
Iteration 173/1000 | Loss: 0.00001486
Iteration 174/1000 | Loss: 0.00001486
Iteration 175/1000 | Loss: 0.00001486
Iteration 176/1000 | Loss: 0.00001486
Iteration 177/1000 | Loss: 0.00001486
Iteration 178/1000 | Loss: 0.00001486
Iteration 179/1000 | Loss: 0.00001485
Iteration 180/1000 | Loss: 0.00001485
Iteration 181/1000 | Loss: 0.00001485
Iteration 182/1000 | Loss: 0.00001485
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001485
Iteration 185/1000 | Loss: 0.00001485
Iteration 186/1000 | Loss: 0.00001485
Iteration 187/1000 | Loss: 0.00001485
Iteration 188/1000 | Loss: 0.00001485
Iteration 189/1000 | Loss: 0.00001485
Iteration 190/1000 | Loss: 0.00001485
Iteration 191/1000 | Loss: 0.00001485
Iteration 192/1000 | Loss: 0.00001485
Iteration 193/1000 | Loss: 0.00001485
Iteration 194/1000 | Loss: 0.00001485
Iteration 195/1000 | Loss: 0.00001485
Iteration 196/1000 | Loss: 0.00001485
Iteration 197/1000 | Loss: 0.00001485
Iteration 198/1000 | Loss: 0.00001485
Iteration 199/1000 | Loss: 0.00001485
Iteration 200/1000 | Loss: 0.00001485
Iteration 201/1000 | Loss: 0.00001485
Iteration 202/1000 | Loss: 0.00001485
Iteration 203/1000 | Loss: 0.00001485
Iteration 204/1000 | Loss: 0.00001485
Iteration 205/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.4845265468466096e-05, 1.4845265468466096e-05, 1.4845265468466096e-05, 1.4845265468466096e-05, 1.4845265468466096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4845265468466096e-05

Optimization complete. Final v2v error: 3.349539279937744 mm

Highest mean error: 3.8641014099121094 mm for frame 46

Lowest mean error: 3.0886106491088867 mm for frame 115

Saving results

Total time: 95.63956880569458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_us_2802/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_us_2802/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800962
Iteration 2/25 | Loss: 0.00139896
Iteration 3/25 | Loss: 0.00112283
Iteration 4/25 | Loss: 0.00110668
Iteration 5/25 | Loss: 0.00110468
Iteration 6/25 | Loss: 0.00110468
Iteration 7/25 | Loss: 0.00110468
Iteration 8/25 | Loss: 0.00110468
Iteration 9/25 | Loss: 0.00110468
Iteration 10/25 | Loss: 0.00110468
Iteration 11/25 | Loss: 0.00110468
Iteration 12/25 | Loss: 0.00110468
Iteration 13/25 | Loss: 0.00110468
Iteration 14/25 | Loss: 0.00110468
Iteration 15/25 | Loss: 0.00110468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001104682101868093, 0.001104682101868093, 0.001104682101868093, 0.001104682101868093, 0.001104682101868093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001104682101868093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12312245
Iteration 2/25 | Loss: 0.00273118
Iteration 3/25 | Loss: 0.00273118
Iteration 4/25 | Loss: 0.00273117
Iteration 5/25 | Loss: 0.00273117
Iteration 6/25 | Loss: 0.00273117
Iteration 7/25 | Loss: 0.00273117
Iteration 8/25 | Loss: 0.00273117
Iteration 9/25 | Loss: 0.00273117
Iteration 10/25 | Loss: 0.00273117
Iteration 11/25 | Loss: 0.00273117
Iteration 12/25 | Loss: 0.00273117
Iteration 13/25 | Loss: 0.00273117
Iteration 14/25 | Loss: 0.00273117
Iteration 15/25 | Loss: 0.00273117
Iteration 16/25 | Loss: 0.00273117
Iteration 17/25 | Loss: 0.00273117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002731172600761056, 0.002731172600761056, 0.002731172600761056, 0.002731172600761056, 0.002731172600761056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002731172600761056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273117
Iteration 2/1000 | Loss: 0.00003348
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00001981
Iteration 5/1000 | Loss: 0.00001813
Iteration 6/1000 | Loss: 0.00001735
Iteration 7/1000 | Loss: 0.00001678
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001500
Iteration 17/1000 | Loss: 0.00001498
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001482
Iteration 22/1000 | Loss: 0.00001481
Iteration 23/1000 | Loss: 0.00001480
Iteration 24/1000 | Loss: 0.00001480
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001477
Iteration 30/1000 | Loss: 0.00001477
Iteration 31/1000 | Loss: 0.00001477
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001461
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001460
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001457
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001456
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.4532603017869405e-05, 1.4532603017869405e-05, 1.4532603017869405e-05, 1.4532603017869405e-05, 1.4532603017869405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4532603017869405e-05

Optimization complete. Final v2v error: 3.282097339630127 mm

Highest mean error: 3.536968946456909 mm for frame 86

Lowest mean error: 2.7383923530578613 mm for frame 24

Saving results

Total time: 40.2512948513031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00653628
Iteration 2/25 | Loss: 0.00166094
Iteration 3/25 | Loss: 0.00142417
Iteration 4/25 | Loss: 0.00139216
Iteration 5/25 | Loss: 0.00138444
Iteration 6/25 | Loss: 0.00138256
Iteration 7/25 | Loss: 0.00138199
Iteration 8/25 | Loss: 0.00138197
Iteration 9/25 | Loss: 0.00138197
Iteration 10/25 | Loss: 0.00138197
Iteration 11/25 | Loss: 0.00138197
Iteration 12/25 | Loss: 0.00138197
Iteration 13/25 | Loss: 0.00138197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013819707091897726, 0.0013819707091897726, 0.0013819707091897726, 0.0013819707091897726, 0.0013819707091897726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013819707091897726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27077985
Iteration 2/25 | Loss: 0.00242480
Iteration 3/25 | Loss: 0.00242477
Iteration 4/25 | Loss: 0.00242477
Iteration 5/25 | Loss: 0.00242477
Iteration 6/25 | Loss: 0.00242477
Iteration 7/25 | Loss: 0.00242477
Iteration 8/25 | Loss: 0.00242477
Iteration 9/25 | Loss: 0.00242477
Iteration 10/25 | Loss: 0.00242477
Iteration 11/25 | Loss: 0.00242477
Iteration 12/25 | Loss: 0.00242477
Iteration 13/25 | Loss: 0.00242477
Iteration 14/25 | Loss: 0.00242477
Iteration 15/25 | Loss: 0.00242477
Iteration 16/25 | Loss: 0.00242477
Iteration 17/25 | Loss: 0.00242477
Iteration 18/25 | Loss: 0.00242477
Iteration 19/25 | Loss: 0.00242477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002424767939373851, 0.002424767939373851, 0.002424767939373851, 0.002424767939373851, 0.002424767939373851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002424767939373851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242477
Iteration 2/1000 | Loss: 0.00005018
Iteration 3/1000 | Loss: 0.00003573
Iteration 4/1000 | Loss: 0.00002993
Iteration 5/1000 | Loss: 0.00002718
Iteration 6/1000 | Loss: 0.00002605
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002454
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002334
Iteration 12/1000 | Loss: 0.00002316
Iteration 13/1000 | Loss: 0.00002298
Iteration 14/1000 | Loss: 0.00002288
Iteration 15/1000 | Loss: 0.00002283
Iteration 16/1000 | Loss: 0.00002281
Iteration 17/1000 | Loss: 0.00002277
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002272
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002270
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002266
Iteration 26/1000 | Loss: 0.00002264
Iteration 27/1000 | Loss: 0.00002261
Iteration 28/1000 | Loss: 0.00002259
Iteration 29/1000 | Loss: 0.00002259
Iteration 30/1000 | Loss: 0.00002259
Iteration 31/1000 | Loss: 0.00002258
Iteration 32/1000 | Loss: 0.00002258
Iteration 33/1000 | Loss: 0.00002257
Iteration 34/1000 | Loss: 0.00002257
Iteration 35/1000 | Loss: 0.00002256
Iteration 36/1000 | Loss: 0.00002256
Iteration 37/1000 | Loss: 0.00002256
Iteration 38/1000 | Loss: 0.00002256
Iteration 39/1000 | Loss: 0.00002255
Iteration 40/1000 | Loss: 0.00002255
Iteration 41/1000 | Loss: 0.00002255
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002254
Iteration 44/1000 | Loss: 0.00002254
Iteration 45/1000 | Loss: 0.00002253
Iteration 46/1000 | Loss: 0.00002253
Iteration 47/1000 | Loss: 0.00002253
Iteration 48/1000 | Loss: 0.00002253
Iteration 49/1000 | Loss: 0.00002252
Iteration 50/1000 | Loss: 0.00002252
Iteration 51/1000 | Loss: 0.00002252
Iteration 52/1000 | Loss: 0.00002252
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002251
Iteration 55/1000 | Loss: 0.00002251
Iteration 56/1000 | Loss: 0.00002251
Iteration 57/1000 | Loss: 0.00002250
Iteration 58/1000 | Loss: 0.00002250
Iteration 59/1000 | Loss: 0.00002250
Iteration 60/1000 | Loss: 0.00002250
Iteration 61/1000 | Loss: 0.00002250
Iteration 62/1000 | Loss: 0.00002250
Iteration 63/1000 | Loss: 0.00002250
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002249
Iteration 66/1000 | Loss: 0.00002249
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002249
Iteration 70/1000 | Loss: 0.00002248
Iteration 71/1000 | Loss: 0.00002248
Iteration 72/1000 | Loss: 0.00002248
Iteration 73/1000 | Loss: 0.00002247
Iteration 74/1000 | Loss: 0.00002247
Iteration 75/1000 | Loss: 0.00002247
Iteration 76/1000 | Loss: 0.00002247
Iteration 77/1000 | Loss: 0.00002247
Iteration 78/1000 | Loss: 0.00002247
Iteration 79/1000 | Loss: 0.00002247
Iteration 80/1000 | Loss: 0.00002247
Iteration 81/1000 | Loss: 0.00002247
Iteration 82/1000 | Loss: 0.00002247
Iteration 83/1000 | Loss: 0.00002247
Iteration 84/1000 | Loss: 0.00002246
Iteration 85/1000 | Loss: 0.00002246
Iteration 86/1000 | Loss: 0.00002246
Iteration 87/1000 | Loss: 0.00002246
Iteration 88/1000 | Loss: 0.00002246
Iteration 89/1000 | Loss: 0.00002246
Iteration 90/1000 | Loss: 0.00002246
Iteration 91/1000 | Loss: 0.00002246
Iteration 92/1000 | Loss: 0.00002246
Iteration 93/1000 | Loss: 0.00002245
Iteration 94/1000 | Loss: 0.00002245
Iteration 95/1000 | Loss: 0.00002245
Iteration 96/1000 | Loss: 0.00002245
Iteration 97/1000 | Loss: 0.00002245
Iteration 98/1000 | Loss: 0.00002245
Iteration 99/1000 | Loss: 0.00002245
Iteration 100/1000 | Loss: 0.00002245
Iteration 101/1000 | Loss: 0.00002245
Iteration 102/1000 | Loss: 0.00002245
Iteration 103/1000 | Loss: 0.00002245
Iteration 104/1000 | Loss: 0.00002245
Iteration 105/1000 | Loss: 0.00002245
Iteration 106/1000 | Loss: 0.00002244
Iteration 107/1000 | Loss: 0.00002244
Iteration 108/1000 | Loss: 0.00002244
Iteration 109/1000 | Loss: 0.00002244
Iteration 110/1000 | Loss: 0.00002244
Iteration 111/1000 | Loss: 0.00002244
Iteration 112/1000 | Loss: 0.00002244
Iteration 113/1000 | Loss: 0.00002244
Iteration 114/1000 | Loss: 0.00002244
Iteration 115/1000 | Loss: 0.00002244
Iteration 116/1000 | Loss: 0.00002243
Iteration 117/1000 | Loss: 0.00002243
Iteration 118/1000 | Loss: 0.00002243
Iteration 119/1000 | Loss: 0.00002243
Iteration 120/1000 | Loss: 0.00002243
Iteration 121/1000 | Loss: 0.00002243
Iteration 122/1000 | Loss: 0.00002243
Iteration 123/1000 | Loss: 0.00002243
Iteration 124/1000 | Loss: 0.00002243
Iteration 125/1000 | Loss: 0.00002243
Iteration 126/1000 | Loss: 0.00002243
Iteration 127/1000 | Loss: 0.00002243
Iteration 128/1000 | Loss: 0.00002243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.2434138372773305e-05, 2.2434138372773305e-05, 2.2434138372773305e-05, 2.2434138372773305e-05, 2.2434138372773305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2434138372773305e-05

Optimization complete. Final v2v error: 4.1074628829956055 mm

Highest mean error: 4.910079479217529 mm for frame 88

Lowest mean error: 3.6440017223358154 mm for frame 131

Saving results

Total time: 40.20260572433472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507515
Iteration 2/25 | Loss: 0.00145485
Iteration 3/25 | Loss: 0.00134732
Iteration 4/25 | Loss: 0.00133438
Iteration 5/25 | Loss: 0.00132899
Iteration 6/25 | Loss: 0.00132813
Iteration 7/25 | Loss: 0.00132813
Iteration 8/25 | Loss: 0.00132813
Iteration 9/25 | Loss: 0.00132813
Iteration 10/25 | Loss: 0.00132813
Iteration 11/25 | Loss: 0.00132813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013281309511512518, 0.0013281309511512518, 0.0013281309511512518, 0.0013281309511512518, 0.0013281309511512518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013281309511512518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55232501
Iteration 2/25 | Loss: 0.00221902
Iteration 3/25 | Loss: 0.00221902
Iteration 4/25 | Loss: 0.00221902
Iteration 5/25 | Loss: 0.00221902
Iteration 6/25 | Loss: 0.00221902
Iteration 7/25 | Loss: 0.00221902
Iteration 8/25 | Loss: 0.00221902
Iteration 9/25 | Loss: 0.00221902
Iteration 10/25 | Loss: 0.00221902
Iteration 11/25 | Loss: 0.00221902
Iteration 12/25 | Loss: 0.00221902
Iteration 13/25 | Loss: 0.00221902
Iteration 14/25 | Loss: 0.00221902
Iteration 15/25 | Loss: 0.00221902
Iteration 16/25 | Loss: 0.00221902
Iteration 17/25 | Loss: 0.00221902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022190192248672247, 0.0022190192248672247, 0.0022190192248672247, 0.0022190192248672247, 0.0022190192248672247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022190192248672247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221902
Iteration 2/1000 | Loss: 0.00002682
Iteration 3/1000 | Loss: 0.00002334
Iteration 4/1000 | Loss: 0.00002173
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00002047
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001995
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001994
Iteration 12/1000 | Loss: 0.00001994
Iteration 13/1000 | Loss: 0.00001993
Iteration 14/1000 | Loss: 0.00001993
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001988
Iteration 17/1000 | Loss: 0.00001987
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001981
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001981
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001979
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001977
Iteration 28/1000 | Loss: 0.00001977
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001976
Iteration 31/1000 | Loss: 0.00001976
Iteration 32/1000 | Loss: 0.00001976
Iteration 33/1000 | Loss: 0.00001976
Iteration 34/1000 | Loss: 0.00001976
Iteration 35/1000 | Loss: 0.00001975
Iteration 36/1000 | Loss: 0.00001975
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001975
Iteration 39/1000 | Loss: 0.00001975
Iteration 40/1000 | Loss: 0.00001975
Iteration 41/1000 | Loss: 0.00001974
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001974
Iteration 45/1000 | Loss: 0.00001974
Iteration 46/1000 | Loss: 0.00001974
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001973
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001971
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001971
Iteration 60/1000 | Loss: 0.00001971
Iteration 61/1000 | Loss: 0.00001971
Iteration 62/1000 | Loss: 0.00001971
Iteration 63/1000 | Loss: 0.00001971
Iteration 64/1000 | Loss: 0.00001971
Iteration 65/1000 | Loss: 0.00001971
Iteration 66/1000 | Loss: 0.00001971
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001971
Iteration 70/1000 | Loss: 0.00001971
Iteration 71/1000 | Loss: 0.00001971
Iteration 72/1000 | Loss: 0.00001971
Iteration 73/1000 | Loss: 0.00001971
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001971
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001971
Iteration 84/1000 | Loss: 0.00001971
Iteration 85/1000 | Loss: 0.00001971
Iteration 86/1000 | Loss: 0.00001971
Iteration 87/1000 | Loss: 0.00001971
Iteration 88/1000 | Loss: 0.00001971
Iteration 89/1000 | Loss: 0.00001971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.9707196770468727e-05, 1.9707196770468727e-05, 1.9707196770468727e-05, 1.9707196770468727e-05, 1.9707196770468727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9707196770468727e-05

Optimization complete. Final v2v error: 3.9491019248962402 mm

Highest mean error: 4.174888610839844 mm for frame 36

Lowest mean error: 3.7865140438079834 mm for frame 114

Saving results

Total time: 29.133745193481445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928173
Iteration 2/25 | Loss: 0.00175634
Iteration 3/25 | Loss: 0.00153359
Iteration 4/25 | Loss: 0.00147086
Iteration 5/25 | Loss: 0.00145303
Iteration 6/25 | Loss: 0.00145079
Iteration 7/25 | Loss: 0.00145079
Iteration 8/25 | Loss: 0.00145079
Iteration 9/25 | Loss: 0.00145079
Iteration 10/25 | Loss: 0.00145079
Iteration 11/25 | Loss: 0.00145079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014507854357361794, 0.0014507854357361794, 0.0014507854357361794, 0.0014507854357361794, 0.0014507854357361794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014507854357361794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58532333
Iteration 2/25 | Loss: 0.00250244
Iteration 3/25 | Loss: 0.00250244
Iteration 4/25 | Loss: 0.00250244
Iteration 5/25 | Loss: 0.00250244
Iteration 6/25 | Loss: 0.00250244
Iteration 7/25 | Loss: 0.00250244
Iteration 8/25 | Loss: 0.00250244
Iteration 9/25 | Loss: 0.00250244
Iteration 10/25 | Loss: 0.00250244
Iteration 11/25 | Loss: 0.00250244
Iteration 12/25 | Loss: 0.00250244
Iteration 13/25 | Loss: 0.00250244
Iteration 14/25 | Loss: 0.00250244
Iteration 15/25 | Loss: 0.00250244
Iteration 16/25 | Loss: 0.00250244
Iteration 17/25 | Loss: 0.00250244
Iteration 18/25 | Loss: 0.00250244
Iteration 19/25 | Loss: 0.00250244
Iteration 20/25 | Loss: 0.00250244
Iteration 21/25 | Loss: 0.00250244
Iteration 22/25 | Loss: 0.00250244
Iteration 23/25 | Loss: 0.00250244
Iteration 24/25 | Loss: 0.00250244
Iteration 25/25 | Loss: 0.00250244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250244
Iteration 2/1000 | Loss: 0.00005429
Iteration 3/1000 | Loss: 0.00003896
Iteration 4/1000 | Loss: 0.00003656
Iteration 5/1000 | Loss: 0.00003516
Iteration 6/1000 | Loss: 0.00003399
Iteration 7/1000 | Loss: 0.00003320
Iteration 8/1000 | Loss: 0.00003253
Iteration 9/1000 | Loss: 0.00003219
Iteration 10/1000 | Loss: 0.00003191
Iteration 11/1000 | Loss: 0.00003171
Iteration 12/1000 | Loss: 0.00003163
Iteration 13/1000 | Loss: 0.00003162
Iteration 14/1000 | Loss: 0.00003161
Iteration 15/1000 | Loss: 0.00003161
Iteration 16/1000 | Loss: 0.00003161
Iteration 17/1000 | Loss: 0.00003159
Iteration 18/1000 | Loss: 0.00003159
Iteration 19/1000 | Loss: 0.00003158
Iteration 20/1000 | Loss: 0.00003157
Iteration 21/1000 | Loss: 0.00003157
Iteration 22/1000 | Loss: 0.00003156
Iteration 23/1000 | Loss: 0.00003156
Iteration 24/1000 | Loss: 0.00003155
Iteration 25/1000 | Loss: 0.00003154
Iteration 26/1000 | Loss: 0.00003154
Iteration 27/1000 | Loss: 0.00003154
Iteration 28/1000 | Loss: 0.00003154
Iteration 29/1000 | Loss: 0.00003153
Iteration 30/1000 | Loss: 0.00003153
Iteration 31/1000 | Loss: 0.00003153
Iteration 32/1000 | Loss: 0.00003153
Iteration 33/1000 | Loss: 0.00003153
Iteration 34/1000 | Loss: 0.00003153
Iteration 35/1000 | Loss: 0.00003153
Iteration 36/1000 | Loss: 0.00003153
Iteration 37/1000 | Loss: 0.00003153
Iteration 38/1000 | Loss: 0.00003153
Iteration 39/1000 | Loss: 0.00003153
Iteration 40/1000 | Loss: 0.00003153
Iteration 41/1000 | Loss: 0.00003152
Iteration 42/1000 | Loss: 0.00003151
Iteration 43/1000 | Loss: 0.00003151
Iteration 44/1000 | Loss: 0.00003151
Iteration 45/1000 | Loss: 0.00003150
Iteration 46/1000 | Loss: 0.00003150
Iteration 47/1000 | Loss: 0.00003150
Iteration 48/1000 | Loss: 0.00003150
Iteration 49/1000 | Loss: 0.00003150
Iteration 50/1000 | Loss: 0.00003150
Iteration 51/1000 | Loss: 0.00003150
Iteration 52/1000 | Loss: 0.00003150
Iteration 53/1000 | Loss: 0.00003150
Iteration 54/1000 | Loss: 0.00003150
Iteration 55/1000 | Loss: 0.00003150
Iteration 56/1000 | Loss: 0.00003149
Iteration 57/1000 | Loss: 0.00003149
Iteration 58/1000 | Loss: 0.00003148
Iteration 59/1000 | Loss: 0.00003148
Iteration 60/1000 | Loss: 0.00003148
Iteration 61/1000 | Loss: 0.00003148
Iteration 62/1000 | Loss: 0.00003148
Iteration 63/1000 | Loss: 0.00003147
Iteration 64/1000 | Loss: 0.00003147
Iteration 65/1000 | Loss: 0.00003147
Iteration 66/1000 | Loss: 0.00003147
Iteration 67/1000 | Loss: 0.00003147
Iteration 68/1000 | Loss: 0.00003147
Iteration 69/1000 | Loss: 0.00003147
Iteration 70/1000 | Loss: 0.00003147
Iteration 71/1000 | Loss: 0.00003147
Iteration 72/1000 | Loss: 0.00003147
Iteration 73/1000 | Loss: 0.00003147
Iteration 74/1000 | Loss: 0.00003147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [3.14723038172815e-05, 3.14723038172815e-05, 3.14723038172815e-05, 3.14723038172815e-05, 3.14723038172815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.14723038172815e-05

Optimization complete. Final v2v error: 4.742870807647705 mm

Highest mean error: 5.2898454666137695 mm for frame 207

Lowest mean error: 4.3946452140808105 mm for frame 110

Saving results

Total time: 31.924700021743774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455762
Iteration 2/25 | Loss: 0.00143430
Iteration 3/25 | Loss: 0.00134899
Iteration 4/25 | Loss: 0.00133659
Iteration 5/25 | Loss: 0.00133321
Iteration 6/25 | Loss: 0.00133241
Iteration 7/25 | Loss: 0.00133236
Iteration 8/25 | Loss: 0.00133236
Iteration 9/25 | Loss: 0.00133236
Iteration 10/25 | Loss: 0.00133236
Iteration 11/25 | Loss: 0.00133236
Iteration 12/25 | Loss: 0.00133236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00133236194960773, 0.00133236194960773, 0.00133236194960773, 0.00133236194960773, 0.00133236194960773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133236194960773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69699883
Iteration 2/25 | Loss: 0.00216554
Iteration 3/25 | Loss: 0.00216554
Iteration 4/25 | Loss: 0.00216554
Iteration 5/25 | Loss: 0.00216554
Iteration 6/25 | Loss: 0.00216554
Iteration 7/25 | Loss: 0.00216554
Iteration 8/25 | Loss: 0.00216554
Iteration 9/25 | Loss: 0.00216554
Iteration 10/25 | Loss: 0.00216554
Iteration 11/25 | Loss: 0.00216554
Iteration 12/25 | Loss: 0.00216554
Iteration 13/25 | Loss: 0.00216554
Iteration 14/25 | Loss: 0.00216554
Iteration 15/25 | Loss: 0.00216554
Iteration 16/25 | Loss: 0.00216554
Iteration 17/25 | Loss: 0.00216554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021655368618667126, 0.0021655368618667126, 0.0021655368618667126, 0.0021655368618667126, 0.0021655368618667126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021655368618667126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216554
Iteration 2/1000 | Loss: 0.00003713
Iteration 3/1000 | Loss: 0.00002982
Iteration 4/1000 | Loss: 0.00002699
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00002449
Iteration 7/1000 | Loss: 0.00002385
Iteration 8/1000 | Loss: 0.00002332
Iteration 9/1000 | Loss: 0.00002306
Iteration 10/1000 | Loss: 0.00002298
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002279
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002266
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00002263
Iteration 17/1000 | Loss: 0.00002263
Iteration 18/1000 | Loss: 0.00002262
Iteration 19/1000 | Loss: 0.00002262
Iteration 20/1000 | Loss: 0.00002262
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002261
Iteration 23/1000 | Loss: 0.00002261
Iteration 24/1000 | Loss: 0.00002260
Iteration 25/1000 | Loss: 0.00002260
Iteration 26/1000 | Loss: 0.00002259
Iteration 27/1000 | Loss: 0.00002259
Iteration 28/1000 | Loss: 0.00002258
Iteration 29/1000 | Loss: 0.00002257
Iteration 30/1000 | Loss: 0.00002257
Iteration 31/1000 | Loss: 0.00002257
Iteration 32/1000 | Loss: 0.00002256
Iteration 33/1000 | Loss: 0.00002256
Iteration 34/1000 | Loss: 0.00002256
Iteration 35/1000 | Loss: 0.00002256
Iteration 36/1000 | Loss: 0.00002255
Iteration 37/1000 | Loss: 0.00002255
Iteration 38/1000 | Loss: 0.00002255
Iteration 39/1000 | Loss: 0.00002254
Iteration 40/1000 | Loss: 0.00002254
Iteration 41/1000 | Loss: 0.00002254
Iteration 42/1000 | Loss: 0.00002254
Iteration 43/1000 | Loss: 0.00002254
Iteration 44/1000 | Loss: 0.00002254
Iteration 45/1000 | Loss: 0.00002254
Iteration 46/1000 | Loss: 0.00002253
Iteration 47/1000 | Loss: 0.00002253
Iteration 48/1000 | Loss: 0.00002252
Iteration 49/1000 | Loss: 0.00002252
Iteration 50/1000 | Loss: 0.00002252
Iteration 51/1000 | Loss: 0.00002251
Iteration 52/1000 | Loss: 0.00002251
Iteration 53/1000 | Loss: 0.00002251
Iteration 54/1000 | Loss: 0.00002251
Iteration 55/1000 | Loss: 0.00002250
Iteration 56/1000 | Loss: 0.00002250
Iteration 57/1000 | Loss: 0.00002249
Iteration 58/1000 | Loss: 0.00002249
Iteration 59/1000 | Loss: 0.00002249
Iteration 60/1000 | Loss: 0.00002249
Iteration 61/1000 | Loss: 0.00002249
Iteration 62/1000 | Loss: 0.00002249
Iteration 63/1000 | Loss: 0.00002249
Iteration 64/1000 | Loss: 0.00002249
Iteration 65/1000 | Loss: 0.00002249
Iteration 66/1000 | Loss: 0.00002249
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002248
Iteration 69/1000 | Loss: 0.00002248
Iteration 70/1000 | Loss: 0.00002248
Iteration 71/1000 | Loss: 0.00002248
Iteration 72/1000 | Loss: 0.00002248
Iteration 73/1000 | Loss: 0.00002248
Iteration 74/1000 | Loss: 0.00002248
Iteration 75/1000 | Loss: 0.00002248
Iteration 76/1000 | Loss: 0.00002248
Iteration 77/1000 | Loss: 0.00002248
Iteration 78/1000 | Loss: 0.00002247
Iteration 79/1000 | Loss: 0.00002247
Iteration 80/1000 | Loss: 0.00002247
Iteration 81/1000 | Loss: 0.00002247
Iteration 82/1000 | Loss: 0.00002247
Iteration 83/1000 | Loss: 0.00002247
Iteration 84/1000 | Loss: 0.00002247
Iteration 85/1000 | Loss: 0.00002247
Iteration 86/1000 | Loss: 0.00002247
Iteration 87/1000 | Loss: 0.00002247
Iteration 88/1000 | Loss: 0.00002247
Iteration 89/1000 | Loss: 0.00002246
Iteration 90/1000 | Loss: 0.00002246
Iteration 91/1000 | Loss: 0.00002246
Iteration 92/1000 | Loss: 0.00002246
Iteration 93/1000 | Loss: 0.00002246
Iteration 94/1000 | Loss: 0.00002246
Iteration 95/1000 | Loss: 0.00002246
Iteration 96/1000 | Loss: 0.00002246
Iteration 97/1000 | Loss: 0.00002246
Iteration 98/1000 | Loss: 0.00002246
Iteration 99/1000 | Loss: 0.00002246
Iteration 100/1000 | Loss: 0.00002246
Iteration 101/1000 | Loss: 0.00002245
Iteration 102/1000 | Loss: 0.00002245
Iteration 103/1000 | Loss: 0.00002245
Iteration 104/1000 | Loss: 0.00002245
Iteration 105/1000 | Loss: 0.00002245
Iteration 106/1000 | Loss: 0.00002245
Iteration 107/1000 | Loss: 0.00002245
Iteration 108/1000 | Loss: 0.00002245
Iteration 109/1000 | Loss: 0.00002245
Iteration 110/1000 | Loss: 0.00002245
Iteration 111/1000 | Loss: 0.00002245
Iteration 112/1000 | Loss: 0.00002245
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002245
Iteration 117/1000 | Loss: 0.00002245
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002245
Iteration 120/1000 | Loss: 0.00002245
Iteration 121/1000 | Loss: 0.00002245
Iteration 122/1000 | Loss: 0.00002245
Iteration 123/1000 | Loss: 0.00002245
Iteration 124/1000 | Loss: 0.00002245
Iteration 125/1000 | Loss: 0.00002245
Iteration 126/1000 | Loss: 0.00002245
Iteration 127/1000 | Loss: 0.00002245
Iteration 128/1000 | Loss: 0.00002245
Iteration 129/1000 | Loss: 0.00002245
Iteration 130/1000 | Loss: 0.00002245
Iteration 131/1000 | Loss: 0.00002245
Iteration 132/1000 | Loss: 0.00002245
Iteration 133/1000 | Loss: 0.00002245
Iteration 134/1000 | Loss: 0.00002245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.2448562958743423e-05, 2.2448562958743423e-05, 2.2448562958743423e-05, 2.2448562958743423e-05, 2.2448562958743423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2448562958743423e-05

Optimization complete. Final v2v error: 4.158077239990234 mm

Highest mean error: 4.574108123779297 mm for frame 78

Lowest mean error: 3.9701592922210693 mm for frame 24

Saving results

Total time: 33.021026372909546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023806
Iteration 2/25 | Loss: 0.00194426
Iteration 3/25 | Loss: 0.00151221
Iteration 4/25 | Loss: 0.00147401
Iteration 5/25 | Loss: 0.00145735
Iteration 6/25 | Loss: 0.00145411
Iteration 7/25 | Loss: 0.00145398
Iteration 8/25 | Loss: 0.00145398
Iteration 9/25 | Loss: 0.00145398
Iteration 10/25 | Loss: 0.00145398
Iteration 11/25 | Loss: 0.00145398
Iteration 12/25 | Loss: 0.00145398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014539769617840648, 0.0014539769617840648, 0.0014539769617840648, 0.0014539769617840648, 0.0014539769617840648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014539769617840648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20738113
Iteration 2/25 | Loss: 0.00202856
Iteration 3/25 | Loss: 0.00202854
Iteration 4/25 | Loss: 0.00202854
Iteration 5/25 | Loss: 0.00202854
Iteration 6/25 | Loss: 0.00202854
Iteration 7/25 | Loss: 0.00202854
Iteration 8/25 | Loss: 0.00202854
Iteration 9/25 | Loss: 0.00202854
Iteration 10/25 | Loss: 0.00202854
Iteration 11/25 | Loss: 0.00202854
Iteration 12/25 | Loss: 0.00202854
Iteration 13/25 | Loss: 0.00202854
Iteration 14/25 | Loss: 0.00202854
Iteration 15/25 | Loss: 0.00202854
Iteration 16/25 | Loss: 0.00202854
Iteration 17/25 | Loss: 0.00202854
Iteration 18/25 | Loss: 0.00202854
Iteration 19/25 | Loss: 0.00202854
Iteration 20/25 | Loss: 0.00202854
Iteration 21/25 | Loss: 0.00202854
Iteration 22/25 | Loss: 0.00202854
Iteration 23/25 | Loss: 0.00202854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002028541173785925, 0.002028541173785925, 0.002028541173785925, 0.002028541173785925, 0.002028541173785925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002028541173785925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202854
Iteration 2/1000 | Loss: 0.00007146
Iteration 3/1000 | Loss: 0.00005526
Iteration 4/1000 | Loss: 0.00004852
Iteration 5/1000 | Loss: 0.00004578
Iteration 6/1000 | Loss: 0.00004373
Iteration 7/1000 | Loss: 0.00004243
Iteration 8/1000 | Loss: 0.00004147
Iteration 9/1000 | Loss: 0.00004090
Iteration 10/1000 | Loss: 0.00004053
Iteration 11/1000 | Loss: 0.00004032
Iteration 12/1000 | Loss: 0.00004028
Iteration 13/1000 | Loss: 0.00004025
Iteration 14/1000 | Loss: 0.00004023
Iteration 15/1000 | Loss: 0.00004016
Iteration 16/1000 | Loss: 0.00004016
Iteration 17/1000 | Loss: 0.00004010
Iteration 18/1000 | Loss: 0.00004007
Iteration 19/1000 | Loss: 0.00004006
Iteration 20/1000 | Loss: 0.00004002
Iteration 21/1000 | Loss: 0.00003998
Iteration 22/1000 | Loss: 0.00003998
Iteration 23/1000 | Loss: 0.00003998
Iteration 24/1000 | Loss: 0.00003997
Iteration 25/1000 | Loss: 0.00003997
Iteration 26/1000 | Loss: 0.00003996
Iteration 27/1000 | Loss: 0.00003994
Iteration 28/1000 | Loss: 0.00003994
Iteration 29/1000 | Loss: 0.00003994
Iteration 30/1000 | Loss: 0.00003994
Iteration 31/1000 | Loss: 0.00003994
Iteration 32/1000 | Loss: 0.00003994
Iteration 33/1000 | Loss: 0.00003994
Iteration 34/1000 | Loss: 0.00003994
Iteration 35/1000 | Loss: 0.00003993
Iteration 36/1000 | Loss: 0.00003993
Iteration 37/1000 | Loss: 0.00003992
Iteration 38/1000 | Loss: 0.00003992
Iteration 39/1000 | Loss: 0.00003992
Iteration 40/1000 | Loss: 0.00003991
Iteration 41/1000 | Loss: 0.00003991
Iteration 42/1000 | Loss: 0.00003991
Iteration 43/1000 | Loss: 0.00003991
Iteration 44/1000 | Loss: 0.00003990
Iteration 45/1000 | Loss: 0.00003990
Iteration 46/1000 | Loss: 0.00003990
Iteration 47/1000 | Loss: 0.00003990
Iteration 48/1000 | Loss: 0.00003989
Iteration 49/1000 | Loss: 0.00003989
Iteration 50/1000 | Loss: 0.00003988
Iteration 51/1000 | Loss: 0.00003988
Iteration 52/1000 | Loss: 0.00003988
Iteration 53/1000 | Loss: 0.00003988
Iteration 54/1000 | Loss: 0.00003988
Iteration 55/1000 | Loss: 0.00003988
Iteration 56/1000 | Loss: 0.00003987
Iteration 57/1000 | Loss: 0.00003987
Iteration 58/1000 | Loss: 0.00003987
Iteration 59/1000 | Loss: 0.00003987
Iteration 60/1000 | Loss: 0.00003987
Iteration 61/1000 | Loss: 0.00003987
Iteration 62/1000 | Loss: 0.00003987
Iteration 63/1000 | Loss: 0.00003986
Iteration 64/1000 | Loss: 0.00003986
Iteration 65/1000 | Loss: 0.00003986
Iteration 66/1000 | Loss: 0.00003986
Iteration 67/1000 | Loss: 0.00003986
Iteration 68/1000 | Loss: 0.00003986
Iteration 69/1000 | Loss: 0.00003985
Iteration 70/1000 | Loss: 0.00003985
Iteration 71/1000 | Loss: 0.00003985
Iteration 72/1000 | Loss: 0.00003985
Iteration 73/1000 | Loss: 0.00003985
Iteration 74/1000 | Loss: 0.00003984
Iteration 75/1000 | Loss: 0.00003984
Iteration 76/1000 | Loss: 0.00003984
Iteration 77/1000 | Loss: 0.00003984
Iteration 78/1000 | Loss: 0.00003984
Iteration 79/1000 | Loss: 0.00003983
Iteration 80/1000 | Loss: 0.00003983
Iteration 81/1000 | Loss: 0.00003983
Iteration 82/1000 | Loss: 0.00003982
Iteration 83/1000 | Loss: 0.00003982
Iteration 84/1000 | Loss: 0.00003982
Iteration 85/1000 | Loss: 0.00003982
Iteration 86/1000 | Loss: 0.00003982
Iteration 87/1000 | Loss: 0.00003982
Iteration 88/1000 | Loss: 0.00003981
Iteration 89/1000 | Loss: 0.00003981
Iteration 90/1000 | Loss: 0.00003981
Iteration 91/1000 | Loss: 0.00003981
Iteration 92/1000 | Loss: 0.00003981
Iteration 93/1000 | Loss: 0.00003981
Iteration 94/1000 | Loss: 0.00003981
Iteration 95/1000 | Loss: 0.00003981
Iteration 96/1000 | Loss: 0.00003981
Iteration 97/1000 | Loss: 0.00003981
Iteration 98/1000 | Loss: 0.00003981
Iteration 99/1000 | Loss: 0.00003981
Iteration 100/1000 | Loss: 0.00003981
Iteration 101/1000 | Loss: 0.00003981
Iteration 102/1000 | Loss: 0.00003980
Iteration 103/1000 | Loss: 0.00003980
Iteration 104/1000 | Loss: 0.00003980
Iteration 105/1000 | Loss: 0.00003980
Iteration 106/1000 | Loss: 0.00003980
Iteration 107/1000 | Loss: 0.00003980
Iteration 108/1000 | Loss: 0.00003979
Iteration 109/1000 | Loss: 0.00003979
Iteration 110/1000 | Loss: 0.00003979
Iteration 111/1000 | Loss: 0.00003979
Iteration 112/1000 | Loss: 0.00003979
Iteration 113/1000 | Loss: 0.00003979
Iteration 114/1000 | Loss: 0.00003979
Iteration 115/1000 | Loss: 0.00003979
Iteration 116/1000 | Loss: 0.00003979
Iteration 117/1000 | Loss: 0.00003979
Iteration 118/1000 | Loss: 0.00003979
Iteration 119/1000 | Loss: 0.00003978
Iteration 120/1000 | Loss: 0.00003978
Iteration 121/1000 | Loss: 0.00003978
Iteration 122/1000 | Loss: 0.00003978
Iteration 123/1000 | Loss: 0.00003978
Iteration 124/1000 | Loss: 0.00003977
Iteration 125/1000 | Loss: 0.00003977
Iteration 126/1000 | Loss: 0.00003977
Iteration 127/1000 | Loss: 0.00003977
Iteration 128/1000 | Loss: 0.00003977
Iteration 129/1000 | Loss: 0.00003977
Iteration 130/1000 | Loss: 0.00003977
Iteration 131/1000 | Loss: 0.00003977
Iteration 132/1000 | Loss: 0.00003977
Iteration 133/1000 | Loss: 0.00003977
Iteration 134/1000 | Loss: 0.00003977
Iteration 135/1000 | Loss: 0.00003977
Iteration 136/1000 | Loss: 0.00003976
Iteration 137/1000 | Loss: 0.00003976
Iteration 138/1000 | Loss: 0.00003976
Iteration 139/1000 | Loss: 0.00003976
Iteration 140/1000 | Loss: 0.00003976
Iteration 141/1000 | Loss: 0.00003976
Iteration 142/1000 | Loss: 0.00003976
Iteration 143/1000 | Loss: 0.00003976
Iteration 144/1000 | Loss: 0.00003976
Iteration 145/1000 | Loss: 0.00003976
Iteration 146/1000 | Loss: 0.00003976
Iteration 147/1000 | Loss: 0.00003976
Iteration 148/1000 | Loss: 0.00003976
Iteration 149/1000 | Loss: 0.00003976
Iteration 150/1000 | Loss: 0.00003976
Iteration 151/1000 | Loss: 0.00003976
Iteration 152/1000 | Loss: 0.00003976
Iteration 153/1000 | Loss: 0.00003976
Iteration 154/1000 | Loss: 0.00003976
Iteration 155/1000 | Loss: 0.00003976
Iteration 156/1000 | Loss: 0.00003976
Iteration 157/1000 | Loss: 0.00003976
Iteration 158/1000 | Loss: 0.00003976
Iteration 159/1000 | Loss: 0.00003976
Iteration 160/1000 | Loss: 0.00003976
Iteration 161/1000 | Loss: 0.00003976
Iteration 162/1000 | Loss: 0.00003976
Iteration 163/1000 | Loss: 0.00003976
Iteration 164/1000 | Loss: 0.00003976
Iteration 165/1000 | Loss: 0.00003976
Iteration 166/1000 | Loss: 0.00003976
Iteration 167/1000 | Loss: 0.00003976
Iteration 168/1000 | Loss: 0.00003976
Iteration 169/1000 | Loss: 0.00003976
Iteration 170/1000 | Loss: 0.00003976
Iteration 171/1000 | Loss: 0.00003976
Iteration 172/1000 | Loss: 0.00003976
Iteration 173/1000 | Loss: 0.00003976
Iteration 174/1000 | Loss: 0.00003976
Iteration 175/1000 | Loss: 0.00003976
Iteration 176/1000 | Loss: 0.00003976
Iteration 177/1000 | Loss: 0.00003976
Iteration 178/1000 | Loss: 0.00003976
Iteration 179/1000 | Loss: 0.00003976
Iteration 180/1000 | Loss: 0.00003976
Iteration 181/1000 | Loss: 0.00003976
Iteration 182/1000 | Loss: 0.00003976
Iteration 183/1000 | Loss: 0.00003976
Iteration 184/1000 | Loss: 0.00003976
Iteration 185/1000 | Loss: 0.00003976
Iteration 186/1000 | Loss: 0.00003976
Iteration 187/1000 | Loss: 0.00003976
Iteration 188/1000 | Loss: 0.00003976
Iteration 189/1000 | Loss: 0.00003976
Iteration 190/1000 | Loss: 0.00003976
Iteration 191/1000 | Loss: 0.00003976
Iteration 192/1000 | Loss: 0.00003976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [3.976172592956573e-05, 3.976172592956573e-05, 3.976172592956573e-05, 3.976172592956573e-05, 3.976172592956573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.976172592956573e-05

Optimization complete. Final v2v error: 5.2800798416137695 mm

Highest mean error: 6.1862053871154785 mm for frame 102

Lowest mean error: 4.401618003845215 mm for frame 121

Saving results

Total time: 39.96151638031006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01140759
Iteration 2/25 | Loss: 0.00436160
Iteration 3/25 | Loss: 0.00296329
Iteration 4/25 | Loss: 0.00205309
Iteration 5/25 | Loss: 0.00181378
Iteration 6/25 | Loss: 0.00171878
Iteration 7/25 | Loss: 0.00153662
Iteration 8/25 | Loss: 0.00147843
Iteration 9/25 | Loss: 0.00143895
Iteration 10/25 | Loss: 0.00140547
Iteration 11/25 | Loss: 0.00138493
Iteration 12/25 | Loss: 0.00138427
Iteration 13/25 | Loss: 0.00138851
Iteration 14/25 | Loss: 0.00138188
Iteration 15/25 | Loss: 0.00136841
Iteration 16/25 | Loss: 0.00135024
Iteration 17/25 | Loss: 0.00135649
Iteration 18/25 | Loss: 0.00135204
Iteration 19/25 | Loss: 0.00134016
Iteration 20/25 | Loss: 0.00133619
Iteration 21/25 | Loss: 0.00133000
Iteration 22/25 | Loss: 0.00132880
Iteration 23/25 | Loss: 0.00132438
Iteration 24/25 | Loss: 0.00132118
Iteration 25/25 | Loss: 0.00132998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66032887
Iteration 2/25 | Loss: 0.00541061
Iteration 3/25 | Loss: 0.00541061
Iteration 4/25 | Loss: 0.00541061
Iteration 5/25 | Loss: 0.00541061
Iteration 6/25 | Loss: 0.00541060
Iteration 7/25 | Loss: 0.00541061
Iteration 8/25 | Loss: 0.00541060
Iteration 9/25 | Loss: 0.00541060
Iteration 10/25 | Loss: 0.00541060
Iteration 11/25 | Loss: 0.00541060
Iteration 12/25 | Loss: 0.00541060
Iteration 13/25 | Loss: 0.00541060
Iteration 14/25 | Loss: 0.00541060
Iteration 15/25 | Loss: 0.00541060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0054106032475829124, 0.0054106032475829124, 0.0054106032475829124, 0.0054106032475829124, 0.0054106032475829124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0054106032475829124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00541060
Iteration 2/1000 | Loss: 0.00088387
Iteration 3/1000 | Loss: 0.00134180
Iteration 4/1000 | Loss: 0.00149912
Iteration 5/1000 | Loss: 0.00079056
Iteration 6/1000 | Loss: 0.00093825
Iteration 7/1000 | Loss: 0.00252423
Iteration 8/1000 | Loss: 0.00191043
Iteration 9/1000 | Loss: 0.00105230
Iteration 10/1000 | Loss: 0.00092637
Iteration 11/1000 | Loss: 0.00105231
Iteration 12/1000 | Loss: 0.00110357
Iteration 13/1000 | Loss: 0.00093321
Iteration 14/1000 | Loss: 0.00196825
Iteration 15/1000 | Loss: 0.00133286
Iteration 16/1000 | Loss: 0.00091421
Iteration 17/1000 | Loss: 0.00113109
Iteration 18/1000 | Loss: 0.00101785
Iteration 19/1000 | Loss: 0.00103029
Iteration 20/1000 | Loss: 0.00135578
Iteration 21/1000 | Loss: 0.00140622
Iteration 22/1000 | Loss: 0.00093268
Iteration 23/1000 | Loss: 0.00085650
Iteration 24/1000 | Loss: 0.00110759
Iteration 25/1000 | Loss: 0.00115735
Iteration 26/1000 | Loss: 0.00098506
Iteration 27/1000 | Loss: 0.00114108
Iteration 28/1000 | Loss: 0.00087782
Iteration 29/1000 | Loss: 0.00088040
Iteration 30/1000 | Loss: 0.00105647
Iteration 31/1000 | Loss: 0.00188154
Iteration 32/1000 | Loss: 0.00106401
Iteration 33/1000 | Loss: 0.00119642
Iteration 34/1000 | Loss: 0.00136061
Iteration 35/1000 | Loss: 0.00114220
Iteration 36/1000 | Loss: 0.00122094
Iteration 37/1000 | Loss: 0.00120186
Iteration 38/1000 | Loss: 0.00098030
Iteration 39/1000 | Loss: 0.00099548
Iteration 40/1000 | Loss: 0.00139119
Iteration 41/1000 | Loss: 0.00238045
Iteration 42/1000 | Loss: 0.00166789
Iteration 43/1000 | Loss: 0.00193946
Iteration 44/1000 | Loss: 0.00095248
Iteration 45/1000 | Loss: 0.00177641
Iteration 46/1000 | Loss: 0.00084722
Iteration 47/1000 | Loss: 0.00110157
Iteration 48/1000 | Loss: 0.00084387
Iteration 49/1000 | Loss: 0.00083282
Iteration 50/1000 | Loss: 0.00093102
Iteration 51/1000 | Loss: 0.00047343
Iteration 52/1000 | Loss: 0.00071246
Iteration 53/1000 | Loss: 0.00085209
Iteration 54/1000 | Loss: 0.00064782
Iteration 55/1000 | Loss: 0.00090701
Iteration 56/1000 | Loss: 0.00079253
Iteration 57/1000 | Loss: 0.00068339
Iteration 58/1000 | Loss: 0.00059036
Iteration 59/1000 | Loss: 0.00067473
Iteration 60/1000 | Loss: 0.00041208
Iteration 61/1000 | Loss: 0.00145646
Iteration 62/1000 | Loss: 0.00143754
Iteration 63/1000 | Loss: 0.00026508
Iteration 64/1000 | Loss: 0.00083475
Iteration 65/1000 | Loss: 0.00054141
Iteration 66/1000 | Loss: 0.00024085
Iteration 67/1000 | Loss: 0.00037876
Iteration 68/1000 | Loss: 0.00042965
Iteration 69/1000 | Loss: 0.00033675
Iteration 70/1000 | Loss: 0.00031638
Iteration 71/1000 | Loss: 0.00014777
Iteration 72/1000 | Loss: 0.00036654
Iteration 73/1000 | Loss: 0.00025018
Iteration 74/1000 | Loss: 0.00022909
Iteration 75/1000 | Loss: 0.00109986
Iteration 76/1000 | Loss: 0.00104248
Iteration 77/1000 | Loss: 0.00048180
Iteration 78/1000 | Loss: 0.00051939
Iteration 79/1000 | Loss: 0.00025587
Iteration 80/1000 | Loss: 0.00014268
Iteration 81/1000 | Loss: 0.00025599
Iteration 82/1000 | Loss: 0.00028257
Iteration 83/1000 | Loss: 0.00039483
Iteration 84/1000 | Loss: 0.00038801
Iteration 85/1000 | Loss: 0.00036985
Iteration 86/1000 | Loss: 0.00016777
Iteration 87/1000 | Loss: 0.00065086
Iteration 88/1000 | Loss: 0.00063315
Iteration 89/1000 | Loss: 0.00047332
Iteration 90/1000 | Loss: 0.00034549
Iteration 91/1000 | Loss: 0.00036231
Iteration 92/1000 | Loss: 0.00033205
Iteration 93/1000 | Loss: 0.00054870
Iteration 94/1000 | Loss: 0.00020770
Iteration 95/1000 | Loss: 0.00030989
Iteration 96/1000 | Loss: 0.00043118
Iteration 97/1000 | Loss: 0.00029263
Iteration 98/1000 | Loss: 0.00016958
Iteration 99/1000 | Loss: 0.00029099
Iteration 100/1000 | Loss: 0.00044889
Iteration 101/1000 | Loss: 0.00037000
Iteration 102/1000 | Loss: 0.00032778
Iteration 103/1000 | Loss: 0.00040535
Iteration 104/1000 | Loss: 0.00019889
Iteration 105/1000 | Loss: 0.00032270
Iteration 106/1000 | Loss: 0.00022705
Iteration 107/1000 | Loss: 0.00020762
Iteration 108/1000 | Loss: 0.00041075
Iteration 109/1000 | Loss: 0.00038906
Iteration 110/1000 | Loss: 0.00037900
Iteration 111/1000 | Loss: 0.00049846
Iteration 112/1000 | Loss: 0.00034662
Iteration 113/1000 | Loss: 0.00116463
Iteration 114/1000 | Loss: 0.00052144
Iteration 115/1000 | Loss: 0.00022811
Iteration 116/1000 | Loss: 0.00057037
Iteration 117/1000 | Loss: 0.00044293
Iteration 118/1000 | Loss: 0.00052732
Iteration 119/1000 | Loss: 0.00032491
Iteration 120/1000 | Loss: 0.00133189
Iteration 121/1000 | Loss: 0.00010443
Iteration 122/1000 | Loss: 0.00059630
Iteration 123/1000 | Loss: 0.00032018
Iteration 124/1000 | Loss: 0.00029036
Iteration 125/1000 | Loss: 0.00028187
Iteration 126/1000 | Loss: 0.00020277
Iteration 127/1000 | Loss: 0.00029050
Iteration 128/1000 | Loss: 0.00007038
Iteration 129/1000 | Loss: 0.00006817
Iteration 130/1000 | Loss: 0.00006589
Iteration 131/1000 | Loss: 0.00006416
Iteration 132/1000 | Loss: 0.00068827
Iteration 133/1000 | Loss: 0.00007356
Iteration 134/1000 | Loss: 0.00006377
Iteration 135/1000 | Loss: 0.00006050
Iteration 136/1000 | Loss: 0.00005950
Iteration 137/1000 | Loss: 0.00005874
Iteration 138/1000 | Loss: 0.00005817
Iteration 139/1000 | Loss: 0.00005787
Iteration 140/1000 | Loss: 0.00005757
Iteration 141/1000 | Loss: 0.00005708
Iteration 142/1000 | Loss: 0.00005659
Iteration 143/1000 | Loss: 0.00005631
Iteration 144/1000 | Loss: 0.00005595
Iteration 145/1000 | Loss: 0.00127935
Iteration 146/1000 | Loss: 0.00100206
Iteration 147/1000 | Loss: 0.00006059
Iteration 148/1000 | Loss: 0.00005706
Iteration 149/1000 | Loss: 0.00005636
Iteration 150/1000 | Loss: 0.00005593
Iteration 151/1000 | Loss: 0.00117199
Iteration 152/1000 | Loss: 0.00008026
Iteration 153/1000 | Loss: 0.00006532
Iteration 154/1000 | Loss: 0.00005718
Iteration 155/1000 | Loss: 0.00005489
Iteration 156/1000 | Loss: 0.00005364
Iteration 157/1000 | Loss: 0.00005285
Iteration 158/1000 | Loss: 0.00005243
Iteration 159/1000 | Loss: 0.00005222
Iteration 160/1000 | Loss: 0.00005222
Iteration 161/1000 | Loss: 0.00005222
Iteration 162/1000 | Loss: 0.00005221
Iteration 163/1000 | Loss: 0.00005220
Iteration 164/1000 | Loss: 0.00005219
Iteration 165/1000 | Loss: 0.00005218
Iteration 166/1000 | Loss: 0.00005218
Iteration 167/1000 | Loss: 0.00005214
Iteration 168/1000 | Loss: 0.00005213
Iteration 169/1000 | Loss: 0.00005209
Iteration 170/1000 | Loss: 0.00005205
Iteration 171/1000 | Loss: 0.00005204
Iteration 172/1000 | Loss: 0.00005203
Iteration 173/1000 | Loss: 0.00005202
Iteration 174/1000 | Loss: 0.00005201
Iteration 175/1000 | Loss: 0.00005201
Iteration 176/1000 | Loss: 0.00005200
Iteration 177/1000 | Loss: 0.00005200
Iteration 178/1000 | Loss: 0.00005199
Iteration 179/1000 | Loss: 0.00005199
Iteration 180/1000 | Loss: 0.00005199
Iteration 181/1000 | Loss: 0.00005199
Iteration 182/1000 | Loss: 0.00005198
Iteration 183/1000 | Loss: 0.00005198
Iteration 184/1000 | Loss: 0.00005197
Iteration 185/1000 | Loss: 0.00005194
Iteration 186/1000 | Loss: 0.00005192
Iteration 187/1000 | Loss: 0.00005192
Iteration 188/1000 | Loss: 0.00005191
Iteration 189/1000 | Loss: 0.00005191
Iteration 190/1000 | Loss: 0.00005190
Iteration 191/1000 | Loss: 0.00005188
Iteration 192/1000 | Loss: 0.00005188
Iteration 193/1000 | Loss: 0.00005188
Iteration 194/1000 | Loss: 0.00005188
Iteration 195/1000 | Loss: 0.00005188
Iteration 196/1000 | Loss: 0.00005188
Iteration 197/1000 | Loss: 0.00005187
Iteration 198/1000 | Loss: 0.00005187
Iteration 199/1000 | Loss: 0.00005187
Iteration 200/1000 | Loss: 0.00005187
Iteration 201/1000 | Loss: 0.00005187
Iteration 202/1000 | Loss: 0.00005185
Iteration 203/1000 | Loss: 0.00005185
Iteration 204/1000 | Loss: 0.00005185
Iteration 205/1000 | Loss: 0.00005185
Iteration 206/1000 | Loss: 0.00005185
Iteration 207/1000 | Loss: 0.00005185
Iteration 208/1000 | Loss: 0.00005185
Iteration 209/1000 | Loss: 0.00005185
Iteration 210/1000 | Loss: 0.00005184
Iteration 211/1000 | Loss: 0.00005184
Iteration 212/1000 | Loss: 0.00005184
Iteration 213/1000 | Loss: 0.00005184
Iteration 214/1000 | Loss: 0.00005184
Iteration 215/1000 | Loss: 0.00005183
Iteration 216/1000 | Loss: 0.00005178
Iteration 217/1000 | Loss: 0.00005171
Iteration 218/1000 | Loss: 0.00005155
Iteration 219/1000 | Loss: 0.00005153
Iteration 220/1000 | Loss: 0.00077495
Iteration 221/1000 | Loss: 0.00150674
Iteration 222/1000 | Loss: 0.00141561
Iteration 223/1000 | Loss: 0.00008778
Iteration 224/1000 | Loss: 0.00006969
Iteration 225/1000 | Loss: 0.00121831
Iteration 226/1000 | Loss: 0.00006947
Iteration 227/1000 | Loss: 0.00049271
Iteration 228/1000 | Loss: 0.00028367
Iteration 229/1000 | Loss: 0.00018616
Iteration 230/1000 | Loss: 0.00034945
Iteration 231/1000 | Loss: 0.00132078
Iteration 232/1000 | Loss: 0.00042805
Iteration 233/1000 | Loss: 0.00117573
Iteration 234/1000 | Loss: 0.00036628
Iteration 235/1000 | Loss: 0.00049256
Iteration 236/1000 | Loss: 0.00005561
Iteration 237/1000 | Loss: 0.00005142
Iteration 238/1000 | Loss: 0.00004994
Iteration 239/1000 | Loss: 0.00004911
Iteration 240/1000 | Loss: 0.00021476
Iteration 241/1000 | Loss: 0.00006151
Iteration 242/1000 | Loss: 0.00005368
Iteration 243/1000 | Loss: 0.00005153
Iteration 244/1000 | Loss: 0.00004983
Iteration 245/1000 | Loss: 0.00045160
Iteration 246/1000 | Loss: 0.00005231
Iteration 247/1000 | Loss: 0.00004861
Iteration 248/1000 | Loss: 0.00060326
Iteration 249/1000 | Loss: 0.00070873
Iteration 250/1000 | Loss: 0.00006982
Iteration 251/1000 | Loss: 0.00005793
Iteration 252/1000 | Loss: 0.00005600
Iteration 253/1000 | Loss: 0.00005499
Iteration 254/1000 | Loss: 0.00005375
Iteration 255/1000 | Loss: 0.00067378
Iteration 256/1000 | Loss: 0.00021723
Iteration 257/1000 | Loss: 0.00005932
Iteration 258/1000 | Loss: 0.00077771
Iteration 259/1000 | Loss: 0.00061396
Iteration 260/1000 | Loss: 0.00078620
Iteration 261/1000 | Loss: 0.00007459
Iteration 262/1000 | Loss: 0.00005965
Iteration 263/1000 | Loss: 0.00069207
Iteration 264/1000 | Loss: 0.00048766
Iteration 265/1000 | Loss: 0.00026962
Iteration 266/1000 | Loss: 0.00033380
Iteration 267/1000 | Loss: 0.00021159
Iteration 268/1000 | Loss: 0.00061004
Iteration 269/1000 | Loss: 0.00057578
Iteration 270/1000 | Loss: 0.00062398
Iteration 271/1000 | Loss: 0.00075218
Iteration 272/1000 | Loss: 0.00044164
Iteration 273/1000 | Loss: 0.00058118
Iteration 274/1000 | Loss: 0.00006738
Iteration 275/1000 | Loss: 0.00037821
Iteration 276/1000 | Loss: 0.00032266
Iteration 277/1000 | Loss: 0.00028919
Iteration 278/1000 | Loss: 0.00004812
Iteration 279/1000 | Loss: 0.00004746
Iteration 280/1000 | Loss: 0.00004682
Iteration 281/1000 | Loss: 0.00004645
Iteration 282/1000 | Loss: 0.00004616
Iteration 283/1000 | Loss: 0.00064424
Iteration 284/1000 | Loss: 0.00077408
Iteration 285/1000 | Loss: 0.00075282
Iteration 286/1000 | Loss: 0.00061839
Iteration 287/1000 | Loss: 0.00087871
Iteration 288/1000 | Loss: 0.00076783
Iteration 289/1000 | Loss: 0.00096362
Iteration 290/1000 | Loss: 0.00085575
Iteration 291/1000 | Loss: 0.00127245
Iteration 292/1000 | Loss: 0.00160845
Iteration 293/1000 | Loss: 0.00106248
Iteration 294/1000 | Loss: 0.00037501
Iteration 295/1000 | Loss: 0.00026549
Iteration 296/1000 | Loss: 0.00101309
Iteration 297/1000 | Loss: 0.00060522
Iteration 298/1000 | Loss: 0.00092841
Iteration 299/1000 | Loss: 0.00010853
Iteration 300/1000 | Loss: 0.00005064
Iteration 301/1000 | Loss: 0.00025821
Iteration 302/1000 | Loss: 0.00040315
Iteration 303/1000 | Loss: 0.00147386
Iteration 304/1000 | Loss: 0.00020432
Iteration 305/1000 | Loss: 0.00013112
Iteration 306/1000 | Loss: 0.00005297
Iteration 307/1000 | Loss: 0.00035555
Iteration 308/1000 | Loss: 0.00051436
Iteration 309/1000 | Loss: 0.00021707
Iteration 310/1000 | Loss: 0.00039580
Iteration 311/1000 | Loss: 0.00008655
Iteration 312/1000 | Loss: 0.00008715
Iteration 313/1000 | Loss: 0.00005421
Iteration 314/1000 | Loss: 0.00100100
Iteration 315/1000 | Loss: 0.00031070
Iteration 316/1000 | Loss: 0.00095118
Iteration 317/1000 | Loss: 0.00025318
Iteration 318/1000 | Loss: 0.00084780
Iteration 319/1000 | Loss: 0.00024061
Iteration 320/1000 | Loss: 0.00015326
Iteration 321/1000 | Loss: 0.00013051
Iteration 322/1000 | Loss: 0.00005000
Iteration 323/1000 | Loss: 0.00004813
Iteration 324/1000 | Loss: 0.00004664
Iteration 325/1000 | Loss: 0.00092540
Iteration 326/1000 | Loss: 0.00023342
Iteration 327/1000 | Loss: 0.00034916
Iteration 328/1000 | Loss: 0.00021220
Iteration 329/1000 | Loss: 0.00020127
Iteration 330/1000 | Loss: 0.00005520
Iteration 331/1000 | Loss: 0.00004901
Iteration 332/1000 | Loss: 0.00004701
Iteration 333/1000 | Loss: 0.00004546
Iteration 334/1000 | Loss: 0.00004456
Iteration 335/1000 | Loss: 0.00004408
Iteration 336/1000 | Loss: 0.00004375
Iteration 337/1000 | Loss: 0.00035872
Iteration 338/1000 | Loss: 0.00018631
Iteration 339/1000 | Loss: 0.00031530
Iteration 340/1000 | Loss: 0.00006159
Iteration 341/1000 | Loss: 0.00005186
Iteration 342/1000 | Loss: 0.00004912
Iteration 343/1000 | Loss: 0.00088948
Iteration 344/1000 | Loss: 0.00053055
Iteration 345/1000 | Loss: 0.00091814
Iteration 346/1000 | Loss: 0.00006613
Iteration 347/1000 | Loss: 0.00005272
Iteration 348/1000 | Loss: 0.00004664
Iteration 349/1000 | Loss: 0.00004466
Iteration 350/1000 | Loss: 0.00004391
Iteration 351/1000 | Loss: 0.00004356
Iteration 352/1000 | Loss: 0.00004340
Iteration 353/1000 | Loss: 0.00004328
Iteration 354/1000 | Loss: 0.00004326
Iteration 355/1000 | Loss: 0.00004325
Iteration 356/1000 | Loss: 0.00004325
Iteration 357/1000 | Loss: 0.00004324
Iteration 358/1000 | Loss: 0.00004324
Iteration 359/1000 | Loss: 0.00004323
Iteration 360/1000 | Loss: 0.00004323
Iteration 361/1000 | Loss: 0.00004323
Iteration 362/1000 | Loss: 0.00004322
Iteration 363/1000 | Loss: 0.00004321
Iteration 364/1000 | Loss: 0.00004321
Iteration 365/1000 | Loss: 0.00004321
Iteration 366/1000 | Loss: 0.00004320
Iteration 367/1000 | Loss: 0.00004319
Iteration 368/1000 | Loss: 0.00004319
Iteration 369/1000 | Loss: 0.00004319
Iteration 370/1000 | Loss: 0.00004319
Iteration 371/1000 | Loss: 0.00004319
Iteration 372/1000 | Loss: 0.00004319
Iteration 373/1000 | Loss: 0.00004319
Iteration 374/1000 | Loss: 0.00004318
Iteration 375/1000 | Loss: 0.00004318
Iteration 376/1000 | Loss: 0.00004318
Iteration 377/1000 | Loss: 0.00004318
Iteration 378/1000 | Loss: 0.00004318
Iteration 379/1000 | Loss: 0.00004318
Iteration 380/1000 | Loss: 0.00004318
Iteration 381/1000 | Loss: 0.00004318
Iteration 382/1000 | Loss: 0.00004318
Iteration 383/1000 | Loss: 0.00004318
Iteration 384/1000 | Loss: 0.00004318
Iteration 385/1000 | Loss: 0.00004318
Iteration 386/1000 | Loss: 0.00004317
Iteration 387/1000 | Loss: 0.00004317
Iteration 388/1000 | Loss: 0.00004317
Iteration 389/1000 | Loss: 0.00004317
Iteration 390/1000 | Loss: 0.00004317
Iteration 391/1000 | Loss: 0.00004316
Iteration 392/1000 | Loss: 0.00004316
Iteration 393/1000 | Loss: 0.00004316
Iteration 394/1000 | Loss: 0.00004316
Iteration 395/1000 | Loss: 0.00004316
Iteration 396/1000 | Loss: 0.00004315
Iteration 397/1000 | Loss: 0.00004315
Iteration 398/1000 | Loss: 0.00004315
Iteration 399/1000 | Loss: 0.00004315
Iteration 400/1000 | Loss: 0.00004315
Iteration 401/1000 | Loss: 0.00004315
Iteration 402/1000 | Loss: 0.00004315
Iteration 403/1000 | Loss: 0.00004315
Iteration 404/1000 | Loss: 0.00004315
Iteration 405/1000 | Loss: 0.00004315
Iteration 406/1000 | Loss: 0.00004315
Iteration 407/1000 | Loss: 0.00004315
Iteration 408/1000 | Loss: 0.00004315
Iteration 409/1000 | Loss: 0.00004315
Iteration 410/1000 | Loss: 0.00004315
Iteration 411/1000 | Loss: 0.00004315
Iteration 412/1000 | Loss: 0.00004315
Iteration 413/1000 | Loss: 0.00004315
Iteration 414/1000 | Loss: 0.00004315
Iteration 415/1000 | Loss: 0.00004315
Iteration 416/1000 | Loss: 0.00004315
Iteration 417/1000 | Loss: 0.00004315
Iteration 418/1000 | Loss: 0.00004315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 418. Stopping optimization.
Last 5 losses: [4.315026308177039e-05, 4.315026308177039e-05, 4.315026308177039e-05, 4.315026308177039e-05, 4.315026308177039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.315026308177039e-05

Optimization complete. Final v2v error: 4.90356969833374 mm

Highest mean error: 15.646764755249023 mm for frame 35

Lowest mean error: 3.7655301094055176 mm for frame 149

Saving results

Total time: 482.54523277282715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035040
Iteration 2/25 | Loss: 0.00163188
Iteration 3/25 | Loss: 0.00146150
Iteration 4/25 | Loss: 0.00143351
Iteration 5/25 | Loss: 0.00142751
Iteration 6/25 | Loss: 0.00142720
Iteration 7/25 | Loss: 0.00142720
Iteration 8/25 | Loss: 0.00142720
Iteration 9/25 | Loss: 0.00142720
Iteration 10/25 | Loss: 0.00142720
Iteration 11/25 | Loss: 0.00142720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014271970139816403, 0.0014271970139816403, 0.0014271970139816403, 0.0014271970139816403, 0.0014271970139816403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014271970139816403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59619105
Iteration 2/25 | Loss: 0.00198417
Iteration 3/25 | Loss: 0.00198414
Iteration 4/25 | Loss: 0.00198414
Iteration 5/25 | Loss: 0.00198414
Iteration 6/25 | Loss: 0.00198414
Iteration 7/25 | Loss: 0.00198414
Iteration 8/25 | Loss: 0.00198414
Iteration 9/25 | Loss: 0.00198414
Iteration 10/25 | Loss: 0.00198414
Iteration 11/25 | Loss: 0.00198414
Iteration 12/25 | Loss: 0.00198414
Iteration 13/25 | Loss: 0.00198414
Iteration 14/25 | Loss: 0.00198414
Iteration 15/25 | Loss: 0.00198414
Iteration 16/25 | Loss: 0.00198414
Iteration 17/25 | Loss: 0.00198414
Iteration 18/25 | Loss: 0.00198414
Iteration 19/25 | Loss: 0.00198414
Iteration 20/25 | Loss: 0.00198414
Iteration 21/25 | Loss: 0.00198414
Iteration 22/25 | Loss: 0.00198414
Iteration 23/25 | Loss: 0.00198414
Iteration 24/25 | Loss: 0.00198414
Iteration 25/25 | Loss: 0.00198414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001984138274565339, 0.001984138274565339, 0.001984138274565339, 0.001984138274565339, 0.001984138274565339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001984138274565339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198414
Iteration 2/1000 | Loss: 0.00005068
Iteration 3/1000 | Loss: 0.00003536
Iteration 4/1000 | Loss: 0.00003109
Iteration 5/1000 | Loss: 0.00002961
Iteration 6/1000 | Loss: 0.00002817
Iteration 7/1000 | Loss: 0.00002723
Iteration 8/1000 | Loss: 0.00002666
Iteration 9/1000 | Loss: 0.00002633
Iteration 10/1000 | Loss: 0.00002613
Iteration 11/1000 | Loss: 0.00002610
Iteration 12/1000 | Loss: 0.00002595
Iteration 13/1000 | Loss: 0.00002593
Iteration 14/1000 | Loss: 0.00002591
Iteration 15/1000 | Loss: 0.00002590
Iteration 16/1000 | Loss: 0.00002590
Iteration 17/1000 | Loss: 0.00002589
Iteration 18/1000 | Loss: 0.00002589
Iteration 19/1000 | Loss: 0.00002589
Iteration 20/1000 | Loss: 0.00002588
Iteration 21/1000 | Loss: 0.00002585
Iteration 22/1000 | Loss: 0.00002580
Iteration 23/1000 | Loss: 0.00002577
Iteration 24/1000 | Loss: 0.00002576
Iteration 25/1000 | Loss: 0.00002576
Iteration 26/1000 | Loss: 0.00002576
Iteration 27/1000 | Loss: 0.00002576
Iteration 28/1000 | Loss: 0.00002575
Iteration 29/1000 | Loss: 0.00002575
Iteration 30/1000 | Loss: 0.00002574
Iteration 31/1000 | Loss: 0.00002574
Iteration 32/1000 | Loss: 0.00002573
Iteration 33/1000 | Loss: 0.00002573
Iteration 34/1000 | Loss: 0.00002573
Iteration 35/1000 | Loss: 0.00002572
Iteration 36/1000 | Loss: 0.00002572
Iteration 37/1000 | Loss: 0.00002572
Iteration 38/1000 | Loss: 0.00002572
Iteration 39/1000 | Loss: 0.00002572
Iteration 40/1000 | Loss: 0.00002572
Iteration 41/1000 | Loss: 0.00002572
Iteration 42/1000 | Loss: 0.00002571
Iteration 43/1000 | Loss: 0.00002571
Iteration 44/1000 | Loss: 0.00002569
Iteration 45/1000 | Loss: 0.00002568
Iteration 46/1000 | Loss: 0.00002568
Iteration 47/1000 | Loss: 0.00002567
Iteration 48/1000 | Loss: 0.00002567
Iteration 49/1000 | Loss: 0.00002563
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00002561
Iteration 52/1000 | Loss: 0.00002561
Iteration 53/1000 | Loss: 0.00002561
Iteration 54/1000 | Loss: 0.00002561
Iteration 55/1000 | Loss: 0.00002561
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002560
Iteration 58/1000 | Loss: 0.00002560
Iteration 59/1000 | Loss: 0.00002560
Iteration 60/1000 | Loss: 0.00002560
Iteration 61/1000 | Loss: 0.00002559
Iteration 62/1000 | Loss: 0.00002559
Iteration 63/1000 | Loss: 0.00002559
Iteration 64/1000 | Loss: 0.00002558
Iteration 65/1000 | Loss: 0.00002558
Iteration 66/1000 | Loss: 0.00002558
Iteration 67/1000 | Loss: 0.00002557
Iteration 68/1000 | Loss: 0.00002557
Iteration 69/1000 | Loss: 0.00002557
Iteration 70/1000 | Loss: 0.00002557
Iteration 71/1000 | Loss: 0.00002557
Iteration 72/1000 | Loss: 0.00002557
Iteration 73/1000 | Loss: 0.00002557
Iteration 74/1000 | Loss: 0.00002557
Iteration 75/1000 | Loss: 0.00002557
Iteration 76/1000 | Loss: 0.00002556
Iteration 77/1000 | Loss: 0.00002556
Iteration 78/1000 | Loss: 0.00002556
Iteration 79/1000 | Loss: 0.00002556
Iteration 80/1000 | Loss: 0.00002556
Iteration 81/1000 | Loss: 0.00002556
Iteration 82/1000 | Loss: 0.00002556
Iteration 83/1000 | Loss: 0.00002556
Iteration 84/1000 | Loss: 0.00002556
Iteration 85/1000 | Loss: 0.00002555
Iteration 86/1000 | Loss: 0.00002555
Iteration 87/1000 | Loss: 0.00002555
Iteration 88/1000 | Loss: 0.00002555
Iteration 89/1000 | Loss: 0.00002555
Iteration 90/1000 | Loss: 0.00002554
Iteration 91/1000 | Loss: 0.00002554
Iteration 92/1000 | Loss: 0.00002554
Iteration 93/1000 | Loss: 0.00002554
Iteration 94/1000 | Loss: 0.00002554
Iteration 95/1000 | Loss: 0.00002553
Iteration 96/1000 | Loss: 0.00002553
Iteration 97/1000 | Loss: 0.00002553
Iteration 98/1000 | Loss: 0.00002553
Iteration 99/1000 | Loss: 0.00002553
Iteration 100/1000 | Loss: 0.00002553
Iteration 101/1000 | Loss: 0.00002553
Iteration 102/1000 | Loss: 0.00002552
Iteration 103/1000 | Loss: 0.00002552
Iteration 104/1000 | Loss: 0.00002552
Iteration 105/1000 | Loss: 0.00002552
Iteration 106/1000 | Loss: 0.00002552
Iteration 107/1000 | Loss: 0.00002552
Iteration 108/1000 | Loss: 0.00002552
Iteration 109/1000 | Loss: 0.00002552
Iteration 110/1000 | Loss: 0.00002552
Iteration 111/1000 | Loss: 0.00002552
Iteration 112/1000 | Loss: 0.00002552
Iteration 113/1000 | Loss: 0.00002552
Iteration 114/1000 | Loss: 0.00002552
Iteration 115/1000 | Loss: 0.00002552
Iteration 116/1000 | Loss: 0.00002551
Iteration 117/1000 | Loss: 0.00002551
Iteration 118/1000 | Loss: 0.00002551
Iteration 119/1000 | Loss: 0.00002551
Iteration 120/1000 | Loss: 0.00002551
Iteration 121/1000 | Loss: 0.00002551
Iteration 122/1000 | Loss: 0.00002551
Iteration 123/1000 | Loss: 0.00002551
Iteration 124/1000 | Loss: 0.00002551
Iteration 125/1000 | Loss: 0.00002551
Iteration 126/1000 | Loss: 0.00002551
Iteration 127/1000 | Loss: 0.00002551
Iteration 128/1000 | Loss: 0.00002551
Iteration 129/1000 | Loss: 0.00002550
Iteration 130/1000 | Loss: 0.00002550
Iteration 131/1000 | Loss: 0.00002550
Iteration 132/1000 | Loss: 0.00002550
Iteration 133/1000 | Loss: 0.00002550
Iteration 134/1000 | Loss: 0.00002550
Iteration 135/1000 | Loss: 0.00002550
Iteration 136/1000 | Loss: 0.00002550
Iteration 137/1000 | Loss: 0.00002550
Iteration 138/1000 | Loss: 0.00002550
Iteration 139/1000 | Loss: 0.00002549
Iteration 140/1000 | Loss: 0.00002549
Iteration 141/1000 | Loss: 0.00002549
Iteration 142/1000 | Loss: 0.00002549
Iteration 143/1000 | Loss: 0.00002549
Iteration 144/1000 | Loss: 0.00002549
Iteration 145/1000 | Loss: 0.00002549
Iteration 146/1000 | Loss: 0.00002549
Iteration 147/1000 | Loss: 0.00002549
Iteration 148/1000 | Loss: 0.00002549
Iteration 149/1000 | Loss: 0.00002549
Iteration 150/1000 | Loss: 0.00002549
Iteration 151/1000 | Loss: 0.00002549
Iteration 152/1000 | Loss: 0.00002548
Iteration 153/1000 | Loss: 0.00002548
Iteration 154/1000 | Loss: 0.00002548
Iteration 155/1000 | Loss: 0.00002548
Iteration 156/1000 | Loss: 0.00002548
Iteration 157/1000 | Loss: 0.00002548
Iteration 158/1000 | Loss: 0.00002548
Iteration 159/1000 | Loss: 0.00002548
Iteration 160/1000 | Loss: 0.00002548
Iteration 161/1000 | Loss: 0.00002548
Iteration 162/1000 | Loss: 0.00002548
Iteration 163/1000 | Loss: 0.00002547
Iteration 164/1000 | Loss: 0.00002547
Iteration 165/1000 | Loss: 0.00002547
Iteration 166/1000 | Loss: 0.00002547
Iteration 167/1000 | Loss: 0.00002547
Iteration 168/1000 | Loss: 0.00002547
Iteration 169/1000 | Loss: 0.00002547
Iteration 170/1000 | Loss: 0.00002547
Iteration 171/1000 | Loss: 0.00002547
Iteration 172/1000 | Loss: 0.00002547
Iteration 173/1000 | Loss: 0.00002547
Iteration 174/1000 | Loss: 0.00002547
Iteration 175/1000 | Loss: 0.00002547
Iteration 176/1000 | Loss: 0.00002547
Iteration 177/1000 | Loss: 0.00002547
Iteration 178/1000 | Loss: 0.00002547
Iteration 179/1000 | Loss: 0.00002547
Iteration 180/1000 | Loss: 0.00002547
Iteration 181/1000 | Loss: 0.00002547
Iteration 182/1000 | Loss: 0.00002547
Iteration 183/1000 | Loss: 0.00002547
Iteration 184/1000 | Loss: 0.00002547
Iteration 185/1000 | Loss: 0.00002547
Iteration 186/1000 | Loss: 0.00002546
Iteration 187/1000 | Loss: 0.00002546
Iteration 188/1000 | Loss: 0.00002546
Iteration 189/1000 | Loss: 0.00002546
Iteration 190/1000 | Loss: 0.00002546
Iteration 191/1000 | Loss: 0.00002546
Iteration 192/1000 | Loss: 0.00002546
Iteration 193/1000 | Loss: 0.00002546
Iteration 194/1000 | Loss: 0.00002546
Iteration 195/1000 | Loss: 0.00002546
Iteration 196/1000 | Loss: 0.00002546
Iteration 197/1000 | Loss: 0.00002546
Iteration 198/1000 | Loss: 0.00002546
Iteration 199/1000 | Loss: 0.00002546
Iteration 200/1000 | Loss: 0.00002546
Iteration 201/1000 | Loss: 0.00002546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.54649330599932e-05, 2.54649330599932e-05, 2.54649330599932e-05, 2.54649330599932e-05, 2.54649330599932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.54649330599932e-05

Optimization complete. Final v2v error: 4.414948463439941 mm

Highest mean error: 4.913861274719238 mm for frame 237

Lowest mean error: 3.942291736602783 mm for frame 134

Saving results

Total time: 44.83593702316284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507280
Iteration 2/25 | Loss: 0.00157011
Iteration 3/25 | Loss: 0.00149168
Iteration 4/25 | Loss: 0.00147803
Iteration 5/25 | Loss: 0.00147405
Iteration 6/25 | Loss: 0.00147308
Iteration 7/25 | Loss: 0.00147308
Iteration 8/25 | Loss: 0.00147308
Iteration 9/25 | Loss: 0.00147308
Iteration 10/25 | Loss: 0.00147308
Iteration 11/25 | Loss: 0.00147308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001473083859309554, 0.001473083859309554, 0.001473083859309554, 0.001473083859309554, 0.001473083859309554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001473083859309554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15634382
Iteration 2/25 | Loss: 0.00308851
Iteration 3/25 | Loss: 0.00308851
Iteration 4/25 | Loss: 0.00308851
Iteration 5/25 | Loss: 0.00308851
Iteration 6/25 | Loss: 0.00308851
Iteration 7/25 | Loss: 0.00308851
Iteration 8/25 | Loss: 0.00308851
Iteration 9/25 | Loss: 0.00308851
Iteration 10/25 | Loss: 0.00308851
Iteration 11/25 | Loss: 0.00308851
Iteration 12/25 | Loss: 0.00308851
Iteration 13/25 | Loss: 0.00308851
Iteration 14/25 | Loss: 0.00308851
Iteration 15/25 | Loss: 0.00308851
Iteration 16/25 | Loss: 0.00308851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0030885084997862577, 0.0030885084997862577, 0.0030885084997862577, 0.0030885084997862577, 0.0030885084997862577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030885084997862577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308851
Iteration 2/1000 | Loss: 0.00005591
Iteration 3/1000 | Loss: 0.00003620
Iteration 4/1000 | Loss: 0.00003205
Iteration 5/1000 | Loss: 0.00003073
Iteration 6/1000 | Loss: 0.00002977
Iteration 7/1000 | Loss: 0.00002860
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002742
Iteration 11/1000 | Loss: 0.00002716
Iteration 12/1000 | Loss: 0.00002699
Iteration 13/1000 | Loss: 0.00002694
Iteration 14/1000 | Loss: 0.00002694
Iteration 15/1000 | Loss: 0.00002694
Iteration 16/1000 | Loss: 0.00002694
Iteration 17/1000 | Loss: 0.00002694
Iteration 18/1000 | Loss: 0.00002694
Iteration 19/1000 | Loss: 0.00002694
Iteration 20/1000 | Loss: 0.00002694
Iteration 21/1000 | Loss: 0.00002693
Iteration 22/1000 | Loss: 0.00002693
Iteration 23/1000 | Loss: 0.00002693
Iteration 24/1000 | Loss: 0.00002693
Iteration 25/1000 | Loss: 0.00002692
Iteration 26/1000 | Loss: 0.00002691
Iteration 27/1000 | Loss: 0.00002690
Iteration 28/1000 | Loss: 0.00002690
Iteration 29/1000 | Loss: 0.00002689
Iteration 30/1000 | Loss: 0.00002689
Iteration 31/1000 | Loss: 0.00002689
Iteration 32/1000 | Loss: 0.00002687
Iteration 33/1000 | Loss: 0.00002686
Iteration 34/1000 | Loss: 0.00002686
Iteration 35/1000 | Loss: 0.00002686
Iteration 36/1000 | Loss: 0.00002685
Iteration 37/1000 | Loss: 0.00002685
Iteration 38/1000 | Loss: 0.00002685
Iteration 39/1000 | Loss: 0.00002685
Iteration 40/1000 | Loss: 0.00002685
Iteration 41/1000 | Loss: 0.00002685
Iteration 42/1000 | Loss: 0.00002685
Iteration 43/1000 | Loss: 0.00002685
Iteration 44/1000 | Loss: 0.00002684
Iteration 45/1000 | Loss: 0.00002684
Iteration 46/1000 | Loss: 0.00002683
Iteration 47/1000 | Loss: 0.00002683
Iteration 48/1000 | Loss: 0.00002683
Iteration 49/1000 | Loss: 0.00002683
Iteration 50/1000 | Loss: 0.00002683
Iteration 51/1000 | Loss: 0.00002683
Iteration 52/1000 | Loss: 0.00002683
Iteration 53/1000 | Loss: 0.00002683
Iteration 54/1000 | Loss: 0.00002683
Iteration 55/1000 | Loss: 0.00002682
Iteration 56/1000 | Loss: 0.00002682
Iteration 57/1000 | Loss: 0.00002682
Iteration 58/1000 | Loss: 0.00002682
Iteration 59/1000 | Loss: 0.00002682
Iteration 60/1000 | Loss: 0.00002682
Iteration 61/1000 | Loss: 0.00002682
Iteration 62/1000 | Loss: 0.00002681
Iteration 63/1000 | Loss: 0.00002681
Iteration 64/1000 | Loss: 0.00002681
Iteration 65/1000 | Loss: 0.00002681
Iteration 66/1000 | Loss: 0.00002681
Iteration 67/1000 | Loss: 0.00002681
Iteration 68/1000 | Loss: 0.00002681
Iteration 69/1000 | Loss: 0.00002680
Iteration 70/1000 | Loss: 0.00002679
Iteration 71/1000 | Loss: 0.00002679
Iteration 72/1000 | Loss: 0.00002678
Iteration 73/1000 | Loss: 0.00002678
Iteration 74/1000 | Loss: 0.00002677
Iteration 75/1000 | Loss: 0.00002676
Iteration 76/1000 | Loss: 0.00002676
Iteration 77/1000 | Loss: 0.00002676
Iteration 78/1000 | Loss: 0.00002676
Iteration 79/1000 | Loss: 0.00002676
Iteration 80/1000 | Loss: 0.00002676
Iteration 81/1000 | Loss: 0.00002675
Iteration 82/1000 | Loss: 0.00002675
Iteration 83/1000 | Loss: 0.00002675
Iteration 84/1000 | Loss: 0.00002675
Iteration 85/1000 | Loss: 0.00002675
Iteration 86/1000 | Loss: 0.00002675
Iteration 87/1000 | Loss: 0.00002675
Iteration 88/1000 | Loss: 0.00002675
Iteration 89/1000 | Loss: 0.00002675
Iteration 90/1000 | Loss: 0.00002675
Iteration 91/1000 | Loss: 0.00002675
Iteration 92/1000 | Loss: 0.00002675
Iteration 93/1000 | Loss: 0.00002675
Iteration 94/1000 | Loss: 0.00002675
Iteration 95/1000 | Loss: 0.00002674
Iteration 96/1000 | Loss: 0.00002674
Iteration 97/1000 | Loss: 0.00002674
Iteration 98/1000 | Loss: 0.00002673
Iteration 99/1000 | Loss: 0.00002673
Iteration 100/1000 | Loss: 0.00002673
Iteration 101/1000 | Loss: 0.00002673
Iteration 102/1000 | Loss: 0.00002673
Iteration 103/1000 | Loss: 0.00002672
Iteration 104/1000 | Loss: 0.00002672
Iteration 105/1000 | Loss: 0.00002672
Iteration 106/1000 | Loss: 0.00002672
Iteration 107/1000 | Loss: 0.00002671
Iteration 108/1000 | Loss: 0.00002671
Iteration 109/1000 | Loss: 0.00002671
Iteration 110/1000 | Loss: 0.00002671
Iteration 111/1000 | Loss: 0.00002671
Iteration 112/1000 | Loss: 0.00002670
Iteration 113/1000 | Loss: 0.00002670
Iteration 114/1000 | Loss: 0.00002670
Iteration 115/1000 | Loss: 0.00002670
Iteration 116/1000 | Loss: 0.00002670
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002670
Iteration 119/1000 | Loss: 0.00002670
Iteration 120/1000 | Loss: 0.00002670
Iteration 121/1000 | Loss: 0.00002670
Iteration 122/1000 | Loss: 0.00002670
Iteration 123/1000 | Loss: 0.00002670
Iteration 124/1000 | Loss: 0.00002670
Iteration 125/1000 | Loss: 0.00002670
Iteration 126/1000 | Loss: 0.00002670
Iteration 127/1000 | Loss: 0.00002670
Iteration 128/1000 | Loss: 0.00002670
Iteration 129/1000 | Loss: 0.00002670
Iteration 130/1000 | Loss: 0.00002670
Iteration 131/1000 | Loss: 0.00002670
Iteration 132/1000 | Loss: 0.00002670
Iteration 133/1000 | Loss: 0.00002670
Iteration 134/1000 | Loss: 0.00002670
Iteration 135/1000 | Loss: 0.00002670
Iteration 136/1000 | Loss: 0.00002670
Iteration 137/1000 | Loss: 0.00002670
Iteration 138/1000 | Loss: 0.00002670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.6699499358073808e-05, 2.6699499358073808e-05, 2.6699499358073808e-05, 2.6699499358073808e-05, 2.6699499358073808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6699499358073808e-05

Optimization complete. Final v2v error: 4.451245307922363 mm

Highest mean error: 4.725657939910889 mm for frame 13

Lowest mean error: 4.3105669021606445 mm for frame 70

Saving results

Total time: 33.618234157562256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973248
Iteration 2/25 | Loss: 0.00188285
Iteration 3/25 | Loss: 0.00152483
Iteration 4/25 | Loss: 0.00141940
Iteration 5/25 | Loss: 0.00135882
Iteration 6/25 | Loss: 0.00135231
Iteration 7/25 | Loss: 0.00134456
Iteration 8/25 | Loss: 0.00133501
Iteration 9/25 | Loss: 0.00132957
Iteration 10/25 | Loss: 0.00132800
Iteration 11/25 | Loss: 0.00132744
Iteration 12/25 | Loss: 0.00132721
Iteration 13/25 | Loss: 0.00132716
Iteration 14/25 | Loss: 0.00132716
Iteration 15/25 | Loss: 0.00132715
Iteration 16/25 | Loss: 0.00132715
Iteration 17/25 | Loss: 0.00132715
Iteration 18/25 | Loss: 0.00132715
Iteration 19/25 | Loss: 0.00132715
Iteration 20/25 | Loss: 0.00132715
Iteration 21/25 | Loss: 0.00132715
Iteration 22/25 | Loss: 0.00132715
Iteration 23/25 | Loss: 0.00132715
Iteration 24/25 | Loss: 0.00132715
Iteration 25/25 | Loss: 0.00132715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.03640366
Iteration 2/25 | Loss: 0.00233006
Iteration 3/25 | Loss: 0.00223390
Iteration 4/25 | Loss: 0.00223390
Iteration 5/25 | Loss: 0.00223390
Iteration 6/25 | Loss: 0.00223389
Iteration 7/25 | Loss: 0.00223389
Iteration 8/25 | Loss: 0.00223389
Iteration 9/25 | Loss: 0.00223389
Iteration 10/25 | Loss: 0.00223389
Iteration 11/25 | Loss: 0.00223389
Iteration 12/25 | Loss: 0.00223389
Iteration 13/25 | Loss: 0.00223389
Iteration 14/25 | Loss: 0.00223389
Iteration 15/25 | Loss: 0.00223389
Iteration 16/25 | Loss: 0.00223389
Iteration 17/25 | Loss: 0.00223389
Iteration 18/25 | Loss: 0.00223389
Iteration 19/25 | Loss: 0.00223389
Iteration 20/25 | Loss: 0.00223389
Iteration 21/25 | Loss: 0.00223389
Iteration 22/25 | Loss: 0.00223389
Iteration 23/25 | Loss: 0.00223389
Iteration 24/25 | Loss: 0.00223389
Iteration 25/25 | Loss: 0.00223389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223389
Iteration 2/1000 | Loss: 0.00005728
Iteration 3/1000 | Loss: 0.00008796
Iteration 4/1000 | Loss: 0.00009005
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00006375
Iteration 7/1000 | Loss: 0.00006187
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002362
Iteration 10/1000 | Loss: 0.00002325
Iteration 11/1000 | Loss: 0.00002307
Iteration 12/1000 | Loss: 0.00002301
Iteration 13/1000 | Loss: 0.00002289
Iteration 14/1000 | Loss: 0.00002288
Iteration 15/1000 | Loss: 0.00002287
Iteration 16/1000 | Loss: 0.00002276
Iteration 17/1000 | Loss: 0.00002275
Iteration 18/1000 | Loss: 0.00002273
Iteration 19/1000 | Loss: 0.00002272
Iteration 20/1000 | Loss: 0.00002272
Iteration 21/1000 | Loss: 0.00002272
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002271
Iteration 24/1000 | Loss: 0.00002271
Iteration 25/1000 | Loss: 0.00002270
Iteration 26/1000 | Loss: 0.00002269
Iteration 27/1000 | Loss: 0.00002266
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002263
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002262
Iteration 33/1000 | Loss: 0.00002262
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002260
Iteration 36/1000 | Loss: 0.00002260
Iteration 37/1000 | Loss: 0.00002259
Iteration 38/1000 | Loss: 0.00003379
Iteration 39/1000 | Loss: 0.00003379
Iteration 40/1000 | Loss: 0.00005764
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002253
Iteration 43/1000 | Loss: 0.00002253
Iteration 44/1000 | Loss: 0.00002253
Iteration 45/1000 | Loss: 0.00002253
Iteration 46/1000 | Loss: 0.00002253
Iteration 47/1000 | Loss: 0.00002253
Iteration 48/1000 | Loss: 0.00002253
Iteration 49/1000 | Loss: 0.00002253
Iteration 50/1000 | Loss: 0.00002253
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002252
Iteration 55/1000 | Loss: 0.00002252
Iteration 56/1000 | Loss: 0.00002252
Iteration 57/1000 | Loss: 0.00002252
Iteration 58/1000 | Loss: 0.00002252
Iteration 59/1000 | Loss: 0.00002252
Iteration 60/1000 | Loss: 0.00002251
Iteration 61/1000 | Loss: 0.00002251
Iteration 62/1000 | Loss: 0.00002251
Iteration 63/1000 | Loss: 0.00002251
Iteration 64/1000 | Loss: 0.00002251
Iteration 65/1000 | Loss: 0.00002251
Iteration 66/1000 | Loss: 0.00002251
Iteration 67/1000 | Loss: 0.00002251
Iteration 68/1000 | Loss: 0.00002251
Iteration 69/1000 | Loss: 0.00002251
Iteration 70/1000 | Loss: 0.00002250
Iteration 71/1000 | Loss: 0.00002250
Iteration 72/1000 | Loss: 0.00002250
Iteration 73/1000 | Loss: 0.00002250
Iteration 74/1000 | Loss: 0.00002250
Iteration 75/1000 | Loss: 0.00002250
Iteration 76/1000 | Loss: 0.00002250
Iteration 77/1000 | Loss: 0.00002250
Iteration 78/1000 | Loss: 0.00002250
Iteration 79/1000 | Loss: 0.00002250
Iteration 80/1000 | Loss: 0.00002250
Iteration 81/1000 | Loss: 0.00002250
Iteration 82/1000 | Loss: 0.00002250
Iteration 83/1000 | Loss: 0.00002250
Iteration 84/1000 | Loss: 0.00002250
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002250
Iteration 87/1000 | Loss: 0.00002249
Iteration 88/1000 | Loss: 0.00002249
Iteration 89/1000 | Loss: 0.00002249
Iteration 90/1000 | Loss: 0.00002249
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002249
Iteration 99/1000 | Loss: 0.00002249
Iteration 100/1000 | Loss: 0.00002249
Iteration 101/1000 | Loss: 0.00002249
Iteration 102/1000 | Loss: 0.00002248
Iteration 103/1000 | Loss: 0.00002248
Iteration 104/1000 | Loss: 0.00002248
Iteration 105/1000 | Loss: 0.00002248
Iteration 106/1000 | Loss: 0.00002248
Iteration 107/1000 | Loss: 0.00002248
Iteration 108/1000 | Loss: 0.00002248
Iteration 109/1000 | Loss: 0.00002248
Iteration 110/1000 | Loss: 0.00002248
Iteration 111/1000 | Loss: 0.00002248
Iteration 112/1000 | Loss: 0.00002248
Iteration 113/1000 | Loss: 0.00002248
Iteration 114/1000 | Loss: 0.00002248
Iteration 115/1000 | Loss: 0.00002248
Iteration 116/1000 | Loss: 0.00002247
Iteration 117/1000 | Loss: 0.00002247
Iteration 118/1000 | Loss: 0.00002247
Iteration 119/1000 | Loss: 0.00002247
Iteration 120/1000 | Loss: 0.00002247
Iteration 121/1000 | Loss: 0.00002247
Iteration 122/1000 | Loss: 0.00002247
Iteration 123/1000 | Loss: 0.00002247
Iteration 124/1000 | Loss: 0.00002247
Iteration 125/1000 | Loss: 0.00002247
Iteration 126/1000 | Loss: 0.00002247
Iteration 127/1000 | Loss: 0.00002247
Iteration 128/1000 | Loss: 0.00002247
Iteration 129/1000 | Loss: 0.00002247
Iteration 130/1000 | Loss: 0.00002247
Iteration 131/1000 | Loss: 0.00002247
Iteration 132/1000 | Loss: 0.00002247
Iteration 133/1000 | Loss: 0.00002247
Iteration 134/1000 | Loss: 0.00002247
Iteration 135/1000 | Loss: 0.00002247
Iteration 136/1000 | Loss: 0.00002247
Iteration 137/1000 | Loss: 0.00002247
Iteration 138/1000 | Loss: 0.00002247
Iteration 139/1000 | Loss: 0.00002247
Iteration 140/1000 | Loss: 0.00002247
Iteration 141/1000 | Loss: 0.00002247
Iteration 142/1000 | Loss: 0.00002247
Iteration 143/1000 | Loss: 0.00002247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.2468253519036807e-05, 2.2468253519036807e-05, 2.2468253519036807e-05, 2.2468253519036807e-05, 2.2468253519036807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2468253519036807e-05

Optimization complete. Final v2v error: 4.1619062423706055 mm

Highest mean error: 4.635756015777588 mm for frame 187

Lowest mean error: 3.723644495010376 mm for frame 235

Saving results

Total time: 60.9047646522522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161440
Iteration 2/25 | Loss: 0.01161439
Iteration 3/25 | Loss: 0.01161439
Iteration 4/25 | Loss: 0.00333508
Iteration 5/25 | Loss: 0.00190788
Iteration 6/25 | Loss: 0.00195316
Iteration 7/25 | Loss: 0.00190429
Iteration 8/25 | Loss: 0.00169643
Iteration 9/25 | Loss: 0.00160721
Iteration 10/25 | Loss: 0.00157575
Iteration 11/25 | Loss: 0.00153052
Iteration 12/25 | Loss: 0.00148896
Iteration 13/25 | Loss: 0.00150503
Iteration 14/25 | Loss: 0.00145562
Iteration 15/25 | Loss: 0.00144024
Iteration 16/25 | Loss: 0.00142571
Iteration 17/25 | Loss: 0.00141860
Iteration 18/25 | Loss: 0.00143033
Iteration 19/25 | Loss: 0.00143739
Iteration 20/25 | Loss: 0.00143618
Iteration 21/25 | Loss: 0.00144080
Iteration 22/25 | Loss: 0.00144486
Iteration 23/25 | Loss: 0.00142915
Iteration 24/25 | Loss: 0.00141624
Iteration 25/25 | Loss: 0.00141630

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73243558
Iteration 2/25 | Loss: 0.01144789
Iteration 3/25 | Loss: 0.00742659
Iteration 4/25 | Loss: 0.00742659
Iteration 5/25 | Loss: 0.00742659
Iteration 6/25 | Loss: 0.00742659
Iteration 7/25 | Loss: 0.00742659
Iteration 8/25 | Loss: 0.00742659
Iteration 9/25 | Loss: 0.00742659
Iteration 10/25 | Loss: 0.00742659
Iteration 11/25 | Loss: 0.00742659
Iteration 12/25 | Loss: 0.00742659
Iteration 13/25 | Loss: 0.00742659
Iteration 14/25 | Loss: 0.00742659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007426591124385595, 0.007426591124385595, 0.007426591124385595, 0.007426591124385595, 0.007426591124385595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007426591124385595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00742659
Iteration 2/1000 | Loss: 0.00519860
Iteration 3/1000 | Loss: 0.00575349
Iteration 4/1000 | Loss: 0.00987480
Iteration 5/1000 | Loss: 0.00620920
Iteration 6/1000 | Loss: 0.00959231
Iteration 7/1000 | Loss: 0.00517737
Iteration 8/1000 | Loss: 0.00474355
Iteration 9/1000 | Loss: 0.00592401
Iteration 10/1000 | Loss: 0.00768263
Iteration 11/1000 | Loss: 0.00454621
Iteration 12/1000 | Loss: 0.00579299
Iteration 13/1000 | Loss: 0.00656259
Iteration 14/1000 | Loss: 0.00586855
Iteration 15/1000 | Loss: 0.00301070
Iteration 16/1000 | Loss: 0.00713115
Iteration 17/1000 | Loss: 0.00644487
Iteration 18/1000 | Loss: 0.00528700
Iteration 19/1000 | Loss: 0.00943784
Iteration 20/1000 | Loss: 0.00338599
Iteration 21/1000 | Loss: 0.00537411
Iteration 22/1000 | Loss: 0.00547055
Iteration 23/1000 | Loss: 0.00467980
Iteration 24/1000 | Loss: 0.00553035
Iteration 25/1000 | Loss: 0.00549805
Iteration 26/1000 | Loss: 0.00403532
Iteration 27/1000 | Loss: 0.00387466
Iteration 28/1000 | Loss: 0.00217085
Iteration 29/1000 | Loss: 0.00508806
Iteration 30/1000 | Loss: 0.00293380
Iteration 31/1000 | Loss: 0.00287261
Iteration 32/1000 | Loss: 0.00695591
Iteration 33/1000 | Loss: 0.00291983
Iteration 34/1000 | Loss: 0.00271923
Iteration 35/1000 | Loss: 0.00238225
Iteration 36/1000 | Loss: 0.00239863
Iteration 37/1000 | Loss: 0.00255290
Iteration 38/1000 | Loss: 0.00379939
Iteration 39/1000 | Loss: 0.01026220
Iteration 40/1000 | Loss: 0.00345454
Iteration 41/1000 | Loss: 0.00231256
Iteration 42/1000 | Loss: 0.00377433
Iteration 43/1000 | Loss: 0.00182474
Iteration 44/1000 | Loss: 0.00390563
Iteration 45/1000 | Loss: 0.00367925
Iteration 46/1000 | Loss: 0.00258422
Iteration 47/1000 | Loss: 0.00458391
Iteration 48/1000 | Loss: 0.00421308
Iteration 49/1000 | Loss: 0.00257846
Iteration 50/1000 | Loss: 0.00193073
Iteration 51/1000 | Loss: 0.00219761
Iteration 52/1000 | Loss: 0.00511322
Iteration 53/1000 | Loss: 0.00370394
Iteration 54/1000 | Loss: 0.00447445
Iteration 55/1000 | Loss: 0.00453910
Iteration 56/1000 | Loss: 0.00496009
Iteration 57/1000 | Loss: 0.00472470
Iteration 58/1000 | Loss: 0.00381156
Iteration 59/1000 | Loss: 0.00747079
Iteration 60/1000 | Loss: 0.00314274
Iteration 61/1000 | Loss: 0.00486934
Iteration 62/1000 | Loss: 0.00327071
Iteration 63/1000 | Loss: 0.00343862
Iteration 64/1000 | Loss: 0.00387770
Iteration 65/1000 | Loss: 0.00386829
Iteration 66/1000 | Loss: 0.00378274
Iteration 67/1000 | Loss: 0.00840631
Iteration 68/1000 | Loss: 0.00568409
Iteration 69/1000 | Loss: 0.00444876
Iteration 70/1000 | Loss: 0.00320512
Iteration 71/1000 | Loss: 0.00833893
Iteration 72/1000 | Loss: 0.01069543
Iteration 73/1000 | Loss: 0.01299848
Iteration 74/1000 | Loss: 0.00418210
Iteration 75/1000 | Loss: 0.00396512
Iteration 76/1000 | Loss: 0.00512089
Iteration 77/1000 | Loss: 0.00298334
Iteration 78/1000 | Loss: 0.00484979
Iteration 79/1000 | Loss: 0.00345071
Iteration 80/1000 | Loss: 0.00472841
Iteration 81/1000 | Loss: 0.00270451
Iteration 82/1000 | Loss: 0.00377258
Iteration 83/1000 | Loss: 0.00285305
Iteration 84/1000 | Loss: 0.00310534
Iteration 85/1000 | Loss: 0.00272181
Iteration 86/1000 | Loss: 0.00508507
Iteration 87/1000 | Loss: 0.00233383
Iteration 88/1000 | Loss: 0.00351666
Iteration 89/1000 | Loss: 0.00282824
Iteration 90/1000 | Loss: 0.00491914
Iteration 91/1000 | Loss: 0.00238825
Iteration 92/1000 | Loss: 0.00342856
Iteration 93/1000 | Loss: 0.00291873
Iteration 94/1000 | Loss: 0.00262986
Iteration 95/1000 | Loss: 0.00260903
Iteration 96/1000 | Loss: 0.00227555
Iteration 97/1000 | Loss: 0.00394805
Iteration 98/1000 | Loss: 0.00487720
Iteration 99/1000 | Loss: 0.00298114
Iteration 100/1000 | Loss: 0.00412635
Iteration 101/1000 | Loss: 0.00589109
Iteration 102/1000 | Loss: 0.00308610
Iteration 103/1000 | Loss: 0.00242288
Iteration 104/1000 | Loss: 0.00222722
Iteration 105/1000 | Loss: 0.00368791
Iteration 106/1000 | Loss: 0.00233362
Iteration 107/1000 | Loss: 0.00232022
Iteration 108/1000 | Loss: 0.00231636
Iteration 109/1000 | Loss: 0.00219244
Iteration 110/1000 | Loss: 0.00548003
Iteration 111/1000 | Loss: 0.00705496
Iteration 112/1000 | Loss: 0.00392844
Iteration 113/1000 | Loss: 0.00250536
Iteration 114/1000 | Loss: 0.00222192
Iteration 115/1000 | Loss: 0.00242709
Iteration 116/1000 | Loss: 0.00231004
Iteration 117/1000 | Loss: 0.00252537
Iteration 118/1000 | Loss: 0.00456660
Iteration 119/1000 | Loss: 0.00227525
Iteration 120/1000 | Loss: 0.00328544
Iteration 121/1000 | Loss: 0.00230622
Iteration 122/1000 | Loss: 0.00636358
Iteration 123/1000 | Loss: 0.00265068
Iteration 124/1000 | Loss: 0.00189492
Iteration 125/1000 | Loss: 0.00196398
Iteration 126/1000 | Loss: 0.00336491
Iteration 127/1000 | Loss: 0.00275828
Iteration 128/1000 | Loss: 0.00374635
Iteration 129/1000 | Loss: 0.00144048
Iteration 130/1000 | Loss: 0.00174031
Iteration 131/1000 | Loss: 0.00265760
Iteration 132/1000 | Loss: 0.00147447
Iteration 133/1000 | Loss: 0.00208056
Iteration 134/1000 | Loss: 0.00160484
Iteration 135/1000 | Loss: 0.00115992
Iteration 136/1000 | Loss: 0.00158177
Iteration 137/1000 | Loss: 0.00107837
Iteration 138/1000 | Loss: 0.00150817
Iteration 139/1000 | Loss: 0.00139775
Iteration 140/1000 | Loss: 0.00123260
Iteration 141/1000 | Loss: 0.00131589
Iteration 142/1000 | Loss: 0.00148388
Iteration 143/1000 | Loss: 0.00250804
Iteration 144/1000 | Loss: 0.00122868
Iteration 145/1000 | Loss: 0.00122095
Iteration 146/1000 | Loss: 0.00150153
Iteration 147/1000 | Loss: 0.00131209
Iteration 148/1000 | Loss: 0.00152007
Iteration 149/1000 | Loss: 0.00244080
Iteration 150/1000 | Loss: 0.00130415
Iteration 151/1000 | Loss: 0.00134007
Iteration 152/1000 | Loss: 0.00127541
Iteration 153/1000 | Loss: 0.00124981
Iteration 154/1000 | Loss: 0.00131853
Iteration 155/1000 | Loss: 0.00168326
Iteration 156/1000 | Loss: 0.00166686
Iteration 157/1000 | Loss: 0.00113461
Iteration 158/1000 | Loss: 0.00160309
Iteration 159/1000 | Loss: 0.00130027
Iteration 160/1000 | Loss: 0.00146299
Iteration 161/1000 | Loss: 0.00132629
Iteration 162/1000 | Loss: 0.00131693
Iteration 163/1000 | Loss: 0.00134594
Iteration 164/1000 | Loss: 0.00127149
Iteration 165/1000 | Loss: 0.00116904
Iteration 166/1000 | Loss: 0.00151330
Iteration 167/1000 | Loss: 0.00135555
Iteration 168/1000 | Loss: 0.00164682
Iteration 169/1000 | Loss: 0.00136582
Iteration 170/1000 | Loss: 0.00238757
Iteration 171/1000 | Loss: 0.00142407
Iteration 172/1000 | Loss: 0.00220996
Iteration 173/1000 | Loss: 0.00133648
Iteration 174/1000 | Loss: 0.00150789
Iteration 175/1000 | Loss: 0.00235417
Iteration 176/1000 | Loss: 0.00156591
Iteration 177/1000 | Loss: 0.00165026
Iteration 178/1000 | Loss: 0.00131191
Iteration 179/1000 | Loss: 0.00555329
Iteration 180/1000 | Loss: 0.00134983
Iteration 181/1000 | Loss: 0.00306288
Iteration 182/1000 | Loss: 0.00141902
Iteration 183/1000 | Loss: 0.00108808
Iteration 184/1000 | Loss: 0.00081200
Iteration 185/1000 | Loss: 0.00092186
Iteration 186/1000 | Loss: 0.00127475
Iteration 187/1000 | Loss: 0.00096429
Iteration 188/1000 | Loss: 0.00116769
Iteration 189/1000 | Loss: 0.00126608
Iteration 190/1000 | Loss: 0.00126709
Iteration 191/1000 | Loss: 0.00230924
Iteration 192/1000 | Loss: 0.00323422
Iteration 193/1000 | Loss: 0.00353995
Iteration 194/1000 | Loss: 0.00199634
Iteration 195/1000 | Loss: 0.00281791
Iteration 196/1000 | Loss: 0.00295091
Iteration 197/1000 | Loss: 0.00111388
Iteration 198/1000 | Loss: 0.00147452
Iteration 199/1000 | Loss: 0.00130391
Iteration 200/1000 | Loss: 0.00148957
Iteration 201/1000 | Loss: 0.00139223
Iteration 202/1000 | Loss: 0.00124274
Iteration 203/1000 | Loss: 0.00218472
Iteration 204/1000 | Loss: 0.00169759
Iteration 205/1000 | Loss: 0.00136680
Iteration 206/1000 | Loss: 0.00181067
Iteration 207/1000 | Loss: 0.00138001
Iteration 208/1000 | Loss: 0.00125398
Iteration 209/1000 | Loss: 0.00124331
Iteration 210/1000 | Loss: 0.00137274
Iteration 211/1000 | Loss: 0.00126824
Iteration 212/1000 | Loss: 0.00131580
Iteration 213/1000 | Loss: 0.00153241
Iteration 214/1000 | Loss: 0.00145070
Iteration 215/1000 | Loss: 0.00251393
Iteration 216/1000 | Loss: 0.00135983
Iteration 217/1000 | Loss: 0.00113511
Iteration 218/1000 | Loss: 0.00115499
Iteration 219/1000 | Loss: 0.00174987
Iteration 220/1000 | Loss: 0.00210544
Iteration 221/1000 | Loss: 0.00151192
Iteration 222/1000 | Loss: 0.00136714
Iteration 223/1000 | Loss: 0.00186225
Iteration 224/1000 | Loss: 0.00185497
Iteration 225/1000 | Loss: 0.00157985
Iteration 226/1000 | Loss: 0.00129444
Iteration 227/1000 | Loss: 0.00122224
Iteration 228/1000 | Loss: 0.00142728
Iteration 229/1000 | Loss: 0.00127905
Iteration 230/1000 | Loss: 0.00108102
Iteration 231/1000 | Loss: 0.00065997
Iteration 232/1000 | Loss: 0.00145942
Iteration 233/1000 | Loss: 0.00169450
Iteration 234/1000 | Loss: 0.00120258
Iteration 235/1000 | Loss: 0.00130719
Iteration 236/1000 | Loss: 0.00128675
Iteration 237/1000 | Loss: 0.00299367
Iteration 238/1000 | Loss: 0.00079478
Iteration 239/1000 | Loss: 0.00077936
Iteration 240/1000 | Loss: 0.00133973
Iteration 241/1000 | Loss: 0.00107698
Iteration 242/1000 | Loss: 0.00105362
Iteration 243/1000 | Loss: 0.00167328
Iteration 244/1000 | Loss: 0.00163222
Iteration 245/1000 | Loss: 0.00232510
Iteration 246/1000 | Loss: 0.00149356
Iteration 247/1000 | Loss: 0.00150324
Iteration 248/1000 | Loss: 0.00130661
Iteration 249/1000 | Loss: 0.00102841
Iteration 250/1000 | Loss: 0.00105534
Iteration 251/1000 | Loss: 0.00286243
Iteration 252/1000 | Loss: 0.00113614
Iteration 253/1000 | Loss: 0.00146088
Iteration 254/1000 | Loss: 0.00105132
Iteration 255/1000 | Loss: 0.00147146
Iteration 256/1000 | Loss: 0.00092631
Iteration 257/1000 | Loss: 0.00090036
Iteration 258/1000 | Loss: 0.00103719
Iteration 259/1000 | Loss: 0.00154249
Iteration 260/1000 | Loss: 0.00150211
Iteration 261/1000 | Loss: 0.00195168
Iteration 262/1000 | Loss: 0.00134216
Iteration 263/1000 | Loss: 0.00161260
Iteration 264/1000 | Loss: 0.00100089
Iteration 265/1000 | Loss: 0.00110015
Iteration 266/1000 | Loss: 0.00182109
Iteration 267/1000 | Loss: 0.00107757
Iteration 268/1000 | Loss: 0.00279587
Iteration 269/1000 | Loss: 0.00148146
Iteration 270/1000 | Loss: 0.00086589
Iteration 271/1000 | Loss: 0.00121790
Iteration 272/1000 | Loss: 0.00101398
Iteration 273/1000 | Loss: 0.00223399
Iteration 274/1000 | Loss: 0.00117740
Iteration 275/1000 | Loss: 0.00203745
Iteration 276/1000 | Loss: 0.00101597
Iteration 277/1000 | Loss: 0.00143748
Iteration 278/1000 | Loss: 0.00239538
Iteration 279/1000 | Loss: 0.00206496
Iteration 280/1000 | Loss: 0.00132046
Iteration 281/1000 | Loss: 0.00104413
Iteration 282/1000 | Loss: 0.00135143
Iteration 283/1000 | Loss: 0.00075342
Iteration 284/1000 | Loss: 0.00132292
Iteration 285/1000 | Loss: 0.00183783
Iteration 286/1000 | Loss: 0.00106643
Iteration 287/1000 | Loss: 0.00087945
Iteration 288/1000 | Loss: 0.00304061
Iteration 289/1000 | Loss: 0.00607114
Iteration 290/1000 | Loss: 0.00396580
Iteration 291/1000 | Loss: 0.00613527
Iteration 292/1000 | Loss: 0.00258717
Iteration 293/1000 | Loss: 0.00498909
Iteration 294/1000 | Loss: 0.00171755
Iteration 295/1000 | Loss: 0.00335132
Iteration 296/1000 | Loss: 0.00458715
Iteration 297/1000 | Loss: 0.00171876
Iteration 298/1000 | Loss: 0.00121593
Iteration 299/1000 | Loss: 0.00209766
Iteration 300/1000 | Loss: 0.00250311
Iteration 301/1000 | Loss: 0.00180939
Iteration 302/1000 | Loss: 0.00087702
Iteration 303/1000 | Loss: 0.00072944
Iteration 304/1000 | Loss: 0.00163309
Iteration 305/1000 | Loss: 0.00125596
Iteration 306/1000 | Loss: 0.00207154
Iteration 307/1000 | Loss: 0.00245037
Iteration 308/1000 | Loss: 0.00146941
Iteration 309/1000 | Loss: 0.00421100
Iteration 310/1000 | Loss: 0.00065680
Iteration 311/1000 | Loss: 0.00220407
Iteration 312/1000 | Loss: 0.00114102
Iteration 313/1000 | Loss: 0.00124742
Iteration 314/1000 | Loss: 0.00115809
Iteration 315/1000 | Loss: 0.00114772
Iteration 316/1000 | Loss: 0.00110808
Iteration 317/1000 | Loss: 0.00077118
Iteration 318/1000 | Loss: 0.00087740
Iteration 319/1000 | Loss: 0.00083402
Iteration 320/1000 | Loss: 0.00152825
Iteration 321/1000 | Loss: 0.00097748
Iteration 322/1000 | Loss: 0.00109537
Iteration 323/1000 | Loss: 0.00099937
Iteration 324/1000 | Loss: 0.00272442
Iteration 325/1000 | Loss: 0.00109472
Iteration 326/1000 | Loss: 0.00097133
Iteration 327/1000 | Loss: 0.00085230
Iteration 328/1000 | Loss: 0.00189962
Iteration 329/1000 | Loss: 0.00167531
Iteration 330/1000 | Loss: 0.00181250
Iteration 331/1000 | Loss: 0.00103278
Iteration 332/1000 | Loss: 0.00082725
Iteration 333/1000 | Loss: 0.00127090
Iteration 334/1000 | Loss: 0.00085062
Iteration 335/1000 | Loss: 0.00116580
Iteration 336/1000 | Loss: 0.00090140
Iteration 337/1000 | Loss: 0.00120333
Iteration 338/1000 | Loss: 0.00110958
Iteration 339/1000 | Loss: 0.00120929
Iteration 340/1000 | Loss: 0.00122321
Iteration 341/1000 | Loss: 0.00119045
Iteration 342/1000 | Loss: 0.00125515
Iteration 343/1000 | Loss: 0.00095852
Iteration 344/1000 | Loss: 0.00142166
Iteration 345/1000 | Loss: 0.00096485
Iteration 346/1000 | Loss: 0.00072971
Iteration 347/1000 | Loss: 0.00113871
Iteration 348/1000 | Loss: 0.00107884
Iteration 349/1000 | Loss: 0.00105911
Iteration 350/1000 | Loss: 0.00100075
Iteration 351/1000 | Loss: 0.00171996
Iteration 352/1000 | Loss: 0.00038449
Iteration 353/1000 | Loss: 0.00031034
Iteration 354/1000 | Loss: 0.00021631
Iteration 355/1000 | Loss: 0.00008055
Iteration 356/1000 | Loss: 0.00050739
Iteration 357/1000 | Loss: 0.00026320
Iteration 358/1000 | Loss: 0.00005672
Iteration 359/1000 | Loss: 0.00028702
Iteration 360/1000 | Loss: 0.00007591
Iteration 361/1000 | Loss: 0.00050990
Iteration 362/1000 | Loss: 0.00082905
Iteration 363/1000 | Loss: 0.00022803
Iteration 364/1000 | Loss: 0.00066067
Iteration 365/1000 | Loss: 0.00057482
Iteration 366/1000 | Loss: 0.00043992
Iteration 367/1000 | Loss: 0.00054848
Iteration 368/1000 | Loss: 0.00010015
Iteration 369/1000 | Loss: 0.00006562
Iteration 370/1000 | Loss: 0.00011084
Iteration 371/1000 | Loss: 0.00040639
Iteration 372/1000 | Loss: 0.00058583
Iteration 373/1000 | Loss: 0.00093213
Iteration 374/1000 | Loss: 0.00023316
Iteration 375/1000 | Loss: 0.00095829
Iteration 376/1000 | Loss: 0.00092054
Iteration 377/1000 | Loss: 0.00086215
Iteration 378/1000 | Loss: 0.00014891
Iteration 379/1000 | Loss: 0.00011983
Iteration 380/1000 | Loss: 0.00005056
Iteration 381/1000 | Loss: 0.00007812
Iteration 382/1000 | Loss: 0.00046962
Iteration 383/1000 | Loss: 0.00019356
Iteration 384/1000 | Loss: 0.00039114
Iteration 385/1000 | Loss: 0.00016260
Iteration 386/1000 | Loss: 0.00004745
Iteration 387/1000 | Loss: 0.00004693
Iteration 388/1000 | Loss: 0.00038593
Iteration 389/1000 | Loss: 0.00018965
Iteration 390/1000 | Loss: 0.00075809
Iteration 391/1000 | Loss: 0.00005397
Iteration 392/1000 | Loss: 0.00048269
Iteration 393/1000 | Loss: 0.00036675
Iteration 394/1000 | Loss: 0.00005286
Iteration 395/1000 | Loss: 0.00004580
Iteration 396/1000 | Loss: 0.00050047
Iteration 397/1000 | Loss: 0.00057607
Iteration 398/1000 | Loss: 0.00093294
Iteration 399/1000 | Loss: 0.00040705
Iteration 400/1000 | Loss: 0.00039504
Iteration 401/1000 | Loss: 0.00022110
Iteration 402/1000 | Loss: 0.00012484
Iteration 403/1000 | Loss: 0.00038879
Iteration 404/1000 | Loss: 0.00006726
Iteration 405/1000 | Loss: 0.00008937
Iteration 406/1000 | Loss: 0.00004701
Iteration 407/1000 | Loss: 0.00005761
Iteration 408/1000 | Loss: 0.00006151
Iteration 409/1000 | Loss: 0.00022427
Iteration 410/1000 | Loss: 0.00004143
Iteration 411/1000 | Loss: 0.00031587
Iteration 412/1000 | Loss: 0.00005634
Iteration 413/1000 | Loss: 0.00011010
Iteration 414/1000 | Loss: 0.00018241
Iteration 415/1000 | Loss: 0.00004214
Iteration 416/1000 | Loss: 0.00007055
Iteration 417/1000 | Loss: 0.00030291
Iteration 418/1000 | Loss: 0.00011657
Iteration 419/1000 | Loss: 0.00004044
Iteration 420/1000 | Loss: 0.00017307
Iteration 421/1000 | Loss: 0.00034889
Iteration 422/1000 | Loss: 0.00004613
Iteration 423/1000 | Loss: 0.00005550
Iteration 424/1000 | Loss: 0.00004016
Iteration 425/1000 | Loss: 0.00044844
Iteration 426/1000 | Loss: 0.00005556
Iteration 427/1000 | Loss: 0.00006633
Iteration 428/1000 | Loss: 0.00004019
Iteration 429/1000 | Loss: 0.00010596
Iteration 430/1000 | Loss: 0.00065580
Iteration 431/1000 | Loss: 0.00004451
Iteration 432/1000 | Loss: 0.00003917
Iteration 433/1000 | Loss: 0.00003852
Iteration 434/1000 | Loss: 0.00003829
Iteration 435/1000 | Loss: 0.00004727
Iteration 436/1000 | Loss: 0.00004268
Iteration 437/1000 | Loss: 0.00003809
Iteration 438/1000 | Loss: 0.00003807
Iteration 439/1000 | Loss: 0.00004002
Iteration 440/1000 | Loss: 0.00003795
Iteration 441/1000 | Loss: 0.00003793
Iteration 442/1000 | Loss: 0.00003791
Iteration 443/1000 | Loss: 0.00003791
Iteration 444/1000 | Loss: 0.00003791
Iteration 445/1000 | Loss: 0.00003791
Iteration 446/1000 | Loss: 0.00003791
Iteration 447/1000 | Loss: 0.00003791
Iteration 448/1000 | Loss: 0.00003791
Iteration 449/1000 | Loss: 0.00003791
Iteration 450/1000 | Loss: 0.00003791
Iteration 451/1000 | Loss: 0.00003791
Iteration 452/1000 | Loss: 0.00003791
Iteration 453/1000 | Loss: 0.00003791
Iteration 454/1000 | Loss: 0.00003791
Iteration 455/1000 | Loss: 0.00003790
Iteration 456/1000 | Loss: 0.00003790
Iteration 457/1000 | Loss: 0.00003790
Iteration 458/1000 | Loss: 0.00003790
Iteration 459/1000 | Loss: 0.00003790
Iteration 460/1000 | Loss: 0.00003790
Iteration 461/1000 | Loss: 0.00003789
Iteration 462/1000 | Loss: 0.00003789
Iteration 463/1000 | Loss: 0.00003787
Iteration 464/1000 | Loss: 0.00003786
Iteration 465/1000 | Loss: 0.00003786
Iteration 466/1000 | Loss: 0.00003786
Iteration 467/1000 | Loss: 0.00003786
Iteration 468/1000 | Loss: 0.00003785
Iteration 469/1000 | Loss: 0.00003785
Iteration 470/1000 | Loss: 0.00003784
Iteration 471/1000 | Loss: 0.00003784
Iteration 472/1000 | Loss: 0.00003781
Iteration 473/1000 | Loss: 0.00003780
Iteration 474/1000 | Loss: 0.00003780
Iteration 475/1000 | Loss: 0.00003788
Iteration 476/1000 | Loss: 0.00003785
Iteration 477/1000 | Loss: 0.00003781
Iteration 478/1000 | Loss: 0.00003781
Iteration 479/1000 | Loss: 0.00003780
Iteration 480/1000 | Loss: 0.00003780
Iteration 481/1000 | Loss: 0.00003779
Iteration 482/1000 | Loss: 0.00003779
Iteration 483/1000 | Loss: 0.00003778
Iteration 484/1000 | Loss: 0.00003778
Iteration 485/1000 | Loss: 0.00003778
Iteration 486/1000 | Loss: 0.00003775
Iteration 487/1000 | Loss: 0.00003774
Iteration 488/1000 | Loss: 0.00003774
Iteration 489/1000 | Loss: 0.00003774
Iteration 490/1000 | Loss: 0.00003773
Iteration 491/1000 | Loss: 0.00003773
Iteration 492/1000 | Loss: 0.00003773
Iteration 493/1000 | Loss: 0.00003772
Iteration 494/1000 | Loss: 0.00003772
Iteration 495/1000 | Loss: 0.00003771
Iteration 496/1000 | Loss: 0.00003771
Iteration 497/1000 | Loss: 0.00003770
Iteration 498/1000 | Loss: 0.00003770
Iteration 499/1000 | Loss: 0.00003770
Iteration 500/1000 | Loss: 0.00003770
Iteration 501/1000 | Loss: 0.00003769
Iteration 502/1000 | Loss: 0.00003769
Iteration 503/1000 | Loss: 0.00003769
Iteration 504/1000 | Loss: 0.00003769
Iteration 505/1000 | Loss: 0.00003769
Iteration 506/1000 | Loss: 0.00003768
Iteration 507/1000 | Loss: 0.00003768
Iteration 508/1000 | Loss: 0.00003768
Iteration 509/1000 | Loss: 0.00003768
Iteration 510/1000 | Loss: 0.00003768
Iteration 511/1000 | Loss: 0.00003767
Iteration 512/1000 | Loss: 0.00003767
Iteration 513/1000 | Loss: 0.00003767
Iteration 514/1000 | Loss: 0.00003767
Iteration 515/1000 | Loss: 0.00003766
Iteration 516/1000 | Loss: 0.00003766
Iteration 517/1000 | Loss: 0.00003766
Iteration 518/1000 | Loss: 0.00003766
Iteration 519/1000 | Loss: 0.00003765
Iteration 520/1000 | Loss: 0.00003765
Iteration 521/1000 | Loss: 0.00003765
Iteration 522/1000 | Loss: 0.00003765
Iteration 523/1000 | Loss: 0.00003765
Iteration 524/1000 | Loss: 0.00003765
Iteration 525/1000 | Loss: 0.00004211
Iteration 526/1000 | Loss: 0.00003785
Iteration 527/1000 | Loss: 0.00003765
Iteration 528/1000 | Loss: 0.00003765
Iteration 529/1000 | Loss: 0.00003764
Iteration 530/1000 | Loss: 0.00003764
Iteration 531/1000 | Loss: 0.00003764
Iteration 532/1000 | Loss: 0.00003764
Iteration 533/1000 | Loss: 0.00003764
Iteration 534/1000 | Loss: 0.00003764
Iteration 535/1000 | Loss: 0.00003764
Iteration 536/1000 | Loss: 0.00003764
Iteration 537/1000 | Loss: 0.00003764
Iteration 538/1000 | Loss: 0.00003764
Iteration 539/1000 | Loss: 0.00003764
Iteration 540/1000 | Loss: 0.00003764
Iteration 541/1000 | Loss: 0.00003764
Iteration 542/1000 | Loss: 0.00003764
Iteration 543/1000 | Loss: 0.00003764
Iteration 544/1000 | Loss: 0.00003764
Iteration 545/1000 | Loss: 0.00003764
Iteration 546/1000 | Loss: 0.00003764
Iteration 547/1000 | Loss: 0.00003764
Iteration 548/1000 | Loss: 0.00003764
Iteration 549/1000 | Loss: 0.00003764
Iteration 550/1000 | Loss: 0.00003764
Iteration 551/1000 | Loss: 0.00003764
Iteration 552/1000 | Loss: 0.00003764
Iteration 553/1000 | Loss: 0.00003764
Iteration 554/1000 | Loss: 0.00003764
Iteration 555/1000 | Loss: 0.00003764
Iteration 556/1000 | Loss: 0.00003764
Iteration 557/1000 | Loss: 0.00003764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 557. Stopping optimization.
Last 5 losses: [3.7644691474270076e-05, 3.7644691474270076e-05, 3.7644691474270076e-05, 3.7644691474270076e-05, 3.7644691474270076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7644691474270076e-05

Optimization complete. Final v2v error: 4.464958667755127 mm

Highest mean error: 22.39670753479004 mm for frame 120

Lowest mean error: 3.357067823410034 mm for frame 189

Saving results

Total time: 755.3636734485626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511854
Iteration 2/25 | Loss: 0.00161175
Iteration 3/25 | Loss: 0.00135833
Iteration 4/25 | Loss: 0.00134160
Iteration 5/25 | Loss: 0.00133608
Iteration 6/25 | Loss: 0.00133440
Iteration 7/25 | Loss: 0.00133423
Iteration 8/25 | Loss: 0.00133423
Iteration 9/25 | Loss: 0.00133423
Iteration 10/25 | Loss: 0.00133423
Iteration 11/25 | Loss: 0.00133423
Iteration 12/25 | Loss: 0.00133423
Iteration 13/25 | Loss: 0.00133423
Iteration 14/25 | Loss: 0.00133423
Iteration 15/25 | Loss: 0.00133423
Iteration 16/25 | Loss: 0.00133423
Iteration 17/25 | Loss: 0.00133423
Iteration 18/25 | Loss: 0.00133423
Iteration 19/25 | Loss: 0.00133423
Iteration 20/25 | Loss: 0.00133423
Iteration 21/25 | Loss: 0.00133423
Iteration 22/25 | Loss: 0.00133423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001334227854385972, 0.001334227854385972, 0.001334227854385972, 0.001334227854385972, 0.001334227854385972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001334227854385972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67000675
Iteration 2/25 | Loss: 0.00222317
Iteration 3/25 | Loss: 0.00222317
Iteration 4/25 | Loss: 0.00222317
Iteration 5/25 | Loss: 0.00222317
Iteration 6/25 | Loss: 0.00222317
Iteration 7/25 | Loss: 0.00222317
Iteration 8/25 | Loss: 0.00222317
Iteration 9/25 | Loss: 0.00222317
Iteration 10/25 | Loss: 0.00222317
Iteration 11/25 | Loss: 0.00222317
Iteration 12/25 | Loss: 0.00222317
Iteration 13/25 | Loss: 0.00222317
Iteration 14/25 | Loss: 0.00222317
Iteration 15/25 | Loss: 0.00222317
Iteration 16/25 | Loss: 0.00222317
Iteration 17/25 | Loss: 0.00222317
Iteration 18/25 | Loss: 0.00222317
Iteration 19/25 | Loss: 0.00222317
Iteration 20/25 | Loss: 0.00222317
Iteration 21/25 | Loss: 0.00222317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022231703624129295, 0.0022231703624129295, 0.0022231703624129295, 0.0022231703624129295, 0.0022231703624129295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022231703624129295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222317
Iteration 2/1000 | Loss: 0.00003599
Iteration 3/1000 | Loss: 0.00002914
Iteration 4/1000 | Loss: 0.00002694
Iteration 5/1000 | Loss: 0.00002588
Iteration 6/1000 | Loss: 0.00002506
Iteration 7/1000 | Loss: 0.00002458
Iteration 8/1000 | Loss: 0.00002420
Iteration 9/1000 | Loss: 0.00002411
Iteration 10/1000 | Loss: 0.00002394
Iteration 11/1000 | Loss: 0.00002378
Iteration 12/1000 | Loss: 0.00002374
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00002364
Iteration 15/1000 | Loss: 0.00002360
Iteration 16/1000 | Loss: 0.00002360
Iteration 17/1000 | Loss: 0.00002360
Iteration 18/1000 | Loss: 0.00002359
Iteration 19/1000 | Loss: 0.00002359
Iteration 20/1000 | Loss: 0.00002358
Iteration 21/1000 | Loss: 0.00002358
Iteration 22/1000 | Loss: 0.00002358
Iteration 23/1000 | Loss: 0.00002356
Iteration 24/1000 | Loss: 0.00002356
Iteration 25/1000 | Loss: 0.00002356
Iteration 26/1000 | Loss: 0.00002356
Iteration 27/1000 | Loss: 0.00002356
Iteration 28/1000 | Loss: 0.00002356
Iteration 29/1000 | Loss: 0.00002356
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002355
Iteration 32/1000 | Loss: 0.00002353
Iteration 33/1000 | Loss: 0.00002353
Iteration 34/1000 | Loss: 0.00002353
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00002352
Iteration 37/1000 | Loss: 0.00002352
Iteration 38/1000 | Loss: 0.00002351
Iteration 39/1000 | Loss: 0.00002351
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002350
Iteration 42/1000 | Loss: 0.00002350
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002349
Iteration 45/1000 | Loss: 0.00002349
Iteration 46/1000 | Loss: 0.00002349
Iteration 47/1000 | Loss: 0.00002349
Iteration 48/1000 | Loss: 0.00002349
Iteration 49/1000 | Loss: 0.00002349
Iteration 50/1000 | Loss: 0.00002348
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002345
Iteration 60/1000 | Loss: 0.00002345
Iteration 61/1000 | Loss: 0.00002345
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002344
Iteration 64/1000 | Loss: 0.00002344
Iteration 65/1000 | Loss: 0.00002344
Iteration 66/1000 | Loss: 0.00002344
Iteration 67/1000 | Loss: 0.00002344
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002343
Iteration 77/1000 | Loss: 0.00002343
Iteration 78/1000 | Loss: 0.00002342
Iteration 79/1000 | Loss: 0.00002342
Iteration 80/1000 | Loss: 0.00002342
Iteration 81/1000 | Loss: 0.00002342
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002341
Iteration 84/1000 | Loss: 0.00002341
Iteration 85/1000 | Loss: 0.00002341
Iteration 86/1000 | Loss: 0.00002341
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002341
Iteration 93/1000 | Loss: 0.00002341
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002340
Iteration 98/1000 | Loss: 0.00002340
Iteration 99/1000 | Loss: 0.00002340
Iteration 100/1000 | Loss: 0.00002340
Iteration 101/1000 | Loss: 0.00002340
Iteration 102/1000 | Loss: 0.00002340
Iteration 103/1000 | Loss: 0.00002340
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002340
Iteration 107/1000 | Loss: 0.00002340
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002340
Iteration 110/1000 | Loss: 0.00002340
Iteration 111/1000 | Loss: 0.00002340
Iteration 112/1000 | Loss: 0.00002340
Iteration 113/1000 | Loss: 0.00002340
Iteration 114/1000 | Loss: 0.00002340
Iteration 115/1000 | Loss: 0.00002340
Iteration 116/1000 | Loss: 0.00002340
Iteration 117/1000 | Loss: 0.00002340
Iteration 118/1000 | Loss: 0.00002340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.3403161321766675e-05, 2.3403161321766675e-05, 2.3403161321766675e-05, 2.3403161321766675e-05, 2.3403161321766675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3403161321766675e-05

Optimization complete. Final v2v error: 4.201144218444824 mm

Highest mean error: 4.8679633140563965 mm for frame 22

Lowest mean error: 3.7613778114318848 mm for frame 107

Saving results

Total time: 32.4068706035614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500910
Iteration 2/25 | Loss: 0.00146565
Iteration 3/25 | Loss: 0.00133242
Iteration 4/25 | Loss: 0.00131772
Iteration 5/25 | Loss: 0.00131435
Iteration 6/25 | Loss: 0.00131359
Iteration 7/25 | Loss: 0.00131359
Iteration 8/25 | Loss: 0.00131359
Iteration 9/25 | Loss: 0.00131359
Iteration 10/25 | Loss: 0.00131359
Iteration 11/25 | Loss: 0.00131359
Iteration 12/25 | Loss: 0.00131359
Iteration 13/25 | Loss: 0.00131359
Iteration 14/25 | Loss: 0.00131359
Iteration 15/25 | Loss: 0.00131359
Iteration 16/25 | Loss: 0.00131359
Iteration 17/25 | Loss: 0.00131359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013135885819792747, 0.0013135885819792747, 0.0013135885819792747, 0.0013135885819792747, 0.0013135885819792747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013135885819792747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59018731
Iteration 2/25 | Loss: 0.00219349
Iteration 3/25 | Loss: 0.00219349
Iteration 4/25 | Loss: 0.00219349
Iteration 5/25 | Loss: 0.00219349
Iteration 6/25 | Loss: 0.00219349
Iteration 7/25 | Loss: 0.00219348
Iteration 8/25 | Loss: 0.00219348
Iteration 9/25 | Loss: 0.00219348
Iteration 10/25 | Loss: 0.00219348
Iteration 11/25 | Loss: 0.00219348
Iteration 12/25 | Loss: 0.00219348
Iteration 13/25 | Loss: 0.00219348
Iteration 14/25 | Loss: 0.00219348
Iteration 15/25 | Loss: 0.00219348
Iteration 16/25 | Loss: 0.00219348
Iteration 17/25 | Loss: 0.00219348
Iteration 18/25 | Loss: 0.00219348
Iteration 19/25 | Loss: 0.00219348
Iteration 20/25 | Loss: 0.00219348
Iteration 21/25 | Loss: 0.00219348
Iteration 22/25 | Loss: 0.00219348
Iteration 23/25 | Loss: 0.00219348
Iteration 24/25 | Loss: 0.00219348
Iteration 25/25 | Loss: 0.00219348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002193483989685774, 0.002193483989685774, 0.002193483989685774, 0.002193483989685774, 0.002193483989685774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002193483989685774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219348
Iteration 2/1000 | Loss: 0.00003552
Iteration 3/1000 | Loss: 0.00002953
Iteration 4/1000 | Loss: 0.00002766
Iteration 5/1000 | Loss: 0.00002667
Iteration 6/1000 | Loss: 0.00002599
Iteration 7/1000 | Loss: 0.00002566
Iteration 8/1000 | Loss: 0.00002563
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002558
Iteration 11/1000 | Loss: 0.00002558
Iteration 12/1000 | Loss: 0.00002557
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002552
Iteration 15/1000 | Loss: 0.00002550
Iteration 16/1000 | Loss: 0.00002549
Iteration 17/1000 | Loss: 0.00002546
Iteration 18/1000 | Loss: 0.00002545
Iteration 19/1000 | Loss: 0.00002545
Iteration 20/1000 | Loss: 0.00002543
Iteration 21/1000 | Loss: 0.00002531
Iteration 22/1000 | Loss: 0.00002531
Iteration 23/1000 | Loss: 0.00002531
Iteration 24/1000 | Loss: 0.00002531
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002525
Iteration 27/1000 | Loss: 0.00002524
Iteration 28/1000 | Loss: 0.00002521
Iteration 29/1000 | Loss: 0.00002521
Iteration 30/1000 | Loss: 0.00002521
Iteration 31/1000 | Loss: 0.00002521
Iteration 32/1000 | Loss: 0.00002520
Iteration 33/1000 | Loss: 0.00002520
Iteration 34/1000 | Loss: 0.00002520
Iteration 35/1000 | Loss: 0.00002520
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002519
Iteration 38/1000 | Loss: 0.00002519
Iteration 39/1000 | Loss: 0.00002518
Iteration 40/1000 | Loss: 0.00002517
Iteration 41/1000 | Loss: 0.00002517
Iteration 42/1000 | Loss: 0.00002517
Iteration 43/1000 | Loss: 0.00002516
Iteration 44/1000 | Loss: 0.00002514
Iteration 45/1000 | Loss: 0.00002514
Iteration 46/1000 | Loss: 0.00002514
Iteration 47/1000 | Loss: 0.00002514
Iteration 48/1000 | Loss: 0.00002514
Iteration 49/1000 | Loss: 0.00002514
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002513
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00002513
Iteration 56/1000 | Loss: 0.00002512
Iteration 57/1000 | Loss: 0.00002512
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00002511
Iteration 61/1000 | Loss: 0.00002511
Iteration 62/1000 | Loss: 0.00002511
Iteration 63/1000 | Loss: 0.00002511
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002510
Iteration 67/1000 | Loss: 0.00002510
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002509
Iteration 70/1000 | Loss: 0.00002509
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002508
Iteration 73/1000 | Loss: 0.00002508
Iteration 74/1000 | Loss: 0.00002508
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002507
Iteration 78/1000 | Loss: 0.00002507
Iteration 79/1000 | Loss: 0.00002506
Iteration 80/1000 | Loss: 0.00002506
Iteration 81/1000 | Loss: 0.00002506
Iteration 82/1000 | Loss: 0.00002506
Iteration 83/1000 | Loss: 0.00002506
Iteration 84/1000 | Loss: 0.00002505
Iteration 85/1000 | Loss: 0.00002505
Iteration 86/1000 | Loss: 0.00002505
Iteration 87/1000 | Loss: 0.00002505
Iteration 88/1000 | Loss: 0.00002505
Iteration 89/1000 | Loss: 0.00002505
Iteration 90/1000 | Loss: 0.00002505
Iteration 91/1000 | Loss: 0.00002505
Iteration 92/1000 | Loss: 0.00002505
Iteration 93/1000 | Loss: 0.00002505
Iteration 94/1000 | Loss: 0.00002505
Iteration 95/1000 | Loss: 0.00002505
Iteration 96/1000 | Loss: 0.00002504
Iteration 97/1000 | Loss: 0.00002504
Iteration 98/1000 | Loss: 0.00002504
Iteration 99/1000 | Loss: 0.00002504
Iteration 100/1000 | Loss: 0.00002504
Iteration 101/1000 | Loss: 0.00002504
Iteration 102/1000 | Loss: 0.00002504
Iteration 103/1000 | Loss: 0.00002504
Iteration 104/1000 | Loss: 0.00002504
Iteration 105/1000 | Loss: 0.00002503
Iteration 106/1000 | Loss: 0.00002503
Iteration 107/1000 | Loss: 0.00002503
Iteration 108/1000 | Loss: 0.00002503
Iteration 109/1000 | Loss: 0.00002503
Iteration 110/1000 | Loss: 0.00002503
Iteration 111/1000 | Loss: 0.00002503
Iteration 112/1000 | Loss: 0.00002503
Iteration 113/1000 | Loss: 0.00002503
Iteration 114/1000 | Loss: 0.00002502
Iteration 115/1000 | Loss: 0.00002502
Iteration 116/1000 | Loss: 0.00002502
Iteration 117/1000 | Loss: 0.00002502
Iteration 118/1000 | Loss: 0.00002502
Iteration 119/1000 | Loss: 0.00002502
Iteration 120/1000 | Loss: 0.00002502
Iteration 121/1000 | Loss: 0.00002502
Iteration 122/1000 | Loss: 0.00002502
Iteration 123/1000 | Loss: 0.00002502
Iteration 124/1000 | Loss: 0.00002502
Iteration 125/1000 | Loss: 0.00002502
Iteration 126/1000 | Loss: 0.00002502
Iteration 127/1000 | Loss: 0.00002502
Iteration 128/1000 | Loss: 0.00002502
Iteration 129/1000 | Loss: 0.00002502
Iteration 130/1000 | Loss: 0.00002502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.5016275685629807e-05, 2.5016275685629807e-05, 2.5016275685629807e-05, 2.5016275685629807e-05, 2.5016275685629807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5016275685629807e-05

Optimization complete. Final v2v error: 4.401155471801758 mm

Highest mean error: 4.825848579406738 mm for frame 36

Lowest mean error: 4.049386501312256 mm for frame 4

Saving results

Total time: 31.03874897956848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446185
Iteration 2/25 | Loss: 0.00141739
Iteration 3/25 | Loss: 0.00134435
Iteration 4/25 | Loss: 0.00133480
Iteration 5/25 | Loss: 0.00133387
Iteration 6/25 | Loss: 0.00133387
Iteration 7/25 | Loss: 0.00133387
Iteration 8/25 | Loss: 0.00133387
Iteration 9/25 | Loss: 0.00133387
Iteration 10/25 | Loss: 0.00133387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013338695280253887, 0.0013338695280253887, 0.0013338695280253887, 0.0013338695280253887, 0.0013338695280253887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013338695280253887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55660760
Iteration 2/25 | Loss: 0.00209673
Iteration 3/25 | Loss: 0.00209672
Iteration 4/25 | Loss: 0.00209672
Iteration 5/25 | Loss: 0.00209672
Iteration 6/25 | Loss: 0.00209672
Iteration 7/25 | Loss: 0.00209672
Iteration 8/25 | Loss: 0.00209672
Iteration 9/25 | Loss: 0.00209672
Iteration 10/25 | Loss: 0.00209672
Iteration 11/25 | Loss: 0.00209672
Iteration 12/25 | Loss: 0.00209672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002096723299473524, 0.002096723299473524, 0.002096723299473524, 0.002096723299473524, 0.002096723299473524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002096723299473524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209672
Iteration 2/1000 | Loss: 0.00003726
Iteration 3/1000 | Loss: 0.00003198
Iteration 4/1000 | Loss: 0.00002855
Iteration 5/1000 | Loss: 0.00002676
Iteration 6/1000 | Loss: 0.00002573
Iteration 7/1000 | Loss: 0.00002524
Iteration 8/1000 | Loss: 0.00002502
Iteration 9/1000 | Loss: 0.00002480
Iteration 10/1000 | Loss: 0.00002477
Iteration 11/1000 | Loss: 0.00002472
Iteration 12/1000 | Loss: 0.00002472
Iteration 13/1000 | Loss: 0.00002471
Iteration 14/1000 | Loss: 0.00002471
Iteration 15/1000 | Loss: 0.00002470
Iteration 16/1000 | Loss: 0.00002466
Iteration 17/1000 | Loss: 0.00002463
Iteration 18/1000 | Loss: 0.00002462
Iteration 19/1000 | Loss: 0.00002462
Iteration 20/1000 | Loss: 0.00002462
Iteration 21/1000 | Loss: 0.00002462
Iteration 22/1000 | Loss: 0.00002461
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002461
Iteration 25/1000 | Loss: 0.00002460
Iteration 26/1000 | Loss: 0.00002455
Iteration 27/1000 | Loss: 0.00002454
Iteration 28/1000 | Loss: 0.00002454
Iteration 29/1000 | Loss: 0.00002454
Iteration 30/1000 | Loss: 0.00002454
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002453
Iteration 33/1000 | Loss: 0.00002452
Iteration 34/1000 | Loss: 0.00002452
Iteration 35/1000 | Loss: 0.00002452
Iteration 36/1000 | Loss: 0.00002451
Iteration 37/1000 | Loss: 0.00002451
Iteration 38/1000 | Loss: 0.00002451
Iteration 39/1000 | Loss: 0.00002450
Iteration 40/1000 | Loss: 0.00002450
Iteration 41/1000 | Loss: 0.00002449
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002448
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002447
Iteration 47/1000 | Loss: 0.00002446
Iteration 48/1000 | Loss: 0.00002446
Iteration 49/1000 | Loss: 0.00002445
Iteration 50/1000 | Loss: 0.00002445
Iteration 51/1000 | Loss: 0.00002445
Iteration 52/1000 | Loss: 0.00002445
Iteration 53/1000 | Loss: 0.00002445
Iteration 54/1000 | Loss: 0.00002445
Iteration 55/1000 | Loss: 0.00002445
Iteration 56/1000 | Loss: 0.00002445
Iteration 57/1000 | Loss: 0.00002445
Iteration 58/1000 | Loss: 0.00002445
Iteration 59/1000 | Loss: 0.00002444
Iteration 60/1000 | Loss: 0.00002444
Iteration 61/1000 | Loss: 0.00002444
Iteration 62/1000 | Loss: 0.00002443
Iteration 63/1000 | Loss: 0.00002442
Iteration 64/1000 | Loss: 0.00002442
Iteration 65/1000 | Loss: 0.00002441
Iteration 66/1000 | Loss: 0.00002441
Iteration 67/1000 | Loss: 0.00002441
Iteration 68/1000 | Loss: 0.00002441
Iteration 69/1000 | Loss: 0.00002441
Iteration 70/1000 | Loss: 0.00002441
Iteration 71/1000 | Loss: 0.00002441
Iteration 72/1000 | Loss: 0.00002441
Iteration 73/1000 | Loss: 0.00002441
Iteration 74/1000 | Loss: 0.00002441
Iteration 75/1000 | Loss: 0.00002441
Iteration 76/1000 | Loss: 0.00002441
Iteration 77/1000 | Loss: 0.00002441
Iteration 78/1000 | Loss: 0.00002441
Iteration 79/1000 | Loss: 0.00002441
Iteration 80/1000 | Loss: 0.00002441
Iteration 81/1000 | Loss: 0.00002441
Iteration 82/1000 | Loss: 0.00002441
Iteration 83/1000 | Loss: 0.00002441
Iteration 84/1000 | Loss: 0.00002441
Iteration 85/1000 | Loss: 0.00002441
Iteration 86/1000 | Loss: 0.00002441
Iteration 87/1000 | Loss: 0.00002441
Iteration 88/1000 | Loss: 0.00002441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.4405861040577292e-05, 2.4405861040577292e-05, 2.4405861040577292e-05, 2.4405861040577292e-05, 2.4405861040577292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4405861040577292e-05

Optimization complete. Final v2v error: 4.2930169105529785 mm

Highest mean error: 4.481320858001709 mm for frame 151

Lowest mean error: 3.943863868713379 mm for frame 262

Saving results

Total time: 30.19325613975525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443245
Iteration 2/25 | Loss: 0.00158701
Iteration 3/25 | Loss: 0.00146717
Iteration 4/25 | Loss: 0.00145036
Iteration 5/25 | Loss: 0.00144403
Iteration 6/25 | Loss: 0.00144217
Iteration 7/25 | Loss: 0.00144189
Iteration 8/25 | Loss: 0.00144189
Iteration 9/25 | Loss: 0.00144189
Iteration 10/25 | Loss: 0.00144189
Iteration 11/25 | Loss: 0.00144189
Iteration 12/25 | Loss: 0.00144189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014418940991163254, 0.0014418940991163254, 0.0014418940991163254, 0.0014418940991163254, 0.0014418940991163254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014418940991163254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57324219
Iteration 2/25 | Loss: 0.00308362
Iteration 3/25 | Loss: 0.00308362
Iteration 4/25 | Loss: 0.00308361
Iteration 5/25 | Loss: 0.00308361
Iteration 6/25 | Loss: 0.00308361
Iteration 7/25 | Loss: 0.00308361
Iteration 8/25 | Loss: 0.00308361
Iteration 9/25 | Loss: 0.00308361
Iteration 10/25 | Loss: 0.00308361
Iteration 11/25 | Loss: 0.00308361
Iteration 12/25 | Loss: 0.00308361
Iteration 13/25 | Loss: 0.00308361
Iteration 14/25 | Loss: 0.00308361
Iteration 15/25 | Loss: 0.00308361
Iteration 16/25 | Loss: 0.00308361
Iteration 17/25 | Loss: 0.00308361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003083612537011504, 0.003083612537011504, 0.003083612537011504, 0.003083612537011504, 0.003083612537011504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003083612537011504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308361
Iteration 2/1000 | Loss: 0.00008314
Iteration 3/1000 | Loss: 0.00006240
Iteration 4/1000 | Loss: 0.00004654
Iteration 5/1000 | Loss: 0.00004213
Iteration 6/1000 | Loss: 0.00003892
Iteration 7/1000 | Loss: 0.00003749
Iteration 8/1000 | Loss: 0.00003633
Iteration 9/1000 | Loss: 0.00003511
Iteration 10/1000 | Loss: 0.00003423
Iteration 11/1000 | Loss: 0.00003371
Iteration 12/1000 | Loss: 0.00003337
Iteration 13/1000 | Loss: 0.00003312
Iteration 14/1000 | Loss: 0.00003303
Iteration 15/1000 | Loss: 0.00003291
Iteration 16/1000 | Loss: 0.00003289
Iteration 17/1000 | Loss: 0.00003288
Iteration 18/1000 | Loss: 0.00003286
Iteration 19/1000 | Loss: 0.00003285
Iteration 20/1000 | Loss: 0.00003285
Iteration 21/1000 | Loss: 0.00003285
Iteration 22/1000 | Loss: 0.00003284
Iteration 23/1000 | Loss: 0.00003284
Iteration 24/1000 | Loss: 0.00003283
Iteration 25/1000 | Loss: 0.00003283
Iteration 26/1000 | Loss: 0.00003283
Iteration 27/1000 | Loss: 0.00003280
Iteration 28/1000 | Loss: 0.00003277
Iteration 29/1000 | Loss: 0.00003275
Iteration 30/1000 | Loss: 0.00003275
Iteration 31/1000 | Loss: 0.00003274
Iteration 32/1000 | Loss: 0.00003273
Iteration 33/1000 | Loss: 0.00003273
Iteration 34/1000 | Loss: 0.00003273
Iteration 35/1000 | Loss: 0.00003272
Iteration 36/1000 | Loss: 0.00003270
Iteration 37/1000 | Loss: 0.00003269
Iteration 38/1000 | Loss: 0.00003269
Iteration 39/1000 | Loss: 0.00003269
Iteration 40/1000 | Loss: 0.00003268
Iteration 41/1000 | Loss: 0.00003268
Iteration 42/1000 | Loss: 0.00003267
Iteration 43/1000 | Loss: 0.00003267
Iteration 44/1000 | Loss: 0.00003266
Iteration 45/1000 | Loss: 0.00003266
Iteration 46/1000 | Loss: 0.00003266
Iteration 47/1000 | Loss: 0.00003265
Iteration 48/1000 | Loss: 0.00003265
Iteration 49/1000 | Loss: 0.00003264
Iteration 50/1000 | Loss: 0.00003264
Iteration 51/1000 | Loss: 0.00003264
Iteration 52/1000 | Loss: 0.00003264
Iteration 53/1000 | Loss: 0.00003263
Iteration 54/1000 | Loss: 0.00003263
Iteration 55/1000 | Loss: 0.00003263
Iteration 56/1000 | Loss: 0.00003263
Iteration 57/1000 | Loss: 0.00003262
Iteration 58/1000 | Loss: 0.00003262
Iteration 59/1000 | Loss: 0.00003262
Iteration 60/1000 | Loss: 0.00003262
Iteration 61/1000 | Loss: 0.00003262
Iteration 62/1000 | Loss: 0.00003262
Iteration 63/1000 | Loss: 0.00003262
Iteration 64/1000 | Loss: 0.00003261
Iteration 65/1000 | Loss: 0.00003261
Iteration 66/1000 | Loss: 0.00003261
Iteration 67/1000 | Loss: 0.00003261
Iteration 68/1000 | Loss: 0.00003261
Iteration 69/1000 | Loss: 0.00003261
Iteration 70/1000 | Loss: 0.00003260
Iteration 71/1000 | Loss: 0.00003260
Iteration 72/1000 | Loss: 0.00003260
Iteration 73/1000 | Loss: 0.00003260
Iteration 74/1000 | Loss: 0.00003259
Iteration 75/1000 | Loss: 0.00003259
Iteration 76/1000 | Loss: 0.00003259
Iteration 77/1000 | Loss: 0.00003259
Iteration 78/1000 | Loss: 0.00003259
Iteration 79/1000 | Loss: 0.00003258
Iteration 80/1000 | Loss: 0.00003258
Iteration 81/1000 | Loss: 0.00003258
Iteration 82/1000 | Loss: 0.00003258
Iteration 83/1000 | Loss: 0.00003258
Iteration 84/1000 | Loss: 0.00003258
Iteration 85/1000 | Loss: 0.00003258
Iteration 86/1000 | Loss: 0.00003257
Iteration 87/1000 | Loss: 0.00003257
Iteration 88/1000 | Loss: 0.00003257
Iteration 89/1000 | Loss: 0.00003257
Iteration 90/1000 | Loss: 0.00003257
Iteration 91/1000 | Loss: 0.00003257
Iteration 92/1000 | Loss: 0.00003257
Iteration 93/1000 | Loss: 0.00003257
Iteration 94/1000 | Loss: 0.00003257
Iteration 95/1000 | Loss: 0.00003257
Iteration 96/1000 | Loss: 0.00003257
Iteration 97/1000 | Loss: 0.00003257
Iteration 98/1000 | Loss: 0.00003256
Iteration 99/1000 | Loss: 0.00003256
Iteration 100/1000 | Loss: 0.00003256
Iteration 101/1000 | Loss: 0.00003256
Iteration 102/1000 | Loss: 0.00003256
Iteration 103/1000 | Loss: 0.00003256
Iteration 104/1000 | Loss: 0.00003256
Iteration 105/1000 | Loss: 0.00003256
Iteration 106/1000 | Loss: 0.00003256
Iteration 107/1000 | Loss: 0.00003256
Iteration 108/1000 | Loss: 0.00003256
Iteration 109/1000 | Loss: 0.00003256
Iteration 110/1000 | Loss: 0.00003255
Iteration 111/1000 | Loss: 0.00003255
Iteration 112/1000 | Loss: 0.00003255
Iteration 113/1000 | Loss: 0.00003255
Iteration 114/1000 | Loss: 0.00003255
Iteration 115/1000 | Loss: 0.00003255
Iteration 116/1000 | Loss: 0.00003255
Iteration 117/1000 | Loss: 0.00003255
Iteration 118/1000 | Loss: 0.00003254
Iteration 119/1000 | Loss: 0.00003254
Iteration 120/1000 | Loss: 0.00003254
Iteration 121/1000 | Loss: 0.00003254
Iteration 122/1000 | Loss: 0.00003254
Iteration 123/1000 | Loss: 0.00003254
Iteration 124/1000 | Loss: 0.00003254
Iteration 125/1000 | Loss: 0.00003254
Iteration 126/1000 | Loss: 0.00003253
Iteration 127/1000 | Loss: 0.00003253
Iteration 128/1000 | Loss: 0.00003253
Iteration 129/1000 | Loss: 0.00003253
Iteration 130/1000 | Loss: 0.00003253
Iteration 131/1000 | Loss: 0.00003253
Iteration 132/1000 | Loss: 0.00003253
Iteration 133/1000 | Loss: 0.00003253
Iteration 134/1000 | Loss: 0.00003253
Iteration 135/1000 | Loss: 0.00003253
Iteration 136/1000 | Loss: 0.00003253
Iteration 137/1000 | Loss: 0.00003252
Iteration 138/1000 | Loss: 0.00003252
Iteration 139/1000 | Loss: 0.00003252
Iteration 140/1000 | Loss: 0.00003252
Iteration 141/1000 | Loss: 0.00003252
Iteration 142/1000 | Loss: 0.00003252
Iteration 143/1000 | Loss: 0.00003252
Iteration 144/1000 | Loss: 0.00003252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.25240362144541e-05, 3.25240362144541e-05, 3.25240362144541e-05, 3.25240362144541e-05, 3.25240362144541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.25240362144541e-05

Optimization complete. Final v2v error: 4.829770088195801 mm

Highest mean error: 5.108962535858154 mm for frame 0

Lowest mean error: 4.288365364074707 mm for frame 92

Saving results

Total time: 40.585662841796875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01177825
Iteration 2/25 | Loss: 0.00247073
Iteration 3/25 | Loss: 0.00209972
Iteration 4/25 | Loss: 0.00198231
Iteration 5/25 | Loss: 0.00188022
Iteration 6/25 | Loss: 0.00173474
Iteration 7/25 | Loss: 0.00162123
Iteration 8/25 | Loss: 0.00159691
Iteration 9/25 | Loss: 0.00158192
Iteration 10/25 | Loss: 0.00157627
Iteration 11/25 | Loss: 0.00157282
Iteration 12/25 | Loss: 0.00157248
Iteration 13/25 | Loss: 0.00157201
Iteration 14/25 | Loss: 0.00157081
Iteration 15/25 | Loss: 0.00157054
Iteration 16/25 | Loss: 0.00157200
Iteration 17/25 | Loss: 0.00157031
Iteration 18/25 | Loss: 0.00157210
Iteration 19/25 | Loss: 0.00157020
Iteration 20/25 | Loss: 0.00157219
Iteration 21/25 | Loss: 0.00156999
Iteration 22/25 | Loss: 0.00157075
Iteration 23/25 | Loss: 0.00157020
Iteration 24/25 | Loss: 0.00157364
Iteration 25/25 | Loss: 0.00157215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04481363
Iteration 2/25 | Loss: 0.00329084
Iteration 3/25 | Loss: 0.00329037
Iteration 4/25 | Loss: 0.00329037
Iteration 5/25 | Loss: 0.00329037
Iteration 6/25 | Loss: 0.00329037
Iteration 7/25 | Loss: 0.00329037
Iteration 8/25 | Loss: 0.00329037
Iteration 9/25 | Loss: 0.00329037
Iteration 10/25 | Loss: 0.00329037
Iteration 11/25 | Loss: 0.00329037
Iteration 12/25 | Loss: 0.00329037
Iteration 13/25 | Loss: 0.00329037
Iteration 14/25 | Loss: 0.00329037
Iteration 15/25 | Loss: 0.00329037
Iteration 16/25 | Loss: 0.00329037
Iteration 17/25 | Loss: 0.00329037
Iteration 18/25 | Loss: 0.00329037
Iteration 19/25 | Loss: 0.00329037
Iteration 20/25 | Loss: 0.00329037
Iteration 21/25 | Loss: 0.00329037
Iteration 22/25 | Loss: 0.00329037
Iteration 23/25 | Loss: 0.00329037
Iteration 24/25 | Loss: 0.00329037
Iteration 25/25 | Loss: 0.00329037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329037
Iteration 2/1000 | Loss: 0.00015789
Iteration 3/1000 | Loss: 0.00023398
Iteration 4/1000 | Loss: 0.00034875
Iteration 5/1000 | Loss: 0.00024169
Iteration 6/1000 | Loss: 0.00014689
Iteration 7/1000 | Loss: 0.00026651
Iteration 8/1000 | Loss: 0.00021419
Iteration 9/1000 | Loss: 0.00029611
Iteration 10/1000 | Loss: 0.00024852
Iteration 11/1000 | Loss: 0.00024568
Iteration 12/1000 | Loss: 0.00022911
Iteration 13/1000 | Loss: 0.00022664
Iteration 14/1000 | Loss: 0.00049249
Iteration 15/1000 | Loss: 0.00043369
Iteration 16/1000 | Loss: 0.00036432
Iteration 17/1000 | Loss: 0.00008773
Iteration 18/1000 | Loss: 0.00007943
Iteration 19/1000 | Loss: 0.00006274
Iteration 20/1000 | Loss: 0.00008983
Iteration 21/1000 | Loss: 0.00006941
Iteration 22/1000 | Loss: 0.00006893
Iteration 23/1000 | Loss: 0.00007697
Iteration 24/1000 | Loss: 0.00006953
Iteration 25/1000 | Loss: 0.00006450
Iteration 26/1000 | Loss: 0.00007313
Iteration 27/1000 | Loss: 0.00006503
Iteration 28/1000 | Loss: 0.00007805
Iteration 29/1000 | Loss: 0.00008317
Iteration 30/1000 | Loss: 0.00006950
Iteration 31/1000 | Loss: 0.00006583
Iteration 32/1000 | Loss: 0.00006860
Iteration 33/1000 | Loss: 0.00009320
Iteration 34/1000 | Loss: 0.00007830
Iteration 35/1000 | Loss: 0.00007229
Iteration 36/1000 | Loss: 0.00007944
Iteration 37/1000 | Loss: 0.00006837
Iteration 38/1000 | Loss: 0.00008551
Iteration 39/1000 | Loss: 0.00006363
Iteration 40/1000 | Loss: 0.00006965
Iteration 41/1000 | Loss: 0.00006349
Iteration 42/1000 | Loss: 0.00006343
Iteration 43/1000 | Loss: 0.00006363
Iteration 44/1000 | Loss: 0.00005862
Iteration 45/1000 | Loss: 0.00005060
Iteration 46/1000 | Loss: 0.00005244
Iteration 47/1000 | Loss: 0.00005560
Iteration 48/1000 | Loss: 0.00005659
Iteration 49/1000 | Loss: 0.00005856
Iteration 50/1000 | Loss: 0.00005786
Iteration 51/1000 | Loss: 0.00005890
Iteration 52/1000 | Loss: 0.00005805
Iteration 53/1000 | Loss: 0.00005957
Iteration 54/1000 | Loss: 0.00006558
Iteration 55/1000 | Loss: 0.00005820
Iteration 56/1000 | Loss: 0.00005977
Iteration 57/1000 | Loss: 0.00006334
Iteration 58/1000 | Loss: 0.00006559
Iteration 59/1000 | Loss: 0.00005789
Iteration 60/1000 | Loss: 0.00006598
Iteration 61/1000 | Loss: 0.00007422
Iteration 62/1000 | Loss: 0.00005098
Iteration 63/1000 | Loss: 0.00004874
Iteration 64/1000 | Loss: 0.00004757
Iteration 65/1000 | Loss: 0.00004720
Iteration 66/1000 | Loss: 0.00004693
Iteration 67/1000 | Loss: 0.00004666
Iteration 68/1000 | Loss: 0.00004647
Iteration 69/1000 | Loss: 0.00004631
Iteration 70/1000 | Loss: 0.00004628
Iteration 71/1000 | Loss: 0.00004620
Iteration 72/1000 | Loss: 0.00004619
Iteration 73/1000 | Loss: 0.00004619
Iteration 74/1000 | Loss: 0.00004614
Iteration 75/1000 | Loss: 0.00004612
Iteration 76/1000 | Loss: 0.00004606
Iteration 77/1000 | Loss: 0.00004605
Iteration 78/1000 | Loss: 0.00004605
Iteration 79/1000 | Loss: 0.00004602
Iteration 80/1000 | Loss: 0.00004601
Iteration 81/1000 | Loss: 0.00004598
Iteration 82/1000 | Loss: 0.00004598
Iteration 83/1000 | Loss: 0.00004594
Iteration 84/1000 | Loss: 0.00004594
Iteration 85/1000 | Loss: 0.00004592
Iteration 86/1000 | Loss: 0.00004591
Iteration 87/1000 | Loss: 0.00004591
Iteration 88/1000 | Loss: 0.00004591
Iteration 89/1000 | Loss: 0.00004590
Iteration 90/1000 | Loss: 0.00004590
Iteration 91/1000 | Loss: 0.00004590
Iteration 92/1000 | Loss: 0.00004590
Iteration 93/1000 | Loss: 0.00004590
Iteration 94/1000 | Loss: 0.00004589
Iteration 95/1000 | Loss: 0.00004589
Iteration 96/1000 | Loss: 0.00004589
Iteration 97/1000 | Loss: 0.00004588
Iteration 98/1000 | Loss: 0.00004588
Iteration 99/1000 | Loss: 0.00004587
Iteration 100/1000 | Loss: 0.00004587
Iteration 101/1000 | Loss: 0.00004587
Iteration 102/1000 | Loss: 0.00004586
Iteration 103/1000 | Loss: 0.00004586
Iteration 104/1000 | Loss: 0.00004586
Iteration 105/1000 | Loss: 0.00004586
Iteration 106/1000 | Loss: 0.00004586
Iteration 107/1000 | Loss: 0.00004585
Iteration 108/1000 | Loss: 0.00004585
Iteration 109/1000 | Loss: 0.00004585
Iteration 110/1000 | Loss: 0.00004584
Iteration 111/1000 | Loss: 0.00004584
Iteration 112/1000 | Loss: 0.00004584
Iteration 113/1000 | Loss: 0.00004584
Iteration 114/1000 | Loss: 0.00004584
Iteration 115/1000 | Loss: 0.00004584
Iteration 116/1000 | Loss: 0.00004584
Iteration 117/1000 | Loss: 0.00004583
Iteration 118/1000 | Loss: 0.00004583
Iteration 119/1000 | Loss: 0.00004583
Iteration 120/1000 | Loss: 0.00004583
Iteration 121/1000 | Loss: 0.00004583
Iteration 122/1000 | Loss: 0.00004583
Iteration 123/1000 | Loss: 0.00004583
Iteration 124/1000 | Loss: 0.00004583
Iteration 125/1000 | Loss: 0.00004583
Iteration 126/1000 | Loss: 0.00004583
Iteration 127/1000 | Loss: 0.00004583
Iteration 128/1000 | Loss: 0.00004583
Iteration 129/1000 | Loss: 0.00004582
Iteration 130/1000 | Loss: 0.00004582
Iteration 131/1000 | Loss: 0.00004582
Iteration 132/1000 | Loss: 0.00004582
Iteration 133/1000 | Loss: 0.00004581
Iteration 134/1000 | Loss: 0.00004581
Iteration 135/1000 | Loss: 0.00004581
Iteration 136/1000 | Loss: 0.00004581
Iteration 137/1000 | Loss: 0.00004580
Iteration 138/1000 | Loss: 0.00004580
Iteration 139/1000 | Loss: 0.00004580
Iteration 140/1000 | Loss: 0.00004580
Iteration 141/1000 | Loss: 0.00004580
Iteration 142/1000 | Loss: 0.00004579
Iteration 143/1000 | Loss: 0.00004579
Iteration 144/1000 | Loss: 0.00004579
Iteration 145/1000 | Loss: 0.00004579
Iteration 146/1000 | Loss: 0.00004579
Iteration 147/1000 | Loss: 0.00004578
Iteration 148/1000 | Loss: 0.00004578
Iteration 149/1000 | Loss: 0.00004578
Iteration 150/1000 | Loss: 0.00004578
Iteration 151/1000 | Loss: 0.00004578
Iteration 152/1000 | Loss: 0.00004578
Iteration 153/1000 | Loss: 0.00004578
Iteration 154/1000 | Loss: 0.00004578
Iteration 155/1000 | Loss: 0.00004578
Iteration 156/1000 | Loss: 0.00004578
Iteration 157/1000 | Loss: 0.00004577
Iteration 158/1000 | Loss: 0.00004577
Iteration 159/1000 | Loss: 0.00004577
Iteration 160/1000 | Loss: 0.00004577
Iteration 161/1000 | Loss: 0.00004577
Iteration 162/1000 | Loss: 0.00004577
Iteration 163/1000 | Loss: 0.00004577
Iteration 164/1000 | Loss: 0.00004576
Iteration 165/1000 | Loss: 0.00004576
Iteration 166/1000 | Loss: 0.00004576
Iteration 167/1000 | Loss: 0.00004576
Iteration 168/1000 | Loss: 0.00004576
Iteration 169/1000 | Loss: 0.00004576
Iteration 170/1000 | Loss: 0.00004576
Iteration 171/1000 | Loss: 0.00004575
Iteration 172/1000 | Loss: 0.00004575
Iteration 173/1000 | Loss: 0.00004575
Iteration 174/1000 | Loss: 0.00004575
Iteration 175/1000 | Loss: 0.00004575
Iteration 176/1000 | Loss: 0.00004575
Iteration 177/1000 | Loss: 0.00004575
Iteration 178/1000 | Loss: 0.00004575
Iteration 179/1000 | Loss: 0.00004575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [4.5752782170893624e-05, 4.5752782170893624e-05, 4.5752782170893624e-05, 4.5752782170893624e-05, 4.5752782170893624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5752782170893624e-05

Optimization complete. Final v2v error: 5.702236175537109 mm

Highest mean error: 6.839137077331543 mm for frame 88

Lowest mean error: 4.584136486053467 mm for frame 123

Saving results

Total time: 156.19734454154968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079160
Iteration 2/25 | Loss: 0.00188481
Iteration 3/25 | Loss: 0.00147066
Iteration 4/25 | Loss: 0.00145050
Iteration 5/25 | Loss: 0.00147821
Iteration 6/25 | Loss: 0.00143900
Iteration 7/25 | Loss: 0.00141617
Iteration 8/25 | Loss: 0.00141016
Iteration 9/25 | Loss: 0.00141004
Iteration 10/25 | Loss: 0.00140362
Iteration 11/25 | Loss: 0.00138909
Iteration 12/25 | Loss: 0.00139548
Iteration 13/25 | Loss: 0.00139230
Iteration 14/25 | Loss: 0.00138620
Iteration 15/25 | Loss: 0.00138553
Iteration 16/25 | Loss: 0.00138408
Iteration 17/25 | Loss: 0.00138275
Iteration 18/25 | Loss: 0.00138235
Iteration 19/25 | Loss: 0.00138228
Iteration 20/25 | Loss: 0.00138228
Iteration 21/25 | Loss: 0.00138228
Iteration 22/25 | Loss: 0.00138228
Iteration 23/25 | Loss: 0.00138227
Iteration 24/25 | Loss: 0.00138227
Iteration 25/25 | Loss: 0.00138227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.08774185
Iteration 2/25 | Loss: 0.00335407
Iteration 3/25 | Loss: 0.00229600
Iteration 4/25 | Loss: 0.00229600
Iteration 5/25 | Loss: 0.00229600
Iteration 6/25 | Loss: 0.00229600
Iteration 7/25 | Loss: 0.00229600
Iteration 8/25 | Loss: 0.00229599
Iteration 9/25 | Loss: 0.00229599
Iteration 10/25 | Loss: 0.00229599
Iteration 11/25 | Loss: 0.00229599
Iteration 12/25 | Loss: 0.00229599
Iteration 13/25 | Loss: 0.00229599
Iteration 14/25 | Loss: 0.00229599
Iteration 15/25 | Loss: 0.00229599
Iteration 16/25 | Loss: 0.00229599
Iteration 17/25 | Loss: 0.00229599
Iteration 18/25 | Loss: 0.00229599
Iteration 19/25 | Loss: 0.00229599
Iteration 20/25 | Loss: 0.00229599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022959948983043432, 0.0022959948983043432, 0.0022959948983043432, 0.0022959948983043432, 0.0022959948983043432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022959948983043432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229599
Iteration 2/1000 | Loss: 0.00004768
Iteration 3/1000 | Loss: 0.00003632
Iteration 4/1000 | Loss: 0.00003281
Iteration 5/1000 | Loss: 0.00003095
Iteration 6/1000 | Loss: 0.00046412
Iteration 7/1000 | Loss: 0.00003881
Iteration 8/1000 | Loss: 0.00002949
Iteration 9/1000 | Loss: 0.00002752
Iteration 10/1000 | Loss: 0.00002615
Iteration 11/1000 | Loss: 0.00002549
Iteration 12/1000 | Loss: 0.00002502
Iteration 13/1000 | Loss: 0.00002479
Iteration 14/1000 | Loss: 0.00002465
Iteration 15/1000 | Loss: 0.00002449
Iteration 16/1000 | Loss: 0.00002440
Iteration 17/1000 | Loss: 0.00002439
Iteration 18/1000 | Loss: 0.00002437
Iteration 19/1000 | Loss: 0.00002437
Iteration 20/1000 | Loss: 0.00002434
Iteration 21/1000 | Loss: 0.00002434
Iteration 22/1000 | Loss: 0.00002433
Iteration 23/1000 | Loss: 0.00002432
Iteration 24/1000 | Loss: 0.00002431
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002428
Iteration 28/1000 | Loss: 0.00002427
Iteration 29/1000 | Loss: 0.00002427
Iteration 30/1000 | Loss: 0.00002427
Iteration 31/1000 | Loss: 0.00002426
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002424
Iteration 36/1000 | Loss: 0.00002424
Iteration 37/1000 | Loss: 0.00002424
Iteration 38/1000 | Loss: 0.00002424
Iteration 39/1000 | Loss: 0.00002424
Iteration 40/1000 | Loss: 0.00002424
Iteration 41/1000 | Loss: 0.00002424
Iteration 42/1000 | Loss: 0.00002424
Iteration 43/1000 | Loss: 0.00002424
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002422
Iteration 49/1000 | Loss: 0.00002422
Iteration 50/1000 | Loss: 0.00002422
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002422
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002421
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002421
Iteration 63/1000 | Loss: 0.00002421
Iteration 64/1000 | Loss: 0.00002421
Iteration 65/1000 | Loss: 0.00002421
Iteration 66/1000 | Loss: 0.00002420
Iteration 67/1000 | Loss: 0.00002420
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002420
Iteration 70/1000 | Loss: 0.00002420
Iteration 71/1000 | Loss: 0.00002420
Iteration 72/1000 | Loss: 0.00002420
Iteration 73/1000 | Loss: 0.00002420
Iteration 74/1000 | Loss: 0.00002420
Iteration 75/1000 | Loss: 0.00002420
Iteration 76/1000 | Loss: 0.00002420
Iteration 77/1000 | Loss: 0.00002419
Iteration 78/1000 | Loss: 0.00002419
Iteration 79/1000 | Loss: 0.00002419
Iteration 80/1000 | Loss: 0.00002419
Iteration 81/1000 | Loss: 0.00002418
Iteration 82/1000 | Loss: 0.00002418
Iteration 83/1000 | Loss: 0.00002418
Iteration 84/1000 | Loss: 0.00002418
Iteration 85/1000 | Loss: 0.00002418
Iteration 86/1000 | Loss: 0.00002418
Iteration 87/1000 | Loss: 0.00002418
Iteration 88/1000 | Loss: 0.00002418
Iteration 89/1000 | Loss: 0.00002418
Iteration 90/1000 | Loss: 0.00002418
Iteration 91/1000 | Loss: 0.00002418
Iteration 92/1000 | Loss: 0.00002418
Iteration 93/1000 | Loss: 0.00002418
Iteration 94/1000 | Loss: 0.00002418
Iteration 95/1000 | Loss: 0.00002417
Iteration 96/1000 | Loss: 0.00002417
Iteration 97/1000 | Loss: 0.00002417
Iteration 98/1000 | Loss: 0.00002417
Iteration 99/1000 | Loss: 0.00002417
Iteration 100/1000 | Loss: 0.00002417
Iteration 101/1000 | Loss: 0.00002417
Iteration 102/1000 | Loss: 0.00002417
Iteration 103/1000 | Loss: 0.00002417
Iteration 104/1000 | Loss: 0.00002417
Iteration 105/1000 | Loss: 0.00002417
Iteration 106/1000 | Loss: 0.00002417
Iteration 107/1000 | Loss: 0.00002417
Iteration 108/1000 | Loss: 0.00002417
Iteration 109/1000 | Loss: 0.00002417
Iteration 110/1000 | Loss: 0.00002417
Iteration 111/1000 | Loss: 0.00002417
Iteration 112/1000 | Loss: 0.00002417
Iteration 113/1000 | Loss: 0.00002417
Iteration 114/1000 | Loss: 0.00002417
Iteration 115/1000 | Loss: 0.00002417
Iteration 116/1000 | Loss: 0.00002417
Iteration 117/1000 | Loss: 0.00002417
Iteration 118/1000 | Loss: 0.00002417
Iteration 119/1000 | Loss: 0.00002417
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00002417
Iteration 122/1000 | Loss: 0.00002417
Iteration 123/1000 | Loss: 0.00002417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.416998177068308e-05, 2.416998177068308e-05, 2.416998177068308e-05, 2.416998177068308e-05, 2.416998177068308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.416998177068308e-05

Optimization complete. Final v2v error: 4.310664176940918 mm

Highest mean error: 4.9147844314575195 mm for frame 81

Lowest mean error: 3.999372959136963 mm for frame 6

Saving results

Total time: 59.78986191749573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106453
Iteration 2/25 | Loss: 0.00207642
Iteration 3/25 | Loss: 0.00156723
Iteration 4/25 | Loss: 0.00152130
Iteration 5/25 | Loss: 0.00151014
Iteration 6/25 | Loss: 0.00150755
Iteration 7/25 | Loss: 0.00150695
Iteration 8/25 | Loss: 0.00150695
Iteration 9/25 | Loss: 0.00150695
Iteration 10/25 | Loss: 0.00150695
Iteration 11/25 | Loss: 0.00150695
Iteration 12/25 | Loss: 0.00150695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015069517539814115, 0.0015069517539814115, 0.0015069517539814115, 0.0015069517539814115, 0.0015069517539814115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015069517539814115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02644444
Iteration 2/25 | Loss: 0.00184705
Iteration 3/25 | Loss: 0.00184705
Iteration 4/25 | Loss: 0.00184705
Iteration 5/25 | Loss: 0.00184705
Iteration 6/25 | Loss: 0.00184704
Iteration 7/25 | Loss: 0.00184704
Iteration 8/25 | Loss: 0.00184704
Iteration 9/25 | Loss: 0.00184704
Iteration 10/25 | Loss: 0.00184704
Iteration 11/25 | Loss: 0.00184704
Iteration 12/25 | Loss: 0.00184704
Iteration 13/25 | Loss: 0.00184704
Iteration 14/25 | Loss: 0.00184704
Iteration 15/25 | Loss: 0.00184704
Iteration 16/25 | Loss: 0.00184704
Iteration 17/25 | Loss: 0.00184704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001847044681198895, 0.001847044681198895, 0.001847044681198895, 0.001847044681198895, 0.001847044681198895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001847044681198895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184704
Iteration 2/1000 | Loss: 0.00007231
Iteration 3/1000 | Loss: 0.00005582
Iteration 4/1000 | Loss: 0.00005118
Iteration 5/1000 | Loss: 0.00004904
Iteration 6/1000 | Loss: 0.00004710
Iteration 7/1000 | Loss: 0.00004601
Iteration 8/1000 | Loss: 0.00004518
Iteration 9/1000 | Loss: 0.00004460
Iteration 10/1000 | Loss: 0.00004425
Iteration 11/1000 | Loss: 0.00004397
Iteration 12/1000 | Loss: 0.00004385
Iteration 13/1000 | Loss: 0.00004380
Iteration 14/1000 | Loss: 0.00004376
Iteration 15/1000 | Loss: 0.00004375
Iteration 16/1000 | Loss: 0.00004374
Iteration 17/1000 | Loss: 0.00004370
Iteration 18/1000 | Loss: 0.00004365
Iteration 19/1000 | Loss: 0.00004364
Iteration 20/1000 | Loss: 0.00004362
Iteration 21/1000 | Loss: 0.00004359
Iteration 22/1000 | Loss: 0.00004359
Iteration 23/1000 | Loss: 0.00004359
Iteration 24/1000 | Loss: 0.00004358
Iteration 25/1000 | Loss: 0.00004358
Iteration 26/1000 | Loss: 0.00004356
Iteration 27/1000 | Loss: 0.00004356
Iteration 28/1000 | Loss: 0.00004355
Iteration 29/1000 | Loss: 0.00004355
Iteration 30/1000 | Loss: 0.00004355
Iteration 31/1000 | Loss: 0.00004354
Iteration 32/1000 | Loss: 0.00004353
Iteration 33/1000 | Loss: 0.00004352
Iteration 34/1000 | Loss: 0.00004352
Iteration 35/1000 | Loss: 0.00004351
Iteration 36/1000 | Loss: 0.00004351
Iteration 37/1000 | Loss: 0.00004351
Iteration 38/1000 | Loss: 0.00004350
Iteration 39/1000 | Loss: 0.00004350
Iteration 40/1000 | Loss: 0.00004350
Iteration 41/1000 | Loss: 0.00004350
Iteration 42/1000 | Loss: 0.00004349
Iteration 43/1000 | Loss: 0.00004349
Iteration 44/1000 | Loss: 0.00004349
Iteration 45/1000 | Loss: 0.00004349
Iteration 46/1000 | Loss: 0.00004348
Iteration 47/1000 | Loss: 0.00004348
Iteration 48/1000 | Loss: 0.00004348
Iteration 49/1000 | Loss: 0.00004348
Iteration 50/1000 | Loss: 0.00004347
Iteration 51/1000 | Loss: 0.00004347
Iteration 52/1000 | Loss: 0.00004347
Iteration 53/1000 | Loss: 0.00004347
Iteration 54/1000 | Loss: 0.00004347
Iteration 55/1000 | Loss: 0.00004347
Iteration 56/1000 | Loss: 0.00004346
Iteration 57/1000 | Loss: 0.00004346
Iteration 58/1000 | Loss: 0.00004346
Iteration 59/1000 | Loss: 0.00004346
Iteration 60/1000 | Loss: 0.00004346
Iteration 61/1000 | Loss: 0.00004346
Iteration 62/1000 | Loss: 0.00004345
Iteration 63/1000 | Loss: 0.00004345
Iteration 64/1000 | Loss: 0.00004345
Iteration 65/1000 | Loss: 0.00004345
Iteration 66/1000 | Loss: 0.00004345
Iteration 67/1000 | Loss: 0.00004345
Iteration 68/1000 | Loss: 0.00004345
Iteration 69/1000 | Loss: 0.00004345
Iteration 70/1000 | Loss: 0.00004345
Iteration 71/1000 | Loss: 0.00004345
Iteration 72/1000 | Loss: 0.00004345
Iteration 73/1000 | Loss: 0.00004344
Iteration 74/1000 | Loss: 0.00004344
Iteration 75/1000 | Loss: 0.00004344
Iteration 76/1000 | Loss: 0.00004343
Iteration 77/1000 | Loss: 0.00004343
Iteration 78/1000 | Loss: 0.00004343
Iteration 79/1000 | Loss: 0.00004343
Iteration 80/1000 | Loss: 0.00004342
Iteration 81/1000 | Loss: 0.00004342
Iteration 82/1000 | Loss: 0.00004342
Iteration 83/1000 | Loss: 0.00004342
Iteration 84/1000 | Loss: 0.00004342
Iteration 85/1000 | Loss: 0.00004342
Iteration 86/1000 | Loss: 0.00004342
Iteration 87/1000 | Loss: 0.00004342
Iteration 88/1000 | Loss: 0.00004342
Iteration 89/1000 | Loss: 0.00004342
Iteration 90/1000 | Loss: 0.00004341
Iteration 91/1000 | Loss: 0.00004341
Iteration 92/1000 | Loss: 0.00004341
Iteration 93/1000 | Loss: 0.00004341
Iteration 94/1000 | Loss: 0.00004341
Iteration 95/1000 | Loss: 0.00004341
Iteration 96/1000 | Loss: 0.00004341
Iteration 97/1000 | Loss: 0.00004341
Iteration 98/1000 | Loss: 0.00004340
Iteration 99/1000 | Loss: 0.00004340
Iteration 100/1000 | Loss: 0.00004340
Iteration 101/1000 | Loss: 0.00004340
Iteration 102/1000 | Loss: 0.00004340
Iteration 103/1000 | Loss: 0.00004340
Iteration 104/1000 | Loss: 0.00004340
Iteration 105/1000 | Loss: 0.00004339
Iteration 106/1000 | Loss: 0.00004339
Iteration 107/1000 | Loss: 0.00004339
Iteration 108/1000 | Loss: 0.00004339
Iteration 109/1000 | Loss: 0.00004339
Iteration 110/1000 | Loss: 0.00004339
Iteration 111/1000 | Loss: 0.00004339
Iteration 112/1000 | Loss: 0.00004339
Iteration 113/1000 | Loss: 0.00004338
Iteration 114/1000 | Loss: 0.00004338
Iteration 115/1000 | Loss: 0.00004338
Iteration 116/1000 | Loss: 0.00004338
Iteration 117/1000 | Loss: 0.00004338
Iteration 118/1000 | Loss: 0.00004337
Iteration 119/1000 | Loss: 0.00004337
Iteration 120/1000 | Loss: 0.00004337
Iteration 121/1000 | Loss: 0.00004337
Iteration 122/1000 | Loss: 0.00004337
Iteration 123/1000 | Loss: 0.00004336
Iteration 124/1000 | Loss: 0.00004336
Iteration 125/1000 | Loss: 0.00004336
Iteration 126/1000 | Loss: 0.00004335
Iteration 127/1000 | Loss: 0.00004335
Iteration 128/1000 | Loss: 0.00004335
Iteration 129/1000 | Loss: 0.00004335
Iteration 130/1000 | Loss: 0.00004335
Iteration 131/1000 | Loss: 0.00004335
Iteration 132/1000 | Loss: 0.00004335
Iteration 133/1000 | Loss: 0.00004335
Iteration 134/1000 | Loss: 0.00004334
Iteration 135/1000 | Loss: 0.00004334
Iteration 136/1000 | Loss: 0.00004334
Iteration 137/1000 | Loss: 0.00004334
Iteration 138/1000 | Loss: 0.00004334
Iteration 139/1000 | Loss: 0.00004334
Iteration 140/1000 | Loss: 0.00004334
Iteration 141/1000 | Loss: 0.00004334
Iteration 142/1000 | Loss: 0.00004334
Iteration 143/1000 | Loss: 0.00004334
Iteration 144/1000 | Loss: 0.00004334
Iteration 145/1000 | Loss: 0.00004333
Iteration 146/1000 | Loss: 0.00004333
Iteration 147/1000 | Loss: 0.00004333
Iteration 148/1000 | Loss: 0.00004333
Iteration 149/1000 | Loss: 0.00004333
Iteration 150/1000 | Loss: 0.00004333
Iteration 151/1000 | Loss: 0.00004333
Iteration 152/1000 | Loss: 0.00004333
Iteration 153/1000 | Loss: 0.00004332
Iteration 154/1000 | Loss: 0.00004332
Iteration 155/1000 | Loss: 0.00004332
Iteration 156/1000 | Loss: 0.00004332
Iteration 157/1000 | Loss: 0.00004332
Iteration 158/1000 | Loss: 0.00004332
Iteration 159/1000 | Loss: 0.00004332
Iteration 160/1000 | Loss: 0.00004331
Iteration 161/1000 | Loss: 0.00004331
Iteration 162/1000 | Loss: 0.00004331
Iteration 163/1000 | Loss: 0.00004331
Iteration 164/1000 | Loss: 0.00004331
Iteration 165/1000 | Loss: 0.00004331
Iteration 166/1000 | Loss: 0.00004331
Iteration 167/1000 | Loss: 0.00004331
Iteration 168/1000 | Loss: 0.00004331
Iteration 169/1000 | Loss: 0.00004331
Iteration 170/1000 | Loss: 0.00004331
Iteration 171/1000 | Loss: 0.00004330
Iteration 172/1000 | Loss: 0.00004330
Iteration 173/1000 | Loss: 0.00004330
Iteration 174/1000 | Loss: 0.00004330
Iteration 175/1000 | Loss: 0.00004330
Iteration 176/1000 | Loss: 0.00004330
Iteration 177/1000 | Loss: 0.00004330
Iteration 178/1000 | Loss: 0.00004330
Iteration 179/1000 | Loss: 0.00004330
Iteration 180/1000 | Loss: 0.00004330
Iteration 181/1000 | Loss: 0.00004330
Iteration 182/1000 | Loss: 0.00004330
Iteration 183/1000 | Loss: 0.00004330
Iteration 184/1000 | Loss: 0.00004330
Iteration 185/1000 | Loss: 0.00004330
Iteration 186/1000 | Loss: 0.00004330
Iteration 187/1000 | Loss: 0.00004330
Iteration 188/1000 | Loss: 0.00004330
Iteration 189/1000 | Loss: 0.00004330
Iteration 190/1000 | Loss: 0.00004330
Iteration 191/1000 | Loss: 0.00004330
Iteration 192/1000 | Loss: 0.00004330
Iteration 193/1000 | Loss: 0.00004330
Iteration 194/1000 | Loss: 0.00004330
Iteration 195/1000 | Loss: 0.00004330
Iteration 196/1000 | Loss: 0.00004330
Iteration 197/1000 | Loss: 0.00004330
Iteration 198/1000 | Loss: 0.00004330
Iteration 199/1000 | Loss: 0.00004330
Iteration 200/1000 | Loss: 0.00004330
Iteration 201/1000 | Loss: 0.00004330
Iteration 202/1000 | Loss: 0.00004330
Iteration 203/1000 | Loss: 0.00004330
Iteration 204/1000 | Loss: 0.00004330
Iteration 205/1000 | Loss: 0.00004330
Iteration 206/1000 | Loss: 0.00004330
Iteration 207/1000 | Loss: 0.00004330
Iteration 208/1000 | Loss: 0.00004330
Iteration 209/1000 | Loss: 0.00004330
Iteration 210/1000 | Loss: 0.00004330
Iteration 211/1000 | Loss: 0.00004330
Iteration 212/1000 | Loss: 0.00004330
Iteration 213/1000 | Loss: 0.00004330
Iteration 214/1000 | Loss: 0.00004330
Iteration 215/1000 | Loss: 0.00004330
Iteration 216/1000 | Loss: 0.00004330
Iteration 217/1000 | Loss: 0.00004330
Iteration 218/1000 | Loss: 0.00004330
Iteration 219/1000 | Loss: 0.00004330
Iteration 220/1000 | Loss: 0.00004330
Iteration 221/1000 | Loss: 0.00004330
Iteration 222/1000 | Loss: 0.00004330
Iteration 223/1000 | Loss: 0.00004330
Iteration 224/1000 | Loss: 0.00004330
Iteration 225/1000 | Loss: 0.00004330
Iteration 226/1000 | Loss: 0.00004330
Iteration 227/1000 | Loss: 0.00004330
Iteration 228/1000 | Loss: 0.00004330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [4.32979068136774e-05, 4.32979068136774e-05, 4.32979068136774e-05, 4.32979068136774e-05, 4.32979068136774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.32979068136774e-05

Optimization complete. Final v2v error: 5.388943672180176 mm

Highest mean error: 6.11962890625 mm for frame 104

Lowest mean error: 4.427268981933594 mm for frame 53

Saving results

Total time: 48.80839419364929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01150153
Iteration 2/25 | Loss: 0.01150153
Iteration 3/25 | Loss: 0.00312677
Iteration 4/25 | Loss: 0.00221189
Iteration 5/25 | Loss: 0.00206963
Iteration 6/25 | Loss: 0.00190959
Iteration 7/25 | Loss: 0.00165420
Iteration 8/25 | Loss: 0.00163064
Iteration 9/25 | Loss: 0.00160350
Iteration 10/25 | Loss: 0.00158074
Iteration 11/25 | Loss: 0.00158715
Iteration 12/25 | Loss: 0.00157160
Iteration 13/25 | Loss: 0.00156721
Iteration 14/25 | Loss: 0.00155938
Iteration 15/25 | Loss: 0.00156887
Iteration 16/25 | Loss: 0.00156012
Iteration 17/25 | Loss: 0.00155119
Iteration 18/25 | Loss: 0.00154835
Iteration 19/25 | Loss: 0.00155039
Iteration 20/25 | Loss: 0.00154853
Iteration 21/25 | Loss: 0.00154765
Iteration 22/25 | Loss: 0.00153761
Iteration 23/25 | Loss: 0.00153842
Iteration 24/25 | Loss: 0.00153334
Iteration 25/25 | Loss: 0.00153082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63318348
Iteration 2/25 | Loss: 0.00369110
Iteration 3/25 | Loss: 0.00362724
Iteration 4/25 | Loss: 0.00362724
Iteration 5/25 | Loss: 0.00362724
Iteration 6/25 | Loss: 0.00362724
Iteration 7/25 | Loss: 0.00362724
Iteration 8/25 | Loss: 0.00362724
Iteration 9/25 | Loss: 0.00362724
Iteration 10/25 | Loss: 0.00362724
Iteration 11/25 | Loss: 0.00362724
Iteration 12/25 | Loss: 0.00362724
Iteration 13/25 | Loss: 0.00362724
Iteration 14/25 | Loss: 0.00362724
Iteration 15/25 | Loss: 0.00362724
Iteration 16/25 | Loss: 0.00362724
Iteration 17/25 | Loss: 0.00362724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003627239493653178, 0.003627239493653178, 0.003627239493653178, 0.003627239493653178, 0.003627239493653178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003627239493653178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362724
Iteration 2/1000 | Loss: 0.00046119
Iteration 3/1000 | Loss: 0.00024404
Iteration 4/1000 | Loss: 0.00021661
Iteration 5/1000 | Loss: 0.00010232
Iteration 6/1000 | Loss: 0.00023925
Iteration 7/1000 | Loss: 0.00023082
Iteration 8/1000 | Loss: 0.00011413
Iteration 9/1000 | Loss: 0.00011360
Iteration 10/1000 | Loss: 0.00047444
Iteration 11/1000 | Loss: 0.00007804
Iteration 12/1000 | Loss: 0.00084253
Iteration 13/1000 | Loss: 0.00092016
Iteration 14/1000 | Loss: 0.00008707
Iteration 15/1000 | Loss: 0.00081264
Iteration 16/1000 | Loss: 0.00190801
Iteration 17/1000 | Loss: 0.00019842
Iteration 18/1000 | Loss: 0.00012379
Iteration 19/1000 | Loss: 0.00017039
Iteration 20/1000 | Loss: 0.00100346
Iteration 21/1000 | Loss: 0.00015530
Iteration 22/1000 | Loss: 0.00008813
Iteration 23/1000 | Loss: 0.00006961
Iteration 24/1000 | Loss: 0.00012911
Iteration 25/1000 | Loss: 0.00019993
Iteration 26/1000 | Loss: 0.00015511
Iteration 27/1000 | Loss: 0.00007771
Iteration 28/1000 | Loss: 0.00015675
Iteration 29/1000 | Loss: 0.00019302
Iteration 30/1000 | Loss: 0.00016704
Iteration 31/1000 | Loss: 0.00007255
Iteration 32/1000 | Loss: 0.00022775
Iteration 33/1000 | Loss: 0.00017847
Iteration 34/1000 | Loss: 0.00021889
Iteration 35/1000 | Loss: 0.00017936
Iteration 36/1000 | Loss: 0.00018933
Iteration 37/1000 | Loss: 0.00018936
Iteration 38/1000 | Loss: 0.00018166
Iteration 39/1000 | Loss: 0.00019981
Iteration 40/1000 | Loss: 0.00016802
Iteration 41/1000 | Loss: 0.00007703
Iteration 42/1000 | Loss: 0.00020485
Iteration 43/1000 | Loss: 0.00005569
Iteration 44/1000 | Loss: 0.00007358
Iteration 45/1000 | Loss: 0.00009762
Iteration 46/1000 | Loss: 0.00006441
Iteration 47/1000 | Loss: 0.00007222
Iteration 48/1000 | Loss: 0.00006530
Iteration 49/1000 | Loss: 0.00006347
Iteration 50/1000 | Loss: 0.00006061
Iteration 51/1000 | Loss: 0.00005858
Iteration 52/1000 | Loss: 0.00006030
Iteration 53/1000 | Loss: 0.00005388
Iteration 54/1000 | Loss: 0.00004130
Iteration 55/1000 | Loss: 0.00005555
Iteration 56/1000 | Loss: 0.00038995
Iteration 57/1000 | Loss: 0.00027281
Iteration 58/1000 | Loss: 0.00007110
Iteration 59/1000 | Loss: 0.00012698
Iteration 60/1000 | Loss: 0.00005261
Iteration 61/1000 | Loss: 0.00005244
Iteration 62/1000 | Loss: 0.00004316
Iteration 63/1000 | Loss: 0.00004462
Iteration 64/1000 | Loss: 0.00004934
Iteration 65/1000 | Loss: 0.00006370
Iteration 66/1000 | Loss: 0.00003761
Iteration 67/1000 | Loss: 0.00004473
Iteration 68/1000 | Loss: 0.00003777
Iteration 69/1000 | Loss: 0.00003859
Iteration 70/1000 | Loss: 0.00003615
Iteration 71/1000 | Loss: 0.00004371
Iteration 72/1000 | Loss: 0.00003580
Iteration 73/1000 | Loss: 0.00003575
Iteration 74/1000 | Loss: 0.00003575
Iteration 75/1000 | Loss: 0.00003575
Iteration 76/1000 | Loss: 0.00003575
Iteration 77/1000 | Loss: 0.00003575
Iteration 78/1000 | Loss: 0.00003575
Iteration 79/1000 | Loss: 0.00003575
Iteration 80/1000 | Loss: 0.00003575
Iteration 81/1000 | Loss: 0.00003575
Iteration 82/1000 | Loss: 0.00003575
Iteration 83/1000 | Loss: 0.00003574
Iteration 84/1000 | Loss: 0.00003574
Iteration 85/1000 | Loss: 0.00003565
Iteration 86/1000 | Loss: 0.00003556
Iteration 87/1000 | Loss: 0.00003554
Iteration 88/1000 | Loss: 0.00003553
Iteration 89/1000 | Loss: 0.00003552
Iteration 90/1000 | Loss: 0.00005796
Iteration 91/1000 | Loss: 0.00003549
Iteration 92/1000 | Loss: 0.00003549
Iteration 93/1000 | Loss: 0.00003549
Iteration 94/1000 | Loss: 0.00003549
Iteration 95/1000 | Loss: 0.00003549
Iteration 96/1000 | Loss: 0.00003549
Iteration 97/1000 | Loss: 0.00003548
Iteration 98/1000 | Loss: 0.00003547
Iteration 99/1000 | Loss: 0.00003547
Iteration 100/1000 | Loss: 0.00003547
Iteration 101/1000 | Loss: 0.00003546
Iteration 102/1000 | Loss: 0.00003546
Iteration 103/1000 | Loss: 0.00003546
Iteration 104/1000 | Loss: 0.00003546
Iteration 105/1000 | Loss: 0.00003546
Iteration 106/1000 | Loss: 0.00003546
Iteration 107/1000 | Loss: 0.00003545
Iteration 108/1000 | Loss: 0.00003545
Iteration 109/1000 | Loss: 0.00003544
Iteration 110/1000 | Loss: 0.00003544
Iteration 111/1000 | Loss: 0.00003544
Iteration 112/1000 | Loss: 0.00003544
Iteration 113/1000 | Loss: 0.00003543
Iteration 114/1000 | Loss: 0.00003543
Iteration 115/1000 | Loss: 0.00003543
Iteration 116/1000 | Loss: 0.00003543
Iteration 117/1000 | Loss: 0.00003543
Iteration 118/1000 | Loss: 0.00003543
Iteration 119/1000 | Loss: 0.00003543
Iteration 120/1000 | Loss: 0.00003543
Iteration 121/1000 | Loss: 0.00003543
Iteration 122/1000 | Loss: 0.00003543
Iteration 123/1000 | Loss: 0.00003543
Iteration 124/1000 | Loss: 0.00003543
Iteration 125/1000 | Loss: 0.00003542
Iteration 126/1000 | Loss: 0.00003542
Iteration 127/1000 | Loss: 0.00003542
Iteration 128/1000 | Loss: 0.00005109
Iteration 129/1000 | Loss: 0.00003548
Iteration 130/1000 | Loss: 0.00003540
Iteration 131/1000 | Loss: 0.00003538
Iteration 132/1000 | Loss: 0.00003538
Iteration 133/1000 | Loss: 0.00003538
Iteration 134/1000 | Loss: 0.00003538
Iteration 135/1000 | Loss: 0.00003538
Iteration 136/1000 | Loss: 0.00003537
Iteration 137/1000 | Loss: 0.00003537
Iteration 138/1000 | Loss: 0.00003537
Iteration 139/1000 | Loss: 0.00003537
Iteration 140/1000 | Loss: 0.00003537
Iteration 141/1000 | Loss: 0.00003537
Iteration 142/1000 | Loss: 0.00003537
Iteration 143/1000 | Loss: 0.00003537
Iteration 144/1000 | Loss: 0.00003537
Iteration 145/1000 | Loss: 0.00003537
Iteration 146/1000 | Loss: 0.00003537
Iteration 147/1000 | Loss: 0.00003536
Iteration 148/1000 | Loss: 0.00003536
Iteration 149/1000 | Loss: 0.00003536
Iteration 150/1000 | Loss: 0.00003536
Iteration 151/1000 | Loss: 0.00003536
Iteration 152/1000 | Loss: 0.00003536
Iteration 153/1000 | Loss: 0.00003536
Iteration 154/1000 | Loss: 0.00003536
Iteration 155/1000 | Loss: 0.00003536
Iteration 156/1000 | Loss: 0.00003536
Iteration 157/1000 | Loss: 0.00003536
Iteration 158/1000 | Loss: 0.00003536
Iteration 159/1000 | Loss: 0.00003536
Iteration 160/1000 | Loss: 0.00003536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.536313306540251e-05, 3.536313306540251e-05, 3.536313306540251e-05, 3.536313306540251e-05, 3.536313306540251e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.536313306540251e-05

Optimization complete. Final v2v error: 5.124195575714111 mm

Highest mean error: 6.319336414337158 mm for frame 52

Lowest mean error: 4.5657958984375 mm for frame 157

Saving results

Total time: 180.35293245315552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892182
Iteration 2/25 | Loss: 0.00207529
Iteration 3/25 | Loss: 0.00162574
Iteration 4/25 | Loss: 0.00152024
Iteration 5/25 | Loss: 0.00149201
Iteration 6/25 | Loss: 0.00147792
Iteration 7/25 | Loss: 0.00146594
Iteration 8/25 | Loss: 0.00146254
Iteration 9/25 | Loss: 0.00146181
Iteration 10/25 | Loss: 0.00146156
Iteration 11/25 | Loss: 0.00146315
Iteration 12/25 | Loss: 0.00145862
Iteration 13/25 | Loss: 0.00145800
Iteration 14/25 | Loss: 0.00145794
Iteration 15/25 | Loss: 0.00145794
Iteration 16/25 | Loss: 0.00145791
Iteration 17/25 | Loss: 0.00145791
Iteration 18/25 | Loss: 0.00145791
Iteration 19/25 | Loss: 0.00145791
Iteration 20/25 | Loss: 0.00145791
Iteration 21/25 | Loss: 0.00145791
Iteration 22/25 | Loss: 0.00145791
Iteration 23/25 | Loss: 0.00145791
Iteration 24/25 | Loss: 0.00145791
Iteration 25/25 | Loss: 0.00145791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60967219
Iteration 2/25 | Loss: 0.00260267
Iteration 3/25 | Loss: 0.00260266
Iteration 4/25 | Loss: 0.00260266
Iteration 5/25 | Loss: 0.00260266
Iteration 6/25 | Loss: 0.00260265
Iteration 7/25 | Loss: 0.00260265
Iteration 8/25 | Loss: 0.00260265
Iteration 9/25 | Loss: 0.00260265
Iteration 10/25 | Loss: 0.00260265
Iteration 11/25 | Loss: 0.00260265
Iteration 12/25 | Loss: 0.00260265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002602654742076993, 0.002602654742076993, 0.002602654742076993, 0.002602654742076993, 0.002602654742076993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002602654742076993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260265
Iteration 2/1000 | Loss: 0.00005585
Iteration 3/1000 | Loss: 0.00004418
Iteration 4/1000 | Loss: 0.00004111
Iteration 5/1000 | Loss: 0.00003871
Iteration 6/1000 | Loss: 0.00003758
Iteration 7/1000 | Loss: 0.00003675
Iteration 8/1000 | Loss: 0.00003615
Iteration 9/1000 | Loss: 0.00003584
Iteration 10/1000 | Loss: 0.00003551
Iteration 11/1000 | Loss: 0.00003521
Iteration 12/1000 | Loss: 0.00003512
Iteration 13/1000 | Loss: 0.00003512
Iteration 14/1000 | Loss: 0.00003507
Iteration 15/1000 | Loss: 0.00003504
Iteration 16/1000 | Loss: 0.00003499
Iteration 17/1000 | Loss: 0.00003494
Iteration 18/1000 | Loss: 0.00003489
Iteration 19/1000 | Loss: 0.00003487
Iteration 20/1000 | Loss: 0.00003487
Iteration 21/1000 | Loss: 0.00003487
Iteration 22/1000 | Loss: 0.00003486
Iteration 23/1000 | Loss: 0.00003486
Iteration 24/1000 | Loss: 0.00003486
Iteration 25/1000 | Loss: 0.00003486
Iteration 26/1000 | Loss: 0.00003486
Iteration 27/1000 | Loss: 0.00003486
Iteration 28/1000 | Loss: 0.00003486
Iteration 29/1000 | Loss: 0.00003486
Iteration 30/1000 | Loss: 0.00003486
Iteration 31/1000 | Loss: 0.00003486
Iteration 32/1000 | Loss: 0.00003486
Iteration 33/1000 | Loss: 0.00003485
Iteration 34/1000 | Loss: 0.00003485
Iteration 35/1000 | Loss: 0.00003484
Iteration 36/1000 | Loss: 0.00003483
Iteration 37/1000 | Loss: 0.00003483
Iteration 38/1000 | Loss: 0.00003482
Iteration 39/1000 | Loss: 0.00003482
Iteration 40/1000 | Loss: 0.00003481
Iteration 41/1000 | Loss: 0.00003481
Iteration 42/1000 | Loss: 0.00003481
Iteration 43/1000 | Loss: 0.00003481
Iteration 44/1000 | Loss: 0.00003480
Iteration 45/1000 | Loss: 0.00003479
Iteration 46/1000 | Loss: 0.00003479
Iteration 47/1000 | Loss: 0.00003478
Iteration 48/1000 | Loss: 0.00003478
Iteration 49/1000 | Loss: 0.00003478
Iteration 50/1000 | Loss: 0.00003478
Iteration 51/1000 | Loss: 0.00003478
Iteration 52/1000 | Loss: 0.00003478
Iteration 53/1000 | Loss: 0.00003477
Iteration 54/1000 | Loss: 0.00003477
Iteration 55/1000 | Loss: 0.00003477
Iteration 56/1000 | Loss: 0.00003476
Iteration 57/1000 | Loss: 0.00003476
Iteration 58/1000 | Loss: 0.00003476
Iteration 59/1000 | Loss: 0.00003476
Iteration 60/1000 | Loss: 0.00003475
Iteration 61/1000 | Loss: 0.00003475
Iteration 62/1000 | Loss: 0.00003475
Iteration 63/1000 | Loss: 0.00003475
Iteration 64/1000 | Loss: 0.00003475
Iteration 65/1000 | Loss: 0.00003475
Iteration 66/1000 | Loss: 0.00003475
Iteration 67/1000 | Loss: 0.00003475
Iteration 68/1000 | Loss: 0.00003475
Iteration 69/1000 | Loss: 0.00003475
Iteration 70/1000 | Loss: 0.00003474
Iteration 71/1000 | Loss: 0.00003474
Iteration 72/1000 | Loss: 0.00003474
Iteration 73/1000 | Loss: 0.00003474
Iteration 74/1000 | Loss: 0.00003474
Iteration 75/1000 | Loss: 0.00003474
Iteration 76/1000 | Loss: 0.00003474
Iteration 77/1000 | Loss: 0.00003473
Iteration 78/1000 | Loss: 0.00003473
Iteration 79/1000 | Loss: 0.00003473
Iteration 80/1000 | Loss: 0.00003473
Iteration 81/1000 | Loss: 0.00003473
Iteration 82/1000 | Loss: 0.00003473
Iteration 83/1000 | Loss: 0.00003473
Iteration 84/1000 | Loss: 0.00003473
Iteration 85/1000 | Loss: 0.00003473
Iteration 86/1000 | Loss: 0.00003473
Iteration 87/1000 | Loss: 0.00003473
Iteration 88/1000 | Loss: 0.00003473
Iteration 89/1000 | Loss: 0.00003473
Iteration 90/1000 | Loss: 0.00003473
Iteration 91/1000 | Loss: 0.00003473
Iteration 92/1000 | Loss: 0.00003473
Iteration 93/1000 | Loss: 0.00003473
Iteration 94/1000 | Loss: 0.00003473
Iteration 95/1000 | Loss: 0.00003473
Iteration 96/1000 | Loss: 0.00003473
Iteration 97/1000 | Loss: 0.00003473
Iteration 98/1000 | Loss: 0.00003473
Iteration 99/1000 | Loss: 0.00003473
Iteration 100/1000 | Loss: 0.00003473
Iteration 101/1000 | Loss: 0.00003473
Iteration 102/1000 | Loss: 0.00003473
Iteration 103/1000 | Loss: 0.00003473
Iteration 104/1000 | Loss: 0.00003473
Iteration 105/1000 | Loss: 0.00003473
Iteration 106/1000 | Loss: 0.00003473
Iteration 107/1000 | Loss: 0.00003473
Iteration 108/1000 | Loss: 0.00003473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [3.472826938377693e-05, 3.472826938377693e-05, 3.472826938377693e-05, 3.472826938377693e-05, 3.472826938377693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.472826938377693e-05

Optimization complete. Final v2v error: 5.173130035400391 mm

Highest mean error: 5.5601887702941895 mm for frame 104

Lowest mean error: 4.328715801239014 mm for frame 0

Saving results

Total time: 54.80834102630615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436359
Iteration 2/25 | Loss: 0.00143703
Iteration 3/25 | Loss: 0.00135301
Iteration 4/25 | Loss: 0.00134130
Iteration 5/25 | Loss: 0.00133734
Iteration 6/25 | Loss: 0.00133638
Iteration 7/25 | Loss: 0.00133638
Iteration 8/25 | Loss: 0.00133638
Iteration 9/25 | Loss: 0.00133638
Iteration 10/25 | Loss: 0.00133638
Iteration 11/25 | Loss: 0.00133638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013363813050091267, 0.0013363813050091267, 0.0013363813050091267, 0.0013363813050091267, 0.0013363813050091267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013363813050091267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65379250
Iteration 2/25 | Loss: 0.00225072
Iteration 3/25 | Loss: 0.00225072
Iteration 4/25 | Loss: 0.00225072
Iteration 5/25 | Loss: 0.00225072
Iteration 6/25 | Loss: 0.00225072
Iteration 7/25 | Loss: 0.00225072
Iteration 8/25 | Loss: 0.00225072
Iteration 9/25 | Loss: 0.00225072
Iteration 10/25 | Loss: 0.00225072
Iteration 11/25 | Loss: 0.00225072
Iteration 12/25 | Loss: 0.00225072
Iteration 13/25 | Loss: 0.00225072
Iteration 14/25 | Loss: 0.00225072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00225072237662971, 0.00225072237662971, 0.00225072237662971, 0.00225072237662971, 0.00225072237662971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00225072237662971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225072
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00002400
Iteration 4/1000 | Loss: 0.00002269
Iteration 5/1000 | Loss: 0.00002185
Iteration 6/1000 | Loss: 0.00002116
Iteration 7/1000 | Loss: 0.00002078
Iteration 8/1000 | Loss: 0.00002070
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002054
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002051
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002051
Iteration 16/1000 | Loss: 0.00002050
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002043
Iteration 19/1000 | Loss: 0.00002040
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002038
Iteration 22/1000 | Loss: 0.00002037
Iteration 23/1000 | Loss: 0.00002037
Iteration 24/1000 | Loss: 0.00002037
Iteration 25/1000 | Loss: 0.00002037
Iteration 26/1000 | Loss: 0.00002037
Iteration 27/1000 | Loss: 0.00002036
Iteration 28/1000 | Loss: 0.00002036
Iteration 29/1000 | Loss: 0.00002036
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002035
Iteration 32/1000 | Loss: 0.00002035
Iteration 33/1000 | Loss: 0.00002035
Iteration 34/1000 | Loss: 0.00002034
Iteration 35/1000 | Loss: 0.00002034
Iteration 36/1000 | Loss: 0.00002034
Iteration 37/1000 | Loss: 0.00002034
Iteration 38/1000 | Loss: 0.00002034
Iteration 39/1000 | Loss: 0.00002034
Iteration 40/1000 | Loss: 0.00002034
Iteration 41/1000 | Loss: 0.00002034
Iteration 42/1000 | Loss: 0.00002034
Iteration 43/1000 | Loss: 0.00002034
Iteration 44/1000 | Loss: 0.00002034
Iteration 45/1000 | Loss: 0.00002033
Iteration 46/1000 | Loss: 0.00002032
Iteration 47/1000 | Loss: 0.00002031
Iteration 48/1000 | Loss: 0.00002031
Iteration 49/1000 | Loss: 0.00002031
Iteration 50/1000 | Loss: 0.00002031
Iteration 51/1000 | Loss: 0.00002031
Iteration 52/1000 | Loss: 0.00002031
Iteration 53/1000 | Loss: 0.00002031
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002031
Iteration 59/1000 | Loss: 0.00002030
Iteration 60/1000 | Loss: 0.00002030
Iteration 61/1000 | Loss: 0.00002030
Iteration 62/1000 | Loss: 0.00002029
Iteration 63/1000 | Loss: 0.00002029
Iteration 64/1000 | Loss: 0.00002029
Iteration 65/1000 | Loss: 0.00002029
Iteration 66/1000 | Loss: 0.00002028
Iteration 67/1000 | Loss: 0.00002028
Iteration 68/1000 | Loss: 0.00002028
Iteration 69/1000 | Loss: 0.00002028
Iteration 70/1000 | Loss: 0.00002028
Iteration 71/1000 | Loss: 0.00002028
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002027
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002027
Iteration 82/1000 | Loss: 0.00002027
Iteration 83/1000 | Loss: 0.00002027
Iteration 84/1000 | Loss: 0.00002027
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002026
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002026
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002026
Iteration 92/1000 | Loss: 0.00002026
Iteration 93/1000 | Loss: 0.00002026
Iteration 94/1000 | Loss: 0.00002026
Iteration 95/1000 | Loss: 0.00002025
Iteration 96/1000 | Loss: 0.00002025
Iteration 97/1000 | Loss: 0.00002025
Iteration 98/1000 | Loss: 0.00002025
Iteration 99/1000 | Loss: 0.00002025
Iteration 100/1000 | Loss: 0.00002025
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00002025
Iteration 105/1000 | Loss: 0.00002025
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002025
Iteration 110/1000 | Loss: 0.00002025
Iteration 111/1000 | Loss: 0.00002025
Iteration 112/1000 | Loss: 0.00002025
Iteration 113/1000 | Loss: 0.00002025
Iteration 114/1000 | Loss: 0.00002025
Iteration 115/1000 | Loss: 0.00002025
Iteration 116/1000 | Loss: 0.00002025
Iteration 117/1000 | Loss: 0.00002025
Iteration 118/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.0252755348337814e-05, 2.0252755348337814e-05, 2.0252755348337814e-05, 2.0252755348337814e-05, 2.0252755348337814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0252755348337814e-05

Optimization complete. Final v2v error: 3.9472272396087646 mm

Highest mean error: 4.551563739776611 mm for frame 16

Lowest mean error: 3.5587966442108154 mm for frame 86

Saving results

Total time: 29.2614905834198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01180005
Iteration 2/25 | Loss: 0.00185528
Iteration 3/25 | Loss: 0.00187519
Iteration 4/25 | Loss: 0.00163369
Iteration 5/25 | Loss: 0.00151481
Iteration 6/25 | Loss: 0.00151270
Iteration 7/25 | Loss: 0.00146798
Iteration 8/25 | Loss: 0.00146724
Iteration 9/25 | Loss: 0.00144568
Iteration 10/25 | Loss: 0.00140373
Iteration 11/25 | Loss: 0.00139701
Iteration 12/25 | Loss: 0.00142980
Iteration 13/25 | Loss: 0.00139352
Iteration 14/25 | Loss: 0.00139270
Iteration 15/25 | Loss: 0.00139242
Iteration 16/25 | Loss: 0.00139221
Iteration 17/25 | Loss: 0.00139204
Iteration 18/25 | Loss: 0.00142677
Iteration 19/25 | Loss: 0.00139386
Iteration 20/25 | Loss: 0.00139316
Iteration 21/25 | Loss: 0.00139297
Iteration 22/25 | Loss: 0.00142553
Iteration 23/25 | Loss: 0.00141225
Iteration 24/25 | Loss: 0.00142205
Iteration 25/25 | Loss: 0.00141441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.73219633
Iteration 2/25 | Loss: 0.00663264
Iteration 3/25 | Loss: 0.00850771
Iteration 4/25 | Loss: 0.00636405
Iteration 5/25 | Loss: 0.00615937
Iteration 6/25 | Loss: 0.00459110
Iteration 7/25 | Loss: 0.00441534
Iteration 8/25 | Loss: 0.00441534
Iteration 9/25 | Loss: 0.00441534
Iteration 10/25 | Loss: 0.00441534
Iteration 11/25 | Loss: 0.00441534
Iteration 12/25 | Loss: 0.00441534
Iteration 13/25 | Loss: 0.00441534
Iteration 14/25 | Loss: 0.00441534
Iteration 15/25 | Loss: 0.00441534
Iteration 16/25 | Loss: 0.00441534
Iteration 17/25 | Loss: 0.00441534
Iteration 18/25 | Loss: 0.00441534
Iteration 19/25 | Loss: 0.00441534
Iteration 20/25 | Loss: 0.00441534
Iteration 21/25 | Loss: 0.00441534
Iteration 22/25 | Loss: 0.00441534
Iteration 23/25 | Loss: 0.00441534
Iteration 24/25 | Loss: 0.00441534
Iteration 25/25 | Loss: 0.00441534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00441534
Iteration 2/1000 | Loss: 0.00115428
Iteration 3/1000 | Loss: 0.00011800
Iteration 4/1000 | Loss: 0.00006518
Iteration 5/1000 | Loss: 0.00003324
Iteration 6/1000 | Loss: 0.00002747
Iteration 7/1000 | Loss: 0.00002553
Iteration 8/1000 | Loss: 0.00002469
Iteration 9/1000 | Loss: 0.00002398
Iteration 10/1000 | Loss: 0.00002364
Iteration 11/1000 | Loss: 0.00002336
Iteration 12/1000 | Loss: 0.00002323
Iteration 13/1000 | Loss: 0.00002313
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002311
Iteration 16/1000 | Loss: 0.00002310
Iteration 17/1000 | Loss: 0.00002309
Iteration 18/1000 | Loss: 0.00002308
Iteration 19/1000 | Loss: 0.00002308
Iteration 20/1000 | Loss: 0.00002307
Iteration 21/1000 | Loss: 0.00002307
Iteration 22/1000 | Loss: 0.00002306
Iteration 23/1000 | Loss: 0.00002306
Iteration 24/1000 | Loss: 0.00002306
Iteration 25/1000 | Loss: 0.00002306
Iteration 26/1000 | Loss: 0.00002305
Iteration 27/1000 | Loss: 0.00002305
Iteration 28/1000 | Loss: 0.00002305
Iteration 29/1000 | Loss: 0.00002305
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002304
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00002303
Iteration 34/1000 | Loss: 0.00002303
Iteration 35/1000 | Loss: 0.00002302
Iteration 36/1000 | Loss: 0.00002302
Iteration 37/1000 | Loss: 0.00002302
Iteration 38/1000 | Loss: 0.00002302
Iteration 39/1000 | Loss: 0.00002301
Iteration 40/1000 | Loss: 0.00002301
Iteration 41/1000 | Loss: 0.00002301
Iteration 42/1000 | Loss: 0.00002301
Iteration 43/1000 | Loss: 0.00002301
Iteration 44/1000 | Loss: 0.00002301
Iteration 45/1000 | Loss: 0.00002300
Iteration 46/1000 | Loss: 0.00002300
Iteration 47/1000 | Loss: 0.00002300
Iteration 48/1000 | Loss: 0.00002300
Iteration 49/1000 | Loss: 0.00002300
Iteration 50/1000 | Loss: 0.00002300
Iteration 51/1000 | Loss: 0.00002298
Iteration 52/1000 | Loss: 0.00002298
Iteration 53/1000 | Loss: 0.00002298
Iteration 54/1000 | Loss: 0.00002297
Iteration 55/1000 | Loss: 0.00002297
Iteration 56/1000 | Loss: 0.00002297
Iteration 57/1000 | Loss: 0.00002297
Iteration 58/1000 | Loss: 0.00002297
Iteration 59/1000 | Loss: 0.00002297
Iteration 60/1000 | Loss: 0.00002296
Iteration 61/1000 | Loss: 0.00002296
Iteration 62/1000 | Loss: 0.00002296
Iteration 63/1000 | Loss: 0.00002296
Iteration 64/1000 | Loss: 0.00002296
Iteration 65/1000 | Loss: 0.00002296
Iteration 66/1000 | Loss: 0.00002296
Iteration 67/1000 | Loss: 0.00002296
Iteration 68/1000 | Loss: 0.00002296
Iteration 69/1000 | Loss: 0.00002296
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002295
Iteration 72/1000 | Loss: 0.00002294
Iteration 73/1000 | Loss: 0.00002294
Iteration 74/1000 | Loss: 0.00002294
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002293
Iteration 78/1000 | Loss: 0.00002293
Iteration 79/1000 | Loss: 0.00002293
Iteration 80/1000 | Loss: 0.00002293
Iteration 81/1000 | Loss: 0.00002293
Iteration 82/1000 | Loss: 0.00002293
Iteration 83/1000 | Loss: 0.00002293
Iteration 84/1000 | Loss: 0.00002292
Iteration 85/1000 | Loss: 0.00002292
Iteration 86/1000 | Loss: 0.00002292
Iteration 87/1000 | Loss: 0.00002292
Iteration 88/1000 | Loss: 0.00002292
Iteration 89/1000 | Loss: 0.00002291
Iteration 90/1000 | Loss: 0.00002291
Iteration 91/1000 | Loss: 0.00002291
Iteration 92/1000 | Loss: 0.00002291
Iteration 93/1000 | Loss: 0.00002291
Iteration 94/1000 | Loss: 0.00002291
Iteration 95/1000 | Loss: 0.00002291
Iteration 96/1000 | Loss: 0.00002291
Iteration 97/1000 | Loss: 0.00002291
Iteration 98/1000 | Loss: 0.00002291
Iteration 99/1000 | Loss: 0.00002291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.2912521671969444e-05, 2.2912521671969444e-05, 2.2912521671969444e-05, 2.2912521671969444e-05, 2.2912521671969444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2912521671969444e-05

Optimization complete. Final v2v error: 4.200919151306152 mm

Highest mean error: 5.022765159606934 mm for frame 0

Lowest mean error: 3.7875943183898926 mm for frame 101

Saving results

Total time: 79.15509700775146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00257577
Iteration 2/25 | Loss: 0.00153754
Iteration 3/25 | Loss: 0.00145203
Iteration 4/25 | Loss: 0.00142327
Iteration 5/25 | Loss: 0.00141586
Iteration 6/25 | Loss: 0.00141379
Iteration 7/25 | Loss: 0.00141361
Iteration 8/25 | Loss: 0.00141361
Iteration 9/25 | Loss: 0.00141361
Iteration 10/25 | Loss: 0.00141361
Iteration 11/25 | Loss: 0.00141361
Iteration 12/25 | Loss: 0.00141361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014136078534647822, 0.0014136078534647822, 0.0014136078534647822, 0.0014136078534647822, 0.0014136078534647822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014136078534647822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64979362
Iteration 2/25 | Loss: 0.00376871
Iteration 3/25 | Loss: 0.00376871
Iteration 4/25 | Loss: 0.00376871
Iteration 5/25 | Loss: 0.00376871
Iteration 6/25 | Loss: 0.00376871
Iteration 7/25 | Loss: 0.00376871
Iteration 8/25 | Loss: 0.00376871
Iteration 9/25 | Loss: 0.00376871
Iteration 10/25 | Loss: 0.00376871
Iteration 11/25 | Loss: 0.00376871
Iteration 12/25 | Loss: 0.00376871
Iteration 13/25 | Loss: 0.00376871
Iteration 14/25 | Loss: 0.00376871
Iteration 15/25 | Loss: 0.00376871
Iteration 16/25 | Loss: 0.00376871
Iteration 17/25 | Loss: 0.00376871
Iteration 18/25 | Loss: 0.00376871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0037687066942453384, 0.0037687066942453384, 0.0037687066942453384, 0.0037687066942453384, 0.0037687066942453384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037687066942453384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00376871
Iteration 2/1000 | Loss: 0.00005089
Iteration 3/1000 | Loss: 0.00003518
Iteration 4/1000 | Loss: 0.00002718
Iteration 5/1000 | Loss: 0.00002526
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002280
Iteration 9/1000 | Loss: 0.00002226
Iteration 10/1000 | Loss: 0.00002189
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002128
Iteration 15/1000 | Loss: 0.00002122
Iteration 16/1000 | Loss: 0.00002107
Iteration 17/1000 | Loss: 0.00002101
Iteration 18/1000 | Loss: 0.00002100
Iteration 19/1000 | Loss: 0.00002100
Iteration 20/1000 | Loss: 0.00002098
Iteration 21/1000 | Loss: 0.00002098
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002097
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002095
Iteration 26/1000 | Loss: 0.00002095
Iteration 27/1000 | Loss: 0.00002094
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002085
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002084
Iteration 37/1000 | Loss: 0.00002084
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002075
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002074
Iteration 47/1000 | Loss: 0.00002073
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002073
Iteration 50/1000 | Loss: 0.00002073
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002072
Iteration 53/1000 | Loss: 0.00002072
Iteration 54/1000 | Loss: 0.00002072
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002067
Iteration 76/1000 | Loss: 0.00002067
Iteration 77/1000 | Loss: 0.00002067
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002065
Iteration 81/1000 | Loss: 0.00002065
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002064
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002063
Iteration 87/1000 | Loss: 0.00002063
Iteration 88/1000 | Loss: 0.00002062
Iteration 89/1000 | Loss: 0.00002062
Iteration 90/1000 | Loss: 0.00002062
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002061
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002061
Iteration 95/1000 | Loss: 0.00002061
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002060
Iteration 104/1000 | Loss: 0.00002060
Iteration 105/1000 | Loss: 0.00002060
Iteration 106/1000 | Loss: 0.00002059
Iteration 107/1000 | Loss: 0.00002059
Iteration 108/1000 | Loss: 0.00002059
Iteration 109/1000 | Loss: 0.00002059
Iteration 110/1000 | Loss: 0.00002059
Iteration 111/1000 | Loss: 0.00002059
Iteration 112/1000 | Loss: 0.00002059
Iteration 113/1000 | Loss: 0.00002059
Iteration 114/1000 | Loss: 0.00002059
Iteration 115/1000 | Loss: 0.00002059
Iteration 116/1000 | Loss: 0.00002059
Iteration 117/1000 | Loss: 0.00002059
Iteration 118/1000 | Loss: 0.00002059
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002058
Iteration 123/1000 | Loss: 0.00002058
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002058
Iteration 126/1000 | Loss: 0.00002058
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002057
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002056
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002056
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002054
Iteration 144/1000 | Loss: 0.00002054
Iteration 145/1000 | Loss: 0.00002053
Iteration 146/1000 | Loss: 0.00002053
Iteration 147/1000 | Loss: 0.00002053
Iteration 148/1000 | Loss: 0.00002053
Iteration 149/1000 | Loss: 0.00002052
Iteration 150/1000 | Loss: 0.00002052
Iteration 151/1000 | Loss: 0.00002052
Iteration 152/1000 | Loss: 0.00002052
Iteration 153/1000 | Loss: 0.00002052
Iteration 154/1000 | Loss: 0.00002051
Iteration 155/1000 | Loss: 0.00002051
Iteration 156/1000 | Loss: 0.00002051
Iteration 157/1000 | Loss: 0.00002051
Iteration 158/1000 | Loss: 0.00002051
Iteration 159/1000 | Loss: 0.00002050
Iteration 160/1000 | Loss: 0.00002049
Iteration 161/1000 | Loss: 0.00002049
Iteration 162/1000 | Loss: 0.00002049
Iteration 163/1000 | Loss: 0.00002049
Iteration 164/1000 | Loss: 0.00002049
Iteration 165/1000 | Loss: 0.00002049
Iteration 166/1000 | Loss: 0.00002048
Iteration 167/1000 | Loss: 0.00002048
Iteration 168/1000 | Loss: 0.00002048
Iteration 169/1000 | Loss: 0.00002048
Iteration 170/1000 | Loss: 0.00002048
Iteration 171/1000 | Loss: 0.00002048
Iteration 172/1000 | Loss: 0.00002048
Iteration 173/1000 | Loss: 0.00002048
Iteration 174/1000 | Loss: 0.00002048
Iteration 175/1000 | Loss: 0.00002048
Iteration 176/1000 | Loss: 0.00002048
Iteration 177/1000 | Loss: 0.00002048
Iteration 178/1000 | Loss: 0.00002048
Iteration 179/1000 | Loss: 0.00002048
Iteration 180/1000 | Loss: 0.00002048
Iteration 181/1000 | Loss: 0.00002048
Iteration 182/1000 | Loss: 0.00002048
Iteration 183/1000 | Loss: 0.00002048
Iteration 184/1000 | Loss: 0.00002048
Iteration 185/1000 | Loss: 0.00002048
Iteration 186/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.0476070858421735e-05, 2.0476070858421735e-05, 2.0476070858421735e-05, 2.0476070858421735e-05, 2.0476070858421735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0476070858421735e-05

Optimization complete. Final v2v error: 3.85435152053833 mm

Highest mean error: 4.154672145843506 mm for frame 22

Lowest mean error: 3.5796525478363037 mm for frame 151

Saving results

Total time: 44.38651967048645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914144
Iteration 2/25 | Loss: 0.00148862
Iteration 3/25 | Loss: 0.00140172
Iteration 4/25 | Loss: 0.00138829
Iteration 5/25 | Loss: 0.00138435
Iteration 6/25 | Loss: 0.00138339
Iteration 7/25 | Loss: 0.00138339
Iteration 8/25 | Loss: 0.00138339
Iteration 9/25 | Loss: 0.00138339
Iteration 10/25 | Loss: 0.00138339
Iteration 11/25 | Loss: 0.00138339
Iteration 12/25 | Loss: 0.00138339
Iteration 13/25 | Loss: 0.00138339
Iteration 14/25 | Loss: 0.00138339
Iteration 15/25 | Loss: 0.00138339
Iteration 16/25 | Loss: 0.00138339
Iteration 17/25 | Loss: 0.00138339
Iteration 18/25 | Loss: 0.00138339
Iteration 19/25 | Loss: 0.00138339
Iteration 20/25 | Loss: 0.00138339
Iteration 21/25 | Loss: 0.00138339
Iteration 22/25 | Loss: 0.00138339
Iteration 23/25 | Loss: 0.00138339
Iteration 24/25 | Loss: 0.00138339
Iteration 25/25 | Loss: 0.00138339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62580156
Iteration 2/25 | Loss: 0.00337407
Iteration 3/25 | Loss: 0.00337407
Iteration 4/25 | Loss: 0.00337406
Iteration 5/25 | Loss: 0.00337406
Iteration 6/25 | Loss: 0.00337406
Iteration 7/25 | Loss: 0.00337406
Iteration 8/25 | Loss: 0.00337406
Iteration 9/25 | Loss: 0.00337406
Iteration 10/25 | Loss: 0.00337406
Iteration 11/25 | Loss: 0.00337406
Iteration 12/25 | Loss: 0.00337406
Iteration 13/25 | Loss: 0.00337406
Iteration 14/25 | Loss: 0.00337406
Iteration 15/25 | Loss: 0.00337406
Iteration 16/25 | Loss: 0.00337406
Iteration 17/25 | Loss: 0.00337406
Iteration 18/25 | Loss: 0.00337406
Iteration 19/25 | Loss: 0.00337406
Iteration 20/25 | Loss: 0.00337406
Iteration 21/25 | Loss: 0.00337406
Iteration 22/25 | Loss: 0.00337406
Iteration 23/25 | Loss: 0.00337406
Iteration 24/25 | Loss: 0.00337406
Iteration 25/25 | Loss: 0.00337406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00337406
Iteration 2/1000 | Loss: 0.00005983
Iteration 3/1000 | Loss: 0.00004157
Iteration 4/1000 | Loss: 0.00003367
Iteration 5/1000 | Loss: 0.00003031
Iteration 6/1000 | Loss: 0.00002884
Iteration 7/1000 | Loss: 0.00002772
Iteration 8/1000 | Loss: 0.00002696
Iteration 9/1000 | Loss: 0.00002623
Iteration 10/1000 | Loss: 0.00002567
Iteration 11/1000 | Loss: 0.00002529
Iteration 12/1000 | Loss: 0.00002503
Iteration 13/1000 | Loss: 0.00002490
Iteration 14/1000 | Loss: 0.00002476
Iteration 15/1000 | Loss: 0.00002475
Iteration 16/1000 | Loss: 0.00002474
Iteration 17/1000 | Loss: 0.00002474
Iteration 18/1000 | Loss: 0.00002469
Iteration 19/1000 | Loss: 0.00002466
Iteration 20/1000 | Loss: 0.00002458
Iteration 21/1000 | Loss: 0.00002455
Iteration 22/1000 | Loss: 0.00002454
Iteration 23/1000 | Loss: 0.00002454
Iteration 24/1000 | Loss: 0.00002453
Iteration 25/1000 | Loss: 0.00002450
Iteration 26/1000 | Loss: 0.00002450
Iteration 27/1000 | Loss: 0.00002445
Iteration 28/1000 | Loss: 0.00002444
Iteration 29/1000 | Loss: 0.00002444
Iteration 30/1000 | Loss: 0.00002444
Iteration 31/1000 | Loss: 0.00002442
Iteration 32/1000 | Loss: 0.00002442
Iteration 33/1000 | Loss: 0.00002441
Iteration 34/1000 | Loss: 0.00002440
Iteration 35/1000 | Loss: 0.00002438
Iteration 36/1000 | Loss: 0.00002438
Iteration 37/1000 | Loss: 0.00002437
Iteration 38/1000 | Loss: 0.00002437
Iteration 39/1000 | Loss: 0.00002436
Iteration 40/1000 | Loss: 0.00002436
Iteration 41/1000 | Loss: 0.00002436
Iteration 42/1000 | Loss: 0.00002436
Iteration 43/1000 | Loss: 0.00002435
Iteration 44/1000 | Loss: 0.00002435
Iteration 45/1000 | Loss: 0.00002435
Iteration 46/1000 | Loss: 0.00002435
Iteration 47/1000 | Loss: 0.00002434
Iteration 48/1000 | Loss: 0.00002434
Iteration 49/1000 | Loss: 0.00002433
Iteration 50/1000 | Loss: 0.00002433
Iteration 51/1000 | Loss: 0.00002433
Iteration 52/1000 | Loss: 0.00002431
Iteration 53/1000 | Loss: 0.00002431
Iteration 54/1000 | Loss: 0.00002431
Iteration 55/1000 | Loss: 0.00002431
Iteration 56/1000 | Loss: 0.00002430
Iteration 57/1000 | Loss: 0.00002430
Iteration 58/1000 | Loss: 0.00002430
Iteration 59/1000 | Loss: 0.00002429
Iteration 60/1000 | Loss: 0.00002429
Iteration 61/1000 | Loss: 0.00002429
Iteration 62/1000 | Loss: 0.00002429
Iteration 63/1000 | Loss: 0.00002428
Iteration 64/1000 | Loss: 0.00002428
Iteration 65/1000 | Loss: 0.00002428
Iteration 66/1000 | Loss: 0.00002427
Iteration 67/1000 | Loss: 0.00002427
Iteration 68/1000 | Loss: 0.00002427
Iteration 69/1000 | Loss: 0.00002427
Iteration 70/1000 | Loss: 0.00002427
Iteration 71/1000 | Loss: 0.00002427
Iteration 72/1000 | Loss: 0.00002427
Iteration 73/1000 | Loss: 0.00002427
Iteration 74/1000 | Loss: 0.00002427
Iteration 75/1000 | Loss: 0.00002427
Iteration 76/1000 | Loss: 0.00002427
Iteration 77/1000 | Loss: 0.00002426
Iteration 78/1000 | Loss: 0.00002426
Iteration 79/1000 | Loss: 0.00002426
Iteration 80/1000 | Loss: 0.00002426
Iteration 81/1000 | Loss: 0.00002426
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002425
Iteration 85/1000 | Loss: 0.00002425
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002425
Iteration 88/1000 | Loss: 0.00002425
Iteration 89/1000 | Loss: 0.00002425
Iteration 90/1000 | Loss: 0.00002424
Iteration 91/1000 | Loss: 0.00002424
Iteration 92/1000 | Loss: 0.00002424
Iteration 93/1000 | Loss: 0.00002423
Iteration 94/1000 | Loss: 0.00002423
Iteration 95/1000 | Loss: 0.00002423
Iteration 96/1000 | Loss: 0.00002422
Iteration 97/1000 | Loss: 0.00002422
Iteration 98/1000 | Loss: 0.00002422
Iteration 99/1000 | Loss: 0.00002421
Iteration 100/1000 | Loss: 0.00002421
Iteration 101/1000 | Loss: 0.00002421
Iteration 102/1000 | Loss: 0.00002421
Iteration 103/1000 | Loss: 0.00002421
Iteration 104/1000 | Loss: 0.00002421
Iteration 105/1000 | Loss: 0.00002421
Iteration 106/1000 | Loss: 0.00002421
Iteration 107/1000 | Loss: 0.00002421
Iteration 108/1000 | Loss: 0.00002421
Iteration 109/1000 | Loss: 0.00002421
Iteration 110/1000 | Loss: 0.00002421
Iteration 111/1000 | Loss: 0.00002421
Iteration 112/1000 | Loss: 0.00002421
Iteration 113/1000 | Loss: 0.00002421
Iteration 114/1000 | Loss: 0.00002421
Iteration 115/1000 | Loss: 0.00002421
Iteration 116/1000 | Loss: 0.00002421
Iteration 117/1000 | Loss: 0.00002421
Iteration 118/1000 | Loss: 0.00002421
Iteration 119/1000 | Loss: 0.00002421
Iteration 120/1000 | Loss: 0.00002421
Iteration 121/1000 | Loss: 0.00002421
Iteration 122/1000 | Loss: 0.00002421
Iteration 123/1000 | Loss: 0.00002421
Iteration 124/1000 | Loss: 0.00002421
Iteration 125/1000 | Loss: 0.00002421
Iteration 126/1000 | Loss: 0.00002421
Iteration 127/1000 | Loss: 0.00002421
Iteration 128/1000 | Loss: 0.00002421
Iteration 129/1000 | Loss: 0.00002421
Iteration 130/1000 | Loss: 0.00002421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.4210057745222002e-05, 2.4210057745222002e-05, 2.4210057745222002e-05, 2.4210057745222002e-05, 2.4210057745222002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4210057745222002e-05

Optimization complete. Final v2v error: 4.2434797286987305 mm

Highest mean error: 5.626692771911621 mm for frame 31

Lowest mean error: 3.4778764247894287 mm for frame 157

Saving results

Total time: 44.96973776817322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00675423
Iteration 2/25 | Loss: 0.00170613
Iteration 3/25 | Loss: 0.00145609
Iteration 4/25 | Loss: 0.00138658
Iteration 5/25 | Loss: 0.00137360
Iteration 6/25 | Loss: 0.00137128
Iteration 7/25 | Loss: 0.00136894
Iteration 8/25 | Loss: 0.00136497
Iteration 9/25 | Loss: 0.00136310
Iteration 10/25 | Loss: 0.00136190
Iteration 11/25 | Loss: 0.00136137
Iteration 12/25 | Loss: 0.00136113
Iteration 13/25 | Loss: 0.00136104
Iteration 14/25 | Loss: 0.00136103
Iteration 15/25 | Loss: 0.00136103
Iteration 16/25 | Loss: 0.00136103
Iteration 17/25 | Loss: 0.00136102
Iteration 18/25 | Loss: 0.00136102
Iteration 19/25 | Loss: 0.00136102
Iteration 20/25 | Loss: 0.00136102
Iteration 21/25 | Loss: 0.00136102
Iteration 22/25 | Loss: 0.00136102
Iteration 23/25 | Loss: 0.00136102
Iteration 24/25 | Loss: 0.00136102
Iteration 25/25 | Loss: 0.00136102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.87511396
Iteration 2/25 | Loss: 0.00241768
Iteration 3/25 | Loss: 0.00241735
Iteration 4/25 | Loss: 0.00241735
Iteration 5/25 | Loss: 0.00241735
Iteration 6/25 | Loss: 0.00241735
Iteration 7/25 | Loss: 0.00241735
Iteration 8/25 | Loss: 0.00241735
Iteration 9/25 | Loss: 0.00241735
Iteration 10/25 | Loss: 0.00241735
Iteration 11/25 | Loss: 0.00241735
Iteration 12/25 | Loss: 0.00241735
Iteration 13/25 | Loss: 0.00241735
Iteration 14/25 | Loss: 0.00241735
Iteration 15/25 | Loss: 0.00241735
Iteration 16/25 | Loss: 0.00241735
Iteration 17/25 | Loss: 0.00241735
Iteration 18/25 | Loss: 0.00241735
Iteration 19/25 | Loss: 0.00241735
Iteration 20/25 | Loss: 0.00241735
Iteration 21/25 | Loss: 0.00241735
Iteration 22/25 | Loss: 0.00241735
Iteration 23/25 | Loss: 0.00241735
Iteration 24/25 | Loss: 0.00241735
Iteration 25/25 | Loss: 0.00241735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241735
Iteration 2/1000 | Loss: 0.00004194
Iteration 3/1000 | Loss: 0.00003338
Iteration 4/1000 | Loss: 0.00003084
Iteration 5/1000 | Loss: 0.00002924
Iteration 6/1000 | Loss: 0.00002806
Iteration 7/1000 | Loss: 0.00002739
Iteration 8/1000 | Loss: 0.00002693
Iteration 9/1000 | Loss: 0.00002665
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002663
Iteration 12/1000 | Loss: 0.00002659
Iteration 13/1000 | Loss: 0.00002658
Iteration 14/1000 | Loss: 0.00002648
Iteration 15/1000 | Loss: 0.00002642
Iteration 16/1000 | Loss: 0.00002640
Iteration 17/1000 | Loss: 0.00002639
Iteration 18/1000 | Loss: 0.00002639
Iteration 19/1000 | Loss: 0.00002638
Iteration 20/1000 | Loss: 0.00002637
Iteration 21/1000 | Loss: 0.00002637
Iteration 22/1000 | Loss: 0.00002636
Iteration 23/1000 | Loss: 0.00002635
Iteration 24/1000 | Loss: 0.00002635
Iteration 25/1000 | Loss: 0.00002634
Iteration 26/1000 | Loss: 0.00002633
Iteration 27/1000 | Loss: 0.00002631
Iteration 28/1000 | Loss: 0.00002630
Iteration 29/1000 | Loss: 0.00002630
Iteration 30/1000 | Loss: 0.00002630
Iteration 31/1000 | Loss: 0.00002629
Iteration 32/1000 | Loss: 0.00002629
Iteration 33/1000 | Loss: 0.00002628
Iteration 34/1000 | Loss: 0.00002628
Iteration 35/1000 | Loss: 0.00002627
Iteration 36/1000 | Loss: 0.00002627
Iteration 37/1000 | Loss: 0.00002627
Iteration 38/1000 | Loss: 0.00002626
Iteration 39/1000 | Loss: 0.00002626
Iteration 40/1000 | Loss: 0.00002626
Iteration 41/1000 | Loss: 0.00002626
Iteration 42/1000 | Loss: 0.00002626
Iteration 43/1000 | Loss: 0.00002626
Iteration 44/1000 | Loss: 0.00002625
Iteration 45/1000 | Loss: 0.00002625
Iteration 46/1000 | Loss: 0.00002624
Iteration 47/1000 | Loss: 0.00002624
Iteration 48/1000 | Loss: 0.00002624
Iteration 49/1000 | Loss: 0.00002623
Iteration 50/1000 | Loss: 0.00002623
Iteration 51/1000 | Loss: 0.00002622
Iteration 52/1000 | Loss: 0.00002622
Iteration 53/1000 | Loss: 0.00002622
Iteration 54/1000 | Loss: 0.00002622
Iteration 55/1000 | Loss: 0.00002622
Iteration 56/1000 | Loss: 0.00002622
Iteration 57/1000 | Loss: 0.00002621
Iteration 58/1000 | Loss: 0.00002621
Iteration 59/1000 | Loss: 0.00002619
Iteration 60/1000 | Loss: 0.00002619
Iteration 61/1000 | Loss: 0.00002619
Iteration 62/1000 | Loss: 0.00002619
Iteration 63/1000 | Loss: 0.00002619
Iteration 64/1000 | Loss: 0.00002618
Iteration 65/1000 | Loss: 0.00002618
Iteration 66/1000 | Loss: 0.00002617
Iteration 67/1000 | Loss: 0.00002617
Iteration 68/1000 | Loss: 0.00002617
Iteration 69/1000 | Loss: 0.00002616
Iteration 70/1000 | Loss: 0.00002616
Iteration 71/1000 | Loss: 0.00002616
Iteration 72/1000 | Loss: 0.00002616
Iteration 73/1000 | Loss: 0.00002615
Iteration 74/1000 | Loss: 0.00002615
Iteration 75/1000 | Loss: 0.00002615
Iteration 76/1000 | Loss: 0.00002615
Iteration 77/1000 | Loss: 0.00002614
Iteration 78/1000 | Loss: 0.00002614
Iteration 79/1000 | Loss: 0.00002614
Iteration 80/1000 | Loss: 0.00002613
Iteration 81/1000 | Loss: 0.00002613
Iteration 82/1000 | Loss: 0.00002612
Iteration 83/1000 | Loss: 0.00002612
Iteration 84/1000 | Loss: 0.00002612
Iteration 85/1000 | Loss: 0.00002612
Iteration 86/1000 | Loss: 0.00002611
Iteration 87/1000 | Loss: 0.00002611
Iteration 88/1000 | Loss: 0.00002611
Iteration 89/1000 | Loss: 0.00002610
Iteration 90/1000 | Loss: 0.00002610
Iteration 91/1000 | Loss: 0.00002610
Iteration 92/1000 | Loss: 0.00002610
Iteration 93/1000 | Loss: 0.00002609
Iteration 94/1000 | Loss: 0.00002609
Iteration 95/1000 | Loss: 0.00002609
Iteration 96/1000 | Loss: 0.00002609
Iteration 97/1000 | Loss: 0.00002609
Iteration 98/1000 | Loss: 0.00002608
Iteration 99/1000 | Loss: 0.00002608
Iteration 100/1000 | Loss: 0.00002608
Iteration 101/1000 | Loss: 0.00002608
Iteration 102/1000 | Loss: 0.00002608
Iteration 103/1000 | Loss: 0.00002608
Iteration 104/1000 | Loss: 0.00002608
Iteration 105/1000 | Loss: 0.00002607
Iteration 106/1000 | Loss: 0.00002607
Iteration 107/1000 | Loss: 0.00002607
Iteration 108/1000 | Loss: 0.00002607
Iteration 109/1000 | Loss: 0.00002607
Iteration 110/1000 | Loss: 0.00002607
Iteration 111/1000 | Loss: 0.00002607
Iteration 112/1000 | Loss: 0.00002607
Iteration 113/1000 | Loss: 0.00002607
Iteration 114/1000 | Loss: 0.00002607
Iteration 115/1000 | Loss: 0.00002607
Iteration 116/1000 | Loss: 0.00002607
Iteration 117/1000 | Loss: 0.00002607
Iteration 118/1000 | Loss: 0.00002607
Iteration 119/1000 | Loss: 0.00002607
Iteration 120/1000 | Loss: 0.00002606
Iteration 121/1000 | Loss: 0.00002606
Iteration 122/1000 | Loss: 0.00002606
Iteration 123/1000 | Loss: 0.00002606
Iteration 124/1000 | Loss: 0.00002606
Iteration 125/1000 | Loss: 0.00002606
Iteration 126/1000 | Loss: 0.00002606
Iteration 127/1000 | Loss: 0.00002606
Iteration 128/1000 | Loss: 0.00002606
Iteration 129/1000 | Loss: 0.00002606
Iteration 130/1000 | Loss: 0.00002606
Iteration 131/1000 | Loss: 0.00002606
Iteration 132/1000 | Loss: 0.00002606
Iteration 133/1000 | Loss: 0.00002606
Iteration 134/1000 | Loss: 0.00002606
Iteration 135/1000 | Loss: 0.00002605
Iteration 136/1000 | Loss: 0.00002605
Iteration 137/1000 | Loss: 0.00002605
Iteration 138/1000 | Loss: 0.00002605
Iteration 139/1000 | Loss: 0.00002605
Iteration 140/1000 | Loss: 0.00002605
Iteration 141/1000 | Loss: 0.00002605
Iteration 142/1000 | Loss: 0.00002605
Iteration 143/1000 | Loss: 0.00002605
Iteration 144/1000 | Loss: 0.00002605
Iteration 145/1000 | Loss: 0.00002605
Iteration 146/1000 | Loss: 0.00002605
Iteration 147/1000 | Loss: 0.00002605
Iteration 148/1000 | Loss: 0.00002605
Iteration 149/1000 | Loss: 0.00002605
Iteration 150/1000 | Loss: 0.00002605
Iteration 151/1000 | Loss: 0.00002605
Iteration 152/1000 | Loss: 0.00002605
Iteration 153/1000 | Loss: 0.00002605
Iteration 154/1000 | Loss: 0.00002605
Iteration 155/1000 | Loss: 0.00002605
Iteration 156/1000 | Loss: 0.00002605
Iteration 157/1000 | Loss: 0.00002605
Iteration 158/1000 | Loss: 0.00002605
Iteration 159/1000 | Loss: 0.00002605
Iteration 160/1000 | Loss: 0.00002605
Iteration 161/1000 | Loss: 0.00002605
Iteration 162/1000 | Loss: 0.00002605
Iteration 163/1000 | Loss: 0.00002605
Iteration 164/1000 | Loss: 0.00002605
Iteration 165/1000 | Loss: 0.00002605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.6049359803437255e-05, 2.6049359803437255e-05, 2.6049359803437255e-05, 2.6049359803437255e-05, 2.6049359803437255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6049359803437255e-05

Optimization complete. Final v2v error: 4.444972515106201 mm

Highest mean error: 5.112120628356934 mm for frame 57

Lowest mean error: 3.994237184524536 mm for frame 198

Saving results

Total time: 54.391005754470825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_1818/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_1818/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593848
Iteration 2/25 | Loss: 0.00169822
Iteration 3/25 | Loss: 0.00145730
Iteration 4/25 | Loss: 0.00143312
Iteration 5/25 | Loss: 0.00142590
Iteration 6/25 | Loss: 0.00142455
Iteration 7/25 | Loss: 0.00142447
Iteration 8/25 | Loss: 0.00142447
Iteration 9/25 | Loss: 0.00142447
Iteration 10/25 | Loss: 0.00142447
Iteration 11/25 | Loss: 0.00142447
Iteration 12/25 | Loss: 0.00142447
Iteration 13/25 | Loss: 0.00142447
Iteration 14/25 | Loss: 0.00142447
Iteration 15/25 | Loss: 0.00142447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014244688209146261, 0.0014244688209146261, 0.0014244688209146261, 0.0014244688209146261, 0.0014244688209146261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014244688209146261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54534876
Iteration 2/25 | Loss: 0.00213070
Iteration 3/25 | Loss: 0.00213067
Iteration 4/25 | Loss: 0.00213067
Iteration 5/25 | Loss: 0.00213067
Iteration 6/25 | Loss: 0.00213067
Iteration 7/25 | Loss: 0.00213067
Iteration 8/25 | Loss: 0.00213067
Iteration 9/25 | Loss: 0.00213067
Iteration 10/25 | Loss: 0.00213067
Iteration 11/25 | Loss: 0.00213067
Iteration 12/25 | Loss: 0.00213067
Iteration 13/25 | Loss: 0.00213067
Iteration 14/25 | Loss: 0.00213067
Iteration 15/25 | Loss: 0.00213067
Iteration 16/25 | Loss: 0.00213067
Iteration 17/25 | Loss: 0.00213067
Iteration 18/25 | Loss: 0.00213067
Iteration 19/25 | Loss: 0.00213067
Iteration 20/25 | Loss: 0.00213067
Iteration 21/25 | Loss: 0.00213067
Iteration 22/25 | Loss: 0.00213067
Iteration 23/25 | Loss: 0.00213067
Iteration 24/25 | Loss: 0.00213067
Iteration 25/25 | Loss: 0.00213067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213067
Iteration 2/1000 | Loss: 0.00005486
Iteration 3/1000 | Loss: 0.00004330
Iteration 4/1000 | Loss: 0.00003993
Iteration 5/1000 | Loss: 0.00003791
Iteration 6/1000 | Loss: 0.00003654
Iteration 7/1000 | Loss: 0.00003520
Iteration 8/1000 | Loss: 0.00003441
Iteration 9/1000 | Loss: 0.00003391
Iteration 10/1000 | Loss: 0.00003355
Iteration 11/1000 | Loss: 0.00003336
Iteration 12/1000 | Loss: 0.00003329
Iteration 13/1000 | Loss: 0.00003325
Iteration 14/1000 | Loss: 0.00003324
Iteration 15/1000 | Loss: 0.00003318
Iteration 16/1000 | Loss: 0.00003317
Iteration 17/1000 | Loss: 0.00003315
Iteration 18/1000 | Loss: 0.00003312
Iteration 19/1000 | Loss: 0.00003309
Iteration 20/1000 | Loss: 0.00003309
Iteration 21/1000 | Loss: 0.00003308
Iteration 22/1000 | Loss: 0.00003308
Iteration 23/1000 | Loss: 0.00003307
Iteration 24/1000 | Loss: 0.00003307
Iteration 25/1000 | Loss: 0.00003306
Iteration 26/1000 | Loss: 0.00003305
Iteration 27/1000 | Loss: 0.00003305
Iteration 28/1000 | Loss: 0.00003305
Iteration 29/1000 | Loss: 0.00003303
Iteration 30/1000 | Loss: 0.00003303
Iteration 31/1000 | Loss: 0.00003303
Iteration 32/1000 | Loss: 0.00003303
Iteration 33/1000 | Loss: 0.00003303
Iteration 34/1000 | Loss: 0.00003303
Iteration 35/1000 | Loss: 0.00003303
Iteration 36/1000 | Loss: 0.00003303
Iteration 37/1000 | Loss: 0.00003302
Iteration 38/1000 | Loss: 0.00003302
Iteration 39/1000 | Loss: 0.00003301
Iteration 40/1000 | Loss: 0.00003301
Iteration 41/1000 | Loss: 0.00003301
Iteration 42/1000 | Loss: 0.00003301
Iteration 43/1000 | Loss: 0.00003300
Iteration 44/1000 | Loss: 0.00003300
Iteration 45/1000 | Loss: 0.00003300
Iteration 46/1000 | Loss: 0.00003300
Iteration 47/1000 | Loss: 0.00003300
Iteration 48/1000 | Loss: 0.00003299
Iteration 49/1000 | Loss: 0.00003299
Iteration 50/1000 | Loss: 0.00003299
Iteration 51/1000 | Loss: 0.00003299
Iteration 52/1000 | Loss: 0.00003299
Iteration 53/1000 | Loss: 0.00003298
Iteration 54/1000 | Loss: 0.00003298
Iteration 55/1000 | Loss: 0.00003298
Iteration 56/1000 | Loss: 0.00003298
Iteration 57/1000 | Loss: 0.00003297
Iteration 58/1000 | Loss: 0.00003297
Iteration 59/1000 | Loss: 0.00003297
Iteration 60/1000 | Loss: 0.00003297
Iteration 61/1000 | Loss: 0.00003297
Iteration 62/1000 | Loss: 0.00003297
Iteration 63/1000 | Loss: 0.00003297
Iteration 64/1000 | Loss: 0.00003297
Iteration 65/1000 | Loss: 0.00003297
Iteration 66/1000 | Loss: 0.00003297
Iteration 67/1000 | Loss: 0.00003297
Iteration 68/1000 | Loss: 0.00003297
Iteration 69/1000 | Loss: 0.00003297
Iteration 70/1000 | Loss: 0.00003297
Iteration 71/1000 | Loss: 0.00003297
Iteration 72/1000 | Loss: 0.00003297
Iteration 73/1000 | Loss: 0.00003297
Iteration 74/1000 | Loss: 0.00003297
Iteration 75/1000 | Loss: 0.00003297
Iteration 76/1000 | Loss: 0.00003297
Iteration 77/1000 | Loss: 0.00003297
Iteration 78/1000 | Loss: 0.00003297
Iteration 79/1000 | Loss: 0.00003297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.2968386221909896e-05, 3.2968386221909896e-05, 3.2968386221909896e-05, 3.2968386221909896e-05, 3.2968386221909896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2968386221909896e-05

Optimization complete. Final v2v error: 4.9667253494262695 mm

Highest mean error: 5.395735740661621 mm for frame 48

Lowest mean error: 4.488339424133301 mm for frame 98

Saving results

Total time: 31.952199697494507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970786
Iteration 2/25 | Loss: 0.00161091
Iteration 3/25 | Loss: 0.00110264
Iteration 4/25 | Loss: 0.00108041
Iteration 5/25 | Loss: 0.00107369
Iteration 6/25 | Loss: 0.00107283
Iteration 7/25 | Loss: 0.00107283
Iteration 8/25 | Loss: 0.00107283
Iteration 9/25 | Loss: 0.00107283
Iteration 10/25 | Loss: 0.00107283
Iteration 11/25 | Loss: 0.00107283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010728329652920365, 0.0010728329652920365, 0.0010728329652920365, 0.0010728329652920365, 0.0010728329652920365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010728329652920365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83724219
Iteration 2/25 | Loss: 0.00057754
Iteration 3/25 | Loss: 0.00057754
Iteration 4/25 | Loss: 0.00057754
Iteration 5/25 | Loss: 0.00057754
Iteration 6/25 | Loss: 0.00057754
Iteration 7/25 | Loss: 0.00057754
Iteration 8/25 | Loss: 0.00057754
Iteration 9/25 | Loss: 0.00057754
Iteration 10/25 | Loss: 0.00057754
Iteration 11/25 | Loss: 0.00057754
Iteration 12/25 | Loss: 0.00057754
Iteration 13/25 | Loss: 0.00057754
Iteration 14/25 | Loss: 0.00057754
Iteration 15/25 | Loss: 0.00057754
Iteration 16/25 | Loss: 0.00057754
Iteration 17/25 | Loss: 0.00057754
Iteration 18/25 | Loss: 0.00057754
Iteration 19/25 | Loss: 0.00057754
Iteration 20/25 | Loss: 0.00057754
Iteration 21/25 | Loss: 0.00057754
Iteration 22/25 | Loss: 0.00057754
Iteration 23/25 | Loss: 0.00057754
Iteration 24/25 | Loss: 0.00057754
Iteration 25/25 | Loss: 0.00057754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057754
Iteration 2/1000 | Loss: 0.00005357
Iteration 3/1000 | Loss: 0.00003767
Iteration 4/1000 | Loss: 0.00003535
Iteration 5/1000 | Loss: 0.00003374
Iteration 6/1000 | Loss: 0.00003284
Iteration 7/1000 | Loss: 0.00003220
Iteration 8/1000 | Loss: 0.00003172
Iteration 9/1000 | Loss: 0.00003139
Iteration 10/1000 | Loss: 0.00003107
Iteration 11/1000 | Loss: 0.00003078
Iteration 12/1000 | Loss: 0.00003060
Iteration 13/1000 | Loss: 0.00003041
Iteration 14/1000 | Loss: 0.00003023
Iteration 15/1000 | Loss: 0.00003002
Iteration 16/1000 | Loss: 0.00002987
Iteration 17/1000 | Loss: 0.00002983
Iteration 18/1000 | Loss: 0.00002982
Iteration 19/1000 | Loss: 0.00002982
Iteration 20/1000 | Loss: 0.00002980
Iteration 21/1000 | Loss: 0.00002980
Iteration 22/1000 | Loss: 0.00002977
Iteration 23/1000 | Loss: 0.00002969
Iteration 24/1000 | Loss: 0.00002959
Iteration 25/1000 | Loss: 0.00002958
Iteration 26/1000 | Loss: 0.00002958
Iteration 27/1000 | Loss: 0.00002955
Iteration 28/1000 | Loss: 0.00002954
Iteration 29/1000 | Loss: 0.00002954
Iteration 30/1000 | Loss: 0.00002954
Iteration 31/1000 | Loss: 0.00002954
Iteration 32/1000 | Loss: 0.00002952
Iteration 33/1000 | Loss: 0.00002951
Iteration 34/1000 | Loss: 0.00002951
Iteration 35/1000 | Loss: 0.00002951
Iteration 36/1000 | Loss: 0.00002951
Iteration 37/1000 | Loss: 0.00002948
Iteration 38/1000 | Loss: 0.00002948
Iteration 39/1000 | Loss: 0.00002948
Iteration 40/1000 | Loss: 0.00002948
Iteration 41/1000 | Loss: 0.00002948
Iteration 42/1000 | Loss: 0.00002948
Iteration 43/1000 | Loss: 0.00002948
Iteration 44/1000 | Loss: 0.00002948
Iteration 45/1000 | Loss: 0.00002948
Iteration 46/1000 | Loss: 0.00002947
Iteration 47/1000 | Loss: 0.00002947
Iteration 48/1000 | Loss: 0.00002947
Iteration 49/1000 | Loss: 0.00002947
Iteration 50/1000 | Loss: 0.00002947
Iteration 51/1000 | Loss: 0.00002947
Iteration 52/1000 | Loss: 0.00002947
Iteration 53/1000 | Loss: 0.00002947
Iteration 54/1000 | Loss: 0.00002947
Iteration 55/1000 | Loss: 0.00002947
Iteration 56/1000 | Loss: 0.00002947
Iteration 57/1000 | Loss: 0.00002946
Iteration 58/1000 | Loss: 0.00002945
Iteration 59/1000 | Loss: 0.00002945
Iteration 60/1000 | Loss: 0.00002945
Iteration 61/1000 | Loss: 0.00002945
Iteration 62/1000 | Loss: 0.00002945
Iteration 63/1000 | Loss: 0.00002945
Iteration 64/1000 | Loss: 0.00002944
Iteration 65/1000 | Loss: 0.00002944
Iteration 66/1000 | Loss: 0.00002944
Iteration 67/1000 | Loss: 0.00002943
Iteration 68/1000 | Loss: 0.00002943
Iteration 69/1000 | Loss: 0.00002943
Iteration 70/1000 | Loss: 0.00002943
Iteration 71/1000 | Loss: 0.00002942
Iteration 72/1000 | Loss: 0.00002942
Iteration 73/1000 | Loss: 0.00002942
Iteration 74/1000 | Loss: 0.00002941
Iteration 75/1000 | Loss: 0.00002941
Iteration 76/1000 | Loss: 0.00002941
Iteration 77/1000 | Loss: 0.00002941
Iteration 78/1000 | Loss: 0.00002941
Iteration 79/1000 | Loss: 0.00002941
Iteration 80/1000 | Loss: 0.00002941
Iteration 81/1000 | Loss: 0.00002941
Iteration 82/1000 | Loss: 0.00002941
Iteration 83/1000 | Loss: 0.00002940
Iteration 84/1000 | Loss: 0.00002940
Iteration 85/1000 | Loss: 0.00002940
Iteration 86/1000 | Loss: 0.00002940
Iteration 87/1000 | Loss: 0.00002940
Iteration 88/1000 | Loss: 0.00002940
Iteration 89/1000 | Loss: 0.00002940
Iteration 90/1000 | Loss: 0.00002939
Iteration 91/1000 | Loss: 0.00002939
Iteration 92/1000 | Loss: 0.00002939
Iteration 93/1000 | Loss: 0.00002939
Iteration 94/1000 | Loss: 0.00002939
Iteration 95/1000 | Loss: 0.00002938
Iteration 96/1000 | Loss: 0.00002938
Iteration 97/1000 | Loss: 0.00002938
Iteration 98/1000 | Loss: 0.00002938
Iteration 99/1000 | Loss: 0.00002938
Iteration 100/1000 | Loss: 0.00002937
Iteration 101/1000 | Loss: 0.00002937
Iteration 102/1000 | Loss: 0.00002937
Iteration 103/1000 | Loss: 0.00002937
Iteration 104/1000 | Loss: 0.00002937
Iteration 105/1000 | Loss: 0.00002937
Iteration 106/1000 | Loss: 0.00002936
Iteration 107/1000 | Loss: 0.00002936
Iteration 108/1000 | Loss: 0.00002936
Iteration 109/1000 | Loss: 0.00002936
Iteration 110/1000 | Loss: 0.00002936
Iteration 111/1000 | Loss: 0.00002935
Iteration 112/1000 | Loss: 0.00002935
Iteration 113/1000 | Loss: 0.00002935
Iteration 114/1000 | Loss: 0.00002935
Iteration 115/1000 | Loss: 0.00002935
Iteration 116/1000 | Loss: 0.00002935
Iteration 117/1000 | Loss: 0.00002935
Iteration 118/1000 | Loss: 0.00002935
Iteration 119/1000 | Loss: 0.00002935
Iteration 120/1000 | Loss: 0.00002935
Iteration 121/1000 | Loss: 0.00002935
Iteration 122/1000 | Loss: 0.00002935
Iteration 123/1000 | Loss: 0.00002935
Iteration 124/1000 | Loss: 0.00002935
Iteration 125/1000 | Loss: 0.00002935
Iteration 126/1000 | Loss: 0.00002935
Iteration 127/1000 | Loss: 0.00002935
Iteration 128/1000 | Loss: 0.00002935
Iteration 129/1000 | Loss: 0.00002934
Iteration 130/1000 | Loss: 0.00002934
Iteration 131/1000 | Loss: 0.00002934
Iteration 132/1000 | Loss: 0.00002934
Iteration 133/1000 | Loss: 0.00002934
Iteration 134/1000 | Loss: 0.00002934
Iteration 135/1000 | Loss: 0.00002934
Iteration 136/1000 | Loss: 0.00002934
Iteration 137/1000 | Loss: 0.00002934
Iteration 138/1000 | Loss: 0.00002934
Iteration 139/1000 | Loss: 0.00002933
Iteration 140/1000 | Loss: 0.00002933
Iteration 141/1000 | Loss: 0.00002933
Iteration 142/1000 | Loss: 0.00002933
Iteration 143/1000 | Loss: 0.00002933
Iteration 144/1000 | Loss: 0.00002933
Iteration 145/1000 | Loss: 0.00002933
Iteration 146/1000 | Loss: 0.00002933
Iteration 147/1000 | Loss: 0.00002933
Iteration 148/1000 | Loss: 0.00002933
Iteration 149/1000 | Loss: 0.00002933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.9329530661925673e-05, 2.9329530661925673e-05, 2.9329530661925673e-05, 2.9329530661925673e-05, 2.9329530661925673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9329530661925673e-05

Optimization complete. Final v2v error: 4.424075603485107 mm

Highest mean error: 5.529876232147217 mm for frame 93

Lowest mean error: 3.3112854957580566 mm for frame 1

Saving results

Total time: 51.613689661026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941439
Iteration 2/25 | Loss: 0.00132442
Iteration 3/25 | Loss: 0.00108921
Iteration 4/25 | Loss: 0.00089425
Iteration 5/25 | Loss: 0.00088561
Iteration 6/25 | Loss: 0.00088227
Iteration 7/25 | Loss: 0.00088093
Iteration 8/25 | Loss: 0.00088511
Iteration 9/25 | Loss: 0.00088052
Iteration 10/25 | Loss: 0.00087760
Iteration 11/25 | Loss: 0.00087718
Iteration 12/25 | Loss: 0.00087715
Iteration 13/25 | Loss: 0.00087714
Iteration 14/25 | Loss: 0.00087714
Iteration 15/25 | Loss: 0.00087714
Iteration 16/25 | Loss: 0.00087713
Iteration 17/25 | Loss: 0.00087713
Iteration 18/25 | Loss: 0.00087713
Iteration 19/25 | Loss: 0.00087713
Iteration 20/25 | Loss: 0.00087713
Iteration 21/25 | Loss: 0.00087713
Iteration 22/25 | Loss: 0.00087713
Iteration 23/25 | Loss: 0.00087713
Iteration 24/25 | Loss: 0.00087713
Iteration 25/25 | Loss: 0.00087713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51364541
Iteration 2/25 | Loss: 0.00048526
Iteration 3/25 | Loss: 0.00041718
Iteration 4/25 | Loss: 0.00041718
Iteration 5/25 | Loss: 0.00041718
Iteration 6/25 | Loss: 0.00041718
Iteration 7/25 | Loss: 0.00041718
Iteration 8/25 | Loss: 0.00041718
Iteration 9/25 | Loss: 0.00041718
Iteration 10/25 | Loss: 0.00041717
Iteration 11/25 | Loss: 0.00041717
Iteration 12/25 | Loss: 0.00041717
Iteration 13/25 | Loss: 0.00041717
Iteration 14/25 | Loss: 0.00041717
Iteration 15/25 | Loss: 0.00041717
Iteration 16/25 | Loss: 0.00041717
Iteration 17/25 | Loss: 0.00041717
Iteration 18/25 | Loss: 0.00041717
Iteration 19/25 | Loss: 0.00041717
Iteration 20/25 | Loss: 0.00041717
Iteration 21/25 | Loss: 0.00041717
Iteration 22/25 | Loss: 0.00041717
Iteration 23/25 | Loss: 0.00041717
Iteration 24/25 | Loss: 0.00041717
Iteration 25/25 | Loss: 0.00041717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041717
Iteration 2/1000 | Loss: 0.00003537
Iteration 3/1000 | Loss: 0.00019557
Iteration 4/1000 | Loss: 0.00001765
Iteration 5/1000 | Loss: 0.00001580
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001435
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001361
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001310
Iteration 13/1000 | Loss: 0.00001304
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001293
Iteration 17/1000 | Loss: 0.00001292
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001284
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001282
Iteration 34/1000 | Loss: 0.00001281
Iteration 35/1000 | Loss: 0.00001280
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001278
Iteration 41/1000 | Loss: 0.00001277
Iteration 42/1000 | Loss: 0.00001277
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001276
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001275
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001270
Iteration 78/1000 | Loss: 0.00001270
Iteration 79/1000 | Loss: 0.00001270
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001269
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001268
Iteration 98/1000 | Loss: 0.00001268
Iteration 99/1000 | Loss: 0.00001268
Iteration 100/1000 | Loss: 0.00001268
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001268
Iteration 103/1000 | Loss: 0.00001268
Iteration 104/1000 | Loss: 0.00001268
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001268
Iteration 107/1000 | Loss: 0.00001268
Iteration 108/1000 | Loss: 0.00001268
Iteration 109/1000 | Loss: 0.00001268
Iteration 110/1000 | Loss: 0.00001268
Iteration 111/1000 | Loss: 0.00001267
Iteration 112/1000 | Loss: 0.00001267
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001267
Iteration 116/1000 | Loss: 0.00001267
Iteration 117/1000 | Loss: 0.00001267
Iteration 118/1000 | Loss: 0.00001267
Iteration 119/1000 | Loss: 0.00001267
Iteration 120/1000 | Loss: 0.00001267
Iteration 121/1000 | Loss: 0.00001267
Iteration 122/1000 | Loss: 0.00001267
Iteration 123/1000 | Loss: 0.00001267
Iteration 124/1000 | Loss: 0.00001267
Iteration 125/1000 | Loss: 0.00001267
Iteration 126/1000 | Loss: 0.00001267
Iteration 127/1000 | Loss: 0.00001267
Iteration 128/1000 | Loss: 0.00001267
Iteration 129/1000 | Loss: 0.00001267
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2673360288317781e-05, 1.2673360288317781e-05, 1.2673360288317781e-05, 1.2673360288317781e-05, 1.2673360288317781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2673360288317781e-05

Optimization complete. Final v2v error: 3.0006754398345947 mm

Highest mean error: 3.7144081592559814 mm for frame 38

Lowest mean error: 2.4572646617889404 mm for frame 121

Saving results

Total time: 48.580957651138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953561
Iteration 2/25 | Loss: 0.00136628
Iteration 3/25 | Loss: 0.00114950
Iteration 4/25 | Loss: 0.00094996
Iteration 5/25 | Loss: 0.00091932
Iteration 6/25 | Loss: 0.00090650
Iteration 7/25 | Loss: 0.00089668
Iteration 8/25 | Loss: 0.00089681
Iteration 9/25 | Loss: 0.00089149
Iteration 10/25 | Loss: 0.00088581
Iteration 11/25 | Loss: 0.00088495
Iteration 12/25 | Loss: 0.00088443
Iteration 13/25 | Loss: 0.00088418
Iteration 14/25 | Loss: 0.00088411
Iteration 15/25 | Loss: 0.00088411
Iteration 16/25 | Loss: 0.00088411
Iteration 17/25 | Loss: 0.00088411
Iteration 18/25 | Loss: 0.00088411
Iteration 19/25 | Loss: 0.00088411
Iteration 20/25 | Loss: 0.00088411
Iteration 21/25 | Loss: 0.00088411
Iteration 22/25 | Loss: 0.00088411
Iteration 23/25 | Loss: 0.00088410
Iteration 24/25 | Loss: 0.00088410
Iteration 25/25 | Loss: 0.00088410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50968969
Iteration 2/25 | Loss: 0.00051681
Iteration 3/25 | Loss: 0.00046393
Iteration 4/25 | Loss: 0.00046393
Iteration 5/25 | Loss: 0.00046392
Iteration 6/25 | Loss: 0.00046392
Iteration 7/25 | Loss: 0.00046392
Iteration 8/25 | Loss: 0.00046392
Iteration 9/25 | Loss: 0.00046392
Iteration 10/25 | Loss: 0.00046392
Iteration 11/25 | Loss: 0.00046392
Iteration 12/25 | Loss: 0.00046392
Iteration 13/25 | Loss: 0.00046392
Iteration 14/25 | Loss: 0.00046392
Iteration 15/25 | Loss: 0.00046392
Iteration 16/25 | Loss: 0.00046392
Iteration 17/25 | Loss: 0.00046392
Iteration 18/25 | Loss: 0.00046392
Iteration 19/25 | Loss: 0.00046392
Iteration 20/25 | Loss: 0.00046392
Iteration 21/25 | Loss: 0.00046392
Iteration 22/25 | Loss: 0.00046392
Iteration 23/25 | Loss: 0.00046392
Iteration 24/25 | Loss: 0.00046392
Iteration 25/25 | Loss: 0.00046392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00046392306103371084, 0.00046392306103371084, 0.00046392306103371084, 0.00046392306103371084, 0.00046392306103371084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046392306103371084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046392
Iteration 2/1000 | Loss: 0.00003632
Iteration 3/1000 | Loss: 0.00015331
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001660
Iteration 7/1000 | Loss: 0.00001599
Iteration 8/1000 | Loss: 0.00002838
Iteration 9/1000 | Loss: 0.00001535
Iteration 10/1000 | Loss: 0.00002432
Iteration 11/1000 | Loss: 0.00002011
Iteration 12/1000 | Loss: 0.00001497
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00046848
Iteration 15/1000 | Loss: 0.00001810
Iteration 16/1000 | Loss: 0.00001521
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00001708
Iteration 19/1000 | Loss: 0.00001369
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001307
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001284
Iteration 53/1000 | Loss: 0.00001284
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001277
Iteration 65/1000 | Loss: 0.00001277
Iteration 66/1000 | Loss: 0.00001276
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00004047
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00002168
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001263
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001262
Iteration 89/1000 | Loss: 0.00001262
Iteration 90/1000 | Loss: 0.00001262
Iteration 91/1000 | Loss: 0.00001262
Iteration 92/1000 | Loss: 0.00001262
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001261
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001261
Iteration 102/1000 | Loss: 0.00001261
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001261
Iteration 107/1000 | Loss: 0.00001261
Iteration 108/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.2612564205483068e-05, 1.2612564205483068e-05, 1.2612564205483068e-05, 1.2612564205483068e-05, 1.2612564205483068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2612564205483068e-05

Optimization complete. Final v2v error: 2.9841725826263428 mm

Highest mean error: 4.513678073883057 mm for frame 178

Lowest mean error: 2.4368553161621094 mm for frame 121

Saving results

Total time: 71.68557596206665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_009/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_009/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560015
Iteration 2/25 | Loss: 0.00090938
Iteration 3/25 | Loss: 0.00081460
Iteration 4/25 | Loss: 0.00080285
Iteration 5/25 | Loss: 0.00079904
Iteration 6/25 | Loss: 0.00079786
Iteration 7/25 | Loss: 0.00079786
Iteration 8/25 | Loss: 0.00079786
Iteration 9/25 | Loss: 0.00079786
Iteration 10/25 | Loss: 0.00079786
Iteration 11/25 | Loss: 0.00079786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007978618377819657, 0.0007978618377819657, 0.0007978618377819657, 0.0007978618377819657, 0.0007978618377819657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007978618377819657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75821590
Iteration 2/25 | Loss: 0.00045325
Iteration 3/25 | Loss: 0.00045325
Iteration 4/25 | Loss: 0.00045325
Iteration 5/25 | Loss: 0.00045325
Iteration 6/25 | Loss: 0.00045325
Iteration 7/25 | Loss: 0.00045325
Iteration 8/25 | Loss: 0.00045325
Iteration 9/25 | Loss: 0.00045325
Iteration 10/25 | Loss: 0.00045324
Iteration 11/25 | Loss: 0.00045324
Iteration 12/25 | Loss: 0.00045324
Iteration 13/25 | Loss: 0.00045324
Iteration 14/25 | Loss: 0.00045324
Iteration 15/25 | Loss: 0.00045324
Iteration 16/25 | Loss: 0.00045324
Iteration 17/25 | Loss: 0.00045324
Iteration 18/25 | Loss: 0.00045324
Iteration 19/25 | Loss: 0.00045324
Iteration 20/25 | Loss: 0.00045324
Iteration 21/25 | Loss: 0.00045324
Iteration 22/25 | Loss: 0.00045324
Iteration 23/25 | Loss: 0.00045324
Iteration 24/25 | Loss: 0.00045324
Iteration 25/25 | Loss: 0.00045324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045324
Iteration 2/1000 | Loss: 0.00001771
Iteration 3/1000 | Loss: 0.00001081
Iteration 4/1000 | Loss: 0.00000893
Iteration 5/1000 | Loss: 0.00000833
Iteration 6/1000 | Loss: 0.00000793
Iteration 7/1000 | Loss: 0.00000771
Iteration 8/1000 | Loss: 0.00000753
Iteration 9/1000 | Loss: 0.00000751
Iteration 10/1000 | Loss: 0.00000749
Iteration 11/1000 | Loss: 0.00000749
Iteration 12/1000 | Loss: 0.00000748
Iteration 13/1000 | Loss: 0.00000747
Iteration 14/1000 | Loss: 0.00000746
Iteration 15/1000 | Loss: 0.00000740
Iteration 16/1000 | Loss: 0.00000739
Iteration 17/1000 | Loss: 0.00000739
Iteration 18/1000 | Loss: 0.00000738
Iteration 19/1000 | Loss: 0.00000735
Iteration 20/1000 | Loss: 0.00000733
Iteration 21/1000 | Loss: 0.00000732
Iteration 22/1000 | Loss: 0.00000729
Iteration 23/1000 | Loss: 0.00000729
Iteration 24/1000 | Loss: 0.00000729
Iteration 25/1000 | Loss: 0.00000728
Iteration 26/1000 | Loss: 0.00000728
Iteration 27/1000 | Loss: 0.00000727
Iteration 28/1000 | Loss: 0.00000726
Iteration 29/1000 | Loss: 0.00000725
Iteration 30/1000 | Loss: 0.00000725
Iteration 31/1000 | Loss: 0.00000725
Iteration 32/1000 | Loss: 0.00000724
Iteration 33/1000 | Loss: 0.00000724
Iteration 34/1000 | Loss: 0.00000724
Iteration 35/1000 | Loss: 0.00000724
Iteration 36/1000 | Loss: 0.00000724
Iteration 37/1000 | Loss: 0.00000723
Iteration 38/1000 | Loss: 0.00000723
Iteration 39/1000 | Loss: 0.00000722
Iteration 40/1000 | Loss: 0.00000721
Iteration 41/1000 | Loss: 0.00000721
Iteration 42/1000 | Loss: 0.00000721
Iteration 43/1000 | Loss: 0.00000721
Iteration 44/1000 | Loss: 0.00000721
Iteration 45/1000 | Loss: 0.00000721
Iteration 46/1000 | Loss: 0.00000720
Iteration 47/1000 | Loss: 0.00000720
Iteration 48/1000 | Loss: 0.00000720
Iteration 49/1000 | Loss: 0.00000720
Iteration 50/1000 | Loss: 0.00000720
Iteration 51/1000 | Loss: 0.00000720
Iteration 52/1000 | Loss: 0.00000720
Iteration 53/1000 | Loss: 0.00000719
Iteration 54/1000 | Loss: 0.00000719
Iteration 55/1000 | Loss: 0.00000718
Iteration 56/1000 | Loss: 0.00000718
Iteration 57/1000 | Loss: 0.00000717
Iteration 58/1000 | Loss: 0.00000717
Iteration 59/1000 | Loss: 0.00000717
Iteration 60/1000 | Loss: 0.00000716
Iteration 61/1000 | Loss: 0.00000716
Iteration 62/1000 | Loss: 0.00000716
Iteration 63/1000 | Loss: 0.00000716
Iteration 64/1000 | Loss: 0.00000715
Iteration 65/1000 | Loss: 0.00000715
Iteration 66/1000 | Loss: 0.00000715
Iteration 67/1000 | Loss: 0.00000715
Iteration 68/1000 | Loss: 0.00000715
Iteration 69/1000 | Loss: 0.00000715
Iteration 70/1000 | Loss: 0.00000715
Iteration 71/1000 | Loss: 0.00000714
Iteration 72/1000 | Loss: 0.00000714
Iteration 73/1000 | Loss: 0.00000714
Iteration 74/1000 | Loss: 0.00000714
Iteration 75/1000 | Loss: 0.00000714
Iteration 76/1000 | Loss: 0.00000713
Iteration 77/1000 | Loss: 0.00000713
Iteration 78/1000 | Loss: 0.00000713
Iteration 79/1000 | Loss: 0.00000713
Iteration 80/1000 | Loss: 0.00000713
Iteration 81/1000 | Loss: 0.00000712
Iteration 82/1000 | Loss: 0.00000712
Iteration 83/1000 | Loss: 0.00000712
Iteration 84/1000 | Loss: 0.00000712
Iteration 85/1000 | Loss: 0.00000711
Iteration 86/1000 | Loss: 0.00000711
Iteration 87/1000 | Loss: 0.00000711
Iteration 88/1000 | Loss: 0.00000711
Iteration 89/1000 | Loss: 0.00000711
Iteration 90/1000 | Loss: 0.00000711
Iteration 91/1000 | Loss: 0.00000710
Iteration 92/1000 | Loss: 0.00000710
Iteration 93/1000 | Loss: 0.00000710
Iteration 94/1000 | Loss: 0.00000710
Iteration 95/1000 | Loss: 0.00000709
Iteration 96/1000 | Loss: 0.00000709
Iteration 97/1000 | Loss: 0.00000708
Iteration 98/1000 | Loss: 0.00000708
Iteration 99/1000 | Loss: 0.00000708
Iteration 100/1000 | Loss: 0.00000708
Iteration 101/1000 | Loss: 0.00000707
Iteration 102/1000 | Loss: 0.00000707
Iteration 103/1000 | Loss: 0.00000707
Iteration 104/1000 | Loss: 0.00000707
Iteration 105/1000 | Loss: 0.00000707
Iteration 106/1000 | Loss: 0.00000707
Iteration 107/1000 | Loss: 0.00000706
Iteration 108/1000 | Loss: 0.00000706
Iteration 109/1000 | Loss: 0.00000706
Iteration 110/1000 | Loss: 0.00000706
Iteration 111/1000 | Loss: 0.00000706
Iteration 112/1000 | Loss: 0.00000706
Iteration 113/1000 | Loss: 0.00000705
Iteration 114/1000 | Loss: 0.00000705
Iteration 115/1000 | Loss: 0.00000705
Iteration 116/1000 | Loss: 0.00000705
Iteration 117/1000 | Loss: 0.00000705
Iteration 118/1000 | Loss: 0.00000705
Iteration 119/1000 | Loss: 0.00000705
Iteration 120/1000 | Loss: 0.00000705
Iteration 121/1000 | Loss: 0.00000705
Iteration 122/1000 | Loss: 0.00000705
Iteration 123/1000 | Loss: 0.00000705
Iteration 124/1000 | Loss: 0.00000705
Iteration 125/1000 | Loss: 0.00000705
Iteration 126/1000 | Loss: 0.00000705
Iteration 127/1000 | Loss: 0.00000704
Iteration 128/1000 | Loss: 0.00000704
Iteration 129/1000 | Loss: 0.00000704
Iteration 130/1000 | Loss: 0.00000704
Iteration 131/1000 | Loss: 0.00000704
Iteration 132/1000 | Loss: 0.00000704
Iteration 133/1000 | Loss: 0.00000704
Iteration 134/1000 | Loss: 0.00000703
Iteration 135/1000 | Loss: 0.00000703
Iteration 136/1000 | Loss: 0.00000703
Iteration 137/1000 | Loss: 0.00000703
Iteration 138/1000 | Loss: 0.00000703
Iteration 139/1000 | Loss: 0.00000703
Iteration 140/1000 | Loss: 0.00000703
Iteration 141/1000 | Loss: 0.00000703
Iteration 142/1000 | Loss: 0.00000703
Iteration 143/1000 | Loss: 0.00000703
Iteration 144/1000 | Loss: 0.00000703
Iteration 145/1000 | Loss: 0.00000703
Iteration 146/1000 | Loss: 0.00000702
Iteration 147/1000 | Loss: 0.00000701
Iteration 148/1000 | Loss: 0.00000701
Iteration 149/1000 | Loss: 0.00000701
Iteration 150/1000 | Loss: 0.00000701
Iteration 151/1000 | Loss: 0.00000701
Iteration 152/1000 | Loss: 0.00000701
Iteration 153/1000 | Loss: 0.00000701
Iteration 154/1000 | Loss: 0.00000701
Iteration 155/1000 | Loss: 0.00000701
Iteration 156/1000 | Loss: 0.00000701
Iteration 157/1000 | Loss: 0.00000701
Iteration 158/1000 | Loss: 0.00000701
Iteration 159/1000 | Loss: 0.00000701
Iteration 160/1000 | Loss: 0.00000701
Iteration 161/1000 | Loss: 0.00000701
Iteration 162/1000 | Loss: 0.00000701
Iteration 163/1000 | Loss: 0.00000700
Iteration 164/1000 | Loss: 0.00000700
Iteration 165/1000 | Loss: 0.00000700
Iteration 166/1000 | Loss: 0.00000700
Iteration 167/1000 | Loss: 0.00000699
Iteration 168/1000 | Loss: 0.00000699
Iteration 169/1000 | Loss: 0.00000699
Iteration 170/1000 | Loss: 0.00000699
Iteration 171/1000 | Loss: 0.00000699
Iteration 172/1000 | Loss: 0.00000699
Iteration 173/1000 | Loss: 0.00000699
Iteration 174/1000 | Loss: 0.00000699
Iteration 175/1000 | Loss: 0.00000699
Iteration 176/1000 | Loss: 0.00000699
Iteration 177/1000 | Loss: 0.00000699
Iteration 178/1000 | Loss: 0.00000699
Iteration 179/1000 | Loss: 0.00000699
Iteration 180/1000 | Loss: 0.00000699
Iteration 181/1000 | Loss: 0.00000699
Iteration 182/1000 | Loss: 0.00000699
Iteration 183/1000 | Loss: 0.00000699
Iteration 184/1000 | Loss: 0.00000699
Iteration 185/1000 | Loss: 0.00000699
Iteration 186/1000 | Loss: 0.00000699
Iteration 187/1000 | Loss: 0.00000698
Iteration 188/1000 | Loss: 0.00000698
Iteration 189/1000 | Loss: 0.00000698
Iteration 190/1000 | Loss: 0.00000698
Iteration 191/1000 | Loss: 0.00000698
Iteration 192/1000 | Loss: 0.00000698
Iteration 193/1000 | Loss: 0.00000698
Iteration 194/1000 | Loss: 0.00000698
Iteration 195/1000 | Loss: 0.00000698
Iteration 196/1000 | Loss: 0.00000698
Iteration 197/1000 | Loss: 0.00000698
Iteration 198/1000 | Loss: 0.00000698
Iteration 199/1000 | Loss: 0.00000697
Iteration 200/1000 | Loss: 0.00000697
Iteration 201/1000 | Loss: 0.00000697
Iteration 202/1000 | Loss: 0.00000697
Iteration 203/1000 | Loss: 0.00000697
Iteration 204/1000 | Loss: 0.00000697
Iteration 205/1000 | Loss: 0.00000697
Iteration 206/1000 | Loss: 0.00000697
Iteration 207/1000 | Loss: 0.00000697
Iteration 208/1000 | Loss: 0.00000697
Iteration 209/1000 | Loss: 0.00000697
Iteration 210/1000 | Loss: 0.00000696
Iteration 211/1000 | Loss: 0.00000696
Iteration 212/1000 | Loss: 0.00000696
Iteration 213/1000 | Loss: 0.00000696
Iteration 214/1000 | Loss: 0.00000696
Iteration 215/1000 | Loss: 0.00000696
Iteration 216/1000 | Loss: 0.00000696
Iteration 217/1000 | Loss: 0.00000696
Iteration 218/1000 | Loss: 0.00000696
Iteration 219/1000 | Loss: 0.00000696
Iteration 220/1000 | Loss: 0.00000696
Iteration 221/1000 | Loss: 0.00000696
Iteration 222/1000 | Loss: 0.00000696
Iteration 223/1000 | Loss: 0.00000696
Iteration 224/1000 | Loss: 0.00000696
Iteration 225/1000 | Loss: 0.00000696
Iteration 226/1000 | Loss: 0.00000696
Iteration 227/1000 | Loss: 0.00000696
Iteration 228/1000 | Loss: 0.00000696
Iteration 229/1000 | Loss: 0.00000696
Iteration 230/1000 | Loss: 0.00000696
Iteration 231/1000 | Loss: 0.00000696
Iteration 232/1000 | Loss: 0.00000696
Iteration 233/1000 | Loss: 0.00000696
Iteration 234/1000 | Loss: 0.00000696
Iteration 235/1000 | Loss: 0.00000696
Iteration 236/1000 | Loss: 0.00000696
Iteration 237/1000 | Loss: 0.00000696
Iteration 238/1000 | Loss: 0.00000696
Iteration 239/1000 | Loss: 0.00000696
Iteration 240/1000 | Loss: 0.00000696
Iteration 241/1000 | Loss: 0.00000696
Iteration 242/1000 | Loss: 0.00000696
Iteration 243/1000 | Loss: 0.00000696
Iteration 244/1000 | Loss: 0.00000696
Iteration 245/1000 | Loss: 0.00000696
Iteration 246/1000 | Loss: 0.00000696
Iteration 247/1000 | Loss: 0.00000696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [6.959604888834292e-06, 6.959604888834292e-06, 6.959604888834292e-06, 6.959604888834292e-06, 6.959604888834292e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.959604888834292e-06

Optimization complete. Final v2v error: 2.2631328105926514 mm

Highest mean error: 2.633211374282837 mm for frame 111

Lowest mean error: 2.092432975769043 mm for frame 41

Saving results

Total time: 37.583431243896484
