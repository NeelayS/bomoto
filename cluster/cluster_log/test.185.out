Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=185, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10360-10415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364634
Iteration 2/25 | Loss: 0.00144471
Iteration 3/25 | Loss: 0.00102554
Iteration 4/25 | Loss: 0.00096692
Iteration 5/25 | Loss: 0.00095508
Iteration 6/25 | Loss: 0.00095254
Iteration 7/25 | Loss: 0.00095186
Iteration 8/25 | Loss: 0.00095186
Iteration 9/25 | Loss: 0.00095186
Iteration 10/25 | Loss: 0.00095186
Iteration 11/25 | Loss: 0.00095186
Iteration 12/25 | Loss: 0.00095186
Iteration 13/25 | Loss: 0.00095186
Iteration 14/25 | Loss: 0.00095186
Iteration 15/25 | Loss: 0.00095186
Iteration 16/25 | Loss: 0.00095186
Iteration 17/25 | Loss: 0.00095186
Iteration 18/25 | Loss: 0.00095186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000951861497014761, 0.000951861497014761, 0.000951861497014761, 0.000951861497014761, 0.000951861497014761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000951861497014761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32449567
Iteration 2/25 | Loss: 0.00057409
Iteration 3/25 | Loss: 0.00057408
Iteration 4/25 | Loss: 0.00057408
Iteration 5/25 | Loss: 0.00057408
Iteration 6/25 | Loss: 0.00057408
Iteration 7/25 | Loss: 0.00057408
Iteration 8/25 | Loss: 0.00057408
Iteration 9/25 | Loss: 0.00057408
Iteration 10/25 | Loss: 0.00057408
Iteration 11/25 | Loss: 0.00057408
Iteration 12/25 | Loss: 0.00057408
Iteration 13/25 | Loss: 0.00057408
Iteration 14/25 | Loss: 0.00057408
Iteration 15/25 | Loss: 0.00057408
Iteration 16/25 | Loss: 0.00057408
Iteration 17/25 | Loss: 0.00057408
Iteration 18/25 | Loss: 0.00057408
Iteration 19/25 | Loss: 0.00057408
Iteration 20/25 | Loss: 0.00057408
Iteration 21/25 | Loss: 0.00057408
Iteration 22/25 | Loss: 0.00057408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005740828346461058, 0.0005740828346461058, 0.0005740828346461058, 0.0005740828346461058, 0.0005740828346461058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005740828346461058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057408
Iteration 2/1000 | Loss: 0.00003981
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001216
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001026
Iteration 8/1000 | Loss: 0.00000990
Iteration 9/1000 | Loss: 0.00000968
Iteration 10/1000 | Loss: 0.00000947
Iteration 11/1000 | Loss: 0.00000943
Iteration 12/1000 | Loss: 0.00000937
Iteration 13/1000 | Loss: 0.00000932
Iteration 14/1000 | Loss: 0.00000925
Iteration 15/1000 | Loss: 0.00000923
Iteration 16/1000 | Loss: 0.00000921
Iteration 17/1000 | Loss: 0.00000920
Iteration 18/1000 | Loss: 0.00000920
Iteration 19/1000 | Loss: 0.00000916
Iteration 20/1000 | Loss: 0.00000913
Iteration 21/1000 | Loss: 0.00000909
Iteration 22/1000 | Loss: 0.00000909
Iteration 23/1000 | Loss: 0.00000902
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000900
Iteration 26/1000 | Loss: 0.00000899
Iteration 27/1000 | Loss: 0.00000899
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000896
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000895
Iteration 33/1000 | Loss: 0.00000894
Iteration 34/1000 | Loss: 0.00000894
Iteration 35/1000 | Loss: 0.00000893
Iteration 36/1000 | Loss: 0.00000893
Iteration 37/1000 | Loss: 0.00000892
Iteration 38/1000 | Loss: 0.00000892
Iteration 39/1000 | Loss: 0.00000891
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000890
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000889
Iteration 44/1000 | Loss: 0.00000888
Iteration 45/1000 | Loss: 0.00000888
Iteration 46/1000 | Loss: 0.00000888
Iteration 47/1000 | Loss: 0.00000887
Iteration 48/1000 | Loss: 0.00000887
Iteration 49/1000 | Loss: 0.00000886
Iteration 50/1000 | Loss: 0.00000886
Iteration 51/1000 | Loss: 0.00000885
Iteration 52/1000 | Loss: 0.00000885
Iteration 53/1000 | Loss: 0.00000885
Iteration 54/1000 | Loss: 0.00000884
Iteration 55/1000 | Loss: 0.00000884
Iteration 56/1000 | Loss: 0.00000884
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000883
Iteration 59/1000 | Loss: 0.00000883
Iteration 60/1000 | Loss: 0.00000882
Iteration 61/1000 | Loss: 0.00000882
Iteration 62/1000 | Loss: 0.00000882
Iteration 63/1000 | Loss: 0.00000882
Iteration 64/1000 | Loss: 0.00000882
Iteration 65/1000 | Loss: 0.00000882
Iteration 66/1000 | Loss: 0.00000882
Iteration 67/1000 | Loss: 0.00000882
Iteration 68/1000 | Loss: 0.00000882
Iteration 69/1000 | Loss: 0.00000881
Iteration 70/1000 | Loss: 0.00000881
Iteration 71/1000 | Loss: 0.00000881
Iteration 72/1000 | Loss: 0.00000880
Iteration 73/1000 | Loss: 0.00000880
Iteration 74/1000 | Loss: 0.00000880
Iteration 75/1000 | Loss: 0.00000880
Iteration 76/1000 | Loss: 0.00000879
Iteration 77/1000 | Loss: 0.00000879
Iteration 78/1000 | Loss: 0.00000879
Iteration 79/1000 | Loss: 0.00000879
Iteration 80/1000 | Loss: 0.00000879
Iteration 81/1000 | Loss: 0.00000879
Iteration 82/1000 | Loss: 0.00000879
Iteration 83/1000 | Loss: 0.00000879
Iteration 84/1000 | Loss: 0.00000879
Iteration 85/1000 | Loss: 0.00000879
Iteration 86/1000 | Loss: 0.00000879
Iteration 87/1000 | Loss: 0.00000879
Iteration 88/1000 | Loss: 0.00000879
Iteration 89/1000 | Loss: 0.00000879
Iteration 90/1000 | Loss: 0.00000879
Iteration 91/1000 | Loss: 0.00000879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [8.79007075127447e-06, 8.79007075127447e-06, 8.79007075127447e-06, 8.79007075127447e-06, 8.79007075127447e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.79007075127447e-06

Optimization complete. Final v2v error: 2.545440673828125 mm

Highest mean error: 2.77182936668396 mm for frame 11

Lowest mean error: 2.162468910217285 mm for frame 3

Saving results

Total time: 35.111931562423706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894034
Iteration 2/25 | Loss: 0.00143558
Iteration 3/25 | Loss: 0.00112731
Iteration 4/25 | Loss: 0.00107974
Iteration 5/25 | Loss: 0.00107681
Iteration 6/25 | Loss: 0.00107275
Iteration 7/25 | Loss: 0.00105964
Iteration 8/25 | Loss: 0.00103926
Iteration 9/25 | Loss: 0.00103033
Iteration 10/25 | Loss: 0.00102963
Iteration 11/25 | Loss: 0.00103529
Iteration 12/25 | Loss: 0.00104520
Iteration 13/25 | Loss: 0.00102749
Iteration 14/25 | Loss: 0.00101313
Iteration 15/25 | Loss: 0.00100579
Iteration 16/25 | Loss: 0.00100144
Iteration 17/25 | Loss: 0.00100067
Iteration 18/25 | Loss: 0.00100039
Iteration 19/25 | Loss: 0.00100033
Iteration 20/25 | Loss: 0.00100032
Iteration 21/25 | Loss: 0.00100032
Iteration 22/25 | Loss: 0.00100032
Iteration 23/25 | Loss: 0.00100032
Iteration 24/25 | Loss: 0.00100032
Iteration 25/25 | Loss: 0.00100032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.25886440
Iteration 2/25 | Loss: 0.00113995
Iteration 3/25 | Loss: 0.00113995
Iteration 4/25 | Loss: 0.00113995
Iteration 5/25 | Loss: 0.00113994
Iteration 6/25 | Loss: 0.00113994
Iteration 7/25 | Loss: 0.00113994
Iteration 8/25 | Loss: 0.00113994
Iteration 9/25 | Loss: 0.00113994
Iteration 10/25 | Loss: 0.00113994
Iteration 11/25 | Loss: 0.00113994
Iteration 12/25 | Loss: 0.00113994
Iteration 13/25 | Loss: 0.00113994
Iteration 14/25 | Loss: 0.00113994
Iteration 15/25 | Loss: 0.00113994
Iteration 16/25 | Loss: 0.00113994
Iteration 17/25 | Loss: 0.00113994
Iteration 18/25 | Loss: 0.00113994
Iteration 19/25 | Loss: 0.00113994
Iteration 20/25 | Loss: 0.00113994
Iteration 21/25 | Loss: 0.00113994
Iteration 22/25 | Loss: 0.00113994
Iteration 23/25 | Loss: 0.00113994
Iteration 24/25 | Loss: 0.00113994
Iteration 25/25 | Loss: 0.00113994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113994
Iteration 2/1000 | Loss: 0.00009155
Iteration 3/1000 | Loss: 0.00008273
Iteration 4/1000 | Loss: 0.00002342
Iteration 5/1000 | Loss: 0.00002059
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001733
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001593
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001564
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001562
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001561
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001558
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001557
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001556
Iteration 44/1000 | Loss: 0.00001556
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001555
Iteration 47/1000 | Loss: 0.00001555
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001555
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001554
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001550
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001547
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001546
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001537
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001537
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001536
Iteration 156/1000 | Loss: 0.00001536
Iteration 157/1000 | Loss: 0.00001536
Iteration 158/1000 | Loss: 0.00001536
Iteration 159/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.53647724800976e-05, 1.53647724800976e-05, 1.53647724800976e-05, 1.53647724800976e-05, 1.53647724800976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53647724800976e-05

Optimization complete. Final v2v error: 3.129293203353882 mm

Highest mean error: 5.684145450592041 mm for frame 95

Lowest mean error: 2.313892364501953 mm for frame 5

Saving results

Total time: 70.54557776451111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369017
Iteration 2/25 | Loss: 0.00099821
Iteration 3/25 | Loss: 0.00095344
Iteration 4/25 | Loss: 0.00094794
Iteration 5/25 | Loss: 0.00094638
Iteration 6/25 | Loss: 0.00094638
Iteration 7/25 | Loss: 0.00094638
Iteration 8/25 | Loss: 0.00094638
Iteration 9/25 | Loss: 0.00094638
Iteration 10/25 | Loss: 0.00094638
Iteration 11/25 | Loss: 0.00094638
Iteration 12/25 | Loss: 0.00094638
Iteration 13/25 | Loss: 0.00094638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009463790920563042, 0.0009463790920563042, 0.0009463790920563042, 0.0009463790920563042, 0.0009463790920563042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009463790920563042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45517659
Iteration 2/25 | Loss: 0.00109068
Iteration 3/25 | Loss: 0.00109068
Iteration 4/25 | Loss: 0.00109068
Iteration 5/25 | Loss: 0.00109068
Iteration 6/25 | Loss: 0.00109068
Iteration 7/25 | Loss: 0.00109068
Iteration 8/25 | Loss: 0.00109068
Iteration 9/25 | Loss: 0.00109068
Iteration 10/25 | Loss: 0.00109068
Iteration 11/25 | Loss: 0.00109068
Iteration 12/25 | Loss: 0.00109068
Iteration 13/25 | Loss: 0.00109068
Iteration 14/25 | Loss: 0.00109068
Iteration 15/25 | Loss: 0.00109068
Iteration 16/25 | Loss: 0.00109068
Iteration 17/25 | Loss: 0.00109068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010906760580837727, 0.0010906760580837727, 0.0010906760580837727, 0.0010906760580837727, 0.0010906760580837727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010906760580837727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109068
Iteration 2/1000 | Loss: 0.00001719
Iteration 3/1000 | Loss: 0.00001006
Iteration 4/1000 | Loss: 0.00000834
Iteration 5/1000 | Loss: 0.00000779
Iteration 6/1000 | Loss: 0.00000770
Iteration 7/1000 | Loss: 0.00000725
Iteration 8/1000 | Loss: 0.00000705
Iteration 9/1000 | Loss: 0.00000688
Iteration 10/1000 | Loss: 0.00000677
Iteration 11/1000 | Loss: 0.00000675
Iteration 12/1000 | Loss: 0.00000672
Iteration 13/1000 | Loss: 0.00000672
Iteration 14/1000 | Loss: 0.00000672
Iteration 15/1000 | Loss: 0.00000671
Iteration 16/1000 | Loss: 0.00000671
Iteration 17/1000 | Loss: 0.00000667
Iteration 18/1000 | Loss: 0.00000667
Iteration 19/1000 | Loss: 0.00000667
Iteration 20/1000 | Loss: 0.00000666
Iteration 21/1000 | Loss: 0.00000666
Iteration 22/1000 | Loss: 0.00000666
Iteration 23/1000 | Loss: 0.00000662
Iteration 24/1000 | Loss: 0.00000661
Iteration 25/1000 | Loss: 0.00000660
Iteration 26/1000 | Loss: 0.00000658
Iteration 27/1000 | Loss: 0.00000658
Iteration 28/1000 | Loss: 0.00000657
Iteration 29/1000 | Loss: 0.00000656
Iteration 30/1000 | Loss: 0.00000656
Iteration 31/1000 | Loss: 0.00000656
Iteration 32/1000 | Loss: 0.00000655
Iteration 33/1000 | Loss: 0.00000655
Iteration 34/1000 | Loss: 0.00000654
Iteration 35/1000 | Loss: 0.00000654
Iteration 36/1000 | Loss: 0.00000654
Iteration 37/1000 | Loss: 0.00000653
Iteration 38/1000 | Loss: 0.00000653
Iteration 39/1000 | Loss: 0.00000653
Iteration 40/1000 | Loss: 0.00000653
Iteration 41/1000 | Loss: 0.00000653
Iteration 42/1000 | Loss: 0.00000652
Iteration 43/1000 | Loss: 0.00000652
Iteration 44/1000 | Loss: 0.00000652
Iteration 45/1000 | Loss: 0.00000652
Iteration 46/1000 | Loss: 0.00000652
Iteration 47/1000 | Loss: 0.00000651
Iteration 48/1000 | Loss: 0.00000651
Iteration 49/1000 | Loss: 0.00000650
Iteration 50/1000 | Loss: 0.00000650
Iteration 51/1000 | Loss: 0.00000650
Iteration 52/1000 | Loss: 0.00000650
Iteration 53/1000 | Loss: 0.00000649
Iteration 54/1000 | Loss: 0.00000649
Iteration 55/1000 | Loss: 0.00000648
Iteration 56/1000 | Loss: 0.00000648
Iteration 57/1000 | Loss: 0.00000648
Iteration 58/1000 | Loss: 0.00000647
Iteration 59/1000 | Loss: 0.00000647
Iteration 60/1000 | Loss: 0.00000647
Iteration 61/1000 | Loss: 0.00000647
Iteration 62/1000 | Loss: 0.00000647
Iteration 63/1000 | Loss: 0.00000647
Iteration 64/1000 | Loss: 0.00000647
Iteration 65/1000 | Loss: 0.00000647
Iteration 66/1000 | Loss: 0.00000646
Iteration 67/1000 | Loss: 0.00000646
Iteration 68/1000 | Loss: 0.00000646
Iteration 69/1000 | Loss: 0.00000646
Iteration 70/1000 | Loss: 0.00000646
Iteration 71/1000 | Loss: 0.00000646
Iteration 72/1000 | Loss: 0.00000646
Iteration 73/1000 | Loss: 0.00000646
Iteration 74/1000 | Loss: 0.00000646
Iteration 75/1000 | Loss: 0.00000646
Iteration 76/1000 | Loss: 0.00000646
Iteration 77/1000 | Loss: 0.00000646
Iteration 78/1000 | Loss: 0.00000646
Iteration 79/1000 | Loss: 0.00000645
Iteration 80/1000 | Loss: 0.00000645
Iteration 81/1000 | Loss: 0.00000645
Iteration 82/1000 | Loss: 0.00000645
Iteration 83/1000 | Loss: 0.00000645
Iteration 84/1000 | Loss: 0.00000645
Iteration 85/1000 | Loss: 0.00000645
Iteration 86/1000 | Loss: 0.00000645
Iteration 87/1000 | Loss: 0.00000645
Iteration 88/1000 | Loss: 0.00000645
Iteration 89/1000 | Loss: 0.00000645
Iteration 90/1000 | Loss: 0.00000645
Iteration 91/1000 | Loss: 0.00000645
Iteration 92/1000 | Loss: 0.00000644
Iteration 93/1000 | Loss: 0.00000644
Iteration 94/1000 | Loss: 0.00000644
Iteration 95/1000 | Loss: 0.00000644
Iteration 96/1000 | Loss: 0.00000644
Iteration 97/1000 | Loss: 0.00000644
Iteration 98/1000 | Loss: 0.00000644
Iteration 99/1000 | Loss: 0.00000644
Iteration 100/1000 | Loss: 0.00000644
Iteration 101/1000 | Loss: 0.00000644
Iteration 102/1000 | Loss: 0.00000643
Iteration 103/1000 | Loss: 0.00000643
Iteration 104/1000 | Loss: 0.00000643
Iteration 105/1000 | Loss: 0.00000643
Iteration 106/1000 | Loss: 0.00000643
Iteration 107/1000 | Loss: 0.00000643
Iteration 108/1000 | Loss: 0.00000643
Iteration 109/1000 | Loss: 0.00000643
Iteration 110/1000 | Loss: 0.00000643
Iteration 111/1000 | Loss: 0.00000643
Iteration 112/1000 | Loss: 0.00000643
Iteration 113/1000 | Loss: 0.00000643
Iteration 114/1000 | Loss: 0.00000643
Iteration 115/1000 | Loss: 0.00000643
Iteration 116/1000 | Loss: 0.00000643
Iteration 117/1000 | Loss: 0.00000643
Iteration 118/1000 | Loss: 0.00000643
Iteration 119/1000 | Loss: 0.00000642
Iteration 120/1000 | Loss: 0.00000642
Iteration 121/1000 | Loss: 0.00000642
Iteration 122/1000 | Loss: 0.00000642
Iteration 123/1000 | Loss: 0.00000642
Iteration 124/1000 | Loss: 0.00000642
Iteration 125/1000 | Loss: 0.00000642
Iteration 126/1000 | Loss: 0.00000642
Iteration 127/1000 | Loss: 0.00000642
Iteration 128/1000 | Loss: 0.00000642
Iteration 129/1000 | Loss: 0.00000641
Iteration 130/1000 | Loss: 0.00000641
Iteration 131/1000 | Loss: 0.00000641
Iteration 132/1000 | Loss: 0.00000641
Iteration 133/1000 | Loss: 0.00000641
Iteration 134/1000 | Loss: 0.00000641
Iteration 135/1000 | Loss: 0.00000641
Iteration 136/1000 | Loss: 0.00000641
Iteration 137/1000 | Loss: 0.00000641
Iteration 138/1000 | Loss: 0.00000641
Iteration 139/1000 | Loss: 0.00000641
Iteration 140/1000 | Loss: 0.00000641
Iteration 141/1000 | Loss: 0.00000641
Iteration 142/1000 | Loss: 0.00000641
Iteration 143/1000 | Loss: 0.00000641
Iteration 144/1000 | Loss: 0.00000641
Iteration 145/1000 | Loss: 0.00000641
Iteration 146/1000 | Loss: 0.00000641
Iteration 147/1000 | Loss: 0.00000641
Iteration 148/1000 | Loss: 0.00000641
Iteration 149/1000 | Loss: 0.00000641
Iteration 150/1000 | Loss: 0.00000641
Iteration 151/1000 | Loss: 0.00000641
Iteration 152/1000 | Loss: 0.00000641
Iteration 153/1000 | Loss: 0.00000641
Iteration 154/1000 | Loss: 0.00000641
Iteration 155/1000 | Loss: 0.00000641
Iteration 156/1000 | Loss: 0.00000641
Iteration 157/1000 | Loss: 0.00000641
Iteration 158/1000 | Loss: 0.00000641
Iteration 159/1000 | Loss: 0.00000641
Iteration 160/1000 | Loss: 0.00000641
Iteration 161/1000 | Loss: 0.00000641
Iteration 162/1000 | Loss: 0.00000641
Iteration 163/1000 | Loss: 0.00000641
Iteration 164/1000 | Loss: 0.00000641
Iteration 165/1000 | Loss: 0.00000641
Iteration 166/1000 | Loss: 0.00000641
Iteration 167/1000 | Loss: 0.00000641
Iteration 168/1000 | Loss: 0.00000641
Iteration 169/1000 | Loss: 0.00000641
Iteration 170/1000 | Loss: 0.00000641
Iteration 171/1000 | Loss: 0.00000641
Iteration 172/1000 | Loss: 0.00000641
Iteration 173/1000 | Loss: 0.00000641
Iteration 174/1000 | Loss: 0.00000641
Iteration 175/1000 | Loss: 0.00000641
Iteration 176/1000 | Loss: 0.00000641
Iteration 177/1000 | Loss: 0.00000641
Iteration 178/1000 | Loss: 0.00000641
Iteration 179/1000 | Loss: 0.00000641
Iteration 180/1000 | Loss: 0.00000641
Iteration 181/1000 | Loss: 0.00000641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [6.410450623661745e-06, 6.410450623661745e-06, 6.410450623661745e-06, 6.410450623661745e-06, 6.410450623661745e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.410450623661745e-06

Optimization complete. Final v2v error: 2.229688882827759 mm

Highest mean error: 2.5050787925720215 mm for frame 149

Lowest mean error: 1.9780519008636475 mm for frame 191

Saving results

Total time: 31.815340995788574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458513
Iteration 2/25 | Loss: 0.00108302
Iteration 3/25 | Loss: 0.00098849
Iteration 4/25 | Loss: 0.00097635
Iteration 5/25 | Loss: 0.00097375
Iteration 6/25 | Loss: 0.00097375
Iteration 7/25 | Loss: 0.00097375
Iteration 8/25 | Loss: 0.00097375
Iteration 9/25 | Loss: 0.00097375
Iteration 10/25 | Loss: 0.00097375
Iteration 11/25 | Loss: 0.00097375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009737531654536724, 0.0009737531654536724, 0.0009737531654536724, 0.0009737531654536724, 0.0009737531654536724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009737531654536724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37933826
Iteration 2/25 | Loss: 0.00105499
Iteration 3/25 | Loss: 0.00105498
Iteration 4/25 | Loss: 0.00105498
Iteration 5/25 | Loss: 0.00105498
Iteration 6/25 | Loss: 0.00105498
Iteration 7/25 | Loss: 0.00105498
Iteration 8/25 | Loss: 0.00105498
Iteration 9/25 | Loss: 0.00105498
Iteration 10/25 | Loss: 0.00105498
Iteration 11/25 | Loss: 0.00105498
Iteration 12/25 | Loss: 0.00105498
Iteration 13/25 | Loss: 0.00105498
Iteration 14/25 | Loss: 0.00105498
Iteration 15/25 | Loss: 0.00105498
Iteration 16/25 | Loss: 0.00105498
Iteration 17/25 | Loss: 0.00105498
Iteration 18/25 | Loss: 0.00105498
Iteration 19/25 | Loss: 0.00105498
Iteration 20/25 | Loss: 0.00105498
Iteration 21/25 | Loss: 0.00105498
Iteration 22/25 | Loss: 0.00105498
Iteration 23/25 | Loss: 0.00105498
Iteration 24/25 | Loss: 0.00105498
Iteration 25/25 | Loss: 0.00105498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010549782309681177, 0.0010549782309681177, 0.0010549782309681177, 0.0010549782309681177, 0.0010549782309681177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010549782309681177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105498
Iteration 2/1000 | Loss: 0.00002362
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001305
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001231
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001161
Iteration 15/1000 | Loss: 0.00001160
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001145
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001128
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001126
Iteration 30/1000 | Loss: 0.00001125
Iteration 31/1000 | Loss: 0.00001125
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001124
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001121
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001120
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001118
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001118
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001118
Iteration 50/1000 | Loss: 0.00001118
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001118
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001117
Iteration 57/1000 | Loss: 0.00001117
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001116
Iteration 60/1000 | Loss: 0.00001116
Iteration 61/1000 | Loss: 0.00001116
Iteration 62/1000 | Loss: 0.00001115
Iteration 63/1000 | Loss: 0.00001115
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001113
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.112524751079036e-05, 1.112524751079036e-05, 1.112524751079036e-05, 1.112524751079036e-05, 1.112524751079036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.112524751079036e-05

Optimization complete. Final v2v error: 2.839244842529297 mm

Highest mean error: 3.23453950881958 mm for frame 83

Lowest mean error: 2.4486780166625977 mm for frame 185

Saving results

Total time: 35.30270266532898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769114
Iteration 2/25 | Loss: 0.00192848
Iteration 3/25 | Loss: 0.00127333
Iteration 4/25 | Loss: 0.00109880
Iteration 5/25 | Loss: 0.00103938
Iteration 6/25 | Loss: 0.00104271
Iteration 7/25 | Loss: 0.00100825
Iteration 8/25 | Loss: 0.00099857
Iteration 9/25 | Loss: 0.00099481
Iteration 10/25 | Loss: 0.00098930
Iteration 11/25 | Loss: 0.00098527
Iteration 12/25 | Loss: 0.00098444
Iteration 13/25 | Loss: 0.00098410
Iteration 14/25 | Loss: 0.00098399
Iteration 15/25 | Loss: 0.00098391
Iteration 16/25 | Loss: 0.00098390
Iteration 17/25 | Loss: 0.00098388
Iteration 18/25 | Loss: 0.00098388
Iteration 19/25 | Loss: 0.00098388
Iteration 20/25 | Loss: 0.00098388
Iteration 21/25 | Loss: 0.00098388
Iteration 22/25 | Loss: 0.00098387
Iteration 23/25 | Loss: 0.00098387
Iteration 24/25 | Loss: 0.00098387
Iteration 25/25 | Loss: 0.00098387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06673574
Iteration 2/25 | Loss: 0.00120549
Iteration 3/25 | Loss: 0.00114264
Iteration 4/25 | Loss: 0.00114264
Iteration 5/25 | Loss: 0.00114264
Iteration 6/25 | Loss: 0.00114264
Iteration 7/25 | Loss: 0.00114264
Iteration 8/25 | Loss: 0.00114264
Iteration 9/25 | Loss: 0.00114264
Iteration 10/25 | Loss: 0.00114264
Iteration 11/25 | Loss: 0.00114264
Iteration 12/25 | Loss: 0.00114264
Iteration 13/25 | Loss: 0.00114264
Iteration 14/25 | Loss: 0.00114264
Iteration 15/25 | Loss: 0.00114264
Iteration 16/25 | Loss: 0.00114264
Iteration 17/25 | Loss: 0.00114264
Iteration 18/25 | Loss: 0.00114264
Iteration 19/25 | Loss: 0.00114264
Iteration 20/25 | Loss: 0.00114264
Iteration 21/25 | Loss: 0.00114264
Iteration 22/25 | Loss: 0.00114264
Iteration 23/25 | Loss: 0.00114264
Iteration 24/25 | Loss: 0.00114264
Iteration 25/25 | Loss: 0.00114264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114264
Iteration 2/1000 | Loss: 0.00003002
Iteration 3/1000 | Loss: 0.00008233
Iteration 4/1000 | Loss: 0.00023803
Iteration 5/1000 | Loss: 0.00001048
Iteration 6/1000 | Loss: 0.00005715
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00000969
Iteration 9/1000 | Loss: 0.00007511
Iteration 10/1000 | Loss: 0.00000929
Iteration 11/1000 | Loss: 0.00000900
Iteration 12/1000 | Loss: 0.00000892
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00005767
Iteration 15/1000 | Loss: 0.00006274
Iteration 16/1000 | Loss: 0.00004823
Iteration 17/1000 | Loss: 0.00001673
Iteration 18/1000 | Loss: 0.00001393
Iteration 19/1000 | Loss: 0.00002429
Iteration 20/1000 | Loss: 0.00002706
Iteration 21/1000 | Loss: 0.00001993
Iteration 22/1000 | Loss: 0.00014943
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00002401
Iteration 25/1000 | Loss: 0.00000936
Iteration 26/1000 | Loss: 0.00002733
Iteration 27/1000 | Loss: 0.00001508
Iteration 28/1000 | Loss: 0.00002140
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00000847
Iteration 32/1000 | Loss: 0.00000846
Iteration 33/1000 | Loss: 0.00000846
Iteration 34/1000 | Loss: 0.00000846
Iteration 35/1000 | Loss: 0.00000845
Iteration 36/1000 | Loss: 0.00000845
Iteration 37/1000 | Loss: 0.00000845
Iteration 38/1000 | Loss: 0.00000845
Iteration 39/1000 | Loss: 0.00000845
Iteration 40/1000 | Loss: 0.00000845
Iteration 41/1000 | Loss: 0.00000845
Iteration 42/1000 | Loss: 0.00000845
Iteration 43/1000 | Loss: 0.00000845
Iteration 44/1000 | Loss: 0.00000845
Iteration 45/1000 | Loss: 0.00000845
Iteration 46/1000 | Loss: 0.00000845
Iteration 47/1000 | Loss: 0.00000845
Iteration 48/1000 | Loss: 0.00000845
Iteration 49/1000 | Loss: 0.00000845
Iteration 50/1000 | Loss: 0.00000845
Iteration 51/1000 | Loss: 0.00000845
Iteration 52/1000 | Loss: 0.00000845
Iteration 53/1000 | Loss: 0.00000845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [8.453203918179497e-06, 8.453203918179497e-06, 8.453203918179497e-06, 8.453203918179497e-06, 8.453203918179497e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.453203918179497e-06

Optimization complete. Final v2v error: 2.4403839111328125 mm

Highest mean error: 8.55402660369873 mm for frame 127

Lowest mean error: 2.14841628074646 mm for frame 0

Saving results

Total time: 71.82245945930481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732355
Iteration 2/25 | Loss: 0.00110635
Iteration 3/25 | Loss: 0.00099631
Iteration 4/25 | Loss: 0.00098633
Iteration 5/25 | Loss: 0.00098414
Iteration 6/25 | Loss: 0.00098414
Iteration 7/25 | Loss: 0.00098414
Iteration 8/25 | Loss: 0.00098414
Iteration 9/25 | Loss: 0.00098414
Iteration 10/25 | Loss: 0.00098414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.000984142767265439, 0.000984142767265439, 0.000984142767265439, 0.000984142767265439, 0.000984142767265439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000984142767265439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 16.79051781
Iteration 2/25 | Loss: 0.00105286
Iteration 3/25 | Loss: 0.00105281
Iteration 4/25 | Loss: 0.00105280
Iteration 5/25 | Loss: 0.00105280
Iteration 6/25 | Loss: 0.00105280
Iteration 7/25 | Loss: 0.00105280
Iteration 8/25 | Loss: 0.00105280
Iteration 9/25 | Loss: 0.00105280
Iteration 10/25 | Loss: 0.00105280
Iteration 11/25 | Loss: 0.00105280
Iteration 12/25 | Loss: 0.00105280
Iteration 13/25 | Loss: 0.00105280
Iteration 14/25 | Loss: 0.00105280
Iteration 15/25 | Loss: 0.00105280
Iteration 16/25 | Loss: 0.00105280
Iteration 17/25 | Loss: 0.00105280
Iteration 18/25 | Loss: 0.00105280
Iteration 19/25 | Loss: 0.00105280
Iteration 20/25 | Loss: 0.00105280
Iteration 21/25 | Loss: 0.00105280
Iteration 22/25 | Loss: 0.00105280
Iteration 23/25 | Loss: 0.00105280
Iteration 24/25 | Loss: 0.00105280
Iteration 25/25 | Loss: 0.00105280

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105280
Iteration 2/1000 | Loss: 0.00001901
Iteration 3/1000 | Loss: 0.00001187
Iteration 4/1000 | Loss: 0.00001042
Iteration 5/1000 | Loss: 0.00000997
Iteration 6/1000 | Loss: 0.00000954
Iteration 7/1000 | Loss: 0.00000939
Iteration 8/1000 | Loss: 0.00000938
Iteration 9/1000 | Loss: 0.00000913
Iteration 10/1000 | Loss: 0.00000913
Iteration 11/1000 | Loss: 0.00000901
Iteration 12/1000 | Loss: 0.00000900
Iteration 13/1000 | Loss: 0.00000900
Iteration 14/1000 | Loss: 0.00000899
Iteration 15/1000 | Loss: 0.00000889
Iteration 16/1000 | Loss: 0.00000884
Iteration 17/1000 | Loss: 0.00000882
Iteration 18/1000 | Loss: 0.00000882
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000879
Iteration 22/1000 | Loss: 0.00000879
Iteration 23/1000 | Loss: 0.00000878
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000877
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000877
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000875
Iteration 34/1000 | Loss: 0.00000875
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000875
Iteration 37/1000 | Loss: 0.00000875
Iteration 38/1000 | Loss: 0.00000874
Iteration 39/1000 | Loss: 0.00000874
Iteration 40/1000 | Loss: 0.00000874
Iteration 41/1000 | Loss: 0.00000873
Iteration 42/1000 | Loss: 0.00000873
Iteration 43/1000 | Loss: 0.00000873
Iteration 44/1000 | Loss: 0.00000873
Iteration 45/1000 | Loss: 0.00000872
Iteration 46/1000 | Loss: 0.00000872
Iteration 47/1000 | Loss: 0.00000871
Iteration 48/1000 | Loss: 0.00000870
Iteration 49/1000 | Loss: 0.00000870
Iteration 50/1000 | Loss: 0.00000870
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000869
Iteration 53/1000 | Loss: 0.00000869
Iteration 54/1000 | Loss: 0.00000869
Iteration 55/1000 | Loss: 0.00000868
Iteration 56/1000 | Loss: 0.00000868
Iteration 57/1000 | Loss: 0.00000867
Iteration 58/1000 | Loss: 0.00000867
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000867
Iteration 61/1000 | Loss: 0.00000866
Iteration 62/1000 | Loss: 0.00000866
Iteration 63/1000 | Loss: 0.00000866
Iteration 64/1000 | Loss: 0.00000866
Iteration 65/1000 | Loss: 0.00000865
Iteration 66/1000 | Loss: 0.00000865
Iteration 67/1000 | Loss: 0.00000865
Iteration 68/1000 | Loss: 0.00000865
Iteration 69/1000 | Loss: 0.00000865
Iteration 70/1000 | Loss: 0.00000865
Iteration 71/1000 | Loss: 0.00000865
Iteration 72/1000 | Loss: 0.00000864
Iteration 73/1000 | Loss: 0.00000864
Iteration 74/1000 | Loss: 0.00000864
Iteration 75/1000 | Loss: 0.00000864
Iteration 76/1000 | Loss: 0.00000864
Iteration 77/1000 | Loss: 0.00000864
Iteration 78/1000 | Loss: 0.00000864
Iteration 79/1000 | Loss: 0.00000863
Iteration 80/1000 | Loss: 0.00000863
Iteration 81/1000 | Loss: 0.00000863
Iteration 82/1000 | Loss: 0.00000863
Iteration 83/1000 | Loss: 0.00000863
Iteration 84/1000 | Loss: 0.00000863
Iteration 85/1000 | Loss: 0.00000863
Iteration 86/1000 | Loss: 0.00000863
Iteration 87/1000 | Loss: 0.00000863
Iteration 88/1000 | Loss: 0.00000863
Iteration 89/1000 | Loss: 0.00000862
Iteration 90/1000 | Loss: 0.00000862
Iteration 91/1000 | Loss: 0.00000862
Iteration 92/1000 | Loss: 0.00000862
Iteration 93/1000 | Loss: 0.00000862
Iteration 94/1000 | Loss: 0.00000862
Iteration 95/1000 | Loss: 0.00000862
Iteration 96/1000 | Loss: 0.00000861
Iteration 97/1000 | Loss: 0.00000861
Iteration 98/1000 | Loss: 0.00000861
Iteration 99/1000 | Loss: 0.00000861
Iteration 100/1000 | Loss: 0.00000860
Iteration 101/1000 | Loss: 0.00000860
Iteration 102/1000 | Loss: 0.00000860
Iteration 103/1000 | Loss: 0.00000860
Iteration 104/1000 | Loss: 0.00000860
Iteration 105/1000 | Loss: 0.00000860
Iteration 106/1000 | Loss: 0.00000860
Iteration 107/1000 | Loss: 0.00000860
Iteration 108/1000 | Loss: 0.00000859
Iteration 109/1000 | Loss: 0.00000859
Iteration 110/1000 | Loss: 0.00000859
Iteration 111/1000 | Loss: 0.00000859
Iteration 112/1000 | Loss: 0.00000858
Iteration 113/1000 | Loss: 0.00000858
Iteration 114/1000 | Loss: 0.00000858
Iteration 115/1000 | Loss: 0.00000858
Iteration 116/1000 | Loss: 0.00000858
Iteration 117/1000 | Loss: 0.00000858
Iteration 118/1000 | Loss: 0.00000857
Iteration 119/1000 | Loss: 0.00000857
Iteration 120/1000 | Loss: 0.00000857
Iteration 121/1000 | Loss: 0.00000857
Iteration 122/1000 | Loss: 0.00000857
Iteration 123/1000 | Loss: 0.00000857
Iteration 124/1000 | Loss: 0.00000857
Iteration 125/1000 | Loss: 0.00000857
Iteration 126/1000 | Loss: 0.00000856
Iteration 127/1000 | Loss: 0.00000856
Iteration 128/1000 | Loss: 0.00000856
Iteration 129/1000 | Loss: 0.00000856
Iteration 130/1000 | Loss: 0.00000856
Iteration 131/1000 | Loss: 0.00000856
Iteration 132/1000 | Loss: 0.00000855
Iteration 133/1000 | Loss: 0.00000855
Iteration 134/1000 | Loss: 0.00000855
Iteration 135/1000 | Loss: 0.00000855
Iteration 136/1000 | Loss: 0.00000855
Iteration 137/1000 | Loss: 0.00000855
Iteration 138/1000 | Loss: 0.00000855
Iteration 139/1000 | Loss: 0.00000855
Iteration 140/1000 | Loss: 0.00000855
Iteration 141/1000 | Loss: 0.00000855
Iteration 142/1000 | Loss: 0.00000855
Iteration 143/1000 | Loss: 0.00000855
Iteration 144/1000 | Loss: 0.00000854
Iteration 145/1000 | Loss: 0.00000854
Iteration 146/1000 | Loss: 0.00000854
Iteration 147/1000 | Loss: 0.00000854
Iteration 148/1000 | Loss: 0.00000854
Iteration 149/1000 | Loss: 0.00000854
Iteration 150/1000 | Loss: 0.00000854
Iteration 151/1000 | Loss: 0.00000854
Iteration 152/1000 | Loss: 0.00000853
Iteration 153/1000 | Loss: 0.00000853
Iteration 154/1000 | Loss: 0.00000853
Iteration 155/1000 | Loss: 0.00000853
Iteration 156/1000 | Loss: 0.00000853
Iteration 157/1000 | Loss: 0.00000852
Iteration 158/1000 | Loss: 0.00000852
Iteration 159/1000 | Loss: 0.00000852
Iteration 160/1000 | Loss: 0.00000852
Iteration 161/1000 | Loss: 0.00000851
Iteration 162/1000 | Loss: 0.00000851
Iteration 163/1000 | Loss: 0.00000851
Iteration 164/1000 | Loss: 0.00000850
Iteration 165/1000 | Loss: 0.00000850
Iteration 166/1000 | Loss: 0.00000850
Iteration 167/1000 | Loss: 0.00000850
Iteration 168/1000 | Loss: 0.00000850
Iteration 169/1000 | Loss: 0.00000849
Iteration 170/1000 | Loss: 0.00000849
Iteration 171/1000 | Loss: 0.00000849
Iteration 172/1000 | Loss: 0.00000849
Iteration 173/1000 | Loss: 0.00000849
Iteration 174/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [8.494471330777742e-06, 8.494471330777742e-06, 8.494471330777742e-06, 8.494471330777742e-06, 8.494471330777742e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.494471330777742e-06

Optimization complete. Final v2v error: 2.4985647201538086 mm

Highest mean error: 3.0361368656158447 mm for frame 147

Lowest mean error: 2.1015784740448 mm for frame 190

Saving results

Total time: 38.072368144989014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642672
Iteration 2/25 | Loss: 0.00120447
Iteration 3/25 | Loss: 0.00105249
Iteration 4/25 | Loss: 0.00102094
Iteration 5/25 | Loss: 0.00101341
Iteration 6/25 | Loss: 0.00101182
Iteration 7/25 | Loss: 0.00101090
Iteration 8/25 | Loss: 0.00100640
Iteration 9/25 | Loss: 0.00100392
Iteration 10/25 | Loss: 0.00100128
Iteration 11/25 | Loss: 0.00100080
Iteration 12/25 | Loss: 0.00100059
Iteration 13/25 | Loss: 0.00100056
Iteration 14/25 | Loss: 0.00100055
Iteration 15/25 | Loss: 0.00100053
Iteration 16/25 | Loss: 0.00100053
Iteration 17/25 | Loss: 0.00100053
Iteration 18/25 | Loss: 0.00100052
Iteration 19/25 | Loss: 0.00100052
Iteration 20/25 | Loss: 0.00100052
Iteration 21/25 | Loss: 0.00100052
Iteration 22/25 | Loss: 0.00100052
Iteration 23/25 | Loss: 0.00100052
Iteration 24/25 | Loss: 0.00100052
Iteration 25/25 | Loss: 0.00100052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40812695
Iteration 2/25 | Loss: 0.00111421
Iteration 3/25 | Loss: 0.00111421
Iteration 4/25 | Loss: 0.00111421
Iteration 5/25 | Loss: 0.00111421
Iteration 6/25 | Loss: 0.00111421
Iteration 7/25 | Loss: 0.00111421
Iteration 8/25 | Loss: 0.00111421
Iteration 9/25 | Loss: 0.00111421
Iteration 10/25 | Loss: 0.00111421
Iteration 11/25 | Loss: 0.00111421
Iteration 12/25 | Loss: 0.00111421
Iteration 13/25 | Loss: 0.00111421
Iteration 14/25 | Loss: 0.00111421
Iteration 15/25 | Loss: 0.00111421
Iteration 16/25 | Loss: 0.00111421
Iteration 17/25 | Loss: 0.00111421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011142066214233637, 0.0011142066214233637, 0.0011142066214233637, 0.0011142066214233637, 0.0011142066214233637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011142066214233637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111421
Iteration 2/1000 | Loss: 0.00004411
Iteration 3/1000 | Loss: 0.00002899
Iteration 4/1000 | Loss: 0.00002217
Iteration 5/1000 | Loss: 0.00002032
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001676
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001595
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001592
Iteration 22/1000 | Loss: 0.00001592
Iteration 23/1000 | Loss: 0.00001589
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001587
Iteration 28/1000 | Loss: 0.00001586
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001585
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001581
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001578
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001573
Iteration 60/1000 | Loss: 0.00001573
Iteration 61/1000 | Loss: 0.00001573
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001570
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001569
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001568
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001567
Iteration 87/1000 | Loss: 0.00001567
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001566
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001565
Iteration 96/1000 | Loss: 0.00001565
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001563
Iteration 107/1000 | Loss: 0.00001563
Iteration 108/1000 | Loss: 0.00001563
Iteration 109/1000 | Loss: 0.00001563
Iteration 110/1000 | Loss: 0.00001563
Iteration 111/1000 | Loss: 0.00001562
Iteration 112/1000 | Loss: 0.00001562
Iteration 113/1000 | Loss: 0.00001562
Iteration 114/1000 | Loss: 0.00001562
Iteration 115/1000 | Loss: 0.00001562
Iteration 116/1000 | Loss: 0.00001562
Iteration 117/1000 | Loss: 0.00001562
Iteration 118/1000 | Loss: 0.00001561
Iteration 119/1000 | Loss: 0.00001561
Iteration 120/1000 | Loss: 0.00001561
Iteration 121/1000 | Loss: 0.00001561
Iteration 122/1000 | Loss: 0.00001561
Iteration 123/1000 | Loss: 0.00001561
Iteration 124/1000 | Loss: 0.00001561
Iteration 125/1000 | Loss: 0.00001561
Iteration 126/1000 | Loss: 0.00001561
Iteration 127/1000 | Loss: 0.00001561
Iteration 128/1000 | Loss: 0.00001560
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001560
Iteration 132/1000 | Loss: 0.00001560
Iteration 133/1000 | Loss: 0.00001560
Iteration 134/1000 | Loss: 0.00001560
Iteration 135/1000 | Loss: 0.00001560
Iteration 136/1000 | Loss: 0.00001560
Iteration 137/1000 | Loss: 0.00001559
Iteration 138/1000 | Loss: 0.00001559
Iteration 139/1000 | Loss: 0.00001559
Iteration 140/1000 | Loss: 0.00001559
Iteration 141/1000 | Loss: 0.00001559
Iteration 142/1000 | Loss: 0.00001559
Iteration 143/1000 | Loss: 0.00001559
Iteration 144/1000 | Loss: 0.00001559
Iteration 145/1000 | Loss: 0.00001559
Iteration 146/1000 | Loss: 0.00001559
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001559
Iteration 150/1000 | Loss: 0.00001559
Iteration 151/1000 | Loss: 0.00001559
Iteration 152/1000 | Loss: 0.00001559
Iteration 153/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5593584976159036e-05, 1.5593584976159036e-05, 1.5593584976159036e-05, 1.5593584976159036e-05, 1.5593584976159036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5593584976159036e-05

Optimization complete. Final v2v error: 3.33554744720459 mm

Highest mean error: 4.229410648345947 mm for frame 159

Lowest mean error: 2.742003917694092 mm for frame 33

Saving results

Total time: 62.01681041717529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537618
Iteration 2/25 | Loss: 0.00137852
Iteration 3/25 | Loss: 0.00116001
Iteration 4/25 | Loss: 0.00113831
Iteration 5/25 | Loss: 0.00113533
Iteration 6/25 | Loss: 0.00113486
Iteration 7/25 | Loss: 0.00113486
Iteration 8/25 | Loss: 0.00113486
Iteration 9/25 | Loss: 0.00113486
Iteration 10/25 | Loss: 0.00113486
Iteration 11/25 | Loss: 0.00113486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011348604457452893, 0.0011348604457452893, 0.0011348604457452893, 0.0011348604457452893, 0.0011348604457452893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011348604457452893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36887801
Iteration 2/25 | Loss: 0.00093127
Iteration 3/25 | Loss: 0.00093127
Iteration 4/25 | Loss: 0.00093127
Iteration 5/25 | Loss: 0.00093127
Iteration 6/25 | Loss: 0.00093127
Iteration 7/25 | Loss: 0.00093127
Iteration 8/25 | Loss: 0.00093127
Iteration 9/25 | Loss: 0.00093127
Iteration 10/25 | Loss: 0.00093127
Iteration 11/25 | Loss: 0.00093127
Iteration 12/25 | Loss: 0.00093127
Iteration 13/25 | Loss: 0.00093127
Iteration 14/25 | Loss: 0.00093127
Iteration 15/25 | Loss: 0.00093127
Iteration 16/25 | Loss: 0.00093127
Iteration 17/25 | Loss: 0.00093127
Iteration 18/25 | Loss: 0.00093127
Iteration 19/25 | Loss: 0.00093127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009312687325291336, 0.0009312687325291336, 0.0009312687325291336, 0.0009312687325291336, 0.0009312687325291336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009312687325291336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093127
Iteration 2/1000 | Loss: 0.00005747
Iteration 3/1000 | Loss: 0.00003324
Iteration 4/1000 | Loss: 0.00002428
Iteration 5/1000 | Loss: 0.00002212
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001972
Iteration 9/1000 | Loss: 0.00001943
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001877
Iteration 12/1000 | Loss: 0.00001850
Iteration 13/1000 | Loss: 0.00001827
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001815
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001802
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001796
Iteration 22/1000 | Loss: 0.00001795
Iteration 23/1000 | Loss: 0.00001795
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001792
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001782
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001778
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001772
Iteration 69/1000 | Loss: 0.00001772
Iteration 70/1000 | Loss: 0.00001772
Iteration 71/1000 | Loss: 0.00001772
Iteration 72/1000 | Loss: 0.00001771
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001769
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001768
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001766
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001766
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00001765
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001765
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001764
Iteration 90/1000 | Loss: 0.00001764
Iteration 91/1000 | Loss: 0.00001764
Iteration 92/1000 | Loss: 0.00001764
Iteration 93/1000 | Loss: 0.00001764
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001761
Iteration 104/1000 | Loss: 0.00001761
Iteration 105/1000 | Loss: 0.00001761
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001759
Iteration 109/1000 | Loss: 0.00001759
Iteration 110/1000 | Loss: 0.00001759
Iteration 111/1000 | Loss: 0.00001759
Iteration 112/1000 | Loss: 0.00001759
Iteration 113/1000 | Loss: 0.00001759
Iteration 114/1000 | Loss: 0.00001758
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001758
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001757
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001756
Iteration 125/1000 | Loss: 0.00001756
Iteration 126/1000 | Loss: 0.00001756
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001755
Iteration 129/1000 | Loss: 0.00001755
Iteration 130/1000 | Loss: 0.00001755
Iteration 131/1000 | Loss: 0.00001755
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Iteration 138/1000 | Loss: 0.00001754
Iteration 139/1000 | Loss: 0.00001753
Iteration 140/1000 | Loss: 0.00001753
Iteration 141/1000 | Loss: 0.00001753
Iteration 142/1000 | Loss: 0.00001752
Iteration 143/1000 | Loss: 0.00001752
Iteration 144/1000 | Loss: 0.00001752
Iteration 145/1000 | Loss: 0.00001752
Iteration 146/1000 | Loss: 0.00001752
Iteration 147/1000 | Loss: 0.00001752
Iteration 148/1000 | Loss: 0.00001752
Iteration 149/1000 | Loss: 0.00001752
Iteration 150/1000 | Loss: 0.00001752
Iteration 151/1000 | Loss: 0.00001752
Iteration 152/1000 | Loss: 0.00001752
Iteration 153/1000 | Loss: 0.00001752
Iteration 154/1000 | Loss: 0.00001752
Iteration 155/1000 | Loss: 0.00001752
Iteration 156/1000 | Loss: 0.00001752
Iteration 157/1000 | Loss: 0.00001752
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001750
Iteration 161/1000 | Loss: 0.00001750
Iteration 162/1000 | Loss: 0.00001750
Iteration 163/1000 | Loss: 0.00001750
Iteration 164/1000 | Loss: 0.00001750
Iteration 165/1000 | Loss: 0.00001749
Iteration 166/1000 | Loss: 0.00001749
Iteration 167/1000 | Loss: 0.00001749
Iteration 168/1000 | Loss: 0.00001749
Iteration 169/1000 | Loss: 0.00001749
Iteration 170/1000 | Loss: 0.00001749
Iteration 171/1000 | Loss: 0.00001749
Iteration 172/1000 | Loss: 0.00001749
Iteration 173/1000 | Loss: 0.00001749
Iteration 174/1000 | Loss: 0.00001749
Iteration 175/1000 | Loss: 0.00001749
Iteration 176/1000 | Loss: 0.00001748
Iteration 177/1000 | Loss: 0.00001748
Iteration 178/1000 | Loss: 0.00001748
Iteration 179/1000 | Loss: 0.00001748
Iteration 180/1000 | Loss: 0.00001748
Iteration 181/1000 | Loss: 0.00001748
Iteration 182/1000 | Loss: 0.00001748
Iteration 183/1000 | Loss: 0.00001748
Iteration 184/1000 | Loss: 0.00001748
Iteration 185/1000 | Loss: 0.00001748
Iteration 186/1000 | Loss: 0.00001748
Iteration 187/1000 | Loss: 0.00001748
Iteration 188/1000 | Loss: 0.00001748
Iteration 189/1000 | Loss: 0.00001747
Iteration 190/1000 | Loss: 0.00001747
Iteration 191/1000 | Loss: 0.00001747
Iteration 192/1000 | Loss: 0.00001747
Iteration 193/1000 | Loss: 0.00001747
Iteration 194/1000 | Loss: 0.00001747
Iteration 195/1000 | Loss: 0.00001747
Iteration 196/1000 | Loss: 0.00001747
Iteration 197/1000 | Loss: 0.00001747
Iteration 198/1000 | Loss: 0.00001747
Iteration 199/1000 | Loss: 0.00001747
Iteration 200/1000 | Loss: 0.00001747
Iteration 201/1000 | Loss: 0.00001747
Iteration 202/1000 | Loss: 0.00001747
Iteration 203/1000 | Loss: 0.00001747
Iteration 204/1000 | Loss: 0.00001747
Iteration 205/1000 | Loss: 0.00001747
Iteration 206/1000 | Loss: 0.00001747
Iteration 207/1000 | Loss: 0.00001747
Iteration 208/1000 | Loss: 0.00001747
Iteration 209/1000 | Loss: 0.00001747
Iteration 210/1000 | Loss: 0.00001747
Iteration 211/1000 | Loss: 0.00001747
Iteration 212/1000 | Loss: 0.00001747
Iteration 213/1000 | Loss: 0.00001747
Iteration 214/1000 | Loss: 0.00001747
Iteration 215/1000 | Loss: 0.00001747
Iteration 216/1000 | Loss: 0.00001747
Iteration 217/1000 | Loss: 0.00001747
Iteration 218/1000 | Loss: 0.00001747
Iteration 219/1000 | Loss: 0.00001747
Iteration 220/1000 | Loss: 0.00001747
Iteration 221/1000 | Loss: 0.00001747
Iteration 222/1000 | Loss: 0.00001747
Iteration 223/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.74747528944863e-05, 1.74747528944863e-05, 1.74747528944863e-05, 1.74747528944863e-05, 1.74747528944863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.74747528944863e-05

Optimization complete. Final v2v error: 3.522655487060547 mm

Highest mean error: 3.8278791904449463 mm for frame 115

Lowest mean error: 3.0007686614990234 mm for frame 14

Saving results

Total time: 44.26461577415466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045709
Iteration 2/25 | Loss: 0.01045709
Iteration 3/25 | Loss: 0.01045709
Iteration 4/25 | Loss: 0.01045708
Iteration 5/25 | Loss: 0.01045708
Iteration 6/25 | Loss: 0.00197406
Iteration 7/25 | Loss: 0.00152031
Iteration 8/25 | Loss: 0.00150851
Iteration 9/25 | Loss: 0.00128400
Iteration 10/25 | Loss: 0.00119682
Iteration 11/25 | Loss: 0.00120295
Iteration 12/25 | Loss: 0.00110939
Iteration 13/25 | Loss: 0.00107785
Iteration 14/25 | Loss: 0.00104613
Iteration 15/25 | Loss: 0.00103731
Iteration 16/25 | Loss: 0.00103400
Iteration 17/25 | Loss: 0.00103259
Iteration 18/25 | Loss: 0.00103168
Iteration 19/25 | Loss: 0.00103765
Iteration 20/25 | Loss: 0.00102777
Iteration 21/25 | Loss: 0.00102031
Iteration 22/25 | Loss: 0.00101359
Iteration 23/25 | Loss: 0.00101264
Iteration 24/25 | Loss: 0.00101002
Iteration 25/25 | Loss: 0.00100495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37864172
Iteration 2/25 | Loss: 0.00162765
Iteration 3/25 | Loss: 0.00141389
Iteration 4/25 | Loss: 0.00141389
Iteration 5/25 | Loss: 0.00141388
Iteration 6/25 | Loss: 0.00141388
Iteration 7/25 | Loss: 0.00141388
Iteration 8/25 | Loss: 0.00141388
Iteration 9/25 | Loss: 0.00141388
Iteration 10/25 | Loss: 0.00141388
Iteration 11/25 | Loss: 0.00141388
Iteration 12/25 | Loss: 0.00141388
Iteration 13/25 | Loss: 0.00141388
Iteration 14/25 | Loss: 0.00141388
Iteration 15/25 | Loss: 0.00141388
Iteration 16/25 | Loss: 0.00141388
Iteration 17/25 | Loss: 0.00141388
Iteration 18/25 | Loss: 0.00141388
Iteration 19/25 | Loss: 0.00141388
Iteration 20/25 | Loss: 0.00141388
Iteration 21/25 | Loss: 0.00141388
Iteration 22/25 | Loss: 0.00141388
Iteration 23/25 | Loss: 0.00141388
Iteration 24/25 | Loss: 0.00141388
Iteration 25/25 | Loss: 0.00141388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141388
Iteration 2/1000 | Loss: 0.00064339
Iteration 3/1000 | Loss: 0.00035048
Iteration 4/1000 | Loss: 0.00035893
Iteration 5/1000 | Loss: 0.00165690
Iteration 6/1000 | Loss: 0.00025589
Iteration 7/1000 | Loss: 0.00043782
Iteration 8/1000 | Loss: 0.00034564
Iteration 9/1000 | Loss: 0.00049249
Iteration 10/1000 | Loss: 0.00036185
Iteration 11/1000 | Loss: 0.00025303
Iteration 12/1000 | Loss: 0.00015958
Iteration 13/1000 | Loss: 0.00045844
Iteration 14/1000 | Loss: 0.00013513
Iteration 15/1000 | Loss: 0.00010341
Iteration 16/1000 | Loss: 0.00011947
Iteration 17/1000 | Loss: 0.00008386
Iteration 18/1000 | Loss: 0.00007224
Iteration 19/1000 | Loss: 0.00034299
Iteration 20/1000 | Loss: 0.00024304
Iteration 21/1000 | Loss: 0.00042982
Iteration 22/1000 | Loss: 0.00014289
Iteration 23/1000 | Loss: 0.00007960
Iteration 24/1000 | Loss: 0.00006003
Iteration 25/1000 | Loss: 0.00014458
Iteration 26/1000 | Loss: 0.00011808
Iteration 27/1000 | Loss: 0.00008421
Iteration 28/1000 | Loss: 0.00007092
Iteration 29/1000 | Loss: 0.00004669
Iteration 30/1000 | Loss: 0.00005612
Iteration 31/1000 | Loss: 0.00006603
Iteration 32/1000 | Loss: 0.00004288
Iteration 33/1000 | Loss: 0.00005432
Iteration 34/1000 | Loss: 0.00021435
Iteration 35/1000 | Loss: 0.00018747
Iteration 36/1000 | Loss: 0.00017951
Iteration 37/1000 | Loss: 0.00019238
Iteration 38/1000 | Loss: 0.00016880
Iteration 39/1000 | Loss: 0.00018168
Iteration 40/1000 | Loss: 0.00019560
Iteration 41/1000 | Loss: 0.00019898
Iteration 42/1000 | Loss: 0.00015232
Iteration 43/1000 | Loss: 0.00017292
Iteration 44/1000 | Loss: 0.00013331
Iteration 45/1000 | Loss: 0.00005901
Iteration 46/1000 | Loss: 0.00006135
Iteration 47/1000 | Loss: 0.00019640
Iteration 48/1000 | Loss: 0.00032861
Iteration 49/1000 | Loss: 0.00047618
Iteration 50/1000 | Loss: 0.00031499
Iteration 51/1000 | Loss: 0.00043244
Iteration 52/1000 | Loss: 0.00028534
Iteration 53/1000 | Loss: 0.00011475
Iteration 54/1000 | Loss: 0.00013704
Iteration 55/1000 | Loss: 0.00010286
Iteration 56/1000 | Loss: 0.00012899
Iteration 57/1000 | Loss: 0.00013546
Iteration 58/1000 | Loss: 0.00016148
Iteration 59/1000 | Loss: 0.00049169
Iteration 60/1000 | Loss: 0.00018505
Iteration 61/1000 | Loss: 0.00008058
Iteration 62/1000 | Loss: 0.00004460
Iteration 63/1000 | Loss: 0.00004765
Iteration 64/1000 | Loss: 0.00003838
Iteration 65/1000 | Loss: 0.00005289
Iteration 66/1000 | Loss: 0.00004061
Iteration 67/1000 | Loss: 0.00006264
Iteration 68/1000 | Loss: 0.00006556
Iteration 69/1000 | Loss: 0.00003734
Iteration 70/1000 | Loss: 0.00004721
Iteration 71/1000 | Loss: 0.00005820
Iteration 72/1000 | Loss: 0.00004403
Iteration 73/1000 | Loss: 0.00003905
Iteration 74/1000 | Loss: 0.00003651
Iteration 75/1000 | Loss: 0.00004191
Iteration 76/1000 | Loss: 0.00004183
Iteration 77/1000 | Loss: 0.00004013
Iteration 78/1000 | Loss: 0.00003881
Iteration 79/1000 | Loss: 0.00004008
Iteration 80/1000 | Loss: 0.00004897
Iteration 81/1000 | Loss: 0.00004315
Iteration 82/1000 | Loss: 0.00008170
Iteration 83/1000 | Loss: 0.00003788
Iteration 84/1000 | Loss: 0.00004255
Iteration 85/1000 | Loss: 0.00005778
Iteration 86/1000 | Loss: 0.00004333
Iteration 87/1000 | Loss: 0.00005387
Iteration 88/1000 | Loss: 0.00004345
Iteration 89/1000 | Loss: 0.00003264
Iteration 90/1000 | Loss: 0.00035240
Iteration 91/1000 | Loss: 0.00022422
Iteration 92/1000 | Loss: 0.00027681
Iteration 93/1000 | Loss: 0.00009954
Iteration 94/1000 | Loss: 0.00005316
Iteration 95/1000 | Loss: 0.00032187
Iteration 96/1000 | Loss: 0.00024753
Iteration 97/1000 | Loss: 0.00036364
Iteration 98/1000 | Loss: 0.00003441
Iteration 99/1000 | Loss: 0.00026252
Iteration 100/1000 | Loss: 0.00003904
Iteration 101/1000 | Loss: 0.00044541
Iteration 102/1000 | Loss: 0.00025853
Iteration 103/1000 | Loss: 0.00004502
Iteration 104/1000 | Loss: 0.00003402
Iteration 105/1000 | Loss: 0.00004364
Iteration 106/1000 | Loss: 0.00003278
Iteration 107/1000 | Loss: 0.00004419
Iteration 108/1000 | Loss: 0.00003774
Iteration 109/1000 | Loss: 0.00057652
Iteration 110/1000 | Loss: 0.00071820
Iteration 111/1000 | Loss: 0.00071591
Iteration 112/1000 | Loss: 0.00063291
Iteration 113/1000 | Loss: 0.00024277
Iteration 114/1000 | Loss: 0.00010895
Iteration 115/1000 | Loss: 0.00031142
Iteration 116/1000 | Loss: 0.00038146
Iteration 117/1000 | Loss: 0.00036244
Iteration 118/1000 | Loss: 0.00035610
Iteration 119/1000 | Loss: 0.00003872
Iteration 120/1000 | Loss: 0.00004406
Iteration 121/1000 | Loss: 0.00003252
Iteration 122/1000 | Loss: 0.00004343
Iteration 123/1000 | Loss: 0.00003667
Iteration 124/1000 | Loss: 0.00004991
Iteration 125/1000 | Loss: 0.00003219
Iteration 126/1000 | Loss: 0.00004191
Iteration 127/1000 | Loss: 0.00003927
Iteration 128/1000 | Loss: 0.00002909
Iteration 129/1000 | Loss: 0.00002905
Iteration 130/1000 | Loss: 0.00002903
Iteration 131/1000 | Loss: 0.00026880
Iteration 132/1000 | Loss: 0.00048257
Iteration 133/1000 | Loss: 0.00030381
Iteration 134/1000 | Loss: 0.00053253
Iteration 135/1000 | Loss: 0.00028092
Iteration 136/1000 | Loss: 0.00013936
Iteration 137/1000 | Loss: 0.00010736
Iteration 138/1000 | Loss: 0.00003924
Iteration 139/1000 | Loss: 0.00003929
Iteration 140/1000 | Loss: 0.00007892
Iteration 141/1000 | Loss: 0.00004123
Iteration 142/1000 | Loss: 0.00004910
Iteration 143/1000 | Loss: 0.00002724
Iteration 144/1000 | Loss: 0.00032587
Iteration 145/1000 | Loss: 0.00017158
Iteration 146/1000 | Loss: 0.00004982
Iteration 147/1000 | Loss: 0.00011527
Iteration 148/1000 | Loss: 0.00002837
Iteration 149/1000 | Loss: 0.00004974
Iteration 150/1000 | Loss: 0.00014826
Iteration 151/1000 | Loss: 0.00066202
Iteration 152/1000 | Loss: 0.00043951
Iteration 153/1000 | Loss: 0.00011883
Iteration 154/1000 | Loss: 0.00002895
Iteration 155/1000 | Loss: 0.00002570
Iteration 156/1000 | Loss: 0.00002492
Iteration 157/1000 | Loss: 0.00007450
Iteration 158/1000 | Loss: 0.00002567
Iteration 159/1000 | Loss: 0.00004252
Iteration 160/1000 | Loss: 0.00002712
Iteration 161/1000 | Loss: 0.00002678
Iteration 162/1000 | Loss: 0.00003238
Iteration 163/1000 | Loss: 0.00005772
Iteration 164/1000 | Loss: 0.00013502
Iteration 165/1000 | Loss: 0.00002600
Iteration 166/1000 | Loss: 0.00002421
Iteration 167/1000 | Loss: 0.00003502
Iteration 168/1000 | Loss: 0.00002445
Iteration 169/1000 | Loss: 0.00003956
Iteration 170/1000 | Loss: 0.00002405
Iteration 171/1000 | Loss: 0.00002405
Iteration 172/1000 | Loss: 0.00002404
Iteration 173/1000 | Loss: 0.00002402
Iteration 174/1000 | Loss: 0.00002401
Iteration 175/1000 | Loss: 0.00002400
Iteration 176/1000 | Loss: 0.00002396
Iteration 177/1000 | Loss: 0.00002395
Iteration 178/1000 | Loss: 0.00002393
Iteration 179/1000 | Loss: 0.00023627
Iteration 180/1000 | Loss: 0.00006522
Iteration 181/1000 | Loss: 0.00011623
Iteration 182/1000 | Loss: 0.00002729
Iteration 183/1000 | Loss: 0.00002555
Iteration 184/1000 | Loss: 0.00002386
Iteration 185/1000 | Loss: 0.00002305
Iteration 186/1000 | Loss: 0.00002267
Iteration 187/1000 | Loss: 0.00002256
Iteration 188/1000 | Loss: 0.00002255
Iteration 189/1000 | Loss: 0.00002250
Iteration 190/1000 | Loss: 0.00002244
Iteration 191/1000 | Loss: 0.00002243
Iteration 192/1000 | Loss: 0.00002242
Iteration 193/1000 | Loss: 0.00002241
Iteration 194/1000 | Loss: 0.00002239
Iteration 195/1000 | Loss: 0.00002239
Iteration 196/1000 | Loss: 0.00002239
Iteration 197/1000 | Loss: 0.00002238
Iteration 198/1000 | Loss: 0.00002237
Iteration 199/1000 | Loss: 0.00002234
Iteration 200/1000 | Loss: 0.00002234
Iteration 201/1000 | Loss: 0.00002233
Iteration 202/1000 | Loss: 0.00002231
Iteration 203/1000 | Loss: 0.00002231
Iteration 204/1000 | Loss: 0.00002225
Iteration 205/1000 | Loss: 0.00002224
Iteration 206/1000 | Loss: 0.00002224
Iteration 207/1000 | Loss: 0.00002221
Iteration 208/1000 | Loss: 0.00002216
Iteration 209/1000 | Loss: 0.00002216
Iteration 210/1000 | Loss: 0.00004141
Iteration 211/1000 | Loss: 0.00002215
Iteration 212/1000 | Loss: 0.00002213
Iteration 213/1000 | Loss: 0.00002213
Iteration 214/1000 | Loss: 0.00002212
Iteration 215/1000 | Loss: 0.00002211
Iteration 216/1000 | Loss: 0.00002211
Iteration 217/1000 | Loss: 0.00002210
Iteration 218/1000 | Loss: 0.00002210
Iteration 219/1000 | Loss: 0.00002210
Iteration 220/1000 | Loss: 0.00002209
Iteration 221/1000 | Loss: 0.00002209
Iteration 222/1000 | Loss: 0.00002208
Iteration 223/1000 | Loss: 0.00002208
Iteration 224/1000 | Loss: 0.00002207
Iteration 225/1000 | Loss: 0.00021836
Iteration 226/1000 | Loss: 0.00003137
Iteration 227/1000 | Loss: 0.00002516
Iteration 228/1000 | Loss: 0.00002359
Iteration 229/1000 | Loss: 0.00002327
Iteration 230/1000 | Loss: 0.00003453
Iteration 231/1000 | Loss: 0.00002110
Iteration 232/1000 | Loss: 0.00002087
Iteration 233/1000 | Loss: 0.00002075
Iteration 234/1000 | Loss: 0.00002072
Iteration 235/1000 | Loss: 0.00002071
Iteration 236/1000 | Loss: 0.00002070
Iteration 237/1000 | Loss: 0.00002070
Iteration 238/1000 | Loss: 0.00002069
Iteration 239/1000 | Loss: 0.00002068
Iteration 240/1000 | Loss: 0.00002067
Iteration 241/1000 | Loss: 0.00002059
Iteration 242/1000 | Loss: 0.00002059
Iteration 243/1000 | Loss: 0.00002056
Iteration 244/1000 | Loss: 0.00002056
Iteration 245/1000 | Loss: 0.00002055
Iteration 246/1000 | Loss: 0.00002055
Iteration 247/1000 | Loss: 0.00002054
Iteration 248/1000 | Loss: 0.00002053
Iteration 249/1000 | Loss: 0.00002052
Iteration 250/1000 | Loss: 0.00002050
Iteration 251/1000 | Loss: 0.00002050
Iteration 252/1000 | Loss: 0.00002049
Iteration 253/1000 | Loss: 0.00002048
Iteration 254/1000 | Loss: 0.00002048
Iteration 255/1000 | Loss: 0.00002047
Iteration 256/1000 | Loss: 0.00002047
Iteration 257/1000 | Loss: 0.00002047
Iteration 258/1000 | Loss: 0.00002047
Iteration 259/1000 | Loss: 0.00002047
Iteration 260/1000 | Loss: 0.00002047
Iteration 261/1000 | Loss: 0.00002047
Iteration 262/1000 | Loss: 0.00002047
Iteration 263/1000 | Loss: 0.00002047
Iteration 264/1000 | Loss: 0.00002047
Iteration 265/1000 | Loss: 0.00002047
Iteration 266/1000 | Loss: 0.00002047
Iteration 267/1000 | Loss: 0.00002047
Iteration 268/1000 | Loss: 0.00002047
Iteration 269/1000 | Loss: 0.00002047
Iteration 270/1000 | Loss: 0.00002046
Iteration 271/1000 | Loss: 0.00002046
Iteration 272/1000 | Loss: 0.00002046
Iteration 273/1000 | Loss: 0.00002046
Iteration 274/1000 | Loss: 0.00002046
Iteration 275/1000 | Loss: 0.00002046
Iteration 276/1000 | Loss: 0.00002045
Iteration 277/1000 | Loss: 0.00002045
Iteration 278/1000 | Loss: 0.00002045
Iteration 279/1000 | Loss: 0.00002044
Iteration 280/1000 | Loss: 0.00002044
Iteration 281/1000 | Loss: 0.00002044
Iteration 282/1000 | Loss: 0.00002043
Iteration 283/1000 | Loss: 0.00002043
Iteration 284/1000 | Loss: 0.00002043
Iteration 285/1000 | Loss: 0.00002042
Iteration 286/1000 | Loss: 0.00002042
Iteration 287/1000 | Loss: 0.00002041
Iteration 288/1000 | Loss: 0.00002041
Iteration 289/1000 | Loss: 0.00002041
Iteration 290/1000 | Loss: 0.00002041
Iteration 291/1000 | Loss: 0.00002041
Iteration 292/1000 | Loss: 0.00002041
Iteration 293/1000 | Loss: 0.00002040
Iteration 294/1000 | Loss: 0.00002040
Iteration 295/1000 | Loss: 0.00002040
Iteration 296/1000 | Loss: 0.00002040
Iteration 297/1000 | Loss: 0.00002040
Iteration 298/1000 | Loss: 0.00002040
Iteration 299/1000 | Loss: 0.00002040
Iteration 300/1000 | Loss: 0.00002039
Iteration 301/1000 | Loss: 0.00002039
Iteration 302/1000 | Loss: 0.00002039
Iteration 303/1000 | Loss: 0.00002039
Iteration 304/1000 | Loss: 0.00002038
Iteration 305/1000 | Loss: 0.00002038
Iteration 306/1000 | Loss: 0.00002038
Iteration 307/1000 | Loss: 0.00002038
Iteration 308/1000 | Loss: 0.00002038
Iteration 309/1000 | Loss: 0.00002038
Iteration 310/1000 | Loss: 0.00002038
Iteration 311/1000 | Loss: 0.00002038
Iteration 312/1000 | Loss: 0.00002038
Iteration 313/1000 | Loss: 0.00002038
Iteration 314/1000 | Loss: 0.00002037
Iteration 315/1000 | Loss: 0.00002037
Iteration 316/1000 | Loss: 0.00002036
Iteration 317/1000 | Loss: 0.00002036
Iteration 318/1000 | Loss: 0.00002036
Iteration 319/1000 | Loss: 0.00002036
Iteration 320/1000 | Loss: 0.00002036
Iteration 321/1000 | Loss: 0.00002036
Iteration 322/1000 | Loss: 0.00002035
Iteration 323/1000 | Loss: 0.00002035
Iteration 324/1000 | Loss: 0.00002035
Iteration 325/1000 | Loss: 0.00002035
Iteration 326/1000 | Loss: 0.00002035
Iteration 327/1000 | Loss: 0.00002035
Iteration 328/1000 | Loss: 0.00002035
Iteration 329/1000 | Loss: 0.00002035
Iteration 330/1000 | Loss: 0.00002034
Iteration 331/1000 | Loss: 0.00002034
Iteration 332/1000 | Loss: 0.00002034
Iteration 333/1000 | Loss: 0.00002034
Iteration 334/1000 | Loss: 0.00002034
Iteration 335/1000 | Loss: 0.00002034
Iteration 336/1000 | Loss: 0.00002034
Iteration 337/1000 | Loss: 0.00002034
Iteration 338/1000 | Loss: 0.00002034
Iteration 339/1000 | Loss: 0.00002034
Iteration 340/1000 | Loss: 0.00002033
Iteration 341/1000 | Loss: 0.00002033
Iteration 342/1000 | Loss: 0.00002033
Iteration 343/1000 | Loss: 0.00002033
Iteration 344/1000 | Loss: 0.00002033
Iteration 345/1000 | Loss: 0.00002033
Iteration 346/1000 | Loss: 0.00002033
Iteration 347/1000 | Loss: 0.00002033
Iteration 348/1000 | Loss: 0.00002033
Iteration 349/1000 | Loss: 0.00002033
Iteration 350/1000 | Loss: 0.00002033
Iteration 351/1000 | Loss: 0.00002033
Iteration 352/1000 | Loss: 0.00002032
Iteration 353/1000 | Loss: 0.00002032
Iteration 354/1000 | Loss: 0.00002032
Iteration 355/1000 | Loss: 0.00002032
Iteration 356/1000 | Loss: 0.00002032
Iteration 357/1000 | Loss: 0.00002032
Iteration 358/1000 | Loss: 0.00003113
Iteration 359/1000 | Loss: 0.00002032
Iteration 360/1000 | Loss: 0.00002192
Iteration 361/1000 | Loss: 0.00002030
Iteration 362/1000 | Loss: 0.00002030
Iteration 363/1000 | Loss: 0.00002030
Iteration 364/1000 | Loss: 0.00002030
Iteration 365/1000 | Loss: 0.00002030
Iteration 366/1000 | Loss: 0.00002030
Iteration 367/1000 | Loss: 0.00002029
Iteration 368/1000 | Loss: 0.00002029
Iteration 369/1000 | Loss: 0.00002029
Iteration 370/1000 | Loss: 0.00002029
Iteration 371/1000 | Loss: 0.00002029
Iteration 372/1000 | Loss: 0.00002028
Iteration 373/1000 | Loss: 0.00002028
Iteration 374/1000 | Loss: 0.00002028
Iteration 375/1000 | Loss: 0.00002028
Iteration 376/1000 | Loss: 0.00002028
Iteration 377/1000 | Loss: 0.00002028
Iteration 378/1000 | Loss: 0.00002028
Iteration 379/1000 | Loss: 0.00002028
Iteration 380/1000 | Loss: 0.00002028
Iteration 381/1000 | Loss: 0.00002028
Iteration 382/1000 | Loss: 0.00002028
Iteration 383/1000 | Loss: 0.00002028
Iteration 384/1000 | Loss: 0.00002028
Iteration 385/1000 | Loss: 0.00002028
Iteration 386/1000 | Loss: 0.00002028
Iteration 387/1000 | Loss: 0.00002028
Iteration 388/1000 | Loss: 0.00002028
Iteration 389/1000 | Loss: 0.00002028
Iteration 390/1000 | Loss: 0.00002028
Iteration 391/1000 | Loss: 0.00002028
Iteration 392/1000 | Loss: 0.00002028
Iteration 393/1000 | Loss: 0.00002028
Iteration 394/1000 | Loss: 0.00002028
Iteration 395/1000 | Loss: 0.00002028
Iteration 396/1000 | Loss: 0.00002028
Iteration 397/1000 | Loss: 0.00002028
Iteration 398/1000 | Loss: 0.00002028
Iteration 399/1000 | Loss: 0.00002028
Iteration 400/1000 | Loss: 0.00002028
Iteration 401/1000 | Loss: 0.00002028
Iteration 402/1000 | Loss: 0.00002028
Iteration 403/1000 | Loss: 0.00002028
Iteration 404/1000 | Loss: 0.00002028
Iteration 405/1000 | Loss: 0.00002028
Iteration 406/1000 | Loss: 0.00002028
Iteration 407/1000 | Loss: 0.00002028
Iteration 408/1000 | Loss: 0.00002028
Iteration 409/1000 | Loss: 0.00002028
Iteration 410/1000 | Loss: 0.00002028
Iteration 411/1000 | Loss: 0.00002028
Iteration 412/1000 | Loss: 0.00002028
Iteration 413/1000 | Loss: 0.00002028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 413. Stopping optimization.
Last 5 losses: [2.02803985303035e-05, 2.02803985303035e-05, 2.02803985303035e-05, 2.02803985303035e-05, 2.02803985303035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.02803985303035e-05

Optimization complete. Final v2v error: 2.475153684616089 mm

Highest mean error: 10.108841896057129 mm for frame 180

Lowest mean error: 1.9029967784881592 mm for frame 0

Saving results

Total time: 369.4957158565521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064064
Iteration 2/25 | Loss: 0.00169351
Iteration 3/25 | Loss: 0.00140594
Iteration 4/25 | Loss: 0.00104836
Iteration 5/25 | Loss: 0.00103222
Iteration 6/25 | Loss: 0.00102732
Iteration 7/25 | Loss: 0.00101915
Iteration 8/25 | Loss: 0.00101819
Iteration 9/25 | Loss: 0.00101668
Iteration 10/25 | Loss: 0.00101293
Iteration 11/25 | Loss: 0.00101247
Iteration 12/25 | Loss: 0.00101224
Iteration 13/25 | Loss: 0.00101137
Iteration 14/25 | Loss: 0.00101058
Iteration 15/25 | Loss: 0.00101041
Iteration 16/25 | Loss: 0.00101037
Iteration 17/25 | Loss: 0.00101037
Iteration 18/25 | Loss: 0.00101037
Iteration 19/25 | Loss: 0.00101037
Iteration 20/25 | Loss: 0.00101037
Iteration 21/25 | Loss: 0.00101037
Iteration 22/25 | Loss: 0.00101037
Iteration 23/25 | Loss: 0.00101037
Iteration 24/25 | Loss: 0.00101036
Iteration 25/25 | Loss: 0.00101036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58111382
Iteration 2/25 | Loss: 0.00119150
Iteration 3/25 | Loss: 0.00111935
Iteration 4/25 | Loss: 0.00111934
Iteration 5/25 | Loss: 0.00111934
Iteration 6/25 | Loss: 0.00111934
Iteration 7/25 | Loss: 0.00111934
Iteration 8/25 | Loss: 0.00111934
Iteration 9/25 | Loss: 0.00111934
Iteration 10/25 | Loss: 0.00111934
Iteration 11/25 | Loss: 0.00111934
Iteration 12/25 | Loss: 0.00111934
Iteration 13/25 | Loss: 0.00111934
Iteration 14/25 | Loss: 0.00111934
Iteration 15/25 | Loss: 0.00111934
Iteration 16/25 | Loss: 0.00111934
Iteration 17/25 | Loss: 0.00111934
Iteration 18/25 | Loss: 0.00111934
Iteration 19/25 | Loss: 0.00111934
Iteration 20/25 | Loss: 0.00111934
Iteration 21/25 | Loss: 0.00111934
Iteration 22/25 | Loss: 0.00111934
Iteration 23/25 | Loss: 0.00111934
Iteration 24/25 | Loss: 0.00111934
Iteration 25/25 | Loss: 0.00111934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111934
Iteration 2/1000 | Loss: 0.00005215
Iteration 3/1000 | Loss: 0.00009538
Iteration 4/1000 | Loss: 0.00005860
Iteration 5/1000 | Loss: 0.00001631
Iteration 6/1000 | Loss: 0.00001371
Iteration 7/1000 | Loss: 0.00004005
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00014911
Iteration 10/1000 | Loss: 0.00001637
Iteration 11/1000 | Loss: 0.00006478
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00004422
Iteration 14/1000 | Loss: 0.00015598
Iteration 15/1000 | Loss: 0.00001344
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001154
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001150
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001149
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001145
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001144
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001129
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001124
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001120
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00008037
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00004066
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00006415
Iteration 121/1000 | Loss: 0.00006415
Iteration 122/1000 | Loss: 0.00034471
Iteration 123/1000 | Loss: 0.00011010
Iteration 124/1000 | Loss: 0.00031143
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00009690
Iteration 127/1000 | Loss: 0.00010020
Iteration 128/1000 | Loss: 0.00008884
Iteration 129/1000 | Loss: 0.00006636
Iteration 130/1000 | Loss: 0.00002289
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001111
Iteration 135/1000 | Loss: 0.00001110
Iteration 136/1000 | Loss: 0.00001110
Iteration 137/1000 | Loss: 0.00001110
Iteration 138/1000 | Loss: 0.00001110
Iteration 139/1000 | Loss: 0.00001110
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001110
Iteration 146/1000 | Loss: 0.00001110
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001108
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001104
Iteration 168/1000 | Loss: 0.00001104
Iteration 169/1000 | Loss: 0.00001104
Iteration 170/1000 | Loss: 0.00001104
Iteration 171/1000 | Loss: 0.00001104
Iteration 172/1000 | Loss: 0.00001104
Iteration 173/1000 | Loss: 0.00010053
Iteration 174/1000 | Loss: 0.00001108
Iteration 175/1000 | Loss: 0.00001107
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001101
Iteration 200/1000 | Loss: 0.00001101
Iteration 201/1000 | Loss: 0.00001101
Iteration 202/1000 | Loss: 0.00001101
Iteration 203/1000 | Loss: 0.00001101
Iteration 204/1000 | Loss: 0.00001101
Iteration 205/1000 | Loss: 0.00001101
Iteration 206/1000 | Loss: 0.00001101
Iteration 207/1000 | Loss: 0.00001101
Iteration 208/1000 | Loss: 0.00001101
Iteration 209/1000 | Loss: 0.00001101
Iteration 210/1000 | Loss: 0.00001101
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Iteration 219/1000 | Loss: 0.00001101
Iteration 220/1000 | Loss: 0.00001101
Iteration 221/1000 | Loss: 0.00001101
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001101
Iteration 225/1000 | Loss: 0.00001101
Iteration 226/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.100598274206277e-05, 1.100598274206277e-05, 1.100598274206277e-05, 1.100598274206277e-05, 1.100598274206277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.100598274206277e-05

Optimization complete. Final v2v error: 2.8645710945129395 mm

Highest mean error: 3.455317258834839 mm for frame 9

Lowest mean error: 2.290346145629883 mm for frame 145

Saving results

Total time: 93.4195020198822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396796
Iteration 2/25 | Loss: 0.00129272
Iteration 3/25 | Loss: 0.00102869
Iteration 4/25 | Loss: 0.00100405
Iteration 5/25 | Loss: 0.00100144
Iteration 6/25 | Loss: 0.00100119
Iteration 7/25 | Loss: 0.00100119
Iteration 8/25 | Loss: 0.00100119
Iteration 9/25 | Loss: 0.00100119
Iteration 10/25 | Loss: 0.00100119
Iteration 11/25 | Loss: 0.00100119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001001186901703477, 0.001001186901703477, 0.001001186901703477, 0.001001186901703477, 0.001001186901703477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001186901703477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36044753
Iteration 2/25 | Loss: 0.00070248
Iteration 3/25 | Loss: 0.00070247
Iteration 4/25 | Loss: 0.00070247
Iteration 5/25 | Loss: 0.00070247
Iteration 6/25 | Loss: 0.00070247
Iteration 7/25 | Loss: 0.00070247
Iteration 8/25 | Loss: 0.00070247
Iteration 9/25 | Loss: 0.00070247
Iteration 10/25 | Loss: 0.00070247
Iteration 11/25 | Loss: 0.00070247
Iteration 12/25 | Loss: 0.00070247
Iteration 13/25 | Loss: 0.00070247
Iteration 14/25 | Loss: 0.00070247
Iteration 15/25 | Loss: 0.00070247
Iteration 16/25 | Loss: 0.00070247
Iteration 17/25 | Loss: 0.00070247
Iteration 18/25 | Loss: 0.00070247
Iteration 19/25 | Loss: 0.00070247
Iteration 20/25 | Loss: 0.00070247
Iteration 21/25 | Loss: 0.00070247
Iteration 22/25 | Loss: 0.00070247
Iteration 23/25 | Loss: 0.00070247
Iteration 24/25 | Loss: 0.00070247
Iteration 25/25 | Loss: 0.00070247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070247
Iteration 2/1000 | Loss: 0.00002510
Iteration 3/1000 | Loss: 0.00001505
Iteration 4/1000 | Loss: 0.00001287
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001125
Iteration 7/1000 | Loss: 0.00001073
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00000993
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000989
Iteration 13/1000 | Loss: 0.00000985
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000977
Iteration 20/1000 | Loss: 0.00000970
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000969
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000968
Iteration 25/1000 | Loss: 0.00000967
Iteration 26/1000 | Loss: 0.00000966
Iteration 27/1000 | Loss: 0.00000966
Iteration 28/1000 | Loss: 0.00000961
Iteration 29/1000 | Loss: 0.00000961
Iteration 30/1000 | Loss: 0.00000958
Iteration 31/1000 | Loss: 0.00000958
Iteration 32/1000 | Loss: 0.00000958
Iteration 33/1000 | Loss: 0.00000958
Iteration 34/1000 | Loss: 0.00000958
Iteration 35/1000 | Loss: 0.00000957
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000954
Iteration 38/1000 | Loss: 0.00000954
Iteration 39/1000 | Loss: 0.00000954
Iteration 40/1000 | Loss: 0.00000954
Iteration 41/1000 | Loss: 0.00000953
Iteration 42/1000 | Loss: 0.00000953
Iteration 43/1000 | Loss: 0.00000953
Iteration 44/1000 | Loss: 0.00000953
Iteration 45/1000 | Loss: 0.00000953
Iteration 46/1000 | Loss: 0.00000953
Iteration 47/1000 | Loss: 0.00000953
Iteration 48/1000 | Loss: 0.00000953
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000953
Iteration 51/1000 | Loss: 0.00000952
Iteration 52/1000 | Loss: 0.00000952
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000952
Iteration 55/1000 | Loss: 0.00000952
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000950
Iteration 59/1000 | Loss: 0.00000950
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000949
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000948
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000947
Iteration 73/1000 | Loss: 0.00000947
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000947
Iteration 76/1000 | Loss: 0.00000947
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000946
Iteration 85/1000 | Loss: 0.00000946
Iteration 86/1000 | Loss: 0.00000946
Iteration 87/1000 | Loss: 0.00000946
Iteration 88/1000 | Loss: 0.00000946
Iteration 89/1000 | Loss: 0.00000946
Iteration 90/1000 | Loss: 0.00000946
Iteration 91/1000 | Loss: 0.00000945
Iteration 92/1000 | Loss: 0.00000945
Iteration 93/1000 | Loss: 0.00000945
Iteration 94/1000 | Loss: 0.00000944
Iteration 95/1000 | Loss: 0.00000944
Iteration 96/1000 | Loss: 0.00000944
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000943
Iteration 100/1000 | Loss: 0.00000943
Iteration 101/1000 | Loss: 0.00000943
Iteration 102/1000 | Loss: 0.00000943
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000942
Iteration 109/1000 | Loss: 0.00000942
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000939
Iteration 117/1000 | Loss: 0.00000939
Iteration 118/1000 | Loss: 0.00000939
Iteration 119/1000 | Loss: 0.00000938
Iteration 120/1000 | Loss: 0.00000938
Iteration 121/1000 | Loss: 0.00000938
Iteration 122/1000 | Loss: 0.00000938
Iteration 123/1000 | Loss: 0.00000938
Iteration 124/1000 | Loss: 0.00000938
Iteration 125/1000 | Loss: 0.00000938
Iteration 126/1000 | Loss: 0.00000938
Iteration 127/1000 | Loss: 0.00000938
Iteration 128/1000 | Loss: 0.00000937
Iteration 129/1000 | Loss: 0.00000937
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000936
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000935
Iteration 137/1000 | Loss: 0.00000935
Iteration 138/1000 | Loss: 0.00000935
Iteration 139/1000 | Loss: 0.00000935
Iteration 140/1000 | Loss: 0.00000935
Iteration 141/1000 | Loss: 0.00000935
Iteration 142/1000 | Loss: 0.00000934
Iteration 143/1000 | Loss: 0.00000934
Iteration 144/1000 | Loss: 0.00000934
Iteration 145/1000 | Loss: 0.00000934
Iteration 146/1000 | Loss: 0.00000933
Iteration 147/1000 | Loss: 0.00000933
Iteration 148/1000 | Loss: 0.00000933
Iteration 149/1000 | Loss: 0.00000933
Iteration 150/1000 | Loss: 0.00000933
Iteration 151/1000 | Loss: 0.00000932
Iteration 152/1000 | Loss: 0.00000932
Iteration 153/1000 | Loss: 0.00000931
Iteration 154/1000 | Loss: 0.00000931
Iteration 155/1000 | Loss: 0.00000931
Iteration 156/1000 | Loss: 0.00000930
Iteration 157/1000 | Loss: 0.00000930
Iteration 158/1000 | Loss: 0.00000929
Iteration 159/1000 | Loss: 0.00000929
Iteration 160/1000 | Loss: 0.00000928
Iteration 161/1000 | Loss: 0.00000928
Iteration 162/1000 | Loss: 0.00000928
Iteration 163/1000 | Loss: 0.00000928
Iteration 164/1000 | Loss: 0.00000928
Iteration 165/1000 | Loss: 0.00000928
Iteration 166/1000 | Loss: 0.00000928
Iteration 167/1000 | Loss: 0.00000928
Iteration 168/1000 | Loss: 0.00000928
Iteration 169/1000 | Loss: 0.00000927
Iteration 170/1000 | Loss: 0.00000927
Iteration 171/1000 | Loss: 0.00000927
Iteration 172/1000 | Loss: 0.00000927
Iteration 173/1000 | Loss: 0.00000927
Iteration 174/1000 | Loss: 0.00000927
Iteration 175/1000 | Loss: 0.00000927
Iteration 176/1000 | Loss: 0.00000926
Iteration 177/1000 | Loss: 0.00000926
Iteration 178/1000 | Loss: 0.00000926
Iteration 179/1000 | Loss: 0.00000925
Iteration 180/1000 | Loss: 0.00000925
Iteration 181/1000 | Loss: 0.00000925
Iteration 182/1000 | Loss: 0.00000925
Iteration 183/1000 | Loss: 0.00000925
Iteration 184/1000 | Loss: 0.00000925
Iteration 185/1000 | Loss: 0.00000925
Iteration 186/1000 | Loss: 0.00000925
Iteration 187/1000 | Loss: 0.00000924
Iteration 188/1000 | Loss: 0.00000924
Iteration 189/1000 | Loss: 0.00000924
Iteration 190/1000 | Loss: 0.00000924
Iteration 191/1000 | Loss: 0.00000924
Iteration 192/1000 | Loss: 0.00000924
Iteration 193/1000 | Loss: 0.00000924
Iteration 194/1000 | Loss: 0.00000923
Iteration 195/1000 | Loss: 0.00000923
Iteration 196/1000 | Loss: 0.00000923
Iteration 197/1000 | Loss: 0.00000923
Iteration 198/1000 | Loss: 0.00000923
Iteration 199/1000 | Loss: 0.00000923
Iteration 200/1000 | Loss: 0.00000923
Iteration 201/1000 | Loss: 0.00000923
Iteration 202/1000 | Loss: 0.00000923
Iteration 203/1000 | Loss: 0.00000923
Iteration 204/1000 | Loss: 0.00000923
Iteration 205/1000 | Loss: 0.00000923
Iteration 206/1000 | Loss: 0.00000923
Iteration 207/1000 | Loss: 0.00000923
Iteration 208/1000 | Loss: 0.00000923
Iteration 209/1000 | Loss: 0.00000923
Iteration 210/1000 | Loss: 0.00000923
Iteration 211/1000 | Loss: 0.00000923
Iteration 212/1000 | Loss: 0.00000922
Iteration 213/1000 | Loss: 0.00000922
Iteration 214/1000 | Loss: 0.00000922
Iteration 215/1000 | Loss: 0.00000922
Iteration 216/1000 | Loss: 0.00000922
Iteration 217/1000 | Loss: 0.00000922
Iteration 218/1000 | Loss: 0.00000922
Iteration 219/1000 | Loss: 0.00000922
Iteration 220/1000 | Loss: 0.00000922
Iteration 221/1000 | Loss: 0.00000922
Iteration 222/1000 | Loss: 0.00000922
Iteration 223/1000 | Loss: 0.00000922
Iteration 224/1000 | Loss: 0.00000922
Iteration 225/1000 | Loss: 0.00000922
Iteration 226/1000 | Loss: 0.00000922
Iteration 227/1000 | Loss: 0.00000921
Iteration 228/1000 | Loss: 0.00000921
Iteration 229/1000 | Loss: 0.00000921
Iteration 230/1000 | Loss: 0.00000921
Iteration 231/1000 | Loss: 0.00000921
Iteration 232/1000 | Loss: 0.00000921
Iteration 233/1000 | Loss: 0.00000921
Iteration 234/1000 | Loss: 0.00000921
Iteration 235/1000 | Loss: 0.00000921
Iteration 236/1000 | Loss: 0.00000921
Iteration 237/1000 | Loss: 0.00000921
Iteration 238/1000 | Loss: 0.00000920
Iteration 239/1000 | Loss: 0.00000920
Iteration 240/1000 | Loss: 0.00000920
Iteration 241/1000 | Loss: 0.00000920
Iteration 242/1000 | Loss: 0.00000920
Iteration 243/1000 | Loss: 0.00000920
Iteration 244/1000 | Loss: 0.00000920
Iteration 245/1000 | Loss: 0.00000920
Iteration 246/1000 | Loss: 0.00000920
Iteration 247/1000 | Loss: 0.00000920
Iteration 248/1000 | Loss: 0.00000920
Iteration 249/1000 | Loss: 0.00000920
Iteration 250/1000 | Loss: 0.00000920
Iteration 251/1000 | Loss: 0.00000920
Iteration 252/1000 | Loss: 0.00000920
Iteration 253/1000 | Loss: 0.00000919
Iteration 254/1000 | Loss: 0.00000919
Iteration 255/1000 | Loss: 0.00000919
Iteration 256/1000 | Loss: 0.00000919
Iteration 257/1000 | Loss: 0.00000919
Iteration 258/1000 | Loss: 0.00000919
Iteration 259/1000 | Loss: 0.00000919
Iteration 260/1000 | Loss: 0.00000919
Iteration 261/1000 | Loss: 0.00000919
Iteration 262/1000 | Loss: 0.00000919
Iteration 263/1000 | Loss: 0.00000919
Iteration 264/1000 | Loss: 0.00000919
Iteration 265/1000 | Loss: 0.00000919
Iteration 266/1000 | Loss: 0.00000919
Iteration 267/1000 | Loss: 0.00000919
Iteration 268/1000 | Loss: 0.00000918
Iteration 269/1000 | Loss: 0.00000918
Iteration 270/1000 | Loss: 0.00000918
Iteration 271/1000 | Loss: 0.00000918
Iteration 272/1000 | Loss: 0.00000918
Iteration 273/1000 | Loss: 0.00000918
Iteration 274/1000 | Loss: 0.00000918
Iteration 275/1000 | Loss: 0.00000918
Iteration 276/1000 | Loss: 0.00000918
Iteration 277/1000 | Loss: 0.00000917
Iteration 278/1000 | Loss: 0.00000917
Iteration 279/1000 | Loss: 0.00000917
Iteration 280/1000 | Loss: 0.00000917
Iteration 281/1000 | Loss: 0.00000917
Iteration 282/1000 | Loss: 0.00000917
Iteration 283/1000 | Loss: 0.00000917
Iteration 284/1000 | Loss: 0.00000917
Iteration 285/1000 | Loss: 0.00000917
Iteration 286/1000 | Loss: 0.00000917
Iteration 287/1000 | Loss: 0.00000917
Iteration 288/1000 | Loss: 0.00000917
Iteration 289/1000 | Loss: 0.00000917
Iteration 290/1000 | Loss: 0.00000917
Iteration 291/1000 | Loss: 0.00000917
Iteration 292/1000 | Loss: 0.00000917
Iteration 293/1000 | Loss: 0.00000917
Iteration 294/1000 | Loss: 0.00000917
Iteration 295/1000 | Loss: 0.00000917
Iteration 296/1000 | Loss: 0.00000917
Iteration 297/1000 | Loss: 0.00000917
Iteration 298/1000 | Loss: 0.00000917
Iteration 299/1000 | Loss: 0.00000917
Iteration 300/1000 | Loss: 0.00000917
Iteration 301/1000 | Loss: 0.00000917
Iteration 302/1000 | Loss: 0.00000917
Iteration 303/1000 | Loss: 0.00000917
Iteration 304/1000 | Loss: 0.00000917
Iteration 305/1000 | Loss: 0.00000917
Iteration 306/1000 | Loss: 0.00000917
Iteration 307/1000 | Loss: 0.00000917
Iteration 308/1000 | Loss: 0.00000917
Iteration 309/1000 | Loss: 0.00000917
Iteration 310/1000 | Loss: 0.00000917
Iteration 311/1000 | Loss: 0.00000917
Iteration 312/1000 | Loss: 0.00000917
Iteration 313/1000 | Loss: 0.00000917
Iteration 314/1000 | Loss: 0.00000917
Iteration 315/1000 | Loss: 0.00000917
Iteration 316/1000 | Loss: 0.00000917
Iteration 317/1000 | Loss: 0.00000917
Iteration 318/1000 | Loss: 0.00000917
Iteration 319/1000 | Loss: 0.00000917
Iteration 320/1000 | Loss: 0.00000917
Iteration 321/1000 | Loss: 0.00000917
Iteration 322/1000 | Loss: 0.00000917
Iteration 323/1000 | Loss: 0.00000917
Iteration 324/1000 | Loss: 0.00000917
Iteration 325/1000 | Loss: 0.00000917
Iteration 326/1000 | Loss: 0.00000917
Iteration 327/1000 | Loss: 0.00000917
Iteration 328/1000 | Loss: 0.00000917
Iteration 329/1000 | Loss: 0.00000917
Iteration 330/1000 | Loss: 0.00000917
Iteration 331/1000 | Loss: 0.00000917
Iteration 332/1000 | Loss: 0.00000917
Iteration 333/1000 | Loss: 0.00000917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [9.171878446068149e-06, 9.171878446068149e-06, 9.171878446068149e-06, 9.171878446068149e-06, 9.171878446068149e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.171878446068149e-06

Optimization complete. Final v2v error: 2.650930404663086 mm

Highest mean error: 2.844186782836914 mm for frame 32

Lowest mean error: 2.266329288482666 mm for frame 0

Saving results

Total time: 44.34435224533081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986424
Iteration 2/25 | Loss: 0.00238342
Iteration 3/25 | Loss: 0.00180500
Iteration 4/25 | Loss: 0.00174109
Iteration 5/25 | Loss: 0.00156398
Iteration 6/25 | Loss: 0.00150186
Iteration 7/25 | Loss: 0.00134819
Iteration 8/25 | Loss: 0.00127826
Iteration 9/25 | Loss: 0.00127742
Iteration 10/25 | Loss: 0.00123763
Iteration 11/25 | Loss: 0.00118660
Iteration 12/25 | Loss: 0.00115246
Iteration 13/25 | Loss: 0.00113032
Iteration 14/25 | Loss: 0.00112926
Iteration 15/25 | Loss: 0.00112372
Iteration 16/25 | Loss: 0.00111744
Iteration 17/25 | Loss: 0.00111185
Iteration 18/25 | Loss: 0.00110714
Iteration 19/25 | Loss: 0.00110064
Iteration 20/25 | Loss: 0.00110354
Iteration 21/25 | Loss: 0.00110110
Iteration 22/25 | Loss: 0.00109868
Iteration 23/25 | Loss: 0.00109769
Iteration 24/25 | Loss: 0.00109742
Iteration 25/25 | Loss: 0.00109733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34788454
Iteration 2/25 | Loss: 0.00126326
Iteration 3/25 | Loss: 0.00126324
Iteration 4/25 | Loss: 0.00126324
Iteration 5/25 | Loss: 0.00126324
Iteration 6/25 | Loss: 0.00126324
Iteration 7/25 | Loss: 0.00126324
Iteration 8/25 | Loss: 0.00126323
Iteration 9/25 | Loss: 0.00126323
Iteration 10/25 | Loss: 0.00126323
Iteration 11/25 | Loss: 0.00126323
Iteration 12/25 | Loss: 0.00126323
Iteration 13/25 | Loss: 0.00126323
Iteration 14/25 | Loss: 0.00126323
Iteration 15/25 | Loss: 0.00126323
Iteration 16/25 | Loss: 0.00126323
Iteration 17/25 | Loss: 0.00126323
Iteration 18/25 | Loss: 0.00126323
Iteration 19/25 | Loss: 0.00126323
Iteration 20/25 | Loss: 0.00126323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012632336001843214, 0.0012632336001843214, 0.0012632336001843214, 0.0012632336001843214, 0.0012632336001843214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012632336001843214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126323
Iteration 2/1000 | Loss: 0.00107216
Iteration 3/1000 | Loss: 0.00015648
Iteration 4/1000 | Loss: 0.00039733
Iteration 5/1000 | Loss: 0.00021937
Iteration 6/1000 | Loss: 0.00007916
Iteration 7/1000 | Loss: 0.00022400
Iteration 8/1000 | Loss: 0.00037209
Iteration 9/1000 | Loss: 0.00006431
Iteration 10/1000 | Loss: 0.00015194
Iteration 11/1000 | Loss: 0.00004702
Iteration 12/1000 | Loss: 0.00022708
Iteration 13/1000 | Loss: 0.00014099
Iteration 14/1000 | Loss: 0.00006439
Iteration 15/1000 | Loss: 0.00004231
Iteration 16/1000 | Loss: 0.00003419
Iteration 17/1000 | Loss: 0.00002900
Iteration 18/1000 | Loss: 0.00003827
Iteration 19/1000 | Loss: 0.00002431
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00001983
Iteration 22/1000 | Loss: 0.00015264
Iteration 23/1000 | Loss: 0.00002259
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001696
Iteration 27/1000 | Loss: 0.00015512
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00015915
Iteration 31/1000 | Loss: 0.00002090
Iteration 32/1000 | Loss: 0.00001840
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001501
Iteration 35/1000 | Loss: 0.00001467
Iteration 36/1000 | Loss: 0.00001438
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001365
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001363
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001354
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001342
Iteration 82/1000 | Loss: 0.00001341
Iteration 83/1000 | Loss: 0.00001341
Iteration 84/1000 | Loss: 0.00001341
Iteration 85/1000 | Loss: 0.00001341
Iteration 86/1000 | Loss: 0.00001341
Iteration 87/1000 | Loss: 0.00001341
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001341
Iteration 90/1000 | Loss: 0.00001341
Iteration 91/1000 | Loss: 0.00001341
Iteration 92/1000 | Loss: 0.00001341
Iteration 93/1000 | Loss: 0.00001341
Iteration 94/1000 | Loss: 0.00001341
Iteration 95/1000 | Loss: 0.00001340
Iteration 96/1000 | Loss: 0.00001340
Iteration 97/1000 | Loss: 0.00001340
Iteration 98/1000 | Loss: 0.00001340
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001339
Iteration 101/1000 | Loss: 0.00001339
Iteration 102/1000 | Loss: 0.00001339
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001339
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001339
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001339
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001337
Iteration 118/1000 | Loss: 0.00001337
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001336
Iteration 129/1000 | Loss: 0.00001336
Iteration 130/1000 | Loss: 0.00001336
Iteration 131/1000 | Loss: 0.00001336
Iteration 132/1000 | Loss: 0.00001336
Iteration 133/1000 | Loss: 0.00001336
Iteration 134/1000 | Loss: 0.00001336
Iteration 135/1000 | Loss: 0.00001336
Iteration 136/1000 | Loss: 0.00001336
Iteration 137/1000 | Loss: 0.00001336
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001335
Iteration 142/1000 | Loss: 0.00001335
Iteration 143/1000 | Loss: 0.00001335
Iteration 144/1000 | Loss: 0.00001335
Iteration 145/1000 | Loss: 0.00001335
Iteration 146/1000 | Loss: 0.00001335
Iteration 147/1000 | Loss: 0.00001335
Iteration 148/1000 | Loss: 0.00001335
Iteration 149/1000 | Loss: 0.00001335
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001335
Iteration 152/1000 | Loss: 0.00001335
Iteration 153/1000 | Loss: 0.00001334
Iteration 154/1000 | Loss: 0.00001334
Iteration 155/1000 | Loss: 0.00001334
Iteration 156/1000 | Loss: 0.00001334
Iteration 157/1000 | Loss: 0.00001334
Iteration 158/1000 | Loss: 0.00001334
Iteration 159/1000 | Loss: 0.00001334
Iteration 160/1000 | Loss: 0.00001334
Iteration 161/1000 | Loss: 0.00001334
Iteration 162/1000 | Loss: 0.00001334
Iteration 163/1000 | Loss: 0.00001334
Iteration 164/1000 | Loss: 0.00001334
Iteration 165/1000 | Loss: 0.00001334
Iteration 166/1000 | Loss: 0.00001334
Iteration 167/1000 | Loss: 0.00001334
Iteration 168/1000 | Loss: 0.00001334
Iteration 169/1000 | Loss: 0.00001334
Iteration 170/1000 | Loss: 0.00001334
Iteration 171/1000 | Loss: 0.00001333
Iteration 172/1000 | Loss: 0.00001333
Iteration 173/1000 | Loss: 0.00001333
Iteration 174/1000 | Loss: 0.00001333
Iteration 175/1000 | Loss: 0.00001333
Iteration 176/1000 | Loss: 0.00001333
Iteration 177/1000 | Loss: 0.00001333
Iteration 178/1000 | Loss: 0.00001333
Iteration 179/1000 | Loss: 0.00001333
Iteration 180/1000 | Loss: 0.00001333
Iteration 181/1000 | Loss: 0.00001333
Iteration 182/1000 | Loss: 0.00001333
Iteration 183/1000 | Loss: 0.00001333
Iteration 184/1000 | Loss: 0.00001333
Iteration 185/1000 | Loss: 0.00001333
Iteration 186/1000 | Loss: 0.00001333
Iteration 187/1000 | Loss: 0.00001333
Iteration 188/1000 | Loss: 0.00001333
Iteration 189/1000 | Loss: 0.00001333
Iteration 190/1000 | Loss: 0.00001333
Iteration 191/1000 | Loss: 0.00001333
Iteration 192/1000 | Loss: 0.00001333
Iteration 193/1000 | Loss: 0.00001333
Iteration 194/1000 | Loss: 0.00001333
Iteration 195/1000 | Loss: 0.00001333
Iteration 196/1000 | Loss: 0.00001333
Iteration 197/1000 | Loss: 0.00001333
Iteration 198/1000 | Loss: 0.00001333
Iteration 199/1000 | Loss: 0.00001333
Iteration 200/1000 | Loss: 0.00001333
Iteration 201/1000 | Loss: 0.00001333
Iteration 202/1000 | Loss: 0.00001333
Iteration 203/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.3334379218576942e-05, 1.3334379218576942e-05, 1.3334379218576942e-05, 1.3334379218576942e-05, 1.3334379218576942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3334379218576942e-05

Optimization complete. Final v2v error: 2.84325909614563 mm

Highest mean error: 10.509637832641602 mm for frame 74

Lowest mean error: 2.4339241981506348 mm for frame 177

Saving results

Total time: 127.88862824440002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802882
Iteration 2/25 | Loss: 0.00128451
Iteration 3/25 | Loss: 0.00102028
Iteration 4/25 | Loss: 0.00100413
Iteration 5/25 | Loss: 0.00099902
Iteration 6/25 | Loss: 0.00099675
Iteration 7/25 | Loss: 0.00099658
Iteration 8/25 | Loss: 0.00099645
Iteration 9/25 | Loss: 0.00099645
Iteration 10/25 | Loss: 0.00099645
Iteration 11/25 | Loss: 0.00099645
Iteration 12/25 | Loss: 0.00099645
Iteration 13/25 | Loss: 0.00099645
Iteration 14/25 | Loss: 0.00099645
Iteration 15/25 | Loss: 0.00099645
Iteration 16/25 | Loss: 0.00099645
Iteration 17/25 | Loss: 0.00099645
Iteration 18/25 | Loss: 0.00099645
Iteration 19/25 | Loss: 0.00099645
Iteration 20/25 | Loss: 0.00099645
Iteration 21/25 | Loss: 0.00099645
Iteration 22/25 | Loss: 0.00099645
Iteration 23/25 | Loss: 0.00099645
Iteration 24/25 | Loss: 0.00099645
Iteration 25/25 | Loss: 0.00099645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16966963
Iteration 2/25 | Loss: 0.00117549
Iteration 3/25 | Loss: 0.00117549
Iteration 4/25 | Loss: 0.00117549
Iteration 5/25 | Loss: 0.00117549
Iteration 6/25 | Loss: 0.00117549
Iteration 7/25 | Loss: 0.00117549
Iteration 8/25 | Loss: 0.00117548
Iteration 9/25 | Loss: 0.00117548
Iteration 10/25 | Loss: 0.00117548
Iteration 11/25 | Loss: 0.00117548
Iteration 12/25 | Loss: 0.00117548
Iteration 13/25 | Loss: 0.00117548
Iteration 14/25 | Loss: 0.00117548
Iteration 15/25 | Loss: 0.00117548
Iteration 16/25 | Loss: 0.00117548
Iteration 17/25 | Loss: 0.00117548
Iteration 18/25 | Loss: 0.00117548
Iteration 19/25 | Loss: 0.00117548
Iteration 20/25 | Loss: 0.00117548
Iteration 21/25 | Loss: 0.00117548
Iteration 22/25 | Loss: 0.00117548
Iteration 23/25 | Loss: 0.00117548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001175484387204051, 0.001175484387204051, 0.001175484387204051, 0.001175484387204051, 0.001175484387204051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175484387204051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117548
Iteration 2/1000 | Loss: 0.00006705
Iteration 3/1000 | Loss: 0.00003301
Iteration 4/1000 | Loss: 0.00002485
Iteration 5/1000 | Loss: 0.00002162
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00001861
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001599
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001566
Iteration 15/1000 | Loss: 0.00001553
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001520
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001514
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001498
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001490
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001489
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001488
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001487
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001486
Iteration 52/1000 | Loss: 0.00001486
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001485
Iteration 55/1000 | Loss: 0.00001485
Iteration 56/1000 | Loss: 0.00001485
Iteration 57/1000 | Loss: 0.00001485
Iteration 58/1000 | Loss: 0.00001484
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001484
Iteration 62/1000 | Loss: 0.00001484
Iteration 63/1000 | Loss: 0.00001483
Iteration 64/1000 | Loss: 0.00001483
Iteration 65/1000 | Loss: 0.00001483
Iteration 66/1000 | Loss: 0.00001483
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001483
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001482
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001480
Iteration 83/1000 | Loss: 0.00001480
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001476
Iteration 103/1000 | Loss: 0.00001476
Iteration 104/1000 | Loss: 0.00001476
Iteration 105/1000 | Loss: 0.00001476
Iteration 106/1000 | Loss: 0.00001476
Iteration 107/1000 | Loss: 0.00001476
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001475
Iteration 112/1000 | Loss: 0.00001475
Iteration 113/1000 | Loss: 0.00001475
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Iteration 118/1000 | Loss: 0.00001474
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001472
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001472
Iteration 136/1000 | Loss: 0.00001472
Iteration 137/1000 | Loss: 0.00001472
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001470
Iteration 149/1000 | Loss: 0.00001470
Iteration 150/1000 | Loss: 0.00001470
Iteration 151/1000 | Loss: 0.00001470
Iteration 152/1000 | Loss: 0.00001470
Iteration 153/1000 | Loss: 0.00001470
Iteration 154/1000 | Loss: 0.00001470
Iteration 155/1000 | Loss: 0.00001470
Iteration 156/1000 | Loss: 0.00001470
Iteration 157/1000 | Loss: 0.00001470
Iteration 158/1000 | Loss: 0.00001469
Iteration 159/1000 | Loss: 0.00001469
Iteration 160/1000 | Loss: 0.00001469
Iteration 161/1000 | Loss: 0.00001469
Iteration 162/1000 | Loss: 0.00001469
Iteration 163/1000 | Loss: 0.00001469
Iteration 164/1000 | Loss: 0.00001468
Iteration 165/1000 | Loss: 0.00001468
Iteration 166/1000 | Loss: 0.00001468
Iteration 167/1000 | Loss: 0.00001468
Iteration 168/1000 | Loss: 0.00001468
Iteration 169/1000 | Loss: 0.00001468
Iteration 170/1000 | Loss: 0.00001468
Iteration 171/1000 | Loss: 0.00001468
Iteration 172/1000 | Loss: 0.00001468
Iteration 173/1000 | Loss: 0.00001468
Iteration 174/1000 | Loss: 0.00001468
Iteration 175/1000 | Loss: 0.00001468
Iteration 176/1000 | Loss: 0.00001468
Iteration 177/1000 | Loss: 0.00001468
Iteration 178/1000 | Loss: 0.00001468
Iteration 179/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.4678300431114621e-05, 1.4678300431114621e-05, 1.4678300431114621e-05, 1.4678300431114621e-05, 1.4678300431114621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4678300431114621e-05

Optimization complete. Final v2v error: 3.090376138687134 mm

Highest mean error: 4.654098033905029 mm for frame 65

Lowest mean error: 2.4177680015563965 mm for frame 25

Saving results

Total time: 48.90172362327576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906674
Iteration 2/25 | Loss: 0.00143641
Iteration 3/25 | Loss: 0.00118222
Iteration 4/25 | Loss: 0.00116329
Iteration 5/25 | Loss: 0.00116203
Iteration 6/25 | Loss: 0.00116203
Iteration 7/25 | Loss: 0.00116203
Iteration 8/25 | Loss: 0.00116203
Iteration 9/25 | Loss: 0.00116190
Iteration 10/25 | Loss: 0.00116190
Iteration 11/25 | Loss: 0.00116190
Iteration 12/25 | Loss: 0.00116190
Iteration 13/25 | Loss: 0.00116190
Iteration 14/25 | Loss: 0.00116190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011619048891589046, 0.0011619048891589046, 0.0011619048891589046, 0.0011619048891589046, 0.0011619048891589046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011619048891589046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28109097
Iteration 2/25 | Loss: 0.00063435
Iteration 3/25 | Loss: 0.00063427
Iteration 4/25 | Loss: 0.00063427
Iteration 5/25 | Loss: 0.00063427
Iteration 6/25 | Loss: 0.00063427
Iteration 7/25 | Loss: 0.00063427
Iteration 8/25 | Loss: 0.00063427
Iteration 9/25 | Loss: 0.00063427
Iteration 10/25 | Loss: 0.00063427
Iteration 11/25 | Loss: 0.00063427
Iteration 12/25 | Loss: 0.00063427
Iteration 13/25 | Loss: 0.00063427
Iteration 14/25 | Loss: 0.00063427
Iteration 15/25 | Loss: 0.00063427
Iteration 16/25 | Loss: 0.00063427
Iteration 17/25 | Loss: 0.00063427
Iteration 18/25 | Loss: 0.00063427
Iteration 19/25 | Loss: 0.00063427
Iteration 20/25 | Loss: 0.00063427
Iteration 21/25 | Loss: 0.00063427
Iteration 22/25 | Loss: 0.00063427
Iteration 23/25 | Loss: 0.00063427
Iteration 24/25 | Loss: 0.00063427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006342705455608666, 0.0006342705455608666, 0.0006342705455608666, 0.0006342705455608666, 0.0006342705455608666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006342705455608666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063427
Iteration 2/1000 | Loss: 0.00003786
Iteration 3/1000 | Loss: 0.00002388
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00002039
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001931
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001860
Iteration 10/1000 | Loss: 0.00001829
Iteration 11/1000 | Loss: 0.00001811
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001800
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001784
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001767
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001765
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001765
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001755
Iteration 39/1000 | Loss: 0.00001755
Iteration 40/1000 | Loss: 0.00001755
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001752
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001752
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001751
Iteration 55/1000 | Loss: 0.00001751
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001750
Iteration 61/1000 | Loss: 0.00001750
Iteration 62/1000 | Loss: 0.00001750
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001749
Iteration 69/1000 | Loss: 0.00001749
Iteration 70/1000 | Loss: 0.00001749
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001748
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001745
Iteration 90/1000 | Loss: 0.00001745
Iteration 91/1000 | Loss: 0.00001745
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001744
Iteration 96/1000 | Loss: 0.00001744
Iteration 97/1000 | Loss: 0.00001744
Iteration 98/1000 | Loss: 0.00001744
Iteration 99/1000 | Loss: 0.00001744
Iteration 100/1000 | Loss: 0.00001743
Iteration 101/1000 | Loss: 0.00001743
Iteration 102/1000 | Loss: 0.00001743
Iteration 103/1000 | Loss: 0.00001743
Iteration 104/1000 | Loss: 0.00001743
Iteration 105/1000 | Loss: 0.00001743
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001741
Iteration 125/1000 | Loss: 0.00001741
Iteration 126/1000 | Loss: 0.00001741
Iteration 127/1000 | Loss: 0.00001741
Iteration 128/1000 | Loss: 0.00001741
Iteration 129/1000 | Loss: 0.00001741
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001740
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001738
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001738
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001737
Iteration 157/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.7373891751049086e-05, 1.7373891751049086e-05, 1.7373891751049086e-05, 1.7373891751049086e-05, 1.7373891751049086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7373891751049086e-05

Optimization complete. Final v2v error: 3.4796063899993896 mm

Highest mean error: 3.6046884059906006 mm for frame 2

Lowest mean error: 3.1483354568481445 mm for frame 237

Saving results

Total time: 43.10057759284973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1213/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1213/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410271
Iteration 2/25 | Loss: 0.00103761
Iteration 3/25 | Loss: 0.00097732
Iteration 4/25 | Loss: 0.00096392
Iteration 5/25 | Loss: 0.00095993
Iteration 6/25 | Loss: 0.00095887
Iteration 7/25 | Loss: 0.00095887
Iteration 8/25 | Loss: 0.00095887
Iteration 9/25 | Loss: 0.00095887
Iteration 10/25 | Loss: 0.00095887
Iteration 11/25 | Loss: 0.00095887
Iteration 12/25 | Loss: 0.00095887
Iteration 13/25 | Loss: 0.00095887
Iteration 14/25 | Loss: 0.00095887
Iteration 15/25 | Loss: 0.00095887
Iteration 16/25 | Loss: 0.00095887
Iteration 17/25 | Loss: 0.00095887
Iteration 18/25 | Loss: 0.00095887
Iteration 19/25 | Loss: 0.00095887
Iteration 20/25 | Loss: 0.00095887
Iteration 21/25 | Loss: 0.00095887
Iteration 22/25 | Loss: 0.00095887
Iteration 23/25 | Loss: 0.00095887
Iteration 24/25 | Loss: 0.00095887
Iteration 25/25 | Loss: 0.00095887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.56094074
Iteration 2/25 | Loss: 0.00109617
Iteration 3/25 | Loss: 0.00109615
Iteration 4/25 | Loss: 0.00109615
Iteration 5/25 | Loss: 0.00109615
Iteration 6/25 | Loss: 0.00109615
Iteration 7/25 | Loss: 0.00109615
Iteration 8/25 | Loss: 0.00109615
Iteration 9/25 | Loss: 0.00109615
Iteration 10/25 | Loss: 0.00109615
Iteration 11/25 | Loss: 0.00109615
Iteration 12/25 | Loss: 0.00109615
Iteration 13/25 | Loss: 0.00109615
Iteration 14/25 | Loss: 0.00109615
Iteration 15/25 | Loss: 0.00109615
Iteration 16/25 | Loss: 0.00109615
Iteration 17/25 | Loss: 0.00109615
Iteration 18/25 | Loss: 0.00109615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010961496736854315, 0.0010961496736854315, 0.0010961496736854315, 0.0010961496736854315, 0.0010961496736854315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010961496736854315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109615
Iteration 2/1000 | Loss: 0.00001802
Iteration 3/1000 | Loss: 0.00001119
Iteration 4/1000 | Loss: 0.00000985
Iteration 5/1000 | Loss: 0.00000937
Iteration 6/1000 | Loss: 0.00000925
Iteration 7/1000 | Loss: 0.00000897
Iteration 8/1000 | Loss: 0.00000886
Iteration 9/1000 | Loss: 0.00000885
Iteration 10/1000 | Loss: 0.00000882
Iteration 11/1000 | Loss: 0.00000867
Iteration 12/1000 | Loss: 0.00000867
Iteration 13/1000 | Loss: 0.00000867
Iteration 14/1000 | Loss: 0.00000866
Iteration 15/1000 | Loss: 0.00000865
Iteration 16/1000 | Loss: 0.00000857
Iteration 17/1000 | Loss: 0.00000857
Iteration 18/1000 | Loss: 0.00000856
Iteration 19/1000 | Loss: 0.00000854
Iteration 20/1000 | Loss: 0.00000853
Iteration 21/1000 | Loss: 0.00000853
Iteration 22/1000 | Loss: 0.00000852
Iteration 23/1000 | Loss: 0.00000852
Iteration 24/1000 | Loss: 0.00000851
Iteration 25/1000 | Loss: 0.00000851
Iteration 26/1000 | Loss: 0.00000851
Iteration 27/1000 | Loss: 0.00000851
Iteration 28/1000 | Loss: 0.00000850
Iteration 29/1000 | Loss: 0.00000850
Iteration 30/1000 | Loss: 0.00000849
Iteration 31/1000 | Loss: 0.00000849
Iteration 32/1000 | Loss: 0.00000849
Iteration 33/1000 | Loss: 0.00000849
Iteration 34/1000 | Loss: 0.00000849
Iteration 35/1000 | Loss: 0.00000848
Iteration 36/1000 | Loss: 0.00000848
Iteration 37/1000 | Loss: 0.00000848
Iteration 38/1000 | Loss: 0.00000848
Iteration 39/1000 | Loss: 0.00000848
Iteration 40/1000 | Loss: 0.00000848
Iteration 41/1000 | Loss: 0.00000848
Iteration 42/1000 | Loss: 0.00000848
Iteration 43/1000 | Loss: 0.00000848
Iteration 44/1000 | Loss: 0.00000848
Iteration 45/1000 | Loss: 0.00000847
Iteration 46/1000 | Loss: 0.00000847
Iteration 47/1000 | Loss: 0.00000847
Iteration 48/1000 | Loss: 0.00000847
Iteration 49/1000 | Loss: 0.00000846
Iteration 50/1000 | Loss: 0.00000846
Iteration 51/1000 | Loss: 0.00000846
Iteration 52/1000 | Loss: 0.00000845
Iteration 53/1000 | Loss: 0.00000845
Iteration 54/1000 | Loss: 0.00000845
Iteration 55/1000 | Loss: 0.00000845
Iteration 56/1000 | Loss: 0.00000844
Iteration 57/1000 | Loss: 0.00000844
Iteration 58/1000 | Loss: 0.00000844
Iteration 59/1000 | Loss: 0.00000844
Iteration 60/1000 | Loss: 0.00000843
Iteration 61/1000 | Loss: 0.00000843
Iteration 62/1000 | Loss: 0.00000843
Iteration 63/1000 | Loss: 0.00000843
Iteration 64/1000 | Loss: 0.00000843
Iteration 65/1000 | Loss: 0.00000842
Iteration 66/1000 | Loss: 0.00000842
Iteration 67/1000 | Loss: 0.00000842
Iteration 68/1000 | Loss: 0.00000841
Iteration 69/1000 | Loss: 0.00000841
Iteration 70/1000 | Loss: 0.00000841
Iteration 71/1000 | Loss: 0.00000841
Iteration 72/1000 | Loss: 0.00000841
Iteration 73/1000 | Loss: 0.00000841
Iteration 74/1000 | Loss: 0.00000841
Iteration 75/1000 | Loss: 0.00000841
Iteration 76/1000 | Loss: 0.00000840
Iteration 77/1000 | Loss: 0.00000840
Iteration 78/1000 | Loss: 0.00000840
Iteration 79/1000 | Loss: 0.00000840
Iteration 80/1000 | Loss: 0.00000840
Iteration 81/1000 | Loss: 0.00000840
Iteration 82/1000 | Loss: 0.00000840
Iteration 83/1000 | Loss: 0.00000840
Iteration 84/1000 | Loss: 0.00000839
Iteration 85/1000 | Loss: 0.00000839
Iteration 86/1000 | Loss: 0.00000839
Iteration 87/1000 | Loss: 0.00000839
Iteration 88/1000 | Loss: 0.00000839
Iteration 89/1000 | Loss: 0.00000839
Iteration 90/1000 | Loss: 0.00000839
Iteration 91/1000 | Loss: 0.00000839
Iteration 92/1000 | Loss: 0.00000839
Iteration 93/1000 | Loss: 0.00000839
Iteration 94/1000 | Loss: 0.00000838
Iteration 95/1000 | Loss: 0.00000838
Iteration 96/1000 | Loss: 0.00000838
Iteration 97/1000 | Loss: 0.00000838
Iteration 98/1000 | Loss: 0.00000837
Iteration 99/1000 | Loss: 0.00000837
Iteration 100/1000 | Loss: 0.00000837
Iteration 101/1000 | Loss: 0.00000837
Iteration 102/1000 | Loss: 0.00000837
Iteration 103/1000 | Loss: 0.00000837
Iteration 104/1000 | Loss: 0.00000837
Iteration 105/1000 | Loss: 0.00000836
Iteration 106/1000 | Loss: 0.00000836
Iteration 107/1000 | Loss: 0.00000836
Iteration 108/1000 | Loss: 0.00000836
Iteration 109/1000 | Loss: 0.00000835
Iteration 110/1000 | Loss: 0.00000835
Iteration 111/1000 | Loss: 0.00000835
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000835
Iteration 115/1000 | Loss: 0.00000835
Iteration 116/1000 | Loss: 0.00000835
Iteration 117/1000 | Loss: 0.00000835
Iteration 118/1000 | Loss: 0.00000835
Iteration 119/1000 | Loss: 0.00000835
Iteration 120/1000 | Loss: 0.00000835
Iteration 121/1000 | Loss: 0.00000835
Iteration 122/1000 | Loss: 0.00000835
Iteration 123/1000 | Loss: 0.00000835
Iteration 124/1000 | Loss: 0.00000835
Iteration 125/1000 | Loss: 0.00000835
Iteration 126/1000 | Loss: 0.00000835
Iteration 127/1000 | Loss: 0.00000835
Iteration 128/1000 | Loss: 0.00000835
Iteration 129/1000 | Loss: 0.00000835
Iteration 130/1000 | Loss: 0.00000835
Iteration 131/1000 | Loss: 0.00000834
Iteration 132/1000 | Loss: 0.00000834
Iteration 133/1000 | Loss: 0.00000834
Iteration 134/1000 | Loss: 0.00000834
Iteration 135/1000 | Loss: 0.00000834
Iteration 136/1000 | Loss: 0.00000834
Iteration 137/1000 | Loss: 0.00000834
Iteration 138/1000 | Loss: 0.00000834
Iteration 139/1000 | Loss: 0.00000834
Iteration 140/1000 | Loss: 0.00000834
Iteration 141/1000 | Loss: 0.00000834
Iteration 142/1000 | Loss: 0.00000834
Iteration 143/1000 | Loss: 0.00000834
Iteration 144/1000 | Loss: 0.00000834
Iteration 145/1000 | Loss: 0.00000834
Iteration 146/1000 | Loss: 0.00000834
Iteration 147/1000 | Loss: 0.00000834
Iteration 148/1000 | Loss: 0.00000834
Iteration 149/1000 | Loss: 0.00000834
Iteration 150/1000 | Loss: 0.00000834
Iteration 151/1000 | Loss: 0.00000834
Iteration 152/1000 | Loss: 0.00000834
Iteration 153/1000 | Loss: 0.00000833
Iteration 154/1000 | Loss: 0.00000833
Iteration 155/1000 | Loss: 0.00000833
Iteration 156/1000 | Loss: 0.00000833
Iteration 157/1000 | Loss: 0.00000833
Iteration 158/1000 | Loss: 0.00000833
Iteration 159/1000 | Loss: 0.00000833
Iteration 160/1000 | Loss: 0.00000833
Iteration 161/1000 | Loss: 0.00000833
Iteration 162/1000 | Loss: 0.00000833
Iteration 163/1000 | Loss: 0.00000833
Iteration 164/1000 | Loss: 0.00000833
Iteration 165/1000 | Loss: 0.00000833
Iteration 166/1000 | Loss: 0.00000833
Iteration 167/1000 | Loss: 0.00000832
Iteration 168/1000 | Loss: 0.00000832
Iteration 169/1000 | Loss: 0.00000832
Iteration 170/1000 | Loss: 0.00000832
Iteration 171/1000 | Loss: 0.00000832
Iteration 172/1000 | Loss: 0.00000832
Iteration 173/1000 | Loss: 0.00000832
Iteration 174/1000 | Loss: 0.00000832
Iteration 175/1000 | Loss: 0.00000832
Iteration 176/1000 | Loss: 0.00000832
Iteration 177/1000 | Loss: 0.00000832
Iteration 178/1000 | Loss: 0.00000832
Iteration 179/1000 | Loss: 0.00000832
Iteration 180/1000 | Loss: 0.00000832
Iteration 181/1000 | Loss: 0.00000832
Iteration 182/1000 | Loss: 0.00000832
Iteration 183/1000 | Loss: 0.00000832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [8.318829713971354e-06, 8.318829713971354e-06, 8.318829713971354e-06, 8.318829713971354e-06, 8.318829713971354e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.318829713971354e-06

Optimization complete. Final v2v error: 2.4594080448150635 mm

Highest mean error: 2.856017827987671 mm for frame 126

Lowest mean error: 2.01592755317688 mm for frame 190

Saving results

Total time: 35.71256875991821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412102
Iteration 2/25 | Loss: 0.00156511
Iteration 3/25 | Loss: 0.00149457
Iteration 4/25 | Loss: 0.00148387
Iteration 5/25 | Loss: 0.00148001
Iteration 6/25 | Loss: 0.00147872
Iteration 7/25 | Loss: 0.00147854
Iteration 8/25 | Loss: 0.00147854
Iteration 9/25 | Loss: 0.00147854
Iteration 10/25 | Loss: 0.00147854
Iteration 11/25 | Loss: 0.00147854
Iteration 12/25 | Loss: 0.00147854
Iteration 13/25 | Loss: 0.00147854
Iteration 14/25 | Loss: 0.00147854
Iteration 15/25 | Loss: 0.00147854
Iteration 16/25 | Loss: 0.00147854
Iteration 17/25 | Loss: 0.00147854
Iteration 18/25 | Loss: 0.00147854
Iteration 19/25 | Loss: 0.00147854
Iteration 20/25 | Loss: 0.00147854
Iteration 21/25 | Loss: 0.00147854
Iteration 22/25 | Loss: 0.00147854
Iteration 23/25 | Loss: 0.00147854
Iteration 24/25 | Loss: 0.00147854
Iteration 25/25 | Loss: 0.00147854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014785381499677896, 0.0014785381499677896, 0.0014785381499677896, 0.0014785381499677896, 0.0014785381499677896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014785381499677896

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18797123
Iteration 2/25 | Loss: 0.00337884
Iteration 3/25 | Loss: 0.00337884
Iteration 4/25 | Loss: 0.00337884
Iteration 5/25 | Loss: 0.00337884
Iteration 6/25 | Loss: 0.00337884
Iteration 7/25 | Loss: 0.00337884
Iteration 8/25 | Loss: 0.00337884
Iteration 9/25 | Loss: 0.00337884
Iteration 10/25 | Loss: 0.00337884
Iteration 11/25 | Loss: 0.00337884
Iteration 12/25 | Loss: 0.00337884
Iteration 13/25 | Loss: 0.00337884
Iteration 14/25 | Loss: 0.00337884
Iteration 15/25 | Loss: 0.00337884
Iteration 16/25 | Loss: 0.00337884
Iteration 17/25 | Loss: 0.00337884
Iteration 18/25 | Loss: 0.00337884
Iteration 19/25 | Loss: 0.00337884
Iteration 20/25 | Loss: 0.00337884
Iteration 21/25 | Loss: 0.00337884
Iteration 22/25 | Loss: 0.00337884
Iteration 23/25 | Loss: 0.00337884
Iteration 24/25 | Loss: 0.00337884
Iteration 25/25 | Loss: 0.00337884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00337884
Iteration 2/1000 | Loss: 0.00007732
Iteration 3/1000 | Loss: 0.00005286
Iteration 4/1000 | Loss: 0.00003627
Iteration 5/1000 | Loss: 0.00003023
Iteration 6/1000 | Loss: 0.00002784
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002369
Iteration 9/1000 | Loss: 0.00002276
Iteration 10/1000 | Loss: 0.00002201
Iteration 11/1000 | Loss: 0.00002162
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002052
Iteration 16/1000 | Loss: 0.00002037
Iteration 17/1000 | Loss: 0.00002029
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00002019
Iteration 21/1000 | Loss: 0.00002016
Iteration 22/1000 | Loss: 0.00002016
Iteration 23/1000 | Loss: 0.00002015
Iteration 24/1000 | Loss: 0.00002015
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00002005
Iteration 37/1000 | Loss: 0.00002005
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00002004
Iteration 40/1000 | Loss: 0.00002004
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002003
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002002
Iteration 45/1000 | Loss: 0.00002002
Iteration 46/1000 | Loss: 0.00002002
Iteration 47/1000 | Loss: 0.00002002
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00002001
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002001
Iteration 53/1000 | Loss: 0.00002001
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002000
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001999
Iteration 62/1000 | Loss: 0.00001999
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001997
Iteration 66/1000 | Loss: 0.00001997
Iteration 67/1000 | Loss: 0.00001997
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001996
Iteration 70/1000 | Loss: 0.00001995
Iteration 71/1000 | Loss: 0.00001995
Iteration 72/1000 | Loss: 0.00001995
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001991
Iteration 78/1000 | Loss: 0.00001990
Iteration 79/1000 | Loss: 0.00001990
Iteration 80/1000 | Loss: 0.00001989
Iteration 81/1000 | Loss: 0.00001989
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001987
Iteration 86/1000 | Loss: 0.00001987
Iteration 87/1000 | Loss: 0.00001986
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001986
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001985
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001983
Iteration 100/1000 | Loss: 0.00001983
Iteration 101/1000 | Loss: 0.00001983
Iteration 102/1000 | Loss: 0.00001983
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001981
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001980
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001979
Iteration 115/1000 | Loss: 0.00001979
Iteration 116/1000 | Loss: 0.00001979
Iteration 117/1000 | Loss: 0.00001979
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001978
Iteration 121/1000 | Loss: 0.00001978
Iteration 122/1000 | Loss: 0.00001978
Iteration 123/1000 | Loss: 0.00001978
Iteration 124/1000 | Loss: 0.00001978
Iteration 125/1000 | Loss: 0.00001978
Iteration 126/1000 | Loss: 0.00001977
Iteration 127/1000 | Loss: 0.00001977
Iteration 128/1000 | Loss: 0.00001977
Iteration 129/1000 | Loss: 0.00001977
Iteration 130/1000 | Loss: 0.00001976
Iteration 131/1000 | Loss: 0.00001976
Iteration 132/1000 | Loss: 0.00001976
Iteration 133/1000 | Loss: 0.00001976
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001975
Iteration 136/1000 | Loss: 0.00001975
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001973
Iteration 142/1000 | Loss: 0.00001973
Iteration 143/1000 | Loss: 0.00001973
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001970
Iteration 152/1000 | Loss: 0.00001970
Iteration 153/1000 | Loss: 0.00001970
Iteration 154/1000 | Loss: 0.00001969
Iteration 155/1000 | Loss: 0.00001969
Iteration 156/1000 | Loss: 0.00001969
Iteration 157/1000 | Loss: 0.00001969
Iteration 158/1000 | Loss: 0.00001968
Iteration 159/1000 | Loss: 0.00001968
Iteration 160/1000 | Loss: 0.00001968
Iteration 161/1000 | Loss: 0.00001967
Iteration 162/1000 | Loss: 0.00001967
Iteration 163/1000 | Loss: 0.00001967
Iteration 164/1000 | Loss: 0.00001966
Iteration 165/1000 | Loss: 0.00001966
Iteration 166/1000 | Loss: 0.00001966
Iteration 167/1000 | Loss: 0.00001966
Iteration 168/1000 | Loss: 0.00001966
Iteration 169/1000 | Loss: 0.00001965
Iteration 170/1000 | Loss: 0.00001965
Iteration 171/1000 | Loss: 0.00001965
Iteration 172/1000 | Loss: 0.00001965
Iteration 173/1000 | Loss: 0.00001965
Iteration 174/1000 | Loss: 0.00001965
Iteration 175/1000 | Loss: 0.00001965
Iteration 176/1000 | Loss: 0.00001965
Iteration 177/1000 | Loss: 0.00001965
Iteration 178/1000 | Loss: 0.00001965
Iteration 179/1000 | Loss: 0.00001964
Iteration 180/1000 | Loss: 0.00001964
Iteration 181/1000 | Loss: 0.00001964
Iteration 182/1000 | Loss: 0.00001964
Iteration 183/1000 | Loss: 0.00001964
Iteration 184/1000 | Loss: 0.00001964
Iteration 185/1000 | Loss: 0.00001964
Iteration 186/1000 | Loss: 0.00001964
Iteration 187/1000 | Loss: 0.00001964
Iteration 188/1000 | Loss: 0.00001964
Iteration 189/1000 | Loss: 0.00001964
Iteration 190/1000 | Loss: 0.00001964
Iteration 191/1000 | Loss: 0.00001964
Iteration 192/1000 | Loss: 0.00001963
Iteration 193/1000 | Loss: 0.00001963
Iteration 194/1000 | Loss: 0.00001963
Iteration 195/1000 | Loss: 0.00001963
Iteration 196/1000 | Loss: 0.00001963
Iteration 197/1000 | Loss: 0.00001963
Iteration 198/1000 | Loss: 0.00001963
Iteration 199/1000 | Loss: 0.00001963
Iteration 200/1000 | Loss: 0.00001963
Iteration 201/1000 | Loss: 0.00001962
Iteration 202/1000 | Loss: 0.00001962
Iteration 203/1000 | Loss: 0.00001962
Iteration 204/1000 | Loss: 0.00001962
Iteration 205/1000 | Loss: 0.00001962
Iteration 206/1000 | Loss: 0.00001962
Iteration 207/1000 | Loss: 0.00001962
Iteration 208/1000 | Loss: 0.00001962
Iteration 209/1000 | Loss: 0.00001962
Iteration 210/1000 | Loss: 0.00001962
Iteration 211/1000 | Loss: 0.00001962
Iteration 212/1000 | Loss: 0.00001962
Iteration 213/1000 | Loss: 0.00001962
Iteration 214/1000 | Loss: 0.00001962
Iteration 215/1000 | Loss: 0.00001962
Iteration 216/1000 | Loss: 0.00001962
Iteration 217/1000 | Loss: 0.00001962
Iteration 218/1000 | Loss: 0.00001961
Iteration 219/1000 | Loss: 0.00001961
Iteration 220/1000 | Loss: 0.00001961
Iteration 221/1000 | Loss: 0.00001961
Iteration 222/1000 | Loss: 0.00001961
Iteration 223/1000 | Loss: 0.00001961
Iteration 224/1000 | Loss: 0.00001961
Iteration 225/1000 | Loss: 0.00001961
Iteration 226/1000 | Loss: 0.00001961
Iteration 227/1000 | Loss: 0.00001961
Iteration 228/1000 | Loss: 0.00001961
Iteration 229/1000 | Loss: 0.00001961
Iteration 230/1000 | Loss: 0.00001960
Iteration 231/1000 | Loss: 0.00001960
Iteration 232/1000 | Loss: 0.00001960
Iteration 233/1000 | Loss: 0.00001960
Iteration 234/1000 | Loss: 0.00001960
Iteration 235/1000 | Loss: 0.00001960
Iteration 236/1000 | Loss: 0.00001960
Iteration 237/1000 | Loss: 0.00001960
Iteration 238/1000 | Loss: 0.00001960
Iteration 239/1000 | Loss: 0.00001960
Iteration 240/1000 | Loss: 0.00001960
Iteration 241/1000 | Loss: 0.00001959
Iteration 242/1000 | Loss: 0.00001959
Iteration 243/1000 | Loss: 0.00001959
Iteration 244/1000 | Loss: 0.00001959
Iteration 245/1000 | Loss: 0.00001959
Iteration 246/1000 | Loss: 0.00001959
Iteration 247/1000 | Loss: 0.00001959
Iteration 248/1000 | Loss: 0.00001959
Iteration 249/1000 | Loss: 0.00001959
Iteration 250/1000 | Loss: 0.00001959
Iteration 251/1000 | Loss: 0.00001959
Iteration 252/1000 | Loss: 0.00001958
Iteration 253/1000 | Loss: 0.00001958
Iteration 254/1000 | Loss: 0.00001958
Iteration 255/1000 | Loss: 0.00001958
Iteration 256/1000 | Loss: 0.00001958
Iteration 257/1000 | Loss: 0.00001958
Iteration 258/1000 | Loss: 0.00001958
Iteration 259/1000 | Loss: 0.00001958
Iteration 260/1000 | Loss: 0.00001958
Iteration 261/1000 | Loss: 0.00001958
Iteration 262/1000 | Loss: 0.00001958
Iteration 263/1000 | Loss: 0.00001958
Iteration 264/1000 | Loss: 0.00001958
Iteration 265/1000 | Loss: 0.00001958
Iteration 266/1000 | Loss: 0.00001958
Iteration 267/1000 | Loss: 0.00001958
Iteration 268/1000 | Loss: 0.00001958
Iteration 269/1000 | Loss: 0.00001958
Iteration 270/1000 | Loss: 0.00001958
Iteration 271/1000 | Loss: 0.00001958
Iteration 272/1000 | Loss: 0.00001958
Iteration 273/1000 | Loss: 0.00001958
Iteration 274/1000 | Loss: 0.00001958
Iteration 275/1000 | Loss: 0.00001958
Iteration 276/1000 | Loss: 0.00001958
Iteration 277/1000 | Loss: 0.00001958
Iteration 278/1000 | Loss: 0.00001958
Iteration 279/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.9582161257858388e-05, 1.9582161257858388e-05, 1.9582161257858388e-05, 1.9582161257858388e-05, 1.9582161257858388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9582161257858388e-05

Optimization complete. Final v2v error: 3.7414326667785645 mm

Highest mean error: 4.786170482635498 mm for frame 96

Lowest mean error: 2.891498565673828 mm for frame 105

Saving results

Total time: 54.47175073623657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458201
Iteration 2/25 | Loss: 0.00157911
Iteration 3/25 | Loss: 0.00149042
Iteration 4/25 | Loss: 0.00147779
Iteration 5/25 | Loss: 0.00147528
Iteration 6/25 | Loss: 0.00147528
Iteration 7/25 | Loss: 0.00147528
Iteration 8/25 | Loss: 0.00147528
Iteration 9/25 | Loss: 0.00147528
Iteration 10/25 | Loss: 0.00147528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014752759598195553, 0.0014752759598195553, 0.0014752759598195553, 0.0014752759598195553, 0.0014752759598195553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014752759598195553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.13833904
Iteration 2/25 | Loss: 0.00236754
Iteration 3/25 | Loss: 0.00236754
Iteration 4/25 | Loss: 0.00236753
Iteration 5/25 | Loss: 0.00236753
Iteration 6/25 | Loss: 0.00236753
Iteration 7/25 | Loss: 0.00236753
Iteration 8/25 | Loss: 0.00236753
Iteration 9/25 | Loss: 0.00236753
Iteration 10/25 | Loss: 0.00236753
Iteration 11/25 | Loss: 0.00236753
Iteration 12/25 | Loss: 0.00236753
Iteration 13/25 | Loss: 0.00236753
Iteration 14/25 | Loss: 0.00236753
Iteration 15/25 | Loss: 0.00236753
Iteration 16/25 | Loss: 0.00236753
Iteration 17/25 | Loss: 0.00236753
Iteration 18/25 | Loss: 0.00236753
Iteration 19/25 | Loss: 0.00236753
Iteration 20/25 | Loss: 0.00236753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023675295524299145, 0.0023675295524299145, 0.0023675295524299145, 0.0023675295524299145, 0.0023675295524299145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023675295524299145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236753
Iteration 2/1000 | Loss: 0.00002398
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001561
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001466
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001406
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001391
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001380
Iteration 21/1000 | Loss: 0.00001379
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001368
Iteration 36/1000 | Loss: 0.00001368
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001366
Iteration 40/1000 | Loss: 0.00001366
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001363
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001363
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001346
Iteration 82/1000 | Loss: 0.00001346
Iteration 83/1000 | Loss: 0.00001345
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001345
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001344
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001343
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001343
Iteration 116/1000 | Loss: 0.00001343
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001341
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001340
Iteration 145/1000 | Loss: 0.00001340
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00001339
Iteration 149/1000 | Loss: 0.00001339
Iteration 150/1000 | Loss: 0.00001339
Iteration 151/1000 | Loss: 0.00001339
Iteration 152/1000 | Loss: 0.00001338
Iteration 153/1000 | Loss: 0.00001338
Iteration 154/1000 | Loss: 0.00001338
Iteration 155/1000 | Loss: 0.00001338
Iteration 156/1000 | Loss: 0.00001338
Iteration 157/1000 | Loss: 0.00001338
Iteration 158/1000 | Loss: 0.00001338
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001338
Iteration 161/1000 | Loss: 0.00001338
Iteration 162/1000 | Loss: 0.00001338
Iteration 163/1000 | Loss: 0.00001337
Iteration 164/1000 | Loss: 0.00001337
Iteration 165/1000 | Loss: 0.00001337
Iteration 166/1000 | Loss: 0.00001337
Iteration 167/1000 | Loss: 0.00001337
Iteration 168/1000 | Loss: 0.00001337
Iteration 169/1000 | Loss: 0.00001337
Iteration 170/1000 | Loss: 0.00001337
Iteration 171/1000 | Loss: 0.00001337
Iteration 172/1000 | Loss: 0.00001336
Iteration 173/1000 | Loss: 0.00001336
Iteration 174/1000 | Loss: 0.00001336
Iteration 175/1000 | Loss: 0.00001336
Iteration 176/1000 | Loss: 0.00001336
Iteration 177/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3363631296670064e-05, 1.3363631296670064e-05, 1.3363631296670064e-05, 1.3363631296670064e-05, 1.3363631296670064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3363631296670064e-05

Optimization complete. Final v2v error: 3.154665470123291 mm

Highest mean error: 3.4716296195983887 mm for frame 229

Lowest mean error: 3.001518487930298 mm for frame 113

Saving results

Total time: 48.283294677734375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005296
Iteration 2/25 | Loss: 0.00280362
Iteration 3/25 | Loss: 0.00225312
Iteration 4/25 | Loss: 0.00182404
Iteration 5/25 | Loss: 0.00181711
Iteration 6/25 | Loss: 0.00183715
Iteration 7/25 | Loss: 0.00171563
Iteration 8/25 | Loss: 0.00171848
Iteration 9/25 | Loss: 0.00163139
Iteration 10/25 | Loss: 0.00159141
Iteration 11/25 | Loss: 0.00156585
Iteration 12/25 | Loss: 0.00156265
Iteration 13/25 | Loss: 0.00155030
Iteration 14/25 | Loss: 0.00154431
Iteration 15/25 | Loss: 0.00152764
Iteration 16/25 | Loss: 0.00153783
Iteration 17/25 | Loss: 0.00154047
Iteration 18/25 | Loss: 0.00152870
Iteration 19/25 | Loss: 0.00152435
Iteration 20/25 | Loss: 0.00151456
Iteration 21/25 | Loss: 0.00151140
Iteration 22/25 | Loss: 0.00150586
Iteration 23/25 | Loss: 0.00150245
Iteration 24/25 | Loss: 0.00149960
Iteration 25/25 | Loss: 0.00150069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33671260
Iteration 2/25 | Loss: 0.00263695
Iteration 3/25 | Loss: 0.00263695
Iteration 4/25 | Loss: 0.00263695
Iteration 5/25 | Loss: 0.00263695
Iteration 6/25 | Loss: 0.00263694
Iteration 7/25 | Loss: 0.00263694
Iteration 8/25 | Loss: 0.00263694
Iteration 9/25 | Loss: 0.00263694
Iteration 10/25 | Loss: 0.00263694
Iteration 11/25 | Loss: 0.00263694
Iteration 12/25 | Loss: 0.00263694
Iteration 13/25 | Loss: 0.00263694
Iteration 14/25 | Loss: 0.00263694
Iteration 15/25 | Loss: 0.00263694
Iteration 16/25 | Loss: 0.00263694
Iteration 17/25 | Loss: 0.00263694
Iteration 18/25 | Loss: 0.00263694
Iteration 19/25 | Loss: 0.00263694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002636943245306611, 0.002636943245306611, 0.002636943245306611, 0.002636943245306611, 0.002636943245306611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002636943245306611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263694
Iteration 2/1000 | Loss: 0.00025084
Iteration 3/1000 | Loss: 0.00005401
Iteration 4/1000 | Loss: 0.00025585
Iteration 5/1000 | Loss: 0.00061348
Iteration 6/1000 | Loss: 0.00039227
Iteration 7/1000 | Loss: 0.00036197
Iteration 8/1000 | Loss: 0.00029631
Iteration 9/1000 | Loss: 0.00039672
Iteration 10/1000 | Loss: 0.00065835
Iteration 11/1000 | Loss: 0.00016200
Iteration 12/1000 | Loss: 0.00059289
Iteration 13/1000 | Loss: 0.00009261
Iteration 14/1000 | Loss: 0.00017254
Iteration 15/1000 | Loss: 0.00008918
Iteration 16/1000 | Loss: 0.00035321
Iteration 17/1000 | Loss: 0.00040920
Iteration 18/1000 | Loss: 0.00029790
Iteration 19/1000 | Loss: 0.00035868
Iteration 20/1000 | Loss: 0.00034709
Iteration 21/1000 | Loss: 0.00042263
Iteration 22/1000 | Loss: 0.00014992
Iteration 23/1000 | Loss: 0.00013816
Iteration 24/1000 | Loss: 0.00023742
Iteration 25/1000 | Loss: 0.00048225
Iteration 26/1000 | Loss: 0.00032728
Iteration 27/1000 | Loss: 0.00040261
Iteration 28/1000 | Loss: 0.00031236
Iteration 29/1000 | Loss: 0.00034758
Iteration 30/1000 | Loss: 0.00023553
Iteration 31/1000 | Loss: 0.00038647
Iteration 32/1000 | Loss: 0.00017130
Iteration 33/1000 | Loss: 0.00029646
Iteration 34/1000 | Loss: 0.00023925
Iteration 35/1000 | Loss: 0.00017388
Iteration 36/1000 | Loss: 0.00017135
Iteration 37/1000 | Loss: 0.00026693
Iteration 38/1000 | Loss: 0.00022680
Iteration 39/1000 | Loss: 0.00013104
Iteration 40/1000 | Loss: 0.00022952
Iteration 41/1000 | Loss: 0.00010245
Iteration 42/1000 | Loss: 0.00009942
Iteration 43/1000 | Loss: 0.00009630
Iteration 44/1000 | Loss: 0.00013909
Iteration 45/1000 | Loss: 0.00017482
Iteration 46/1000 | Loss: 0.00026743
Iteration 47/1000 | Loss: 0.00018225
Iteration 48/1000 | Loss: 0.00020465
Iteration 49/1000 | Loss: 0.00020582
Iteration 50/1000 | Loss: 0.00017167
Iteration 51/1000 | Loss: 0.00012573
Iteration 52/1000 | Loss: 0.00007881
Iteration 53/1000 | Loss: 0.00073769
Iteration 54/1000 | Loss: 0.00027984
Iteration 55/1000 | Loss: 0.00017068
Iteration 56/1000 | Loss: 0.00005885
Iteration 57/1000 | Loss: 0.00009877
Iteration 58/1000 | Loss: 0.00012336
Iteration 59/1000 | Loss: 0.00006890
Iteration 60/1000 | Loss: 0.00022500
Iteration 61/1000 | Loss: 0.00012912
Iteration 62/1000 | Loss: 0.00004032
Iteration 63/1000 | Loss: 0.00026276
Iteration 64/1000 | Loss: 0.00027886
Iteration 65/1000 | Loss: 0.00050837
Iteration 66/1000 | Loss: 0.00026637
Iteration 67/1000 | Loss: 0.00025237
Iteration 68/1000 | Loss: 0.00017501
Iteration 69/1000 | Loss: 0.00038344
Iteration 70/1000 | Loss: 0.00033329
Iteration 71/1000 | Loss: 0.00038456
Iteration 72/1000 | Loss: 0.00004565
Iteration 73/1000 | Loss: 0.00004161
Iteration 74/1000 | Loss: 0.00029674
Iteration 75/1000 | Loss: 0.00027893
Iteration 76/1000 | Loss: 0.00023677
Iteration 77/1000 | Loss: 0.00009896
Iteration 78/1000 | Loss: 0.00035356
Iteration 79/1000 | Loss: 0.00043850
Iteration 80/1000 | Loss: 0.00028966
Iteration 81/1000 | Loss: 0.00026805
Iteration 82/1000 | Loss: 0.00022260
Iteration 83/1000 | Loss: 0.00012074
Iteration 84/1000 | Loss: 0.00035934
Iteration 85/1000 | Loss: 0.00029259
Iteration 86/1000 | Loss: 0.00013552
Iteration 87/1000 | Loss: 0.00023530
Iteration 88/1000 | Loss: 0.00051997
Iteration 89/1000 | Loss: 0.00159880
Iteration 90/1000 | Loss: 0.00085069
Iteration 91/1000 | Loss: 0.00084704
Iteration 92/1000 | Loss: 0.00076791
Iteration 93/1000 | Loss: 0.00061703
Iteration 94/1000 | Loss: 0.00023017
Iteration 95/1000 | Loss: 0.00068290
Iteration 96/1000 | Loss: 0.00004912
Iteration 97/1000 | Loss: 0.00003912
Iteration 98/1000 | Loss: 0.00003134
Iteration 99/1000 | Loss: 0.00007364
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00004163
Iteration 102/1000 | Loss: 0.00002087
Iteration 103/1000 | Loss: 0.00018592
Iteration 104/1000 | Loss: 0.00016818
Iteration 105/1000 | Loss: 0.00018494
Iteration 106/1000 | Loss: 0.00003691
Iteration 107/1000 | Loss: 0.00010927
Iteration 108/1000 | Loss: 0.00002530
Iteration 109/1000 | Loss: 0.00012609
Iteration 110/1000 | Loss: 0.00017633
Iteration 111/1000 | Loss: 0.00005328
Iteration 112/1000 | Loss: 0.00004365
Iteration 113/1000 | Loss: 0.00042792
Iteration 114/1000 | Loss: 0.00008362
Iteration 115/1000 | Loss: 0.00005235
Iteration 116/1000 | Loss: 0.00008673
Iteration 117/1000 | Loss: 0.00012173
Iteration 118/1000 | Loss: 0.00010487
Iteration 119/1000 | Loss: 0.00010717
Iteration 120/1000 | Loss: 0.00016323
Iteration 121/1000 | Loss: 0.00009886
Iteration 122/1000 | Loss: 0.00015611
Iteration 123/1000 | Loss: 0.00011367
Iteration 124/1000 | Loss: 0.00015561
Iteration 125/1000 | Loss: 0.00019601
Iteration 126/1000 | Loss: 0.00013621
Iteration 127/1000 | Loss: 0.00018085
Iteration 128/1000 | Loss: 0.00014354
Iteration 129/1000 | Loss: 0.00010278
Iteration 130/1000 | Loss: 0.00024273
Iteration 131/1000 | Loss: 0.00019780
Iteration 132/1000 | Loss: 0.00002282
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001832
Iteration 136/1000 | Loss: 0.00001783
Iteration 137/1000 | Loss: 0.00025252
Iteration 138/1000 | Loss: 0.00010617
Iteration 139/1000 | Loss: 0.00018699
Iteration 140/1000 | Loss: 0.00008628
Iteration 141/1000 | Loss: 0.00008924
Iteration 142/1000 | Loss: 0.00002287
Iteration 143/1000 | Loss: 0.00002202
Iteration 144/1000 | Loss: 0.00010063
Iteration 145/1000 | Loss: 0.00001919
Iteration 146/1000 | Loss: 0.00001816
Iteration 147/1000 | Loss: 0.00010430
Iteration 148/1000 | Loss: 0.00001754
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001640
Iteration 151/1000 | Loss: 0.00004458
Iteration 152/1000 | Loss: 0.00001562
Iteration 153/1000 | Loss: 0.00001511
Iteration 154/1000 | Loss: 0.00001474
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001436
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001432
Iteration 165/1000 | Loss: 0.00001432
Iteration 166/1000 | Loss: 0.00001431
Iteration 167/1000 | Loss: 0.00001431
Iteration 168/1000 | Loss: 0.00001431
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001429
Iteration 173/1000 | Loss: 0.00001429
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001428
Iteration 176/1000 | Loss: 0.00001428
Iteration 177/1000 | Loss: 0.00001428
Iteration 178/1000 | Loss: 0.00001427
Iteration 179/1000 | Loss: 0.00001427
Iteration 180/1000 | Loss: 0.00001427
Iteration 181/1000 | Loss: 0.00001427
Iteration 182/1000 | Loss: 0.00001427
Iteration 183/1000 | Loss: 0.00001427
Iteration 184/1000 | Loss: 0.00001426
Iteration 185/1000 | Loss: 0.00001426
Iteration 186/1000 | Loss: 0.00001426
Iteration 187/1000 | Loss: 0.00001426
Iteration 188/1000 | Loss: 0.00001426
Iteration 189/1000 | Loss: 0.00001426
Iteration 190/1000 | Loss: 0.00001426
Iteration 191/1000 | Loss: 0.00001426
Iteration 192/1000 | Loss: 0.00001426
Iteration 193/1000 | Loss: 0.00001426
Iteration 194/1000 | Loss: 0.00001426
Iteration 195/1000 | Loss: 0.00001426
Iteration 196/1000 | Loss: 0.00001426
Iteration 197/1000 | Loss: 0.00001425
Iteration 198/1000 | Loss: 0.00001425
Iteration 199/1000 | Loss: 0.00001425
Iteration 200/1000 | Loss: 0.00001425
Iteration 201/1000 | Loss: 0.00001425
Iteration 202/1000 | Loss: 0.00001424
Iteration 203/1000 | Loss: 0.00001424
Iteration 204/1000 | Loss: 0.00001424
Iteration 205/1000 | Loss: 0.00001424
Iteration 206/1000 | Loss: 0.00001424
Iteration 207/1000 | Loss: 0.00001424
Iteration 208/1000 | Loss: 0.00001424
Iteration 209/1000 | Loss: 0.00001424
Iteration 210/1000 | Loss: 0.00001423
Iteration 211/1000 | Loss: 0.00001423
Iteration 212/1000 | Loss: 0.00001423
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Iteration 216/1000 | Loss: 0.00001423
Iteration 217/1000 | Loss: 0.00001423
Iteration 218/1000 | Loss: 0.00001423
Iteration 219/1000 | Loss: 0.00001423
Iteration 220/1000 | Loss: 0.00001423
Iteration 221/1000 | Loss: 0.00001422
Iteration 222/1000 | Loss: 0.00001422
Iteration 223/1000 | Loss: 0.00001422
Iteration 224/1000 | Loss: 0.00001422
Iteration 225/1000 | Loss: 0.00001422
Iteration 226/1000 | Loss: 0.00001422
Iteration 227/1000 | Loss: 0.00001422
Iteration 228/1000 | Loss: 0.00001422
Iteration 229/1000 | Loss: 0.00001422
Iteration 230/1000 | Loss: 0.00001422
Iteration 231/1000 | Loss: 0.00001422
Iteration 232/1000 | Loss: 0.00001422
Iteration 233/1000 | Loss: 0.00001422
Iteration 234/1000 | Loss: 0.00001422
Iteration 235/1000 | Loss: 0.00001422
Iteration 236/1000 | Loss: 0.00001421
Iteration 237/1000 | Loss: 0.00001421
Iteration 238/1000 | Loss: 0.00001421
Iteration 239/1000 | Loss: 0.00001421
Iteration 240/1000 | Loss: 0.00001421
Iteration 241/1000 | Loss: 0.00001421
Iteration 242/1000 | Loss: 0.00001421
Iteration 243/1000 | Loss: 0.00001421
Iteration 244/1000 | Loss: 0.00001421
Iteration 245/1000 | Loss: 0.00001421
Iteration 246/1000 | Loss: 0.00001421
Iteration 247/1000 | Loss: 0.00001421
Iteration 248/1000 | Loss: 0.00001421
Iteration 249/1000 | Loss: 0.00001421
Iteration 250/1000 | Loss: 0.00001421
Iteration 251/1000 | Loss: 0.00001421
Iteration 252/1000 | Loss: 0.00001421
Iteration 253/1000 | Loss: 0.00001421
Iteration 254/1000 | Loss: 0.00001421
Iteration 255/1000 | Loss: 0.00001421
Iteration 256/1000 | Loss: 0.00001421
Iteration 257/1000 | Loss: 0.00001421
Iteration 258/1000 | Loss: 0.00001420
Iteration 259/1000 | Loss: 0.00001420
Iteration 260/1000 | Loss: 0.00001420
Iteration 261/1000 | Loss: 0.00001420
Iteration 262/1000 | Loss: 0.00001420
Iteration 263/1000 | Loss: 0.00001420
Iteration 264/1000 | Loss: 0.00001420
Iteration 265/1000 | Loss: 0.00001420
Iteration 266/1000 | Loss: 0.00001420
Iteration 267/1000 | Loss: 0.00001420
Iteration 268/1000 | Loss: 0.00001420
Iteration 269/1000 | Loss: 0.00001420
Iteration 270/1000 | Loss: 0.00001420
Iteration 271/1000 | Loss: 0.00001420
Iteration 272/1000 | Loss: 0.00001420
Iteration 273/1000 | Loss: 0.00001420
Iteration 274/1000 | Loss: 0.00001420
Iteration 275/1000 | Loss: 0.00001420
Iteration 276/1000 | Loss: 0.00001420
Iteration 277/1000 | Loss: 0.00001420
Iteration 278/1000 | Loss: 0.00001420
Iteration 279/1000 | Loss: 0.00001420
Iteration 280/1000 | Loss: 0.00001420
Iteration 281/1000 | Loss: 0.00001420
Iteration 282/1000 | Loss: 0.00001420
Iteration 283/1000 | Loss: 0.00001420
Iteration 284/1000 | Loss: 0.00001420
Iteration 285/1000 | Loss: 0.00001420
Iteration 286/1000 | Loss: 0.00001420
Iteration 287/1000 | Loss: 0.00001420
Iteration 288/1000 | Loss: 0.00001420
Iteration 289/1000 | Loss: 0.00001420
Iteration 290/1000 | Loss: 0.00001420
Iteration 291/1000 | Loss: 0.00001420
Iteration 292/1000 | Loss: 0.00001420
Iteration 293/1000 | Loss: 0.00001420
Iteration 294/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.4201452358975075e-05, 1.4201452358975075e-05, 1.4201452358975075e-05, 1.4201452358975075e-05, 1.4201452358975075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4201452358975075e-05

Optimization complete. Final v2v error: 3.2476415634155273 mm

Highest mean error: 4.110340118408203 mm for frame 57

Lowest mean error: 3.0198347568511963 mm for frame 22

Saving results

Total time: 283.2781994342804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760196
Iteration 2/25 | Loss: 0.00165546
Iteration 3/25 | Loss: 0.00151365
Iteration 4/25 | Loss: 0.00149772
Iteration 5/25 | Loss: 0.00149468
Iteration 6/25 | Loss: 0.00149468
Iteration 7/25 | Loss: 0.00149468
Iteration 8/25 | Loss: 0.00149468
Iteration 9/25 | Loss: 0.00149468
Iteration 10/25 | Loss: 0.00149468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001494683907367289, 0.001494683907367289, 0.001494683907367289, 0.001494683907367289, 0.001494683907367289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001494683907367289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21708894
Iteration 2/25 | Loss: 0.00257704
Iteration 3/25 | Loss: 0.00257704
Iteration 4/25 | Loss: 0.00257704
Iteration 5/25 | Loss: 0.00257704
Iteration 6/25 | Loss: 0.00257704
Iteration 7/25 | Loss: 0.00257704
Iteration 8/25 | Loss: 0.00257704
Iteration 9/25 | Loss: 0.00257704
Iteration 10/25 | Loss: 0.00257704
Iteration 11/25 | Loss: 0.00257704
Iteration 12/25 | Loss: 0.00257704
Iteration 13/25 | Loss: 0.00257704
Iteration 14/25 | Loss: 0.00257704
Iteration 15/25 | Loss: 0.00257704
Iteration 16/25 | Loss: 0.00257704
Iteration 17/25 | Loss: 0.00257704
Iteration 18/25 | Loss: 0.00257704
Iteration 19/25 | Loss: 0.00257704
Iteration 20/25 | Loss: 0.00257704
Iteration 21/25 | Loss: 0.00257704
Iteration 22/25 | Loss: 0.00257704
Iteration 23/25 | Loss: 0.00257704
Iteration 24/25 | Loss: 0.00257704
Iteration 25/25 | Loss: 0.00257704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002577040111646056, 0.002577040111646056, 0.002577040111646056, 0.002577040111646056, 0.002577040111646056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002577040111646056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257704
Iteration 2/1000 | Loss: 0.00003352
Iteration 3/1000 | Loss: 0.00002379
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001582
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001443
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001406
Iteration 22/1000 | Loss: 0.00001394
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001360
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001344
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001341
Iteration 61/1000 | Loss: 0.00001341
Iteration 62/1000 | Loss: 0.00001341
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001336
Iteration 74/1000 | Loss: 0.00001336
Iteration 75/1000 | Loss: 0.00001336
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001326
Iteration 102/1000 | Loss: 0.00001326
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001318
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001317
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001317
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001316
Iteration 148/1000 | Loss: 0.00001316
Iteration 149/1000 | Loss: 0.00001316
Iteration 150/1000 | Loss: 0.00001316
Iteration 151/1000 | Loss: 0.00001316
Iteration 152/1000 | Loss: 0.00001316
Iteration 153/1000 | Loss: 0.00001315
Iteration 154/1000 | Loss: 0.00001315
Iteration 155/1000 | Loss: 0.00001315
Iteration 156/1000 | Loss: 0.00001315
Iteration 157/1000 | Loss: 0.00001315
Iteration 158/1000 | Loss: 0.00001315
Iteration 159/1000 | Loss: 0.00001315
Iteration 160/1000 | Loss: 0.00001315
Iteration 161/1000 | Loss: 0.00001315
Iteration 162/1000 | Loss: 0.00001315
Iteration 163/1000 | Loss: 0.00001315
Iteration 164/1000 | Loss: 0.00001315
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001314
Iteration 168/1000 | Loss: 0.00001314
Iteration 169/1000 | Loss: 0.00001314
Iteration 170/1000 | Loss: 0.00001314
Iteration 171/1000 | Loss: 0.00001314
Iteration 172/1000 | Loss: 0.00001314
Iteration 173/1000 | Loss: 0.00001314
Iteration 174/1000 | Loss: 0.00001314
Iteration 175/1000 | Loss: 0.00001314
Iteration 176/1000 | Loss: 0.00001314
Iteration 177/1000 | Loss: 0.00001314
Iteration 178/1000 | Loss: 0.00001314
Iteration 179/1000 | Loss: 0.00001314
Iteration 180/1000 | Loss: 0.00001314
Iteration 181/1000 | Loss: 0.00001314
Iteration 182/1000 | Loss: 0.00001314
Iteration 183/1000 | Loss: 0.00001314
Iteration 184/1000 | Loss: 0.00001314
Iteration 185/1000 | Loss: 0.00001314
Iteration 186/1000 | Loss: 0.00001314
Iteration 187/1000 | Loss: 0.00001314
Iteration 188/1000 | Loss: 0.00001314
Iteration 189/1000 | Loss: 0.00001314
Iteration 190/1000 | Loss: 0.00001314
Iteration 191/1000 | Loss: 0.00001314
Iteration 192/1000 | Loss: 0.00001314
Iteration 193/1000 | Loss: 0.00001314
Iteration 194/1000 | Loss: 0.00001314
Iteration 195/1000 | Loss: 0.00001314
Iteration 196/1000 | Loss: 0.00001314
Iteration 197/1000 | Loss: 0.00001314
Iteration 198/1000 | Loss: 0.00001314
Iteration 199/1000 | Loss: 0.00001314
Iteration 200/1000 | Loss: 0.00001314
Iteration 201/1000 | Loss: 0.00001314
Iteration 202/1000 | Loss: 0.00001314
Iteration 203/1000 | Loss: 0.00001314
Iteration 204/1000 | Loss: 0.00001314
Iteration 205/1000 | Loss: 0.00001314
Iteration 206/1000 | Loss: 0.00001314
Iteration 207/1000 | Loss: 0.00001314
Iteration 208/1000 | Loss: 0.00001314
Iteration 209/1000 | Loss: 0.00001314
Iteration 210/1000 | Loss: 0.00001314
Iteration 211/1000 | Loss: 0.00001314
Iteration 212/1000 | Loss: 0.00001314
Iteration 213/1000 | Loss: 0.00001314
Iteration 214/1000 | Loss: 0.00001314
Iteration 215/1000 | Loss: 0.00001314
Iteration 216/1000 | Loss: 0.00001314
Iteration 217/1000 | Loss: 0.00001314
Iteration 218/1000 | Loss: 0.00001314
Iteration 219/1000 | Loss: 0.00001314
Iteration 220/1000 | Loss: 0.00001314
Iteration 221/1000 | Loss: 0.00001314
Iteration 222/1000 | Loss: 0.00001314
Iteration 223/1000 | Loss: 0.00001314
Iteration 224/1000 | Loss: 0.00001314
Iteration 225/1000 | Loss: 0.00001314
Iteration 226/1000 | Loss: 0.00001314
Iteration 227/1000 | Loss: 0.00001314
Iteration 228/1000 | Loss: 0.00001314
Iteration 229/1000 | Loss: 0.00001314
Iteration 230/1000 | Loss: 0.00001314
Iteration 231/1000 | Loss: 0.00001314
Iteration 232/1000 | Loss: 0.00001314
Iteration 233/1000 | Loss: 0.00001314
Iteration 234/1000 | Loss: 0.00001314
Iteration 235/1000 | Loss: 0.00001314
Iteration 236/1000 | Loss: 0.00001314
Iteration 237/1000 | Loss: 0.00001314
Iteration 238/1000 | Loss: 0.00001314
Iteration 239/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.314478231506655e-05, 1.314478231506655e-05, 1.314478231506655e-05, 1.314478231506655e-05, 1.314478231506655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.314478231506655e-05

Optimization complete. Final v2v error: 3.1318459510803223 mm

Highest mean error: 3.493194818496704 mm for frame 121

Lowest mean error: 2.8190219402313232 mm for frame 62

Saving results

Total time: 51.37241339683533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967902
Iteration 2/25 | Loss: 0.00175268
Iteration 3/25 | Loss: 0.00160324
Iteration 4/25 | Loss: 0.00158594
Iteration 5/25 | Loss: 0.00158194
Iteration 6/25 | Loss: 0.00156323
Iteration 7/25 | Loss: 0.00153273
Iteration 8/25 | Loss: 0.00152625
Iteration 9/25 | Loss: 0.00151768
Iteration 10/25 | Loss: 0.00152267
Iteration 11/25 | Loss: 0.00152207
Iteration 12/25 | Loss: 0.00151305
Iteration 13/25 | Loss: 0.00151144
Iteration 14/25 | Loss: 0.00150715
Iteration 15/25 | Loss: 0.00150674
Iteration 16/25 | Loss: 0.00150663
Iteration 17/25 | Loss: 0.00150661
Iteration 18/25 | Loss: 0.00150661
Iteration 19/25 | Loss: 0.00150661
Iteration 20/25 | Loss: 0.00150660
Iteration 21/25 | Loss: 0.00150660
Iteration 22/25 | Loss: 0.00150660
Iteration 23/25 | Loss: 0.00150660
Iteration 24/25 | Loss: 0.00150660
Iteration 25/25 | Loss: 0.00150660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77960300
Iteration 2/25 | Loss: 0.00250074
Iteration 3/25 | Loss: 0.00250074
Iteration 4/25 | Loss: 0.00250074
Iteration 5/25 | Loss: 0.00250074
Iteration 6/25 | Loss: 0.00250074
Iteration 7/25 | Loss: 0.00250074
Iteration 8/25 | Loss: 0.00250074
Iteration 9/25 | Loss: 0.00250074
Iteration 10/25 | Loss: 0.00250074
Iteration 11/25 | Loss: 0.00250074
Iteration 12/25 | Loss: 0.00250074
Iteration 13/25 | Loss: 0.00250074
Iteration 14/25 | Loss: 0.00250074
Iteration 15/25 | Loss: 0.00250074
Iteration 16/25 | Loss: 0.00250074
Iteration 17/25 | Loss: 0.00250074
Iteration 18/25 | Loss: 0.00250074
Iteration 19/25 | Loss: 0.00250074
Iteration 20/25 | Loss: 0.00250074
Iteration 21/25 | Loss: 0.00250074
Iteration 22/25 | Loss: 0.00250074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002500735456123948, 0.002500735456123948, 0.002500735456123948, 0.002500735456123948, 0.002500735456123948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002500735456123948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250074
Iteration 2/1000 | Loss: 0.00002939
Iteration 3/1000 | Loss: 0.00002091
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001826
Iteration 6/1000 | Loss: 0.00001777
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001556
Iteration 13/1000 | Loss: 0.00001533
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001512
Iteration 16/1000 | Loss: 0.00001508
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001504
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001488
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001487
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001483
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001474
Iteration 71/1000 | Loss: 0.00001474
Iteration 72/1000 | Loss: 0.00001474
Iteration 73/1000 | Loss: 0.00001474
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001471
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001471
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001468
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001467
Iteration 113/1000 | Loss: 0.00001467
Iteration 114/1000 | Loss: 0.00001467
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001466
Iteration 118/1000 | Loss: 0.00001466
Iteration 119/1000 | Loss: 0.00001466
Iteration 120/1000 | Loss: 0.00001466
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001465
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001464
Iteration 137/1000 | Loss: 0.00001464
Iteration 138/1000 | Loss: 0.00001464
Iteration 139/1000 | Loss: 0.00001464
Iteration 140/1000 | Loss: 0.00001464
Iteration 141/1000 | Loss: 0.00001464
Iteration 142/1000 | Loss: 0.00001464
Iteration 143/1000 | Loss: 0.00001464
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001463
Iteration 148/1000 | Loss: 0.00001463
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001463
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001462
Iteration 156/1000 | Loss: 0.00001462
Iteration 157/1000 | Loss: 0.00001462
Iteration 158/1000 | Loss: 0.00001462
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001461
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001461
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4609284335165285e-05, 1.4609284335165285e-05, 1.4609284335165285e-05, 1.4609284335165285e-05, 1.4609284335165285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4609284335165285e-05

Optimization complete. Final v2v error: 3.3099162578582764 mm

Highest mean error: 3.6172471046447754 mm for frame 99

Lowest mean error: 3.044527530670166 mm for frame 112

Saving results

Total time: 62.971543312072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504151
Iteration 2/25 | Loss: 0.00159648
Iteration 3/25 | Loss: 0.00150998
Iteration 4/25 | Loss: 0.00149316
Iteration 5/25 | Loss: 0.00148904
Iteration 6/25 | Loss: 0.00148875
Iteration 7/25 | Loss: 0.00148875
Iteration 8/25 | Loss: 0.00148875
Iteration 9/25 | Loss: 0.00148875
Iteration 10/25 | Loss: 0.00148875
Iteration 11/25 | Loss: 0.00148875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014887497527524829, 0.0014887497527524829, 0.0014887497527524829, 0.0014887497527524829, 0.0014887497527524829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014887497527524829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.65791702
Iteration 2/25 | Loss: 0.00239819
Iteration 3/25 | Loss: 0.00239818
Iteration 4/25 | Loss: 0.00239818
Iteration 5/25 | Loss: 0.00239818
Iteration 6/25 | Loss: 0.00239818
Iteration 7/25 | Loss: 0.00239818
Iteration 8/25 | Loss: 0.00239818
Iteration 9/25 | Loss: 0.00239818
Iteration 10/25 | Loss: 0.00239818
Iteration 11/25 | Loss: 0.00239818
Iteration 12/25 | Loss: 0.00239818
Iteration 13/25 | Loss: 0.00239818
Iteration 14/25 | Loss: 0.00239818
Iteration 15/25 | Loss: 0.00239818
Iteration 16/25 | Loss: 0.00239818
Iteration 17/25 | Loss: 0.00239818
Iteration 18/25 | Loss: 0.00239818
Iteration 19/25 | Loss: 0.00239818
Iteration 20/25 | Loss: 0.00239818
Iteration 21/25 | Loss: 0.00239818
Iteration 22/25 | Loss: 0.00239818
Iteration 23/25 | Loss: 0.00239818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0023981796111911535, 0.0023981796111911535, 0.0023981796111911535, 0.0023981796111911535, 0.0023981796111911535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023981796111911535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239818
Iteration 2/1000 | Loss: 0.00003115
Iteration 3/1000 | Loss: 0.00002607
Iteration 4/1000 | Loss: 0.00002411
Iteration 5/1000 | Loss: 0.00002270
Iteration 6/1000 | Loss: 0.00002173
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00002042
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001911
Iteration 12/1000 | Loss: 0.00001883
Iteration 13/1000 | Loss: 0.00001862
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001814
Iteration 20/1000 | Loss: 0.00001814
Iteration 21/1000 | Loss: 0.00001810
Iteration 22/1000 | Loss: 0.00001809
Iteration 23/1000 | Loss: 0.00001809
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001807
Iteration 28/1000 | Loss: 0.00001807
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001806
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001804
Iteration 33/1000 | Loss: 0.00001803
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001800
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001792
Iteration 42/1000 | Loss: 0.00001792
Iteration 43/1000 | Loss: 0.00001790
Iteration 44/1000 | Loss: 0.00001789
Iteration 45/1000 | Loss: 0.00001789
Iteration 46/1000 | Loss: 0.00001789
Iteration 47/1000 | Loss: 0.00001788
Iteration 48/1000 | Loss: 0.00001788
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001775
Iteration 91/1000 | Loss: 0.00001775
Iteration 92/1000 | Loss: 0.00001775
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001773
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001773
Iteration 102/1000 | Loss: 0.00001773
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001772
Iteration 108/1000 | Loss: 0.00001772
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001770
Iteration 121/1000 | Loss: 0.00001770
Iteration 122/1000 | Loss: 0.00001770
Iteration 123/1000 | Loss: 0.00001770
Iteration 124/1000 | Loss: 0.00001770
Iteration 125/1000 | Loss: 0.00001770
Iteration 126/1000 | Loss: 0.00001770
Iteration 127/1000 | Loss: 0.00001770
Iteration 128/1000 | Loss: 0.00001770
Iteration 129/1000 | Loss: 0.00001770
Iteration 130/1000 | Loss: 0.00001769
Iteration 131/1000 | Loss: 0.00001769
Iteration 132/1000 | Loss: 0.00001769
Iteration 133/1000 | Loss: 0.00001769
Iteration 134/1000 | Loss: 0.00001769
Iteration 135/1000 | Loss: 0.00001769
Iteration 136/1000 | Loss: 0.00001769
Iteration 137/1000 | Loss: 0.00001769
Iteration 138/1000 | Loss: 0.00001769
Iteration 139/1000 | Loss: 0.00001769
Iteration 140/1000 | Loss: 0.00001769
Iteration 141/1000 | Loss: 0.00001769
Iteration 142/1000 | Loss: 0.00001769
Iteration 143/1000 | Loss: 0.00001769
Iteration 144/1000 | Loss: 0.00001769
Iteration 145/1000 | Loss: 0.00001769
Iteration 146/1000 | Loss: 0.00001769
Iteration 147/1000 | Loss: 0.00001769
Iteration 148/1000 | Loss: 0.00001769
Iteration 149/1000 | Loss: 0.00001769
Iteration 150/1000 | Loss: 0.00001769
Iteration 151/1000 | Loss: 0.00001769
Iteration 152/1000 | Loss: 0.00001769
Iteration 153/1000 | Loss: 0.00001769
Iteration 154/1000 | Loss: 0.00001769
Iteration 155/1000 | Loss: 0.00001769
Iteration 156/1000 | Loss: 0.00001769
Iteration 157/1000 | Loss: 0.00001769
Iteration 158/1000 | Loss: 0.00001769
Iteration 159/1000 | Loss: 0.00001769
Iteration 160/1000 | Loss: 0.00001769
Iteration 161/1000 | Loss: 0.00001769
Iteration 162/1000 | Loss: 0.00001769
Iteration 163/1000 | Loss: 0.00001769
Iteration 164/1000 | Loss: 0.00001769
Iteration 165/1000 | Loss: 0.00001769
Iteration 166/1000 | Loss: 0.00001769
Iteration 167/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.768963920767419e-05, 1.768963920767419e-05, 1.768963920767419e-05, 1.768963920767419e-05, 1.768963920767419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.768963920767419e-05

Optimization complete. Final v2v error: 3.633434295654297 mm

Highest mean error: 3.900214433670044 mm for frame 96

Lowest mean error: 3.3504865169525146 mm for frame 217

Saving results

Total time: 48.99161648750305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772122
Iteration 2/25 | Loss: 0.00168202
Iteration 3/25 | Loss: 0.00152390
Iteration 4/25 | Loss: 0.00150605
Iteration 5/25 | Loss: 0.00150347
Iteration 6/25 | Loss: 0.00150347
Iteration 7/25 | Loss: 0.00150347
Iteration 8/25 | Loss: 0.00150347
Iteration 9/25 | Loss: 0.00150347
Iteration 10/25 | Loss: 0.00150347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015034694224596024, 0.0015034694224596024, 0.0015034694224596024, 0.0015034694224596024, 0.0015034694224596024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015034694224596024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20229721
Iteration 2/25 | Loss: 0.00233653
Iteration 3/25 | Loss: 0.00233653
Iteration 4/25 | Loss: 0.00233652
Iteration 5/25 | Loss: 0.00233652
Iteration 6/25 | Loss: 0.00233652
Iteration 7/25 | Loss: 0.00233652
Iteration 8/25 | Loss: 0.00233652
Iteration 9/25 | Loss: 0.00233652
Iteration 10/25 | Loss: 0.00233652
Iteration 11/25 | Loss: 0.00233652
Iteration 12/25 | Loss: 0.00233652
Iteration 13/25 | Loss: 0.00233652
Iteration 14/25 | Loss: 0.00233652
Iteration 15/25 | Loss: 0.00233652
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023365227971225977, 0.0023365227971225977, 0.0023365227971225977, 0.0023365227971225977, 0.0023365227971225977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023365227971225977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233652
Iteration 2/1000 | Loss: 0.00003614
Iteration 3/1000 | Loss: 0.00002758
Iteration 4/1000 | Loss: 0.00002481
Iteration 5/1000 | Loss: 0.00002331
Iteration 6/1000 | Loss: 0.00002235
Iteration 7/1000 | Loss: 0.00002167
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002019
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001924
Iteration 15/1000 | Loss: 0.00001921
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001906
Iteration 18/1000 | Loss: 0.00001900
Iteration 19/1000 | Loss: 0.00001895
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001894
Iteration 22/1000 | Loss: 0.00001893
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001890
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001885
Iteration 30/1000 | Loss: 0.00001883
Iteration 31/1000 | Loss: 0.00001883
Iteration 32/1000 | Loss: 0.00001882
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001880
Iteration 38/1000 | Loss: 0.00001880
Iteration 39/1000 | Loss: 0.00001879
Iteration 40/1000 | Loss: 0.00001879
Iteration 41/1000 | Loss: 0.00001878
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001876
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001873
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001872
Iteration 67/1000 | Loss: 0.00001872
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001869
Iteration 74/1000 | Loss: 0.00001869
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001868
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001864
Iteration 91/1000 | Loss: 0.00001864
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001863
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001860
Iteration 102/1000 | Loss: 0.00001860
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001856
Iteration 117/1000 | Loss: 0.00001856
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001856
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001854
Iteration 129/1000 | Loss: 0.00001854
Iteration 130/1000 | Loss: 0.00001854
Iteration 131/1000 | Loss: 0.00001854
Iteration 132/1000 | Loss: 0.00001854
Iteration 133/1000 | Loss: 0.00001854
Iteration 134/1000 | Loss: 0.00001854
Iteration 135/1000 | Loss: 0.00001854
Iteration 136/1000 | Loss: 0.00001854
Iteration 137/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.854301262937952e-05, 1.854301262937952e-05, 1.854301262937952e-05, 1.854301262937952e-05, 1.854301262937952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.854301262937952e-05

Optimization complete. Final v2v error: 3.6940560340881348 mm

Highest mean error: 4.446136474609375 mm for frame 175

Lowest mean error: 3.291667938232422 mm for frame 136

Saving results

Total time: 46.2620804309845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809859
Iteration 2/25 | Loss: 0.00178310
Iteration 3/25 | Loss: 0.00161364
Iteration 4/25 | Loss: 0.00160213
Iteration 5/25 | Loss: 0.00160157
Iteration 6/25 | Loss: 0.00160157
Iteration 7/25 | Loss: 0.00160157
Iteration 8/25 | Loss: 0.00160157
Iteration 9/25 | Loss: 0.00160157
Iteration 10/25 | Loss: 0.00160157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0016015716828405857, 0.0016015716828405857, 0.0016015716828405857, 0.0016015716828405857, 0.0016015716828405857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016015716828405857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74862361
Iteration 2/25 | Loss: 0.00200411
Iteration 3/25 | Loss: 0.00200408
Iteration 4/25 | Loss: 0.00200408
Iteration 5/25 | Loss: 0.00200408
Iteration 6/25 | Loss: 0.00200408
Iteration 7/25 | Loss: 0.00200408
Iteration 8/25 | Loss: 0.00200408
Iteration 9/25 | Loss: 0.00200408
Iteration 10/25 | Loss: 0.00200408
Iteration 11/25 | Loss: 0.00200408
Iteration 12/25 | Loss: 0.00200408
Iteration 13/25 | Loss: 0.00200408
Iteration 14/25 | Loss: 0.00200408
Iteration 15/25 | Loss: 0.00200408
Iteration 16/25 | Loss: 0.00200408
Iteration 17/25 | Loss: 0.00200408
Iteration 18/25 | Loss: 0.00200408
Iteration 19/25 | Loss: 0.00200408
Iteration 20/25 | Loss: 0.00200408
Iteration 21/25 | Loss: 0.00200408
Iteration 22/25 | Loss: 0.00200408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0020040790550410748, 0.0020040790550410748, 0.0020040790550410748, 0.0020040790550410748, 0.0020040790550410748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020040790550410748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200408
Iteration 2/1000 | Loss: 0.00004944
Iteration 3/1000 | Loss: 0.00003157
Iteration 4/1000 | Loss: 0.00002529
Iteration 5/1000 | Loss: 0.00002315
Iteration 6/1000 | Loss: 0.00002205
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002104
Iteration 9/1000 | Loss: 0.00002061
Iteration 10/1000 | Loss: 0.00002020
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001970
Iteration 13/1000 | Loss: 0.00001945
Iteration 14/1000 | Loss: 0.00001929
Iteration 15/1000 | Loss: 0.00001917
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001895
Iteration 21/1000 | Loss: 0.00001894
Iteration 22/1000 | Loss: 0.00001894
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001893
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001891
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001864
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001857
Iteration 60/1000 | Loss: 0.00001857
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001853
Iteration 70/1000 | Loss: 0.00001853
Iteration 71/1000 | Loss: 0.00001853
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001852
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001851
Iteration 76/1000 | Loss: 0.00001851
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001850
Iteration 85/1000 | Loss: 0.00001850
Iteration 86/1000 | Loss: 0.00001850
Iteration 87/1000 | Loss: 0.00001850
Iteration 88/1000 | Loss: 0.00001850
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00001849
Iteration 91/1000 | Loss: 0.00001849
Iteration 92/1000 | Loss: 0.00001848
Iteration 93/1000 | Loss: 0.00001848
Iteration 94/1000 | Loss: 0.00001848
Iteration 95/1000 | Loss: 0.00001848
Iteration 96/1000 | Loss: 0.00001847
Iteration 97/1000 | Loss: 0.00001847
Iteration 98/1000 | Loss: 0.00001847
Iteration 99/1000 | Loss: 0.00001847
Iteration 100/1000 | Loss: 0.00001847
Iteration 101/1000 | Loss: 0.00001847
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001845
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001842
Iteration 130/1000 | Loss: 0.00001842
Iteration 131/1000 | Loss: 0.00001842
Iteration 132/1000 | Loss: 0.00001842
Iteration 133/1000 | Loss: 0.00001842
Iteration 134/1000 | Loss: 0.00001841
Iteration 135/1000 | Loss: 0.00001841
Iteration 136/1000 | Loss: 0.00001841
Iteration 137/1000 | Loss: 0.00001841
Iteration 138/1000 | Loss: 0.00001841
Iteration 139/1000 | Loss: 0.00001841
Iteration 140/1000 | Loss: 0.00001841
Iteration 141/1000 | Loss: 0.00001841
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001840
Iteration 144/1000 | Loss: 0.00001840
Iteration 145/1000 | Loss: 0.00001840
Iteration 146/1000 | Loss: 0.00001840
Iteration 147/1000 | Loss: 0.00001840
Iteration 148/1000 | Loss: 0.00001840
Iteration 149/1000 | Loss: 0.00001840
Iteration 150/1000 | Loss: 0.00001840
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.8396900486550294e-05, 1.8396900486550294e-05, 1.8396900486550294e-05, 1.8396900486550294e-05, 1.8396900486550294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8396900486550294e-05

Optimization complete. Final v2v error: 3.6357429027557373 mm

Highest mean error: 3.957573890686035 mm for frame 137

Lowest mean error: 3.4302759170532227 mm for frame 5

Saving results

Total time: 47.95672011375427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455315
Iteration 2/25 | Loss: 0.00156620
Iteration 3/25 | Loss: 0.00149369
Iteration 4/25 | Loss: 0.00148329
Iteration 5/25 | Loss: 0.00148086
Iteration 6/25 | Loss: 0.00148086
Iteration 7/25 | Loss: 0.00148086
Iteration 8/25 | Loss: 0.00148086
Iteration 9/25 | Loss: 0.00148086
Iteration 10/25 | Loss: 0.00148086
Iteration 11/25 | Loss: 0.00148086
Iteration 12/25 | Loss: 0.00148086
Iteration 13/25 | Loss: 0.00148086
Iteration 14/25 | Loss: 0.00148086
Iteration 15/25 | Loss: 0.00148086
Iteration 16/25 | Loss: 0.00148086
Iteration 17/25 | Loss: 0.00148086
Iteration 18/25 | Loss: 0.00148086
Iteration 19/25 | Loss: 0.00148086
Iteration 20/25 | Loss: 0.00148086
Iteration 21/25 | Loss: 0.00148086
Iteration 22/25 | Loss: 0.00148086
Iteration 23/25 | Loss: 0.00148086
Iteration 24/25 | Loss: 0.00148086
Iteration 25/25 | Loss: 0.00148086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25127959
Iteration 2/25 | Loss: 0.00248666
Iteration 3/25 | Loss: 0.00248666
Iteration 4/25 | Loss: 0.00248666
Iteration 5/25 | Loss: 0.00248666
Iteration 6/25 | Loss: 0.00248666
Iteration 7/25 | Loss: 0.00248666
Iteration 8/25 | Loss: 0.00248666
Iteration 9/25 | Loss: 0.00248666
Iteration 10/25 | Loss: 0.00248666
Iteration 11/25 | Loss: 0.00248666
Iteration 12/25 | Loss: 0.00248666
Iteration 13/25 | Loss: 0.00248666
Iteration 14/25 | Loss: 0.00248666
Iteration 15/25 | Loss: 0.00248666
Iteration 16/25 | Loss: 0.00248666
Iteration 17/25 | Loss: 0.00248666
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024866561871021986, 0.0024866561871021986, 0.0024866561871021986, 0.0024866561871021986, 0.0024866561871021986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024866561871021986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248666
Iteration 2/1000 | Loss: 0.00002789
Iteration 3/1000 | Loss: 0.00002357
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001859
Iteration 9/1000 | Loss: 0.00001820
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001688
Iteration 17/1000 | Loss: 0.00001683
Iteration 18/1000 | Loss: 0.00001679
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001642
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001639
Iteration 58/1000 | Loss: 0.00001639
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001636
Iteration 75/1000 | Loss: 0.00001636
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001634
Iteration 78/1000 | Loss: 0.00001634
Iteration 79/1000 | Loss: 0.00001634
Iteration 80/1000 | Loss: 0.00001634
Iteration 81/1000 | Loss: 0.00001634
Iteration 82/1000 | Loss: 0.00001634
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001632
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001630
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001628
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001627
Iteration 109/1000 | Loss: 0.00001627
Iteration 110/1000 | Loss: 0.00001627
Iteration 111/1000 | Loss: 0.00001627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6274250810965896e-05, 1.6274250810965896e-05, 1.6274250810965896e-05, 1.6274250810965896e-05, 1.6274250810965896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6274250810965896e-05

Optimization complete. Final v2v error: 3.4525909423828125 mm

Highest mean error: 3.8870978355407715 mm for frame 130

Lowest mean error: 3.187013626098633 mm for frame 18

Saving results

Total time: 46.732279539108276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955367
Iteration 2/25 | Loss: 0.00955367
Iteration 3/25 | Loss: 0.00290404
Iteration 4/25 | Loss: 0.00227494
Iteration 5/25 | Loss: 0.00214340
Iteration 6/25 | Loss: 0.00204935
Iteration 7/25 | Loss: 0.00204905
Iteration 8/25 | Loss: 0.00203177
Iteration 9/25 | Loss: 0.00197340
Iteration 10/25 | Loss: 0.00194815
Iteration 11/25 | Loss: 0.00193210
Iteration 12/25 | Loss: 0.00191440
Iteration 13/25 | Loss: 0.00188943
Iteration 14/25 | Loss: 0.00186907
Iteration 15/25 | Loss: 0.00184384
Iteration 16/25 | Loss: 0.00184179
Iteration 17/25 | Loss: 0.00183881
Iteration 18/25 | Loss: 0.00184619
Iteration 19/25 | Loss: 0.00184306
Iteration 20/25 | Loss: 0.00180393
Iteration 21/25 | Loss: 0.00180949
Iteration 22/25 | Loss: 0.00177223
Iteration 23/25 | Loss: 0.00175665
Iteration 24/25 | Loss: 0.00175345
Iteration 25/25 | Loss: 0.00174697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23684788
Iteration 2/25 | Loss: 0.00678486
Iteration 3/25 | Loss: 0.00399794
Iteration 4/25 | Loss: 0.00396382
Iteration 5/25 | Loss: 0.00396382
Iteration 6/25 | Loss: 0.00396382
Iteration 7/25 | Loss: 0.00396382
Iteration 8/25 | Loss: 0.00396382
Iteration 9/25 | Loss: 0.00396381
Iteration 10/25 | Loss: 0.00396381
Iteration 11/25 | Loss: 0.00396381
Iteration 12/25 | Loss: 0.00396381
Iteration 13/25 | Loss: 0.00396381
Iteration 14/25 | Loss: 0.00396381
Iteration 15/25 | Loss: 0.00396381
Iteration 16/25 | Loss: 0.00396381
Iteration 17/25 | Loss: 0.00396381
Iteration 18/25 | Loss: 0.00396381
Iteration 19/25 | Loss: 0.00396381
Iteration 20/25 | Loss: 0.00396381
Iteration 21/25 | Loss: 0.00396381
Iteration 22/25 | Loss: 0.00396381
Iteration 23/25 | Loss: 0.00396381
Iteration 24/25 | Loss: 0.00396381
Iteration 25/25 | Loss: 0.00396381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00396381
Iteration 2/1000 | Loss: 0.00309074
Iteration 3/1000 | Loss: 0.01145271
Iteration 4/1000 | Loss: 0.00759121
Iteration 5/1000 | Loss: 0.00886382
Iteration 6/1000 | Loss: 0.00396968
Iteration 7/1000 | Loss: 0.00553154
Iteration 8/1000 | Loss: 0.00187191
Iteration 9/1000 | Loss: 0.00288803
Iteration 10/1000 | Loss: 0.00163133
Iteration 11/1000 | Loss: 0.00067812
Iteration 12/1000 | Loss: 0.00022236
Iteration 13/1000 | Loss: 0.00235735
Iteration 14/1000 | Loss: 0.00135451
Iteration 15/1000 | Loss: 0.00335011
Iteration 16/1000 | Loss: 0.00296066
Iteration 17/1000 | Loss: 0.00239591
Iteration 18/1000 | Loss: 0.00205376
Iteration 19/1000 | Loss: 0.00093115
Iteration 20/1000 | Loss: 0.00054361
Iteration 21/1000 | Loss: 0.00066538
Iteration 22/1000 | Loss: 0.00048053
Iteration 23/1000 | Loss: 0.00028977
Iteration 24/1000 | Loss: 0.00052486
Iteration 25/1000 | Loss: 0.00027435
Iteration 26/1000 | Loss: 0.00089054
Iteration 27/1000 | Loss: 0.00053857
Iteration 28/1000 | Loss: 0.00064871
Iteration 29/1000 | Loss: 0.00065799
Iteration 30/1000 | Loss: 0.00034748
Iteration 31/1000 | Loss: 0.00021968
Iteration 32/1000 | Loss: 0.00044330
Iteration 33/1000 | Loss: 0.00049435
Iteration 34/1000 | Loss: 0.00038772
Iteration 35/1000 | Loss: 0.00168857
Iteration 36/1000 | Loss: 0.00077688
Iteration 37/1000 | Loss: 0.00051600
Iteration 38/1000 | Loss: 0.00049160
Iteration 39/1000 | Loss: 0.00030301
Iteration 40/1000 | Loss: 0.00027643
Iteration 41/1000 | Loss: 0.00019348
Iteration 42/1000 | Loss: 0.00021470
Iteration 43/1000 | Loss: 0.00037375
Iteration 44/1000 | Loss: 0.00315185
Iteration 45/1000 | Loss: 0.00051191
Iteration 46/1000 | Loss: 0.00041433
Iteration 47/1000 | Loss: 0.00031747
Iteration 48/1000 | Loss: 0.00025261
Iteration 49/1000 | Loss: 0.00040252
Iteration 50/1000 | Loss: 0.00019793
Iteration 51/1000 | Loss: 0.00016223
Iteration 52/1000 | Loss: 0.00033177
Iteration 53/1000 | Loss: 0.00030355
Iteration 54/1000 | Loss: 0.00032513
Iteration 55/1000 | Loss: 0.00093886
Iteration 56/1000 | Loss: 0.00089322
Iteration 57/1000 | Loss: 0.00074224
Iteration 58/1000 | Loss: 0.00093709
Iteration 59/1000 | Loss: 0.00069879
Iteration 60/1000 | Loss: 0.00044093
Iteration 61/1000 | Loss: 0.00124736
Iteration 62/1000 | Loss: 0.00148744
Iteration 63/1000 | Loss: 0.00146424
Iteration 64/1000 | Loss: 0.00053101
Iteration 65/1000 | Loss: 0.00192645
Iteration 66/1000 | Loss: 0.00022335
Iteration 67/1000 | Loss: 0.00053477
Iteration 68/1000 | Loss: 0.00013812
Iteration 69/1000 | Loss: 0.00040014
Iteration 70/1000 | Loss: 0.00084422
Iteration 71/1000 | Loss: 0.00062885
Iteration 72/1000 | Loss: 0.00061770
Iteration 73/1000 | Loss: 0.00014220
Iteration 74/1000 | Loss: 0.00013667
Iteration 75/1000 | Loss: 0.00050868
Iteration 76/1000 | Loss: 0.00013403
Iteration 77/1000 | Loss: 0.00020019
Iteration 78/1000 | Loss: 0.00082821
Iteration 79/1000 | Loss: 0.00012021
Iteration 80/1000 | Loss: 0.00029265
Iteration 81/1000 | Loss: 0.00027934
Iteration 82/1000 | Loss: 0.00015116
Iteration 83/1000 | Loss: 0.00033504
Iteration 84/1000 | Loss: 0.00012485
Iteration 85/1000 | Loss: 0.00045006
Iteration 86/1000 | Loss: 0.00045086
Iteration 87/1000 | Loss: 0.00008871
Iteration 88/1000 | Loss: 0.00024338
Iteration 89/1000 | Loss: 0.00008451
Iteration 90/1000 | Loss: 0.00045133
Iteration 91/1000 | Loss: 0.00047513
Iteration 92/1000 | Loss: 0.00098083
Iteration 93/1000 | Loss: 0.00011414
Iteration 94/1000 | Loss: 0.00055768
Iteration 95/1000 | Loss: 0.00011679
Iteration 96/1000 | Loss: 0.00026607
Iteration 97/1000 | Loss: 0.00007251
Iteration 98/1000 | Loss: 0.00011733
Iteration 99/1000 | Loss: 0.00006008
Iteration 100/1000 | Loss: 0.00018191
Iteration 101/1000 | Loss: 0.00005319
Iteration 102/1000 | Loss: 0.00005155
Iteration 103/1000 | Loss: 0.00016749
Iteration 104/1000 | Loss: 0.00010913
Iteration 105/1000 | Loss: 0.00013169
Iteration 106/1000 | Loss: 0.00078695
Iteration 107/1000 | Loss: 0.00038300
Iteration 108/1000 | Loss: 0.00031217
Iteration 109/1000 | Loss: 0.00006144
Iteration 110/1000 | Loss: 0.00004973
Iteration 111/1000 | Loss: 0.00010028
Iteration 112/1000 | Loss: 0.00008317
Iteration 113/1000 | Loss: 0.00066844
Iteration 114/1000 | Loss: 0.00023819
Iteration 115/1000 | Loss: 0.00017238
Iteration 116/1000 | Loss: 0.00039022
Iteration 117/1000 | Loss: 0.00004276
Iteration 118/1000 | Loss: 0.00017008
Iteration 119/1000 | Loss: 0.00016996
Iteration 120/1000 | Loss: 0.00004484
Iteration 121/1000 | Loss: 0.00008354
Iteration 122/1000 | Loss: 0.00017759
Iteration 123/1000 | Loss: 0.00024644
Iteration 124/1000 | Loss: 0.00206233
Iteration 125/1000 | Loss: 0.00037720
Iteration 126/1000 | Loss: 0.00068410
Iteration 127/1000 | Loss: 0.00132534
Iteration 128/1000 | Loss: 0.00034658
Iteration 129/1000 | Loss: 0.00010222
Iteration 130/1000 | Loss: 0.00014536
Iteration 131/1000 | Loss: 0.00006194
Iteration 132/1000 | Loss: 0.00032069
Iteration 133/1000 | Loss: 0.00005191
Iteration 134/1000 | Loss: 0.00003880
Iteration 135/1000 | Loss: 0.00022385
Iteration 136/1000 | Loss: 0.00007512
Iteration 137/1000 | Loss: 0.00003593
Iteration 138/1000 | Loss: 0.00009088
Iteration 139/1000 | Loss: 0.00003481
Iteration 140/1000 | Loss: 0.00009910
Iteration 141/1000 | Loss: 0.00006298
Iteration 142/1000 | Loss: 0.00003622
Iteration 143/1000 | Loss: 0.00003073
Iteration 144/1000 | Loss: 0.00014321
Iteration 145/1000 | Loss: 0.00003647
Iteration 146/1000 | Loss: 0.00042521
Iteration 147/1000 | Loss: 0.00013216
Iteration 148/1000 | Loss: 0.00013524
Iteration 149/1000 | Loss: 0.00005121
Iteration 150/1000 | Loss: 0.00006449
Iteration 151/1000 | Loss: 0.00019078
Iteration 152/1000 | Loss: 0.00003420
Iteration 153/1000 | Loss: 0.00011785
Iteration 154/1000 | Loss: 0.00003065
Iteration 155/1000 | Loss: 0.00011583
Iteration 156/1000 | Loss: 0.00009496
Iteration 157/1000 | Loss: 0.00006304
Iteration 158/1000 | Loss: 0.00003118
Iteration 159/1000 | Loss: 0.00002907
Iteration 160/1000 | Loss: 0.00018690
Iteration 161/1000 | Loss: 0.00049617
Iteration 162/1000 | Loss: 0.00061290
Iteration 163/1000 | Loss: 0.00023391
Iteration 164/1000 | Loss: 0.00004568
Iteration 165/1000 | Loss: 0.00008908
Iteration 166/1000 | Loss: 0.00005171
Iteration 167/1000 | Loss: 0.00003616
Iteration 168/1000 | Loss: 0.00002987
Iteration 169/1000 | Loss: 0.00002735
Iteration 170/1000 | Loss: 0.00004212
Iteration 171/1000 | Loss: 0.00031901
Iteration 172/1000 | Loss: 0.00031909
Iteration 173/1000 | Loss: 0.00005459
Iteration 174/1000 | Loss: 0.00003553
Iteration 175/1000 | Loss: 0.00019176
Iteration 176/1000 | Loss: 0.00012570
Iteration 177/1000 | Loss: 0.00006328
Iteration 178/1000 | Loss: 0.00003600
Iteration 179/1000 | Loss: 0.00002646
Iteration 180/1000 | Loss: 0.00002564
Iteration 181/1000 | Loss: 0.00002569
Iteration 182/1000 | Loss: 0.00002451
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002416
Iteration 185/1000 | Loss: 0.00007264
Iteration 186/1000 | Loss: 0.00002921
Iteration 187/1000 | Loss: 0.00005258
Iteration 188/1000 | Loss: 0.00002348
Iteration 189/1000 | Loss: 0.00002963
Iteration 190/1000 | Loss: 0.00002421
Iteration 191/1000 | Loss: 0.00021795
Iteration 192/1000 | Loss: 0.00008815
Iteration 193/1000 | Loss: 0.00010198
Iteration 194/1000 | Loss: 0.00007645
Iteration 195/1000 | Loss: 0.00007647
Iteration 196/1000 | Loss: 0.00005453
Iteration 197/1000 | Loss: 0.00005210
Iteration 198/1000 | Loss: 0.00007442
Iteration 199/1000 | Loss: 0.00002586
Iteration 200/1000 | Loss: 0.00002433
Iteration 201/1000 | Loss: 0.00002387
Iteration 202/1000 | Loss: 0.00002223
Iteration 203/1000 | Loss: 0.00002248
Iteration 204/1000 | Loss: 0.00004909
Iteration 205/1000 | Loss: 0.00036142
Iteration 206/1000 | Loss: 0.00007846
Iteration 207/1000 | Loss: 0.00013732
Iteration 208/1000 | Loss: 0.00003052
Iteration 209/1000 | Loss: 0.00007298
Iteration 210/1000 | Loss: 0.00004748
Iteration 211/1000 | Loss: 0.00002218
Iteration 212/1000 | Loss: 0.00005963
Iteration 213/1000 | Loss: 0.00003832
Iteration 214/1000 | Loss: 0.00008146
Iteration 215/1000 | Loss: 0.00005647
Iteration 216/1000 | Loss: 0.00005712
Iteration 217/1000 | Loss: 0.00003440
Iteration 218/1000 | Loss: 0.00004460
Iteration 219/1000 | Loss: 0.00002041
Iteration 220/1000 | Loss: 0.00002013
Iteration 221/1000 | Loss: 0.00001951
Iteration 222/1000 | Loss: 0.00003146
Iteration 223/1000 | Loss: 0.00002816
Iteration 224/1000 | Loss: 0.00002117
Iteration 225/1000 | Loss: 0.00002134
Iteration 226/1000 | Loss: 0.00002419
Iteration 227/1000 | Loss: 0.00002013
Iteration 228/1000 | Loss: 0.00002569
Iteration 229/1000 | Loss: 0.00001979
Iteration 230/1000 | Loss: 0.00002150
Iteration 231/1000 | Loss: 0.00001963
Iteration 232/1000 | Loss: 0.00001948
Iteration 233/1000 | Loss: 0.00002387
Iteration 234/1000 | Loss: 0.00002152
Iteration 235/1000 | Loss: 0.00002035
Iteration 236/1000 | Loss: 0.00002034
Iteration 237/1000 | Loss: 0.00004068
Iteration 238/1000 | Loss: 0.00002015
Iteration 239/1000 | Loss: 0.00002302
Iteration 240/1000 | Loss: 0.00001982
Iteration 241/1000 | Loss: 0.00004198
Iteration 242/1000 | Loss: 0.00002291
Iteration 243/1000 | Loss: 0.00003245
Iteration 244/1000 | Loss: 0.00002019
Iteration 245/1000 | Loss: 0.00002740
Iteration 246/1000 | Loss: 0.00002001
Iteration 247/1000 | Loss: 0.00002052
Iteration 248/1000 | Loss: 0.00001964
Iteration 249/1000 | Loss: 0.00001944
Iteration 250/1000 | Loss: 0.00001863
Iteration 251/1000 | Loss: 0.00001902
Iteration 252/1000 | Loss: 0.00001913
Iteration 253/1000 | Loss: 0.00001913
Iteration 254/1000 | Loss: 0.00004239
Iteration 255/1000 | Loss: 0.00001952
Iteration 256/1000 | Loss: 0.00003928
Iteration 257/1000 | Loss: 0.00001946
Iteration 258/1000 | Loss: 0.00001954
Iteration 259/1000 | Loss: 0.00006265
Iteration 260/1000 | Loss: 0.00024078
Iteration 261/1000 | Loss: 0.00008766
Iteration 262/1000 | Loss: 0.00006447
Iteration 263/1000 | Loss: 0.00014522
Iteration 264/1000 | Loss: 0.00016030
Iteration 265/1000 | Loss: 0.00002719
Iteration 266/1000 | Loss: 0.00003104
Iteration 267/1000 | Loss: 0.00002419
Iteration 268/1000 | Loss: 0.00004126
Iteration 269/1000 | Loss: 0.00002232
Iteration 270/1000 | Loss: 0.00002093
Iteration 271/1000 | Loss: 0.00002021
Iteration 272/1000 | Loss: 0.00002122
Iteration 273/1000 | Loss: 0.00001925
Iteration 274/1000 | Loss: 0.00005276
Iteration 275/1000 | Loss: 0.00002870
Iteration 276/1000 | Loss: 0.00004530
Iteration 277/1000 | Loss: 0.00002318
Iteration 278/1000 | Loss: 0.00004645
Iteration 279/1000 | Loss: 0.00002405
Iteration 280/1000 | Loss: 0.00001939
Iteration 281/1000 | Loss: 0.00005662
Iteration 282/1000 | Loss: 0.00002640
Iteration 283/1000 | Loss: 0.00003304
Iteration 284/1000 | Loss: 0.00002420
Iteration 285/1000 | Loss: 0.00006348
Iteration 286/1000 | Loss: 0.00002923
Iteration 287/1000 | Loss: 0.00004503
Iteration 288/1000 | Loss: 0.00002125
Iteration 289/1000 | Loss: 0.00002092
Iteration 290/1000 | Loss: 0.00002277
Iteration 291/1000 | Loss: 0.00002011
Iteration 292/1000 | Loss: 0.00001967
Iteration 293/1000 | Loss: 0.00001966
Iteration 294/1000 | Loss: 0.00001995
Iteration 295/1000 | Loss: 0.00002054
Iteration 296/1000 | Loss: 0.00002359
Iteration 297/1000 | Loss: 0.00003513
Iteration 298/1000 | Loss: 0.00010236
Iteration 299/1000 | Loss: 0.00006794
Iteration 300/1000 | Loss: 0.00003536
Iteration 301/1000 | Loss: 0.00001969
Iteration 302/1000 | Loss: 0.00002341
Iteration 303/1000 | Loss: 0.00001935
Iteration 304/1000 | Loss: 0.00002021
Iteration 305/1000 | Loss: 0.00002157
Iteration 306/1000 | Loss: 0.00003861
Iteration 307/1000 | Loss: 0.00002116
Iteration 308/1000 | Loss: 0.00006442
Iteration 309/1000 | Loss: 0.00001983
Iteration 310/1000 | Loss: 0.00004265
Iteration 311/1000 | Loss: 0.00001957
Iteration 312/1000 | Loss: 0.00001974
Iteration 313/1000 | Loss: 0.00001938
Iteration 314/1000 | Loss: 0.00006361
Iteration 315/1000 | Loss: 0.00002461
Iteration 316/1000 | Loss: 0.00001965
Iteration 317/1000 | Loss: 0.00001949
Iteration 318/1000 | Loss: 0.00001958
Iteration 319/1000 | Loss: 0.00001926
Iteration 320/1000 | Loss: 0.00001949
Iteration 321/1000 | Loss: 0.00003990
Iteration 322/1000 | Loss: 0.00001925
Iteration 323/1000 | Loss: 0.00002245
Iteration 324/1000 | Loss: 0.00003594
Iteration 325/1000 | Loss: 0.00002118
Iteration 326/1000 | Loss: 0.00001937
Iteration 327/1000 | Loss: 0.00001936
Iteration 328/1000 | Loss: 0.00001951
Iteration 329/1000 | Loss: 0.00001936
Iteration 330/1000 | Loss: 0.00001935
Iteration 331/1000 | Loss: 0.00001954
Iteration 332/1000 | Loss: 0.00002354
Iteration 333/1000 | Loss: 0.00003990
Iteration 334/1000 | Loss: 0.00002559
Iteration 335/1000 | Loss: 0.00001943
Iteration 336/1000 | Loss: 0.00005612
Iteration 337/1000 | Loss: 0.00001955
Iteration 338/1000 | Loss: 0.00002856
Iteration 339/1000 | Loss: 0.00001978
Iteration 340/1000 | Loss: 0.00002188
Iteration 341/1000 | Loss: 0.00001976
Iteration 342/1000 | Loss: 0.00002050
Iteration 343/1000 | Loss: 0.00001963
Iteration 344/1000 | Loss: 0.00001955
Iteration 345/1000 | Loss: 0.00002449
Iteration 346/1000 | Loss: 0.00001963
Iteration 347/1000 | Loss: 0.00001990
Iteration 348/1000 | Loss: 0.00002095
Iteration 349/1000 | Loss: 0.00001961
Iteration 350/1000 | Loss: 0.00002045
Iteration 351/1000 | Loss: 0.00001939
Iteration 352/1000 | Loss: 0.00001955
Iteration 353/1000 | Loss: 0.00005209
Iteration 354/1000 | Loss: 0.00001944
Iteration 355/1000 | Loss: 0.00004337
Iteration 356/1000 | Loss: 0.00003092
Iteration 357/1000 | Loss: 0.00002942
Iteration 358/1000 | Loss: 0.00001997
Iteration 359/1000 | Loss: 0.00002356
Iteration 360/1000 | Loss: 0.00001956
Iteration 361/1000 | Loss: 0.00001928
Iteration 362/1000 | Loss: 0.00006920
Iteration 363/1000 | Loss: 0.00002459
Iteration 364/1000 | Loss: 0.00001971
Iteration 365/1000 | Loss: 0.00001944
Iteration 366/1000 | Loss: 0.00001962
Iteration 367/1000 | Loss: 0.00001927
Iteration 368/1000 | Loss: 0.00001944
Iteration 369/1000 | Loss: 0.00001925
Iteration 370/1000 | Loss: 0.00001948
Iteration 371/1000 | Loss: 0.00001818
Iteration 372/1000 | Loss: 0.00001923
Iteration 373/1000 | Loss: 0.00001928
Iteration 374/1000 | Loss: 0.00001873
Iteration 375/1000 | Loss: 0.00002652
Iteration 376/1000 | Loss: 0.00002019
Iteration 377/1000 | Loss: 0.00001954
Iteration 378/1000 | Loss: 0.00001924
Iteration 379/1000 | Loss: 0.00001950
Iteration 380/1000 | Loss: 0.00008936
Iteration 381/1000 | Loss: 0.00001811
Iteration 382/1000 | Loss: 0.00005108
Iteration 383/1000 | Loss: 0.00002968
Iteration 384/1000 | Loss: 0.00002535
Iteration 385/1000 | Loss: 0.00001968
Iteration 386/1000 | Loss: 0.00001947
Iteration 387/1000 | Loss: 0.00002525
Iteration 388/1000 | Loss: 0.00001996
Iteration 389/1000 | Loss: 0.00002882
Iteration 390/1000 | Loss: 0.00005400
Iteration 391/1000 | Loss: 0.00001964
Iteration 392/1000 | Loss: 0.00001945
Iteration 393/1000 | Loss: 0.00001960
Iteration 394/1000 | Loss: 0.00001945
Iteration 395/1000 | Loss: 0.00002787
Iteration 396/1000 | Loss: 0.00002152
Iteration 397/1000 | Loss: 0.00001954
Iteration 398/1000 | Loss: 0.00001937
Iteration 399/1000 | Loss: 0.00001946
Iteration 400/1000 | Loss: 0.00001943
Iteration 401/1000 | Loss: 0.00001954
Iteration 402/1000 | Loss: 0.00003111
Iteration 403/1000 | Loss: 0.00004931
Iteration 404/1000 | Loss: 0.00001947
Iteration 405/1000 | Loss: 0.00001947
Iteration 406/1000 | Loss: 0.00001947
Iteration 407/1000 | Loss: 0.00001947
Iteration 408/1000 | Loss: 0.00001946
Iteration 409/1000 | Loss: 0.00001946
Iteration 410/1000 | Loss: 0.00001946
Iteration 411/1000 | Loss: 0.00001946
Iteration 412/1000 | Loss: 0.00001946
Iteration 413/1000 | Loss: 0.00001946
Iteration 414/1000 | Loss: 0.00001946
Iteration 415/1000 | Loss: 0.00001945
Iteration 416/1000 | Loss: 0.00001945
Iteration 417/1000 | Loss: 0.00002706
Iteration 418/1000 | Loss: 0.00004942
Iteration 419/1000 | Loss: 0.00008617
Iteration 420/1000 | Loss: 0.00009189
Iteration 421/1000 | Loss: 0.00006665
Iteration 422/1000 | Loss: 0.00001908
Iteration 423/1000 | Loss: 0.00006027
Iteration 424/1000 | Loss: 0.00012755
Iteration 425/1000 | Loss: 0.00001871
Iteration 426/1000 | Loss: 0.00003345
Iteration 427/1000 | Loss: 0.00003471
Iteration 428/1000 | Loss: 0.00001812
Iteration 429/1000 | Loss: 0.00001807
Iteration 430/1000 | Loss: 0.00004857
Iteration 431/1000 | Loss: 0.00001787
Iteration 432/1000 | Loss: 0.00001784
Iteration 433/1000 | Loss: 0.00001783
Iteration 434/1000 | Loss: 0.00001783
Iteration 435/1000 | Loss: 0.00001782
Iteration 436/1000 | Loss: 0.00001780
Iteration 437/1000 | Loss: 0.00001780
Iteration 438/1000 | Loss: 0.00001779
Iteration 439/1000 | Loss: 0.00001777
Iteration 440/1000 | Loss: 0.00001777
Iteration 441/1000 | Loss: 0.00001776
Iteration 442/1000 | Loss: 0.00001772
Iteration 443/1000 | Loss: 0.00001771
Iteration 444/1000 | Loss: 0.00001771
Iteration 445/1000 | Loss: 0.00001771
Iteration 446/1000 | Loss: 0.00001769
Iteration 447/1000 | Loss: 0.00001769
Iteration 448/1000 | Loss: 0.00001768
Iteration 449/1000 | Loss: 0.00001768
Iteration 450/1000 | Loss: 0.00001768
Iteration 451/1000 | Loss: 0.00001768
Iteration 452/1000 | Loss: 0.00001767
Iteration 453/1000 | Loss: 0.00001767
Iteration 454/1000 | Loss: 0.00001767
Iteration 455/1000 | Loss: 0.00001767
Iteration 456/1000 | Loss: 0.00001767
Iteration 457/1000 | Loss: 0.00001767
Iteration 458/1000 | Loss: 0.00001767
Iteration 459/1000 | Loss: 0.00001767
Iteration 460/1000 | Loss: 0.00001767
Iteration 461/1000 | Loss: 0.00001767
Iteration 462/1000 | Loss: 0.00001767
Iteration 463/1000 | Loss: 0.00001767
Iteration 464/1000 | Loss: 0.00001766
Iteration 465/1000 | Loss: 0.00001766
Iteration 466/1000 | Loss: 0.00001766
Iteration 467/1000 | Loss: 0.00001766
Iteration 468/1000 | Loss: 0.00001766
Iteration 469/1000 | Loss: 0.00001766
Iteration 470/1000 | Loss: 0.00001766
Iteration 471/1000 | Loss: 0.00001766
Iteration 472/1000 | Loss: 0.00001766
Iteration 473/1000 | Loss: 0.00001766
Iteration 474/1000 | Loss: 0.00001766
Iteration 475/1000 | Loss: 0.00001766
Iteration 476/1000 | Loss: 0.00001766
Iteration 477/1000 | Loss: 0.00001766
Iteration 478/1000 | Loss: 0.00001766
Iteration 479/1000 | Loss: 0.00001766
Iteration 480/1000 | Loss: 0.00001766
Iteration 481/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 481. Stopping optimization.
Last 5 losses: [1.7663889593677595e-05, 1.7663889593677595e-05, 1.7663889593677595e-05, 1.7663889593677595e-05, 1.7663889593677595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7663889593677595e-05

Optimization complete. Final v2v error: 3.368039131164551 mm

Highest mean error: 10.243610382080078 mm for frame 37

Lowest mean error: 2.955272674560547 mm for frame 67

Saving results

Total time: 707.9354071617126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982081
Iteration 2/25 | Loss: 0.00561118
Iteration 3/25 | Loss: 0.00255761
Iteration 4/25 | Loss: 0.00208927
Iteration 5/25 | Loss: 0.00216603
Iteration 6/25 | Loss: 0.00187099
Iteration 7/25 | Loss: 0.00164464
Iteration 8/25 | Loss: 0.00157455
Iteration 9/25 | Loss: 0.00162217
Iteration 10/25 | Loss: 0.00154011
Iteration 11/25 | Loss: 0.00151193
Iteration 12/25 | Loss: 0.00151297
Iteration 13/25 | Loss: 0.00150042
Iteration 14/25 | Loss: 0.00149353
Iteration 15/25 | Loss: 0.00148411
Iteration 16/25 | Loss: 0.00148078
Iteration 17/25 | Loss: 0.00147930
Iteration 18/25 | Loss: 0.00148041
Iteration 19/25 | Loss: 0.00147962
Iteration 20/25 | Loss: 0.00147890
Iteration 21/25 | Loss: 0.00147888
Iteration 22/25 | Loss: 0.00147888
Iteration 23/25 | Loss: 0.00147888
Iteration 24/25 | Loss: 0.00147888
Iteration 25/25 | Loss: 0.00147888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22444475
Iteration 2/25 | Loss: 0.00272781
Iteration 3/25 | Loss: 0.00270221
Iteration 4/25 | Loss: 0.00270221
Iteration 5/25 | Loss: 0.00270221
Iteration 6/25 | Loss: 0.00270221
Iteration 7/25 | Loss: 0.00270221
Iteration 8/25 | Loss: 0.00270221
Iteration 9/25 | Loss: 0.00270221
Iteration 10/25 | Loss: 0.00270221
Iteration 11/25 | Loss: 0.00270221
Iteration 12/25 | Loss: 0.00270221
Iteration 13/25 | Loss: 0.00270221
Iteration 14/25 | Loss: 0.00270221
Iteration 15/25 | Loss: 0.00270221
Iteration 16/25 | Loss: 0.00270221
Iteration 17/25 | Loss: 0.00270220
Iteration 18/25 | Loss: 0.00270220
Iteration 19/25 | Loss: 0.00270220
Iteration 20/25 | Loss: 0.00270220
Iteration 21/25 | Loss: 0.00270220
Iteration 22/25 | Loss: 0.00270220
Iteration 23/25 | Loss: 0.00270220
Iteration 24/25 | Loss: 0.00270220
Iteration 25/25 | Loss: 0.00270220

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270220
Iteration 2/1000 | Loss: 0.00007700
Iteration 3/1000 | Loss: 0.00004500
Iteration 4/1000 | Loss: 0.00003382
Iteration 5/1000 | Loss: 0.00003361
Iteration 6/1000 | Loss: 0.00003450
Iteration 7/1000 | Loss: 0.00003004
Iteration 8/1000 | Loss: 0.00002979
Iteration 9/1000 | Loss: 0.00003051
Iteration 10/1000 | Loss: 0.00002756
Iteration 11/1000 | Loss: 0.00003776
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002682
Iteration 14/1000 | Loss: 0.00002836
Iteration 15/1000 | Loss: 0.00002881
Iteration 16/1000 | Loss: 0.00003091
Iteration 17/1000 | Loss: 0.00004767
Iteration 18/1000 | Loss: 0.00002642
Iteration 19/1000 | Loss: 0.00002904
Iteration 20/1000 | Loss: 0.00004038
Iteration 21/1000 | Loss: 0.00004110
Iteration 22/1000 | Loss: 0.00003060
Iteration 23/1000 | Loss: 0.00004587
Iteration 24/1000 | Loss: 0.00002559
Iteration 25/1000 | Loss: 0.00002555
Iteration 26/1000 | Loss: 0.00002555
Iteration 27/1000 | Loss: 0.00002555
Iteration 28/1000 | Loss: 0.00002554
Iteration 29/1000 | Loss: 0.00002571
Iteration 30/1000 | Loss: 0.00002571
Iteration 31/1000 | Loss: 0.00002867
Iteration 32/1000 | Loss: 0.00005824
Iteration 33/1000 | Loss: 0.00002698
Iteration 34/1000 | Loss: 0.00002797
Iteration 35/1000 | Loss: 0.00002533
Iteration 36/1000 | Loss: 0.00002546
Iteration 37/1000 | Loss: 0.00002546
Iteration 38/1000 | Loss: 0.00002545
Iteration 39/1000 | Loss: 0.00002542
Iteration 40/1000 | Loss: 0.00002523
Iteration 41/1000 | Loss: 0.00002523
Iteration 42/1000 | Loss: 0.00002523
Iteration 43/1000 | Loss: 0.00002523
Iteration 44/1000 | Loss: 0.00002522
Iteration 45/1000 | Loss: 0.00002522
Iteration 46/1000 | Loss: 0.00002522
Iteration 47/1000 | Loss: 0.00002522
Iteration 48/1000 | Loss: 0.00002521
Iteration 49/1000 | Loss: 0.00002521
Iteration 50/1000 | Loss: 0.00002521
Iteration 51/1000 | Loss: 0.00002521
Iteration 52/1000 | Loss: 0.00002521
Iteration 53/1000 | Loss: 0.00002521
Iteration 54/1000 | Loss: 0.00002521
Iteration 55/1000 | Loss: 0.00002521
Iteration 56/1000 | Loss: 0.00002520
Iteration 57/1000 | Loss: 0.00002520
Iteration 58/1000 | Loss: 0.00002520
Iteration 59/1000 | Loss: 0.00002520
Iteration 60/1000 | Loss: 0.00002520
Iteration 61/1000 | Loss: 0.00002519
Iteration 62/1000 | Loss: 0.00002519
Iteration 63/1000 | Loss: 0.00002519
Iteration 64/1000 | Loss: 0.00002519
Iteration 65/1000 | Loss: 0.00002519
Iteration 66/1000 | Loss: 0.00002519
Iteration 67/1000 | Loss: 0.00002519
Iteration 68/1000 | Loss: 0.00002818
Iteration 69/1000 | Loss: 0.00002514
Iteration 70/1000 | Loss: 0.00002514
Iteration 71/1000 | Loss: 0.00002513
Iteration 72/1000 | Loss: 0.00002513
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002594
Iteration 75/1000 | Loss: 0.00002646
Iteration 76/1000 | Loss: 0.00002648
Iteration 77/1000 | Loss: 0.00002537
Iteration 78/1000 | Loss: 0.00003418
Iteration 79/1000 | Loss: 0.00002503
Iteration 80/1000 | Loss: 0.00002503
Iteration 81/1000 | Loss: 0.00002503
Iteration 82/1000 | Loss: 0.00002503
Iteration 83/1000 | Loss: 0.00002503
Iteration 84/1000 | Loss: 0.00002503
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002503
Iteration 87/1000 | Loss: 0.00002502
Iteration 88/1000 | Loss: 0.00002502
Iteration 89/1000 | Loss: 0.00002502
Iteration 90/1000 | Loss: 0.00002501
Iteration 91/1000 | Loss: 0.00002499
Iteration 92/1000 | Loss: 0.00002500
Iteration 93/1000 | Loss: 0.00002500
Iteration 94/1000 | Loss: 0.00002499
Iteration 95/1000 | Loss: 0.00002498
Iteration 96/1000 | Loss: 0.00002498
Iteration 97/1000 | Loss: 0.00002498
Iteration 98/1000 | Loss: 0.00002497
Iteration 99/1000 | Loss: 0.00002497
Iteration 100/1000 | Loss: 0.00002497
Iteration 101/1000 | Loss: 0.00002497
Iteration 102/1000 | Loss: 0.00002497
Iteration 103/1000 | Loss: 0.00002497
Iteration 104/1000 | Loss: 0.00002496
Iteration 105/1000 | Loss: 0.00002496
Iteration 106/1000 | Loss: 0.00002496
Iteration 107/1000 | Loss: 0.00002496
Iteration 108/1000 | Loss: 0.00002496
Iteration 109/1000 | Loss: 0.00002496
Iteration 110/1000 | Loss: 0.00002496
Iteration 111/1000 | Loss: 0.00002496
Iteration 112/1000 | Loss: 0.00002496
Iteration 113/1000 | Loss: 0.00002496
Iteration 114/1000 | Loss: 0.00002610
Iteration 115/1000 | Loss: 0.00002774
Iteration 116/1000 | Loss: 0.00003063
Iteration 117/1000 | Loss: 0.00002580
Iteration 118/1000 | Loss: 0.00002774
Iteration 119/1000 | Loss: 0.00002492
Iteration 120/1000 | Loss: 0.00002490
Iteration 121/1000 | Loss: 0.00002490
Iteration 122/1000 | Loss: 0.00002490
Iteration 123/1000 | Loss: 0.00002490
Iteration 124/1000 | Loss: 0.00002490
Iteration 125/1000 | Loss: 0.00002490
Iteration 126/1000 | Loss: 0.00002490
Iteration 127/1000 | Loss: 0.00002490
Iteration 128/1000 | Loss: 0.00002490
Iteration 129/1000 | Loss: 0.00002490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.4899567506508902e-05, 2.4899567506508902e-05, 2.4899567506508902e-05, 2.4899567506508902e-05, 2.4899567506508902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4899567506508902e-05

Optimization complete. Final v2v error: 3.4450843334198 mm

Highest mean error: 9.887608528137207 mm for frame 29

Lowest mean error: 2.756197929382324 mm for frame 14

Saving results

Total time: 97.19385814666748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382158
Iteration 2/25 | Loss: 0.00158474
Iteration 3/25 | Loss: 0.00147567
Iteration 4/25 | Loss: 0.00146266
Iteration 5/25 | Loss: 0.00145876
Iteration 6/25 | Loss: 0.00145876
Iteration 7/25 | Loss: 0.00145876
Iteration 8/25 | Loss: 0.00145876
Iteration 9/25 | Loss: 0.00145876
Iteration 10/25 | Loss: 0.00145876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014587579062208533, 0.0014587579062208533, 0.0014587579062208533, 0.0014587579062208533, 0.0014587579062208533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014587579062208533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65918422
Iteration 2/25 | Loss: 0.00269863
Iteration 3/25 | Loss: 0.00269863
Iteration 4/25 | Loss: 0.00269863
Iteration 5/25 | Loss: 0.00269863
Iteration 6/25 | Loss: 0.00269862
Iteration 7/25 | Loss: 0.00269862
Iteration 8/25 | Loss: 0.00269862
Iteration 9/25 | Loss: 0.00269862
Iteration 10/25 | Loss: 0.00269862
Iteration 11/25 | Loss: 0.00269862
Iteration 12/25 | Loss: 0.00269862
Iteration 13/25 | Loss: 0.00269862
Iteration 14/25 | Loss: 0.00269862
Iteration 15/25 | Loss: 0.00269862
Iteration 16/25 | Loss: 0.00269862
Iteration 17/25 | Loss: 0.00269862
Iteration 18/25 | Loss: 0.00269862
Iteration 19/25 | Loss: 0.00269862
Iteration 20/25 | Loss: 0.00269862
Iteration 21/25 | Loss: 0.00269862
Iteration 22/25 | Loss: 0.00269862
Iteration 23/25 | Loss: 0.00269862
Iteration 24/25 | Loss: 0.00269862
Iteration 25/25 | Loss: 0.00269862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269862
Iteration 2/1000 | Loss: 0.00003009
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00001966
Iteration 5/1000 | Loss: 0.00001796
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001637
Iteration 8/1000 | Loss: 0.00001584
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001484
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001345
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001336
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001332
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001331
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001327
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001324
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001320
Iteration 65/1000 | Loss: 0.00001320
Iteration 66/1000 | Loss: 0.00001319
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001313
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001310
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001307
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001306
Iteration 90/1000 | Loss: 0.00001306
Iteration 91/1000 | Loss: 0.00001306
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001305
Iteration 102/1000 | Loss: 0.00001305
Iteration 103/1000 | Loss: 0.00001305
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001304
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001302
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001301
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001300
Iteration 142/1000 | Loss: 0.00001300
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Iteration 167/1000 | Loss: 0.00001298
Iteration 168/1000 | Loss: 0.00001298
Iteration 169/1000 | Loss: 0.00001298
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001298
Iteration 176/1000 | Loss: 0.00001298
Iteration 177/1000 | Loss: 0.00001298
Iteration 178/1000 | Loss: 0.00001297
Iteration 179/1000 | Loss: 0.00001297
Iteration 180/1000 | Loss: 0.00001297
Iteration 181/1000 | Loss: 0.00001297
Iteration 182/1000 | Loss: 0.00001297
Iteration 183/1000 | Loss: 0.00001297
Iteration 184/1000 | Loss: 0.00001297
Iteration 185/1000 | Loss: 0.00001297
Iteration 186/1000 | Loss: 0.00001297
Iteration 187/1000 | Loss: 0.00001297
Iteration 188/1000 | Loss: 0.00001297
Iteration 189/1000 | Loss: 0.00001297
Iteration 190/1000 | Loss: 0.00001297
Iteration 191/1000 | Loss: 0.00001297
Iteration 192/1000 | Loss: 0.00001296
Iteration 193/1000 | Loss: 0.00001296
Iteration 194/1000 | Loss: 0.00001296
Iteration 195/1000 | Loss: 0.00001296
Iteration 196/1000 | Loss: 0.00001296
Iteration 197/1000 | Loss: 0.00001296
Iteration 198/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.296478421863867e-05, 1.296478421863867e-05, 1.296478421863867e-05, 1.296478421863867e-05, 1.296478421863867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.296478421863867e-05

Optimization complete. Final v2v error: 3.1406052112579346 mm

Highest mean error: 3.51145601272583 mm for frame 75

Lowest mean error: 2.7397115230560303 mm for frame 10

Saving results

Total time: 52.05879521369934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807310
Iteration 2/25 | Loss: 0.00156334
Iteration 3/25 | Loss: 0.00147607
Iteration 4/25 | Loss: 0.00146602
Iteration 5/25 | Loss: 0.00146406
Iteration 6/25 | Loss: 0.00146404
Iteration 7/25 | Loss: 0.00146404
Iteration 8/25 | Loss: 0.00146404
Iteration 9/25 | Loss: 0.00146404
Iteration 10/25 | Loss: 0.00146404
Iteration 11/25 | Loss: 0.00146404
Iteration 12/25 | Loss: 0.00146404
Iteration 13/25 | Loss: 0.00146404
Iteration 14/25 | Loss: 0.00146404
Iteration 15/25 | Loss: 0.00146404
Iteration 16/25 | Loss: 0.00146404
Iteration 17/25 | Loss: 0.00146404
Iteration 18/25 | Loss: 0.00146404
Iteration 19/25 | Loss: 0.00146404
Iteration 20/25 | Loss: 0.00146404
Iteration 21/25 | Loss: 0.00146404
Iteration 22/25 | Loss: 0.00146404
Iteration 23/25 | Loss: 0.00146404
Iteration 24/25 | Loss: 0.00146404
Iteration 25/25 | Loss: 0.00146404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23413599
Iteration 2/25 | Loss: 0.00214579
Iteration 3/25 | Loss: 0.00214577
Iteration 4/25 | Loss: 0.00214577
Iteration 5/25 | Loss: 0.00214577
Iteration 6/25 | Loss: 0.00214577
Iteration 7/25 | Loss: 0.00214577
Iteration 8/25 | Loss: 0.00214577
Iteration 9/25 | Loss: 0.00214577
Iteration 10/25 | Loss: 0.00214577
Iteration 11/25 | Loss: 0.00214577
Iteration 12/25 | Loss: 0.00214577
Iteration 13/25 | Loss: 0.00214577
Iteration 14/25 | Loss: 0.00214577
Iteration 15/25 | Loss: 0.00214577
Iteration 16/25 | Loss: 0.00214577
Iteration 17/25 | Loss: 0.00214577
Iteration 18/25 | Loss: 0.00214577
Iteration 19/25 | Loss: 0.00214577
Iteration 20/25 | Loss: 0.00214577
Iteration 21/25 | Loss: 0.00214577
Iteration 22/25 | Loss: 0.00214577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0021457660477608442, 0.0021457660477608442, 0.0021457660477608442, 0.0021457660477608442, 0.0021457660477608442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021457660477608442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214577
Iteration 2/1000 | Loss: 0.00002390
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001510
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001382
Iteration 9/1000 | Loss: 0.00001361
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001233
Iteration 25/1000 | Loss: 0.00001231
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001226
Iteration 31/1000 | Loss: 0.00001226
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001221
Iteration 40/1000 | Loss: 0.00001221
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001192
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001181
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001180
Iteration 150/1000 | Loss: 0.00001180
Iteration 151/1000 | Loss: 0.00001180
Iteration 152/1000 | Loss: 0.00001180
Iteration 153/1000 | Loss: 0.00001180
Iteration 154/1000 | Loss: 0.00001180
Iteration 155/1000 | Loss: 0.00001180
Iteration 156/1000 | Loss: 0.00001180
Iteration 157/1000 | Loss: 0.00001180
Iteration 158/1000 | Loss: 0.00001179
Iteration 159/1000 | Loss: 0.00001179
Iteration 160/1000 | Loss: 0.00001179
Iteration 161/1000 | Loss: 0.00001179
Iteration 162/1000 | Loss: 0.00001179
Iteration 163/1000 | Loss: 0.00001179
Iteration 164/1000 | Loss: 0.00001179
Iteration 165/1000 | Loss: 0.00001179
Iteration 166/1000 | Loss: 0.00001179
Iteration 167/1000 | Loss: 0.00001179
Iteration 168/1000 | Loss: 0.00001179
Iteration 169/1000 | Loss: 0.00001179
Iteration 170/1000 | Loss: 0.00001179
Iteration 171/1000 | Loss: 0.00001179
Iteration 172/1000 | Loss: 0.00001179
Iteration 173/1000 | Loss: 0.00001179
Iteration 174/1000 | Loss: 0.00001179
Iteration 175/1000 | Loss: 0.00001179
Iteration 176/1000 | Loss: 0.00001179
Iteration 177/1000 | Loss: 0.00001179
Iteration 178/1000 | Loss: 0.00001179
Iteration 179/1000 | Loss: 0.00001179
Iteration 180/1000 | Loss: 0.00001179
Iteration 181/1000 | Loss: 0.00001179
Iteration 182/1000 | Loss: 0.00001179
Iteration 183/1000 | Loss: 0.00001179
Iteration 184/1000 | Loss: 0.00001179
Iteration 185/1000 | Loss: 0.00001179
Iteration 186/1000 | Loss: 0.00001179
Iteration 187/1000 | Loss: 0.00001179
Iteration 188/1000 | Loss: 0.00001179
Iteration 189/1000 | Loss: 0.00001179
Iteration 190/1000 | Loss: 0.00001179
Iteration 191/1000 | Loss: 0.00001179
Iteration 192/1000 | Loss: 0.00001179
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001179
Iteration 196/1000 | Loss: 0.00001179
Iteration 197/1000 | Loss: 0.00001179
Iteration 198/1000 | Loss: 0.00001179
Iteration 199/1000 | Loss: 0.00001179
Iteration 200/1000 | Loss: 0.00001179
Iteration 201/1000 | Loss: 0.00001179
Iteration 202/1000 | Loss: 0.00001179
Iteration 203/1000 | Loss: 0.00001179
Iteration 204/1000 | Loss: 0.00001179
Iteration 205/1000 | Loss: 0.00001179
Iteration 206/1000 | Loss: 0.00001179
Iteration 207/1000 | Loss: 0.00001179
Iteration 208/1000 | Loss: 0.00001179
Iteration 209/1000 | Loss: 0.00001179
Iteration 210/1000 | Loss: 0.00001179
Iteration 211/1000 | Loss: 0.00001179
Iteration 212/1000 | Loss: 0.00001179
Iteration 213/1000 | Loss: 0.00001179
Iteration 214/1000 | Loss: 0.00001179
Iteration 215/1000 | Loss: 0.00001179
Iteration 216/1000 | Loss: 0.00001179
Iteration 217/1000 | Loss: 0.00001179
Iteration 218/1000 | Loss: 0.00001179
Iteration 219/1000 | Loss: 0.00001179
Iteration 220/1000 | Loss: 0.00001179
Iteration 221/1000 | Loss: 0.00001179
Iteration 222/1000 | Loss: 0.00001179
Iteration 223/1000 | Loss: 0.00001179
Iteration 224/1000 | Loss: 0.00001179
Iteration 225/1000 | Loss: 0.00001179
Iteration 226/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.1789323252742179e-05, 1.1789323252742179e-05, 1.1789323252742179e-05, 1.1789323252742179e-05, 1.1789323252742179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1789323252742179e-05

Optimization complete. Final v2v error: 2.9967501163482666 mm

Highest mean error: 3.179816246032715 mm for frame 24

Lowest mean error: 2.899197578430176 mm for frame 64

Saving results

Total time: 45.29913330078125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430520
Iteration 2/25 | Loss: 0.00155094
Iteration 3/25 | Loss: 0.00149212
Iteration 4/25 | Loss: 0.00147728
Iteration 5/25 | Loss: 0.00147503
Iteration 6/25 | Loss: 0.00147503
Iteration 7/25 | Loss: 0.00147503
Iteration 8/25 | Loss: 0.00147503
Iteration 9/25 | Loss: 0.00147503
Iteration 10/25 | Loss: 0.00147503
Iteration 11/25 | Loss: 0.00147503
Iteration 12/25 | Loss: 0.00147503
Iteration 13/25 | Loss: 0.00147503
Iteration 14/25 | Loss: 0.00147503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001475025317631662, 0.001475025317631662, 0.001475025317631662, 0.001475025317631662, 0.001475025317631662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001475025317631662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24608624
Iteration 2/25 | Loss: 0.00233299
Iteration 3/25 | Loss: 0.00233298
Iteration 4/25 | Loss: 0.00233298
Iteration 5/25 | Loss: 0.00233298
Iteration 6/25 | Loss: 0.00233298
Iteration 7/25 | Loss: 0.00233298
Iteration 8/25 | Loss: 0.00233298
Iteration 9/25 | Loss: 0.00233298
Iteration 10/25 | Loss: 0.00233298
Iteration 11/25 | Loss: 0.00233298
Iteration 12/25 | Loss: 0.00233298
Iteration 13/25 | Loss: 0.00233298
Iteration 14/25 | Loss: 0.00233298
Iteration 15/25 | Loss: 0.00233298
Iteration 16/25 | Loss: 0.00233298
Iteration 17/25 | Loss: 0.00233298
Iteration 18/25 | Loss: 0.00233298
Iteration 19/25 | Loss: 0.00233298
Iteration 20/25 | Loss: 0.00233298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023329798132181168, 0.0023329798132181168, 0.0023329798132181168, 0.0023329798132181168, 0.0023329798132181168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023329798132181168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233298
Iteration 2/1000 | Loss: 0.00002530
Iteration 3/1000 | Loss: 0.00002238
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001776
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001638
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001589
Iteration 14/1000 | Loss: 0.00001582
Iteration 15/1000 | Loss: 0.00001568
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00001516
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001493
Iteration 25/1000 | Loss: 0.00001493
Iteration 26/1000 | Loss: 0.00001493
Iteration 27/1000 | Loss: 0.00001492
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001481
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001481
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001473
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001473
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001467
Iteration 133/1000 | Loss: 0.00001467
Iteration 134/1000 | Loss: 0.00001467
Iteration 135/1000 | Loss: 0.00001467
Iteration 136/1000 | Loss: 0.00001467
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001466
Iteration 140/1000 | Loss: 0.00001466
Iteration 141/1000 | Loss: 0.00001466
Iteration 142/1000 | Loss: 0.00001466
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001466
Iteration 147/1000 | Loss: 0.00001466
Iteration 148/1000 | Loss: 0.00001466
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Iteration 155/1000 | Loss: 0.00001466
Iteration 156/1000 | Loss: 0.00001466
Iteration 157/1000 | Loss: 0.00001466
Iteration 158/1000 | Loss: 0.00001466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4658055079053156e-05, 1.4658055079053156e-05, 1.4658055079053156e-05, 1.4658055079053156e-05, 1.4658055079053156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4658055079053156e-05

Optimization complete. Final v2v error: 3.3250157833099365 mm

Highest mean error: 3.394716262817383 mm for frame 162

Lowest mean error: 3.257570266723633 mm for frame 47

Saving results

Total time: 47.64021611213684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458104
Iteration 2/25 | Loss: 0.00169072
Iteration 3/25 | Loss: 0.00157462
Iteration 4/25 | Loss: 0.00156420
Iteration 5/25 | Loss: 0.00156221
Iteration 6/25 | Loss: 0.00156219
Iteration 7/25 | Loss: 0.00156219
Iteration 8/25 | Loss: 0.00156219
Iteration 9/25 | Loss: 0.00156219
Iteration 10/25 | Loss: 0.00156219
Iteration 11/25 | Loss: 0.00156219
Iteration 12/25 | Loss: 0.00156219
Iteration 13/25 | Loss: 0.00156219
Iteration 14/25 | Loss: 0.00156219
Iteration 15/25 | Loss: 0.00156219
Iteration 16/25 | Loss: 0.00156219
Iteration 17/25 | Loss: 0.00156219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001562191522680223, 0.001562191522680223, 0.001562191522680223, 0.001562191522680223, 0.001562191522680223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001562191522680223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20212233
Iteration 2/25 | Loss: 0.00204858
Iteration 3/25 | Loss: 0.00204857
Iteration 4/25 | Loss: 0.00204857
Iteration 5/25 | Loss: 0.00204857
Iteration 6/25 | Loss: 0.00204856
Iteration 7/25 | Loss: 0.00204856
Iteration 8/25 | Loss: 0.00204856
Iteration 9/25 | Loss: 0.00204856
Iteration 10/25 | Loss: 0.00204856
Iteration 11/25 | Loss: 0.00204856
Iteration 12/25 | Loss: 0.00204856
Iteration 13/25 | Loss: 0.00204856
Iteration 14/25 | Loss: 0.00204856
Iteration 15/25 | Loss: 0.00204856
Iteration 16/25 | Loss: 0.00204856
Iteration 17/25 | Loss: 0.00204856
Iteration 18/25 | Loss: 0.00204856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002048562979325652, 0.002048562979325652, 0.002048562979325652, 0.002048562979325652, 0.002048562979325652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002048562979325652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204856
Iteration 2/1000 | Loss: 0.00003561
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001636
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001587
Iteration 9/1000 | Loss: 0.00001579
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001486
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001444
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001442
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001441
Iteration 35/1000 | Loss: 0.00001441
Iteration 36/1000 | Loss: 0.00001441
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001436
Iteration 46/1000 | Loss: 0.00001436
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001433
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001431
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001429
Iteration 65/1000 | Loss: 0.00001428
Iteration 66/1000 | Loss: 0.00001428
Iteration 67/1000 | Loss: 0.00001428
Iteration 68/1000 | Loss: 0.00001428
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001427
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001421
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001420
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001418
Iteration 109/1000 | Loss: 0.00001418
Iteration 110/1000 | Loss: 0.00001418
Iteration 111/1000 | Loss: 0.00001417
Iteration 112/1000 | Loss: 0.00001417
Iteration 113/1000 | Loss: 0.00001417
Iteration 114/1000 | Loss: 0.00001417
Iteration 115/1000 | Loss: 0.00001417
Iteration 116/1000 | Loss: 0.00001417
Iteration 117/1000 | Loss: 0.00001417
Iteration 118/1000 | Loss: 0.00001417
Iteration 119/1000 | Loss: 0.00001417
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001416
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001416
Iteration 129/1000 | Loss: 0.00001416
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001416
Iteration 138/1000 | Loss: 0.00001416
Iteration 139/1000 | Loss: 0.00001416
Iteration 140/1000 | Loss: 0.00001416
Iteration 141/1000 | Loss: 0.00001416
Iteration 142/1000 | Loss: 0.00001416
Iteration 143/1000 | Loss: 0.00001416
Iteration 144/1000 | Loss: 0.00001416
Iteration 145/1000 | Loss: 0.00001416
Iteration 146/1000 | Loss: 0.00001416
Iteration 147/1000 | Loss: 0.00001416
Iteration 148/1000 | Loss: 0.00001416
Iteration 149/1000 | Loss: 0.00001416
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001416
Iteration 152/1000 | Loss: 0.00001416
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.4155186363495886e-05, 1.4155186363495886e-05, 1.4155186363495886e-05, 1.4155186363495886e-05, 1.4155186363495886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4155186363495886e-05

Optimization complete. Final v2v error: 3.2428078651428223 mm

Highest mean error: 3.5094563961029053 mm for frame 94

Lowest mean error: 3.0974347591400146 mm for frame 155

Saving results

Total time: 39.26300764083862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405828
Iteration 2/25 | Loss: 0.00152180
Iteration 3/25 | Loss: 0.00146027
Iteration 4/25 | Loss: 0.00145164
Iteration 5/25 | Loss: 0.00144878
Iteration 6/25 | Loss: 0.00144865
Iteration 7/25 | Loss: 0.00144865
Iteration 8/25 | Loss: 0.00144865
Iteration 9/25 | Loss: 0.00144865
Iteration 10/25 | Loss: 0.00144865
Iteration 11/25 | Loss: 0.00144865
Iteration 12/25 | Loss: 0.00144865
Iteration 13/25 | Loss: 0.00144865
Iteration 14/25 | Loss: 0.00144865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014486477011814713, 0.0014486477011814713, 0.0014486477011814713, 0.0014486477011814713, 0.0014486477011814713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014486477011814713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.49045157
Iteration 2/25 | Loss: 0.00243803
Iteration 3/25 | Loss: 0.00243802
Iteration 4/25 | Loss: 0.00243802
Iteration 5/25 | Loss: 0.00243802
Iteration 6/25 | Loss: 0.00243802
Iteration 7/25 | Loss: 0.00243802
Iteration 8/25 | Loss: 0.00243802
Iteration 9/25 | Loss: 0.00243802
Iteration 10/25 | Loss: 0.00243802
Iteration 11/25 | Loss: 0.00243802
Iteration 12/25 | Loss: 0.00243802
Iteration 13/25 | Loss: 0.00243802
Iteration 14/25 | Loss: 0.00243802
Iteration 15/25 | Loss: 0.00243802
Iteration 16/25 | Loss: 0.00243802
Iteration 17/25 | Loss: 0.00243802
Iteration 18/25 | Loss: 0.00243802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0024380190297961235, 0.0024380190297961235, 0.0024380190297961235, 0.0024380190297961235, 0.0024380190297961235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024380190297961235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243802
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001896
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001610
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001440
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001354
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001334
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001311
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001309
Iteration 24/1000 | Loss: 0.00001306
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001292
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001282
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001267
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001264
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001261
Iteration 47/1000 | Loss: 0.00001261
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001252
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001251
Iteration 86/1000 | Loss: 0.00001251
Iteration 87/1000 | Loss: 0.00001251
Iteration 88/1000 | Loss: 0.00001250
Iteration 89/1000 | Loss: 0.00001250
Iteration 90/1000 | Loss: 0.00001250
Iteration 91/1000 | Loss: 0.00001250
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001249
Iteration 95/1000 | Loss: 0.00001249
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001248
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001247
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001247
Iteration 121/1000 | Loss: 0.00001247
Iteration 122/1000 | Loss: 0.00001247
Iteration 123/1000 | Loss: 0.00001247
Iteration 124/1000 | Loss: 0.00001247
Iteration 125/1000 | Loss: 0.00001247
Iteration 126/1000 | Loss: 0.00001247
Iteration 127/1000 | Loss: 0.00001247
Iteration 128/1000 | Loss: 0.00001247
Iteration 129/1000 | Loss: 0.00001247
Iteration 130/1000 | Loss: 0.00001247
Iteration 131/1000 | Loss: 0.00001247
Iteration 132/1000 | Loss: 0.00001247
Iteration 133/1000 | Loss: 0.00001247
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001247
Iteration 137/1000 | Loss: 0.00001247
Iteration 138/1000 | Loss: 0.00001247
Iteration 139/1000 | Loss: 0.00001247
Iteration 140/1000 | Loss: 0.00001247
Iteration 141/1000 | Loss: 0.00001247
Iteration 142/1000 | Loss: 0.00001247
Iteration 143/1000 | Loss: 0.00001247
Iteration 144/1000 | Loss: 0.00001247
Iteration 145/1000 | Loss: 0.00001247
Iteration 146/1000 | Loss: 0.00001247
Iteration 147/1000 | Loss: 0.00001247
Iteration 148/1000 | Loss: 0.00001247
Iteration 149/1000 | Loss: 0.00001247
Iteration 150/1000 | Loss: 0.00001247
Iteration 151/1000 | Loss: 0.00001247
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001247
Iteration 154/1000 | Loss: 0.00001247
Iteration 155/1000 | Loss: 0.00001247
Iteration 156/1000 | Loss: 0.00001247
Iteration 157/1000 | Loss: 0.00001247
Iteration 158/1000 | Loss: 0.00001247
Iteration 159/1000 | Loss: 0.00001247
Iteration 160/1000 | Loss: 0.00001247
Iteration 161/1000 | Loss: 0.00001247
Iteration 162/1000 | Loss: 0.00001247
Iteration 163/1000 | Loss: 0.00001247
Iteration 164/1000 | Loss: 0.00001247
Iteration 165/1000 | Loss: 0.00001247
Iteration 166/1000 | Loss: 0.00001247
Iteration 167/1000 | Loss: 0.00001247
Iteration 168/1000 | Loss: 0.00001247
Iteration 169/1000 | Loss: 0.00001247
Iteration 170/1000 | Loss: 0.00001247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.247382715519052e-05, 1.247382715519052e-05, 1.247382715519052e-05, 1.247382715519052e-05, 1.247382715519052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.247382715519052e-05

Optimization complete. Final v2v error: 3.0691962242126465 mm

Highest mean error: 3.358062505722046 mm for frame 109

Lowest mean error: 2.917811155319214 mm for frame 168

Saving results

Total time: 41.61081886291504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017933
Iteration 2/25 | Loss: 0.01017933
Iteration 3/25 | Loss: 0.01017932
Iteration 4/25 | Loss: 0.01017932
Iteration 5/25 | Loss: 0.01017932
Iteration 6/25 | Loss: 0.01017932
Iteration 7/25 | Loss: 0.01017932
Iteration 8/25 | Loss: 0.01017932
Iteration 9/25 | Loss: 0.01017932
Iteration 10/25 | Loss: 0.01017932
Iteration 11/25 | Loss: 0.01017932
Iteration 12/25 | Loss: 0.01017931
Iteration 13/25 | Loss: 0.01017931
Iteration 14/25 | Loss: 0.01017931
Iteration 15/25 | Loss: 0.01017931
Iteration 16/25 | Loss: 0.01017931
Iteration 17/25 | Loss: 0.01017931
Iteration 18/25 | Loss: 0.01017931
Iteration 19/25 | Loss: 0.01017931
Iteration 20/25 | Loss: 0.01017931
Iteration 21/25 | Loss: 0.01017931
Iteration 22/25 | Loss: 0.01017931
Iteration 23/25 | Loss: 0.01017930
Iteration 24/25 | Loss: 0.01017930
Iteration 25/25 | Loss: 0.01017930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36082006
Iteration 2/25 | Loss: 0.13660531
Iteration 3/25 | Loss: 0.13328534
Iteration 4/25 | Loss: 0.12383752
Iteration 5/25 | Loss: 0.12383751
Iteration 6/25 | Loss: 0.12383749
Iteration 7/25 | Loss: 0.12383749
Iteration 8/25 | Loss: 0.12383749
Iteration 9/25 | Loss: 0.12383749
Iteration 10/25 | Loss: 0.12383749
Iteration 11/25 | Loss: 0.12383748
Iteration 12/25 | Loss: 0.12383748
Iteration 13/25 | Loss: 0.12383748
Iteration 14/25 | Loss: 0.12383748
Iteration 15/25 | Loss: 0.12383748
Iteration 16/25 | Loss: 0.12383748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.12383747845888138, 0.12383747845888138, 0.12383747845888138, 0.12383747845888138, 0.12383747845888138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12383747845888138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12383748
Iteration 2/1000 | Loss: 0.00154114
Iteration 3/1000 | Loss: 0.00048146
Iteration 4/1000 | Loss: 0.00014044
Iteration 5/1000 | Loss: 0.00007155
Iteration 6/1000 | Loss: 0.00004776
Iteration 7/1000 | Loss: 0.00003220
Iteration 8/1000 | Loss: 0.00002757
Iteration 9/1000 | Loss: 0.00002390
Iteration 10/1000 | Loss: 0.00002087
Iteration 11/1000 | Loss: 0.00001894
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001540
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001334
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001215
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001204
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001186
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001183
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001171
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001156
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001155
Iteration 116/1000 | Loss: 0.00001155
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001150
Iteration 132/1000 | Loss: 0.00001150
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001149
Iteration 136/1000 | Loss: 0.00001149
Iteration 137/1000 | Loss: 0.00001149
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001148
Iteration 141/1000 | Loss: 0.00001148
Iteration 142/1000 | Loss: 0.00001148
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001147
Iteration 150/1000 | Loss: 0.00001147
Iteration 151/1000 | Loss: 0.00001147
Iteration 152/1000 | Loss: 0.00001147
Iteration 153/1000 | Loss: 0.00001147
Iteration 154/1000 | Loss: 0.00001147
Iteration 155/1000 | Loss: 0.00001147
Iteration 156/1000 | Loss: 0.00001147
Iteration 157/1000 | Loss: 0.00001147
Iteration 158/1000 | Loss: 0.00001147
Iteration 159/1000 | Loss: 0.00001146
Iteration 160/1000 | Loss: 0.00001146
Iteration 161/1000 | Loss: 0.00001146
Iteration 162/1000 | Loss: 0.00001146
Iteration 163/1000 | Loss: 0.00001146
Iteration 164/1000 | Loss: 0.00001146
Iteration 165/1000 | Loss: 0.00001146
Iteration 166/1000 | Loss: 0.00001146
Iteration 167/1000 | Loss: 0.00001146
Iteration 168/1000 | Loss: 0.00001146
Iteration 169/1000 | Loss: 0.00001146
Iteration 170/1000 | Loss: 0.00001146
Iteration 171/1000 | Loss: 0.00001146
Iteration 172/1000 | Loss: 0.00001146
Iteration 173/1000 | Loss: 0.00001146
Iteration 174/1000 | Loss: 0.00001146
Iteration 175/1000 | Loss: 0.00001146
Iteration 176/1000 | Loss: 0.00001146
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1457817890914157e-05, 1.1457817890914157e-05, 1.1457817890914157e-05, 1.1457817890914157e-05, 1.1457817890914157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1457817890914157e-05

Optimization complete. Final v2v error: 2.961615800857544 mm

Highest mean error: 3.3584649562835693 mm for frame 79

Lowest mean error: 2.765774965286255 mm for frame 170

Saving results

Total time: 69.03338599205017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816072
Iteration 2/25 | Loss: 0.00255596
Iteration 3/25 | Loss: 0.00213220
Iteration 4/25 | Loss: 0.00201483
Iteration 5/25 | Loss: 0.00182970
Iteration 6/25 | Loss: 0.00168975
Iteration 7/25 | Loss: 0.00167402
Iteration 8/25 | Loss: 0.00166866
Iteration 9/25 | Loss: 0.00166228
Iteration 10/25 | Loss: 0.00165564
Iteration 11/25 | Loss: 0.00165163
Iteration 12/25 | Loss: 0.00164963
Iteration 13/25 | Loss: 0.00165118
Iteration 14/25 | Loss: 0.00164969
Iteration 15/25 | Loss: 0.00164544
Iteration 16/25 | Loss: 0.00164083
Iteration 17/25 | Loss: 0.00163935
Iteration 18/25 | Loss: 0.00163915
Iteration 19/25 | Loss: 0.00163799
Iteration 20/25 | Loss: 0.00163692
Iteration 21/25 | Loss: 0.00163636
Iteration 22/25 | Loss: 0.00163604
Iteration 23/25 | Loss: 0.00163597
Iteration 24/25 | Loss: 0.00163597
Iteration 25/25 | Loss: 0.00163596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22172034
Iteration 2/25 | Loss: 0.00183785
Iteration 3/25 | Loss: 0.00183783
Iteration 4/25 | Loss: 0.00183783
Iteration 5/25 | Loss: 0.00183783
Iteration 6/25 | Loss: 0.00183783
Iteration 7/25 | Loss: 0.00183783
Iteration 8/25 | Loss: 0.00183783
Iteration 9/25 | Loss: 0.00183783
Iteration 10/25 | Loss: 0.00183782
Iteration 11/25 | Loss: 0.00183782
Iteration 12/25 | Loss: 0.00183782
Iteration 13/25 | Loss: 0.00183782
Iteration 14/25 | Loss: 0.00183782
Iteration 15/25 | Loss: 0.00183782
Iteration 16/25 | Loss: 0.00183782
Iteration 17/25 | Loss: 0.00183782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018378245877102017, 0.0018378245877102017, 0.0018378245877102017, 0.0018378245877102017, 0.0018378245877102017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018378245877102017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183782
Iteration 2/1000 | Loss: 0.00004694
Iteration 3/1000 | Loss: 0.00003102
Iteration 4/1000 | Loss: 0.00002760
Iteration 5/1000 | Loss: 0.00002607
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00002435
Iteration 8/1000 | Loss: 0.00002379
Iteration 9/1000 | Loss: 0.00002346
Iteration 10/1000 | Loss: 0.00002316
Iteration 11/1000 | Loss: 0.00002310
Iteration 12/1000 | Loss: 0.00002304
Iteration 13/1000 | Loss: 0.00002297
Iteration 14/1000 | Loss: 0.00002289
Iteration 15/1000 | Loss: 0.00002289
Iteration 16/1000 | Loss: 0.00002283
Iteration 17/1000 | Loss: 0.00002283
Iteration 18/1000 | Loss: 0.00002281
Iteration 19/1000 | Loss: 0.00002279
Iteration 20/1000 | Loss: 0.00002272
Iteration 21/1000 | Loss: 0.00002272
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00002270
Iteration 24/1000 | Loss: 0.00002270
Iteration 25/1000 | Loss: 0.00002270
Iteration 26/1000 | Loss: 0.00002270
Iteration 27/1000 | Loss: 0.00002270
Iteration 28/1000 | Loss: 0.00002270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 28. Stopping optimization.
Last 5 losses: [2.2703572540194727e-05, 2.2703572540194727e-05, 2.2703572540194727e-05, 2.2703572540194727e-05, 2.2703572540194727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2703572540194727e-05

Optimization complete. Final v2v error: 4.084349632263184 mm

Highest mean error: 4.689642906188965 mm for frame 158

Lowest mean error: 3.8534598350524902 mm for frame 43

Saving results

Total time: 66.29166579246521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545772
Iteration 2/25 | Loss: 0.00169374
Iteration 3/25 | Loss: 0.00154304
Iteration 4/25 | Loss: 0.00149508
Iteration 5/25 | Loss: 0.00148961
Iteration 6/25 | Loss: 0.00148854
Iteration 7/25 | Loss: 0.00148826
Iteration 8/25 | Loss: 0.00148814
Iteration 9/25 | Loss: 0.00148813
Iteration 10/25 | Loss: 0.00148811
Iteration 11/25 | Loss: 0.00148810
Iteration 12/25 | Loss: 0.00148810
Iteration 13/25 | Loss: 0.00148810
Iteration 14/25 | Loss: 0.00148809
Iteration 15/25 | Loss: 0.00148809
Iteration 16/25 | Loss: 0.00148809
Iteration 17/25 | Loss: 0.00148809
Iteration 18/25 | Loss: 0.00148809
Iteration 19/25 | Loss: 0.00148809
Iteration 20/25 | Loss: 0.00148808
Iteration 21/25 | Loss: 0.00148808
Iteration 22/25 | Loss: 0.00148808
Iteration 23/25 | Loss: 0.00148808
Iteration 24/25 | Loss: 0.00148808
Iteration 25/25 | Loss: 0.00148808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.10963011
Iteration 2/25 | Loss: 0.00246284
Iteration 3/25 | Loss: 0.00246282
Iteration 4/25 | Loss: 0.00246282
Iteration 5/25 | Loss: 0.00246282
Iteration 6/25 | Loss: 0.00246282
Iteration 7/25 | Loss: 0.00246282
Iteration 8/25 | Loss: 0.00246282
Iteration 9/25 | Loss: 0.00246282
Iteration 10/25 | Loss: 0.00246282
Iteration 11/25 | Loss: 0.00246282
Iteration 12/25 | Loss: 0.00246282
Iteration 13/25 | Loss: 0.00246282
Iteration 14/25 | Loss: 0.00246282
Iteration 15/25 | Loss: 0.00246282
Iteration 16/25 | Loss: 0.00246282
Iteration 17/25 | Loss: 0.00246282
Iteration 18/25 | Loss: 0.00246282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0024628182873129845, 0.0024628182873129845, 0.0024628182873129845, 0.0024628182873129845, 0.0024628182873129845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024628182873129845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246282
Iteration 2/1000 | Loss: 0.00002621
Iteration 3/1000 | Loss: 0.00001992
Iteration 4/1000 | Loss: 0.00001838
Iteration 5/1000 | Loss: 0.00002354
Iteration 6/1000 | Loss: 0.00001651
Iteration 7/1000 | Loss: 0.00001599
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001471
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001457
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001438
Iteration 20/1000 | Loss: 0.00001436
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001426
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001411
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001409
Iteration 32/1000 | Loss: 0.00001409
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001402
Iteration 46/1000 | Loss: 0.00001402
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001399
Iteration 50/1000 | Loss: 0.00001399
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001391
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001390
Iteration 94/1000 | Loss: 0.00001390
Iteration 95/1000 | Loss: 0.00001390
Iteration 96/1000 | Loss: 0.00001390
Iteration 97/1000 | Loss: 0.00001390
Iteration 98/1000 | Loss: 0.00001390
Iteration 99/1000 | Loss: 0.00001390
Iteration 100/1000 | Loss: 0.00001390
Iteration 101/1000 | Loss: 0.00001390
Iteration 102/1000 | Loss: 0.00001390
Iteration 103/1000 | Loss: 0.00001390
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001390
Iteration 117/1000 | Loss: 0.00001390
Iteration 118/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.389957560604671e-05, 1.389957560604671e-05, 1.389957560604671e-05, 1.389957560604671e-05, 1.389957560604671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.389957560604671e-05

Optimization complete. Final v2v error: 3.1791632175445557 mm

Highest mean error: 3.815248489379883 mm for frame 166

Lowest mean error: 2.8723561763763428 mm for frame 88

Saving results

Total time: 51.41403603553772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491048
Iteration 2/25 | Loss: 0.00155769
Iteration 3/25 | Loss: 0.00149961
Iteration 4/25 | Loss: 0.00149028
Iteration 5/25 | Loss: 0.00148898
Iteration 6/25 | Loss: 0.00148898
Iteration 7/25 | Loss: 0.00148898
Iteration 8/25 | Loss: 0.00148898
Iteration 9/25 | Loss: 0.00148898
Iteration 10/25 | Loss: 0.00148898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001488982466980815, 0.001488982466980815, 0.001488982466980815, 0.001488982466980815, 0.001488982466980815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001488982466980815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48224306
Iteration 2/25 | Loss: 0.00262697
Iteration 3/25 | Loss: 0.00262696
Iteration 4/25 | Loss: 0.00262696
Iteration 5/25 | Loss: 0.00262696
Iteration 6/25 | Loss: 0.00262696
Iteration 7/25 | Loss: 0.00262696
Iteration 8/25 | Loss: 0.00262696
Iteration 9/25 | Loss: 0.00262696
Iteration 10/25 | Loss: 0.00262696
Iteration 11/25 | Loss: 0.00262696
Iteration 12/25 | Loss: 0.00262696
Iteration 13/25 | Loss: 0.00262696
Iteration 14/25 | Loss: 0.00262696
Iteration 15/25 | Loss: 0.00262696
Iteration 16/25 | Loss: 0.00262696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002626958070322871, 0.002626958070322871, 0.002626958070322871, 0.002626958070322871, 0.002626958070322871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002626958070322871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00262696
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00002343
Iteration 4/1000 | Loss: 0.00002153
Iteration 5/1000 | Loss: 0.00002018
Iteration 6/1000 | Loss: 0.00001947
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001758
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001660
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001611
Iteration 74/1000 | Loss: 0.00001611
Iteration 75/1000 | Loss: 0.00001611
Iteration 76/1000 | Loss: 0.00001611
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001610
Iteration 79/1000 | Loss: 0.00001610
Iteration 80/1000 | Loss: 0.00001609
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001609
Iteration 84/1000 | Loss: 0.00001609
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001608
Iteration 90/1000 | Loss: 0.00001607
Iteration 91/1000 | Loss: 0.00001607
Iteration 92/1000 | Loss: 0.00001607
Iteration 93/1000 | Loss: 0.00001607
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001605
Iteration 103/1000 | Loss: 0.00001605
Iteration 104/1000 | Loss: 0.00001605
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001604
Iteration 109/1000 | Loss: 0.00001604
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001604
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001604
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001603
Iteration 118/1000 | Loss: 0.00001603
Iteration 119/1000 | Loss: 0.00001603
Iteration 120/1000 | Loss: 0.00001603
Iteration 121/1000 | Loss: 0.00001603
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001603
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001603
Iteration 133/1000 | Loss: 0.00001603
Iteration 134/1000 | Loss: 0.00001603
Iteration 135/1000 | Loss: 0.00001603
Iteration 136/1000 | Loss: 0.00001603
Iteration 137/1000 | Loss: 0.00001603
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Iteration 144/1000 | Loss: 0.00001603
Iteration 145/1000 | Loss: 0.00001603
Iteration 146/1000 | Loss: 0.00001603
Iteration 147/1000 | Loss: 0.00001603
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001603
Iteration 151/1000 | Loss: 0.00001603
Iteration 152/1000 | Loss: 0.00001603
Iteration 153/1000 | Loss: 0.00001603
Iteration 154/1000 | Loss: 0.00001603
Iteration 155/1000 | Loss: 0.00001603
Iteration 156/1000 | Loss: 0.00001603
Iteration 157/1000 | Loss: 0.00001603
Iteration 158/1000 | Loss: 0.00001603
Iteration 159/1000 | Loss: 0.00001603
Iteration 160/1000 | Loss: 0.00001603
Iteration 161/1000 | Loss: 0.00001603
Iteration 162/1000 | Loss: 0.00001603
Iteration 163/1000 | Loss: 0.00001603
Iteration 164/1000 | Loss: 0.00001603
Iteration 165/1000 | Loss: 0.00001603
Iteration 166/1000 | Loss: 0.00001603
Iteration 167/1000 | Loss: 0.00001603
Iteration 168/1000 | Loss: 0.00001603
Iteration 169/1000 | Loss: 0.00001603
Iteration 170/1000 | Loss: 0.00001603
Iteration 171/1000 | Loss: 0.00001603
Iteration 172/1000 | Loss: 0.00001603
Iteration 173/1000 | Loss: 0.00001603
Iteration 174/1000 | Loss: 0.00001603
Iteration 175/1000 | Loss: 0.00001603
Iteration 176/1000 | Loss: 0.00001603
Iteration 177/1000 | Loss: 0.00001603
Iteration 178/1000 | Loss: 0.00001603
Iteration 179/1000 | Loss: 0.00001603
Iteration 180/1000 | Loss: 0.00001603
Iteration 181/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.6025012882892042e-05, 1.6025012882892042e-05, 1.6025012882892042e-05, 1.6025012882892042e-05, 1.6025012882892042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6025012882892042e-05

Optimization complete. Final v2v error: 3.433957576751709 mm

Highest mean error: 3.713771104812622 mm for frame 266

Lowest mean error: 3.2303225994110107 mm for frame 10

Saving results

Total time: 45.95717191696167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590580
Iteration 2/25 | Loss: 0.00176350
Iteration 3/25 | Loss: 0.00160425
Iteration 4/25 | Loss: 0.00158924
Iteration 5/25 | Loss: 0.00158457
Iteration 6/25 | Loss: 0.00158356
Iteration 7/25 | Loss: 0.00158315
Iteration 8/25 | Loss: 0.00158270
Iteration 9/25 | Loss: 0.00158306
Iteration 10/25 | Loss: 0.00158269
Iteration 11/25 | Loss: 0.00158267
Iteration 12/25 | Loss: 0.00158247
Iteration 13/25 | Loss: 0.00158156
Iteration 14/25 | Loss: 0.00158270
Iteration 15/25 | Loss: 0.00158282
Iteration 16/25 | Loss: 0.00158224
Iteration 17/25 | Loss: 0.00158253
Iteration 18/25 | Loss: 0.00158245
Iteration 19/25 | Loss: 0.00158256
Iteration 20/25 | Loss: 0.00158239
Iteration 21/25 | Loss: 0.00158247
Iteration 22/25 | Loss: 0.00158238
Iteration 23/25 | Loss: 0.00158201
Iteration 24/25 | Loss: 0.00158263
Iteration 25/25 | Loss: 0.00158255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20623958
Iteration 2/25 | Loss: 0.00280369
Iteration 3/25 | Loss: 0.00280368
Iteration 4/25 | Loss: 0.00280368
Iteration 5/25 | Loss: 0.00280367
Iteration 6/25 | Loss: 0.00280367
Iteration 7/25 | Loss: 0.00280367
Iteration 8/25 | Loss: 0.00280367
Iteration 9/25 | Loss: 0.00280367
Iteration 10/25 | Loss: 0.00280367
Iteration 11/25 | Loss: 0.00280367
Iteration 12/25 | Loss: 0.00280367
Iteration 13/25 | Loss: 0.00280367
Iteration 14/25 | Loss: 0.00280367
Iteration 15/25 | Loss: 0.00280367
Iteration 16/25 | Loss: 0.00280367
Iteration 17/25 | Loss: 0.00280367
Iteration 18/25 | Loss: 0.00280367
Iteration 19/25 | Loss: 0.00280367
Iteration 20/25 | Loss: 0.00280367
Iteration 21/25 | Loss: 0.00280367
Iteration 22/25 | Loss: 0.00280367
Iteration 23/25 | Loss: 0.00280367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002803671872243285, 0.002803671872243285, 0.002803671872243285, 0.002803671872243285, 0.002803671872243285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002803671872243285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280367
Iteration 2/1000 | Loss: 0.00004096
Iteration 3/1000 | Loss: 0.00003239
Iteration 4/1000 | Loss: 0.00003148
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002803
Iteration 7/1000 | Loss: 0.00003224
Iteration 8/1000 | Loss: 0.00003026
Iteration 9/1000 | Loss: 0.00003206
Iteration 10/1000 | Loss: 0.00002888
Iteration 11/1000 | Loss: 0.00002654
Iteration 12/1000 | Loss: 0.00003127
Iteration 13/1000 | Loss: 0.00002844
Iteration 14/1000 | Loss: 0.00002830
Iteration 15/1000 | Loss: 0.00002786
Iteration 16/1000 | Loss: 0.00002406
Iteration 17/1000 | Loss: 0.00002336
Iteration 18/1000 | Loss: 0.00002588
Iteration 19/1000 | Loss: 0.00002867
Iteration 20/1000 | Loss: 0.00002627
Iteration 21/1000 | Loss: 0.00002850
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002782
Iteration 24/1000 | Loss: 0.00002569
Iteration 25/1000 | Loss: 0.00002778
Iteration 26/1000 | Loss: 0.00002681
Iteration 27/1000 | Loss: 0.00002866
Iteration 28/1000 | Loss: 0.00002978
Iteration 29/1000 | Loss: 0.00002898
Iteration 30/1000 | Loss: 0.00003111
Iteration 31/1000 | Loss: 0.00002847
Iteration 32/1000 | Loss: 0.00003174
Iteration 33/1000 | Loss: 0.00002535
Iteration 34/1000 | Loss: 0.00002352
Iteration 35/1000 | Loss: 0.00002306
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002260
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002246
Iteration 40/1000 | Loss: 0.00002246
Iteration 41/1000 | Loss: 0.00002246
Iteration 42/1000 | Loss: 0.00002242
Iteration 43/1000 | Loss: 0.00002242
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002240
Iteration 46/1000 | Loss: 0.00002239
Iteration 47/1000 | Loss: 0.00002238
Iteration 48/1000 | Loss: 0.00002238
Iteration 49/1000 | Loss: 0.00002238
Iteration 50/1000 | Loss: 0.00002238
Iteration 51/1000 | Loss: 0.00002237
Iteration 52/1000 | Loss: 0.00002237
Iteration 53/1000 | Loss: 0.00002236
Iteration 54/1000 | Loss: 0.00002235
Iteration 55/1000 | Loss: 0.00002235
Iteration 56/1000 | Loss: 0.00002234
Iteration 57/1000 | Loss: 0.00002233
Iteration 58/1000 | Loss: 0.00002233
Iteration 59/1000 | Loss: 0.00002233
Iteration 60/1000 | Loss: 0.00002233
Iteration 61/1000 | Loss: 0.00002233
Iteration 62/1000 | Loss: 0.00002233
Iteration 63/1000 | Loss: 0.00002233
Iteration 64/1000 | Loss: 0.00002233
Iteration 65/1000 | Loss: 0.00002233
Iteration 66/1000 | Loss: 0.00002233
Iteration 67/1000 | Loss: 0.00002233
Iteration 68/1000 | Loss: 0.00002232
Iteration 69/1000 | Loss: 0.00002232
Iteration 70/1000 | Loss: 0.00002231
Iteration 71/1000 | Loss: 0.00002231
Iteration 72/1000 | Loss: 0.00002230
Iteration 73/1000 | Loss: 0.00002230
Iteration 74/1000 | Loss: 0.00002230
Iteration 75/1000 | Loss: 0.00002230
Iteration 76/1000 | Loss: 0.00002230
Iteration 77/1000 | Loss: 0.00002230
Iteration 78/1000 | Loss: 0.00002230
Iteration 79/1000 | Loss: 0.00002230
Iteration 80/1000 | Loss: 0.00002230
Iteration 81/1000 | Loss: 0.00002230
Iteration 82/1000 | Loss: 0.00002230
Iteration 83/1000 | Loss: 0.00002230
Iteration 84/1000 | Loss: 0.00002229
Iteration 85/1000 | Loss: 0.00002229
Iteration 86/1000 | Loss: 0.00002229
Iteration 87/1000 | Loss: 0.00002229
Iteration 88/1000 | Loss: 0.00002229
Iteration 89/1000 | Loss: 0.00002229
Iteration 90/1000 | Loss: 0.00002229
Iteration 91/1000 | Loss: 0.00002228
Iteration 92/1000 | Loss: 0.00002228
Iteration 93/1000 | Loss: 0.00002228
Iteration 94/1000 | Loss: 0.00002228
Iteration 95/1000 | Loss: 0.00002228
Iteration 96/1000 | Loss: 0.00002228
Iteration 97/1000 | Loss: 0.00002228
Iteration 98/1000 | Loss: 0.00002228
Iteration 99/1000 | Loss: 0.00002227
Iteration 100/1000 | Loss: 0.00002227
Iteration 101/1000 | Loss: 0.00002227
Iteration 102/1000 | Loss: 0.00002226
Iteration 103/1000 | Loss: 0.00002225
Iteration 104/1000 | Loss: 0.00002225
Iteration 105/1000 | Loss: 0.00002225
Iteration 106/1000 | Loss: 0.00002224
Iteration 107/1000 | Loss: 0.00002223
Iteration 108/1000 | Loss: 0.00002223
Iteration 109/1000 | Loss: 0.00002222
Iteration 110/1000 | Loss: 0.00002222
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002221
Iteration 115/1000 | Loss: 0.00002221
Iteration 116/1000 | Loss: 0.00002221
Iteration 117/1000 | Loss: 0.00002221
Iteration 118/1000 | Loss: 0.00002221
Iteration 119/1000 | Loss: 0.00002221
Iteration 120/1000 | Loss: 0.00002221
Iteration 121/1000 | Loss: 0.00002221
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002220
Iteration 125/1000 | Loss: 0.00002220
Iteration 126/1000 | Loss: 0.00002220
Iteration 127/1000 | Loss: 0.00002220
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002220
Iteration 131/1000 | Loss: 0.00002219
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002219
Iteration 135/1000 | Loss: 0.00002219
Iteration 136/1000 | Loss: 0.00002219
Iteration 137/1000 | Loss: 0.00002219
Iteration 138/1000 | Loss: 0.00002219
Iteration 139/1000 | Loss: 0.00002219
Iteration 140/1000 | Loss: 0.00002219
Iteration 141/1000 | Loss: 0.00002219
Iteration 142/1000 | Loss: 0.00002218
Iteration 143/1000 | Loss: 0.00002218
Iteration 144/1000 | Loss: 0.00002218
Iteration 145/1000 | Loss: 0.00002218
Iteration 146/1000 | Loss: 0.00002218
Iteration 147/1000 | Loss: 0.00002218
Iteration 148/1000 | Loss: 0.00002218
Iteration 149/1000 | Loss: 0.00002218
Iteration 150/1000 | Loss: 0.00002218
Iteration 151/1000 | Loss: 0.00002218
Iteration 152/1000 | Loss: 0.00002218
Iteration 153/1000 | Loss: 0.00002218
Iteration 154/1000 | Loss: 0.00002218
Iteration 155/1000 | Loss: 0.00002218
Iteration 156/1000 | Loss: 0.00002218
Iteration 157/1000 | Loss: 0.00002218
Iteration 158/1000 | Loss: 0.00002218
Iteration 159/1000 | Loss: 0.00002218
Iteration 160/1000 | Loss: 0.00002218
Iteration 161/1000 | Loss: 0.00002218
Iteration 162/1000 | Loss: 0.00002218
Iteration 163/1000 | Loss: 0.00002218
Iteration 164/1000 | Loss: 0.00002218
Iteration 165/1000 | Loss: 0.00002218
Iteration 166/1000 | Loss: 0.00002218
Iteration 167/1000 | Loss: 0.00002218
Iteration 168/1000 | Loss: 0.00002218
Iteration 169/1000 | Loss: 0.00002218
Iteration 170/1000 | Loss: 0.00002218
Iteration 171/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.2182959583005868e-05, 2.2182959583005868e-05, 2.2182959583005868e-05, 2.2182959583005868e-05, 2.2182959583005868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2182959583005868e-05

Optimization complete. Final v2v error: 4.063526153564453 mm

Highest mean error: 4.94856595993042 mm for frame 60

Lowest mean error: 3.8107597827911377 mm for frame 112

Saving results

Total time: 118.19388437271118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798642
Iteration 2/25 | Loss: 0.00156108
Iteration 3/25 | Loss: 0.00147771
Iteration 4/25 | Loss: 0.00146785
Iteration 5/25 | Loss: 0.00146571
Iteration 6/25 | Loss: 0.00146560
Iteration 7/25 | Loss: 0.00146560
Iteration 8/25 | Loss: 0.00146560
Iteration 9/25 | Loss: 0.00146560
Iteration 10/25 | Loss: 0.00146560
Iteration 11/25 | Loss: 0.00146560
Iteration 12/25 | Loss: 0.00146560
Iteration 13/25 | Loss: 0.00146560
Iteration 14/25 | Loss: 0.00146560
Iteration 15/25 | Loss: 0.00146560
Iteration 16/25 | Loss: 0.00146560
Iteration 17/25 | Loss: 0.00146560
Iteration 18/25 | Loss: 0.00146560
Iteration 19/25 | Loss: 0.00146560
Iteration 20/25 | Loss: 0.00146560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014656037092208862, 0.0014656037092208862, 0.0014656037092208862, 0.0014656037092208862, 0.0014656037092208862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014656037092208862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23479819
Iteration 2/25 | Loss: 0.00217384
Iteration 3/25 | Loss: 0.00217382
Iteration 4/25 | Loss: 0.00217382
Iteration 5/25 | Loss: 0.00217382
Iteration 6/25 | Loss: 0.00217382
Iteration 7/25 | Loss: 0.00217382
Iteration 8/25 | Loss: 0.00217382
Iteration 9/25 | Loss: 0.00217382
Iteration 10/25 | Loss: 0.00217382
Iteration 11/25 | Loss: 0.00217382
Iteration 12/25 | Loss: 0.00217382
Iteration 13/25 | Loss: 0.00217382
Iteration 14/25 | Loss: 0.00217382
Iteration 15/25 | Loss: 0.00217382
Iteration 16/25 | Loss: 0.00217382
Iteration 17/25 | Loss: 0.00217382
Iteration 18/25 | Loss: 0.00217382
Iteration 19/25 | Loss: 0.00217382
Iteration 20/25 | Loss: 0.00217382
Iteration 21/25 | Loss: 0.00217382
Iteration 22/25 | Loss: 0.00217382
Iteration 23/25 | Loss: 0.00217382
Iteration 24/25 | Loss: 0.00217382
Iteration 25/25 | Loss: 0.00217382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217382
Iteration 2/1000 | Loss: 0.00002458
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001613
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001281
Iteration 10/1000 | Loss: 0.00001249
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001195
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001180
Iteration 32/1000 | Loss: 0.00001180
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001161
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001148
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001147
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001146
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001143
Iteration 107/1000 | Loss: 0.00001143
Iteration 108/1000 | Loss: 0.00001143
Iteration 109/1000 | Loss: 0.00001143
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001143
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001142
Iteration 120/1000 | Loss: 0.00001142
Iteration 121/1000 | Loss: 0.00001142
Iteration 122/1000 | Loss: 0.00001142
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001141
Iteration 136/1000 | Loss: 0.00001141
Iteration 137/1000 | Loss: 0.00001141
Iteration 138/1000 | Loss: 0.00001141
Iteration 139/1000 | Loss: 0.00001141
Iteration 140/1000 | Loss: 0.00001141
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001140
Iteration 145/1000 | Loss: 0.00001140
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001138
Iteration 154/1000 | Loss: 0.00001138
Iteration 155/1000 | Loss: 0.00001138
Iteration 156/1000 | Loss: 0.00001138
Iteration 157/1000 | Loss: 0.00001138
Iteration 158/1000 | Loss: 0.00001138
Iteration 159/1000 | Loss: 0.00001138
Iteration 160/1000 | Loss: 0.00001138
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001138
Iteration 163/1000 | Loss: 0.00001138
Iteration 164/1000 | Loss: 0.00001138
Iteration 165/1000 | Loss: 0.00001138
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001137
Iteration 172/1000 | Loss: 0.00001137
Iteration 173/1000 | Loss: 0.00001137
Iteration 174/1000 | Loss: 0.00001137
Iteration 175/1000 | Loss: 0.00001137
Iteration 176/1000 | Loss: 0.00001137
Iteration 177/1000 | Loss: 0.00001137
Iteration 178/1000 | Loss: 0.00001136
Iteration 179/1000 | Loss: 0.00001136
Iteration 180/1000 | Loss: 0.00001136
Iteration 181/1000 | Loss: 0.00001136
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001135
Iteration 188/1000 | Loss: 0.00001135
Iteration 189/1000 | Loss: 0.00001135
Iteration 190/1000 | Loss: 0.00001135
Iteration 191/1000 | Loss: 0.00001135
Iteration 192/1000 | Loss: 0.00001135
Iteration 193/1000 | Loss: 0.00001135
Iteration 194/1000 | Loss: 0.00001135
Iteration 195/1000 | Loss: 0.00001135
Iteration 196/1000 | Loss: 0.00001135
Iteration 197/1000 | Loss: 0.00001135
Iteration 198/1000 | Loss: 0.00001135
Iteration 199/1000 | Loss: 0.00001135
Iteration 200/1000 | Loss: 0.00001135
Iteration 201/1000 | Loss: 0.00001135
Iteration 202/1000 | Loss: 0.00001135
Iteration 203/1000 | Loss: 0.00001135
Iteration 204/1000 | Loss: 0.00001135
Iteration 205/1000 | Loss: 0.00001135
Iteration 206/1000 | Loss: 0.00001135
Iteration 207/1000 | Loss: 0.00001135
Iteration 208/1000 | Loss: 0.00001135
Iteration 209/1000 | Loss: 0.00001135
Iteration 210/1000 | Loss: 0.00001135
Iteration 211/1000 | Loss: 0.00001135
Iteration 212/1000 | Loss: 0.00001135
Iteration 213/1000 | Loss: 0.00001135
Iteration 214/1000 | Loss: 0.00001135
Iteration 215/1000 | Loss: 0.00001135
Iteration 216/1000 | Loss: 0.00001135
Iteration 217/1000 | Loss: 0.00001135
Iteration 218/1000 | Loss: 0.00001135
Iteration 219/1000 | Loss: 0.00001135
Iteration 220/1000 | Loss: 0.00001135
Iteration 221/1000 | Loss: 0.00001135
Iteration 222/1000 | Loss: 0.00001135
Iteration 223/1000 | Loss: 0.00001135
Iteration 224/1000 | Loss: 0.00001135
Iteration 225/1000 | Loss: 0.00001135
Iteration 226/1000 | Loss: 0.00001135
Iteration 227/1000 | Loss: 0.00001135
Iteration 228/1000 | Loss: 0.00001135
Iteration 229/1000 | Loss: 0.00001135
Iteration 230/1000 | Loss: 0.00001135
Iteration 231/1000 | Loss: 0.00001135
Iteration 232/1000 | Loss: 0.00001135
Iteration 233/1000 | Loss: 0.00001135
Iteration 234/1000 | Loss: 0.00001135
Iteration 235/1000 | Loss: 0.00001135
Iteration 236/1000 | Loss: 0.00001135
Iteration 237/1000 | Loss: 0.00001135
Iteration 238/1000 | Loss: 0.00001135
Iteration 239/1000 | Loss: 0.00001135
Iteration 240/1000 | Loss: 0.00001135
Iteration 241/1000 | Loss: 0.00001135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.1347632607794367e-05, 1.1347632607794367e-05, 1.1347632607794367e-05, 1.1347632607794367e-05, 1.1347632607794367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1347632607794367e-05

Optimization complete. Final v2v error: 2.938462018966675 mm

Highest mean error: 3.126343011856079 mm for frame 29

Lowest mean error: 2.8029394149780273 mm for frame 63

Saving results

Total time: 42.8244354724884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775098
Iteration 2/25 | Loss: 0.00153689
Iteration 3/25 | Loss: 0.00146263
Iteration 4/25 | Loss: 0.00145479
Iteration 5/25 | Loss: 0.00145270
Iteration 6/25 | Loss: 0.00145270
Iteration 7/25 | Loss: 0.00145270
Iteration 8/25 | Loss: 0.00145270
Iteration 9/25 | Loss: 0.00145270
Iteration 10/25 | Loss: 0.00145270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014527000021189451, 0.0014527000021189451, 0.0014527000021189451, 0.0014527000021189451, 0.0014527000021189451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014527000021189451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24738896
Iteration 2/25 | Loss: 0.00234561
Iteration 3/25 | Loss: 0.00234560
Iteration 4/25 | Loss: 0.00234560
Iteration 5/25 | Loss: 0.00234560
Iteration 6/25 | Loss: 0.00234560
Iteration 7/25 | Loss: 0.00234560
Iteration 8/25 | Loss: 0.00234560
Iteration 9/25 | Loss: 0.00234560
Iteration 10/25 | Loss: 0.00234560
Iteration 11/25 | Loss: 0.00234560
Iteration 12/25 | Loss: 0.00234560
Iteration 13/25 | Loss: 0.00234560
Iteration 14/25 | Loss: 0.00234560
Iteration 15/25 | Loss: 0.00234560
Iteration 16/25 | Loss: 0.00234560
Iteration 17/25 | Loss: 0.00234560
Iteration 18/25 | Loss: 0.00234560
Iteration 19/25 | Loss: 0.00234560
Iteration 20/25 | Loss: 0.00234560
Iteration 21/25 | Loss: 0.00234560
Iteration 22/25 | Loss: 0.00234560
Iteration 23/25 | Loss: 0.00234560
Iteration 24/25 | Loss: 0.00234560
Iteration 25/25 | Loss: 0.00234560

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234560
Iteration 2/1000 | Loss: 0.00002821
Iteration 3/1000 | Loss: 0.00002012
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001501
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001399
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001207
Iteration 53/1000 | Loss: 0.00001207
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001202
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001197
Iteration 71/1000 | Loss: 0.00001197
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001196
Iteration 76/1000 | Loss: 0.00001196
Iteration 77/1000 | Loss: 0.00001196
Iteration 78/1000 | Loss: 0.00001196
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001191
Iteration 110/1000 | Loss: 0.00001191
Iteration 111/1000 | Loss: 0.00001191
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001190
Iteration 118/1000 | Loss: 0.00001190
Iteration 119/1000 | Loss: 0.00001190
Iteration 120/1000 | Loss: 0.00001190
Iteration 121/1000 | Loss: 0.00001190
Iteration 122/1000 | Loss: 0.00001190
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001190
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001185
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001184
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001182
Iteration 151/1000 | Loss: 0.00001182
Iteration 152/1000 | Loss: 0.00001182
Iteration 153/1000 | Loss: 0.00001182
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001181
Iteration 156/1000 | Loss: 0.00001181
Iteration 157/1000 | Loss: 0.00001181
Iteration 158/1000 | Loss: 0.00001181
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001180
Iteration 162/1000 | Loss: 0.00001180
Iteration 163/1000 | Loss: 0.00001180
Iteration 164/1000 | Loss: 0.00001180
Iteration 165/1000 | Loss: 0.00001180
Iteration 166/1000 | Loss: 0.00001180
Iteration 167/1000 | Loss: 0.00001180
Iteration 168/1000 | Loss: 0.00001180
Iteration 169/1000 | Loss: 0.00001179
Iteration 170/1000 | Loss: 0.00001179
Iteration 171/1000 | Loss: 0.00001179
Iteration 172/1000 | Loss: 0.00001179
Iteration 173/1000 | Loss: 0.00001179
Iteration 174/1000 | Loss: 0.00001179
Iteration 175/1000 | Loss: 0.00001179
Iteration 176/1000 | Loss: 0.00001179
Iteration 177/1000 | Loss: 0.00001179
Iteration 178/1000 | Loss: 0.00001178
Iteration 179/1000 | Loss: 0.00001178
Iteration 180/1000 | Loss: 0.00001178
Iteration 181/1000 | Loss: 0.00001178
Iteration 182/1000 | Loss: 0.00001178
Iteration 183/1000 | Loss: 0.00001178
Iteration 184/1000 | Loss: 0.00001178
Iteration 185/1000 | Loss: 0.00001178
Iteration 186/1000 | Loss: 0.00001177
Iteration 187/1000 | Loss: 0.00001177
Iteration 188/1000 | Loss: 0.00001176
Iteration 189/1000 | Loss: 0.00001175
Iteration 190/1000 | Loss: 0.00001175
Iteration 191/1000 | Loss: 0.00001174
Iteration 192/1000 | Loss: 0.00001174
Iteration 193/1000 | Loss: 0.00001174
Iteration 194/1000 | Loss: 0.00001174
Iteration 195/1000 | Loss: 0.00001174
Iteration 196/1000 | Loss: 0.00001174
Iteration 197/1000 | Loss: 0.00001174
Iteration 198/1000 | Loss: 0.00001174
Iteration 199/1000 | Loss: 0.00001174
Iteration 200/1000 | Loss: 0.00001174
Iteration 201/1000 | Loss: 0.00001173
Iteration 202/1000 | Loss: 0.00001173
Iteration 203/1000 | Loss: 0.00001173
Iteration 204/1000 | Loss: 0.00001173
Iteration 205/1000 | Loss: 0.00001173
Iteration 206/1000 | Loss: 0.00001173
Iteration 207/1000 | Loss: 0.00001173
Iteration 208/1000 | Loss: 0.00001173
Iteration 209/1000 | Loss: 0.00001172
Iteration 210/1000 | Loss: 0.00001172
Iteration 211/1000 | Loss: 0.00001172
Iteration 212/1000 | Loss: 0.00001172
Iteration 213/1000 | Loss: 0.00001172
Iteration 214/1000 | Loss: 0.00001172
Iteration 215/1000 | Loss: 0.00001172
Iteration 216/1000 | Loss: 0.00001172
Iteration 217/1000 | Loss: 0.00001172
Iteration 218/1000 | Loss: 0.00001172
Iteration 219/1000 | Loss: 0.00001172
Iteration 220/1000 | Loss: 0.00001171
Iteration 221/1000 | Loss: 0.00001171
Iteration 222/1000 | Loss: 0.00001171
Iteration 223/1000 | Loss: 0.00001171
Iteration 224/1000 | Loss: 0.00001171
Iteration 225/1000 | Loss: 0.00001171
Iteration 226/1000 | Loss: 0.00001171
Iteration 227/1000 | Loss: 0.00001171
Iteration 228/1000 | Loss: 0.00001171
Iteration 229/1000 | Loss: 0.00001171
Iteration 230/1000 | Loss: 0.00001171
Iteration 231/1000 | Loss: 0.00001171
Iteration 232/1000 | Loss: 0.00001171
Iteration 233/1000 | Loss: 0.00001170
Iteration 234/1000 | Loss: 0.00001170
Iteration 235/1000 | Loss: 0.00001170
Iteration 236/1000 | Loss: 0.00001170
Iteration 237/1000 | Loss: 0.00001170
Iteration 238/1000 | Loss: 0.00001169
Iteration 239/1000 | Loss: 0.00001169
Iteration 240/1000 | Loss: 0.00001169
Iteration 241/1000 | Loss: 0.00001169
Iteration 242/1000 | Loss: 0.00001169
Iteration 243/1000 | Loss: 0.00001169
Iteration 244/1000 | Loss: 0.00001169
Iteration 245/1000 | Loss: 0.00001169
Iteration 246/1000 | Loss: 0.00001168
Iteration 247/1000 | Loss: 0.00001168
Iteration 248/1000 | Loss: 0.00001168
Iteration 249/1000 | Loss: 0.00001168
Iteration 250/1000 | Loss: 0.00001168
Iteration 251/1000 | Loss: 0.00001167
Iteration 252/1000 | Loss: 0.00001167
Iteration 253/1000 | Loss: 0.00001167
Iteration 254/1000 | Loss: 0.00001167
Iteration 255/1000 | Loss: 0.00001167
Iteration 256/1000 | Loss: 0.00001167
Iteration 257/1000 | Loss: 0.00001167
Iteration 258/1000 | Loss: 0.00001167
Iteration 259/1000 | Loss: 0.00001167
Iteration 260/1000 | Loss: 0.00001167
Iteration 261/1000 | Loss: 0.00001166
Iteration 262/1000 | Loss: 0.00001166
Iteration 263/1000 | Loss: 0.00001166
Iteration 264/1000 | Loss: 0.00001166
Iteration 265/1000 | Loss: 0.00001166
Iteration 266/1000 | Loss: 0.00001166
Iteration 267/1000 | Loss: 0.00001166
Iteration 268/1000 | Loss: 0.00001166
Iteration 269/1000 | Loss: 0.00001166
Iteration 270/1000 | Loss: 0.00001166
Iteration 271/1000 | Loss: 0.00001165
Iteration 272/1000 | Loss: 0.00001165
Iteration 273/1000 | Loss: 0.00001165
Iteration 274/1000 | Loss: 0.00001165
Iteration 275/1000 | Loss: 0.00001165
Iteration 276/1000 | Loss: 0.00001164
Iteration 277/1000 | Loss: 0.00001164
Iteration 278/1000 | Loss: 0.00001164
Iteration 279/1000 | Loss: 0.00001164
Iteration 280/1000 | Loss: 0.00001164
Iteration 281/1000 | Loss: 0.00001164
Iteration 282/1000 | Loss: 0.00001164
Iteration 283/1000 | Loss: 0.00001164
Iteration 284/1000 | Loss: 0.00001164
Iteration 285/1000 | Loss: 0.00001164
Iteration 286/1000 | Loss: 0.00001164
Iteration 287/1000 | Loss: 0.00001164
Iteration 288/1000 | Loss: 0.00001164
Iteration 289/1000 | Loss: 0.00001164
Iteration 290/1000 | Loss: 0.00001164
Iteration 291/1000 | Loss: 0.00001164
Iteration 292/1000 | Loss: 0.00001164
Iteration 293/1000 | Loss: 0.00001164
Iteration 294/1000 | Loss: 0.00001164
Iteration 295/1000 | Loss: 0.00001164
Iteration 296/1000 | Loss: 0.00001164
Iteration 297/1000 | Loss: 0.00001164
Iteration 298/1000 | Loss: 0.00001164
Iteration 299/1000 | Loss: 0.00001164
Iteration 300/1000 | Loss: 0.00001164
Iteration 301/1000 | Loss: 0.00001164
Iteration 302/1000 | Loss: 0.00001164
Iteration 303/1000 | Loss: 0.00001164
Iteration 304/1000 | Loss: 0.00001164
Iteration 305/1000 | Loss: 0.00001164
Iteration 306/1000 | Loss: 0.00001164
Iteration 307/1000 | Loss: 0.00001164
Iteration 308/1000 | Loss: 0.00001164
Iteration 309/1000 | Loss: 0.00001164
Iteration 310/1000 | Loss: 0.00001164
Iteration 311/1000 | Loss: 0.00001164
Iteration 312/1000 | Loss: 0.00001164
Iteration 313/1000 | Loss: 0.00001164
Iteration 314/1000 | Loss: 0.00001164
Iteration 315/1000 | Loss: 0.00001164
Iteration 316/1000 | Loss: 0.00001164
Iteration 317/1000 | Loss: 0.00001164
Iteration 318/1000 | Loss: 0.00001164
Iteration 319/1000 | Loss: 0.00001164
Iteration 320/1000 | Loss: 0.00001164
Iteration 321/1000 | Loss: 0.00001164
Iteration 322/1000 | Loss: 0.00001164
Iteration 323/1000 | Loss: 0.00001164
Iteration 324/1000 | Loss: 0.00001164
Iteration 325/1000 | Loss: 0.00001164
Iteration 326/1000 | Loss: 0.00001164
Iteration 327/1000 | Loss: 0.00001164
Iteration 328/1000 | Loss: 0.00001164
Iteration 329/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [1.1636112503765617e-05, 1.1636112503765617e-05, 1.1636112503765617e-05, 1.1636112503765617e-05, 1.1636112503765617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1636112503765617e-05

Optimization complete. Final v2v error: 2.930410623550415 mm

Highest mean error: 3.1399433612823486 mm for frame 45

Lowest mean error: 2.7907636165618896 mm for frame 162

Saving results

Total time: 50.51621866226196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684869
Iteration 2/25 | Loss: 0.00170381
Iteration 3/25 | Loss: 0.00156777
Iteration 4/25 | Loss: 0.00155238
Iteration 5/25 | Loss: 0.00155116
Iteration 6/25 | Loss: 0.00155116
Iteration 7/25 | Loss: 0.00155116
Iteration 8/25 | Loss: 0.00155116
Iteration 9/25 | Loss: 0.00155116
Iteration 10/25 | Loss: 0.00155116
Iteration 11/25 | Loss: 0.00155116
Iteration 12/25 | Loss: 0.00155116
Iteration 13/25 | Loss: 0.00155116
Iteration 14/25 | Loss: 0.00155116
Iteration 15/25 | Loss: 0.00155116
Iteration 16/25 | Loss: 0.00155116
Iteration 17/25 | Loss: 0.00155116
Iteration 18/25 | Loss: 0.00155116
Iteration 19/25 | Loss: 0.00155116
Iteration 20/25 | Loss: 0.00155116
Iteration 21/25 | Loss: 0.00155116
Iteration 22/25 | Loss: 0.00155116
Iteration 23/25 | Loss: 0.00155116
Iteration 24/25 | Loss: 0.00155116
Iteration 25/25 | Loss: 0.00155116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015511647798120975, 0.0015511647798120975, 0.0015511647798120975, 0.0015511647798120975, 0.0015511647798120975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015511647798120975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20702946
Iteration 2/25 | Loss: 0.00215294
Iteration 3/25 | Loss: 0.00215292
Iteration 4/25 | Loss: 0.00215291
Iteration 5/25 | Loss: 0.00215291
Iteration 6/25 | Loss: 0.00215291
Iteration 7/25 | Loss: 0.00215291
Iteration 8/25 | Loss: 0.00215291
Iteration 9/25 | Loss: 0.00215291
Iteration 10/25 | Loss: 0.00215291
Iteration 11/25 | Loss: 0.00215291
Iteration 12/25 | Loss: 0.00215291
Iteration 13/25 | Loss: 0.00215291
Iteration 14/25 | Loss: 0.00215291
Iteration 15/25 | Loss: 0.00215291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021529130171984434, 0.0021529130171984434, 0.0021529130171984434, 0.0021529130171984434, 0.0021529130171984434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021529130171984434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215291
Iteration 2/1000 | Loss: 0.00003628
Iteration 3/1000 | Loss: 0.00002506
Iteration 4/1000 | Loss: 0.00002330
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002159
Iteration 7/1000 | Loss: 0.00002125
Iteration 8/1000 | Loss: 0.00002087
Iteration 9/1000 | Loss: 0.00002050
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001954
Iteration 14/1000 | Loss: 0.00001919
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001851
Iteration 17/1000 | Loss: 0.00001839
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001803
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001796
Iteration 32/1000 | Loss: 0.00001796
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001796
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001796
Iteration 39/1000 | Loss: 0.00001796
Iteration 40/1000 | Loss: 0.00001795
Iteration 41/1000 | Loss: 0.00001795
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001793
Iteration 47/1000 | Loss: 0.00001792
Iteration 48/1000 | Loss: 0.00001792
Iteration 49/1000 | Loss: 0.00001792
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001791
Iteration 52/1000 | Loss: 0.00001791
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001791
Iteration 55/1000 | Loss: 0.00001791
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001788
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001783
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001782
Iteration 96/1000 | Loss: 0.00001782
Iteration 97/1000 | Loss: 0.00001782
Iteration 98/1000 | Loss: 0.00001782
Iteration 99/1000 | Loss: 0.00001782
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.7817910702433437e-05, 1.7817910702433437e-05, 1.7817910702433437e-05, 1.7817910702433437e-05, 1.7817910702433437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7817910702433437e-05

Optimization complete. Final v2v error: 3.6139254570007324 mm

Highest mean error: 3.8004205226898193 mm for frame 91

Lowest mean error: 3.449641466140747 mm for frame 194

Saving results

Total time: 44.97183036804199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846727
Iteration 2/25 | Loss: 0.00203204
Iteration 3/25 | Loss: 0.00162674
Iteration 4/25 | Loss: 0.00157377
Iteration 5/25 | Loss: 0.00163565
Iteration 6/25 | Loss: 0.00154702
Iteration 7/25 | Loss: 0.00152007
Iteration 8/25 | Loss: 0.00151321
Iteration 9/25 | Loss: 0.00149658
Iteration 10/25 | Loss: 0.00149268
Iteration 11/25 | Loss: 0.00149239
Iteration 12/25 | Loss: 0.00149235
Iteration 13/25 | Loss: 0.00149235
Iteration 14/25 | Loss: 0.00149235
Iteration 15/25 | Loss: 0.00149235
Iteration 16/25 | Loss: 0.00149235
Iteration 17/25 | Loss: 0.00149235
Iteration 18/25 | Loss: 0.00149235
Iteration 19/25 | Loss: 0.00149234
Iteration 20/25 | Loss: 0.00149234
Iteration 21/25 | Loss: 0.00149234
Iteration 22/25 | Loss: 0.00149234
Iteration 23/25 | Loss: 0.00149234
Iteration 24/25 | Loss: 0.00149234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014923446578904986, 0.0014923446578904986, 0.0014923446578904986, 0.0014923446578904986, 0.0014923446578904986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014923446578904986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32335377
Iteration 2/25 | Loss: 0.00260104
Iteration 3/25 | Loss: 0.00260104
Iteration 4/25 | Loss: 0.00260104
Iteration 5/25 | Loss: 0.00260104
Iteration 6/25 | Loss: 0.00260104
Iteration 7/25 | Loss: 0.00260104
Iteration 8/25 | Loss: 0.00260104
Iteration 9/25 | Loss: 0.00260104
Iteration 10/25 | Loss: 0.00260104
Iteration 11/25 | Loss: 0.00260104
Iteration 12/25 | Loss: 0.00260104
Iteration 13/25 | Loss: 0.00260104
Iteration 14/25 | Loss: 0.00260104
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0026010372675955296, 0.0026010372675955296, 0.0026010372675955296, 0.0026010372675955296, 0.0026010372675955296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026010372675955296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260104
Iteration 2/1000 | Loss: 0.00004210
Iteration 3/1000 | Loss: 0.00008489
Iteration 4/1000 | Loss: 0.00007919
Iteration 5/1000 | Loss: 0.00002452
Iteration 6/1000 | Loss: 0.00005276
Iteration 7/1000 | Loss: 0.00002256
Iteration 8/1000 | Loss: 0.00002130
Iteration 9/1000 | Loss: 0.00006420
Iteration 10/1000 | Loss: 0.00002039
Iteration 11/1000 | Loss: 0.00001992
Iteration 12/1000 | Loss: 0.00001942
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001839
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00008187
Iteration 20/1000 | Loss: 0.00001782
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001765
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001746
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001744
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001727
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001727
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001723
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001721
Iteration 92/1000 | Loss: 0.00001721
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001715
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001714
Iteration 113/1000 | Loss: 0.00001714
Iteration 114/1000 | Loss: 0.00001713
Iteration 115/1000 | Loss: 0.00001713
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001712
Iteration 119/1000 | Loss: 0.00001712
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001709
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001709
Iteration 127/1000 | Loss: 0.00001709
Iteration 128/1000 | Loss: 0.00001708
Iteration 129/1000 | Loss: 0.00001708
Iteration 130/1000 | Loss: 0.00001708
Iteration 131/1000 | Loss: 0.00001708
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001707
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001707
Iteration 141/1000 | Loss: 0.00001707
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001707
Iteration 146/1000 | Loss: 0.00001706
Iteration 147/1000 | Loss: 0.00001706
Iteration 148/1000 | Loss: 0.00001706
Iteration 149/1000 | Loss: 0.00001706
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001705
Iteration 155/1000 | Loss: 0.00001705
Iteration 156/1000 | Loss: 0.00001705
Iteration 157/1000 | Loss: 0.00001705
Iteration 158/1000 | Loss: 0.00001705
Iteration 159/1000 | Loss: 0.00001705
Iteration 160/1000 | Loss: 0.00001704
Iteration 161/1000 | Loss: 0.00001704
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001704
Iteration 164/1000 | Loss: 0.00001704
Iteration 165/1000 | Loss: 0.00001704
Iteration 166/1000 | Loss: 0.00001704
Iteration 167/1000 | Loss: 0.00001704
Iteration 168/1000 | Loss: 0.00001704
Iteration 169/1000 | Loss: 0.00001704
Iteration 170/1000 | Loss: 0.00001704
Iteration 171/1000 | Loss: 0.00001704
Iteration 172/1000 | Loss: 0.00001704
Iteration 173/1000 | Loss: 0.00001703
Iteration 174/1000 | Loss: 0.00001703
Iteration 175/1000 | Loss: 0.00001703
Iteration 176/1000 | Loss: 0.00001703
Iteration 177/1000 | Loss: 0.00001703
Iteration 178/1000 | Loss: 0.00001703
Iteration 179/1000 | Loss: 0.00001703
Iteration 180/1000 | Loss: 0.00001702
Iteration 181/1000 | Loss: 0.00001702
Iteration 182/1000 | Loss: 0.00001702
Iteration 183/1000 | Loss: 0.00001702
Iteration 184/1000 | Loss: 0.00001702
Iteration 185/1000 | Loss: 0.00001702
Iteration 186/1000 | Loss: 0.00001702
Iteration 187/1000 | Loss: 0.00001702
Iteration 188/1000 | Loss: 0.00001702
Iteration 189/1000 | Loss: 0.00001702
Iteration 190/1000 | Loss: 0.00001702
Iteration 191/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.7021682651829906e-05, 1.7021682651829906e-05, 1.7021682651829906e-05, 1.7021682651829906e-05, 1.7021682651829906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7021682651829906e-05

Optimization complete. Final v2v error: 3.5762264728546143 mm

Highest mean error: 4.413379192352295 mm for frame 73

Lowest mean error: 3.2604310512542725 mm for frame 48

Saving results

Total time: 61.32053589820862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477116
Iteration 2/25 | Loss: 0.00155724
Iteration 3/25 | Loss: 0.00149215
Iteration 4/25 | Loss: 0.00148010
Iteration 5/25 | Loss: 0.00147580
Iteration 6/25 | Loss: 0.00147482
Iteration 7/25 | Loss: 0.00147482
Iteration 8/25 | Loss: 0.00147482
Iteration 9/25 | Loss: 0.00147482
Iteration 10/25 | Loss: 0.00147482
Iteration 11/25 | Loss: 0.00147482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014748231042176485, 0.0014748231042176485, 0.0014748231042176485, 0.0014748231042176485, 0.0014748231042176485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014748231042176485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.72578144
Iteration 2/25 | Loss: 0.00233512
Iteration 3/25 | Loss: 0.00233512
Iteration 4/25 | Loss: 0.00233512
Iteration 5/25 | Loss: 0.00233512
Iteration 6/25 | Loss: 0.00233512
Iteration 7/25 | Loss: 0.00233511
Iteration 8/25 | Loss: 0.00233511
Iteration 9/25 | Loss: 0.00233511
Iteration 10/25 | Loss: 0.00233511
Iteration 11/25 | Loss: 0.00233511
Iteration 12/25 | Loss: 0.00233511
Iteration 13/25 | Loss: 0.00233511
Iteration 14/25 | Loss: 0.00233511
Iteration 15/25 | Loss: 0.00233511
Iteration 16/25 | Loss: 0.00233511
Iteration 17/25 | Loss: 0.00233511
Iteration 18/25 | Loss: 0.00233511
Iteration 19/25 | Loss: 0.00233511
Iteration 20/25 | Loss: 0.00233511
Iteration 21/25 | Loss: 0.00233511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00233511277474463, 0.00233511277474463, 0.00233511277474463, 0.00233511277474463, 0.00233511277474463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00233511277474463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233511
Iteration 2/1000 | Loss: 0.00003724
Iteration 3/1000 | Loss: 0.00002539
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001917
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001749
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001591
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001514
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001491
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001459
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001455
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001451
Iteration 34/1000 | Loss: 0.00001450
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001449
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001441
Iteration 46/1000 | Loss: 0.00001440
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001437
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001436
Iteration 69/1000 | Loss: 0.00001436
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001433
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001430
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001427
Iteration 120/1000 | Loss: 0.00001426
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001425
Iteration 126/1000 | Loss: 0.00001425
Iteration 127/1000 | Loss: 0.00001425
Iteration 128/1000 | Loss: 0.00001425
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001425
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001422
Iteration 157/1000 | Loss: 0.00001422
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001422
Iteration 161/1000 | Loss: 0.00001422
Iteration 162/1000 | Loss: 0.00001422
Iteration 163/1000 | Loss: 0.00001422
Iteration 164/1000 | Loss: 0.00001422
Iteration 165/1000 | Loss: 0.00001422
Iteration 166/1000 | Loss: 0.00001422
Iteration 167/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.422360583092086e-05, 1.422360583092086e-05, 1.422360583092086e-05, 1.422360583092086e-05, 1.422360583092086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.422360583092086e-05

Optimization complete. Final v2v error: 3.278521776199341 mm

Highest mean error: 3.5959062576293945 mm for frame 106

Lowest mean error: 3.126272678375244 mm for frame 31

Saving results

Total time: 44.7593092918396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783134
Iteration 2/25 | Loss: 0.00152914
Iteration 3/25 | Loss: 0.00145837
Iteration 4/25 | Loss: 0.00145064
Iteration 5/25 | Loss: 0.00144867
Iteration 6/25 | Loss: 0.00144866
Iteration 7/25 | Loss: 0.00144867
Iteration 8/25 | Loss: 0.00144867
Iteration 9/25 | Loss: 0.00144867
Iteration 10/25 | Loss: 0.00144866
Iteration 11/25 | Loss: 0.00144866
Iteration 12/25 | Loss: 0.00144867
Iteration 13/25 | Loss: 0.00144866
Iteration 14/25 | Loss: 0.00144866
Iteration 15/25 | Loss: 0.00144866
Iteration 16/25 | Loss: 0.00144866
Iteration 17/25 | Loss: 0.00144867
Iteration 18/25 | Loss: 0.00144866
Iteration 19/25 | Loss: 0.00144866
Iteration 20/25 | Loss: 0.00144866
Iteration 21/25 | Loss: 0.00144867
Iteration 22/25 | Loss: 0.00144867
Iteration 23/25 | Loss: 0.00144867
Iteration 24/25 | Loss: 0.00144866
Iteration 25/25 | Loss: 0.00144866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24522281
Iteration 2/25 | Loss: 0.00244472
Iteration 3/25 | Loss: 0.00244472
Iteration 4/25 | Loss: 0.00244472
Iteration 5/25 | Loss: 0.00244471
Iteration 6/25 | Loss: 0.00244471
Iteration 7/25 | Loss: 0.00244471
Iteration 8/25 | Loss: 0.00244471
Iteration 9/25 | Loss: 0.00244471
Iteration 10/25 | Loss: 0.00244471
Iteration 11/25 | Loss: 0.00244471
Iteration 12/25 | Loss: 0.00244471
Iteration 13/25 | Loss: 0.00244471
Iteration 14/25 | Loss: 0.00244471
Iteration 15/25 | Loss: 0.00244471
Iteration 16/25 | Loss: 0.00244471
Iteration 17/25 | Loss: 0.00244471
Iteration 18/25 | Loss: 0.00244471
Iteration 19/25 | Loss: 0.00244471
Iteration 20/25 | Loss: 0.00244471
Iteration 21/25 | Loss: 0.00244471
Iteration 22/25 | Loss: 0.00244471
Iteration 23/25 | Loss: 0.00244471
Iteration 24/25 | Loss: 0.00244471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0024447133764624596, 0.0024447133764624596, 0.0024447133764624596, 0.0024447133764624596, 0.0024447133764624596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024447133764624596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244471
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001721
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001382
Iteration 9/1000 | Loss: 0.00001341
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001294
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001246
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001237
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001213
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001209
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001205
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001195
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001176
Iteration 139/1000 | Loss: 0.00001176
Iteration 140/1000 | Loss: 0.00001176
Iteration 141/1000 | Loss: 0.00001176
Iteration 142/1000 | Loss: 0.00001176
Iteration 143/1000 | Loss: 0.00001175
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001175
Iteration 147/1000 | Loss: 0.00001175
Iteration 148/1000 | Loss: 0.00001175
Iteration 149/1000 | Loss: 0.00001175
Iteration 150/1000 | Loss: 0.00001174
Iteration 151/1000 | Loss: 0.00001174
Iteration 152/1000 | Loss: 0.00001174
Iteration 153/1000 | Loss: 0.00001173
Iteration 154/1000 | Loss: 0.00001173
Iteration 155/1000 | Loss: 0.00001173
Iteration 156/1000 | Loss: 0.00001173
Iteration 157/1000 | Loss: 0.00001173
Iteration 158/1000 | Loss: 0.00001173
Iteration 159/1000 | Loss: 0.00001173
Iteration 160/1000 | Loss: 0.00001173
Iteration 161/1000 | Loss: 0.00001172
Iteration 162/1000 | Loss: 0.00001172
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001171
Iteration 165/1000 | Loss: 0.00001171
Iteration 166/1000 | Loss: 0.00001171
Iteration 167/1000 | Loss: 0.00001170
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001170
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001170
Iteration 172/1000 | Loss: 0.00001169
Iteration 173/1000 | Loss: 0.00001169
Iteration 174/1000 | Loss: 0.00001169
Iteration 175/1000 | Loss: 0.00001169
Iteration 176/1000 | Loss: 0.00001168
Iteration 177/1000 | Loss: 0.00001168
Iteration 178/1000 | Loss: 0.00001168
Iteration 179/1000 | Loss: 0.00001168
Iteration 180/1000 | Loss: 0.00001168
Iteration 181/1000 | Loss: 0.00001167
Iteration 182/1000 | Loss: 0.00001167
Iteration 183/1000 | Loss: 0.00001167
Iteration 184/1000 | Loss: 0.00001167
Iteration 185/1000 | Loss: 0.00001167
Iteration 186/1000 | Loss: 0.00001166
Iteration 187/1000 | Loss: 0.00001166
Iteration 188/1000 | Loss: 0.00001166
Iteration 189/1000 | Loss: 0.00001166
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001164
Iteration 194/1000 | Loss: 0.00001164
Iteration 195/1000 | Loss: 0.00001164
Iteration 196/1000 | Loss: 0.00001164
Iteration 197/1000 | Loss: 0.00001164
Iteration 198/1000 | Loss: 0.00001164
Iteration 199/1000 | Loss: 0.00001164
Iteration 200/1000 | Loss: 0.00001164
Iteration 201/1000 | Loss: 0.00001164
Iteration 202/1000 | Loss: 0.00001164
Iteration 203/1000 | Loss: 0.00001164
Iteration 204/1000 | Loss: 0.00001164
Iteration 205/1000 | Loss: 0.00001163
Iteration 206/1000 | Loss: 0.00001163
Iteration 207/1000 | Loss: 0.00001163
Iteration 208/1000 | Loss: 0.00001163
Iteration 209/1000 | Loss: 0.00001163
Iteration 210/1000 | Loss: 0.00001162
Iteration 211/1000 | Loss: 0.00001162
Iteration 212/1000 | Loss: 0.00001162
Iteration 213/1000 | Loss: 0.00001161
Iteration 214/1000 | Loss: 0.00001161
Iteration 215/1000 | Loss: 0.00001161
Iteration 216/1000 | Loss: 0.00001161
Iteration 217/1000 | Loss: 0.00001161
Iteration 218/1000 | Loss: 0.00001161
Iteration 219/1000 | Loss: 0.00001161
Iteration 220/1000 | Loss: 0.00001161
Iteration 221/1000 | Loss: 0.00001161
Iteration 222/1000 | Loss: 0.00001161
Iteration 223/1000 | Loss: 0.00001161
Iteration 224/1000 | Loss: 0.00001161
Iteration 225/1000 | Loss: 0.00001161
Iteration 226/1000 | Loss: 0.00001161
Iteration 227/1000 | Loss: 0.00001161
Iteration 228/1000 | Loss: 0.00001161
Iteration 229/1000 | Loss: 0.00001161
Iteration 230/1000 | Loss: 0.00001161
Iteration 231/1000 | Loss: 0.00001161
Iteration 232/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.1613878996286076e-05, 1.1613878996286076e-05, 1.1613878996286076e-05, 1.1613878996286076e-05, 1.1613878996286076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1613878996286076e-05

Optimization complete. Final v2v error: 2.929725408554077 mm

Highest mean error: 3.1444509029388428 mm for frame 52

Lowest mean error: 2.796079635620117 mm for frame 3

Saving results

Total time: 47.04631805419922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00306994
Iteration 2/25 | Loss: 0.00159824
Iteration 3/25 | Loss: 0.00147522
Iteration 4/25 | Loss: 0.00145900
Iteration 5/25 | Loss: 0.00145319
Iteration 6/25 | Loss: 0.00145088
Iteration 7/25 | Loss: 0.00144981
Iteration 8/25 | Loss: 0.00144918
Iteration 9/25 | Loss: 0.00144901
Iteration 10/25 | Loss: 0.00144901
Iteration 11/25 | Loss: 0.00144901
Iteration 12/25 | Loss: 0.00144901
Iteration 13/25 | Loss: 0.00144901
Iteration 14/25 | Loss: 0.00144901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014490056782960892, 0.0014490056782960892, 0.0014490056782960892, 0.0014490056782960892, 0.0014490056782960892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014490056782960892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18089163
Iteration 2/25 | Loss: 0.00344630
Iteration 3/25 | Loss: 0.00344630
Iteration 4/25 | Loss: 0.00344630
Iteration 5/25 | Loss: 0.00344630
Iteration 6/25 | Loss: 0.00344630
Iteration 7/25 | Loss: 0.00344630
Iteration 8/25 | Loss: 0.00344630
Iteration 9/25 | Loss: 0.00344630
Iteration 10/25 | Loss: 0.00344630
Iteration 11/25 | Loss: 0.00344630
Iteration 12/25 | Loss: 0.00344630
Iteration 13/25 | Loss: 0.00344630
Iteration 14/25 | Loss: 0.00344630
Iteration 15/25 | Loss: 0.00344629
Iteration 16/25 | Loss: 0.00344629
Iteration 17/25 | Loss: 0.00344630
Iteration 18/25 | Loss: 0.00344630
Iteration 19/25 | Loss: 0.00344630
Iteration 20/25 | Loss: 0.00344630
Iteration 21/25 | Loss: 0.00344630
Iteration 22/25 | Loss: 0.00344630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0034462951589375734, 0.0034462951589375734, 0.0034462951589375734, 0.0034462951589375734, 0.0034462951589375734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034462951589375734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344630
Iteration 2/1000 | Loss: 0.00004125
Iteration 3/1000 | Loss: 0.00002558
Iteration 4/1000 | Loss: 0.00002258
Iteration 5/1000 | Loss: 0.00002072
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001859
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001748
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001605
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001589
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001582
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001581
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001573
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001566
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001559
Iteration 90/1000 | Loss: 0.00001558
Iteration 91/1000 | Loss: 0.00001558
Iteration 92/1000 | Loss: 0.00001557
Iteration 93/1000 | Loss: 0.00001557
Iteration 94/1000 | Loss: 0.00001557
Iteration 95/1000 | Loss: 0.00001557
Iteration 96/1000 | Loss: 0.00001557
Iteration 97/1000 | Loss: 0.00001556
Iteration 98/1000 | Loss: 0.00001556
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001555
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001552
Iteration 117/1000 | Loss: 0.00001552
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001547
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001547
Iteration 143/1000 | Loss: 0.00001547
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001547
Iteration 146/1000 | Loss: 0.00001547
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001545
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001545
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001543
Iteration 160/1000 | Loss: 0.00001543
Iteration 161/1000 | Loss: 0.00001543
Iteration 162/1000 | Loss: 0.00001543
Iteration 163/1000 | Loss: 0.00001543
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001542
Iteration 166/1000 | Loss: 0.00001542
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001540
Iteration 181/1000 | Loss: 0.00001539
Iteration 182/1000 | Loss: 0.00001539
Iteration 183/1000 | Loss: 0.00001539
Iteration 184/1000 | Loss: 0.00001539
Iteration 185/1000 | Loss: 0.00001539
Iteration 186/1000 | Loss: 0.00001539
Iteration 187/1000 | Loss: 0.00001539
Iteration 188/1000 | Loss: 0.00001539
Iteration 189/1000 | Loss: 0.00001539
Iteration 190/1000 | Loss: 0.00001539
Iteration 191/1000 | Loss: 0.00001539
Iteration 192/1000 | Loss: 0.00001539
Iteration 193/1000 | Loss: 0.00001539
Iteration 194/1000 | Loss: 0.00001538
Iteration 195/1000 | Loss: 0.00001538
Iteration 196/1000 | Loss: 0.00001538
Iteration 197/1000 | Loss: 0.00001538
Iteration 198/1000 | Loss: 0.00001538
Iteration 199/1000 | Loss: 0.00001538
Iteration 200/1000 | Loss: 0.00001537
Iteration 201/1000 | Loss: 0.00001537
Iteration 202/1000 | Loss: 0.00001537
Iteration 203/1000 | Loss: 0.00001537
Iteration 204/1000 | Loss: 0.00001537
Iteration 205/1000 | Loss: 0.00001537
Iteration 206/1000 | Loss: 0.00001537
Iteration 207/1000 | Loss: 0.00001537
Iteration 208/1000 | Loss: 0.00001537
Iteration 209/1000 | Loss: 0.00001537
Iteration 210/1000 | Loss: 0.00001537
Iteration 211/1000 | Loss: 0.00001537
Iteration 212/1000 | Loss: 0.00001537
Iteration 213/1000 | Loss: 0.00001537
Iteration 214/1000 | Loss: 0.00001537
Iteration 215/1000 | Loss: 0.00001537
Iteration 216/1000 | Loss: 0.00001537
Iteration 217/1000 | Loss: 0.00001537
Iteration 218/1000 | Loss: 0.00001537
Iteration 219/1000 | Loss: 0.00001537
Iteration 220/1000 | Loss: 0.00001537
Iteration 221/1000 | Loss: 0.00001537
Iteration 222/1000 | Loss: 0.00001537
Iteration 223/1000 | Loss: 0.00001537
Iteration 224/1000 | Loss: 0.00001537
Iteration 225/1000 | Loss: 0.00001537
Iteration 226/1000 | Loss: 0.00001537
Iteration 227/1000 | Loss: 0.00001537
Iteration 228/1000 | Loss: 0.00001537
Iteration 229/1000 | Loss: 0.00001537
Iteration 230/1000 | Loss: 0.00001537
Iteration 231/1000 | Loss: 0.00001537
Iteration 232/1000 | Loss: 0.00001537
Iteration 233/1000 | Loss: 0.00001537
Iteration 234/1000 | Loss: 0.00001537
Iteration 235/1000 | Loss: 0.00001537
Iteration 236/1000 | Loss: 0.00001537
Iteration 237/1000 | Loss: 0.00001537
Iteration 238/1000 | Loss: 0.00001537
Iteration 239/1000 | Loss: 0.00001537
Iteration 240/1000 | Loss: 0.00001537
Iteration 241/1000 | Loss: 0.00001537
Iteration 242/1000 | Loss: 0.00001537
Iteration 243/1000 | Loss: 0.00001537
Iteration 244/1000 | Loss: 0.00001537
Iteration 245/1000 | Loss: 0.00001537
Iteration 246/1000 | Loss: 0.00001537
Iteration 247/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.5369720131275244e-05, 1.5369720131275244e-05, 1.5369720131275244e-05, 1.5369720131275244e-05, 1.5369720131275244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5369720131275244e-05

Optimization complete. Final v2v error: 3.344820261001587 mm

Highest mean error: 4.042051792144775 mm for frame 107

Lowest mean error: 2.9747776985168457 mm for frame 13

Saving results

Total time: 52.506624937057495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032662
Iteration 2/25 | Loss: 0.01032661
Iteration 3/25 | Loss: 0.01032661
Iteration 4/25 | Loss: 0.01032661
Iteration 5/25 | Loss: 0.01032661
Iteration 6/25 | Loss: 0.01032661
Iteration 7/25 | Loss: 0.01032661
Iteration 8/25 | Loss: 0.01032661
Iteration 9/25 | Loss: 0.01032661
Iteration 10/25 | Loss: 0.01032661
Iteration 11/25 | Loss: 0.01032661
Iteration 12/25 | Loss: 0.01032661
Iteration 13/25 | Loss: 0.01032661
Iteration 14/25 | Loss: 0.01032661
Iteration 15/25 | Loss: 0.01032661
Iteration 16/25 | Loss: 0.01032661
Iteration 17/25 | Loss: 0.01032661
Iteration 18/25 | Loss: 0.01032661
Iteration 19/25 | Loss: 0.01032661
Iteration 20/25 | Loss: 0.01032661
Iteration 21/25 | Loss: 0.01032661
Iteration 22/25 | Loss: 0.01032661
Iteration 23/25 | Loss: 0.01032661
Iteration 24/25 | Loss: 0.01032660
Iteration 25/25 | Loss: 0.01032660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70011437
Iteration 2/25 | Loss: 0.09264059
Iteration 3/25 | Loss: 0.09255450
Iteration 4/25 | Loss: 0.09227657
Iteration 5/25 | Loss: 0.09228800
Iteration 6/25 | Loss: 0.09225804
Iteration 7/25 | Loss: 0.09225801
Iteration 8/25 | Loss: 0.09225801
Iteration 9/25 | Loss: 0.09225801
Iteration 10/25 | Loss: 0.09225800
Iteration 11/25 | Loss: 0.09225800
Iteration 12/25 | Loss: 0.09225800
Iteration 13/25 | Loss: 0.09225799
Iteration 14/25 | Loss: 0.09225799
Iteration 15/25 | Loss: 0.09225799
Iteration 16/25 | Loss: 0.09225799
Iteration 17/25 | Loss: 0.09225799
Iteration 18/25 | Loss: 0.09225799
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.09225799143314362, 0.09225799143314362, 0.09225799143314362, 0.09225799143314362, 0.09225799143314362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09225799143314362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09225799
Iteration 2/1000 | Loss: 0.00775741
Iteration 3/1000 | Loss: 0.00598457
Iteration 4/1000 | Loss: 0.00112804
Iteration 5/1000 | Loss: 0.00381756
Iteration 6/1000 | Loss: 0.00178308
Iteration 7/1000 | Loss: 0.00127795
Iteration 8/1000 | Loss: 0.00235490
Iteration 9/1000 | Loss: 0.00025119
Iteration 10/1000 | Loss: 0.00042296
Iteration 11/1000 | Loss: 0.00185943
Iteration 12/1000 | Loss: 0.00089208
Iteration 13/1000 | Loss: 0.00104857
Iteration 14/1000 | Loss: 0.00006481
Iteration 15/1000 | Loss: 0.00060109
Iteration 16/1000 | Loss: 0.00007955
Iteration 17/1000 | Loss: 0.00150203
Iteration 18/1000 | Loss: 0.00043003
Iteration 19/1000 | Loss: 0.00138757
Iteration 20/1000 | Loss: 0.00090827
Iteration 21/1000 | Loss: 0.00041954
Iteration 22/1000 | Loss: 0.00007163
Iteration 23/1000 | Loss: 0.00024577
Iteration 24/1000 | Loss: 0.00091389
Iteration 25/1000 | Loss: 0.00051060
Iteration 26/1000 | Loss: 0.00007410
Iteration 27/1000 | Loss: 0.00005289
Iteration 28/1000 | Loss: 0.00004057
Iteration 29/1000 | Loss: 0.00003658
Iteration 30/1000 | Loss: 0.00016356
Iteration 31/1000 | Loss: 0.00014093
Iteration 32/1000 | Loss: 0.00003403
Iteration 33/1000 | Loss: 0.00003265
Iteration 34/1000 | Loss: 0.00075662
Iteration 35/1000 | Loss: 0.00006118
Iteration 36/1000 | Loss: 0.00024296
Iteration 37/1000 | Loss: 0.00062237
Iteration 38/1000 | Loss: 0.00005687
Iteration 39/1000 | Loss: 0.00018259
Iteration 40/1000 | Loss: 0.00041101
Iteration 41/1000 | Loss: 0.00003459
Iteration 42/1000 | Loss: 0.00002984
Iteration 43/1000 | Loss: 0.00002885
Iteration 44/1000 | Loss: 0.00080871
Iteration 45/1000 | Loss: 0.00094594
Iteration 46/1000 | Loss: 0.00007698
Iteration 47/1000 | Loss: 0.00002684
Iteration 48/1000 | Loss: 0.00003221
Iteration 49/1000 | Loss: 0.00002416
Iteration 50/1000 | Loss: 0.00004159
Iteration 51/1000 | Loss: 0.00002325
Iteration 52/1000 | Loss: 0.00002270
Iteration 53/1000 | Loss: 0.00002240
Iteration 54/1000 | Loss: 0.00004608
Iteration 55/1000 | Loss: 0.00015778
Iteration 56/1000 | Loss: 0.00037984
Iteration 57/1000 | Loss: 0.00004922
Iteration 58/1000 | Loss: 0.00002243
Iteration 59/1000 | Loss: 0.00002147
Iteration 60/1000 | Loss: 0.00010704
Iteration 61/1000 | Loss: 0.00048439
Iteration 62/1000 | Loss: 0.00025048
Iteration 63/1000 | Loss: 0.00009550
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002094
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002060
Iteration 68/1000 | Loss: 0.00002060
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002051
Iteration 71/1000 | Loss: 0.00002050
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002046
Iteration 77/1000 | Loss: 0.00002934
Iteration 78/1000 | Loss: 0.00002037
Iteration 79/1000 | Loss: 0.00002037
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002033
Iteration 82/1000 | Loss: 0.00014000
Iteration 83/1000 | Loss: 0.00073895
Iteration 84/1000 | Loss: 0.00007013
Iteration 85/1000 | Loss: 0.00003801
Iteration 86/1000 | Loss: 0.00002036
Iteration 87/1000 | Loss: 0.00002021
Iteration 88/1000 | Loss: 0.00002016
Iteration 89/1000 | Loss: 0.00002014
Iteration 90/1000 | Loss: 0.00002014
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002012
Iteration 96/1000 | Loss: 0.00002012
Iteration 97/1000 | Loss: 0.00002011
Iteration 98/1000 | Loss: 0.00002011
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002008
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002007
Iteration 107/1000 | Loss: 0.00002007
Iteration 108/1000 | Loss: 0.00002007
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00009585
Iteration 118/1000 | Loss: 0.00009939
Iteration 119/1000 | Loss: 0.00002499
Iteration 120/1000 | Loss: 0.00004759
Iteration 121/1000 | Loss: 0.00002181
Iteration 122/1000 | Loss: 0.00002196
Iteration 123/1000 | Loss: 0.00002011
Iteration 124/1000 | Loss: 0.00002007
Iteration 125/1000 | Loss: 0.00002007
Iteration 126/1000 | Loss: 0.00002007
Iteration 127/1000 | Loss: 0.00002007
Iteration 128/1000 | Loss: 0.00002416
Iteration 129/1000 | Loss: 0.00002017
Iteration 130/1000 | Loss: 0.00002011
Iteration 131/1000 | Loss: 0.00002009
Iteration 132/1000 | Loss: 0.00002009
Iteration 133/1000 | Loss: 0.00002009
Iteration 134/1000 | Loss: 0.00002009
Iteration 135/1000 | Loss: 0.00002009
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002012
Iteration 138/1000 | Loss: 0.00002012
Iteration 139/1000 | Loss: 0.00002010
Iteration 140/1000 | Loss: 0.00002010
Iteration 141/1000 | Loss: 0.00002010
Iteration 142/1000 | Loss: 0.00002010
Iteration 143/1000 | Loss: 0.00002010
Iteration 144/1000 | Loss: 0.00002010
Iteration 145/1000 | Loss: 0.00002010
Iteration 146/1000 | Loss: 0.00002010
Iteration 147/1000 | Loss: 0.00002010
Iteration 148/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.0100635083508678e-05, 2.0100635083508678e-05, 2.0100635083508678e-05, 2.0100635083508678e-05, 2.0100635083508678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0100635083508678e-05

Optimization complete. Final v2v error: 3.8530080318450928 mm

Highest mean error: 4.8562235832214355 mm for frame 110

Lowest mean error: 3.4717280864715576 mm for frame 129

Saving results

Total time: 147.66084098815918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523875
Iteration 2/25 | Loss: 0.00160596
Iteration 3/25 | Loss: 0.00152034
Iteration 4/25 | Loss: 0.00151170
Iteration 5/25 | Loss: 0.00150944
Iteration 6/25 | Loss: 0.00150926
Iteration 7/25 | Loss: 0.00150926
Iteration 8/25 | Loss: 0.00150926
Iteration 9/25 | Loss: 0.00150926
Iteration 10/25 | Loss: 0.00150926
Iteration 11/25 | Loss: 0.00150926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001509259338490665, 0.001509259338490665, 0.001509259338490665, 0.001509259338490665, 0.001509259338490665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001509259338490665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.30671024
Iteration 2/25 | Loss: 0.00207887
Iteration 3/25 | Loss: 0.00207885
Iteration 4/25 | Loss: 0.00207885
Iteration 5/25 | Loss: 0.00207885
Iteration 6/25 | Loss: 0.00207885
Iteration 7/25 | Loss: 0.00207885
Iteration 8/25 | Loss: 0.00207885
Iteration 9/25 | Loss: 0.00207885
Iteration 10/25 | Loss: 0.00207885
Iteration 11/25 | Loss: 0.00207885
Iteration 12/25 | Loss: 0.00207885
Iteration 13/25 | Loss: 0.00207885
Iteration 14/25 | Loss: 0.00207885
Iteration 15/25 | Loss: 0.00207885
Iteration 16/25 | Loss: 0.00207885
Iteration 17/25 | Loss: 0.00207885
Iteration 18/25 | Loss: 0.00207885
Iteration 19/25 | Loss: 0.00207885
Iteration 20/25 | Loss: 0.00207885
Iteration 21/25 | Loss: 0.00207885
Iteration 22/25 | Loss: 0.00207885
Iteration 23/25 | Loss: 0.00207885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002078848658129573, 0.002078848658129573, 0.002078848658129573, 0.002078848658129573, 0.002078848658129573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002078848658129573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207885
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00002003
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001867
Iteration 8/1000 | Loss: 0.00001810
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001714
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001657
Iteration 13/1000 | Loss: 0.00001639
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001610
Iteration 17/1000 | Loss: 0.00001605
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001604
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001596
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001593
Iteration 24/1000 | Loss: 0.00001592
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001591
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001590
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001584
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001584
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001571
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001570
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001568
Iteration 48/1000 | Loss: 0.00001568
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001563
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001563
Iteration 78/1000 | Loss: 0.00001563
Iteration 79/1000 | Loss: 0.00001563
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001563
Iteration 82/1000 | Loss: 0.00001563
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001562
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001561
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001558
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001558
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001557
Iteration 144/1000 | Loss: 0.00001557
Iteration 145/1000 | Loss: 0.00001556
Iteration 146/1000 | Loss: 0.00001556
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001556
Iteration 150/1000 | Loss: 0.00001556
Iteration 151/1000 | Loss: 0.00001556
Iteration 152/1000 | Loss: 0.00001556
Iteration 153/1000 | Loss: 0.00001556
Iteration 154/1000 | Loss: 0.00001556
Iteration 155/1000 | Loss: 0.00001556
Iteration 156/1000 | Loss: 0.00001556
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001555
Iteration 162/1000 | Loss: 0.00001555
Iteration 163/1000 | Loss: 0.00001555
Iteration 164/1000 | Loss: 0.00001555
Iteration 165/1000 | Loss: 0.00001555
Iteration 166/1000 | Loss: 0.00001555
Iteration 167/1000 | Loss: 0.00001555
Iteration 168/1000 | Loss: 0.00001555
Iteration 169/1000 | Loss: 0.00001555
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001554
Iteration 172/1000 | Loss: 0.00001554
Iteration 173/1000 | Loss: 0.00001554
Iteration 174/1000 | Loss: 0.00001554
Iteration 175/1000 | Loss: 0.00001554
Iteration 176/1000 | Loss: 0.00001554
Iteration 177/1000 | Loss: 0.00001554
Iteration 178/1000 | Loss: 0.00001554
Iteration 179/1000 | Loss: 0.00001554
Iteration 180/1000 | Loss: 0.00001554
Iteration 181/1000 | Loss: 0.00001554
Iteration 182/1000 | Loss: 0.00001554
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001553
Iteration 185/1000 | Loss: 0.00001553
Iteration 186/1000 | Loss: 0.00001553
Iteration 187/1000 | Loss: 0.00001553
Iteration 188/1000 | Loss: 0.00001553
Iteration 189/1000 | Loss: 0.00001553
Iteration 190/1000 | Loss: 0.00001553
Iteration 191/1000 | Loss: 0.00001553
Iteration 192/1000 | Loss: 0.00001553
Iteration 193/1000 | Loss: 0.00001553
Iteration 194/1000 | Loss: 0.00001553
Iteration 195/1000 | Loss: 0.00001553
Iteration 196/1000 | Loss: 0.00001553
Iteration 197/1000 | Loss: 0.00001553
Iteration 198/1000 | Loss: 0.00001553
Iteration 199/1000 | Loss: 0.00001553
Iteration 200/1000 | Loss: 0.00001553
Iteration 201/1000 | Loss: 0.00001553
Iteration 202/1000 | Loss: 0.00001553
Iteration 203/1000 | Loss: 0.00001553
Iteration 204/1000 | Loss: 0.00001553
Iteration 205/1000 | Loss: 0.00001553
Iteration 206/1000 | Loss: 0.00001553
Iteration 207/1000 | Loss: 0.00001553
Iteration 208/1000 | Loss: 0.00001553
Iteration 209/1000 | Loss: 0.00001553
Iteration 210/1000 | Loss: 0.00001553
Iteration 211/1000 | Loss: 0.00001553
Iteration 212/1000 | Loss: 0.00001553
Iteration 213/1000 | Loss: 0.00001553
Iteration 214/1000 | Loss: 0.00001553
Iteration 215/1000 | Loss: 0.00001553
Iteration 216/1000 | Loss: 0.00001553
Iteration 217/1000 | Loss: 0.00001553
Iteration 218/1000 | Loss: 0.00001553
Iteration 219/1000 | Loss: 0.00001553
Iteration 220/1000 | Loss: 0.00001553
Iteration 221/1000 | Loss: 0.00001553
Iteration 222/1000 | Loss: 0.00001553
Iteration 223/1000 | Loss: 0.00001553
Iteration 224/1000 | Loss: 0.00001553
Iteration 225/1000 | Loss: 0.00001553
Iteration 226/1000 | Loss: 0.00001553
Iteration 227/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.5532039469690062e-05, 1.5532039469690062e-05, 1.5532039469690062e-05, 1.5532039469690062e-05, 1.5532039469690062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5532039469690062e-05

Optimization complete. Final v2v error: 3.3511130809783936 mm

Highest mean error: 3.9283394813537598 mm for frame 68

Lowest mean error: 3.0063068866729736 mm for frame 26

Saving results

Total time: 45.19797468185425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478914
Iteration 2/25 | Loss: 0.00156129
Iteration 3/25 | Loss: 0.00149387
Iteration 4/25 | Loss: 0.00147952
Iteration 5/25 | Loss: 0.00147440
Iteration 6/25 | Loss: 0.00147366
Iteration 7/25 | Loss: 0.00147366
Iteration 8/25 | Loss: 0.00147359
Iteration 9/25 | Loss: 0.00147359
Iteration 10/25 | Loss: 0.00147359
Iteration 11/25 | Loss: 0.00147359
Iteration 12/25 | Loss: 0.00147359
Iteration 13/25 | Loss: 0.00147359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014735945733264089, 0.0014735945733264089, 0.0014735945733264089, 0.0014735945733264089, 0.0014735945733264089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014735945733264089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40238857
Iteration 2/25 | Loss: 0.00232503
Iteration 3/25 | Loss: 0.00232503
Iteration 4/25 | Loss: 0.00232503
Iteration 5/25 | Loss: 0.00232503
Iteration 6/25 | Loss: 0.00232503
Iteration 7/25 | Loss: 0.00232503
Iteration 8/25 | Loss: 0.00232503
Iteration 9/25 | Loss: 0.00232503
Iteration 10/25 | Loss: 0.00232503
Iteration 11/25 | Loss: 0.00232503
Iteration 12/25 | Loss: 0.00232503
Iteration 13/25 | Loss: 0.00232503
Iteration 14/25 | Loss: 0.00232503
Iteration 15/25 | Loss: 0.00232503
Iteration 16/25 | Loss: 0.00232503
Iteration 17/25 | Loss: 0.00232503
Iteration 18/25 | Loss: 0.00232503
Iteration 19/25 | Loss: 0.00232503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002325027948245406, 0.002325027948245406, 0.002325027948245406, 0.002325027948245406, 0.002325027948245406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002325027948245406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232503
Iteration 2/1000 | Loss: 0.00003285
Iteration 3/1000 | Loss: 0.00002571
Iteration 4/1000 | Loss: 0.00002360
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002164
Iteration 7/1000 | Loss: 0.00002083
Iteration 8/1000 | Loss: 0.00002027
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001895
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001832
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001807
Iteration 20/1000 | Loss: 0.00001806
Iteration 21/1000 | Loss: 0.00001804
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001785
Iteration 32/1000 | Loss: 0.00001784
Iteration 33/1000 | Loss: 0.00001784
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001777
Iteration 38/1000 | Loss: 0.00001776
Iteration 39/1000 | Loss: 0.00001776
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001772
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001771
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001769
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001763
Iteration 63/1000 | Loss: 0.00001763
Iteration 64/1000 | Loss: 0.00001763
Iteration 65/1000 | Loss: 0.00001763
Iteration 66/1000 | Loss: 0.00001763
Iteration 67/1000 | Loss: 0.00001763
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001757
Iteration 82/1000 | Loss: 0.00001756
Iteration 83/1000 | Loss: 0.00001756
Iteration 84/1000 | Loss: 0.00001756
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001755
Iteration 87/1000 | Loss: 0.00001755
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001754
Iteration 92/1000 | Loss: 0.00001754
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001753
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001752
Iteration 105/1000 | Loss: 0.00001752
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001751
Iteration 110/1000 | Loss: 0.00001751
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001745
Iteration 148/1000 | Loss: 0.00001745
Iteration 149/1000 | Loss: 0.00001745
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Iteration 159/1000 | Loss: 0.00001745
Iteration 160/1000 | Loss: 0.00001745
Iteration 161/1000 | Loss: 0.00001745
Iteration 162/1000 | Loss: 0.00001745
Iteration 163/1000 | Loss: 0.00001745
Iteration 164/1000 | Loss: 0.00001745
Iteration 165/1000 | Loss: 0.00001745
Iteration 166/1000 | Loss: 0.00001745
Iteration 167/1000 | Loss: 0.00001745
Iteration 168/1000 | Loss: 0.00001745
Iteration 169/1000 | Loss: 0.00001745
Iteration 170/1000 | Loss: 0.00001745
Iteration 171/1000 | Loss: 0.00001745
Iteration 172/1000 | Loss: 0.00001745
Iteration 173/1000 | Loss: 0.00001745
Iteration 174/1000 | Loss: 0.00001745
Iteration 175/1000 | Loss: 0.00001745
Iteration 176/1000 | Loss: 0.00001745
Iteration 177/1000 | Loss: 0.00001745
Iteration 178/1000 | Loss: 0.00001745
Iteration 179/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.744596374919638e-05, 1.744596374919638e-05, 1.744596374919638e-05, 1.744596374919638e-05, 1.744596374919638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.744596374919638e-05

Optimization complete. Final v2v error: 3.550264358520508 mm

Highest mean error: 3.7804455757141113 mm for frame 78

Lowest mean error: 3.243666410446167 mm for frame 133

Saving results

Total time: 44.685399770736694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00707256
Iteration 2/25 | Loss: 0.00178350
Iteration 3/25 | Loss: 0.00165239
Iteration 4/25 | Loss: 0.00162709
Iteration 5/25 | Loss: 0.00162046
Iteration 6/25 | Loss: 0.00161904
Iteration 7/25 | Loss: 0.00161904
Iteration 8/25 | Loss: 0.00161904
Iteration 9/25 | Loss: 0.00161904
Iteration 10/25 | Loss: 0.00161904
Iteration 11/25 | Loss: 0.00161904
Iteration 12/25 | Loss: 0.00161904
Iteration 13/25 | Loss: 0.00161904
Iteration 14/25 | Loss: 0.00161904
Iteration 15/25 | Loss: 0.00161904
Iteration 16/25 | Loss: 0.00161904
Iteration 17/25 | Loss: 0.00161904
Iteration 18/25 | Loss: 0.00161904
Iteration 19/25 | Loss: 0.00161904
Iteration 20/25 | Loss: 0.00161904
Iteration 21/25 | Loss: 0.00161904
Iteration 22/25 | Loss: 0.00161904
Iteration 23/25 | Loss: 0.00161904
Iteration 24/25 | Loss: 0.00161904
Iteration 25/25 | Loss: 0.00161904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14611328
Iteration 2/25 | Loss: 0.00205143
Iteration 3/25 | Loss: 0.00205135
Iteration 4/25 | Loss: 0.00205135
Iteration 5/25 | Loss: 0.00205135
Iteration 6/25 | Loss: 0.00205135
Iteration 7/25 | Loss: 0.00205135
Iteration 8/25 | Loss: 0.00205135
Iteration 9/25 | Loss: 0.00205135
Iteration 10/25 | Loss: 0.00205135
Iteration 11/25 | Loss: 0.00205135
Iteration 12/25 | Loss: 0.00205135
Iteration 13/25 | Loss: 0.00205135
Iteration 14/25 | Loss: 0.00205135
Iteration 15/25 | Loss: 0.00205135
Iteration 16/25 | Loss: 0.00205135
Iteration 17/25 | Loss: 0.00205135
Iteration 18/25 | Loss: 0.00205135
Iteration 19/25 | Loss: 0.00205135
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020513494964689016, 0.0020513494964689016, 0.0020513494964689016, 0.0020513494964689016, 0.0020513494964689016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020513494964689016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205135
Iteration 2/1000 | Loss: 0.00009465
Iteration 3/1000 | Loss: 0.00005736
Iteration 4/1000 | Loss: 0.00004639
Iteration 5/1000 | Loss: 0.00004303
Iteration 6/1000 | Loss: 0.00004144
Iteration 7/1000 | Loss: 0.00004009
Iteration 8/1000 | Loss: 0.00003929
Iteration 9/1000 | Loss: 0.00003856
Iteration 10/1000 | Loss: 0.00003810
Iteration 11/1000 | Loss: 0.00003770
Iteration 12/1000 | Loss: 0.00003741
Iteration 13/1000 | Loss: 0.00003713
Iteration 14/1000 | Loss: 0.00003684
Iteration 15/1000 | Loss: 0.00003661
Iteration 16/1000 | Loss: 0.00003651
Iteration 17/1000 | Loss: 0.00003633
Iteration 18/1000 | Loss: 0.00003631
Iteration 19/1000 | Loss: 0.00003625
Iteration 20/1000 | Loss: 0.00003620
Iteration 21/1000 | Loss: 0.00003620
Iteration 22/1000 | Loss: 0.00003619
Iteration 23/1000 | Loss: 0.00003619
Iteration 24/1000 | Loss: 0.00003617
Iteration 25/1000 | Loss: 0.00003617
Iteration 26/1000 | Loss: 0.00003611
Iteration 27/1000 | Loss: 0.00003608
Iteration 28/1000 | Loss: 0.00003608
Iteration 29/1000 | Loss: 0.00003607
Iteration 30/1000 | Loss: 0.00003606
Iteration 31/1000 | Loss: 0.00003605
Iteration 32/1000 | Loss: 0.00003605
Iteration 33/1000 | Loss: 0.00003604
Iteration 34/1000 | Loss: 0.00003604
Iteration 35/1000 | Loss: 0.00003604
Iteration 36/1000 | Loss: 0.00003603
Iteration 37/1000 | Loss: 0.00003603
Iteration 38/1000 | Loss: 0.00003602
Iteration 39/1000 | Loss: 0.00003602
Iteration 40/1000 | Loss: 0.00003602
Iteration 41/1000 | Loss: 0.00003601
Iteration 42/1000 | Loss: 0.00003601
Iteration 43/1000 | Loss: 0.00003601
Iteration 44/1000 | Loss: 0.00003601
Iteration 45/1000 | Loss: 0.00003601
Iteration 46/1000 | Loss: 0.00003601
Iteration 47/1000 | Loss: 0.00003601
Iteration 48/1000 | Loss: 0.00003600
Iteration 49/1000 | Loss: 0.00003600
Iteration 50/1000 | Loss: 0.00003600
Iteration 51/1000 | Loss: 0.00003599
Iteration 52/1000 | Loss: 0.00003598
Iteration 53/1000 | Loss: 0.00003598
Iteration 54/1000 | Loss: 0.00003598
Iteration 55/1000 | Loss: 0.00003597
Iteration 56/1000 | Loss: 0.00003597
Iteration 57/1000 | Loss: 0.00003596
Iteration 58/1000 | Loss: 0.00003596
Iteration 59/1000 | Loss: 0.00003595
Iteration 60/1000 | Loss: 0.00003595
Iteration 61/1000 | Loss: 0.00003595
Iteration 62/1000 | Loss: 0.00003594
Iteration 63/1000 | Loss: 0.00003594
Iteration 64/1000 | Loss: 0.00003594
Iteration 65/1000 | Loss: 0.00003593
Iteration 66/1000 | Loss: 0.00003593
Iteration 67/1000 | Loss: 0.00003593
Iteration 68/1000 | Loss: 0.00003592
Iteration 69/1000 | Loss: 0.00003592
Iteration 70/1000 | Loss: 0.00003592
Iteration 71/1000 | Loss: 0.00003592
Iteration 72/1000 | Loss: 0.00003591
Iteration 73/1000 | Loss: 0.00003591
Iteration 74/1000 | Loss: 0.00003591
Iteration 75/1000 | Loss: 0.00003590
Iteration 76/1000 | Loss: 0.00003590
Iteration 77/1000 | Loss: 0.00003590
Iteration 78/1000 | Loss: 0.00003589
Iteration 79/1000 | Loss: 0.00003589
Iteration 80/1000 | Loss: 0.00003589
Iteration 81/1000 | Loss: 0.00003589
Iteration 82/1000 | Loss: 0.00003588
Iteration 83/1000 | Loss: 0.00003588
Iteration 84/1000 | Loss: 0.00003587
Iteration 85/1000 | Loss: 0.00003587
Iteration 86/1000 | Loss: 0.00003587
Iteration 87/1000 | Loss: 0.00003587
Iteration 88/1000 | Loss: 0.00003587
Iteration 89/1000 | Loss: 0.00003587
Iteration 90/1000 | Loss: 0.00003586
Iteration 91/1000 | Loss: 0.00003586
Iteration 92/1000 | Loss: 0.00003586
Iteration 93/1000 | Loss: 0.00003586
Iteration 94/1000 | Loss: 0.00003585
Iteration 95/1000 | Loss: 0.00003585
Iteration 96/1000 | Loss: 0.00003585
Iteration 97/1000 | Loss: 0.00003584
Iteration 98/1000 | Loss: 0.00003584
Iteration 99/1000 | Loss: 0.00003584
Iteration 100/1000 | Loss: 0.00003583
Iteration 101/1000 | Loss: 0.00003583
Iteration 102/1000 | Loss: 0.00003582
Iteration 103/1000 | Loss: 0.00003582
Iteration 104/1000 | Loss: 0.00003582
Iteration 105/1000 | Loss: 0.00003581
Iteration 106/1000 | Loss: 0.00003581
Iteration 107/1000 | Loss: 0.00003581
Iteration 108/1000 | Loss: 0.00003581
Iteration 109/1000 | Loss: 0.00003580
Iteration 110/1000 | Loss: 0.00003580
Iteration 111/1000 | Loss: 0.00003580
Iteration 112/1000 | Loss: 0.00003579
Iteration 113/1000 | Loss: 0.00003579
Iteration 114/1000 | Loss: 0.00003579
Iteration 115/1000 | Loss: 0.00003579
Iteration 116/1000 | Loss: 0.00003578
Iteration 117/1000 | Loss: 0.00003578
Iteration 118/1000 | Loss: 0.00003578
Iteration 119/1000 | Loss: 0.00003578
Iteration 120/1000 | Loss: 0.00003578
Iteration 121/1000 | Loss: 0.00003577
Iteration 122/1000 | Loss: 0.00003577
Iteration 123/1000 | Loss: 0.00003577
Iteration 124/1000 | Loss: 0.00003577
Iteration 125/1000 | Loss: 0.00003577
Iteration 126/1000 | Loss: 0.00003576
Iteration 127/1000 | Loss: 0.00003576
Iteration 128/1000 | Loss: 0.00003576
Iteration 129/1000 | Loss: 0.00003576
Iteration 130/1000 | Loss: 0.00003575
Iteration 131/1000 | Loss: 0.00003575
Iteration 132/1000 | Loss: 0.00003575
Iteration 133/1000 | Loss: 0.00003575
Iteration 134/1000 | Loss: 0.00003574
Iteration 135/1000 | Loss: 0.00003574
Iteration 136/1000 | Loss: 0.00003574
Iteration 137/1000 | Loss: 0.00003574
Iteration 138/1000 | Loss: 0.00003574
Iteration 139/1000 | Loss: 0.00003574
Iteration 140/1000 | Loss: 0.00003574
Iteration 141/1000 | Loss: 0.00003574
Iteration 142/1000 | Loss: 0.00003573
Iteration 143/1000 | Loss: 0.00003573
Iteration 144/1000 | Loss: 0.00003573
Iteration 145/1000 | Loss: 0.00003573
Iteration 146/1000 | Loss: 0.00003573
Iteration 147/1000 | Loss: 0.00003573
Iteration 148/1000 | Loss: 0.00003573
Iteration 149/1000 | Loss: 0.00003572
Iteration 150/1000 | Loss: 0.00003572
Iteration 151/1000 | Loss: 0.00003572
Iteration 152/1000 | Loss: 0.00003572
Iteration 153/1000 | Loss: 0.00003572
Iteration 154/1000 | Loss: 0.00003572
Iteration 155/1000 | Loss: 0.00003572
Iteration 156/1000 | Loss: 0.00003572
Iteration 157/1000 | Loss: 0.00003572
Iteration 158/1000 | Loss: 0.00003571
Iteration 159/1000 | Loss: 0.00003571
Iteration 160/1000 | Loss: 0.00003571
Iteration 161/1000 | Loss: 0.00003571
Iteration 162/1000 | Loss: 0.00003571
Iteration 163/1000 | Loss: 0.00003571
Iteration 164/1000 | Loss: 0.00003571
Iteration 165/1000 | Loss: 0.00003571
Iteration 166/1000 | Loss: 0.00003571
Iteration 167/1000 | Loss: 0.00003571
Iteration 168/1000 | Loss: 0.00003571
Iteration 169/1000 | Loss: 0.00003570
Iteration 170/1000 | Loss: 0.00003570
Iteration 171/1000 | Loss: 0.00003570
Iteration 172/1000 | Loss: 0.00003570
Iteration 173/1000 | Loss: 0.00003570
Iteration 174/1000 | Loss: 0.00003570
Iteration 175/1000 | Loss: 0.00003570
Iteration 176/1000 | Loss: 0.00003569
Iteration 177/1000 | Loss: 0.00003569
Iteration 178/1000 | Loss: 0.00003569
Iteration 179/1000 | Loss: 0.00003569
Iteration 180/1000 | Loss: 0.00003569
Iteration 181/1000 | Loss: 0.00003569
Iteration 182/1000 | Loss: 0.00003569
Iteration 183/1000 | Loss: 0.00003569
Iteration 184/1000 | Loss: 0.00003569
Iteration 185/1000 | Loss: 0.00003569
Iteration 186/1000 | Loss: 0.00003569
Iteration 187/1000 | Loss: 0.00003569
Iteration 188/1000 | Loss: 0.00003569
Iteration 189/1000 | Loss: 0.00003569
Iteration 190/1000 | Loss: 0.00003569
Iteration 191/1000 | Loss: 0.00003569
Iteration 192/1000 | Loss: 0.00003569
Iteration 193/1000 | Loss: 0.00003569
Iteration 194/1000 | Loss: 0.00003569
Iteration 195/1000 | Loss: 0.00003569
Iteration 196/1000 | Loss: 0.00003568
Iteration 197/1000 | Loss: 0.00003568
Iteration 198/1000 | Loss: 0.00003568
Iteration 199/1000 | Loss: 0.00003568
Iteration 200/1000 | Loss: 0.00003568
Iteration 201/1000 | Loss: 0.00003568
Iteration 202/1000 | Loss: 0.00003568
Iteration 203/1000 | Loss: 0.00003568
Iteration 204/1000 | Loss: 0.00003568
Iteration 205/1000 | Loss: 0.00003568
Iteration 206/1000 | Loss: 0.00003568
Iteration 207/1000 | Loss: 0.00003568
Iteration 208/1000 | Loss: 0.00003568
Iteration 209/1000 | Loss: 0.00003568
Iteration 210/1000 | Loss: 0.00003568
Iteration 211/1000 | Loss: 0.00003568
Iteration 212/1000 | Loss: 0.00003568
Iteration 213/1000 | Loss: 0.00003568
Iteration 214/1000 | Loss: 0.00003568
Iteration 215/1000 | Loss: 0.00003568
Iteration 216/1000 | Loss: 0.00003568
Iteration 217/1000 | Loss: 0.00003568
Iteration 218/1000 | Loss: 0.00003568
Iteration 219/1000 | Loss: 0.00003568
Iteration 220/1000 | Loss: 0.00003568
Iteration 221/1000 | Loss: 0.00003568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [3.567670864867978e-05, 3.567670864867978e-05, 3.567670864867978e-05, 3.567670864867978e-05, 3.567670864867978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.567670864867978e-05

Optimization complete. Final v2v error: 4.906810283660889 mm

Highest mean error: 5.883584976196289 mm for frame 108

Lowest mean error: 3.6900041103363037 mm for frame 13

Saving results

Total time: 49.05911350250244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382501
Iteration 2/25 | Loss: 0.00150846
Iteration 3/25 | Loss: 0.00147136
Iteration 4/25 | Loss: 0.00146305
Iteration 5/25 | Loss: 0.00146037
Iteration 6/25 | Loss: 0.00146037
Iteration 7/25 | Loss: 0.00146037
Iteration 8/25 | Loss: 0.00146037
Iteration 9/25 | Loss: 0.00146037
Iteration 10/25 | Loss: 0.00146037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001460369792766869, 0.001460369792766869, 0.001460369792766869, 0.001460369792766869, 0.001460369792766869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001460369792766869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25758052
Iteration 2/25 | Loss: 0.00292483
Iteration 3/25 | Loss: 0.00292483
Iteration 4/25 | Loss: 0.00292483
Iteration 5/25 | Loss: 0.00292483
Iteration 6/25 | Loss: 0.00292483
Iteration 7/25 | Loss: 0.00292483
Iteration 8/25 | Loss: 0.00292483
Iteration 9/25 | Loss: 0.00292483
Iteration 10/25 | Loss: 0.00292483
Iteration 11/25 | Loss: 0.00292483
Iteration 12/25 | Loss: 0.00292483
Iteration 13/25 | Loss: 0.00292483
Iteration 14/25 | Loss: 0.00292483
Iteration 15/25 | Loss: 0.00292483
Iteration 16/25 | Loss: 0.00292483
Iteration 17/25 | Loss: 0.00292483
Iteration 18/25 | Loss: 0.00292483
Iteration 19/25 | Loss: 0.00292483
Iteration 20/25 | Loss: 0.00292483
Iteration 21/25 | Loss: 0.00292483
Iteration 22/25 | Loss: 0.00292483
Iteration 23/25 | Loss: 0.00292483
Iteration 24/25 | Loss: 0.00292483
Iteration 25/25 | Loss: 0.00292483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00292483
Iteration 2/1000 | Loss: 0.00002727
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001739
Iteration 8/1000 | Loss: 0.00001717
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001625
Iteration 12/1000 | Loss: 0.00001624
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001589
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001559
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001558
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001557
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001556
Iteration 33/1000 | Loss: 0.00001556
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001547
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001545
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001543
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001541
Iteration 72/1000 | Loss: 0.00001541
Iteration 73/1000 | Loss: 0.00001541
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001540
Iteration 76/1000 | Loss: 0.00001540
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001539
Iteration 79/1000 | Loss: 0.00001539
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001538
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001535
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001534
Iteration 100/1000 | Loss: 0.00001534
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001533
Iteration 106/1000 | Loss: 0.00001533
Iteration 107/1000 | Loss: 0.00001533
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001533
Iteration 111/1000 | Loss: 0.00001533
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001532
Iteration 116/1000 | Loss: 0.00001532
Iteration 117/1000 | Loss: 0.00001532
Iteration 118/1000 | Loss: 0.00001532
Iteration 119/1000 | Loss: 0.00001532
Iteration 120/1000 | Loss: 0.00001532
Iteration 121/1000 | Loss: 0.00001532
Iteration 122/1000 | Loss: 0.00001532
Iteration 123/1000 | Loss: 0.00001532
Iteration 124/1000 | Loss: 0.00001532
Iteration 125/1000 | Loss: 0.00001532
Iteration 126/1000 | Loss: 0.00001532
Iteration 127/1000 | Loss: 0.00001532
Iteration 128/1000 | Loss: 0.00001532
Iteration 129/1000 | Loss: 0.00001532
Iteration 130/1000 | Loss: 0.00001532
Iteration 131/1000 | Loss: 0.00001532
Iteration 132/1000 | Loss: 0.00001532
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001532
Iteration 135/1000 | Loss: 0.00001532
Iteration 136/1000 | Loss: 0.00001532
Iteration 137/1000 | Loss: 0.00001532
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001532
Iteration 140/1000 | Loss: 0.00001532
Iteration 141/1000 | Loss: 0.00001532
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001532
Iteration 144/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5318626537919044e-05, 1.5318626537919044e-05, 1.5318626537919044e-05, 1.5318626537919044e-05, 1.5318626537919044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5318626537919044e-05

Optimization complete. Final v2v error: 3.300163984298706 mm

Highest mean error: 3.4832961559295654 mm for frame 87

Lowest mean error: 3.1165249347686768 mm for frame 25

Saving results

Total time: 38.60804200172424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944407
Iteration 2/25 | Loss: 0.00944407
Iteration 3/25 | Loss: 0.00944407
Iteration 4/25 | Loss: 0.00944406
Iteration 5/25 | Loss: 0.00944406
Iteration 6/25 | Loss: 0.00944406
Iteration 7/25 | Loss: 0.00944405
Iteration 8/25 | Loss: 0.00944405
Iteration 9/25 | Loss: 0.00944405
Iteration 10/25 | Loss: 0.00944404
Iteration 11/25 | Loss: 0.00944404
Iteration 12/25 | Loss: 0.00944403
Iteration 13/25 | Loss: 0.00944403
Iteration 14/25 | Loss: 0.00944403
Iteration 15/25 | Loss: 0.00944403
Iteration 16/25 | Loss: 0.00944402
Iteration 17/25 | Loss: 0.00944402
Iteration 18/25 | Loss: 0.00944402
Iteration 19/25 | Loss: 0.00944402
Iteration 20/25 | Loss: 0.00944401
Iteration 21/25 | Loss: 0.00944401
Iteration 22/25 | Loss: 0.00944401
Iteration 23/25 | Loss: 0.00944401
Iteration 24/25 | Loss: 0.00944400
Iteration 25/25 | Loss: 0.00944400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43659222
Iteration 2/25 | Loss: 0.16538042
Iteration 3/25 | Loss: 0.16434067
Iteration 4/25 | Loss: 0.16310234
Iteration 5/25 | Loss: 0.16301657
Iteration 6/25 | Loss: 0.16301657
Iteration 7/25 | Loss: 0.16301656
Iteration 8/25 | Loss: 0.16301656
Iteration 9/25 | Loss: 0.16301656
Iteration 10/25 | Loss: 0.16301656
Iteration 11/25 | Loss: 0.16301656
Iteration 12/25 | Loss: 0.16301656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.16301655769348145, 0.16301655769348145, 0.16301655769348145, 0.16301655769348145, 0.16301655769348145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16301655769348145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16301656
Iteration 2/1000 | Loss: 0.00591733
Iteration 3/1000 | Loss: 0.00686078
Iteration 4/1000 | Loss: 0.00201830
Iteration 5/1000 | Loss: 0.00196798
Iteration 6/1000 | Loss: 0.00056056
Iteration 7/1000 | Loss: 0.00045980
Iteration 8/1000 | Loss: 0.00192727
Iteration 9/1000 | Loss: 0.00018773
Iteration 10/1000 | Loss: 0.00042108
Iteration 11/1000 | Loss: 0.00015310
Iteration 12/1000 | Loss: 0.00036130
Iteration 13/1000 | Loss: 0.00013018
Iteration 14/1000 | Loss: 0.00014422
Iteration 15/1000 | Loss: 0.00028195
Iteration 16/1000 | Loss: 0.00016081
Iteration 17/1000 | Loss: 0.00034206
Iteration 18/1000 | Loss: 0.00005100
Iteration 19/1000 | Loss: 0.00011033
Iteration 20/1000 | Loss: 0.00011054
Iteration 21/1000 | Loss: 0.00026062
Iteration 22/1000 | Loss: 0.00006313
Iteration 23/1000 | Loss: 0.00007459
Iteration 24/1000 | Loss: 0.00062415
Iteration 25/1000 | Loss: 0.00012072
Iteration 26/1000 | Loss: 0.00003706
Iteration 27/1000 | Loss: 0.00004736
Iteration 28/1000 | Loss: 0.00010904
Iteration 29/1000 | Loss: 0.00003533
Iteration 30/1000 | Loss: 0.00003629
Iteration 31/1000 | Loss: 0.00004297
Iteration 32/1000 | Loss: 0.00003292
Iteration 33/1000 | Loss: 0.00009209
Iteration 34/1000 | Loss: 0.00002729
Iteration 35/1000 | Loss: 0.00007850
Iteration 36/1000 | Loss: 0.00003749
Iteration 37/1000 | Loss: 0.00003025
Iteration 38/1000 | Loss: 0.00002505
Iteration 39/1000 | Loss: 0.00003123
Iteration 40/1000 | Loss: 0.00003326
Iteration 41/1000 | Loss: 0.00002447
Iteration 42/1000 | Loss: 0.00002450
Iteration 43/1000 | Loss: 0.00002409
Iteration 44/1000 | Loss: 0.00002407
Iteration 45/1000 | Loss: 0.00002406
Iteration 46/1000 | Loss: 0.00002406
Iteration 47/1000 | Loss: 0.00002405
Iteration 48/1000 | Loss: 0.00002415
Iteration 49/1000 | Loss: 0.00002415
Iteration 50/1000 | Loss: 0.00002393
Iteration 51/1000 | Loss: 0.00002392
Iteration 52/1000 | Loss: 0.00002392
Iteration 53/1000 | Loss: 0.00002392
Iteration 54/1000 | Loss: 0.00002392
Iteration 55/1000 | Loss: 0.00002391
Iteration 56/1000 | Loss: 0.00002391
Iteration 57/1000 | Loss: 0.00002390
Iteration 58/1000 | Loss: 0.00002421
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002420
Iteration 61/1000 | Loss: 0.00002425
Iteration 62/1000 | Loss: 0.00002564
Iteration 63/1000 | Loss: 0.00004771
Iteration 64/1000 | Loss: 0.00004109
Iteration 65/1000 | Loss: 0.00002429
Iteration 66/1000 | Loss: 0.00002362
Iteration 67/1000 | Loss: 0.00002361
Iteration 68/1000 | Loss: 0.00002361
Iteration 69/1000 | Loss: 0.00002361
Iteration 70/1000 | Loss: 0.00002361
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002361
Iteration 73/1000 | Loss: 0.00002361
Iteration 74/1000 | Loss: 0.00002361
Iteration 75/1000 | Loss: 0.00002361
Iteration 76/1000 | Loss: 0.00002361
Iteration 77/1000 | Loss: 0.00002361
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002360
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002359
Iteration 89/1000 | Loss: 0.00002382
Iteration 90/1000 | Loss: 0.00002993
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00003364
Iteration 93/1000 | Loss: 0.00003225
Iteration 94/1000 | Loss: 0.00002385
Iteration 95/1000 | Loss: 0.00002844
Iteration 96/1000 | Loss: 0.00002347
Iteration 97/1000 | Loss: 0.00002375
Iteration 98/1000 | Loss: 0.00002798
Iteration 99/1000 | Loss: 0.00002836
Iteration 100/1000 | Loss: 0.00002760
Iteration 101/1000 | Loss: 0.00002335
Iteration 102/1000 | Loss: 0.00002335
Iteration 103/1000 | Loss: 0.00002334
Iteration 104/1000 | Loss: 0.00002334
Iteration 105/1000 | Loss: 0.00002334
Iteration 106/1000 | Loss: 0.00002334
Iteration 107/1000 | Loss: 0.00002334
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002333
Iteration 111/1000 | Loss: 0.00002333
Iteration 112/1000 | Loss: 0.00002333
Iteration 113/1000 | Loss: 0.00002332
Iteration 114/1000 | Loss: 0.00002332
Iteration 115/1000 | Loss: 0.00002332
Iteration 116/1000 | Loss: 0.00002332
Iteration 117/1000 | Loss: 0.00002332
Iteration 118/1000 | Loss: 0.00002332
Iteration 119/1000 | Loss: 0.00002332
Iteration 120/1000 | Loss: 0.00002335
Iteration 121/1000 | Loss: 0.00002332
Iteration 122/1000 | Loss: 0.00002332
Iteration 123/1000 | Loss: 0.00002332
Iteration 124/1000 | Loss: 0.00002332
Iteration 125/1000 | Loss: 0.00002332
Iteration 126/1000 | Loss: 0.00002332
Iteration 127/1000 | Loss: 0.00002332
Iteration 128/1000 | Loss: 0.00002332
Iteration 129/1000 | Loss: 0.00002332
Iteration 130/1000 | Loss: 0.00002332
Iteration 131/1000 | Loss: 0.00002332
Iteration 132/1000 | Loss: 0.00002332
Iteration 133/1000 | Loss: 0.00002332
Iteration 134/1000 | Loss: 0.00002332
Iteration 135/1000 | Loss: 0.00002332
Iteration 136/1000 | Loss: 0.00002332
Iteration 137/1000 | Loss: 0.00002332
Iteration 138/1000 | Loss: 0.00002332
Iteration 139/1000 | Loss: 0.00002332
Iteration 140/1000 | Loss: 0.00002332
Iteration 141/1000 | Loss: 0.00002332
Iteration 142/1000 | Loss: 0.00002332
Iteration 143/1000 | Loss: 0.00002332
Iteration 144/1000 | Loss: 0.00002332
Iteration 145/1000 | Loss: 0.00002332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.331943323952146e-05, 2.331943323952146e-05, 2.331943323952146e-05, 2.331943323952146e-05, 2.331943323952146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.331943323952146e-05

Optimization complete. Final v2v error: 4.128443241119385 mm

Highest mean error: 4.568614959716797 mm for frame 15

Lowest mean error: 3.817948341369629 mm for frame 53

Saving results

Total time: 107.02046418190002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802588
Iteration 2/25 | Loss: 0.00169088
Iteration 3/25 | Loss: 0.00151478
Iteration 4/25 | Loss: 0.00150120
Iteration 5/25 | Loss: 0.00149963
Iteration 6/25 | Loss: 0.00149963
Iteration 7/25 | Loss: 0.00149963
Iteration 8/25 | Loss: 0.00149963
Iteration 9/25 | Loss: 0.00149963
Iteration 10/25 | Loss: 0.00149963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014996308600530028, 0.0014996308600530028, 0.0014996308600530028, 0.0014996308600530028, 0.0014996308600530028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014996308600530028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88683105
Iteration 2/25 | Loss: 0.00149347
Iteration 3/25 | Loss: 0.00149347
Iteration 4/25 | Loss: 0.00149347
Iteration 5/25 | Loss: 0.00149347
Iteration 6/25 | Loss: 0.00149347
Iteration 7/25 | Loss: 0.00149346
Iteration 8/25 | Loss: 0.00149346
Iteration 9/25 | Loss: 0.00149346
Iteration 10/25 | Loss: 0.00149346
Iteration 11/25 | Loss: 0.00149346
Iteration 12/25 | Loss: 0.00149346
Iteration 13/25 | Loss: 0.00149346
Iteration 14/25 | Loss: 0.00149346
Iteration 15/25 | Loss: 0.00149346
Iteration 16/25 | Loss: 0.00149346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014934636419638991, 0.0014934636419638991, 0.0014934636419638991, 0.0014934636419638991, 0.0014934636419638991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014934636419638991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149346
Iteration 2/1000 | Loss: 0.00003722
Iteration 3/1000 | Loss: 0.00002958
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002380
Iteration 8/1000 | Loss: 0.00002324
Iteration 9/1000 | Loss: 0.00002286
Iteration 10/1000 | Loss: 0.00002253
Iteration 11/1000 | Loss: 0.00002237
Iteration 12/1000 | Loss: 0.00002230
Iteration 13/1000 | Loss: 0.00002218
Iteration 14/1000 | Loss: 0.00002203
Iteration 15/1000 | Loss: 0.00002202
Iteration 16/1000 | Loss: 0.00002202
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002179
Iteration 19/1000 | Loss: 0.00002167
Iteration 20/1000 | Loss: 0.00002165
Iteration 21/1000 | Loss: 0.00002161
Iteration 22/1000 | Loss: 0.00002161
Iteration 23/1000 | Loss: 0.00002161
Iteration 24/1000 | Loss: 0.00002154
Iteration 25/1000 | Loss: 0.00002153
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00002144
Iteration 29/1000 | Loss: 0.00002144
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002144
Iteration 32/1000 | Loss: 0.00002144
Iteration 33/1000 | Loss: 0.00002144
Iteration 34/1000 | Loss: 0.00002143
Iteration 35/1000 | Loss: 0.00002143
Iteration 36/1000 | Loss: 0.00002143
Iteration 37/1000 | Loss: 0.00002143
Iteration 38/1000 | Loss: 0.00002133
Iteration 39/1000 | Loss: 0.00002133
Iteration 40/1000 | Loss: 0.00002133
Iteration 41/1000 | Loss: 0.00002133
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002133
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002132
Iteration 48/1000 | Loss: 0.00002132
Iteration 49/1000 | Loss: 0.00002132
Iteration 50/1000 | Loss: 0.00002132
Iteration 51/1000 | Loss: 0.00002130
Iteration 52/1000 | Loss: 0.00002130
Iteration 53/1000 | Loss: 0.00002130
Iteration 54/1000 | Loss: 0.00002130
Iteration 55/1000 | Loss: 0.00002130
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00002129
Iteration 58/1000 | Loss: 0.00002129
Iteration 59/1000 | Loss: 0.00002129
Iteration 60/1000 | Loss: 0.00002129
Iteration 61/1000 | Loss: 0.00002128
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002128
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002128
Iteration 67/1000 | Loss: 0.00002128
Iteration 68/1000 | Loss: 0.00002127
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002127
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002126
Iteration 74/1000 | Loss: 0.00002126
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002125
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002125
Iteration 91/1000 | Loss: 0.00002125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.1252884835121222e-05, 2.1252884835121222e-05, 2.1252884835121222e-05, 2.1252884835121222e-05, 2.1252884835121222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1252884835121222e-05

Optimization complete. Final v2v error: 3.886805295944214 mm

Highest mean error: 3.9550554752349854 mm for frame 106

Lowest mean error: 3.8095743656158447 mm for frame 36

Saving results

Total time: 36.1787805557251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480307
Iteration 2/25 | Loss: 0.00155194
Iteration 3/25 | Loss: 0.00149752
Iteration 4/25 | Loss: 0.00148827
Iteration 5/25 | Loss: 0.00148621
Iteration 6/25 | Loss: 0.00148621
Iteration 7/25 | Loss: 0.00148621
Iteration 8/25 | Loss: 0.00148621
Iteration 9/25 | Loss: 0.00148621
Iteration 10/25 | Loss: 0.00148621
Iteration 11/25 | Loss: 0.00148621
Iteration 12/25 | Loss: 0.00148621
Iteration 13/25 | Loss: 0.00148621
Iteration 14/25 | Loss: 0.00148621
Iteration 15/25 | Loss: 0.00148621
Iteration 16/25 | Loss: 0.00148621
Iteration 17/25 | Loss: 0.00148621
Iteration 18/25 | Loss: 0.00148621
Iteration 19/25 | Loss: 0.00148621
Iteration 20/25 | Loss: 0.00148621
Iteration 21/25 | Loss: 0.00148621
Iteration 22/25 | Loss: 0.00148621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001486208406277001, 0.001486208406277001, 0.001486208406277001, 0.001486208406277001, 0.001486208406277001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001486208406277001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26664817
Iteration 2/25 | Loss: 0.00243481
Iteration 3/25 | Loss: 0.00243481
Iteration 4/25 | Loss: 0.00243481
Iteration 5/25 | Loss: 0.00243481
Iteration 6/25 | Loss: 0.00243481
Iteration 7/25 | Loss: 0.00243481
Iteration 8/25 | Loss: 0.00243481
Iteration 9/25 | Loss: 0.00243481
Iteration 10/25 | Loss: 0.00243481
Iteration 11/25 | Loss: 0.00243481
Iteration 12/25 | Loss: 0.00243481
Iteration 13/25 | Loss: 0.00243481
Iteration 14/25 | Loss: 0.00243481
Iteration 15/25 | Loss: 0.00243481
Iteration 16/25 | Loss: 0.00243481
Iteration 17/25 | Loss: 0.00243481
Iteration 18/25 | Loss: 0.00243481
Iteration 19/25 | Loss: 0.00243481
Iteration 20/25 | Loss: 0.00243481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0024348071310669184, 0.0024348071310669184, 0.0024348071310669184, 0.0024348071310669184, 0.0024348071310669184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024348071310669184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243481
Iteration 2/1000 | Loss: 0.00002809
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001971
Iteration 6/1000 | Loss: 0.00001919
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001843
Iteration 9/1000 | Loss: 0.00001800
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001745
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001705
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001680
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001674
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001666
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001649
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001639
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001631
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001628
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001619
Iteration 90/1000 | Loss: 0.00001619
Iteration 91/1000 | Loss: 0.00001619
Iteration 92/1000 | Loss: 0.00001618
Iteration 93/1000 | Loss: 0.00001618
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001614
Iteration 117/1000 | Loss: 0.00001614
Iteration 118/1000 | Loss: 0.00001614
Iteration 119/1000 | Loss: 0.00001614
Iteration 120/1000 | Loss: 0.00001614
Iteration 121/1000 | Loss: 0.00001614
Iteration 122/1000 | Loss: 0.00001613
Iteration 123/1000 | Loss: 0.00001613
Iteration 124/1000 | Loss: 0.00001613
Iteration 125/1000 | Loss: 0.00001613
Iteration 126/1000 | Loss: 0.00001613
Iteration 127/1000 | Loss: 0.00001613
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Iteration 131/1000 | Loss: 0.00001613
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001612
Iteration 134/1000 | Loss: 0.00001612
Iteration 135/1000 | Loss: 0.00001612
Iteration 136/1000 | Loss: 0.00001612
Iteration 137/1000 | Loss: 0.00001612
Iteration 138/1000 | Loss: 0.00001612
Iteration 139/1000 | Loss: 0.00001612
Iteration 140/1000 | Loss: 0.00001612
Iteration 141/1000 | Loss: 0.00001612
Iteration 142/1000 | Loss: 0.00001612
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001611
Iteration 146/1000 | Loss: 0.00001611
Iteration 147/1000 | Loss: 0.00001611
Iteration 148/1000 | Loss: 0.00001611
Iteration 149/1000 | Loss: 0.00001611
Iteration 150/1000 | Loss: 0.00001611
Iteration 151/1000 | Loss: 0.00001611
Iteration 152/1000 | Loss: 0.00001611
Iteration 153/1000 | Loss: 0.00001611
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001610
Iteration 157/1000 | Loss: 0.00001610
Iteration 158/1000 | Loss: 0.00001610
Iteration 159/1000 | Loss: 0.00001610
Iteration 160/1000 | Loss: 0.00001610
Iteration 161/1000 | Loss: 0.00001610
Iteration 162/1000 | Loss: 0.00001610
Iteration 163/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.6104857422760688e-05, 1.6104857422760688e-05, 1.6104857422760688e-05, 1.6104857422760688e-05, 1.6104857422760688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6104857422760688e-05

Optimization complete. Final v2v error: 3.381131172180176 mm

Highest mean error: 3.5829837322235107 mm for frame 211

Lowest mean error: 3.187551736831665 mm for frame 17

Saving results

Total time: 49.555978298187256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984513
Iteration 2/25 | Loss: 0.00291531
Iteration 3/25 | Loss: 0.00231699
Iteration 4/25 | Loss: 0.00210299
Iteration 5/25 | Loss: 0.00209713
Iteration 6/25 | Loss: 0.00188941
Iteration 7/25 | Loss: 0.00184863
Iteration 8/25 | Loss: 0.00176153
Iteration 9/25 | Loss: 0.00173153
Iteration 10/25 | Loss: 0.00170176
Iteration 11/25 | Loss: 0.00170336
Iteration 12/25 | Loss: 0.00169589
Iteration 13/25 | Loss: 0.00169422
Iteration 14/25 | Loss: 0.00168702
Iteration 15/25 | Loss: 0.00168395
Iteration 16/25 | Loss: 0.00167903
Iteration 17/25 | Loss: 0.00167874
Iteration 18/25 | Loss: 0.00167626
Iteration 19/25 | Loss: 0.00167916
Iteration 20/25 | Loss: 0.00167517
Iteration 21/25 | Loss: 0.00167692
Iteration 22/25 | Loss: 0.00167290
Iteration 23/25 | Loss: 0.00166894
Iteration 24/25 | Loss: 0.00166852
Iteration 25/25 | Loss: 0.00166846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33976567
Iteration 2/25 | Loss: 0.00597296
Iteration 3/25 | Loss: 0.00506987
Iteration 4/25 | Loss: 0.00506987
Iteration 5/25 | Loss: 0.00506986
Iteration 6/25 | Loss: 0.00506986
Iteration 7/25 | Loss: 0.00506986
Iteration 8/25 | Loss: 0.00506986
Iteration 9/25 | Loss: 0.00506986
Iteration 10/25 | Loss: 0.00506986
Iteration 11/25 | Loss: 0.00506986
Iteration 12/25 | Loss: 0.00506986
Iteration 13/25 | Loss: 0.00506986
Iteration 14/25 | Loss: 0.00506986
Iteration 15/25 | Loss: 0.00506986
Iteration 16/25 | Loss: 0.00506986
Iteration 17/25 | Loss: 0.00506986
Iteration 18/25 | Loss: 0.00506986
Iteration 19/25 | Loss: 0.00506986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005069860722869635, 0.005069860722869635, 0.005069860722869635, 0.005069860722869635, 0.005069860722869635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005069860722869635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00506986
Iteration 2/1000 | Loss: 0.00064762
Iteration 3/1000 | Loss: 0.00183812
Iteration 4/1000 | Loss: 0.00090987
Iteration 5/1000 | Loss: 0.00100314
Iteration 6/1000 | Loss: 0.00018265
Iteration 7/1000 | Loss: 0.00032641
Iteration 8/1000 | Loss: 0.00017106
Iteration 9/1000 | Loss: 0.00067805
Iteration 10/1000 | Loss: 0.00099064
Iteration 11/1000 | Loss: 0.00012538
Iteration 12/1000 | Loss: 0.00018485
Iteration 13/1000 | Loss: 0.00053561
Iteration 14/1000 | Loss: 0.00015522
Iteration 15/1000 | Loss: 0.00011491
Iteration 16/1000 | Loss: 0.00059631
Iteration 17/1000 | Loss: 0.00129331
Iteration 18/1000 | Loss: 0.00016660
Iteration 19/1000 | Loss: 0.00034547
Iteration 20/1000 | Loss: 0.00013093
Iteration 21/1000 | Loss: 0.00011078
Iteration 22/1000 | Loss: 0.00011357
Iteration 23/1000 | Loss: 0.00010797
Iteration 24/1000 | Loss: 0.00049039
Iteration 25/1000 | Loss: 0.00011235
Iteration 26/1000 | Loss: 0.00010574
Iteration 27/1000 | Loss: 0.00010480
Iteration 28/1000 | Loss: 0.00010365
Iteration 29/1000 | Loss: 0.00018833
Iteration 30/1000 | Loss: 0.00010247
Iteration 31/1000 | Loss: 0.00010181
Iteration 32/1000 | Loss: 0.00048603
Iteration 33/1000 | Loss: 0.00109774
Iteration 34/1000 | Loss: 0.00056526
Iteration 35/1000 | Loss: 0.00051624
Iteration 36/1000 | Loss: 0.00010366
Iteration 37/1000 | Loss: 0.00011529
Iteration 38/1000 | Loss: 0.00010673
Iteration 39/1000 | Loss: 0.00024042
Iteration 40/1000 | Loss: 0.00011234
Iteration 41/1000 | Loss: 0.00049737
Iteration 42/1000 | Loss: 0.00009857
Iteration 43/1000 | Loss: 0.00027925
Iteration 44/1000 | Loss: 0.00010373
Iteration 45/1000 | Loss: 0.00011113
Iteration 46/1000 | Loss: 0.00009294
Iteration 47/1000 | Loss: 0.00009175
Iteration 48/1000 | Loss: 0.00010298
Iteration 49/1000 | Loss: 0.00013450
Iteration 50/1000 | Loss: 0.00010857
Iteration 51/1000 | Loss: 0.00010669
Iteration 52/1000 | Loss: 0.00008992
Iteration 53/1000 | Loss: 0.00008963
Iteration 54/1000 | Loss: 0.00011701
Iteration 55/1000 | Loss: 0.00008914
Iteration 56/1000 | Loss: 0.00008882
Iteration 57/1000 | Loss: 0.00019663
Iteration 58/1000 | Loss: 0.00009035
Iteration 59/1000 | Loss: 0.00008858
Iteration 60/1000 | Loss: 0.00008833
Iteration 61/1000 | Loss: 0.00008827
Iteration 62/1000 | Loss: 0.00008827
Iteration 63/1000 | Loss: 0.00008825
Iteration 64/1000 | Loss: 0.00008818
Iteration 65/1000 | Loss: 0.00013312
Iteration 66/1000 | Loss: 0.00008839
Iteration 67/1000 | Loss: 0.00008804
Iteration 68/1000 | Loss: 0.00008801
Iteration 69/1000 | Loss: 0.00008801
Iteration 70/1000 | Loss: 0.00008801
Iteration 71/1000 | Loss: 0.00008801
Iteration 72/1000 | Loss: 0.00008801
Iteration 73/1000 | Loss: 0.00008801
Iteration 74/1000 | Loss: 0.00008800
Iteration 75/1000 | Loss: 0.00008800
Iteration 76/1000 | Loss: 0.00008800
Iteration 77/1000 | Loss: 0.00008799
Iteration 78/1000 | Loss: 0.00008794
Iteration 79/1000 | Loss: 0.00008793
Iteration 80/1000 | Loss: 0.00015974
Iteration 81/1000 | Loss: 0.00008950
Iteration 82/1000 | Loss: 0.00011866
Iteration 83/1000 | Loss: 0.00008791
Iteration 84/1000 | Loss: 0.00008777
Iteration 85/1000 | Loss: 0.00008776
Iteration 86/1000 | Loss: 0.00008776
Iteration 87/1000 | Loss: 0.00008775
Iteration 88/1000 | Loss: 0.00008775
Iteration 89/1000 | Loss: 0.00008775
Iteration 90/1000 | Loss: 0.00008775
Iteration 91/1000 | Loss: 0.00008775
Iteration 92/1000 | Loss: 0.00008775
Iteration 93/1000 | Loss: 0.00008775
Iteration 94/1000 | Loss: 0.00008775
Iteration 95/1000 | Loss: 0.00008775
Iteration 96/1000 | Loss: 0.00008775
Iteration 97/1000 | Loss: 0.00008775
Iteration 98/1000 | Loss: 0.00008775
Iteration 99/1000 | Loss: 0.00008775
Iteration 100/1000 | Loss: 0.00008774
Iteration 101/1000 | Loss: 0.00008774
Iteration 102/1000 | Loss: 0.00008774
Iteration 103/1000 | Loss: 0.00008774
Iteration 104/1000 | Loss: 0.00008774
Iteration 105/1000 | Loss: 0.00008774
Iteration 106/1000 | Loss: 0.00008774
Iteration 107/1000 | Loss: 0.00008774
Iteration 108/1000 | Loss: 0.00008774
Iteration 109/1000 | Loss: 0.00008774
Iteration 110/1000 | Loss: 0.00012810
Iteration 111/1000 | Loss: 0.00008780
Iteration 112/1000 | Loss: 0.00008777
Iteration 113/1000 | Loss: 0.00008777
Iteration 114/1000 | Loss: 0.00008777
Iteration 115/1000 | Loss: 0.00008777
Iteration 116/1000 | Loss: 0.00008777
Iteration 117/1000 | Loss: 0.00008777
Iteration 118/1000 | Loss: 0.00008776
Iteration 119/1000 | Loss: 0.00008776
Iteration 120/1000 | Loss: 0.00008776
Iteration 121/1000 | Loss: 0.00008776
Iteration 122/1000 | Loss: 0.00008776
Iteration 123/1000 | Loss: 0.00008776
Iteration 124/1000 | Loss: 0.00008776
Iteration 125/1000 | Loss: 0.00008776
Iteration 126/1000 | Loss: 0.00008776
Iteration 127/1000 | Loss: 0.00008776
Iteration 128/1000 | Loss: 0.00008775
Iteration 129/1000 | Loss: 0.00008775
Iteration 130/1000 | Loss: 0.00008775
Iteration 131/1000 | Loss: 0.00008774
Iteration 132/1000 | Loss: 0.00008774
Iteration 133/1000 | Loss: 0.00008774
Iteration 134/1000 | Loss: 0.00008773
Iteration 135/1000 | Loss: 0.00008773
Iteration 136/1000 | Loss: 0.00008772
Iteration 137/1000 | Loss: 0.00008772
Iteration 138/1000 | Loss: 0.00008771
Iteration 139/1000 | Loss: 0.00008771
Iteration 140/1000 | Loss: 0.00008771
Iteration 141/1000 | Loss: 0.00008771
Iteration 142/1000 | Loss: 0.00008771
Iteration 143/1000 | Loss: 0.00008771
Iteration 144/1000 | Loss: 0.00008770
Iteration 145/1000 | Loss: 0.00008770
Iteration 146/1000 | Loss: 0.00008770
Iteration 147/1000 | Loss: 0.00008770
Iteration 148/1000 | Loss: 0.00008770
Iteration 149/1000 | Loss: 0.00008770
Iteration 150/1000 | Loss: 0.00008770
Iteration 151/1000 | Loss: 0.00008770
Iteration 152/1000 | Loss: 0.00008770
Iteration 153/1000 | Loss: 0.00008770
Iteration 154/1000 | Loss: 0.00008770
Iteration 155/1000 | Loss: 0.00008769
Iteration 156/1000 | Loss: 0.00008769
Iteration 157/1000 | Loss: 0.00008769
Iteration 158/1000 | Loss: 0.00008769
Iteration 159/1000 | Loss: 0.00008769
Iteration 160/1000 | Loss: 0.00008768
Iteration 161/1000 | Loss: 0.00008768
Iteration 162/1000 | Loss: 0.00008768
Iteration 163/1000 | Loss: 0.00008768
Iteration 164/1000 | Loss: 0.00008768
Iteration 165/1000 | Loss: 0.00008768
Iteration 166/1000 | Loss: 0.00008768
Iteration 167/1000 | Loss: 0.00008768
Iteration 168/1000 | Loss: 0.00008767
Iteration 169/1000 | Loss: 0.00008767
Iteration 170/1000 | Loss: 0.00008767
Iteration 171/1000 | Loss: 0.00008767
Iteration 172/1000 | Loss: 0.00008767
Iteration 173/1000 | Loss: 0.00008766
Iteration 174/1000 | Loss: 0.00008766
Iteration 175/1000 | Loss: 0.00008766
Iteration 176/1000 | Loss: 0.00008766
Iteration 177/1000 | Loss: 0.00008766
Iteration 178/1000 | Loss: 0.00008766
Iteration 179/1000 | Loss: 0.00008766
Iteration 180/1000 | Loss: 0.00008766
Iteration 181/1000 | Loss: 0.00008766
Iteration 182/1000 | Loss: 0.00008766
Iteration 183/1000 | Loss: 0.00008766
Iteration 184/1000 | Loss: 0.00008766
Iteration 185/1000 | Loss: 0.00008766
Iteration 186/1000 | Loss: 0.00008766
Iteration 187/1000 | Loss: 0.00008765
Iteration 188/1000 | Loss: 0.00008765
Iteration 189/1000 | Loss: 0.00008765
Iteration 190/1000 | Loss: 0.00008765
Iteration 191/1000 | Loss: 0.00008765
Iteration 192/1000 | Loss: 0.00008765
Iteration 193/1000 | Loss: 0.00008765
Iteration 194/1000 | Loss: 0.00008765
Iteration 195/1000 | Loss: 0.00008765
Iteration 196/1000 | Loss: 0.00008765
Iteration 197/1000 | Loss: 0.00008764
Iteration 198/1000 | Loss: 0.00008764
Iteration 199/1000 | Loss: 0.00008764
Iteration 200/1000 | Loss: 0.00008764
Iteration 201/1000 | Loss: 0.00008764
Iteration 202/1000 | Loss: 0.00008764
Iteration 203/1000 | Loss: 0.00008764
Iteration 204/1000 | Loss: 0.00008763
Iteration 205/1000 | Loss: 0.00008763
Iteration 206/1000 | Loss: 0.00008763
Iteration 207/1000 | Loss: 0.00008763
Iteration 208/1000 | Loss: 0.00008763
Iteration 209/1000 | Loss: 0.00008763
Iteration 210/1000 | Loss: 0.00008763
Iteration 211/1000 | Loss: 0.00008763
Iteration 212/1000 | Loss: 0.00008763
Iteration 213/1000 | Loss: 0.00008763
Iteration 214/1000 | Loss: 0.00008763
Iteration 215/1000 | Loss: 0.00008762
Iteration 216/1000 | Loss: 0.00008762
Iteration 217/1000 | Loss: 0.00008762
Iteration 218/1000 | Loss: 0.00008762
Iteration 219/1000 | Loss: 0.00008762
Iteration 220/1000 | Loss: 0.00008762
Iteration 221/1000 | Loss: 0.00008761
Iteration 222/1000 | Loss: 0.00008761
Iteration 223/1000 | Loss: 0.00008761
Iteration 224/1000 | Loss: 0.00008761
Iteration 225/1000 | Loss: 0.00008761
Iteration 226/1000 | Loss: 0.00008760
Iteration 227/1000 | Loss: 0.00008760
Iteration 228/1000 | Loss: 0.00008760
Iteration 229/1000 | Loss: 0.00008760
Iteration 230/1000 | Loss: 0.00008760
Iteration 231/1000 | Loss: 0.00008760
Iteration 232/1000 | Loss: 0.00008760
Iteration 233/1000 | Loss: 0.00008760
Iteration 234/1000 | Loss: 0.00008760
Iteration 235/1000 | Loss: 0.00008760
Iteration 236/1000 | Loss: 0.00008760
Iteration 237/1000 | Loss: 0.00008760
Iteration 238/1000 | Loss: 0.00008760
Iteration 239/1000 | Loss: 0.00008760
Iteration 240/1000 | Loss: 0.00008760
Iteration 241/1000 | Loss: 0.00008760
Iteration 242/1000 | Loss: 0.00008760
Iteration 243/1000 | Loss: 0.00008759
Iteration 244/1000 | Loss: 0.00008759
Iteration 245/1000 | Loss: 0.00008759
Iteration 246/1000 | Loss: 0.00008759
Iteration 247/1000 | Loss: 0.00008759
Iteration 248/1000 | Loss: 0.00008759
Iteration 249/1000 | Loss: 0.00008759
Iteration 250/1000 | Loss: 0.00008759
Iteration 251/1000 | Loss: 0.00008758
Iteration 252/1000 | Loss: 0.00008758
Iteration 253/1000 | Loss: 0.00008758
Iteration 254/1000 | Loss: 0.00008758
Iteration 255/1000 | Loss: 0.00008758
Iteration 256/1000 | Loss: 0.00008758
Iteration 257/1000 | Loss: 0.00008758
Iteration 258/1000 | Loss: 0.00008758
Iteration 259/1000 | Loss: 0.00008758
Iteration 260/1000 | Loss: 0.00008758
Iteration 261/1000 | Loss: 0.00008758
Iteration 262/1000 | Loss: 0.00008758
Iteration 263/1000 | Loss: 0.00008758
Iteration 264/1000 | Loss: 0.00008758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [8.758203330216929e-05, 8.758203330216929e-05, 8.758203330216929e-05, 8.758203330216929e-05, 8.758203330216929e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.758203330216929e-05

Optimization complete. Final v2v error: 5.314583778381348 mm

Highest mean error: 11.733787536621094 mm for frame 21

Lowest mean error: 3.6213877201080322 mm for frame 4

Saving results

Total time: 153.31468558311462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006443
Iteration 2/25 | Loss: 0.01006443
Iteration 3/25 | Loss: 0.01006443
Iteration 4/25 | Loss: 0.01006442
Iteration 5/25 | Loss: 0.01006442
Iteration 6/25 | Loss: 0.01006442
Iteration 7/25 | Loss: 0.01006442
Iteration 8/25 | Loss: 0.01006442
Iteration 9/25 | Loss: 0.01006442
Iteration 10/25 | Loss: 0.01006442
Iteration 11/25 | Loss: 0.01006442
Iteration 12/25 | Loss: 0.01006442
Iteration 13/25 | Loss: 0.01006441
Iteration 14/25 | Loss: 0.01006441
Iteration 15/25 | Loss: 0.01006441
Iteration 16/25 | Loss: 0.01006441
Iteration 17/25 | Loss: 0.01006441
Iteration 18/25 | Loss: 0.01006441
Iteration 19/25 | Loss: 0.01006441
Iteration 20/25 | Loss: 0.01006441
Iteration 21/25 | Loss: 0.01006441
Iteration 22/25 | Loss: 0.01006441
Iteration 23/25 | Loss: 0.01006441
Iteration 24/25 | Loss: 0.01006441
Iteration 25/25 | Loss: 0.01006440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85137951
Iteration 2/25 | Loss: 0.11367156
Iteration 3/25 | Loss: 0.11098607
Iteration 4/25 | Loss: 0.11379164
Iteration 5/25 | Loss: 0.11417769
Iteration 6/25 | Loss: 0.11342181
Iteration 7/25 | Loss: 0.11268166
Iteration 8/25 | Loss: 0.11349275
Iteration 9/25 | Loss: 0.10996299
Iteration 10/25 | Loss: 0.10989276
Iteration 11/25 | Loss: 0.10974754
Iteration 12/25 | Loss: 0.10970095
Iteration 13/25 | Loss: 0.10970093
Iteration 14/25 | Loss: 0.10970093
Iteration 15/25 | Loss: 0.10970093
Iteration 16/25 | Loss: 0.10970093
Iteration 17/25 | Loss: 0.10970092
Iteration 18/25 | Loss: 0.10970092
Iteration 19/25 | Loss: 0.10970092
Iteration 20/25 | Loss: 0.10970092
Iteration 21/25 | Loss: 0.10970092
Iteration 22/25 | Loss: 0.10970093
Iteration 23/25 | Loss: 0.10970093
Iteration 24/25 | Loss: 0.10970092
Iteration 25/25 | Loss: 0.10970092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10970092
Iteration 2/1000 | Loss: 0.00122285
Iteration 3/1000 | Loss: 0.00171402
Iteration 4/1000 | Loss: 0.00345711
Iteration 5/1000 | Loss: 0.00026014
Iteration 6/1000 | Loss: 0.00022612
Iteration 7/1000 | Loss: 0.00017554
Iteration 8/1000 | Loss: 0.00014508
Iteration 9/1000 | Loss: 0.00007549
Iteration 10/1000 | Loss: 0.00012600
Iteration 11/1000 | Loss: 0.00005721
Iteration 12/1000 | Loss: 0.00006025
Iteration 13/1000 | Loss: 0.00009923
Iteration 14/1000 | Loss: 0.00004676
Iteration 15/1000 | Loss: 0.00022425
Iteration 16/1000 | Loss: 0.00004097
Iteration 17/1000 | Loss: 0.00009060
Iteration 18/1000 | Loss: 0.00004812
Iteration 19/1000 | Loss: 0.00003986
Iteration 20/1000 | Loss: 0.00003414
Iteration 21/1000 | Loss: 0.00004086
Iteration 22/1000 | Loss: 0.00013882
Iteration 23/1000 | Loss: 0.00004038
Iteration 24/1000 | Loss: 0.00027886
Iteration 25/1000 | Loss: 0.00003089
Iteration 26/1000 | Loss: 0.00003440
Iteration 27/1000 | Loss: 0.00007675
Iteration 28/1000 | Loss: 0.00011439
Iteration 29/1000 | Loss: 0.00006469
Iteration 30/1000 | Loss: 0.00029409
Iteration 31/1000 | Loss: 0.00004949
Iteration 32/1000 | Loss: 0.00007181
Iteration 33/1000 | Loss: 0.00002760
Iteration 34/1000 | Loss: 0.00004795
Iteration 35/1000 | Loss: 0.00002715
Iteration 36/1000 | Loss: 0.00003581
Iteration 37/1000 | Loss: 0.00007160
Iteration 38/1000 | Loss: 0.00009169
Iteration 39/1000 | Loss: 0.00003256
Iteration 40/1000 | Loss: 0.00004835
Iteration 41/1000 | Loss: 0.00002575
Iteration 42/1000 | Loss: 0.00004533
Iteration 43/1000 | Loss: 0.00002542
Iteration 44/1000 | Loss: 0.00006668
Iteration 45/1000 | Loss: 0.00020571
Iteration 46/1000 | Loss: 0.00003009
Iteration 47/1000 | Loss: 0.00002768
Iteration 48/1000 | Loss: 0.00002496
Iteration 49/1000 | Loss: 0.00002681
Iteration 50/1000 | Loss: 0.00002479
Iteration 51/1000 | Loss: 0.00002769
Iteration 52/1000 | Loss: 0.00004085
Iteration 53/1000 | Loss: 0.00027261
Iteration 54/1000 | Loss: 0.00002786
Iteration 55/1000 | Loss: 0.00002927
Iteration 56/1000 | Loss: 0.00003751
Iteration 57/1000 | Loss: 0.00002444
Iteration 58/1000 | Loss: 0.00002443
Iteration 59/1000 | Loss: 0.00002443
Iteration 60/1000 | Loss: 0.00002443
Iteration 61/1000 | Loss: 0.00002443
Iteration 62/1000 | Loss: 0.00002443
Iteration 63/1000 | Loss: 0.00002442
Iteration 64/1000 | Loss: 0.00002442
Iteration 65/1000 | Loss: 0.00002442
Iteration 66/1000 | Loss: 0.00002442
Iteration 67/1000 | Loss: 0.00002441
Iteration 68/1000 | Loss: 0.00002441
Iteration 69/1000 | Loss: 0.00002440
Iteration 70/1000 | Loss: 0.00002440
Iteration 71/1000 | Loss: 0.00002440
Iteration 72/1000 | Loss: 0.00002439
Iteration 73/1000 | Loss: 0.00002439
Iteration 74/1000 | Loss: 0.00002439
Iteration 75/1000 | Loss: 0.00002439
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002438
Iteration 78/1000 | Loss: 0.00002438
Iteration 79/1000 | Loss: 0.00002438
Iteration 80/1000 | Loss: 0.00002437
Iteration 81/1000 | Loss: 0.00002437
Iteration 82/1000 | Loss: 0.00003011
Iteration 83/1000 | Loss: 0.00002432
Iteration 84/1000 | Loss: 0.00002432
Iteration 85/1000 | Loss: 0.00002432
Iteration 86/1000 | Loss: 0.00002431
Iteration 87/1000 | Loss: 0.00002431
Iteration 88/1000 | Loss: 0.00002431
Iteration 89/1000 | Loss: 0.00002431
Iteration 90/1000 | Loss: 0.00002430
Iteration 91/1000 | Loss: 0.00006459
Iteration 92/1000 | Loss: 0.00002800
Iteration 93/1000 | Loss: 0.00005036
Iteration 94/1000 | Loss: 0.00007221
Iteration 95/1000 | Loss: 0.00016806
Iteration 96/1000 | Loss: 0.00002806
Iteration 97/1000 | Loss: 0.00002416
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002416
Iteration 100/1000 | Loss: 0.00002416
Iteration 101/1000 | Loss: 0.00002416
Iteration 102/1000 | Loss: 0.00002416
Iteration 103/1000 | Loss: 0.00002416
Iteration 104/1000 | Loss: 0.00002416
Iteration 105/1000 | Loss: 0.00002416
Iteration 106/1000 | Loss: 0.00002416
Iteration 107/1000 | Loss: 0.00002415
Iteration 108/1000 | Loss: 0.00002415
Iteration 109/1000 | Loss: 0.00002415
Iteration 110/1000 | Loss: 0.00002415
Iteration 111/1000 | Loss: 0.00002415
Iteration 112/1000 | Loss: 0.00002415
Iteration 113/1000 | Loss: 0.00002415
Iteration 114/1000 | Loss: 0.00002415
Iteration 115/1000 | Loss: 0.00002415
Iteration 116/1000 | Loss: 0.00002415
Iteration 117/1000 | Loss: 0.00002415
Iteration 118/1000 | Loss: 0.00002414
Iteration 119/1000 | Loss: 0.00002414
Iteration 120/1000 | Loss: 0.00004304
Iteration 121/1000 | Loss: 0.00002446
Iteration 122/1000 | Loss: 0.00004082
Iteration 123/1000 | Loss: 0.00005260
Iteration 124/1000 | Loss: 0.00003661
Iteration 125/1000 | Loss: 0.00003272
Iteration 126/1000 | Loss: 0.00002413
Iteration 127/1000 | Loss: 0.00002411
Iteration 128/1000 | Loss: 0.00002411
Iteration 129/1000 | Loss: 0.00002410
Iteration 130/1000 | Loss: 0.00002410
Iteration 131/1000 | Loss: 0.00002410
Iteration 132/1000 | Loss: 0.00002409
Iteration 133/1000 | Loss: 0.00002409
Iteration 134/1000 | Loss: 0.00002409
Iteration 135/1000 | Loss: 0.00002409
Iteration 136/1000 | Loss: 0.00002409
Iteration 137/1000 | Loss: 0.00002408
Iteration 138/1000 | Loss: 0.00002408
Iteration 139/1000 | Loss: 0.00002408
Iteration 140/1000 | Loss: 0.00002408
Iteration 141/1000 | Loss: 0.00002408
Iteration 142/1000 | Loss: 0.00002407
Iteration 143/1000 | Loss: 0.00002407
Iteration 144/1000 | Loss: 0.00002407
Iteration 145/1000 | Loss: 0.00002406
Iteration 146/1000 | Loss: 0.00002562
Iteration 147/1000 | Loss: 0.00002404
Iteration 148/1000 | Loss: 0.00002404
Iteration 149/1000 | Loss: 0.00002404
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002403
Iteration 152/1000 | Loss: 0.00002403
Iteration 153/1000 | Loss: 0.00002402
Iteration 154/1000 | Loss: 0.00002402
Iteration 155/1000 | Loss: 0.00002402
Iteration 156/1000 | Loss: 0.00002402
Iteration 157/1000 | Loss: 0.00002401
Iteration 158/1000 | Loss: 0.00002401
Iteration 159/1000 | Loss: 0.00002397
Iteration 160/1000 | Loss: 0.00002397
Iteration 161/1000 | Loss: 0.00002396
Iteration 162/1000 | Loss: 0.00002396
Iteration 163/1000 | Loss: 0.00002396
Iteration 164/1000 | Loss: 0.00002396
Iteration 165/1000 | Loss: 0.00002396
Iteration 166/1000 | Loss: 0.00002396
Iteration 167/1000 | Loss: 0.00002396
Iteration 168/1000 | Loss: 0.00002396
Iteration 169/1000 | Loss: 0.00002396
Iteration 170/1000 | Loss: 0.00002396
Iteration 171/1000 | Loss: 0.00002396
Iteration 172/1000 | Loss: 0.00002395
Iteration 173/1000 | Loss: 0.00002395
Iteration 174/1000 | Loss: 0.00002395
Iteration 175/1000 | Loss: 0.00002395
Iteration 176/1000 | Loss: 0.00002394
Iteration 177/1000 | Loss: 0.00002394
Iteration 178/1000 | Loss: 0.00002394
Iteration 179/1000 | Loss: 0.00002393
Iteration 180/1000 | Loss: 0.00002393
Iteration 181/1000 | Loss: 0.00002393
Iteration 182/1000 | Loss: 0.00002393
Iteration 183/1000 | Loss: 0.00002393
Iteration 184/1000 | Loss: 0.00002393
Iteration 185/1000 | Loss: 0.00002393
Iteration 186/1000 | Loss: 0.00002393
Iteration 187/1000 | Loss: 0.00002393
Iteration 188/1000 | Loss: 0.00002393
Iteration 189/1000 | Loss: 0.00002393
Iteration 190/1000 | Loss: 0.00002392
Iteration 191/1000 | Loss: 0.00002392
Iteration 192/1000 | Loss: 0.00002392
Iteration 193/1000 | Loss: 0.00002392
Iteration 194/1000 | Loss: 0.00002392
Iteration 195/1000 | Loss: 0.00002392
Iteration 196/1000 | Loss: 0.00002392
Iteration 197/1000 | Loss: 0.00002392
Iteration 198/1000 | Loss: 0.00002391
Iteration 199/1000 | Loss: 0.00002391
Iteration 200/1000 | Loss: 0.00002391
Iteration 201/1000 | Loss: 0.00002391
Iteration 202/1000 | Loss: 0.00002391
Iteration 203/1000 | Loss: 0.00002391
Iteration 204/1000 | Loss: 0.00002391
Iteration 205/1000 | Loss: 0.00002391
Iteration 206/1000 | Loss: 0.00002391
Iteration 207/1000 | Loss: 0.00002391
Iteration 208/1000 | Loss: 0.00002391
Iteration 209/1000 | Loss: 0.00002390
Iteration 210/1000 | Loss: 0.00002390
Iteration 211/1000 | Loss: 0.00002390
Iteration 212/1000 | Loss: 0.00002390
Iteration 213/1000 | Loss: 0.00002390
Iteration 214/1000 | Loss: 0.00002390
Iteration 215/1000 | Loss: 0.00002389
Iteration 216/1000 | Loss: 0.00002389
Iteration 217/1000 | Loss: 0.00002389
Iteration 218/1000 | Loss: 0.00002389
Iteration 219/1000 | Loss: 0.00002389
Iteration 220/1000 | Loss: 0.00002389
Iteration 221/1000 | Loss: 0.00002389
Iteration 222/1000 | Loss: 0.00006863
Iteration 223/1000 | Loss: 0.00002905
Iteration 224/1000 | Loss: 0.00002511
Iteration 225/1000 | Loss: 0.00002391
Iteration 226/1000 | Loss: 0.00002391
Iteration 227/1000 | Loss: 0.00002391
Iteration 228/1000 | Loss: 0.00002391
Iteration 229/1000 | Loss: 0.00002391
Iteration 230/1000 | Loss: 0.00002391
Iteration 231/1000 | Loss: 0.00002391
Iteration 232/1000 | Loss: 0.00002391
Iteration 233/1000 | Loss: 0.00002391
Iteration 234/1000 | Loss: 0.00002391
Iteration 235/1000 | Loss: 0.00002391
Iteration 236/1000 | Loss: 0.00002388
Iteration 237/1000 | Loss: 0.00002388
Iteration 238/1000 | Loss: 0.00004194
Iteration 239/1000 | Loss: 0.00002396
Iteration 240/1000 | Loss: 0.00002387
Iteration 241/1000 | Loss: 0.00002387
Iteration 242/1000 | Loss: 0.00002387
Iteration 243/1000 | Loss: 0.00002387
Iteration 244/1000 | Loss: 0.00002387
Iteration 245/1000 | Loss: 0.00002386
Iteration 246/1000 | Loss: 0.00002386
Iteration 247/1000 | Loss: 0.00002386
Iteration 248/1000 | Loss: 0.00002386
Iteration 249/1000 | Loss: 0.00002386
Iteration 250/1000 | Loss: 0.00002386
Iteration 251/1000 | Loss: 0.00002386
Iteration 252/1000 | Loss: 0.00002386
Iteration 253/1000 | Loss: 0.00002386
Iteration 254/1000 | Loss: 0.00002386
Iteration 255/1000 | Loss: 0.00002386
Iteration 256/1000 | Loss: 0.00002386
Iteration 257/1000 | Loss: 0.00002386
Iteration 258/1000 | Loss: 0.00002385
Iteration 259/1000 | Loss: 0.00002385
Iteration 260/1000 | Loss: 0.00002385
Iteration 261/1000 | Loss: 0.00002385
Iteration 262/1000 | Loss: 0.00002385
Iteration 263/1000 | Loss: 0.00002385
Iteration 264/1000 | Loss: 0.00002385
Iteration 265/1000 | Loss: 0.00002385
Iteration 266/1000 | Loss: 0.00002385
Iteration 267/1000 | Loss: 0.00002385
Iteration 268/1000 | Loss: 0.00002385
Iteration 269/1000 | Loss: 0.00002385
Iteration 270/1000 | Loss: 0.00002385
Iteration 271/1000 | Loss: 0.00002385
Iteration 272/1000 | Loss: 0.00002385
Iteration 273/1000 | Loss: 0.00002385
Iteration 274/1000 | Loss: 0.00002385
Iteration 275/1000 | Loss: 0.00002385
Iteration 276/1000 | Loss: 0.00002385
Iteration 277/1000 | Loss: 0.00002385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [2.3846088879508898e-05, 2.3846088879508898e-05, 2.3846088879508898e-05, 2.3846088879508898e-05, 2.3846088879508898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3846088879508898e-05

Optimization complete. Final v2v error: 4.149571895599365 mm

Highest mean error: 5.694047451019287 mm for frame 192

Lowest mean error: 3.200972080230713 mm for frame 86

Saving results

Total time: 152.57254147529602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430420
Iteration 2/25 | Loss: 0.00157261
Iteration 3/25 | Loss: 0.00149435
Iteration 4/25 | Loss: 0.00148783
Iteration 5/25 | Loss: 0.00148631
Iteration 6/25 | Loss: 0.00148631
Iteration 7/25 | Loss: 0.00148631
Iteration 8/25 | Loss: 0.00148631
Iteration 9/25 | Loss: 0.00148631
Iteration 10/25 | Loss: 0.00148631
Iteration 11/25 | Loss: 0.00148631
Iteration 12/25 | Loss: 0.00148631
Iteration 13/25 | Loss: 0.00148631
Iteration 14/25 | Loss: 0.00148631
Iteration 15/25 | Loss: 0.00148631
Iteration 16/25 | Loss: 0.00148631
Iteration 17/25 | Loss: 0.00148631
Iteration 18/25 | Loss: 0.00148631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014863059623166919, 0.0014863059623166919, 0.0014863059623166919, 0.0014863059623166919, 0.0014863059623166919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014863059623166919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.91865897
Iteration 2/25 | Loss: 0.00202991
Iteration 3/25 | Loss: 0.00202990
Iteration 4/25 | Loss: 0.00202990
Iteration 5/25 | Loss: 0.00202990
Iteration 6/25 | Loss: 0.00202990
Iteration 7/25 | Loss: 0.00202990
Iteration 8/25 | Loss: 0.00202990
Iteration 9/25 | Loss: 0.00202990
Iteration 10/25 | Loss: 0.00202990
Iteration 11/25 | Loss: 0.00202990
Iteration 12/25 | Loss: 0.00202990
Iteration 13/25 | Loss: 0.00202990
Iteration 14/25 | Loss: 0.00202990
Iteration 15/25 | Loss: 0.00202990
Iteration 16/25 | Loss: 0.00202990
Iteration 17/25 | Loss: 0.00202990
Iteration 18/25 | Loss: 0.00202990
Iteration 19/25 | Loss: 0.00202990
Iteration 20/25 | Loss: 0.00202990
Iteration 21/25 | Loss: 0.00202990
Iteration 22/25 | Loss: 0.00202990
Iteration 23/25 | Loss: 0.00202990
Iteration 24/25 | Loss: 0.00202990
Iteration 25/25 | Loss: 0.00202990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202990
Iteration 2/1000 | Loss: 0.00002913
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001849
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001748
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001647
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001612
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001604
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001601
Iteration 32/1000 | Loss: 0.00001601
Iteration 33/1000 | Loss: 0.00001600
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001593
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001592
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001588
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001587
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001583
Iteration 126/1000 | Loss: 0.00001583
Iteration 127/1000 | Loss: 0.00001583
Iteration 128/1000 | Loss: 0.00001583
Iteration 129/1000 | Loss: 0.00001583
Iteration 130/1000 | Loss: 0.00001583
Iteration 131/1000 | Loss: 0.00001583
Iteration 132/1000 | Loss: 0.00001582
Iteration 133/1000 | Loss: 0.00001582
Iteration 134/1000 | Loss: 0.00001582
Iteration 135/1000 | Loss: 0.00001582
Iteration 136/1000 | Loss: 0.00001582
Iteration 137/1000 | Loss: 0.00001581
Iteration 138/1000 | Loss: 0.00001581
Iteration 139/1000 | Loss: 0.00001581
Iteration 140/1000 | Loss: 0.00001581
Iteration 141/1000 | Loss: 0.00001581
Iteration 142/1000 | Loss: 0.00001581
Iteration 143/1000 | Loss: 0.00001580
Iteration 144/1000 | Loss: 0.00001580
Iteration 145/1000 | Loss: 0.00001580
Iteration 146/1000 | Loss: 0.00001580
Iteration 147/1000 | Loss: 0.00001580
Iteration 148/1000 | Loss: 0.00001580
Iteration 149/1000 | Loss: 0.00001580
Iteration 150/1000 | Loss: 0.00001580
Iteration 151/1000 | Loss: 0.00001580
Iteration 152/1000 | Loss: 0.00001580
Iteration 153/1000 | Loss: 0.00001580
Iteration 154/1000 | Loss: 0.00001580
Iteration 155/1000 | Loss: 0.00001580
Iteration 156/1000 | Loss: 0.00001580
Iteration 157/1000 | Loss: 0.00001580
Iteration 158/1000 | Loss: 0.00001580
Iteration 159/1000 | Loss: 0.00001580
Iteration 160/1000 | Loss: 0.00001580
Iteration 161/1000 | Loss: 0.00001580
Iteration 162/1000 | Loss: 0.00001579
Iteration 163/1000 | Loss: 0.00001579
Iteration 164/1000 | Loss: 0.00001579
Iteration 165/1000 | Loss: 0.00001579
Iteration 166/1000 | Loss: 0.00001579
Iteration 167/1000 | Loss: 0.00001579
Iteration 168/1000 | Loss: 0.00001579
Iteration 169/1000 | Loss: 0.00001579
Iteration 170/1000 | Loss: 0.00001578
Iteration 171/1000 | Loss: 0.00001578
Iteration 172/1000 | Loss: 0.00001578
Iteration 173/1000 | Loss: 0.00001578
Iteration 174/1000 | Loss: 0.00001578
Iteration 175/1000 | Loss: 0.00001578
Iteration 176/1000 | Loss: 0.00001578
Iteration 177/1000 | Loss: 0.00001578
Iteration 178/1000 | Loss: 0.00001578
Iteration 179/1000 | Loss: 0.00001578
Iteration 180/1000 | Loss: 0.00001578
Iteration 181/1000 | Loss: 0.00001578
Iteration 182/1000 | Loss: 0.00001578
Iteration 183/1000 | Loss: 0.00001578
Iteration 184/1000 | Loss: 0.00001578
Iteration 185/1000 | Loss: 0.00001578
Iteration 186/1000 | Loss: 0.00001578
Iteration 187/1000 | Loss: 0.00001578
Iteration 188/1000 | Loss: 0.00001578
Iteration 189/1000 | Loss: 0.00001578
Iteration 190/1000 | Loss: 0.00001578
Iteration 191/1000 | Loss: 0.00001578
Iteration 192/1000 | Loss: 0.00001578
Iteration 193/1000 | Loss: 0.00001578
Iteration 194/1000 | Loss: 0.00001578
Iteration 195/1000 | Loss: 0.00001578
Iteration 196/1000 | Loss: 0.00001578
Iteration 197/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.5776458894833922e-05, 1.5776458894833922e-05, 1.5776458894833922e-05, 1.5776458894833922e-05, 1.5776458894833922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5776458894833922e-05

Optimization complete. Final v2v error: 3.424513578414917 mm

Highest mean error: 3.996610641479492 mm for frame 103

Lowest mean error: 3.1060874462127686 mm for frame 0

Saving results

Total time: 41.33365559577942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407907
Iteration 2/25 | Loss: 0.00160462
Iteration 3/25 | Loss: 0.00150056
Iteration 4/25 | Loss: 0.00148118
Iteration 5/25 | Loss: 0.00147563
Iteration 6/25 | Loss: 0.00147501
Iteration 7/25 | Loss: 0.00147501
Iteration 8/25 | Loss: 0.00147501
Iteration 9/25 | Loss: 0.00147501
Iteration 10/25 | Loss: 0.00147501
Iteration 11/25 | Loss: 0.00147501
Iteration 12/25 | Loss: 0.00147501
Iteration 13/25 | Loss: 0.00147501
Iteration 14/25 | Loss: 0.00147501
Iteration 15/25 | Loss: 0.00147501
Iteration 16/25 | Loss: 0.00147501
Iteration 17/25 | Loss: 0.00147501
Iteration 18/25 | Loss: 0.00147501
Iteration 19/25 | Loss: 0.00147501
Iteration 20/25 | Loss: 0.00147501
Iteration 21/25 | Loss: 0.00147501
Iteration 22/25 | Loss: 0.00147501
Iteration 23/25 | Loss: 0.00147501
Iteration 24/25 | Loss: 0.00147501
Iteration 25/25 | Loss: 0.00147501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19506145
Iteration 2/25 | Loss: 0.00272343
Iteration 3/25 | Loss: 0.00272343
Iteration 4/25 | Loss: 0.00272343
Iteration 5/25 | Loss: 0.00272343
Iteration 6/25 | Loss: 0.00272343
Iteration 7/25 | Loss: 0.00272342
Iteration 8/25 | Loss: 0.00272342
Iteration 9/25 | Loss: 0.00272342
Iteration 10/25 | Loss: 0.00272342
Iteration 11/25 | Loss: 0.00272342
Iteration 12/25 | Loss: 0.00272342
Iteration 13/25 | Loss: 0.00272342
Iteration 14/25 | Loss: 0.00272342
Iteration 15/25 | Loss: 0.00272342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002723423996940255, 0.002723423996940255, 0.002723423996940255, 0.002723423996940255, 0.002723423996940255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002723423996940255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272342
Iteration 2/1000 | Loss: 0.00004921
Iteration 3/1000 | Loss: 0.00003665
Iteration 4/1000 | Loss: 0.00003140
Iteration 5/1000 | Loss: 0.00002847
Iteration 6/1000 | Loss: 0.00002678
Iteration 7/1000 | Loss: 0.00002550
Iteration 8/1000 | Loss: 0.00002468
Iteration 9/1000 | Loss: 0.00002400
Iteration 10/1000 | Loss: 0.00002353
Iteration 11/1000 | Loss: 0.00002319
Iteration 12/1000 | Loss: 0.00002279
Iteration 13/1000 | Loss: 0.00002248
Iteration 14/1000 | Loss: 0.00002219
Iteration 15/1000 | Loss: 0.00002188
Iteration 16/1000 | Loss: 0.00002182
Iteration 17/1000 | Loss: 0.00002173
Iteration 18/1000 | Loss: 0.00002171
Iteration 19/1000 | Loss: 0.00002156
Iteration 20/1000 | Loss: 0.00002145
Iteration 21/1000 | Loss: 0.00002144
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002137
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002137
Iteration 27/1000 | Loss: 0.00002137
Iteration 28/1000 | Loss: 0.00002136
Iteration 29/1000 | Loss: 0.00002136
Iteration 30/1000 | Loss: 0.00002135
Iteration 31/1000 | Loss: 0.00002134
Iteration 32/1000 | Loss: 0.00002134
Iteration 33/1000 | Loss: 0.00002134
Iteration 34/1000 | Loss: 0.00002133
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002131
Iteration 37/1000 | Loss: 0.00002131
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002128
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002126
Iteration 48/1000 | Loss: 0.00002126
Iteration 49/1000 | Loss: 0.00002125
Iteration 50/1000 | Loss: 0.00002125
Iteration 51/1000 | Loss: 0.00002124
Iteration 52/1000 | Loss: 0.00002123
Iteration 53/1000 | Loss: 0.00002120
Iteration 54/1000 | Loss: 0.00002119
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002115
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002114
Iteration 60/1000 | Loss: 0.00002114
Iteration 61/1000 | Loss: 0.00002113
Iteration 62/1000 | Loss: 0.00002113
Iteration 63/1000 | Loss: 0.00002113
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002108
Iteration 72/1000 | Loss: 0.00002108
Iteration 73/1000 | Loss: 0.00002108
Iteration 74/1000 | Loss: 0.00002108
Iteration 75/1000 | Loss: 0.00002107
Iteration 76/1000 | Loss: 0.00002107
Iteration 77/1000 | Loss: 0.00002107
Iteration 78/1000 | Loss: 0.00002107
Iteration 79/1000 | Loss: 0.00002107
Iteration 80/1000 | Loss: 0.00002107
Iteration 81/1000 | Loss: 0.00002107
Iteration 82/1000 | Loss: 0.00002106
Iteration 83/1000 | Loss: 0.00002105
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002103
Iteration 86/1000 | Loss: 0.00002103
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002102
Iteration 90/1000 | Loss: 0.00002102
Iteration 91/1000 | Loss: 0.00002102
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002100
Iteration 96/1000 | Loss: 0.00002100
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002099
Iteration 101/1000 | Loss: 0.00002099
Iteration 102/1000 | Loss: 0.00002099
Iteration 103/1000 | Loss: 0.00002099
Iteration 104/1000 | Loss: 0.00002099
Iteration 105/1000 | Loss: 0.00002099
Iteration 106/1000 | Loss: 0.00002099
Iteration 107/1000 | Loss: 0.00002099
Iteration 108/1000 | Loss: 0.00002099
Iteration 109/1000 | Loss: 0.00002099
Iteration 110/1000 | Loss: 0.00002099
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002098
Iteration 113/1000 | Loss: 0.00002098
Iteration 114/1000 | Loss: 0.00002098
Iteration 115/1000 | Loss: 0.00002098
Iteration 116/1000 | Loss: 0.00002098
Iteration 117/1000 | Loss: 0.00002098
Iteration 118/1000 | Loss: 0.00002097
Iteration 119/1000 | Loss: 0.00002097
Iteration 120/1000 | Loss: 0.00002097
Iteration 121/1000 | Loss: 0.00002097
Iteration 122/1000 | Loss: 0.00002097
Iteration 123/1000 | Loss: 0.00002097
Iteration 124/1000 | Loss: 0.00002097
Iteration 125/1000 | Loss: 0.00002097
Iteration 126/1000 | Loss: 0.00002097
Iteration 127/1000 | Loss: 0.00002097
Iteration 128/1000 | Loss: 0.00002097
Iteration 129/1000 | Loss: 0.00002097
Iteration 130/1000 | Loss: 0.00002097
Iteration 131/1000 | Loss: 0.00002097
Iteration 132/1000 | Loss: 0.00002096
Iteration 133/1000 | Loss: 0.00002096
Iteration 134/1000 | Loss: 0.00002096
Iteration 135/1000 | Loss: 0.00002096
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002095
Iteration 138/1000 | Loss: 0.00002095
Iteration 139/1000 | Loss: 0.00002094
Iteration 140/1000 | Loss: 0.00002094
Iteration 141/1000 | Loss: 0.00002094
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002094
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002094
Iteration 146/1000 | Loss: 0.00002093
Iteration 147/1000 | Loss: 0.00002093
Iteration 148/1000 | Loss: 0.00002093
Iteration 149/1000 | Loss: 0.00002093
Iteration 150/1000 | Loss: 0.00002093
Iteration 151/1000 | Loss: 0.00002093
Iteration 152/1000 | Loss: 0.00002093
Iteration 153/1000 | Loss: 0.00002092
Iteration 154/1000 | Loss: 0.00002092
Iteration 155/1000 | Loss: 0.00002092
Iteration 156/1000 | Loss: 0.00002092
Iteration 157/1000 | Loss: 0.00002092
Iteration 158/1000 | Loss: 0.00002092
Iteration 159/1000 | Loss: 0.00002092
Iteration 160/1000 | Loss: 0.00002091
Iteration 161/1000 | Loss: 0.00002091
Iteration 162/1000 | Loss: 0.00002091
Iteration 163/1000 | Loss: 0.00002091
Iteration 164/1000 | Loss: 0.00002091
Iteration 165/1000 | Loss: 0.00002090
Iteration 166/1000 | Loss: 0.00002090
Iteration 167/1000 | Loss: 0.00002090
Iteration 168/1000 | Loss: 0.00002090
Iteration 169/1000 | Loss: 0.00002089
Iteration 170/1000 | Loss: 0.00002089
Iteration 171/1000 | Loss: 0.00002089
Iteration 172/1000 | Loss: 0.00002089
Iteration 173/1000 | Loss: 0.00002088
Iteration 174/1000 | Loss: 0.00002088
Iteration 175/1000 | Loss: 0.00002088
Iteration 176/1000 | Loss: 0.00002088
Iteration 177/1000 | Loss: 0.00002088
Iteration 178/1000 | Loss: 0.00002088
Iteration 179/1000 | Loss: 0.00002088
Iteration 180/1000 | Loss: 0.00002088
Iteration 181/1000 | Loss: 0.00002088
Iteration 182/1000 | Loss: 0.00002087
Iteration 183/1000 | Loss: 0.00002087
Iteration 184/1000 | Loss: 0.00002087
Iteration 185/1000 | Loss: 0.00002087
Iteration 186/1000 | Loss: 0.00002087
Iteration 187/1000 | Loss: 0.00002087
Iteration 188/1000 | Loss: 0.00002087
Iteration 189/1000 | Loss: 0.00002087
Iteration 190/1000 | Loss: 0.00002087
Iteration 191/1000 | Loss: 0.00002087
Iteration 192/1000 | Loss: 0.00002087
Iteration 193/1000 | Loss: 0.00002087
Iteration 194/1000 | Loss: 0.00002087
Iteration 195/1000 | Loss: 0.00002087
Iteration 196/1000 | Loss: 0.00002087
Iteration 197/1000 | Loss: 0.00002086
Iteration 198/1000 | Loss: 0.00002086
Iteration 199/1000 | Loss: 0.00002086
Iteration 200/1000 | Loss: 0.00002086
Iteration 201/1000 | Loss: 0.00002086
Iteration 202/1000 | Loss: 0.00002086
Iteration 203/1000 | Loss: 0.00002086
Iteration 204/1000 | Loss: 0.00002086
Iteration 205/1000 | Loss: 0.00002086
Iteration 206/1000 | Loss: 0.00002086
Iteration 207/1000 | Loss: 0.00002086
Iteration 208/1000 | Loss: 0.00002085
Iteration 209/1000 | Loss: 0.00002085
Iteration 210/1000 | Loss: 0.00002085
Iteration 211/1000 | Loss: 0.00002085
Iteration 212/1000 | Loss: 0.00002084
Iteration 213/1000 | Loss: 0.00002084
Iteration 214/1000 | Loss: 0.00002084
Iteration 215/1000 | Loss: 0.00002084
Iteration 216/1000 | Loss: 0.00002084
Iteration 217/1000 | Loss: 0.00002083
Iteration 218/1000 | Loss: 0.00002083
Iteration 219/1000 | Loss: 0.00002083
Iteration 220/1000 | Loss: 0.00002083
Iteration 221/1000 | Loss: 0.00002083
Iteration 222/1000 | Loss: 0.00002083
Iteration 223/1000 | Loss: 0.00002083
Iteration 224/1000 | Loss: 0.00002083
Iteration 225/1000 | Loss: 0.00002083
Iteration 226/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.0834691895288415e-05, 2.0834691895288415e-05, 2.0834691895288415e-05, 2.0834691895288415e-05, 2.0834691895288415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0834691895288415e-05

Optimization complete. Final v2v error: 3.8335652351379395 mm

Highest mean error: 4.38327693939209 mm for frame 61

Lowest mean error: 3.4760446548461914 mm for frame 132

Saving results

Total time: 50.69958758354187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739631
Iteration 2/25 | Loss: 0.00186835
Iteration 3/25 | Loss: 0.00160031
Iteration 4/25 | Loss: 0.00156613
Iteration 5/25 | Loss: 0.00155658
Iteration 6/25 | Loss: 0.00154733
Iteration 7/25 | Loss: 0.00153372
Iteration 8/25 | Loss: 0.00152584
Iteration 9/25 | Loss: 0.00152292
Iteration 10/25 | Loss: 0.00152124
Iteration 11/25 | Loss: 0.00152049
Iteration 12/25 | Loss: 0.00152049
Iteration 13/25 | Loss: 0.00152034
Iteration 14/25 | Loss: 0.00152046
Iteration 15/25 | Loss: 0.00152013
Iteration 16/25 | Loss: 0.00151988
Iteration 17/25 | Loss: 0.00151934
Iteration 18/25 | Loss: 0.00152160
Iteration 19/25 | Loss: 0.00152072
Iteration 20/25 | Loss: 0.00151911
Iteration 21/25 | Loss: 0.00151590
Iteration 22/25 | Loss: 0.00151560
Iteration 23/25 | Loss: 0.00151547
Iteration 24/25 | Loss: 0.00151577
Iteration 25/25 | Loss: 0.00151542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.46741819
Iteration 2/25 | Loss: 0.00240410
Iteration 3/25 | Loss: 0.00240407
Iteration 4/25 | Loss: 0.00240407
Iteration 5/25 | Loss: 0.00240407
Iteration 6/25 | Loss: 0.00240406
Iteration 7/25 | Loss: 0.00240406
Iteration 8/25 | Loss: 0.00240406
Iteration 9/25 | Loss: 0.00240406
Iteration 10/25 | Loss: 0.00240406
Iteration 11/25 | Loss: 0.00240406
Iteration 12/25 | Loss: 0.00240406
Iteration 13/25 | Loss: 0.00240406
Iteration 14/25 | Loss: 0.00240406
Iteration 15/25 | Loss: 0.00240406
Iteration 16/25 | Loss: 0.00240406
Iteration 17/25 | Loss: 0.00240406
Iteration 18/25 | Loss: 0.00240406
Iteration 19/25 | Loss: 0.00240406
Iteration 20/25 | Loss: 0.00240406
Iteration 21/25 | Loss: 0.00240406
Iteration 22/25 | Loss: 0.00240406
Iteration 23/25 | Loss: 0.00240406
Iteration 24/25 | Loss: 0.00240406
Iteration 25/25 | Loss: 0.00240406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240406
Iteration 2/1000 | Loss: 0.00003160
Iteration 3/1000 | Loss: 0.00002720
Iteration 4/1000 | Loss: 0.00002254
Iteration 5/1000 | Loss: 0.00002581
Iteration 6/1000 | Loss: 0.00002032
Iteration 7/1000 | Loss: 0.00001960
Iteration 8/1000 | Loss: 0.00001918
Iteration 9/1000 | Loss: 0.00002374
Iteration 10/1000 | Loss: 0.00001844
Iteration 11/1000 | Loss: 0.00002091
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001968
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001772
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00002488
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00002462
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001715
Iteration 44/1000 | Loss: 0.00001715
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.714947029540781e-05, 1.714947029540781e-05, 1.714947029540781e-05, 1.714947029540781e-05, 1.714947029540781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.714947029540781e-05

Optimization complete. Final v2v error: 3.540811777114868 mm

Highest mean error: 3.915311336517334 mm for frame 193

Lowest mean error: 3.1693859100341797 mm for frame 15

Saving results

Total time: 83.72403979301453
