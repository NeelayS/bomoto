Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=227, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12712-12767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998805
Iteration 2/25 | Loss: 0.00177292
Iteration 3/25 | Loss: 0.00164460
Iteration 4/25 | Loss: 0.00162043
Iteration 5/25 | Loss: 0.00160834
Iteration 6/25 | Loss: 0.00160618
Iteration 7/25 | Loss: 0.00160610
Iteration 8/25 | Loss: 0.00160610
Iteration 9/25 | Loss: 0.00160610
Iteration 10/25 | Loss: 0.00160610
Iteration 11/25 | Loss: 0.00160610
Iteration 12/25 | Loss: 0.00160610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016060959314927459, 0.0016060959314927459, 0.0016060959314927459, 0.0016060959314927459, 0.0016060959314927459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016060959314927459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32500124
Iteration 2/25 | Loss: 0.00189196
Iteration 3/25 | Loss: 0.00189194
Iteration 4/25 | Loss: 0.00189194
Iteration 5/25 | Loss: 0.00189194
Iteration 6/25 | Loss: 0.00189194
Iteration 7/25 | Loss: 0.00189194
Iteration 8/25 | Loss: 0.00189194
Iteration 9/25 | Loss: 0.00189194
Iteration 10/25 | Loss: 0.00189194
Iteration 11/25 | Loss: 0.00189194
Iteration 12/25 | Loss: 0.00189194
Iteration 13/25 | Loss: 0.00189194
Iteration 14/25 | Loss: 0.00189194
Iteration 15/25 | Loss: 0.00189194
Iteration 16/25 | Loss: 0.00189194
Iteration 17/25 | Loss: 0.00189194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018919360591098666, 0.0018919360591098666, 0.0018919360591098666, 0.0018919360591098666, 0.0018919360591098666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018919360591098666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189194
Iteration 2/1000 | Loss: 0.00007467
Iteration 3/1000 | Loss: 0.00004534
Iteration 4/1000 | Loss: 0.00004076
Iteration 5/1000 | Loss: 0.00003753
Iteration 6/1000 | Loss: 0.00003610
Iteration 7/1000 | Loss: 0.00003505
Iteration 8/1000 | Loss: 0.00003445
Iteration 9/1000 | Loss: 0.00003389
Iteration 10/1000 | Loss: 0.00003353
Iteration 11/1000 | Loss: 0.00003325
Iteration 12/1000 | Loss: 0.00003299
Iteration 13/1000 | Loss: 0.00003281
Iteration 14/1000 | Loss: 0.00003280
Iteration 15/1000 | Loss: 0.00003275
Iteration 16/1000 | Loss: 0.00003274
Iteration 17/1000 | Loss: 0.00003272
Iteration 18/1000 | Loss: 0.00003270
Iteration 19/1000 | Loss: 0.00003269
Iteration 20/1000 | Loss: 0.00003269
Iteration 21/1000 | Loss: 0.00003269
Iteration 22/1000 | Loss: 0.00003268
Iteration 23/1000 | Loss: 0.00003268
Iteration 24/1000 | Loss: 0.00003267
Iteration 25/1000 | Loss: 0.00003267
Iteration 26/1000 | Loss: 0.00003267
Iteration 27/1000 | Loss: 0.00003267
Iteration 28/1000 | Loss: 0.00003266
Iteration 29/1000 | Loss: 0.00003265
Iteration 30/1000 | Loss: 0.00003265
Iteration 31/1000 | Loss: 0.00003265
Iteration 32/1000 | Loss: 0.00003265
Iteration 33/1000 | Loss: 0.00003265
Iteration 34/1000 | Loss: 0.00003264
Iteration 35/1000 | Loss: 0.00003264
Iteration 36/1000 | Loss: 0.00003264
Iteration 37/1000 | Loss: 0.00003264
Iteration 38/1000 | Loss: 0.00003264
Iteration 39/1000 | Loss: 0.00003264
Iteration 40/1000 | Loss: 0.00003263
Iteration 41/1000 | Loss: 0.00003260
Iteration 42/1000 | Loss: 0.00003260
Iteration 43/1000 | Loss: 0.00003258
Iteration 44/1000 | Loss: 0.00003258
Iteration 45/1000 | Loss: 0.00003258
Iteration 46/1000 | Loss: 0.00003258
Iteration 47/1000 | Loss: 0.00003257
Iteration 48/1000 | Loss: 0.00003255
Iteration 49/1000 | Loss: 0.00003254
Iteration 50/1000 | Loss: 0.00003254
Iteration 51/1000 | Loss: 0.00003253
Iteration 52/1000 | Loss: 0.00003253
Iteration 53/1000 | Loss: 0.00003251
Iteration 54/1000 | Loss: 0.00003250
Iteration 55/1000 | Loss: 0.00003250
Iteration 56/1000 | Loss: 0.00003249
Iteration 57/1000 | Loss: 0.00003248
Iteration 58/1000 | Loss: 0.00003247
Iteration 59/1000 | Loss: 0.00003247
Iteration 60/1000 | Loss: 0.00003246
Iteration 61/1000 | Loss: 0.00003246
Iteration 62/1000 | Loss: 0.00003246
Iteration 63/1000 | Loss: 0.00003246
Iteration 64/1000 | Loss: 0.00003246
Iteration 65/1000 | Loss: 0.00003245
Iteration 66/1000 | Loss: 0.00003245
Iteration 67/1000 | Loss: 0.00003245
Iteration 68/1000 | Loss: 0.00003245
Iteration 69/1000 | Loss: 0.00003245
Iteration 70/1000 | Loss: 0.00003245
Iteration 71/1000 | Loss: 0.00003245
Iteration 72/1000 | Loss: 0.00003245
Iteration 73/1000 | Loss: 0.00003244
Iteration 74/1000 | Loss: 0.00003243
Iteration 75/1000 | Loss: 0.00003242
Iteration 76/1000 | Loss: 0.00003242
Iteration 77/1000 | Loss: 0.00003241
Iteration 78/1000 | Loss: 0.00003241
Iteration 79/1000 | Loss: 0.00003241
Iteration 80/1000 | Loss: 0.00003241
Iteration 81/1000 | Loss: 0.00003239
Iteration 82/1000 | Loss: 0.00003239
Iteration 83/1000 | Loss: 0.00003239
Iteration 84/1000 | Loss: 0.00003239
Iteration 85/1000 | Loss: 0.00003239
Iteration 86/1000 | Loss: 0.00003239
Iteration 87/1000 | Loss: 0.00003239
Iteration 88/1000 | Loss: 0.00003239
Iteration 89/1000 | Loss: 0.00003238
Iteration 90/1000 | Loss: 0.00003238
Iteration 91/1000 | Loss: 0.00003238
Iteration 92/1000 | Loss: 0.00003238
Iteration 93/1000 | Loss: 0.00003238
Iteration 94/1000 | Loss: 0.00003238
Iteration 95/1000 | Loss: 0.00003238
Iteration 96/1000 | Loss: 0.00003238
Iteration 97/1000 | Loss: 0.00003238
Iteration 98/1000 | Loss: 0.00003237
Iteration 99/1000 | Loss: 0.00003236
Iteration 100/1000 | Loss: 0.00003236
Iteration 101/1000 | Loss: 0.00003235
Iteration 102/1000 | Loss: 0.00003235
Iteration 103/1000 | Loss: 0.00003235
Iteration 104/1000 | Loss: 0.00003235
Iteration 105/1000 | Loss: 0.00003234
Iteration 106/1000 | Loss: 0.00003234
Iteration 107/1000 | Loss: 0.00003234
Iteration 108/1000 | Loss: 0.00003234
Iteration 109/1000 | Loss: 0.00003234
Iteration 110/1000 | Loss: 0.00003233
Iteration 111/1000 | Loss: 0.00003233
Iteration 112/1000 | Loss: 0.00003233
Iteration 113/1000 | Loss: 0.00003232
Iteration 114/1000 | Loss: 0.00003232
Iteration 115/1000 | Loss: 0.00003232
Iteration 116/1000 | Loss: 0.00003232
Iteration 117/1000 | Loss: 0.00003232
Iteration 118/1000 | Loss: 0.00003232
Iteration 119/1000 | Loss: 0.00003232
Iteration 120/1000 | Loss: 0.00003232
Iteration 121/1000 | Loss: 0.00003232
Iteration 122/1000 | Loss: 0.00003232
Iteration 123/1000 | Loss: 0.00003232
Iteration 124/1000 | Loss: 0.00003232
Iteration 125/1000 | Loss: 0.00003232
Iteration 126/1000 | Loss: 0.00003232
Iteration 127/1000 | Loss: 0.00003232
Iteration 128/1000 | Loss: 0.00003232
Iteration 129/1000 | Loss: 0.00003232
Iteration 130/1000 | Loss: 0.00003232
Iteration 131/1000 | Loss: 0.00003232
Iteration 132/1000 | Loss: 0.00003232
Iteration 133/1000 | Loss: 0.00003232
Iteration 134/1000 | Loss: 0.00003232
Iteration 135/1000 | Loss: 0.00003232
Iteration 136/1000 | Loss: 0.00003232
Iteration 137/1000 | Loss: 0.00003232
Iteration 138/1000 | Loss: 0.00003232
Iteration 139/1000 | Loss: 0.00003232
Iteration 140/1000 | Loss: 0.00003232
Iteration 141/1000 | Loss: 0.00003232
Iteration 142/1000 | Loss: 0.00003232
Iteration 143/1000 | Loss: 0.00003232
Iteration 144/1000 | Loss: 0.00003232
Iteration 145/1000 | Loss: 0.00003232
Iteration 146/1000 | Loss: 0.00003232
Iteration 147/1000 | Loss: 0.00003232
Iteration 148/1000 | Loss: 0.00003232
Iteration 149/1000 | Loss: 0.00003232
Iteration 150/1000 | Loss: 0.00003232
Iteration 151/1000 | Loss: 0.00003232
Iteration 152/1000 | Loss: 0.00003232
Iteration 153/1000 | Loss: 0.00003232
Iteration 154/1000 | Loss: 0.00003232
Iteration 155/1000 | Loss: 0.00003232
Iteration 156/1000 | Loss: 0.00003232
Iteration 157/1000 | Loss: 0.00003232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.231538721593097e-05, 3.231538721593097e-05, 3.231538721593097e-05, 3.231538721593097e-05, 3.231538721593097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.231538721593097e-05

Optimization complete. Final v2v error: 4.76595401763916 mm

Highest mean error: 6.382853984832764 mm for frame 66

Lowest mean error: 3.82926082611084 mm for frame 1

Saving results

Total time: 40.95525336265564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962070
Iteration 2/25 | Loss: 0.00962070
Iteration 3/25 | Loss: 0.00962070
Iteration 4/25 | Loss: 0.00962070
Iteration 5/25 | Loss: 0.00962070
Iteration 6/25 | Loss: 0.00962069
Iteration 7/25 | Loss: 0.00962069
Iteration 8/25 | Loss: 0.00962069
Iteration 9/25 | Loss: 0.00962069
Iteration 10/25 | Loss: 0.00962069
Iteration 11/25 | Loss: 0.00962069
Iteration 12/25 | Loss: 0.00962069
Iteration 13/25 | Loss: 0.00962069
Iteration 14/25 | Loss: 0.00962069
Iteration 15/25 | Loss: 0.00962069
Iteration 16/25 | Loss: 0.00962069
Iteration 17/25 | Loss: 0.00962068
Iteration 18/25 | Loss: 0.00962068
Iteration 19/25 | Loss: 0.00962068
Iteration 20/25 | Loss: 0.00962068
Iteration 21/25 | Loss: 0.00962068
Iteration 22/25 | Loss: 0.00962068
Iteration 23/25 | Loss: 0.00962068
Iteration 24/25 | Loss: 0.00962068
Iteration 25/25 | Loss: 0.00962068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56435204
Iteration 2/25 | Loss: 0.10367150
Iteration 3/25 | Loss: 0.09670846
Iteration 4/25 | Loss: 0.09533374
Iteration 5/25 | Loss: 0.09533372
Iteration 6/25 | Loss: 0.09533372
Iteration 7/25 | Loss: 0.09533372
Iteration 8/25 | Loss: 0.09533372
Iteration 9/25 | Loss: 0.09533372
Iteration 10/25 | Loss: 0.09533372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.09533371776342392, 0.09533371776342392, 0.09533371776342392, 0.09533371776342392, 0.09533371776342392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09533371776342392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09533371
Iteration 2/1000 | Loss: 0.01183125
Iteration 3/1000 | Loss: 0.00853312
Iteration 4/1000 | Loss: 0.00509358
Iteration 5/1000 | Loss: 0.00511255
Iteration 6/1000 | Loss: 0.00403273
Iteration 7/1000 | Loss: 0.00342237
Iteration 8/1000 | Loss: 0.00220031
Iteration 9/1000 | Loss: 0.00160635
Iteration 10/1000 | Loss: 0.00183935
Iteration 11/1000 | Loss: 0.00101467
Iteration 12/1000 | Loss: 0.00104240
Iteration 13/1000 | Loss: 0.00069218
Iteration 14/1000 | Loss: 0.00077929
Iteration 15/1000 | Loss: 0.00055410
Iteration 16/1000 | Loss: 0.00064934
Iteration 17/1000 | Loss: 0.00049112
Iteration 18/1000 | Loss: 0.00048119
Iteration 19/1000 | Loss: 0.00087252
Iteration 20/1000 | Loss: 0.00121062
Iteration 21/1000 | Loss: 0.00059896
Iteration 22/1000 | Loss: 0.00043973
Iteration 23/1000 | Loss: 0.00040794
Iteration 24/1000 | Loss: 0.00066955
Iteration 25/1000 | Loss: 0.00050298
Iteration 26/1000 | Loss: 0.00045375
Iteration 27/1000 | Loss: 0.00039011
Iteration 28/1000 | Loss: 0.00051394
Iteration 29/1000 | Loss: 0.00092819
Iteration 30/1000 | Loss: 0.00049468
Iteration 31/1000 | Loss: 0.00048813
Iteration 32/1000 | Loss: 0.00042535
Iteration 33/1000 | Loss: 0.00115859
Iteration 34/1000 | Loss: 0.00166366
Iteration 35/1000 | Loss: 0.00099711
Iteration 36/1000 | Loss: 0.00065328
Iteration 37/1000 | Loss: 0.00164156
Iteration 38/1000 | Loss: 0.00070571
Iteration 39/1000 | Loss: 0.00058980
Iteration 40/1000 | Loss: 0.00060773
Iteration 41/1000 | Loss: 0.00045594
Iteration 42/1000 | Loss: 0.00055337
Iteration 43/1000 | Loss: 0.00041413
Iteration 44/1000 | Loss: 0.00033981
Iteration 45/1000 | Loss: 0.00037667
Iteration 46/1000 | Loss: 0.00070524
Iteration 47/1000 | Loss: 0.00066614
Iteration 48/1000 | Loss: 0.00054476
Iteration 49/1000 | Loss: 0.00068399
Iteration 50/1000 | Loss: 0.00054229
Iteration 51/1000 | Loss: 0.00043686
Iteration 52/1000 | Loss: 0.00043164
Iteration 53/1000 | Loss: 0.00059017
Iteration 54/1000 | Loss: 0.00063580
Iteration 55/1000 | Loss: 0.00055743
Iteration 56/1000 | Loss: 0.00054249
Iteration 57/1000 | Loss: 0.00051098
Iteration 58/1000 | Loss: 0.00058568
Iteration 59/1000 | Loss: 0.00059902
Iteration 60/1000 | Loss: 0.00050019
Iteration 61/1000 | Loss: 0.00054652
Iteration 62/1000 | Loss: 0.00030617
Iteration 63/1000 | Loss: 0.00054663
Iteration 64/1000 | Loss: 0.00035181
Iteration 65/1000 | Loss: 0.00037797
Iteration 66/1000 | Loss: 0.00030733
Iteration 67/1000 | Loss: 0.00044552
Iteration 68/1000 | Loss: 0.00039782
Iteration 69/1000 | Loss: 0.00035193
Iteration 70/1000 | Loss: 0.00030982
Iteration 71/1000 | Loss: 0.00030958
Iteration 72/1000 | Loss: 0.00033601
Iteration 73/1000 | Loss: 0.00033851
Iteration 74/1000 | Loss: 0.00028168
Iteration 75/1000 | Loss: 0.00026001
Iteration 76/1000 | Loss: 0.00026368
Iteration 77/1000 | Loss: 0.00056376
Iteration 78/1000 | Loss: 0.00104027
Iteration 79/1000 | Loss: 0.00041490
Iteration 80/1000 | Loss: 0.00042829
Iteration 81/1000 | Loss: 0.00079484
Iteration 82/1000 | Loss: 0.00040643
Iteration 83/1000 | Loss: 0.00044500
Iteration 84/1000 | Loss: 0.00035064
Iteration 85/1000 | Loss: 0.00034197
Iteration 86/1000 | Loss: 0.00119862
Iteration 87/1000 | Loss: 0.00035785
Iteration 88/1000 | Loss: 0.00031698
Iteration 89/1000 | Loss: 0.00030608
Iteration 90/1000 | Loss: 0.00033012
Iteration 91/1000 | Loss: 0.00029460
Iteration 92/1000 | Loss: 0.00024327
Iteration 93/1000 | Loss: 0.00024543
Iteration 94/1000 | Loss: 0.00026772
Iteration 95/1000 | Loss: 0.00038934
Iteration 96/1000 | Loss: 0.00026609
Iteration 97/1000 | Loss: 0.00021678
Iteration 98/1000 | Loss: 0.00031961
Iteration 99/1000 | Loss: 0.00035742
Iteration 100/1000 | Loss: 0.00027230
Iteration 101/1000 | Loss: 0.00033682
Iteration 102/1000 | Loss: 0.00041896
Iteration 103/1000 | Loss: 0.00031614
Iteration 104/1000 | Loss: 0.00030244
Iteration 105/1000 | Loss: 0.00063089
Iteration 106/1000 | Loss: 0.00063808
Iteration 107/1000 | Loss: 0.00038642
Iteration 108/1000 | Loss: 0.00048324
Iteration 109/1000 | Loss: 0.00030587
Iteration 110/1000 | Loss: 0.00026138
Iteration 111/1000 | Loss: 0.00018805
Iteration 112/1000 | Loss: 0.00017939
Iteration 113/1000 | Loss: 0.00024618
Iteration 114/1000 | Loss: 0.00046914
Iteration 115/1000 | Loss: 0.00035427
Iteration 116/1000 | Loss: 0.00027611
Iteration 117/1000 | Loss: 0.00048547
Iteration 118/1000 | Loss: 0.00026561
Iteration 119/1000 | Loss: 0.00017826
Iteration 120/1000 | Loss: 0.00031221
Iteration 121/1000 | Loss: 0.00055022
Iteration 122/1000 | Loss: 0.00032049
Iteration 123/1000 | Loss: 0.00021106
Iteration 124/1000 | Loss: 0.00019883
Iteration 125/1000 | Loss: 0.00024051
Iteration 126/1000 | Loss: 0.00029694
Iteration 127/1000 | Loss: 0.00019282
Iteration 128/1000 | Loss: 0.00017522
Iteration 129/1000 | Loss: 0.00022378
Iteration 130/1000 | Loss: 0.00055605
Iteration 131/1000 | Loss: 0.00037712
Iteration 132/1000 | Loss: 0.00039059
Iteration 133/1000 | Loss: 0.00027973
Iteration 134/1000 | Loss: 0.00024498
Iteration 135/1000 | Loss: 0.00017288
Iteration 136/1000 | Loss: 0.00020858
Iteration 137/1000 | Loss: 0.00016701
Iteration 138/1000 | Loss: 0.00016291
Iteration 139/1000 | Loss: 0.00016004
Iteration 140/1000 | Loss: 0.00015731
Iteration 141/1000 | Loss: 0.00025658
Iteration 142/1000 | Loss: 0.00017969
Iteration 143/1000 | Loss: 0.00016332
Iteration 144/1000 | Loss: 0.00015522
Iteration 145/1000 | Loss: 0.00015339
Iteration 146/1000 | Loss: 0.00052543
Iteration 147/1000 | Loss: 0.00168236
Iteration 148/1000 | Loss: 0.00130203
Iteration 149/1000 | Loss: 0.00115397
Iteration 150/1000 | Loss: 0.00085309
Iteration 151/1000 | Loss: 0.00015962
Iteration 152/1000 | Loss: 0.00033341
Iteration 153/1000 | Loss: 0.00078696
Iteration 154/1000 | Loss: 0.00055611
Iteration 155/1000 | Loss: 0.00016628
Iteration 156/1000 | Loss: 0.00104081
Iteration 157/1000 | Loss: 0.00115329
Iteration 158/1000 | Loss: 0.00042850
Iteration 159/1000 | Loss: 0.00045829
Iteration 160/1000 | Loss: 0.00038363
Iteration 161/1000 | Loss: 0.00098593
Iteration 162/1000 | Loss: 0.00069556
Iteration 163/1000 | Loss: 0.00031350
Iteration 164/1000 | Loss: 0.00029457
Iteration 165/1000 | Loss: 0.00049471
Iteration 166/1000 | Loss: 0.00032237
Iteration 167/1000 | Loss: 0.00042669
Iteration 168/1000 | Loss: 0.00035858
Iteration 169/1000 | Loss: 0.00031060
Iteration 170/1000 | Loss: 0.00025943
Iteration 171/1000 | Loss: 0.00017486
Iteration 172/1000 | Loss: 0.00015550
Iteration 173/1000 | Loss: 0.00014940
Iteration 174/1000 | Loss: 0.00020377
Iteration 175/1000 | Loss: 0.00020323
Iteration 176/1000 | Loss: 0.00014798
Iteration 177/1000 | Loss: 0.00016768
Iteration 178/1000 | Loss: 0.00018588
Iteration 179/1000 | Loss: 0.00014444
Iteration 180/1000 | Loss: 0.00015861
Iteration 181/1000 | Loss: 0.00112888
Iteration 182/1000 | Loss: 0.00145677
Iteration 183/1000 | Loss: 0.00077134
Iteration 184/1000 | Loss: 0.00080721
Iteration 185/1000 | Loss: 0.00064626
Iteration 186/1000 | Loss: 0.00078043
Iteration 187/1000 | Loss: 0.00044336
Iteration 188/1000 | Loss: 0.00015034
Iteration 189/1000 | Loss: 0.00014206
Iteration 190/1000 | Loss: 0.00013969
Iteration 191/1000 | Loss: 0.00058025
Iteration 192/1000 | Loss: 0.00044066
Iteration 193/1000 | Loss: 0.00018932
Iteration 194/1000 | Loss: 0.00026527
Iteration 195/1000 | Loss: 0.00054616
Iteration 196/1000 | Loss: 0.00032807
Iteration 197/1000 | Loss: 0.00014260
Iteration 198/1000 | Loss: 0.00042024
Iteration 199/1000 | Loss: 0.00076719
Iteration 200/1000 | Loss: 0.00047389
Iteration 201/1000 | Loss: 0.00045877
Iteration 202/1000 | Loss: 0.00062348
Iteration 203/1000 | Loss: 0.00032605
Iteration 204/1000 | Loss: 0.00017182
Iteration 205/1000 | Loss: 0.00015746
Iteration 206/1000 | Loss: 0.00014567
Iteration 207/1000 | Loss: 0.00028349
Iteration 208/1000 | Loss: 0.00027698
Iteration 209/1000 | Loss: 0.00025671
Iteration 210/1000 | Loss: 0.00034123
Iteration 211/1000 | Loss: 0.00022401
Iteration 212/1000 | Loss: 0.00021000
Iteration 213/1000 | Loss: 0.00014781
Iteration 214/1000 | Loss: 0.00032352
Iteration 215/1000 | Loss: 0.00038757
Iteration 216/1000 | Loss: 0.00027410
Iteration 217/1000 | Loss: 0.00028732
Iteration 218/1000 | Loss: 0.00025205
Iteration 219/1000 | Loss: 0.00026635
Iteration 220/1000 | Loss: 0.00036424
Iteration 221/1000 | Loss: 0.00043469
Iteration 222/1000 | Loss: 0.00063076
Iteration 223/1000 | Loss: 0.00073665
Iteration 224/1000 | Loss: 0.00057925
Iteration 225/1000 | Loss: 0.00052631
Iteration 226/1000 | Loss: 0.00016038
Iteration 227/1000 | Loss: 0.00014368
Iteration 228/1000 | Loss: 0.00013970
Iteration 229/1000 | Loss: 0.00015323
Iteration 230/1000 | Loss: 0.00013970
Iteration 231/1000 | Loss: 0.00028897
Iteration 232/1000 | Loss: 0.00089632
Iteration 233/1000 | Loss: 0.00035818
Iteration 234/1000 | Loss: 0.00053929
Iteration 235/1000 | Loss: 0.00031995
Iteration 236/1000 | Loss: 0.00053796
Iteration 237/1000 | Loss: 0.00032316
Iteration 238/1000 | Loss: 0.00051558
Iteration 239/1000 | Loss: 0.00029659
Iteration 240/1000 | Loss: 0.00063302
Iteration 241/1000 | Loss: 0.00038806
Iteration 242/1000 | Loss: 0.00062051
Iteration 243/1000 | Loss: 0.00043089
Iteration 244/1000 | Loss: 0.00028132
Iteration 245/1000 | Loss: 0.00017839
Iteration 246/1000 | Loss: 0.00035117
Iteration 247/1000 | Loss: 0.00021691
Iteration 248/1000 | Loss: 0.00014022
Iteration 249/1000 | Loss: 0.00013820
Iteration 250/1000 | Loss: 0.00024597
Iteration 251/1000 | Loss: 0.00071660
Iteration 252/1000 | Loss: 0.00077127
Iteration 253/1000 | Loss: 0.00045794
Iteration 254/1000 | Loss: 0.00076959
Iteration 255/1000 | Loss: 0.00022677
Iteration 256/1000 | Loss: 0.00026239
Iteration 257/1000 | Loss: 0.00020562
Iteration 258/1000 | Loss: 0.00014723
Iteration 259/1000 | Loss: 0.00024103
Iteration 260/1000 | Loss: 0.00017620
Iteration 261/1000 | Loss: 0.00013659
Iteration 262/1000 | Loss: 0.00013497
Iteration 263/1000 | Loss: 0.00045699
Iteration 264/1000 | Loss: 0.00020933
Iteration 265/1000 | Loss: 0.00015376
Iteration 266/1000 | Loss: 0.00026787
Iteration 267/1000 | Loss: 0.00020022
Iteration 268/1000 | Loss: 0.00013914
Iteration 269/1000 | Loss: 0.00020314
Iteration 270/1000 | Loss: 0.00023285
Iteration 271/1000 | Loss: 0.00016159
Iteration 272/1000 | Loss: 0.00014895
Iteration 273/1000 | Loss: 0.00013971
Iteration 274/1000 | Loss: 0.00013816
Iteration 275/1000 | Loss: 0.00013613
Iteration 276/1000 | Loss: 0.00024301
Iteration 277/1000 | Loss: 0.00046222
Iteration 278/1000 | Loss: 0.00025810
Iteration 279/1000 | Loss: 0.00015590
Iteration 280/1000 | Loss: 0.00015584
Iteration 281/1000 | Loss: 0.00014056
Iteration 282/1000 | Loss: 0.00013235
Iteration 283/1000 | Loss: 0.00013038
Iteration 284/1000 | Loss: 0.00012968
Iteration 285/1000 | Loss: 0.00014467
Iteration 286/1000 | Loss: 0.00013951
Iteration 287/1000 | Loss: 0.00012979
Iteration 288/1000 | Loss: 0.00024567
Iteration 289/1000 | Loss: 0.00074654
Iteration 290/1000 | Loss: 0.00056608
Iteration 291/1000 | Loss: 0.00021642
Iteration 292/1000 | Loss: 0.00013933
Iteration 293/1000 | Loss: 0.00013179
Iteration 294/1000 | Loss: 0.00012902
Iteration 295/1000 | Loss: 0.00014176
Iteration 296/1000 | Loss: 0.00024239
Iteration 297/1000 | Loss: 0.00018195
Iteration 298/1000 | Loss: 0.00013863
Iteration 299/1000 | Loss: 0.00013429
Iteration 300/1000 | Loss: 0.00032844
Iteration 301/1000 | Loss: 0.00023820
Iteration 302/1000 | Loss: 0.00036205
Iteration 303/1000 | Loss: 0.00023950
Iteration 304/1000 | Loss: 0.00034177
Iteration 305/1000 | Loss: 0.00026013
Iteration 306/1000 | Loss: 0.00033839
Iteration 307/1000 | Loss: 0.00031241
Iteration 308/1000 | Loss: 0.00074179
Iteration 309/1000 | Loss: 0.00029858
Iteration 310/1000 | Loss: 0.00014998
Iteration 311/1000 | Loss: 0.00013563
Iteration 312/1000 | Loss: 0.00014154
Iteration 313/1000 | Loss: 0.00026450
Iteration 314/1000 | Loss: 0.00019250
Iteration 315/1000 | Loss: 0.00023435
Iteration 316/1000 | Loss: 0.00018480
Iteration 317/1000 | Loss: 0.00024917
Iteration 318/1000 | Loss: 0.00023256
Iteration 319/1000 | Loss: 0.00013463
Iteration 320/1000 | Loss: 0.00012728
Iteration 321/1000 | Loss: 0.00012616
Iteration 322/1000 | Loss: 0.00013072
Iteration 323/1000 | Loss: 0.00031834
Iteration 324/1000 | Loss: 0.00022425
Iteration 325/1000 | Loss: 0.00025693
Iteration 326/1000 | Loss: 0.00018479
Iteration 327/1000 | Loss: 0.00012805
Iteration 328/1000 | Loss: 0.00013620
Iteration 329/1000 | Loss: 0.00012883
Iteration 330/1000 | Loss: 0.00012666
Iteration 331/1000 | Loss: 0.00026296
Iteration 332/1000 | Loss: 0.00027949
Iteration 333/1000 | Loss: 0.00022485
Iteration 334/1000 | Loss: 0.00018449
Iteration 335/1000 | Loss: 0.00027645
Iteration 336/1000 | Loss: 0.00018967
Iteration 337/1000 | Loss: 0.00014399
Iteration 338/1000 | Loss: 0.00014328
Iteration 339/1000 | Loss: 0.00025473
Iteration 340/1000 | Loss: 0.00026378
Iteration 341/1000 | Loss: 0.00017068
Iteration 342/1000 | Loss: 0.00018707
Iteration 343/1000 | Loss: 0.00027067
Iteration 344/1000 | Loss: 0.00026698
Iteration 345/1000 | Loss: 0.00026028
Iteration 346/1000 | Loss: 0.00026444
Iteration 347/1000 | Loss: 0.00014521
Iteration 348/1000 | Loss: 0.00013004
Iteration 349/1000 | Loss: 0.00012753
Iteration 350/1000 | Loss: 0.00012603
Iteration 351/1000 | Loss: 0.00012543
Iteration 352/1000 | Loss: 0.00012530
Iteration 353/1000 | Loss: 0.00012507
Iteration 354/1000 | Loss: 0.00012460
Iteration 355/1000 | Loss: 0.00012422
Iteration 356/1000 | Loss: 0.00040695
Iteration 357/1000 | Loss: 0.00026688
Iteration 358/1000 | Loss: 0.00025833
Iteration 359/1000 | Loss: 0.00013528
Iteration 360/1000 | Loss: 0.00022436
Iteration 361/1000 | Loss: 0.00058862
Iteration 362/1000 | Loss: 0.00031487
Iteration 363/1000 | Loss: 0.00013704
Iteration 364/1000 | Loss: 0.00012786
Iteration 365/1000 | Loss: 0.00012426
Iteration 366/1000 | Loss: 0.00013230
Iteration 367/1000 | Loss: 0.00013177
Iteration 368/1000 | Loss: 0.00013334
Iteration 369/1000 | Loss: 0.00024342
Iteration 370/1000 | Loss: 0.00028805
Iteration 371/1000 | Loss: 0.00030163
Iteration 372/1000 | Loss: 0.00019476
Iteration 373/1000 | Loss: 0.00012571
Iteration 374/1000 | Loss: 0.00025971
Iteration 375/1000 | Loss: 0.00115198
Iteration 376/1000 | Loss: 0.00070003
Iteration 377/1000 | Loss: 0.00027323
Iteration 378/1000 | Loss: 0.00014348
Iteration 379/1000 | Loss: 0.00023899
Iteration 380/1000 | Loss: 0.00021685
Iteration 381/1000 | Loss: 0.00012929
Iteration 382/1000 | Loss: 0.00027828
Iteration 383/1000 | Loss: 0.00021215
Iteration 384/1000 | Loss: 0.00013342
Iteration 385/1000 | Loss: 0.00021850
Iteration 386/1000 | Loss: 0.00028947
Iteration 387/1000 | Loss: 0.00022971
Iteration 388/1000 | Loss: 0.00012747
Iteration 389/1000 | Loss: 0.00025807
Iteration 390/1000 | Loss: 0.00023171
Iteration 391/1000 | Loss: 0.00012396
Iteration 392/1000 | Loss: 0.00013183
Iteration 393/1000 | Loss: 0.00027550
Iteration 394/1000 | Loss: 0.00015580
Iteration 395/1000 | Loss: 0.00013172
Iteration 396/1000 | Loss: 0.00012426
Iteration 397/1000 | Loss: 0.00012312
Iteration 398/1000 | Loss: 0.00012256
Iteration 399/1000 | Loss: 0.00012207
Iteration 400/1000 | Loss: 0.00014047
Iteration 401/1000 | Loss: 0.00058158
Iteration 402/1000 | Loss: 0.00038744
Iteration 403/1000 | Loss: 0.00025047
Iteration 404/1000 | Loss: 0.00020747
Iteration 405/1000 | Loss: 0.00036634
Iteration 406/1000 | Loss: 0.00018975
Iteration 407/1000 | Loss: 0.00023510
Iteration 408/1000 | Loss: 0.00026525
Iteration 409/1000 | Loss: 0.00023388
Iteration 410/1000 | Loss: 0.00018318
Iteration 411/1000 | Loss: 0.00013607
Iteration 412/1000 | Loss: 0.00021761
Iteration 413/1000 | Loss: 0.00019796
Iteration 414/1000 | Loss: 0.00013918
Iteration 415/1000 | Loss: 0.00013373
Iteration 416/1000 | Loss: 0.00014079
Iteration 417/1000 | Loss: 0.00012670
Iteration 418/1000 | Loss: 0.00012437
Iteration 419/1000 | Loss: 0.00012299
Iteration 420/1000 | Loss: 0.00012248
Iteration 421/1000 | Loss: 0.00029777
Iteration 422/1000 | Loss: 0.00037843
Iteration 423/1000 | Loss: 0.00025842
Iteration 424/1000 | Loss: 0.00025068
Iteration 425/1000 | Loss: 0.00021843
Iteration 426/1000 | Loss: 0.00017787
Iteration 427/1000 | Loss: 0.00021983
Iteration 428/1000 | Loss: 0.00016549
Iteration 429/1000 | Loss: 0.00020622
Iteration 430/1000 | Loss: 0.00015739
Iteration 431/1000 | Loss: 0.00019973
Iteration 432/1000 | Loss: 0.00016473
Iteration 433/1000 | Loss: 0.00012445
Iteration 434/1000 | Loss: 0.00012366
Iteration 435/1000 | Loss: 0.00012317
Iteration 436/1000 | Loss: 0.00022692
Iteration 437/1000 | Loss: 0.00013581
Iteration 438/1000 | Loss: 0.00018227
Iteration 439/1000 | Loss: 0.00023807
Iteration 440/1000 | Loss: 0.00034770
Iteration 441/1000 | Loss: 0.00019817
Iteration 442/1000 | Loss: 0.00023872
Iteration 443/1000 | Loss: 0.00018657
Iteration 444/1000 | Loss: 0.00023602
Iteration 445/1000 | Loss: 0.00017642
Iteration 446/1000 | Loss: 0.00024808
Iteration 447/1000 | Loss: 0.00019243
Iteration 448/1000 | Loss: 0.00022555
Iteration 449/1000 | Loss: 0.00016958
Iteration 450/1000 | Loss: 0.00023576
Iteration 451/1000 | Loss: 0.00019930
Iteration 452/1000 | Loss: 0.00021054
Iteration 453/1000 | Loss: 0.00013646
Iteration 454/1000 | Loss: 0.00012241
Iteration 455/1000 | Loss: 0.00012190
Iteration 456/1000 | Loss: 0.00012140
Iteration 457/1000 | Loss: 0.00013630
Iteration 458/1000 | Loss: 0.00012997
Iteration 459/1000 | Loss: 0.00012097
Iteration 460/1000 | Loss: 0.00012074
Iteration 461/1000 | Loss: 0.00014251
Iteration 462/1000 | Loss: 0.00013961
Iteration 463/1000 | Loss: 0.00012362
Iteration 464/1000 | Loss: 0.00012157
Iteration 465/1000 | Loss: 0.00012079
Iteration 466/1000 | Loss: 0.00012031
Iteration 467/1000 | Loss: 0.00013539
Iteration 468/1000 | Loss: 0.00012958
Iteration 469/1000 | Loss: 0.00012042
Iteration 470/1000 | Loss: 0.00011994
Iteration 471/1000 | Loss: 0.00011991
Iteration 472/1000 | Loss: 0.00013422
Iteration 473/1000 | Loss: 0.00013833
Iteration 474/1000 | Loss: 0.00013043
Iteration 475/1000 | Loss: 0.00011982
Iteration 476/1000 | Loss: 0.00013231
Iteration 477/1000 | Loss: 0.00036922
Iteration 478/1000 | Loss: 0.00022178
Iteration 479/1000 | Loss: 0.00033515
Iteration 480/1000 | Loss: 0.00021177
Iteration 481/1000 | Loss: 0.00034796
Iteration 482/1000 | Loss: 0.00021612
Iteration 483/1000 | Loss: 0.00033379
Iteration 484/1000 | Loss: 0.00022018
Iteration 485/1000 | Loss: 0.00012068
Iteration 486/1000 | Loss: 0.00011984
Iteration 487/1000 | Loss: 0.00011956
Iteration 488/1000 | Loss: 0.00011935
Iteration 489/1000 | Loss: 0.00011961
Iteration 490/1000 | Loss: 0.00011940
Iteration 491/1000 | Loss: 0.00011937
Iteration 492/1000 | Loss: 0.00011932
Iteration 493/1000 | Loss: 0.00013330
Iteration 494/1000 | Loss: 0.00012100
Iteration 495/1000 | Loss: 0.00012019
Iteration 496/1000 | Loss: 0.00011924
Iteration 497/1000 | Loss: 0.00011893
Iteration 498/1000 | Loss: 0.00011873
Iteration 499/1000 | Loss: 0.00021077
Iteration 500/1000 | Loss: 0.00017194
Iteration 501/1000 | Loss: 0.00011880
Iteration 502/1000 | Loss: 0.00011908
Iteration 503/1000 | Loss: 0.00011874
Iteration 504/1000 | Loss: 0.00013475
Iteration 505/1000 | Loss: 0.00015780
Iteration 506/1000 | Loss: 0.00013746
Iteration 507/1000 | Loss: 0.00013338
Iteration 508/1000 | Loss: 0.00015538
Iteration 509/1000 | Loss: 0.00013277
Iteration 510/1000 | Loss: 0.00013278
Iteration 511/1000 | Loss: 0.00026396
Iteration 512/1000 | Loss: 0.00018237
Iteration 513/1000 | Loss: 0.00011858
Iteration 514/1000 | Loss: 0.00013148
Iteration 515/1000 | Loss: 0.00019656
Iteration 516/1000 | Loss: 0.00014916
Iteration 517/1000 | Loss: 0.00013310
Iteration 518/1000 | Loss: 0.00018081
Iteration 519/1000 | Loss: 0.00014887
Iteration 520/1000 | Loss: 0.00012964
Iteration 521/1000 | Loss: 0.00016213
Iteration 522/1000 | Loss: 0.00014625
Iteration 523/1000 | Loss: 0.00015112
Iteration 524/1000 | Loss: 0.00013708
Iteration 525/1000 | Loss: 0.00017951
Iteration 526/1000 | Loss: 0.00013781
Iteration 527/1000 | Loss: 0.00013484
Iteration 528/1000 | Loss: 0.00013047
Iteration 529/1000 | Loss: 0.00014078
Iteration 530/1000 | Loss: 0.00012985
Iteration 531/1000 | Loss: 0.00013877
Iteration 532/1000 | Loss: 0.00012996
Iteration 533/1000 | Loss: 0.00012946
Iteration 534/1000 | Loss: 0.00012063
Iteration 535/1000 | Loss: 0.00011884
Iteration 536/1000 | Loss: 0.00011846
Iteration 537/1000 | Loss: 0.00011821
Iteration 538/1000 | Loss: 0.00011816
Iteration 539/1000 | Loss: 0.00011816
Iteration 540/1000 | Loss: 0.00011810
Iteration 541/1000 | Loss: 0.00011810
Iteration 542/1000 | Loss: 0.00011809
Iteration 543/1000 | Loss: 0.00011809
Iteration 544/1000 | Loss: 0.00011809
Iteration 545/1000 | Loss: 0.00011809
Iteration 546/1000 | Loss: 0.00011809
Iteration 547/1000 | Loss: 0.00011809
Iteration 548/1000 | Loss: 0.00013328
Iteration 549/1000 | Loss: 0.00013327
Iteration 550/1000 | Loss: 0.00013327
Iteration 551/1000 | Loss: 0.00013327
Iteration 552/1000 | Loss: 0.00013327
Iteration 553/1000 | Loss: 0.00011809
Iteration 554/1000 | Loss: 0.00011801
Iteration 555/1000 | Loss: 0.00011801
Iteration 556/1000 | Loss: 0.00011801
Iteration 557/1000 | Loss: 0.00011801
Iteration 558/1000 | Loss: 0.00011801
Iteration 559/1000 | Loss: 0.00011801
Iteration 560/1000 | Loss: 0.00011800
Iteration 561/1000 | Loss: 0.00011800
Iteration 562/1000 | Loss: 0.00011800
Iteration 563/1000 | Loss: 0.00011800
Iteration 564/1000 | Loss: 0.00011800
Iteration 565/1000 | Loss: 0.00011799
Iteration 566/1000 | Loss: 0.00011798
Iteration 567/1000 | Loss: 0.00011798
Iteration 568/1000 | Loss: 0.00011798
Iteration 569/1000 | Loss: 0.00011798
Iteration 570/1000 | Loss: 0.00011797
Iteration 571/1000 | Loss: 0.00011797
Iteration 572/1000 | Loss: 0.00011797
Iteration 573/1000 | Loss: 0.00011797
Iteration 574/1000 | Loss: 0.00011796
Iteration 575/1000 | Loss: 0.00011796
Iteration 576/1000 | Loss: 0.00011795
Iteration 577/1000 | Loss: 0.00013303
Iteration 578/1000 | Loss: 0.00013934
Iteration 579/1000 | Loss: 0.00012574
Iteration 580/1000 | Loss: 0.00012091
Iteration 581/1000 | Loss: 0.00013496
Iteration 582/1000 | Loss: 0.00012951
Iteration 583/1000 | Loss: 0.00011910
Iteration 584/1000 | Loss: 0.00011839
Iteration 585/1000 | Loss: 0.00011836
Iteration 586/1000 | Loss: 0.00011801
Iteration 587/1000 | Loss: 0.00011792
Iteration 588/1000 | Loss: 0.00011791
Iteration 589/1000 | Loss: 0.00011790
Iteration 590/1000 | Loss: 0.00011790
Iteration 591/1000 | Loss: 0.00011790
Iteration 592/1000 | Loss: 0.00011789
Iteration 593/1000 | Loss: 0.00011789
Iteration 594/1000 | Loss: 0.00011788
Iteration 595/1000 | Loss: 0.00011788
Iteration 596/1000 | Loss: 0.00011788
Iteration 597/1000 | Loss: 0.00011787
Iteration 598/1000 | Loss: 0.00011787
Iteration 599/1000 | Loss: 0.00011785
Iteration 600/1000 | Loss: 0.00011784
Iteration 601/1000 | Loss: 0.00011784
Iteration 602/1000 | Loss: 0.00011784
Iteration 603/1000 | Loss: 0.00011784
Iteration 604/1000 | Loss: 0.00011784
Iteration 605/1000 | Loss: 0.00011784
Iteration 606/1000 | Loss: 0.00011784
Iteration 607/1000 | Loss: 0.00011784
Iteration 608/1000 | Loss: 0.00011784
Iteration 609/1000 | Loss: 0.00011784
Iteration 610/1000 | Loss: 0.00011783
Iteration 611/1000 | Loss: 0.00011783
Iteration 612/1000 | Loss: 0.00011783
Iteration 613/1000 | Loss: 0.00011782
Iteration 614/1000 | Loss: 0.00011782
Iteration 615/1000 | Loss: 0.00011782
Iteration 616/1000 | Loss: 0.00011782
Iteration 617/1000 | Loss: 0.00011782
Iteration 618/1000 | Loss: 0.00011782
Iteration 619/1000 | Loss: 0.00011782
Iteration 620/1000 | Loss: 0.00011782
Iteration 621/1000 | Loss: 0.00011781
Iteration 622/1000 | Loss: 0.00011781
Iteration 623/1000 | Loss: 0.00011781
Iteration 624/1000 | Loss: 0.00011781
Iteration 625/1000 | Loss: 0.00011781
Iteration 626/1000 | Loss: 0.00011781
Iteration 627/1000 | Loss: 0.00011781
Iteration 628/1000 | Loss: 0.00011781
Iteration 629/1000 | Loss: 0.00011781
Iteration 630/1000 | Loss: 0.00011781
Iteration 631/1000 | Loss: 0.00011781
Iteration 632/1000 | Loss: 0.00011781
Iteration 633/1000 | Loss: 0.00011781
Iteration 634/1000 | Loss: 0.00011781
Iteration 635/1000 | Loss: 0.00011780
Iteration 636/1000 | Loss: 0.00011780
Iteration 637/1000 | Loss: 0.00011780
Iteration 638/1000 | Loss: 0.00011780
Iteration 639/1000 | Loss: 0.00011780
Iteration 640/1000 | Loss: 0.00011780
Iteration 641/1000 | Loss: 0.00011780
Iteration 642/1000 | Loss: 0.00011780
Iteration 643/1000 | Loss: 0.00011780
Iteration 644/1000 | Loss: 0.00011780
Iteration 645/1000 | Loss: 0.00011780
Iteration 646/1000 | Loss: 0.00011780
Iteration 647/1000 | Loss: 0.00011779
Iteration 648/1000 | Loss: 0.00011779
Iteration 649/1000 | Loss: 0.00011779
Iteration 650/1000 | Loss: 0.00011779
Iteration 651/1000 | Loss: 0.00011779
Iteration 652/1000 | Loss: 0.00011779
Iteration 653/1000 | Loss: 0.00011779
Iteration 654/1000 | Loss: 0.00011779
Iteration 655/1000 | Loss: 0.00011779
Iteration 656/1000 | Loss: 0.00011779
Iteration 657/1000 | Loss: 0.00011779
Iteration 658/1000 | Loss: 0.00011779
Iteration 659/1000 | Loss: 0.00011779
Iteration 660/1000 | Loss: 0.00011779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 660. Stopping optimization.
Last 5 losses: [0.00011779127089539543, 0.00011779127089539543, 0.00011779127089539543, 0.00011779127089539543, 0.00011779127089539543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011779127089539543

Optimization complete. Final v2v error: 8.207728385925293 mm

Highest mean error: 19.029094696044922 mm for frame 51

Lowest mean error: 6.229770183563232 mm for frame 243

Saving results

Total time: 908.9874227046967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959657
Iteration 2/25 | Loss: 0.00445201
Iteration 3/25 | Loss: 0.00357532
Iteration 4/25 | Loss: 0.00364298
Iteration 5/25 | Loss: 0.00343240
Iteration 6/25 | Loss: 0.00319320
Iteration 7/25 | Loss: 0.00309408
Iteration 8/25 | Loss: 0.00299453
Iteration 9/25 | Loss: 0.00303992
Iteration 10/25 | Loss: 0.00296049
Iteration 11/25 | Loss: 0.00271550
Iteration 12/25 | Loss: 0.00268353
Iteration 13/25 | Loss: 0.00263758
Iteration 14/25 | Loss: 0.00261114
Iteration 15/25 | Loss: 0.00253710
Iteration 16/25 | Loss: 0.00246381
Iteration 17/25 | Loss: 0.00245553
Iteration 18/25 | Loss: 0.00248551
Iteration 19/25 | Loss: 0.00245723
Iteration 20/25 | Loss: 0.00243757
Iteration 21/25 | Loss: 0.00241676
Iteration 22/25 | Loss: 0.00238224
Iteration 23/25 | Loss: 0.00237019
Iteration 24/25 | Loss: 0.00240619
Iteration 25/25 | Loss: 0.00238975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02978277
Iteration 2/25 | Loss: 0.00514978
Iteration 3/25 | Loss: 0.00514978
Iteration 4/25 | Loss: 0.00514977
Iteration 5/25 | Loss: 0.00514977
Iteration 6/25 | Loss: 0.00514977
Iteration 7/25 | Loss: 0.00514977
Iteration 8/25 | Loss: 0.00514977
Iteration 9/25 | Loss: 0.00514977
Iteration 10/25 | Loss: 0.00514977
Iteration 11/25 | Loss: 0.00514977
Iteration 12/25 | Loss: 0.00514977
Iteration 13/25 | Loss: 0.00514977
Iteration 14/25 | Loss: 0.00514977
Iteration 15/25 | Loss: 0.00514977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005149771925061941, 0.005149771925061941, 0.005149771925061941, 0.005149771925061941, 0.005149771925061941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005149771925061941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00514977
Iteration 2/1000 | Loss: 0.00054446
Iteration 3/1000 | Loss: 0.00029442
Iteration 4/1000 | Loss: 0.00029047
Iteration 5/1000 | Loss: 0.00025808
Iteration 6/1000 | Loss: 0.00022168
Iteration 7/1000 | Loss: 0.00026667
Iteration 8/1000 | Loss: 0.00026101
Iteration 9/1000 | Loss: 0.00032333
Iteration 10/1000 | Loss: 0.00015986
Iteration 11/1000 | Loss: 0.00014901
Iteration 12/1000 | Loss: 0.00014554
Iteration 13/1000 | Loss: 0.00014237
Iteration 14/1000 | Loss: 0.00013945
Iteration 15/1000 | Loss: 0.00013726
Iteration 16/1000 | Loss: 0.00013589
Iteration 17/1000 | Loss: 0.00013458
Iteration 18/1000 | Loss: 0.00028811
Iteration 19/1000 | Loss: 0.00022444
Iteration 20/1000 | Loss: 0.00029576
Iteration 21/1000 | Loss: 0.00014229
Iteration 22/1000 | Loss: 0.00013643
Iteration 23/1000 | Loss: 0.00013659
Iteration 24/1000 | Loss: 0.00013238
Iteration 25/1000 | Loss: 0.00013409
Iteration 26/1000 | Loss: 0.00013423
Iteration 27/1000 | Loss: 0.00013118
Iteration 28/1000 | Loss: 0.00012928
Iteration 29/1000 | Loss: 0.00013076
Iteration 30/1000 | Loss: 0.00013080
Iteration 31/1000 | Loss: 0.00012787
Iteration 32/1000 | Loss: 0.00012649
Iteration 33/1000 | Loss: 0.00023270
Iteration 34/1000 | Loss: 0.00053441
Iteration 35/1000 | Loss: 0.00033379
Iteration 36/1000 | Loss: 0.00048801
Iteration 37/1000 | Loss: 0.00043681
Iteration 38/1000 | Loss: 0.00039771
Iteration 39/1000 | Loss: 0.00016543
Iteration 40/1000 | Loss: 0.00013551
Iteration 41/1000 | Loss: 0.00025843
Iteration 42/1000 | Loss: 0.00025710
Iteration 43/1000 | Loss: 0.00013822
Iteration 44/1000 | Loss: 0.00013234
Iteration 45/1000 | Loss: 0.00012920
Iteration 46/1000 | Loss: 0.00012694
Iteration 47/1000 | Loss: 0.00012601
Iteration 48/1000 | Loss: 0.00012495
Iteration 49/1000 | Loss: 0.00027460
Iteration 50/1000 | Loss: 0.00012535
Iteration 51/1000 | Loss: 0.00013686
Iteration 52/1000 | Loss: 0.00012325
Iteration 53/1000 | Loss: 0.00024164
Iteration 54/1000 | Loss: 0.00023643
Iteration 55/1000 | Loss: 0.00013172
Iteration 56/1000 | Loss: 0.00023989
Iteration 57/1000 | Loss: 0.00013406
Iteration 58/1000 | Loss: 0.00012751
Iteration 59/1000 | Loss: 0.00012348
Iteration 60/1000 | Loss: 0.00012194
Iteration 61/1000 | Loss: 0.00012090
Iteration 62/1000 | Loss: 0.00011946
Iteration 63/1000 | Loss: 0.00011822
Iteration 64/1000 | Loss: 0.00011655
Iteration 65/1000 | Loss: 0.00011505
Iteration 66/1000 | Loss: 0.00011337
Iteration 67/1000 | Loss: 0.00011165
Iteration 68/1000 | Loss: 0.00011044
Iteration 69/1000 | Loss: 0.00021629
Iteration 70/1000 | Loss: 0.00043292
Iteration 71/1000 | Loss: 0.00202729
Iteration 72/1000 | Loss: 0.00157757
Iteration 73/1000 | Loss: 0.00210912
Iteration 74/1000 | Loss: 0.00127517
Iteration 75/1000 | Loss: 0.00182628
Iteration 76/1000 | Loss: 0.00195452
Iteration 77/1000 | Loss: 0.00050862
Iteration 78/1000 | Loss: 0.00082457
Iteration 79/1000 | Loss: 0.00067135
Iteration 80/1000 | Loss: 0.00074767
Iteration 81/1000 | Loss: 0.00047860
Iteration 82/1000 | Loss: 0.00042655
Iteration 83/1000 | Loss: 0.00030659
Iteration 84/1000 | Loss: 0.00021739
Iteration 85/1000 | Loss: 0.00014852
Iteration 86/1000 | Loss: 0.00023585
Iteration 87/1000 | Loss: 0.00012604
Iteration 88/1000 | Loss: 0.00020492
Iteration 89/1000 | Loss: 0.00012191
Iteration 90/1000 | Loss: 0.00031416
Iteration 91/1000 | Loss: 0.00028478
Iteration 92/1000 | Loss: 0.00033432
Iteration 93/1000 | Loss: 0.00027559
Iteration 94/1000 | Loss: 0.00074584
Iteration 95/1000 | Loss: 0.00050524
Iteration 96/1000 | Loss: 0.00029437
Iteration 97/1000 | Loss: 0.00013908
Iteration 98/1000 | Loss: 0.00012691
Iteration 99/1000 | Loss: 0.00024424
Iteration 100/1000 | Loss: 0.00024248
Iteration 101/1000 | Loss: 0.00026064
Iteration 102/1000 | Loss: 0.00038490
Iteration 103/1000 | Loss: 0.00025762
Iteration 104/1000 | Loss: 0.00041023
Iteration 105/1000 | Loss: 0.00022100
Iteration 106/1000 | Loss: 0.00050102
Iteration 107/1000 | Loss: 0.00043590
Iteration 108/1000 | Loss: 0.00052795
Iteration 109/1000 | Loss: 0.00035020
Iteration 110/1000 | Loss: 0.00048642
Iteration 111/1000 | Loss: 0.00035174
Iteration 112/1000 | Loss: 0.00029100
Iteration 113/1000 | Loss: 0.00026046
Iteration 114/1000 | Loss: 0.00022261
Iteration 115/1000 | Loss: 0.00022483
Iteration 116/1000 | Loss: 0.00020478
Iteration 117/1000 | Loss: 0.00021676
Iteration 118/1000 | Loss: 0.00030518
Iteration 119/1000 | Loss: 0.00050162
Iteration 120/1000 | Loss: 0.00025310
Iteration 121/1000 | Loss: 0.00015544
Iteration 122/1000 | Loss: 0.00024480
Iteration 123/1000 | Loss: 0.00016720
Iteration 124/1000 | Loss: 0.00013459
Iteration 125/1000 | Loss: 0.00012245
Iteration 126/1000 | Loss: 0.00057115
Iteration 127/1000 | Loss: 0.00012873
Iteration 128/1000 | Loss: 0.00030900
Iteration 129/1000 | Loss: 0.00024800
Iteration 130/1000 | Loss: 0.00018401
Iteration 131/1000 | Loss: 0.00031078
Iteration 132/1000 | Loss: 0.00012298
Iteration 133/1000 | Loss: 0.00013893
Iteration 134/1000 | Loss: 0.00011478
Iteration 135/1000 | Loss: 0.00023443
Iteration 136/1000 | Loss: 0.00011518
Iteration 137/1000 | Loss: 0.00060135
Iteration 138/1000 | Loss: 0.00061561
Iteration 139/1000 | Loss: 0.00059397
Iteration 140/1000 | Loss: 0.00010910
Iteration 141/1000 | Loss: 0.00022124
Iteration 142/1000 | Loss: 0.00018877
Iteration 143/1000 | Loss: 0.00010254
Iteration 144/1000 | Loss: 0.00010101
Iteration 145/1000 | Loss: 0.00043093
Iteration 146/1000 | Loss: 0.00062223
Iteration 147/1000 | Loss: 0.00037039
Iteration 148/1000 | Loss: 0.00012248
Iteration 149/1000 | Loss: 0.00010756
Iteration 150/1000 | Loss: 0.00009885
Iteration 151/1000 | Loss: 0.00009589
Iteration 152/1000 | Loss: 0.00009423
Iteration 153/1000 | Loss: 0.00021324
Iteration 154/1000 | Loss: 0.00010202
Iteration 155/1000 | Loss: 0.00009655
Iteration 156/1000 | Loss: 0.00009453
Iteration 157/1000 | Loss: 0.00009286
Iteration 158/1000 | Loss: 0.00009235
Iteration 159/1000 | Loss: 0.00044453
Iteration 160/1000 | Loss: 0.00041914
Iteration 161/1000 | Loss: 0.00010468
Iteration 162/1000 | Loss: 0.00009313
Iteration 163/1000 | Loss: 0.00009047
Iteration 164/1000 | Loss: 0.00008930
Iteration 165/1000 | Loss: 0.00008845
Iteration 166/1000 | Loss: 0.00008778
Iteration 167/1000 | Loss: 0.00010983
Iteration 168/1000 | Loss: 0.00010292
Iteration 169/1000 | Loss: 0.00026055
Iteration 170/1000 | Loss: 0.00015687
Iteration 171/1000 | Loss: 0.00010876
Iteration 172/1000 | Loss: 0.00010088
Iteration 173/1000 | Loss: 0.00010800
Iteration 174/1000 | Loss: 0.00010018
Iteration 175/1000 | Loss: 0.00008709
Iteration 176/1000 | Loss: 0.00008694
Iteration 177/1000 | Loss: 0.00008693
Iteration 178/1000 | Loss: 0.00008682
Iteration 179/1000 | Loss: 0.00008681
Iteration 180/1000 | Loss: 0.00008681
Iteration 181/1000 | Loss: 0.00008680
Iteration 182/1000 | Loss: 0.00008679
Iteration 183/1000 | Loss: 0.00008679
Iteration 184/1000 | Loss: 0.00008678
Iteration 185/1000 | Loss: 0.00008677
Iteration 186/1000 | Loss: 0.00008667
Iteration 187/1000 | Loss: 0.00008666
Iteration 188/1000 | Loss: 0.00008665
Iteration 189/1000 | Loss: 0.00008663
Iteration 190/1000 | Loss: 0.00008663
Iteration 191/1000 | Loss: 0.00008659
Iteration 192/1000 | Loss: 0.00008659
Iteration 193/1000 | Loss: 0.00008657
Iteration 194/1000 | Loss: 0.00008654
Iteration 195/1000 | Loss: 0.00008654
Iteration 196/1000 | Loss: 0.00008654
Iteration 197/1000 | Loss: 0.00008654
Iteration 198/1000 | Loss: 0.00008654
Iteration 199/1000 | Loss: 0.00008654
Iteration 200/1000 | Loss: 0.00008654
Iteration 201/1000 | Loss: 0.00008654
Iteration 202/1000 | Loss: 0.00008654
Iteration 203/1000 | Loss: 0.00008654
Iteration 204/1000 | Loss: 0.00008654
Iteration 205/1000 | Loss: 0.00008654
Iteration 206/1000 | Loss: 0.00008653
Iteration 207/1000 | Loss: 0.00008653
Iteration 208/1000 | Loss: 0.00008652
Iteration 209/1000 | Loss: 0.00008652
Iteration 210/1000 | Loss: 0.00008651
Iteration 211/1000 | Loss: 0.00008650
Iteration 212/1000 | Loss: 0.00008650
Iteration 213/1000 | Loss: 0.00008650
Iteration 214/1000 | Loss: 0.00008648
Iteration 215/1000 | Loss: 0.00008648
Iteration 216/1000 | Loss: 0.00008646
Iteration 217/1000 | Loss: 0.00008642
Iteration 218/1000 | Loss: 0.00008642
Iteration 219/1000 | Loss: 0.00008642
Iteration 220/1000 | Loss: 0.00008641
Iteration 221/1000 | Loss: 0.00008641
Iteration 222/1000 | Loss: 0.00008641
Iteration 223/1000 | Loss: 0.00008641
Iteration 224/1000 | Loss: 0.00008641
Iteration 225/1000 | Loss: 0.00008641
Iteration 226/1000 | Loss: 0.00008641
Iteration 227/1000 | Loss: 0.00008640
Iteration 228/1000 | Loss: 0.00008640
Iteration 229/1000 | Loss: 0.00008638
Iteration 230/1000 | Loss: 0.00008637
Iteration 231/1000 | Loss: 0.00008637
Iteration 232/1000 | Loss: 0.00008637
Iteration 233/1000 | Loss: 0.00010408
Iteration 234/1000 | Loss: 0.00015273
Iteration 235/1000 | Loss: 0.00011181
Iteration 236/1000 | Loss: 0.00010353
Iteration 237/1000 | Loss: 0.00016534
Iteration 238/1000 | Loss: 0.00011685
Iteration 239/1000 | Loss: 0.00013354
Iteration 240/1000 | Loss: 0.00012375
Iteration 241/1000 | Loss: 0.00010818
Iteration 242/1000 | Loss: 0.00011477
Iteration 243/1000 | Loss: 0.00014257
Iteration 244/1000 | Loss: 0.00011436
Iteration 245/1000 | Loss: 0.00013841
Iteration 246/1000 | Loss: 0.00009673
Iteration 247/1000 | Loss: 0.00009902
Iteration 248/1000 | Loss: 0.00009075
Iteration 249/1000 | Loss: 0.00008949
Iteration 250/1000 | Loss: 0.00008834
Iteration 251/1000 | Loss: 0.00008747
Iteration 252/1000 | Loss: 0.00008677
Iteration 253/1000 | Loss: 0.00008650
Iteration 254/1000 | Loss: 0.00008640
Iteration 255/1000 | Loss: 0.00008623
Iteration 256/1000 | Loss: 0.00008608
Iteration 257/1000 | Loss: 0.00008608
Iteration 258/1000 | Loss: 0.00008602
Iteration 259/1000 | Loss: 0.00008598
Iteration 260/1000 | Loss: 0.00008597
Iteration 261/1000 | Loss: 0.00008597
Iteration 262/1000 | Loss: 0.00008596
Iteration 263/1000 | Loss: 0.00008596
Iteration 264/1000 | Loss: 0.00008596
Iteration 265/1000 | Loss: 0.00008596
Iteration 266/1000 | Loss: 0.00008596
Iteration 267/1000 | Loss: 0.00008595
Iteration 268/1000 | Loss: 0.00008595
Iteration 269/1000 | Loss: 0.00008595
Iteration 270/1000 | Loss: 0.00008594
Iteration 271/1000 | Loss: 0.00008594
Iteration 272/1000 | Loss: 0.00008594
Iteration 273/1000 | Loss: 0.00008593
Iteration 274/1000 | Loss: 0.00008593
Iteration 275/1000 | Loss: 0.00008592
Iteration 276/1000 | Loss: 0.00008592
Iteration 277/1000 | Loss: 0.00008592
Iteration 278/1000 | Loss: 0.00008591
Iteration 279/1000 | Loss: 0.00008591
Iteration 280/1000 | Loss: 0.00008591
Iteration 281/1000 | Loss: 0.00008590
Iteration 282/1000 | Loss: 0.00008590
Iteration 283/1000 | Loss: 0.00008590
Iteration 284/1000 | Loss: 0.00008590
Iteration 285/1000 | Loss: 0.00008590
Iteration 286/1000 | Loss: 0.00008590
Iteration 287/1000 | Loss: 0.00010198
Iteration 288/1000 | Loss: 0.00008973
Iteration 289/1000 | Loss: 0.00009406
Iteration 290/1000 | Loss: 0.00008588
Iteration 291/1000 | Loss: 0.00008588
Iteration 292/1000 | Loss: 0.00008588
Iteration 293/1000 | Loss: 0.00008588
Iteration 294/1000 | Loss: 0.00008588
Iteration 295/1000 | Loss: 0.00008588
Iteration 296/1000 | Loss: 0.00010183
Iteration 297/1000 | Loss: 0.00011542
Iteration 298/1000 | Loss: 0.00011187
Iteration 299/1000 | Loss: 0.00008947
Iteration 300/1000 | Loss: 0.00010545
Iteration 301/1000 | Loss: 0.00010557
Iteration 302/1000 | Loss: 0.00011534
Iteration 303/1000 | Loss: 0.00011357
Iteration 304/1000 | Loss: 0.00011431
Iteration 305/1000 | Loss: 0.00008988
Iteration 306/1000 | Loss: 0.00008852
Iteration 307/1000 | Loss: 0.00008736
Iteration 308/1000 | Loss: 0.00008693
Iteration 309/1000 | Loss: 0.00008654
Iteration 310/1000 | Loss: 0.00008625
Iteration 311/1000 | Loss: 0.00008608
Iteration 312/1000 | Loss: 0.00008590
Iteration 313/1000 | Loss: 0.00008581
Iteration 314/1000 | Loss: 0.00008581
Iteration 315/1000 | Loss: 0.00008579
Iteration 316/1000 | Loss: 0.00008578
Iteration 317/1000 | Loss: 0.00008578
Iteration 318/1000 | Loss: 0.00008578
Iteration 319/1000 | Loss: 0.00008578
Iteration 320/1000 | Loss: 0.00008578
Iteration 321/1000 | Loss: 0.00008577
Iteration 322/1000 | Loss: 0.00008577
Iteration 323/1000 | Loss: 0.00008576
Iteration 324/1000 | Loss: 0.00008575
Iteration 325/1000 | Loss: 0.00008575
Iteration 326/1000 | Loss: 0.00008575
Iteration 327/1000 | Loss: 0.00008575
Iteration 328/1000 | Loss: 0.00008575
Iteration 329/1000 | Loss: 0.00008575
Iteration 330/1000 | Loss: 0.00008574
Iteration 331/1000 | Loss: 0.00008574
Iteration 332/1000 | Loss: 0.00008572
Iteration 333/1000 | Loss: 0.00008572
Iteration 334/1000 | Loss: 0.00008571
Iteration 335/1000 | Loss: 0.00008571
Iteration 336/1000 | Loss: 0.00008571
Iteration 337/1000 | Loss: 0.00008571
Iteration 338/1000 | Loss: 0.00008571
Iteration 339/1000 | Loss: 0.00008571
Iteration 340/1000 | Loss: 0.00008571
Iteration 341/1000 | Loss: 0.00008571
Iteration 342/1000 | Loss: 0.00008571
Iteration 343/1000 | Loss: 0.00008571
Iteration 344/1000 | Loss: 0.00008570
Iteration 345/1000 | Loss: 0.00008570
Iteration 346/1000 | Loss: 0.00008570
Iteration 347/1000 | Loss: 0.00008570
Iteration 348/1000 | Loss: 0.00008569
Iteration 349/1000 | Loss: 0.00008569
Iteration 350/1000 | Loss: 0.00008569
Iteration 351/1000 | Loss: 0.00008569
Iteration 352/1000 | Loss: 0.00008569
Iteration 353/1000 | Loss: 0.00008569
Iteration 354/1000 | Loss: 0.00008569
Iteration 355/1000 | Loss: 0.00008569
Iteration 356/1000 | Loss: 0.00008569
Iteration 357/1000 | Loss: 0.00008569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [8.568626071792096e-05, 8.568626071792096e-05, 8.568626071792096e-05, 8.568626071792096e-05, 8.568626071792096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.568626071792096e-05

Optimization complete. Final v2v error: 7.293941974639893 mm

Highest mean error: 10.621654510498047 mm for frame 154

Lowest mean error: 6.122086524963379 mm for frame 37

Saving results

Total time: 388.7384114265442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580071
Iteration 2/25 | Loss: 0.00184099
Iteration 3/25 | Loss: 0.00161592
Iteration 4/25 | Loss: 0.00159528
Iteration 5/25 | Loss: 0.00159176
Iteration 6/25 | Loss: 0.00159146
Iteration 7/25 | Loss: 0.00159146
Iteration 8/25 | Loss: 0.00159146
Iteration 9/25 | Loss: 0.00159146
Iteration 10/25 | Loss: 0.00159146
Iteration 11/25 | Loss: 0.00159146
Iteration 12/25 | Loss: 0.00159146
Iteration 13/25 | Loss: 0.00159146
Iteration 14/25 | Loss: 0.00159146
Iteration 15/25 | Loss: 0.00159146
Iteration 16/25 | Loss: 0.00159146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001591455307789147, 0.001591455307789147, 0.001591455307789147, 0.001591455307789147, 0.001591455307789147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001591455307789147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25321150
Iteration 2/25 | Loss: 0.00164219
Iteration 3/25 | Loss: 0.00164219
Iteration 4/25 | Loss: 0.00164219
Iteration 5/25 | Loss: 0.00164219
Iteration 6/25 | Loss: 0.00164219
Iteration 7/25 | Loss: 0.00164219
Iteration 8/25 | Loss: 0.00164219
Iteration 9/25 | Loss: 0.00164219
Iteration 10/25 | Loss: 0.00164219
Iteration 11/25 | Loss: 0.00164219
Iteration 12/25 | Loss: 0.00164219
Iteration 13/25 | Loss: 0.00164219
Iteration 14/25 | Loss: 0.00164219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016421923646703362, 0.0016421923646703362, 0.0016421923646703362, 0.0016421923646703362, 0.0016421923646703362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016421923646703362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164219
Iteration 2/1000 | Loss: 0.00006448
Iteration 3/1000 | Loss: 0.00004189
Iteration 4/1000 | Loss: 0.00003431
Iteration 5/1000 | Loss: 0.00003075
Iteration 6/1000 | Loss: 0.00002854
Iteration 7/1000 | Loss: 0.00002761
Iteration 8/1000 | Loss: 0.00002683
Iteration 9/1000 | Loss: 0.00002643
Iteration 10/1000 | Loss: 0.00002607
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00002567
Iteration 14/1000 | Loss: 0.00002566
Iteration 15/1000 | Loss: 0.00002566
Iteration 16/1000 | Loss: 0.00002565
Iteration 17/1000 | Loss: 0.00002561
Iteration 18/1000 | Loss: 0.00002556
Iteration 19/1000 | Loss: 0.00002556
Iteration 20/1000 | Loss: 0.00002555
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00002553
Iteration 23/1000 | Loss: 0.00002553
Iteration 24/1000 | Loss: 0.00002552
Iteration 25/1000 | Loss: 0.00002552
Iteration 26/1000 | Loss: 0.00002551
Iteration 27/1000 | Loss: 0.00002551
Iteration 28/1000 | Loss: 0.00002551
Iteration 29/1000 | Loss: 0.00002550
Iteration 30/1000 | Loss: 0.00002550
Iteration 31/1000 | Loss: 0.00002550
Iteration 32/1000 | Loss: 0.00002549
Iteration 33/1000 | Loss: 0.00002549
Iteration 34/1000 | Loss: 0.00002548
Iteration 35/1000 | Loss: 0.00002547
Iteration 36/1000 | Loss: 0.00002546
Iteration 37/1000 | Loss: 0.00002544
Iteration 38/1000 | Loss: 0.00002544
Iteration 39/1000 | Loss: 0.00002542
Iteration 40/1000 | Loss: 0.00002541
Iteration 41/1000 | Loss: 0.00002540
Iteration 42/1000 | Loss: 0.00002540
Iteration 43/1000 | Loss: 0.00002540
Iteration 44/1000 | Loss: 0.00002540
Iteration 45/1000 | Loss: 0.00002540
Iteration 46/1000 | Loss: 0.00002540
Iteration 47/1000 | Loss: 0.00002540
Iteration 48/1000 | Loss: 0.00002540
Iteration 49/1000 | Loss: 0.00002540
Iteration 50/1000 | Loss: 0.00002540
Iteration 51/1000 | Loss: 0.00002539
Iteration 52/1000 | Loss: 0.00002539
Iteration 53/1000 | Loss: 0.00002539
Iteration 54/1000 | Loss: 0.00002539
Iteration 55/1000 | Loss: 0.00002538
Iteration 56/1000 | Loss: 0.00002538
Iteration 57/1000 | Loss: 0.00002538
Iteration 58/1000 | Loss: 0.00002538
Iteration 59/1000 | Loss: 0.00002538
Iteration 60/1000 | Loss: 0.00002538
Iteration 61/1000 | Loss: 0.00002537
Iteration 62/1000 | Loss: 0.00002537
Iteration 63/1000 | Loss: 0.00002536
Iteration 64/1000 | Loss: 0.00002536
Iteration 65/1000 | Loss: 0.00002536
Iteration 66/1000 | Loss: 0.00002536
Iteration 67/1000 | Loss: 0.00002536
Iteration 68/1000 | Loss: 0.00002536
Iteration 69/1000 | Loss: 0.00002536
Iteration 70/1000 | Loss: 0.00002536
Iteration 71/1000 | Loss: 0.00002536
Iteration 72/1000 | Loss: 0.00002535
Iteration 73/1000 | Loss: 0.00002535
Iteration 74/1000 | Loss: 0.00002534
Iteration 75/1000 | Loss: 0.00002534
Iteration 76/1000 | Loss: 0.00002534
Iteration 77/1000 | Loss: 0.00002534
Iteration 78/1000 | Loss: 0.00002534
Iteration 79/1000 | Loss: 0.00002534
Iteration 80/1000 | Loss: 0.00002534
Iteration 81/1000 | Loss: 0.00002534
Iteration 82/1000 | Loss: 0.00002534
Iteration 83/1000 | Loss: 0.00002533
Iteration 84/1000 | Loss: 0.00002533
Iteration 85/1000 | Loss: 0.00002533
Iteration 86/1000 | Loss: 0.00002533
Iteration 87/1000 | Loss: 0.00002532
Iteration 88/1000 | Loss: 0.00002532
Iteration 89/1000 | Loss: 0.00002532
Iteration 90/1000 | Loss: 0.00002532
Iteration 91/1000 | Loss: 0.00002532
Iteration 92/1000 | Loss: 0.00002532
Iteration 93/1000 | Loss: 0.00002531
Iteration 94/1000 | Loss: 0.00002531
Iteration 95/1000 | Loss: 0.00002531
Iteration 96/1000 | Loss: 0.00002531
Iteration 97/1000 | Loss: 0.00002531
Iteration 98/1000 | Loss: 0.00002531
Iteration 99/1000 | Loss: 0.00002531
Iteration 100/1000 | Loss: 0.00002531
Iteration 101/1000 | Loss: 0.00002530
Iteration 102/1000 | Loss: 0.00002530
Iteration 103/1000 | Loss: 0.00002530
Iteration 104/1000 | Loss: 0.00002530
Iteration 105/1000 | Loss: 0.00002530
Iteration 106/1000 | Loss: 0.00002530
Iteration 107/1000 | Loss: 0.00002530
Iteration 108/1000 | Loss: 0.00002530
Iteration 109/1000 | Loss: 0.00002530
Iteration 110/1000 | Loss: 0.00002530
Iteration 111/1000 | Loss: 0.00002530
Iteration 112/1000 | Loss: 0.00002530
Iteration 113/1000 | Loss: 0.00002530
Iteration 114/1000 | Loss: 0.00002530
Iteration 115/1000 | Loss: 0.00002530
Iteration 116/1000 | Loss: 0.00002530
Iteration 117/1000 | Loss: 0.00002530
Iteration 118/1000 | Loss: 0.00002530
Iteration 119/1000 | Loss: 0.00002530
Iteration 120/1000 | Loss: 0.00002530
Iteration 121/1000 | Loss: 0.00002530
Iteration 122/1000 | Loss: 0.00002529
Iteration 123/1000 | Loss: 0.00002529
Iteration 124/1000 | Loss: 0.00002529
Iteration 125/1000 | Loss: 0.00002529
Iteration 126/1000 | Loss: 0.00002529
Iteration 127/1000 | Loss: 0.00002529
Iteration 128/1000 | Loss: 0.00002529
Iteration 129/1000 | Loss: 0.00002529
Iteration 130/1000 | Loss: 0.00002529
Iteration 131/1000 | Loss: 0.00002529
Iteration 132/1000 | Loss: 0.00002529
Iteration 133/1000 | Loss: 0.00002529
Iteration 134/1000 | Loss: 0.00002529
Iteration 135/1000 | Loss: 0.00002529
Iteration 136/1000 | Loss: 0.00002529
Iteration 137/1000 | Loss: 0.00002529
Iteration 138/1000 | Loss: 0.00002529
Iteration 139/1000 | Loss: 0.00002529
Iteration 140/1000 | Loss: 0.00002529
Iteration 141/1000 | Loss: 0.00002529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.5294682927778922e-05, 2.5294682927778922e-05, 2.5294682927778922e-05, 2.5294682927778922e-05, 2.5294682927778922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5294682927778922e-05

Optimization complete. Final v2v error: 4.308739185333252 mm

Highest mean error: 4.5544514656066895 mm for frame 106

Lowest mean error: 3.9369566440582275 mm for frame 13

Saving results

Total time: 37.27605080604553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467289
Iteration 2/25 | Loss: 0.00165098
Iteration 3/25 | Loss: 0.00154846
Iteration 4/25 | Loss: 0.00153533
Iteration 5/25 | Loss: 0.00153183
Iteration 6/25 | Loss: 0.00153143
Iteration 7/25 | Loss: 0.00153143
Iteration 8/25 | Loss: 0.00153143
Iteration 9/25 | Loss: 0.00153143
Iteration 10/25 | Loss: 0.00153143
Iteration 11/25 | Loss: 0.00153143
Iteration 12/25 | Loss: 0.00153143
Iteration 13/25 | Loss: 0.00153143
Iteration 14/25 | Loss: 0.00153143
Iteration 15/25 | Loss: 0.00153143
Iteration 16/25 | Loss: 0.00153143
Iteration 17/25 | Loss: 0.00153143
Iteration 18/25 | Loss: 0.00153143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015314314514398575, 0.0015314314514398575, 0.0015314314514398575, 0.0015314314514398575, 0.0015314314514398575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015314314514398575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26343834
Iteration 2/25 | Loss: 0.00186854
Iteration 3/25 | Loss: 0.00186854
Iteration 4/25 | Loss: 0.00186854
Iteration 5/25 | Loss: 0.00186854
Iteration 6/25 | Loss: 0.00186854
Iteration 7/25 | Loss: 0.00186854
Iteration 8/25 | Loss: 0.00186854
Iteration 9/25 | Loss: 0.00186854
Iteration 10/25 | Loss: 0.00186854
Iteration 11/25 | Loss: 0.00186854
Iteration 12/25 | Loss: 0.00186854
Iteration 13/25 | Loss: 0.00186854
Iteration 14/25 | Loss: 0.00186854
Iteration 15/25 | Loss: 0.00186854
Iteration 16/25 | Loss: 0.00186854
Iteration 17/25 | Loss: 0.00186854
Iteration 18/25 | Loss: 0.00186854
Iteration 19/25 | Loss: 0.00186854
Iteration 20/25 | Loss: 0.00186854
Iteration 21/25 | Loss: 0.00186854
Iteration 22/25 | Loss: 0.00186854
Iteration 23/25 | Loss: 0.00186854
Iteration 24/25 | Loss: 0.00186854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0018685375107452273, 0.0018685375107452273, 0.0018685375107452273, 0.0018685375107452273, 0.0018685375107452273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018685375107452273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186854
Iteration 2/1000 | Loss: 0.00005130
Iteration 3/1000 | Loss: 0.00003402
Iteration 4/1000 | Loss: 0.00003074
Iteration 5/1000 | Loss: 0.00002859
Iteration 6/1000 | Loss: 0.00002740
Iteration 7/1000 | Loss: 0.00002674
Iteration 8/1000 | Loss: 0.00002631
Iteration 9/1000 | Loss: 0.00002596
Iteration 10/1000 | Loss: 0.00002563
Iteration 11/1000 | Loss: 0.00002549
Iteration 12/1000 | Loss: 0.00002526
Iteration 13/1000 | Loss: 0.00002518
Iteration 14/1000 | Loss: 0.00002518
Iteration 15/1000 | Loss: 0.00002515
Iteration 16/1000 | Loss: 0.00002515
Iteration 17/1000 | Loss: 0.00002515
Iteration 18/1000 | Loss: 0.00002514
Iteration 19/1000 | Loss: 0.00002511
Iteration 20/1000 | Loss: 0.00002510
Iteration 21/1000 | Loss: 0.00002510
Iteration 22/1000 | Loss: 0.00002509
Iteration 23/1000 | Loss: 0.00002509
Iteration 24/1000 | Loss: 0.00002508
Iteration 25/1000 | Loss: 0.00002507
Iteration 26/1000 | Loss: 0.00002507
Iteration 27/1000 | Loss: 0.00002505
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002502
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002501
Iteration 33/1000 | Loss: 0.00002500
Iteration 34/1000 | Loss: 0.00002496
Iteration 35/1000 | Loss: 0.00002496
Iteration 36/1000 | Loss: 0.00002496
Iteration 37/1000 | Loss: 0.00002495
Iteration 38/1000 | Loss: 0.00002495
Iteration 39/1000 | Loss: 0.00002487
Iteration 40/1000 | Loss: 0.00002484
Iteration 41/1000 | Loss: 0.00002484
Iteration 42/1000 | Loss: 0.00002483
Iteration 43/1000 | Loss: 0.00002483
Iteration 44/1000 | Loss: 0.00002482
Iteration 45/1000 | Loss: 0.00002482
Iteration 46/1000 | Loss: 0.00002482
Iteration 47/1000 | Loss: 0.00002481
Iteration 48/1000 | Loss: 0.00002480
Iteration 49/1000 | Loss: 0.00002479
Iteration 50/1000 | Loss: 0.00002478
Iteration 51/1000 | Loss: 0.00002478
Iteration 52/1000 | Loss: 0.00002477
Iteration 53/1000 | Loss: 0.00002477
Iteration 54/1000 | Loss: 0.00002477
Iteration 55/1000 | Loss: 0.00002476
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002476
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002475
Iteration 60/1000 | Loss: 0.00002475
Iteration 61/1000 | Loss: 0.00002474
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002473
Iteration 64/1000 | Loss: 0.00002473
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002472
Iteration 68/1000 | Loss: 0.00002472
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002472
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002470
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002468
Iteration 84/1000 | Loss: 0.00002468
Iteration 85/1000 | Loss: 0.00002468
Iteration 86/1000 | Loss: 0.00002468
Iteration 87/1000 | Loss: 0.00002468
Iteration 88/1000 | Loss: 0.00002468
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002467
Iteration 92/1000 | Loss: 0.00002467
Iteration 93/1000 | Loss: 0.00002467
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002466
Iteration 97/1000 | Loss: 0.00002466
Iteration 98/1000 | Loss: 0.00002466
Iteration 99/1000 | Loss: 0.00002466
Iteration 100/1000 | Loss: 0.00002466
Iteration 101/1000 | Loss: 0.00002466
Iteration 102/1000 | Loss: 0.00002466
Iteration 103/1000 | Loss: 0.00002466
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002465
Iteration 107/1000 | Loss: 0.00002465
Iteration 108/1000 | Loss: 0.00002465
Iteration 109/1000 | Loss: 0.00002465
Iteration 110/1000 | Loss: 0.00002465
Iteration 111/1000 | Loss: 0.00002465
Iteration 112/1000 | Loss: 0.00002465
Iteration 113/1000 | Loss: 0.00002465
Iteration 114/1000 | Loss: 0.00002465
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002465
Iteration 119/1000 | Loss: 0.00002465
Iteration 120/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.4652979845996015e-05, 2.4652979845996015e-05, 2.4652979845996015e-05, 2.4652979845996015e-05, 2.4652979845996015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4652979845996015e-05

Optimization complete. Final v2v error: 4.192090034484863 mm

Highest mean error: 5.179201602935791 mm for frame 56

Lowest mean error: 3.8137893676757812 mm for frame 35

Saving results

Total time: 41.20389413833618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835883
Iteration 2/25 | Loss: 0.00181586
Iteration 3/25 | Loss: 0.00162508
Iteration 4/25 | Loss: 0.00160625
Iteration 5/25 | Loss: 0.00160306
Iteration 6/25 | Loss: 0.00160259
Iteration 7/25 | Loss: 0.00160259
Iteration 8/25 | Loss: 0.00160259
Iteration 9/25 | Loss: 0.00160259
Iteration 10/25 | Loss: 0.00160259
Iteration 11/25 | Loss: 0.00160259
Iteration 12/25 | Loss: 0.00160259
Iteration 13/25 | Loss: 0.00160259
Iteration 14/25 | Loss: 0.00160259
Iteration 15/25 | Loss: 0.00160259
Iteration 16/25 | Loss: 0.00160259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016025862423703074, 0.0016025862423703074, 0.0016025862423703074, 0.0016025862423703074, 0.0016025862423703074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016025862423703074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.95459509
Iteration 2/25 | Loss: 0.00167960
Iteration 3/25 | Loss: 0.00167953
Iteration 4/25 | Loss: 0.00167953
Iteration 5/25 | Loss: 0.00167953
Iteration 6/25 | Loss: 0.00167953
Iteration 7/25 | Loss: 0.00167953
Iteration 8/25 | Loss: 0.00167953
Iteration 9/25 | Loss: 0.00167953
Iteration 10/25 | Loss: 0.00167953
Iteration 11/25 | Loss: 0.00167953
Iteration 12/25 | Loss: 0.00167953
Iteration 13/25 | Loss: 0.00167953
Iteration 14/25 | Loss: 0.00167953
Iteration 15/25 | Loss: 0.00167953
Iteration 16/25 | Loss: 0.00167953
Iteration 17/25 | Loss: 0.00167953
Iteration 18/25 | Loss: 0.00167953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001679529552347958, 0.001679529552347958, 0.001679529552347958, 0.001679529552347958, 0.001679529552347958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001679529552347958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167953
Iteration 2/1000 | Loss: 0.00006710
Iteration 3/1000 | Loss: 0.00003973
Iteration 4/1000 | Loss: 0.00003414
Iteration 5/1000 | Loss: 0.00003058
Iteration 6/1000 | Loss: 0.00002846
Iteration 7/1000 | Loss: 0.00002758
Iteration 8/1000 | Loss: 0.00002695
Iteration 9/1000 | Loss: 0.00002635
Iteration 10/1000 | Loss: 0.00002597
Iteration 11/1000 | Loss: 0.00002560
Iteration 12/1000 | Loss: 0.00002537
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002513
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002506
Iteration 18/1000 | Loss: 0.00002505
Iteration 19/1000 | Loss: 0.00002503
Iteration 20/1000 | Loss: 0.00002500
Iteration 21/1000 | Loss: 0.00002500
Iteration 22/1000 | Loss: 0.00002499
Iteration 23/1000 | Loss: 0.00002497
Iteration 24/1000 | Loss: 0.00002497
Iteration 25/1000 | Loss: 0.00002497
Iteration 26/1000 | Loss: 0.00002497
Iteration 27/1000 | Loss: 0.00002497
Iteration 28/1000 | Loss: 0.00002496
Iteration 29/1000 | Loss: 0.00002495
Iteration 30/1000 | Loss: 0.00002495
Iteration 31/1000 | Loss: 0.00002494
Iteration 32/1000 | Loss: 0.00002494
Iteration 33/1000 | Loss: 0.00002494
Iteration 34/1000 | Loss: 0.00002494
Iteration 35/1000 | Loss: 0.00002493
Iteration 36/1000 | Loss: 0.00002493
Iteration 37/1000 | Loss: 0.00002493
Iteration 38/1000 | Loss: 0.00002493
Iteration 39/1000 | Loss: 0.00002492
Iteration 40/1000 | Loss: 0.00002492
Iteration 41/1000 | Loss: 0.00002492
Iteration 42/1000 | Loss: 0.00002492
Iteration 43/1000 | Loss: 0.00002492
Iteration 44/1000 | Loss: 0.00002491
Iteration 45/1000 | Loss: 0.00002491
Iteration 46/1000 | Loss: 0.00002491
Iteration 47/1000 | Loss: 0.00002491
Iteration 48/1000 | Loss: 0.00002490
Iteration 49/1000 | Loss: 0.00002490
Iteration 50/1000 | Loss: 0.00002490
Iteration 51/1000 | Loss: 0.00002490
Iteration 52/1000 | Loss: 0.00002490
Iteration 53/1000 | Loss: 0.00002490
Iteration 54/1000 | Loss: 0.00002490
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002490
Iteration 57/1000 | Loss: 0.00002490
Iteration 58/1000 | Loss: 0.00002490
Iteration 59/1000 | Loss: 0.00002489
Iteration 60/1000 | Loss: 0.00002489
Iteration 61/1000 | Loss: 0.00002489
Iteration 62/1000 | Loss: 0.00002488
Iteration 63/1000 | Loss: 0.00002488
Iteration 64/1000 | Loss: 0.00002488
Iteration 65/1000 | Loss: 0.00002487
Iteration 66/1000 | Loss: 0.00002487
Iteration 67/1000 | Loss: 0.00002487
Iteration 68/1000 | Loss: 0.00002487
Iteration 69/1000 | Loss: 0.00002487
Iteration 70/1000 | Loss: 0.00002487
Iteration 71/1000 | Loss: 0.00002487
Iteration 72/1000 | Loss: 0.00002487
Iteration 73/1000 | Loss: 0.00002487
Iteration 74/1000 | Loss: 0.00002487
Iteration 75/1000 | Loss: 0.00002486
Iteration 76/1000 | Loss: 0.00002486
Iteration 77/1000 | Loss: 0.00002486
Iteration 78/1000 | Loss: 0.00002486
Iteration 79/1000 | Loss: 0.00002485
Iteration 80/1000 | Loss: 0.00002485
Iteration 81/1000 | Loss: 0.00002485
Iteration 82/1000 | Loss: 0.00002485
Iteration 83/1000 | Loss: 0.00002485
Iteration 84/1000 | Loss: 0.00002485
Iteration 85/1000 | Loss: 0.00002485
Iteration 86/1000 | Loss: 0.00002484
Iteration 87/1000 | Loss: 0.00002484
Iteration 88/1000 | Loss: 0.00002484
Iteration 89/1000 | Loss: 0.00002483
Iteration 90/1000 | Loss: 0.00002483
Iteration 91/1000 | Loss: 0.00002483
Iteration 92/1000 | Loss: 0.00002483
Iteration 93/1000 | Loss: 0.00002483
Iteration 94/1000 | Loss: 0.00002483
Iteration 95/1000 | Loss: 0.00002483
Iteration 96/1000 | Loss: 0.00002482
Iteration 97/1000 | Loss: 0.00002482
Iteration 98/1000 | Loss: 0.00002482
Iteration 99/1000 | Loss: 0.00002482
Iteration 100/1000 | Loss: 0.00002482
Iteration 101/1000 | Loss: 0.00002482
Iteration 102/1000 | Loss: 0.00002482
Iteration 103/1000 | Loss: 0.00002482
Iteration 104/1000 | Loss: 0.00002481
Iteration 105/1000 | Loss: 0.00002481
Iteration 106/1000 | Loss: 0.00002481
Iteration 107/1000 | Loss: 0.00002480
Iteration 108/1000 | Loss: 0.00002480
Iteration 109/1000 | Loss: 0.00002480
Iteration 110/1000 | Loss: 0.00002480
Iteration 111/1000 | Loss: 0.00002479
Iteration 112/1000 | Loss: 0.00002479
Iteration 113/1000 | Loss: 0.00002479
Iteration 114/1000 | Loss: 0.00002479
Iteration 115/1000 | Loss: 0.00002478
Iteration 116/1000 | Loss: 0.00002478
Iteration 117/1000 | Loss: 0.00002478
Iteration 118/1000 | Loss: 0.00002478
Iteration 119/1000 | Loss: 0.00002478
Iteration 120/1000 | Loss: 0.00002478
Iteration 121/1000 | Loss: 0.00002478
Iteration 122/1000 | Loss: 0.00002478
Iteration 123/1000 | Loss: 0.00002477
Iteration 124/1000 | Loss: 0.00002477
Iteration 125/1000 | Loss: 0.00002477
Iteration 126/1000 | Loss: 0.00002477
Iteration 127/1000 | Loss: 0.00002477
Iteration 128/1000 | Loss: 0.00002477
Iteration 129/1000 | Loss: 0.00002477
Iteration 130/1000 | Loss: 0.00002476
Iteration 131/1000 | Loss: 0.00002476
Iteration 132/1000 | Loss: 0.00002476
Iteration 133/1000 | Loss: 0.00002475
Iteration 134/1000 | Loss: 0.00002475
Iteration 135/1000 | Loss: 0.00002475
Iteration 136/1000 | Loss: 0.00002475
Iteration 137/1000 | Loss: 0.00002475
Iteration 138/1000 | Loss: 0.00002475
Iteration 139/1000 | Loss: 0.00002475
Iteration 140/1000 | Loss: 0.00002475
Iteration 141/1000 | Loss: 0.00002475
Iteration 142/1000 | Loss: 0.00002475
Iteration 143/1000 | Loss: 0.00002475
Iteration 144/1000 | Loss: 0.00002474
Iteration 145/1000 | Loss: 0.00002474
Iteration 146/1000 | Loss: 0.00002474
Iteration 147/1000 | Loss: 0.00002474
Iteration 148/1000 | Loss: 0.00002473
Iteration 149/1000 | Loss: 0.00002473
Iteration 150/1000 | Loss: 0.00002473
Iteration 151/1000 | Loss: 0.00002473
Iteration 152/1000 | Loss: 0.00002473
Iteration 153/1000 | Loss: 0.00002473
Iteration 154/1000 | Loss: 0.00002473
Iteration 155/1000 | Loss: 0.00002472
Iteration 156/1000 | Loss: 0.00002472
Iteration 157/1000 | Loss: 0.00002472
Iteration 158/1000 | Loss: 0.00002472
Iteration 159/1000 | Loss: 0.00002472
Iteration 160/1000 | Loss: 0.00002472
Iteration 161/1000 | Loss: 0.00002471
Iteration 162/1000 | Loss: 0.00002471
Iteration 163/1000 | Loss: 0.00002471
Iteration 164/1000 | Loss: 0.00002471
Iteration 165/1000 | Loss: 0.00002471
Iteration 166/1000 | Loss: 0.00002471
Iteration 167/1000 | Loss: 0.00002471
Iteration 168/1000 | Loss: 0.00002470
Iteration 169/1000 | Loss: 0.00002470
Iteration 170/1000 | Loss: 0.00002470
Iteration 171/1000 | Loss: 0.00002470
Iteration 172/1000 | Loss: 0.00002470
Iteration 173/1000 | Loss: 0.00002470
Iteration 174/1000 | Loss: 0.00002470
Iteration 175/1000 | Loss: 0.00002470
Iteration 176/1000 | Loss: 0.00002470
Iteration 177/1000 | Loss: 0.00002470
Iteration 178/1000 | Loss: 0.00002470
Iteration 179/1000 | Loss: 0.00002470
Iteration 180/1000 | Loss: 0.00002469
Iteration 181/1000 | Loss: 0.00002469
Iteration 182/1000 | Loss: 0.00002469
Iteration 183/1000 | Loss: 0.00002469
Iteration 184/1000 | Loss: 0.00002469
Iteration 185/1000 | Loss: 0.00002469
Iteration 186/1000 | Loss: 0.00002469
Iteration 187/1000 | Loss: 0.00002469
Iteration 188/1000 | Loss: 0.00002469
Iteration 189/1000 | Loss: 0.00002469
Iteration 190/1000 | Loss: 0.00002469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.4692548322491348e-05, 2.4692548322491348e-05, 2.4692548322491348e-05, 2.4692548322491348e-05, 2.4692548322491348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4692548322491348e-05

Optimization complete. Final v2v error: 4.244401931762695 mm

Highest mean error: 4.794150352478027 mm for frame 112

Lowest mean error: 3.84047269821167 mm for frame 212

Saving results

Total time: 43.675440311431885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01138359
Iteration 2/25 | Loss: 0.01138359
Iteration 3/25 | Loss: 0.01138359
Iteration 4/25 | Loss: 0.01138359
Iteration 5/25 | Loss: 0.01138359
Iteration 6/25 | Loss: 0.01138359
Iteration 7/25 | Loss: 0.01138358
Iteration 8/25 | Loss: 0.01138358
Iteration 9/25 | Loss: 0.01138358
Iteration 10/25 | Loss: 0.01138358
Iteration 11/25 | Loss: 0.01138358
Iteration 12/25 | Loss: 0.01138358
Iteration 13/25 | Loss: 0.01138358
Iteration 14/25 | Loss: 0.01138358
Iteration 15/25 | Loss: 0.01138358
Iteration 16/25 | Loss: 0.01138358
Iteration 17/25 | Loss: 0.01138357
Iteration 18/25 | Loss: 0.01138357
Iteration 19/25 | Loss: 0.01138357
Iteration 20/25 | Loss: 0.01138357
Iteration 21/25 | Loss: 0.01138357
Iteration 22/25 | Loss: 0.01138357
Iteration 23/25 | Loss: 0.01138357
Iteration 24/25 | Loss: 0.01138357
Iteration 25/25 | Loss: 0.01138357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80390096
Iteration 2/25 | Loss: 0.06302316
Iteration 3/25 | Loss: 0.06299598
Iteration 4/25 | Loss: 0.06299597
Iteration 5/25 | Loss: 0.06299596
Iteration 6/25 | Loss: 0.06299596
Iteration 7/25 | Loss: 0.06299596
Iteration 8/25 | Loss: 0.06299596
Iteration 9/25 | Loss: 0.06299596
Iteration 10/25 | Loss: 0.06299596
Iteration 11/25 | Loss: 0.06299596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06299595534801483, 0.06299595534801483, 0.06299595534801483, 0.06299595534801483, 0.06299595534801483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06299595534801483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06299596
Iteration 2/1000 | Loss: 0.00393953
Iteration 3/1000 | Loss: 0.00099833
Iteration 4/1000 | Loss: 0.00041392
Iteration 5/1000 | Loss: 0.00022573
Iteration 6/1000 | Loss: 0.00015648
Iteration 7/1000 | Loss: 0.00011502
Iteration 8/1000 | Loss: 0.00009230
Iteration 9/1000 | Loss: 0.00007855
Iteration 10/1000 | Loss: 0.00006879
Iteration 11/1000 | Loss: 0.00043814
Iteration 12/1000 | Loss: 0.00017841
Iteration 13/1000 | Loss: 0.00016268
Iteration 14/1000 | Loss: 0.00015026
Iteration 15/1000 | Loss: 0.00010612
Iteration 16/1000 | Loss: 0.00005207
Iteration 17/1000 | Loss: 0.00004763
Iteration 18/1000 | Loss: 0.00004328
Iteration 19/1000 | Loss: 0.00004058
Iteration 20/1000 | Loss: 0.00004020
Iteration 21/1000 | Loss: 0.00003802
Iteration 22/1000 | Loss: 0.00003656
Iteration 23/1000 | Loss: 0.00003509
Iteration 24/1000 | Loss: 0.00003437
Iteration 25/1000 | Loss: 0.00003377
Iteration 26/1000 | Loss: 0.00018690
Iteration 27/1000 | Loss: 0.00013795
Iteration 28/1000 | Loss: 0.00017998
Iteration 29/1000 | Loss: 0.00010193
Iteration 30/1000 | Loss: 0.00003942
Iteration 31/1000 | Loss: 0.00003623
Iteration 32/1000 | Loss: 0.00003372
Iteration 33/1000 | Loss: 0.00003337
Iteration 34/1000 | Loss: 0.00003261
Iteration 35/1000 | Loss: 0.00003204
Iteration 36/1000 | Loss: 0.00003161
Iteration 37/1000 | Loss: 0.00003135
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003114
Iteration 40/1000 | Loss: 0.00003113
Iteration 41/1000 | Loss: 0.00003112
Iteration 42/1000 | Loss: 0.00003100
Iteration 43/1000 | Loss: 0.00003091
Iteration 44/1000 | Loss: 0.00003080
Iteration 45/1000 | Loss: 0.00003078
Iteration 46/1000 | Loss: 0.00003062
Iteration 47/1000 | Loss: 0.00003051
Iteration 48/1000 | Loss: 0.00003050
Iteration 49/1000 | Loss: 0.00003050
Iteration 50/1000 | Loss: 0.00003049
Iteration 51/1000 | Loss: 0.00003048
Iteration 52/1000 | Loss: 0.00003048
Iteration 53/1000 | Loss: 0.00003047
Iteration 54/1000 | Loss: 0.00003047
Iteration 55/1000 | Loss: 0.00003047
Iteration 56/1000 | Loss: 0.00003046
Iteration 57/1000 | Loss: 0.00003045
Iteration 58/1000 | Loss: 0.00003044
Iteration 59/1000 | Loss: 0.00003044
Iteration 60/1000 | Loss: 0.00003043
Iteration 61/1000 | Loss: 0.00003043
Iteration 62/1000 | Loss: 0.00003043
Iteration 63/1000 | Loss: 0.00003042
Iteration 64/1000 | Loss: 0.00003042
Iteration 65/1000 | Loss: 0.00003042
Iteration 66/1000 | Loss: 0.00003042
Iteration 67/1000 | Loss: 0.00003042
Iteration 68/1000 | Loss: 0.00003042
Iteration 69/1000 | Loss: 0.00003041
Iteration 70/1000 | Loss: 0.00003041
Iteration 71/1000 | Loss: 0.00003041
Iteration 72/1000 | Loss: 0.00003040
Iteration 73/1000 | Loss: 0.00003040
Iteration 74/1000 | Loss: 0.00003040
Iteration 75/1000 | Loss: 0.00003039
Iteration 76/1000 | Loss: 0.00003039
Iteration 77/1000 | Loss: 0.00003038
Iteration 78/1000 | Loss: 0.00003038
Iteration 79/1000 | Loss: 0.00003038
Iteration 80/1000 | Loss: 0.00003038
Iteration 81/1000 | Loss: 0.00003037
Iteration 82/1000 | Loss: 0.00003037
Iteration 83/1000 | Loss: 0.00003037
Iteration 84/1000 | Loss: 0.00003037
Iteration 85/1000 | Loss: 0.00003037
Iteration 86/1000 | Loss: 0.00003037
Iteration 87/1000 | Loss: 0.00003037
Iteration 88/1000 | Loss: 0.00003037
Iteration 89/1000 | Loss: 0.00003036
Iteration 90/1000 | Loss: 0.00003036
Iteration 91/1000 | Loss: 0.00003036
Iteration 92/1000 | Loss: 0.00003036
Iteration 93/1000 | Loss: 0.00003036
Iteration 94/1000 | Loss: 0.00003036
Iteration 95/1000 | Loss: 0.00003036
Iteration 96/1000 | Loss: 0.00003036
Iteration 97/1000 | Loss: 0.00003035
Iteration 98/1000 | Loss: 0.00003035
Iteration 99/1000 | Loss: 0.00003034
Iteration 100/1000 | Loss: 0.00003034
Iteration 101/1000 | Loss: 0.00003034
Iteration 102/1000 | Loss: 0.00003033
Iteration 103/1000 | Loss: 0.00003033
Iteration 104/1000 | Loss: 0.00003033
Iteration 105/1000 | Loss: 0.00003033
Iteration 106/1000 | Loss: 0.00003032
Iteration 107/1000 | Loss: 0.00003032
Iteration 108/1000 | Loss: 0.00003032
Iteration 109/1000 | Loss: 0.00003031
Iteration 110/1000 | Loss: 0.00003031
Iteration 111/1000 | Loss: 0.00003031
Iteration 112/1000 | Loss: 0.00003031
Iteration 113/1000 | Loss: 0.00003030
Iteration 114/1000 | Loss: 0.00003030
Iteration 115/1000 | Loss: 0.00003030
Iteration 116/1000 | Loss: 0.00003030
Iteration 117/1000 | Loss: 0.00003029
Iteration 118/1000 | Loss: 0.00003029
Iteration 119/1000 | Loss: 0.00003029
Iteration 120/1000 | Loss: 0.00003029
Iteration 121/1000 | Loss: 0.00003029
Iteration 122/1000 | Loss: 0.00003028
Iteration 123/1000 | Loss: 0.00003028
Iteration 124/1000 | Loss: 0.00003028
Iteration 125/1000 | Loss: 0.00003027
Iteration 126/1000 | Loss: 0.00003027
Iteration 127/1000 | Loss: 0.00003026
Iteration 128/1000 | Loss: 0.00003026
Iteration 129/1000 | Loss: 0.00003026
Iteration 130/1000 | Loss: 0.00003026
Iteration 131/1000 | Loss: 0.00003026
Iteration 132/1000 | Loss: 0.00003026
Iteration 133/1000 | Loss: 0.00003026
Iteration 134/1000 | Loss: 0.00003026
Iteration 135/1000 | Loss: 0.00003026
Iteration 136/1000 | Loss: 0.00003026
Iteration 137/1000 | Loss: 0.00003026
Iteration 138/1000 | Loss: 0.00003026
Iteration 139/1000 | Loss: 0.00003026
Iteration 140/1000 | Loss: 0.00003025
Iteration 141/1000 | Loss: 0.00003025
Iteration 142/1000 | Loss: 0.00003025
Iteration 143/1000 | Loss: 0.00003025
Iteration 144/1000 | Loss: 0.00003025
Iteration 145/1000 | Loss: 0.00003024
Iteration 146/1000 | Loss: 0.00003024
Iteration 147/1000 | Loss: 0.00003024
Iteration 148/1000 | Loss: 0.00003024
Iteration 149/1000 | Loss: 0.00003024
Iteration 150/1000 | Loss: 0.00003024
Iteration 151/1000 | Loss: 0.00003024
Iteration 152/1000 | Loss: 0.00003024
Iteration 153/1000 | Loss: 0.00003023
Iteration 154/1000 | Loss: 0.00003023
Iteration 155/1000 | Loss: 0.00003023
Iteration 156/1000 | Loss: 0.00003023
Iteration 157/1000 | Loss: 0.00003023
Iteration 158/1000 | Loss: 0.00003023
Iteration 159/1000 | Loss: 0.00003023
Iteration 160/1000 | Loss: 0.00003023
Iteration 161/1000 | Loss: 0.00003023
Iteration 162/1000 | Loss: 0.00003023
Iteration 163/1000 | Loss: 0.00003023
Iteration 164/1000 | Loss: 0.00003023
Iteration 165/1000 | Loss: 0.00003023
Iteration 166/1000 | Loss: 0.00003023
Iteration 167/1000 | Loss: 0.00003023
Iteration 168/1000 | Loss: 0.00003023
Iteration 169/1000 | Loss: 0.00003023
Iteration 170/1000 | Loss: 0.00003023
Iteration 171/1000 | Loss: 0.00003023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.0226150556700304e-05, 3.0226150556700304e-05, 3.0226150556700304e-05, 3.0226150556700304e-05, 3.0226150556700304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0226150556700304e-05

Optimization complete. Final v2v error: 4.285431385040283 mm

Highest mean error: 15.844175338745117 mm for frame 15

Lowest mean error: 3.47369384765625 mm for frame 1

Saving results

Total time: 86.56344628334045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109000
Iteration 2/25 | Loss: 0.00298717
Iteration 3/25 | Loss: 0.00213112
Iteration 4/25 | Loss: 0.00217929
Iteration 5/25 | Loss: 0.00198016
Iteration 6/25 | Loss: 0.00175301
Iteration 7/25 | Loss: 0.00166443
Iteration 8/25 | Loss: 0.00163382
Iteration 9/25 | Loss: 0.00162377
Iteration 10/25 | Loss: 0.00161405
Iteration 11/25 | Loss: 0.00161297
Iteration 12/25 | Loss: 0.00161094
Iteration 13/25 | Loss: 0.00161298
Iteration 14/25 | Loss: 0.00161099
Iteration 15/25 | Loss: 0.00160856
Iteration 16/25 | Loss: 0.00161017
Iteration 17/25 | Loss: 0.00160975
Iteration 18/25 | Loss: 0.00160887
Iteration 19/25 | Loss: 0.00161046
Iteration 20/25 | Loss: 0.00160801
Iteration 21/25 | Loss: 0.00160951
Iteration 22/25 | Loss: 0.00160899
Iteration 23/25 | Loss: 0.00160922
Iteration 24/25 | Loss: 0.00160838
Iteration 25/25 | Loss: 0.00160842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23153317
Iteration 2/25 | Loss: 0.00170516
Iteration 3/25 | Loss: 0.00170516
Iteration 4/25 | Loss: 0.00170516
Iteration 5/25 | Loss: 0.00170516
Iteration 6/25 | Loss: 0.00170516
Iteration 7/25 | Loss: 0.00170516
Iteration 8/25 | Loss: 0.00170516
Iteration 9/25 | Loss: 0.00170516
Iteration 10/25 | Loss: 0.00170516
Iteration 11/25 | Loss: 0.00170516
Iteration 12/25 | Loss: 0.00170516
Iteration 13/25 | Loss: 0.00170516
Iteration 14/25 | Loss: 0.00170516
Iteration 15/25 | Loss: 0.00170516
Iteration 16/25 | Loss: 0.00170516
Iteration 17/25 | Loss: 0.00170516
Iteration 18/25 | Loss: 0.00170516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017051551258191466, 0.0017051551258191466, 0.0017051551258191466, 0.0017051551258191466, 0.0017051551258191466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017051551258191466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170516
Iteration 2/1000 | Loss: 0.00007890
Iteration 3/1000 | Loss: 0.00007050
Iteration 4/1000 | Loss: 0.00006670
Iteration 5/1000 | Loss: 0.00004914
Iteration 6/1000 | Loss: 0.00006724
Iteration 7/1000 | Loss: 0.00005641
Iteration 8/1000 | Loss: 0.00006939
Iteration 9/1000 | Loss: 0.00005735
Iteration 10/1000 | Loss: 0.00006390
Iteration 11/1000 | Loss: 0.00005777
Iteration 12/1000 | Loss: 0.00007141
Iteration 13/1000 | Loss: 0.00004752
Iteration 14/1000 | Loss: 0.00005476
Iteration 15/1000 | Loss: 0.00008469
Iteration 16/1000 | Loss: 0.00006974
Iteration 17/1000 | Loss: 0.00006793
Iteration 18/1000 | Loss: 0.00007756
Iteration 19/1000 | Loss: 0.00004222
Iteration 20/1000 | Loss: 0.00005871
Iteration 21/1000 | Loss: 0.00005376
Iteration 22/1000 | Loss: 0.00006144
Iteration 23/1000 | Loss: 0.00005719
Iteration 24/1000 | Loss: 0.00005947
Iteration 25/1000 | Loss: 0.00005870
Iteration 26/1000 | Loss: 0.00006260
Iteration 27/1000 | Loss: 0.00006253
Iteration 28/1000 | Loss: 0.00005968
Iteration 29/1000 | Loss: 0.00006013
Iteration 30/1000 | Loss: 0.00005846
Iteration 31/1000 | Loss: 0.00004524
Iteration 32/1000 | Loss: 0.00004875
Iteration 33/1000 | Loss: 0.00005217
Iteration 34/1000 | Loss: 0.00004776
Iteration 35/1000 | Loss: 0.00004688
Iteration 36/1000 | Loss: 0.00005488
Iteration 37/1000 | Loss: 0.00005116
Iteration 38/1000 | Loss: 0.00004894
Iteration 39/1000 | Loss: 0.00005631
Iteration 40/1000 | Loss: 0.00005529
Iteration 41/1000 | Loss: 0.00004476
Iteration 42/1000 | Loss: 0.00005395
Iteration 43/1000 | Loss: 0.00005299
Iteration 44/1000 | Loss: 0.00005542
Iteration 45/1000 | Loss: 0.00004755
Iteration 46/1000 | Loss: 0.00005484
Iteration 47/1000 | Loss: 0.00004878
Iteration 48/1000 | Loss: 0.00004797
Iteration 49/1000 | Loss: 0.00004804
Iteration 50/1000 | Loss: 0.00004834
Iteration 51/1000 | Loss: 0.00004706
Iteration 52/1000 | Loss: 0.00004491
Iteration 53/1000 | Loss: 0.00005487
Iteration 54/1000 | Loss: 0.00003786
Iteration 55/1000 | Loss: 0.00005621
Iteration 56/1000 | Loss: 0.00003606
Iteration 57/1000 | Loss: 0.00005041
Iteration 58/1000 | Loss: 0.00003597
Iteration 59/1000 | Loss: 0.00004734
Iteration 60/1000 | Loss: 0.00004689
Iteration 61/1000 | Loss: 0.00003955
Iteration 62/1000 | Loss: 0.00005310
Iteration 63/1000 | Loss: 0.00005374
Iteration 64/1000 | Loss: 0.00003044
Iteration 65/1000 | Loss: 0.00002925
Iteration 66/1000 | Loss: 0.00002869
Iteration 67/1000 | Loss: 0.00002828
Iteration 68/1000 | Loss: 0.00002798
Iteration 69/1000 | Loss: 0.00002785
Iteration 70/1000 | Loss: 0.00002776
Iteration 71/1000 | Loss: 0.00002776
Iteration 72/1000 | Loss: 0.00002776
Iteration 73/1000 | Loss: 0.00002776
Iteration 74/1000 | Loss: 0.00002775
Iteration 75/1000 | Loss: 0.00002775
Iteration 76/1000 | Loss: 0.00002773
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002771
Iteration 79/1000 | Loss: 0.00002771
Iteration 80/1000 | Loss: 0.00002771
Iteration 81/1000 | Loss: 0.00002771
Iteration 82/1000 | Loss: 0.00002771
Iteration 83/1000 | Loss: 0.00002771
Iteration 84/1000 | Loss: 0.00002770
Iteration 85/1000 | Loss: 0.00002770
Iteration 86/1000 | Loss: 0.00002769
Iteration 87/1000 | Loss: 0.00002768
Iteration 88/1000 | Loss: 0.00002768
Iteration 89/1000 | Loss: 0.00002768
Iteration 90/1000 | Loss: 0.00002768
Iteration 91/1000 | Loss: 0.00002768
Iteration 92/1000 | Loss: 0.00002768
Iteration 93/1000 | Loss: 0.00002768
Iteration 94/1000 | Loss: 0.00002767
Iteration 95/1000 | Loss: 0.00002767
Iteration 96/1000 | Loss: 0.00002767
Iteration 97/1000 | Loss: 0.00002767
Iteration 98/1000 | Loss: 0.00002767
Iteration 99/1000 | Loss: 0.00002767
Iteration 100/1000 | Loss: 0.00002767
Iteration 101/1000 | Loss: 0.00002766
Iteration 102/1000 | Loss: 0.00002766
Iteration 103/1000 | Loss: 0.00002766
Iteration 104/1000 | Loss: 0.00002766
Iteration 105/1000 | Loss: 0.00002766
Iteration 106/1000 | Loss: 0.00002766
Iteration 107/1000 | Loss: 0.00002766
Iteration 108/1000 | Loss: 0.00002765
Iteration 109/1000 | Loss: 0.00002765
Iteration 110/1000 | Loss: 0.00002765
Iteration 111/1000 | Loss: 0.00002765
Iteration 112/1000 | Loss: 0.00002765
Iteration 113/1000 | Loss: 0.00002765
Iteration 114/1000 | Loss: 0.00002765
Iteration 115/1000 | Loss: 0.00002765
Iteration 116/1000 | Loss: 0.00002764
Iteration 117/1000 | Loss: 0.00002764
Iteration 118/1000 | Loss: 0.00002764
Iteration 119/1000 | Loss: 0.00002764
Iteration 120/1000 | Loss: 0.00002763
Iteration 121/1000 | Loss: 0.00002763
Iteration 122/1000 | Loss: 0.00002763
Iteration 123/1000 | Loss: 0.00002763
Iteration 124/1000 | Loss: 0.00002763
Iteration 125/1000 | Loss: 0.00002762
Iteration 126/1000 | Loss: 0.00002762
Iteration 127/1000 | Loss: 0.00002762
Iteration 128/1000 | Loss: 0.00002762
Iteration 129/1000 | Loss: 0.00002762
Iteration 130/1000 | Loss: 0.00002762
Iteration 131/1000 | Loss: 0.00002762
Iteration 132/1000 | Loss: 0.00002762
Iteration 133/1000 | Loss: 0.00002762
Iteration 134/1000 | Loss: 0.00002762
Iteration 135/1000 | Loss: 0.00002762
Iteration 136/1000 | Loss: 0.00002762
Iteration 137/1000 | Loss: 0.00002762
Iteration 138/1000 | Loss: 0.00002762
Iteration 139/1000 | Loss: 0.00002762
Iteration 140/1000 | Loss: 0.00002762
Iteration 141/1000 | Loss: 0.00002762
Iteration 142/1000 | Loss: 0.00002762
Iteration 143/1000 | Loss: 0.00002762
Iteration 144/1000 | Loss: 0.00002762
Iteration 145/1000 | Loss: 0.00002762
Iteration 146/1000 | Loss: 0.00002762
Iteration 147/1000 | Loss: 0.00002762
Iteration 148/1000 | Loss: 0.00002762
Iteration 149/1000 | Loss: 0.00002762
Iteration 150/1000 | Loss: 0.00002762
Iteration 151/1000 | Loss: 0.00002762
Iteration 152/1000 | Loss: 0.00002762
Iteration 153/1000 | Loss: 0.00002762
Iteration 154/1000 | Loss: 0.00002762
Iteration 155/1000 | Loss: 0.00002762
Iteration 156/1000 | Loss: 0.00002762
Iteration 157/1000 | Loss: 0.00002762
Iteration 158/1000 | Loss: 0.00002762
Iteration 159/1000 | Loss: 0.00002762
Iteration 160/1000 | Loss: 0.00002762
Iteration 161/1000 | Loss: 0.00002762
Iteration 162/1000 | Loss: 0.00002762
Iteration 163/1000 | Loss: 0.00002762
Iteration 164/1000 | Loss: 0.00002762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.7620924811344594e-05, 2.7620924811344594e-05, 2.7620924811344594e-05, 2.7620924811344594e-05, 2.7620924811344594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7620924811344594e-05

Optimization complete. Final v2v error: 4.366045951843262 mm

Highest mean error: 5.191850662231445 mm for frame 198

Lowest mean error: 4.195400238037109 mm for frame 123

Saving results

Total time: 171.4514307975769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032365
Iteration 2/25 | Loss: 0.00196698
Iteration 3/25 | Loss: 0.00180360
Iteration 4/25 | Loss: 0.00166508
Iteration 5/25 | Loss: 0.00163344
Iteration 6/25 | Loss: 0.00162871
Iteration 7/25 | Loss: 0.00163016
Iteration 8/25 | Loss: 0.00161587
Iteration 9/25 | Loss: 0.00161227
Iteration 10/25 | Loss: 0.00160952
Iteration 11/25 | Loss: 0.00160972
Iteration 12/25 | Loss: 0.00160946
Iteration 13/25 | Loss: 0.00160730
Iteration 14/25 | Loss: 0.00160961
Iteration 15/25 | Loss: 0.00160786
Iteration 16/25 | Loss: 0.00160685
Iteration 17/25 | Loss: 0.00160569
Iteration 18/25 | Loss: 0.00160406
Iteration 19/25 | Loss: 0.00160385
Iteration 20/25 | Loss: 0.00160384
Iteration 21/25 | Loss: 0.00160384
Iteration 22/25 | Loss: 0.00160384
Iteration 23/25 | Loss: 0.00160383
Iteration 24/25 | Loss: 0.00160383
Iteration 25/25 | Loss: 0.00160383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39891231
Iteration 2/25 | Loss: 0.00172849
Iteration 3/25 | Loss: 0.00172847
Iteration 4/25 | Loss: 0.00172847
Iteration 5/25 | Loss: 0.00172847
Iteration 6/25 | Loss: 0.00172847
Iteration 7/25 | Loss: 0.00172847
Iteration 8/25 | Loss: 0.00172847
Iteration 9/25 | Loss: 0.00172847
Iteration 10/25 | Loss: 0.00172847
Iteration 11/25 | Loss: 0.00172847
Iteration 12/25 | Loss: 0.00172847
Iteration 13/25 | Loss: 0.00172847
Iteration 14/25 | Loss: 0.00172847
Iteration 15/25 | Loss: 0.00172847
Iteration 16/25 | Loss: 0.00172847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017284691566601396, 0.0017284691566601396, 0.0017284691566601396, 0.0017284691566601396, 0.0017284691566601396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017284691566601396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172847
Iteration 2/1000 | Loss: 0.00006722
Iteration 3/1000 | Loss: 0.00004263
Iteration 4/1000 | Loss: 0.00003574
Iteration 5/1000 | Loss: 0.00003238
Iteration 6/1000 | Loss: 0.00003045
Iteration 7/1000 | Loss: 0.00002979
Iteration 8/1000 | Loss: 0.00002901
Iteration 9/1000 | Loss: 0.00004349
Iteration 10/1000 | Loss: 0.00002878
Iteration 11/1000 | Loss: 0.00002796
Iteration 12/1000 | Loss: 0.00002789
Iteration 13/1000 | Loss: 0.00002762
Iteration 14/1000 | Loss: 0.00002762
Iteration 15/1000 | Loss: 0.00002756
Iteration 16/1000 | Loss: 0.00002756
Iteration 17/1000 | Loss: 0.00002752
Iteration 18/1000 | Loss: 0.00002743
Iteration 19/1000 | Loss: 0.00002742
Iteration 20/1000 | Loss: 0.00002741
Iteration 21/1000 | Loss: 0.00002741
Iteration 22/1000 | Loss: 0.00002735
Iteration 23/1000 | Loss: 0.00002733
Iteration 24/1000 | Loss: 0.00005060
Iteration 25/1000 | Loss: 0.00002955
Iteration 26/1000 | Loss: 0.00002723
Iteration 27/1000 | Loss: 0.00002723
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00002723
Iteration 30/1000 | Loss: 0.00002723
Iteration 31/1000 | Loss: 0.00002723
Iteration 32/1000 | Loss: 0.00002722
Iteration 33/1000 | Loss: 0.00002722
Iteration 34/1000 | Loss: 0.00002722
Iteration 35/1000 | Loss: 0.00002722
Iteration 36/1000 | Loss: 0.00003143
Iteration 37/1000 | Loss: 0.00007840
Iteration 38/1000 | Loss: 0.00003233
Iteration 39/1000 | Loss: 0.00002714
Iteration 40/1000 | Loss: 0.00002714
Iteration 41/1000 | Loss: 0.00002714
Iteration 42/1000 | Loss: 0.00002714
Iteration 43/1000 | Loss: 0.00002714
Iteration 44/1000 | Loss: 0.00002713
Iteration 45/1000 | Loss: 0.00002713
Iteration 46/1000 | Loss: 0.00002713
Iteration 47/1000 | Loss: 0.00002712
Iteration 48/1000 | Loss: 0.00002712
Iteration 49/1000 | Loss: 0.00002712
Iteration 50/1000 | Loss: 0.00002711
Iteration 51/1000 | Loss: 0.00002711
Iteration 52/1000 | Loss: 0.00002711
Iteration 53/1000 | Loss: 0.00002711
Iteration 54/1000 | Loss: 0.00002711
Iteration 55/1000 | Loss: 0.00002711
Iteration 56/1000 | Loss: 0.00002710
Iteration 57/1000 | Loss: 0.00002710
Iteration 58/1000 | Loss: 0.00002716
Iteration 59/1000 | Loss: 0.00002709
Iteration 60/1000 | Loss: 0.00002709
Iteration 61/1000 | Loss: 0.00002708
Iteration 62/1000 | Loss: 0.00002708
Iteration 63/1000 | Loss: 0.00002708
Iteration 64/1000 | Loss: 0.00002708
Iteration 65/1000 | Loss: 0.00002708
Iteration 66/1000 | Loss: 0.00002708
Iteration 67/1000 | Loss: 0.00002709
Iteration 68/1000 | Loss: 0.00002708
Iteration 69/1000 | Loss: 0.00002708
Iteration 70/1000 | Loss: 0.00002708
Iteration 71/1000 | Loss: 0.00002708
Iteration 72/1000 | Loss: 0.00002708
Iteration 73/1000 | Loss: 0.00002708
Iteration 74/1000 | Loss: 0.00002708
Iteration 75/1000 | Loss: 0.00002708
Iteration 76/1000 | Loss: 0.00002708
Iteration 77/1000 | Loss: 0.00002707
Iteration 78/1000 | Loss: 0.00002707
Iteration 79/1000 | Loss: 0.00002707
Iteration 80/1000 | Loss: 0.00002707
Iteration 81/1000 | Loss: 0.00002707
Iteration 82/1000 | Loss: 0.00002707
Iteration 83/1000 | Loss: 0.00002707
Iteration 84/1000 | Loss: 0.00002707
Iteration 85/1000 | Loss: 0.00002706
Iteration 86/1000 | Loss: 0.00002706
Iteration 87/1000 | Loss: 0.00002706
Iteration 88/1000 | Loss: 0.00002706
Iteration 89/1000 | Loss: 0.00002705
Iteration 90/1000 | Loss: 0.00002705
Iteration 91/1000 | Loss: 0.00002704
Iteration 92/1000 | Loss: 0.00002704
Iteration 93/1000 | Loss: 0.00002704
Iteration 94/1000 | Loss: 0.00002704
Iteration 95/1000 | Loss: 0.00002704
Iteration 96/1000 | Loss: 0.00002703
Iteration 97/1000 | Loss: 0.00002703
Iteration 98/1000 | Loss: 0.00002703
Iteration 99/1000 | Loss: 0.00002703
Iteration 100/1000 | Loss: 0.00002703
Iteration 101/1000 | Loss: 0.00002703
Iteration 102/1000 | Loss: 0.00002703
Iteration 103/1000 | Loss: 0.00002703
Iteration 104/1000 | Loss: 0.00002703
Iteration 105/1000 | Loss: 0.00002702
Iteration 106/1000 | Loss: 0.00002702
Iteration 107/1000 | Loss: 0.00002702
Iteration 108/1000 | Loss: 0.00002701
Iteration 109/1000 | Loss: 0.00002701
Iteration 110/1000 | Loss: 0.00002701
Iteration 111/1000 | Loss: 0.00002701
Iteration 112/1000 | Loss: 0.00002701
Iteration 113/1000 | Loss: 0.00002701
Iteration 114/1000 | Loss: 0.00002701
Iteration 115/1000 | Loss: 0.00002701
Iteration 116/1000 | Loss: 0.00002701
Iteration 117/1000 | Loss: 0.00002700
Iteration 118/1000 | Loss: 0.00002700
Iteration 119/1000 | Loss: 0.00002700
Iteration 120/1000 | Loss: 0.00002700
Iteration 121/1000 | Loss: 0.00002700
Iteration 122/1000 | Loss: 0.00002700
Iteration 123/1000 | Loss: 0.00002700
Iteration 124/1000 | Loss: 0.00002700
Iteration 125/1000 | Loss: 0.00002700
Iteration 126/1000 | Loss: 0.00002700
Iteration 127/1000 | Loss: 0.00002700
Iteration 128/1000 | Loss: 0.00002700
Iteration 129/1000 | Loss: 0.00002700
Iteration 130/1000 | Loss: 0.00002700
Iteration 131/1000 | Loss: 0.00002700
Iteration 132/1000 | Loss: 0.00002700
Iteration 133/1000 | Loss: 0.00002700
Iteration 134/1000 | Loss: 0.00002700
Iteration 135/1000 | Loss: 0.00002700
Iteration 136/1000 | Loss: 0.00002700
Iteration 137/1000 | Loss: 0.00002700
Iteration 138/1000 | Loss: 0.00002700
Iteration 139/1000 | Loss: 0.00002700
Iteration 140/1000 | Loss: 0.00002700
Iteration 141/1000 | Loss: 0.00002700
Iteration 142/1000 | Loss: 0.00002700
Iteration 143/1000 | Loss: 0.00002700
Iteration 144/1000 | Loss: 0.00002700
Iteration 145/1000 | Loss: 0.00002700
Iteration 146/1000 | Loss: 0.00002700
Iteration 147/1000 | Loss: 0.00002700
Iteration 148/1000 | Loss: 0.00002700
Iteration 149/1000 | Loss: 0.00002700
Iteration 150/1000 | Loss: 0.00002700
Iteration 151/1000 | Loss: 0.00002700
Iteration 152/1000 | Loss: 0.00002700
Iteration 153/1000 | Loss: 0.00002700
Iteration 154/1000 | Loss: 0.00002700
Iteration 155/1000 | Loss: 0.00002700
Iteration 156/1000 | Loss: 0.00002700
Iteration 157/1000 | Loss: 0.00002700
Iteration 158/1000 | Loss: 0.00002700
Iteration 159/1000 | Loss: 0.00002700
Iteration 160/1000 | Loss: 0.00002700
Iteration 161/1000 | Loss: 0.00002700
Iteration 162/1000 | Loss: 0.00002700
Iteration 163/1000 | Loss: 0.00002700
Iteration 164/1000 | Loss: 0.00002700
Iteration 165/1000 | Loss: 0.00002700
Iteration 166/1000 | Loss: 0.00002700
Iteration 167/1000 | Loss: 0.00002700
Iteration 168/1000 | Loss: 0.00002700
Iteration 169/1000 | Loss: 0.00002700
Iteration 170/1000 | Loss: 0.00002700
Iteration 171/1000 | Loss: 0.00002700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.699958895391319e-05, 2.699958895391319e-05, 2.699958895391319e-05, 2.699958895391319e-05, 2.699958895391319e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.699958895391319e-05

Optimization complete. Final v2v error: 4.546925067901611 mm

Highest mean error: 5.344234466552734 mm for frame 36

Lowest mean error: 3.9549269676208496 mm for frame 0

Saving results

Total time: 72.95885848999023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446541
Iteration 2/25 | Loss: 0.00165559
Iteration 3/25 | Loss: 0.00157245
Iteration 4/25 | Loss: 0.00156707
Iteration 5/25 | Loss: 0.00156504
Iteration 6/25 | Loss: 0.00156442
Iteration 7/25 | Loss: 0.00156437
Iteration 8/25 | Loss: 0.00156437
Iteration 9/25 | Loss: 0.00156437
Iteration 10/25 | Loss: 0.00156437
Iteration 11/25 | Loss: 0.00156437
Iteration 12/25 | Loss: 0.00156437
Iteration 13/25 | Loss: 0.00156437
Iteration 14/25 | Loss: 0.00156437
Iteration 15/25 | Loss: 0.00156437
Iteration 16/25 | Loss: 0.00156437
Iteration 17/25 | Loss: 0.00156437
Iteration 18/25 | Loss: 0.00156437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015643679071217775, 0.0015643679071217775, 0.0015643679071217775, 0.0015643679071217775, 0.0015643679071217775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015643679071217775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82552302
Iteration 2/25 | Loss: 0.00204317
Iteration 3/25 | Loss: 0.00204317
Iteration 4/25 | Loss: 0.00204317
Iteration 5/25 | Loss: 0.00204316
Iteration 6/25 | Loss: 0.00204316
Iteration 7/25 | Loss: 0.00204316
Iteration 8/25 | Loss: 0.00204316
Iteration 9/25 | Loss: 0.00204316
Iteration 10/25 | Loss: 0.00204316
Iteration 11/25 | Loss: 0.00204316
Iteration 12/25 | Loss: 0.00204316
Iteration 13/25 | Loss: 0.00204316
Iteration 14/25 | Loss: 0.00204316
Iteration 15/25 | Loss: 0.00204316
Iteration 16/25 | Loss: 0.00204316
Iteration 17/25 | Loss: 0.00204316
Iteration 18/25 | Loss: 0.00204316
Iteration 19/25 | Loss: 0.00204316
Iteration 20/25 | Loss: 0.00204316
Iteration 21/25 | Loss: 0.00204316
Iteration 22/25 | Loss: 0.00204316
Iteration 23/25 | Loss: 0.00204316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020431627053767443, 0.0020431627053767443, 0.0020431627053767443, 0.0020431627053767443, 0.0020431627053767443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020431627053767443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204316
Iteration 2/1000 | Loss: 0.00004968
Iteration 3/1000 | Loss: 0.00003258
Iteration 4/1000 | Loss: 0.00002944
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002660
Iteration 7/1000 | Loss: 0.00002597
Iteration 8/1000 | Loss: 0.00002558
Iteration 9/1000 | Loss: 0.00002529
Iteration 10/1000 | Loss: 0.00002516
Iteration 11/1000 | Loss: 0.00002496
Iteration 12/1000 | Loss: 0.00002478
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002473
Iteration 15/1000 | Loss: 0.00002472
Iteration 16/1000 | Loss: 0.00002469
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002459
Iteration 19/1000 | Loss: 0.00002457
Iteration 20/1000 | Loss: 0.00002457
Iteration 21/1000 | Loss: 0.00002456
Iteration 22/1000 | Loss: 0.00002456
Iteration 23/1000 | Loss: 0.00002455
Iteration 24/1000 | Loss: 0.00002454
Iteration 25/1000 | Loss: 0.00002453
Iteration 26/1000 | Loss: 0.00002452
Iteration 27/1000 | Loss: 0.00002452
Iteration 28/1000 | Loss: 0.00002451
Iteration 29/1000 | Loss: 0.00002451
Iteration 30/1000 | Loss: 0.00002450
Iteration 31/1000 | Loss: 0.00002450
Iteration 32/1000 | Loss: 0.00002449
Iteration 33/1000 | Loss: 0.00002444
Iteration 34/1000 | Loss: 0.00002444
Iteration 35/1000 | Loss: 0.00002442
Iteration 36/1000 | Loss: 0.00002441
Iteration 37/1000 | Loss: 0.00002441
Iteration 38/1000 | Loss: 0.00002440
Iteration 39/1000 | Loss: 0.00002440
Iteration 40/1000 | Loss: 0.00002440
Iteration 41/1000 | Loss: 0.00002438
Iteration 42/1000 | Loss: 0.00002437
Iteration 43/1000 | Loss: 0.00002436
Iteration 44/1000 | Loss: 0.00002436
Iteration 45/1000 | Loss: 0.00002435
Iteration 46/1000 | Loss: 0.00002435
Iteration 47/1000 | Loss: 0.00002434
Iteration 48/1000 | Loss: 0.00002434
Iteration 49/1000 | Loss: 0.00002434
Iteration 50/1000 | Loss: 0.00002433
Iteration 51/1000 | Loss: 0.00002433
Iteration 52/1000 | Loss: 0.00002432
Iteration 53/1000 | Loss: 0.00002431
Iteration 54/1000 | Loss: 0.00002430
Iteration 55/1000 | Loss: 0.00002430
Iteration 56/1000 | Loss: 0.00002430
Iteration 57/1000 | Loss: 0.00002429
Iteration 58/1000 | Loss: 0.00002429
Iteration 59/1000 | Loss: 0.00002429
Iteration 60/1000 | Loss: 0.00002429
Iteration 61/1000 | Loss: 0.00002428
Iteration 62/1000 | Loss: 0.00002428
Iteration 63/1000 | Loss: 0.00002427
Iteration 64/1000 | Loss: 0.00002427
Iteration 65/1000 | Loss: 0.00002427
Iteration 66/1000 | Loss: 0.00002427
Iteration 67/1000 | Loss: 0.00002426
Iteration 68/1000 | Loss: 0.00002426
Iteration 69/1000 | Loss: 0.00002426
Iteration 70/1000 | Loss: 0.00002426
Iteration 71/1000 | Loss: 0.00002426
Iteration 72/1000 | Loss: 0.00002426
Iteration 73/1000 | Loss: 0.00002426
Iteration 74/1000 | Loss: 0.00002426
Iteration 75/1000 | Loss: 0.00002426
Iteration 76/1000 | Loss: 0.00002426
Iteration 77/1000 | Loss: 0.00002426
Iteration 78/1000 | Loss: 0.00002426
Iteration 79/1000 | Loss: 0.00002426
Iteration 80/1000 | Loss: 0.00002426
Iteration 81/1000 | Loss: 0.00002426
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002426
Iteration 85/1000 | Loss: 0.00002426
Iteration 86/1000 | Loss: 0.00002426
Iteration 87/1000 | Loss: 0.00002426
Iteration 88/1000 | Loss: 0.00002426
Iteration 89/1000 | Loss: 0.00002426
Iteration 90/1000 | Loss: 0.00002426
Iteration 91/1000 | Loss: 0.00002426
Iteration 92/1000 | Loss: 0.00002426
Iteration 93/1000 | Loss: 0.00002426
Iteration 94/1000 | Loss: 0.00002426
Iteration 95/1000 | Loss: 0.00002426
Iteration 96/1000 | Loss: 0.00002426
Iteration 97/1000 | Loss: 0.00002426
Iteration 98/1000 | Loss: 0.00002426
Iteration 99/1000 | Loss: 0.00002426
Iteration 100/1000 | Loss: 0.00002426
Iteration 101/1000 | Loss: 0.00002426
Iteration 102/1000 | Loss: 0.00002426
Iteration 103/1000 | Loss: 0.00002426
Iteration 104/1000 | Loss: 0.00002426
Iteration 105/1000 | Loss: 0.00002426
Iteration 106/1000 | Loss: 0.00002426
Iteration 107/1000 | Loss: 0.00002426
Iteration 108/1000 | Loss: 0.00002426
Iteration 109/1000 | Loss: 0.00002426
Iteration 110/1000 | Loss: 0.00002426
Iteration 111/1000 | Loss: 0.00002426
Iteration 112/1000 | Loss: 0.00002426
Iteration 113/1000 | Loss: 0.00002426
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002426
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.426072023808956e-05, 2.426072023808956e-05, 2.426072023808956e-05, 2.426072023808956e-05, 2.426072023808956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.426072023808956e-05

Optimization complete. Final v2v error: 4.213613510131836 mm

Highest mean error: 4.978968143463135 mm for frame 60

Lowest mean error: 3.921748638153076 mm for frame 133

Saving results

Total time: 35.75353479385376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487546
Iteration 2/25 | Loss: 0.00175107
Iteration 3/25 | Loss: 0.00160947
Iteration 4/25 | Loss: 0.00159374
Iteration 5/25 | Loss: 0.00159066
Iteration 6/25 | Loss: 0.00159046
Iteration 7/25 | Loss: 0.00159046
Iteration 8/25 | Loss: 0.00159046
Iteration 9/25 | Loss: 0.00159046
Iteration 10/25 | Loss: 0.00159046
Iteration 11/25 | Loss: 0.00159046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015904627507552505, 0.0015904627507552505, 0.0015904627507552505, 0.0015904627507552505, 0.0015904627507552505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015904627507552505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23034775
Iteration 2/25 | Loss: 0.00171432
Iteration 3/25 | Loss: 0.00171432
Iteration 4/25 | Loss: 0.00171431
Iteration 5/25 | Loss: 0.00171431
Iteration 6/25 | Loss: 0.00171431
Iteration 7/25 | Loss: 0.00171431
Iteration 8/25 | Loss: 0.00171431
Iteration 9/25 | Loss: 0.00171431
Iteration 10/25 | Loss: 0.00171431
Iteration 11/25 | Loss: 0.00171431
Iteration 12/25 | Loss: 0.00171431
Iteration 13/25 | Loss: 0.00171431
Iteration 14/25 | Loss: 0.00171431
Iteration 15/25 | Loss: 0.00171431
Iteration 16/25 | Loss: 0.00171431
Iteration 17/25 | Loss: 0.00171431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017143130535259843, 0.0017143130535259843, 0.0017143130535259843, 0.0017143130535259843, 0.0017143130535259843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017143130535259843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171431
Iteration 2/1000 | Loss: 0.00005642
Iteration 3/1000 | Loss: 0.00003661
Iteration 4/1000 | Loss: 0.00003122
Iteration 5/1000 | Loss: 0.00002828
Iteration 6/1000 | Loss: 0.00002709
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00002584
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002525
Iteration 11/1000 | Loss: 0.00002514
Iteration 12/1000 | Loss: 0.00002512
Iteration 13/1000 | Loss: 0.00002510
Iteration 14/1000 | Loss: 0.00002509
Iteration 15/1000 | Loss: 0.00002502
Iteration 16/1000 | Loss: 0.00002502
Iteration 17/1000 | Loss: 0.00002501
Iteration 18/1000 | Loss: 0.00002496
Iteration 19/1000 | Loss: 0.00002495
Iteration 20/1000 | Loss: 0.00002495
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00002491
Iteration 23/1000 | Loss: 0.00002491
Iteration 24/1000 | Loss: 0.00002491
Iteration 25/1000 | Loss: 0.00002491
Iteration 26/1000 | Loss: 0.00002491
Iteration 27/1000 | Loss: 0.00002490
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002490
Iteration 30/1000 | Loss: 0.00002489
Iteration 31/1000 | Loss: 0.00002488
Iteration 32/1000 | Loss: 0.00002487
Iteration 33/1000 | Loss: 0.00002487
Iteration 34/1000 | Loss: 0.00002486
Iteration 35/1000 | Loss: 0.00002486
Iteration 36/1000 | Loss: 0.00002486
Iteration 37/1000 | Loss: 0.00002486
Iteration 38/1000 | Loss: 0.00002485
Iteration 39/1000 | Loss: 0.00002485
Iteration 40/1000 | Loss: 0.00002485
Iteration 41/1000 | Loss: 0.00002485
Iteration 42/1000 | Loss: 0.00002484
Iteration 43/1000 | Loss: 0.00002484
Iteration 44/1000 | Loss: 0.00002484
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002484
Iteration 48/1000 | Loss: 0.00002484
Iteration 49/1000 | Loss: 0.00002483
Iteration 50/1000 | Loss: 0.00002482
Iteration 51/1000 | Loss: 0.00002482
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002481
Iteration 54/1000 | Loss: 0.00002481
Iteration 55/1000 | Loss: 0.00002480
Iteration 56/1000 | Loss: 0.00002479
Iteration 57/1000 | Loss: 0.00002479
Iteration 58/1000 | Loss: 0.00002478
Iteration 59/1000 | Loss: 0.00002477
Iteration 60/1000 | Loss: 0.00002477
Iteration 61/1000 | Loss: 0.00002477
Iteration 62/1000 | Loss: 0.00002477
Iteration 63/1000 | Loss: 0.00002477
Iteration 64/1000 | Loss: 0.00002477
Iteration 65/1000 | Loss: 0.00002476
Iteration 66/1000 | Loss: 0.00002476
Iteration 67/1000 | Loss: 0.00002475
Iteration 68/1000 | Loss: 0.00002474
Iteration 69/1000 | Loss: 0.00002474
Iteration 70/1000 | Loss: 0.00002474
Iteration 71/1000 | Loss: 0.00002473
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002469
Iteration 79/1000 | Loss: 0.00002469
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002469
Iteration 87/1000 | Loss: 0.00002469
Iteration 88/1000 | Loss: 0.00002469
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002467
Iteration 92/1000 | Loss: 0.00002467
Iteration 93/1000 | Loss: 0.00002467
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002467
Iteration 97/1000 | Loss: 0.00002467
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002467
Iteration 100/1000 | Loss: 0.00002467
Iteration 101/1000 | Loss: 0.00002467
Iteration 102/1000 | Loss: 0.00002466
Iteration 103/1000 | Loss: 0.00002466
Iteration 104/1000 | Loss: 0.00002466
Iteration 105/1000 | Loss: 0.00002466
Iteration 106/1000 | Loss: 0.00002466
Iteration 107/1000 | Loss: 0.00002466
Iteration 108/1000 | Loss: 0.00002466
Iteration 109/1000 | Loss: 0.00002466
Iteration 110/1000 | Loss: 0.00002466
Iteration 111/1000 | Loss: 0.00002466
Iteration 112/1000 | Loss: 0.00002466
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002466
Iteration 115/1000 | Loss: 0.00002466
Iteration 116/1000 | Loss: 0.00002466
Iteration 117/1000 | Loss: 0.00002466
Iteration 118/1000 | Loss: 0.00002466
Iteration 119/1000 | Loss: 0.00002466
Iteration 120/1000 | Loss: 0.00002466
Iteration 121/1000 | Loss: 0.00002466
Iteration 122/1000 | Loss: 0.00002466
Iteration 123/1000 | Loss: 0.00002466
Iteration 124/1000 | Loss: 0.00002466
Iteration 125/1000 | Loss: 0.00002466
Iteration 126/1000 | Loss: 0.00002466
Iteration 127/1000 | Loss: 0.00002466
Iteration 128/1000 | Loss: 0.00002466
Iteration 129/1000 | Loss: 0.00002466
Iteration 130/1000 | Loss: 0.00002466
Iteration 131/1000 | Loss: 0.00002466
Iteration 132/1000 | Loss: 0.00002466
Iteration 133/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.4664634111104533e-05, 2.4664634111104533e-05, 2.4664634111104533e-05, 2.4664634111104533e-05, 2.4664634111104533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4664634111104533e-05

Optimization complete. Final v2v error: 4.173631191253662 mm

Highest mean error: 4.509394645690918 mm for frame 145

Lowest mean error: 3.9941418170928955 mm for frame 21

Saving results

Total time: 31.524035453796387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547374
Iteration 2/25 | Loss: 0.00181802
Iteration 3/25 | Loss: 0.00157311
Iteration 4/25 | Loss: 0.00155427
Iteration 5/25 | Loss: 0.00155015
Iteration 6/25 | Loss: 0.00154760
Iteration 7/25 | Loss: 0.00154754
Iteration 8/25 | Loss: 0.00154754
Iteration 9/25 | Loss: 0.00154754
Iteration 10/25 | Loss: 0.00154754
Iteration 11/25 | Loss: 0.00154754
Iteration 12/25 | Loss: 0.00154754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015475404215976596, 0.0015475404215976596, 0.0015475404215976596, 0.0015475404215976596, 0.0015475404215976596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015475404215976596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18371987
Iteration 2/25 | Loss: 0.00185167
Iteration 3/25 | Loss: 0.00185165
Iteration 4/25 | Loss: 0.00185165
Iteration 5/25 | Loss: 0.00185165
Iteration 6/25 | Loss: 0.00185165
Iteration 7/25 | Loss: 0.00185164
Iteration 8/25 | Loss: 0.00185164
Iteration 9/25 | Loss: 0.00185164
Iteration 10/25 | Loss: 0.00185164
Iteration 11/25 | Loss: 0.00185164
Iteration 12/25 | Loss: 0.00185164
Iteration 13/25 | Loss: 0.00185164
Iteration 14/25 | Loss: 0.00185164
Iteration 15/25 | Loss: 0.00185164
Iteration 16/25 | Loss: 0.00185164
Iteration 17/25 | Loss: 0.00185164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018516437849029899, 0.0018516437849029899, 0.0018516437849029899, 0.0018516437849029899, 0.0018516437849029899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018516437849029899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185164
Iteration 2/1000 | Loss: 0.00009370
Iteration 3/1000 | Loss: 0.00005623
Iteration 4/1000 | Loss: 0.00005045
Iteration 5/1000 | Loss: 0.00004638
Iteration 6/1000 | Loss: 0.00004372
Iteration 7/1000 | Loss: 0.00004222
Iteration 8/1000 | Loss: 0.00004126
Iteration 9/1000 | Loss: 0.00004060
Iteration 10/1000 | Loss: 0.00003999
Iteration 11/1000 | Loss: 0.00003924
Iteration 12/1000 | Loss: 0.00003867
Iteration 13/1000 | Loss: 0.00003823
Iteration 14/1000 | Loss: 0.00003792
Iteration 15/1000 | Loss: 0.00003765
Iteration 16/1000 | Loss: 0.00003758
Iteration 17/1000 | Loss: 0.00003734
Iteration 18/1000 | Loss: 0.00003708
Iteration 19/1000 | Loss: 0.00003700
Iteration 20/1000 | Loss: 0.00003692
Iteration 21/1000 | Loss: 0.00003679
Iteration 22/1000 | Loss: 0.00003679
Iteration 23/1000 | Loss: 0.00003665
Iteration 24/1000 | Loss: 0.00003662
Iteration 25/1000 | Loss: 0.00003656
Iteration 26/1000 | Loss: 0.00003656
Iteration 27/1000 | Loss: 0.00003656
Iteration 28/1000 | Loss: 0.00003656
Iteration 29/1000 | Loss: 0.00003656
Iteration 30/1000 | Loss: 0.00003656
Iteration 31/1000 | Loss: 0.00003656
Iteration 32/1000 | Loss: 0.00003655
Iteration 33/1000 | Loss: 0.00003653
Iteration 34/1000 | Loss: 0.00003652
Iteration 35/1000 | Loss: 0.00003652
Iteration 36/1000 | Loss: 0.00003652
Iteration 37/1000 | Loss: 0.00003651
Iteration 38/1000 | Loss: 0.00003651
Iteration 39/1000 | Loss: 0.00003651
Iteration 40/1000 | Loss: 0.00003651
Iteration 41/1000 | Loss: 0.00003651
Iteration 42/1000 | Loss: 0.00003651
Iteration 43/1000 | Loss: 0.00003650
Iteration 44/1000 | Loss: 0.00003649
Iteration 45/1000 | Loss: 0.00003649
Iteration 46/1000 | Loss: 0.00003649
Iteration 47/1000 | Loss: 0.00003649
Iteration 48/1000 | Loss: 0.00003649
Iteration 49/1000 | Loss: 0.00003649
Iteration 50/1000 | Loss: 0.00003649
Iteration 51/1000 | Loss: 0.00003649
Iteration 52/1000 | Loss: 0.00003649
Iteration 53/1000 | Loss: 0.00003648
Iteration 54/1000 | Loss: 0.00003648
Iteration 55/1000 | Loss: 0.00003648
Iteration 56/1000 | Loss: 0.00003648
Iteration 57/1000 | Loss: 0.00003648
Iteration 58/1000 | Loss: 0.00003647
Iteration 59/1000 | Loss: 0.00003647
Iteration 60/1000 | Loss: 0.00003647
Iteration 61/1000 | Loss: 0.00003647
Iteration 62/1000 | Loss: 0.00003647
Iteration 63/1000 | Loss: 0.00003647
Iteration 64/1000 | Loss: 0.00003647
Iteration 65/1000 | Loss: 0.00003646
Iteration 66/1000 | Loss: 0.00003646
Iteration 67/1000 | Loss: 0.00003646
Iteration 68/1000 | Loss: 0.00003646
Iteration 69/1000 | Loss: 0.00003646
Iteration 70/1000 | Loss: 0.00003646
Iteration 71/1000 | Loss: 0.00003646
Iteration 72/1000 | Loss: 0.00003646
Iteration 73/1000 | Loss: 0.00003646
Iteration 74/1000 | Loss: 0.00003645
Iteration 75/1000 | Loss: 0.00003645
Iteration 76/1000 | Loss: 0.00003645
Iteration 77/1000 | Loss: 0.00003645
Iteration 78/1000 | Loss: 0.00003645
Iteration 79/1000 | Loss: 0.00003645
Iteration 80/1000 | Loss: 0.00003645
Iteration 81/1000 | Loss: 0.00003645
Iteration 82/1000 | Loss: 0.00003645
Iteration 83/1000 | Loss: 0.00003645
Iteration 84/1000 | Loss: 0.00003645
Iteration 85/1000 | Loss: 0.00003645
Iteration 86/1000 | Loss: 0.00003645
Iteration 87/1000 | Loss: 0.00003645
Iteration 88/1000 | Loss: 0.00003645
Iteration 89/1000 | Loss: 0.00003645
Iteration 90/1000 | Loss: 0.00003645
Iteration 91/1000 | Loss: 0.00003645
Iteration 92/1000 | Loss: 0.00003645
Iteration 93/1000 | Loss: 0.00003645
Iteration 94/1000 | Loss: 0.00003645
Iteration 95/1000 | Loss: 0.00003645
Iteration 96/1000 | Loss: 0.00003645
Iteration 97/1000 | Loss: 0.00003645
Iteration 98/1000 | Loss: 0.00003645
Iteration 99/1000 | Loss: 0.00003645
Iteration 100/1000 | Loss: 0.00003645
Iteration 101/1000 | Loss: 0.00003645
Iteration 102/1000 | Loss: 0.00003645
Iteration 103/1000 | Loss: 0.00003645
Iteration 104/1000 | Loss: 0.00003645
Iteration 105/1000 | Loss: 0.00003645
Iteration 106/1000 | Loss: 0.00003645
Iteration 107/1000 | Loss: 0.00003645
Iteration 108/1000 | Loss: 0.00003645
Iteration 109/1000 | Loss: 0.00003645
Iteration 110/1000 | Loss: 0.00003645
Iteration 111/1000 | Loss: 0.00003645
Iteration 112/1000 | Loss: 0.00003645
Iteration 113/1000 | Loss: 0.00003645
Iteration 114/1000 | Loss: 0.00003645
Iteration 115/1000 | Loss: 0.00003645
Iteration 116/1000 | Loss: 0.00003645
Iteration 117/1000 | Loss: 0.00003645
Iteration 118/1000 | Loss: 0.00003645
Iteration 119/1000 | Loss: 0.00003645
Iteration 120/1000 | Loss: 0.00003645
Iteration 121/1000 | Loss: 0.00003645
Iteration 122/1000 | Loss: 0.00003645
Iteration 123/1000 | Loss: 0.00003645
Iteration 124/1000 | Loss: 0.00003645
Iteration 125/1000 | Loss: 0.00003645
Iteration 126/1000 | Loss: 0.00003645
Iteration 127/1000 | Loss: 0.00003645
Iteration 128/1000 | Loss: 0.00003644
Iteration 129/1000 | Loss: 0.00003644
Iteration 130/1000 | Loss: 0.00003644
Iteration 131/1000 | Loss: 0.00003644
Iteration 132/1000 | Loss: 0.00003644
Iteration 133/1000 | Loss: 0.00003644
Iteration 134/1000 | Loss: 0.00003644
Iteration 135/1000 | Loss: 0.00003644
Iteration 136/1000 | Loss: 0.00003644
Iteration 137/1000 | Loss: 0.00003644
Iteration 138/1000 | Loss: 0.00003644
Iteration 139/1000 | Loss: 0.00003644
Iteration 140/1000 | Loss: 0.00003644
Iteration 141/1000 | Loss: 0.00003644
Iteration 142/1000 | Loss: 0.00003644
Iteration 143/1000 | Loss: 0.00003644
Iteration 144/1000 | Loss: 0.00003644
Iteration 145/1000 | Loss: 0.00003644
Iteration 146/1000 | Loss: 0.00003644
Iteration 147/1000 | Loss: 0.00003644
Iteration 148/1000 | Loss: 0.00003644
Iteration 149/1000 | Loss: 0.00003644
Iteration 150/1000 | Loss: 0.00003644
Iteration 151/1000 | Loss: 0.00003644
Iteration 152/1000 | Loss: 0.00003644
Iteration 153/1000 | Loss: 0.00003644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.6444325814954937e-05, 3.6444325814954937e-05, 3.6444325814954937e-05, 3.6444325814954937e-05, 3.6444325814954937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6444325814954937e-05

Optimization complete. Final v2v error: 4.7072529792785645 mm

Highest mean error: 7.518191337585449 mm for frame 79

Lowest mean error: 3.6704800128936768 mm for frame 50

Saving results

Total time: 48.51650810241699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_44_us_2302/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_44_us_2302/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421270
Iteration 2/25 | Loss: 0.00172330
Iteration 3/25 | Loss: 0.00158333
Iteration 4/25 | Loss: 0.00157330
Iteration 5/25 | Loss: 0.00157133
Iteration 6/25 | Loss: 0.00157082
Iteration 7/25 | Loss: 0.00157082
Iteration 8/25 | Loss: 0.00157082
Iteration 9/25 | Loss: 0.00157082
Iteration 10/25 | Loss: 0.00157082
Iteration 11/25 | Loss: 0.00157082
Iteration 12/25 | Loss: 0.00157082
Iteration 13/25 | Loss: 0.00157082
Iteration 14/25 | Loss: 0.00157082
Iteration 15/25 | Loss: 0.00157082
Iteration 16/25 | Loss: 0.00157082
Iteration 17/25 | Loss: 0.00157082
Iteration 18/25 | Loss: 0.00157082
Iteration 19/25 | Loss: 0.00157082
Iteration 20/25 | Loss: 0.00157082
Iteration 21/25 | Loss: 0.00157082
Iteration 22/25 | Loss: 0.00157082
Iteration 23/25 | Loss: 0.00157082
Iteration 24/25 | Loss: 0.00157082
Iteration 25/25 | Loss: 0.00157082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39453423
Iteration 2/25 | Loss: 0.00186454
Iteration 3/25 | Loss: 0.00186454
Iteration 4/25 | Loss: 0.00186454
Iteration 5/25 | Loss: 0.00186454
Iteration 6/25 | Loss: 0.00186454
Iteration 7/25 | Loss: 0.00186454
Iteration 8/25 | Loss: 0.00186454
Iteration 9/25 | Loss: 0.00186453
Iteration 10/25 | Loss: 0.00186453
Iteration 11/25 | Loss: 0.00186453
Iteration 12/25 | Loss: 0.00186453
Iteration 13/25 | Loss: 0.00186453
Iteration 14/25 | Loss: 0.00186453
Iteration 15/25 | Loss: 0.00186453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00186453468631953, 0.00186453468631953, 0.00186453468631953, 0.00186453468631953, 0.00186453468631953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00186453468631953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186453
Iteration 2/1000 | Loss: 0.00007279
Iteration 3/1000 | Loss: 0.00004202
Iteration 4/1000 | Loss: 0.00003214
Iteration 5/1000 | Loss: 0.00002868
Iteration 6/1000 | Loss: 0.00002679
Iteration 7/1000 | Loss: 0.00002535
Iteration 8/1000 | Loss: 0.00002455
Iteration 9/1000 | Loss: 0.00002408
Iteration 10/1000 | Loss: 0.00002383
Iteration 11/1000 | Loss: 0.00002352
Iteration 12/1000 | Loss: 0.00002328
Iteration 13/1000 | Loss: 0.00002317
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00002300
Iteration 16/1000 | Loss: 0.00002299
Iteration 17/1000 | Loss: 0.00002298
Iteration 18/1000 | Loss: 0.00002298
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002295
Iteration 21/1000 | Loss: 0.00002293
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002293
Iteration 24/1000 | Loss: 0.00002293
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002290
Iteration 28/1000 | Loss: 0.00002290
Iteration 29/1000 | Loss: 0.00002289
Iteration 30/1000 | Loss: 0.00002289
Iteration 31/1000 | Loss: 0.00002289
Iteration 32/1000 | Loss: 0.00002289
Iteration 33/1000 | Loss: 0.00002289
Iteration 34/1000 | Loss: 0.00002289
Iteration 35/1000 | Loss: 0.00002289
Iteration 36/1000 | Loss: 0.00002289
Iteration 37/1000 | Loss: 0.00002289
Iteration 38/1000 | Loss: 0.00002288
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002286
Iteration 42/1000 | Loss: 0.00002286
Iteration 43/1000 | Loss: 0.00002286
Iteration 44/1000 | Loss: 0.00002286
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002285
Iteration 47/1000 | Loss: 0.00002285
Iteration 48/1000 | Loss: 0.00002285
Iteration 49/1000 | Loss: 0.00002285
Iteration 50/1000 | Loss: 0.00002285
Iteration 51/1000 | Loss: 0.00002285
Iteration 52/1000 | Loss: 0.00002284
Iteration 53/1000 | Loss: 0.00002284
Iteration 54/1000 | Loss: 0.00002284
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002282
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002282
Iteration 60/1000 | Loss: 0.00002282
Iteration 61/1000 | Loss: 0.00002282
Iteration 62/1000 | Loss: 0.00002282
Iteration 63/1000 | Loss: 0.00002282
Iteration 64/1000 | Loss: 0.00002282
Iteration 65/1000 | Loss: 0.00002282
Iteration 66/1000 | Loss: 0.00002282
Iteration 67/1000 | Loss: 0.00002281
Iteration 68/1000 | Loss: 0.00002281
Iteration 69/1000 | Loss: 0.00002281
Iteration 70/1000 | Loss: 0.00002280
Iteration 71/1000 | Loss: 0.00002280
Iteration 72/1000 | Loss: 0.00002279
Iteration 73/1000 | Loss: 0.00002279
Iteration 74/1000 | Loss: 0.00002277
Iteration 75/1000 | Loss: 0.00002277
Iteration 76/1000 | Loss: 0.00002277
Iteration 77/1000 | Loss: 0.00002277
Iteration 78/1000 | Loss: 0.00002277
Iteration 79/1000 | Loss: 0.00002277
Iteration 80/1000 | Loss: 0.00002277
Iteration 81/1000 | Loss: 0.00002277
Iteration 82/1000 | Loss: 0.00002276
Iteration 83/1000 | Loss: 0.00002276
Iteration 84/1000 | Loss: 0.00002276
Iteration 85/1000 | Loss: 0.00002275
Iteration 86/1000 | Loss: 0.00002274
Iteration 87/1000 | Loss: 0.00002274
Iteration 88/1000 | Loss: 0.00002274
Iteration 89/1000 | Loss: 0.00002274
Iteration 90/1000 | Loss: 0.00002273
Iteration 91/1000 | Loss: 0.00002272
Iteration 92/1000 | Loss: 0.00002272
Iteration 93/1000 | Loss: 0.00002272
Iteration 94/1000 | Loss: 0.00002272
Iteration 95/1000 | Loss: 0.00002272
Iteration 96/1000 | Loss: 0.00002272
Iteration 97/1000 | Loss: 0.00002271
Iteration 98/1000 | Loss: 0.00002271
Iteration 99/1000 | Loss: 0.00002271
Iteration 100/1000 | Loss: 0.00002271
Iteration 101/1000 | Loss: 0.00002271
Iteration 102/1000 | Loss: 0.00002270
Iteration 103/1000 | Loss: 0.00002269
Iteration 104/1000 | Loss: 0.00002269
Iteration 105/1000 | Loss: 0.00002269
Iteration 106/1000 | Loss: 0.00002269
Iteration 107/1000 | Loss: 0.00002269
Iteration 108/1000 | Loss: 0.00002269
Iteration 109/1000 | Loss: 0.00002269
Iteration 110/1000 | Loss: 0.00002268
Iteration 111/1000 | Loss: 0.00002268
Iteration 112/1000 | Loss: 0.00002268
Iteration 113/1000 | Loss: 0.00002268
Iteration 114/1000 | Loss: 0.00002268
Iteration 115/1000 | Loss: 0.00002268
Iteration 116/1000 | Loss: 0.00002268
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002266
Iteration 120/1000 | Loss: 0.00002266
Iteration 121/1000 | Loss: 0.00002266
Iteration 122/1000 | Loss: 0.00002266
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002265
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002265
Iteration 129/1000 | Loss: 0.00002265
Iteration 130/1000 | Loss: 0.00002265
Iteration 131/1000 | Loss: 0.00002264
Iteration 132/1000 | Loss: 0.00002264
Iteration 133/1000 | Loss: 0.00002264
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002263
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002263
Iteration 138/1000 | Loss: 0.00002263
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00002263
Iteration 141/1000 | Loss: 0.00002262
Iteration 142/1000 | Loss: 0.00002262
Iteration 143/1000 | Loss: 0.00002262
Iteration 144/1000 | Loss: 0.00002262
Iteration 145/1000 | Loss: 0.00002262
Iteration 146/1000 | Loss: 0.00002262
Iteration 147/1000 | Loss: 0.00002262
Iteration 148/1000 | Loss: 0.00002262
Iteration 149/1000 | Loss: 0.00002262
Iteration 150/1000 | Loss: 0.00002262
Iteration 151/1000 | Loss: 0.00002262
Iteration 152/1000 | Loss: 0.00002261
Iteration 153/1000 | Loss: 0.00002261
Iteration 154/1000 | Loss: 0.00002261
Iteration 155/1000 | Loss: 0.00002261
Iteration 156/1000 | Loss: 0.00002261
Iteration 157/1000 | Loss: 0.00002261
Iteration 158/1000 | Loss: 0.00002261
Iteration 159/1000 | Loss: 0.00002261
Iteration 160/1000 | Loss: 0.00002261
Iteration 161/1000 | Loss: 0.00002261
Iteration 162/1000 | Loss: 0.00002261
Iteration 163/1000 | Loss: 0.00002261
Iteration 164/1000 | Loss: 0.00002261
Iteration 165/1000 | Loss: 0.00002261
Iteration 166/1000 | Loss: 0.00002261
Iteration 167/1000 | Loss: 0.00002261
Iteration 168/1000 | Loss: 0.00002261
Iteration 169/1000 | Loss: 0.00002261
Iteration 170/1000 | Loss: 0.00002261
Iteration 171/1000 | Loss: 0.00002261
Iteration 172/1000 | Loss: 0.00002261
Iteration 173/1000 | Loss: 0.00002261
Iteration 174/1000 | Loss: 0.00002261
Iteration 175/1000 | Loss: 0.00002261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.2611136955674738e-05, 2.2611136955674738e-05, 2.2611136955674738e-05, 2.2611136955674738e-05, 2.2611136955674738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2611136955674738e-05

Optimization complete. Final v2v error: 4.10931396484375 mm

Highest mean error: 5.163950443267822 mm for frame 47

Lowest mean error: 3.775949478149414 mm for frame 73

Saving results

Total time: 40.3951997756958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888460
Iteration 2/25 | Loss: 0.00137167
Iteration 3/25 | Loss: 0.00121591
Iteration 4/25 | Loss: 0.00120065
Iteration 5/25 | Loss: 0.00119749
Iteration 6/25 | Loss: 0.00119709
Iteration 7/25 | Loss: 0.00119709
Iteration 8/25 | Loss: 0.00119709
Iteration 9/25 | Loss: 0.00119709
Iteration 10/25 | Loss: 0.00119709
Iteration 11/25 | Loss: 0.00119709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011970882769674063, 0.0011970882769674063, 0.0011970882769674063, 0.0011970882769674063, 0.0011970882769674063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011970882769674063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27260518
Iteration 2/25 | Loss: 0.00271037
Iteration 3/25 | Loss: 0.00271037
Iteration 4/25 | Loss: 0.00271036
Iteration 5/25 | Loss: 0.00271036
Iteration 6/25 | Loss: 0.00271036
Iteration 7/25 | Loss: 0.00271036
Iteration 8/25 | Loss: 0.00271036
Iteration 9/25 | Loss: 0.00271036
Iteration 10/25 | Loss: 0.00271036
Iteration 11/25 | Loss: 0.00271036
Iteration 12/25 | Loss: 0.00271036
Iteration 13/25 | Loss: 0.00271036
Iteration 14/25 | Loss: 0.00271036
Iteration 15/25 | Loss: 0.00271036
Iteration 16/25 | Loss: 0.00271036
Iteration 17/25 | Loss: 0.00271036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002710362197831273, 0.002710362197831273, 0.002710362197831273, 0.002710362197831273, 0.002710362197831273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002710362197831273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271036
Iteration 2/1000 | Loss: 0.00003006
Iteration 3/1000 | Loss: 0.00001704
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001270
Iteration 7/1000 | Loss: 0.00001228
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001150
Iteration 11/1000 | Loss: 0.00001149
Iteration 12/1000 | Loss: 0.00001141
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001130
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001129
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001122
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001118
Iteration 28/1000 | Loss: 0.00001118
Iteration 29/1000 | Loss: 0.00001117
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001114
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001102
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001101
Iteration 58/1000 | Loss: 0.00001101
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001100
Iteration 62/1000 | Loss: 0.00001100
Iteration 63/1000 | Loss: 0.00001100
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001099
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001098
Iteration 74/1000 | Loss: 0.00001098
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001095
Iteration 108/1000 | Loss: 0.00001095
Iteration 109/1000 | Loss: 0.00001095
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001095
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001094
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001093
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001093
Iteration 127/1000 | Loss: 0.00001093
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.092675120162312e-05, 1.092675120162312e-05, 1.092675120162312e-05, 1.092675120162312e-05, 1.092675120162312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.092675120162312e-05

Optimization complete. Final v2v error: 2.7999179363250732 mm

Highest mean error: 3.690458297729492 mm for frame 116

Lowest mean error: 2.4174563884735107 mm for frame 196

Saving results

Total time: 35.41348481178284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722848
Iteration 2/25 | Loss: 0.00148592
Iteration 3/25 | Loss: 0.00129973
Iteration 4/25 | Loss: 0.00128550
Iteration 5/25 | Loss: 0.00128103
Iteration 6/25 | Loss: 0.00128076
Iteration 7/25 | Loss: 0.00128076
Iteration 8/25 | Loss: 0.00128076
Iteration 9/25 | Loss: 0.00128076
Iteration 10/25 | Loss: 0.00128076
Iteration 11/25 | Loss: 0.00128076
Iteration 12/25 | Loss: 0.00128076
Iteration 13/25 | Loss: 0.00128076
Iteration 14/25 | Loss: 0.00128076
Iteration 15/25 | Loss: 0.00128076
Iteration 16/25 | Loss: 0.00128076
Iteration 17/25 | Loss: 0.00128076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012807573657482862, 0.0012807573657482862, 0.0012807573657482862, 0.0012807573657482862, 0.0012807573657482862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012807573657482862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38674688
Iteration 2/25 | Loss: 0.00203214
Iteration 3/25 | Loss: 0.00203212
Iteration 4/25 | Loss: 0.00203212
Iteration 5/25 | Loss: 0.00203212
Iteration 6/25 | Loss: 0.00203212
Iteration 7/25 | Loss: 0.00203212
Iteration 8/25 | Loss: 0.00203212
Iteration 9/25 | Loss: 0.00203212
Iteration 10/25 | Loss: 0.00203212
Iteration 11/25 | Loss: 0.00203212
Iteration 12/25 | Loss: 0.00203212
Iteration 13/25 | Loss: 0.00203212
Iteration 14/25 | Loss: 0.00203212
Iteration 15/25 | Loss: 0.00203212
Iteration 16/25 | Loss: 0.00203212
Iteration 17/25 | Loss: 0.00203212
Iteration 18/25 | Loss: 0.00203212
Iteration 19/25 | Loss: 0.00203212
Iteration 20/25 | Loss: 0.00203212
Iteration 21/25 | Loss: 0.00203212
Iteration 22/25 | Loss: 0.00203212
Iteration 23/25 | Loss: 0.00203212
Iteration 24/25 | Loss: 0.00203212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020321172196418047, 0.0020321172196418047, 0.0020321172196418047, 0.0020321172196418047, 0.0020321172196418047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020321172196418047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203212
Iteration 2/1000 | Loss: 0.00005015
Iteration 3/1000 | Loss: 0.00003570
Iteration 4/1000 | Loss: 0.00003297
Iteration 5/1000 | Loss: 0.00003185
Iteration 6/1000 | Loss: 0.00003142
Iteration 7/1000 | Loss: 0.00003101
Iteration 8/1000 | Loss: 0.00003070
Iteration 9/1000 | Loss: 0.00003035
Iteration 10/1000 | Loss: 0.00003012
Iteration 11/1000 | Loss: 0.00002992
Iteration 12/1000 | Loss: 0.00002976
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002936
Iteration 15/1000 | Loss: 0.00002932
Iteration 16/1000 | Loss: 0.00002919
Iteration 17/1000 | Loss: 0.00002904
Iteration 18/1000 | Loss: 0.00002900
Iteration 19/1000 | Loss: 0.00002884
Iteration 20/1000 | Loss: 0.00002878
Iteration 21/1000 | Loss: 0.00002874
Iteration 22/1000 | Loss: 0.00002869
Iteration 23/1000 | Loss: 0.00002862
Iteration 24/1000 | Loss: 0.00002859
Iteration 25/1000 | Loss: 0.00002855
Iteration 26/1000 | Loss: 0.00002853
Iteration 27/1000 | Loss: 0.00002853
Iteration 28/1000 | Loss: 0.00002852
Iteration 29/1000 | Loss: 0.00002852
Iteration 30/1000 | Loss: 0.00002852
Iteration 31/1000 | Loss: 0.00002851
Iteration 32/1000 | Loss: 0.00002851
Iteration 33/1000 | Loss: 0.00002850
Iteration 34/1000 | Loss: 0.00002850
Iteration 35/1000 | Loss: 0.00002850
Iteration 36/1000 | Loss: 0.00002850
Iteration 37/1000 | Loss: 0.00002850
Iteration 38/1000 | Loss: 0.00002850
Iteration 39/1000 | Loss: 0.00002850
Iteration 40/1000 | Loss: 0.00002850
Iteration 41/1000 | Loss: 0.00002850
Iteration 42/1000 | Loss: 0.00002850
Iteration 43/1000 | Loss: 0.00002850
Iteration 44/1000 | Loss: 0.00002850
Iteration 45/1000 | Loss: 0.00002850
Iteration 46/1000 | Loss: 0.00002850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [2.849739212251734e-05, 2.849739212251734e-05, 2.849739212251734e-05, 2.849739212251734e-05, 2.849739212251734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.849739212251734e-05

Optimization complete. Final v2v error: 4.1854963302612305 mm

Highest mean error: 4.680007457733154 mm for frame 227

Lowest mean error: 3.669419765472412 mm for frame 183

Saving results

Total time: 44.070677280426025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878016
Iteration 2/25 | Loss: 0.00178294
Iteration 3/25 | Loss: 0.00144013
Iteration 4/25 | Loss: 0.00139146
Iteration 5/25 | Loss: 0.00136631
Iteration 6/25 | Loss: 0.00135760
Iteration 7/25 | Loss: 0.00135550
Iteration 8/25 | Loss: 0.00135803
Iteration 9/25 | Loss: 0.00135186
Iteration 10/25 | Loss: 0.00135087
Iteration 11/25 | Loss: 0.00135404
Iteration 12/25 | Loss: 0.00135403
Iteration 13/25 | Loss: 0.00135403
Iteration 14/25 | Loss: 0.00135370
Iteration 15/25 | Loss: 0.00135453
Iteration 16/25 | Loss: 0.00135195
Iteration 17/25 | Loss: 0.00135001
Iteration 18/25 | Loss: 0.00134939
Iteration 19/25 | Loss: 0.00134917
Iteration 20/25 | Loss: 0.00134900
Iteration 21/25 | Loss: 0.00134892
Iteration 22/25 | Loss: 0.00134891
Iteration 23/25 | Loss: 0.00134891
Iteration 24/25 | Loss: 0.00134891
Iteration 25/25 | Loss: 0.00134891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19945550
Iteration 2/25 | Loss: 0.00235626
Iteration 3/25 | Loss: 0.00207062
Iteration 4/25 | Loss: 0.00207062
Iteration 5/25 | Loss: 0.00207062
Iteration 6/25 | Loss: 0.00207062
Iteration 7/25 | Loss: 0.00207062
Iteration 8/25 | Loss: 0.00207062
Iteration 9/25 | Loss: 0.00207062
Iteration 10/25 | Loss: 0.00207062
Iteration 11/25 | Loss: 0.00207062
Iteration 12/25 | Loss: 0.00207062
Iteration 13/25 | Loss: 0.00207062
Iteration 14/25 | Loss: 0.00207062
Iteration 15/25 | Loss: 0.00207062
Iteration 16/25 | Loss: 0.00207062
Iteration 17/25 | Loss: 0.00207062
Iteration 18/25 | Loss: 0.00207062
Iteration 19/25 | Loss: 0.00207062
Iteration 20/25 | Loss: 0.00207062
Iteration 21/25 | Loss: 0.00207062
Iteration 22/25 | Loss: 0.00207062
Iteration 23/25 | Loss: 0.00207062
Iteration 24/25 | Loss: 0.00207062
Iteration 25/25 | Loss: 0.00207062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207062
Iteration 2/1000 | Loss: 0.00007509
Iteration 3/1000 | Loss: 0.00005209
Iteration 4/1000 | Loss: 0.00004307
Iteration 5/1000 | Loss: 0.00004086
Iteration 6/1000 | Loss: 0.00003971
Iteration 7/1000 | Loss: 0.00003903
Iteration 8/1000 | Loss: 0.00016118
Iteration 9/1000 | Loss: 0.00020898
Iteration 10/1000 | Loss: 0.00004384
Iteration 11/1000 | Loss: 0.00003892
Iteration 12/1000 | Loss: 0.00003791
Iteration 13/1000 | Loss: 0.00003762
Iteration 14/1000 | Loss: 0.00003730
Iteration 15/1000 | Loss: 0.00003704
Iteration 16/1000 | Loss: 0.00003686
Iteration 17/1000 | Loss: 0.00003666
Iteration 18/1000 | Loss: 0.00003650
Iteration 19/1000 | Loss: 0.00003648
Iteration 20/1000 | Loss: 0.00003632
Iteration 21/1000 | Loss: 0.00003627
Iteration 22/1000 | Loss: 0.00003626
Iteration 23/1000 | Loss: 0.00003622
Iteration 24/1000 | Loss: 0.00003621
Iteration 25/1000 | Loss: 0.00003621
Iteration 26/1000 | Loss: 0.00003620
Iteration 27/1000 | Loss: 0.00003619
Iteration 28/1000 | Loss: 0.00003609
Iteration 29/1000 | Loss: 0.00003600
Iteration 30/1000 | Loss: 0.00003598
Iteration 31/1000 | Loss: 0.00003591
Iteration 32/1000 | Loss: 0.00003589
Iteration 33/1000 | Loss: 0.00003587
Iteration 34/1000 | Loss: 0.00003586
Iteration 35/1000 | Loss: 0.00003576
Iteration 36/1000 | Loss: 0.00003574
Iteration 37/1000 | Loss: 0.00003571
Iteration 38/1000 | Loss: 0.00003570
Iteration 39/1000 | Loss: 0.00003570
Iteration 40/1000 | Loss: 0.00003567
Iteration 41/1000 | Loss: 0.00003567
Iteration 42/1000 | Loss: 0.00003567
Iteration 43/1000 | Loss: 0.00003566
Iteration 44/1000 | Loss: 0.00003566
Iteration 45/1000 | Loss: 0.00003566
Iteration 46/1000 | Loss: 0.00003566
Iteration 47/1000 | Loss: 0.00003566
Iteration 48/1000 | Loss: 0.00003566
Iteration 49/1000 | Loss: 0.00003565
Iteration 50/1000 | Loss: 0.00003561
Iteration 51/1000 | Loss: 0.00003561
Iteration 52/1000 | Loss: 0.00003561
Iteration 53/1000 | Loss: 0.00003561
Iteration 54/1000 | Loss: 0.00003560
Iteration 55/1000 | Loss: 0.00003560
Iteration 56/1000 | Loss: 0.00003558
Iteration 57/1000 | Loss: 0.00003558
Iteration 58/1000 | Loss: 0.00003557
Iteration 59/1000 | Loss: 0.00003557
Iteration 60/1000 | Loss: 0.00003557
Iteration 61/1000 | Loss: 0.00003557
Iteration 62/1000 | Loss: 0.00003556
Iteration 63/1000 | Loss: 0.00003555
Iteration 64/1000 | Loss: 0.00003554
Iteration 65/1000 | Loss: 0.00003554
Iteration 66/1000 | Loss: 0.00003553
Iteration 67/1000 | Loss: 0.00003553
Iteration 68/1000 | Loss: 0.00003552
Iteration 69/1000 | Loss: 0.00003552
Iteration 70/1000 | Loss: 0.00003551
Iteration 71/1000 | Loss: 0.00003551
Iteration 72/1000 | Loss: 0.00003551
Iteration 73/1000 | Loss: 0.00003551
Iteration 74/1000 | Loss: 0.00003551
Iteration 75/1000 | Loss: 0.00003550
Iteration 76/1000 | Loss: 0.00003550
Iteration 77/1000 | Loss: 0.00003550
Iteration 78/1000 | Loss: 0.00003550
Iteration 79/1000 | Loss: 0.00003550
Iteration 80/1000 | Loss: 0.00003550
Iteration 81/1000 | Loss: 0.00003549
Iteration 82/1000 | Loss: 0.00003548
Iteration 83/1000 | Loss: 0.00003548
Iteration 84/1000 | Loss: 0.00003548
Iteration 85/1000 | Loss: 0.00003548
Iteration 86/1000 | Loss: 0.00003548
Iteration 87/1000 | Loss: 0.00003547
Iteration 88/1000 | Loss: 0.00003547
Iteration 89/1000 | Loss: 0.00003547
Iteration 90/1000 | Loss: 0.00003547
Iteration 91/1000 | Loss: 0.00003547
Iteration 92/1000 | Loss: 0.00003545
Iteration 93/1000 | Loss: 0.00003545
Iteration 94/1000 | Loss: 0.00003545
Iteration 95/1000 | Loss: 0.00003545
Iteration 96/1000 | Loss: 0.00003545
Iteration 97/1000 | Loss: 0.00003545
Iteration 98/1000 | Loss: 0.00003545
Iteration 99/1000 | Loss: 0.00003545
Iteration 100/1000 | Loss: 0.00003545
Iteration 101/1000 | Loss: 0.00003544
Iteration 102/1000 | Loss: 0.00003544
Iteration 103/1000 | Loss: 0.00003544
Iteration 104/1000 | Loss: 0.00003544
Iteration 105/1000 | Loss: 0.00003544
Iteration 106/1000 | Loss: 0.00003544
Iteration 107/1000 | Loss: 0.00003544
Iteration 108/1000 | Loss: 0.00003543
Iteration 109/1000 | Loss: 0.00003543
Iteration 110/1000 | Loss: 0.00003543
Iteration 111/1000 | Loss: 0.00003542
Iteration 112/1000 | Loss: 0.00003542
Iteration 113/1000 | Loss: 0.00003542
Iteration 114/1000 | Loss: 0.00003542
Iteration 115/1000 | Loss: 0.00003542
Iteration 116/1000 | Loss: 0.00003541
Iteration 117/1000 | Loss: 0.00003541
Iteration 118/1000 | Loss: 0.00003541
Iteration 119/1000 | Loss: 0.00003541
Iteration 120/1000 | Loss: 0.00003540
Iteration 121/1000 | Loss: 0.00003540
Iteration 122/1000 | Loss: 0.00003540
Iteration 123/1000 | Loss: 0.00003540
Iteration 124/1000 | Loss: 0.00003540
Iteration 125/1000 | Loss: 0.00003540
Iteration 126/1000 | Loss: 0.00003540
Iteration 127/1000 | Loss: 0.00003539
Iteration 128/1000 | Loss: 0.00003539
Iteration 129/1000 | Loss: 0.00003539
Iteration 130/1000 | Loss: 0.00003539
Iteration 131/1000 | Loss: 0.00003539
Iteration 132/1000 | Loss: 0.00003539
Iteration 133/1000 | Loss: 0.00003539
Iteration 134/1000 | Loss: 0.00003539
Iteration 135/1000 | Loss: 0.00003539
Iteration 136/1000 | Loss: 0.00003539
Iteration 137/1000 | Loss: 0.00003539
Iteration 138/1000 | Loss: 0.00003539
Iteration 139/1000 | Loss: 0.00003539
Iteration 140/1000 | Loss: 0.00003538
Iteration 141/1000 | Loss: 0.00003537
Iteration 142/1000 | Loss: 0.00003537
Iteration 143/1000 | Loss: 0.00003537
Iteration 144/1000 | Loss: 0.00003536
Iteration 145/1000 | Loss: 0.00003536
Iteration 146/1000 | Loss: 0.00003536
Iteration 147/1000 | Loss: 0.00003536
Iteration 148/1000 | Loss: 0.00003536
Iteration 149/1000 | Loss: 0.00003536
Iteration 150/1000 | Loss: 0.00003536
Iteration 151/1000 | Loss: 0.00003536
Iteration 152/1000 | Loss: 0.00003536
Iteration 153/1000 | Loss: 0.00003536
Iteration 154/1000 | Loss: 0.00003536
Iteration 155/1000 | Loss: 0.00003536
Iteration 156/1000 | Loss: 0.00003536
Iteration 157/1000 | Loss: 0.00003536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.536115036695264e-05, 3.536115036695264e-05, 3.536115036695264e-05, 3.536115036695264e-05, 3.536115036695264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.536115036695264e-05

Optimization complete. Final v2v error: 4.819947719573975 mm

Highest mean error: 10.603501319885254 mm for frame 23

Lowest mean error: 4.077329635620117 mm for frame 0

Saving results

Total time: 91.87064576148987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417605
Iteration 2/25 | Loss: 0.00140700
Iteration 3/25 | Loss: 0.00131123
Iteration 4/25 | Loss: 0.00129908
Iteration 5/25 | Loss: 0.00129446
Iteration 6/25 | Loss: 0.00129372
Iteration 7/25 | Loss: 0.00129372
Iteration 8/25 | Loss: 0.00129372
Iteration 9/25 | Loss: 0.00129372
Iteration 10/25 | Loss: 0.00129372
Iteration 11/25 | Loss: 0.00129372
Iteration 12/25 | Loss: 0.00129372
Iteration 13/25 | Loss: 0.00129372
Iteration 14/25 | Loss: 0.00129372
Iteration 15/25 | Loss: 0.00129372
Iteration 16/25 | Loss: 0.00129372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012937244027853012, 0.0012937244027853012, 0.0012937244027853012, 0.0012937244027853012, 0.0012937244027853012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012937244027853012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11401510
Iteration 2/25 | Loss: 0.00545673
Iteration 3/25 | Loss: 0.00545673
Iteration 4/25 | Loss: 0.00545673
Iteration 5/25 | Loss: 0.00545673
Iteration 6/25 | Loss: 0.00545673
Iteration 7/25 | Loss: 0.00545673
Iteration 8/25 | Loss: 0.00545673
Iteration 9/25 | Loss: 0.00545673
Iteration 10/25 | Loss: 0.00545673
Iteration 11/25 | Loss: 0.00545673
Iteration 12/25 | Loss: 0.00545673
Iteration 13/25 | Loss: 0.00545673
Iteration 14/25 | Loss: 0.00545673
Iteration 15/25 | Loss: 0.00545673
Iteration 16/25 | Loss: 0.00545673
Iteration 17/25 | Loss: 0.00545673
Iteration 18/25 | Loss: 0.00545673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0054567307233810425, 0.0054567307233810425, 0.0054567307233810425, 0.0054567307233810425, 0.0054567307233810425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0054567307233810425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00545673
Iteration 2/1000 | Loss: 0.00006672
Iteration 3/1000 | Loss: 0.00003518
Iteration 4/1000 | Loss: 0.00002451
Iteration 5/1000 | Loss: 0.00002083
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001822
Iteration 9/1000 | Loss: 0.00001788
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001742
Iteration 12/1000 | Loss: 0.00001724
Iteration 13/1000 | Loss: 0.00001722
Iteration 14/1000 | Loss: 0.00001720
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001711
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00001703
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001702
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001702
Iteration 40/1000 | Loss: 0.00001702
Iteration 41/1000 | Loss: 0.00001702
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001699
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001696
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001688
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001681
Iteration 80/1000 | Loss: 0.00001681
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001678
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001678
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001677
Iteration 108/1000 | Loss: 0.00001677
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001676
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001676
Iteration 114/1000 | Loss: 0.00001675
Iteration 115/1000 | Loss: 0.00001675
Iteration 116/1000 | Loss: 0.00001675
Iteration 117/1000 | Loss: 0.00001675
Iteration 118/1000 | Loss: 0.00001675
Iteration 119/1000 | Loss: 0.00001675
Iteration 120/1000 | Loss: 0.00001675
Iteration 121/1000 | Loss: 0.00001675
Iteration 122/1000 | Loss: 0.00001675
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001675
Iteration 126/1000 | Loss: 0.00001674
Iteration 127/1000 | Loss: 0.00001674
Iteration 128/1000 | Loss: 0.00001674
Iteration 129/1000 | Loss: 0.00001674
Iteration 130/1000 | Loss: 0.00001674
Iteration 131/1000 | Loss: 0.00001674
Iteration 132/1000 | Loss: 0.00001674
Iteration 133/1000 | Loss: 0.00001674
Iteration 134/1000 | Loss: 0.00001674
Iteration 135/1000 | Loss: 0.00001674
Iteration 136/1000 | Loss: 0.00001674
Iteration 137/1000 | Loss: 0.00001674
Iteration 138/1000 | Loss: 0.00001674
Iteration 139/1000 | Loss: 0.00001674
Iteration 140/1000 | Loss: 0.00001674
Iteration 141/1000 | Loss: 0.00001674
Iteration 142/1000 | Loss: 0.00001674
Iteration 143/1000 | Loss: 0.00001674
Iteration 144/1000 | Loss: 0.00001674
Iteration 145/1000 | Loss: 0.00001673
Iteration 146/1000 | Loss: 0.00001673
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001673
Iteration 149/1000 | Loss: 0.00001673
Iteration 150/1000 | Loss: 0.00001673
Iteration 151/1000 | Loss: 0.00001673
Iteration 152/1000 | Loss: 0.00001673
Iteration 153/1000 | Loss: 0.00001673
Iteration 154/1000 | Loss: 0.00001673
Iteration 155/1000 | Loss: 0.00001673
Iteration 156/1000 | Loss: 0.00001672
Iteration 157/1000 | Loss: 0.00001672
Iteration 158/1000 | Loss: 0.00001672
Iteration 159/1000 | Loss: 0.00001672
Iteration 160/1000 | Loss: 0.00001672
Iteration 161/1000 | Loss: 0.00001672
Iteration 162/1000 | Loss: 0.00001672
Iteration 163/1000 | Loss: 0.00001672
Iteration 164/1000 | Loss: 0.00001672
Iteration 165/1000 | Loss: 0.00001672
Iteration 166/1000 | Loss: 0.00001671
Iteration 167/1000 | Loss: 0.00001671
Iteration 168/1000 | Loss: 0.00001671
Iteration 169/1000 | Loss: 0.00001671
Iteration 170/1000 | Loss: 0.00001671
Iteration 171/1000 | Loss: 0.00001671
Iteration 172/1000 | Loss: 0.00001671
Iteration 173/1000 | Loss: 0.00001671
Iteration 174/1000 | Loss: 0.00001671
Iteration 175/1000 | Loss: 0.00001671
Iteration 176/1000 | Loss: 0.00001671
Iteration 177/1000 | Loss: 0.00001671
Iteration 178/1000 | Loss: 0.00001670
Iteration 179/1000 | Loss: 0.00001670
Iteration 180/1000 | Loss: 0.00001670
Iteration 181/1000 | Loss: 0.00001670
Iteration 182/1000 | Loss: 0.00001670
Iteration 183/1000 | Loss: 0.00001670
Iteration 184/1000 | Loss: 0.00001670
Iteration 185/1000 | Loss: 0.00001670
Iteration 186/1000 | Loss: 0.00001670
Iteration 187/1000 | Loss: 0.00001670
Iteration 188/1000 | Loss: 0.00001670
Iteration 189/1000 | Loss: 0.00001669
Iteration 190/1000 | Loss: 0.00001669
Iteration 191/1000 | Loss: 0.00001669
Iteration 192/1000 | Loss: 0.00001669
Iteration 193/1000 | Loss: 0.00001669
Iteration 194/1000 | Loss: 0.00001669
Iteration 195/1000 | Loss: 0.00001669
Iteration 196/1000 | Loss: 0.00001669
Iteration 197/1000 | Loss: 0.00001669
Iteration 198/1000 | Loss: 0.00001669
Iteration 199/1000 | Loss: 0.00001669
Iteration 200/1000 | Loss: 0.00001669
Iteration 201/1000 | Loss: 0.00001669
Iteration 202/1000 | Loss: 0.00001669
Iteration 203/1000 | Loss: 0.00001669
Iteration 204/1000 | Loss: 0.00001669
Iteration 205/1000 | Loss: 0.00001669
Iteration 206/1000 | Loss: 0.00001669
Iteration 207/1000 | Loss: 0.00001669
Iteration 208/1000 | Loss: 0.00001669
Iteration 209/1000 | Loss: 0.00001669
Iteration 210/1000 | Loss: 0.00001669
Iteration 211/1000 | Loss: 0.00001669
Iteration 212/1000 | Loss: 0.00001669
Iteration 213/1000 | Loss: 0.00001669
Iteration 214/1000 | Loss: 0.00001669
Iteration 215/1000 | Loss: 0.00001669
Iteration 216/1000 | Loss: 0.00001669
Iteration 217/1000 | Loss: 0.00001669
Iteration 218/1000 | Loss: 0.00001669
Iteration 219/1000 | Loss: 0.00001669
Iteration 220/1000 | Loss: 0.00001669
Iteration 221/1000 | Loss: 0.00001669
Iteration 222/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.6694384612492286e-05, 1.6694384612492286e-05, 1.6694384612492286e-05, 1.6694384612492286e-05, 1.6694384612492286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6694384612492286e-05

Optimization complete. Final v2v error: 3.3877198696136475 mm

Highest mean error: 3.9473400115966797 mm for frame 68

Lowest mean error: 3.0219364166259766 mm for frame 204

Saving results

Total time: 46.743699073791504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619523
Iteration 2/25 | Loss: 0.00134556
Iteration 3/25 | Loss: 0.00125011
Iteration 4/25 | Loss: 0.00123331
Iteration 5/25 | Loss: 0.00122580
Iteration 6/25 | Loss: 0.00122334
Iteration 7/25 | Loss: 0.00122316
Iteration 8/25 | Loss: 0.00122316
Iteration 9/25 | Loss: 0.00122316
Iteration 10/25 | Loss: 0.00122316
Iteration 11/25 | Loss: 0.00122316
Iteration 12/25 | Loss: 0.00122316
Iteration 13/25 | Loss: 0.00122316
Iteration 14/25 | Loss: 0.00122316
Iteration 15/25 | Loss: 0.00122316
Iteration 16/25 | Loss: 0.00122316
Iteration 17/25 | Loss: 0.00122316
Iteration 18/25 | Loss: 0.00122316
Iteration 19/25 | Loss: 0.00122316
Iteration 20/25 | Loss: 0.00122316
Iteration 21/25 | Loss: 0.00122316
Iteration 22/25 | Loss: 0.00122316
Iteration 23/25 | Loss: 0.00122316
Iteration 24/25 | Loss: 0.00122316
Iteration 25/25 | Loss: 0.00122316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.72928190
Iteration 2/25 | Loss: 0.00351514
Iteration 3/25 | Loss: 0.00351514
Iteration 4/25 | Loss: 0.00351514
Iteration 5/25 | Loss: 0.00351514
Iteration 6/25 | Loss: 0.00351514
Iteration 7/25 | Loss: 0.00351514
Iteration 8/25 | Loss: 0.00351514
Iteration 9/25 | Loss: 0.00351514
Iteration 10/25 | Loss: 0.00351514
Iteration 11/25 | Loss: 0.00351514
Iteration 12/25 | Loss: 0.00351514
Iteration 13/25 | Loss: 0.00351514
Iteration 14/25 | Loss: 0.00351514
Iteration 15/25 | Loss: 0.00351514
Iteration 16/25 | Loss: 0.00351514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0035151413176208735, 0.0035151413176208735, 0.0035151413176208735, 0.0035151413176208735, 0.0035151413176208735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035151413176208735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351514
Iteration 2/1000 | Loss: 0.00002817
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001467
Iteration 7/1000 | Loss: 0.00001415
Iteration 8/1000 | Loss: 0.00001376
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001350
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001331
Iteration 16/1000 | Loss: 0.00001330
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001323
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001312
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001311
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001307
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001298
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001297
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001296
Iteration 56/1000 | Loss: 0.00001296
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001296
Iteration 61/1000 | Loss: 0.00001295
Iteration 62/1000 | Loss: 0.00001295
Iteration 63/1000 | Loss: 0.00001295
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001294
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001291
Iteration 72/1000 | Loss: 0.00001291
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001290
Iteration 83/1000 | Loss: 0.00001290
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001289
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001289
Iteration 90/1000 | Loss: 0.00001289
Iteration 91/1000 | Loss: 0.00001289
Iteration 92/1000 | Loss: 0.00001289
Iteration 93/1000 | Loss: 0.00001289
Iteration 94/1000 | Loss: 0.00001288
Iteration 95/1000 | Loss: 0.00001288
Iteration 96/1000 | Loss: 0.00001288
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001288
Iteration 99/1000 | Loss: 0.00001288
Iteration 100/1000 | Loss: 0.00001288
Iteration 101/1000 | Loss: 0.00001288
Iteration 102/1000 | Loss: 0.00001288
Iteration 103/1000 | Loss: 0.00001288
Iteration 104/1000 | Loss: 0.00001288
Iteration 105/1000 | Loss: 0.00001288
Iteration 106/1000 | Loss: 0.00001287
Iteration 107/1000 | Loss: 0.00001287
Iteration 108/1000 | Loss: 0.00001287
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001287
Iteration 116/1000 | Loss: 0.00001287
Iteration 117/1000 | Loss: 0.00001287
Iteration 118/1000 | Loss: 0.00001287
Iteration 119/1000 | Loss: 0.00001287
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001287
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001287
Iteration 126/1000 | Loss: 0.00001287
Iteration 127/1000 | Loss: 0.00001287
Iteration 128/1000 | Loss: 0.00001287
Iteration 129/1000 | Loss: 0.00001287
Iteration 130/1000 | Loss: 0.00001287
Iteration 131/1000 | Loss: 0.00001287
Iteration 132/1000 | Loss: 0.00001287
Iteration 133/1000 | Loss: 0.00001287
Iteration 134/1000 | Loss: 0.00001287
Iteration 135/1000 | Loss: 0.00001287
Iteration 136/1000 | Loss: 0.00001287
Iteration 137/1000 | Loss: 0.00001287
Iteration 138/1000 | Loss: 0.00001287
Iteration 139/1000 | Loss: 0.00001287
Iteration 140/1000 | Loss: 0.00001287
Iteration 141/1000 | Loss: 0.00001287
Iteration 142/1000 | Loss: 0.00001287
Iteration 143/1000 | Loss: 0.00001287
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001287
Iteration 149/1000 | Loss: 0.00001287
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2872814295405988e-05, 1.2872814295405988e-05, 1.2872814295405988e-05, 1.2872814295405988e-05, 1.2872814295405988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2872814295405988e-05

Optimization complete. Final v2v error: 3.084066152572632 mm

Highest mean error: 3.3362579345703125 mm for frame 120

Lowest mean error: 2.8598945140838623 mm for frame 0

Saving results

Total time: 36.20441389083862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776919
Iteration 2/25 | Loss: 0.00149511
Iteration 3/25 | Loss: 0.00128813
Iteration 4/25 | Loss: 0.00126921
Iteration 5/25 | Loss: 0.00127068
Iteration 6/25 | Loss: 0.00126467
Iteration 7/25 | Loss: 0.00125884
Iteration 8/25 | Loss: 0.00124443
Iteration 9/25 | Loss: 0.00124183
Iteration 10/25 | Loss: 0.00124124
Iteration 11/25 | Loss: 0.00124100
Iteration 12/25 | Loss: 0.00124089
Iteration 13/25 | Loss: 0.00124089
Iteration 14/25 | Loss: 0.00124089
Iteration 15/25 | Loss: 0.00124089
Iteration 16/25 | Loss: 0.00124089
Iteration 17/25 | Loss: 0.00124089
Iteration 18/25 | Loss: 0.00124089
Iteration 19/25 | Loss: 0.00124089
Iteration 20/25 | Loss: 0.00124089
Iteration 21/25 | Loss: 0.00124088
Iteration 22/25 | Loss: 0.00124088
Iteration 23/25 | Loss: 0.00124088
Iteration 24/25 | Loss: 0.00124088
Iteration 25/25 | Loss: 0.00124088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.28912258
Iteration 2/25 | Loss: 0.00256999
Iteration 3/25 | Loss: 0.00256987
Iteration 4/25 | Loss: 0.00256987
Iteration 5/25 | Loss: 0.00256987
Iteration 6/25 | Loss: 0.00256987
Iteration 7/25 | Loss: 0.00256987
Iteration 8/25 | Loss: 0.00256987
Iteration 9/25 | Loss: 0.00256987
Iteration 10/25 | Loss: 0.00256987
Iteration 11/25 | Loss: 0.00256987
Iteration 12/25 | Loss: 0.00256987
Iteration 13/25 | Loss: 0.00256987
Iteration 14/25 | Loss: 0.00256987
Iteration 15/25 | Loss: 0.00256987
Iteration 16/25 | Loss: 0.00256987
Iteration 17/25 | Loss: 0.00256987
Iteration 18/25 | Loss: 0.00256987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0025698693934828043, 0.0025698693934828043, 0.0025698693934828043, 0.0025698693934828043, 0.0025698693934828043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025698693934828043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256987
Iteration 2/1000 | Loss: 0.00003801
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001760
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001645
Iteration 8/1000 | Loss: 0.00001609
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001529
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001508
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001508
Iteration 40/1000 | Loss: 0.00001508
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001507
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001505
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001503
Iteration 55/1000 | Loss: 0.00001503
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00001502
Iteration 58/1000 | Loss: 0.00001501
Iteration 59/1000 | Loss: 0.00001501
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001499
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001497
Iteration 77/1000 | Loss: 0.00001497
Iteration 78/1000 | Loss: 0.00001497
Iteration 79/1000 | Loss: 0.00001497
Iteration 80/1000 | Loss: 0.00001497
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001496
Iteration 83/1000 | Loss: 0.00001496
Iteration 84/1000 | Loss: 0.00001496
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001495
Iteration 91/1000 | Loss: 0.00001495
Iteration 92/1000 | Loss: 0.00001494
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001494
Iteration 95/1000 | Loss: 0.00001493
Iteration 96/1000 | Loss: 0.00001493
Iteration 97/1000 | Loss: 0.00001493
Iteration 98/1000 | Loss: 0.00001493
Iteration 99/1000 | Loss: 0.00001493
Iteration 100/1000 | Loss: 0.00001493
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001491
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001491
Iteration 114/1000 | Loss: 0.00001491
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001490
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Iteration 129/1000 | Loss: 0.00001490
Iteration 130/1000 | Loss: 0.00001490
Iteration 131/1000 | Loss: 0.00001490
Iteration 132/1000 | Loss: 0.00001490
Iteration 133/1000 | Loss: 0.00001490
Iteration 134/1000 | Loss: 0.00001490
Iteration 135/1000 | Loss: 0.00001490
Iteration 136/1000 | Loss: 0.00001490
Iteration 137/1000 | Loss: 0.00001490
Iteration 138/1000 | Loss: 0.00001490
Iteration 139/1000 | Loss: 0.00001490
Iteration 140/1000 | Loss: 0.00001490
Iteration 141/1000 | Loss: 0.00001490
Iteration 142/1000 | Loss: 0.00001490
Iteration 143/1000 | Loss: 0.00001490
Iteration 144/1000 | Loss: 0.00001490
Iteration 145/1000 | Loss: 0.00001490
Iteration 146/1000 | Loss: 0.00001490
Iteration 147/1000 | Loss: 0.00001490
Iteration 148/1000 | Loss: 0.00001490
Iteration 149/1000 | Loss: 0.00001490
Iteration 150/1000 | Loss: 0.00001490
Iteration 151/1000 | Loss: 0.00001490
Iteration 152/1000 | Loss: 0.00001490
Iteration 153/1000 | Loss: 0.00001490
Iteration 154/1000 | Loss: 0.00001490
Iteration 155/1000 | Loss: 0.00001490
Iteration 156/1000 | Loss: 0.00001490
Iteration 157/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.490120939706685e-05, 1.490120939706685e-05, 1.490120939706685e-05, 1.490120939706685e-05, 1.490120939706685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.490120939706685e-05

Optimization complete. Final v2v error: 3.2560808658599854 mm

Highest mean error: 3.8167035579681396 mm for frame 68

Lowest mean error: 2.8031516075134277 mm for frame 39

Saving results

Total time: 54.79433298110962
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977641
Iteration 2/25 | Loss: 0.00977641
Iteration 3/25 | Loss: 0.00977641
Iteration 4/25 | Loss: 0.00977641
Iteration 5/25 | Loss: 0.00977641
Iteration 6/25 | Loss: 0.00977641
Iteration 7/25 | Loss: 0.00977641
Iteration 8/25 | Loss: 0.00977641
Iteration 9/25 | Loss: 0.00977640
Iteration 10/25 | Loss: 0.00977640
Iteration 11/25 | Loss: 0.00977640
Iteration 12/25 | Loss: 0.00977640
Iteration 13/25 | Loss: 0.00977640
Iteration 14/25 | Loss: 0.00977640
Iteration 15/25 | Loss: 0.00977640
Iteration 16/25 | Loss: 0.00977639
Iteration 17/25 | Loss: 0.00977639
Iteration 18/25 | Loss: 0.00977639
Iteration 19/25 | Loss: 0.00977639
Iteration 20/25 | Loss: 0.00977639
Iteration 21/25 | Loss: 0.00977639
Iteration 22/25 | Loss: 0.00977639
Iteration 23/25 | Loss: 0.00977638
Iteration 24/25 | Loss: 0.00977638
Iteration 25/25 | Loss: 0.00977638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32655418
Iteration 2/25 | Loss: 0.18482479
Iteration 3/25 | Loss: 0.18482240
Iteration 4/25 | Loss: 0.18482240
Iteration 5/25 | Loss: 0.18482235
Iteration 6/25 | Loss: 0.18482235
Iteration 7/25 | Loss: 0.18482235
Iteration 8/25 | Loss: 0.18482235
Iteration 9/25 | Loss: 0.18482232
Iteration 10/25 | Loss: 0.18482232
Iteration 11/25 | Loss: 0.18482232
Iteration 12/25 | Loss: 0.18482232
Iteration 13/25 | Loss: 0.18482232
Iteration 14/25 | Loss: 0.18482232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.18482232093811035, 0.18482232093811035, 0.18482232093811035, 0.18482232093811035, 0.18482232093811035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18482232093811035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18482232
Iteration 2/1000 | Loss: 0.00326886
Iteration 3/1000 | Loss: 0.00130607
Iteration 4/1000 | Loss: 0.00076528
Iteration 5/1000 | Loss: 0.00043876
Iteration 6/1000 | Loss: 0.00025669
Iteration 7/1000 | Loss: 0.00016396
Iteration 8/1000 | Loss: 0.00011006
Iteration 9/1000 | Loss: 0.00008413
Iteration 10/1000 | Loss: 0.00006536
Iteration 11/1000 | Loss: 0.00005530
Iteration 12/1000 | Loss: 0.00004566
Iteration 13/1000 | Loss: 0.00004176
Iteration 14/1000 | Loss: 0.00003664
Iteration 15/1000 | Loss: 0.00003383
Iteration 16/1000 | Loss: 0.00003062
Iteration 17/1000 | Loss: 0.00005841
Iteration 18/1000 | Loss: 0.00004412
Iteration 19/1000 | Loss: 0.00003586
Iteration 20/1000 | Loss: 0.00003147
Iteration 21/1000 | Loss: 0.00002715
Iteration 22/1000 | Loss: 0.00002492
Iteration 23/1000 | Loss: 0.00003648
Iteration 24/1000 | Loss: 0.00002810
Iteration 25/1000 | Loss: 0.00002549
Iteration 26/1000 | Loss: 0.00002390
Iteration 27/1000 | Loss: 0.00002248
Iteration 28/1000 | Loss: 0.00002177
Iteration 29/1000 | Loss: 0.00002110
Iteration 30/1000 | Loss: 0.00002068
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00001986
Iteration 34/1000 | Loss: 0.00001964
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001933
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001898
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001896
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001867
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001866
Iteration 47/1000 | Loss: 0.00001866
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001852
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00001852
Iteration 121/1000 | Loss: 0.00001852
Iteration 122/1000 | Loss: 0.00001852
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001851
Iteration 135/1000 | Loss: 0.00001851
Iteration 136/1000 | Loss: 0.00001851
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.8498272766009904e-05, 1.8498272766009904e-05, 1.8498272766009904e-05, 1.8498272766009904e-05, 1.8498272766009904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8498272766009904e-05

Optimization complete. Final v2v error: 3.386147975921631 mm

Highest mean error: 9.730585098266602 mm for frame 31

Lowest mean error: 2.918133020401001 mm for frame 78

Saving results

Total time: 80.24530029296875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424134
Iteration 2/25 | Loss: 0.00132370
Iteration 3/25 | Loss: 0.00125044
Iteration 4/25 | Loss: 0.00123440
Iteration 5/25 | Loss: 0.00122727
Iteration 6/25 | Loss: 0.00122510
Iteration 7/25 | Loss: 0.00122495
Iteration 8/25 | Loss: 0.00122495
Iteration 9/25 | Loss: 0.00122495
Iteration 10/25 | Loss: 0.00122495
Iteration 11/25 | Loss: 0.00122495
Iteration 12/25 | Loss: 0.00122495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001224953099153936, 0.001224953099153936, 0.001224953099153936, 0.001224953099153936, 0.001224953099153936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001224953099153936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35374749
Iteration 2/25 | Loss: 0.00330299
Iteration 3/25 | Loss: 0.00330299
Iteration 4/25 | Loss: 0.00330299
Iteration 5/25 | Loss: 0.00330299
Iteration 6/25 | Loss: 0.00330299
Iteration 7/25 | Loss: 0.00330299
Iteration 8/25 | Loss: 0.00330299
Iteration 9/25 | Loss: 0.00330299
Iteration 10/25 | Loss: 0.00330299
Iteration 11/25 | Loss: 0.00330299
Iteration 12/25 | Loss: 0.00330299
Iteration 13/25 | Loss: 0.00330299
Iteration 14/25 | Loss: 0.00330299
Iteration 15/25 | Loss: 0.00330299
Iteration 16/25 | Loss: 0.00330299
Iteration 17/25 | Loss: 0.00330299
Iteration 18/25 | Loss: 0.00330299
Iteration 19/25 | Loss: 0.00330299
Iteration 20/25 | Loss: 0.00330299
Iteration 21/25 | Loss: 0.00330299
Iteration 22/25 | Loss: 0.00330299
Iteration 23/25 | Loss: 0.00330299
Iteration 24/25 | Loss: 0.00330299
Iteration 25/25 | Loss: 0.00330299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00330299
Iteration 2/1000 | Loss: 0.00003623
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00001912
Iteration 5/1000 | Loss: 0.00001780
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001606
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001493
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001459
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001455
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001453
Iteration 86/1000 | Loss: 0.00001453
Iteration 87/1000 | Loss: 0.00001453
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001452
Iteration 93/1000 | Loss: 0.00001452
Iteration 94/1000 | Loss: 0.00001452
Iteration 95/1000 | Loss: 0.00001451
Iteration 96/1000 | Loss: 0.00001451
Iteration 97/1000 | Loss: 0.00001451
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001451
Iteration 100/1000 | Loss: 0.00001451
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001451
Iteration 103/1000 | Loss: 0.00001450
Iteration 104/1000 | Loss: 0.00001450
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001450
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001449
Iteration 109/1000 | Loss: 0.00001449
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001448
Iteration 113/1000 | Loss: 0.00001448
Iteration 114/1000 | Loss: 0.00001448
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001448
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001447
Iteration 121/1000 | Loss: 0.00001447
Iteration 122/1000 | Loss: 0.00001447
Iteration 123/1000 | Loss: 0.00001447
Iteration 124/1000 | Loss: 0.00001447
Iteration 125/1000 | Loss: 0.00001447
Iteration 126/1000 | Loss: 0.00001447
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001447
Iteration 135/1000 | Loss: 0.00001447
Iteration 136/1000 | Loss: 0.00001447
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.4470490896201227e-05, 1.4470490896201227e-05, 1.4470490896201227e-05, 1.4470490896201227e-05, 1.4470490896201227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4470490896201227e-05

Optimization complete. Final v2v error: 3.2376132011413574 mm

Highest mean error: 3.658231496810913 mm for frame 86

Lowest mean error: 2.9900929927825928 mm for frame 3

Saving results

Total time: 36.680959701538086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502455
Iteration 2/25 | Loss: 0.00160532
Iteration 3/25 | Loss: 0.00143498
Iteration 4/25 | Loss: 0.00140661
Iteration 5/25 | Loss: 0.00139781
Iteration 6/25 | Loss: 0.00139280
Iteration 7/25 | Loss: 0.00139310
Iteration 8/25 | Loss: 0.00139138
Iteration 9/25 | Loss: 0.00139055
Iteration 10/25 | Loss: 0.00139056
Iteration 11/25 | Loss: 0.00139100
Iteration 12/25 | Loss: 0.00139063
Iteration 13/25 | Loss: 0.00139018
Iteration 14/25 | Loss: 0.00139093
Iteration 15/25 | Loss: 0.00139032
Iteration 16/25 | Loss: 0.00138997
Iteration 17/25 | Loss: 0.00138944
Iteration 18/25 | Loss: 0.00139049
Iteration 19/25 | Loss: 0.00139015
Iteration 20/25 | Loss: 0.00139014
Iteration 21/25 | Loss: 0.00139062
Iteration 22/25 | Loss: 0.00139010
Iteration 23/25 | Loss: 0.00138996
Iteration 24/25 | Loss: 0.00139008
Iteration 25/25 | Loss: 0.00139010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13269067
Iteration 2/25 | Loss: 0.00496257
Iteration 3/25 | Loss: 0.00496256
Iteration 4/25 | Loss: 0.00496256
Iteration 5/25 | Loss: 0.00496256
Iteration 6/25 | Loss: 0.00496256
Iteration 7/25 | Loss: 0.00496256
Iteration 8/25 | Loss: 0.00496256
Iteration 9/25 | Loss: 0.00496256
Iteration 10/25 | Loss: 0.00496256
Iteration 11/25 | Loss: 0.00496256
Iteration 12/25 | Loss: 0.00496256
Iteration 13/25 | Loss: 0.00496256
Iteration 14/25 | Loss: 0.00496256
Iteration 15/25 | Loss: 0.00496256
Iteration 16/25 | Loss: 0.00496256
Iteration 17/25 | Loss: 0.00496256
Iteration 18/25 | Loss: 0.00496256
Iteration 19/25 | Loss: 0.00496256
Iteration 20/25 | Loss: 0.00496256
Iteration 21/25 | Loss: 0.00496256
Iteration 22/25 | Loss: 0.00496256
Iteration 23/25 | Loss: 0.00496256
Iteration 24/25 | Loss: 0.00496256
Iteration 25/25 | Loss: 0.00496256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00496256
Iteration 2/1000 | Loss: 0.00025991
Iteration 3/1000 | Loss: 0.00016729
Iteration 4/1000 | Loss: 0.00014872
Iteration 5/1000 | Loss: 0.00014045
Iteration 6/1000 | Loss: 0.00013388
Iteration 7/1000 | Loss: 0.00013718
Iteration 8/1000 | Loss: 0.00011689
Iteration 9/1000 | Loss: 0.00012119
Iteration 10/1000 | Loss: 0.00010842
Iteration 11/1000 | Loss: 0.00011676
Iteration 12/1000 | Loss: 0.00010284
Iteration 13/1000 | Loss: 0.00009913
Iteration 14/1000 | Loss: 0.00009707
Iteration 15/1000 | Loss: 0.00009835
Iteration 16/1000 | Loss: 0.00010152
Iteration 17/1000 | Loss: 0.00010477
Iteration 18/1000 | Loss: 0.00010053
Iteration 19/1000 | Loss: 0.00010603
Iteration 20/1000 | Loss: 0.00010214
Iteration 21/1000 | Loss: 0.00009429
Iteration 22/1000 | Loss: 0.00033571
Iteration 23/1000 | Loss: 0.00010679
Iteration 24/1000 | Loss: 0.00172717
Iteration 25/1000 | Loss: 0.00017688
Iteration 26/1000 | Loss: 0.00408476
Iteration 27/1000 | Loss: 0.00287837
Iteration 28/1000 | Loss: 0.00015214
Iteration 29/1000 | Loss: 0.00009791
Iteration 30/1000 | Loss: 0.00468296
Iteration 31/1000 | Loss: 0.00162624
Iteration 32/1000 | Loss: 0.00016802
Iteration 33/1000 | Loss: 0.00213577
Iteration 34/1000 | Loss: 0.00031583
Iteration 35/1000 | Loss: 0.00022923
Iteration 36/1000 | Loss: 0.00084013
Iteration 37/1000 | Loss: 0.00040906
Iteration 38/1000 | Loss: 0.00008524
Iteration 39/1000 | Loss: 0.00023305
Iteration 40/1000 | Loss: 0.00046167
Iteration 41/1000 | Loss: 0.00008219
Iteration 42/1000 | Loss: 0.00006483
Iteration 43/1000 | Loss: 0.00006104
Iteration 44/1000 | Loss: 0.00005666
Iteration 45/1000 | Loss: 0.00005391
Iteration 46/1000 | Loss: 0.00005184
Iteration 47/1000 | Loss: 0.00056396
Iteration 48/1000 | Loss: 0.00007214
Iteration 49/1000 | Loss: 0.00004857
Iteration 50/1000 | Loss: 0.00004680
Iteration 51/1000 | Loss: 0.00004584
Iteration 52/1000 | Loss: 0.00004468
Iteration 53/1000 | Loss: 0.00004397
Iteration 54/1000 | Loss: 0.00004319
Iteration 55/1000 | Loss: 0.00004257
Iteration 56/1000 | Loss: 0.00004213
Iteration 57/1000 | Loss: 0.00004186
Iteration 58/1000 | Loss: 0.00053189
Iteration 59/1000 | Loss: 0.00034697
Iteration 60/1000 | Loss: 0.00004335
Iteration 61/1000 | Loss: 0.00004178
Iteration 62/1000 | Loss: 0.00055995
Iteration 63/1000 | Loss: 0.00033232
Iteration 64/1000 | Loss: 0.00004353
Iteration 65/1000 | Loss: 0.00004145
Iteration 66/1000 | Loss: 0.00004124
Iteration 67/1000 | Loss: 0.00056644
Iteration 68/1000 | Loss: 0.00005438
Iteration 69/1000 | Loss: 0.00004029
Iteration 70/1000 | Loss: 0.00003961
Iteration 71/1000 | Loss: 0.00003911
Iteration 72/1000 | Loss: 0.00003877
Iteration 73/1000 | Loss: 0.00003837
Iteration 74/1000 | Loss: 0.00003834
Iteration 75/1000 | Loss: 0.00003826
Iteration 76/1000 | Loss: 0.00003814
Iteration 77/1000 | Loss: 0.00003810
Iteration 78/1000 | Loss: 0.00003802
Iteration 79/1000 | Loss: 0.00003801
Iteration 80/1000 | Loss: 0.00003801
Iteration 81/1000 | Loss: 0.00003799
Iteration 82/1000 | Loss: 0.00003799
Iteration 83/1000 | Loss: 0.00003798
Iteration 84/1000 | Loss: 0.00003797
Iteration 85/1000 | Loss: 0.00003797
Iteration 86/1000 | Loss: 0.00003797
Iteration 87/1000 | Loss: 0.00003797
Iteration 88/1000 | Loss: 0.00003797
Iteration 89/1000 | Loss: 0.00003797
Iteration 90/1000 | Loss: 0.00003797
Iteration 91/1000 | Loss: 0.00003797
Iteration 92/1000 | Loss: 0.00003797
Iteration 93/1000 | Loss: 0.00003796
Iteration 94/1000 | Loss: 0.00003796
Iteration 95/1000 | Loss: 0.00003795
Iteration 96/1000 | Loss: 0.00003794
Iteration 97/1000 | Loss: 0.00003794
Iteration 98/1000 | Loss: 0.00003793
Iteration 99/1000 | Loss: 0.00003792
Iteration 100/1000 | Loss: 0.00003792
Iteration 101/1000 | Loss: 0.00003791
Iteration 102/1000 | Loss: 0.00003791
Iteration 103/1000 | Loss: 0.00003791
Iteration 104/1000 | Loss: 0.00003790
Iteration 105/1000 | Loss: 0.00003790
Iteration 106/1000 | Loss: 0.00003790
Iteration 107/1000 | Loss: 0.00003789
Iteration 108/1000 | Loss: 0.00003789
Iteration 109/1000 | Loss: 0.00003789
Iteration 110/1000 | Loss: 0.00003789
Iteration 111/1000 | Loss: 0.00003789
Iteration 112/1000 | Loss: 0.00003788
Iteration 113/1000 | Loss: 0.00003788
Iteration 114/1000 | Loss: 0.00003788
Iteration 115/1000 | Loss: 0.00003788
Iteration 116/1000 | Loss: 0.00003788
Iteration 117/1000 | Loss: 0.00003788
Iteration 118/1000 | Loss: 0.00003788
Iteration 119/1000 | Loss: 0.00003787
Iteration 120/1000 | Loss: 0.00003787
Iteration 121/1000 | Loss: 0.00003787
Iteration 122/1000 | Loss: 0.00003787
Iteration 123/1000 | Loss: 0.00003787
Iteration 124/1000 | Loss: 0.00003787
Iteration 125/1000 | Loss: 0.00003787
Iteration 126/1000 | Loss: 0.00003787
Iteration 127/1000 | Loss: 0.00003787
Iteration 128/1000 | Loss: 0.00003786
Iteration 129/1000 | Loss: 0.00003786
Iteration 130/1000 | Loss: 0.00003786
Iteration 131/1000 | Loss: 0.00003786
Iteration 132/1000 | Loss: 0.00003785
Iteration 133/1000 | Loss: 0.00003785
Iteration 134/1000 | Loss: 0.00003785
Iteration 135/1000 | Loss: 0.00003784
Iteration 136/1000 | Loss: 0.00003784
Iteration 137/1000 | Loss: 0.00003784
Iteration 138/1000 | Loss: 0.00003784
Iteration 139/1000 | Loss: 0.00003784
Iteration 140/1000 | Loss: 0.00003783
Iteration 141/1000 | Loss: 0.00003783
Iteration 142/1000 | Loss: 0.00003783
Iteration 143/1000 | Loss: 0.00003783
Iteration 144/1000 | Loss: 0.00003783
Iteration 145/1000 | Loss: 0.00003783
Iteration 146/1000 | Loss: 0.00003782
Iteration 147/1000 | Loss: 0.00003782
Iteration 148/1000 | Loss: 0.00003782
Iteration 149/1000 | Loss: 0.00003782
Iteration 150/1000 | Loss: 0.00003781
Iteration 151/1000 | Loss: 0.00003781
Iteration 152/1000 | Loss: 0.00003781
Iteration 153/1000 | Loss: 0.00003781
Iteration 154/1000 | Loss: 0.00003781
Iteration 155/1000 | Loss: 0.00003781
Iteration 156/1000 | Loss: 0.00003781
Iteration 157/1000 | Loss: 0.00003781
Iteration 158/1000 | Loss: 0.00003781
Iteration 159/1000 | Loss: 0.00003781
Iteration 160/1000 | Loss: 0.00003781
Iteration 161/1000 | Loss: 0.00003781
Iteration 162/1000 | Loss: 0.00003781
Iteration 163/1000 | Loss: 0.00003781
Iteration 164/1000 | Loss: 0.00003781
Iteration 165/1000 | Loss: 0.00003781
Iteration 166/1000 | Loss: 0.00003781
Iteration 167/1000 | Loss: 0.00003781
Iteration 168/1000 | Loss: 0.00003781
Iteration 169/1000 | Loss: 0.00003781
Iteration 170/1000 | Loss: 0.00003780
Iteration 171/1000 | Loss: 0.00003780
Iteration 172/1000 | Loss: 0.00003780
Iteration 173/1000 | Loss: 0.00003780
Iteration 174/1000 | Loss: 0.00003780
Iteration 175/1000 | Loss: 0.00003780
Iteration 176/1000 | Loss: 0.00003780
Iteration 177/1000 | Loss: 0.00003780
Iteration 178/1000 | Loss: 0.00003780
Iteration 179/1000 | Loss: 0.00003780
Iteration 180/1000 | Loss: 0.00003780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [3.780487895710394e-05, 3.780487895710394e-05, 3.780487895710394e-05, 3.780487895710394e-05, 3.780487895710394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.780487895710394e-05

Optimization complete. Final v2v error: 4.001380443572998 mm

Highest mean error: 11.862643241882324 mm for frame 55

Lowest mean error: 3.174471855163574 mm for frame 19

Saving results

Total time: 166.3474543094635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038037
Iteration 2/25 | Loss: 0.00310343
Iteration 3/25 | Loss: 0.00222074
Iteration 4/25 | Loss: 0.00212911
Iteration 5/25 | Loss: 0.00201064
Iteration 6/25 | Loss: 0.00196148
Iteration 7/25 | Loss: 0.00189659
Iteration 8/25 | Loss: 0.00189830
Iteration 9/25 | Loss: 0.00186969
Iteration 10/25 | Loss: 0.00175936
Iteration 11/25 | Loss: 0.00172054
Iteration 12/25 | Loss: 0.00171000
Iteration 13/25 | Loss: 0.00168489
Iteration 14/25 | Loss: 0.00168736
Iteration 15/25 | Loss: 0.00169636
Iteration 16/25 | Loss: 0.00166911
Iteration 17/25 | Loss: 0.00165402
Iteration 18/25 | Loss: 0.00165835
Iteration 19/25 | Loss: 0.00165527
Iteration 20/25 | Loss: 0.00166177
Iteration 21/25 | Loss: 0.00165984
Iteration 22/25 | Loss: 0.00166530
Iteration 23/25 | Loss: 0.00167320
Iteration 24/25 | Loss: 0.00167205
Iteration 25/25 | Loss: 0.00166700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11578977
Iteration 2/25 | Loss: 0.00728308
Iteration 3/25 | Loss: 0.00675021
Iteration 4/25 | Loss: 0.00654205
Iteration 5/25 | Loss: 0.00658776
Iteration 6/25 | Loss: 0.00654194
Iteration 7/25 | Loss: 0.00654194
Iteration 8/25 | Loss: 0.00654194
Iteration 9/25 | Loss: 0.00654194
Iteration 10/25 | Loss: 0.00654194
Iteration 11/25 | Loss: 0.00654194
Iteration 12/25 | Loss: 0.00654194
Iteration 13/25 | Loss: 0.00654194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0065419371239840984, 0.0065419371239840984, 0.0065419371239840984, 0.0065419371239840984, 0.0065419371239840984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0065419371239840984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00654194
Iteration 2/1000 | Loss: 0.00119808
Iteration 3/1000 | Loss: 0.00119312
Iteration 4/1000 | Loss: 0.00081052
Iteration 5/1000 | Loss: 0.00053706
Iteration 6/1000 | Loss: 0.00063516
Iteration 7/1000 | Loss: 0.00156987
Iteration 8/1000 | Loss: 0.00358155
Iteration 9/1000 | Loss: 0.00070847
Iteration 10/1000 | Loss: 0.00065915
Iteration 11/1000 | Loss: 0.00168245
Iteration 12/1000 | Loss: 0.00056758
Iteration 13/1000 | Loss: 0.00035603
Iteration 14/1000 | Loss: 0.00081494
Iteration 15/1000 | Loss: 0.00154176
Iteration 16/1000 | Loss: 0.00218038
Iteration 17/1000 | Loss: 0.00246405
Iteration 18/1000 | Loss: 0.00051480
Iteration 19/1000 | Loss: 0.00072858
Iteration 20/1000 | Loss: 0.00030143
Iteration 21/1000 | Loss: 0.00025358
Iteration 22/1000 | Loss: 0.00028402
Iteration 23/1000 | Loss: 0.00054401
Iteration 24/1000 | Loss: 0.00047472
Iteration 25/1000 | Loss: 0.00039090
Iteration 26/1000 | Loss: 0.00021904
Iteration 27/1000 | Loss: 0.00023986
Iteration 28/1000 | Loss: 0.00027354
Iteration 29/1000 | Loss: 0.00022795
Iteration 30/1000 | Loss: 0.00017472
Iteration 31/1000 | Loss: 0.00026177
Iteration 32/1000 | Loss: 0.00093613
Iteration 33/1000 | Loss: 0.00036331
Iteration 34/1000 | Loss: 0.00027644
Iteration 35/1000 | Loss: 0.00018353
Iteration 36/1000 | Loss: 0.00024312
Iteration 37/1000 | Loss: 0.00018514
Iteration 38/1000 | Loss: 0.00026245
Iteration 39/1000 | Loss: 0.00031738
Iteration 40/1000 | Loss: 0.00020818
Iteration 41/1000 | Loss: 0.00030678
Iteration 42/1000 | Loss: 0.00040152
Iteration 43/1000 | Loss: 0.00028064
Iteration 44/1000 | Loss: 0.00022762
Iteration 45/1000 | Loss: 0.00031004
Iteration 46/1000 | Loss: 0.00034756
Iteration 47/1000 | Loss: 0.00028120
Iteration 48/1000 | Loss: 0.00021657
Iteration 49/1000 | Loss: 0.00027055
Iteration 50/1000 | Loss: 0.00025652
Iteration 51/1000 | Loss: 0.00038096
Iteration 52/1000 | Loss: 0.00017311
Iteration 53/1000 | Loss: 0.00026195
Iteration 54/1000 | Loss: 0.00030865
Iteration 55/1000 | Loss: 0.00017316
Iteration 56/1000 | Loss: 0.00024607
Iteration 57/1000 | Loss: 0.00027921
Iteration 58/1000 | Loss: 0.00031528
Iteration 59/1000 | Loss: 0.00028809
Iteration 60/1000 | Loss: 0.00026338
Iteration 61/1000 | Loss: 0.00027784
Iteration 62/1000 | Loss: 0.00024898
Iteration 63/1000 | Loss: 0.00027720
Iteration 64/1000 | Loss: 0.00017297
Iteration 65/1000 | Loss: 0.00024973
Iteration 66/1000 | Loss: 0.00021749
Iteration 67/1000 | Loss: 0.00032934
Iteration 68/1000 | Loss: 0.00042509
Iteration 69/1000 | Loss: 0.00021159
Iteration 70/1000 | Loss: 0.00017571
Iteration 71/1000 | Loss: 0.00050170
Iteration 72/1000 | Loss: 0.00023380
Iteration 73/1000 | Loss: 0.00043551
Iteration 74/1000 | Loss: 0.00033220
Iteration 75/1000 | Loss: 0.00040201
Iteration 76/1000 | Loss: 0.00057667
Iteration 77/1000 | Loss: 0.00051705
Iteration 78/1000 | Loss: 0.00056232
Iteration 79/1000 | Loss: 0.00050697
Iteration 80/1000 | Loss: 0.00019235
Iteration 81/1000 | Loss: 0.00030880
Iteration 82/1000 | Loss: 0.00022574
Iteration 83/1000 | Loss: 0.00017132
Iteration 84/1000 | Loss: 0.00016605
Iteration 85/1000 | Loss: 0.00025453
Iteration 86/1000 | Loss: 0.00023419
Iteration 87/1000 | Loss: 0.00022242
Iteration 88/1000 | Loss: 0.00017067
Iteration 89/1000 | Loss: 0.00016014
Iteration 90/1000 | Loss: 0.00028562
Iteration 91/1000 | Loss: 0.00018343
Iteration 92/1000 | Loss: 0.00016077
Iteration 93/1000 | Loss: 0.00037656
Iteration 94/1000 | Loss: 0.00016021
Iteration 95/1000 | Loss: 0.00031656
Iteration 96/1000 | Loss: 0.00037201
Iteration 97/1000 | Loss: 0.00021231
Iteration 98/1000 | Loss: 0.00015345
Iteration 99/1000 | Loss: 0.00044847
Iteration 100/1000 | Loss: 0.00019108
Iteration 101/1000 | Loss: 0.00016554
Iteration 102/1000 | Loss: 0.00016167
Iteration 103/1000 | Loss: 0.00027838
Iteration 104/1000 | Loss: 0.00033750
Iteration 105/1000 | Loss: 0.00019622
Iteration 106/1000 | Loss: 0.00015864
Iteration 107/1000 | Loss: 0.00030621
Iteration 108/1000 | Loss: 0.00017343
Iteration 109/1000 | Loss: 0.00015012
Iteration 110/1000 | Loss: 0.00015162
Iteration 111/1000 | Loss: 0.00014415
Iteration 112/1000 | Loss: 0.00014848
Iteration 113/1000 | Loss: 0.00029925
Iteration 114/1000 | Loss: 0.00015249
Iteration 115/1000 | Loss: 0.00030531
Iteration 116/1000 | Loss: 0.00016353
Iteration 117/1000 | Loss: 0.00015157
Iteration 118/1000 | Loss: 0.00019173
Iteration 119/1000 | Loss: 0.00046946
Iteration 120/1000 | Loss: 0.00033122
Iteration 121/1000 | Loss: 0.00017760
Iteration 122/1000 | Loss: 0.00151628
Iteration 123/1000 | Loss: 0.00111242
Iteration 124/1000 | Loss: 0.00098930
Iteration 125/1000 | Loss: 0.00038525
Iteration 126/1000 | Loss: 0.00047561
Iteration 127/1000 | Loss: 0.00020965
Iteration 128/1000 | Loss: 0.00026534
Iteration 129/1000 | Loss: 0.00015650
Iteration 130/1000 | Loss: 0.00028297
Iteration 131/1000 | Loss: 0.00014625
Iteration 132/1000 | Loss: 0.00039083
Iteration 133/1000 | Loss: 0.00071841
Iteration 134/1000 | Loss: 0.00017987
Iteration 135/1000 | Loss: 0.00030515
Iteration 136/1000 | Loss: 0.00034523
Iteration 137/1000 | Loss: 0.00029056
Iteration 138/1000 | Loss: 0.00011958
Iteration 139/1000 | Loss: 0.00026253
Iteration 140/1000 | Loss: 0.00029237
Iteration 141/1000 | Loss: 0.00014410
Iteration 142/1000 | Loss: 0.00011592
Iteration 143/1000 | Loss: 0.00026612
Iteration 144/1000 | Loss: 0.00013482
Iteration 145/1000 | Loss: 0.00019282
Iteration 146/1000 | Loss: 0.00012820
Iteration 147/1000 | Loss: 0.00030393
Iteration 148/1000 | Loss: 0.00012149
Iteration 149/1000 | Loss: 0.00010795
Iteration 150/1000 | Loss: 0.00009974
Iteration 151/1000 | Loss: 0.00009899
Iteration 152/1000 | Loss: 0.00036191
Iteration 153/1000 | Loss: 0.00012305
Iteration 154/1000 | Loss: 0.00009676
Iteration 155/1000 | Loss: 0.00009617
Iteration 156/1000 | Loss: 0.00008880
Iteration 157/1000 | Loss: 0.00035922
Iteration 158/1000 | Loss: 0.00024377
Iteration 159/1000 | Loss: 0.00013368
Iteration 160/1000 | Loss: 0.00010420
Iteration 161/1000 | Loss: 0.00009333
Iteration 162/1000 | Loss: 0.00047834
Iteration 163/1000 | Loss: 0.00036867
Iteration 164/1000 | Loss: 0.00042205
Iteration 165/1000 | Loss: 0.00036769
Iteration 166/1000 | Loss: 0.00043204
Iteration 167/1000 | Loss: 0.00011224
Iteration 168/1000 | Loss: 0.00013248
Iteration 169/1000 | Loss: 0.00020640
Iteration 170/1000 | Loss: 0.00030336
Iteration 171/1000 | Loss: 0.00010261
Iteration 172/1000 | Loss: 0.00008111
Iteration 173/1000 | Loss: 0.00007383
Iteration 174/1000 | Loss: 0.00022403
Iteration 175/1000 | Loss: 0.00020333
Iteration 176/1000 | Loss: 0.00037882
Iteration 177/1000 | Loss: 0.00026457
Iteration 178/1000 | Loss: 0.00039153
Iteration 179/1000 | Loss: 0.00010591
Iteration 180/1000 | Loss: 0.00007647
Iteration 181/1000 | Loss: 0.00006562
Iteration 182/1000 | Loss: 0.00036416
Iteration 183/1000 | Loss: 0.00009958
Iteration 184/1000 | Loss: 0.00009202
Iteration 185/1000 | Loss: 0.00007630
Iteration 186/1000 | Loss: 0.00005573
Iteration 187/1000 | Loss: 0.00005887
Iteration 188/1000 | Loss: 0.00004812
Iteration 189/1000 | Loss: 0.00004608
Iteration 190/1000 | Loss: 0.00005207
Iteration 191/1000 | Loss: 0.00005079
Iteration 192/1000 | Loss: 0.00005093
Iteration 193/1000 | Loss: 0.00005634
Iteration 194/1000 | Loss: 0.00004648
Iteration 195/1000 | Loss: 0.00005599
Iteration 196/1000 | Loss: 0.00005525
Iteration 197/1000 | Loss: 0.00005553
Iteration 198/1000 | Loss: 0.00005529
Iteration 199/1000 | Loss: 0.00005536
Iteration 200/1000 | Loss: 0.00005532
Iteration 201/1000 | Loss: 0.00005596
Iteration 202/1000 | Loss: 0.00006742
Iteration 203/1000 | Loss: 0.00004448
Iteration 204/1000 | Loss: 0.00005556
Iteration 205/1000 | Loss: 0.00004674
Iteration 206/1000 | Loss: 0.00004843
Iteration 207/1000 | Loss: 0.00005301
Iteration 208/1000 | Loss: 0.00004275
Iteration 209/1000 | Loss: 0.00005226
Iteration 210/1000 | Loss: 0.00005050
Iteration 211/1000 | Loss: 0.00004802
Iteration 212/1000 | Loss: 0.00004679
Iteration 213/1000 | Loss: 0.00005247
Iteration 214/1000 | Loss: 0.00004954
Iteration 215/1000 | Loss: 0.00005346
Iteration 216/1000 | Loss: 0.00005566
Iteration 217/1000 | Loss: 0.00005361
Iteration 218/1000 | Loss: 0.00005507
Iteration 219/1000 | Loss: 0.00005451
Iteration 220/1000 | Loss: 0.00005158
Iteration 221/1000 | Loss: 0.00004618
Iteration 222/1000 | Loss: 0.00005108
Iteration 223/1000 | Loss: 0.00005349
Iteration 224/1000 | Loss: 0.00005433
Iteration 225/1000 | Loss: 0.00005435
Iteration 226/1000 | Loss: 0.00005409
Iteration 227/1000 | Loss: 0.00005394
Iteration 228/1000 | Loss: 0.00005496
Iteration 229/1000 | Loss: 0.00005429
Iteration 230/1000 | Loss: 0.00005472
Iteration 231/1000 | Loss: 0.00005466
Iteration 232/1000 | Loss: 0.00005302
Iteration 233/1000 | Loss: 0.00005332
Iteration 234/1000 | Loss: 0.00005466
Iteration 235/1000 | Loss: 0.00005302
Iteration 236/1000 | Loss: 0.00004935
Iteration 237/1000 | Loss: 0.00004935
Iteration 238/1000 | Loss: 0.00004935
Iteration 239/1000 | Loss: 0.00004935
Iteration 240/1000 | Loss: 0.00004935
Iteration 241/1000 | Loss: 0.00004935
Iteration 242/1000 | Loss: 0.00004935
Iteration 243/1000 | Loss: 0.00004935
Iteration 244/1000 | Loss: 0.00004935
Iteration 245/1000 | Loss: 0.00004935
Iteration 246/1000 | Loss: 0.00004935
Iteration 247/1000 | Loss: 0.00004935
Iteration 248/1000 | Loss: 0.00004935
Iteration 249/1000 | Loss: 0.00004935
Iteration 250/1000 | Loss: 0.00004935
Iteration 251/1000 | Loss: 0.00004935
Iteration 252/1000 | Loss: 0.00004935
Iteration 253/1000 | Loss: 0.00004935
Iteration 254/1000 | Loss: 0.00004935
Iteration 255/1000 | Loss: 0.00004935
Iteration 256/1000 | Loss: 0.00004935
Iteration 257/1000 | Loss: 0.00004935
Iteration 258/1000 | Loss: 0.00004935
Iteration 259/1000 | Loss: 0.00004935
Iteration 260/1000 | Loss: 0.00004935
Iteration 261/1000 | Loss: 0.00004935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [4.9345042498316616e-05, 4.9345042498316616e-05, 4.9345042498316616e-05, 4.9345042498316616e-05, 4.9345042498316616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9345042498316616e-05

Optimization complete. Final v2v error: 4.26107120513916 mm

Highest mean error: 29.85335350036621 mm for frame 162

Lowest mean error: 3.231480360031128 mm for frame 120

Saving results

Total time: 416.244549036026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426482
Iteration 2/25 | Loss: 0.00132813
Iteration 3/25 | Loss: 0.00125280
Iteration 4/25 | Loss: 0.00124527
Iteration 5/25 | Loss: 0.00124315
Iteration 6/25 | Loss: 0.00124255
Iteration 7/25 | Loss: 0.00124254
Iteration 8/25 | Loss: 0.00124254
Iteration 9/25 | Loss: 0.00124254
Iteration 10/25 | Loss: 0.00124254
Iteration 11/25 | Loss: 0.00124254
Iteration 12/25 | Loss: 0.00124254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012425407767295837, 0.0012425407767295837, 0.0012425407767295837, 0.0012425407767295837, 0.0012425407767295837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012425407767295837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37171412
Iteration 2/25 | Loss: 0.00293507
Iteration 3/25 | Loss: 0.00293506
Iteration 4/25 | Loss: 0.00293506
Iteration 5/25 | Loss: 0.00293506
Iteration 6/25 | Loss: 0.00293506
Iteration 7/25 | Loss: 0.00293506
Iteration 8/25 | Loss: 0.00293506
Iteration 9/25 | Loss: 0.00293506
Iteration 10/25 | Loss: 0.00293506
Iteration 11/25 | Loss: 0.00293506
Iteration 12/25 | Loss: 0.00293506
Iteration 13/25 | Loss: 0.00293506
Iteration 14/25 | Loss: 0.00293506
Iteration 15/25 | Loss: 0.00293506
Iteration 16/25 | Loss: 0.00293506
Iteration 17/25 | Loss: 0.00293506
Iteration 18/25 | Loss: 0.00293506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002935060765594244, 0.002935060765594244, 0.002935060765594244, 0.002935060765594244, 0.002935060765594244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002935060765594244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293506
Iteration 2/1000 | Loss: 0.00003683
Iteration 3/1000 | Loss: 0.00002264
Iteration 4/1000 | Loss: 0.00001897
Iteration 5/1000 | Loss: 0.00001705
Iteration 6/1000 | Loss: 0.00001648
Iteration 7/1000 | Loss: 0.00001597
Iteration 8/1000 | Loss: 0.00001557
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001509
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001468
Iteration 33/1000 | Loss: 0.00001468
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001467
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001466
Iteration 38/1000 | Loss: 0.00001466
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001463
Iteration 44/1000 | Loss: 0.00001463
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001462
Iteration 50/1000 | Loss: 0.00001461
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001461
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001460
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001460
Iteration 58/1000 | Loss: 0.00001460
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001459
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001457
Iteration 73/1000 | Loss: 0.00001457
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001456
Iteration 77/1000 | Loss: 0.00001456
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00001451
Iteration 125/1000 | Loss: 0.00001451
Iteration 126/1000 | Loss: 0.00001451
Iteration 127/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.4511569133901503e-05, 1.4511569133901503e-05, 1.4511569133901503e-05, 1.4511569133901503e-05, 1.4511569133901503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4511569133901503e-05

Optimization complete. Final v2v error: 3.2478973865509033 mm

Highest mean error: 3.776785373687744 mm for frame 78

Lowest mean error: 2.983180046081543 mm for frame 115

Saving results

Total time: 36.3760347366333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930370
Iteration 2/25 | Loss: 0.00169490
Iteration 3/25 | Loss: 0.00139693
Iteration 4/25 | Loss: 0.00134818
Iteration 5/25 | Loss: 0.00134564
Iteration 6/25 | Loss: 0.00133977
Iteration 7/25 | Loss: 0.00133137
Iteration 8/25 | Loss: 0.00132144
Iteration 9/25 | Loss: 0.00130627
Iteration 10/25 | Loss: 0.00131562
Iteration 11/25 | Loss: 0.00130909
Iteration 12/25 | Loss: 0.00129559
Iteration 13/25 | Loss: 0.00128604
Iteration 14/25 | Loss: 0.00128210
Iteration 15/25 | Loss: 0.00128113
Iteration 16/25 | Loss: 0.00128084
Iteration 17/25 | Loss: 0.00128070
Iteration 18/25 | Loss: 0.00128069
Iteration 19/25 | Loss: 0.00128069
Iteration 20/25 | Loss: 0.00128068
Iteration 21/25 | Loss: 0.00128068
Iteration 22/25 | Loss: 0.00128068
Iteration 23/25 | Loss: 0.00128068
Iteration 24/25 | Loss: 0.00128067
Iteration 25/25 | Loss: 0.00128067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.33556700
Iteration 2/25 | Loss: 0.00316830
Iteration 3/25 | Loss: 0.00316830
Iteration 4/25 | Loss: 0.00316830
Iteration 5/25 | Loss: 0.00316830
Iteration 6/25 | Loss: 0.00316830
Iteration 7/25 | Loss: 0.00316830
Iteration 8/25 | Loss: 0.00316829
Iteration 9/25 | Loss: 0.00316830
Iteration 10/25 | Loss: 0.00316830
Iteration 11/25 | Loss: 0.00316829
Iteration 12/25 | Loss: 0.00316829
Iteration 13/25 | Loss: 0.00316829
Iteration 14/25 | Loss: 0.00316829
Iteration 15/25 | Loss: 0.00316829
Iteration 16/25 | Loss: 0.00316829
Iteration 17/25 | Loss: 0.00316829
Iteration 18/25 | Loss: 0.00316829
Iteration 19/25 | Loss: 0.00316829
Iteration 20/25 | Loss: 0.00316829
Iteration 21/25 | Loss: 0.00316829
Iteration 22/25 | Loss: 0.00316829
Iteration 23/25 | Loss: 0.00316829
Iteration 24/25 | Loss: 0.00316829
Iteration 25/25 | Loss: 0.00316829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316829
Iteration 2/1000 | Loss: 0.00013639
Iteration 3/1000 | Loss: 0.00011451
Iteration 4/1000 | Loss: 0.00003769
Iteration 5/1000 | Loss: 0.00003008
Iteration 6/1000 | Loss: 0.00002777
Iteration 7/1000 | Loss: 0.00002551
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002347
Iteration 10/1000 | Loss: 0.00002262
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002094
Iteration 16/1000 | Loss: 0.00002081
Iteration 17/1000 | Loss: 0.00002080
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002070
Iteration 21/1000 | Loss: 0.00002067
Iteration 22/1000 | Loss: 0.00002066
Iteration 23/1000 | Loss: 0.00002066
Iteration 24/1000 | Loss: 0.00002065
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002058
Iteration 28/1000 | Loss: 0.00002058
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002056
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00002056
Iteration 47/1000 | Loss: 0.00002056
Iteration 48/1000 | Loss: 0.00002056
Iteration 49/1000 | Loss: 0.00002055
Iteration 50/1000 | Loss: 0.00002055
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002054
Iteration 53/1000 | Loss: 0.00002054
Iteration 54/1000 | Loss: 0.00002054
Iteration 55/1000 | Loss: 0.00002053
Iteration 56/1000 | Loss: 0.00002053
Iteration 57/1000 | Loss: 0.00002052
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002050
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002048
Iteration 63/1000 | Loss: 0.00002045
Iteration 64/1000 | Loss: 0.00002045
Iteration 65/1000 | Loss: 0.00002045
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002043
Iteration 68/1000 | Loss: 0.00002043
Iteration 69/1000 | Loss: 0.00002043
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002042
Iteration 75/1000 | Loss: 0.00002042
Iteration 76/1000 | Loss: 0.00002042
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002042
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002041
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002041
Iteration 86/1000 | Loss: 0.00002040
Iteration 87/1000 | Loss: 0.00002040
Iteration 88/1000 | Loss: 0.00002040
Iteration 89/1000 | Loss: 0.00002039
Iteration 90/1000 | Loss: 0.00002039
Iteration 91/1000 | Loss: 0.00002039
Iteration 92/1000 | Loss: 0.00002038
Iteration 93/1000 | Loss: 0.00002038
Iteration 94/1000 | Loss: 0.00002038
Iteration 95/1000 | Loss: 0.00002037
Iteration 96/1000 | Loss: 0.00002037
Iteration 97/1000 | Loss: 0.00002037
Iteration 98/1000 | Loss: 0.00002037
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002036
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002035
Iteration 103/1000 | Loss: 0.00002035
Iteration 104/1000 | Loss: 0.00002035
Iteration 105/1000 | Loss: 0.00002035
Iteration 106/1000 | Loss: 0.00002035
Iteration 107/1000 | Loss: 0.00002034
Iteration 108/1000 | Loss: 0.00002034
Iteration 109/1000 | Loss: 0.00002034
Iteration 110/1000 | Loss: 0.00002034
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002031
Iteration 123/1000 | Loss: 0.00002031
Iteration 124/1000 | Loss: 0.00002031
Iteration 125/1000 | Loss: 0.00002031
Iteration 126/1000 | Loss: 0.00002030
Iteration 127/1000 | Loss: 0.00002030
Iteration 128/1000 | Loss: 0.00002030
Iteration 129/1000 | Loss: 0.00002030
Iteration 130/1000 | Loss: 0.00002030
Iteration 131/1000 | Loss: 0.00002030
Iteration 132/1000 | Loss: 0.00002029
Iteration 133/1000 | Loss: 0.00002028
Iteration 134/1000 | Loss: 0.00002028
Iteration 135/1000 | Loss: 0.00002028
Iteration 136/1000 | Loss: 0.00002028
Iteration 137/1000 | Loss: 0.00002028
Iteration 138/1000 | Loss: 0.00002028
Iteration 139/1000 | Loss: 0.00002028
Iteration 140/1000 | Loss: 0.00002028
Iteration 141/1000 | Loss: 0.00002028
Iteration 142/1000 | Loss: 0.00002027
Iteration 143/1000 | Loss: 0.00002027
Iteration 144/1000 | Loss: 0.00002026
Iteration 145/1000 | Loss: 0.00002026
Iteration 146/1000 | Loss: 0.00002026
Iteration 147/1000 | Loss: 0.00002026
Iteration 148/1000 | Loss: 0.00002026
Iteration 149/1000 | Loss: 0.00002026
Iteration 150/1000 | Loss: 0.00002026
Iteration 151/1000 | Loss: 0.00002026
Iteration 152/1000 | Loss: 0.00002026
Iteration 153/1000 | Loss: 0.00002026
Iteration 154/1000 | Loss: 0.00002026
Iteration 155/1000 | Loss: 0.00002026
Iteration 156/1000 | Loss: 0.00002026
Iteration 157/1000 | Loss: 0.00002026
Iteration 158/1000 | Loss: 0.00002026
Iteration 159/1000 | Loss: 0.00002025
Iteration 160/1000 | Loss: 0.00002025
Iteration 161/1000 | Loss: 0.00002025
Iteration 162/1000 | Loss: 0.00002025
Iteration 163/1000 | Loss: 0.00002025
Iteration 164/1000 | Loss: 0.00002025
Iteration 165/1000 | Loss: 0.00002025
Iteration 166/1000 | Loss: 0.00002024
Iteration 167/1000 | Loss: 0.00002024
Iteration 168/1000 | Loss: 0.00002024
Iteration 169/1000 | Loss: 0.00002024
Iteration 170/1000 | Loss: 0.00002024
Iteration 171/1000 | Loss: 0.00002024
Iteration 172/1000 | Loss: 0.00002024
Iteration 173/1000 | Loss: 0.00002024
Iteration 174/1000 | Loss: 0.00002023
Iteration 175/1000 | Loss: 0.00002023
Iteration 176/1000 | Loss: 0.00002022
Iteration 177/1000 | Loss: 0.00002022
Iteration 178/1000 | Loss: 0.00002022
Iteration 179/1000 | Loss: 0.00002022
Iteration 180/1000 | Loss: 0.00002021
Iteration 181/1000 | Loss: 0.00002021
Iteration 182/1000 | Loss: 0.00002021
Iteration 183/1000 | Loss: 0.00002021
Iteration 184/1000 | Loss: 0.00002020
Iteration 185/1000 | Loss: 0.00002020
Iteration 186/1000 | Loss: 0.00002020
Iteration 187/1000 | Loss: 0.00002019
Iteration 188/1000 | Loss: 0.00002019
Iteration 189/1000 | Loss: 0.00002019
Iteration 190/1000 | Loss: 0.00002019
Iteration 191/1000 | Loss: 0.00002019
Iteration 192/1000 | Loss: 0.00002019
Iteration 193/1000 | Loss: 0.00002019
Iteration 194/1000 | Loss: 0.00002018
Iteration 195/1000 | Loss: 0.00002018
Iteration 196/1000 | Loss: 0.00002018
Iteration 197/1000 | Loss: 0.00002018
Iteration 198/1000 | Loss: 0.00002018
Iteration 199/1000 | Loss: 0.00002018
Iteration 200/1000 | Loss: 0.00002018
Iteration 201/1000 | Loss: 0.00002018
Iteration 202/1000 | Loss: 0.00002018
Iteration 203/1000 | Loss: 0.00002018
Iteration 204/1000 | Loss: 0.00002018
Iteration 205/1000 | Loss: 0.00002018
Iteration 206/1000 | Loss: 0.00002018
Iteration 207/1000 | Loss: 0.00002018
Iteration 208/1000 | Loss: 0.00002018
Iteration 209/1000 | Loss: 0.00002018
Iteration 210/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.0182325897621922e-05, 2.0182325897621922e-05, 2.0182325897621922e-05, 2.0182325897621922e-05, 2.0182325897621922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0182325897621922e-05

Optimization complete. Final v2v error: 3.705305814743042 mm

Highest mean error: 6.901075839996338 mm for frame 93

Lowest mean error: 3.05810546875 mm for frame 0

Saving results

Total time: 73.29815793037415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394309
Iteration 2/25 | Loss: 0.00132666
Iteration 3/25 | Loss: 0.00124141
Iteration 4/25 | Loss: 0.00123139
Iteration 5/25 | Loss: 0.00122670
Iteration 6/25 | Loss: 0.00122519
Iteration 7/25 | Loss: 0.00122519
Iteration 8/25 | Loss: 0.00122519
Iteration 9/25 | Loss: 0.00122519
Iteration 10/25 | Loss: 0.00122519
Iteration 11/25 | Loss: 0.00122519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001225185813382268, 0.001225185813382268, 0.001225185813382268, 0.001225185813382268, 0.001225185813382268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225185813382268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23664284
Iteration 2/25 | Loss: 0.00340638
Iteration 3/25 | Loss: 0.00340638
Iteration 4/25 | Loss: 0.00340638
Iteration 5/25 | Loss: 0.00340637
Iteration 6/25 | Loss: 0.00340637
Iteration 7/25 | Loss: 0.00340637
Iteration 8/25 | Loss: 0.00340637
Iteration 9/25 | Loss: 0.00340637
Iteration 10/25 | Loss: 0.00340637
Iteration 11/25 | Loss: 0.00340637
Iteration 12/25 | Loss: 0.00340637
Iteration 13/25 | Loss: 0.00340637
Iteration 14/25 | Loss: 0.00340637
Iteration 15/25 | Loss: 0.00340637
Iteration 16/25 | Loss: 0.00340637
Iteration 17/25 | Loss: 0.00340637
Iteration 18/25 | Loss: 0.00340637
Iteration 19/25 | Loss: 0.00340637
Iteration 20/25 | Loss: 0.00340637
Iteration 21/25 | Loss: 0.00340637
Iteration 22/25 | Loss: 0.00340637
Iteration 23/25 | Loss: 0.00340637
Iteration 24/25 | Loss: 0.00340637
Iteration 25/25 | Loss: 0.00340637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00340637
Iteration 2/1000 | Loss: 0.00002424
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001342
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001155
Iteration 11/1000 | Loss: 0.00001148
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001143
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001130
Iteration 26/1000 | Loss: 0.00001130
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001129
Iteration 29/1000 | Loss: 0.00001126
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001122
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001116
Iteration 39/1000 | Loss: 0.00001116
Iteration 40/1000 | Loss: 0.00001116
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001113
Iteration 51/1000 | Loss: 0.00001113
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001112
Iteration 56/1000 | Loss: 0.00001112
Iteration 57/1000 | Loss: 0.00001112
Iteration 58/1000 | Loss: 0.00001112
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001112
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001111
Iteration 64/1000 | Loss: 0.00001111
Iteration 65/1000 | Loss: 0.00001111
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001109
Iteration 72/1000 | Loss: 0.00001109
Iteration 73/1000 | Loss: 0.00001109
Iteration 74/1000 | Loss: 0.00001109
Iteration 75/1000 | Loss: 0.00001109
Iteration 76/1000 | Loss: 0.00001109
Iteration 77/1000 | Loss: 0.00001109
Iteration 78/1000 | Loss: 0.00001109
Iteration 79/1000 | Loss: 0.00001109
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001108
Iteration 88/1000 | Loss: 0.00001108
Iteration 89/1000 | Loss: 0.00001108
Iteration 90/1000 | Loss: 0.00001108
Iteration 91/1000 | Loss: 0.00001108
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.1076255759689957e-05, 1.1076255759689957e-05, 1.1076255759689957e-05, 1.1076255759689957e-05, 1.1076255759689957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1076255759689957e-05

Optimization complete. Final v2v error: 2.8621771335601807 mm

Highest mean error: 3.3877625465393066 mm for frame 12

Lowest mean error: 2.6567301750183105 mm for frame 141

Saving results

Total time: 33.877893924713135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105541
Iteration 2/25 | Loss: 0.01105541
Iteration 3/25 | Loss: 0.01105541
Iteration 4/25 | Loss: 0.01105541
Iteration 5/25 | Loss: 0.01105541
Iteration 6/25 | Loss: 0.01105541
Iteration 7/25 | Loss: 0.01105541
Iteration 8/25 | Loss: 0.01105541
Iteration 9/25 | Loss: 0.01105541
Iteration 10/25 | Loss: 0.01105541
Iteration 11/25 | Loss: 0.01105541
Iteration 12/25 | Loss: 0.01105541
Iteration 13/25 | Loss: 0.01105541
Iteration 14/25 | Loss: 0.01105541
Iteration 15/25 | Loss: 0.01105541
Iteration 16/25 | Loss: 0.01105541
Iteration 17/25 | Loss: 0.01105541
Iteration 18/25 | Loss: 0.01105541
Iteration 19/25 | Loss: 0.01105541
Iteration 20/25 | Loss: 0.01105541
Iteration 21/25 | Loss: 0.01105541
Iteration 22/25 | Loss: 0.01105541
Iteration 23/25 | Loss: 0.01105541
Iteration 24/25 | Loss: 0.01105541
Iteration 25/25 | Loss: 0.01105541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55453610
Iteration 2/25 | Loss: 0.06376313
Iteration 3/25 | Loss: 0.06251020
Iteration 4/25 | Loss: 0.06207793
Iteration 5/25 | Loss: 0.06207793
Iteration 6/25 | Loss: 0.06207793
Iteration 7/25 | Loss: 0.06207792
Iteration 8/25 | Loss: 0.06207792
Iteration 9/25 | Loss: 0.06207792
Iteration 10/25 | Loss: 0.06207792
Iteration 11/25 | Loss: 0.06207792
Iteration 12/25 | Loss: 0.06207792
Iteration 13/25 | Loss: 0.06207792
Iteration 14/25 | Loss: 0.06207792
Iteration 15/25 | Loss: 0.06207792
Iteration 16/25 | Loss: 0.06207792
Iteration 17/25 | Loss: 0.06207792
Iteration 18/25 | Loss: 0.06207792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.06207791715860367, 0.06207791715860367, 0.06207791715860367, 0.06207791715860367, 0.06207791715860367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06207791715860367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06207792
Iteration 2/1000 | Loss: 0.00278399
Iteration 3/1000 | Loss: 0.00093894
Iteration 4/1000 | Loss: 0.00063279
Iteration 5/1000 | Loss: 0.00048013
Iteration 6/1000 | Loss: 0.00062085
Iteration 7/1000 | Loss: 0.00033405
Iteration 8/1000 | Loss: 0.00013863
Iteration 9/1000 | Loss: 0.00037759
Iteration 10/1000 | Loss: 0.00036342
Iteration 11/1000 | Loss: 0.00007929
Iteration 12/1000 | Loss: 0.00014605
Iteration 13/1000 | Loss: 0.00013374
Iteration 14/1000 | Loss: 0.00012389
Iteration 15/1000 | Loss: 0.00004130
Iteration 16/1000 | Loss: 0.00006301
Iteration 17/1000 | Loss: 0.00003577
Iteration 18/1000 | Loss: 0.00011116
Iteration 19/1000 | Loss: 0.00011499
Iteration 20/1000 | Loss: 0.00008924
Iteration 21/1000 | Loss: 0.00003983
Iteration 22/1000 | Loss: 0.00003254
Iteration 23/1000 | Loss: 0.00005633
Iteration 24/1000 | Loss: 0.00005680
Iteration 25/1000 | Loss: 0.00003759
Iteration 26/1000 | Loss: 0.00009556
Iteration 27/1000 | Loss: 0.00006720
Iteration 28/1000 | Loss: 0.00002926
Iteration 29/1000 | Loss: 0.00002855
Iteration 30/1000 | Loss: 0.00004027
Iteration 31/1000 | Loss: 0.00002722
Iteration 32/1000 | Loss: 0.00004095
Iteration 33/1000 | Loss: 0.00002638
Iteration 34/1000 | Loss: 0.00010654
Iteration 35/1000 | Loss: 0.00002640
Iteration 36/1000 | Loss: 0.00007322
Iteration 37/1000 | Loss: 0.00008268
Iteration 38/1000 | Loss: 0.00007019
Iteration 39/1000 | Loss: 0.00006674
Iteration 40/1000 | Loss: 0.00003704
Iteration 41/1000 | Loss: 0.00006708
Iteration 42/1000 | Loss: 0.00015490
Iteration 43/1000 | Loss: 0.00009833
Iteration 44/1000 | Loss: 0.00008940
Iteration 45/1000 | Loss: 0.00007197
Iteration 46/1000 | Loss: 0.00008064
Iteration 47/1000 | Loss: 0.00015406
Iteration 48/1000 | Loss: 0.00008051
Iteration 49/1000 | Loss: 0.00005780
Iteration 50/1000 | Loss: 0.00020680
Iteration 51/1000 | Loss: 0.00011013
Iteration 52/1000 | Loss: 0.00006879
Iteration 53/1000 | Loss: 0.00005839
Iteration 54/1000 | Loss: 0.00009847
Iteration 55/1000 | Loss: 0.00005978
Iteration 56/1000 | Loss: 0.00006292
Iteration 57/1000 | Loss: 0.00005924
Iteration 58/1000 | Loss: 0.00006254
Iteration 59/1000 | Loss: 0.00007585
Iteration 60/1000 | Loss: 0.00006737
Iteration 61/1000 | Loss: 0.00005232
Iteration 62/1000 | Loss: 0.00007683
Iteration 63/1000 | Loss: 0.00005357
Iteration 64/1000 | Loss: 0.00008887
Iteration 65/1000 | Loss: 0.00006282
Iteration 66/1000 | Loss: 0.00007742
Iteration 67/1000 | Loss: 0.00005940
Iteration 68/1000 | Loss: 0.00010991
Iteration 69/1000 | Loss: 0.00005062
Iteration 70/1000 | Loss: 0.00006071
Iteration 71/1000 | Loss: 0.00005557
Iteration 72/1000 | Loss: 0.00007992
Iteration 73/1000 | Loss: 0.00005233
Iteration 74/1000 | Loss: 0.00008455
Iteration 75/1000 | Loss: 0.00005649
Iteration 76/1000 | Loss: 0.00007225
Iteration 77/1000 | Loss: 0.00011201
Iteration 78/1000 | Loss: 0.00012168
Iteration 79/1000 | Loss: 0.00008701
Iteration 80/1000 | Loss: 0.00005005
Iteration 81/1000 | Loss: 0.00004790
Iteration 82/1000 | Loss: 0.00007131
Iteration 83/1000 | Loss: 0.00012853
Iteration 84/1000 | Loss: 0.00003155
Iteration 85/1000 | Loss: 0.00002645
Iteration 86/1000 | Loss: 0.00004773
Iteration 87/1000 | Loss: 0.00002664
Iteration 88/1000 | Loss: 0.00003853
Iteration 89/1000 | Loss: 0.00003284
Iteration 90/1000 | Loss: 0.00002493
Iteration 91/1000 | Loss: 0.00006586
Iteration 92/1000 | Loss: 0.00004333
Iteration 93/1000 | Loss: 0.00009282
Iteration 94/1000 | Loss: 0.00004905
Iteration 95/1000 | Loss: 0.00004026
Iteration 96/1000 | Loss: 0.00002934
Iteration 97/1000 | Loss: 0.00002712
Iteration 98/1000 | Loss: 0.00005007
Iteration 99/1000 | Loss: 0.00003156
Iteration 100/1000 | Loss: 0.00005115
Iteration 101/1000 | Loss: 0.00002788
Iteration 102/1000 | Loss: 0.00004035
Iteration 103/1000 | Loss: 0.00002673
Iteration 104/1000 | Loss: 0.00007308
Iteration 105/1000 | Loss: 0.00002850
Iteration 106/1000 | Loss: 0.00005934
Iteration 107/1000 | Loss: 0.00006997
Iteration 108/1000 | Loss: 0.00053095
Iteration 109/1000 | Loss: 0.00018509
Iteration 110/1000 | Loss: 0.00026929
Iteration 111/1000 | Loss: 0.00004939
Iteration 112/1000 | Loss: 0.00003038
Iteration 113/1000 | Loss: 0.00004900
Iteration 114/1000 | Loss: 0.00003819
Iteration 115/1000 | Loss: 0.00005021
Iteration 116/1000 | Loss: 0.00003040
Iteration 117/1000 | Loss: 0.00003021
Iteration 118/1000 | Loss: 0.00002625
Iteration 119/1000 | Loss: 0.00004850
Iteration 120/1000 | Loss: 0.00002625
Iteration 121/1000 | Loss: 0.00006719
Iteration 122/1000 | Loss: 0.00004946
Iteration 123/1000 | Loss: 0.00006322
Iteration 124/1000 | Loss: 0.00006437
Iteration 125/1000 | Loss: 0.00003620
Iteration 126/1000 | Loss: 0.00005477
Iteration 127/1000 | Loss: 0.00004731
Iteration 128/1000 | Loss: 0.00015039
Iteration 129/1000 | Loss: 0.00006736
Iteration 130/1000 | Loss: 0.00010001
Iteration 131/1000 | Loss: 0.00003191
Iteration 132/1000 | Loss: 0.00008013
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00005027
Iteration 135/1000 | Loss: 0.00002727
Iteration 136/1000 | Loss: 0.00005807
Iteration 137/1000 | Loss: 0.00002904
Iteration 138/1000 | Loss: 0.00007424
Iteration 139/1000 | Loss: 0.00002827
Iteration 140/1000 | Loss: 0.00004799
Iteration 141/1000 | Loss: 0.00003279
Iteration 142/1000 | Loss: 0.00006127
Iteration 143/1000 | Loss: 0.00004362
Iteration 144/1000 | Loss: 0.00005688
Iteration 145/1000 | Loss: 0.00002967
Iteration 146/1000 | Loss: 0.00004924
Iteration 147/1000 | Loss: 0.00002765
Iteration 148/1000 | Loss: 0.00007388
Iteration 149/1000 | Loss: 0.00002856
Iteration 150/1000 | Loss: 0.00005450
Iteration 151/1000 | Loss: 0.00003328
Iteration 152/1000 | Loss: 0.00005144
Iteration 153/1000 | Loss: 0.00003389
Iteration 154/1000 | Loss: 0.00003300
Iteration 155/1000 | Loss: 0.00002654
Iteration 156/1000 | Loss: 0.00004971
Iteration 157/1000 | Loss: 0.00003376
Iteration 158/1000 | Loss: 0.00005503
Iteration 159/1000 | Loss: 0.00003163
Iteration 160/1000 | Loss: 0.00005917
Iteration 161/1000 | Loss: 0.00004230
Iteration 162/1000 | Loss: 0.00005209
Iteration 163/1000 | Loss: 0.00003483
Iteration 164/1000 | Loss: 0.00005405
Iteration 165/1000 | Loss: 0.00003999
Iteration 166/1000 | Loss: 0.00005216
Iteration 167/1000 | Loss: 0.00003731
Iteration 168/1000 | Loss: 0.00003848
Iteration 169/1000 | Loss: 0.00002718
Iteration 170/1000 | Loss: 0.00002759
Iteration 171/1000 | Loss: 0.00002523
Iteration 172/1000 | Loss: 0.00007983
Iteration 173/1000 | Loss: 0.00003147
Iteration 174/1000 | Loss: 0.00003321
Iteration 175/1000 | Loss: 0.00002474
Iteration 176/1000 | Loss: 0.00004081
Iteration 177/1000 | Loss: 0.00008859
Iteration 178/1000 | Loss: 0.00015979
Iteration 179/1000 | Loss: 0.00015903
Iteration 180/1000 | Loss: 0.00032185
Iteration 181/1000 | Loss: 0.00010827
Iteration 182/1000 | Loss: 0.00010291
Iteration 183/1000 | Loss: 0.00005275
Iteration 184/1000 | Loss: 0.00004772
Iteration 185/1000 | Loss: 0.00017095
Iteration 186/1000 | Loss: 0.00005084
Iteration 187/1000 | Loss: 0.00002856
Iteration 188/1000 | Loss: 0.00004333
Iteration 189/1000 | Loss: 0.00002881
Iteration 190/1000 | Loss: 0.00002650
Iteration 191/1000 | Loss: 0.00002833
Iteration 192/1000 | Loss: 0.00002709
Iteration 193/1000 | Loss: 0.00002636
Iteration 194/1000 | Loss: 0.00002870
Iteration 195/1000 | Loss: 0.00002869
Iteration 196/1000 | Loss: 0.00007883
Iteration 197/1000 | Loss: 0.00003455
Iteration 198/1000 | Loss: 0.00002512
Iteration 199/1000 | Loss: 0.00003040
Iteration 200/1000 | Loss: 0.00006006
Iteration 201/1000 | Loss: 0.00003391
Iteration 202/1000 | Loss: 0.00002590
Iteration 203/1000 | Loss: 0.00003339
Iteration 204/1000 | Loss: 0.00002665
Iteration 205/1000 | Loss: 0.00002996
Iteration 206/1000 | Loss: 0.00002638
Iteration 207/1000 | Loss: 0.00002320
Iteration 208/1000 | Loss: 0.00002777
Iteration 209/1000 | Loss: 0.00003108
Iteration 210/1000 | Loss: 0.00002686
Iteration 211/1000 | Loss: 0.00002831
Iteration 212/1000 | Loss: 0.00002838
Iteration 213/1000 | Loss: 0.00002685
Iteration 214/1000 | Loss: 0.00002900
Iteration 215/1000 | Loss: 0.00002649
Iteration 216/1000 | Loss: 0.00003622
Iteration 217/1000 | Loss: 0.00002836
Iteration 218/1000 | Loss: 0.00002954
Iteration 219/1000 | Loss: 0.00002653
Iteration 220/1000 | Loss: 0.00002922
Iteration 221/1000 | Loss: 0.00002578
Iteration 222/1000 | Loss: 0.00002795
Iteration 223/1000 | Loss: 0.00002735
Iteration 224/1000 | Loss: 0.00002863
Iteration 225/1000 | Loss: 0.00002710
Iteration 226/1000 | Loss: 0.00003389
Iteration 227/1000 | Loss: 0.00002955
Iteration 228/1000 | Loss: 0.00002857
Iteration 229/1000 | Loss: 0.00002774
Iteration 230/1000 | Loss: 0.00002872
Iteration 231/1000 | Loss: 0.00002705
Iteration 232/1000 | Loss: 0.00003672
Iteration 233/1000 | Loss: 0.00002959
Iteration 234/1000 | Loss: 0.00005365
Iteration 235/1000 | Loss: 0.00006418
Iteration 236/1000 | Loss: 0.00003853
Iteration 237/1000 | Loss: 0.00002887
Iteration 238/1000 | Loss: 0.00002399
Iteration 239/1000 | Loss: 0.00002318
Iteration 240/1000 | Loss: 0.00002299
Iteration 241/1000 | Loss: 0.00002295
Iteration 242/1000 | Loss: 0.00002292
Iteration 243/1000 | Loss: 0.00002335
Iteration 244/1000 | Loss: 0.00002762
Iteration 245/1000 | Loss: 0.00005480
Iteration 246/1000 | Loss: 0.00009184
Iteration 247/1000 | Loss: 0.00005073
Iteration 248/1000 | Loss: 0.00004068
Iteration 249/1000 | Loss: 0.00012624
Iteration 250/1000 | Loss: 0.00002615
Iteration 251/1000 | Loss: 0.00002408
Iteration 252/1000 | Loss: 0.00002410
Iteration 253/1000 | Loss: 0.00004081
Iteration 254/1000 | Loss: 0.00002583
Iteration 255/1000 | Loss: 0.00002504
Iteration 256/1000 | Loss: 0.00003124
Iteration 257/1000 | Loss: 0.00002852
Iteration 258/1000 | Loss: 0.00002394
Iteration 259/1000 | Loss: 0.00002410
Iteration 260/1000 | Loss: 0.00003212
Iteration 261/1000 | Loss: 0.00002578
Iteration 262/1000 | Loss: 0.00002440
Iteration 263/1000 | Loss: 0.00002380
Iteration 264/1000 | Loss: 0.00002727
Iteration 265/1000 | Loss: 0.00002392
Iteration 266/1000 | Loss: 0.00004483
Iteration 267/1000 | Loss: 0.00004356
Iteration 268/1000 | Loss: 0.00002363
Iteration 269/1000 | Loss: 0.00002677
Iteration 270/1000 | Loss: 0.00002676
Iteration 271/1000 | Loss: 0.00014360
Iteration 272/1000 | Loss: 0.00002985
Iteration 273/1000 | Loss: 0.00002458
Iteration 274/1000 | Loss: 0.00002370
Iteration 275/1000 | Loss: 0.00005498
Iteration 276/1000 | Loss: 0.00002392
Iteration 277/1000 | Loss: 0.00002347
Iteration 278/1000 | Loss: 0.00002363
Iteration 279/1000 | Loss: 0.00002353
Iteration 280/1000 | Loss: 0.00005718
Iteration 281/1000 | Loss: 0.00002367
Iteration 282/1000 | Loss: 0.00005004
Iteration 283/1000 | Loss: 0.00002279
Iteration 284/1000 | Loss: 0.00003771
Iteration 285/1000 | Loss: 0.00002276
Iteration 286/1000 | Loss: 0.00002253
Iteration 287/1000 | Loss: 0.00002253
Iteration 288/1000 | Loss: 0.00002253
Iteration 289/1000 | Loss: 0.00002253
Iteration 290/1000 | Loss: 0.00002253
Iteration 291/1000 | Loss: 0.00002253
Iteration 292/1000 | Loss: 0.00002253
Iteration 293/1000 | Loss: 0.00002253
Iteration 294/1000 | Loss: 0.00002253
Iteration 295/1000 | Loss: 0.00002253
Iteration 296/1000 | Loss: 0.00002253
Iteration 297/1000 | Loss: 0.00002255
Iteration 298/1000 | Loss: 0.00002255
Iteration 299/1000 | Loss: 0.00002255
Iteration 300/1000 | Loss: 0.00002255
Iteration 301/1000 | Loss: 0.00002254
Iteration 302/1000 | Loss: 0.00002254
Iteration 303/1000 | Loss: 0.00002254
Iteration 304/1000 | Loss: 0.00002254
Iteration 305/1000 | Loss: 0.00002253
Iteration 306/1000 | Loss: 0.00002253
Iteration 307/1000 | Loss: 0.00002253
Iteration 308/1000 | Loss: 0.00002253
Iteration 309/1000 | Loss: 0.00002253
Iteration 310/1000 | Loss: 0.00002253
Iteration 311/1000 | Loss: 0.00002253
Iteration 312/1000 | Loss: 0.00002253
Iteration 313/1000 | Loss: 0.00002253
Iteration 314/1000 | Loss: 0.00002253
Iteration 315/1000 | Loss: 0.00002253
Iteration 316/1000 | Loss: 0.00002253
Iteration 317/1000 | Loss: 0.00002253
Iteration 318/1000 | Loss: 0.00002253
Iteration 319/1000 | Loss: 0.00002253
Iteration 320/1000 | Loss: 0.00002253
Iteration 321/1000 | Loss: 0.00002253
Iteration 322/1000 | Loss: 0.00002252
Iteration 323/1000 | Loss: 0.00002252
Iteration 324/1000 | Loss: 0.00002252
Iteration 325/1000 | Loss: 0.00002252
Iteration 326/1000 | Loss: 0.00002251
Iteration 327/1000 | Loss: 0.00002251
Iteration 328/1000 | Loss: 0.00002250
Iteration 329/1000 | Loss: 0.00002250
Iteration 330/1000 | Loss: 0.00002253
Iteration 331/1000 | Loss: 0.00002253
Iteration 332/1000 | Loss: 0.00002252
Iteration 333/1000 | Loss: 0.00002249
Iteration 334/1000 | Loss: 0.00002249
Iteration 335/1000 | Loss: 0.00002249
Iteration 336/1000 | Loss: 0.00002249
Iteration 337/1000 | Loss: 0.00002251
Iteration 338/1000 | Loss: 0.00002251
Iteration 339/1000 | Loss: 0.00002251
Iteration 340/1000 | Loss: 0.00002251
Iteration 341/1000 | Loss: 0.00002251
Iteration 342/1000 | Loss: 0.00002251
Iteration 343/1000 | Loss: 0.00002251
Iteration 344/1000 | Loss: 0.00002251
Iteration 345/1000 | Loss: 0.00002251
Iteration 346/1000 | Loss: 0.00002251
Iteration 347/1000 | Loss: 0.00002251
Iteration 348/1000 | Loss: 0.00002251
Iteration 349/1000 | Loss: 0.00002251
Iteration 350/1000 | Loss: 0.00002251
Iteration 351/1000 | Loss: 0.00002251
Iteration 352/1000 | Loss: 0.00002251
Iteration 353/1000 | Loss: 0.00002251
Iteration 354/1000 | Loss: 0.00002251
Iteration 355/1000 | Loss: 0.00002251
Iteration 356/1000 | Loss: 0.00002251
Iteration 357/1000 | Loss: 0.00002251
Iteration 358/1000 | Loss: 0.00002251
Iteration 359/1000 | Loss: 0.00002251
Iteration 360/1000 | Loss: 0.00002251
Iteration 361/1000 | Loss: 0.00002251
Iteration 362/1000 | Loss: 0.00002251
Iteration 363/1000 | Loss: 0.00002251
Iteration 364/1000 | Loss: 0.00002251
Iteration 365/1000 | Loss: 0.00002251
Iteration 366/1000 | Loss: 0.00002251
Iteration 367/1000 | Loss: 0.00002251
Iteration 368/1000 | Loss: 0.00002251
Iteration 369/1000 | Loss: 0.00002251
Iteration 370/1000 | Loss: 0.00002251
Iteration 371/1000 | Loss: 0.00002251
Iteration 372/1000 | Loss: 0.00002251
Iteration 373/1000 | Loss: 0.00002251
Iteration 374/1000 | Loss: 0.00002251
Iteration 375/1000 | Loss: 0.00002251
Iteration 376/1000 | Loss: 0.00002251
Iteration 377/1000 | Loss: 0.00002251
Iteration 378/1000 | Loss: 0.00002251
Iteration 379/1000 | Loss: 0.00002251
Iteration 380/1000 | Loss: 0.00002251
Iteration 381/1000 | Loss: 0.00002251
Iteration 382/1000 | Loss: 0.00002251
Iteration 383/1000 | Loss: 0.00002251
Iteration 384/1000 | Loss: 0.00002251
Iteration 385/1000 | Loss: 0.00002251
Iteration 386/1000 | Loss: 0.00002251
Iteration 387/1000 | Loss: 0.00002251
Iteration 388/1000 | Loss: 0.00002251
Iteration 389/1000 | Loss: 0.00002251
Iteration 390/1000 | Loss: 0.00002251
Iteration 391/1000 | Loss: 0.00002251
Iteration 392/1000 | Loss: 0.00002251
Iteration 393/1000 | Loss: 0.00002251
Iteration 394/1000 | Loss: 0.00002251
Iteration 395/1000 | Loss: 0.00002251
Iteration 396/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 396. Stopping optimization.
Last 5 losses: [2.2513497242471203e-05, 2.2513497242471203e-05, 2.2513497242471203e-05, 2.2513497242471203e-05, 2.2513497242471203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2513497242471203e-05

Optimization complete. Final v2v error: 3.987154483795166 mm

Highest mean error: 10.589137077331543 mm for frame 124

Lowest mean error: 3.6077401638031006 mm for frame 0

Saving results

Total time: 467.82709312438965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792322
Iteration 2/25 | Loss: 0.00163118
Iteration 3/25 | Loss: 0.00146598
Iteration 4/25 | Loss: 0.00126267
Iteration 5/25 | Loss: 0.00124594
Iteration 6/25 | Loss: 0.00123167
Iteration 7/25 | Loss: 0.00122658
Iteration 8/25 | Loss: 0.00122589
Iteration 9/25 | Loss: 0.00122446
Iteration 10/25 | Loss: 0.00122394
Iteration 11/25 | Loss: 0.00122385
Iteration 12/25 | Loss: 0.00122385
Iteration 13/25 | Loss: 0.00122382
Iteration 14/25 | Loss: 0.00122381
Iteration 15/25 | Loss: 0.00122378
Iteration 16/25 | Loss: 0.00122378
Iteration 17/25 | Loss: 0.00122378
Iteration 18/25 | Loss: 0.00122378
Iteration 19/25 | Loss: 0.00122377
Iteration 20/25 | Loss: 0.00122377
Iteration 21/25 | Loss: 0.00122377
Iteration 22/25 | Loss: 0.00122377
Iteration 23/25 | Loss: 0.00122377
Iteration 24/25 | Loss: 0.00122377
Iteration 25/25 | Loss: 0.00122377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69353485
Iteration 2/25 | Loss: 0.00333805
Iteration 3/25 | Loss: 0.00333805
Iteration 4/25 | Loss: 0.00333804
Iteration 5/25 | Loss: 0.00333804
Iteration 6/25 | Loss: 0.00333804
Iteration 7/25 | Loss: 0.00333804
Iteration 8/25 | Loss: 0.00333804
Iteration 9/25 | Loss: 0.00333804
Iteration 10/25 | Loss: 0.00333804
Iteration 11/25 | Loss: 0.00333804
Iteration 12/25 | Loss: 0.00333804
Iteration 13/25 | Loss: 0.00333804
Iteration 14/25 | Loss: 0.00333804
Iteration 15/25 | Loss: 0.00333804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003338041715323925, 0.003338041715323925, 0.003338041715323925, 0.003338041715323925, 0.003338041715323925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003338041715323925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00333804
Iteration 2/1000 | Loss: 0.00002606
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001597
Iteration 6/1000 | Loss: 0.00001527
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001444
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001402
Iteration 12/1000 | Loss: 0.00001388
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001369
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001369
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001366
Iteration 36/1000 | Loss: 0.00001366
Iteration 37/1000 | Loss: 0.00001365
Iteration 38/1000 | Loss: 0.00001365
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001361
Iteration 53/1000 | Loss: 0.00001361
Iteration 54/1000 | Loss: 0.00001360
Iteration 55/1000 | Loss: 0.00001360
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001360
Iteration 58/1000 | Loss: 0.00001360
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.3541682164941449e-05, 1.3541682164941449e-05, 1.3541682164941449e-05, 1.3541682164941449e-05, 1.3541682164941449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3541682164941449e-05

Optimization complete. Final v2v error: 3.138263463973999 mm

Highest mean error: 3.470005750656128 mm for frame 134

Lowest mean error: 2.8509411811828613 mm for frame 13

Saving results

Total time: 44.623363971710205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790363
Iteration 2/25 | Loss: 0.00162468
Iteration 3/25 | Loss: 0.00131202
Iteration 4/25 | Loss: 0.00127433
Iteration 5/25 | Loss: 0.00126849
Iteration 6/25 | Loss: 0.00126761
Iteration 7/25 | Loss: 0.00126761
Iteration 8/25 | Loss: 0.00126761
Iteration 9/25 | Loss: 0.00126761
Iteration 10/25 | Loss: 0.00126761
Iteration 11/25 | Loss: 0.00126761
Iteration 12/25 | Loss: 0.00126761
Iteration 13/25 | Loss: 0.00126761
Iteration 14/25 | Loss: 0.00126761
Iteration 15/25 | Loss: 0.00126761
Iteration 16/25 | Loss: 0.00126761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012676084879785776, 0.0012676084879785776, 0.0012676084879785776, 0.0012676084879785776, 0.0012676084879785776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012676084879785776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11887693
Iteration 2/25 | Loss: 0.00268439
Iteration 3/25 | Loss: 0.00268439
Iteration 4/25 | Loss: 0.00268439
Iteration 5/25 | Loss: 0.00268439
Iteration 6/25 | Loss: 0.00268439
Iteration 7/25 | Loss: 0.00268439
Iteration 8/25 | Loss: 0.00268439
Iteration 9/25 | Loss: 0.00268439
Iteration 10/25 | Loss: 0.00268439
Iteration 11/25 | Loss: 0.00268439
Iteration 12/25 | Loss: 0.00268439
Iteration 13/25 | Loss: 0.00268439
Iteration 14/25 | Loss: 0.00268439
Iteration 15/25 | Loss: 0.00268439
Iteration 16/25 | Loss: 0.00268439
Iteration 17/25 | Loss: 0.00268439
Iteration 18/25 | Loss: 0.00268439
Iteration 19/25 | Loss: 0.00268439
Iteration 20/25 | Loss: 0.00268439
Iteration 21/25 | Loss: 0.00268439
Iteration 22/25 | Loss: 0.00268439
Iteration 23/25 | Loss: 0.00268439
Iteration 24/25 | Loss: 0.00268439
Iteration 25/25 | Loss: 0.00268439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268439
Iteration 2/1000 | Loss: 0.00004039
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002359
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00002032
Iteration 7/1000 | Loss: 0.00001929
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00001735
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001707
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001705
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001702
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001699
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001686
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001684
Iteration 33/1000 | Loss: 0.00001682
Iteration 34/1000 | Loss: 0.00001682
Iteration 35/1000 | Loss: 0.00001682
Iteration 36/1000 | Loss: 0.00001682
Iteration 37/1000 | Loss: 0.00001681
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001678
Iteration 46/1000 | Loss: 0.00001678
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001677
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001676
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001675
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001670
Iteration 70/1000 | Loss: 0.00001669
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001669
Iteration 73/1000 | Loss: 0.00001669
Iteration 74/1000 | Loss: 0.00001669
Iteration 75/1000 | Loss: 0.00001668
Iteration 76/1000 | Loss: 0.00001668
Iteration 77/1000 | Loss: 0.00001668
Iteration 78/1000 | Loss: 0.00001668
Iteration 79/1000 | Loss: 0.00001668
Iteration 80/1000 | Loss: 0.00001668
Iteration 81/1000 | Loss: 0.00001668
Iteration 82/1000 | Loss: 0.00001668
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001668
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001667
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001666
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001661
Iteration 118/1000 | Loss: 0.00001661
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00001661
Iteration 122/1000 | Loss: 0.00001661
Iteration 123/1000 | Loss: 0.00001661
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001656
Iteration 136/1000 | Loss: 0.00001656
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001655
Iteration 140/1000 | Loss: 0.00001655
Iteration 141/1000 | Loss: 0.00001655
Iteration 142/1000 | Loss: 0.00001655
Iteration 143/1000 | Loss: 0.00001655
Iteration 144/1000 | Loss: 0.00001655
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.6550244254176505e-05, 1.6550244254176505e-05, 1.6550244254176505e-05, 1.6550244254176505e-05, 1.6550244254176505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6550244254176505e-05

Optimization complete. Final v2v error: 3.462139129638672 mm

Highest mean error: 3.694628953933716 mm for frame 206

Lowest mean error: 3.2856035232543945 mm for frame 47

Saving results

Total time: 43.90872550010681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397943
Iteration 2/25 | Loss: 0.00144808
Iteration 3/25 | Loss: 0.00128608
Iteration 4/25 | Loss: 0.00126781
Iteration 5/25 | Loss: 0.00126265
Iteration 6/25 | Loss: 0.00126189
Iteration 7/25 | Loss: 0.00126189
Iteration 8/25 | Loss: 0.00126189
Iteration 9/25 | Loss: 0.00126189
Iteration 10/25 | Loss: 0.00126189
Iteration 11/25 | Loss: 0.00126189
Iteration 12/25 | Loss: 0.00126189
Iteration 13/25 | Loss: 0.00126189
Iteration 14/25 | Loss: 0.00126189
Iteration 15/25 | Loss: 0.00126189
Iteration 16/25 | Loss: 0.00126189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012618888868018985, 0.0012618888868018985, 0.0012618888868018985, 0.0012618888868018985, 0.0012618888868018985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012618888868018985

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72010911
Iteration 2/25 | Loss: 0.00314218
Iteration 3/25 | Loss: 0.00314217
Iteration 4/25 | Loss: 0.00314217
Iteration 5/25 | Loss: 0.00314217
Iteration 6/25 | Loss: 0.00314217
Iteration 7/25 | Loss: 0.00314217
Iteration 8/25 | Loss: 0.00314217
Iteration 9/25 | Loss: 0.00314217
Iteration 10/25 | Loss: 0.00314217
Iteration 11/25 | Loss: 0.00314217
Iteration 12/25 | Loss: 0.00314217
Iteration 13/25 | Loss: 0.00314217
Iteration 14/25 | Loss: 0.00314217
Iteration 15/25 | Loss: 0.00314217
Iteration 16/25 | Loss: 0.00314217
Iteration 17/25 | Loss: 0.00314217
Iteration 18/25 | Loss: 0.00314217
Iteration 19/25 | Loss: 0.00314217
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0031421699095517397, 0.0031421699095517397, 0.0031421699095517397, 0.0031421699095517397, 0.0031421699095517397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031421699095517397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314217
Iteration 2/1000 | Loss: 0.00005817
Iteration 3/1000 | Loss: 0.00003107
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002444
Iteration 6/1000 | Loss: 0.00002362
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00002287
Iteration 9/1000 | Loss: 0.00002259
Iteration 10/1000 | Loss: 0.00002239
Iteration 11/1000 | Loss: 0.00002233
Iteration 12/1000 | Loss: 0.00002227
Iteration 13/1000 | Loss: 0.00002226
Iteration 14/1000 | Loss: 0.00002225
Iteration 15/1000 | Loss: 0.00002224
Iteration 16/1000 | Loss: 0.00002224
Iteration 17/1000 | Loss: 0.00002221
Iteration 18/1000 | Loss: 0.00002214
Iteration 19/1000 | Loss: 0.00002198
Iteration 20/1000 | Loss: 0.00002180
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002156
Iteration 23/1000 | Loss: 0.00002156
Iteration 24/1000 | Loss: 0.00002155
Iteration 25/1000 | Loss: 0.00002155
Iteration 26/1000 | Loss: 0.00002136
Iteration 27/1000 | Loss: 0.00002121
Iteration 28/1000 | Loss: 0.00002121
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002108
Iteration 32/1000 | Loss: 0.00002099
Iteration 33/1000 | Loss: 0.00002096
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002080
Iteration 36/1000 | Loss: 0.00002077
Iteration 37/1000 | Loss: 0.00002071
Iteration 38/1000 | Loss: 0.00002068
Iteration 39/1000 | Loss: 0.00002067
Iteration 40/1000 | Loss: 0.00002067
Iteration 41/1000 | Loss: 0.00002067
Iteration 42/1000 | Loss: 0.00002066
Iteration 43/1000 | Loss: 0.00002066
Iteration 44/1000 | Loss: 0.00002065
Iteration 45/1000 | Loss: 0.00002065
Iteration 46/1000 | Loss: 0.00002065
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002062
Iteration 49/1000 | Loss: 0.00002061
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00002060
Iteration 52/1000 | Loss: 0.00002060
Iteration 53/1000 | Loss: 0.00002060
Iteration 54/1000 | Loss: 0.00002060
Iteration 55/1000 | Loss: 0.00002060
Iteration 56/1000 | Loss: 0.00002060
Iteration 57/1000 | Loss: 0.00002059
Iteration 58/1000 | Loss: 0.00002059
Iteration 59/1000 | Loss: 0.00002059
Iteration 60/1000 | Loss: 0.00002059
Iteration 61/1000 | Loss: 0.00002059
Iteration 62/1000 | Loss: 0.00002059
Iteration 63/1000 | Loss: 0.00002059
Iteration 64/1000 | Loss: 0.00002059
Iteration 65/1000 | Loss: 0.00002059
Iteration 66/1000 | Loss: 0.00002059
Iteration 67/1000 | Loss: 0.00002059
Iteration 68/1000 | Loss: 0.00002059
Iteration 69/1000 | Loss: 0.00002058
Iteration 70/1000 | Loss: 0.00002058
Iteration 71/1000 | Loss: 0.00002058
Iteration 72/1000 | Loss: 0.00002057
Iteration 73/1000 | Loss: 0.00002057
Iteration 74/1000 | Loss: 0.00002057
Iteration 75/1000 | Loss: 0.00002057
Iteration 76/1000 | Loss: 0.00002057
Iteration 77/1000 | Loss: 0.00002057
Iteration 78/1000 | Loss: 0.00002057
Iteration 79/1000 | Loss: 0.00002056
Iteration 80/1000 | Loss: 0.00002056
Iteration 81/1000 | Loss: 0.00002056
Iteration 82/1000 | Loss: 0.00002056
Iteration 83/1000 | Loss: 0.00002056
Iteration 84/1000 | Loss: 0.00002056
Iteration 85/1000 | Loss: 0.00002056
Iteration 86/1000 | Loss: 0.00002055
Iteration 87/1000 | Loss: 0.00002055
Iteration 88/1000 | Loss: 0.00002055
Iteration 89/1000 | Loss: 0.00002055
Iteration 90/1000 | Loss: 0.00002055
Iteration 91/1000 | Loss: 0.00002055
Iteration 92/1000 | Loss: 0.00002055
Iteration 93/1000 | Loss: 0.00002055
Iteration 94/1000 | Loss: 0.00002055
Iteration 95/1000 | Loss: 0.00002055
Iteration 96/1000 | Loss: 0.00002055
Iteration 97/1000 | Loss: 0.00002055
Iteration 98/1000 | Loss: 0.00002055
Iteration 99/1000 | Loss: 0.00002054
Iteration 100/1000 | Loss: 0.00002054
Iteration 101/1000 | Loss: 0.00002054
Iteration 102/1000 | Loss: 0.00002054
Iteration 103/1000 | Loss: 0.00002054
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002053
Iteration 106/1000 | Loss: 0.00002053
Iteration 107/1000 | Loss: 0.00002053
Iteration 108/1000 | Loss: 0.00002053
Iteration 109/1000 | Loss: 0.00002053
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00002051
Iteration 112/1000 | Loss: 0.00002051
Iteration 113/1000 | Loss: 0.00002051
Iteration 114/1000 | Loss: 0.00002050
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002049
Iteration 118/1000 | Loss: 0.00002049
Iteration 119/1000 | Loss: 0.00002049
Iteration 120/1000 | Loss: 0.00002048
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002047
Iteration 125/1000 | Loss: 0.00002047
Iteration 126/1000 | Loss: 0.00002047
Iteration 127/1000 | Loss: 0.00002047
Iteration 128/1000 | Loss: 0.00002047
Iteration 129/1000 | Loss: 0.00002047
Iteration 130/1000 | Loss: 0.00002047
Iteration 131/1000 | Loss: 0.00002047
Iteration 132/1000 | Loss: 0.00002047
Iteration 133/1000 | Loss: 0.00002047
Iteration 134/1000 | Loss: 0.00002047
Iteration 135/1000 | Loss: 0.00002046
Iteration 136/1000 | Loss: 0.00002046
Iteration 137/1000 | Loss: 0.00002046
Iteration 138/1000 | Loss: 0.00002046
Iteration 139/1000 | Loss: 0.00002046
Iteration 140/1000 | Loss: 0.00002045
Iteration 141/1000 | Loss: 0.00002045
Iteration 142/1000 | Loss: 0.00002045
Iteration 143/1000 | Loss: 0.00002045
Iteration 144/1000 | Loss: 0.00002045
Iteration 145/1000 | Loss: 0.00002045
Iteration 146/1000 | Loss: 0.00002045
Iteration 147/1000 | Loss: 0.00002044
Iteration 148/1000 | Loss: 0.00002044
Iteration 149/1000 | Loss: 0.00002044
Iteration 150/1000 | Loss: 0.00002044
Iteration 151/1000 | Loss: 0.00002044
Iteration 152/1000 | Loss: 0.00002044
Iteration 153/1000 | Loss: 0.00002043
Iteration 154/1000 | Loss: 0.00002043
Iteration 155/1000 | Loss: 0.00002043
Iteration 156/1000 | Loss: 0.00002043
Iteration 157/1000 | Loss: 0.00002043
Iteration 158/1000 | Loss: 0.00002043
Iteration 159/1000 | Loss: 0.00002043
Iteration 160/1000 | Loss: 0.00002043
Iteration 161/1000 | Loss: 0.00002043
Iteration 162/1000 | Loss: 0.00002043
Iteration 163/1000 | Loss: 0.00002043
Iteration 164/1000 | Loss: 0.00002042
Iteration 165/1000 | Loss: 0.00002042
Iteration 166/1000 | Loss: 0.00002042
Iteration 167/1000 | Loss: 0.00002042
Iteration 168/1000 | Loss: 0.00002042
Iteration 169/1000 | Loss: 0.00002042
Iteration 170/1000 | Loss: 0.00002042
Iteration 171/1000 | Loss: 0.00002042
Iteration 172/1000 | Loss: 0.00002042
Iteration 173/1000 | Loss: 0.00002042
Iteration 174/1000 | Loss: 0.00002042
Iteration 175/1000 | Loss: 0.00002042
Iteration 176/1000 | Loss: 0.00002042
Iteration 177/1000 | Loss: 0.00002042
Iteration 178/1000 | Loss: 0.00002042
Iteration 179/1000 | Loss: 0.00002042
Iteration 180/1000 | Loss: 0.00002042
Iteration 181/1000 | Loss: 0.00002042
Iteration 182/1000 | Loss: 0.00002041
Iteration 183/1000 | Loss: 0.00002041
Iteration 184/1000 | Loss: 0.00002041
Iteration 185/1000 | Loss: 0.00002041
Iteration 186/1000 | Loss: 0.00002041
Iteration 187/1000 | Loss: 0.00002041
Iteration 188/1000 | Loss: 0.00002041
Iteration 189/1000 | Loss: 0.00002041
Iteration 190/1000 | Loss: 0.00002041
Iteration 191/1000 | Loss: 0.00002041
Iteration 192/1000 | Loss: 0.00002041
Iteration 193/1000 | Loss: 0.00002041
Iteration 194/1000 | Loss: 0.00002041
Iteration 195/1000 | Loss: 0.00002041
Iteration 196/1000 | Loss: 0.00002041
Iteration 197/1000 | Loss: 0.00002041
Iteration 198/1000 | Loss: 0.00002041
Iteration 199/1000 | Loss: 0.00002041
Iteration 200/1000 | Loss: 0.00002041
Iteration 201/1000 | Loss: 0.00002041
Iteration 202/1000 | Loss: 0.00002041
Iteration 203/1000 | Loss: 0.00002041
Iteration 204/1000 | Loss: 0.00002041
Iteration 205/1000 | Loss: 0.00002041
Iteration 206/1000 | Loss: 0.00002041
Iteration 207/1000 | Loss: 0.00002041
Iteration 208/1000 | Loss: 0.00002041
Iteration 209/1000 | Loss: 0.00002041
Iteration 210/1000 | Loss: 0.00002041
Iteration 211/1000 | Loss: 0.00002041
Iteration 212/1000 | Loss: 0.00002041
Iteration 213/1000 | Loss: 0.00002041
Iteration 214/1000 | Loss: 0.00002041
Iteration 215/1000 | Loss: 0.00002041
Iteration 216/1000 | Loss: 0.00002041
Iteration 217/1000 | Loss: 0.00002041
Iteration 218/1000 | Loss: 0.00002041
Iteration 219/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.0411396690178663e-05, 2.0411396690178663e-05, 2.0411396690178663e-05, 2.0411396690178663e-05, 2.0411396690178663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0411396690178663e-05

Optimization complete. Final v2v error: 3.7884206771850586 mm

Highest mean error: 3.913839340209961 mm for frame 83

Lowest mean error: 3.6528310775756836 mm for frame 113

Saving results

Total time: 54.78473234176636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00661951
Iteration 2/25 | Loss: 0.00155142
Iteration 3/25 | Loss: 0.00131506
Iteration 4/25 | Loss: 0.00126490
Iteration 5/25 | Loss: 0.00125646
Iteration 6/25 | Loss: 0.00125808
Iteration 7/25 | Loss: 0.00125040
Iteration 8/25 | Loss: 0.00124475
Iteration 9/25 | Loss: 0.00124144
Iteration 10/25 | Loss: 0.00124015
Iteration 11/25 | Loss: 0.00123945
Iteration 12/25 | Loss: 0.00123834
Iteration 13/25 | Loss: 0.00123746
Iteration 14/25 | Loss: 0.00123730
Iteration 15/25 | Loss: 0.00123728
Iteration 16/25 | Loss: 0.00123728
Iteration 17/25 | Loss: 0.00123727
Iteration 18/25 | Loss: 0.00123727
Iteration 19/25 | Loss: 0.00123726
Iteration 20/25 | Loss: 0.00123726
Iteration 21/25 | Loss: 0.00123725
Iteration 22/25 | Loss: 0.00123725
Iteration 23/25 | Loss: 0.00123725
Iteration 24/25 | Loss: 0.00123724
Iteration 25/25 | Loss: 0.00123724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56052899
Iteration 2/25 | Loss: 0.00292541
Iteration 3/25 | Loss: 0.00291464
Iteration 4/25 | Loss: 0.00291464
Iteration 5/25 | Loss: 0.00291464
Iteration 6/25 | Loss: 0.00291464
Iteration 7/25 | Loss: 0.00291464
Iteration 8/25 | Loss: 0.00291464
Iteration 9/25 | Loss: 0.00291464
Iteration 10/25 | Loss: 0.00291464
Iteration 11/25 | Loss: 0.00291464
Iteration 12/25 | Loss: 0.00291464
Iteration 13/25 | Loss: 0.00291464
Iteration 14/25 | Loss: 0.00291464
Iteration 15/25 | Loss: 0.00291464
Iteration 16/25 | Loss: 0.00291464
Iteration 17/25 | Loss: 0.00291464
Iteration 18/25 | Loss: 0.00291464
Iteration 19/25 | Loss: 0.00291464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002914639888331294, 0.002914639888331294, 0.002914639888331294, 0.002914639888331294, 0.002914639888331294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002914639888331294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00291464
Iteration 2/1000 | Loss: 0.00005201
Iteration 3/1000 | Loss: 0.00002705
Iteration 4/1000 | Loss: 0.00005492
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001676
Iteration 8/1000 | Loss: 0.00001609
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001488
Iteration 12/1000 | Loss: 0.00001700
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001462
Iteration 17/1000 | Loss: 0.00001462
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001459
Iteration 20/1000 | Loss: 0.00001459
Iteration 21/1000 | Loss: 0.00001459
Iteration 22/1000 | Loss: 0.00001458
Iteration 23/1000 | Loss: 0.00001458
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001454
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001429
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001425
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001424
Iteration 83/1000 | Loss: 0.00001424
Iteration 84/1000 | Loss: 0.00001424
Iteration 85/1000 | Loss: 0.00001424
Iteration 86/1000 | Loss: 0.00001424
Iteration 87/1000 | Loss: 0.00001424
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001422
Iteration 96/1000 | Loss: 0.00001422
Iteration 97/1000 | Loss: 0.00001422
Iteration 98/1000 | Loss: 0.00001422
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.422499099135166e-05, 1.422499099135166e-05, 1.422499099135166e-05, 1.422499099135166e-05, 1.422499099135166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.422499099135166e-05

Optimization complete. Final v2v error: 3.212028980255127 mm

Highest mean error: 8.465490341186523 mm for frame 144

Lowest mean error: 2.536330223083496 mm for frame 131

Saving results

Total time: 61.80661940574646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958588
Iteration 2/25 | Loss: 0.00158678
Iteration 3/25 | Loss: 0.00142209
Iteration 4/25 | Loss: 0.00138481
Iteration 5/25 | Loss: 0.00137274
Iteration 6/25 | Loss: 0.00136848
Iteration 7/25 | Loss: 0.00136725
Iteration 8/25 | Loss: 0.00136716
Iteration 9/25 | Loss: 0.00136716
Iteration 10/25 | Loss: 0.00136716
Iteration 11/25 | Loss: 0.00136716
Iteration 12/25 | Loss: 0.00136716
Iteration 13/25 | Loss: 0.00136716
Iteration 14/25 | Loss: 0.00136716
Iteration 15/25 | Loss: 0.00136716
Iteration 16/25 | Loss: 0.00136716
Iteration 17/25 | Loss: 0.00136716
Iteration 18/25 | Loss: 0.00136716
Iteration 19/25 | Loss: 0.00136716
Iteration 20/25 | Loss: 0.00136716
Iteration 21/25 | Loss: 0.00136716
Iteration 22/25 | Loss: 0.00136716
Iteration 23/25 | Loss: 0.00136716
Iteration 24/25 | Loss: 0.00136716
Iteration 25/25 | Loss: 0.00136716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.18808664
Iteration 2/25 | Loss: 0.00268942
Iteration 3/25 | Loss: 0.00268942
Iteration 4/25 | Loss: 0.00268942
Iteration 5/25 | Loss: 0.00268942
Iteration 6/25 | Loss: 0.00268942
Iteration 7/25 | Loss: 0.00268942
Iteration 8/25 | Loss: 0.00268942
Iteration 9/25 | Loss: 0.00268942
Iteration 10/25 | Loss: 0.00268942
Iteration 11/25 | Loss: 0.00268942
Iteration 12/25 | Loss: 0.00268942
Iteration 13/25 | Loss: 0.00268942
Iteration 14/25 | Loss: 0.00268942
Iteration 15/25 | Loss: 0.00268942
Iteration 16/25 | Loss: 0.00268942
Iteration 17/25 | Loss: 0.00268942
Iteration 18/25 | Loss: 0.00268942
Iteration 19/25 | Loss: 0.00268942
Iteration 20/25 | Loss: 0.00268942
Iteration 21/25 | Loss: 0.00268942
Iteration 22/25 | Loss: 0.00268942
Iteration 23/25 | Loss: 0.00268942
Iteration 24/25 | Loss: 0.00268942
Iteration 25/25 | Loss: 0.00268942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268942
Iteration 2/1000 | Loss: 0.00009718
Iteration 3/1000 | Loss: 0.00005858
Iteration 4/1000 | Loss: 0.00004414
Iteration 5/1000 | Loss: 0.00003597
Iteration 6/1000 | Loss: 0.00003420
Iteration 7/1000 | Loss: 0.00003310
Iteration 8/1000 | Loss: 0.00003200
Iteration 9/1000 | Loss: 0.00003141
Iteration 10/1000 | Loss: 0.00003081
Iteration 11/1000 | Loss: 0.00003015
Iteration 12/1000 | Loss: 0.00002979
Iteration 13/1000 | Loss: 0.00002977
Iteration 14/1000 | Loss: 0.00002950
Iteration 15/1000 | Loss: 0.00002927
Iteration 16/1000 | Loss: 0.00002908
Iteration 17/1000 | Loss: 0.00002904
Iteration 18/1000 | Loss: 0.00002889
Iteration 19/1000 | Loss: 0.00002889
Iteration 20/1000 | Loss: 0.00002881
Iteration 21/1000 | Loss: 0.00002878
Iteration 22/1000 | Loss: 0.00002877
Iteration 23/1000 | Loss: 0.00002876
Iteration 24/1000 | Loss: 0.00002874
Iteration 25/1000 | Loss: 0.00002873
Iteration 26/1000 | Loss: 0.00002872
Iteration 27/1000 | Loss: 0.00002872
Iteration 28/1000 | Loss: 0.00002872
Iteration 29/1000 | Loss: 0.00002871
Iteration 30/1000 | Loss: 0.00002871
Iteration 31/1000 | Loss: 0.00002871
Iteration 32/1000 | Loss: 0.00002870
Iteration 33/1000 | Loss: 0.00002870
Iteration 34/1000 | Loss: 0.00002869
Iteration 35/1000 | Loss: 0.00002866
Iteration 36/1000 | Loss: 0.00002864
Iteration 37/1000 | Loss: 0.00002864
Iteration 38/1000 | Loss: 0.00002861
Iteration 39/1000 | Loss: 0.00002861
Iteration 40/1000 | Loss: 0.00002860
Iteration 41/1000 | Loss: 0.00002860
Iteration 42/1000 | Loss: 0.00002859
Iteration 43/1000 | Loss: 0.00002859
Iteration 44/1000 | Loss: 0.00002859
Iteration 45/1000 | Loss: 0.00002859
Iteration 46/1000 | Loss: 0.00002859
Iteration 47/1000 | Loss: 0.00002859
Iteration 48/1000 | Loss: 0.00002859
Iteration 49/1000 | Loss: 0.00002858
Iteration 50/1000 | Loss: 0.00002858
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002858
Iteration 54/1000 | Loss: 0.00002858
Iteration 55/1000 | Loss: 0.00002857
Iteration 56/1000 | Loss: 0.00002857
Iteration 57/1000 | Loss: 0.00002857
Iteration 58/1000 | Loss: 0.00002857
Iteration 59/1000 | Loss: 0.00002857
Iteration 60/1000 | Loss: 0.00002857
Iteration 61/1000 | Loss: 0.00002857
Iteration 62/1000 | Loss: 0.00002857
Iteration 63/1000 | Loss: 0.00002857
Iteration 64/1000 | Loss: 0.00002857
Iteration 65/1000 | Loss: 0.00002857
Iteration 66/1000 | Loss: 0.00002857
Iteration 67/1000 | Loss: 0.00002857
Iteration 68/1000 | Loss: 0.00002857
Iteration 69/1000 | Loss: 0.00002856
Iteration 70/1000 | Loss: 0.00002856
Iteration 71/1000 | Loss: 0.00002855
Iteration 72/1000 | Loss: 0.00002855
Iteration 73/1000 | Loss: 0.00002854
Iteration 74/1000 | Loss: 0.00002854
Iteration 75/1000 | Loss: 0.00002854
Iteration 76/1000 | Loss: 0.00002854
Iteration 77/1000 | Loss: 0.00002854
Iteration 78/1000 | Loss: 0.00002854
Iteration 79/1000 | Loss: 0.00002853
Iteration 80/1000 | Loss: 0.00002853
Iteration 81/1000 | Loss: 0.00002853
Iteration 82/1000 | Loss: 0.00002853
Iteration 83/1000 | Loss: 0.00002853
Iteration 84/1000 | Loss: 0.00002853
Iteration 85/1000 | Loss: 0.00002853
Iteration 86/1000 | Loss: 0.00002853
Iteration 87/1000 | Loss: 0.00002853
Iteration 88/1000 | Loss: 0.00002853
Iteration 89/1000 | Loss: 0.00002853
Iteration 90/1000 | Loss: 0.00002852
Iteration 91/1000 | Loss: 0.00002852
Iteration 92/1000 | Loss: 0.00002852
Iteration 93/1000 | Loss: 0.00002852
Iteration 94/1000 | Loss: 0.00002851
Iteration 95/1000 | Loss: 0.00002851
Iteration 96/1000 | Loss: 0.00002851
Iteration 97/1000 | Loss: 0.00002851
Iteration 98/1000 | Loss: 0.00002851
Iteration 99/1000 | Loss: 0.00002850
Iteration 100/1000 | Loss: 0.00002850
Iteration 101/1000 | Loss: 0.00002850
Iteration 102/1000 | Loss: 0.00002850
Iteration 103/1000 | Loss: 0.00002850
Iteration 104/1000 | Loss: 0.00002850
Iteration 105/1000 | Loss: 0.00002849
Iteration 106/1000 | Loss: 0.00002849
Iteration 107/1000 | Loss: 0.00002849
Iteration 108/1000 | Loss: 0.00002849
Iteration 109/1000 | Loss: 0.00002849
Iteration 110/1000 | Loss: 0.00002849
Iteration 111/1000 | Loss: 0.00002848
Iteration 112/1000 | Loss: 0.00002848
Iteration 113/1000 | Loss: 0.00002848
Iteration 114/1000 | Loss: 0.00002848
Iteration 115/1000 | Loss: 0.00002848
Iteration 116/1000 | Loss: 0.00002847
Iteration 117/1000 | Loss: 0.00002847
Iteration 118/1000 | Loss: 0.00002847
Iteration 119/1000 | Loss: 0.00002847
Iteration 120/1000 | Loss: 0.00002847
Iteration 121/1000 | Loss: 0.00002847
Iteration 122/1000 | Loss: 0.00002847
Iteration 123/1000 | Loss: 0.00002847
Iteration 124/1000 | Loss: 0.00002847
Iteration 125/1000 | Loss: 0.00002847
Iteration 126/1000 | Loss: 0.00002847
Iteration 127/1000 | Loss: 0.00002847
Iteration 128/1000 | Loss: 0.00002847
Iteration 129/1000 | Loss: 0.00002847
Iteration 130/1000 | Loss: 0.00002846
Iteration 131/1000 | Loss: 0.00002846
Iteration 132/1000 | Loss: 0.00002846
Iteration 133/1000 | Loss: 0.00002846
Iteration 134/1000 | Loss: 0.00002846
Iteration 135/1000 | Loss: 0.00002846
Iteration 136/1000 | Loss: 0.00002846
Iteration 137/1000 | Loss: 0.00002846
Iteration 138/1000 | Loss: 0.00002846
Iteration 139/1000 | Loss: 0.00002846
Iteration 140/1000 | Loss: 0.00002846
Iteration 141/1000 | Loss: 0.00002846
Iteration 142/1000 | Loss: 0.00002846
Iteration 143/1000 | Loss: 0.00002846
Iteration 144/1000 | Loss: 0.00002846
Iteration 145/1000 | Loss: 0.00002846
Iteration 146/1000 | Loss: 0.00002846
Iteration 147/1000 | Loss: 0.00002846
Iteration 148/1000 | Loss: 0.00002846
Iteration 149/1000 | Loss: 0.00002846
Iteration 150/1000 | Loss: 0.00002846
Iteration 151/1000 | Loss: 0.00002846
Iteration 152/1000 | Loss: 0.00002846
Iteration 153/1000 | Loss: 0.00002846
Iteration 154/1000 | Loss: 0.00002846
Iteration 155/1000 | Loss: 0.00002846
Iteration 156/1000 | Loss: 0.00002846
Iteration 157/1000 | Loss: 0.00002846
Iteration 158/1000 | Loss: 0.00002846
Iteration 159/1000 | Loss: 0.00002846
Iteration 160/1000 | Loss: 0.00002846
Iteration 161/1000 | Loss: 0.00002846
Iteration 162/1000 | Loss: 0.00002846
Iteration 163/1000 | Loss: 0.00002846
Iteration 164/1000 | Loss: 0.00002846
Iteration 165/1000 | Loss: 0.00002846
Iteration 166/1000 | Loss: 0.00002846
Iteration 167/1000 | Loss: 0.00002846
Iteration 168/1000 | Loss: 0.00002846
Iteration 169/1000 | Loss: 0.00002846
Iteration 170/1000 | Loss: 0.00002846
Iteration 171/1000 | Loss: 0.00002846
Iteration 172/1000 | Loss: 0.00002846
Iteration 173/1000 | Loss: 0.00002846
Iteration 174/1000 | Loss: 0.00002846
Iteration 175/1000 | Loss: 0.00002846
Iteration 176/1000 | Loss: 0.00002846
Iteration 177/1000 | Loss: 0.00002846
Iteration 178/1000 | Loss: 0.00002846
Iteration 179/1000 | Loss: 0.00002846
Iteration 180/1000 | Loss: 0.00002846
Iteration 181/1000 | Loss: 0.00002846
Iteration 182/1000 | Loss: 0.00002846
Iteration 183/1000 | Loss: 0.00002846
Iteration 184/1000 | Loss: 0.00002846
Iteration 185/1000 | Loss: 0.00002846
Iteration 186/1000 | Loss: 0.00002846
Iteration 187/1000 | Loss: 0.00002846
Iteration 188/1000 | Loss: 0.00002846
Iteration 189/1000 | Loss: 0.00002846
Iteration 190/1000 | Loss: 0.00002846
Iteration 191/1000 | Loss: 0.00002846
Iteration 192/1000 | Loss: 0.00002846
Iteration 193/1000 | Loss: 0.00002846
Iteration 194/1000 | Loss: 0.00002846
Iteration 195/1000 | Loss: 0.00002846
Iteration 196/1000 | Loss: 0.00002846
Iteration 197/1000 | Loss: 0.00002846
Iteration 198/1000 | Loss: 0.00002846
Iteration 199/1000 | Loss: 0.00002846
Iteration 200/1000 | Loss: 0.00002846
Iteration 201/1000 | Loss: 0.00002846
Iteration 202/1000 | Loss: 0.00002846
Iteration 203/1000 | Loss: 0.00002846
Iteration 204/1000 | Loss: 0.00002846
Iteration 205/1000 | Loss: 0.00002846
Iteration 206/1000 | Loss: 0.00002846
Iteration 207/1000 | Loss: 0.00002846
Iteration 208/1000 | Loss: 0.00002846
Iteration 209/1000 | Loss: 0.00002846
Iteration 210/1000 | Loss: 0.00002846
Iteration 211/1000 | Loss: 0.00002846
Iteration 212/1000 | Loss: 0.00002846
Iteration 213/1000 | Loss: 0.00002846
Iteration 214/1000 | Loss: 0.00002846
Iteration 215/1000 | Loss: 0.00002846
Iteration 216/1000 | Loss: 0.00002846
Iteration 217/1000 | Loss: 0.00002846
Iteration 218/1000 | Loss: 0.00002846
Iteration 219/1000 | Loss: 0.00002846
Iteration 220/1000 | Loss: 0.00002846
Iteration 221/1000 | Loss: 0.00002846
Iteration 222/1000 | Loss: 0.00002846
Iteration 223/1000 | Loss: 0.00002846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.8463577109505422e-05, 2.8463577109505422e-05, 2.8463577109505422e-05, 2.8463577109505422e-05, 2.8463577109505422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8463577109505422e-05

Optimization complete. Final v2v error: 4.618343830108643 mm

Highest mean error: 4.954565525054932 mm for frame 79

Lowest mean error: 4.281349182128906 mm for frame 8

Saving results

Total time: 46.645390033721924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383640
Iteration 2/25 | Loss: 0.00131274
Iteration 3/25 | Loss: 0.00125387
Iteration 4/25 | Loss: 0.00124818
Iteration 5/25 | Loss: 0.00124678
Iteration 6/25 | Loss: 0.00124627
Iteration 7/25 | Loss: 0.00124627
Iteration 8/25 | Loss: 0.00124627
Iteration 9/25 | Loss: 0.00124627
Iteration 10/25 | Loss: 0.00124627
Iteration 11/25 | Loss: 0.00124627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012462742161005735, 0.0012462742161005735, 0.0012462742161005735, 0.0012462742161005735, 0.0012462742161005735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012462742161005735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18202984
Iteration 2/25 | Loss: 0.00342385
Iteration 3/25 | Loss: 0.00342385
Iteration 4/25 | Loss: 0.00342385
Iteration 5/25 | Loss: 0.00342385
Iteration 6/25 | Loss: 0.00342385
Iteration 7/25 | Loss: 0.00342385
Iteration 8/25 | Loss: 0.00342385
Iteration 9/25 | Loss: 0.00342385
Iteration 10/25 | Loss: 0.00342385
Iteration 11/25 | Loss: 0.00342385
Iteration 12/25 | Loss: 0.00342385
Iteration 13/25 | Loss: 0.00342385
Iteration 14/25 | Loss: 0.00342385
Iteration 15/25 | Loss: 0.00342385
Iteration 16/25 | Loss: 0.00342385
Iteration 17/25 | Loss: 0.00342385
Iteration 18/25 | Loss: 0.00342385
Iteration 19/25 | Loss: 0.00342385
Iteration 20/25 | Loss: 0.00342385
Iteration 21/25 | Loss: 0.00342385
Iteration 22/25 | Loss: 0.00342385
Iteration 23/25 | Loss: 0.00342385
Iteration 24/25 | Loss: 0.00342385
Iteration 25/25 | Loss: 0.00342385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342385
Iteration 2/1000 | Loss: 0.00003690
Iteration 3/1000 | Loss: 0.00001958
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001467
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001336
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001266
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001255
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001218
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001216
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001209
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001204
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001204
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001204
Iteration 141/1000 | Loss: 0.00001204
Iteration 142/1000 | Loss: 0.00001204
Iteration 143/1000 | Loss: 0.00001204
Iteration 144/1000 | Loss: 0.00001204
Iteration 145/1000 | Loss: 0.00001204
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001204
Iteration 150/1000 | Loss: 0.00001204
Iteration 151/1000 | Loss: 0.00001204
Iteration 152/1000 | Loss: 0.00001204
Iteration 153/1000 | Loss: 0.00001204
Iteration 154/1000 | Loss: 0.00001204
Iteration 155/1000 | Loss: 0.00001204
Iteration 156/1000 | Loss: 0.00001204
Iteration 157/1000 | Loss: 0.00001204
Iteration 158/1000 | Loss: 0.00001204
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001204
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001204
Iteration 169/1000 | Loss: 0.00001204
Iteration 170/1000 | Loss: 0.00001204
Iteration 171/1000 | Loss: 0.00001204
Iteration 172/1000 | Loss: 0.00001204
Iteration 173/1000 | Loss: 0.00001204
Iteration 174/1000 | Loss: 0.00001204
Iteration 175/1000 | Loss: 0.00001204
Iteration 176/1000 | Loss: 0.00001204
Iteration 177/1000 | Loss: 0.00001204
Iteration 178/1000 | Loss: 0.00001204
Iteration 179/1000 | Loss: 0.00001204
Iteration 180/1000 | Loss: 0.00001204
Iteration 181/1000 | Loss: 0.00001204
Iteration 182/1000 | Loss: 0.00001204
Iteration 183/1000 | Loss: 0.00001204
Iteration 184/1000 | Loss: 0.00001204
Iteration 185/1000 | Loss: 0.00001204
Iteration 186/1000 | Loss: 0.00001204
Iteration 187/1000 | Loss: 0.00001204
Iteration 188/1000 | Loss: 0.00001204
Iteration 189/1000 | Loss: 0.00001204
Iteration 190/1000 | Loss: 0.00001204
Iteration 191/1000 | Loss: 0.00001204
Iteration 192/1000 | Loss: 0.00001204
Iteration 193/1000 | Loss: 0.00001204
Iteration 194/1000 | Loss: 0.00001204
Iteration 195/1000 | Loss: 0.00001204
Iteration 196/1000 | Loss: 0.00001204
Iteration 197/1000 | Loss: 0.00001204
Iteration 198/1000 | Loss: 0.00001204
Iteration 199/1000 | Loss: 0.00001204
Iteration 200/1000 | Loss: 0.00001204
Iteration 201/1000 | Loss: 0.00001204
Iteration 202/1000 | Loss: 0.00001204
Iteration 203/1000 | Loss: 0.00001204
Iteration 204/1000 | Loss: 0.00001204
Iteration 205/1000 | Loss: 0.00001204
Iteration 206/1000 | Loss: 0.00001204
Iteration 207/1000 | Loss: 0.00001204
Iteration 208/1000 | Loss: 0.00001204
Iteration 209/1000 | Loss: 0.00001204
Iteration 210/1000 | Loss: 0.00001204
Iteration 211/1000 | Loss: 0.00001204
Iteration 212/1000 | Loss: 0.00001204
Iteration 213/1000 | Loss: 0.00001204
Iteration 214/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2036151929351036e-05, 1.2036151929351036e-05, 1.2036151929351036e-05, 1.2036151929351036e-05, 1.2036151929351036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2036151929351036e-05

Optimization complete. Final v2v error: 2.9938786029815674 mm

Highest mean error: 3.275766134262085 mm for frame 75

Lowest mean error: 2.66623854637146 mm for frame 3

Saving results

Total time: 37.16471076011658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00661747
Iteration 2/25 | Loss: 0.00185320
Iteration 3/25 | Loss: 0.00138581
Iteration 4/25 | Loss: 0.00136021
Iteration 5/25 | Loss: 0.00135808
Iteration 6/25 | Loss: 0.00135808
Iteration 7/25 | Loss: 0.00135808
Iteration 8/25 | Loss: 0.00135808
Iteration 9/25 | Loss: 0.00135808
Iteration 10/25 | Loss: 0.00135808
Iteration 11/25 | Loss: 0.00135808
Iteration 12/25 | Loss: 0.00135808
Iteration 13/25 | Loss: 0.00135808
Iteration 14/25 | Loss: 0.00135808
Iteration 15/25 | Loss: 0.00135808
Iteration 16/25 | Loss: 0.00135808
Iteration 17/25 | Loss: 0.00135808
Iteration 18/25 | Loss: 0.00135808
Iteration 19/25 | Loss: 0.00135808
Iteration 20/25 | Loss: 0.00135808
Iteration 21/25 | Loss: 0.00135808
Iteration 22/25 | Loss: 0.00135808
Iteration 23/25 | Loss: 0.00135808
Iteration 24/25 | Loss: 0.00135808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013580787926912308, 0.0013580787926912308, 0.0013580787926912308, 0.0013580787926912308, 0.0013580787926912308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013580787926912308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20533633
Iteration 2/25 | Loss: 0.00221102
Iteration 3/25 | Loss: 0.00221100
Iteration 4/25 | Loss: 0.00221100
Iteration 5/25 | Loss: 0.00221100
Iteration 6/25 | Loss: 0.00221100
Iteration 7/25 | Loss: 0.00221100
Iteration 8/25 | Loss: 0.00221100
Iteration 9/25 | Loss: 0.00221100
Iteration 10/25 | Loss: 0.00221100
Iteration 11/25 | Loss: 0.00221100
Iteration 12/25 | Loss: 0.00221100
Iteration 13/25 | Loss: 0.00221100
Iteration 14/25 | Loss: 0.00221100
Iteration 15/25 | Loss: 0.00221100
Iteration 16/25 | Loss: 0.00221100
Iteration 17/25 | Loss: 0.00221100
Iteration 18/25 | Loss: 0.00221100
Iteration 19/25 | Loss: 0.00221100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0022109977435320616, 0.0022109977435320616, 0.0022109977435320616, 0.0022109977435320616, 0.0022109977435320616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022109977435320616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221100
Iteration 2/1000 | Loss: 0.00006780
Iteration 3/1000 | Loss: 0.00004332
Iteration 4/1000 | Loss: 0.00003381
Iteration 5/1000 | Loss: 0.00003061
Iteration 6/1000 | Loss: 0.00002915
Iteration 7/1000 | Loss: 0.00002832
Iteration 8/1000 | Loss: 0.00002787
Iteration 9/1000 | Loss: 0.00002751
Iteration 10/1000 | Loss: 0.00002728
Iteration 11/1000 | Loss: 0.00002712
Iteration 12/1000 | Loss: 0.00002696
Iteration 13/1000 | Loss: 0.00002686
Iteration 14/1000 | Loss: 0.00002680
Iteration 15/1000 | Loss: 0.00002680
Iteration 16/1000 | Loss: 0.00002679
Iteration 17/1000 | Loss: 0.00002679
Iteration 18/1000 | Loss: 0.00002678
Iteration 19/1000 | Loss: 0.00002678
Iteration 20/1000 | Loss: 0.00002676
Iteration 21/1000 | Loss: 0.00002672
Iteration 22/1000 | Loss: 0.00002671
Iteration 23/1000 | Loss: 0.00002671
Iteration 24/1000 | Loss: 0.00002668
Iteration 25/1000 | Loss: 0.00002666
Iteration 26/1000 | Loss: 0.00002665
Iteration 27/1000 | Loss: 0.00002665
Iteration 28/1000 | Loss: 0.00002664
Iteration 29/1000 | Loss: 0.00002664
Iteration 30/1000 | Loss: 0.00002663
Iteration 31/1000 | Loss: 0.00002662
Iteration 32/1000 | Loss: 0.00002661
Iteration 33/1000 | Loss: 0.00002661
Iteration 34/1000 | Loss: 0.00002660
Iteration 35/1000 | Loss: 0.00002655
Iteration 36/1000 | Loss: 0.00002655
Iteration 37/1000 | Loss: 0.00002655
Iteration 38/1000 | Loss: 0.00002655
Iteration 39/1000 | Loss: 0.00002655
Iteration 40/1000 | Loss: 0.00002655
Iteration 41/1000 | Loss: 0.00002655
Iteration 42/1000 | Loss: 0.00002654
Iteration 43/1000 | Loss: 0.00002654
Iteration 44/1000 | Loss: 0.00002654
Iteration 45/1000 | Loss: 0.00002654
Iteration 46/1000 | Loss: 0.00002653
Iteration 47/1000 | Loss: 0.00002653
Iteration 48/1000 | Loss: 0.00002653
Iteration 49/1000 | Loss: 0.00002652
Iteration 50/1000 | Loss: 0.00002651
Iteration 51/1000 | Loss: 0.00002650
Iteration 52/1000 | Loss: 0.00002650
Iteration 53/1000 | Loss: 0.00002649
Iteration 54/1000 | Loss: 0.00002649
Iteration 55/1000 | Loss: 0.00002649
Iteration 56/1000 | Loss: 0.00002649
Iteration 57/1000 | Loss: 0.00002649
Iteration 58/1000 | Loss: 0.00002649
Iteration 59/1000 | Loss: 0.00002649
Iteration 60/1000 | Loss: 0.00002649
Iteration 61/1000 | Loss: 0.00002648
Iteration 62/1000 | Loss: 0.00002647
Iteration 63/1000 | Loss: 0.00002643
Iteration 64/1000 | Loss: 0.00002643
Iteration 65/1000 | Loss: 0.00002641
Iteration 66/1000 | Loss: 0.00002641
Iteration 67/1000 | Loss: 0.00002640
Iteration 68/1000 | Loss: 0.00002640
Iteration 69/1000 | Loss: 0.00002639
Iteration 70/1000 | Loss: 0.00002639
Iteration 71/1000 | Loss: 0.00002638
Iteration 72/1000 | Loss: 0.00002638
Iteration 73/1000 | Loss: 0.00002637
Iteration 74/1000 | Loss: 0.00002637
Iteration 75/1000 | Loss: 0.00002637
Iteration 76/1000 | Loss: 0.00002637
Iteration 77/1000 | Loss: 0.00002637
Iteration 78/1000 | Loss: 0.00002637
Iteration 79/1000 | Loss: 0.00002637
Iteration 80/1000 | Loss: 0.00002637
Iteration 81/1000 | Loss: 0.00002637
Iteration 82/1000 | Loss: 0.00002637
Iteration 83/1000 | Loss: 0.00002637
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002636
Iteration 91/1000 | Loss: 0.00002635
Iteration 92/1000 | Loss: 0.00002634
Iteration 93/1000 | Loss: 0.00002634
Iteration 94/1000 | Loss: 0.00002634
Iteration 95/1000 | Loss: 0.00002633
Iteration 96/1000 | Loss: 0.00002633
Iteration 97/1000 | Loss: 0.00002633
Iteration 98/1000 | Loss: 0.00002633
Iteration 99/1000 | Loss: 0.00002633
Iteration 100/1000 | Loss: 0.00002633
Iteration 101/1000 | Loss: 0.00002633
Iteration 102/1000 | Loss: 0.00002633
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002631
Iteration 105/1000 | Loss: 0.00002631
Iteration 106/1000 | Loss: 0.00002630
Iteration 107/1000 | Loss: 0.00002630
Iteration 108/1000 | Loss: 0.00002630
Iteration 109/1000 | Loss: 0.00002630
Iteration 110/1000 | Loss: 0.00002630
Iteration 111/1000 | Loss: 0.00002629
Iteration 112/1000 | Loss: 0.00002629
Iteration 113/1000 | Loss: 0.00002629
Iteration 114/1000 | Loss: 0.00002629
Iteration 115/1000 | Loss: 0.00002627
Iteration 116/1000 | Loss: 0.00002627
Iteration 117/1000 | Loss: 0.00002627
Iteration 118/1000 | Loss: 0.00002627
Iteration 119/1000 | Loss: 0.00002627
Iteration 120/1000 | Loss: 0.00002626
Iteration 121/1000 | Loss: 0.00002626
Iteration 122/1000 | Loss: 0.00002626
Iteration 123/1000 | Loss: 0.00002626
Iteration 124/1000 | Loss: 0.00002625
Iteration 125/1000 | Loss: 0.00002625
Iteration 126/1000 | Loss: 0.00002625
Iteration 127/1000 | Loss: 0.00002625
Iteration 128/1000 | Loss: 0.00002625
Iteration 129/1000 | Loss: 0.00002625
Iteration 130/1000 | Loss: 0.00002625
Iteration 131/1000 | Loss: 0.00002625
Iteration 132/1000 | Loss: 0.00002625
Iteration 133/1000 | Loss: 0.00002625
Iteration 134/1000 | Loss: 0.00002625
Iteration 135/1000 | Loss: 0.00002625
Iteration 136/1000 | Loss: 0.00002624
Iteration 137/1000 | Loss: 0.00002624
Iteration 138/1000 | Loss: 0.00002623
Iteration 139/1000 | Loss: 0.00002623
Iteration 140/1000 | Loss: 0.00002623
Iteration 141/1000 | Loss: 0.00002623
Iteration 142/1000 | Loss: 0.00002623
Iteration 143/1000 | Loss: 0.00002623
Iteration 144/1000 | Loss: 0.00002623
Iteration 145/1000 | Loss: 0.00002623
Iteration 146/1000 | Loss: 0.00002622
Iteration 147/1000 | Loss: 0.00002622
Iteration 148/1000 | Loss: 0.00002622
Iteration 149/1000 | Loss: 0.00002622
Iteration 150/1000 | Loss: 0.00002622
Iteration 151/1000 | Loss: 0.00002622
Iteration 152/1000 | Loss: 0.00002622
Iteration 153/1000 | Loss: 0.00002621
Iteration 154/1000 | Loss: 0.00002621
Iteration 155/1000 | Loss: 0.00002621
Iteration 156/1000 | Loss: 0.00002621
Iteration 157/1000 | Loss: 0.00002621
Iteration 158/1000 | Loss: 0.00002621
Iteration 159/1000 | Loss: 0.00002621
Iteration 160/1000 | Loss: 0.00002621
Iteration 161/1000 | Loss: 0.00002621
Iteration 162/1000 | Loss: 0.00002621
Iteration 163/1000 | Loss: 0.00002621
Iteration 164/1000 | Loss: 0.00002621
Iteration 165/1000 | Loss: 0.00002620
Iteration 166/1000 | Loss: 0.00002620
Iteration 167/1000 | Loss: 0.00002620
Iteration 168/1000 | Loss: 0.00002620
Iteration 169/1000 | Loss: 0.00002620
Iteration 170/1000 | Loss: 0.00002620
Iteration 171/1000 | Loss: 0.00002620
Iteration 172/1000 | Loss: 0.00002619
Iteration 173/1000 | Loss: 0.00002619
Iteration 174/1000 | Loss: 0.00002619
Iteration 175/1000 | Loss: 0.00002619
Iteration 176/1000 | Loss: 0.00002619
Iteration 177/1000 | Loss: 0.00002619
Iteration 178/1000 | Loss: 0.00002619
Iteration 179/1000 | Loss: 0.00002618
Iteration 180/1000 | Loss: 0.00002618
Iteration 181/1000 | Loss: 0.00002618
Iteration 182/1000 | Loss: 0.00002618
Iteration 183/1000 | Loss: 0.00002617
Iteration 184/1000 | Loss: 0.00002617
Iteration 185/1000 | Loss: 0.00002617
Iteration 186/1000 | Loss: 0.00002617
Iteration 187/1000 | Loss: 0.00002617
Iteration 188/1000 | Loss: 0.00002616
Iteration 189/1000 | Loss: 0.00002616
Iteration 190/1000 | Loss: 0.00002616
Iteration 191/1000 | Loss: 0.00002616
Iteration 192/1000 | Loss: 0.00002616
Iteration 193/1000 | Loss: 0.00002616
Iteration 194/1000 | Loss: 0.00002616
Iteration 195/1000 | Loss: 0.00002616
Iteration 196/1000 | Loss: 0.00002616
Iteration 197/1000 | Loss: 0.00002616
Iteration 198/1000 | Loss: 0.00002616
Iteration 199/1000 | Loss: 0.00002616
Iteration 200/1000 | Loss: 0.00002616
Iteration 201/1000 | Loss: 0.00002616
Iteration 202/1000 | Loss: 0.00002616
Iteration 203/1000 | Loss: 0.00002615
Iteration 204/1000 | Loss: 0.00002615
Iteration 205/1000 | Loss: 0.00002615
Iteration 206/1000 | Loss: 0.00002615
Iteration 207/1000 | Loss: 0.00002615
Iteration 208/1000 | Loss: 0.00002615
Iteration 209/1000 | Loss: 0.00002615
Iteration 210/1000 | Loss: 0.00002615
Iteration 211/1000 | Loss: 0.00002615
Iteration 212/1000 | Loss: 0.00002615
Iteration 213/1000 | Loss: 0.00002615
Iteration 214/1000 | Loss: 0.00002615
Iteration 215/1000 | Loss: 0.00002615
Iteration 216/1000 | Loss: 0.00002615
Iteration 217/1000 | Loss: 0.00002615
Iteration 218/1000 | Loss: 0.00002615
Iteration 219/1000 | Loss: 0.00002615
Iteration 220/1000 | Loss: 0.00002615
Iteration 221/1000 | Loss: 0.00002615
Iteration 222/1000 | Loss: 0.00002615
Iteration 223/1000 | Loss: 0.00002615
Iteration 224/1000 | Loss: 0.00002615
Iteration 225/1000 | Loss: 0.00002615
Iteration 226/1000 | Loss: 0.00002615
Iteration 227/1000 | Loss: 0.00002615
Iteration 228/1000 | Loss: 0.00002615
Iteration 229/1000 | Loss: 0.00002615
Iteration 230/1000 | Loss: 0.00002615
Iteration 231/1000 | Loss: 0.00002615
Iteration 232/1000 | Loss: 0.00002615
Iteration 233/1000 | Loss: 0.00002615
Iteration 234/1000 | Loss: 0.00002615
Iteration 235/1000 | Loss: 0.00002615
Iteration 236/1000 | Loss: 0.00002615
Iteration 237/1000 | Loss: 0.00002615
Iteration 238/1000 | Loss: 0.00002615
Iteration 239/1000 | Loss: 0.00002615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.614626646391116e-05, 2.614626646391116e-05, 2.614626646391116e-05, 2.614626646391116e-05, 2.614626646391116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.614626646391116e-05

Optimization complete. Final v2v error: 4.148551940917969 mm

Highest mean error: 4.570006847381592 mm for frame 64

Lowest mean error: 3.583489418029785 mm for frame 176

Saving results

Total time: 46.293933391571045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410329
Iteration 2/25 | Loss: 0.00148780
Iteration 3/25 | Loss: 0.00127558
Iteration 4/25 | Loss: 0.00125244
Iteration 5/25 | Loss: 0.00124905
Iteration 6/25 | Loss: 0.00124841
Iteration 7/25 | Loss: 0.00124841
Iteration 8/25 | Loss: 0.00124841
Iteration 9/25 | Loss: 0.00124841
Iteration 10/25 | Loss: 0.00124841
Iteration 11/25 | Loss: 0.00124841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012484125327318907, 0.0012484125327318907, 0.0012484125327318907, 0.0012484125327318907, 0.0012484125327318907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012484125327318907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14564908
Iteration 2/25 | Loss: 0.00288081
Iteration 3/25 | Loss: 0.00288081
Iteration 4/25 | Loss: 0.00288080
Iteration 5/25 | Loss: 0.00288080
Iteration 6/25 | Loss: 0.00288080
Iteration 7/25 | Loss: 0.00288080
Iteration 8/25 | Loss: 0.00288080
Iteration 9/25 | Loss: 0.00288080
Iteration 10/25 | Loss: 0.00288080
Iteration 11/25 | Loss: 0.00288080
Iteration 12/25 | Loss: 0.00288080
Iteration 13/25 | Loss: 0.00288080
Iteration 14/25 | Loss: 0.00288080
Iteration 15/25 | Loss: 0.00288080
Iteration 16/25 | Loss: 0.00288080
Iteration 17/25 | Loss: 0.00288080
Iteration 18/25 | Loss: 0.00288080
Iteration 19/25 | Loss: 0.00288080
Iteration 20/25 | Loss: 0.00288080
Iteration 21/25 | Loss: 0.00288080
Iteration 22/25 | Loss: 0.00288080
Iteration 23/25 | Loss: 0.00288080
Iteration 24/25 | Loss: 0.00288080
Iteration 25/25 | Loss: 0.00288080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288080
Iteration 2/1000 | Loss: 0.00003189
Iteration 3/1000 | Loss: 0.00002034
Iteration 4/1000 | Loss: 0.00001757
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001324
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001316
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001316
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001274
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001264
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001261
Iteration 55/1000 | Loss: 0.00001260
Iteration 56/1000 | Loss: 0.00001260
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001251
Iteration 88/1000 | Loss: 0.00001251
Iteration 89/1000 | Loss: 0.00001251
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001251
Iteration 101/1000 | Loss: 0.00001251
Iteration 102/1000 | Loss: 0.00001251
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001251
Iteration 107/1000 | Loss: 0.00001251
Iteration 108/1000 | Loss: 0.00001251
Iteration 109/1000 | Loss: 0.00001251
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.2510116903285962e-05, 1.2510116903285962e-05, 1.2510116903285962e-05, 1.2510116903285962e-05, 1.2510116903285962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2510116903285962e-05

Optimization complete. Final v2v error: 3.099247694015503 mm

Highest mean error: 3.335825204849243 mm for frame 23

Lowest mean error: 2.842663049697876 mm for frame 8

Saving results

Total time: 35.91474461555481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862714
Iteration 2/25 | Loss: 0.00180586
Iteration 3/25 | Loss: 0.00144490
Iteration 4/25 | Loss: 0.00142947
Iteration 5/25 | Loss: 0.00142538
Iteration 6/25 | Loss: 0.00142459
Iteration 7/25 | Loss: 0.00142459
Iteration 8/25 | Loss: 0.00142459
Iteration 9/25 | Loss: 0.00142459
Iteration 10/25 | Loss: 0.00142459
Iteration 11/25 | Loss: 0.00142459
Iteration 12/25 | Loss: 0.00142459
Iteration 13/25 | Loss: 0.00142459
Iteration 14/25 | Loss: 0.00142459
Iteration 15/25 | Loss: 0.00142459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014245854690670967, 0.0014245854690670967, 0.0014245854690670967, 0.0014245854690670967, 0.0014245854690670967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014245854690670967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.46425277
Iteration 2/25 | Loss: 0.00173532
Iteration 3/25 | Loss: 0.00173531
Iteration 4/25 | Loss: 0.00173531
Iteration 5/25 | Loss: 0.00173531
Iteration 6/25 | Loss: 0.00173531
Iteration 7/25 | Loss: 0.00173531
Iteration 8/25 | Loss: 0.00173531
Iteration 9/25 | Loss: 0.00173531
Iteration 10/25 | Loss: 0.00173531
Iteration 11/25 | Loss: 0.00173531
Iteration 12/25 | Loss: 0.00173531
Iteration 13/25 | Loss: 0.00173531
Iteration 14/25 | Loss: 0.00173531
Iteration 15/25 | Loss: 0.00173531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017353108851239085, 0.0017353108851239085, 0.0017353108851239085, 0.0017353108851239085, 0.0017353108851239085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017353108851239085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173531
Iteration 2/1000 | Loss: 0.00014018
Iteration 3/1000 | Loss: 0.00009620
Iteration 4/1000 | Loss: 0.00008031
Iteration 5/1000 | Loss: 0.00007028
Iteration 6/1000 | Loss: 0.00006707
Iteration 7/1000 | Loss: 0.00006562
Iteration 8/1000 | Loss: 0.00006433
Iteration 9/1000 | Loss: 0.00006334
Iteration 10/1000 | Loss: 0.00006228
Iteration 11/1000 | Loss: 0.00006144
Iteration 12/1000 | Loss: 0.00006054
Iteration 13/1000 | Loss: 0.00005994
Iteration 14/1000 | Loss: 0.00005905
Iteration 15/1000 | Loss: 0.00005816
Iteration 16/1000 | Loss: 0.00005760
Iteration 17/1000 | Loss: 0.00005708
Iteration 18/1000 | Loss: 0.00005664
Iteration 19/1000 | Loss: 0.00005622
Iteration 20/1000 | Loss: 0.00005598
Iteration 21/1000 | Loss: 0.00005576
Iteration 22/1000 | Loss: 0.00005564
Iteration 23/1000 | Loss: 0.00005545
Iteration 24/1000 | Loss: 0.00005525
Iteration 25/1000 | Loss: 0.00005516
Iteration 26/1000 | Loss: 0.00005515
Iteration 27/1000 | Loss: 0.00005503
Iteration 28/1000 | Loss: 0.00005501
Iteration 29/1000 | Loss: 0.00005500
Iteration 30/1000 | Loss: 0.00005500
Iteration 31/1000 | Loss: 0.00005500
Iteration 32/1000 | Loss: 0.00005500
Iteration 33/1000 | Loss: 0.00005500
Iteration 34/1000 | Loss: 0.00005500
Iteration 35/1000 | Loss: 0.00005500
Iteration 36/1000 | Loss: 0.00005500
Iteration 37/1000 | Loss: 0.00005499
Iteration 38/1000 | Loss: 0.00005499
Iteration 39/1000 | Loss: 0.00005497
Iteration 40/1000 | Loss: 0.00005497
Iteration 41/1000 | Loss: 0.00005497
Iteration 42/1000 | Loss: 0.00005497
Iteration 43/1000 | Loss: 0.00005496
Iteration 44/1000 | Loss: 0.00005496
Iteration 45/1000 | Loss: 0.00005496
Iteration 46/1000 | Loss: 0.00005496
Iteration 47/1000 | Loss: 0.00005496
Iteration 48/1000 | Loss: 0.00005496
Iteration 49/1000 | Loss: 0.00005496
Iteration 50/1000 | Loss: 0.00005496
Iteration 51/1000 | Loss: 0.00005496
Iteration 52/1000 | Loss: 0.00005495
Iteration 53/1000 | Loss: 0.00005495
Iteration 54/1000 | Loss: 0.00005495
Iteration 55/1000 | Loss: 0.00005495
Iteration 56/1000 | Loss: 0.00005495
Iteration 57/1000 | Loss: 0.00005493
Iteration 58/1000 | Loss: 0.00005493
Iteration 59/1000 | Loss: 0.00005489
Iteration 60/1000 | Loss: 0.00005489
Iteration 61/1000 | Loss: 0.00005488
Iteration 62/1000 | Loss: 0.00005487
Iteration 63/1000 | Loss: 0.00005486
Iteration 64/1000 | Loss: 0.00005486
Iteration 65/1000 | Loss: 0.00005486
Iteration 66/1000 | Loss: 0.00005486
Iteration 67/1000 | Loss: 0.00005486
Iteration 68/1000 | Loss: 0.00005485
Iteration 69/1000 | Loss: 0.00005485
Iteration 70/1000 | Loss: 0.00005485
Iteration 71/1000 | Loss: 0.00005485
Iteration 72/1000 | Loss: 0.00005485
Iteration 73/1000 | Loss: 0.00005485
Iteration 74/1000 | Loss: 0.00005485
Iteration 75/1000 | Loss: 0.00005485
Iteration 76/1000 | Loss: 0.00005485
Iteration 77/1000 | Loss: 0.00005485
Iteration 78/1000 | Loss: 0.00005485
Iteration 79/1000 | Loss: 0.00005485
Iteration 80/1000 | Loss: 0.00005485
Iteration 81/1000 | Loss: 0.00005485
Iteration 82/1000 | Loss: 0.00005485
Iteration 83/1000 | Loss: 0.00005485
Iteration 84/1000 | Loss: 0.00005485
Iteration 85/1000 | Loss: 0.00005485
Iteration 86/1000 | Loss: 0.00005485
Iteration 87/1000 | Loss: 0.00005485
Iteration 88/1000 | Loss: 0.00005485
Iteration 89/1000 | Loss: 0.00005485
Iteration 90/1000 | Loss: 0.00005485
Iteration 91/1000 | Loss: 0.00005485
Iteration 92/1000 | Loss: 0.00005485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [5.485035217134282e-05, 5.485035217134282e-05, 5.485035217134282e-05, 5.485035217134282e-05, 5.485035217134282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.485035217134282e-05

Optimization complete. Final v2v error: 5.9420270919799805 mm

Highest mean error: 7.346565246582031 mm for frame 143

Lowest mean error: 5.618088245391846 mm for frame 53

Saving results

Total time: 51.02086615562439
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870564
Iteration 2/25 | Loss: 0.00145111
Iteration 3/25 | Loss: 0.00128856
Iteration 4/25 | Loss: 0.00127222
Iteration 5/25 | Loss: 0.00126802
Iteration 6/25 | Loss: 0.00126681
Iteration 7/25 | Loss: 0.00126681
Iteration 8/25 | Loss: 0.00126681
Iteration 9/25 | Loss: 0.00126681
Iteration 10/25 | Loss: 0.00126681
Iteration 11/25 | Loss: 0.00126681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012668119743466377, 0.0012668119743466377, 0.0012668119743466377, 0.0012668119743466377, 0.0012668119743466377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012668119743466377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57649302
Iteration 2/25 | Loss: 0.00305022
Iteration 3/25 | Loss: 0.00305022
Iteration 4/25 | Loss: 0.00305022
Iteration 5/25 | Loss: 0.00305022
Iteration 6/25 | Loss: 0.00305022
Iteration 7/25 | Loss: 0.00305022
Iteration 8/25 | Loss: 0.00305022
Iteration 9/25 | Loss: 0.00305022
Iteration 10/25 | Loss: 0.00305022
Iteration 11/25 | Loss: 0.00305022
Iteration 12/25 | Loss: 0.00305022
Iteration 13/25 | Loss: 0.00305022
Iteration 14/25 | Loss: 0.00305022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003050221363082528, 0.003050221363082528, 0.003050221363082528, 0.003050221363082528, 0.003050221363082528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003050221363082528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305022
Iteration 2/1000 | Loss: 0.00005270
Iteration 3/1000 | Loss: 0.00003199
Iteration 4/1000 | Loss: 0.00002559
Iteration 5/1000 | Loss: 0.00002308
Iteration 6/1000 | Loss: 0.00002179
Iteration 7/1000 | Loss: 0.00002103
Iteration 8/1000 | Loss: 0.00002030
Iteration 9/1000 | Loss: 0.00001976
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00001918
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001887
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001877
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001876
Iteration 28/1000 | Loss: 0.00001876
Iteration 29/1000 | Loss: 0.00001875
Iteration 30/1000 | Loss: 0.00001872
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001871
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001871
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001871
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001869
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001862
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001862
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001860
Iteration 80/1000 | Loss: 0.00001860
Iteration 81/1000 | Loss: 0.00001860
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001858
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001857
Iteration 100/1000 | Loss: 0.00001857
Iteration 101/1000 | Loss: 0.00001857
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001855
Iteration 112/1000 | Loss: 0.00001855
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001854
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.852370405686088e-05, 1.852370405686088e-05, 1.852370405686088e-05, 1.852370405686088e-05, 1.852370405686088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.852370405686088e-05

Optimization complete. Final v2v error: 3.4730734825134277 mm

Highest mean error: 5.199670314788818 mm for frame 156

Lowest mean error: 2.9295144081115723 mm for frame 130

Saving results

Total time: 43.674994707107544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2713/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2713/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820322
Iteration 2/25 | Loss: 0.00157156
Iteration 3/25 | Loss: 0.00135394
Iteration 4/25 | Loss: 0.00132396
Iteration 5/25 | Loss: 0.00131952
Iteration 6/25 | Loss: 0.00131904
Iteration 7/25 | Loss: 0.00131904
Iteration 8/25 | Loss: 0.00131904
Iteration 9/25 | Loss: 0.00131904
Iteration 10/25 | Loss: 0.00131904
Iteration 11/25 | Loss: 0.00131904
Iteration 12/25 | Loss: 0.00131904
Iteration 13/25 | Loss: 0.00131904
Iteration 14/25 | Loss: 0.00131904
Iteration 15/25 | Loss: 0.00131904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013190426398068666, 0.0013190426398068666, 0.0013190426398068666, 0.0013190426398068666, 0.0013190426398068666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013190426398068666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12843645
Iteration 2/25 | Loss: 0.00329995
Iteration 3/25 | Loss: 0.00329995
Iteration 4/25 | Loss: 0.00329995
Iteration 5/25 | Loss: 0.00329995
Iteration 6/25 | Loss: 0.00329994
Iteration 7/25 | Loss: 0.00329995
Iteration 8/25 | Loss: 0.00329994
Iteration 9/25 | Loss: 0.00329994
Iteration 10/25 | Loss: 0.00329994
Iteration 11/25 | Loss: 0.00329994
Iteration 12/25 | Loss: 0.00329994
Iteration 13/25 | Loss: 0.00329994
Iteration 14/25 | Loss: 0.00329994
Iteration 15/25 | Loss: 0.00329994
Iteration 16/25 | Loss: 0.00329994
Iteration 17/25 | Loss: 0.00329994
Iteration 18/25 | Loss: 0.00329994
Iteration 19/25 | Loss: 0.00329994
Iteration 20/25 | Loss: 0.00329994
Iteration 21/25 | Loss: 0.00329994
Iteration 22/25 | Loss: 0.00329994
Iteration 23/25 | Loss: 0.00329994
Iteration 24/25 | Loss: 0.00329994
Iteration 25/25 | Loss: 0.00329994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329994
Iteration 2/1000 | Loss: 0.00005819
Iteration 3/1000 | Loss: 0.00003109
Iteration 4/1000 | Loss: 0.00002257
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001888
Iteration 7/1000 | Loss: 0.00001800
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001453
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001450
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001447
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001444
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001442
Iteration 59/1000 | Loss: 0.00001442
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001441
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001439
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001436
Iteration 103/1000 | Loss: 0.00001436
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001436
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001435
Iteration 114/1000 | Loss: 0.00001435
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001433
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001430
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001429
Iteration 149/1000 | Loss: 0.00001429
Iteration 150/1000 | Loss: 0.00001429
Iteration 151/1000 | Loss: 0.00001429
Iteration 152/1000 | Loss: 0.00001429
Iteration 153/1000 | Loss: 0.00001429
Iteration 154/1000 | Loss: 0.00001428
Iteration 155/1000 | Loss: 0.00001428
Iteration 156/1000 | Loss: 0.00001428
Iteration 157/1000 | Loss: 0.00001428
Iteration 158/1000 | Loss: 0.00001428
Iteration 159/1000 | Loss: 0.00001428
Iteration 160/1000 | Loss: 0.00001428
Iteration 161/1000 | Loss: 0.00001428
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001427
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Iteration 182/1000 | Loss: 0.00001426
Iteration 183/1000 | Loss: 0.00001426
Iteration 184/1000 | Loss: 0.00001426
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001425
Iteration 189/1000 | Loss: 0.00001425
Iteration 190/1000 | Loss: 0.00001425
Iteration 191/1000 | Loss: 0.00001425
Iteration 192/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.4252946130000055e-05, 1.4252946130000055e-05, 1.4252946130000055e-05, 1.4252946130000055e-05, 1.4252946130000055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4252946130000055e-05

Optimization complete. Final v2v error: 3.3737192153930664 mm

Highest mean error: 3.773237466812134 mm for frame 188

Lowest mean error: 3.043412446975708 mm for frame 1

Saving results

Total time: 49.76195669174194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830080
Iteration 2/25 | Loss: 0.00141471
Iteration 3/25 | Loss: 0.00128452
Iteration 4/25 | Loss: 0.00126815
Iteration 5/25 | Loss: 0.00126476
Iteration 6/25 | Loss: 0.00126408
Iteration 7/25 | Loss: 0.00126408
Iteration 8/25 | Loss: 0.00126408
Iteration 9/25 | Loss: 0.00126408
Iteration 10/25 | Loss: 0.00126408
Iteration 11/25 | Loss: 0.00126408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012640772620216012, 0.0012640772620216012, 0.0012640772620216012, 0.0012640772620216012, 0.0012640772620216012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012640772620216012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02973759
Iteration 2/25 | Loss: 0.00221539
Iteration 3/25 | Loss: 0.00221537
Iteration 4/25 | Loss: 0.00221537
Iteration 5/25 | Loss: 0.00221537
Iteration 6/25 | Loss: 0.00221537
Iteration 7/25 | Loss: 0.00221537
Iteration 8/25 | Loss: 0.00221537
Iteration 9/25 | Loss: 0.00221537
Iteration 10/25 | Loss: 0.00221537
Iteration 11/25 | Loss: 0.00221537
Iteration 12/25 | Loss: 0.00221537
Iteration 13/25 | Loss: 0.00221537
Iteration 14/25 | Loss: 0.00221537
Iteration 15/25 | Loss: 0.00221537
Iteration 16/25 | Loss: 0.00221537
Iteration 17/25 | Loss: 0.00221537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022153679747134447, 0.0022153679747134447, 0.0022153679747134447, 0.0022153679747134447, 0.0022153679747134447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022153679747134447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221537
Iteration 2/1000 | Loss: 0.00004381
Iteration 3/1000 | Loss: 0.00002410
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002038
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00001756
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001753
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001742
Iteration 23/1000 | Loss: 0.00001739
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001737
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001717
Iteration 93/1000 | Loss: 0.00001717
Iteration 94/1000 | Loss: 0.00001717
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001716
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001715
Iteration 109/1000 | Loss: 0.00001715
Iteration 110/1000 | Loss: 0.00001715
Iteration 111/1000 | Loss: 0.00001715
Iteration 112/1000 | Loss: 0.00001715
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001714
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001714
Iteration 124/1000 | Loss: 0.00001714
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001713
Iteration 130/1000 | Loss: 0.00001713
Iteration 131/1000 | Loss: 0.00001713
Iteration 132/1000 | Loss: 0.00001713
Iteration 133/1000 | Loss: 0.00001713
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001713
Iteration 152/1000 | Loss: 0.00001713
Iteration 153/1000 | Loss: 0.00001713
Iteration 154/1000 | Loss: 0.00001713
Iteration 155/1000 | Loss: 0.00001713
Iteration 156/1000 | Loss: 0.00001713
Iteration 157/1000 | Loss: 0.00001713
Iteration 158/1000 | Loss: 0.00001713
Iteration 159/1000 | Loss: 0.00001713
Iteration 160/1000 | Loss: 0.00001713
Iteration 161/1000 | Loss: 0.00001713
Iteration 162/1000 | Loss: 0.00001713
Iteration 163/1000 | Loss: 0.00001713
Iteration 164/1000 | Loss: 0.00001713
Iteration 165/1000 | Loss: 0.00001713
Iteration 166/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.713312121864874e-05, 1.713312121864874e-05, 1.713312121864874e-05, 1.713312121864874e-05, 1.713312121864874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.713312121864874e-05

Optimization complete. Final v2v error: 3.5862488746643066 mm

Highest mean error: 4.413039207458496 mm for frame 52

Lowest mean error: 3.1731514930725098 mm for frame 207

Saving results

Total time: 41.638941049575806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838508
Iteration 2/25 | Loss: 0.00153300
Iteration 3/25 | Loss: 0.00136542
Iteration 4/25 | Loss: 0.00133657
Iteration 5/25 | Loss: 0.00132947
Iteration 6/25 | Loss: 0.00132691
Iteration 7/25 | Loss: 0.00132691
Iteration 8/25 | Loss: 0.00132691
Iteration 9/25 | Loss: 0.00132691
Iteration 10/25 | Loss: 0.00132691
Iteration 11/25 | Loss: 0.00132691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013269061455503106, 0.0013269061455503106, 0.0013269061455503106, 0.0013269061455503106, 0.0013269061455503106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013269061455503106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90519655
Iteration 2/25 | Loss: 0.00413514
Iteration 3/25 | Loss: 0.00413508
Iteration 4/25 | Loss: 0.00413508
Iteration 5/25 | Loss: 0.00413508
Iteration 6/25 | Loss: 0.00413508
Iteration 7/25 | Loss: 0.00413508
Iteration 8/25 | Loss: 0.00413508
Iteration 9/25 | Loss: 0.00413508
Iteration 10/25 | Loss: 0.00413508
Iteration 11/25 | Loss: 0.00413508
Iteration 12/25 | Loss: 0.00413508
Iteration 13/25 | Loss: 0.00413508
Iteration 14/25 | Loss: 0.00413508
Iteration 15/25 | Loss: 0.00413508
Iteration 16/25 | Loss: 0.00413508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004135077819228172, 0.004135077819228172, 0.004135077819228172, 0.004135077819228172, 0.004135077819228172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004135077819228172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00413508
Iteration 2/1000 | Loss: 0.00009185
Iteration 3/1000 | Loss: 0.00004044
Iteration 4/1000 | Loss: 0.00002989
Iteration 5/1000 | Loss: 0.00002591
Iteration 6/1000 | Loss: 0.00002410
Iteration 7/1000 | Loss: 0.00002296
Iteration 8/1000 | Loss: 0.00002215
Iteration 9/1000 | Loss: 0.00002139
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001921
Iteration 17/1000 | Loss: 0.00001920
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001916
Iteration 20/1000 | Loss: 0.00001912
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001905
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001904
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001901
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001897
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001890
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001889
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001887
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001884
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001883
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.8830753106158227e-05, 1.8830753106158227e-05, 1.8830753106158227e-05, 1.8830753106158227e-05, 1.8830753106158227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8830753106158227e-05

Optimization complete. Final v2v error: 3.8303451538085938 mm

Highest mean error: 4.2149248123168945 mm for frame 108

Lowest mean error: 3.177250385284424 mm for frame 216

Saving results

Total time: 45.14965295791626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740019
Iteration 2/25 | Loss: 0.00188207
Iteration 3/25 | Loss: 0.00157040
Iteration 4/25 | Loss: 0.00150148
Iteration 5/25 | Loss: 0.00145889
Iteration 6/25 | Loss: 0.00142239
Iteration 7/25 | Loss: 0.00139617
Iteration 8/25 | Loss: 0.00139045
Iteration 9/25 | Loss: 0.00139253
Iteration 10/25 | Loss: 0.00135702
Iteration 11/25 | Loss: 0.00134503
Iteration 12/25 | Loss: 0.00135171
Iteration 13/25 | Loss: 0.00135166
Iteration 14/25 | Loss: 0.00134312
Iteration 15/25 | Loss: 0.00133899
Iteration 16/25 | Loss: 0.00134806
Iteration 17/25 | Loss: 0.00134080
Iteration 18/25 | Loss: 0.00133930
Iteration 19/25 | Loss: 0.00133724
Iteration 20/25 | Loss: 0.00133693
Iteration 21/25 | Loss: 0.00133678
Iteration 22/25 | Loss: 0.00133673
Iteration 23/25 | Loss: 0.00133673
Iteration 24/25 | Loss: 0.00133673
Iteration 25/25 | Loss: 0.00133672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43062830
Iteration 2/25 | Loss: 0.00675216
Iteration 3/25 | Loss: 0.00566797
Iteration 4/25 | Loss: 0.00566797
Iteration 5/25 | Loss: 0.00566797
Iteration 6/25 | Loss: 0.00566796
Iteration 7/25 | Loss: 0.00566796
Iteration 8/25 | Loss: 0.00566796
Iteration 9/25 | Loss: 0.00566796
Iteration 10/25 | Loss: 0.00566796
Iteration 11/25 | Loss: 0.00566796
Iteration 12/25 | Loss: 0.00566796
Iteration 13/25 | Loss: 0.00566796
Iteration 14/25 | Loss: 0.00566796
Iteration 15/25 | Loss: 0.00566796
Iteration 16/25 | Loss: 0.00566796
Iteration 17/25 | Loss: 0.00566796
Iteration 18/25 | Loss: 0.00566796
Iteration 19/25 | Loss: 0.00566796
Iteration 20/25 | Loss: 0.00566796
Iteration 21/25 | Loss: 0.00566796
Iteration 22/25 | Loss: 0.00566796
Iteration 23/25 | Loss: 0.00566796
Iteration 24/25 | Loss: 0.00566796
Iteration 25/25 | Loss: 0.00566796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0056679630652070045, 0.0056679630652070045, 0.0056679630652070045, 0.0056679630652070045, 0.0056679630652070045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0056679630652070045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00566796
Iteration 2/1000 | Loss: 0.00296724
Iteration 3/1000 | Loss: 0.00139074
Iteration 4/1000 | Loss: 0.00014842
Iteration 5/1000 | Loss: 0.00008225
Iteration 6/1000 | Loss: 0.00027739
Iteration 7/1000 | Loss: 0.00041926
Iteration 8/1000 | Loss: 0.00006180
Iteration 9/1000 | Loss: 0.00027952
Iteration 10/1000 | Loss: 0.00010672
Iteration 11/1000 | Loss: 0.00012434
Iteration 12/1000 | Loss: 0.00003264
Iteration 13/1000 | Loss: 0.00037754
Iteration 14/1000 | Loss: 0.00021901
Iteration 15/1000 | Loss: 0.00026992
Iteration 16/1000 | Loss: 0.00032825
Iteration 17/1000 | Loss: 0.00018556
Iteration 18/1000 | Loss: 0.00003421
Iteration 19/1000 | Loss: 0.00047197
Iteration 20/1000 | Loss: 0.00017177
Iteration 21/1000 | Loss: 0.00047168
Iteration 22/1000 | Loss: 0.00003282
Iteration 23/1000 | Loss: 0.00002576
Iteration 24/1000 | Loss: 0.00002363
Iteration 25/1000 | Loss: 0.00002266
Iteration 26/1000 | Loss: 0.00002211
Iteration 27/1000 | Loss: 0.00032398
Iteration 28/1000 | Loss: 0.00003023
Iteration 29/1000 | Loss: 0.00002262
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001860
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001828
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001804
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00001790
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001786
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00051696
Iteration 56/1000 | Loss: 0.00003634
Iteration 57/1000 | Loss: 0.00002496
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001727
Iteration 60/1000 | Loss: 0.00001670
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001571
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001568
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001563
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001551
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001550
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001543
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001539
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001537
Iteration 111/1000 | Loss: 0.00001537
Iteration 112/1000 | Loss: 0.00001533
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001527
Iteration 125/1000 | Loss: 0.00001527
Iteration 126/1000 | Loss: 0.00001527
Iteration 127/1000 | Loss: 0.00001526
Iteration 128/1000 | Loss: 0.00001526
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001524
Iteration 133/1000 | Loss: 0.00001524
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001522
Iteration 137/1000 | Loss: 0.00001522
Iteration 138/1000 | Loss: 0.00001522
Iteration 139/1000 | Loss: 0.00001521
Iteration 140/1000 | Loss: 0.00001521
Iteration 141/1000 | Loss: 0.00001521
Iteration 142/1000 | Loss: 0.00001520
Iteration 143/1000 | Loss: 0.00001520
Iteration 144/1000 | Loss: 0.00001520
Iteration 145/1000 | Loss: 0.00001520
Iteration 146/1000 | Loss: 0.00001520
Iteration 147/1000 | Loss: 0.00001520
Iteration 148/1000 | Loss: 0.00001520
Iteration 149/1000 | Loss: 0.00001520
Iteration 150/1000 | Loss: 0.00001519
Iteration 151/1000 | Loss: 0.00001519
Iteration 152/1000 | Loss: 0.00001519
Iteration 153/1000 | Loss: 0.00001519
Iteration 154/1000 | Loss: 0.00001519
Iteration 155/1000 | Loss: 0.00001519
Iteration 156/1000 | Loss: 0.00001519
Iteration 157/1000 | Loss: 0.00001519
Iteration 158/1000 | Loss: 0.00001519
Iteration 159/1000 | Loss: 0.00001519
Iteration 160/1000 | Loss: 0.00001519
Iteration 161/1000 | Loss: 0.00001519
Iteration 162/1000 | Loss: 0.00001519
Iteration 163/1000 | Loss: 0.00001519
Iteration 164/1000 | Loss: 0.00001519
Iteration 165/1000 | Loss: 0.00001519
Iteration 166/1000 | Loss: 0.00001519
Iteration 167/1000 | Loss: 0.00001519
Iteration 168/1000 | Loss: 0.00001519
Iteration 169/1000 | Loss: 0.00001519
Iteration 170/1000 | Loss: 0.00001519
Iteration 171/1000 | Loss: 0.00001519
Iteration 172/1000 | Loss: 0.00001519
Iteration 173/1000 | Loss: 0.00001519
Iteration 174/1000 | Loss: 0.00001519
Iteration 175/1000 | Loss: 0.00001519
Iteration 176/1000 | Loss: 0.00001519
Iteration 177/1000 | Loss: 0.00001519
Iteration 178/1000 | Loss: 0.00001519
Iteration 179/1000 | Loss: 0.00001519
Iteration 180/1000 | Loss: 0.00001519
Iteration 181/1000 | Loss: 0.00001519
Iteration 182/1000 | Loss: 0.00001519
Iteration 183/1000 | Loss: 0.00001519
Iteration 184/1000 | Loss: 0.00001519
Iteration 185/1000 | Loss: 0.00001519
Iteration 186/1000 | Loss: 0.00001519
Iteration 187/1000 | Loss: 0.00001519
Iteration 188/1000 | Loss: 0.00001519
Iteration 189/1000 | Loss: 0.00001519
Iteration 190/1000 | Loss: 0.00001519
Iteration 191/1000 | Loss: 0.00001519
Iteration 192/1000 | Loss: 0.00001519
Iteration 193/1000 | Loss: 0.00001519
Iteration 194/1000 | Loss: 0.00001519
Iteration 195/1000 | Loss: 0.00001519
Iteration 196/1000 | Loss: 0.00001519
Iteration 197/1000 | Loss: 0.00001519
Iteration 198/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.519246961834142e-05, 1.519246961834142e-05, 1.519246961834142e-05, 1.519246961834142e-05, 1.519246961834142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.519246961834142e-05

Optimization complete. Final v2v error: 3.2429540157318115 mm

Highest mean error: 12.285628318786621 mm for frame 3

Lowest mean error: 2.8572471141815186 mm for frame 235

Saving results

Total time: 135.88470554351807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070419
Iteration 2/25 | Loss: 0.00283715
Iteration 3/25 | Loss: 0.00207734
Iteration 4/25 | Loss: 0.00171532
Iteration 5/25 | Loss: 0.00148327
Iteration 6/25 | Loss: 0.00142480
Iteration 7/25 | Loss: 0.00138417
Iteration 8/25 | Loss: 0.00130610
Iteration 9/25 | Loss: 0.00125727
Iteration 10/25 | Loss: 0.00124512
Iteration 11/25 | Loss: 0.00124111
Iteration 12/25 | Loss: 0.00123969
Iteration 13/25 | Loss: 0.00123928
Iteration 14/25 | Loss: 0.00123909
Iteration 15/25 | Loss: 0.00123902
Iteration 16/25 | Loss: 0.00123901
Iteration 17/25 | Loss: 0.00123901
Iteration 18/25 | Loss: 0.00123901
Iteration 19/25 | Loss: 0.00123901
Iteration 20/25 | Loss: 0.00123901
Iteration 21/25 | Loss: 0.00123900
Iteration 22/25 | Loss: 0.00123900
Iteration 23/25 | Loss: 0.00123900
Iteration 24/25 | Loss: 0.00123900
Iteration 25/25 | Loss: 0.00123900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12061679
Iteration 2/25 | Loss: 0.00358649
Iteration 3/25 | Loss: 0.00358649
Iteration 4/25 | Loss: 0.00358649
Iteration 5/25 | Loss: 0.00358649
Iteration 6/25 | Loss: 0.00358649
Iteration 7/25 | Loss: 0.00358648
Iteration 8/25 | Loss: 0.00358648
Iteration 9/25 | Loss: 0.00358648
Iteration 10/25 | Loss: 0.00358648
Iteration 11/25 | Loss: 0.00358648
Iteration 12/25 | Loss: 0.00358648
Iteration 13/25 | Loss: 0.00358648
Iteration 14/25 | Loss: 0.00358648
Iteration 15/25 | Loss: 0.00358648
Iteration 16/25 | Loss: 0.00358648
Iteration 17/25 | Loss: 0.00358648
Iteration 18/25 | Loss: 0.00358648
Iteration 19/25 | Loss: 0.00358648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0035864838864654303, 0.0035864838864654303, 0.0035864838864654303, 0.0035864838864654303, 0.0035864838864654303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035864838864654303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00358648
Iteration 2/1000 | Loss: 0.00005634
Iteration 3/1000 | Loss: 0.00003133
Iteration 4/1000 | Loss: 0.00002789
Iteration 5/1000 | Loss: 0.00002527
Iteration 6/1000 | Loss: 0.00005673
Iteration 7/1000 | Loss: 0.00004433
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002095
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001850
Iteration 13/1000 | Loss: 0.00001820
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001790
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00007920
Iteration 19/1000 | Loss: 0.00002155
Iteration 20/1000 | Loss: 0.00001977
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001763
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001758
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001757
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001756
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001756
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001755
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001753
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001752
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001751
Iteration 69/1000 | Loss: 0.00001751
Iteration 70/1000 | Loss: 0.00001751
Iteration 71/1000 | Loss: 0.00001751
Iteration 72/1000 | Loss: 0.00001751
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001750
Iteration 75/1000 | Loss: 0.00001750
Iteration 76/1000 | Loss: 0.00001750
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00001750
Iteration 79/1000 | Loss: 0.00001750
Iteration 80/1000 | Loss: 0.00001749
Iteration 81/1000 | Loss: 0.00001749
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001749
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001746
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001745
Iteration 103/1000 | Loss: 0.00001745
Iteration 104/1000 | Loss: 0.00001745
Iteration 105/1000 | Loss: 0.00001745
Iteration 106/1000 | Loss: 0.00001745
Iteration 107/1000 | Loss: 0.00001745
Iteration 108/1000 | Loss: 0.00001745
Iteration 109/1000 | Loss: 0.00001745
Iteration 110/1000 | Loss: 0.00001745
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001744
Iteration 114/1000 | Loss: 0.00001744
Iteration 115/1000 | Loss: 0.00001744
Iteration 116/1000 | Loss: 0.00001744
Iteration 117/1000 | Loss: 0.00001744
Iteration 118/1000 | Loss: 0.00001744
Iteration 119/1000 | Loss: 0.00001743
Iteration 120/1000 | Loss: 0.00001743
Iteration 121/1000 | Loss: 0.00001743
Iteration 122/1000 | Loss: 0.00001743
Iteration 123/1000 | Loss: 0.00001743
Iteration 124/1000 | Loss: 0.00001742
Iteration 125/1000 | Loss: 0.00001742
Iteration 126/1000 | Loss: 0.00005355
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001743
Iteration 129/1000 | Loss: 0.00001742
Iteration 130/1000 | Loss: 0.00001741
Iteration 131/1000 | Loss: 0.00001741
Iteration 132/1000 | Loss: 0.00001741
Iteration 133/1000 | Loss: 0.00001741
Iteration 134/1000 | Loss: 0.00001741
Iteration 135/1000 | Loss: 0.00001741
Iteration 136/1000 | Loss: 0.00001740
Iteration 137/1000 | Loss: 0.00001740
Iteration 138/1000 | Loss: 0.00001740
Iteration 139/1000 | Loss: 0.00001740
Iteration 140/1000 | Loss: 0.00001740
Iteration 141/1000 | Loss: 0.00001740
Iteration 142/1000 | Loss: 0.00001740
Iteration 143/1000 | Loss: 0.00001740
Iteration 144/1000 | Loss: 0.00001740
Iteration 145/1000 | Loss: 0.00001740
Iteration 146/1000 | Loss: 0.00001740
Iteration 147/1000 | Loss: 0.00001740
Iteration 148/1000 | Loss: 0.00001740
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001739
Iteration 153/1000 | Loss: 0.00001739
Iteration 154/1000 | Loss: 0.00001738
Iteration 155/1000 | Loss: 0.00001738
Iteration 156/1000 | Loss: 0.00001738
Iteration 157/1000 | Loss: 0.00001738
Iteration 158/1000 | Loss: 0.00001737
Iteration 159/1000 | Loss: 0.00001737
Iteration 160/1000 | Loss: 0.00001736
Iteration 161/1000 | Loss: 0.00001736
Iteration 162/1000 | Loss: 0.00001735
Iteration 163/1000 | Loss: 0.00001735
Iteration 164/1000 | Loss: 0.00001735
Iteration 165/1000 | Loss: 0.00001735
Iteration 166/1000 | Loss: 0.00001734
Iteration 167/1000 | Loss: 0.00001734
Iteration 168/1000 | Loss: 0.00001734
Iteration 169/1000 | Loss: 0.00001734
Iteration 170/1000 | Loss: 0.00001734
Iteration 171/1000 | Loss: 0.00001734
Iteration 172/1000 | Loss: 0.00001734
Iteration 173/1000 | Loss: 0.00001733
Iteration 174/1000 | Loss: 0.00001733
Iteration 175/1000 | Loss: 0.00001733
Iteration 176/1000 | Loss: 0.00001733
Iteration 177/1000 | Loss: 0.00001733
Iteration 178/1000 | Loss: 0.00005623
Iteration 179/1000 | Loss: 0.00001757
Iteration 180/1000 | Loss: 0.00001734
Iteration 181/1000 | Loss: 0.00001732
Iteration 182/1000 | Loss: 0.00001732
Iteration 183/1000 | Loss: 0.00001731
Iteration 184/1000 | Loss: 0.00001731
Iteration 185/1000 | Loss: 0.00001731
Iteration 186/1000 | Loss: 0.00001731
Iteration 187/1000 | Loss: 0.00001731
Iteration 188/1000 | Loss: 0.00001731
Iteration 189/1000 | Loss: 0.00001731
Iteration 190/1000 | Loss: 0.00001731
Iteration 191/1000 | Loss: 0.00001731
Iteration 192/1000 | Loss: 0.00001731
Iteration 193/1000 | Loss: 0.00001731
Iteration 194/1000 | Loss: 0.00001731
Iteration 195/1000 | Loss: 0.00001731
Iteration 196/1000 | Loss: 0.00001731
Iteration 197/1000 | Loss: 0.00001731
Iteration 198/1000 | Loss: 0.00001731
Iteration 199/1000 | Loss: 0.00001731
Iteration 200/1000 | Loss: 0.00001730
Iteration 201/1000 | Loss: 0.00001730
Iteration 202/1000 | Loss: 0.00001730
Iteration 203/1000 | Loss: 0.00001730
Iteration 204/1000 | Loss: 0.00001730
Iteration 205/1000 | Loss: 0.00001730
Iteration 206/1000 | Loss: 0.00001730
Iteration 207/1000 | Loss: 0.00001730
Iteration 208/1000 | Loss: 0.00001730
Iteration 209/1000 | Loss: 0.00001730
Iteration 210/1000 | Loss: 0.00001730
Iteration 211/1000 | Loss: 0.00001730
Iteration 212/1000 | Loss: 0.00001730
Iteration 213/1000 | Loss: 0.00001730
Iteration 214/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.7300430044997483e-05, 1.7300430044997483e-05, 1.7300430044997483e-05, 1.7300430044997483e-05, 1.7300430044997483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7300430044997483e-05

Optimization complete. Final v2v error: 3.5128185749053955 mm

Highest mean error: 9.496191024780273 mm for frame 107

Lowest mean error: 3.1743993759155273 mm for frame 186

Saving results

Total time: 84.04533767700195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714128
Iteration 2/25 | Loss: 0.00181357
Iteration 3/25 | Loss: 0.00140359
Iteration 4/25 | Loss: 0.00132449
Iteration 5/25 | Loss: 0.00129117
Iteration 6/25 | Loss: 0.00128477
Iteration 7/25 | Loss: 0.00127889
Iteration 8/25 | Loss: 0.00127471
Iteration 9/25 | Loss: 0.00127397
Iteration 10/25 | Loss: 0.00127895
Iteration 11/25 | Loss: 0.00127820
Iteration 12/25 | Loss: 0.00127372
Iteration 13/25 | Loss: 0.00127279
Iteration 14/25 | Loss: 0.00127406
Iteration 15/25 | Loss: 0.00127319
Iteration 16/25 | Loss: 0.00127126
Iteration 17/25 | Loss: 0.00126993
Iteration 18/25 | Loss: 0.00126970
Iteration 19/25 | Loss: 0.00126964
Iteration 20/25 | Loss: 0.00126963
Iteration 21/25 | Loss: 0.00126963
Iteration 22/25 | Loss: 0.00126963
Iteration 23/25 | Loss: 0.00126963
Iteration 24/25 | Loss: 0.00126963
Iteration 25/25 | Loss: 0.00126963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03639078
Iteration 2/25 | Loss: 0.00289350
Iteration 3/25 | Loss: 0.00289346
Iteration 4/25 | Loss: 0.00289346
Iteration 5/25 | Loss: 0.00289346
Iteration 6/25 | Loss: 0.00289346
Iteration 7/25 | Loss: 0.00289346
Iteration 8/25 | Loss: 0.00289346
Iteration 9/25 | Loss: 0.00289346
Iteration 10/25 | Loss: 0.00289346
Iteration 11/25 | Loss: 0.00289346
Iteration 12/25 | Loss: 0.00289346
Iteration 13/25 | Loss: 0.00289346
Iteration 14/25 | Loss: 0.00289346
Iteration 15/25 | Loss: 0.00289346
Iteration 16/25 | Loss: 0.00289346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002893456956371665, 0.002893456956371665, 0.002893456956371665, 0.002893456956371665, 0.002893456956371665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002893456956371665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289346
Iteration 2/1000 | Loss: 0.00005831
Iteration 3/1000 | Loss: 0.00003553
Iteration 4/1000 | Loss: 0.00003060
Iteration 5/1000 | Loss: 0.00002825
Iteration 6/1000 | Loss: 0.00002657
Iteration 7/1000 | Loss: 0.00002533
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002377
Iteration 10/1000 | Loss: 0.00002313
Iteration 11/1000 | Loss: 0.00002274
Iteration 12/1000 | Loss: 0.00002248
Iteration 13/1000 | Loss: 0.00014813
Iteration 14/1000 | Loss: 0.00002701
Iteration 15/1000 | Loss: 0.00002351
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002054
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002008
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001994
Iteration 24/1000 | Loss: 0.00001990
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001980
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001965
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001959
Iteration 41/1000 | Loss: 0.00001956
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001953
Iteration 44/1000 | Loss: 0.00001952
Iteration 45/1000 | Loss: 0.00001952
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001950
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001949
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001949
Iteration 74/1000 | Loss: 0.00001949
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001949
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001949
Iteration 87/1000 | Loss: 0.00001949
Iteration 88/1000 | Loss: 0.00001949
Iteration 89/1000 | Loss: 0.00001949
Iteration 90/1000 | Loss: 0.00001949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.9485823941067792e-05, 1.9485823941067792e-05, 1.9485823941067792e-05, 1.9485823941067792e-05, 1.9485823941067792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9485823941067792e-05

Optimization complete. Final v2v error: 3.905217409133911 mm

Highest mean error: 4.712208271026611 mm for frame 18

Lowest mean error: 3.3168344497680664 mm for frame 0

Saving results

Total time: 72.00108242034912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863591
Iteration 2/25 | Loss: 0.00169084
Iteration 3/25 | Loss: 0.00134207
Iteration 4/25 | Loss: 0.00127962
Iteration 5/25 | Loss: 0.00127654
Iteration 6/25 | Loss: 0.00125220
Iteration 7/25 | Loss: 0.00124651
Iteration 8/25 | Loss: 0.00124575
Iteration 9/25 | Loss: 0.00124551
Iteration 10/25 | Loss: 0.00124769
Iteration 11/25 | Loss: 0.00124418
Iteration 12/25 | Loss: 0.00124378
Iteration 13/25 | Loss: 0.00124365
Iteration 14/25 | Loss: 0.00124361
Iteration 15/25 | Loss: 0.00124361
Iteration 16/25 | Loss: 0.00124361
Iteration 17/25 | Loss: 0.00124361
Iteration 18/25 | Loss: 0.00124361
Iteration 19/25 | Loss: 0.00124361
Iteration 20/25 | Loss: 0.00124361
Iteration 21/25 | Loss: 0.00124360
Iteration 22/25 | Loss: 0.00124360
Iteration 23/25 | Loss: 0.00124360
Iteration 24/25 | Loss: 0.00124360
Iteration 25/25 | Loss: 0.00124360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13197744
Iteration 2/25 | Loss: 0.00340036
Iteration 3/25 | Loss: 0.00340035
Iteration 4/25 | Loss: 0.00340035
Iteration 5/25 | Loss: 0.00340035
Iteration 6/25 | Loss: 0.00340035
Iteration 7/25 | Loss: 0.00340035
Iteration 8/25 | Loss: 0.00340035
Iteration 9/25 | Loss: 0.00340035
Iteration 10/25 | Loss: 0.00340035
Iteration 11/25 | Loss: 0.00340035
Iteration 12/25 | Loss: 0.00340035
Iteration 13/25 | Loss: 0.00340035
Iteration 14/25 | Loss: 0.00340035
Iteration 15/25 | Loss: 0.00340035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0034003485925495625, 0.0034003485925495625, 0.0034003485925495625, 0.0034003485925495625, 0.0034003485925495625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034003485925495625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00340035
Iteration 2/1000 | Loss: 0.00005985
Iteration 3/1000 | Loss: 0.00003280
Iteration 4/1000 | Loss: 0.00002782
Iteration 5/1000 | Loss: 0.00002508
Iteration 6/1000 | Loss: 0.00002344
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00002376
Iteration 13/1000 | Loss: 0.00001990
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001868
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001808
Iteration 18/1000 | Loss: 0.00001788
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001763
Iteration 21/1000 | Loss: 0.00001760
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001722
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001718
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001713
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001710
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001708
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001705
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001704
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001704
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.7043903426383622e-05, 1.7043903426383622e-05, 1.7043903426383622e-05, 1.7043903426383622e-05, 1.7043903426383622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7043903426383622e-05

Optimization complete. Final v2v error: 3.51908802986145 mm

Highest mean error: 4.360920429229736 mm for frame 237

Lowest mean error: 2.9178919792175293 mm for frame 52

Saving results

Total time: 72.54166960716248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676965
Iteration 2/25 | Loss: 0.00139828
Iteration 3/25 | Loss: 0.00123624
Iteration 4/25 | Loss: 0.00121773
Iteration 5/25 | Loss: 0.00121414
Iteration 6/25 | Loss: 0.00121369
Iteration 7/25 | Loss: 0.00121369
Iteration 8/25 | Loss: 0.00121369
Iteration 9/25 | Loss: 0.00121369
Iteration 10/25 | Loss: 0.00121369
Iteration 11/25 | Loss: 0.00121369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012136941077187657, 0.0012136941077187657, 0.0012136941077187657, 0.0012136941077187657, 0.0012136941077187657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012136941077187657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.81026745
Iteration 2/25 | Loss: 0.00319351
Iteration 3/25 | Loss: 0.00319350
Iteration 4/25 | Loss: 0.00319350
Iteration 5/25 | Loss: 0.00319350
Iteration 6/25 | Loss: 0.00319350
Iteration 7/25 | Loss: 0.00319350
Iteration 8/25 | Loss: 0.00319350
Iteration 9/25 | Loss: 0.00319350
Iteration 10/25 | Loss: 0.00319350
Iteration 11/25 | Loss: 0.00319350
Iteration 12/25 | Loss: 0.00319350
Iteration 13/25 | Loss: 0.00319350
Iteration 14/25 | Loss: 0.00319350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003193496959283948, 0.003193496959283948, 0.003193496959283948, 0.003193496959283948, 0.003193496959283948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003193496959283948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319350
Iteration 2/1000 | Loss: 0.00004681
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002372
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001837
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001642
Iteration 11/1000 | Loss: 0.00001601
Iteration 12/1000 | Loss: 0.00001573
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001557
Iteration 15/1000 | Loss: 0.00001547
Iteration 16/1000 | Loss: 0.00001540
Iteration 17/1000 | Loss: 0.00001539
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001535
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001529
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001528
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001520
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001519
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001513
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001511
Iteration 58/1000 | Loss: 0.00001510
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001509
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001509
Iteration 68/1000 | Loss: 0.00001509
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001508
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001508
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001508
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001505
Iteration 84/1000 | Loss: 0.00001505
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001505
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5046722182887606e-05, 1.5046722182887606e-05, 1.5046722182887606e-05, 1.5046722182887606e-05, 1.5046722182887606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5046722182887606e-05

Optimization complete. Final v2v error: 3.3474643230438232 mm

Highest mean error: 4.048886299133301 mm for frame 5

Lowest mean error: 2.816051721572876 mm for frame 56

Saving results

Total time: 40.898738384246826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414607
Iteration 2/25 | Loss: 0.00134003
Iteration 3/25 | Loss: 0.00124637
Iteration 4/25 | Loss: 0.00123677
Iteration 5/25 | Loss: 0.00123363
Iteration 6/25 | Loss: 0.00123274
Iteration 7/25 | Loss: 0.00123274
Iteration 8/25 | Loss: 0.00123274
Iteration 9/25 | Loss: 0.00123274
Iteration 10/25 | Loss: 0.00123274
Iteration 11/25 | Loss: 0.00123274
Iteration 12/25 | Loss: 0.00123274
Iteration 13/25 | Loss: 0.00123274
Iteration 14/25 | Loss: 0.00123274
Iteration 15/25 | Loss: 0.00123274
Iteration 16/25 | Loss: 0.00123274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012327401200309396, 0.0012327401200309396, 0.0012327401200309396, 0.0012327401200309396, 0.0012327401200309396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012327401200309396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21033752
Iteration 2/25 | Loss: 0.00348552
Iteration 3/25 | Loss: 0.00348552
Iteration 4/25 | Loss: 0.00348552
Iteration 5/25 | Loss: 0.00348552
Iteration 6/25 | Loss: 0.00348552
Iteration 7/25 | Loss: 0.00348552
Iteration 8/25 | Loss: 0.00348552
Iteration 9/25 | Loss: 0.00348552
Iteration 10/25 | Loss: 0.00348552
Iteration 11/25 | Loss: 0.00348552
Iteration 12/25 | Loss: 0.00348552
Iteration 13/25 | Loss: 0.00348552
Iteration 14/25 | Loss: 0.00348552
Iteration 15/25 | Loss: 0.00348552
Iteration 16/25 | Loss: 0.00348552
Iteration 17/25 | Loss: 0.00348552
Iteration 18/25 | Loss: 0.00348552
Iteration 19/25 | Loss: 0.00348552
Iteration 20/25 | Loss: 0.00348552
Iteration 21/25 | Loss: 0.00348552
Iteration 22/25 | Loss: 0.00348552
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0034855182748287916, 0.0034855182748287916, 0.0034855182748287916, 0.0034855182748287916, 0.0034855182748287916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034855182748287916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348552
Iteration 2/1000 | Loss: 0.00004880
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002032
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001425
Iteration 11/1000 | Loss: 0.00001422
Iteration 12/1000 | Loss: 0.00001414
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001397
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001370
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001369
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001369
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001367
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001365
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001363
Iteration 62/1000 | Loss: 0.00001363
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001356
Iteration 88/1000 | Loss: 0.00001356
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001355
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001354
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001353
Iteration 118/1000 | Loss: 0.00001353
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001353
Iteration 121/1000 | Loss: 0.00001353
Iteration 122/1000 | Loss: 0.00001353
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001351
Iteration 136/1000 | Loss: 0.00001351
Iteration 137/1000 | Loss: 0.00001351
Iteration 138/1000 | Loss: 0.00001351
Iteration 139/1000 | Loss: 0.00001351
Iteration 140/1000 | Loss: 0.00001351
Iteration 141/1000 | Loss: 0.00001351
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001351
Iteration 147/1000 | Loss: 0.00001351
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001349
Iteration 158/1000 | Loss: 0.00001349
Iteration 159/1000 | Loss: 0.00001349
Iteration 160/1000 | Loss: 0.00001349
Iteration 161/1000 | Loss: 0.00001349
Iteration 162/1000 | Loss: 0.00001349
Iteration 163/1000 | Loss: 0.00001349
Iteration 164/1000 | Loss: 0.00001349
Iteration 165/1000 | Loss: 0.00001348
Iteration 166/1000 | Loss: 0.00001348
Iteration 167/1000 | Loss: 0.00001348
Iteration 168/1000 | Loss: 0.00001347
Iteration 169/1000 | Loss: 0.00001347
Iteration 170/1000 | Loss: 0.00001347
Iteration 171/1000 | Loss: 0.00001346
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001346
Iteration 176/1000 | Loss: 0.00001346
Iteration 177/1000 | Loss: 0.00001346
Iteration 178/1000 | Loss: 0.00001346
Iteration 179/1000 | Loss: 0.00001345
Iteration 180/1000 | Loss: 0.00001345
Iteration 181/1000 | Loss: 0.00001344
Iteration 182/1000 | Loss: 0.00001343
Iteration 183/1000 | Loss: 0.00001343
Iteration 184/1000 | Loss: 0.00001343
Iteration 185/1000 | Loss: 0.00001343
Iteration 186/1000 | Loss: 0.00001343
Iteration 187/1000 | Loss: 0.00001342
Iteration 188/1000 | Loss: 0.00001342
Iteration 189/1000 | Loss: 0.00001342
Iteration 190/1000 | Loss: 0.00001342
Iteration 191/1000 | Loss: 0.00001342
Iteration 192/1000 | Loss: 0.00001341
Iteration 193/1000 | Loss: 0.00001341
Iteration 194/1000 | Loss: 0.00001341
Iteration 195/1000 | Loss: 0.00001341
Iteration 196/1000 | Loss: 0.00001341
Iteration 197/1000 | Loss: 0.00001341
Iteration 198/1000 | Loss: 0.00001341
Iteration 199/1000 | Loss: 0.00001341
Iteration 200/1000 | Loss: 0.00001341
Iteration 201/1000 | Loss: 0.00001340
Iteration 202/1000 | Loss: 0.00001340
Iteration 203/1000 | Loss: 0.00001340
Iteration 204/1000 | Loss: 0.00001340
Iteration 205/1000 | Loss: 0.00001340
Iteration 206/1000 | Loss: 0.00001340
Iteration 207/1000 | Loss: 0.00001340
Iteration 208/1000 | Loss: 0.00001340
Iteration 209/1000 | Loss: 0.00001340
Iteration 210/1000 | Loss: 0.00001340
Iteration 211/1000 | Loss: 0.00001340
Iteration 212/1000 | Loss: 0.00001340
Iteration 213/1000 | Loss: 0.00001339
Iteration 214/1000 | Loss: 0.00001339
Iteration 215/1000 | Loss: 0.00001339
Iteration 216/1000 | Loss: 0.00001339
Iteration 217/1000 | Loss: 0.00001339
Iteration 218/1000 | Loss: 0.00001339
Iteration 219/1000 | Loss: 0.00001339
Iteration 220/1000 | Loss: 0.00001339
Iteration 221/1000 | Loss: 0.00001339
Iteration 222/1000 | Loss: 0.00001338
Iteration 223/1000 | Loss: 0.00001338
Iteration 224/1000 | Loss: 0.00001338
Iteration 225/1000 | Loss: 0.00001338
Iteration 226/1000 | Loss: 0.00001338
Iteration 227/1000 | Loss: 0.00001338
Iteration 228/1000 | Loss: 0.00001338
Iteration 229/1000 | Loss: 0.00001338
Iteration 230/1000 | Loss: 0.00001338
Iteration 231/1000 | Loss: 0.00001338
Iteration 232/1000 | Loss: 0.00001338
Iteration 233/1000 | Loss: 0.00001337
Iteration 234/1000 | Loss: 0.00001337
Iteration 235/1000 | Loss: 0.00001337
Iteration 236/1000 | Loss: 0.00001337
Iteration 237/1000 | Loss: 0.00001337
Iteration 238/1000 | Loss: 0.00001336
Iteration 239/1000 | Loss: 0.00001336
Iteration 240/1000 | Loss: 0.00001336
Iteration 241/1000 | Loss: 0.00001336
Iteration 242/1000 | Loss: 0.00001336
Iteration 243/1000 | Loss: 0.00001336
Iteration 244/1000 | Loss: 0.00001336
Iteration 245/1000 | Loss: 0.00001336
Iteration 246/1000 | Loss: 0.00001336
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001335
Iteration 249/1000 | Loss: 0.00001335
Iteration 250/1000 | Loss: 0.00001335
Iteration 251/1000 | Loss: 0.00001335
Iteration 252/1000 | Loss: 0.00001335
Iteration 253/1000 | Loss: 0.00001335
Iteration 254/1000 | Loss: 0.00001334
Iteration 255/1000 | Loss: 0.00001334
Iteration 256/1000 | Loss: 0.00001334
Iteration 257/1000 | Loss: 0.00001334
Iteration 258/1000 | Loss: 0.00001334
Iteration 259/1000 | Loss: 0.00001334
Iteration 260/1000 | Loss: 0.00001334
Iteration 261/1000 | Loss: 0.00001334
Iteration 262/1000 | Loss: 0.00001334
Iteration 263/1000 | Loss: 0.00001334
Iteration 264/1000 | Loss: 0.00001334
Iteration 265/1000 | Loss: 0.00001334
Iteration 266/1000 | Loss: 0.00001334
Iteration 267/1000 | Loss: 0.00001334
Iteration 268/1000 | Loss: 0.00001334
Iteration 269/1000 | Loss: 0.00001334
Iteration 270/1000 | Loss: 0.00001334
Iteration 271/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.3338617463887203e-05, 1.3338617463887203e-05, 1.3338617463887203e-05, 1.3338617463887203e-05, 1.3338617463887203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3338617463887203e-05

Optimization complete. Final v2v error: 3.2293434143066406 mm

Highest mean error: 3.746147871017456 mm for frame 71

Lowest mean error: 2.913733720779419 mm for frame 52

Saving results

Total time: 44.73082494735718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053631
Iteration 2/25 | Loss: 0.01053631
Iteration 3/25 | Loss: 0.01053631
Iteration 4/25 | Loss: 0.01053630
Iteration 5/25 | Loss: 0.01053630
Iteration 6/25 | Loss: 0.01053630
Iteration 7/25 | Loss: 0.01053630
Iteration 8/25 | Loss: 0.01053630
Iteration 9/25 | Loss: 0.01053629
Iteration 10/25 | Loss: 0.01053629
Iteration 11/25 | Loss: 0.01053629
Iteration 12/25 | Loss: 0.01053629
Iteration 13/25 | Loss: 0.01053629
Iteration 14/25 | Loss: 0.01053629
Iteration 15/25 | Loss: 0.01053628
Iteration 16/25 | Loss: 0.01053628
Iteration 17/25 | Loss: 0.01053628
Iteration 18/25 | Loss: 0.01053627
Iteration 19/25 | Loss: 0.01053627
Iteration 20/25 | Loss: 0.01053627
Iteration 21/25 | Loss: 0.01053627
Iteration 22/25 | Loss: 0.01053627
Iteration 23/25 | Loss: 0.01053627
Iteration 24/25 | Loss: 0.01053627
Iteration 25/25 | Loss: 0.01053626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77126759
Iteration 2/25 | Loss: 0.19285785
Iteration 3/25 | Loss: 0.19253682
Iteration 4/25 | Loss: 0.19253674
Iteration 5/25 | Loss: 0.19253674
Iteration 6/25 | Loss: 0.19253674
Iteration 7/25 | Loss: 0.19253670
Iteration 8/25 | Loss: 0.19253670
Iteration 9/25 | Loss: 0.19253670
Iteration 10/25 | Loss: 0.19253670
Iteration 11/25 | Loss: 0.19253670
Iteration 12/25 | Loss: 0.19253670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.19253669679164886, 0.19253669679164886, 0.19253669679164886, 0.19253669679164886, 0.19253669679164886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.19253669679164886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.19253670
Iteration 2/1000 | Loss: 0.00543689
Iteration 3/1000 | Loss: 0.01320835
Iteration 4/1000 | Loss: 0.00056911
Iteration 5/1000 | Loss: 0.00037447
Iteration 6/1000 | Loss: 0.00026369
Iteration 7/1000 | Loss: 0.00020760
Iteration 8/1000 | Loss: 0.00016717
Iteration 9/1000 | Loss: 0.00013997
Iteration 10/1000 | Loss: 0.00026588
Iteration 11/1000 | Loss: 0.00011766
Iteration 12/1000 | Loss: 0.00010764
Iteration 13/1000 | Loss: 0.00014632
Iteration 14/1000 | Loss: 0.00010025
Iteration 15/1000 | Loss: 0.00009289
Iteration 16/1000 | Loss: 0.00008672
Iteration 17/1000 | Loss: 0.00019496
Iteration 18/1000 | Loss: 0.00029133
Iteration 19/1000 | Loss: 0.00009115
Iteration 20/1000 | Loss: 0.00007842
Iteration 21/1000 | Loss: 0.00007415
Iteration 22/1000 | Loss: 0.00006886
Iteration 23/1000 | Loss: 0.00006514
Iteration 24/1000 | Loss: 0.00006301
Iteration 25/1000 | Loss: 0.00006150
Iteration 26/1000 | Loss: 0.00006030
Iteration 27/1000 | Loss: 0.00005940
Iteration 28/1000 | Loss: 0.00005863
Iteration 29/1000 | Loss: 0.00005783
Iteration 30/1000 | Loss: 0.00005684
Iteration 31/1000 | Loss: 0.00005617
Iteration 32/1000 | Loss: 0.00005561
Iteration 33/1000 | Loss: 0.00005504
Iteration 34/1000 | Loss: 0.00005643
Iteration 35/1000 | Loss: 0.00005419
Iteration 36/1000 | Loss: 0.00005373
Iteration 37/1000 | Loss: 0.00006143
Iteration 38/1000 | Loss: 0.00005318
Iteration 39/1000 | Loss: 0.00005254
Iteration 40/1000 | Loss: 0.00005193
Iteration 41/1000 | Loss: 0.00005570
Iteration 42/1000 | Loss: 0.00005101
Iteration 43/1000 | Loss: 0.00005051
Iteration 44/1000 | Loss: 0.00005014
Iteration 45/1000 | Loss: 0.00004983
Iteration 46/1000 | Loss: 0.00004963
Iteration 47/1000 | Loss: 0.00004959
Iteration 48/1000 | Loss: 0.00004958
Iteration 49/1000 | Loss: 0.00004951
Iteration 50/1000 | Loss: 0.00004951
Iteration 51/1000 | Loss: 0.00004947
Iteration 52/1000 | Loss: 0.00004945
Iteration 53/1000 | Loss: 0.00004945
Iteration 54/1000 | Loss: 0.00004945
Iteration 55/1000 | Loss: 0.00004944
Iteration 56/1000 | Loss: 0.00004944
Iteration 57/1000 | Loss: 0.00004941
Iteration 58/1000 | Loss: 0.00004941
Iteration 59/1000 | Loss: 0.00004940
Iteration 60/1000 | Loss: 0.00004940
Iteration 61/1000 | Loss: 0.00004940
Iteration 62/1000 | Loss: 0.00004940
Iteration 63/1000 | Loss: 0.00004940
Iteration 64/1000 | Loss: 0.00004940
Iteration 65/1000 | Loss: 0.00004940
Iteration 66/1000 | Loss: 0.00004940
Iteration 67/1000 | Loss: 0.00004940
Iteration 68/1000 | Loss: 0.00004940
Iteration 69/1000 | Loss: 0.00004940
Iteration 70/1000 | Loss: 0.00004940
Iteration 71/1000 | Loss: 0.00004939
Iteration 72/1000 | Loss: 0.00004939
Iteration 73/1000 | Loss: 0.00004937
Iteration 74/1000 | Loss: 0.00004936
Iteration 75/1000 | Loss: 0.00004936
Iteration 76/1000 | Loss: 0.00004935
Iteration 77/1000 | Loss: 0.00004935
Iteration 78/1000 | Loss: 0.00004935
Iteration 79/1000 | Loss: 0.00004934
Iteration 80/1000 | Loss: 0.00004934
Iteration 81/1000 | Loss: 0.00004933
Iteration 82/1000 | Loss: 0.00004933
Iteration 83/1000 | Loss: 0.00004932
Iteration 84/1000 | Loss: 0.00004932
Iteration 85/1000 | Loss: 0.00004932
Iteration 86/1000 | Loss: 0.00004932
Iteration 87/1000 | Loss: 0.00004932
Iteration 88/1000 | Loss: 0.00004932
Iteration 89/1000 | Loss: 0.00004932
Iteration 90/1000 | Loss: 0.00004932
Iteration 91/1000 | Loss: 0.00004932
Iteration 92/1000 | Loss: 0.00004931
Iteration 93/1000 | Loss: 0.00004931
Iteration 94/1000 | Loss: 0.00004931
Iteration 95/1000 | Loss: 0.00004931
Iteration 96/1000 | Loss: 0.00004931
Iteration 97/1000 | Loss: 0.00004931
Iteration 98/1000 | Loss: 0.00004931
Iteration 99/1000 | Loss: 0.00004930
Iteration 100/1000 | Loss: 0.00004930
Iteration 101/1000 | Loss: 0.00004930
Iteration 102/1000 | Loss: 0.00004930
Iteration 103/1000 | Loss: 0.00004930
Iteration 104/1000 | Loss: 0.00004930
Iteration 105/1000 | Loss: 0.00004930
Iteration 106/1000 | Loss: 0.00004929
Iteration 107/1000 | Loss: 0.00004929
Iteration 108/1000 | Loss: 0.00004929
Iteration 109/1000 | Loss: 0.00004929
Iteration 110/1000 | Loss: 0.00004929
Iteration 111/1000 | Loss: 0.00004928
Iteration 112/1000 | Loss: 0.00004928
Iteration 113/1000 | Loss: 0.00004928
Iteration 114/1000 | Loss: 0.00004928
Iteration 115/1000 | Loss: 0.00004928
Iteration 116/1000 | Loss: 0.00004928
Iteration 117/1000 | Loss: 0.00004928
Iteration 118/1000 | Loss: 0.00004928
Iteration 119/1000 | Loss: 0.00004928
Iteration 120/1000 | Loss: 0.00004928
Iteration 121/1000 | Loss: 0.00004928
Iteration 122/1000 | Loss: 0.00004928
Iteration 123/1000 | Loss: 0.00004927
Iteration 124/1000 | Loss: 0.00004927
Iteration 125/1000 | Loss: 0.00004927
Iteration 126/1000 | Loss: 0.00004927
Iteration 127/1000 | Loss: 0.00004927
Iteration 128/1000 | Loss: 0.00004927
Iteration 129/1000 | Loss: 0.00004927
Iteration 130/1000 | Loss: 0.00004927
Iteration 131/1000 | Loss: 0.00004927
Iteration 132/1000 | Loss: 0.00004927
Iteration 133/1000 | Loss: 0.00004927
Iteration 134/1000 | Loss: 0.00004926
Iteration 135/1000 | Loss: 0.00004926
Iteration 136/1000 | Loss: 0.00004926
Iteration 137/1000 | Loss: 0.00004926
Iteration 138/1000 | Loss: 0.00004926
Iteration 139/1000 | Loss: 0.00004926
Iteration 140/1000 | Loss: 0.00004926
Iteration 141/1000 | Loss: 0.00004926
Iteration 142/1000 | Loss: 0.00004926
Iteration 143/1000 | Loss: 0.00004926
Iteration 144/1000 | Loss: 0.00004925
Iteration 145/1000 | Loss: 0.00004925
Iteration 146/1000 | Loss: 0.00004925
Iteration 147/1000 | Loss: 0.00004925
Iteration 148/1000 | Loss: 0.00004925
Iteration 149/1000 | Loss: 0.00004925
Iteration 150/1000 | Loss: 0.00004925
Iteration 151/1000 | Loss: 0.00004924
Iteration 152/1000 | Loss: 0.00004924
Iteration 153/1000 | Loss: 0.00004924
Iteration 154/1000 | Loss: 0.00004924
Iteration 155/1000 | Loss: 0.00004924
Iteration 156/1000 | Loss: 0.00004924
Iteration 157/1000 | Loss: 0.00004923
Iteration 158/1000 | Loss: 0.00004923
Iteration 159/1000 | Loss: 0.00004923
Iteration 160/1000 | Loss: 0.00004923
Iteration 161/1000 | Loss: 0.00004923
Iteration 162/1000 | Loss: 0.00004923
Iteration 163/1000 | Loss: 0.00004923
Iteration 164/1000 | Loss: 0.00004923
Iteration 165/1000 | Loss: 0.00004923
Iteration 166/1000 | Loss: 0.00004923
Iteration 167/1000 | Loss: 0.00004922
Iteration 168/1000 | Loss: 0.00004922
Iteration 169/1000 | Loss: 0.00004922
Iteration 170/1000 | Loss: 0.00004922
Iteration 171/1000 | Loss: 0.00004922
Iteration 172/1000 | Loss: 0.00004922
Iteration 173/1000 | Loss: 0.00004921
Iteration 174/1000 | Loss: 0.00004921
Iteration 175/1000 | Loss: 0.00004921
Iteration 176/1000 | Loss: 0.00004921
Iteration 177/1000 | Loss: 0.00004921
Iteration 178/1000 | Loss: 0.00004921
Iteration 179/1000 | Loss: 0.00004921
Iteration 180/1000 | Loss: 0.00004921
Iteration 181/1000 | Loss: 0.00004921
Iteration 182/1000 | Loss: 0.00004921
Iteration 183/1000 | Loss: 0.00004921
Iteration 184/1000 | Loss: 0.00004921
Iteration 185/1000 | Loss: 0.00004921
Iteration 186/1000 | Loss: 0.00004921
Iteration 187/1000 | Loss: 0.00004920
Iteration 188/1000 | Loss: 0.00004920
Iteration 189/1000 | Loss: 0.00004920
Iteration 190/1000 | Loss: 0.00004920
Iteration 191/1000 | Loss: 0.00004920
Iteration 192/1000 | Loss: 0.00004920
Iteration 193/1000 | Loss: 0.00004920
Iteration 194/1000 | Loss: 0.00004920
Iteration 195/1000 | Loss: 0.00004920
Iteration 196/1000 | Loss: 0.00004920
Iteration 197/1000 | Loss: 0.00004920
Iteration 198/1000 | Loss: 0.00004920
Iteration 199/1000 | Loss: 0.00004919
Iteration 200/1000 | Loss: 0.00004919
Iteration 201/1000 | Loss: 0.00004919
Iteration 202/1000 | Loss: 0.00004919
Iteration 203/1000 | Loss: 0.00004919
Iteration 204/1000 | Loss: 0.00004919
Iteration 205/1000 | Loss: 0.00004919
Iteration 206/1000 | Loss: 0.00004919
Iteration 207/1000 | Loss: 0.00004919
Iteration 208/1000 | Loss: 0.00004919
Iteration 209/1000 | Loss: 0.00004919
Iteration 210/1000 | Loss: 0.00004919
Iteration 211/1000 | Loss: 0.00004919
Iteration 212/1000 | Loss: 0.00004919
Iteration 213/1000 | Loss: 0.00004918
Iteration 214/1000 | Loss: 0.00004918
Iteration 215/1000 | Loss: 0.00004918
Iteration 216/1000 | Loss: 0.00004918
Iteration 217/1000 | Loss: 0.00004918
Iteration 218/1000 | Loss: 0.00004918
Iteration 219/1000 | Loss: 0.00004918
Iteration 220/1000 | Loss: 0.00004918
Iteration 221/1000 | Loss: 0.00004918
Iteration 222/1000 | Loss: 0.00004918
Iteration 223/1000 | Loss: 0.00004918
Iteration 224/1000 | Loss: 0.00004918
Iteration 225/1000 | Loss: 0.00004918
Iteration 226/1000 | Loss: 0.00004918
Iteration 227/1000 | Loss: 0.00004918
Iteration 228/1000 | Loss: 0.00004918
Iteration 229/1000 | Loss: 0.00004918
Iteration 230/1000 | Loss: 0.00004918
Iteration 231/1000 | Loss: 0.00004918
Iteration 232/1000 | Loss: 0.00004918
Iteration 233/1000 | Loss: 0.00004918
Iteration 234/1000 | Loss: 0.00004918
Iteration 235/1000 | Loss: 0.00004918
Iteration 236/1000 | Loss: 0.00004918
Iteration 237/1000 | Loss: 0.00004918
Iteration 238/1000 | Loss: 0.00004918
Iteration 239/1000 | Loss: 0.00004918
Iteration 240/1000 | Loss: 0.00004918
Iteration 241/1000 | Loss: 0.00004918
Iteration 242/1000 | Loss: 0.00004918
Iteration 243/1000 | Loss: 0.00004918
Iteration 244/1000 | Loss: 0.00004918
Iteration 245/1000 | Loss: 0.00004918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [4.91803657496348e-05, 4.91803657496348e-05, 4.91803657496348e-05, 4.91803657496348e-05, 4.91803657496348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.91803657496348e-05

Optimization complete. Final v2v error: 4.625365734100342 mm

Highest mean error: 20.705312728881836 mm for frame 43

Lowest mean error: 3.8110268115997314 mm for frame 40

Saving results

Total time: 98.325434923172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136723
Iteration 2/25 | Loss: 0.00204676
Iteration 3/25 | Loss: 0.00143797
Iteration 4/25 | Loss: 0.00137849
Iteration 5/25 | Loss: 0.00136801
Iteration 6/25 | Loss: 0.00136129
Iteration 7/25 | Loss: 0.00136343
Iteration 8/25 | Loss: 0.00136631
Iteration 9/25 | Loss: 0.00137077
Iteration 10/25 | Loss: 0.00138188
Iteration 11/25 | Loss: 0.00137772
Iteration 12/25 | Loss: 0.00136610
Iteration 13/25 | Loss: 0.00136945
Iteration 14/25 | Loss: 0.00136448
Iteration 15/25 | Loss: 0.00136326
Iteration 16/25 | Loss: 0.00135769
Iteration 17/25 | Loss: 0.00136139
Iteration 18/25 | Loss: 0.00136039
Iteration 19/25 | Loss: 0.00135742
Iteration 20/25 | Loss: 0.00135651
Iteration 21/25 | Loss: 0.00135605
Iteration 22/25 | Loss: 0.00135702
Iteration 23/25 | Loss: 0.00135752
Iteration 24/25 | Loss: 0.00135925
Iteration 25/25 | Loss: 0.00135667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93764406
Iteration 2/25 | Loss: 0.00293557
Iteration 3/25 | Loss: 0.00293557
Iteration 4/25 | Loss: 0.00293557
Iteration 5/25 | Loss: 0.00293557
Iteration 6/25 | Loss: 0.00293557
Iteration 7/25 | Loss: 0.00293557
Iteration 8/25 | Loss: 0.00293557
Iteration 9/25 | Loss: 0.00293557
Iteration 10/25 | Loss: 0.00293557
Iteration 11/25 | Loss: 0.00293557
Iteration 12/25 | Loss: 0.00293557
Iteration 13/25 | Loss: 0.00293557
Iteration 14/25 | Loss: 0.00293557
Iteration 15/25 | Loss: 0.00293557
Iteration 16/25 | Loss: 0.00293557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029355657752603292, 0.0029355657752603292, 0.0029355657752603292, 0.0029355657752603292, 0.0029355657752603292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029355657752603292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293557
Iteration 2/1000 | Loss: 0.00060390
Iteration 3/1000 | Loss: 0.00050573
Iteration 4/1000 | Loss: 0.00074090
Iteration 5/1000 | Loss: 0.00048861
Iteration 6/1000 | Loss: 0.00068113
Iteration 7/1000 | Loss: 0.00063362
Iteration 8/1000 | Loss: 0.00075591
Iteration 9/1000 | Loss: 0.00077660
Iteration 10/1000 | Loss: 0.00048631
Iteration 11/1000 | Loss: 0.00070072
Iteration 12/1000 | Loss: 0.00080308
Iteration 13/1000 | Loss: 0.00064195
Iteration 14/1000 | Loss: 0.00050919
Iteration 15/1000 | Loss: 0.00052811
Iteration 16/1000 | Loss: 0.00072926
Iteration 17/1000 | Loss: 0.00088607
Iteration 18/1000 | Loss: 0.00107406
Iteration 19/1000 | Loss: 0.00086583
Iteration 20/1000 | Loss: 0.00053056
Iteration 21/1000 | Loss: 0.00112380
Iteration 22/1000 | Loss: 0.00098051
Iteration 23/1000 | Loss: 0.00118690
Iteration 24/1000 | Loss: 0.00081428
Iteration 25/1000 | Loss: 0.00056107
Iteration 26/1000 | Loss: 0.00048928
Iteration 27/1000 | Loss: 0.00049921
Iteration 28/1000 | Loss: 0.00054587
Iteration 29/1000 | Loss: 0.00035707
Iteration 30/1000 | Loss: 0.00047431
Iteration 31/1000 | Loss: 0.00054494
Iteration 32/1000 | Loss: 0.00058595
Iteration 33/1000 | Loss: 0.00087375
Iteration 34/1000 | Loss: 0.00091507
Iteration 35/1000 | Loss: 0.00104885
Iteration 36/1000 | Loss: 0.00120256
Iteration 37/1000 | Loss: 0.00109248
Iteration 38/1000 | Loss: 0.00054260
Iteration 39/1000 | Loss: 0.00084485
Iteration 40/1000 | Loss: 0.00045081
Iteration 41/1000 | Loss: 0.00034317
Iteration 42/1000 | Loss: 0.00027984
Iteration 43/1000 | Loss: 0.00039551
Iteration 44/1000 | Loss: 0.00048424
Iteration 45/1000 | Loss: 0.00043189
Iteration 46/1000 | Loss: 0.00044903
Iteration 47/1000 | Loss: 0.00042435
Iteration 48/1000 | Loss: 0.00053131
Iteration 49/1000 | Loss: 0.00051758
Iteration 50/1000 | Loss: 0.00042399
Iteration 51/1000 | Loss: 0.00056408
Iteration 52/1000 | Loss: 0.00072993
Iteration 53/1000 | Loss: 0.00046406
Iteration 54/1000 | Loss: 0.00049223
Iteration 55/1000 | Loss: 0.00067021
Iteration 56/1000 | Loss: 0.00031579
Iteration 57/1000 | Loss: 0.00053415
Iteration 58/1000 | Loss: 0.00053600
Iteration 59/1000 | Loss: 0.00042745
Iteration 60/1000 | Loss: 0.00045509
Iteration 61/1000 | Loss: 0.00054045
Iteration 62/1000 | Loss: 0.00046922
Iteration 63/1000 | Loss: 0.00041019
Iteration 64/1000 | Loss: 0.00051045
Iteration 65/1000 | Loss: 0.00046201
Iteration 66/1000 | Loss: 0.00045798
Iteration 67/1000 | Loss: 0.00039261
Iteration 68/1000 | Loss: 0.00051248
Iteration 69/1000 | Loss: 0.00059203
Iteration 70/1000 | Loss: 0.00055251
Iteration 71/1000 | Loss: 0.00089116
Iteration 72/1000 | Loss: 0.00082160
Iteration 73/1000 | Loss: 0.00055830
Iteration 74/1000 | Loss: 0.00055432
Iteration 75/1000 | Loss: 0.00055961
Iteration 76/1000 | Loss: 0.00053976
Iteration 77/1000 | Loss: 0.00057753
Iteration 78/1000 | Loss: 0.00016601
Iteration 79/1000 | Loss: 0.00050282
Iteration 80/1000 | Loss: 0.00047932
Iteration 81/1000 | Loss: 0.00046622
Iteration 82/1000 | Loss: 0.00038393
Iteration 83/1000 | Loss: 0.00041764
Iteration 84/1000 | Loss: 0.00046480
Iteration 85/1000 | Loss: 0.00042002
Iteration 86/1000 | Loss: 0.00048220
Iteration 87/1000 | Loss: 0.00049688
Iteration 88/1000 | Loss: 0.00047633
Iteration 89/1000 | Loss: 0.00037066
Iteration 90/1000 | Loss: 0.00031111
Iteration 91/1000 | Loss: 0.00035460
Iteration 92/1000 | Loss: 0.00036539
Iteration 93/1000 | Loss: 0.00051059
Iteration 94/1000 | Loss: 0.00041033
Iteration 95/1000 | Loss: 0.00033590
Iteration 96/1000 | Loss: 0.00035033
Iteration 97/1000 | Loss: 0.00033749
Iteration 98/1000 | Loss: 0.00034756
Iteration 99/1000 | Loss: 0.00028268
Iteration 100/1000 | Loss: 0.00037345
Iteration 101/1000 | Loss: 0.00051900
Iteration 102/1000 | Loss: 0.00047077
Iteration 103/1000 | Loss: 0.00043720
Iteration 104/1000 | Loss: 0.00044388
Iteration 105/1000 | Loss: 0.00046659
Iteration 106/1000 | Loss: 0.00041360
Iteration 107/1000 | Loss: 0.00045473
Iteration 108/1000 | Loss: 0.00050322
Iteration 109/1000 | Loss: 0.00049714
Iteration 110/1000 | Loss: 0.00034641
Iteration 111/1000 | Loss: 0.00051477
Iteration 112/1000 | Loss: 0.00050103
Iteration 113/1000 | Loss: 0.00028749
Iteration 114/1000 | Loss: 0.00041891
Iteration 115/1000 | Loss: 0.00057470
Iteration 116/1000 | Loss: 0.00074658
Iteration 117/1000 | Loss: 0.00044578
Iteration 118/1000 | Loss: 0.00029315
Iteration 119/1000 | Loss: 0.00051890
Iteration 120/1000 | Loss: 0.00051630
Iteration 121/1000 | Loss: 0.00056578
Iteration 122/1000 | Loss: 0.00051208
Iteration 123/1000 | Loss: 0.00064142
Iteration 124/1000 | Loss: 0.00054259
Iteration 125/1000 | Loss: 0.00072019
Iteration 126/1000 | Loss: 0.00050739
Iteration 127/1000 | Loss: 0.00066198
Iteration 128/1000 | Loss: 0.00051389
Iteration 129/1000 | Loss: 0.00042728
Iteration 130/1000 | Loss: 0.00072322
Iteration 131/1000 | Loss: 0.00058860
Iteration 132/1000 | Loss: 0.00071314
Iteration 133/1000 | Loss: 0.00050177
Iteration 134/1000 | Loss: 0.00076372
Iteration 135/1000 | Loss: 0.00039145
Iteration 136/1000 | Loss: 0.00032008
Iteration 137/1000 | Loss: 0.00053470
Iteration 138/1000 | Loss: 0.00041703
Iteration 139/1000 | Loss: 0.00031818
Iteration 140/1000 | Loss: 0.00020911
Iteration 141/1000 | Loss: 0.00039026
Iteration 142/1000 | Loss: 0.00049224
Iteration 143/1000 | Loss: 0.00034883
Iteration 144/1000 | Loss: 0.00029479
Iteration 145/1000 | Loss: 0.00040217
Iteration 146/1000 | Loss: 0.00045643
Iteration 147/1000 | Loss: 0.00046510
Iteration 148/1000 | Loss: 0.00043129
Iteration 149/1000 | Loss: 0.00042575
Iteration 150/1000 | Loss: 0.00050195
Iteration 151/1000 | Loss: 0.00012000
Iteration 152/1000 | Loss: 0.00034659
Iteration 153/1000 | Loss: 0.00021786
Iteration 154/1000 | Loss: 0.00031798
Iteration 155/1000 | Loss: 0.00038226
Iteration 156/1000 | Loss: 0.00032730
Iteration 157/1000 | Loss: 0.00035670
Iteration 158/1000 | Loss: 0.00037818
Iteration 159/1000 | Loss: 0.00039017
Iteration 160/1000 | Loss: 0.00048685
Iteration 161/1000 | Loss: 0.00044242
Iteration 162/1000 | Loss: 0.00044020
Iteration 163/1000 | Loss: 0.00045473
Iteration 164/1000 | Loss: 0.00033949
Iteration 165/1000 | Loss: 0.00043868
Iteration 166/1000 | Loss: 0.00041079
Iteration 167/1000 | Loss: 0.00047870
Iteration 168/1000 | Loss: 0.00045229
Iteration 169/1000 | Loss: 0.00027282
Iteration 170/1000 | Loss: 0.00037284
Iteration 171/1000 | Loss: 0.00052351
Iteration 172/1000 | Loss: 0.00047135
Iteration 173/1000 | Loss: 0.00039073
Iteration 174/1000 | Loss: 0.00037821
Iteration 175/1000 | Loss: 0.00031954
Iteration 176/1000 | Loss: 0.00046857
Iteration 177/1000 | Loss: 0.00050814
Iteration 178/1000 | Loss: 0.00048069
Iteration 179/1000 | Loss: 0.00044405
Iteration 180/1000 | Loss: 0.00040587
Iteration 181/1000 | Loss: 0.00015193
Iteration 182/1000 | Loss: 0.00034949
Iteration 183/1000 | Loss: 0.00082658
Iteration 184/1000 | Loss: 0.00056416
Iteration 185/1000 | Loss: 0.00031532
Iteration 186/1000 | Loss: 0.00032896
Iteration 187/1000 | Loss: 0.00074640
Iteration 188/1000 | Loss: 0.00053924
Iteration 189/1000 | Loss: 0.00021127
Iteration 190/1000 | Loss: 0.00028547
Iteration 191/1000 | Loss: 0.00027361
Iteration 192/1000 | Loss: 0.00022666
Iteration 193/1000 | Loss: 0.00020545
Iteration 194/1000 | Loss: 0.00034479
Iteration 195/1000 | Loss: 0.00032709
Iteration 196/1000 | Loss: 0.00027695
Iteration 197/1000 | Loss: 0.00026005
Iteration 198/1000 | Loss: 0.00027538
Iteration 199/1000 | Loss: 0.00033757
Iteration 200/1000 | Loss: 0.00036828
Iteration 201/1000 | Loss: 0.00031442
Iteration 202/1000 | Loss: 0.00031848
Iteration 203/1000 | Loss: 0.00044301
Iteration 204/1000 | Loss: 0.00049510
Iteration 205/1000 | Loss: 0.00040913
Iteration 206/1000 | Loss: 0.00051838
Iteration 207/1000 | Loss: 0.00038655
Iteration 208/1000 | Loss: 0.00028890
Iteration 209/1000 | Loss: 0.00032377
Iteration 210/1000 | Loss: 0.00028276
Iteration 211/1000 | Loss: 0.00031427
Iteration 212/1000 | Loss: 0.00028948
Iteration 213/1000 | Loss: 0.00033289
Iteration 214/1000 | Loss: 0.00025442
Iteration 215/1000 | Loss: 0.00038118
Iteration 216/1000 | Loss: 0.00032241
Iteration 217/1000 | Loss: 0.00038360
Iteration 218/1000 | Loss: 0.00074969
Iteration 219/1000 | Loss: 0.00032353
Iteration 220/1000 | Loss: 0.00033759
Iteration 221/1000 | Loss: 0.00026202
Iteration 222/1000 | Loss: 0.00033791
Iteration 223/1000 | Loss: 0.00017775
Iteration 224/1000 | Loss: 0.00051931
Iteration 225/1000 | Loss: 0.00041553
Iteration 226/1000 | Loss: 0.00040023
Iteration 227/1000 | Loss: 0.00038329
Iteration 228/1000 | Loss: 0.00038103
Iteration 229/1000 | Loss: 0.00039730
Iteration 230/1000 | Loss: 0.00038175
Iteration 231/1000 | Loss: 0.00044350
Iteration 232/1000 | Loss: 0.00034122
Iteration 233/1000 | Loss: 0.00022099
Iteration 234/1000 | Loss: 0.00039941
Iteration 235/1000 | Loss: 0.00030555
Iteration 236/1000 | Loss: 0.00044717
Iteration 237/1000 | Loss: 0.00036925
Iteration 238/1000 | Loss: 0.00038004
Iteration 239/1000 | Loss: 0.00028018
Iteration 240/1000 | Loss: 0.00028764
Iteration 241/1000 | Loss: 0.00031061
Iteration 242/1000 | Loss: 0.00031229
Iteration 243/1000 | Loss: 0.00023325
Iteration 244/1000 | Loss: 0.00043547
Iteration 245/1000 | Loss: 0.00033434
Iteration 246/1000 | Loss: 0.00042486
Iteration 247/1000 | Loss: 0.00042067
Iteration 248/1000 | Loss: 0.00047789
Iteration 249/1000 | Loss: 0.00022613
Iteration 250/1000 | Loss: 0.00014469
Iteration 251/1000 | Loss: 0.00039434
Iteration 252/1000 | Loss: 0.00039410
Iteration 253/1000 | Loss: 0.00037582
Iteration 254/1000 | Loss: 0.00029876
Iteration 255/1000 | Loss: 0.00018193
Iteration 256/1000 | Loss: 0.00031909
Iteration 257/1000 | Loss: 0.00027431
Iteration 258/1000 | Loss: 0.00037636
Iteration 259/1000 | Loss: 0.00027222
Iteration 260/1000 | Loss: 0.00025781
Iteration 261/1000 | Loss: 0.00035048
Iteration 262/1000 | Loss: 0.00018577
Iteration 263/1000 | Loss: 0.00025674
Iteration 264/1000 | Loss: 0.00023747
Iteration 265/1000 | Loss: 0.00023483
Iteration 266/1000 | Loss: 0.00025845
Iteration 267/1000 | Loss: 0.00027050
Iteration 268/1000 | Loss: 0.00022909
Iteration 269/1000 | Loss: 0.00023537
Iteration 270/1000 | Loss: 0.00023322
Iteration 271/1000 | Loss: 0.00039870
Iteration 272/1000 | Loss: 0.00044079
Iteration 273/1000 | Loss: 0.00045382
Iteration 274/1000 | Loss: 0.00051374
Iteration 275/1000 | Loss: 0.00040252
Iteration 276/1000 | Loss: 0.00024286
Iteration 277/1000 | Loss: 0.00012556
Iteration 278/1000 | Loss: 0.00013420
Iteration 279/1000 | Loss: 0.00024357
Iteration 280/1000 | Loss: 0.00017760
Iteration 281/1000 | Loss: 0.00017383
Iteration 282/1000 | Loss: 0.00050347
Iteration 283/1000 | Loss: 0.00061764
Iteration 284/1000 | Loss: 0.00035129
Iteration 285/1000 | Loss: 0.00058239
Iteration 286/1000 | Loss: 0.00036310
Iteration 287/1000 | Loss: 0.00074025
Iteration 288/1000 | Loss: 0.00031541
Iteration 289/1000 | Loss: 0.00029478
Iteration 290/1000 | Loss: 0.00069221
Iteration 291/1000 | Loss: 0.00048769
Iteration 292/1000 | Loss: 0.00080122
Iteration 293/1000 | Loss: 0.00026352
Iteration 294/1000 | Loss: 0.00026933
Iteration 295/1000 | Loss: 0.00020673
Iteration 296/1000 | Loss: 0.00024830
Iteration 297/1000 | Loss: 0.00024960
Iteration 298/1000 | Loss: 0.00024274
Iteration 299/1000 | Loss: 0.00021917
Iteration 300/1000 | Loss: 0.00022615
Iteration 301/1000 | Loss: 0.00022376
Iteration 302/1000 | Loss: 0.00021749
Iteration 303/1000 | Loss: 0.00016263
Iteration 304/1000 | Loss: 0.00007277
Iteration 305/1000 | Loss: 0.00016899
Iteration 306/1000 | Loss: 0.00018085
Iteration 307/1000 | Loss: 0.00025048
Iteration 308/1000 | Loss: 0.00022200
Iteration 309/1000 | Loss: 0.00021171
Iteration 310/1000 | Loss: 0.00016315
Iteration 311/1000 | Loss: 0.00023336
Iteration 312/1000 | Loss: 0.00018563
Iteration 313/1000 | Loss: 0.00026411
Iteration 314/1000 | Loss: 0.00012687
Iteration 315/1000 | Loss: 0.00026328
Iteration 316/1000 | Loss: 0.00014912
Iteration 317/1000 | Loss: 0.00026468
Iteration 318/1000 | Loss: 0.00024673
Iteration 319/1000 | Loss: 0.00021276
Iteration 320/1000 | Loss: 0.00016966
Iteration 321/1000 | Loss: 0.00006592
Iteration 322/1000 | Loss: 0.00016050
Iteration 323/1000 | Loss: 0.00026665
Iteration 324/1000 | Loss: 0.00017131
Iteration 325/1000 | Loss: 0.00027737
Iteration 326/1000 | Loss: 0.00015421
Iteration 327/1000 | Loss: 0.00022138
Iteration 328/1000 | Loss: 0.00024801
Iteration 329/1000 | Loss: 0.00018746
Iteration 330/1000 | Loss: 0.00021911
Iteration 331/1000 | Loss: 0.00011531
Iteration 332/1000 | Loss: 0.00017827
Iteration 333/1000 | Loss: 0.00026418
Iteration 334/1000 | Loss: 0.00030687
Iteration 335/1000 | Loss: 0.00030376
Iteration 336/1000 | Loss: 0.00035850
Iteration 337/1000 | Loss: 0.00027889
Iteration 338/1000 | Loss: 0.00056701
Iteration 339/1000 | Loss: 0.00028209
Iteration 340/1000 | Loss: 0.00051794
Iteration 341/1000 | Loss: 0.00023652
Iteration 342/1000 | Loss: 0.00024201
Iteration 343/1000 | Loss: 0.00044912
Iteration 344/1000 | Loss: 0.00029143
Iteration 345/1000 | Loss: 0.00054440
Iteration 346/1000 | Loss: 0.00075334
Iteration 347/1000 | Loss: 0.00062291
Iteration 348/1000 | Loss: 0.00079220
Iteration 349/1000 | Loss: 0.00017539
Iteration 350/1000 | Loss: 0.00056125
Iteration 351/1000 | Loss: 0.00020731
Iteration 352/1000 | Loss: 0.00018478
Iteration 353/1000 | Loss: 0.00018022
Iteration 354/1000 | Loss: 0.00013093
Iteration 355/1000 | Loss: 0.00023234
Iteration 356/1000 | Loss: 0.00023712
Iteration 357/1000 | Loss: 0.00012708
Iteration 358/1000 | Loss: 0.00022922
Iteration 359/1000 | Loss: 0.00053803
Iteration 360/1000 | Loss: 0.00040124
Iteration 361/1000 | Loss: 0.00022670
Iteration 362/1000 | Loss: 0.00035740
Iteration 363/1000 | Loss: 0.00060058
Iteration 364/1000 | Loss: 0.00048049
Iteration 365/1000 | Loss: 0.00040022
Iteration 366/1000 | Loss: 0.00029904
Iteration 367/1000 | Loss: 0.00030975
Iteration 368/1000 | Loss: 0.00052113
Iteration 369/1000 | Loss: 0.00029233
Iteration 370/1000 | Loss: 0.00038014
Iteration 371/1000 | Loss: 0.00073924
Iteration 372/1000 | Loss: 0.00087505
Iteration 373/1000 | Loss: 0.00050005
Iteration 374/1000 | Loss: 0.00026620
Iteration 375/1000 | Loss: 0.00017804
Iteration 376/1000 | Loss: 0.00040371
Iteration 377/1000 | Loss: 0.00035611
Iteration 378/1000 | Loss: 0.00051392
Iteration 379/1000 | Loss: 0.00033373
Iteration 380/1000 | Loss: 0.00043757
Iteration 381/1000 | Loss: 0.00082671
Iteration 382/1000 | Loss: 0.00050784
Iteration 383/1000 | Loss: 0.00025981
Iteration 384/1000 | Loss: 0.00046651
Iteration 385/1000 | Loss: 0.00072987
Iteration 386/1000 | Loss: 0.00047311
Iteration 387/1000 | Loss: 0.00022387
Iteration 388/1000 | Loss: 0.00043271
Iteration 389/1000 | Loss: 0.00067152
Iteration 390/1000 | Loss: 0.00068323
Iteration 391/1000 | Loss: 0.00020717
Iteration 392/1000 | Loss: 0.00019926
Iteration 393/1000 | Loss: 0.00019035
Iteration 394/1000 | Loss: 0.00013767
Iteration 395/1000 | Loss: 0.00018791
Iteration 396/1000 | Loss: 0.00014762
Iteration 397/1000 | Loss: 0.00013573
Iteration 398/1000 | Loss: 0.00022400
Iteration 399/1000 | Loss: 0.00025407
Iteration 400/1000 | Loss: 0.00026100
Iteration 401/1000 | Loss: 0.00024400
Iteration 402/1000 | Loss: 0.00026231
Iteration 403/1000 | Loss: 0.00051825
Iteration 404/1000 | Loss: 0.00013809
Iteration 405/1000 | Loss: 0.00034963
Iteration 406/1000 | Loss: 0.00022121
Iteration 407/1000 | Loss: 0.00034553
Iteration 408/1000 | Loss: 0.00024743
Iteration 409/1000 | Loss: 0.00029738
Iteration 410/1000 | Loss: 0.00023707
Iteration 411/1000 | Loss: 0.00023763
Iteration 412/1000 | Loss: 0.00023987
Iteration 413/1000 | Loss: 0.00024178
Iteration 414/1000 | Loss: 0.00014677
Iteration 415/1000 | Loss: 0.00017378
Iteration 416/1000 | Loss: 0.00011871
Iteration 417/1000 | Loss: 0.00009699
Iteration 418/1000 | Loss: 0.00025150
Iteration 419/1000 | Loss: 0.00014130
Iteration 420/1000 | Loss: 0.00014614
Iteration 421/1000 | Loss: 0.00014923
Iteration 422/1000 | Loss: 0.00014231
Iteration 423/1000 | Loss: 0.00011978
Iteration 424/1000 | Loss: 0.00010305
Iteration 425/1000 | Loss: 0.00013088
Iteration 426/1000 | Loss: 0.00010888
Iteration 427/1000 | Loss: 0.00014458
Iteration 428/1000 | Loss: 0.00016903
Iteration 429/1000 | Loss: 0.00009120
Iteration 430/1000 | Loss: 0.00013156
Iteration 431/1000 | Loss: 0.00010315
Iteration 432/1000 | Loss: 0.00010677
Iteration 433/1000 | Loss: 0.00017082
Iteration 434/1000 | Loss: 0.00015117
Iteration 435/1000 | Loss: 0.00016046
Iteration 436/1000 | Loss: 0.00014385
Iteration 437/1000 | Loss: 0.00014429
Iteration 438/1000 | Loss: 0.00014615
Iteration 439/1000 | Loss: 0.00014189
Iteration 440/1000 | Loss: 0.00010781
Iteration 441/1000 | Loss: 0.00044640
Iteration 442/1000 | Loss: 0.00015541
Iteration 443/1000 | Loss: 0.00015616
Iteration 444/1000 | Loss: 0.00013397
Iteration 445/1000 | Loss: 0.00016690
Iteration 446/1000 | Loss: 0.00013176
Iteration 447/1000 | Loss: 0.00017749
Iteration 448/1000 | Loss: 0.00004682
Iteration 449/1000 | Loss: 0.00003398
Iteration 450/1000 | Loss: 0.00003073
Iteration 451/1000 | Loss: 0.00002977
Iteration 452/1000 | Loss: 0.00002906
Iteration 453/1000 | Loss: 0.00002830
Iteration 454/1000 | Loss: 0.00002782
Iteration 455/1000 | Loss: 0.00002749
Iteration 456/1000 | Loss: 0.00002718
Iteration 457/1000 | Loss: 0.00002691
Iteration 458/1000 | Loss: 0.00002668
Iteration 459/1000 | Loss: 0.00002651
Iteration 460/1000 | Loss: 0.00002645
Iteration 461/1000 | Loss: 0.00002636
Iteration 462/1000 | Loss: 0.00002629
Iteration 463/1000 | Loss: 0.00002629
Iteration 464/1000 | Loss: 0.00002625
Iteration 465/1000 | Loss: 0.00002625
Iteration 466/1000 | Loss: 0.00002625
Iteration 467/1000 | Loss: 0.00002624
Iteration 468/1000 | Loss: 0.00002624
Iteration 469/1000 | Loss: 0.00002624
Iteration 470/1000 | Loss: 0.00002623
Iteration 471/1000 | Loss: 0.00002623
Iteration 472/1000 | Loss: 0.00002623
Iteration 473/1000 | Loss: 0.00002623
Iteration 474/1000 | Loss: 0.00002622
Iteration 475/1000 | Loss: 0.00002622
Iteration 476/1000 | Loss: 0.00002622
Iteration 477/1000 | Loss: 0.00002622
Iteration 478/1000 | Loss: 0.00002622
Iteration 479/1000 | Loss: 0.00002621
Iteration 480/1000 | Loss: 0.00002621
Iteration 481/1000 | Loss: 0.00002621
Iteration 482/1000 | Loss: 0.00002620
Iteration 483/1000 | Loss: 0.00002620
Iteration 484/1000 | Loss: 0.00002620
Iteration 485/1000 | Loss: 0.00002620
Iteration 486/1000 | Loss: 0.00002620
Iteration 487/1000 | Loss: 0.00002619
Iteration 488/1000 | Loss: 0.00002619
Iteration 489/1000 | Loss: 0.00002619
Iteration 490/1000 | Loss: 0.00002619
Iteration 491/1000 | Loss: 0.00002619
Iteration 492/1000 | Loss: 0.00002618
Iteration 493/1000 | Loss: 0.00002618
Iteration 494/1000 | Loss: 0.00002617
Iteration 495/1000 | Loss: 0.00002617
Iteration 496/1000 | Loss: 0.00002617
Iteration 497/1000 | Loss: 0.00002617
Iteration 498/1000 | Loss: 0.00002617
Iteration 499/1000 | Loss: 0.00002617
Iteration 500/1000 | Loss: 0.00002616
Iteration 501/1000 | Loss: 0.00002616
Iteration 502/1000 | Loss: 0.00002616
Iteration 503/1000 | Loss: 0.00002616
Iteration 504/1000 | Loss: 0.00002616
Iteration 505/1000 | Loss: 0.00002616
Iteration 506/1000 | Loss: 0.00002616
Iteration 507/1000 | Loss: 0.00002616
Iteration 508/1000 | Loss: 0.00002616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 508. Stopping optimization.
Last 5 losses: [2.61640871030977e-05, 2.61640871030977e-05, 2.61640871030977e-05, 2.61640871030977e-05, 2.61640871030977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.61640871030977e-05

Optimization complete. Final v2v error: 4.094202518463135 mm

Highest mean error: 5.610418796539307 mm for frame 160

Lowest mean error: 3.103032112121582 mm for frame 1

Saving results

Total time: 751.9564301967621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080789
Iteration 2/25 | Loss: 0.00275496
Iteration 3/25 | Loss: 0.00224744
Iteration 4/25 | Loss: 0.00237226
Iteration 5/25 | Loss: 0.00196657
Iteration 6/25 | Loss: 0.00168163
Iteration 7/25 | Loss: 0.00140874
Iteration 8/25 | Loss: 0.00133779
Iteration 9/25 | Loss: 0.00131289
Iteration 10/25 | Loss: 0.00130757
Iteration 11/25 | Loss: 0.00129829
Iteration 12/25 | Loss: 0.00129430
Iteration 13/25 | Loss: 0.00129263
Iteration 14/25 | Loss: 0.00129204
Iteration 15/25 | Loss: 0.00129161
Iteration 16/25 | Loss: 0.00128847
Iteration 17/25 | Loss: 0.00128589
Iteration 18/25 | Loss: 0.00128675
Iteration 19/25 | Loss: 0.00127957
Iteration 20/25 | Loss: 0.00127593
Iteration 21/25 | Loss: 0.00127762
Iteration 22/25 | Loss: 0.00127579
Iteration 23/25 | Loss: 0.00127496
Iteration 24/25 | Loss: 0.00127485
Iteration 25/25 | Loss: 0.00127484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03516126
Iteration 2/25 | Loss: 0.00259063
Iteration 3/25 | Loss: 0.00259063
Iteration 4/25 | Loss: 0.00259063
Iteration 5/25 | Loss: 0.00259063
Iteration 6/25 | Loss: 0.00259063
Iteration 7/25 | Loss: 0.00259063
Iteration 8/25 | Loss: 0.00259063
Iteration 9/25 | Loss: 0.00259063
Iteration 10/25 | Loss: 0.00259063
Iteration 11/25 | Loss: 0.00259063
Iteration 12/25 | Loss: 0.00259063
Iteration 13/25 | Loss: 0.00259063
Iteration 14/25 | Loss: 0.00259063
Iteration 15/25 | Loss: 0.00259063
Iteration 16/25 | Loss: 0.00259063
Iteration 17/25 | Loss: 0.00259063
Iteration 18/25 | Loss: 0.00259063
Iteration 19/25 | Loss: 0.00259063
Iteration 20/25 | Loss: 0.00259063
Iteration 21/25 | Loss: 0.00259063
Iteration 22/25 | Loss: 0.00259063
Iteration 23/25 | Loss: 0.00259063
Iteration 24/25 | Loss: 0.00259063
Iteration 25/25 | Loss: 0.00259063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259063
Iteration 2/1000 | Loss: 0.00004812
Iteration 3/1000 | Loss: 0.00006766
Iteration 4/1000 | Loss: 0.00005062
Iteration 5/1000 | Loss: 0.00003657
Iteration 6/1000 | Loss: 0.00002331
Iteration 7/1000 | Loss: 0.00002221
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002020
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001873
Iteration 15/1000 | Loss: 0.00001873
Iteration 16/1000 | Loss: 0.00001868
Iteration 17/1000 | Loss: 0.00001865
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001846
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001845
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001845
Iteration 32/1000 | Loss: 0.00001845
Iteration 33/1000 | Loss: 0.00001844
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001844
Iteration 36/1000 | Loss: 0.00001844
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001843
Iteration 39/1000 | Loss: 0.00001843
Iteration 40/1000 | Loss: 0.00001843
Iteration 41/1000 | Loss: 0.00001842
Iteration 42/1000 | Loss: 0.00001842
Iteration 43/1000 | Loss: 0.00001842
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001841
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001841
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001841
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001840
Iteration 68/1000 | Loss: 0.00001840
Iteration 69/1000 | Loss: 0.00001840
Iteration 70/1000 | Loss: 0.00001840
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001840
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001839
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001839
Iteration 82/1000 | Loss: 0.00001839
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.836929004639387e-05, 1.836929004639387e-05, 1.836929004639387e-05, 1.836929004639387e-05, 1.836929004639387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.836929004639387e-05

Optimization complete. Final v2v error: 3.704371929168701 mm

Highest mean error: 3.838228464126587 mm for frame 80

Lowest mean error: 3.440390110015869 mm for frame 120

Saving results

Total time: 67.26330757141113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432986
Iteration 2/25 | Loss: 0.00156527
Iteration 3/25 | Loss: 0.00126560
Iteration 4/25 | Loss: 0.00122181
Iteration 5/25 | Loss: 0.00121641
Iteration 6/25 | Loss: 0.00121471
Iteration 7/25 | Loss: 0.00121434
Iteration 8/25 | Loss: 0.00121434
Iteration 9/25 | Loss: 0.00121434
Iteration 10/25 | Loss: 0.00121434
Iteration 11/25 | Loss: 0.00121434
Iteration 12/25 | Loss: 0.00121434
Iteration 13/25 | Loss: 0.00121434
Iteration 14/25 | Loss: 0.00121434
Iteration 15/25 | Loss: 0.00121434
Iteration 16/25 | Loss: 0.00121434
Iteration 17/25 | Loss: 0.00121434
Iteration 18/25 | Loss: 0.00121434
Iteration 19/25 | Loss: 0.00121434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012143388157710433, 0.0012143388157710433, 0.0012143388157710433, 0.0012143388157710433, 0.0012143388157710433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012143388157710433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08940065
Iteration 2/25 | Loss: 0.00362155
Iteration 3/25 | Loss: 0.00362155
Iteration 4/25 | Loss: 0.00362155
Iteration 5/25 | Loss: 0.00362155
Iteration 6/25 | Loss: 0.00362155
Iteration 7/25 | Loss: 0.00362155
Iteration 8/25 | Loss: 0.00362155
Iteration 9/25 | Loss: 0.00362155
Iteration 10/25 | Loss: 0.00362155
Iteration 11/25 | Loss: 0.00362155
Iteration 12/25 | Loss: 0.00362155
Iteration 13/25 | Loss: 0.00362155
Iteration 14/25 | Loss: 0.00362155
Iteration 15/25 | Loss: 0.00362155
Iteration 16/25 | Loss: 0.00362155
Iteration 17/25 | Loss: 0.00362155
Iteration 18/25 | Loss: 0.00362155
Iteration 19/25 | Loss: 0.00362155
Iteration 20/25 | Loss: 0.00362155
Iteration 21/25 | Loss: 0.00362155
Iteration 22/25 | Loss: 0.00362155
Iteration 23/25 | Loss: 0.00362155
Iteration 24/25 | Loss: 0.00362155
Iteration 25/25 | Loss: 0.00362155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0036215493455529213, 0.0036215493455529213, 0.0036215493455529213, 0.0036215493455529213, 0.0036215493455529213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036215493455529213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362155
Iteration 2/1000 | Loss: 0.00005734
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002083
Iteration 5/1000 | Loss: 0.00001781
Iteration 6/1000 | Loss: 0.00001632
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001479
Iteration 9/1000 | Loss: 0.00001403
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001233
Iteration 23/1000 | Loss: 0.00001228
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001226
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001224
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001219
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001217
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001213
Iteration 96/1000 | Loss: 0.00001213
Iteration 97/1000 | Loss: 0.00001213
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001210
Iteration 116/1000 | Loss: 0.00001210
Iteration 117/1000 | Loss: 0.00001210
Iteration 118/1000 | Loss: 0.00001210
Iteration 119/1000 | Loss: 0.00001210
Iteration 120/1000 | Loss: 0.00001210
Iteration 121/1000 | Loss: 0.00001210
Iteration 122/1000 | Loss: 0.00001210
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001210
Iteration 125/1000 | Loss: 0.00001210
Iteration 126/1000 | Loss: 0.00001210
Iteration 127/1000 | Loss: 0.00001210
Iteration 128/1000 | Loss: 0.00001210
Iteration 129/1000 | Loss: 0.00001210
Iteration 130/1000 | Loss: 0.00001210
Iteration 131/1000 | Loss: 0.00001210
Iteration 132/1000 | Loss: 0.00001210
Iteration 133/1000 | Loss: 0.00001210
Iteration 134/1000 | Loss: 0.00001210
Iteration 135/1000 | Loss: 0.00001210
Iteration 136/1000 | Loss: 0.00001210
Iteration 137/1000 | Loss: 0.00001210
Iteration 138/1000 | Loss: 0.00001210
Iteration 139/1000 | Loss: 0.00001210
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001210
Iteration 142/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.2099562809453346e-05, 1.2099562809453346e-05, 1.2099562809453346e-05, 1.2099562809453346e-05, 1.2099562809453346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2099562809453346e-05

Optimization complete. Final v2v error: 3.013787031173706 mm

Highest mean error: 3.3604493141174316 mm for frame 73

Lowest mean error: 2.689096450805664 mm for frame 114

Saving results

Total time: 40.169809103012085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867161
Iteration 2/25 | Loss: 0.00132066
Iteration 3/25 | Loss: 0.00119273
Iteration 4/25 | Loss: 0.00118541
Iteration 5/25 | Loss: 0.00118454
Iteration 6/25 | Loss: 0.00118454
Iteration 7/25 | Loss: 0.00118454
Iteration 8/25 | Loss: 0.00118454
Iteration 9/25 | Loss: 0.00118454
Iteration 10/25 | Loss: 0.00118454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001184541266411543, 0.001184541266411543, 0.001184541266411543, 0.001184541266411543, 0.001184541266411543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184541266411543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12923694
Iteration 2/25 | Loss: 0.00342930
Iteration 3/25 | Loss: 0.00342930
Iteration 4/25 | Loss: 0.00342930
Iteration 5/25 | Loss: 0.00342930
Iteration 6/25 | Loss: 0.00342930
Iteration 7/25 | Loss: 0.00342930
Iteration 8/25 | Loss: 0.00342930
Iteration 9/25 | Loss: 0.00342930
Iteration 10/25 | Loss: 0.00342930
Iteration 11/25 | Loss: 0.00342930
Iteration 12/25 | Loss: 0.00342930
Iteration 13/25 | Loss: 0.00342930
Iteration 14/25 | Loss: 0.00342930
Iteration 15/25 | Loss: 0.00342930
Iteration 16/25 | Loss: 0.00342930
Iteration 17/25 | Loss: 0.00342930
Iteration 18/25 | Loss: 0.00342930
Iteration 19/25 | Loss: 0.00342930
Iteration 20/25 | Loss: 0.00342930
Iteration 21/25 | Loss: 0.00342930
Iteration 22/25 | Loss: 0.00342930
Iteration 23/25 | Loss: 0.00342930
Iteration 24/25 | Loss: 0.00342930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00342929782345891, 0.00342929782345891, 0.00342929782345891, 0.00342929782345891, 0.00342929782345891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00342929782345891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342930
Iteration 2/1000 | Loss: 0.00003545
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001108
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001107
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001105
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001100
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001095
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001091
Iteration 58/1000 | Loss: 0.00001091
Iteration 59/1000 | Loss: 0.00001091
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001088
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001088
Iteration 74/1000 | Loss: 0.00001088
Iteration 75/1000 | Loss: 0.00001088
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001088
Iteration 81/1000 | Loss: 0.00001088
Iteration 82/1000 | Loss: 0.00001087
Iteration 83/1000 | Loss: 0.00001087
Iteration 84/1000 | Loss: 0.00001087
Iteration 85/1000 | Loss: 0.00001086
Iteration 86/1000 | Loss: 0.00001086
Iteration 87/1000 | Loss: 0.00001086
Iteration 88/1000 | Loss: 0.00001086
Iteration 89/1000 | Loss: 0.00001086
Iteration 90/1000 | Loss: 0.00001086
Iteration 91/1000 | Loss: 0.00001085
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001085
Iteration 94/1000 | Loss: 0.00001085
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001085
Iteration 99/1000 | Loss: 0.00001085
Iteration 100/1000 | Loss: 0.00001085
Iteration 101/1000 | Loss: 0.00001085
Iteration 102/1000 | Loss: 0.00001085
Iteration 103/1000 | Loss: 0.00001085
Iteration 104/1000 | Loss: 0.00001085
Iteration 105/1000 | Loss: 0.00001085
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001084
Iteration 111/1000 | Loss: 0.00001084
Iteration 112/1000 | Loss: 0.00001084
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001084
Iteration 120/1000 | Loss: 0.00001084
Iteration 121/1000 | Loss: 0.00001084
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001083
Iteration 124/1000 | Loss: 0.00001083
Iteration 125/1000 | Loss: 0.00001083
Iteration 126/1000 | Loss: 0.00001083
Iteration 127/1000 | Loss: 0.00001083
Iteration 128/1000 | Loss: 0.00001083
Iteration 129/1000 | Loss: 0.00001083
Iteration 130/1000 | Loss: 0.00001083
Iteration 131/1000 | Loss: 0.00001083
Iteration 132/1000 | Loss: 0.00001083
Iteration 133/1000 | Loss: 0.00001083
Iteration 134/1000 | Loss: 0.00001083
Iteration 135/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.083096049114829e-05, 1.083096049114829e-05, 1.083096049114829e-05, 1.083096049114829e-05, 1.083096049114829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.083096049114829e-05

Optimization complete. Final v2v error: 2.827019691467285 mm

Highest mean error: 3.1610875129699707 mm for frame 0

Lowest mean error: 2.5970091819763184 mm for frame 214

Saving results

Total time: 35.92036318778992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397517
Iteration 2/25 | Loss: 0.00129364
Iteration 3/25 | Loss: 0.00121549
Iteration 4/25 | Loss: 0.00120394
Iteration 5/25 | Loss: 0.00119963
Iteration 6/25 | Loss: 0.00119843
Iteration 7/25 | Loss: 0.00119836
Iteration 8/25 | Loss: 0.00119836
Iteration 9/25 | Loss: 0.00119836
Iteration 10/25 | Loss: 0.00119836
Iteration 11/25 | Loss: 0.00119836
Iteration 12/25 | Loss: 0.00119836
Iteration 13/25 | Loss: 0.00119836
Iteration 14/25 | Loss: 0.00119836
Iteration 15/25 | Loss: 0.00119836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001198361162096262, 0.001198361162096262, 0.001198361162096262, 0.001198361162096262, 0.001198361162096262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001198361162096262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62902725
Iteration 2/25 | Loss: 0.00342956
Iteration 3/25 | Loss: 0.00342956
Iteration 4/25 | Loss: 0.00342956
Iteration 5/25 | Loss: 0.00342955
Iteration 6/25 | Loss: 0.00342955
Iteration 7/25 | Loss: 0.00342955
Iteration 8/25 | Loss: 0.00342955
Iteration 9/25 | Loss: 0.00342955
Iteration 10/25 | Loss: 0.00342955
Iteration 11/25 | Loss: 0.00342955
Iteration 12/25 | Loss: 0.00342955
Iteration 13/25 | Loss: 0.00342955
Iteration 14/25 | Loss: 0.00342955
Iteration 15/25 | Loss: 0.00342955
Iteration 16/25 | Loss: 0.00342955
Iteration 17/25 | Loss: 0.00342955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0034295525401830673, 0.0034295525401830673, 0.0034295525401830673, 0.0034295525401830673, 0.0034295525401830673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034295525401830673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342955
Iteration 2/1000 | Loss: 0.00003913
Iteration 3/1000 | Loss: 0.00002397
Iteration 4/1000 | Loss: 0.00002115
Iteration 5/1000 | Loss: 0.00001880
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001582
Iteration 9/1000 | Loss: 0.00001533
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001459
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001402
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001402
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001402
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001396
Iteration 35/1000 | Loss: 0.00001396
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001395
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001393
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001392
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001391
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001390
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001384
Iteration 74/1000 | Loss: 0.00001384
Iteration 75/1000 | Loss: 0.00001384
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001383
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001381
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001380
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001380
Iteration 112/1000 | Loss: 0.00001380
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001380
Iteration 115/1000 | Loss: 0.00001380
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001380
Iteration 118/1000 | Loss: 0.00001380
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001380
Iteration 121/1000 | Loss: 0.00001380
Iteration 122/1000 | Loss: 0.00001380
Iteration 123/1000 | Loss: 0.00001380
Iteration 124/1000 | Loss: 0.00001380
Iteration 125/1000 | Loss: 0.00001380
Iteration 126/1000 | Loss: 0.00001380
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3799053704133257e-05, 1.3799053704133257e-05, 1.3799053704133257e-05, 1.3799053704133257e-05, 1.3799053704133257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3799053704133257e-05

Optimization complete. Final v2v error: 3.2993829250335693 mm

Highest mean error: 3.663435220718384 mm for frame 75

Lowest mean error: 3.092726707458496 mm for frame 90

Saving results

Total time: 37.389057874679565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028422
Iteration 2/25 | Loss: 0.00168757
Iteration 3/25 | Loss: 0.00130240
Iteration 4/25 | Loss: 0.00124453
Iteration 5/25 | Loss: 0.00124122
Iteration 6/25 | Loss: 0.00122732
Iteration 7/25 | Loss: 0.00121998
Iteration 8/25 | Loss: 0.00120834
Iteration 9/25 | Loss: 0.00120545
Iteration 10/25 | Loss: 0.00120077
Iteration 11/25 | Loss: 0.00119978
Iteration 12/25 | Loss: 0.00119953
Iteration 13/25 | Loss: 0.00119946
Iteration 14/25 | Loss: 0.00119946
Iteration 15/25 | Loss: 0.00119946
Iteration 16/25 | Loss: 0.00119946
Iteration 17/25 | Loss: 0.00119946
Iteration 18/25 | Loss: 0.00119946
Iteration 19/25 | Loss: 0.00119945
Iteration 20/25 | Loss: 0.00119945
Iteration 21/25 | Loss: 0.00119945
Iteration 22/25 | Loss: 0.00119945
Iteration 23/25 | Loss: 0.00119945
Iteration 24/25 | Loss: 0.00119945
Iteration 25/25 | Loss: 0.00119945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64213502
Iteration 2/25 | Loss: 0.00330333
Iteration 3/25 | Loss: 0.00330332
Iteration 4/25 | Loss: 0.00330332
Iteration 5/25 | Loss: 0.00330332
Iteration 6/25 | Loss: 0.00326980
Iteration 7/25 | Loss: 0.00326980
Iteration 8/25 | Loss: 0.00326979
Iteration 9/25 | Loss: 0.00326979
Iteration 10/25 | Loss: 0.00326979
Iteration 11/25 | Loss: 0.00326979
Iteration 12/25 | Loss: 0.00326979
Iteration 13/25 | Loss: 0.00326979
Iteration 14/25 | Loss: 0.00326979
Iteration 15/25 | Loss: 0.00326979
Iteration 16/25 | Loss: 0.00326979
Iteration 17/25 | Loss: 0.00326979
Iteration 18/25 | Loss: 0.00326979
Iteration 19/25 | Loss: 0.00326979
Iteration 20/25 | Loss: 0.00326979
Iteration 21/25 | Loss: 0.00326979
Iteration 22/25 | Loss: 0.00326979
Iteration 23/25 | Loss: 0.00326979
Iteration 24/25 | Loss: 0.00326979
Iteration 25/25 | Loss: 0.00326979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326979
Iteration 2/1000 | Loss: 0.00006933
Iteration 3/1000 | Loss: 0.00002502
Iteration 4/1000 | Loss: 0.00006576
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00001967
Iteration 7/1000 | Loss: 0.00001851
Iteration 8/1000 | Loss: 0.00004699
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00005108
Iteration 11/1000 | Loss: 0.00002754
Iteration 12/1000 | Loss: 0.00002430
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00002837
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001547
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00002130
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00003076
Iteration 23/1000 | Loss: 0.00001827
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001514
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00002076
Iteration 28/1000 | Loss: 0.00017919
Iteration 29/1000 | Loss: 0.00006621
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001500
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001497
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001496
Iteration 38/1000 | Loss: 0.00001496
Iteration 39/1000 | Loss: 0.00001496
Iteration 40/1000 | Loss: 0.00001496
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001495
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001494
Iteration 54/1000 | Loss: 0.00001494
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001493
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001486
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001485
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001483
Iteration 89/1000 | Loss: 0.00001483
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001481
Iteration 99/1000 | Loss: 0.00001481
Iteration 100/1000 | Loss: 0.00001481
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001480
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00003144
Iteration 105/1000 | Loss: 0.00009033
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001475
Iteration 111/1000 | Loss: 0.00001475
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001470
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001470
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001470
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001470
Iteration 131/1000 | Loss: 0.00001470
Iteration 132/1000 | Loss: 0.00001470
Iteration 133/1000 | Loss: 0.00001470
Iteration 134/1000 | Loss: 0.00001470
Iteration 135/1000 | Loss: 0.00001470
Iteration 136/1000 | Loss: 0.00001470
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001470
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Iteration 143/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4695381651108619e-05, 1.4695381651108619e-05, 1.4695381651108619e-05, 1.4695381651108619e-05, 1.4695381651108619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4695381651108619e-05

Optimization complete. Final v2v error: 3.3077566623687744 mm

Highest mean error: 3.6444921493530273 mm for frame 116

Lowest mean error: 2.9852867126464844 mm for frame 227

Saving results

Total time: 84.31398677825928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469164
Iteration 2/25 | Loss: 0.00151058
Iteration 3/25 | Loss: 0.00132470
Iteration 4/25 | Loss: 0.00130183
Iteration 5/25 | Loss: 0.00129993
Iteration 6/25 | Loss: 0.00129993
Iteration 7/25 | Loss: 0.00129993
Iteration 8/25 | Loss: 0.00129993
Iteration 9/25 | Loss: 0.00129993
Iteration 10/25 | Loss: 0.00129993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012999315513297915, 0.0012999315513297915, 0.0012999315513297915, 0.0012999315513297915, 0.0012999315513297915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012999315513297915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13992643
Iteration 2/25 | Loss: 0.00317598
Iteration 3/25 | Loss: 0.00317597
Iteration 4/25 | Loss: 0.00317597
Iteration 5/25 | Loss: 0.00317596
Iteration 6/25 | Loss: 0.00317596
Iteration 7/25 | Loss: 0.00317596
Iteration 8/25 | Loss: 0.00317596
Iteration 9/25 | Loss: 0.00317596
Iteration 10/25 | Loss: 0.00317596
Iteration 11/25 | Loss: 0.00317596
Iteration 12/25 | Loss: 0.00317596
Iteration 13/25 | Loss: 0.00317596
Iteration 14/25 | Loss: 0.00317596
Iteration 15/25 | Loss: 0.00317596
Iteration 16/25 | Loss: 0.00317596
Iteration 17/25 | Loss: 0.00317596
Iteration 18/25 | Loss: 0.00317596
Iteration 19/25 | Loss: 0.00317596
Iteration 20/25 | Loss: 0.00317596
Iteration 21/25 | Loss: 0.00317596
Iteration 22/25 | Loss: 0.00317596
Iteration 23/25 | Loss: 0.00317596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003175963182002306, 0.003175963182002306, 0.003175963182002306, 0.003175963182002306, 0.003175963182002306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003175963182002306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00317596
Iteration 2/1000 | Loss: 0.00004881
Iteration 3/1000 | Loss: 0.00002913
Iteration 4/1000 | Loss: 0.00002580
Iteration 5/1000 | Loss: 0.00002407
Iteration 6/1000 | Loss: 0.00002272
Iteration 7/1000 | Loss: 0.00002201
Iteration 8/1000 | Loss: 0.00002139
Iteration 9/1000 | Loss: 0.00002078
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00001993
Iteration 12/1000 | Loss: 0.00001964
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001950
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001946
Iteration 17/1000 | Loss: 0.00001943
Iteration 18/1000 | Loss: 0.00001942
Iteration 19/1000 | Loss: 0.00001942
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001932
Iteration 23/1000 | Loss: 0.00001931
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001927
Iteration 28/1000 | Loss: 0.00001927
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001926
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001922
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001921
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001916
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001910
Iteration 102/1000 | Loss: 0.00001910
Iteration 103/1000 | Loss: 0.00001910
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001909
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001906
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001905
Iteration 132/1000 | Loss: 0.00001905
Iteration 133/1000 | Loss: 0.00001905
Iteration 134/1000 | Loss: 0.00001905
Iteration 135/1000 | Loss: 0.00001905
Iteration 136/1000 | Loss: 0.00001905
Iteration 137/1000 | Loss: 0.00001905
Iteration 138/1000 | Loss: 0.00001905
Iteration 139/1000 | Loss: 0.00001905
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001904
Iteration 142/1000 | Loss: 0.00001904
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001902
Iteration 148/1000 | Loss: 0.00001902
Iteration 149/1000 | Loss: 0.00001902
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Iteration 153/1000 | Loss: 0.00001900
Iteration 154/1000 | Loss: 0.00001900
Iteration 155/1000 | Loss: 0.00001900
Iteration 156/1000 | Loss: 0.00001900
Iteration 157/1000 | Loss: 0.00001900
Iteration 158/1000 | Loss: 0.00001900
Iteration 159/1000 | Loss: 0.00001899
Iteration 160/1000 | Loss: 0.00001899
Iteration 161/1000 | Loss: 0.00001899
Iteration 162/1000 | Loss: 0.00001899
Iteration 163/1000 | Loss: 0.00001899
Iteration 164/1000 | Loss: 0.00001899
Iteration 165/1000 | Loss: 0.00001899
Iteration 166/1000 | Loss: 0.00001899
Iteration 167/1000 | Loss: 0.00001899
Iteration 168/1000 | Loss: 0.00001899
Iteration 169/1000 | Loss: 0.00001899
Iteration 170/1000 | Loss: 0.00001898
Iteration 171/1000 | Loss: 0.00001898
Iteration 172/1000 | Loss: 0.00001898
Iteration 173/1000 | Loss: 0.00001898
Iteration 174/1000 | Loss: 0.00001898
Iteration 175/1000 | Loss: 0.00001897
Iteration 176/1000 | Loss: 0.00001897
Iteration 177/1000 | Loss: 0.00001897
Iteration 178/1000 | Loss: 0.00001897
Iteration 179/1000 | Loss: 0.00001897
Iteration 180/1000 | Loss: 0.00001896
Iteration 181/1000 | Loss: 0.00001896
Iteration 182/1000 | Loss: 0.00001896
Iteration 183/1000 | Loss: 0.00001896
Iteration 184/1000 | Loss: 0.00001896
Iteration 185/1000 | Loss: 0.00001895
Iteration 186/1000 | Loss: 0.00001895
Iteration 187/1000 | Loss: 0.00001895
Iteration 188/1000 | Loss: 0.00001895
Iteration 189/1000 | Loss: 0.00001895
Iteration 190/1000 | Loss: 0.00001895
Iteration 191/1000 | Loss: 0.00001894
Iteration 192/1000 | Loss: 0.00001894
Iteration 193/1000 | Loss: 0.00001894
Iteration 194/1000 | Loss: 0.00001894
Iteration 195/1000 | Loss: 0.00001894
Iteration 196/1000 | Loss: 0.00001894
Iteration 197/1000 | Loss: 0.00001894
Iteration 198/1000 | Loss: 0.00001893
Iteration 199/1000 | Loss: 0.00001893
Iteration 200/1000 | Loss: 0.00001893
Iteration 201/1000 | Loss: 0.00001893
Iteration 202/1000 | Loss: 0.00001893
Iteration 203/1000 | Loss: 0.00001893
Iteration 204/1000 | Loss: 0.00001892
Iteration 205/1000 | Loss: 0.00001892
Iteration 206/1000 | Loss: 0.00001892
Iteration 207/1000 | Loss: 0.00001892
Iteration 208/1000 | Loss: 0.00001892
Iteration 209/1000 | Loss: 0.00001892
Iteration 210/1000 | Loss: 0.00001891
Iteration 211/1000 | Loss: 0.00001891
Iteration 212/1000 | Loss: 0.00001891
Iteration 213/1000 | Loss: 0.00001891
Iteration 214/1000 | Loss: 0.00001891
Iteration 215/1000 | Loss: 0.00001891
Iteration 216/1000 | Loss: 0.00001891
Iteration 217/1000 | Loss: 0.00001891
Iteration 218/1000 | Loss: 0.00001891
Iteration 219/1000 | Loss: 0.00001891
Iteration 220/1000 | Loss: 0.00001891
Iteration 221/1000 | Loss: 0.00001891
Iteration 222/1000 | Loss: 0.00001890
Iteration 223/1000 | Loss: 0.00001890
Iteration 224/1000 | Loss: 0.00001890
Iteration 225/1000 | Loss: 0.00001890
Iteration 226/1000 | Loss: 0.00001890
Iteration 227/1000 | Loss: 0.00001890
Iteration 228/1000 | Loss: 0.00001890
Iteration 229/1000 | Loss: 0.00001890
Iteration 230/1000 | Loss: 0.00001890
Iteration 231/1000 | Loss: 0.00001889
Iteration 232/1000 | Loss: 0.00001889
Iteration 233/1000 | Loss: 0.00001889
Iteration 234/1000 | Loss: 0.00001889
Iteration 235/1000 | Loss: 0.00001889
Iteration 236/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.8894865206675604e-05, 1.8894865206675604e-05, 1.8894865206675604e-05, 1.8894865206675604e-05, 1.8894865206675604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8894865206675604e-05

Optimization complete. Final v2v error: 3.7288482189178467 mm

Highest mean error: 3.960812568664551 mm for frame 70

Lowest mean error: 3.382134199142456 mm for frame 0

Saving results

Total time: 49.70435976982117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608771
Iteration 2/25 | Loss: 0.00131557
Iteration 3/25 | Loss: 0.00120700
Iteration 4/25 | Loss: 0.00119818
Iteration 5/25 | Loss: 0.00119676
Iteration 6/25 | Loss: 0.00119676
Iteration 7/25 | Loss: 0.00119676
Iteration 8/25 | Loss: 0.00119676
Iteration 9/25 | Loss: 0.00119676
Iteration 10/25 | Loss: 0.00119676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011967574246227741, 0.0011967574246227741, 0.0011967574246227741, 0.0011967574246227741, 0.0011967574246227741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011967574246227741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.47778034
Iteration 2/25 | Loss: 0.00322987
Iteration 3/25 | Loss: 0.00322983
Iteration 4/25 | Loss: 0.00322983
Iteration 5/25 | Loss: 0.00322983
Iteration 6/25 | Loss: 0.00322983
Iteration 7/25 | Loss: 0.00322983
Iteration 8/25 | Loss: 0.00322983
Iteration 9/25 | Loss: 0.00322983
Iteration 10/25 | Loss: 0.00322983
Iteration 11/25 | Loss: 0.00322983
Iteration 12/25 | Loss: 0.00322983
Iteration 13/25 | Loss: 0.00322983
Iteration 14/25 | Loss: 0.00322983
Iteration 15/25 | Loss: 0.00322983
Iteration 16/25 | Loss: 0.00322983
Iteration 17/25 | Loss: 0.00322983
Iteration 18/25 | Loss: 0.00322983
Iteration 19/25 | Loss: 0.00322983
Iteration 20/25 | Loss: 0.00322983
Iteration 21/25 | Loss: 0.00322983
Iteration 22/25 | Loss: 0.00322983
Iteration 23/25 | Loss: 0.00322983
Iteration 24/25 | Loss: 0.00322983
Iteration 25/25 | Loss: 0.00322983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322983
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00001970
Iteration 4/1000 | Loss: 0.00001761
Iteration 5/1000 | Loss: 0.00001639
Iteration 6/1000 | Loss: 0.00001553
Iteration 7/1000 | Loss: 0.00001497
Iteration 8/1000 | Loss: 0.00001441
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001377
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001359
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001358
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001338
Iteration 51/1000 | Loss: 0.00001338
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001325
Iteration 139/1000 | Loss: 0.00001325
Iteration 140/1000 | Loss: 0.00001325
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001324
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Iteration 147/1000 | Loss: 0.00001324
Iteration 148/1000 | Loss: 0.00001324
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Iteration 152/1000 | Loss: 0.00001323
Iteration 153/1000 | Loss: 0.00001323
Iteration 154/1000 | Loss: 0.00001322
Iteration 155/1000 | Loss: 0.00001322
Iteration 156/1000 | Loss: 0.00001322
Iteration 157/1000 | Loss: 0.00001322
Iteration 158/1000 | Loss: 0.00001322
Iteration 159/1000 | Loss: 0.00001322
Iteration 160/1000 | Loss: 0.00001321
Iteration 161/1000 | Loss: 0.00001321
Iteration 162/1000 | Loss: 0.00001321
Iteration 163/1000 | Loss: 0.00001321
Iteration 164/1000 | Loss: 0.00001321
Iteration 165/1000 | Loss: 0.00001321
Iteration 166/1000 | Loss: 0.00001321
Iteration 167/1000 | Loss: 0.00001321
Iteration 168/1000 | Loss: 0.00001321
Iteration 169/1000 | Loss: 0.00001321
Iteration 170/1000 | Loss: 0.00001321
Iteration 171/1000 | Loss: 0.00001320
Iteration 172/1000 | Loss: 0.00001320
Iteration 173/1000 | Loss: 0.00001320
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001320
Iteration 176/1000 | Loss: 0.00001320
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001320
Iteration 179/1000 | Loss: 0.00001320
Iteration 180/1000 | Loss: 0.00001320
Iteration 181/1000 | Loss: 0.00001319
Iteration 182/1000 | Loss: 0.00001319
Iteration 183/1000 | Loss: 0.00001319
Iteration 184/1000 | Loss: 0.00001319
Iteration 185/1000 | Loss: 0.00001319
Iteration 186/1000 | Loss: 0.00001319
Iteration 187/1000 | Loss: 0.00001319
Iteration 188/1000 | Loss: 0.00001318
Iteration 189/1000 | Loss: 0.00001318
Iteration 190/1000 | Loss: 0.00001318
Iteration 191/1000 | Loss: 0.00001318
Iteration 192/1000 | Loss: 0.00001318
Iteration 193/1000 | Loss: 0.00001318
Iteration 194/1000 | Loss: 0.00001318
Iteration 195/1000 | Loss: 0.00001318
Iteration 196/1000 | Loss: 0.00001318
Iteration 197/1000 | Loss: 0.00001318
Iteration 198/1000 | Loss: 0.00001318
Iteration 199/1000 | Loss: 0.00001317
Iteration 200/1000 | Loss: 0.00001317
Iteration 201/1000 | Loss: 0.00001317
Iteration 202/1000 | Loss: 0.00001317
Iteration 203/1000 | Loss: 0.00001317
Iteration 204/1000 | Loss: 0.00001317
Iteration 205/1000 | Loss: 0.00001317
Iteration 206/1000 | Loss: 0.00001317
Iteration 207/1000 | Loss: 0.00001317
Iteration 208/1000 | Loss: 0.00001317
Iteration 209/1000 | Loss: 0.00001317
Iteration 210/1000 | Loss: 0.00001317
Iteration 211/1000 | Loss: 0.00001317
Iteration 212/1000 | Loss: 0.00001317
Iteration 213/1000 | Loss: 0.00001316
Iteration 214/1000 | Loss: 0.00001316
Iteration 215/1000 | Loss: 0.00001316
Iteration 216/1000 | Loss: 0.00001316
Iteration 217/1000 | Loss: 0.00001316
Iteration 218/1000 | Loss: 0.00001316
Iteration 219/1000 | Loss: 0.00001316
Iteration 220/1000 | Loss: 0.00001316
Iteration 221/1000 | Loss: 0.00001316
Iteration 222/1000 | Loss: 0.00001316
Iteration 223/1000 | Loss: 0.00001316
Iteration 224/1000 | Loss: 0.00001316
Iteration 225/1000 | Loss: 0.00001316
Iteration 226/1000 | Loss: 0.00001316
Iteration 227/1000 | Loss: 0.00001316
Iteration 228/1000 | Loss: 0.00001316
Iteration 229/1000 | Loss: 0.00001316
Iteration 230/1000 | Loss: 0.00001316
Iteration 231/1000 | Loss: 0.00001316
Iteration 232/1000 | Loss: 0.00001316
Iteration 233/1000 | Loss: 0.00001316
Iteration 234/1000 | Loss: 0.00001316
Iteration 235/1000 | Loss: 0.00001316
Iteration 236/1000 | Loss: 0.00001316
Iteration 237/1000 | Loss: 0.00001316
Iteration 238/1000 | Loss: 0.00001316
Iteration 239/1000 | Loss: 0.00001316
Iteration 240/1000 | Loss: 0.00001316
Iteration 241/1000 | Loss: 0.00001316
Iteration 242/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.3162814866518602e-05, 1.3162814866518602e-05, 1.3162814866518602e-05, 1.3162814866518602e-05, 1.3162814866518602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3162814866518602e-05

Optimization complete. Final v2v error: 3.1182258129119873 mm

Highest mean error: 3.406813383102417 mm for frame 120

Lowest mean error: 2.7103264331817627 mm for frame 200

Saving results

Total time: 47.1985399723053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_nl_1184/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_nl_1184/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00639925
Iteration 2/25 | Loss: 0.00168751
Iteration 3/25 | Loss: 0.00137432
Iteration 4/25 | Loss: 0.00131889
Iteration 5/25 | Loss: 0.00129264
Iteration 6/25 | Loss: 0.00129827
Iteration 7/25 | Loss: 0.00128863
Iteration 8/25 | Loss: 0.00127868
Iteration 9/25 | Loss: 0.00128331
Iteration 10/25 | Loss: 0.00127323
Iteration 11/25 | Loss: 0.00127465
Iteration 12/25 | Loss: 0.00126757
Iteration 13/25 | Loss: 0.00126958
Iteration 14/25 | Loss: 0.00126709
Iteration 15/25 | Loss: 0.00126442
Iteration 16/25 | Loss: 0.00126778
Iteration 17/25 | Loss: 0.00126378
Iteration 18/25 | Loss: 0.00126228
Iteration 19/25 | Loss: 0.00126556
Iteration 20/25 | Loss: 0.00126187
Iteration 21/25 | Loss: 0.00126546
Iteration 22/25 | Loss: 0.00126644
Iteration 23/25 | Loss: 0.00126233
Iteration 24/25 | Loss: 0.00126339
Iteration 25/25 | Loss: 0.00125989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51512182
Iteration 2/25 | Loss: 0.00447329
Iteration 3/25 | Loss: 0.00437040
Iteration 4/25 | Loss: 0.00437040
Iteration 5/25 | Loss: 0.00437040
Iteration 6/25 | Loss: 0.00437040
Iteration 7/25 | Loss: 0.00437040
Iteration 8/25 | Loss: 0.00437039
Iteration 9/25 | Loss: 0.00437039
Iteration 10/25 | Loss: 0.00437039
Iteration 11/25 | Loss: 0.00437039
Iteration 12/25 | Loss: 0.00437039
Iteration 13/25 | Loss: 0.00437039
Iteration 14/25 | Loss: 0.00437039
Iteration 15/25 | Loss: 0.00437039
Iteration 16/25 | Loss: 0.00437039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004370393697172403, 0.004370393697172403, 0.004370393697172403, 0.004370393697172403, 0.004370393697172403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004370393697172403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00437039
Iteration 2/1000 | Loss: 0.00013180
Iteration 3/1000 | Loss: 0.00007941
Iteration 4/1000 | Loss: 0.00003289
Iteration 5/1000 | Loss: 0.00002738
Iteration 6/1000 | Loss: 0.00003223
Iteration 7/1000 | Loss: 0.00004438
Iteration 8/1000 | Loss: 0.00002209
Iteration 9/1000 | Loss: 0.00003007
Iteration 10/1000 | Loss: 0.00003794
Iteration 11/1000 | Loss: 0.00002093
Iteration 12/1000 | Loss: 0.00004304
Iteration 13/1000 | Loss: 0.00002025
Iteration 14/1000 | Loss: 0.00002002
Iteration 15/1000 | Loss: 0.00042445
Iteration 16/1000 | Loss: 0.00002372
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00003499
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00002690
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001821
Iteration 23/1000 | Loss: 0.00001803
Iteration 24/1000 | Loss: 0.00001801
Iteration 25/1000 | Loss: 0.00001800
Iteration 26/1000 | Loss: 0.00001799
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001798
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00003473
Iteration 39/1000 | Loss: 0.00003473
Iteration 40/1000 | Loss: 0.00021562
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001784
Iteration 61/1000 | Loss: 0.00001784
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00003400
Iteration 66/1000 | Loss: 0.00001943
Iteration 67/1000 | Loss: 0.00002193
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001774
Iteration 73/1000 | Loss: 0.00001774
Iteration 74/1000 | Loss: 0.00001774
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001774
Iteration 77/1000 | Loss: 0.00001774
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001773
Iteration 82/1000 | Loss: 0.00001773
Iteration 83/1000 | Loss: 0.00001773
Iteration 84/1000 | Loss: 0.00001773
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001772
Iteration 90/1000 | Loss: 0.00001772
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001768
Iteration 95/1000 | Loss: 0.00001768
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001766
Iteration 103/1000 | Loss: 0.00001766
Iteration 104/1000 | Loss: 0.00001764
Iteration 105/1000 | Loss: 0.00001764
Iteration 106/1000 | Loss: 0.00001764
Iteration 107/1000 | Loss: 0.00001763
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001763
Iteration 113/1000 | Loss: 0.00001763
Iteration 114/1000 | Loss: 0.00001763
Iteration 115/1000 | Loss: 0.00001762
Iteration 116/1000 | Loss: 0.00001762
Iteration 117/1000 | Loss: 0.00001762
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001761
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001760
Iteration 123/1000 | Loss: 0.00001760
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Iteration 139/1000 | Loss: 0.00001759
Iteration 140/1000 | Loss: 0.00001758
Iteration 141/1000 | Loss: 0.00001758
Iteration 142/1000 | Loss: 0.00001758
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001758
Iteration 145/1000 | Loss: 0.00001757
Iteration 146/1000 | Loss: 0.00001757
Iteration 147/1000 | Loss: 0.00001757
Iteration 148/1000 | Loss: 0.00001757
Iteration 149/1000 | Loss: 0.00001756
Iteration 150/1000 | Loss: 0.00001756
Iteration 151/1000 | Loss: 0.00001756
Iteration 152/1000 | Loss: 0.00001756
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001756
Iteration 155/1000 | Loss: 0.00001756
Iteration 156/1000 | Loss: 0.00001756
Iteration 157/1000 | Loss: 0.00001756
Iteration 158/1000 | Loss: 0.00001755
Iteration 159/1000 | Loss: 0.00001755
Iteration 160/1000 | Loss: 0.00001759
Iteration 161/1000 | Loss: 0.00001759
Iteration 162/1000 | Loss: 0.00001759
Iteration 163/1000 | Loss: 0.00001759
Iteration 164/1000 | Loss: 0.00001759
Iteration 165/1000 | Loss: 0.00001759
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001758
Iteration 169/1000 | Loss: 0.00001758
Iteration 170/1000 | Loss: 0.00001757
Iteration 171/1000 | Loss: 0.00001757
Iteration 172/1000 | Loss: 0.00001757
Iteration 173/1000 | Loss: 0.00001757
Iteration 174/1000 | Loss: 0.00001757
Iteration 175/1000 | Loss: 0.00001757
Iteration 176/1000 | Loss: 0.00001757
Iteration 177/1000 | Loss: 0.00001756
Iteration 178/1000 | Loss: 0.00001756
Iteration 179/1000 | Loss: 0.00001756
Iteration 180/1000 | Loss: 0.00001756
Iteration 181/1000 | Loss: 0.00001756
Iteration 182/1000 | Loss: 0.00001756
Iteration 183/1000 | Loss: 0.00001760
Iteration 184/1000 | Loss: 0.00001760
Iteration 185/1000 | Loss: 0.00001760
Iteration 186/1000 | Loss: 0.00001759
Iteration 187/1000 | Loss: 0.00001759
Iteration 188/1000 | Loss: 0.00001759
Iteration 189/1000 | Loss: 0.00001759
Iteration 190/1000 | Loss: 0.00001759
Iteration 191/1000 | Loss: 0.00001758
Iteration 192/1000 | Loss: 0.00001758
Iteration 193/1000 | Loss: 0.00001758
Iteration 194/1000 | Loss: 0.00001758
Iteration 195/1000 | Loss: 0.00001758
Iteration 196/1000 | Loss: 0.00001757
Iteration 197/1000 | Loss: 0.00001757
Iteration 198/1000 | Loss: 0.00001757
Iteration 199/1000 | Loss: 0.00001757
Iteration 200/1000 | Loss: 0.00001757
Iteration 201/1000 | Loss: 0.00001756
Iteration 202/1000 | Loss: 0.00001756
Iteration 203/1000 | Loss: 0.00001752
Iteration 204/1000 | Loss: 0.00001752
Iteration 205/1000 | Loss: 0.00001752
Iteration 206/1000 | Loss: 0.00001752
Iteration 207/1000 | Loss: 0.00001752
Iteration 208/1000 | Loss: 0.00001752
Iteration 209/1000 | Loss: 0.00001752
Iteration 210/1000 | Loss: 0.00001752
Iteration 211/1000 | Loss: 0.00001755
Iteration 212/1000 | Loss: 0.00001752
Iteration 213/1000 | Loss: 0.00001751
Iteration 214/1000 | Loss: 0.00001751
Iteration 215/1000 | Loss: 0.00001751
Iteration 216/1000 | Loss: 0.00001751
Iteration 217/1000 | Loss: 0.00001751
Iteration 218/1000 | Loss: 0.00001751
Iteration 219/1000 | Loss: 0.00001751
Iteration 220/1000 | Loss: 0.00001751
Iteration 221/1000 | Loss: 0.00001751
Iteration 222/1000 | Loss: 0.00001751
Iteration 223/1000 | Loss: 0.00001751
Iteration 224/1000 | Loss: 0.00001751
Iteration 225/1000 | Loss: 0.00001751
Iteration 226/1000 | Loss: 0.00001751
Iteration 227/1000 | Loss: 0.00001751
Iteration 228/1000 | Loss: 0.00001751
Iteration 229/1000 | Loss: 0.00001751
Iteration 230/1000 | Loss: 0.00001751
Iteration 231/1000 | Loss: 0.00001751
Iteration 232/1000 | Loss: 0.00001751
Iteration 233/1000 | Loss: 0.00001751
Iteration 234/1000 | Loss: 0.00001751
Iteration 235/1000 | Loss: 0.00001751
Iteration 236/1000 | Loss: 0.00001751
Iteration 237/1000 | Loss: 0.00001751
Iteration 238/1000 | Loss: 0.00001751
Iteration 239/1000 | Loss: 0.00001751
Iteration 240/1000 | Loss: 0.00001751
Iteration 241/1000 | Loss: 0.00001751
Iteration 242/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.7514543287688866e-05, 1.7514543287688866e-05, 1.7514543287688866e-05, 1.7514543287688866e-05, 1.7514543287688866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7514543287688866e-05

Optimization complete. Final v2v error: 3.640547275543213 mm

Highest mean error: 9.986832618713379 mm for frame 191

Lowest mean error: 3.0375888347625732 mm for frame 210

Saving results

Total time: 118.77994418144226
