Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=131, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7336-7391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419595
Iteration 2/25 | Loss: 0.00135554
Iteration 3/25 | Loss: 0.00127647
Iteration 4/25 | Loss: 0.00126686
Iteration 5/25 | Loss: 0.00126346
Iteration 6/25 | Loss: 0.00126287
Iteration 7/25 | Loss: 0.00126287
Iteration 8/25 | Loss: 0.00126287
Iteration 9/25 | Loss: 0.00126287
Iteration 10/25 | Loss: 0.00126287
Iteration 11/25 | Loss: 0.00126287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012628675904124975, 0.0012628675904124975, 0.0012628675904124975, 0.0012628675904124975, 0.0012628675904124975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012628675904124975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43009865
Iteration 2/25 | Loss: 0.00086772
Iteration 3/25 | Loss: 0.00086772
Iteration 4/25 | Loss: 0.00086772
Iteration 5/25 | Loss: 0.00086772
Iteration 6/25 | Loss: 0.00086772
Iteration 7/25 | Loss: 0.00086772
Iteration 8/25 | Loss: 0.00086772
Iteration 9/25 | Loss: 0.00086772
Iteration 10/25 | Loss: 0.00086772
Iteration 11/25 | Loss: 0.00086772
Iteration 12/25 | Loss: 0.00086772
Iteration 13/25 | Loss: 0.00086772
Iteration 14/25 | Loss: 0.00086772
Iteration 15/25 | Loss: 0.00086772
Iteration 16/25 | Loss: 0.00086772
Iteration 17/25 | Loss: 0.00086772
Iteration 18/25 | Loss: 0.00086772
Iteration 19/25 | Loss: 0.00086772
Iteration 20/25 | Loss: 0.00086772
Iteration 21/25 | Loss: 0.00086772
Iteration 22/25 | Loss: 0.00086772
Iteration 23/25 | Loss: 0.00086772
Iteration 24/25 | Loss: 0.00086772
Iteration 25/25 | Loss: 0.00086772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086772
Iteration 2/1000 | Loss: 0.00003327
Iteration 3/1000 | Loss: 0.00002098
Iteration 4/1000 | Loss: 0.00001800
Iteration 5/1000 | Loss: 0.00001660
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001489
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001455
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001423
Iteration 24/1000 | Loss: 0.00001422
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001421
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001420
Iteration 30/1000 | Loss: 0.00001420
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001420
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001417
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001414
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001413
Iteration 51/1000 | Loss: 0.00001413
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001412
Iteration 59/1000 | Loss: 0.00001412
Iteration 60/1000 | Loss: 0.00001412
Iteration 61/1000 | Loss: 0.00001412
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001411
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001410
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001402
Iteration 100/1000 | Loss: 0.00001402
Iteration 101/1000 | Loss: 0.00001402
Iteration 102/1000 | Loss: 0.00001402
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001402
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001400
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001399
Iteration 131/1000 | Loss: 0.00001399
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001398
Iteration 136/1000 | Loss: 0.00001397
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001397
Iteration 139/1000 | Loss: 0.00001397
Iteration 140/1000 | Loss: 0.00001397
Iteration 141/1000 | Loss: 0.00001397
Iteration 142/1000 | Loss: 0.00001396
Iteration 143/1000 | Loss: 0.00001396
Iteration 144/1000 | Loss: 0.00001396
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001394
Iteration 166/1000 | Loss: 0.00001394
Iteration 167/1000 | Loss: 0.00001394
Iteration 168/1000 | Loss: 0.00001394
Iteration 169/1000 | Loss: 0.00001394
Iteration 170/1000 | Loss: 0.00001394
Iteration 171/1000 | Loss: 0.00001393
Iteration 172/1000 | Loss: 0.00001393
Iteration 173/1000 | Loss: 0.00001393
Iteration 174/1000 | Loss: 0.00001393
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001393
Iteration 180/1000 | Loss: 0.00001393
Iteration 181/1000 | Loss: 0.00001393
Iteration 182/1000 | Loss: 0.00001393
Iteration 183/1000 | Loss: 0.00001393
Iteration 184/1000 | Loss: 0.00001393
Iteration 185/1000 | Loss: 0.00001393
Iteration 186/1000 | Loss: 0.00001393
Iteration 187/1000 | Loss: 0.00001393
Iteration 188/1000 | Loss: 0.00001393
Iteration 189/1000 | Loss: 0.00001393
Iteration 190/1000 | Loss: 0.00001392
Iteration 191/1000 | Loss: 0.00001392
Iteration 192/1000 | Loss: 0.00001392
Iteration 193/1000 | Loss: 0.00001392
Iteration 194/1000 | Loss: 0.00001392
Iteration 195/1000 | Loss: 0.00001392
Iteration 196/1000 | Loss: 0.00001392
Iteration 197/1000 | Loss: 0.00001392
Iteration 198/1000 | Loss: 0.00001392
Iteration 199/1000 | Loss: 0.00001392
Iteration 200/1000 | Loss: 0.00001392
Iteration 201/1000 | Loss: 0.00001392
Iteration 202/1000 | Loss: 0.00001392
Iteration 203/1000 | Loss: 0.00001392
Iteration 204/1000 | Loss: 0.00001392
Iteration 205/1000 | Loss: 0.00001392
Iteration 206/1000 | Loss: 0.00001392
Iteration 207/1000 | Loss: 0.00001392
Iteration 208/1000 | Loss: 0.00001392
Iteration 209/1000 | Loss: 0.00001392
Iteration 210/1000 | Loss: 0.00001392
Iteration 211/1000 | Loss: 0.00001392
Iteration 212/1000 | Loss: 0.00001392
Iteration 213/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.3920229321229272e-05, 1.3920229321229272e-05, 1.3920229321229272e-05, 1.3920229321229272e-05, 1.3920229321229272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3920229321229272e-05

Optimization complete. Final v2v error: 3.171637773513794 mm

Highest mean error: 3.5386626720428467 mm for frame 5

Lowest mean error: 3.018646717071533 mm for frame 111

Saving results

Total time: 41.628217458724976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862928
Iteration 2/25 | Loss: 0.00129282
Iteration 3/25 | Loss: 0.00123076
Iteration 4/25 | Loss: 0.00122419
Iteration 5/25 | Loss: 0.00122225
Iteration 6/25 | Loss: 0.00122215
Iteration 7/25 | Loss: 0.00122215
Iteration 8/25 | Loss: 0.00122215
Iteration 9/25 | Loss: 0.00122215
Iteration 10/25 | Loss: 0.00122215
Iteration 11/25 | Loss: 0.00122215
Iteration 12/25 | Loss: 0.00122215
Iteration 13/25 | Loss: 0.00122215
Iteration 14/25 | Loss: 0.00122215
Iteration 15/25 | Loss: 0.00122215
Iteration 16/25 | Loss: 0.00122215
Iteration 17/25 | Loss: 0.00122215
Iteration 18/25 | Loss: 0.00122215
Iteration 19/25 | Loss: 0.00122215
Iteration 20/25 | Loss: 0.00122215
Iteration 21/25 | Loss: 0.00122215
Iteration 22/25 | Loss: 0.00122215
Iteration 23/25 | Loss: 0.00122215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012221455108374357, 0.0012221455108374357, 0.0012221455108374357, 0.0012221455108374357, 0.0012221455108374357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012221455108374357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77249551
Iteration 2/25 | Loss: 0.00084740
Iteration 3/25 | Loss: 0.00084740
Iteration 4/25 | Loss: 0.00084740
Iteration 5/25 | Loss: 0.00084740
Iteration 6/25 | Loss: 0.00084740
Iteration 7/25 | Loss: 0.00084740
Iteration 8/25 | Loss: 0.00084740
Iteration 9/25 | Loss: 0.00084740
Iteration 10/25 | Loss: 0.00084740
Iteration 11/25 | Loss: 0.00084740
Iteration 12/25 | Loss: 0.00084740
Iteration 13/25 | Loss: 0.00084740
Iteration 14/25 | Loss: 0.00084740
Iteration 15/25 | Loss: 0.00084740
Iteration 16/25 | Loss: 0.00084740
Iteration 17/25 | Loss: 0.00084740
Iteration 18/25 | Loss: 0.00084740
Iteration 19/25 | Loss: 0.00084740
Iteration 20/25 | Loss: 0.00084740
Iteration 21/25 | Loss: 0.00084740
Iteration 22/25 | Loss: 0.00084740
Iteration 23/25 | Loss: 0.00084740
Iteration 24/25 | Loss: 0.00084740
Iteration 25/25 | Loss: 0.00084740

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084740
Iteration 2/1000 | Loss: 0.00002707
Iteration 3/1000 | Loss: 0.00001911
Iteration 4/1000 | Loss: 0.00001659
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001278
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001255
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001231
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001228
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001227
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001225
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001216
Iteration 82/1000 | Loss: 0.00001216
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001216
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001213
Iteration 94/1000 | Loss: 0.00001213
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001203
Iteration 143/1000 | Loss: 0.00001203
Iteration 144/1000 | Loss: 0.00001203
Iteration 145/1000 | Loss: 0.00001203
Iteration 146/1000 | Loss: 0.00001203
Iteration 147/1000 | Loss: 0.00001202
Iteration 148/1000 | Loss: 0.00001202
Iteration 149/1000 | Loss: 0.00001202
Iteration 150/1000 | Loss: 0.00001202
Iteration 151/1000 | Loss: 0.00001201
Iteration 152/1000 | Loss: 0.00001201
Iteration 153/1000 | Loss: 0.00001201
Iteration 154/1000 | Loss: 0.00001201
Iteration 155/1000 | Loss: 0.00001201
Iteration 156/1000 | Loss: 0.00001201
Iteration 157/1000 | Loss: 0.00001201
Iteration 158/1000 | Loss: 0.00001201
Iteration 159/1000 | Loss: 0.00001201
Iteration 160/1000 | Loss: 0.00001201
Iteration 161/1000 | Loss: 0.00001201
Iteration 162/1000 | Loss: 0.00001201
Iteration 163/1000 | Loss: 0.00001201
Iteration 164/1000 | Loss: 0.00001201
Iteration 165/1000 | Loss: 0.00001201
Iteration 166/1000 | Loss: 0.00001201
Iteration 167/1000 | Loss: 0.00001201
Iteration 168/1000 | Loss: 0.00001201
Iteration 169/1000 | Loss: 0.00001200
Iteration 170/1000 | Loss: 0.00001200
Iteration 171/1000 | Loss: 0.00001200
Iteration 172/1000 | Loss: 0.00001200
Iteration 173/1000 | Loss: 0.00001200
Iteration 174/1000 | Loss: 0.00001200
Iteration 175/1000 | Loss: 0.00001200
Iteration 176/1000 | Loss: 0.00001200
Iteration 177/1000 | Loss: 0.00001200
Iteration 178/1000 | Loss: 0.00001200
Iteration 179/1000 | Loss: 0.00001200
Iteration 180/1000 | Loss: 0.00001200
Iteration 181/1000 | Loss: 0.00001200
Iteration 182/1000 | Loss: 0.00001199
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001199
Iteration 185/1000 | Loss: 0.00001199
Iteration 186/1000 | Loss: 0.00001199
Iteration 187/1000 | Loss: 0.00001199
Iteration 188/1000 | Loss: 0.00001199
Iteration 189/1000 | Loss: 0.00001199
Iteration 190/1000 | Loss: 0.00001199
Iteration 191/1000 | Loss: 0.00001199
Iteration 192/1000 | Loss: 0.00001199
Iteration 193/1000 | Loss: 0.00001199
Iteration 194/1000 | Loss: 0.00001199
Iteration 195/1000 | Loss: 0.00001199
Iteration 196/1000 | Loss: 0.00001199
Iteration 197/1000 | Loss: 0.00001199
Iteration 198/1000 | Loss: 0.00001199
Iteration 199/1000 | Loss: 0.00001199
Iteration 200/1000 | Loss: 0.00001199
Iteration 201/1000 | Loss: 0.00001198
Iteration 202/1000 | Loss: 0.00001198
Iteration 203/1000 | Loss: 0.00001198
Iteration 204/1000 | Loss: 0.00001198
Iteration 205/1000 | Loss: 0.00001198
Iteration 206/1000 | Loss: 0.00001198
Iteration 207/1000 | Loss: 0.00001198
Iteration 208/1000 | Loss: 0.00001198
Iteration 209/1000 | Loss: 0.00001198
Iteration 210/1000 | Loss: 0.00001198
Iteration 211/1000 | Loss: 0.00001198
Iteration 212/1000 | Loss: 0.00001198
Iteration 213/1000 | Loss: 0.00001198
Iteration 214/1000 | Loss: 0.00001198
Iteration 215/1000 | Loss: 0.00001198
Iteration 216/1000 | Loss: 0.00001197
Iteration 217/1000 | Loss: 0.00001197
Iteration 218/1000 | Loss: 0.00001197
Iteration 219/1000 | Loss: 0.00001197
Iteration 220/1000 | Loss: 0.00001197
Iteration 221/1000 | Loss: 0.00001197
Iteration 222/1000 | Loss: 0.00001197
Iteration 223/1000 | Loss: 0.00001197
Iteration 224/1000 | Loss: 0.00001197
Iteration 225/1000 | Loss: 0.00001197
Iteration 226/1000 | Loss: 0.00001197
Iteration 227/1000 | Loss: 0.00001197
Iteration 228/1000 | Loss: 0.00001197
Iteration 229/1000 | Loss: 0.00001197
Iteration 230/1000 | Loss: 0.00001197
Iteration 231/1000 | Loss: 0.00001197
Iteration 232/1000 | Loss: 0.00001197
Iteration 233/1000 | Loss: 0.00001197
Iteration 234/1000 | Loss: 0.00001197
Iteration 235/1000 | Loss: 0.00001197
Iteration 236/1000 | Loss: 0.00001197
Iteration 237/1000 | Loss: 0.00001197
Iteration 238/1000 | Loss: 0.00001197
Iteration 239/1000 | Loss: 0.00001197
Iteration 240/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.1969792467425577e-05, 1.1969792467425577e-05, 1.1969792467425577e-05, 1.1969792467425577e-05, 1.1969792467425577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1969792467425577e-05

Optimization complete. Final v2v error: 2.9318063259124756 mm

Highest mean error: 3.779785633087158 mm for frame 94

Lowest mean error: 2.690890312194824 mm for frame 0

Saving results

Total time: 43.65092062950134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048158
Iteration 2/25 | Loss: 0.00142949
Iteration 3/25 | Loss: 0.00126911
Iteration 4/25 | Loss: 0.00124671
Iteration 5/25 | Loss: 0.00124289
Iteration 6/25 | Loss: 0.00124289
Iteration 7/25 | Loss: 0.00124289
Iteration 8/25 | Loss: 0.00124289
Iteration 9/25 | Loss: 0.00124289
Iteration 10/25 | Loss: 0.00124289
Iteration 11/25 | Loss: 0.00124289
Iteration 12/25 | Loss: 0.00124289
Iteration 13/25 | Loss: 0.00124289
Iteration 14/25 | Loss: 0.00124289
Iteration 15/25 | Loss: 0.00124289
Iteration 16/25 | Loss: 0.00124289
Iteration 17/25 | Loss: 0.00124289
Iteration 18/25 | Loss: 0.00124289
Iteration 19/25 | Loss: 0.00124289
Iteration 20/25 | Loss: 0.00124289
Iteration 21/25 | Loss: 0.00124289
Iteration 22/25 | Loss: 0.00124289
Iteration 23/25 | Loss: 0.00124289
Iteration 24/25 | Loss: 0.00124289
Iteration 25/25 | Loss: 0.00124289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19416618
Iteration 2/25 | Loss: 0.00070353
Iteration 3/25 | Loss: 0.00070353
Iteration 4/25 | Loss: 0.00070353
Iteration 5/25 | Loss: 0.00070352
Iteration 6/25 | Loss: 0.00070352
Iteration 7/25 | Loss: 0.00070352
Iteration 8/25 | Loss: 0.00070352
Iteration 9/25 | Loss: 0.00070352
Iteration 10/25 | Loss: 0.00070352
Iteration 11/25 | Loss: 0.00070352
Iteration 12/25 | Loss: 0.00070352
Iteration 13/25 | Loss: 0.00070352
Iteration 14/25 | Loss: 0.00070352
Iteration 15/25 | Loss: 0.00070352
Iteration 16/25 | Loss: 0.00070352
Iteration 17/25 | Loss: 0.00070352
Iteration 18/25 | Loss: 0.00070352
Iteration 19/25 | Loss: 0.00070352
Iteration 20/25 | Loss: 0.00070352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007035235175862908, 0.0007035235175862908, 0.0007035235175862908, 0.0007035235175862908, 0.0007035235175862908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007035235175862908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070352
Iteration 2/1000 | Loss: 0.00003010
Iteration 3/1000 | Loss: 0.00002079
Iteration 4/1000 | Loss: 0.00001800
Iteration 5/1000 | Loss: 0.00001636
Iteration 6/1000 | Loss: 0.00001510
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001256
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001254
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00001254
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001252
Iteration 73/1000 | Loss: 0.00001252
Iteration 74/1000 | Loss: 0.00001252
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001251
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001249
Iteration 90/1000 | Loss: 0.00001249
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001248
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001248
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001247
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001246
Iteration 130/1000 | Loss: 0.00001246
Iteration 131/1000 | Loss: 0.00001246
Iteration 132/1000 | Loss: 0.00001246
Iteration 133/1000 | Loss: 0.00001246
Iteration 134/1000 | Loss: 0.00001246
Iteration 135/1000 | Loss: 0.00001246
Iteration 136/1000 | Loss: 0.00001246
Iteration 137/1000 | Loss: 0.00001246
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.2461480764613952e-05, 1.2461480764613952e-05, 1.2461480764613952e-05, 1.2461480764613952e-05, 1.2461480764613952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2461480764613952e-05

Optimization complete. Final v2v error: 3.0195930004119873 mm

Highest mean error: 3.2592365741729736 mm for frame 204

Lowest mean error: 2.8648805618286133 mm for frame 250

Saving results

Total time: 41.221723318099976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764278
Iteration 2/25 | Loss: 0.00157967
Iteration 3/25 | Loss: 0.00137999
Iteration 4/25 | Loss: 0.00134156
Iteration 5/25 | Loss: 0.00132163
Iteration 6/25 | Loss: 0.00133699
Iteration 7/25 | Loss: 0.00133227
Iteration 8/25 | Loss: 0.00132132
Iteration 9/25 | Loss: 0.00130411
Iteration 10/25 | Loss: 0.00129603
Iteration 11/25 | Loss: 0.00129293
Iteration 12/25 | Loss: 0.00129185
Iteration 13/25 | Loss: 0.00129150
Iteration 14/25 | Loss: 0.00129138
Iteration 15/25 | Loss: 0.00129136
Iteration 16/25 | Loss: 0.00129135
Iteration 17/25 | Loss: 0.00129135
Iteration 18/25 | Loss: 0.00129134
Iteration 19/25 | Loss: 0.00129134
Iteration 20/25 | Loss: 0.00129134
Iteration 21/25 | Loss: 0.00129134
Iteration 22/25 | Loss: 0.00129134
Iteration 23/25 | Loss: 0.00129134
Iteration 24/25 | Loss: 0.00129134
Iteration 25/25 | Loss: 0.00129134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87339413
Iteration 2/25 | Loss: 0.00071768
Iteration 3/25 | Loss: 0.00071767
Iteration 4/25 | Loss: 0.00071767
Iteration 5/25 | Loss: 0.00071767
Iteration 6/25 | Loss: 0.00071767
Iteration 7/25 | Loss: 0.00071767
Iteration 8/25 | Loss: 0.00071767
Iteration 9/25 | Loss: 0.00071767
Iteration 10/25 | Loss: 0.00071767
Iteration 11/25 | Loss: 0.00071767
Iteration 12/25 | Loss: 0.00071767
Iteration 13/25 | Loss: 0.00071767
Iteration 14/25 | Loss: 0.00071767
Iteration 15/25 | Loss: 0.00071767
Iteration 16/25 | Loss: 0.00071767
Iteration 17/25 | Loss: 0.00071767
Iteration 18/25 | Loss: 0.00071767
Iteration 19/25 | Loss: 0.00071767
Iteration 20/25 | Loss: 0.00071767
Iteration 21/25 | Loss: 0.00071767
Iteration 22/25 | Loss: 0.00071767
Iteration 23/25 | Loss: 0.00071767
Iteration 24/25 | Loss: 0.00071767
Iteration 25/25 | Loss: 0.00071767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071767
Iteration 2/1000 | Loss: 0.00003536
Iteration 3/1000 | Loss: 0.00005046
Iteration 4/1000 | Loss: 0.00008643
Iteration 5/1000 | Loss: 0.00002301
Iteration 6/1000 | Loss: 0.00003192
Iteration 7/1000 | Loss: 0.00002170
Iteration 8/1000 | Loss: 0.00002131
Iteration 9/1000 | Loss: 0.00002092
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002051
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00002025
Iteration 15/1000 | Loss: 0.00002025
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00002014
Iteration 19/1000 | Loss: 0.00002003
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00001991
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001986
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001985
Iteration 26/1000 | Loss: 0.00001984
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001980
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001975
Iteration 32/1000 | Loss: 0.00001975
Iteration 33/1000 | Loss: 0.00001975
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001975
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001965
Iteration 50/1000 | Loss: 0.00001964
Iteration 51/1000 | Loss: 0.00001963
Iteration 52/1000 | Loss: 0.00001963
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001962
Iteration 59/1000 | Loss: 0.00001962
Iteration 60/1000 | Loss: 0.00001962
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001957
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001956
Iteration 72/1000 | Loss: 0.00001956
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001955
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001954
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001952
Iteration 85/1000 | Loss: 0.00001952
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001951
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001950
Iteration 101/1000 | Loss: 0.00001950
Iteration 102/1000 | Loss: 0.00001950
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001950
Iteration 106/1000 | Loss: 0.00001950
Iteration 107/1000 | Loss: 0.00001950
Iteration 108/1000 | Loss: 0.00001950
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001949
Iteration 113/1000 | Loss: 0.00001949
Iteration 114/1000 | Loss: 0.00001949
Iteration 115/1000 | Loss: 0.00001949
Iteration 116/1000 | Loss: 0.00001949
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001949
Iteration 119/1000 | Loss: 0.00001949
Iteration 120/1000 | Loss: 0.00001949
Iteration 121/1000 | Loss: 0.00001949
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001948
Iteration 125/1000 | Loss: 0.00001948
Iteration 126/1000 | Loss: 0.00001948
Iteration 127/1000 | Loss: 0.00001948
Iteration 128/1000 | Loss: 0.00001948
Iteration 129/1000 | Loss: 0.00001948
Iteration 130/1000 | Loss: 0.00001948
Iteration 131/1000 | Loss: 0.00001948
Iteration 132/1000 | Loss: 0.00001948
Iteration 133/1000 | Loss: 0.00001948
Iteration 134/1000 | Loss: 0.00001948
Iteration 135/1000 | Loss: 0.00001948
Iteration 136/1000 | Loss: 0.00001948
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001948
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001948
Iteration 141/1000 | Loss: 0.00001948
Iteration 142/1000 | Loss: 0.00001947
Iteration 143/1000 | Loss: 0.00001947
Iteration 144/1000 | Loss: 0.00001947
Iteration 145/1000 | Loss: 0.00001947
Iteration 146/1000 | Loss: 0.00001947
Iteration 147/1000 | Loss: 0.00001947
Iteration 148/1000 | Loss: 0.00001947
Iteration 149/1000 | Loss: 0.00001947
Iteration 150/1000 | Loss: 0.00001947
Iteration 151/1000 | Loss: 0.00001947
Iteration 152/1000 | Loss: 0.00001947
Iteration 153/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.9473056454444304e-05, 1.9473056454444304e-05, 1.9473056454444304e-05, 1.9473056454444304e-05, 1.9473056454444304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9473056454444304e-05

Optimization complete. Final v2v error: 3.5894994735717773 mm

Highest mean error: 4.505639553070068 mm for frame 115

Lowest mean error: 3.122387170791626 mm for frame 76

Saving results

Total time: 58.076069593429565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999645
Iteration 2/25 | Loss: 0.00374810
Iteration 3/25 | Loss: 0.00227010
Iteration 4/25 | Loss: 0.00188782
Iteration 5/25 | Loss: 0.00172444
Iteration 6/25 | Loss: 0.00161026
Iteration 7/25 | Loss: 0.00152699
Iteration 8/25 | Loss: 0.00146956
Iteration 9/25 | Loss: 0.00144118
Iteration 10/25 | Loss: 0.00143337
Iteration 11/25 | Loss: 0.00143045
Iteration 12/25 | Loss: 0.00142742
Iteration 13/25 | Loss: 0.00142595
Iteration 14/25 | Loss: 0.00142375
Iteration 15/25 | Loss: 0.00142367
Iteration 16/25 | Loss: 0.00142529
Iteration 17/25 | Loss: 0.00142265
Iteration 18/25 | Loss: 0.00141745
Iteration 19/25 | Loss: 0.00141476
Iteration 20/25 | Loss: 0.00141348
Iteration 21/25 | Loss: 0.00141308
Iteration 22/25 | Loss: 0.00141146
Iteration 23/25 | Loss: 0.00141194
Iteration 24/25 | Loss: 0.00141142
Iteration 25/25 | Loss: 0.00141140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41734564
Iteration 2/25 | Loss: 0.00223770
Iteration 3/25 | Loss: 0.00176144
Iteration 4/25 | Loss: 0.00176144
Iteration 5/25 | Loss: 0.00176144
Iteration 6/25 | Loss: 0.00176144
Iteration 7/25 | Loss: 0.00176143
Iteration 8/25 | Loss: 0.00176143
Iteration 9/25 | Loss: 0.00176143
Iteration 10/25 | Loss: 0.00176143
Iteration 11/25 | Loss: 0.00176143
Iteration 12/25 | Loss: 0.00176143
Iteration 13/25 | Loss: 0.00176143
Iteration 14/25 | Loss: 0.00176143
Iteration 15/25 | Loss: 0.00176143
Iteration 16/25 | Loss: 0.00176143
Iteration 17/25 | Loss: 0.00176143
Iteration 18/25 | Loss: 0.00176143
Iteration 19/25 | Loss: 0.00176143
Iteration 20/25 | Loss: 0.00176143
Iteration 21/25 | Loss: 0.00176143
Iteration 22/25 | Loss: 0.00176143
Iteration 23/25 | Loss: 0.00176143
Iteration 24/25 | Loss: 0.00176143
Iteration 25/25 | Loss: 0.00176143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176143
Iteration 2/1000 | Loss: 0.00047707
Iteration 3/1000 | Loss: 0.00128402
Iteration 4/1000 | Loss: 0.00015696
Iteration 5/1000 | Loss: 0.00014309
Iteration 6/1000 | Loss: 0.00039140
Iteration 7/1000 | Loss: 0.00019398
Iteration 8/1000 | Loss: 0.00015668
Iteration 9/1000 | Loss: 0.00013585
Iteration 10/1000 | Loss: 0.00012068
Iteration 11/1000 | Loss: 0.00013300
Iteration 12/1000 | Loss: 0.00042676
Iteration 13/1000 | Loss: 0.00010733
Iteration 14/1000 | Loss: 0.00012097
Iteration 15/1000 | Loss: 0.00014067
Iteration 16/1000 | Loss: 0.00011966
Iteration 17/1000 | Loss: 0.00201598
Iteration 18/1000 | Loss: 0.00399042
Iteration 19/1000 | Loss: 0.00029803
Iteration 20/1000 | Loss: 0.00018496
Iteration 21/1000 | Loss: 0.00021701
Iteration 22/1000 | Loss: 0.00014123
Iteration 23/1000 | Loss: 0.00052374
Iteration 24/1000 | Loss: 0.00006956
Iteration 25/1000 | Loss: 0.00004597
Iteration 26/1000 | Loss: 0.00004132
Iteration 27/1000 | Loss: 0.00003231
Iteration 28/1000 | Loss: 0.00004203
Iteration 29/1000 | Loss: 0.00003716
Iteration 30/1000 | Loss: 0.00002657
Iteration 31/1000 | Loss: 0.00002401
Iteration 32/1000 | Loss: 0.00002383
Iteration 33/1000 | Loss: 0.00002447
Iteration 34/1000 | Loss: 0.00003228
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00002308
Iteration 37/1000 | Loss: 0.00006472
Iteration 38/1000 | Loss: 0.00002173
Iteration 39/1000 | Loss: 0.00003348
Iteration 40/1000 | Loss: 0.00006888
Iteration 41/1000 | Loss: 0.00004459
Iteration 42/1000 | Loss: 0.00024807
Iteration 43/1000 | Loss: 0.00002631
Iteration 44/1000 | Loss: 0.00003176
Iteration 45/1000 | Loss: 0.00001853
Iteration 46/1000 | Loss: 0.00001848
Iteration 47/1000 | Loss: 0.00001848
Iteration 48/1000 | Loss: 0.00001975
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00004950
Iteration 51/1000 | Loss: 0.00007823
Iteration 52/1000 | Loss: 0.00003086
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001826
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001825
Iteration 59/1000 | Loss: 0.00001825
Iteration 60/1000 | Loss: 0.00001825
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001825
Iteration 63/1000 | Loss: 0.00001825
Iteration 64/1000 | Loss: 0.00001825
Iteration 65/1000 | Loss: 0.00001825
Iteration 66/1000 | Loss: 0.00001825
Iteration 67/1000 | Loss: 0.00001825
Iteration 68/1000 | Loss: 0.00001825
Iteration 69/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.825113758968655e-05, 1.825113758968655e-05, 1.825113758968655e-05, 1.825113758968655e-05, 1.825113758968655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.825113758968655e-05

Optimization complete. Final v2v error: 3.4658565521240234 mm

Highest mean error: 11.449949264526367 mm for frame 143

Lowest mean error: 3.1055245399475098 mm for frame 169

Saving results

Total time: 130.60904121398926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053699
Iteration 2/25 | Loss: 0.00209487
Iteration 3/25 | Loss: 0.00151178
Iteration 4/25 | Loss: 0.00145538
Iteration 5/25 | Loss: 0.00146835
Iteration 6/25 | Loss: 0.00143341
Iteration 7/25 | Loss: 0.00137181
Iteration 8/25 | Loss: 0.00133837
Iteration 9/25 | Loss: 0.00132107
Iteration 10/25 | Loss: 0.00131948
Iteration 11/25 | Loss: 0.00131948
Iteration 12/25 | Loss: 0.00131897
Iteration 13/25 | Loss: 0.00131834
Iteration 14/25 | Loss: 0.00131641
Iteration 15/25 | Loss: 0.00131779
Iteration 16/25 | Loss: 0.00131888
Iteration 17/25 | Loss: 0.00131759
Iteration 18/25 | Loss: 0.00131732
Iteration 19/25 | Loss: 0.00131803
Iteration 20/25 | Loss: 0.00131676
Iteration 21/25 | Loss: 0.00131662
Iteration 22/25 | Loss: 0.00131848
Iteration 23/25 | Loss: 0.00131590
Iteration 24/25 | Loss: 0.00131330
Iteration 25/25 | Loss: 0.00131504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33172441
Iteration 2/25 | Loss: 0.00085425
Iteration 3/25 | Loss: 0.00085425
Iteration 4/25 | Loss: 0.00085425
Iteration 5/25 | Loss: 0.00085425
Iteration 6/25 | Loss: 0.00085425
Iteration 7/25 | Loss: 0.00085425
Iteration 8/25 | Loss: 0.00085425
Iteration 9/25 | Loss: 0.00085425
Iteration 10/25 | Loss: 0.00085424
Iteration 11/25 | Loss: 0.00085424
Iteration 12/25 | Loss: 0.00085424
Iteration 13/25 | Loss: 0.00085424
Iteration 14/25 | Loss: 0.00085424
Iteration 15/25 | Loss: 0.00085424
Iteration 16/25 | Loss: 0.00085424
Iteration 17/25 | Loss: 0.00085424
Iteration 18/25 | Loss: 0.00085424
Iteration 19/25 | Loss: 0.00085424
Iteration 20/25 | Loss: 0.00085424
Iteration 21/25 | Loss: 0.00085424
Iteration 22/25 | Loss: 0.00085424
Iteration 23/25 | Loss: 0.00085424
Iteration 24/25 | Loss: 0.00085424
Iteration 25/25 | Loss: 0.00085424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085424
Iteration 2/1000 | Loss: 0.00012921
Iteration 3/1000 | Loss: 0.00011272
Iteration 4/1000 | Loss: 0.00003673
Iteration 5/1000 | Loss: 0.00005815
Iteration 6/1000 | Loss: 0.00004710
Iteration 7/1000 | Loss: 0.00012490
Iteration 8/1000 | Loss: 0.00011402
Iteration 9/1000 | Loss: 0.00017385
Iteration 10/1000 | Loss: 0.00016855
Iteration 11/1000 | Loss: 0.00019682
Iteration 12/1000 | Loss: 0.00006874
Iteration 13/1000 | Loss: 0.00034773
Iteration 14/1000 | Loss: 0.00022280
Iteration 15/1000 | Loss: 0.00011502
Iteration 16/1000 | Loss: 0.00017192
Iteration 17/1000 | Loss: 0.00011160
Iteration 18/1000 | Loss: 0.00048403
Iteration 19/1000 | Loss: 0.00035627
Iteration 20/1000 | Loss: 0.00003220
Iteration 21/1000 | Loss: 0.00002678
Iteration 22/1000 | Loss: 0.00002905
Iteration 23/1000 | Loss: 0.00002512
Iteration 24/1000 | Loss: 0.00003952
Iteration 25/1000 | Loss: 0.00002403
Iteration 26/1000 | Loss: 0.00002361
Iteration 27/1000 | Loss: 0.00021207
Iteration 28/1000 | Loss: 0.00015947
Iteration 29/1000 | Loss: 0.00021327
Iteration 30/1000 | Loss: 0.00017506
Iteration 31/1000 | Loss: 0.00017626
Iteration 32/1000 | Loss: 0.00016132
Iteration 33/1000 | Loss: 0.00010830
Iteration 34/1000 | Loss: 0.00022266
Iteration 35/1000 | Loss: 0.00029491
Iteration 36/1000 | Loss: 0.00002941
Iteration 37/1000 | Loss: 0.00009890
Iteration 38/1000 | Loss: 0.00009387
Iteration 39/1000 | Loss: 0.00007919
Iteration 40/1000 | Loss: 0.00014246
Iteration 41/1000 | Loss: 0.00019640
Iteration 42/1000 | Loss: 0.00012096
Iteration 43/1000 | Loss: 0.00011571
Iteration 44/1000 | Loss: 0.00011359
Iteration 45/1000 | Loss: 0.00003331
Iteration 46/1000 | Loss: 0.00003155
Iteration 47/1000 | Loss: 0.00003075
Iteration 48/1000 | Loss: 0.00002687
Iteration 49/1000 | Loss: 0.00017321
Iteration 50/1000 | Loss: 0.00110443
Iteration 51/1000 | Loss: 0.00021408
Iteration 52/1000 | Loss: 0.00028488
Iteration 53/1000 | Loss: 0.00003853
Iteration 54/1000 | Loss: 0.00003540
Iteration 55/1000 | Loss: 0.00002991
Iteration 56/1000 | Loss: 0.00004142
Iteration 57/1000 | Loss: 0.00002320
Iteration 58/1000 | Loss: 0.00006948
Iteration 59/1000 | Loss: 0.00002213
Iteration 60/1000 | Loss: 0.00002141
Iteration 61/1000 | Loss: 0.00004244
Iteration 62/1000 | Loss: 0.00002064
Iteration 63/1000 | Loss: 0.00004867
Iteration 64/1000 | Loss: 0.00002018
Iteration 65/1000 | Loss: 0.00002111
Iteration 66/1000 | Loss: 0.00005657
Iteration 67/1000 | Loss: 0.00003817
Iteration 68/1000 | Loss: 0.00002015
Iteration 69/1000 | Loss: 0.00001977
Iteration 70/1000 | Loss: 0.00001977
Iteration 71/1000 | Loss: 0.00001977
Iteration 72/1000 | Loss: 0.00001977
Iteration 73/1000 | Loss: 0.00001977
Iteration 74/1000 | Loss: 0.00001977
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001976
Iteration 79/1000 | Loss: 0.00003974
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00002135
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001957
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001956
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001956
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00001953
Iteration 104/1000 | Loss: 0.00001952
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00004154
Iteration 107/1000 | Loss: 0.00004631
Iteration 108/1000 | Loss: 0.00002255
Iteration 109/1000 | Loss: 0.00003426
Iteration 110/1000 | Loss: 0.00002030
Iteration 111/1000 | Loss: 0.00002055
Iteration 112/1000 | Loss: 0.00001995
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002399
Iteration 115/1000 | Loss: 0.00001989
Iteration 116/1000 | Loss: 0.00001949
Iteration 117/1000 | Loss: 0.00001945
Iteration 118/1000 | Loss: 0.00001941
Iteration 119/1000 | Loss: 0.00001941
Iteration 120/1000 | Loss: 0.00001941
Iteration 121/1000 | Loss: 0.00001941
Iteration 122/1000 | Loss: 0.00001941
Iteration 123/1000 | Loss: 0.00001941
Iteration 124/1000 | Loss: 0.00001941
Iteration 125/1000 | Loss: 0.00001941
Iteration 126/1000 | Loss: 0.00001941
Iteration 127/1000 | Loss: 0.00001941
Iteration 128/1000 | Loss: 0.00001941
Iteration 129/1000 | Loss: 0.00001941
Iteration 130/1000 | Loss: 0.00001941
Iteration 131/1000 | Loss: 0.00001941
Iteration 132/1000 | Loss: 0.00001941
Iteration 133/1000 | Loss: 0.00001941
Iteration 134/1000 | Loss: 0.00001941
Iteration 135/1000 | Loss: 0.00001941
Iteration 136/1000 | Loss: 0.00001941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.9408062144066207e-05, 1.9408062144066207e-05, 1.9408062144066207e-05, 1.9408062144066207e-05, 1.9408062144066207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9408062144066207e-05

Optimization complete. Final v2v error: 3.5163934230804443 mm

Highest mean error: 5.477293491363525 mm for frame 74

Lowest mean error: 2.7951295375823975 mm for frame 145

Saving results

Total time: 168.2987277507782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074938
Iteration 2/25 | Loss: 0.00322533
Iteration 3/25 | Loss: 0.00239879
Iteration 4/25 | Loss: 0.00291436
Iteration 5/25 | Loss: 0.00275869
Iteration 6/25 | Loss: 0.00176396
Iteration 7/25 | Loss: 0.00164060
Iteration 8/25 | Loss: 0.00162154
Iteration 9/25 | Loss: 0.00161635
Iteration 10/25 | Loss: 0.00161426
Iteration 11/25 | Loss: 0.00160707
Iteration 12/25 | Loss: 0.00160883
Iteration 13/25 | Loss: 0.00160196
Iteration 14/25 | Loss: 0.00160533
Iteration 15/25 | Loss: 0.00160541
Iteration 16/25 | Loss: 0.00160347
Iteration 17/25 | Loss: 0.00160119
Iteration 18/25 | Loss: 0.00160375
Iteration 19/25 | Loss: 0.00160298
Iteration 20/25 | Loss: 0.00160285
Iteration 21/25 | Loss: 0.00160210
Iteration 22/25 | Loss: 0.00160231
Iteration 23/25 | Loss: 0.00160449
Iteration 24/25 | Loss: 0.00159977
Iteration 25/25 | Loss: 0.00160042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72843069
Iteration 2/25 | Loss: 0.00149287
Iteration 3/25 | Loss: 0.00149287
Iteration 4/25 | Loss: 0.00149287
Iteration 5/25 | Loss: 0.00149287
Iteration 6/25 | Loss: 0.00149287
Iteration 7/25 | Loss: 0.00149287
Iteration 8/25 | Loss: 0.00149287
Iteration 9/25 | Loss: 0.00149287
Iteration 10/25 | Loss: 0.00149287
Iteration 11/25 | Loss: 0.00149287
Iteration 12/25 | Loss: 0.00149287
Iteration 13/25 | Loss: 0.00149287
Iteration 14/25 | Loss: 0.00149287
Iteration 15/25 | Loss: 0.00149287
Iteration 16/25 | Loss: 0.00149287
Iteration 17/25 | Loss: 0.00149286
Iteration 18/25 | Loss: 0.00149286
Iteration 19/25 | Loss: 0.00149286
Iteration 20/25 | Loss: 0.00149286
Iteration 21/25 | Loss: 0.00149286
Iteration 22/25 | Loss: 0.00149286
Iteration 23/25 | Loss: 0.00149286
Iteration 24/25 | Loss: 0.00149286
Iteration 25/25 | Loss: 0.00149286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149286
Iteration 2/1000 | Loss: 0.00017596
Iteration 3/1000 | Loss: 0.00018690
Iteration 4/1000 | Loss: 0.00048240
Iteration 5/1000 | Loss: 0.00032752
Iteration 6/1000 | Loss: 0.00042955
Iteration 7/1000 | Loss: 0.00033725
Iteration 8/1000 | Loss: 0.00045501
Iteration 9/1000 | Loss: 0.00031847
Iteration 10/1000 | Loss: 0.00036472
Iteration 11/1000 | Loss: 0.00023259
Iteration 12/1000 | Loss: 0.00040616
Iteration 13/1000 | Loss: 0.00029584
Iteration 14/1000 | Loss: 0.00014473
Iteration 15/1000 | Loss: 0.00018275
Iteration 16/1000 | Loss: 0.00011210
Iteration 17/1000 | Loss: 0.00018351
Iteration 18/1000 | Loss: 0.00021377
Iteration 19/1000 | Loss: 0.00016536
Iteration 20/1000 | Loss: 0.00024298
Iteration 21/1000 | Loss: 0.00021628
Iteration 22/1000 | Loss: 0.00021691
Iteration 23/1000 | Loss: 0.00019805
Iteration 24/1000 | Loss: 0.00029593
Iteration 25/1000 | Loss: 0.00028636
Iteration 26/1000 | Loss: 0.00027187
Iteration 27/1000 | Loss: 0.00029186
Iteration 28/1000 | Loss: 0.00061877
Iteration 29/1000 | Loss: 0.00036496
Iteration 30/1000 | Loss: 0.00025613
Iteration 31/1000 | Loss: 0.00017526
Iteration 32/1000 | Loss: 0.00022693
Iteration 33/1000 | Loss: 0.00041122
Iteration 34/1000 | Loss: 0.00042081
Iteration 35/1000 | Loss: 0.00026582
Iteration 36/1000 | Loss: 0.00026463
Iteration 37/1000 | Loss: 0.00025797
Iteration 38/1000 | Loss: 0.00025167
Iteration 39/1000 | Loss: 0.00021933
Iteration 40/1000 | Loss: 0.00009761
Iteration 41/1000 | Loss: 0.00031150
Iteration 42/1000 | Loss: 0.00031744
Iteration 43/1000 | Loss: 0.00024618
Iteration 44/1000 | Loss: 0.00044461
Iteration 45/1000 | Loss: 0.00029923
Iteration 46/1000 | Loss: 0.00036323
Iteration 47/1000 | Loss: 0.00043991
Iteration 48/1000 | Loss: 0.00030896
Iteration 49/1000 | Loss: 0.00031234
Iteration 50/1000 | Loss: 0.00015660
Iteration 51/1000 | Loss: 0.00018777
Iteration 52/1000 | Loss: 0.00027545
Iteration 53/1000 | Loss: 0.00022423
Iteration 54/1000 | Loss: 0.00021178
Iteration 55/1000 | Loss: 0.00037954
Iteration 56/1000 | Loss: 0.00034721
Iteration 57/1000 | Loss: 0.00023595
Iteration 58/1000 | Loss: 0.00020597
Iteration 59/1000 | Loss: 0.00023707
Iteration 60/1000 | Loss: 0.00022337
Iteration 61/1000 | Loss: 0.00019221
Iteration 62/1000 | Loss: 0.00022704
Iteration 63/1000 | Loss: 0.00022316
Iteration 64/1000 | Loss: 0.00007481
Iteration 65/1000 | Loss: 0.00011020
Iteration 66/1000 | Loss: 0.00016816
Iteration 67/1000 | Loss: 0.00013546
Iteration 68/1000 | Loss: 0.00016132
Iteration 69/1000 | Loss: 0.00013558
Iteration 70/1000 | Loss: 0.00017449
Iteration 71/1000 | Loss: 0.00015212
Iteration 72/1000 | Loss: 0.00022849
Iteration 73/1000 | Loss: 0.00007366
Iteration 74/1000 | Loss: 0.00015245
Iteration 75/1000 | Loss: 0.00010966
Iteration 76/1000 | Loss: 0.00011051
Iteration 77/1000 | Loss: 0.00006471
Iteration 78/1000 | Loss: 0.00007606
Iteration 79/1000 | Loss: 0.00007454
Iteration 80/1000 | Loss: 0.00007922
Iteration 81/1000 | Loss: 0.00007217
Iteration 82/1000 | Loss: 0.00013309
Iteration 83/1000 | Loss: 0.00014034
Iteration 84/1000 | Loss: 0.00013441
Iteration 85/1000 | Loss: 0.00015260
Iteration 86/1000 | Loss: 0.00006781
Iteration 87/1000 | Loss: 0.00006842
Iteration 88/1000 | Loss: 0.00006138
Iteration 89/1000 | Loss: 0.00017312
Iteration 90/1000 | Loss: 0.00026800
Iteration 91/1000 | Loss: 0.00013080
Iteration 92/1000 | Loss: 0.00008852
Iteration 93/1000 | Loss: 0.00010676
Iteration 94/1000 | Loss: 0.00006963
Iteration 95/1000 | Loss: 0.00006158
Iteration 96/1000 | Loss: 0.00012991
Iteration 97/1000 | Loss: 0.00016023
Iteration 98/1000 | Loss: 0.00019552
Iteration 99/1000 | Loss: 0.00019655
Iteration 100/1000 | Loss: 0.00019969
Iteration 101/1000 | Loss: 0.00017033
Iteration 102/1000 | Loss: 0.00010578
Iteration 103/1000 | Loss: 0.00016118
Iteration 104/1000 | Loss: 0.00007371
Iteration 105/1000 | Loss: 0.00011689
Iteration 106/1000 | Loss: 0.00013190
Iteration 107/1000 | Loss: 0.00015358
Iteration 108/1000 | Loss: 0.00011275
Iteration 109/1000 | Loss: 0.00012522
Iteration 110/1000 | Loss: 0.00011529
Iteration 111/1000 | Loss: 0.00014032
Iteration 112/1000 | Loss: 0.00014357
Iteration 113/1000 | Loss: 0.00014194
Iteration 114/1000 | Loss: 0.00022522
Iteration 115/1000 | Loss: 0.00018477
Iteration 116/1000 | Loss: 0.00011655
Iteration 117/1000 | Loss: 0.00014568
Iteration 118/1000 | Loss: 0.00014136
Iteration 119/1000 | Loss: 0.00013981
Iteration 120/1000 | Loss: 0.00014675
Iteration 121/1000 | Loss: 0.00012078
Iteration 122/1000 | Loss: 0.00016750
Iteration 123/1000 | Loss: 0.00007581
Iteration 124/1000 | Loss: 0.00006070
Iteration 125/1000 | Loss: 0.00006309
Iteration 126/1000 | Loss: 0.00006169
Iteration 127/1000 | Loss: 0.00006059
Iteration 128/1000 | Loss: 0.00012355
Iteration 129/1000 | Loss: 0.00012874
Iteration 130/1000 | Loss: 0.00006299
Iteration 131/1000 | Loss: 0.00012290
Iteration 132/1000 | Loss: 0.00017537
Iteration 133/1000 | Loss: 0.00013443
Iteration 134/1000 | Loss: 0.00012994
Iteration 135/1000 | Loss: 0.00013449
Iteration 136/1000 | Loss: 0.00013109
Iteration 137/1000 | Loss: 0.00010845
Iteration 138/1000 | Loss: 0.00013873
Iteration 139/1000 | Loss: 0.00006636
Iteration 140/1000 | Loss: 0.00019637
Iteration 141/1000 | Loss: 0.00007245
Iteration 142/1000 | Loss: 0.00006105
Iteration 143/1000 | Loss: 0.00006858
Iteration 144/1000 | Loss: 0.00005873
Iteration 145/1000 | Loss: 0.00006669
Iteration 146/1000 | Loss: 0.00006363
Iteration 147/1000 | Loss: 0.00009291
Iteration 148/1000 | Loss: 0.00007936
Iteration 149/1000 | Loss: 0.00006284
Iteration 150/1000 | Loss: 0.00009843
Iteration 151/1000 | Loss: 0.00019867
Iteration 152/1000 | Loss: 0.00012431
Iteration 153/1000 | Loss: 0.00016184
Iteration 154/1000 | Loss: 0.00024945
Iteration 155/1000 | Loss: 0.00045321
Iteration 156/1000 | Loss: 0.00034986
Iteration 157/1000 | Loss: 0.00016457
Iteration 158/1000 | Loss: 0.00021064
Iteration 159/1000 | Loss: 0.00045765
Iteration 160/1000 | Loss: 0.00026173
Iteration 161/1000 | Loss: 0.00017795
Iteration 162/1000 | Loss: 0.00013901
Iteration 163/1000 | Loss: 0.00016571
Iteration 164/1000 | Loss: 0.00014182
Iteration 165/1000 | Loss: 0.00007188
Iteration 166/1000 | Loss: 0.00012459
Iteration 167/1000 | Loss: 0.00018612
Iteration 168/1000 | Loss: 0.00005476
Iteration 169/1000 | Loss: 0.00020527
Iteration 170/1000 | Loss: 0.00019367
Iteration 171/1000 | Loss: 0.00007835
Iteration 172/1000 | Loss: 0.00016043
Iteration 173/1000 | Loss: 0.00017321
Iteration 174/1000 | Loss: 0.00016871
Iteration 175/1000 | Loss: 0.00025362
Iteration 176/1000 | Loss: 0.00073668
Iteration 177/1000 | Loss: 0.00064333
Iteration 178/1000 | Loss: 0.00006367
Iteration 179/1000 | Loss: 0.00014675
Iteration 180/1000 | Loss: 0.00020302
Iteration 181/1000 | Loss: 0.00008436
Iteration 182/1000 | Loss: 0.00014350
Iteration 183/1000 | Loss: 0.00009193
Iteration 184/1000 | Loss: 0.00008965
Iteration 185/1000 | Loss: 0.00008825
Iteration 186/1000 | Loss: 0.00010055
Iteration 187/1000 | Loss: 0.00008053
Iteration 188/1000 | Loss: 0.00019812
Iteration 189/1000 | Loss: 0.00007640
Iteration 190/1000 | Loss: 0.00024505
Iteration 191/1000 | Loss: 0.00015441
Iteration 192/1000 | Loss: 0.00015656
Iteration 193/1000 | Loss: 0.00011452
Iteration 194/1000 | Loss: 0.00006184
Iteration 195/1000 | Loss: 0.00007819
Iteration 196/1000 | Loss: 0.00005523
Iteration 197/1000 | Loss: 0.00019959
Iteration 198/1000 | Loss: 0.00012906
Iteration 199/1000 | Loss: 0.00015197
Iteration 200/1000 | Loss: 0.00029751
Iteration 201/1000 | Loss: 0.00021906
Iteration 202/1000 | Loss: 0.00061434
Iteration 203/1000 | Loss: 0.00018624
Iteration 204/1000 | Loss: 0.00005619
Iteration 205/1000 | Loss: 0.00037673
Iteration 206/1000 | Loss: 0.00017899
Iteration 207/1000 | Loss: 0.00017475
Iteration 208/1000 | Loss: 0.00031910
Iteration 209/1000 | Loss: 0.00017433
Iteration 210/1000 | Loss: 0.00013340
Iteration 211/1000 | Loss: 0.00058576
Iteration 212/1000 | Loss: 0.00008755
Iteration 213/1000 | Loss: 0.00010507
Iteration 214/1000 | Loss: 0.00005403
Iteration 215/1000 | Loss: 0.00005201
Iteration 216/1000 | Loss: 0.00005089
Iteration 217/1000 | Loss: 0.00005039
Iteration 218/1000 | Loss: 0.00005007
Iteration 219/1000 | Loss: 0.00008747
Iteration 220/1000 | Loss: 0.00005537
Iteration 221/1000 | Loss: 0.00008102
Iteration 222/1000 | Loss: 0.00009430
Iteration 223/1000 | Loss: 0.00005113
Iteration 224/1000 | Loss: 0.00010058
Iteration 225/1000 | Loss: 0.00014710
Iteration 226/1000 | Loss: 0.00006576
Iteration 227/1000 | Loss: 0.00021465
Iteration 228/1000 | Loss: 0.00005649
Iteration 229/1000 | Loss: 0.00010568
Iteration 230/1000 | Loss: 0.00020516
Iteration 231/1000 | Loss: 0.00014965
Iteration 232/1000 | Loss: 0.00040021
Iteration 233/1000 | Loss: 0.00021232
Iteration 234/1000 | Loss: 0.00029624
Iteration 235/1000 | Loss: 0.00012398
Iteration 236/1000 | Loss: 0.00011112
Iteration 237/1000 | Loss: 0.00005262
Iteration 238/1000 | Loss: 0.00004968
Iteration 239/1000 | Loss: 0.00005106
Iteration 240/1000 | Loss: 0.00004927
Iteration 241/1000 | Loss: 0.00015752
Iteration 242/1000 | Loss: 0.00012985
Iteration 243/1000 | Loss: 0.00021839
Iteration 244/1000 | Loss: 0.00011275
Iteration 245/1000 | Loss: 0.00019656
Iteration 246/1000 | Loss: 0.00021271
Iteration 247/1000 | Loss: 0.00023138
Iteration 248/1000 | Loss: 0.00005185
Iteration 249/1000 | Loss: 0.00004929
Iteration 250/1000 | Loss: 0.00004821
Iteration 251/1000 | Loss: 0.00004775
Iteration 252/1000 | Loss: 0.00004737
Iteration 253/1000 | Loss: 0.00004716
Iteration 254/1000 | Loss: 0.00004691
Iteration 255/1000 | Loss: 0.00018833
Iteration 256/1000 | Loss: 0.00009052
Iteration 257/1000 | Loss: 0.00046433
Iteration 258/1000 | Loss: 0.00005167
Iteration 259/1000 | Loss: 0.00004862
Iteration 260/1000 | Loss: 0.00004790
Iteration 261/1000 | Loss: 0.00004709
Iteration 262/1000 | Loss: 0.00004662
Iteration 263/1000 | Loss: 0.00004636
Iteration 264/1000 | Loss: 0.00004621
Iteration 265/1000 | Loss: 0.00004604
Iteration 266/1000 | Loss: 0.00004602
Iteration 267/1000 | Loss: 0.00004602
Iteration 268/1000 | Loss: 0.00004601
Iteration 269/1000 | Loss: 0.00004600
Iteration 270/1000 | Loss: 0.00004595
Iteration 271/1000 | Loss: 0.00004593
Iteration 272/1000 | Loss: 0.00004593
Iteration 273/1000 | Loss: 0.00004592
Iteration 274/1000 | Loss: 0.00004592
Iteration 275/1000 | Loss: 0.00004592
Iteration 276/1000 | Loss: 0.00004592
Iteration 277/1000 | Loss: 0.00004588
Iteration 278/1000 | Loss: 0.00004581
Iteration 279/1000 | Loss: 0.00004581
Iteration 280/1000 | Loss: 0.00004581
Iteration 281/1000 | Loss: 0.00004581
Iteration 282/1000 | Loss: 0.00004581
Iteration 283/1000 | Loss: 0.00004580
Iteration 284/1000 | Loss: 0.00004579
Iteration 285/1000 | Loss: 0.00004579
Iteration 286/1000 | Loss: 0.00004579
Iteration 287/1000 | Loss: 0.00004579
Iteration 288/1000 | Loss: 0.00004579
Iteration 289/1000 | Loss: 0.00004579
Iteration 290/1000 | Loss: 0.00004579
Iteration 291/1000 | Loss: 0.00004578
Iteration 292/1000 | Loss: 0.00004578
Iteration 293/1000 | Loss: 0.00004578
Iteration 294/1000 | Loss: 0.00004578
Iteration 295/1000 | Loss: 0.00004577
Iteration 296/1000 | Loss: 0.00004577
Iteration 297/1000 | Loss: 0.00004577
Iteration 298/1000 | Loss: 0.00004577
Iteration 299/1000 | Loss: 0.00004577
Iteration 300/1000 | Loss: 0.00004577
Iteration 301/1000 | Loss: 0.00004575
Iteration 302/1000 | Loss: 0.00004575
Iteration 303/1000 | Loss: 0.00004575
Iteration 304/1000 | Loss: 0.00004575
Iteration 305/1000 | Loss: 0.00004575
Iteration 306/1000 | Loss: 0.00004575
Iteration 307/1000 | Loss: 0.00004575
Iteration 308/1000 | Loss: 0.00004575
Iteration 309/1000 | Loss: 0.00004575
Iteration 310/1000 | Loss: 0.00004574
Iteration 311/1000 | Loss: 0.00004574
Iteration 312/1000 | Loss: 0.00004574
Iteration 313/1000 | Loss: 0.00004574
Iteration 314/1000 | Loss: 0.00004574
Iteration 315/1000 | Loss: 0.00004574
Iteration 316/1000 | Loss: 0.00004573
Iteration 317/1000 | Loss: 0.00004573
Iteration 318/1000 | Loss: 0.00004573
Iteration 319/1000 | Loss: 0.00004573
Iteration 320/1000 | Loss: 0.00004572
Iteration 321/1000 | Loss: 0.00004572
Iteration 322/1000 | Loss: 0.00004572
Iteration 323/1000 | Loss: 0.00004572
Iteration 324/1000 | Loss: 0.00004572
Iteration 325/1000 | Loss: 0.00004572
Iteration 326/1000 | Loss: 0.00004572
Iteration 327/1000 | Loss: 0.00004571
Iteration 328/1000 | Loss: 0.00004571
Iteration 329/1000 | Loss: 0.00004571
Iteration 330/1000 | Loss: 0.00004571
Iteration 331/1000 | Loss: 0.00004570
Iteration 332/1000 | Loss: 0.00004570
Iteration 333/1000 | Loss: 0.00004570
Iteration 334/1000 | Loss: 0.00004570
Iteration 335/1000 | Loss: 0.00004570
Iteration 336/1000 | Loss: 0.00004570
Iteration 337/1000 | Loss: 0.00004570
Iteration 338/1000 | Loss: 0.00004570
Iteration 339/1000 | Loss: 0.00004570
Iteration 340/1000 | Loss: 0.00004570
Iteration 341/1000 | Loss: 0.00004570
Iteration 342/1000 | Loss: 0.00004570
Iteration 343/1000 | Loss: 0.00004570
Iteration 344/1000 | Loss: 0.00004570
Iteration 345/1000 | Loss: 0.00004570
Iteration 346/1000 | Loss: 0.00004570
Iteration 347/1000 | Loss: 0.00004570
Iteration 348/1000 | Loss: 0.00004570
Iteration 349/1000 | Loss: 0.00004569
Iteration 350/1000 | Loss: 0.00004569
Iteration 351/1000 | Loss: 0.00004569
Iteration 352/1000 | Loss: 0.00004569
Iteration 353/1000 | Loss: 0.00004569
Iteration 354/1000 | Loss: 0.00004569
Iteration 355/1000 | Loss: 0.00004569
Iteration 356/1000 | Loss: 0.00004569
Iteration 357/1000 | Loss: 0.00004569
Iteration 358/1000 | Loss: 0.00004568
Iteration 359/1000 | Loss: 0.00004568
Iteration 360/1000 | Loss: 0.00004568
Iteration 361/1000 | Loss: 0.00004568
Iteration 362/1000 | Loss: 0.00004568
Iteration 363/1000 | Loss: 0.00004568
Iteration 364/1000 | Loss: 0.00004567
Iteration 365/1000 | Loss: 0.00004567
Iteration 366/1000 | Loss: 0.00004567
Iteration 367/1000 | Loss: 0.00004567
Iteration 368/1000 | Loss: 0.00004567
Iteration 369/1000 | Loss: 0.00004567
Iteration 370/1000 | Loss: 0.00004566
Iteration 371/1000 | Loss: 0.00004566
Iteration 372/1000 | Loss: 0.00004566
Iteration 373/1000 | Loss: 0.00004566
Iteration 374/1000 | Loss: 0.00004566
Iteration 375/1000 | Loss: 0.00004566
Iteration 376/1000 | Loss: 0.00004566
Iteration 377/1000 | Loss: 0.00004566
Iteration 378/1000 | Loss: 0.00004566
Iteration 379/1000 | Loss: 0.00004566
Iteration 380/1000 | Loss: 0.00004566
Iteration 381/1000 | Loss: 0.00004566
Iteration 382/1000 | Loss: 0.00004566
Iteration 383/1000 | Loss: 0.00004566
Iteration 384/1000 | Loss: 0.00004566
Iteration 385/1000 | Loss: 0.00004566
Iteration 386/1000 | Loss: 0.00004565
Iteration 387/1000 | Loss: 0.00004565
Iteration 388/1000 | Loss: 0.00004565
Iteration 389/1000 | Loss: 0.00004565
Iteration 390/1000 | Loss: 0.00004565
Iteration 391/1000 | Loss: 0.00004565
Iteration 392/1000 | Loss: 0.00004565
Iteration 393/1000 | Loss: 0.00004565
Iteration 394/1000 | Loss: 0.00004565
Iteration 395/1000 | Loss: 0.00004565
Iteration 396/1000 | Loss: 0.00004565
Iteration 397/1000 | Loss: 0.00004565
Iteration 398/1000 | Loss: 0.00004565
Iteration 399/1000 | Loss: 0.00004564
Iteration 400/1000 | Loss: 0.00004564
Iteration 401/1000 | Loss: 0.00004564
Iteration 402/1000 | Loss: 0.00004564
Iteration 403/1000 | Loss: 0.00004564
Iteration 404/1000 | Loss: 0.00004564
Iteration 405/1000 | Loss: 0.00004564
Iteration 406/1000 | Loss: 0.00004564
Iteration 407/1000 | Loss: 0.00004564
Iteration 408/1000 | Loss: 0.00004564
Iteration 409/1000 | Loss: 0.00004564
Iteration 410/1000 | Loss: 0.00004564
Iteration 411/1000 | Loss: 0.00004564
Iteration 412/1000 | Loss: 0.00004564
Iteration 413/1000 | Loss: 0.00004564
Iteration 414/1000 | Loss: 0.00004564
Iteration 415/1000 | Loss: 0.00004564
Iteration 416/1000 | Loss: 0.00004564
Iteration 417/1000 | Loss: 0.00004564
Iteration 418/1000 | Loss: 0.00004564
Iteration 419/1000 | Loss: 0.00004564
Iteration 420/1000 | Loss: 0.00004564
Iteration 421/1000 | Loss: 0.00004564
Iteration 422/1000 | Loss: 0.00004564
Iteration 423/1000 | Loss: 0.00004564
Iteration 424/1000 | Loss: 0.00004564
Iteration 425/1000 | Loss: 0.00004564
Iteration 426/1000 | Loss: 0.00004564
Iteration 427/1000 | Loss: 0.00004564
Iteration 428/1000 | Loss: 0.00004564
Iteration 429/1000 | Loss: 0.00004564
Iteration 430/1000 | Loss: 0.00004564
Iteration 431/1000 | Loss: 0.00004564
Iteration 432/1000 | Loss: 0.00004564
Iteration 433/1000 | Loss: 0.00004564
Iteration 434/1000 | Loss: 0.00004564
Iteration 435/1000 | Loss: 0.00004564
Iteration 436/1000 | Loss: 0.00004564
Iteration 437/1000 | Loss: 0.00004564
Iteration 438/1000 | Loss: 0.00004564
Iteration 439/1000 | Loss: 0.00004564
Iteration 440/1000 | Loss: 0.00004564
Iteration 441/1000 | Loss: 0.00004564
Iteration 442/1000 | Loss: 0.00004564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 442. Stopping optimization.
Last 5 losses: [4.5639473682967946e-05, 4.5639473682967946e-05, 4.5639473682967946e-05, 4.5639473682967946e-05, 4.5639473682967946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5639473682967946e-05

Optimization complete. Final v2v error: 5.433022975921631 mm

Highest mean error: 6.159930229187012 mm for frame 246

Lowest mean error: 5.119152069091797 mm for frame 247

Saving results

Total time: 504.5090491771698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068962
Iteration 2/25 | Loss: 0.00194800
Iteration 3/25 | Loss: 0.00152532
Iteration 4/25 | Loss: 0.00148638
Iteration 5/25 | Loss: 0.00147637
Iteration 6/25 | Loss: 0.00147415
Iteration 7/25 | Loss: 0.00147413
Iteration 8/25 | Loss: 0.00147413
Iteration 9/25 | Loss: 0.00147413
Iteration 10/25 | Loss: 0.00147413
Iteration 11/25 | Loss: 0.00147413
Iteration 12/25 | Loss: 0.00147413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014741286868229508, 0.0014741286868229508, 0.0014741286868229508, 0.0014741286868229508, 0.0014741286868229508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014741286868229508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76306343
Iteration 2/25 | Loss: 0.00106224
Iteration 3/25 | Loss: 0.00106219
Iteration 4/25 | Loss: 0.00106219
Iteration 5/25 | Loss: 0.00106219
Iteration 6/25 | Loss: 0.00106219
Iteration 7/25 | Loss: 0.00106219
Iteration 8/25 | Loss: 0.00106219
Iteration 9/25 | Loss: 0.00106219
Iteration 10/25 | Loss: 0.00106219
Iteration 11/25 | Loss: 0.00106219
Iteration 12/25 | Loss: 0.00106219
Iteration 13/25 | Loss: 0.00106219
Iteration 14/25 | Loss: 0.00106219
Iteration 15/25 | Loss: 0.00106219
Iteration 16/25 | Loss: 0.00106219
Iteration 17/25 | Loss: 0.00106219
Iteration 18/25 | Loss: 0.00106219
Iteration 19/25 | Loss: 0.00106219
Iteration 20/25 | Loss: 0.00106219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010621912078931928, 0.0010621912078931928, 0.0010621912078931928, 0.0010621912078931928, 0.0010621912078931928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010621912078931928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106219
Iteration 2/1000 | Loss: 0.00008699
Iteration 3/1000 | Loss: 0.00005546
Iteration 4/1000 | Loss: 0.00004847
Iteration 5/1000 | Loss: 0.00004641
Iteration 6/1000 | Loss: 0.00004457
Iteration 7/1000 | Loss: 0.00004334
Iteration 8/1000 | Loss: 0.00004256
Iteration 9/1000 | Loss: 0.00004189
Iteration 10/1000 | Loss: 0.00004142
Iteration 11/1000 | Loss: 0.00004103
Iteration 12/1000 | Loss: 0.00004078
Iteration 13/1000 | Loss: 0.00004071
Iteration 14/1000 | Loss: 0.00004054
Iteration 15/1000 | Loss: 0.00004037
Iteration 16/1000 | Loss: 0.00004032
Iteration 17/1000 | Loss: 0.00004025
Iteration 18/1000 | Loss: 0.00004022
Iteration 19/1000 | Loss: 0.00004021
Iteration 20/1000 | Loss: 0.00004020
Iteration 21/1000 | Loss: 0.00004018
Iteration 22/1000 | Loss: 0.00004018
Iteration 23/1000 | Loss: 0.00004017
Iteration 24/1000 | Loss: 0.00004015
Iteration 25/1000 | Loss: 0.00004014
Iteration 26/1000 | Loss: 0.00004014
Iteration 27/1000 | Loss: 0.00004011
Iteration 28/1000 | Loss: 0.00004011
Iteration 29/1000 | Loss: 0.00004010
Iteration 30/1000 | Loss: 0.00004007
Iteration 31/1000 | Loss: 0.00004007
Iteration 32/1000 | Loss: 0.00004005
Iteration 33/1000 | Loss: 0.00004001
Iteration 34/1000 | Loss: 0.00003995
Iteration 35/1000 | Loss: 0.00003995
Iteration 36/1000 | Loss: 0.00003993
Iteration 37/1000 | Loss: 0.00003991
Iteration 38/1000 | Loss: 0.00003990
Iteration 39/1000 | Loss: 0.00003990
Iteration 40/1000 | Loss: 0.00003990
Iteration 41/1000 | Loss: 0.00003990
Iteration 42/1000 | Loss: 0.00003990
Iteration 43/1000 | Loss: 0.00003990
Iteration 44/1000 | Loss: 0.00003990
Iteration 45/1000 | Loss: 0.00003990
Iteration 46/1000 | Loss: 0.00003990
Iteration 47/1000 | Loss: 0.00003989
Iteration 48/1000 | Loss: 0.00003989
Iteration 49/1000 | Loss: 0.00003989
Iteration 50/1000 | Loss: 0.00003989
Iteration 51/1000 | Loss: 0.00003988
Iteration 52/1000 | Loss: 0.00003988
Iteration 53/1000 | Loss: 0.00003988
Iteration 54/1000 | Loss: 0.00003987
Iteration 55/1000 | Loss: 0.00003987
Iteration 56/1000 | Loss: 0.00003987
Iteration 57/1000 | Loss: 0.00003987
Iteration 58/1000 | Loss: 0.00003986
Iteration 59/1000 | Loss: 0.00003986
Iteration 60/1000 | Loss: 0.00003986
Iteration 61/1000 | Loss: 0.00003986
Iteration 62/1000 | Loss: 0.00003986
Iteration 63/1000 | Loss: 0.00003985
Iteration 64/1000 | Loss: 0.00003985
Iteration 65/1000 | Loss: 0.00003985
Iteration 66/1000 | Loss: 0.00003985
Iteration 67/1000 | Loss: 0.00003985
Iteration 68/1000 | Loss: 0.00003985
Iteration 69/1000 | Loss: 0.00003985
Iteration 70/1000 | Loss: 0.00003985
Iteration 71/1000 | Loss: 0.00003985
Iteration 72/1000 | Loss: 0.00003985
Iteration 73/1000 | Loss: 0.00003984
Iteration 74/1000 | Loss: 0.00003984
Iteration 75/1000 | Loss: 0.00003984
Iteration 76/1000 | Loss: 0.00003984
Iteration 77/1000 | Loss: 0.00003983
Iteration 78/1000 | Loss: 0.00003983
Iteration 79/1000 | Loss: 0.00003983
Iteration 80/1000 | Loss: 0.00003983
Iteration 81/1000 | Loss: 0.00003983
Iteration 82/1000 | Loss: 0.00003983
Iteration 83/1000 | Loss: 0.00003982
Iteration 84/1000 | Loss: 0.00003982
Iteration 85/1000 | Loss: 0.00003982
Iteration 86/1000 | Loss: 0.00003982
Iteration 87/1000 | Loss: 0.00003982
Iteration 88/1000 | Loss: 0.00003982
Iteration 89/1000 | Loss: 0.00003981
Iteration 90/1000 | Loss: 0.00003981
Iteration 91/1000 | Loss: 0.00003981
Iteration 92/1000 | Loss: 0.00003981
Iteration 93/1000 | Loss: 0.00003981
Iteration 94/1000 | Loss: 0.00003981
Iteration 95/1000 | Loss: 0.00003981
Iteration 96/1000 | Loss: 0.00003981
Iteration 97/1000 | Loss: 0.00003980
Iteration 98/1000 | Loss: 0.00003980
Iteration 99/1000 | Loss: 0.00003980
Iteration 100/1000 | Loss: 0.00003980
Iteration 101/1000 | Loss: 0.00003980
Iteration 102/1000 | Loss: 0.00003980
Iteration 103/1000 | Loss: 0.00003980
Iteration 104/1000 | Loss: 0.00003979
Iteration 105/1000 | Loss: 0.00003979
Iteration 106/1000 | Loss: 0.00003979
Iteration 107/1000 | Loss: 0.00003979
Iteration 108/1000 | Loss: 0.00003979
Iteration 109/1000 | Loss: 0.00003979
Iteration 110/1000 | Loss: 0.00003979
Iteration 111/1000 | Loss: 0.00003979
Iteration 112/1000 | Loss: 0.00003979
Iteration 113/1000 | Loss: 0.00003979
Iteration 114/1000 | Loss: 0.00003979
Iteration 115/1000 | Loss: 0.00003979
Iteration 116/1000 | Loss: 0.00003979
Iteration 117/1000 | Loss: 0.00003979
Iteration 118/1000 | Loss: 0.00003979
Iteration 119/1000 | Loss: 0.00003978
Iteration 120/1000 | Loss: 0.00003978
Iteration 121/1000 | Loss: 0.00003978
Iteration 122/1000 | Loss: 0.00003978
Iteration 123/1000 | Loss: 0.00003978
Iteration 124/1000 | Loss: 0.00003978
Iteration 125/1000 | Loss: 0.00003978
Iteration 126/1000 | Loss: 0.00003978
Iteration 127/1000 | Loss: 0.00003978
Iteration 128/1000 | Loss: 0.00003977
Iteration 129/1000 | Loss: 0.00003977
Iteration 130/1000 | Loss: 0.00003977
Iteration 131/1000 | Loss: 0.00003977
Iteration 132/1000 | Loss: 0.00003977
Iteration 133/1000 | Loss: 0.00003977
Iteration 134/1000 | Loss: 0.00003977
Iteration 135/1000 | Loss: 0.00003977
Iteration 136/1000 | Loss: 0.00003977
Iteration 137/1000 | Loss: 0.00003977
Iteration 138/1000 | Loss: 0.00003977
Iteration 139/1000 | Loss: 0.00003977
Iteration 140/1000 | Loss: 0.00003977
Iteration 141/1000 | Loss: 0.00003977
Iteration 142/1000 | Loss: 0.00003976
Iteration 143/1000 | Loss: 0.00003976
Iteration 144/1000 | Loss: 0.00003976
Iteration 145/1000 | Loss: 0.00003976
Iteration 146/1000 | Loss: 0.00003976
Iteration 147/1000 | Loss: 0.00003975
Iteration 148/1000 | Loss: 0.00003975
Iteration 149/1000 | Loss: 0.00003975
Iteration 150/1000 | Loss: 0.00003975
Iteration 151/1000 | Loss: 0.00003975
Iteration 152/1000 | Loss: 0.00003975
Iteration 153/1000 | Loss: 0.00003975
Iteration 154/1000 | Loss: 0.00003975
Iteration 155/1000 | Loss: 0.00003975
Iteration 156/1000 | Loss: 0.00003975
Iteration 157/1000 | Loss: 0.00003975
Iteration 158/1000 | Loss: 0.00003975
Iteration 159/1000 | Loss: 0.00003975
Iteration 160/1000 | Loss: 0.00003975
Iteration 161/1000 | Loss: 0.00003975
Iteration 162/1000 | Loss: 0.00003975
Iteration 163/1000 | Loss: 0.00003975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.974940409534611e-05, 3.974940409534611e-05, 3.974940409534611e-05, 3.974940409534611e-05, 3.974940409534611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.974940409534611e-05

Optimization complete. Final v2v error: 5.2146501541137695 mm

Highest mean error: 6.334338188171387 mm for frame 211

Lowest mean error: 4.230477333068848 mm for frame 69

Saving results

Total time: 51.63897132873535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514233
Iteration 2/25 | Loss: 0.00138092
Iteration 3/25 | Loss: 0.00126695
Iteration 4/25 | Loss: 0.00124939
Iteration 5/25 | Loss: 0.00124674
Iteration 6/25 | Loss: 0.00124781
Iteration 7/25 | Loss: 0.00124150
Iteration 8/25 | Loss: 0.00124224
Iteration 9/25 | Loss: 0.00124079
Iteration 10/25 | Loss: 0.00124072
Iteration 11/25 | Loss: 0.00124072
Iteration 12/25 | Loss: 0.00124072
Iteration 13/25 | Loss: 0.00124072
Iteration 14/25 | Loss: 0.00124072
Iteration 15/25 | Loss: 0.00124072
Iteration 16/25 | Loss: 0.00124072
Iteration 17/25 | Loss: 0.00124071
Iteration 18/25 | Loss: 0.00124071
Iteration 19/25 | Loss: 0.00124071
Iteration 20/25 | Loss: 0.00124071
Iteration 21/25 | Loss: 0.00124071
Iteration 22/25 | Loss: 0.00124071
Iteration 23/25 | Loss: 0.00124071
Iteration 24/25 | Loss: 0.00124071
Iteration 25/25 | Loss: 0.00124071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.90284967
Iteration 2/25 | Loss: 0.00080892
Iteration 3/25 | Loss: 0.00080199
Iteration 4/25 | Loss: 0.00080199
Iteration 5/25 | Loss: 0.00080199
Iteration 6/25 | Loss: 0.00080199
Iteration 7/25 | Loss: 0.00080199
Iteration 8/25 | Loss: 0.00080199
Iteration 9/25 | Loss: 0.00080199
Iteration 10/25 | Loss: 0.00080199
Iteration 11/25 | Loss: 0.00080199
Iteration 12/25 | Loss: 0.00080199
Iteration 13/25 | Loss: 0.00080199
Iteration 14/25 | Loss: 0.00080199
Iteration 15/25 | Loss: 0.00080199
Iteration 16/25 | Loss: 0.00080199
Iteration 17/25 | Loss: 0.00080199
Iteration 18/25 | Loss: 0.00080199
Iteration 19/25 | Loss: 0.00080199
Iteration 20/25 | Loss: 0.00080199
Iteration 21/25 | Loss: 0.00080199
Iteration 22/25 | Loss: 0.00080199
Iteration 23/25 | Loss: 0.00080199
Iteration 24/25 | Loss: 0.00080199
Iteration 25/25 | Loss: 0.00080199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080199
Iteration 2/1000 | Loss: 0.00004020
Iteration 3/1000 | Loss: 0.00005652
Iteration 4/1000 | Loss: 0.00006605
Iteration 5/1000 | Loss: 0.00008689
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00007323
Iteration 9/1000 | Loss: 0.00001683
Iteration 10/1000 | Loss: 0.00001643
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00005995
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00008553
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00004589
Iteration 19/1000 | Loss: 0.00001531
Iteration 20/1000 | Loss: 0.00001529
Iteration 21/1000 | Loss: 0.00001523
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001522
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00005158
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00003611
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001498
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001497
Iteration 38/1000 | Loss: 0.00001497
Iteration 39/1000 | Loss: 0.00001496
Iteration 40/1000 | Loss: 0.00001496
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001496
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001492
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001492
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00008450
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00004320
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001476
Iteration 70/1000 | Loss: 0.00001476
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001474
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001473
Iteration 77/1000 | Loss: 0.00001473
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001473
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001469
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001468
Iteration 119/1000 | Loss: 0.00001468
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00001466
Iteration 136/1000 | Loss: 0.00001466
Iteration 137/1000 | Loss: 0.00001466
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001465
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001465
Iteration 148/1000 | Loss: 0.00001465
Iteration 149/1000 | Loss: 0.00001465
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001464
Iteration 157/1000 | Loss: 0.00001464
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001464
Iteration 160/1000 | Loss: 0.00001464
Iteration 161/1000 | Loss: 0.00001464
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001464
Iteration 166/1000 | Loss: 0.00001464
Iteration 167/1000 | Loss: 0.00001464
Iteration 168/1000 | Loss: 0.00001464
Iteration 169/1000 | Loss: 0.00001464
Iteration 170/1000 | Loss: 0.00001464
Iteration 171/1000 | Loss: 0.00001464
Iteration 172/1000 | Loss: 0.00001464
Iteration 173/1000 | Loss: 0.00001464
Iteration 174/1000 | Loss: 0.00001464
Iteration 175/1000 | Loss: 0.00001464
Iteration 176/1000 | Loss: 0.00001463
Iteration 177/1000 | Loss: 0.00001463
Iteration 178/1000 | Loss: 0.00001463
Iteration 179/1000 | Loss: 0.00001463
Iteration 180/1000 | Loss: 0.00001463
Iteration 181/1000 | Loss: 0.00001463
Iteration 182/1000 | Loss: 0.00001463
Iteration 183/1000 | Loss: 0.00001463
Iteration 184/1000 | Loss: 0.00001463
Iteration 185/1000 | Loss: 0.00001463
Iteration 186/1000 | Loss: 0.00001463
Iteration 187/1000 | Loss: 0.00001463
Iteration 188/1000 | Loss: 0.00001463
Iteration 189/1000 | Loss: 0.00001463
Iteration 190/1000 | Loss: 0.00001463
Iteration 191/1000 | Loss: 0.00001463
Iteration 192/1000 | Loss: 0.00001463
Iteration 193/1000 | Loss: 0.00001463
Iteration 194/1000 | Loss: 0.00001463
Iteration 195/1000 | Loss: 0.00001463
Iteration 196/1000 | Loss: 0.00001463
Iteration 197/1000 | Loss: 0.00001463
Iteration 198/1000 | Loss: 0.00001463
Iteration 199/1000 | Loss: 0.00001463
Iteration 200/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.4631451449531596e-05, 1.4631451449531596e-05, 1.4631451449531596e-05, 1.4631451449531596e-05, 1.4631451449531596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4631451449531596e-05

Optimization complete. Final v2v error: 3.2199647426605225 mm

Highest mean error: 3.9926161766052246 mm for frame 200

Lowest mean error: 2.9031126499176025 mm for frame 215

Saving results

Total time: 76.64001417160034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889672
Iteration 2/25 | Loss: 0.00151854
Iteration 3/25 | Loss: 0.00136189
Iteration 4/25 | Loss: 0.00134583
Iteration 5/25 | Loss: 0.00134075
Iteration 6/25 | Loss: 0.00133981
Iteration 7/25 | Loss: 0.00133981
Iteration 8/25 | Loss: 0.00133981
Iteration 9/25 | Loss: 0.00133981
Iteration 10/25 | Loss: 0.00133981
Iteration 11/25 | Loss: 0.00133981
Iteration 12/25 | Loss: 0.00133981
Iteration 13/25 | Loss: 0.00133981
Iteration 14/25 | Loss: 0.00133981
Iteration 15/25 | Loss: 0.00133981
Iteration 16/25 | Loss: 0.00133981
Iteration 17/25 | Loss: 0.00133981
Iteration 18/25 | Loss: 0.00133981
Iteration 19/25 | Loss: 0.00133981
Iteration 20/25 | Loss: 0.00133981
Iteration 21/25 | Loss: 0.00133981
Iteration 22/25 | Loss: 0.00133981
Iteration 23/25 | Loss: 0.00133981
Iteration 24/25 | Loss: 0.00133981
Iteration 25/25 | Loss: 0.00133981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74832714
Iteration 2/25 | Loss: 0.00077055
Iteration 3/25 | Loss: 0.00077055
Iteration 4/25 | Loss: 0.00077055
Iteration 5/25 | Loss: 0.00077055
Iteration 6/25 | Loss: 0.00077055
Iteration 7/25 | Loss: 0.00077055
Iteration 8/25 | Loss: 0.00077055
Iteration 9/25 | Loss: 0.00077055
Iteration 10/25 | Loss: 0.00077055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0007705509196966887, 0.0007705509196966887, 0.0007705509196966887, 0.0007705509196966887, 0.0007705509196966887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007705509196966887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077055
Iteration 2/1000 | Loss: 0.00005636
Iteration 3/1000 | Loss: 0.00003674
Iteration 4/1000 | Loss: 0.00003185
Iteration 5/1000 | Loss: 0.00002971
Iteration 6/1000 | Loss: 0.00002833
Iteration 7/1000 | Loss: 0.00002731
Iteration 8/1000 | Loss: 0.00002637
Iteration 9/1000 | Loss: 0.00002581
Iteration 10/1000 | Loss: 0.00002535
Iteration 11/1000 | Loss: 0.00002480
Iteration 12/1000 | Loss: 0.00002438
Iteration 13/1000 | Loss: 0.00002384
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002322
Iteration 16/1000 | Loss: 0.00002297
Iteration 17/1000 | Loss: 0.00002274
Iteration 18/1000 | Loss: 0.00002259
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002239
Iteration 23/1000 | Loss: 0.00002235
Iteration 24/1000 | Loss: 0.00002235
Iteration 25/1000 | Loss: 0.00002235
Iteration 26/1000 | Loss: 0.00002235
Iteration 27/1000 | Loss: 0.00002234
Iteration 28/1000 | Loss: 0.00002234
Iteration 29/1000 | Loss: 0.00002234
Iteration 30/1000 | Loss: 0.00002233
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002232
Iteration 33/1000 | Loss: 0.00002232
Iteration 34/1000 | Loss: 0.00002232
Iteration 35/1000 | Loss: 0.00002232
Iteration 36/1000 | Loss: 0.00002232
Iteration 37/1000 | Loss: 0.00002232
Iteration 38/1000 | Loss: 0.00002232
Iteration 39/1000 | Loss: 0.00002232
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00002232
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002231
Iteration 44/1000 | Loss: 0.00002231
Iteration 45/1000 | Loss: 0.00002231
Iteration 46/1000 | Loss: 0.00002231
Iteration 47/1000 | Loss: 0.00002231
Iteration 48/1000 | Loss: 0.00002231
Iteration 49/1000 | Loss: 0.00002231
Iteration 50/1000 | Loss: 0.00002230
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002229
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002229
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002226
Iteration 63/1000 | Loss: 0.00002226
Iteration 64/1000 | Loss: 0.00002226
Iteration 65/1000 | Loss: 0.00002226
Iteration 66/1000 | Loss: 0.00002226
Iteration 67/1000 | Loss: 0.00002226
Iteration 68/1000 | Loss: 0.00002226
Iteration 69/1000 | Loss: 0.00002222
Iteration 70/1000 | Loss: 0.00002222
Iteration 71/1000 | Loss: 0.00002222
Iteration 72/1000 | Loss: 0.00002221
Iteration 73/1000 | Loss: 0.00002221
Iteration 74/1000 | Loss: 0.00002220
Iteration 75/1000 | Loss: 0.00002219
Iteration 76/1000 | Loss: 0.00002213
Iteration 77/1000 | Loss: 0.00002208
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002204
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002203
Iteration 86/1000 | Loss: 0.00002203
Iteration 87/1000 | Loss: 0.00002203
Iteration 88/1000 | Loss: 0.00002203
Iteration 89/1000 | Loss: 0.00002203
Iteration 90/1000 | Loss: 0.00002203
Iteration 91/1000 | Loss: 0.00002202
Iteration 92/1000 | Loss: 0.00002202
Iteration 93/1000 | Loss: 0.00002202
Iteration 94/1000 | Loss: 0.00002202
Iteration 95/1000 | Loss: 0.00002200
Iteration 96/1000 | Loss: 0.00002200
Iteration 97/1000 | Loss: 0.00002200
Iteration 98/1000 | Loss: 0.00002200
Iteration 99/1000 | Loss: 0.00002200
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002199
Iteration 102/1000 | Loss: 0.00002199
Iteration 103/1000 | Loss: 0.00002198
Iteration 104/1000 | Loss: 0.00002198
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002196
Iteration 108/1000 | Loss: 0.00002196
Iteration 109/1000 | Loss: 0.00002196
Iteration 110/1000 | Loss: 0.00002196
Iteration 111/1000 | Loss: 0.00002196
Iteration 112/1000 | Loss: 0.00002196
Iteration 113/1000 | Loss: 0.00002196
Iteration 114/1000 | Loss: 0.00002196
Iteration 115/1000 | Loss: 0.00002196
Iteration 116/1000 | Loss: 0.00002196
Iteration 117/1000 | Loss: 0.00002196
Iteration 118/1000 | Loss: 0.00002195
Iteration 119/1000 | Loss: 0.00002195
Iteration 120/1000 | Loss: 0.00002195
Iteration 121/1000 | Loss: 0.00002195
Iteration 122/1000 | Loss: 0.00002195
Iteration 123/1000 | Loss: 0.00002195
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002195
Iteration 126/1000 | Loss: 0.00002195
Iteration 127/1000 | Loss: 0.00002195
Iteration 128/1000 | Loss: 0.00002195
Iteration 129/1000 | Loss: 0.00002195
Iteration 130/1000 | Loss: 0.00002194
Iteration 131/1000 | Loss: 0.00002194
Iteration 132/1000 | Loss: 0.00002194
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002194
Iteration 135/1000 | Loss: 0.00002194
Iteration 136/1000 | Loss: 0.00002194
Iteration 137/1000 | Loss: 0.00002194
Iteration 138/1000 | Loss: 0.00002194
Iteration 139/1000 | Loss: 0.00002194
Iteration 140/1000 | Loss: 0.00002194
Iteration 141/1000 | Loss: 0.00002194
Iteration 142/1000 | Loss: 0.00002194
Iteration 143/1000 | Loss: 0.00002194
Iteration 144/1000 | Loss: 0.00002194
Iteration 145/1000 | Loss: 0.00002193
Iteration 146/1000 | Loss: 0.00002193
Iteration 147/1000 | Loss: 0.00002193
Iteration 148/1000 | Loss: 0.00002193
Iteration 149/1000 | Loss: 0.00002193
Iteration 150/1000 | Loss: 0.00002193
Iteration 151/1000 | Loss: 0.00002193
Iteration 152/1000 | Loss: 0.00002193
Iteration 153/1000 | Loss: 0.00002193
Iteration 154/1000 | Loss: 0.00002193
Iteration 155/1000 | Loss: 0.00002193
Iteration 156/1000 | Loss: 0.00002193
Iteration 157/1000 | Loss: 0.00002193
Iteration 158/1000 | Loss: 0.00002193
Iteration 159/1000 | Loss: 0.00002193
Iteration 160/1000 | Loss: 0.00002193
Iteration 161/1000 | Loss: 0.00002192
Iteration 162/1000 | Loss: 0.00002192
Iteration 163/1000 | Loss: 0.00002192
Iteration 164/1000 | Loss: 0.00002192
Iteration 165/1000 | Loss: 0.00002192
Iteration 166/1000 | Loss: 0.00002192
Iteration 167/1000 | Loss: 0.00002192
Iteration 168/1000 | Loss: 0.00002192
Iteration 169/1000 | Loss: 0.00002192
Iteration 170/1000 | Loss: 0.00002192
Iteration 171/1000 | Loss: 0.00002192
Iteration 172/1000 | Loss: 0.00002192
Iteration 173/1000 | Loss: 0.00002192
Iteration 174/1000 | Loss: 0.00002192
Iteration 175/1000 | Loss: 0.00002192
Iteration 176/1000 | Loss: 0.00002191
Iteration 177/1000 | Loss: 0.00002191
Iteration 178/1000 | Loss: 0.00002191
Iteration 179/1000 | Loss: 0.00002191
Iteration 180/1000 | Loss: 0.00002191
Iteration 181/1000 | Loss: 0.00002191
Iteration 182/1000 | Loss: 0.00002191
Iteration 183/1000 | Loss: 0.00002191
Iteration 184/1000 | Loss: 0.00002191
Iteration 185/1000 | Loss: 0.00002190
Iteration 186/1000 | Loss: 0.00002190
Iteration 187/1000 | Loss: 0.00002190
Iteration 188/1000 | Loss: 0.00002190
Iteration 189/1000 | Loss: 0.00002190
Iteration 190/1000 | Loss: 0.00002190
Iteration 191/1000 | Loss: 0.00002190
Iteration 192/1000 | Loss: 0.00002190
Iteration 193/1000 | Loss: 0.00002190
Iteration 194/1000 | Loss: 0.00002189
Iteration 195/1000 | Loss: 0.00002189
Iteration 196/1000 | Loss: 0.00002189
Iteration 197/1000 | Loss: 0.00002189
Iteration 198/1000 | Loss: 0.00002189
Iteration 199/1000 | Loss: 0.00002189
Iteration 200/1000 | Loss: 0.00002188
Iteration 201/1000 | Loss: 0.00002188
Iteration 202/1000 | Loss: 0.00002188
Iteration 203/1000 | Loss: 0.00002188
Iteration 204/1000 | Loss: 0.00002188
Iteration 205/1000 | Loss: 0.00002188
Iteration 206/1000 | Loss: 0.00002188
Iteration 207/1000 | Loss: 0.00002187
Iteration 208/1000 | Loss: 0.00002187
Iteration 209/1000 | Loss: 0.00002187
Iteration 210/1000 | Loss: 0.00002187
Iteration 211/1000 | Loss: 0.00002187
Iteration 212/1000 | Loss: 0.00002187
Iteration 213/1000 | Loss: 0.00002186
Iteration 214/1000 | Loss: 0.00002186
Iteration 215/1000 | Loss: 0.00002186
Iteration 216/1000 | Loss: 0.00002186
Iteration 217/1000 | Loss: 0.00002185
Iteration 218/1000 | Loss: 0.00002185
Iteration 219/1000 | Loss: 0.00002185
Iteration 220/1000 | Loss: 0.00002185
Iteration 221/1000 | Loss: 0.00002185
Iteration 222/1000 | Loss: 0.00002184
Iteration 223/1000 | Loss: 0.00002183
Iteration 224/1000 | Loss: 0.00002183
Iteration 225/1000 | Loss: 0.00002183
Iteration 226/1000 | Loss: 0.00002183
Iteration 227/1000 | Loss: 0.00002183
Iteration 228/1000 | Loss: 0.00002183
Iteration 229/1000 | Loss: 0.00002183
Iteration 230/1000 | Loss: 0.00002183
Iteration 231/1000 | Loss: 0.00002183
Iteration 232/1000 | Loss: 0.00002183
Iteration 233/1000 | Loss: 0.00002182
Iteration 234/1000 | Loss: 0.00002182
Iteration 235/1000 | Loss: 0.00002182
Iteration 236/1000 | Loss: 0.00002182
Iteration 237/1000 | Loss: 0.00002182
Iteration 238/1000 | Loss: 0.00002182
Iteration 239/1000 | Loss: 0.00002182
Iteration 240/1000 | Loss: 0.00002181
Iteration 241/1000 | Loss: 0.00002181
Iteration 242/1000 | Loss: 0.00002181
Iteration 243/1000 | Loss: 0.00002181
Iteration 244/1000 | Loss: 0.00002181
Iteration 245/1000 | Loss: 0.00002181
Iteration 246/1000 | Loss: 0.00002181
Iteration 247/1000 | Loss: 0.00002181
Iteration 248/1000 | Loss: 0.00002181
Iteration 249/1000 | Loss: 0.00002181
Iteration 250/1000 | Loss: 0.00002181
Iteration 251/1000 | Loss: 0.00002181
Iteration 252/1000 | Loss: 0.00002181
Iteration 253/1000 | Loss: 0.00002181
Iteration 254/1000 | Loss: 0.00002181
Iteration 255/1000 | Loss: 0.00002181
Iteration 256/1000 | Loss: 0.00002180
Iteration 257/1000 | Loss: 0.00002180
Iteration 258/1000 | Loss: 0.00002180
Iteration 259/1000 | Loss: 0.00002180
Iteration 260/1000 | Loss: 0.00002180
Iteration 261/1000 | Loss: 0.00002180
Iteration 262/1000 | Loss: 0.00002180
Iteration 263/1000 | Loss: 0.00002180
Iteration 264/1000 | Loss: 0.00002180
Iteration 265/1000 | Loss: 0.00002180
Iteration 266/1000 | Loss: 0.00002180
Iteration 267/1000 | Loss: 0.00002180
Iteration 268/1000 | Loss: 0.00002180
Iteration 269/1000 | Loss: 0.00002180
Iteration 270/1000 | Loss: 0.00002180
Iteration 271/1000 | Loss: 0.00002180
Iteration 272/1000 | Loss: 0.00002180
Iteration 273/1000 | Loss: 0.00002180
Iteration 274/1000 | Loss: 0.00002180
Iteration 275/1000 | Loss: 0.00002180
Iteration 276/1000 | Loss: 0.00002180
Iteration 277/1000 | Loss: 0.00002180
Iteration 278/1000 | Loss: 0.00002180
Iteration 279/1000 | Loss: 0.00002180
Iteration 280/1000 | Loss: 0.00002179
Iteration 281/1000 | Loss: 0.00002179
Iteration 282/1000 | Loss: 0.00002179
Iteration 283/1000 | Loss: 0.00002179
Iteration 284/1000 | Loss: 0.00002179
Iteration 285/1000 | Loss: 0.00002179
Iteration 286/1000 | Loss: 0.00002179
Iteration 287/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [2.179338116548024e-05, 2.179338116548024e-05, 2.179338116548024e-05, 2.179338116548024e-05, 2.179338116548024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.179338116548024e-05

Optimization complete. Final v2v error: 3.834864377975464 mm

Highest mean error: 4.044264316558838 mm for frame 87

Lowest mean error: 3.5711052417755127 mm for frame 163

Saving results

Total time: 57.54028606414795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992982
Iteration 2/25 | Loss: 0.00212179
Iteration 3/25 | Loss: 0.00158898
Iteration 4/25 | Loss: 0.00149002
Iteration 5/25 | Loss: 0.00154192
Iteration 6/25 | Loss: 0.00149746
Iteration 7/25 | Loss: 0.00143681
Iteration 8/25 | Loss: 0.00140599
Iteration 9/25 | Loss: 0.00139428
Iteration 10/25 | Loss: 0.00139217
Iteration 11/25 | Loss: 0.00138895
Iteration 12/25 | Loss: 0.00138821
Iteration 13/25 | Loss: 0.00138815
Iteration 14/25 | Loss: 0.00138815
Iteration 15/25 | Loss: 0.00138815
Iteration 16/25 | Loss: 0.00138815
Iteration 17/25 | Loss: 0.00138815
Iteration 18/25 | Loss: 0.00138815
Iteration 19/25 | Loss: 0.00138814
Iteration 20/25 | Loss: 0.00138814
Iteration 21/25 | Loss: 0.00138814
Iteration 22/25 | Loss: 0.00138814
Iteration 23/25 | Loss: 0.00138814
Iteration 24/25 | Loss: 0.00138814
Iteration 25/25 | Loss: 0.00138814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83368492
Iteration 2/25 | Loss: 0.00112016
Iteration 3/25 | Loss: 0.00104948
Iteration 4/25 | Loss: 0.00104948
Iteration 5/25 | Loss: 0.00104948
Iteration 6/25 | Loss: 0.00104948
Iteration 7/25 | Loss: 0.00104948
Iteration 8/25 | Loss: 0.00104948
Iteration 9/25 | Loss: 0.00104948
Iteration 10/25 | Loss: 0.00104948
Iteration 11/25 | Loss: 0.00104948
Iteration 12/25 | Loss: 0.00104948
Iteration 13/25 | Loss: 0.00104948
Iteration 14/25 | Loss: 0.00104948
Iteration 15/25 | Loss: 0.00104948
Iteration 16/25 | Loss: 0.00104948
Iteration 17/25 | Loss: 0.00104948
Iteration 18/25 | Loss: 0.00104948
Iteration 19/25 | Loss: 0.00104948
Iteration 20/25 | Loss: 0.00104948
Iteration 21/25 | Loss: 0.00104948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010494767921045423, 0.0010494767921045423, 0.0010494767921045423, 0.0010494767921045423, 0.0010494767921045423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010494767921045423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104948
Iteration 2/1000 | Loss: 0.00008768
Iteration 3/1000 | Loss: 0.00005823
Iteration 4/1000 | Loss: 0.00004830
Iteration 5/1000 | Loss: 0.00004543
Iteration 6/1000 | Loss: 0.00004349
Iteration 7/1000 | Loss: 0.00008909
Iteration 8/1000 | Loss: 0.00010843
Iteration 9/1000 | Loss: 0.00012097
Iteration 10/1000 | Loss: 0.00006030
Iteration 11/1000 | Loss: 0.00004168
Iteration 12/1000 | Loss: 0.00004418
Iteration 13/1000 | Loss: 0.00004191
Iteration 14/1000 | Loss: 0.00004149
Iteration 15/1000 | Loss: 0.00004078
Iteration 16/1000 | Loss: 0.00004078
Iteration 17/1000 | Loss: 0.00004078
Iteration 18/1000 | Loss: 0.00004077
Iteration 19/1000 | Loss: 0.00004077
Iteration 20/1000 | Loss: 0.00004076
Iteration 21/1000 | Loss: 0.00004075
Iteration 22/1000 | Loss: 0.00005056
Iteration 23/1000 | Loss: 0.00004163
Iteration 24/1000 | Loss: 0.00004247
Iteration 25/1000 | Loss: 0.00004042
Iteration 26/1000 | Loss: 0.00004042
Iteration 27/1000 | Loss: 0.00004042
Iteration 28/1000 | Loss: 0.00004042
Iteration 29/1000 | Loss: 0.00004042
Iteration 30/1000 | Loss: 0.00004042
Iteration 31/1000 | Loss: 0.00004042
Iteration 32/1000 | Loss: 0.00004041
Iteration 33/1000 | Loss: 0.00004041
Iteration 34/1000 | Loss: 0.00004041
Iteration 35/1000 | Loss: 0.00004041
Iteration 36/1000 | Loss: 0.00004040
Iteration 37/1000 | Loss: 0.00004040
Iteration 38/1000 | Loss: 0.00004040
Iteration 39/1000 | Loss: 0.00004037
Iteration 40/1000 | Loss: 0.00004036
Iteration 41/1000 | Loss: 0.00004032
Iteration 42/1000 | Loss: 0.00004031
Iteration 43/1000 | Loss: 0.00004030
Iteration 44/1000 | Loss: 0.00004027
Iteration 45/1000 | Loss: 0.00004027
Iteration 46/1000 | Loss: 0.00004026
Iteration 47/1000 | Loss: 0.00004026
Iteration 48/1000 | Loss: 0.00004026
Iteration 49/1000 | Loss: 0.00004025
Iteration 50/1000 | Loss: 0.00004024
Iteration 51/1000 | Loss: 0.00004024
Iteration 52/1000 | Loss: 0.00004023
Iteration 53/1000 | Loss: 0.00004023
Iteration 54/1000 | Loss: 0.00004023
Iteration 55/1000 | Loss: 0.00004022
Iteration 56/1000 | Loss: 0.00004022
Iteration 57/1000 | Loss: 0.00004022
Iteration 58/1000 | Loss: 0.00004021
Iteration 59/1000 | Loss: 0.00004021
Iteration 60/1000 | Loss: 0.00004326
Iteration 61/1000 | Loss: 0.00004016
Iteration 62/1000 | Loss: 0.00004015
Iteration 63/1000 | Loss: 0.00004015
Iteration 64/1000 | Loss: 0.00004015
Iteration 65/1000 | Loss: 0.00004015
Iteration 66/1000 | Loss: 0.00004015
Iteration 67/1000 | Loss: 0.00004015
Iteration 68/1000 | Loss: 0.00004015
Iteration 69/1000 | Loss: 0.00004015
Iteration 70/1000 | Loss: 0.00004015
Iteration 71/1000 | Loss: 0.00004015
Iteration 72/1000 | Loss: 0.00004014
Iteration 73/1000 | Loss: 0.00004014
Iteration 74/1000 | Loss: 0.00004014
Iteration 75/1000 | Loss: 0.00004013
Iteration 76/1000 | Loss: 0.00004013
Iteration 77/1000 | Loss: 0.00004013
Iteration 78/1000 | Loss: 0.00004013
Iteration 79/1000 | Loss: 0.00004012
Iteration 80/1000 | Loss: 0.00004012
Iteration 81/1000 | Loss: 0.00004012
Iteration 82/1000 | Loss: 0.00004012
Iteration 83/1000 | Loss: 0.00004012
Iteration 84/1000 | Loss: 0.00004012
Iteration 85/1000 | Loss: 0.00004012
Iteration 86/1000 | Loss: 0.00004257
Iteration 87/1000 | Loss: 0.00004344
Iteration 88/1000 | Loss: 0.00004008
Iteration 89/1000 | Loss: 0.00004008
Iteration 90/1000 | Loss: 0.00004008
Iteration 91/1000 | Loss: 0.00004155
Iteration 92/1000 | Loss: 0.00004007
Iteration 93/1000 | Loss: 0.00004007
Iteration 94/1000 | Loss: 0.00004007
Iteration 95/1000 | Loss: 0.00004007
Iteration 96/1000 | Loss: 0.00004007
Iteration 97/1000 | Loss: 0.00004007
Iteration 98/1000 | Loss: 0.00004007
Iteration 99/1000 | Loss: 0.00004007
Iteration 100/1000 | Loss: 0.00004007
Iteration 101/1000 | Loss: 0.00004007
Iteration 102/1000 | Loss: 0.00004007
Iteration 103/1000 | Loss: 0.00004007
Iteration 104/1000 | Loss: 0.00004007
Iteration 105/1000 | Loss: 0.00004007
Iteration 106/1000 | Loss: 0.00004007
Iteration 107/1000 | Loss: 0.00004007
Iteration 108/1000 | Loss: 0.00004007
Iteration 109/1000 | Loss: 0.00004007
Iteration 110/1000 | Loss: 0.00004006
Iteration 111/1000 | Loss: 0.00004006
Iteration 112/1000 | Loss: 0.00004006
Iteration 113/1000 | Loss: 0.00004006
Iteration 114/1000 | Loss: 0.00004006
Iteration 115/1000 | Loss: 0.00004006
Iteration 116/1000 | Loss: 0.00004005
Iteration 117/1000 | Loss: 0.00004005
Iteration 118/1000 | Loss: 0.00004005
Iteration 119/1000 | Loss: 0.00004004
Iteration 120/1000 | Loss: 0.00004004
Iteration 121/1000 | Loss: 0.00004004
Iteration 122/1000 | Loss: 0.00004004
Iteration 123/1000 | Loss: 0.00004004
Iteration 124/1000 | Loss: 0.00004004
Iteration 125/1000 | Loss: 0.00004004
Iteration 126/1000 | Loss: 0.00004004
Iteration 127/1000 | Loss: 0.00004004
Iteration 128/1000 | Loss: 0.00004004
Iteration 129/1000 | Loss: 0.00004004
Iteration 130/1000 | Loss: 0.00004003
Iteration 131/1000 | Loss: 0.00004003
Iteration 132/1000 | Loss: 0.00004002
Iteration 133/1000 | Loss: 0.00004002
Iteration 134/1000 | Loss: 0.00004002
Iteration 135/1000 | Loss: 0.00004002
Iteration 136/1000 | Loss: 0.00004002
Iteration 137/1000 | Loss: 0.00004002
Iteration 138/1000 | Loss: 0.00004002
Iteration 139/1000 | Loss: 0.00004002
Iteration 140/1000 | Loss: 0.00004002
Iteration 141/1000 | Loss: 0.00004002
Iteration 142/1000 | Loss: 0.00004002
Iteration 143/1000 | Loss: 0.00004002
Iteration 144/1000 | Loss: 0.00004002
Iteration 145/1000 | Loss: 0.00004002
Iteration 146/1000 | Loss: 0.00004002
Iteration 147/1000 | Loss: 0.00004002
Iteration 148/1000 | Loss: 0.00004001
Iteration 149/1000 | Loss: 0.00004001
Iteration 150/1000 | Loss: 0.00004001
Iteration 151/1000 | Loss: 0.00004001
Iteration 152/1000 | Loss: 0.00004001
Iteration 153/1000 | Loss: 0.00004001
Iteration 154/1000 | Loss: 0.00004001
Iteration 155/1000 | Loss: 0.00004001
Iteration 156/1000 | Loss: 0.00004001
Iteration 157/1000 | Loss: 0.00004001
Iteration 158/1000 | Loss: 0.00004001
Iteration 159/1000 | Loss: 0.00004001
Iteration 160/1000 | Loss: 0.00004001
Iteration 161/1000 | Loss: 0.00004001
Iteration 162/1000 | Loss: 0.00004001
Iteration 163/1000 | Loss: 0.00004001
Iteration 164/1000 | Loss: 0.00004000
Iteration 165/1000 | Loss: 0.00004000
Iteration 166/1000 | Loss: 0.00004000
Iteration 167/1000 | Loss: 0.00004000
Iteration 168/1000 | Loss: 0.00004000
Iteration 169/1000 | Loss: 0.00004000
Iteration 170/1000 | Loss: 0.00004000
Iteration 171/1000 | Loss: 0.00003999
Iteration 172/1000 | Loss: 0.00003999
Iteration 173/1000 | Loss: 0.00003999
Iteration 174/1000 | Loss: 0.00003999
Iteration 175/1000 | Loss: 0.00003999
Iteration 176/1000 | Loss: 0.00003999
Iteration 177/1000 | Loss: 0.00003999
Iteration 178/1000 | Loss: 0.00003999
Iteration 179/1000 | Loss: 0.00003999
Iteration 180/1000 | Loss: 0.00003998
Iteration 181/1000 | Loss: 0.00003998
Iteration 182/1000 | Loss: 0.00003998
Iteration 183/1000 | Loss: 0.00003998
Iteration 184/1000 | Loss: 0.00003997
Iteration 185/1000 | Loss: 0.00003997
Iteration 186/1000 | Loss: 0.00003997
Iteration 187/1000 | Loss: 0.00003997
Iteration 188/1000 | Loss: 0.00003997
Iteration 189/1000 | Loss: 0.00003997
Iteration 190/1000 | Loss: 0.00003997
Iteration 191/1000 | Loss: 0.00003997
Iteration 192/1000 | Loss: 0.00003997
Iteration 193/1000 | Loss: 0.00003996
Iteration 194/1000 | Loss: 0.00003996
Iteration 195/1000 | Loss: 0.00003996
Iteration 196/1000 | Loss: 0.00003996
Iteration 197/1000 | Loss: 0.00003996
Iteration 198/1000 | Loss: 0.00003996
Iteration 199/1000 | Loss: 0.00003996
Iteration 200/1000 | Loss: 0.00003996
Iteration 201/1000 | Loss: 0.00003996
Iteration 202/1000 | Loss: 0.00003996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.996425584773533e-05, 3.996425584773533e-05, 3.996425584773533e-05, 3.996425584773533e-05, 3.996425584773533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.996425584773533e-05

Optimization complete. Final v2v error: 4.515745639801025 mm

Highest mean error: 10.732677459716797 mm for frame 116

Lowest mean error: 3.7028539180755615 mm for frame 7

Saving results

Total time: 66.2136549949646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009823
Iteration 2/25 | Loss: 0.00226975
Iteration 3/25 | Loss: 0.00159431
Iteration 4/25 | Loss: 0.00143886
Iteration 5/25 | Loss: 0.00142080
Iteration 6/25 | Loss: 0.00140558
Iteration 7/25 | Loss: 0.00139234
Iteration 8/25 | Loss: 0.00138776
Iteration 9/25 | Loss: 0.00138408
Iteration 10/25 | Loss: 0.00138058
Iteration 11/25 | Loss: 0.00137943
Iteration 12/25 | Loss: 0.00137920
Iteration 13/25 | Loss: 0.00137920
Iteration 14/25 | Loss: 0.00137920
Iteration 15/25 | Loss: 0.00137920
Iteration 16/25 | Loss: 0.00137920
Iteration 17/25 | Loss: 0.00137920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013792025856673717, 0.0013792025856673717, 0.0013792025856673717, 0.0013792025856673717, 0.0013792025856673717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013792025856673717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32936788
Iteration 2/25 | Loss: 0.00093363
Iteration 3/25 | Loss: 0.00093363
Iteration 4/25 | Loss: 0.00093363
Iteration 5/25 | Loss: 0.00093363
Iteration 6/25 | Loss: 0.00093363
Iteration 7/25 | Loss: 0.00093363
Iteration 8/25 | Loss: 0.00093362
Iteration 9/25 | Loss: 0.00093362
Iteration 10/25 | Loss: 0.00093362
Iteration 11/25 | Loss: 0.00093362
Iteration 12/25 | Loss: 0.00093362
Iteration 13/25 | Loss: 0.00093362
Iteration 14/25 | Loss: 0.00093362
Iteration 15/25 | Loss: 0.00093362
Iteration 16/25 | Loss: 0.00093362
Iteration 17/25 | Loss: 0.00093362
Iteration 18/25 | Loss: 0.00093362
Iteration 19/25 | Loss: 0.00093362
Iteration 20/25 | Loss: 0.00093362
Iteration 21/25 | Loss: 0.00093362
Iteration 22/25 | Loss: 0.00093362
Iteration 23/25 | Loss: 0.00093362
Iteration 24/25 | Loss: 0.00093362
Iteration 25/25 | Loss: 0.00093362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093362
Iteration 2/1000 | Loss: 0.00021304
Iteration 3/1000 | Loss: 0.00005743
Iteration 4/1000 | Loss: 0.00003422
Iteration 5/1000 | Loss: 0.00002931
Iteration 6/1000 | Loss: 0.00002722
Iteration 7/1000 | Loss: 0.00002529
Iteration 8/1000 | Loss: 0.00002436
Iteration 9/1000 | Loss: 0.00002392
Iteration 10/1000 | Loss: 0.00002343
Iteration 11/1000 | Loss: 0.00002322
Iteration 12/1000 | Loss: 0.00002301
Iteration 13/1000 | Loss: 0.00002283
Iteration 14/1000 | Loss: 0.00002279
Iteration 15/1000 | Loss: 0.00002276
Iteration 16/1000 | Loss: 0.00002275
Iteration 17/1000 | Loss: 0.00002275
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002272
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00002270
Iteration 24/1000 | Loss: 0.00002268
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002267
Iteration 27/1000 | Loss: 0.00002266
Iteration 28/1000 | Loss: 0.00002266
Iteration 29/1000 | Loss: 0.00002265
Iteration 30/1000 | Loss: 0.00002263
Iteration 31/1000 | Loss: 0.00002259
Iteration 32/1000 | Loss: 0.00002259
Iteration 33/1000 | Loss: 0.00002258
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00002258
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002255
Iteration 44/1000 | Loss: 0.00002255
Iteration 45/1000 | Loss: 0.00002255
Iteration 46/1000 | Loss: 0.00002255
Iteration 47/1000 | Loss: 0.00002255
Iteration 48/1000 | Loss: 0.00002255
Iteration 49/1000 | Loss: 0.00002255
Iteration 50/1000 | Loss: 0.00002254
Iteration 51/1000 | Loss: 0.00002254
Iteration 52/1000 | Loss: 0.00002253
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002252
Iteration 55/1000 | Loss: 0.00002252
Iteration 56/1000 | Loss: 0.00002252
Iteration 57/1000 | Loss: 0.00002252
Iteration 58/1000 | Loss: 0.00002252
Iteration 59/1000 | Loss: 0.00002252
Iteration 60/1000 | Loss: 0.00002252
Iteration 61/1000 | Loss: 0.00002251
Iteration 62/1000 | Loss: 0.00002251
Iteration 63/1000 | Loss: 0.00002251
Iteration 64/1000 | Loss: 0.00002251
Iteration 65/1000 | Loss: 0.00002251
Iteration 66/1000 | Loss: 0.00002250
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002249
Iteration 70/1000 | Loss: 0.00002249
Iteration 71/1000 | Loss: 0.00002248
Iteration 72/1000 | Loss: 0.00002247
Iteration 73/1000 | Loss: 0.00002247
Iteration 74/1000 | Loss: 0.00002247
Iteration 75/1000 | Loss: 0.00002247
Iteration 76/1000 | Loss: 0.00002246
Iteration 77/1000 | Loss: 0.00002246
Iteration 78/1000 | Loss: 0.00002246
Iteration 79/1000 | Loss: 0.00002246
Iteration 80/1000 | Loss: 0.00002246
Iteration 81/1000 | Loss: 0.00002245
Iteration 82/1000 | Loss: 0.00002245
Iteration 83/1000 | Loss: 0.00002245
Iteration 84/1000 | Loss: 0.00002244
Iteration 85/1000 | Loss: 0.00002244
Iteration 86/1000 | Loss: 0.00002244
Iteration 87/1000 | Loss: 0.00002244
Iteration 88/1000 | Loss: 0.00002244
Iteration 89/1000 | Loss: 0.00002244
Iteration 90/1000 | Loss: 0.00002244
Iteration 91/1000 | Loss: 0.00002244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.2441750843427144e-05, 2.2441750843427144e-05, 2.2441750843427144e-05, 2.2441750843427144e-05, 2.2441750843427144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2441750843427144e-05

Optimization complete. Final v2v error: 3.883793830871582 mm

Highest mean error: 5.473246097564697 mm for frame 219

Lowest mean error: 3.4494056701660156 mm for frame 66

Saving results

Total time: 53.158571004867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596958
Iteration 2/25 | Loss: 0.00145621
Iteration 3/25 | Loss: 0.00130728
Iteration 4/25 | Loss: 0.00128982
Iteration 5/25 | Loss: 0.00128639
Iteration 6/25 | Loss: 0.00128639
Iteration 7/25 | Loss: 0.00128639
Iteration 8/25 | Loss: 0.00128639
Iteration 9/25 | Loss: 0.00128639
Iteration 10/25 | Loss: 0.00128639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012863922165706754, 0.0012863922165706754, 0.0012863922165706754, 0.0012863922165706754, 0.0012863922165706754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012863922165706754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48929286
Iteration 2/25 | Loss: 0.00083375
Iteration 3/25 | Loss: 0.00083375
Iteration 4/25 | Loss: 0.00083375
Iteration 5/25 | Loss: 0.00083375
Iteration 6/25 | Loss: 0.00083375
Iteration 7/25 | Loss: 0.00083375
Iteration 8/25 | Loss: 0.00083375
Iteration 9/25 | Loss: 0.00083375
Iteration 10/25 | Loss: 0.00083375
Iteration 11/25 | Loss: 0.00083375
Iteration 12/25 | Loss: 0.00083375
Iteration 13/25 | Loss: 0.00083375
Iteration 14/25 | Loss: 0.00083375
Iteration 15/25 | Loss: 0.00083375
Iteration 16/25 | Loss: 0.00083375
Iteration 17/25 | Loss: 0.00083375
Iteration 18/25 | Loss: 0.00083375
Iteration 19/25 | Loss: 0.00083375
Iteration 20/25 | Loss: 0.00083375
Iteration 21/25 | Loss: 0.00083375
Iteration 22/25 | Loss: 0.00083375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000833749130833894, 0.000833749130833894, 0.000833749130833894, 0.000833749130833894, 0.000833749130833894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000833749130833894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083375
Iteration 2/1000 | Loss: 0.00002856
Iteration 3/1000 | Loss: 0.00001974
Iteration 4/1000 | Loss: 0.00001795
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001635
Iteration 8/1000 | Loss: 0.00001613
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001581
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001556
Iteration 15/1000 | Loss: 0.00001547
Iteration 16/1000 | Loss: 0.00001545
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001544
Iteration 19/1000 | Loss: 0.00001540
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001532
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001524
Iteration 31/1000 | Loss: 0.00001522
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001518
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001513
Iteration 58/1000 | Loss: 0.00001513
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001512
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001509
Iteration 73/1000 | Loss: 0.00001509
Iteration 74/1000 | Loss: 0.00001509
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001508
Iteration 77/1000 | Loss: 0.00001508
Iteration 78/1000 | Loss: 0.00001508
Iteration 79/1000 | Loss: 0.00001508
Iteration 80/1000 | Loss: 0.00001508
Iteration 81/1000 | Loss: 0.00001508
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001504
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001503
Iteration 113/1000 | Loss: 0.00001503
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001502
Iteration 117/1000 | Loss: 0.00001502
Iteration 118/1000 | Loss: 0.00001502
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001502
Iteration 123/1000 | Loss: 0.00001502
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001501
Iteration 126/1000 | Loss: 0.00001501
Iteration 127/1000 | Loss: 0.00001501
Iteration 128/1000 | Loss: 0.00001501
Iteration 129/1000 | Loss: 0.00001501
Iteration 130/1000 | Loss: 0.00001501
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001500
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001499
Iteration 137/1000 | Loss: 0.00001499
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001498
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.4976070815464482e-05, 1.4976070815464482e-05, 1.4976070815464482e-05, 1.4976070815464482e-05, 1.4976070815464482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4976070815464482e-05

Optimization complete. Final v2v error: 3.2590577602386475 mm

Highest mean error: 4.088612079620361 mm for frame 70

Lowest mean error: 2.914824962615967 mm for frame 19

Saving results

Total time: 41.82180666923523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049666
Iteration 2/25 | Loss: 0.01049666
Iteration 3/25 | Loss: 0.00268239
Iteration 4/25 | Loss: 0.00200868
Iteration 5/25 | Loss: 0.00252434
Iteration 6/25 | Loss: 0.00166189
Iteration 7/25 | Loss: 0.00152132
Iteration 8/25 | Loss: 0.00147325
Iteration 9/25 | Loss: 0.00146347
Iteration 10/25 | Loss: 0.00146247
Iteration 11/25 | Loss: 0.00146239
Iteration 12/25 | Loss: 0.00146239
Iteration 13/25 | Loss: 0.00146239
Iteration 14/25 | Loss: 0.00146239
Iteration 15/25 | Loss: 0.00146239
Iteration 16/25 | Loss: 0.00146239
Iteration 17/25 | Loss: 0.00146239
Iteration 18/25 | Loss: 0.00146239
Iteration 19/25 | Loss: 0.00146239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001462385174818337, 0.001462385174818337, 0.001462385174818337, 0.001462385174818337, 0.001462385174818337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001462385174818337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25757456
Iteration 2/25 | Loss: 0.00085743
Iteration 3/25 | Loss: 0.00085743
Iteration 4/25 | Loss: 0.00085743
Iteration 5/25 | Loss: 0.00085743
Iteration 6/25 | Loss: 0.00085743
Iteration 7/25 | Loss: 0.00085743
Iteration 8/25 | Loss: 0.00085743
Iteration 9/25 | Loss: 0.00085743
Iteration 10/25 | Loss: 0.00085743
Iteration 11/25 | Loss: 0.00085743
Iteration 12/25 | Loss: 0.00085743
Iteration 13/25 | Loss: 0.00085743
Iteration 14/25 | Loss: 0.00085743
Iteration 15/25 | Loss: 0.00085743
Iteration 16/25 | Loss: 0.00085743
Iteration 17/25 | Loss: 0.00085743
Iteration 18/25 | Loss: 0.00085743
Iteration 19/25 | Loss: 0.00085743
Iteration 20/25 | Loss: 0.00085743
Iteration 21/25 | Loss: 0.00085743
Iteration 22/25 | Loss: 0.00085743
Iteration 23/25 | Loss: 0.00085743
Iteration 24/25 | Loss: 0.00085743
Iteration 25/25 | Loss: 0.00085743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085743
Iteration 2/1000 | Loss: 0.00004476
Iteration 3/1000 | Loss: 0.00003368
Iteration 4/1000 | Loss: 0.00003041
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002871
Iteration 7/1000 | Loss: 0.00002834
Iteration 8/1000 | Loss: 0.00002813
Iteration 9/1000 | Loss: 0.00002789
Iteration 10/1000 | Loss: 0.00002771
Iteration 11/1000 | Loss: 0.00002765
Iteration 12/1000 | Loss: 0.00002765
Iteration 13/1000 | Loss: 0.00002764
Iteration 14/1000 | Loss: 0.00002764
Iteration 15/1000 | Loss: 0.00002763
Iteration 16/1000 | Loss: 0.00002763
Iteration 17/1000 | Loss: 0.00002762
Iteration 18/1000 | Loss: 0.00002762
Iteration 19/1000 | Loss: 0.00002762
Iteration 20/1000 | Loss: 0.00002762
Iteration 21/1000 | Loss: 0.00002762
Iteration 22/1000 | Loss: 0.00002762
Iteration 23/1000 | Loss: 0.00002762
Iteration 24/1000 | Loss: 0.00002761
Iteration 25/1000 | Loss: 0.00002761
Iteration 26/1000 | Loss: 0.00002761
Iteration 27/1000 | Loss: 0.00002753
Iteration 28/1000 | Loss: 0.00002753
Iteration 29/1000 | Loss: 0.00002752
Iteration 30/1000 | Loss: 0.00002752
Iteration 31/1000 | Loss: 0.00002752
Iteration 32/1000 | Loss: 0.00002750
Iteration 33/1000 | Loss: 0.00002748
Iteration 34/1000 | Loss: 0.00002748
Iteration 35/1000 | Loss: 0.00002748
Iteration 36/1000 | Loss: 0.00002748
Iteration 37/1000 | Loss: 0.00002748
Iteration 38/1000 | Loss: 0.00002748
Iteration 39/1000 | Loss: 0.00002748
Iteration 40/1000 | Loss: 0.00002748
Iteration 41/1000 | Loss: 0.00002748
Iteration 42/1000 | Loss: 0.00002748
Iteration 43/1000 | Loss: 0.00002747
Iteration 44/1000 | Loss: 0.00002747
Iteration 45/1000 | Loss: 0.00002747
Iteration 46/1000 | Loss: 0.00002747
Iteration 47/1000 | Loss: 0.00002746
Iteration 48/1000 | Loss: 0.00002746
Iteration 49/1000 | Loss: 0.00002746
Iteration 50/1000 | Loss: 0.00002745
Iteration 51/1000 | Loss: 0.00002745
Iteration 52/1000 | Loss: 0.00002745
Iteration 53/1000 | Loss: 0.00002745
Iteration 54/1000 | Loss: 0.00002745
Iteration 55/1000 | Loss: 0.00002744
Iteration 56/1000 | Loss: 0.00002744
Iteration 57/1000 | Loss: 0.00002744
Iteration 58/1000 | Loss: 0.00002743
Iteration 59/1000 | Loss: 0.00002743
Iteration 60/1000 | Loss: 0.00002742
Iteration 61/1000 | Loss: 0.00002742
Iteration 62/1000 | Loss: 0.00002742
Iteration 63/1000 | Loss: 0.00002742
Iteration 64/1000 | Loss: 0.00002742
Iteration 65/1000 | Loss: 0.00002742
Iteration 66/1000 | Loss: 0.00002742
Iteration 67/1000 | Loss: 0.00002742
Iteration 68/1000 | Loss: 0.00002741
Iteration 69/1000 | Loss: 0.00002741
Iteration 70/1000 | Loss: 0.00002741
Iteration 71/1000 | Loss: 0.00002741
Iteration 72/1000 | Loss: 0.00002741
Iteration 73/1000 | Loss: 0.00002740
Iteration 74/1000 | Loss: 0.00002740
Iteration 75/1000 | Loss: 0.00002740
Iteration 76/1000 | Loss: 0.00002740
Iteration 77/1000 | Loss: 0.00002740
Iteration 78/1000 | Loss: 0.00002740
Iteration 79/1000 | Loss: 0.00002740
Iteration 80/1000 | Loss: 0.00002739
Iteration 81/1000 | Loss: 0.00002739
Iteration 82/1000 | Loss: 0.00002739
Iteration 83/1000 | Loss: 0.00002739
Iteration 84/1000 | Loss: 0.00002739
Iteration 85/1000 | Loss: 0.00002739
Iteration 86/1000 | Loss: 0.00002738
Iteration 87/1000 | Loss: 0.00002738
Iteration 88/1000 | Loss: 0.00002738
Iteration 89/1000 | Loss: 0.00002738
Iteration 90/1000 | Loss: 0.00002738
Iteration 91/1000 | Loss: 0.00002737
Iteration 92/1000 | Loss: 0.00002737
Iteration 93/1000 | Loss: 0.00002737
Iteration 94/1000 | Loss: 0.00002737
Iteration 95/1000 | Loss: 0.00002737
Iteration 96/1000 | Loss: 0.00002737
Iteration 97/1000 | Loss: 0.00002737
Iteration 98/1000 | Loss: 0.00002737
Iteration 99/1000 | Loss: 0.00002737
Iteration 100/1000 | Loss: 0.00002736
Iteration 101/1000 | Loss: 0.00002736
Iteration 102/1000 | Loss: 0.00002735
Iteration 103/1000 | Loss: 0.00002735
Iteration 104/1000 | Loss: 0.00002735
Iteration 105/1000 | Loss: 0.00002735
Iteration 106/1000 | Loss: 0.00002735
Iteration 107/1000 | Loss: 0.00002735
Iteration 108/1000 | Loss: 0.00002735
Iteration 109/1000 | Loss: 0.00002735
Iteration 110/1000 | Loss: 0.00002735
Iteration 111/1000 | Loss: 0.00002735
Iteration 112/1000 | Loss: 0.00002735
Iteration 113/1000 | Loss: 0.00002735
Iteration 114/1000 | Loss: 0.00002735
Iteration 115/1000 | Loss: 0.00002735
Iteration 116/1000 | Loss: 0.00002735
Iteration 117/1000 | Loss: 0.00002735
Iteration 118/1000 | Loss: 0.00002735
Iteration 119/1000 | Loss: 0.00002735
Iteration 120/1000 | Loss: 0.00002735
Iteration 121/1000 | Loss: 0.00002735
Iteration 122/1000 | Loss: 0.00002735
Iteration 123/1000 | Loss: 0.00002735
Iteration 124/1000 | Loss: 0.00002735
Iteration 125/1000 | Loss: 0.00002735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.7347599825588986e-05, 2.7347599825588986e-05, 2.7347599825588986e-05, 2.7347599825588986e-05, 2.7347599825588986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7347599825588986e-05

Optimization complete. Final v2v error: 4.269306182861328 mm

Highest mean error: 4.695735931396484 mm for frame 238

Lowest mean error: 3.97100567817688 mm for frame 18

Saving results

Total time: 43.916898250579834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499707
Iteration 2/25 | Loss: 0.00159821
Iteration 3/25 | Loss: 0.00145127
Iteration 4/25 | Loss: 0.00143183
Iteration 5/25 | Loss: 0.00142179
Iteration 6/25 | Loss: 0.00141488
Iteration 7/25 | Loss: 0.00139934
Iteration 8/25 | Loss: 0.00138608
Iteration 9/25 | Loss: 0.00138512
Iteration 10/25 | Loss: 0.00138793
Iteration 11/25 | Loss: 0.00137569
Iteration 12/25 | Loss: 0.00136590
Iteration 13/25 | Loss: 0.00136461
Iteration 14/25 | Loss: 0.00136435
Iteration 15/25 | Loss: 0.00136417
Iteration 16/25 | Loss: 0.00136403
Iteration 17/25 | Loss: 0.00136382
Iteration 18/25 | Loss: 0.00136756
Iteration 19/25 | Loss: 0.00136194
Iteration 20/25 | Loss: 0.00136134
Iteration 21/25 | Loss: 0.00136108
Iteration 22/25 | Loss: 0.00136098
Iteration 23/25 | Loss: 0.00136081
Iteration 24/25 | Loss: 0.00136067
Iteration 25/25 | Loss: 0.00136053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43686783
Iteration 2/25 | Loss: 0.00303295
Iteration 3/25 | Loss: 0.00303295
Iteration 4/25 | Loss: 0.00303295
Iteration 5/25 | Loss: 0.00303295
Iteration 6/25 | Loss: 0.00303295
Iteration 7/25 | Loss: 0.00303295
Iteration 8/25 | Loss: 0.00303295
Iteration 9/25 | Loss: 0.00303295
Iteration 10/25 | Loss: 0.00303295
Iteration 11/25 | Loss: 0.00303295
Iteration 12/25 | Loss: 0.00303295
Iteration 13/25 | Loss: 0.00303295
Iteration 14/25 | Loss: 0.00303295
Iteration 15/25 | Loss: 0.00303295
Iteration 16/25 | Loss: 0.00303295
Iteration 17/25 | Loss: 0.00303295
Iteration 18/25 | Loss: 0.00303295
Iteration 19/25 | Loss: 0.00303295
Iteration 20/25 | Loss: 0.00303295
Iteration 21/25 | Loss: 0.00303295
Iteration 22/25 | Loss: 0.00303295
Iteration 23/25 | Loss: 0.00303295
Iteration 24/25 | Loss: 0.00303295
Iteration 25/25 | Loss: 0.00303295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.003032946726307273, 0.003032946726307273, 0.003032946726307273, 0.003032946726307273, 0.003032946726307273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003032946726307273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00303295
Iteration 2/1000 | Loss: 0.00026362
Iteration 3/1000 | Loss: 0.00014331
Iteration 4/1000 | Loss: 0.00011290
Iteration 5/1000 | Loss: 0.00026519
Iteration 6/1000 | Loss: 0.00009801
Iteration 7/1000 | Loss: 0.00027453
Iteration 8/1000 | Loss: 0.00028125
Iteration 9/1000 | Loss: 0.00026769
Iteration 10/1000 | Loss: 0.00009115
Iteration 11/1000 | Loss: 0.00010245
Iteration 12/1000 | Loss: 0.00022298
Iteration 13/1000 | Loss: 0.00023808
Iteration 14/1000 | Loss: 0.00022416
Iteration 15/1000 | Loss: 0.00008695
Iteration 16/1000 | Loss: 0.00008133
Iteration 17/1000 | Loss: 0.00007970
Iteration 18/1000 | Loss: 0.00007780
Iteration 19/1000 | Loss: 0.00007596
Iteration 20/1000 | Loss: 0.00007437
Iteration 21/1000 | Loss: 0.00007249
Iteration 22/1000 | Loss: 0.00025003
Iteration 23/1000 | Loss: 0.00007482
Iteration 24/1000 | Loss: 0.00007056
Iteration 25/1000 | Loss: 0.00024519
Iteration 26/1000 | Loss: 0.00084144
Iteration 27/1000 | Loss: 0.00039134
Iteration 28/1000 | Loss: 0.00027940
Iteration 29/1000 | Loss: 0.00009839
Iteration 30/1000 | Loss: 0.00008884
Iteration 31/1000 | Loss: 0.00008980
Iteration 32/1000 | Loss: 0.00033303
Iteration 33/1000 | Loss: 0.00027408
Iteration 34/1000 | Loss: 0.00009320
Iteration 35/1000 | Loss: 0.00007843
Iteration 36/1000 | Loss: 0.00007489
Iteration 37/1000 | Loss: 0.00028614
Iteration 38/1000 | Loss: 0.00007506
Iteration 39/1000 | Loss: 0.00007259
Iteration 40/1000 | Loss: 0.00007095
Iteration 41/1000 | Loss: 0.00006929
Iteration 42/1000 | Loss: 0.00006796
Iteration 43/1000 | Loss: 0.00006683
Iteration 44/1000 | Loss: 0.00006518
Iteration 45/1000 | Loss: 0.00006397
Iteration 46/1000 | Loss: 0.00006337
Iteration 47/1000 | Loss: 0.00006287
Iteration 48/1000 | Loss: 0.00006237
Iteration 49/1000 | Loss: 0.00006191
Iteration 50/1000 | Loss: 0.00006150
Iteration 51/1000 | Loss: 0.00029185
Iteration 52/1000 | Loss: 0.00006654
Iteration 53/1000 | Loss: 0.00006368
Iteration 54/1000 | Loss: 0.00006294
Iteration 55/1000 | Loss: 0.00006168
Iteration 56/1000 | Loss: 0.00006580
Iteration 57/1000 | Loss: 0.00006986
Iteration 58/1000 | Loss: 0.00006167
Iteration 59/1000 | Loss: 0.00006052
Iteration 60/1000 | Loss: 0.00005995
Iteration 61/1000 | Loss: 0.00005959
Iteration 62/1000 | Loss: 0.00005943
Iteration 63/1000 | Loss: 0.00005918
Iteration 64/1000 | Loss: 0.00005890
Iteration 65/1000 | Loss: 0.00005869
Iteration 66/1000 | Loss: 0.00005852
Iteration 67/1000 | Loss: 0.00005845
Iteration 68/1000 | Loss: 0.00005826
Iteration 69/1000 | Loss: 0.00005825
Iteration 70/1000 | Loss: 0.00005814
Iteration 71/1000 | Loss: 0.00005810
Iteration 72/1000 | Loss: 0.00005809
Iteration 73/1000 | Loss: 0.00005805
Iteration 74/1000 | Loss: 0.00005804
Iteration 75/1000 | Loss: 0.00005801
Iteration 76/1000 | Loss: 0.00005801
Iteration 77/1000 | Loss: 0.00005800
Iteration 78/1000 | Loss: 0.00005800
Iteration 79/1000 | Loss: 0.00005799
Iteration 80/1000 | Loss: 0.00005799
Iteration 81/1000 | Loss: 0.00005798
Iteration 82/1000 | Loss: 0.00005798
Iteration 83/1000 | Loss: 0.00005797
Iteration 84/1000 | Loss: 0.00005797
Iteration 85/1000 | Loss: 0.00005796
Iteration 86/1000 | Loss: 0.00005796
Iteration 87/1000 | Loss: 0.00005796
Iteration 88/1000 | Loss: 0.00005796
Iteration 89/1000 | Loss: 0.00005796
Iteration 90/1000 | Loss: 0.00005795
Iteration 91/1000 | Loss: 0.00005795
Iteration 92/1000 | Loss: 0.00005795
Iteration 93/1000 | Loss: 0.00005795
Iteration 94/1000 | Loss: 0.00005795
Iteration 95/1000 | Loss: 0.00005795
Iteration 96/1000 | Loss: 0.00005795
Iteration 97/1000 | Loss: 0.00005794
Iteration 98/1000 | Loss: 0.00005794
Iteration 99/1000 | Loss: 0.00005794
Iteration 100/1000 | Loss: 0.00005794
Iteration 101/1000 | Loss: 0.00005793
Iteration 102/1000 | Loss: 0.00005793
Iteration 103/1000 | Loss: 0.00005792
Iteration 104/1000 | Loss: 0.00005792
Iteration 105/1000 | Loss: 0.00005792
Iteration 106/1000 | Loss: 0.00005791
Iteration 107/1000 | Loss: 0.00005790
Iteration 108/1000 | Loss: 0.00005790
Iteration 109/1000 | Loss: 0.00005790
Iteration 110/1000 | Loss: 0.00005788
Iteration 111/1000 | Loss: 0.00005788
Iteration 112/1000 | Loss: 0.00005787
Iteration 113/1000 | Loss: 0.00005787
Iteration 114/1000 | Loss: 0.00005787
Iteration 115/1000 | Loss: 0.00005787
Iteration 116/1000 | Loss: 0.00005786
Iteration 117/1000 | Loss: 0.00005784
Iteration 118/1000 | Loss: 0.00005784
Iteration 119/1000 | Loss: 0.00005784
Iteration 120/1000 | Loss: 0.00005783
Iteration 121/1000 | Loss: 0.00005783
Iteration 122/1000 | Loss: 0.00005783
Iteration 123/1000 | Loss: 0.00005783
Iteration 124/1000 | Loss: 0.00005782
Iteration 125/1000 | Loss: 0.00005782
Iteration 126/1000 | Loss: 0.00005782
Iteration 127/1000 | Loss: 0.00005782
Iteration 128/1000 | Loss: 0.00005781
Iteration 129/1000 | Loss: 0.00005781
Iteration 130/1000 | Loss: 0.00005781
Iteration 131/1000 | Loss: 0.00005781
Iteration 132/1000 | Loss: 0.00005780
Iteration 133/1000 | Loss: 0.00005780
Iteration 134/1000 | Loss: 0.00005780
Iteration 135/1000 | Loss: 0.00005780
Iteration 136/1000 | Loss: 0.00005780
Iteration 137/1000 | Loss: 0.00005780
Iteration 138/1000 | Loss: 0.00005780
Iteration 139/1000 | Loss: 0.00005780
Iteration 140/1000 | Loss: 0.00005780
Iteration 141/1000 | Loss: 0.00005780
Iteration 142/1000 | Loss: 0.00005780
Iteration 143/1000 | Loss: 0.00005779
Iteration 144/1000 | Loss: 0.00005779
Iteration 145/1000 | Loss: 0.00005779
Iteration 146/1000 | Loss: 0.00005779
Iteration 147/1000 | Loss: 0.00005779
Iteration 148/1000 | Loss: 0.00005779
Iteration 149/1000 | Loss: 0.00005778
Iteration 150/1000 | Loss: 0.00005778
Iteration 151/1000 | Loss: 0.00005778
Iteration 152/1000 | Loss: 0.00005778
Iteration 153/1000 | Loss: 0.00005778
Iteration 154/1000 | Loss: 0.00005778
Iteration 155/1000 | Loss: 0.00005777
Iteration 156/1000 | Loss: 0.00005777
Iteration 157/1000 | Loss: 0.00005777
Iteration 158/1000 | Loss: 0.00005777
Iteration 159/1000 | Loss: 0.00005777
Iteration 160/1000 | Loss: 0.00005777
Iteration 161/1000 | Loss: 0.00005776
Iteration 162/1000 | Loss: 0.00005776
Iteration 163/1000 | Loss: 0.00005776
Iteration 164/1000 | Loss: 0.00005776
Iteration 165/1000 | Loss: 0.00005776
Iteration 166/1000 | Loss: 0.00005776
Iteration 167/1000 | Loss: 0.00005776
Iteration 168/1000 | Loss: 0.00005776
Iteration 169/1000 | Loss: 0.00005776
Iteration 170/1000 | Loss: 0.00005776
Iteration 171/1000 | Loss: 0.00005775
Iteration 172/1000 | Loss: 0.00005775
Iteration 173/1000 | Loss: 0.00005775
Iteration 174/1000 | Loss: 0.00005775
Iteration 175/1000 | Loss: 0.00005775
Iteration 176/1000 | Loss: 0.00005775
Iteration 177/1000 | Loss: 0.00005775
Iteration 178/1000 | Loss: 0.00005774
Iteration 179/1000 | Loss: 0.00005774
Iteration 180/1000 | Loss: 0.00005774
Iteration 181/1000 | Loss: 0.00005774
Iteration 182/1000 | Loss: 0.00005774
Iteration 183/1000 | Loss: 0.00005774
Iteration 184/1000 | Loss: 0.00005774
Iteration 185/1000 | Loss: 0.00005774
Iteration 186/1000 | Loss: 0.00005774
Iteration 187/1000 | Loss: 0.00005774
Iteration 188/1000 | Loss: 0.00005774
Iteration 189/1000 | Loss: 0.00005774
Iteration 190/1000 | Loss: 0.00005774
Iteration 191/1000 | Loss: 0.00005773
Iteration 192/1000 | Loss: 0.00005773
Iteration 193/1000 | Loss: 0.00005773
Iteration 194/1000 | Loss: 0.00005773
Iteration 195/1000 | Loss: 0.00005773
Iteration 196/1000 | Loss: 0.00005773
Iteration 197/1000 | Loss: 0.00005773
Iteration 198/1000 | Loss: 0.00005772
Iteration 199/1000 | Loss: 0.00005772
Iteration 200/1000 | Loss: 0.00005772
Iteration 201/1000 | Loss: 0.00005772
Iteration 202/1000 | Loss: 0.00005772
Iteration 203/1000 | Loss: 0.00005772
Iteration 204/1000 | Loss: 0.00005772
Iteration 205/1000 | Loss: 0.00005772
Iteration 206/1000 | Loss: 0.00005772
Iteration 207/1000 | Loss: 0.00005772
Iteration 208/1000 | Loss: 0.00005771
Iteration 209/1000 | Loss: 0.00005771
Iteration 210/1000 | Loss: 0.00005771
Iteration 211/1000 | Loss: 0.00005771
Iteration 212/1000 | Loss: 0.00005771
Iteration 213/1000 | Loss: 0.00005771
Iteration 214/1000 | Loss: 0.00005771
Iteration 215/1000 | Loss: 0.00005771
Iteration 216/1000 | Loss: 0.00005771
Iteration 217/1000 | Loss: 0.00005771
Iteration 218/1000 | Loss: 0.00005770
Iteration 219/1000 | Loss: 0.00005770
Iteration 220/1000 | Loss: 0.00005770
Iteration 221/1000 | Loss: 0.00005770
Iteration 222/1000 | Loss: 0.00005770
Iteration 223/1000 | Loss: 0.00005770
Iteration 224/1000 | Loss: 0.00005770
Iteration 225/1000 | Loss: 0.00005770
Iteration 226/1000 | Loss: 0.00005770
Iteration 227/1000 | Loss: 0.00005770
Iteration 228/1000 | Loss: 0.00005770
Iteration 229/1000 | Loss: 0.00005770
Iteration 230/1000 | Loss: 0.00005770
Iteration 231/1000 | Loss: 0.00005770
Iteration 232/1000 | Loss: 0.00005769
Iteration 233/1000 | Loss: 0.00005769
Iteration 234/1000 | Loss: 0.00005769
Iteration 235/1000 | Loss: 0.00005769
Iteration 236/1000 | Loss: 0.00005769
Iteration 237/1000 | Loss: 0.00005769
Iteration 238/1000 | Loss: 0.00005769
Iteration 239/1000 | Loss: 0.00005769
Iteration 240/1000 | Loss: 0.00005769
Iteration 241/1000 | Loss: 0.00005769
Iteration 242/1000 | Loss: 0.00005769
Iteration 243/1000 | Loss: 0.00005769
Iteration 244/1000 | Loss: 0.00005769
Iteration 245/1000 | Loss: 0.00005769
Iteration 246/1000 | Loss: 0.00005769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [5.769131894339807e-05, 5.769131894339807e-05, 5.769131894339807e-05, 5.769131894339807e-05, 5.769131894339807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.769131894339807e-05

Optimization complete. Final v2v error: 4.5694499015808105 mm

Highest mean error: 11.5416898727417 mm for frame 107

Lowest mean error: 2.893970251083374 mm for frame 162

Saving results

Total time: 163.868590593338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971127
Iteration 2/25 | Loss: 0.00172308
Iteration 3/25 | Loss: 0.00148933
Iteration 4/25 | Loss: 0.00146291
Iteration 5/25 | Loss: 0.00145467
Iteration 6/25 | Loss: 0.00145284
Iteration 7/25 | Loss: 0.00145250
Iteration 8/25 | Loss: 0.00145250
Iteration 9/25 | Loss: 0.00145250
Iteration 10/25 | Loss: 0.00145250
Iteration 11/25 | Loss: 0.00145250
Iteration 12/25 | Loss: 0.00145250
Iteration 13/25 | Loss: 0.00145250
Iteration 14/25 | Loss: 0.00145250
Iteration 15/25 | Loss: 0.00145250
Iteration 16/25 | Loss: 0.00145250
Iteration 17/25 | Loss: 0.00145250
Iteration 18/25 | Loss: 0.00145250
Iteration 19/25 | Loss: 0.00145250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014524958096444607, 0.0014524958096444607, 0.0014524958096444607, 0.0014524958096444607, 0.0014524958096444607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014524958096444607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25775218
Iteration 2/25 | Loss: 0.00099093
Iteration 3/25 | Loss: 0.00099089
Iteration 4/25 | Loss: 0.00099089
Iteration 5/25 | Loss: 0.00099089
Iteration 6/25 | Loss: 0.00099089
Iteration 7/25 | Loss: 0.00099089
Iteration 8/25 | Loss: 0.00099089
Iteration 9/25 | Loss: 0.00099089
Iteration 10/25 | Loss: 0.00099089
Iteration 11/25 | Loss: 0.00099089
Iteration 12/25 | Loss: 0.00099089
Iteration 13/25 | Loss: 0.00099089
Iteration 14/25 | Loss: 0.00099089
Iteration 15/25 | Loss: 0.00099089
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009908899664878845, 0.0009908899664878845, 0.0009908899664878845, 0.0009908899664878845, 0.0009908899664878845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009908899664878845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099089
Iteration 2/1000 | Loss: 0.00008937
Iteration 3/1000 | Loss: 0.00005789
Iteration 4/1000 | Loss: 0.00004951
Iteration 5/1000 | Loss: 0.00004668
Iteration 6/1000 | Loss: 0.00004513
Iteration 7/1000 | Loss: 0.00004371
Iteration 8/1000 | Loss: 0.00004281
Iteration 9/1000 | Loss: 0.00004216
Iteration 10/1000 | Loss: 0.00004162
Iteration 11/1000 | Loss: 0.00004115
Iteration 12/1000 | Loss: 0.00004084
Iteration 13/1000 | Loss: 0.00004054
Iteration 14/1000 | Loss: 0.00004033
Iteration 15/1000 | Loss: 0.00004018
Iteration 16/1000 | Loss: 0.00004016
Iteration 17/1000 | Loss: 0.00004002
Iteration 18/1000 | Loss: 0.00003992
Iteration 19/1000 | Loss: 0.00003989
Iteration 20/1000 | Loss: 0.00003988
Iteration 21/1000 | Loss: 0.00003983
Iteration 22/1000 | Loss: 0.00003979
Iteration 23/1000 | Loss: 0.00003979
Iteration 24/1000 | Loss: 0.00003976
Iteration 25/1000 | Loss: 0.00003976
Iteration 26/1000 | Loss: 0.00003970
Iteration 27/1000 | Loss: 0.00003970
Iteration 28/1000 | Loss: 0.00003969
Iteration 29/1000 | Loss: 0.00003969
Iteration 30/1000 | Loss: 0.00003968
Iteration 31/1000 | Loss: 0.00003967
Iteration 32/1000 | Loss: 0.00003964
Iteration 33/1000 | Loss: 0.00003961
Iteration 34/1000 | Loss: 0.00003955
Iteration 35/1000 | Loss: 0.00003955
Iteration 36/1000 | Loss: 0.00003955
Iteration 37/1000 | Loss: 0.00003954
Iteration 38/1000 | Loss: 0.00003953
Iteration 39/1000 | Loss: 0.00003953
Iteration 40/1000 | Loss: 0.00003950
Iteration 41/1000 | Loss: 0.00003948
Iteration 42/1000 | Loss: 0.00003948
Iteration 43/1000 | Loss: 0.00003947
Iteration 44/1000 | Loss: 0.00003946
Iteration 45/1000 | Loss: 0.00003946
Iteration 46/1000 | Loss: 0.00003946
Iteration 47/1000 | Loss: 0.00003945
Iteration 48/1000 | Loss: 0.00003945
Iteration 49/1000 | Loss: 0.00003944
Iteration 50/1000 | Loss: 0.00003944
Iteration 51/1000 | Loss: 0.00003944
Iteration 52/1000 | Loss: 0.00003944
Iteration 53/1000 | Loss: 0.00003943
Iteration 54/1000 | Loss: 0.00003943
Iteration 55/1000 | Loss: 0.00003943
Iteration 56/1000 | Loss: 0.00003943
Iteration 57/1000 | Loss: 0.00003942
Iteration 58/1000 | Loss: 0.00003942
Iteration 59/1000 | Loss: 0.00003942
Iteration 60/1000 | Loss: 0.00003942
Iteration 61/1000 | Loss: 0.00003942
Iteration 62/1000 | Loss: 0.00003941
Iteration 63/1000 | Loss: 0.00003941
Iteration 64/1000 | Loss: 0.00003941
Iteration 65/1000 | Loss: 0.00003940
Iteration 66/1000 | Loss: 0.00003940
Iteration 67/1000 | Loss: 0.00003940
Iteration 68/1000 | Loss: 0.00003939
Iteration 69/1000 | Loss: 0.00003939
Iteration 70/1000 | Loss: 0.00003939
Iteration 71/1000 | Loss: 0.00003939
Iteration 72/1000 | Loss: 0.00003938
Iteration 73/1000 | Loss: 0.00003938
Iteration 74/1000 | Loss: 0.00003938
Iteration 75/1000 | Loss: 0.00003938
Iteration 76/1000 | Loss: 0.00003937
Iteration 77/1000 | Loss: 0.00003937
Iteration 78/1000 | Loss: 0.00003937
Iteration 79/1000 | Loss: 0.00003937
Iteration 80/1000 | Loss: 0.00003936
Iteration 81/1000 | Loss: 0.00003936
Iteration 82/1000 | Loss: 0.00003936
Iteration 83/1000 | Loss: 0.00003936
Iteration 84/1000 | Loss: 0.00003936
Iteration 85/1000 | Loss: 0.00003936
Iteration 86/1000 | Loss: 0.00003935
Iteration 87/1000 | Loss: 0.00003935
Iteration 88/1000 | Loss: 0.00003935
Iteration 89/1000 | Loss: 0.00003935
Iteration 90/1000 | Loss: 0.00003935
Iteration 91/1000 | Loss: 0.00003934
Iteration 92/1000 | Loss: 0.00003934
Iteration 93/1000 | Loss: 0.00003934
Iteration 94/1000 | Loss: 0.00003934
Iteration 95/1000 | Loss: 0.00003934
Iteration 96/1000 | Loss: 0.00003934
Iteration 97/1000 | Loss: 0.00003934
Iteration 98/1000 | Loss: 0.00003934
Iteration 99/1000 | Loss: 0.00003934
Iteration 100/1000 | Loss: 0.00003934
Iteration 101/1000 | Loss: 0.00003934
Iteration 102/1000 | Loss: 0.00003933
Iteration 103/1000 | Loss: 0.00003933
Iteration 104/1000 | Loss: 0.00003933
Iteration 105/1000 | Loss: 0.00003932
Iteration 106/1000 | Loss: 0.00003932
Iteration 107/1000 | Loss: 0.00003932
Iteration 108/1000 | Loss: 0.00003932
Iteration 109/1000 | Loss: 0.00003932
Iteration 110/1000 | Loss: 0.00003932
Iteration 111/1000 | Loss: 0.00003932
Iteration 112/1000 | Loss: 0.00003932
Iteration 113/1000 | Loss: 0.00003932
Iteration 114/1000 | Loss: 0.00003932
Iteration 115/1000 | Loss: 0.00003932
Iteration 116/1000 | Loss: 0.00003932
Iteration 117/1000 | Loss: 0.00003932
Iteration 118/1000 | Loss: 0.00003932
Iteration 119/1000 | Loss: 0.00003932
Iteration 120/1000 | Loss: 0.00003932
Iteration 121/1000 | Loss: 0.00003932
Iteration 122/1000 | Loss: 0.00003932
Iteration 123/1000 | Loss: 0.00003932
Iteration 124/1000 | Loss: 0.00003932
Iteration 125/1000 | Loss: 0.00003932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.932322215405293e-05, 3.932322215405293e-05, 3.932322215405293e-05, 3.932322215405293e-05, 3.932322215405293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.932322215405293e-05

Optimization complete. Final v2v error: 5.0364227294921875 mm

Highest mean error: 6.184330463409424 mm for frame 123

Lowest mean error: 4.084926128387451 mm for frame 78

Saving results

Total time: 53.003660917282104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793680
Iteration 2/25 | Loss: 0.00191413
Iteration 3/25 | Loss: 0.00135027
Iteration 4/25 | Loss: 0.00126051
Iteration 5/25 | Loss: 0.00124796
Iteration 6/25 | Loss: 0.00124949
Iteration 7/25 | Loss: 0.00124694
Iteration 8/25 | Loss: 0.00124354
Iteration 9/25 | Loss: 0.00123781
Iteration 10/25 | Loss: 0.00123780
Iteration 11/25 | Loss: 0.00123630
Iteration 12/25 | Loss: 0.00123322
Iteration 13/25 | Loss: 0.00123566
Iteration 14/25 | Loss: 0.00123287
Iteration 15/25 | Loss: 0.00123284
Iteration 16/25 | Loss: 0.00123284
Iteration 17/25 | Loss: 0.00123284
Iteration 18/25 | Loss: 0.00123284
Iteration 19/25 | Loss: 0.00123284
Iteration 20/25 | Loss: 0.00123284
Iteration 21/25 | Loss: 0.00123284
Iteration 22/25 | Loss: 0.00123283
Iteration 23/25 | Loss: 0.00123283
Iteration 24/25 | Loss: 0.00123283
Iteration 25/25 | Loss: 0.00123283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99162841
Iteration 2/25 | Loss: 0.00081437
Iteration 3/25 | Loss: 0.00081436
Iteration 4/25 | Loss: 0.00077225
Iteration 5/25 | Loss: 0.00077225
Iteration 6/25 | Loss: 0.00077225
Iteration 7/25 | Loss: 0.00077225
Iteration 8/25 | Loss: 0.00077225
Iteration 9/25 | Loss: 0.00077225
Iteration 10/25 | Loss: 0.00077225
Iteration 11/25 | Loss: 0.00077225
Iteration 12/25 | Loss: 0.00077225
Iteration 13/25 | Loss: 0.00077225
Iteration 14/25 | Loss: 0.00077225
Iteration 15/25 | Loss: 0.00077225
Iteration 16/25 | Loss: 0.00077225
Iteration 17/25 | Loss: 0.00077225
Iteration 18/25 | Loss: 0.00077225
Iteration 19/25 | Loss: 0.00077225
Iteration 20/25 | Loss: 0.00077225
Iteration 21/25 | Loss: 0.00077225
Iteration 22/25 | Loss: 0.00077225
Iteration 23/25 | Loss: 0.00077225
Iteration 24/25 | Loss: 0.00077225
Iteration 25/25 | Loss: 0.00077225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077225
Iteration 2/1000 | Loss: 0.00007866
Iteration 3/1000 | Loss: 0.00005383
Iteration 4/1000 | Loss: 0.00002422
Iteration 5/1000 | Loss: 0.00009237
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00005377
Iteration 9/1000 | Loss: 0.00003179
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001954
Iteration 12/1000 | Loss: 0.00001594
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001561
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001547
Iteration 19/1000 | Loss: 0.00001547
Iteration 20/1000 | Loss: 0.00001547
Iteration 21/1000 | Loss: 0.00001546
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001535
Iteration 28/1000 | Loss: 0.00001531
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001515
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001506
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001506
Iteration 45/1000 | Loss: 0.00001506
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001506
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001506
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.5060625628393609e-05, 1.5060625628393609e-05, 1.5060625628393609e-05, 1.5060625628393609e-05, 1.5060625628393609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5060625628393609e-05

Optimization complete. Final v2v error: 3.290109872817993 mm

Highest mean error: 3.6426730155944824 mm for frame 64

Lowest mean error: 3.109680652618408 mm for frame 167

Saving results

Total time: 52.30507493019104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808915
Iteration 2/25 | Loss: 0.00187707
Iteration 3/25 | Loss: 0.00136869
Iteration 4/25 | Loss: 0.00131180
Iteration 5/25 | Loss: 0.00130695
Iteration 6/25 | Loss: 0.00130695
Iteration 7/25 | Loss: 0.00130695
Iteration 8/25 | Loss: 0.00130695
Iteration 9/25 | Loss: 0.00130695
Iteration 10/25 | Loss: 0.00130695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013069475535303354, 0.0013069475535303354, 0.0013069475535303354, 0.0013069475535303354, 0.0013069475535303354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013069475535303354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42726982
Iteration 2/25 | Loss: 0.00066597
Iteration 3/25 | Loss: 0.00066597
Iteration 4/25 | Loss: 0.00066597
Iteration 5/25 | Loss: 0.00066597
Iteration 6/25 | Loss: 0.00066597
Iteration 7/25 | Loss: 0.00066597
Iteration 8/25 | Loss: 0.00066597
Iteration 9/25 | Loss: 0.00066597
Iteration 10/25 | Loss: 0.00066597
Iteration 11/25 | Loss: 0.00066597
Iteration 12/25 | Loss: 0.00066597
Iteration 13/25 | Loss: 0.00066596
Iteration 14/25 | Loss: 0.00066596
Iteration 15/25 | Loss: 0.00066596
Iteration 16/25 | Loss: 0.00066596
Iteration 17/25 | Loss: 0.00066596
Iteration 18/25 | Loss: 0.00066596
Iteration 19/25 | Loss: 0.00066596
Iteration 20/25 | Loss: 0.00066596
Iteration 21/25 | Loss: 0.00066596
Iteration 22/25 | Loss: 0.00066596
Iteration 23/25 | Loss: 0.00066596
Iteration 24/25 | Loss: 0.00066596
Iteration 25/25 | Loss: 0.00066596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006659647333435714, 0.0006659647333435714, 0.0006659647333435714, 0.0006659647333435714, 0.0006659647333435714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006659647333435714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066596
Iteration 2/1000 | Loss: 0.00003468
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002463
Iteration 5/1000 | Loss: 0.00002366
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002254
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002194
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002154
Iteration 14/1000 | Loss: 0.00002146
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002143
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002141
Iteration 20/1000 | Loss: 0.00002139
Iteration 21/1000 | Loss: 0.00002139
Iteration 22/1000 | Loss: 0.00002139
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002136
Iteration 27/1000 | Loss: 0.00002136
Iteration 28/1000 | Loss: 0.00002132
Iteration 29/1000 | Loss: 0.00002127
Iteration 30/1000 | Loss: 0.00002127
Iteration 31/1000 | Loss: 0.00002125
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00002123
Iteration 34/1000 | Loss: 0.00002123
Iteration 35/1000 | Loss: 0.00002123
Iteration 36/1000 | Loss: 0.00002122
Iteration 37/1000 | Loss: 0.00002122
Iteration 38/1000 | Loss: 0.00002122
Iteration 39/1000 | Loss: 0.00002121
Iteration 40/1000 | Loss: 0.00002121
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002119
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002118
Iteration 52/1000 | Loss: 0.00002118
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002116
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002115
Iteration 76/1000 | Loss: 0.00002115
Iteration 77/1000 | Loss: 0.00002115
Iteration 78/1000 | Loss: 0.00002115
Iteration 79/1000 | Loss: 0.00002114
Iteration 80/1000 | Loss: 0.00002114
Iteration 81/1000 | Loss: 0.00002114
Iteration 82/1000 | Loss: 0.00002114
Iteration 83/1000 | Loss: 0.00002114
Iteration 84/1000 | Loss: 0.00002114
Iteration 85/1000 | Loss: 0.00002114
Iteration 86/1000 | Loss: 0.00002114
Iteration 87/1000 | Loss: 0.00002113
Iteration 88/1000 | Loss: 0.00002113
Iteration 89/1000 | Loss: 0.00002113
Iteration 90/1000 | Loss: 0.00002113
Iteration 91/1000 | Loss: 0.00002113
Iteration 92/1000 | Loss: 0.00002112
Iteration 93/1000 | Loss: 0.00002112
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002112
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002110
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002110
Iteration 106/1000 | Loss: 0.00002110
Iteration 107/1000 | Loss: 0.00002110
Iteration 108/1000 | Loss: 0.00002110
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002110
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002110
Iteration 113/1000 | Loss: 0.00002110
Iteration 114/1000 | Loss: 0.00002110
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00002110
Iteration 117/1000 | Loss: 0.00002110
Iteration 118/1000 | Loss: 0.00002110
Iteration 119/1000 | Loss: 0.00002110
Iteration 120/1000 | Loss: 0.00002110
Iteration 121/1000 | Loss: 0.00002110
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002110
Iteration 129/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.1103262042743154e-05, 2.1103262042743154e-05, 2.1103262042743154e-05, 2.1103262042743154e-05, 2.1103262042743154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1103262042743154e-05

Optimization complete. Final v2v error: 3.8225138187408447 mm

Highest mean error: 3.8957982063293457 mm for frame 75

Lowest mean error: 3.4663138389587402 mm for frame 5

Saving results

Total time: 38.811123847961426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448895
Iteration 2/25 | Loss: 0.00139470
Iteration 3/25 | Loss: 0.00128505
Iteration 4/25 | Loss: 0.00127905
Iteration 5/25 | Loss: 0.00127791
Iteration 6/25 | Loss: 0.00127791
Iteration 7/25 | Loss: 0.00127791
Iteration 8/25 | Loss: 0.00127791
Iteration 9/25 | Loss: 0.00127791
Iteration 10/25 | Loss: 0.00127791
Iteration 11/25 | Loss: 0.00127791
Iteration 12/25 | Loss: 0.00127791
Iteration 13/25 | Loss: 0.00127791
Iteration 14/25 | Loss: 0.00127791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012779096141457558, 0.0012779096141457558, 0.0012779096141457558, 0.0012779096141457558, 0.0012779096141457558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012779096141457558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.85194540
Iteration 2/25 | Loss: 0.00070791
Iteration 3/25 | Loss: 0.00070786
Iteration 4/25 | Loss: 0.00070786
Iteration 5/25 | Loss: 0.00070786
Iteration 6/25 | Loss: 0.00070786
Iteration 7/25 | Loss: 0.00070786
Iteration 8/25 | Loss: 0.00070786
Iteration 9/25 | Loss: 0.00070786
Iteration 10/25 | Loss: 0.00070786
Iteration 11/25 | Loss: 0.00070786
Iteration 12/25 | Loss: 0.00070786
Iteration 13/25 | Loss: 0.00070786
Iteration 14/25 | Loss: 0.00070786
Iteration 15/25 | Loss: 0.00070786
Iteration 16/25 | Loss: 0.00070786
Iteration 17/25 | Loss: 0.00070786
Iteration 18/25 | Loss: 0.00070786
Iteration 19/25 | Loss: 0.00070786
Iteration 20/25 | Loss: 0.00070786
Iteration 21/25 | Loss: 0.00070786
Iteration 22/25 | Loss: 0.00070786
Iteration 23/25 | Loss: 0.00070786
Iteration 24/25 | Loss: 0.00070786
Iteration 25/25 | Loss: 0.00070786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070786
Iteration 2/1000 | Loss: 0.00002877
Iteration 3/1000 | Loss: 0.00002003
Iteration 4/1000 | Loss: 0.00001854
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001710
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001583
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001575
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001565
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001562
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001559
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001559
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001558
Iteration 38/1000 | Loss: 0.00001558
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001557
Iteration 41/1000 | Loss: 0.00001557
Iteration 42/1000 | Loss: 0.00001556
Iteration 43/1000 | Loss: 0.00001556
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001555
Iteration 47/1000 | Loss: 0.00001555
Iteration 48/1000 | Loss: 0.00001555
Iteration 49/1000 | Loss: 0.00001555
Iteration 50/1000 | Loss: 0.00001555
Iteration 51/1000 | Loss: 0.00001554
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001554
Iteration 54/1000 | Loss: 0.00001553
Iteration 55/1000 | Loss: 0.00001553
Iteration 56/1000 | Loss: 0.00001553
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001551
Iteration 62/1000 | Loss: 0.00001551
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00001550
Iteration 65/1000 | Loss: 0.00001550
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001541
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00001539
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001537
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001532
Iteration 113/1000 | Loss: 0.00001532
Iteration 114/1000 | Loss: 0.00001532
Iteration 115/1000 | Loss: 0.00001531
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001531
Iteration 118/1000 | Loss: 0.00001531
Iteration 119/1000 | Loss: 0.00001530
Iteration 120/1000 | Loss: 0.00001530
Iteration 121/1000 | Loss: 0.00001530
Iteration 122/1000 | Loss: 0.00001530
Iteration 123/1000 | Loss: 0.00001530
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001527
Iteration 132/1000 | Loss: 0.00001527
Iteration 133/1000 | Loss: 0.00001527
Iteration 134/1000 | Loss: 0.00001527
Iteration 135/1000 | Loss: 0.00001527
Iteration 136/1000 | Loss: 0.00001527
Iteration 137/1000 | Loss: 0.00001527
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001526
Iteration 142/1000 | Loss: 0.00001526
Iteration 143/1000 | Loss: 0.00001526
Iteration 144/1000 | Loss: 0.00001526
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001525
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001525
Iteration 155/1000 | Loss: 0.00001525
Iteration 156/1000 | Loss: 0.00001525
Iteration 157/1000 | Loss: 0.00001525
Iteration 158/1000 | Loss: 0.00001525
Iteration 159/1000 | Loss: 0.00001525
Iteration 160/1000 | Loss: 0.00001524
Iteration 161/1000 | Loss: 0.00001524
Iteration 162/1000 | Loss: 0.00001524
Iteration 163/1000 | Loss: 0.00001524
Iteration 164/1000 | Loss: 0.00001524
Iteration 165/1000 | Loss: 0.00001524
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001523
Iteration 171/1000 | Loss: 0.00001523
Iteration 172/1000 | Loss: 0.00001523
Iteration 173/1000 | Loss: 0.00001523
Iteration 174/1000 | Loss: 0.00001523
Iteration 175/1000 | Loss: 0.00001523
Iteration 176/1000 | Loss: 0.00001523
Iteration 177/1000 | Loss: 0.00001522
Iteration 178/1000 | Loss: 0.00001522
Iteration 179/1000 | Loss: 0.00001522
Iteration 180/1000 | Loss: 0.00001522
Iteration 181/1000 | Loss: 0.00001522
Iteration 182/1000 | Loss: 0.00001522
Iteration 183/1000 | Loss: 0.00001521
Iteration 184/1000 | Loss: 0.00001521
Iteration 185/1000 | Loss: 0.00001521
Iteration 186/1000 | Loss: 0.00001521
Iteration 187/1000 | Loss: 0.00001521
Iteration 188/1000 | Loss: 0.00001521
Iteration 189/1000 | Loss: 0.00001521
Iteration 190/1000 | Loss: 0.00001521
Iteration 191/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.5212672224151902e-05, 1.5212672224151902e-05, 1.5212672224151902e-05, 1.5212672224151902e-05, 1.5212672224151902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5212672224151902e-05

Optimization complete. Final v2v error: 3.2427608966827393 mm

Highest mean error: 3.6243231296539307 mm for frame 78

Lowest mean error: 2.961193799972534 mm for frame 18

Saving results

Total time: 38.40732002258301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826222
Iteration 2/25 | Loss: 0.00131580
Iteration 3/25 | Loss: 0.00124069
Iteration 4/25 | Loss: 0.00122830
Iteration 5/25 | Loss: 0.00122501
Iteration 6/25 | Loss: 0.00122501
Iteration 7/25 | Loss: 0.00122501
Iteration 8/25 | Loss: 0.00122501
Iteration 9/25 | Loss: 0.00122501
Iteration 10/25 | Loss: 0.00122501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001225010841153562, 0.001225010841153562, 0.001225010841153562, 0.001225010841153562, 0.001225010841153562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225010841153562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44300127
Iteration 2/25 | Loss: 0.00072052
Iteration 3/25 | Loss: 0.00072052
Iteration 4/25 | Loss: 0.00072052
Iteration 5/25 | Loss: 0.00072052
Iteration 6/25 | Loss: 0.00072052
Iteration 7/25 | Loss: 0.00072051
Iteration 8/25 | Loss: 0.00072051
Iteration 9/25 | Loss: 0.00072051
Iteration 10/25 | Loss: 0.00072051
Iteration 11/25 | Loss: 0.00072051
Iteration 12/25 | Loss: 0.00072051
Iteration 13/25 | Loss: 0.00072051
Iteration 14/25 | Loss: 0.00072051
Iteration 15/25 | Loss: 0.00072051
Iteration 16/25 | Loss: 0.00072051
Iteration 17/25 | Loss: 0.00072051
Iteration 18/25 | Loss: 0.00072051
Iteration 19/25 | Loss: 0.00072051
Iteration 20/25 | Loss: 0.00072051
Iteration 21/25 | Loss: 0.00072051
Iteration 22/25 | Loss: 0.00072051
Iteration 23/25 | Loss: 0.00072051
Iteration 24/25 | Loss: 0.00072051
Iteration 25/25 | Loss: 0.00072051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072051
Iteration 2/1000 | Loss: 0.00002489
Iteration 3/1000 | Loss: 0.00001961
Iteration 4/1000 | Loss: 0.00001756
Iteration 5/1000 | Loss: 0.00001667
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001478
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001476
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001471
Iteration 15/1000 | Loss: 0.00001464
Iteration 16/1000 | Loss: 0.00001456
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001441
Iteration 19/1000 | Loss: 0.00001440
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001439
Iteration 27/1000 | Loss: 0.00001437
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001435
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001428
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001427
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001426
Iteration 53/1000 | Loss: 0.00001426
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001426
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001417
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001414
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001412
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001409
Iteration 95/1000 | Loss: 0.00001409
Iteration 96/1000 | Loss: 0.00001409
Iteration 97/1000 | Loss: 0.00001408
Iteration 98/1000 | Loss: 0.00001408
Iteration 99/1000 | Loss: 0.00001407
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001399
Iteration 114/1000 | Loss: 0.00001399
Iteration 115/1000 | Loss: 0.00001399
Iteration 116/1000 | Loss: 0.00001399
Iteration 117/1000 | Loss: 0.00001398
Iteration 118/1000 | Loss: 0.00001398
Iteration 119/1000 | Loss: 0.00001398
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001398
Iteration 123/1000 | Loss: 0.00001398
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001396
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001395
Iteration 140/1000 | Loss: 0.00001395
Iteration 141/1000 | Loss: 0.00001395
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001393
Iteration 170/1000 | Loss: 0.00001393
Iteration 171/1000 | Loss: 0.00001393
Iteration 172/1000 | Loss: 0.00001393
Iteration 173/1000 | Loss: 0.00001393
Iteration 174/1000 | Loss: 0.00001393
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001393
Iteration 180/1000 | Loss: 0.00001393
Iteration 181/1000 | Loss: 0.00001393
Iteration 182/1000 | Loss: 0.00001393
Iteration 183/1000 | Loss: 0.00001393
Iteration 184/1000 | Loss: 0.00001393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.3927699001214933e-05, 1.3927699001214933e-05, 1.3927699001214933e-05, 1.3927699001214933e-05, 1.3927699001214933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3927699001214933e-05

Optimization complete. Final v2v error: 3.152390241622925 mm

Highest mean error: 3.563382148742676 mm for frame 97

Lowest mean error: 3.0255441665649414 mm for frame 155

Saving results

Total time: 42.7166907787323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551699
Iteration 2/25 | Loss: 0.00132552
Iteration 3/25 | Loss: 0.00125156
Iteration 4/25 | Loss: 0.00124122
Iteration 5/25 | Loss: 0.00123798
Iteration 6/25 | Loss: 0.00123787
Iteration 7/25 | Loss: 0.00123787
Iteration 8/25 | Loss: 0.00123787
Iteration 9/25 | Loss: 0.00123787
Iteration 10/25 | Loss: 0.00123787
Iteration 11/25 | Loss: 0.00123787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012378671672195196, 0.0012378671672195196, 0.0012378671672195196, 0.0012378671672195196, 0.0012378671672195196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012378671672195196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.00187302
Iteration 2/25 | Loss: 0.00073216
Iteration 3/25 | Loss: 0.00073216
Iteration 4/25 | Loss: 0.00073216
Iteration 5/25 | Loss: 0.00073216
Iteration 6/25 | Loss: 0.00073216
Iteration 7/25 | Loss: 0.00073216
Iteration 8/25 | Loss: 0.00073216
Iteration 9/25 | Loss: 0.00073216
Iteration 10/25 | Loss: 0.00073216
Iteration 11/25 | Loss: 0.00073216
Iteration 12/25 | Loss: 0.00073216
Iteration 13/25 | Loss: 0.00073216
Iteration 14/25 | Loss: 0.00073216
Iteration 15/25 | Loss: 0.00073216
Iteration 16/25 | Loss: 0.00073216
Iteration 17/25 | Loss: 0.00073216
Iteration 18/25 | Loss: 0.00073216
Iteration 19/25 | Loss: 0.00073216
Iteration 20/25 | Loss: 0.00073216
Iteration 21/25 | Loss: 0.00073216
Iteration 22/25 | Loss: 0.00073216
Iteration 23/25 | Loss: 0.00073216
Iteration 24/25 | Loss: 0.00073216
Iteration 25/25 | Loss: 0.00073216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073216
Iteration 2/1000 | Loss: 0.00002982
Iteration 3/1000 | Loss: 0.00001970
Iteration 4/1000 | Loss: 0.00001794
Iteration 5/1000 | Loss: 0.00001695
Iteration 6/1000 | Loss: 0.00001623
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001539
Iteration 9/1000 | Loss: 0.00001509
Iteration 10/1000 | Loss: 0.00001484
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001465
Iteration 13/1000 | Loss: 0.00001463
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001438
Iteration 17/1000 | Loss: 0.00001435
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001426
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001424
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001423
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001403
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001396
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001395
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001383
Iteration 91/1000 | Loss: 0.00001383
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Iteration 114/1000 | Loss: 0.00001379
Iteration 115/1000 | Loss: 0.00001378
Iteration 116/1000 | Loss: 0.00001378
Iteration 117/1000 | Loss: 0.00001378
Iteration 118/1000 | Loss: 0.00001378
Iteration 119/1000 | Loss: 0.00001378
Iteration 120/1000 | Loss: 0.00001378
Iteration 121/1000 | Loss: 0.00001378
Iteration 122/1000 | Loss: 0.00001378
Iteration 123/1000 | Loss: 0.00001378
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001378
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001377
Iteration 132/1000 | Loss: 0.00001377
Iteration 133/1000 | Loss: 0.00001377
Iteration 134/1000 | Loss: 0.00001377
Iteration 135/1000 | Loss: 0.00001377
Iteration 136/1000 | Loss: 0.00001377
Iteration 137/1000 | Loss: 0.00001377
Iteration 138/1000 | Loss: 0.00001377
Iteration 139/1000 | Loss: 0.00001377
Iteration 140/1000 | Loss: 0.00001377
Iteration 141/1000 | Loss: 0.00001377
Iteration 142/1000 | Loss: 0.00001377
Iteration 143/1000 | Loss: 0.00001377
Iteration 144/1000 | Loss: 0.00001377
Iteration 145/1000 | Loss: 0.00001377
Iteration 146/1000 | Loss: 0.00001377
Iteration 147/1000 | Loss: 0.00001377
Iteration 148/1000 | Loss: 0.00001377
Iteration 149/1000 | Loss: 0.00001377
Iteration 150/1000 | Loss: 0.00001377
Iteration 151/1000 | Loss: 0.00001377
Iteration 152/1000 | Loss: 0.00001377
Iteration 153/1000 | Loss: 0.00001377
Iteration 154/1000 | Loss: 0.00001377
Iteration 155/1000 | Loss: 0.00001377
Iteration 156/1000 | Loss: 0.00001377
Iteration 157/1000 | Loss: 0.00001377
Iteration 158/1000 | Loss: 0.00001377
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.376730142510496e-05, 1.376730142510496e-05, 1.376730142510496e-05, 1.376730142510496e-05, 1.376730142510496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.376730142510496e-05

Optimization complete. Final v2v error: 3.1670591831207275 mm

Highest mean error: 3.879629611968994 mm for frame 65

Lowest mean error: 2.8596208095550537 mm for frame 129

Saving results

Total time: 40.76743936538696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380479
Iteration 2/25 | Loss: 0.00130659
Iteration 3/25 | Loss: 0.00122334
Iteration 4/25 | Loss: 0.00121267
Iteration 5/25 | Loss: 0.00120966
Iteration 6/25 | Loss: 0.00120947
Iteration 7/25 | Loss: 0.00120947
Iteration 8/25 | Loss: 0.00120947
Iteration 9/25 | Loss: 0.00120947
Iteration 10/25 | Loss: 0.00120947
Iteration 11/25 | Loss: 0.00120947
Iteration 12/25 | Loss: 0.00120947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012094724224880338, 0.0012094724224880338, 0.0012094724224880338, 0.0012094724224880338, 0.0012094724224880338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012094724224880338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50218892
Iteration 2/25 | Loss: 0.00079108
Iteration 3/25 | Loss: 0.00079108
Iteration 4/25 | Loss: 0.00079108
Iteration 5/25 | Loss: 0.00079108
Iteration 6/25 | Loss: 0.00079108
Iteration 7/25 | Loss: 0.00079108
Iteration 8/25 | Loss: 0.00079108
Iteration 9/25 | Loss: 0.00079108
Iteration 10/25 | Loss: 0.00079108
Iteration 11/25 | Loss: 0.00079108
Iteration 12/25 | Loss: 0.00079108
Iteration 13/25 | Loss: 0.00079108
Iteration 14/25 | Loss: 0.00079108
Iteration 15/25 | Loss: 0.00079108
Iteration 16/25 | Loss: 0.00079108
Iteration 17/25 | Loss: 0.00079108
Iteration 18/25 | Loss: 0.00079108
Iteration 19/25 | Loss: 0.00079108
Iteration 20/25 | Loss: 0.00079108
Iteration 21/25 | Loss: 0.00079108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007910779677331448, 0.0007910779677331448, 0.0007910779677331448, 0.0007910779677331448, 0.0007910779677331448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007910779677331448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079108
Iteration 2/1000 | Loss: 0.00002952
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001470
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001207
Iteration 8/1000 | Loss: 0.00001182
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001143
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001139
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001133
Iteration 22/1000 | Loss: 0.00001132
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001131
Iteration 26/1000 | Loss: 0.00001131
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001126
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001118
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001116
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001100
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001098
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001087
Iteration 107/1000 | Loss: 0.00001087
Iteration 108/1000 | Loss: 0.00001087
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001086
Iteration 111/1000 | Loss: 0.00001086
Iteration 112/1000 | Loss: 0.00001086
Iteration 113/1000 | Loss: 0.00001085
Iteration 114/1000 | Loss: 0.00001085
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001084
Iteration 118/1000 | Loss: 0.00001084
Iteration 119/1000 | Loss: 0.00001084
Iteration 120/1000 | Loss: 0.00001084
Iteration 121/1000 | Loss: 0.00001083
Iteration 122/1000 | Loss: 0.00001083
Iteration 123/1000 | Loss: 0.00001083
Iteration 124/1000 | Loss: 0.00001083
Iteration 125/1000 | Loss: 0.00001083
Iteration 126/1000 | Loss: 0.00001083
Iteration 127/1000 | Loss: 0.00001082
Iteration 128/1000 | Loss: 0.00001082
Iteration 129/1000 | Loss: 0.00001082
Iteration 130/1000 | Loss: 0.00001082
Iteration 131/1000 | Loss: 0.00001081
Iteration 132/1000 | Loss: 0.00001080
Iteration 133/1000 | Loss: 0.00001079
Iteration 134/1000 | Loss: 0.00001079
Iteration 135/1000 | Loss: 0.00001079
Iteration 136/1000 | Loss: 0.00001079
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001078
Iteration 139/1000 | Loss: 0.00001078
Iteration 140/1000 | Loss: 0.00001078
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001076
Iteration 144/1000 | Loss: 0.00001075
Iteration 145/1000 | Loss: 0.00001075
Iteration 146/1000 | Loss: 0.00001075
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001075
Iteration 149/1000 | Loss: 0.00001075
Iteration 150/1000 | Loss: 0.00001075
Iteration 151/1000 | Loss: 0.00001075
Iteration 152/1000 | Loss: 0.00001074
Iteration 153/1000 | Loss: 0.00001074
Iteration 154/1000 | Loss: 0.00001074
Iteration 155/1000 | Loss: 0.00001074
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001073
Iteration 159/1000 | Loss: 0.00001073
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001071
Iteration 168/1000 | Loss: 0.00001071
Iteration 169/1000 | Loss: 0.00001071
Iteration 170/1000 | Loss: 0.00001071
Iteration 171/1000 | Loss: 0.00001071
Iteration 172/1000 | Loss: 0.00001071
Iteration 173/1000 | Loss: 0.00001071
Iteration 174/1000 | Loss: 0.00001071
Iteration 175/1000 | Loss: 0.00001071
Iteration 176/1000 | Loss: 0.00001071
Iteration 177/1000 | Loss: 0.00001071
Iteration 178/1000 | Loss: 0.00001071
Iteration 179/1000 | Loss: 0.00001071
Iteration 180/1000 | Loss: 0.00001071
Iteration 181/1000 | Loss: 0.00001071
Iteration 182/1000 | Loss: 0.00001071
Iteration 183/1000 | Loss: 0.00001071
Iteration 184/1000 | Loss: 0.00001071
Iteration 185/1000 | Loss: 0.00001071
Iteration 186/1000 | Loss: 0.00001071
Iteration 187/1000 | Loss: 0.00001071
Iteration 188/1000 | Loss: 0.00001071
Iteration 189/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.0713544725149404e-05, 1.0713544725149404e-05, 1.0713544725149404e-05, 1.0713544725149404e-05, 1.0713544725149404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0713544725149404e-05

Optimization complete. Final v2v error: 2.8054487705230713 mm

Highest mean error: 3.3023743629455566 mm for frame 100

Lowest mean error: 2.6880552768707275 mm for frame 45

Saving results

Total time: 39.720698595047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453498
Iteration 2/25 | Loss: 0.00147516
Iteration 3/25 | Loss: 0.00130617
Iteration 4/25 | Loss: 0.00126907
Iteration 5/25 | Loss: 0.00126242
Iteration 6/25 | Loss: 0.00127023
Iteration 7/25 | Loss: 0.00126010
Iteration 8/25 | Loss: 0.00125447
Iteration 9/25 | Loss: 0.00125350
Iteration 10/25 | Loss: 0.00125332
Iteration 11/25 | Loss: 0.00125330
Iteration 12/25 | Loss: 0.00125330
Iteration 13/25 | Loss: 0.00125330
Iteration 14/25 | Loss: 0.00125330
Iteration 15/25 | Loss: 0.00125330
Iteration 16/25 | Loss: 0.00125330
Iteration 17/25 | Loss: 0.00125330
Iteration 18/25 | Loss: 0.00125330
Iteration 19/25 | Loss: 0.00125329
Iteration 20/25 | Loss: 0.00125329
Iteration 21/25 | Loss: 0.00125329
Iteration 22/25 | Loss: 0.00125329
Iteration 23/25 | Loss: 0.00125329
Iteration 24/25 | Loss: 0.00125329
Iteration 25/25 | Loss: 0.00125329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42955816
Iteration 2/25 | Loss: 0.00081872
Iteration 3/25 | Loss: 0.00081871
Iteration 4/25 | Loss: 0.00081871
Iteration 5/25 | Loss: 0.00081871
Iteration 6/25 | Loss: 0.00081871
Iteration 7/25 | Loss: 0.00081870
Iteration 8/25 | Loss: 0.00081870
Iteration 9/25 | Loss: 0.00081870
Iteration 10/25 | Loss: 0.00081870
Iteration 11/25 | Loss: 0.00081870
Iteration 12/25 | Loss: 0.00081870
Iteration 13/25 | Loss: 0.00081870
Iteration 14/25 | Loss: 0.00081870
Iteration 15/25 | Loss: 0.00081870
Iteration 16/25 | Loss: 0.00081870
Iteration 17/25 | Loss: 0.00081870
Iteration 18/25 | Loss: 0.00081870
Iteration 19/25 | Loss: 0.00081870
Iteration 20/25 | Loss: 0.00081870
Iteration 21/25 | Loss: 0.00081870
Iteration 22/25 | Loss: 0.00081870
Iteration 23/25 | Loss: 0.00081870
Iteration 24/25 | Loss: 0.00081870
Iteration 25/25 | Loss: 0.00081870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00081870355643332, 0.00081870355643332, 0.00081870355643332, 0.00081870355643332, 0.00081870355643332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00081870355643332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081870
Iteration 2/1000 | Loss: 0.00004533
Iteration 3/1000 | Loss: 0.00003014
Iteration 4/1000 | Loss: 0.00002443
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001930
Iteration 9/1000 | Loss: 0.00001875
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001739
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001724
Iteration 17/1000 | Loss: 0.00001720
Iteration 18/1000 | Loss: 0.00001717
Iteration 19/1000 | Loss: 0.00001716
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001715
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001714
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001698
Iteration 53/1000 | Loss: 0.00001698
Iteration 54/1000 | Loss: 0.00001697
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001695
Iteration 60/1000 | Loss: 0.00001695
Iteration 61/1000 | Loss: 0.00001695
Iteration 62/1000 | Loss: 0.00001694
Iteration 63/1000 | Loss: 0.00001694
Iteration 64/1000 | Loss: 0.00001694
Iteration 65/1000 | Loss: 0.00001694
Iteration 66/1000 | Loss: 0.00001693
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001692
Iteration 69/1000 | Loss: 0.00001692
Iteration 70/1000 | Loss: 0.00001692
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001690
Iteration 75/1000 | Loss: 0.00001690
Iteration 76/1000 | Loss: 0.00001690
Iteration 77/1000 | Loss: 0.00001689
Iteration 78/1000 | Loss: 0.00001689
Iteration 79/1000 | Loss: 0.00001688
Iteration 80/1000 | Loss: 0.00001688
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001687
Iteration 84/1000 | Loss: 0.00001687
Iteration 85/1000 | Loss: 0.00001687
Iteration 86/1000 | Loss: 0.00001687
Iteration 87/1000 | Loss: 0.00001687
Iteration 88/1000 | Loss: 0.00001687
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001686
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00001686
Iteration 96/1000 | Loss: 0.00001686
Iteration 97/1000 | Loss: 0.00001686
Iteration 98/1000 | Loss: 0.00001686
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001685
Iteration 101/1000 | Loss: 0.00001685
Iteration 102/1000 | Loss: 0.00001685
Iteration 103/1000 | Loss: 0.00001685
Iteration 104/1000 | Loss: 0.00001685
Iteration 105/1000 | Loss: 0.00001685
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001684
Iteration 108/1000 | Loss: 0.00001684
Iteration 109/1000 | Loss: 0.00001684
Iteration 110/1000 | Loss: 0.00001684
Iteration 111/1000 | Loss: 0.00001684
Iteration 112/1000 | Loss: 0.00001684
Iteration 113/1000 | Loss: 0.00001684
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001682
Iteration 120/1000 | Loss: 0.00001682
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001681
Iteration 124/1000 | Loss: 0.00001681
Iteration 125/1000 | Loss: 0.00001681
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001679
Iteration 132/1000 | Loss: 0.00001679
Iteration 133/1000 | Loss: 0.00001679
Iteration 134/1000 | Loss: 0.00001679
Iteration 135/1000 | Loss: 0.00001679
Iteration 136/1000 | Loss: 0.00001679
Iteration 137/1000 | Loss: 0.00001679
Iteration 138/1000 | Loss: 0.00001678
Iteration 139/1000 | Loss: 0.00001678
Iteration 140/1000 | Loss: 0.00001678
Iteration 141/1000 | Loss: 0.00001678
Iteration 142/1000 | Loss: 0.00001678
Iteration 143/1000 | Loss: 0.00001678
Iteration 144/1000 | Loss: 0.00001677
Iteration 145/1000 | Loss: 0.00001677
Iteration 146/1000 | Loss: 0.00001677
Iteration 147/1000 | Loss: 0.00001677
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001676
Iteration 151/1000 | Loss: 0.00001676
Iteration 152/1000 | Loss: 0.00001676
Iteration 153/1000 | Loss: 0.00001676
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001676
Iteration 159/1000 | Loss: 0.00001676
Iteration 160/1000 | Loss: 0.00001676
Iteration 161/1000 | Loss: 0.00001675
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001675
Iteration 164/1000 | Loss: 0.00001675
Iteration 165/1000 | Loss: 0.00001675
Iteration 166/1000 | Loss: 0.00001675
Iteration 167/1000 | Loss: 0.00001675
Iteration 168/1000 | Loss: 0.00001675
Iteration 169/1000 | Loss: 0.00001675
Iteration 170/1000 | Loss: 0.00001675
Iteration 171/1000 | Loss: 0.00001675
Iteration 172/1000 | Loss: 0.00001674
Iteration 173/1000 | Loss: 0.00001674
Iteration 174/1000 | Loss: 0.00001674
Iteration 175/1000 | Loss: 0.00001674
Iteration 176/1000 | Loss: 0.00001674
Iteration 177/1000 | Loss: 0.00001674
Iteration 178/1000 | Loss: 0.00001674
Iteration 179/1000 | Loss: 0.00001674
Iteration 180/1000 | Loss: 0.00001674
Iteration 181/1000 | Loss: 0.00001674
Iteration 182/1000 | Loss: 0.00001674
Iteration 183/1000 | Loss: 0.00001674
Iteration 184/1000 | Loss: 0.00001674
Iteration 185/1000 | Loss: 0.00001674
Iteration 186/1000 | Loss: 0.00001674
Iteration 187/1000 | Loss: 0.00001674
Iteration 188/1000 | Loss: 0.00001674
Iteration 189/1000 | Loss: 0.00001674
Iteration 190/1000 | Loss: 0.00001673
Iteration 191/1000 | Loss: 0.00001673
Iteration 192/1000 | Loss: 0.00001673
Iteration 193/1000 | Loss: 0.00001673
Iteration 194/1000 | Loss: 0.00001673
Iteration 195/1000 | Loss: 0.00001673
Iteration 196/1000 | Loss: 0.00001673
Iteration 197/1000 | Loss: 0.00001673
Iteration 198/1000 | Loss: 0.00001673
Iteration 199/1000 | Loss: 0.00001673
Iteration 200/1000 | Loss: 0.00001673
Iteration 201/1000 | Loss: 0.00001673
Iteration 202/1000 | Loss: 0.00001673
Iteration 203/1000 | Loss: 0.00001673
Iteration 204/1000 | Loss: 0.00001672
Iteration 205/1000 | Loss: 0.00001672
Iteration 206/1000 | Loss: 0.00001672
Iteration 207/1000 | Loss: 0.00001672
Iteration 208/1000 | Loss: 0.00001672
Iteration 209/1000 | Loss: 0.00001672
Iteration 210/1000 | Loss: 0.00001672
Iteration 211/1000 | Loss: 0.00001672
Iteration 212/1000 | Loss: 0.00001672
Iteration 213/1000 | Loss: 0.00001672
Iteration 214/1000 | Loss: 0.00001672
Iteration 215/1000 | Loss: 0.00001672
Iteration 216/1000 | Loss: 0.00001672
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001672
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001672
Iteration 221/1000 | Loss: 0.00001672
Iteration 222/1000 | Loss: 0.00001671
Iteration 223/1000 | Loss: 0.00001671
Iteration 224/1000 | Loss: 0.00001671
Iteration 225/1000 | Loss: 0.00001671
Iteration 226/1000 | Loss: 0.00001671
Iteration 227/1000 | Loss: 0.00001671
Iteration 228/1000 | Loss: 0.00001671
Iteration 229/1000 | Loss: 0.00001671
Iteration 230/1000 | Loss: 0.00001671
Iteration 231/1000 | Loss: 0.00001671
Iteration 232/1000 | Loss: 0.00001671
Iteration 233/1000 | Loss: 0.00001671
Iteration 234/1000 | Loss: 0.00001671
Iteration 235/1000 | Loss: 0.00001670
Iteration 236/1000 | Loss: 0.00001670
Iteration 237/1000 | Loss: 0.00001670
Iteration 238/1000 | Loss: 0.00001670
Iteration 239/1000 | Loss: 0.00001670
Iteration 240/1000 | Loss: 0.00001670
Iteration 241/1000 | Loss: 0.00001670
Iteration 242/1000 | Loss: 0.00001670
Iteration 243/1000 | Loss: 0.00001670
Iteration 244/1000 | Loss: 0.00001670
Iteration 245/1000 | Loss: 0.00001670
Iteration 246/1000 | Loss: 0.00001670
Iteration 247/1000 | Loss: 0.00001670
Iteration 248/1000 | Loss: 0.00001670
Iteration 249/1000 | Loss: 0.00001670
Iteration 250/1000 | Loss: 0.00001670
Iteration 251/1000 | Loss: 0.00001670
Iteration 252/1000 | Loss: 0.00001670
Iteration 253/1000 | Loss: 0.00001670
Iteration 254/1000 | Loss: 0.00001670
Iteration 255/1000 | Loss: 0.00001670
Iteration 256/1000 | Loss: 0.00001670
Iteration 257/1000 | Loss: 0.00001670
Iteration 258/1000 | Loss: 0.00001670
Iteration 259/1000 | Loss: 0.00001670
Iteration 260/1000 | Loss: 0.00001670
Iteration 261/1000 | Loss: 0.00001670
Iteration 262/1000 | Loss: 0.00001670
Iteration 263/1000 | Loss: 0.00001670
Iteration 264/1000 | Loss: 0.00001670
Iteration 265/1000 | Loss: 0.00001670
Iteration 266/1000 | Loss: 0.00001670
Iteration 267/1000 | Loss: 0.00001670
Iteration 268/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.6703639630577527e-05, 1.6703639630577527e-05, 1.6703639630577527e-05, 1.6703639630577527e-05, 1.6703639630577527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6703639630577527e-05

Optimization complete. Final v2v error: 3.413221836090088 mm

Highest mean error: 4.501819610595703 mm for frame 110

Lowest mean error: 2.953644037246704 mm for frame 171

Saving results

Total time: 58.44603943824768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422356
Iteration 2/25 | Loss: 0.00136283
Iteration 3/25 | Loss: 0.00127719
Iteration 4/25 | Loss: 0.00126182
Iteration 5/25 | Loss: 0.00125645
Iteration 6/25 | Loss: 0.00125623
Iteration 7/25 | Loss: 0.00125623
Iteration 8/25 | Loss: 0.00125623
Iteration 9/25 | Loss: 0.00125623
Iteration 10/25 | Loss: 0.00125623
Iteration 11/25 | Loss: 0.00125623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012562302872538567, 0.0012562302872538567, 0.0012562302872538567, 0.0012562302872538567, 0.0012562302872538567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012562302872538567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46975732
Iteration 2/25 | Loss: 0.00080690
Iteration 3/25 | Loss: 0.00080689
Iteration 4/25 | Loss: 0.00080689
Iteration 5/25 | Loss: 0.00080689
Iteration 6/25 | Loss: 0.00080689
Iteration 7/25 | Loss: 0.00080689
Iteration 8/25 | Loss: 0.00080689
Iteration 9/25 | Loss: 0.00080689
Iteration 10/25 | Loss: 0.00080689
Iteration 11/25 | Loss: 0.00080689
Iteration 12/25 | Loss: 0.00080689
Iteration 13/25 | Loss: 0.00080689
Iteration 14/25 | Loss: 0.00080689
Iteration 15/25 | Loss: 0.00080689
Iteration 16/25 | Loss: 0.00080689
Iteration 17/25 | Loss: 0.00080689
Iteration 18/25 | Loss: 0.00080689
Iteration 19/25 | Loss: 0.00080689
Iteration 20/25 | Loss: 0.00080689
Iteration 21/25 | Loss: 0.00080689
Iteration 22/25 | Loss: 0.00080689
Iteration 23/25 | Loss: 0.00080689
Iteration 24/25 | Loss: 0.00080689
Iteration 25/25 | Loss: 0.00080689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080689
Iteration 2/1000 | Loss: 0.00003279
Iteration 3/1000 | Loss: 0.00002368
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002125
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00002018
Iteration 8/1000 | Loss: 0.00001970
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001912
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001873
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00001854
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001850
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001838
Iteration 24/1000 | Loss: 0.00001836
Iteration 25/1000 | Loss: 0.00001834
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001814
Iteration 34/1000 | Loss: 0.00001814
Iteration 35/1000 | Loss: 0.00001813
Iteration 36/1000 | Loss: 0.00001812
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001811
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001803
Iteration 67/1000 | Loss: 0.00001803
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001802
Iteration 72/1000 | Loss: 0.00001802
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001801
Iteration 75/1000 | Loss: 0.00001801
Iteration 76/1000 | Loss: 0.00001801
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001801
Iteration 79/1000 | Loss: 0.00001801
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001800
Iteration 82/1000 | Loss: 0.00001800
Iteration 83/1000 | Loss: 0.00001800
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001799
Iteration 91/1000 | Loss: 0.00001799
Iteration 92/1000 | Loss: 0.00001799
Iteration 93/1000 | Loss: 0.00001799
Iteration 94/1000 | Loss: 0.00001799
Iteration 95/1000 | Loss: 0.00001799
Iteration 96/1000 | Loss: 0.00001799
Iteration 97/1000 | Loss: 0.00001798
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001797
Iteration 104/1000 | Loss: 0.00001797
Iteration 105/1000 | Loss: 0.00001797
Iteration 106/1000 | Loss: 0.00001797
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001797
Iteration 113/1000 | Loss: 0.00001797
Iteration 114/1000 | Loss: 0.00001797
Iteration 115/1000 | Loss: 0.00001797
Iteration 116/1000 | Loss: 0.00001797
Iteration 117/1000 | Loss: 0.00001797
Iteration 118/1000 | Loss: 0.00001797
Iteration 119/1000 | Loss: 0.00001797
Iteration 120/1000 | Loss: 0.00001797
Iteration 121/1000 | Loss: 0.00001797
Iteration 122/1000 | Loss: 0.00001797
Iteration 123/1000 | Loss: 0.00001797
Iteration 124/1000 | Loss: 0.00001797
Iteration 125/1000 | Loss: 0.00001797
Iteration 126/1000 | Loss: 0.00001797
Iteration 127/1000 | Loss: 0.00001797
Iteration 128/1000 | Loss: 0.00001797
Iteration 129/1000 | Loss: 0.00001797
Iteration 130/1000 | Loss: 0.00001797
Iteration 131/1000 | Loss: 0.00001797
Iteration 132/1000 | Loss: 0.00001797
Iteration 133/1000 | Loss: 0.00001797
Iteration 134/1000 | Loss: 0.00001797
Iteration 135/1000 | Loss: 0.00001797
Iteration 136/1000 | Loss: 0.00001797
Iteration 137/1000 | Loss: 0.00001797
Iteration 138/1000 | Loss: 0.00001797
Iteration 139/1000 | Loss: 0.00001797
Iteration 140/1000 | Loss: 0.00001797
Iteration 141/1000 | Loss: 0.00001797
Iteration 142/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.7969050531974062e-05, 1.7969050531974062e-05, 1.7969050531974062e-05, 1.7969050531974062e-05, 1.7969050531974062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7969050531974062e-05

Optimization complete. Final v2v error: 3.607351779937744 mm

Highest mean error: 4.044663906097412 mm for frame 25

Lowest mean error: 3.3961141109466553 mm for frame 46

Saving results

Total time: 39.41517472267151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049820
Iteration 2/25 | Loss: 0.01049820
Iteration 3/25 | Loss: 0.01049820
Iteration 4/25 | Loss: 0.01049820
Iteration 5/25 | Loss: 0.01049820
Iteration 6/25 | Loss: 0.01049820
Iteration 7/25 | Loss: 0.01049819
Iteration 8/25 | Loss: 0.01049819
Iteration 9/25 | Loss: 0.01049819
Iteration 10/25 | Loss: 0.01049819
Iteration 11/25 | Loss: 0.01049819
Iteration 12/25 | Loss: 0.01049819
Iteration 13/25 | Loss: 0.01049819
Iteration 14/25 | Loss: 0.01049819
Iteration 15/25 | Loss: 0.01049819
Iteration 16/25 | Loss: 0.01049819
Iteration 17/25 | Loss: 0.01049819
Iteration 18/25 | Loss: 0.01049819
Iteration 19/25 | Loss: 0.01049819
Iteration 20/25 | Loss: 0.01049819
Iteration 21/25 | Loss: 0.01049818
Iteration 22/25 | Loss: 0.01049818
Iteration 23/25 | Loss: 0.01049818
Iteration 24/25 | Loss: 0.01049818
Iteration 25/25 | Loss: 0.01049818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67803454
Iteration 2/25 | Loss: 0.08239752
Iteration 3/25 | Loss: 0.08239752
Iteration 4/25 | Loss: 0.08239751
Iteration 5/25 | Loss: 0.08239750
Iteration 6/25 | Loss: 0.08239749
Iteration 7/25 | Loss: 0.08239749
Iteration 8/25 | Loss: 0.08239749
Iteration 9/25 | Loss: 0.08239749
Iteration 10/25 | Loss: 0.08239749
Iteration 11/25 | Loss: 0.08239749
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08239749073982239, 0.08239749073982239, 0.08239749073982239, 0.08239749073982239, 0.08239749073982239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08239749073982239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08239749
Iteration 2/1000 | Loss: 0.00045766
Iteration 3/1000 | Loss: 0.00014152
Iteration 4/1000 | Loss: 0.00006250
Iteration 5/1000 | Loss: 0.00003649
Iteration 6/1000 | Loss: 0.00002904
Iteration 7/1000 | Loss: 0.00002562
Iteration 8/1000 | Loss: 0.00002288
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00001980
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001796
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001554
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001464
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001380
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001228
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001222
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001220
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001219
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.2128273738198914e-05, 1.2128273738198914e-05, 1.2128273738198914e-05, 1.2128273738198914e-05, 1.2128273738198914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2128273738198914e-05

Optimization complete. Final v2v error: 2.973714590072632 mm

Highest mean error: 3.1576478481292725 mm for frame 158

Lowest mean error: 2.7411601543426514 mm for frame 188

Saving results

Total time: 52.83945846557617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548298
Iteration 2/25 | Loss: 0.00127187
Iteration 3/25 | Loss: 0.00121158
Iteration 4/25 | Loss: 0.00120365
Iteration 5/25 | Loss: 0.00120138
Iteration 6/25 | Loss: 0.00120133
Iteration 7/25 | Loss: 0.00120133
Iteration 8/25 | Loss: 0.00120133
Iteration 9/25 | Loss: 0.00120133
Iteration 10/25 | Loss: 0.00120133
Iteration 11/25 | Loss: 0.00120133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012013253290206194, 0.0012013253290206194, 0.0012013253290206194, 0.0012013253290206194, 0.0012013253290206194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012013253290206194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26308203
Iteration 2/25 | Loss: 0.00076830
Iteration 3/25 | Loss: 0.00076830
Iteration 4/25 | Loss: 0.00076830
Iteration 5/25 | Loss: 0.00076830
Iteration 6/25 | Loss: 0.00076830
Iteration 7/25 | Loss: 0.00076830
Iteration 8/25 | Loss: 0.00076830
Iteration 9/25 | Loss: 0.00076830
Iteration 10/25 | Loss: 0.00076830
Iteration 11/25 | Loss: 0.00076830
Iteration 12/25 | Loss: 0.00076830
Iteration 13/25 | Loss: 0.00076830
Iteration 14/25 | Loss: 0.00076830
Iteration 15/25 | Loss: 0.00076830
Iteration 16/25 | Loss: 0.00076830
Iteration 17/25 | Loss: 0.00076830
Iteration 18/25 | Loss: 0.00076830
Iteration 19/25 | Loss: 0.00076830
Iteration 20/25 | Loss: 0.00076830
Iteration 21/25 | Loss: 0.00076829
Iteration 22/25 | Loss: 0.00076829
Iteration 23/25 | Loss: 0.00076829
Iteration 24/25 | Loss: 0.00076829
Iteration 25/25 | Loss: 0.00076829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076829
Iteration 2/1000 | Loss: 0.00002329
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001440
Iteration 5/1000 | Loss: 0.00001358
Iteration 6/1000 | Loss: 0.00001301
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001231
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001191
Iteration 16/1000 | Loss: 0.00001190
Iteration 17/1000 | Loss: 0.00001184
Iteration 18/1000 | Loss: 0.00001184
Iteration 19/1000 | Loss: 0.00001184
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001156
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001155
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001152
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001150
Iteration 50/1000 | Loss: 0.00001149
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001147
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001146
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001144
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001143
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001142
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001138
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001134
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001134
Iteration 91/1000 | Loss: 0.00001134
Iteration 92/1000 | Loss: 0.00001133
Iteration 93/1000 | Loss: 0.00001133
Iteration 94/1000 | Loss: 0.00001133
Iteration 95/1000 | Loss: 0.00001131
Iteration 96/1000 | Loss: 0.00001131
Iteration 97/1000 | Loss: 0.00001131
Iteration 98/1000 | Loss: 0.00001131
Iteration 99/1000 | Loss: 0.00001130
Iteration 100/1000 | Loss: 0.00001130
Iteration 101/1000 | Loss: 0.00001130
Iteration 102/1000 | Loss: 0.00001130
Iteration 103/1000 | Loss: 0.00001129
Iteration 104/1000 | Loss: 0.00001129
Iteration 105/1000 | Loss: 0.00001129
Iteration 106/1000 | Loss: 0.00001128
Iteration 107/1000 | Loss: 0.00001128
Iteration 108/1000 | Loss: 0.00001127
Iteration 109/1000 | Loss: 0.00001127
Iteration 110/1000 | Loss: 0.00001127
Iteration 111/1000 | Loss: 0.00001127
Iteration 112/1000 | Loss: 0.00001127
Iteration 113/1000 | Loss: 0.00001127
Iteration 114/1000 | Loss: 0.00001126
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001126
Iteration 117/1000 | Loss: 0.00001126
Iteration 118/1000 | Loss: 0.00001126
Iteration 119/1000 | Loss: 0.00001126
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001124
Iteration 129/1000 | Loss: 0.00001124
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001124
Iteration 136/1000 | Loss: 0.00001124
Iteration 137/1000 | Loss: 0.00001124
Iteration 138/1000 | Loss: 0.00001124
Iteration 139/1000 | Loss: 0.00001123
Iteration 140/1000 | Loss: 0.00001123
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001123
Iteration 144/1000 | Loss: 0.00001123
Iteration 145/1000 | Loss: 0.00001123
Iteration 146/1000 | Loss: 0.00001123
Iteration 147/1000 | Loss: 0.00001123
Iteration 148/1000 | Loss: 0.00001123
Iteration 149/1000 | Loss: 0.00001123
Iteration 150/1000 | Loss: 0.00001123
Iteration 151/1000 | Loss: 0.00001123
Iteration 152/1000 | Loss: 0.00001123
Iteration 153/1000 | Loss: 0.00001123
Iteration 154/1000 | Loss: 0.00001123
Iteration 155/1000 | Loss: 0.00001122
Iteration 156/1000 | Loss: 0.00001122
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001122
Iteration 159/1000 | Loss: 0.00001122
Iteration 160/1000 | Loss: 0.00001122
Iteration 161/1000 | Loss: 0.00001122
Iteration 162/1000 | Loss: 0.00001122
Iteration 163/1000 | Loss: 0.00001122
Iteration 164/1000 | Loss: 0.00001122
Iteration 165/1000 | Loss: 0.00001122
Iteration 166/1000 | Loss: 0.00001122
Iteration 167/1000 | Loss: 0.00001121
Iteration 168/1000 | Loss: 0.00001121
Iteration 169/1000 | Loss: 0.00001121
Iteration 170/1000 | Loss: 0.00001121
Iteration 171/1000 | Loss: 0.00001121
Iteration 172/1000 | Loss: 0.00001121
Iteration 173/1000 | Loss: 0.00001121
Iteration 174/1000 | Loss: 0.00001121
Iteration 175/1000 | Loss: 0.00001121
Iteration 176/1000 | Loss: 0.00001121
Iteration 177/1000 | Loss: 0.00001121
Iteration 178/1000 | Loss: 0.00001121
Iteration 179/1000 | Loss: 0.00001121
Iteration 180/1000 | Loss: 0.00001121
Iteration 181/1000 | Loss: 0.00001120
Iteration 182/1000 | Loss: 0.00001120
Iteration 183/1000 | Loss: 0.00001120
Iteration 184/1000 | Loss: 0.00001120
Iteration 185/1000 | Loss: 0.00001120
Iteration 186/1000 | Loss: 0.00001120
Iteration 187/1000 | Loss: 0.00001120
Iteration 188/1000 | Loss: 0.00001120
Iteration 189/1000 | Loss: 0.00001119
Iteration 190/1000 | Loss: 0.00001119
Iteration 191/1000 | Loss: 0.00001119
Iteration 192/1000 | Loss: 0.00001119
Iteration 193/1000 | Loss: 0.00001119
Iteration 194/1000 | Loss: 0.00001119
Iteration 195/1000 | Loss: 0.00001119
Iteration 196/1000 | Loss: 0.00001119
Iteration 197/1000 | Loss: 0.00001119
Iteration 198/1000 | Loss: 0.00001119
Iteration 199/1000 | Loss: 0.00001119
Iteration 200/1000 | Loss: 0.00001119
Iteration 201/1000 | Loss: 0.00001119
Iteration 202/1000 | Loss: 0.00001119
Iteration 203/1000 | Loss: 0.00001118
Iteration 204/1000 | Loss: 0.00001118
Iteration 205/1000 | Loss: 0.00001118
Iteration 206/1000 | Loss: 0.00001118
Iteration 207/1000 | Loss: 0.00001118
Iteration 208/1000 | Loss: 0.00001118
Iteration 209/1000 | Loss: 0.00001118
Iteration 210/1000 | Loss: 0.00001118
Iteration 211/1000 | Loss: 0.00001118
Iteration 212/1000 | Loss: 0.00001118
Iteration 213/1000 | Loss: 0.00001118
Iteration 214/1000 | Loss: 0.00001118
Iteration 215/1000 | Loss: 0.00001118
Iteration 216/1000 | Loss: 0.00001117
Iteration 217/1000 | Loss: 0.00001117
Iteration 218/1000 | Loss: 0.00001117
Iteration 219/1000 | Loss: 0.00001117
Iteration 220/1000 | Loss: 0.00001117
Iteration 221/1000 | Loss: 0.00001117
Iteration 222/1000 | Loss: 0.00001117
Iteration 223/1000 | Loss: 0.00001117
Iteration 224/1000 | Loss: 0.00001117
Iteration 225/1000 | Loss: 0.00001117
Iteration 226/1000 | Loss: 0.00001117
Iteration 227/1000 | Loss: 0.00001117
Iteration 228/1000 | Loss: 0.00001117
Iteration 229/1000 | Loss: 0.00001117
Iteration 230/1000 | Loss: 0.00001117
Iteration 231/1000 | Loss: 0.00001117
Iteration 232/1000 | Loss: 0.00001117
Iteration 233/1000 | Loss: 0.00001117
Iteration 234/1000 | Loss: 0.00001117
Iteration 235/1000 | Loss: 0.00001116
Iteration 236/1000 | Loss: 0.00001116
Iteration 237/1000 | Loss: 0.00001116
Iteration 238/1000 | Loss: 0.00001116
Iteration 239/1000 | Loss: 0.00001116
Iteration 240/1000 | Loss: 0.00001116
Iteration 241/1000 | Loss: 0.00001116
Iteration 242/1000 | Loss: 0.00001116
Iteration 243/1000 | Loss: 0.00001116
Iteration 244/1000 | Loss: 0.00001116
Iteration 245/1000 | Loss: 0.00001116
Iteration 246/1000 | Loss: 0.00001116
Iteration 247/1000 | Loss: 0.00001116
Iteration 248/1000 | Loss: 0.00001116
Iteration 249/1000 | Loss: 0.00001116
Iteration 250/1000 | Loss: 0.00001116
Iteration 251/1000 | Loss: 0.00001116
Iteration 252/1000 | Loss: 0.00001116
Iteration 253/1000 | Loss: 0.00001116
Iteration 254/1000 | Loss: 0.00001116
Iteration 255/1000 | Loss: 0.00001116
Iteration 256/1000 | Loss: 0.00001116
Iteration 257/1000 | Loss: 0.00001116
Iteration 258/1000 | Loss: 0.00001116
Iteration 259/1000 | Loss: 0.00001116
Iteration 260/1000 | Loss: 0.00001116
Iteration 261/1000 | Loss: 0.00001116
Iteration 262/1000 | Loss: 0.00001116
Iteration 263/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.1159773748659063e-05, 1.1159773748659063e-05, 1.1159773748659063e-05, 1.1159773748659063e-05, 1.1159773748659063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1159773748659063e-05

Optimization complete. Final v2v error: 2.8568484783172607 mm

Highest mean error: 3.08284330368042 mm for frame 76

Lowest mean error: 2.75524640083313 mm for frame 137

Saving results

Total time: 42.53121995925903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935929
Iteration 2/25 | Loss: 0.00205332
Iteration 3/25 | Loss: 0.00167991
Iteration 4/25 | Loss: 0.00160488
Iteration 5/25 | Loss: 0.00157639
Iteration 6/25 | Loss: 0.00157857
Iteration 7/25 | Loss: 0.00156709
Iteration 8/25 | Loss: 0.00155825
Iteration 9/25 | Loss: 0.00153944
Iteration 10/25 | Loss: 0.00154332
Iteration 11/25 | Loss: 0.00153686
Iteration 12/25 | Loss: 0.00153288
Iteration 13/25 | Loss: 0.00153147
Iteration 14/25 | Loss: 0.00152895
Iteration 15/25 | Loss: 0.00152240
Iteration 16/25 | Loss: 0.00152345
Iteration 17/25 | Loss: 0.00151503
Iteration 18/25 | Loss: 0.00151778
Iteration 19/25 | Loss: 0.00152092
Iteration 20/25 | Loss: 0.00150910
Iteration 21/25 | Loss: 0.00150387
Iteration 22/25 | Loss: 0.00150348
Iteration 23/25 | Loss: 0.00150782
Iteration 24/25 | Loss: 0.00150911
Iteration 25/25 | Loss: 0.00149612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71440363
Iteration 2/25 | Loss: 0.00143381
Iteration 3/25 | Loss: 0.00143338
Iteration 4/25 | Loss: 0.00143338
Iteration 5/25 | Loss: 0.00143338
Iteration 6/25 | Loss: 0.00143338
Iteration 7/25 | Loss: 0.00143338
Iteration 8/25 | Loss: 0.00143338
Iteration 9/25 | Loss: 0.00143338
Iteration 10/25 | Loss: 0.00143338
Iteration 11/25 | Loss: 0.00143338
Iteration 12/25 | Loss: 0.00143338
Iteration 13/25 | Loss: 0.00143338
Iteration 14/25 | Loss: 0.00143338
Iteration 15/25 | Loss: 0.00143338
Iteration 16/25 | Loss: 0.00143338
Iteration 17/25 | Loss: 0.00143338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014333789004012942, 0.0014333789004012942, 0.0014333789004012942, 0.0014333789004012942, 0.0014333789004012942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014333789004012942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143338
Iteration 2/1000 | Loss: 0.01162715
Iteration 3/1000 | Loss: 0.00602768
Iteration 4/1000 | Loss: 0.00034774
Iteration 5/1000 | Loss: 0.00545933
Iteration 6/1000 | Loss: 0.00152139
Iteration 7/1000 | Loss: 0.00249320
Iteration 8/1000 | Loss: 0.00026985
Iteration 9/1000 | Loss: 0.00013317
Iteration 10/1000 | Loss: 0.00191534
Iteration 11/1000 | Loss: 0.00423658
Iteration 12/1000 | Loss: 0.00308029
Iteration 13/1000 | Loss: 0.00184942
Iteration 14/1000 | Loss: 0.00157463
Iteration 15/1000 | Loss: 0.00141859
Iteration 16/1000 | Loss: 0.00166519
Iteration 17/1000 | Loss: 0.00310255
Iteration 18/1000 | Loss: 0.00222484
Iteration 19/1000 | Loss: 0.00130129
Iteration 20/1000 | Loss: 0.00011762
Iteration 21/1000 | Loss: 0.00098804
Iteration 22/1000 | Loss: 0.00068727
Iteration 23/1000 | Loss: 0.00018952
Iteration 24/1000 | Loss: 0.00014883
Iteration 25/1000 | Loss: 0.00018522
Iteration 26/1000 | Loss: 0.00307286
Iteration 27/1000 | Loss: 0.00152522
Iteration 28/1000 | Loss: 0.00354242
Iteration 29/1000 | Loss: 0.00520251
Iteration 30/1000 | Loss: 0.00272918
Iteration 31/1000 | Loss: 0.00058711
Iteration 32/1000 | Loss: 0.00011318
Iteration 33/1000 | Loss: 0.00009581
Iteration 34/1000 | Loss: 0.00011348
Iteration 35/1000 | Loss: 0.00008331
Iteration 36/1000 | Loss: 0.00007767
Iteration 37/1000 | Loss: 0.00213247
Iteration 38/1000 | Loss: 0.00008976
Iteration 39/1000 | Loss: 0.00007016
Iteration 40/1000 | Loss: 0.00006642
Iteration 41/1000 | Loss: 0.00057543
Iteration 42/1000 | Loss: 0.00031093
Iteration 43/1000 | Loss: 0.00040583
Iteration 44/1000 | Loss: 0.00077251
Iteration 45/1000 | Loss: 0.00083389
Iteration 46/1000 | Loss: 0.00060733
Iteration 47/1000 | Loss: 0.00090265
Iteration 48/1000 | Loss: 0.00054028
Iteration 49/1000 | Loss: 0.00088498
Iteration 50/1000 | Loss: 0.00051018
Iteration 51/1000 | Loss: 0.00008370
Iteration 52/1000 | Loss: 0.00009683
Iteration 53/1000 | Loss: 0.00025027
Iteration 54/1000 | Loss: 0.00013911
Iteration 55/1000 | Loss: 0.00076403
Iteration 56/1000 | Loss: 0.00063424
Iteration 57/1000 | Loss: 0.00043126
Iteration 58/1000 | Loss: 0.00007286
Iteration 59/1000 | Loss: 0.00046081
Iteration 60/1000 | Loss: 0.00044413
Iteration 61/1000 | Loss: 0.00040420
Iteration 62/1000 | Loss: 0.00044407
Iteration 63/1000 | Loss: 0.00028225
Iteration 64/1000 | Loss: 0.00009740
Iteration 65/1000 | Loss: 0.00005439
Iteration 66/1000 | Loss: 0.00005082
Iteration 67/1000 | Loss: 0.00004927
Iteration 68/1000 | Loss: 0.00004822
Iteration 69/1000 | Loss: 0.00004749
Iteration 70/1000 | Loss: 0.00004699
Iteration 71/1000 | Loss: 0.00004638
Iteration 72/1000 | Loss: 0.00004582
Iteration 73/1000 | Loss: 0.00004538
Iteration 74/1000 | Loss: 0.00004502
Iteration 75/1000 | Loss: 0.00004493
Iteration 76/1000 | Loss: 0.00004478
Iteration 77/1000 | Loss: 0.00004468
Iteration 78/1000 | Loss: 0.00004449
Iteration 79/1000 | Loss: 0.00004444
Iteration 80/1000 | Loss: 0.00004443
Iteration 81/1000 | Loss: 0.00004443
Iteration 82/1000 | Loss: 0.00004441
Iteration 83/1000 | Loss: 0.00004431
Iteration 84/1000 | Loss: 0.00004418
Iteration 85/1000 | Loss: 0.00004414
Iteration 86/1000 | Loss: 0.00004410
Iteration 87/1000 | Loss: 0.00004409
Iteration 88/1000 | Loss: 0.00004409
Iteration 89/1000 | Loss: 0.00004409
Iteration 90/1000 | Loss: 0.00004408
Iteration 91/1000 | Loss: 0.00004408
Iteration 92/1000 | Loss: 0.00004408
Iteration 93/1000 | Loss: 0.00004406
Iteration 94/1000 | Loss: 0.00004405
Iteration 95/1000 | Loss: 0.00004404
Iteration 96/1000 | Loss: 0.00004403
Iteration 97/1000 | Loss: 0.00004403
Iteration 98/1000 | Loss: 0.00004402
Iteration 99/1000 | Loss: 0.00004401
Iteration 100/1000 | Loss: 0.00004401
Iteration 101/1000 | Loss: 0.00004401
Iteration 102/1000 | Loss: 0.00004400
Iteration 103/1000 | Loss: 0.00004400
Iteration 104/1000 | Loss: 0.00004399
Iteration 105/1000 | Loss: 0.00004399
Iteration 106/1000 | Loss: 0.00004399
Iteration 107/1000 | Loss: 0.00004398
Iteration 108/1000 | Loss: 0.00004398
Iteration 109/1000 | Loss: 0.00004397
Iteration 110/1000 | Loss: 0.00004397
Iteration 111/1000 | Loss: 0.00004396
Iteration 112/1000 | Loss: 0.00004396
Iteration 113/1000 | Loss: 0.00004396
Iteration 114/1000 | Loss: 0.00004396
Iteration 115/1000 | Loss: 0.00004396
Iteration 116/1000 | Loss: 0.00004396
Iteration 117/1000 | Loss: 0.00004396
Iteration 118/1000 | Loss: 0.00004395
Iteration 119/1000 | Loss: 0.00004395
Iteration 120/1000 | Loss: 0.00004395
Iteration 121/1000 | Loss: 0.00004395
Iteration 122/1000 | Loss: 0.00004394
Iteration 123/1000 | Loss: 0.00004394
Iteration 124/1000 | Loss: 0.00004394
Iteration 125/1000 | Loss: 0.00004394
Iteration 126/1000 | Loss: 0.00004393
Iteration 127/1000 | Loss: 0.00004392
Iteration 128/1000 | Loss: 0.00004392
Iteration 129/1000 | Loss: 0.00004392
Iteration 130/1000 | Loss: 0.00004392
Iteration 131/1000 | Loss: 0.00004391
Iteration 132/1000 | Loss: 0.00004391
Iteration 133/1000 | Loss: 0.00004390
Iteration 134/1000 | Loss: 0.00004390
Iteration 135/1000 | Loss: 0.00004390
Iteration 136/1000 | Loss: 0.00004389
Iteration 137/1000 | Loss: 0.00004389
Iteration 138/1000 | Loss: 0.00004388
Iteration 139/1000 | Loss: 0.00004388
Iteration 140/1000 | Loss: 0.00004388
Iteration 141/1000 | Loss: 0.00004388
Iteration 142/1000 | Loss: 0.00004387
Iteration 143/1000 | Loss: 0.00004387
Iteration 144/1000 | Loss: 0.00004387
Iteration 145/1000 | Loss: 0.00004386
Iteration 146/1000 | Loss: 0.00004386
Iteration 147/1000 | Loss: 0.00004386
Iteration 148/1000 | Loss: 0.00004386
Iteration 149/1000 | Loss: 0.00004385
Iteration 150/1000 | Loss: 0.00004385
Iteration 151/1000 | Loss: 0.00004385
Iteration 152/1000 | Loss: 0.00004385
Iteration 153/1000 | Loss: 0.00004385
Iteration 154/1000 | Loss: 0.00004384
Iteration 155/1000 | Loss: 0.00004384
Iteration 156/1000 | Loss: 0.00004384
Iteration 157/1000 | Loss: 0.00004384
Iteration 158/1000 | Loss: 0.00004384
Iteration 159/1000 | Loss: 0.00004384
Iteration 160/1000 | Loss: 0.00004384
Iteration 161/1000 | Loss: 0.00004383
Iteration 162/1000 | Loss: 0.00004383
Iteration 163/1000 | Loss: 0.00004383
Iteration 164/1000 | Loss: 0.00004383
Iteration 165/1000 | Loss: 0.00004383
Iteration 166/1000 | Loss: 0.00004383
Iteration 167/1000 | Loss: 0.00004382
Iteration 168/1000 | Loss: 0.00004382
Iteration 169/1000 | Loss: 0.00004382
Iteration 170/1000 | Loss: 0.00004382
Iteration 171/1000 | Loss: 0.00004382
Iteration 172/1000 | Loss: 0.00004382
Iteration 173/1000 | Loss: 0.00004381
Iteration 174/1000 | Loss: 0.00004381
Iteration 175/1000 | Loss: 0.00004381
Iteration 176/1000 | Loss: 0.00004381
Iteration 177/1000 | Loss: 0.00004381
Iteration 178/1000 | Loss: 0.00004380
Iteration 179/1000 | Loss: 0.00004380
Iteration 180/1000 | Loss: 0.00004380
Iteration 181/1000 | Loss: 0.00004380
Iteration 182/1000 | Loss: 0.00004380
Iteration 183/1000 | Loss: 0.00004380
Iteration 184/1000 | Loss: 0.00004380
Iteration 185/1000 | Loss: 0.00004380
Iteration 186/1000 | Loss: 0.00004380
Iteration 187/1000 | Loss: 0.00089163
Iteration 188/1000 | Loss: 0.00041393
Iteration 189/1000 | Loss: 0.00004389
Iteration 190/1000 | Loss: 0.00088549
Iteration 191/1000 | Loss: 0.00006613
Iteration 192/1000 | Loss: 0.00005174
Iteration 193/1000 | Loss: 0.00004689
Iteration 194/1000 | Loss: 0.00004460
Iteration 195/1000 | Loss: 0.00004286
Iteration 196/1000 | Loss: 0.00004222
Iteration 197/1000 | Loss: 0.00004207
Iteration 198/1000 | Loss: 0.00004206
Iteration 199/1000 | Loss: 0.00004204
Iteration 200/1000 | Loss: 0.00004204
Iteration 201/1000 | Loss: 0.00004203
Iteration 202/1000 | Loss: 0.00004203
Iteration 203/1000 | Loss: 0.00004203
Iteration 204/1000 | Loss: 0.00004203
Iteration 205/1000 | Loss: 0.00004203
Iteration 206/1000 | Loss: 0.00004202
Iteration 207/1000 | Loss: 0.00004202
Iteration 208/1000 | Loss: 0.00004202
Iteration 209/1000 | Loss: 0.00004202
Iteration 210/1000 | Loss: 0.00004202
Iteration 211/1000 | Loss: 0.00004202
Iteration 212/1000 | Loss: 0.00004202
Iteration 213/1000 | Loss: 0.00004201
Iteration 214/1000 | Loss: 0.00004201
Iteration 215/1000 | Loss: 0.00004201
Iteration 216/1000 | Loss: 0.00004200
Iteration 217/1000 | Loss: 0.00004200
Iteration 218/1000 | Loss: 0.00004200
Iteration 219/1000 | Loss: 0.00004199
Iteration 220/1000 | Loss: 0.00004199
Iteration 221/1000 | Loss: 0.00004199
Iteration 222/1000 | Loss: 0.00004199
Iteration 223/1000 | Loss: 0.00004199
Iteration 224/1000 | Loss: 0.00004199
Iteration 225/1000 | Loss: 0.00004198
Iteration 226/1000 | Loss: 0.00004198
Iteration 227/1000 | Loss: 0.00004198
Iteration 228/1000 | Loss: 0.00004198
Iteration 229/1000 | Loss: 0.00004198
Iteration 230/1000 | Loss: 0.00004198
Iteration 231/1000 | Loss: 0.00004197
Iteration 232/1000 | Loss: 0.00004197
Iteration 233/1000 | Loss: 0.00004197
Iteration 234/1000 | Loss: 0.00004197
Iteration 235/1000 | Loss: 0.00004196
Iteration 236/1000 | Loss: 0.00004196
Iteration 237/1000 | Loss: 0.00004196
Iteration 238/1000 | Loss: 0.00004196
Iteration 239/1000 | Loss: 0.00004196
Iteration 240/1000 | Loss: 0.00004195
Iteration 241/1000 | Loss: 0.00004195
Iteration 242/1000 | Loss: 0.00004195
Iteration 243/1000 | Loss: 0.00004195
Iteration 244/1000 | Loss: 0.00004194
Iteration 245/1000 | Loss: 0.00004194
Iteration 246/1000 | Loss: 0.00004194
Iteration 247/1000 | Loss: 0.00004194
Iteration 248/1000 | Loss: 0.00004194
Iteration 249/1000 | Loss: 0.00004194
Iteration 250/1000 | Loss: 0.00004193
Iteration 251/1000 | Loss: 0.00004193
Iteration 252/1000 | Loss: 0.00004193
Iteration 253/1000 | Loss: 0.00004193
Iteration 254/1000 | Loss: 0.00004193
Iteration 255/1000 | Loss: 0.00004192
Iteration 256/1000 | Loss: 0.00004192
Iteration 257/1000 | Loss: 0.00004192
Iteration 258/1000 | Loss: 0.00004192
Iteration 259/1000 | Loss: 0.00004192
Iteration 260/1000 | Loss: 0.00004192
Iteration 261/1000 | Loss: 0.00004192
Iteration 262/1000 | Loss: 0.00004191
Iteration 263/1000 | Loss: 0.00004191
Iteration 264/1000 | Loss: 0.00004191
Iteration 265/1000 | Loss: 0.00004191
Iteration 266/1000 | Loss: 0.00004191
Iteration 267/1000 | Loss: 0.00004191
Iteration 268/1000 | Loss: 0.00004191
Iteration 269/1000 | Loss: 0.00004191
Iteration 270/1000 | Loss: 0.00004191
Iteration 271/1000 | Loss: 0.00004191
Iteration 272/1000 | Loss: 0.00004190
Iteration 273/1000 | Loss: 0.00004190
Iteration 274/1000 | Loss: 0.00004190
Iteration 275/1000 | Loss: 0.00004190
Iteration 276/1000 | Loss: 0.00004190
Iteration 277/1000 | Loss: 0.00004189
Iteration 278/1000 | Loss: 0.00004189
Iteration 279/1000 | Loss: 0.00004189
Iteration 280/1000 | Loss: 0.00004189
Iteration 281/1000 | Loss: 0.00004189
Iteration 282/1000 | Loss: 0.00004188
Iteration 283/1000 | Loss: 0.00004188
Iteration 284/1000 | Loss: 0.00004188
Iteration 285/1000 | Loss: 0.00004188
Iteration 286/1000 | Loss: 0.00004188
Iteration 287/1000 | Loss: 0.00004188
Iteration 288/1000 | Loss: 0.00004187
Iteration 289/1000 | Loss: 0.00004187
Iteration 290/1000 | Loss: 0.00004187
Iteration 291/1000 | Loss: 0.00004187
Iteration 292/1000 | Loss: 0.00004187
Iteration 293/1000 | Loss: 0.00004187
Iteration 294/1000 | Loss: 0.00004187
Iteration 295/1000 | Loss: 0.00004187
Iteration 296/1000 | Loss: 0.00004187
Iteration 297/1000 | Loss: 0.00004187
Iteration 298/1000 | Loss: 0.00004187
Iteration 299/1000 | Loss: 0.00004187
Iteration 300/1000 | Loss: 0.00004187
Iteration 301/1000 | Loss: 0.00004187
Iteration 302/1000 | Loss: 0.00004187
Iteration 303/1000 | Loss: 0.00004187
Iteration 304/1000 | Loss: 0.00004186
Iteration 305/1000 | Loss: 0.00004186
Iteration 306/1000 | Loss: 0.00004186
Iteration 307/1000 | Loss: 0.00004186
Iteration 308/1000 | Loss: 0.00004186
Iteration 309/1000 | Loss: 0.00004186
Iteration 310/1000 | Loss: 0.00004186
Iteration 311/1000 | Loss: 0.00004186
Iteration 312/1000 | Loss: 0.00004186
Iteration 313/1000 | Loss: 0.00004186
Iteration 314/1000 | Loss: 0.00004186
Iteration 315/1000 | Loss: 0.00004186
Iteration 316/1000 | Loss: 0.00004186
Iteration 317/1000 | Loss: 0.00004186
Iteration 318/1000 | Loss: 0.00004186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [4.186190199106932e-05, 4.186190199106932e-05, 4.186190199106932e-05, 4.186190199106932e-05, 4.186190199106932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.186190199106932e-05

Optimization complete. Final v2v error: 4.85455322265625 mm

Highest mean error: 13.172942161560059 mm for frame 22

Lowest mean error: 3.421370506286621 mm for frame 150

Saving results

Total time: 218.72618865966797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789225
Iteration 2/25 | Loss: 0.00133237
Iteration 3/25 | Loss: 0.00122157
Iteration 4/25 | Loss: 0.00121523
Iteration 5/25 | Loss: 0.00121375
Iteration 6/25 | Loss: 0.00121375
Iteration 7/25 | Loss: 0.00121375
Iteration 8/25 | Loss: 0.00121375
Iteration 9/25 | Loss: 0.00121375
Iteration 10/25 | Loss: 0.00121375
Iteration 11/25 | Loss: 0.00121375
Iteration 12/25 | Loss: 0.00121375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012137535959482193, 0.0012137535959482193, 0.0012137535959482193, 0.0012137535959482193, 0.0012137535959482193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012137535959482193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43521416
Iteration 2/25 | Loss: 0.00074455
Iteration 3/25 | Loss: 0.00074455
Iteration 4/25 | Loss: 0.00074455
Iteration 5/25 | Loss: 0.00074455
Iteration 6/25 | Loss: 0.00074455
Iteration 7/25 | Loss: 0.00074455
Iteration 8/25 | Loss: 0.00074455
Iteration 9/25 | Loss: 0.00074455
Iteration 10/25 | Loss: 0.00074455
Iteration 11/25 | Loss: 0.00074455
Iteration 12/25 | Loss: 0.00074455
Iteration 13/25 | Loss: 0.00074455
Iteration 14/25 | Loss: 0.00074455
Iteration 15/25 | Loss: 0.00074455
Iteration 16/25 | Loss: 0.00074455
Iteration 17/25 | Loss: 0.00074455
Iteration 18/25 | Loss: 0.00074455
Iteration 19/25 | Loss: 0.00074455
Iteration 20/25 | Loss: 0.00074455
Iteration 21/25 | Loss: 0.00074455
Iteration 22/25 | Loss: 0.00074455
Iteration 23/25 | Loss: 0.00074455
Iteration 24/25 | Loss: 0.00074455
Iteration 25/25 | Loss: 0.00074455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074455
Iteration 2/1000 | Loss: 0.00002335
Iteration 3/1000 | Loss: 0.00001644
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001235
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001183
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001167
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001161
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001157
Iteration 35/1000 | Loss: 0.00001157
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001152
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001150
Iteration 50/1000 | Loss: 0.00001150
Iteration 51/1000 | Loss: 0.00001150
Iteration 52/1000 | Loss: 0.00001150
Iteration 53/1000 | Loss: 0.00001150
Iteration 54/1000 | Loss: 0.00001150
Iteration 55/1000 | Loss: 0.00001150
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001142
Iteration 77/1000 | Loss: 0.00001142
Iteration 78/1000 | Loss: 0.00001142
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001140
Iteration 81/1000 | Loss: 0.00001140
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001126
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001126
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001125
Iteration 171/1000 | Loss: 0.00001125
Iteration 172/1000 | Loss: 0.00001125
Iteration 173/1000 | Loss: 0.00001125
Iteration 174/1000 | Loss: 0.00001125
Iteration 175/1000 | Loss: 0.00001125
Iteration 176/1000 | Loss: 0.00001125
Iteration 177/1000 | Loss: 0.00001125
Iteration 178/1000 | Loss: 0.00001125
Iteration 179/1000 | Loss: 0.00001125
Iteration 180/1000 | Loss: 0.00001125
Iteration 181/1000 | Loss: 0.00001125
Iteration 182/1000 | Loss: 0.00001125
Iteration 183/1000 | Loss: 0.00001125
Iteration 184/1000 | Loss: 0.00001125
Iteration 185/1000 | Loss: 0.00001125
Iteration 186/1000 | Loss: 0.00001124
Iteration 187/1000 | Loss: 0.00001124
Iteration 188/1000 | Loss: 0.00001124
Iteration 189/1000 | Loss: 0.00001124
Iteration 190/1000 | Loss: 0.00001124
Iteration 191/1000 | Loss: 0.00001124
Iteration 192/1000 | Loss: 0.00001124
Iteration 193/1000 | Loss: 0.00001124
Iteration 194/1000 | Loss: 0.00001124
Iteration 195/1000 | Loss: 0.00001124
Iteration 196/1000 | Loss: 0.00001124
Iteration 197/1000 | Loss: 0.00001123
Iteration 198/1000 | Loss: 0.00001123
Iteration 199/1000 | Loss: 0.00001123
Iteration 200/1000 | Loss: 0.00001123
Iteration 201/1000 | Loss: 0.00001123
Iteration 202/1000 | Loss: 0.00001123
Iteration 203/1000 | Loss: 0.00001123
Iteration 204/1000 | Loss: 0.00001123
Iteration 205/1000 | Loss: 0.00001123
Iteration 206/1000 | Loss: 0.00001123
Iteration 207/1000 | Loss: 0.00001123
Iteration 208/1000 | Loss: 0.00001123
Iteration 209/1000 | Loss: 0.00001123
Iteration 210/1000 | Loss: 0.00001123
Iteration 211/1000 | Loss: 0.00001123
Iteration 212/1000 | Loss: 0.00001123
Iteration 213/1000 | Loss: 0.00001123
Iteration 214/1000 | Loss: 0.00001123
Iteration 215/1000 | Loss: 0.00001123
Iteration 216/1000 | Loss: 0.00001123
Iteration 217/1000 | Loss: 0.00001123
Iteration 218/1000 | Loss: 0.00001123
Iteration 219/1000 | Loss: 0.00001123
Iteration 220/1000 | Loss: 0.00001123
Iteration 221/1000 | Loss: 0.00001123
Iteration 222/1000 | Loss: 0.00001123
Iteration 223/1000 | Loss: 0.00001123
Iteration 224/1000 | Loss: 0.00001123
Iteration 225/1000 | Loss: 0.00001123
Iteration 226/1000 | Loss: 0.00001123
Iteration 227/1000 | Loss: 0.00001123
Iteration 228/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.1230900781811215e-05, 1.1230900781811215e-05, 1.1230900781811215e-05, 1.1230900781811215e-05, 1.1230900781811215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1230900781811215e-05

Optimization complete. Final v2v error: 2.8513848781585693 mm

Highest mean error: 3.0370593070983887 mm for frame 47

Lowest mean error: 2.6945033073425293 mm for frame 147

Saving results

Total time: 38.57140135765076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431131
Iteration 2/25 | Loss: 0.00130381
Iteration 3/25 | Loss: 0.00124661
Iteration 4/25 | Loss: 0.00123546
Iteration 5/25 | Loss: 0.00123301
Iteration 6/25 | Loss: 0.00123238
Iteration 7/25 | Loss: 0.00123238
Iteration 8/25 | Loss: 0.00123238
Iteration 9/25 | Loss: 0.00123239
Iteration 10/25 | Loss: 0.00123238
Iteration 11/25 | Loss: 0.00123238
Iteration 12/25 | Loss: 0.00123238
Iteration 13/25 | Loss: 0.00123238
Iteration 14/25 | Loss: 0.00123238
Iteration 15/25 | Loss: 0.00123238
Iteration 16/25 | Loss: 0.00123238
Iteration 17/25 | Loss: 0.00123238
Iteration 18/25 | Loss: 0.00123238
Iteration 19/25 | Loss: 0.00123238
Iteration 20/25 | Loss: 0.00123238
Iteration 21/25 | Loss: 0.00123238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012323849368840456, 0.0012323849368840456, 0.0012323849368840456, 0.0012323849368840456, 0.0012323849368840456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012323849368840456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69471812
Iteration 2/25 | Loss: 0.00080204
Iteration 3/25 | Loss: 0.00080204
Iteration 4/25 | Loss: 0.00080204
Iteration 5/25 | Loss: 0.00080204
Iteration 6/25 | Loss: 0.00080203
Iteration 7/25 | Loss: 0.00080203
Iteration 8/25 | Loss: 0.00080203
Iteration 9/25 | Loss: 0.00080203
Iteration 10/25 | Loss: 0.00080203
Iteration 11/25 | Loss: 0.00080203
Iteration 12/25 | Loss: 0.00080203
Iteration 13/25 | Loss: 0.00080203
Iteration 14/25 | Loss: 0.00080203
Iteration 15/25 | Loss: 0.00080203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008020340465009212, 0.0008020340465009212, 0.0008020340465009212, 0.0008020340465009212, 0.0008020340465009212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008020340465009212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080203
Iteration 2/1000 | Loss: 0.00003003
Iteration 3/1000 | Loss: 0.00001863
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001608
Iteration 6/1000 | Loss: 0.00001566
Iteration 7/1000 | Loss: 0.00001528
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001495
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001462
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001443
Iteration 17/1000 | Loss: 0.00001439
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001418
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001417
Iteration 38/1000 | Loss: 0.00001417
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001416
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001415
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001414
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001413
Iteration 51/1000 | Loss: 0.00001413
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001409
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001408
Iteration 67/1000 | Loss: 0.00001408
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001407
Iteration 70/1000 | Loss: 0.00001407
Iteration 71/1000 | Loss: 0.00001407
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001405
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001398
Iteration 89/1000 | Loss: 0.00001398
Iteration 90/1000 | Loss: 0.00001398
Iteration 91/1000 | Loss: 0.00001398
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001394
Iteration 108/1000 | Loss: 0.00001394
Iteration 109/1000 | Loss: 0.00001394
Iteration 110/1000 | Loss: 0.00001394
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001394
Iteration 117/1000 | Loss: 0.00001394
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001393
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001392
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001390
Iteration 144/1000 | Loss: 0.00001390
Iteration 145/1000 | Loss: 0.00001390
Iteration 146/1000 | Loss: 0.00001390
Iteration 147/1000 | Loss: 0.00001389
Iteration 148/1000 | Loss: 0.00001389
Iteration 149/1000 | Loss: 0.00001389
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001388
Iteration 157/1000 | Loss: 0.00001388
Iteration 158/1000 | Loss: 0.00001388
Iteration 159/1000 | Loss: 0.00001388
Iteration 160/1000 | Loss: 0.00001388
Iteration 161/1000 | Loss: 0.00001388
Iteration 162/1000 | Loss: 0.00001388
Iteration 163/1000 | Loss: 0.00001388
Iteration 164/1000 | Loss: 0.00001388
Iteration 165/1000 | Loss: 0.00001388
Iteration 166/1000 | Loss: 0.00001387
Iteration 167/1000 | Loss: 0.00001387
Iteration 168/1000 | Loss: 0.00001387
Iteration 169/1000 | Loss: 0.00001387
Iteration 170/1000 | Loss: 0.00001387
Iteration 171/1000 | Loss: 0.00001387
Iteration 172/1000 | Loss: 0.00001387
Iteration 173/1000 | Loss: 0.00001387
Iteration 174/1000 | Loss: 0.00001387
Iteration 175/1000 | Loss: 0.00001387
Iteration 176/1000 | Loss: 0.00001387
Iteration 177/1000 | Loss: 0.00001387
Iteration 178/1000 | Loss: 0.00001387
Iteration 179/1000 | Loss: 0.00001387
Iteration 180/1000 | Loss: 0.00001387
Iteration 181/1000 | Loss: 0.00001387
Iteration 182/1000 | Loss: 0.00001387
Iteration 183/1000 | Loss: 0.00001387
Iteration 184/1000 | Loss: 0.00001387
Iteration 185/1000 | Loss: 0.00001387
Iteration 186/1000 | Loss: 0.00001387
Iteration 187/1000 | Loss: 0.00001387
Iteration 188/1000 | Loss: 0.00001387
Iteration 189/1000 | Loss: 0.00001387
Iteration 190/1000 | Loss: 0.00001387
Iteration 191/1000 | Loss: 0.00001387
Iteration 192/1000 | Loss: 0.00001387
Iteration 193/1000 | Loss: 0.00001387
Iteration 194/1000 | Loss: 0.00001387
Iteration 195/1000 | Loss: 0.00001387
Iteration 196/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.3870085240341723e-05, 1.3870085240341723e-05, 1.3870085240341723e-05, 1.3870085240341723e-05, 1.3870085240341723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3870085240341723e-05

Optimization complete. Final v2v error: 3.178130865097046 mm

Highest mean error: 3.6675970554351807 mm for frame 62

Lowest mean error: 3.016855001449585 mm for frame 90

Saving results

Total time: 39.18146085739136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850269
Iteration 2/25 | Loss: 0.00151757
Iteration 3/25 | Loss: 0.00136245
Iteration 4/25 | Loss: 0.00134287
Iteration 5/25 | Loss: 0.00133809
Iteration 6/25 | Loss: 0.00133709
Iteration 7/25 | Loss: 0.00133709
Iteration 8/25 | Loss: 0.00133709
Iteration 9/25 | Loss: 0.00133709
Iteration 10/25 | Loss: 0.00133709
Iteration 11/25 | Loss: 0.00133709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013370930682867765, 0.0013370930682867765, 0.0013370930682867765, 0.0013370930682867765, 0.0013370930682867765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013370930682867765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39444327
Iteration 2/25 | Loss: 0.00068704
Iteration 3/25 | Loss: 0.00068703
Iteration 4/25 | Loss: 0.00068703
Iteration 5/25 | Loss: 0.00068703
Iteration 6/25 | Loss: 0.00068703
Iteration 7/25 | Loss: 0.00068703
Iteration 8/25 | Loss: 0.00068703
Iteration 9/25 | Loss: 0.00068703
Iteration 10/25 | Loss: 0.00068703
Iteration 11/25 | Loss: 0.00068703
Iteration 12/25 | Loss: 0.00068703
Iteration 13/25 | Loss: 0.00068703
Iteration 14/25 | Loss: 0.00068703
Iteration 15/25 | Loss: 0.00068703
Iteration 16/25 | Loss: 0.00068703
Iteration 17/25 | Loss: 0.00068703
Iteration 18/25 | Loss: 0.00068703
Iteration 19/25 | Loss: 0.00068703
Iteration 20/25 | Loss: 0.00068703
Iteration 21/25 | Loss: 0.00068703
Iteration 22/25 | Loss: 0.00068703
Iteration 23/25 | Loss: 0.00068703
Iteration 24/25 | Loss: 0.00068703
Iteration 25/25 | Loss: 0.00068703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068703
Iteration 2/1000 | Loss: 0.00006157
Iteration 3/1000 | Loss: 0.00005161
Iteration 4/1000 | Loss: 0.00004406
Iteration 5/1000 | Loss: 0.00004217
Iteration 6/1000 | Loss: 0.00004029
Iteration 7/1000 | Loss: 0.00003916
Iteration 8/1000 | Loss: 0.00003835
Iteration 9/1000 | Loss: 0.00003788
Iteration 10/1000 | Loss: 0.00003745
Iteration 11/1000 | Loss: 0.00003711
Iteration 12/1000 | Loss: 0.00003684
Iteration 13/1000 | Loss: 0.00003661
Iteration 14/1000 | Loss: 0.00003646
Iteration 15/1000 | Loss: 0.00003639
Iteration 16/1000 | Loss: 0.00003632
Iteration 17/1000 | Loss: 0.00003628
Iteration 18/1000 | Loss: 0.00003621
Iteration 19/1000 | Loss: 0.00003610
Iteration 20/1000 | Loss: 0.00003608
Iteration 21/1000 | Loss: 0.00003607
Iteration 22/1000 | Loss: 0.00003607
Iteration 23/1000 | Loss: 0.00003606
Iteration 24/1000 | Loss: 0.00003606
Iteration 25/1000 | Loss: 0.00003606
Iteration 26/1000 | Loss: 0.00003606
Iteration 27/1000 | Loss: 0.00003606
Iteration 28/1000 | Loss: 0.00003606
Iteration 29/1000 | Loss: 0.00003606
Iteration 30/1000 | Loss: 0.00003606
Iteration 31/1000 | Loss: 0.00003605
Iteration 32/1000 | Loss: 0.00003605
Iteration 33/1000 | Loss: 0.00003605
Iteration 34/1000 | Loss: 0.00003602
Iteration 35/1000 | Loss: 0.00003602
Iteration 36/1000 | Loss: 0.00003602
Iteration 37/1000 | Loss: 0.00003602
Iteration 38/1000 | Loss: 0.00003602
Iteration 39/1000 | Loss: 0.00003602
Iteration 40/1000 | Loss: 0.00003602
Iteration 41/1000 | Loss: 0.00003601
Iteration 42/1000 | Loss: 0.00003601
Iteration 43/1000 | Loss: 0.00003601
Iteration 44/1000 | Loss: 0.00003601
Iteration 45/1000 | Loss: 0.00003601
Iteration 46/1000 | Loss: 0.00003601
Iteration 47/1000 | Loss: 0.00003601
Iteration 48/1000 | Loss: 0.00003601
Iteration 49/1000 | Loss: 0.00003601
Iteration 50/1000 | Loss: 0.00003600
Iteration 51/1000 | Loss: 0.00003599
Iteration 52/1000 | Loss: 0.00003599
Iteration 53/1000 | Loss: 0.00003599
Iteration 54/1000 | Loss: 0.00003598
Iteration 55/1000 | Loss: 0.00003598
Iteration 56/1000 | Loss: 0.00003598
Iteration 57/1000 | Loss: 0.00003598
Iteration 58/1000 | Loss: 0.00003597
Iteration 59/1000 | Loss: 0.00003597
Iteration 60/1000 | Loss: 0.00003597
Iteration 61/1000 | Loss: 0.00003596
Iteration 62/1000 | Loss: 0.00003596
Iteration 63/1000 | Loss: 0.00003594
Iteration 64/1000 | Loss: 0.00003594
Iteration 65/1000 | Loss: 0.00003594
Iteration 66/1000 | Loss: 0.00003594
Iteration 67/1000 | Loss: 0.00003594
Iteration 68/1000 | Loss: 0.00003594
Iteration 69/1000 | Loss: 0.00003594
Iteration 70/1000 | Loss: 0.00003593
Iteration 71/1000 | Loss: 0.00003593
Iteration 72/1000 | Loss: 0.00003593
Iteration 73/1000 | Loss: 0.00003593
Iteration 74/1000 | Loss: 0.00003593
Iteration 75/1000 | Loss: 0.00003593
Iteration 76/1000 | Loss: 0.00003593
Iteration 77/1000 | Loss: 0.00003593
Iteration 78/1000 | Loss: 0.00003593
Iteration 79/1000 | Loss: 0.00003592
Iteration 80/1000 | Loss: 0.00003592
Iteration 81/1000 | Loss: 0.00003592
Iteration 82/1000 | Loss: 0.00003592
Iteration 83/1000 | Loss: 0.00003591
Iteration 84/1000 | Loss: 0.00003591
Iteration 85/1000 | Loss: 0.00003591
Iteration 86/1000 | Loss: 0.00003591
Iteration 87/1000 | Loss: 0.00003591
Iteration 88/1000 | Loss: 0.00003591
Iteration 89/1000 | Loss: 0.00003591
Iteration 90/1000 | Loss: 0.00003591
Iteration 91/1000 | Loss: 0.00003591
Iteration 92/1000 | Loss: 0.00003590
Iteration 93/1000 | Loss: 0.00003590
Iteration 94/1000 | Loss: 0.00003590
Iteration 95/1000 | Loss: 0.00003590
Iteration 96/1000 | Loss: 0.00003590
Iteration 97/1000 | Loss: 0.00003590
Iteration 98/1000 | Loss: 0.00003589
Iteration 99/1000 | Loss: 0.00003589
Iteration 100/1000 | Loss: 0.00003589
Iteration 101/1000 | Loss: 0.00003589
Iteration 102/1000 | Loss: 0.00003588
Iteration 103/1000 | Loss: 0.00003588
Iteration 104/1000 | Loss: 0.00003588
Iteration 105/1000 | Loss: 0.00003588
Iteration 106/1000 | Loss: 0.00003588
Iteration 107/1000 | Loss: 0.00003588
Iteration 108/1000 | Loss: 0.00003588
Iteration 109/1000 | Loss: 0.00003587
Iteration 110/1000 | Loss: 0.00003587
Iteration 111/1000 | Loss: 0.00003586
Iteration 112/1000 | Loss: 0.00003586
Iteration 113/1000 | Loss: 0.00003586
Iteration 114/1000 | Loss: 0.00003586
Iteration 115/1000 | Loss: 0.00003586
Iteration 116/1000 | Loss: 0.00003586
Iteration 117/1000 | Loss: 0.00003586
Iteration 118/1000 | Loss: 0.00003585
Iteration 119/1000 | Loss: 0.00003585
Iteration 120/1000 | Loss: 0.00003585
Iteration 121/1000 | Loss: 0.00003585
Iteration 122/1000 | Loss: 0.00003585
Iteration 123/1000 | Loss: 0.00003585
Iteration 124/1000 | Loss: 0.00003585
Iteration 125/1000 | Loss: 0.00003585
Iteration 126/1000 | Loss: 0.00003585
Iteration 127/1000 | Loss: 0.00003585
Iteration 128/1000 | Loss: 0.00003585
Iteration 129/1000 | Loss: 0.00003585
Iteration 130/1000 | Loss: 0.00003584
Iteration 131/1000 | Loss: 0.00003584
Iteration 132/1000 | Loss: 0.00003584
Iteration 133/1000 | Loss: 0.00003584
Iteration 134/1000 | Loss: 0.00003584
Iteration 135/1000 | Loss: 0.00003584
Iteration 136/1000 | Loss: 0.00003584
Iteration 137/1000 | Loss: 0.00003583
Iteration 138/1000 | Loss: 0.00003583
Iteration 139/1000 | Loss: 0.00003583
Iteration 140/1000 | Loss: 0.00003583
Iteration 141/1000 | Loss: 0.00003583
Iteration 142/1000 | Loss: 0.00003583
Iteration 143/1000 | Loss: 0.00003583
Iteration 144/1000 | Loss: 0.00003583
Iteration 145/1000 | Loss: 0.00003582
Iteration 146/1000 | Loss: 0.00003582
Iteration 147/1000 | Loss: 0.00003582
Iteration 148/1000 | Loss: 0.00003582
Iteration 149/1000 | Loss: 0.00003582
Iteration 150/1000 | Loss: 0.00003582
Iteration 151/1000 | Loss: 0.00003582
Iteration 152/1000 | Loss: 0.00003582
Iteration 153/1000 | Loss: 0.00003581
Iteration 154/1000 | Loss: 0.00003581
Iteration 155/1000 | Loss: 0.00003581
Iteration 156/1000 | Loss: 0.00003581
Iteration 157/1000 | Loss: 0.00003581
Iteration 158/1000 | Loss: 0.00003581
Iteration 159/1000 | Loss: 0.00003581
Iteration 160/1000 | Loss: 0.00003581
Iteration 161/1000 | Loss: 0.00003581
Iteration 162/1000 | Loss: 0.00003581
Iteration 163/1000 | Loss: 0.00003581
Iteration 164/1000 | Loss: 0.00003581
Iteration 165/1000 | Loss: 0.00003581
Iteration 166/1000 | Loss: 0.00003581
Iteration 167/1000 | Loss: 0.00003581
Iteration 168/1000 | Loss: 0.00003580
Iteration 169/1000 | Loss: 0.00003580
Iteration 170/1000 | Loss: 0.00003580
Iteration 171/1000 | Loss: 0.00003580
Iteration 172/1000 | Loss: 0.00003580
Iteration 173/1000 | Loss: 0.00003580
Iteration 174/1000 | Loss: 0.00003580
Iteration 175/1000 | Loss: 0.00003580
Iteration 176/1000 | Loss: 0.00003580
Iteration 177/1000 | Loss: 0.00003580
Iteration 178/1000 | Loss: 0.00003580
Iteration 179/1000 | Loss: 0.00003580
Iteration 180/1000 | Loss: 0.00003580
Iteration 181/1000 | Loss: 0.00003580
Iteration 182/1000 | Loss: 0.00003580
Iteration 183/1000 | Loss: 0.00003580
Iteration 184/1000 | Loss: 0.00003580
Iteration 185/1000 | Loss: 0.00003580
Iteration 186/1000 | Loss: 0.00003580
Iteration 187/1000 | Loss: 0.00003580
Iteration 188/1000 | Loss: 0.00003580
Iteration 189/1000 | Loss: 0.00003580
Iteration 190/1000 | Loss: 0.00003580
Iteration 191/1000 | Loss: 0.00003580
Iteration 192/1000 | Loss: 0.00003580
Iteration 193/1000 | Loss: 0.00003580
Iteration 194/1000 | Loss: 0.00003580
Iteration 195/1000 | Loss: 0.00003580
Iteration 196/1000 | Loss: 0.00003580
Iteration 197/1000 | Loss: 0.00003580
Iteration 198/1000 | Loss: 0.00003580
Iteration 199/1000 | Loss: 0.00003580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [3.579537951736711e-05, 3.579537951736711e-05, 3.579537951736711e-05, 3.579537951736711e-05, 3.579537951736711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.579537951736711e-05

Optimization complete. Final v2v error: 4.927839279174805 mm

Highest mean error: 5.453588962554932 mm for frame 27

Lowest mean error: 4.437625885009766 mm for frame 84

Saving results

Total time: 43.29681086540222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960017
Iteration 2/25 | Loss: 0.00960017
Iteration 3/25 | Loss: 0.00960016
Iteration 4/25 | Loss: 0.00960016
Iteration 5/25 | Loss: 0.00960016
Iteration 6/25 | Loss: 0.00960016
Iteration 7/25 | Loss: 0.00960016
Iteration 8/25 | Loss: 0.00960016
Iteration 9/25 | Loss: 0.00960015
Iteration 10/25 | Loss: 0.00960015
Iteration 11/25 | Loss: 0.00960015
Iteration 12/25 | Loss: 0.00960015
Iteration 13/25 | Loss: 0.00960014
Iteration 14/25 | Loss: 0.00960014
Iteration 15/25 | Loss: 0.00960014
Iteration 16/25 | Loss: 0.00960014
Iteration 17/25 | Loss: 0.00960013
Iteration 18/25 | Loss: 0.00960013
Iteration 19/25 | Loss: 0.00960013
Iteration 20/25 | Loss: 0.00960013
Iteration 21/25 | Loss: 0.00960013
Iteration 22/25 | Loss: 0.00960012
Iteration 23/25 | Loss: 0.00960012
Iteration 24/25 | Loss: 0.00960012
Iteration 25/25 | Loss: 0.00960012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73606336
Iteration 2/25 | Loss: 0.20273426
Iteration 3/25 | Loss: 0.20273131
Iteration 4/25 | Loss: 0.20273131
Iteration 5/25 | Loss: 0.20273127
Iteration 6/25 | Loss: 0.20273124
Iteration 7/25 | Loss: 0.20273124
Iteration 8/25 | Loss: 0.20273124
Iteration 9/25 | Loss: 0.20273122
Iteration 10/25 | Loss: 0.20273122
Iteration 11/25 | Loss: 0.20273122
Iteration 12/25 | Loss: 0.20273122
Iteration 13/25 | Loss: 0.20273122
Iteration 14/25 | Loss: 0.20273122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.20273122191429138, 0.20273122191429138, 0.20273122191429138, 0.20273122191429138, 0.20273122191429138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.20273122191429138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.20273122
Iteration 2/1000 | Loss: 0.00302199
Iteration 3/1000 | Loss: 0.00102305
Iteration 4/1000 | Loss: 0.00055825
Iteration 5/1000 | Loss: 0.00035911
Iteration 6/1000 | Loss: 0.00022256
Iteration 7/1000 | Loss: 0.00025159
Iteration 8/1000 | Loss: 0.00051540
Iteration 9/1000 | Loss: 0.00041361
Iteration 10/1000 | Loss: 0.00008932
Iteration 11/1000 | Loss: 0.00007097
Iteration 12/1000 | Loss: 0.00056506
Iteration 13/1000 | Loss: 0.00011871
Iteration 14/1000 | Loss: 0.00005753
Iteration 15/1000 | Loss: 0.00029965
Iteration 16/1000 | Loss: 0.00009632
Iteration 17/1000 | Loss: 0.00015199
Iteration 18/1000 | Loss: 0.00014251
Iteration 19/1000 | Loss: 0.00004613
Iteration 20/1000 | Loss: 0.00004919
Iteration 21/1000 | Loss: 0.00005771
Iteration 22/1000 | Loss: 0.00003755
Iteration 23/1000 | Loss: 0.00007099
Iteration 24/1000 | Loss: 0.00003264
Iteration 25/1000 | Loss: 0.00007712
Iteration 26/1000 | Loss: 0.00003053
Iteration 27/1000 | Loss: 0.00002890
Iteration 28/1000 | Loss: 0.00002323
Iteration 29/1000 | Loss: 0.00002252
Iteration 30/1000 | Loss: 0.00002192
Iteration 31/1000 | Loss: 0.00002148
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002065
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00001994
Iteration 36/1000 | Loss: 0.00001965
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001919
Iteration 42/1000 | Loss: 0.00001919
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001913
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001906
Iteration 47/1000 | Loss: 0.00001906
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001905
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001903
Iteration 52/1000 | Loss: 0.00001903
Iteration 53/1000 | Loss: 0.00001903
Iteration 54/1000 | Loss: 0.00001902
Iteration 55/1000 | Loss: 0.00001902
Iteration 56/1000 | Loss: 0.00001902
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001901
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001899
Iteration 70/1000 | Loss: 0.00001899
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001898
Iteration 73/1000 | Loss: 0.00001898
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001895
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001888
Iteration 109/1000 | Loss: 0.00001888
Iteration 110/1000 | Loss: 0.00001888
Iteration 111/1000 | Loss: 0.00001887
Iteration 112/1000 | Loss: 0.00001887
Iteration 113/1000 | Loss: 0.00001887
Iteration 114/1000 | Loss: 0.00001887
Iteration 115/1000 | Loss: 0.00001887
Iteration 116/1000 | Loss: 0.00001887
Iteration 117/1000 | Loss: 0.00001887
Iteration 118/1000 | Loss: 0.00001887
Iteration 119/1000 | Loss: 0.00001887
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001886
Iteration 122/1000 | Loss: 0.00001886
Iteration 123/1000 | Loss: 0.00001886
Iteration 124/1000 | Loss: 0.00001886
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Iteration 129/1000 | Loss: 0.00001886
Iteration 130/1000 | Loss: 0.00001886
Iteration 131/1000 | Loss: 0.00001886
Iteration 132/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.8861064745578915e-05, 1.8861064745578915e-05, 1.8861064745578915e-05, 1.8861064745578915e-05, 1.8861064745578915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8861064745578915e-05

Optimization complete. Final v2v error: 3.744413137435913 mm

Highest mean error: 3.9280290603637695 mm for frame 23

Lowest mean error: 3.5529017448425293 mm for frame 151

Saving results

Total time: 78.11179089546204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932959
Iteration 2/25 | Loss: 0.00232805
Iteration 3/25 | Loss: 0.00166739
Iteration 4/25 | Loss: 0.00154917
Iteration 5/25 | Loss: 0.00150595
Iteration 6/25 | Loss: 0.00140954
Iteration 7/25 | Loss: 0.00138257
Iteration 8/25 | Loss: 0.00137566
Iteration 9/25 | Loss: 0.00134272
Iteration 10/25 | Loss: 0.00132892
Iteration 11/25 | Loss: 0.00131393
Iteration 12/25 | Loss: 0.00131705
Iteration 13/25 | Loss: 0.00130361
Iteration 14/25 | Loss: 0.00130831
Iteration 15/25 | Loss: 0.00130285
Iteration 16/25 | Loss: 0.00130154
Iteration 17/25 | Loss: 0.00130698
Iteration 18/25 | Loss: 0.00130034
Iteration 19/25 | Loss: 0.00130001
Iteration 20/25 | Loss: 0.00130618
Iteration 21/25 | Loss: 0.00130384
Iteration 22/25 | Loss: 0.00130140
Iteration 23/25 | Loss: 0.00130110
Iteration 24/25 | Loss: 0.00129425
Iteration 25/25 | Loss: 0.00129297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50261521
Iteration 2/25 | Loss: 0.00089596
Iteration 3/25 | Loss: 0.00082514
Iteration 4/25 | Loss: 0.00082514
Iteration 5/25 | Loss: 0.00082514
Iteration 6/25 | Loss: 0.00082514
Iteration 7/25 | Loss: 0.00082514
Iteration 8/25 | Loss: 0.00082514
Iteration 9/25 | Loss: 0.00082514
Iteration 10/25 | Loss: 0.00082514
Iteration 11/25 | Loss: 0.00082513
Iteration 12/25 | Loss: 0.00082513
Iteration 13/25 | Loss: 0.00082513
Iteration 14/25 | Loss: 0.00082513
Iteration 15/25 | Loss: 0.00082513
Iteration 16/25 | Loss: 0.00082513
Iteration 17/25 | Loss: 0.00082513
Iteration 18/25 | Loss: 0.00082513
Iteration 19/25 | Loss: 0.00082513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008251344552263618, 0.0008251344552263618, 0.0008251344552263618, 0.0008251344552263618, 0.0008251344552263618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008251344552263618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082513
Iteration 2/1000 | Loss: 0.00016275
Iteration 3/1000 | Loss: 0.00039814
Iteration 4/1000 | Loss: 0.00005365
Iteration 5/1000 | Loss: 0.00004341
Iteration 6/1000 | Loss: 0.00004891
Iteration 7/1000 | Loss: 0.00011915
Iteration 8/1000 | Loss: 0.00023666
Iteration 9/1000 | Loss: 0.00113795
Iteration 10/1000 | Loss: 0.00023683
Iteration 11/1000 | Loss: 0.00003214
Iteration 12/1000 | Loss: 0.00002775
Iteration 13/1000 | Loss: 0.00005794
Iteration 14/1000 | Loss: 0.00002784
Iteration 15/1000 | Loss: 0.00002620
Iteration 16/1000 | Loss: 0.00031643
Iteration 17/1000 | Loss: 0.00035616
Iteration 18/1000 | Loss: 0.00004455
Iteration 19/1000 | Loss: 0.00007019
Iteration 20/1000 | Loss: 0.00003876
Iteration 21/1000 | Loss: 0.00002866
Iteration 22/1000 | Loss: 0.00002907
Iteration 23/1000 | Loss: 0.00002946
Iteration 24/1000 | Loss: 0.00002585
Iteration 25/1000 | Loss: 0.00002178
Iteration 26/1000 | Loss: 0.00002144
Iteration 27/1000 | Loss: 0.00002115
Iteration 28/1000 | Loss: 0.00002098
Iteration 29/1000 | Loss: 0.00002097
Iteration 30/1000 | Loss: 0.00002096
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002064
Iteration 33/1000 | Loss: 0.00002063
Iteration 34/1000 | Loss: 0.00002063
Iteration 35/1000 | Loss: 0.00002062
Iteration 36/1000 | Loss: 0.00002061
Iteration 37/1000 | Loss: 0.00002061
Iteration 38/1000 | Loss: 0.00002060
Iteration 39/1000 | Loss: 0.00002059
Iteration 40/1000 | Loss: 0.00002059
Iteration 41/1000 | Loss: 0.00002059
Iteration 42/1000 | Loss: 0.00002059
Iteration 43/1000 | Loss: 0.00002058
Iteration 44/1000 | Loss: 0.00002058
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00002058
Iteration 47/1000 | Loss: 0.00002058
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002056
Iteration 51/1000 | Loss: 0.00002056
Iteration 52/1000 | Loss: 0.00002056
Iteration 53/1000 | Loss: 0.00002056
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002055
Iteration 57/1000 | Loss: 0.00002055
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002055
Iteration 62/1000 | Loss: 0.00002054
Iteration 63/1000 | Loss: 0.00002054
Iteration 64/1000 | Loss: 0.00002054
Iteration 65/1000 | Loss: 0.00002053
Iteration 66/1000 | Loss: 0.00002053
Iteration 67/1000 | Loss: 0.00002053
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002052
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002051
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002050
Iteration 76/1000 | Loss: 0.00002050
Iteration 77/1000 | Loss: 0.00002049
Iteration 78/1000 | Loss: 0.00002049
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002045
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002044
Iteration 87/1000 | Loss: 0.00002044
Iteration 88/1000 | Loss: 0.00002044
Iteration 89/1000 | Loss: 0.00002043
Iteration 90/1000 | Loss: 0.00002043
Iteration 91/1000 | Loss: 0.00002043
Iteration 92/1000 | Loss: 0.00002043
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002043
Iteration 98/1000 | Loss: 0.00002043
Iteration 99/1000 | Loss: 0.00002042
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002042
Iteration 104/1000 | Loss: 0.00002042
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002041
Iteration 112/1000 | Loss: 0.00002041
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002040
Iteration 121/1000 | Loss: 0.00002040
Iteration 122/1000 | Loss: 0.00002040
Iteration 123/1000 | Loss: 0.00002039
Iteration 124/1000 | Loss: 0.00002039
Iteration 125/1000 | Loss: 0.00002039
Iteration 126/1000 | Loss: 0.00002038
Iteration 127/1000 | Loss: 0.00002038
Iteration 128/1000 | Loss: 0.00002038
Iteration 129/1000 | Loss: 0.00002038
Iteration 130/1000 | Loss: 0.00002037
Iteration 131/1000 | Loss: 0.00002037
Iteration 132/1000 | Loss: 0.00002037
Iteration 133/1000 | Loss: 0.00002037
Iteration 134/1000 | Loss: 0.00002037
Iteration 135/1000 | Loss: 0.00002037
Iteration 136/1000 | Loss: 0.00002037
Iteration 137/1000 | Loss: 0.00002037
Iteration 138/1000 | Loss: 0.00002037
Iteration 139/1000 | Loss: 0.00002036
Iteration 140/1000 | Loss: 0.00002036
Iteration 141/1000 | Loss: 0.00002036
Iteration 142/1000 | Loss: 0.00002036
Iteration 143/1000 | Loss: 0.00002036
Iteration 144/1000 | Loss: 0.00002036
Iteration 145/1000 | Loss: 0.00002035
Iteration 146/1000 | Loss: 0.00002035
Iteration 147/1000 | Loss: 0.00002035
Iteration 148/1000 | Loss: 0.00002034
Iteration 149/1000 | Loss: 0.00002034
Iteration 150/1000 | Loss: 0.00002034
Iteration 151/1000 | Loss: 0.00002034
Iteration 152/1000 | Loss: 0.00002034
Iteration 153/1000 | Loss: 0.00002034
Iteration 154/1000 | Loss: 0.00002034
Iteration 155/1000 | Loss: 0.00002033
Iteration 156/1000 | Loss: 0.00002033
Iteration 157/1000 | Loss: 0.00002033
Iteration 158/1000 | Loss: 0.00002033
Iteration 159/1000 | Loss: 0.00002033
Iteration 160/1000 | Loss: 0.00002033
Iteration 161/1000 | Loss: 0.00002033
Iteration 162/1000 | Loss: 0.00002033
Iteration 163/1000 | Loss: 0.00002033
Iteration 164/1000 | Loss: 0.00002033
Iteration 165/1000 | Loss: 0.00002032
Iteration 166/1000 | Loss: 0.00002032
Iteration 167/1000 | Loss: 0.00002032
Iteration 168/1000 | Loss: 0.00002032
Iteration 169/1000 | Loss: 0.00002032
Iteration 170/1000 | Loss: 0.00002032
Iteration 171/1000 | Loss: 0.00002032
Iteration 172/1000 | Loss: 0.00002032
Iteration 173/1000 | Loss: 0.00002032
Iteration 174/1000 | Loss: 0.00002032
Iteration 175/1000 | Loss: 0.00002032
Iteration 176/1000 | Loss: 0.00002032
Iteration 177/1000 | Loss: 0.00002031
Iteration 178/1000 | Loss: 0.00002031
Iteration 179/1000 | Loss: 0.00002031
Iteration 180/1000 | Loss: 0.00002031
Iteration 181/1000 | Loss: 0.00002031
Iteration 182/1000 | Loss: 0.00002031
Iteration 183/1000 | Loss: 0.00002031
Iteration 184/1000 | Loss: 0.00002031
Iteration 185/1000 | Loss: 0.00002031
Iteration 186/1000 | Loss: 0.00002031
Iteration 187/1000 | Loss: 0.00002031
Iteration 188/1000 | Loss: 0.00002031
Iteration 189/1000 | Loss: 0.00002031
Iteration 190/1000 | Loss: 0.00002031
Iteration 191/1000 | Loss: 0.00002030
Iteration 192/1000 | Loss: 0.00002030
Iteration 193/1000 | Loss: 0.00002030
Iteration 194/1000 | Loss: 0.00002030
Iteration 195/1000 | Loss: 0.00002030
Iteration 196/1000 | Loss: 0.00002030
Iteration 197/1000 | Loss: 0.00002030
Iteration 198/1000 | Loss: 0.00002030
Iteration 199/1000 | Loss: 0.00002030
Iteration 200/1000 | Loss: 0.00002030
Iteration 201/1000 | Loss: 0.00002030
Iteration 202/1000 | Loss: 0.00002030
Iteration 203/1000 | Loss: 0.00002029
Iteration 204/1000 | Loss: 0.00002029
Iteration 205/1000 | Loss: 0.00002029
Iteration 206/1000 | Loss: 0.00002028
Iteration 207/1000 | Loss: 0.00002028
Iteration 208/1000 | Loss: 0.00002028
Iteration 209/1000 | Loss: 0.00002027
Iteration 210/1000 | Loss: 0.00002027
Iteration 211/1000 | Loss: 0.00002027
Iteration 212/1000 | Loss: 0.00002027
Iteration 213/1000 | Loss: 0.00002027
Iteration 214/1000 | Loss: 0.00002027
Iteration 215/1000 | Loss: 0.00002027
Iteration 216/1000 | Loss: 0.00002027
Iteration 217/1000 | Loss: 0.00002027
Iteration 218/1000 | Loss: 0.00002026
Iteration 219/1000 | Loss: 0.00002026
Iteration 220/1000 | Loss: 0.00002026
Iteration 221/1000 | Loss: 0.00002026
Iteration 222/1000 | Loss: 0.00002026
Iteration 223/1000 | Loss: 0.00002026
Iteration 224/1000 | Loss: 0.00002026
Iteration 225/1000 | Loss: 0.00002026
Iteration 226/1000 | Loss: 0.00002026
Iteration 227/1000 | Loss: 0.00002026
Iteration 228/1000 | Loss: 0.00002026
Iteration 229/1000 | Loss: 0.00002026
Iteration 230/1000 | Loss: 0.00002026
Iteration 231/1000 | Loss: 0.00002026
Iteration 232/1000 | Loss: 0.00002026
Iteration 233/1000 | Loss: 0.00002026
Iteration 234/1000 | Loss: 0.00002026
Iteration 235/1000 | Loss: 0.00002026
Iteration 236/1000 | Loss: 0.00002026
Iteration 237/1000 | Loss: 0.00002026
Iteration 238/1000 | Loss: 0.00002026
Iteration 239/1000 | Loss: 0.00002026
Iteration 240/1000 | Loss: 0.00002026
Iteration 241/1000 | Loss: 0.00002025
Iteration 242/1000 | Loss: 0.00002025
Iteration 243/1000 | Loss: 0.00002025
Iteration 244/1000 | Loss: 0.00002025
Iteration 245/1000 | Loss: 0.00002025
Iteration 246/1000 | Loss: 0.00002025
Iteration 247/1000 | Loss: 0.00002025
Iteration 248/1000 | Loss: 0.00002025
Iteration 249/1000 | Loss: 0.00002025
Iteration 250/1000 | Loss: 0.00002025
Iteration 251/1000 | Loss: 0.00002025
Iteration 252/1000 | Loss: 0.00002025
Iteration 253/1000 | Loss: 0.00002025
Iteration 254/1000 | Loss: 0.00002025
Iteration 255/1000 | Loss: 0.00002025
Iteration 256/1000 | Loss: 0.00002025
Iteration 257/1000 | Loss: 0.00002025
Iteration 258/1000 | Loss: 0.00002025
Iteration 259/1000 | Loss: 0.00002025
Iteration 260/1000 | Loss: 0.00002025
Iteration 261/1000 | Loss: 0.00002025
Iteration 262/1000 | Loss: 0.00002025
Iteration 263/1000 | Loss: 0.00002025
Iteration 264/1000 | Loss: 0.00002025
Iteration 265/1000 | Loss: 0.00002025
Iteration 266/1000 | Loss: 0.00002025
Iteration 267/1000 | Loss: 0.00002025
Iteration 268/1000 | Loss: 0.00002025
Iteration 269/1000 | Loss: 0.00002025
Iteration 270/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [2.024802415689919e-05, 2.024802415689919e-05, 2.024802415689919e-05, 2.024802415689919e-05, 2.024802415689919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.024802415689919e-05

Optimization complete. Final v2v error: 3.8450753688812256 mm

Highest mean error: 4.40081262588501 mm for frame 133

Lowest mean error: 3.424668788909912 mm for frame 100

Saving results

Total time: 121.12740731239319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418771
Iteration 2/25 | Loss: 0.00135288
Iteration 3/25 | Loss: 0.00126632
Iteration 4/25 | Loss: 0.00125341
Iteration 5/25 | Loss: 0.00124922
Iteration 6/25 | Loss: 0.00124905
Iteration 7/25 | Loss: 0.00124905
Iteration 8/25 | Loss: 0.00124905
Iteration 9/25 | Loss: 0.00124905
Iteration 10/25 | Loss: 0.00124905
Iteration 11/25 | Loss: 0.00124905
Iteration 12/25 | Loss: 0.00124905
Iteration 13/25 | Loss: 0.00124905
Iteration 14/25 | Loss: 0.00124905
Iteration 15/25 | Loss: 0.00124905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012490462977439165, 0.0012490462977439165, 0.0012490462977439165, 0.0012490462977439165, 0.0012490462977439165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012490462977439165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43636334
Iteration 2/25 | Loss: 0.00093031
Iteration 3/25 | Loss: 0.00093030
Iteration 4/25 | Loss: 0.00093030
Iteration 5/25 | Loss: 0.00093030
Iteration 6/25 | Loss: 0.00093030
Iteration 7/25 | Loss: 0.00093030
Iteration 8/25 | Loss: 0.00093030
Iteration 9/25 | Loss: 0.00093030
Iteration 10/25 | Loss: 0.00093030
Iteration 11/25 | Loss: 0.00093030
Iteration 12/25 | Loss: 0.00093030
Iteration 13/25 | Loss: 0.00093030
Iteration 14/25 | Loss: 0.00093030
Iteration 15/25 | Loss: 0.00093030
Iteration 16/25 | Loss: 0.00093030
Iteration 17/25 | Loss: 0.00093030
Iteration 18/25 | Loss: 0.00093030
Iteration 19/25 | Loss: 0.00093030
Iteration 20/25 | Loss: 0.00093030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009302996913902462, 0.0009302996913902462, 0.0009302996913902462, 0.0009302996913902462, 0.0009302996913902462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009302996913902462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093030
Iteration 2/1000 | Loss: 0.00002830
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001658
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001547
Iteration 8/1000 | Loss: 0.00001523
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001471
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001465
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001448
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001446
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001443
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001439
Iteration 22/1000 | Loss: 0.00001437
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001432
Iteration 27/1000 | Loss: 0.00001432
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001419
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00001397
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001396
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001382
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001380
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001376
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001375
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001372
Iteration 121/1000 | Loss: 0.00001372
Iteration 122/1000 | Loss: 0.00001372
Iteration 123/1000 | Loss: 0.00001372
Iteration 124/1000 | Loss: 0.00001372
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001368
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001367
Iteration 161/1000 | Loss: 0.00001367
Iteration 162/1000 | Loss: 0.00001367
Iteration 163/1000 | Loss: 0.00001367
Iteration 164/1000 | Loss: 0.00001367
Iteration 165/1000 | Loss: 0.00001367
Iteration 166/1000 | Loss: 0.00001367
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001367
Iteration 169/1000 | Loss: 0.00001367
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001367
Iteration 172/1000 | Loss: 0.00001367
Iteration 173/1000 | Loss: 0.00001367
Iteration 174/1000 | Loss: 0.00001367
Iteration 175/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.366920696455054e-05, 1.366920696455054e-05, 1.366920696455054e-05, 1.366920696455054e-05, 1.366920696455054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.366920696455054e-05

Optimization complete. Final v2v error: 3.1538822650909424 mm

Highest mean error: 3.550023078918457 mm for frame 231

Lowest mean error: 2.9212629795074463 mm for frame 168

Saving results

Total time: 46.242931604385376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782676
Iteration 2/25 | Loss: 0.00136038
Iteration 3/25 | Loss: 0.00123795
Iteration 4/25 | Loss: 0.00122703
Iteration 5/25 | Loss: 0.00122511
Iteration 6/25 | Loss: 0.00122511
Iteration 7/25 | Loss: 0.00122511
Iteration 8/25 | Loss: 0.00122511
Iteration 9/25 | Loss: 0.00122511
Iteration 10/25 | Loss: 0.00122511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012251058360561728, 0.0012251058360561728, 0.0012251058360561728, 0.0012251058360561728, 0.0012251058360561728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012251058360561728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43091345
Iteration 2/25 | Loss: 0.00077112
Iteration 3/25 | Loss: 0.00077112
Iteration 4/25 | Loss: 0.00077112
Iteration 5/25 | Loss: 0.00077112
Iteration 6/25 | Loss: 0.00077112
Iteration 7/25 | Loss: 0.00077112
Iteration 8/25 | Loss: 0.00077112
Iteration 9/25 | Loss: 0.00077112
Iteration 10/25 | Loss: 0.00077112
Iteration 11/25 | Loss: 0.00077112
Iteration 12/25 | Loss: 0.00077112
Iteration 13/25 | Loss: 0.00077112
Iteration 14/25 | Loss: 0.00077112
Iteration 15/25 | Loss: 0.00077112
Iteration 16/25 | Loss: 0.00077112
Iteration 17/25 | Loss: 0.00077112
Iteration 18/25 | Loss: 0.00077112
Iteration 19/25 | Loss: 0.00077112
Iteration 20/25 | Loss: 0.00077112
Iteration 21/25 | Loss: 0.00077112
Iteration 22/25 | Loss: 0.00077112
Iteration 23/25 | Loss: 0.00077112
Iteration 24/25 | Loss: 0.00077112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007711190264672041, 0.0007711190264672041, 0.0007711190264672041, 0.0007711190264672041, 0.0007711190264672041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007711190264672041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077112
Iteration 2/1000 | Loss: 0.00002750
Iteration 3/1000 | Loss: 0.00001705
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001258
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001243
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001239
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001237
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001236
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001233
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001232
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001230
Iteration 67/1000 | Loss: 0.00001230
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001225
Iteration 82/1000 | Loss: 0.00001225
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001224
Iteration 86/1000 | Loss: 0.00001224
Iteration 87/1000 | Loss: 0.00001224
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001223
Iteration 90/1000 | Loss: 0.00001223
Iteration 91/1000 | Loss: 0.00001223
Iteration 92/1000 | Loss: 0.00001223
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001223
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001222
Iteration 99/1000 | Loss: 0.00001221
Iteration 100/1000 | Loss: 0.00001221
Iteration 101/1000 | Loss: 0.00001221
Iteration 102/1000 | Loss: 0.00001220
Iteration 103/1000 | Loss: 0.00001220
Iteration 104/1000 | Loss: 0.00001220
Iteration 105/1000 | Loss: 0.00001219
Iteration 106/1000 | Loss: 0.00001219
Iteration 107/1000 | Loss: 0.00001219
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001217
Iteration 111/1000 | Loss: 0.00001217
Iteration 112/1000 | Loss: 0.00001217
Iteration 113/1000 | Loss: 0.00001217
Iteration 114/1000 | Loss: 0.00001217
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001216
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001215
Iteration 126/1000 | Loss: 0.00001215
Iteration 127/1000 | Loss: 0.00001215
Iteration 128/1000 | Loss: 0.00001215
Iteration 129/1000 | Loss: 0.00001215
Iteration 130/1000 | Loss: 0.00001215
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001214
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001214
Iteration 138/1000 | Loss: 0.00001214
Iteration 139/1000 | Loss: 0.00001214
Iteration 140/1000 | Loss: 0.00001214
Iteration 141/1000 | Loss: 0.00001214
Iteration 142/1000 | Loss: 0.00001214
Iteration 143/1000 | Loss: 0.00001214
Iteration 144/1000 | Loss: 0.00001214
Iteration 145/1000 | Loss: 0.00001213
Iteration 146/1000 | Loss: 0.00001213
Iteration 147/1000 | Loss: 0.00001213
Iteration 148/1000 | Loss: 0.00001213
Iteration 149/1000 | Loss: 0.00001213
Iteration 150/1000 | Loss: 0.00001213
Iteration 151/1000 | Loss: 0.00001213
Iteration 152/1000 | Loss: 0.00001213
Iteration 153/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.213243194797542e-05, 1.213243194797542e-05, 1.213243194797542e-05, 1.213243194797542e-05, 1.213243194797542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.213243194797542e-05

Optimization complete. Final v2v error: 2.9630839824676514 mm

Highest mean error: 3.173274517059326 mm for frame 85

Lowest mean error: 2.7884676456451416 mm for frame 32

Saving results

Total time: 38.49914765357971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00299824
Iteration 2/25 | Loss: 0.00137438
Iteration 3/25 | Loss: 0.00122364
Iteration 4/25 | Loss: 0.00119745
Iteration 5/25 | Loss: 0.00119279
Iteration 6/25 | Loss: 0.00119279
Iteration 7/25 | Loss: 0.00119279
Iteration 8/25 | Loss: 0.00119279
Iteration 9/25 | Loss: 0.00119279
Iteration 10/25 | Loss: 0.00119279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011927903397008777, 0.0011927903397008777, 0.0011927903397008777, 0.0011927903397008777, 0.0011927903397008777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011927903397008777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43486845
Iteration 2/25 | Loss: 0.00062450
Iteration 3/25 | Loss: 0.00062450
Iteration 4/25 | Loss: 0.00062450
Iteration 5/25 | Loss: 0.00062450
Iteration 6/25 | Loss: 0.00062450
Iteration 7/25 | Loss: 0.00062450
Iteration 8/25 | Loss: 0.00062450
Iteration 9/25 | Loss: 0.00062450
Iteration 10/25 | Loss: 0.00062450
Iteration 11/25 | Loss: 0.00062450
Iteration 12/25 | Loss: 0.00062450
Iteration 13/25 | Loss: 0.00062450
Iteration 14/25 | Loss: 0.00062450
Iteration 15/25 | Loss: 0.00062450
Iteration 16/25 | Loss: 0.00062450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006244987598620355, 0.0006244987598620355, 0.0006244987598620355, 0.0006244987598620355, 0.0006244987598620355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006244987598620355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062450
Iteration 2/1000 | Loss: 0.00003953
Iteration 3/1000 | Loss: 0.00002627
Iteration 4/1000 | Loss: 0.00002424
Iteration 5/1000 | Loss: 0.00002307
Iteration 6/1000 | Loss: 0.00002190
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002000
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001890
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001880
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001870
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001863
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001862
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001860
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001859
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001859
Iteration 60/1000 | Loss: 0.00001859
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001856
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001849
Iteration 96/1000 | Loss: 0.00001849
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001848
Iteration 108/1000 | Loss: 0.00001848
Iteration 109/1000 | Loss: 0.00001848
Iteration 110/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.848235297075007e-05, 1.848235297075007e-05, 1.848235297075007e-05, 1.848235297075007e-05, 1.848235297075007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.848235297075007e-05

Optimization complete. Final v2v error: 3.6303553581237793 mm

Highest mean error: 3.9951043128967285 mm for frame 207

Lowest mean error: 3.219026565551758 mm for frame 5

Saving results

Total time: 39.69583463668823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987092
Iteration 2/25 | Loss: 0.00326610
Iteration 3/25 | Loss: 0.00248497
Iteration 4/25 | Loss: 0.00221970
Iteration 5/25 | Loss: 0.00227062
Iteration 6/25 | Loss: 0.00231492
Iteration 7/25 | Loss: 0.00208844
Iteration 8/25 | Loss: 0.00189589
Iteration 9/25 | Loss: 0.00189221
Iteration 10/25 | Loss: 0.00181160
Iteration 11/25 | Loss: 0.00178587
Iteration 12/25 | Loss: 0.00177675
Iteration 13/25 | Loss: 0.00176834
Iteration 14/25 | Loss: 0.00173742
Iteration 15/25 | Loss: 0.00177768
Iteration 16/25 | Loss: 0.00176499
Iteration 17/25 | Loss: 0.00175225
Iteration 18/25 | Loss: 0.00173062
Iteration 19/25 | Loss: 0.00171571
Iteration 20/25 | Loss: 0.00172675
Iteration 21/25 | Loss: 0.00172810
Iteration 22/25 | Loss: 0.00170038
Iteration 23/25 | Loss: 0.00169451
Iteration 24/25 | Loss: 0.00169225
Iteration 25/25 | Loss: 0.00169360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45030689
Iteration 2/25 | Loss: 0.00909446
Iteration 3/25 | Loss: 0.00601080
Iteration 4/25 | Loss: 0.00485495
Iteration 5/25 | Loss: 0.00483996
Iteration 6/25 | Loss: 0.00477215
Iteration 7/25 | Loss: 0.00477215
Iteration 8/25 | Loss: 0.00477215
Iteration 9/25 | Loss: 0.00477215
Iteration 10/25 | Loss: 0.00477215
Iteration 11/25 | Loss: 0.00477215
Iteration 12/25 | Loss: 0.00477215
Iteration 13/25 | Loss: 0.00477215
Iteration 14/25 | Loss: 0.00477215
Iteration 15/25 | Loss: 0.00477215
Iteration 16/25 | Loss: 0.00477215
Iteration 17/25 | Loss: 0.00477215
Iteration 18/25 | Loss: 0.00477215
Iteration 19/25 | Loss: 0.00477215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0047721476294100285, 0.0047721476294100285, 0.0047721476294100285, 0.0047721476294100285, 0.0047721476294100285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047721476294100285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00477215
Iteration 2/1000 | Loss: 0.00056960
Iteration 3/1000 | Loss: 0.00400377
Iteration 4/1000 | Loss: 0.00119198
Iteration 5/1000 | Loss: 0.00253126
Iteration 6/1000 | Loss: 0.00068782
Iteration 7/1000 | Loss: 0.00180988
Iteration 8/1000 | Loss: 0.00027801
Iteration 9/1000 | Loss: 0.00350938
Iteration 10/1000 | Loss: 0.00636881
Iteration 11/1000 | Loss: 0.00032203
Iteration 12/1000 | Loss: 0.00049901
Iteration 13/1000 | Loss: 0.00072996
Iteration 14/1000 | Loss: 0.00161294
Iteration 15/1000 | Loss: 0.00071184
Iteration 16/1000 | Loss: 0.00019880
Iteration 17/1000 | Loss: 0.00039240
Iteration 18/1000 | Loss: 0.00053924
Iteration 19/1000 | Loss: 0.00064616
Iteration 20/1000 | Loss: 0.00058091
Iteration 21/1000 | Loss: 0.00023229
Iteration 22/1000 | Loss: 0.00026355
Iteration 23/1000 | Loss: 0.00089760
Iteration 24/1000 | Loss: 0.00038367
Iteration 25/1000 | Loss: 0.00119326
Iteration 26/1000 | Loss: 0.00075486
Iteration 27/1000 | Loss: 0.00060663
Iteration 28/1000 | Loss: 0.00117108
Iteration 29/1000 | Loss: 0.00022811
Iteration 30/1000 | Loss: 0.00039622
Iteration 31/1000 | Loss: 0.00084133
Iteration 32/1000 | Loss: 0.00061796
Iteration 33/1000 | Loss: 0.00030350
Iteration 34/1000 | Loss: 0.00237848
Iteration 35/1000 | Loss: 0.00022701
Iteration 36/1000 | Loss: 0.00030880
Iteration 37/1000 | Loss: 0.00012508
Iteration 38/1000 | Loss: 0.00011707
Iteration 39/1000 | Loss: 0.00044535
Iteration 40/1000 | Loss: 0.00042365
Iteration 41/1000 | Loss: 0.00010744
Iteration 42/1000 | Loss: 0.00035952
Iteration 43/1000 | Loss: 0.00041399
Iteration 44/1000 | Loss: 0.00040289
Iteration 45/1000 | Loss: 0.00010473
Iteration 46/1000 | Loss: 0.00009805
Iteration 47/1000 | Loss: 0.00020322
Iteration 48/1000 | Loss: 0.00009508
Iteration 49/1000 | Loss: 0.00008943
Iteration 50/1000 | Loss: 0.00038500
Iteration 51/1000 | Loss: 0.00035474
Iteration 52/1000 | Loss: 0.00008569
Iteration 53/1000 | Loss: 0.00008023
Iteration 54/1000 | Loss: 0.00007745
Iteration 55/1000 | Loss: 0.00036714
Iteration 56/1000 | Loss: 0.00061798
Iteration 57/1000 | Loss: 0.00007531
Iteration 58/1000 | Loss: 0.00007214
Iteration 59/1000 | Loss: 0.00006967
Iteration 60/1000 | Loss: 0.00006776
Iteration 61/1000 | Loss: 0.00006625
Iteration 62/1000 | Loss: 0.00034384
Iteration 63/1000 | Loss: 0.00033651
Iteration 64/1000 | Loss: 0.00047013
Iteration 65/1000 | Loss: 0.00015612
Iteration 66/1000 | Loss: 0.00007621
Iteration 67/1000 | Loss: 0.00007164
Iteration 68/1000 | Loss: 0.00006765
Iteration 69/1000 | Loss: 0.00006560
Iteration 70/1000 | Loss: 0.00099088
Iteration 71/1000 | Loss: 0.00043163
Iteration 72/1000 | Loss: 0.00043980
Iteration 73/1000 | Loss: 0.00006708
Iteration 74/1000 | Loss: 0.00006329
Iteration 75/1000 | Loss: 0.00006078
Iteration 76/1000 | Loss: 0.00005889
Iteration 77/1000 | Loss: 0.00005781
Iteration 78/1000 | Loss: 0.00005690
Iteration 79/1000 | Loss: 0.00005615
Iteration 80/1000 | Loss: 0.00005565
Iteration 81/1000 | Loss: 0.00005517
Iteration 82/1000 | Loss: 0.00055987
Iteration 83/1000 | Loss: 0.00023442
Iteration 84/1000 | Loss: 0.00005659
Iteration 85/1000 | Loss: 0.00006435
Iteration 86/1000 | Loss: 0.00006061
Iteration 87/1000 | Loss: 0.00041643
Iteration 88/1000 | Loss: 0.00106380
Iteration 89/1000 | Loss: 0.00092832
Iteration 90/1000 | Loss: 0.00006490
Iteration 91/1000 | Loss: 0.00058853
Iteration 92/1000 | Loss: 0.00128111
Iteration 93/1000 | Loss: 0.00006349
Iteration 94/1000 | Loss: 0.00005782
Iteration 95/1000 | Loss: 0.00005397
Iteration 96/1000 | Loss: 0.00005226
Iteration 97/1000 | Loss: 0.00005060
Iteration 98/1000 | Loss: 0.00004953
Iteration 99/1000 | Loss: 0.00004866
Iteration 100/1000 | Loss: 0.00004803
Iteration 101/1000 | Loss: 0.00004762
Iteration 102/1000 | Loss: 0.00048831
Iteration 103/1000 | Loss: 0.00020681
Iteration 104/1000 | Loss: 0.00004755
Iteration 105/1000 | Loss: 0.00004708
Iteration 106/1000 | Loss: 0.00004702
Iteration 107/1000 | Loss: 0.00004697
Iteration 108/1000 | Loss: 0.00004692
Iteration 109/1000 | Loss: 0.00004675
Iteration 110/1000 | Loss: 0.00048835
Iteration 111/1000 | Loss: 0.00005283
Iteration 112/1000 | Loss: 0.00004693
Iteration 113/1000 | Loss: 0.00004550
Iteration 114/1000 | Loss: 0.00004460
Iteration 115/1000 | Loss: 0.00004397
Iteration 116/1000 | Loss: 0.00004367
Iteration 117/1000 | Loss: 0.00004341
Iteration 118/1000 | Loss: 0.00004334
Iteration 119/1000 | Loss: 0.00004316
Iteration 120/1000 | Loss: 0.00004303
Iteration 121/1000 | Loss: 0.00004300
Iteration 122/1000 | Loss: 0.00004298
Iteration 123/1000 | Loss: 0.00004296
Iteration 124/1000 | Loss: 0.00004295
Iteration 125/1000 | Loss: 0.00004294
Iteration 126/1000 | Loss: 0.00004291
Iteration 127/1000 | Loss: 0.00004288
Iteration 128/1000 | Loss: 0.00004287
Iteration 129/1000 | Loss: 0.00004286
Iteration 130/1000 | Loss: 0.00004286
Iteration 131/1000 | Loss: 0.00004286
Iteration 132/1000 | Loss: 0.00004286
Iteration 133/1000 | Loss: 0.00004284
Iteration 134/1000 | Loss: 0.00004284
Iteration 135/1000 | Loss: 0.00004265
Iteration 136/1000 | Loss: 0.00004247
Iteration 137/1000 | Loss: 0.00004229
Iteration 138/1000 | Loss: 0.00004222
Iteration 139/1000 | Loss: 0.00004205
Iteration 140/1000 | Loss: 0.00004180
Iteration 141/1000 | Loss: 0.00004161
Iteration 142/1000 | Loss: 0.00004160
Iteration 143/1000 | Loss: 0.00004154
Iteration 144/1000 | Loss: 0.00004154
Iteration 145/1000 | Loss: 0.00004153
Iteration 146/1000 | Loss: 0.00004147
Iteration 147/1000 | Loss: 0.00004142
Iteration 148/1000 | Loss: 0.00004138
Iteration 149/1000 | Loss: 0.00004137
Iteration 150/1000 | Loss: 0.00004137
Iteration 151/1000 | Loss: 0.00004134
Iteration 152/1000 | Loss: 0.00004134
Iteration 153/1000 | Loss: 0.00004133
Iteration 154/1000 | Loss: 0.00004133
Iteration 155/1000 | Loss: 0.00004133
Iteration 156/1000 | Loss: 0.00004133
Iteration 157/1000 | Loss: 0.00004133
Iteration 158/1000 | Loss: 0.00004132
Iteration 159/1000 | Loss: 0.00004132
Iteration 160/1000 | Loss: 0.00004132
Iteration 161/1000 | Loss: 0.00004132
Iteration 162/1000 | Loss: 0.00004131
Iteration 163/1000 | Loss: 0.00004131
Iteration 164/1000 | Loss: 0.00004131
Iteration 165/1000 | Loss: 0.00004131
Iteration 166/1000 | Loss: 0.00004130
Iteration 167/1000 | Loss: 0.00004130
Iteration 168/1000 | Loss: 0.00004130
Iteration 169/1000 | Loss: 0.00004129
Iteration 170/1000 | Loss: 0.00004129
Iteration 171/1000 | Loss: 0.00004128
Iteration 172/1000 | Loss: 0.00004128
Iteration 173/1000 | Loss: 0.00004128
Iteration 174/1000 | Loss: 0.00004128
Iteration 175/1000 | Loss: 0.00004128
Iteration 176/1000 | Loss: 0.00004127
Iteration 177/1000 | Loss: 0.00004127
Iteration 178/1000 | Loss: 0.00004127
Iteration 179/1000 | Loss: 0.00004127
Iteration 180/1000 | Loss: 0.00004126
Iteration 181/1000 | Loss: 0.00004126
Iteration 182/1000 | Loss: 0.00004125
Iteration 183/1000 | Loss: 0.00004125
Iteration 184/1000 | Loss: 0.00004125
Iteration 185/1000 | Loss: 0.00004124
Iteration 186/1000 | Loss: 0.00004123
Iteration 187/1000 | Loss: 0.00004122
Iteration 188/1000 | Loss: 0.00004122
Iteration 189/1000 | Loss: 0.00004122
Iteration 190/1000 | Loss: 0.00004122
Iteration 191/1000 | Loss: 0.00004121
Iteration 192/1000 | Loss: 0.00004121
Iteration 193/1000 | Loss: 0.00004121
Iteration 194/1000 | Loss: 0.00004121
Iteration 195/1000 | Loss: 0.00004121
Iteration 196/1000 | Loss: 0.00004121
Iteration 197/1000 | Loss: 0.00004120
Iteration 198/1000 | Loss: 0.00004120
Iteration 199/1000 | Loss: 0.00004119
Iteration 200/1000 | Loss: 0.00004119
Iteration 201/1000 | Loss: 0.00004119
Iteration 202/1000 | Loss: 0.00004119
Iteration 203/1000 | Loss: 0.00004119
Iteration 204/1000 | Loss: 0.00004119
Iteration 205/1000 | Loss: 0.00004119
Iteration 206/1000 | Loss: 0.00004119
Iteration 207/1000 | Loss: 0.00004119
Iteration 208/1000 | Loss: 0.00004119
Iteration 209/1000 | Loss: 0.00004119
Iteration 210/1000 | Loss: 0.00004118
Iteration 211/1000 | Loss: 0.00004118
Iteration 212/1000 | Loss: 0.00004118
Iteration 213/1000 | Loss: 0.00004118
Iteration 214/1000 | Loss: 0.00004118
Iteration 215/1000 | Loss: 0.00004117
Iteration 216/1000 | Loss: 0.00004117
Iteration 217/1000 | Loss: 0.00004117
Iteration 218/1000 | Loss: 0.00004117
Iteration 219/1000 | Loss: 0.00004117
Iteration 220/1000 | Loss: 0.00004116
Iteration 221/1000 | Loss: 0.00004116
Iteration 222/1000 | Loss: 0.00004116
Iteration 223/1000 | Loss: 0.00004116
Iteration 224/1000 | Loss: 0.00004116
Iteration 225/1000 | Loss: 0.00004116
Iteration 226/1000 | Loss: 0.00004116
Iteration 227/1000 | Loss: 0.00004116
Iteration 228/1000 | Loss: 0.00004115
Iteration 229/1000 | Loss: 0.00004115
Iteration 230/1000 | Loss: 0.00004115
Iteration 231/1000 | Loss: 0.00004115
Iteration 232/1000 | Loss: 0.00004115
Iteration 233/1000 | Loss: 0.00004114
Iteration 234/1000 | Loss: 0.00004114
Iteration 235/1000 | Loss: 0.00004114
Iteration 236/1000 | Loss: 0.00004114
Iteration 237/1000 | Loss: 0.00004114
Iteration 238/1000 | Loss: 0.00004114
Iteration 239/1000 | Loss: 0.00004114
Iteration 240/1000 | Loss: 0.00004114
Iteration 241/1000 | Loss: 0.00004114
Iteration 242/1000 | Loss: 0.00004114
Iteration 243/1000 | Loss: 0.00004114
Iteration 244/1000 | Loss: 0.00004114
Iteration 245/1000 | Loss: 0.00004113
Iteration 246/1000 | Loss: 0.00004113
Iteration 247/1000 | Loss: 0.00004113
Iteration 248/1000 | Loss: 0.00004113
Iteration 249/1000 | Loss: 0.00004112
Iteration 250/1000 | Loss: 0.00004112
Iteration 251/1000 | Loss: 0.00004112
Iteration 252/1000 | Loss: 0.00004112
Iteration 253/1000 | Loss: 0.00004112
Iteration 254/1000 | Loss: 0.00004111
Iteration 255/1000 | Loss: 0.00004111
Iteration 256/1000 | Loss: 0.00004111
Iteration 257/1000 | Loss: 0.00004111
Iteration 258/1000 | Loss: 0.00004111
Iteration 259/1000 | Loss: 0.00004111
Iteration 260/1000 | Loss: 0.00004111
Iteration 261/1000 | Loss: 0.00004111
Iteration 262/1000 | Loss: 0.00004111
Iteration 263/1000 | Loss: 0.00004111
Iteration 264/1000 | Loss: 0.00004111
Iteration 265/1000 | Loss: 0.00004111
Iteration 266/1000 | Loss: 0.00004111
Iteration 267/1000 | Loss: 0.00004111
Iteration 268/1000 | Loss: 0.00004111
Iteration 269/1000 | Loss: 0.00004111
Iteration 270/1000 | Loss: 0.00004111
Iteration 271/1000 | Loss: 0.00004110
Iteration 272/1000 | Loss: 0.00004110
Iteration 273/1000 | Loss: 0.00004110
Iteration 274/1000 | Loss: 0.00004110
Iteration 275/1000 | Loss: 0.00004110
Iteration 276/1000 | Loss: 0.00004110
Iteration 277/1000 | Loss: 0.00004110
Iteration 278/1000 | Loss: 0.00004110
Iteration 279/1000 | Loss: 0.00004110
Iteration 280/1000 | Loss: 0.00004110
Iteration 281/1000 | Loss: 0.00004110
Iteration 282/1000 | Loss: 0.00004110
Iteration 283/1000 | Loss: 0.00004110
Iteration 284/1000 | Loss: 0.00004110
Iteration 285/1000 | Loss: 0.00004110
Iteration 286/1000 | Loss: 0.00004109
Iteration 287/1000 | Loss: 0.00004109
Iteration 288/1000 | Loss: 0.00004109
Iteration 289/1000 | Loss: 0.00004109
Iteration 290/1000 | Loss: 0.00004109
Iteration 291/1000 | Loss: 0.00004109
Iteration 292/1000 | Loss: 0.00004109
Iteration 293/1000 | Loss: 0.00004109
Iteration 294/1000 | Loss: 0.00004109
Iteration 295/1000 | Loss: 0.00004109
Iteration 296/1000 | Loss: 0.00004109
Iteration 297/1000 | Loss: 0.00004109
Iteration 298/1000 | Loss: 0.00004109
Iteration 299/1000 | Loss: 0.00004109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [4.109267320018262e-05, 4.109267320018262e-05, 4.109267320018262e-05, 4.109267320018262e-05, 4.109267320018262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.109267320018262e-05

Optimization complete. Final v2v error: 4.144713401794434 mm

Highest mean error: 12.871999740600586 mm for frame 96

Lowest mean error: 3.0677011013031006 mm for frame 199

Saving results

Total time: 260.42248344421387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994926
Iteration 2/25 | Loss: 0.00169942
Iteration 3/25 | Loss: 0.00141235
Iteration 4/25 | Loss: 0.00138901
Iteration 5/25 | Loss: 0.00138209
Iteration 6/25 | Loss: 0.00138091
Iteration 7/25 | Loss: 0.00138091
Iteration 8/25 | Loss: 0.00138091
Iteration 9/25 | Loss: 0.00138091
Iteration 10/25 | Loss: 0.00138091
Iteration 11/25 | Loss: 0.00138091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013809066731482744, 0.0013809066731482744, 0.0013809066731482744, 0.0013809066731482744, 0.0013809066731482744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013809066731482744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04667664
Iteration 2/25 | Loss: 0.00104064
Iteration 3/25 | Loss: 0.00104062
Iteration 4/25 | Loss: 0.00104062
Iteration 5/25 | Loss: 0.00104062
Iteration 6/25 | Loss: 0.00104062
Iteration 7/25 | Loss: 0.00104062
Iteration 8/25 | Loss: 0.00104062
Iteration 9/25 | Loss: 0.00104062
Iteration 10/25 | Loss: 0.00104062
Iteration 11/25 | Loss: 0.00104062
Iteration 12/25 | Loss: 0.00104062
Iteration 13/25 | Loss: 0.00104062
Iteration 14/25 | Loss: 0.00104062
Iteration 15/25 | Loss: 0.00104062
Iteration 16/25 | Loss: 0.00104062
Iteration 17/25 | Loss: 0.00104062
Iteration 18/25 | Loss: 0.00104062
Iteration 19/25 | Loss: 0.00104062
Iteration 20/25 | Loss: 0.00104062
Iteration 21/25 | Loss: 0.00104062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001040616654790938, 0.001040616654790938, 0.001040616654790938, 0.001040616654790938, 0.001040616654790938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001040616654790938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104062
Iteration 2/1000 | Loss: 0.00006007
Iteration 3/1000 | Loss: 0.00003882
Iteration 4/1000 | Loss: 0.00003274
Iteration 5/1000 | Loss: 0.00003065
Iteration 6/1000 | Loss: 0.00002942
Iteration 7/1000 | Loss: 0.00002853
Iteration 8/1000 | Loss: 0.00002785
Iteration 9/1000 | Loss: 0.00002738
Iteration 10/1000 | Loss: 0.00002697
Iteration 11/1000 | Loss: 0.00002662
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002616
Iteration 14/1000 | Loss: 0.00002597
Iteration 15/1000 | Loss: 0.00002587
Iteration 16/1000 | Loss: 0.00002584
Iteration 17/1000 | Loss: 0.00002583
Iteration 18/1000 | Loss: 0.00002582
Iteration 19/1000 | Loss: 0.00002582
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00002579
Iteration 22/1000 | Loss: 0.00002579
Iteration 23/1000 | Loss: 0.00002578
Iteration 24/1000 | Loss: 0.00002578
Iteration 25/1000 | Loss: 0.00002574
Iteration 26/1000 | Loss: 0.00002569
Iteration 27/1000 | Loss: 0.00002566
Iteration 28/1000 | Loss: 0.00002562
Iteration 29/1000 | Loss: 0.00002561
Iteration 30/1000 | Loss: 0.00002561
Iteration 31/1000 | Loss: 0.00002560
Iteration 32/1000 | Loss: 0.00002560
Iteration 33/1000 | Loss: 0.00002559
Iteration 34/1000 | Loss: 0.00002559
Iteration 35/1000 | Loss: 0.00002558
Iteration 36/1000 | Loss: 0.00002558
Iteration 37/1000 | Loss: 0.00002555
Iteration 38/1000 | Loss: 0.00002554
Iteration 39/1000 | Loss: 0.00002553
Iteration 40/1000 | Loss: 0.00002553
Iteration 41/1000 | Loss: 0.00002551
Iteration 42/1000 | Loss: 0.00002551
Iteration 43/1000 | Loss: 0.00002550
Iteration 44/1000 | Loss: 0.00002550
Iteration 45/1000 | Loss: 0.00002549
Iteration 46/1000 | Loss: 0.00002548
Iteration 47/1000 | Loss: 0.00002544
Iteration 48/1000 | Loss: 0.00002543
Iteration 49/1000 | Loss: 0.00002543
Iteration 50/1000 | Loss: 0.00002542
Iteration 51/1000 | Loss: 0.00002542
Iteration 52/1000 | Loss: 0.00002542
Iteration 53/1000 | Loss: 0.00002539
Iteration 54/1000 | Loss: 0.00002537
Iteration 55/1000 | Loss: 0.00002536
Iteration 56/1000 | Loss: 0.00002536
Iteration 57/1000 | Loss: 0.00002536
Iteration 58/1000 | Loss: 0.00002536
Iteration 59/1000 | Loss: 0.00002535
Iteration 60/1000 | Loss: 0.00002535
Iteration 61/1000 | Loss: 0.00002535
Iteration 62/1000 | Loss: 0.00002534
Iteration 63/1000 | Loss: 0.00002534
Iteration 64/1000 | Loss: 0.00002534
Iteration 65/1000 | Loss: 0.00002534
Iteration 66/1000 | Loss: 0.00002534
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002533
Iteration 70/1000 | Loss: 0.00002533
Iteration 71/1000 | Loss: 0.00002533
Iteration 72/1000 | Loss: 0.00002532
Iteration 73/1000 | Loss: 0.00002532
Iteration 74/1000 | Loss: 0.00002532
Iteration 75/1000 | Loss: 0.00002532
Iteration 76/1000 | Loss: 0.00002531
Iteration 77/1000 | Loss: 0.00002531
Iteration 78/1000 | Loss: 0.00002530
Iteration 79/1000 | Loss: 0.00002530
Iteration 80/1000 | Loss: 0.00002530
Iteration 81/1000 | Loss: 0.00002530
Iteration 82/1000 | Loss: 0.00002529
Iteration 83/1000 | Loss: 0.00002529
Iteration 84/1000 | Loss: 0.00002529
Iteration 85/1000 | Loss: 0.00002529
Iteration 86/1000 | Loss: 0.00002529
Iteration 87/1000 | Loss: 0.00002528
Iteration 88/1000 | Loss: 0.00002527
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002526
Iteration 92/1000 | Loss: 0.00002526
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002524
Iteration 97/1000 | Loss: 0.00002524
Iteration 98/1000 | Loss: 0.00002524
Iteration 99/1000 | Loss: 0.00002524
Iteration 100/1000 | Loss: 0.00002524
Iteration 101/1000 | Loss: 0.00002524
Iteration 102/1000 | Loss: 0.00002524
Iteration 103/1000 | Loss: 0.00002524
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002523
Iteration 106/1000 | Loss: 0.00002523
Iteration 107/1000 | Loss: 0.00002523
Iteration 108/1000 | Loss: 0.00002523
Iteration 109/1000 | Loss: 0.00002523
Iteration 110/1000 | Loss: 0.00002522
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002522
Iteration 115/1000 | Loss: 0.00002522
Iteration 116/1000 | Loss: 0.00002522
Iteration 117/1000 | Loss: 0.00002522
Iteration 118/1000 | Loss: 0.00002522
Iteration 119/1000 | Loss: 0.00002522
Iteration 120/1000 | Loss: 0.00002521
Iteration 121/1000 | Loss: 0.00002521
Iteration 122/1000 | Loss: 0.00002521
Iteration 123/1000 | Loss: 0.00002521
Iteration 124/1000 | Loss: 0.00002521
Iteration 125/1000 | Loss: 0.00002521
Iteration 126/1000 | Loss: 0.00002521
Iteration 127/1000 | Loss: 0.00002520
Iteration 128/1000 | Loss: 0.00002520
Iteration 129/1000 | Loss: 0.00002520
Iteration 130/1000 | Loss: 0.00002520
Iteration 131/1000 | Loss: 0.00002520
Iteration 132/1000 | Loss: 0.00002520
Iteration 133/1000 | Loss: 0.00002520
Iteration 134/1000 | Loss: 0.00002520
Iteration 135/1000 | Loss: 0.00002519
Iteration 136/1000 | Loss: 0.00002519
Iteration 137/1000 | Loss: 0.00002519
Iteration 138/1000 | Loss: 0.00002519
Iteration 139/1000 | Loss: 0.00002519
Iteration 140/1000 | Loss: 0.00002519
Iteration 141/1000 | Loss: 0.00002519
Iteration 142/1000 | Loss: 0.00002519
Iteration 143/1000 | Loss: 0.00002519
Iteration 144/1000 | Loss: 0.00002519
Iteration 145/1000 | Loss: 0.00002519
Iteration 146/1000 | Loss: 0.00002518
Iteration 147/1000 | Loss: 0.00002518
Iteration 148/1000 | Loss: 0.00002518
Iteration 149/1000 | Loss: 0.00002518
Iteration 150/1000 | Loss: 0.00002518
Iteration 151/1000 | Loss: 0.00002518
Iteration 152/1000 | Loss: 0.00002518
Iteration 153/1000 | Loss: 0.00002518
Iteration 154/1000 | Loss: 0.00002518
Iteration 155/1000 | Loss: 0.00002518
Iteration 156/1000 | Loss: 0.00002518
Iteration 157/1000 | Loss: 0.00002518
Iteration 158/1000 | Loss: 0.00002518
Iteration 159/1000 | Loss: 0.00002518
Iteration 160/1000 | Loss: 0.00002518
Iteration 161/1000 | Loss: 0.00002517
Iteration 162/1000 | Loss: 0.00002517
Iteration 163/1000 | Loss: 0.00002517
Iteration 164/1000 | Loss: 0.00002517
Iteration 165/1000 | Loss: 0.00002517
Iteration 166/1000 | Loss: 0.00002517
Iteration 167/1000 | Loss: 0.00002517
Iteration 168/1000 | Loss: 0.00002517
Iteration 169/1000 | Loss: 0.00002517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.5174045731546357e-05, 2.5174045731546357e-05, 2.5174045731546357e-05, 2.5174045731546357e-05, 2.5174045731546357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5174045731546357e-05

Optimization complete. Final v2v error: 4.178895950317383 mm

Highest mean error: 4.994568347930908 mm for frame 76

Lowest mean error: 3.6119766235351562 mm for frame 38

Saving results

Total time: 47.75794243812561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795646
Iteration 2/25 | Loss: 0.00167684
Iteration 3/25 | Loss: 0.00137163
Iteration 4/25 | Loss: 0.00134203
Iteration 5/25 | Loss: 0.00133348
Iteration 6/25 | Loss: 0.00133060
Iteration 7/25 | Loss: 0.00133023
Iteration 8/25 | Loss: 0.00133023
Iteration 9/25 | Loss: 0.00133023
Iteration 10/25 | Loss: 0.00133023
Iteration 11/25 | Loss: 0.00133023
Iteration 12/25 | Loss: 0.00133023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013302273582667112, 0.0013302273582667112, 0.0013302273582667112, 0.0013302273582667112, 0.0013302273582667112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013302273582667112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19516206
Iteration 2/25 | Loss: 0.00089883
Iteration 3/25 | Loss: 0.00089882
Iteration 4/25 | Loss: 0.00089882
Iteration 5/25 | Loss: 0.00089882
Iteration 6/25 | Loss: 0.00089882
Iteration 7/25 | Loss: 0.00089882
Iteration 8/25 | Loss: 0.00089882
Iteration 9/25 | Loss: 0.00089882
Iteration 10/25 | Loss: 0.00089882
Iteration 11/25 | Loss: 0.00089882
Iteration 12/25 | Loss: 0.00089882
Iteration 13/25 | Loss: 0.00089882
Iteration 14/25 | Loss: 0.00089882
Iteration 15/25 | Loss: 0.00089882
Iteration 16/25 | Loss: 0.00089882
Iteration 17/25 | Loss: 0.00089882
Iteration 18/25 | Loss: 0.00089882
Iteration 19/25 | Loss: 0.00089882
Iteration 20/25 | Loss: 0.00089882
Iteration 21/25 | Loss: 0.00089882
Iteration 22/25 | Loss: 0.00089882
Iteration 23/25 | Loss: 0.00089882
Iteration 24/25 | Loss: 0.00089882
Iteration 25/25 | Loss: 0.00089882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089882
Iteration 2/1000 | Loss: 0.00008739
Iteration 3/1000 | Loss: 0.00005465
Iteration 4/1000 | Loss: 0.00004292
Iteration 5/1000 | Loss: 0.00004019
Iteration 6/1000 | Loss: 0.00003814
Iteration 7/1000 | Loss: 0.00003676
Iteration 8/1000 | Loss: 0.00003565
Iteration 9/1000 | Loss: 0.00003492
Iteration 10/1000 | Loss: 0.00003444
Iteration 11/1000 | Loss: 0.00003402
Iteration 12/1000 | Loss: 0.00003360
Iteration 13/1000 | Loss: 0.00003336
Iteration 14/1000 | Loss: 0.00003314
Iteration 15/1000 | Loss: 0.00003290
Iteration 16/1000 | Loss: 0.00003276
Iteration 17/1000 | Loss: 0.00003275
Iteration 18/1000 | Loss: 0.00003267
Iteration 19/1000 | Loss: 0.00003258
Iteration 20/1000 | Loss: 0.00003252
Iteration 21/1000 | Loss: 0.00003251
Iteration 22/1000 | Loss: 0.00003250
Iteration 23/1000 | Loss: 0.00003249
Iteration 24/1000 | Loss: 0.00003245
Iteration 25/1000 | Loss: 0.00003244
Iteration 26/1000 | Loss: 0.00003244
Iteration 27/1000 | Loss: 0.00003244
Iteration 28/1000 | Loss: 0.00003244
Iteration 29/1000 | Loss: 0.00003242
Iteration 30/1000 | Loss: 0.00003242
Iteration 31/1000 | Loss: 0.00003242
Iteration 32/1000 | Loss: 0.00003242
Iteration 33/1000 | Loss: 0.00003241
Iteration 34/1000 | Loss: 0.00003241
Iteration 35/1000 | Loss: 0.00003240
Iteration 36/1000 | Loss: 0.00003240
Iteration 37/1000 | Loss: 0.00003240
Iteration 38/1000 | Loss: 0.00003239
Iteration 39/1000 | Loss: 0.00003239
Iteration 40/1000 | Loss: 0.00003239
Iteration 41/1000 | Loss: 0.00003238
Iteration 42/1000 | Loss: 0.00003238
Iteration 43/1000 | Loss: 0.00003237
Iteration 44/1000 | Loss: 0.00003237
Iteration 45/1000 | Loss: 0.00003237
Iteration 46/1000 | Loss: 0.00003236
Iteration 47/1000 | Loss: 0.00003236
Iteration 48/1000 | Loss: 0.00003236
Iteration 49/1000 | Loss: 0.00003236
Iteration 50/1000 | Loss: 0.00003235
Iteration 51/1000 | Loss: 0.00003235
Iteration 52/1000 | Loss: 0.00003235
Iteration 53/1000 | Loss: 0.00003235
Iteration 54/1000 | Loss: 0.00003234
Iteration 55/1000 | Loss: 0.00003234
Iteration 56/1000 | Loss: 0.00003233
Iteration 57/1000 | Loss: 0.00003233
Iteration 58/1000 | Loss: 0.00003232
Iteration 59/1000 | Loss: 0.00003232
Iteration 60/1000 | Loss: 0.00003232
Iteration 61/1000 | Loss: 0.00003232
Iteration 62/1000 | Loss: 0.00003232
Iteration 63/1000 | Loss: 0.00003231
Iteration 64/1000 | Loss: 0.00003231
Iteration 65/1000 | Loss: 0.00003231
Iteration 66/1000 | Loss: 0.00003230
Iteration 67/1000 | Loss: 0.00003230
Iteration 68/1000 | Loss: 0.00003230
Iteration 69/1000 | Loss: 0.00003230
Iteration 70/1000 | Loss: 0.00003230
Iteration 71/1000 | Loss: 0.00003230
Iteration 72/1000 | Loss: 0.00003229
Iteration 73/1000 | Loss: 0.00003229
Iteration 74/1000 | Loss: 0.00003229
Iteration 75/1000 | Loss: 0.00003229
Iteration 76/1000 | Loss: 0.00003229
Iteration 77/1000 | Loss: 0.00003229
Iteration 78/1000 | Loss: 0.00003229
Iteration 79/1000 | Loss: 0.00003229
Iteration 80/1000 | Loss: 0.00003229
Iteration 81/1000 | Loss: 0.00003228
Iteration 82/1000 | Loss: 0.00003227
Iteration 83/1000 | Loss: 0.00003227
Iteration 84/1000 | Loss: 0.00003226
Iteration 85/1000 | Loss: 0.00003226
Iteration 86/1000 | Loss: 0.00003226
Iteration 87/1000 | Loss: 0.00003226
Iteration 88/1000 | Loss: 0.00003226
Iteration 89/1000 | Loss: 0.00003226
Iteration 90/1000 | Loss: 0.00003226
Iteration 91/1000 | Loss: 0.00003226
Iteration 92/1000 | Loss: 0.00003226
Iteration 93/1000 | Loss: 0.00003226
Iteration 94/1000 | Loss: 0.00003225
Iteration 95/1000 | Loss: 0.00003225
Iteration 96/1000 | Loss: 0.00003225
Iteration 97/1000 | Loss: 0.00003224
Iteration 98/1000 | Loss: 0.00003224
Iteration 99/1000 | Loss: 0.00003224
Iteration 100/1000 | Loss: 0.00003224
Iteration 101/1000 | Loss: 0.00003224
Iteration 102/1000 | Loss: 0.00003223
Iteration 103/1000 | Loss: 0.00003223
Iteration 104/1000 | Loss: 0.00003223
Iteration 105/1000 | Loss: 0.00003223
Iteration 106/1000 | Loss: 0.00003222
Iteration 107/1000 | Loss: 0.00003222
Iteration 108/1000 | Loss: 0.00003222
Iteration 109/1000 | Loss: 0.00003222
Iteration 110/1000 | Loss: 0.00003222
Iteration 111/1000 | Loss: 0.00003222
Iteration 112/1000 | Loss: 0.00003222
Iteration 113/1000 | Loss: 0.00003221
Iteration 114/1000 | Loss: 0.00003221
Iteration 115/1000 | Loss: 0.00003221
Iteration 116/1000 | Loss: 0.00003221
Iteration 117/1000 | Loss: 0.00003221
Iteration 118/1000 | Loss: 0.00003221
Iteration 119/1000 | Loss: 0.00003221
Iteration 120/1000 | Loss: 0.00003221
Iteration 121/1000 | Loss: 0.00003221
Iteration 122/1000 | Loss: 0.00003221
Iteration 123/1000 | Loss: 0.00003220
Iteration 124/1000 | Loss: 0.00003220
Iteration 125/1000 | Loss: 0.00003220
Iteration 126/1000 | Loss: 0.00003220
Iteration 127/1000 | Loss: 0.00003220
Iteration 128/1000 | Loss: 0.00003220
Iteration 129/1000 | Loss: 0.00003220
Iteration 130/1000 | Loss: 0.00003220
Iteration 131/1000 | Loss: 0.00003220
Iteration 132/1000 | Loss: 0.00003220
Iteration 133/1000 | Loss: 0.00003219
Iteration 134/1000 | Loss: 0.00003219
Iteration 135/1000 | Loss: 0.00003219
Iteration 136/1000 | Loss: 0.00003219
Iteration 137/1000 | Loss: 0.00003219
Iteration 138/1000 | Loss: 0.00003219
Iteration 139/1000 | Loss: 0.00003219
Iteration 140/1000 | Loss: 0.00003219
Iteration 141/1000 | Loss: 0.00003218
Iteration 142/1000 | Loss: 0.00003218
Iteration 143/1000 | Loss: 0.00003218
Iteration 144/1000 | Loss: 0.00003218
Iteration 145/1000 | Loss: 0.00003218
Iteration 146/1000 | Loss: 0.00003217
Iteration 147/1000 | Loss: 0.00003217
Iteration 148/1000 | Loss: 0.00003217
Iteration 149/1000 | Loss: 0.00003217
Iteration 150/1000 | Loss: 0.00003217
Iteration 151/1000 | Loss: 0.00003217
Iteration 152/1000 | Loss: 0.00003217
Iteration 153/1000 | Loss: 0.00003217
Iteration 154/1000 | Loss: 0.00003217
Iteration 155/1000 | Loss: 0.00003217
Iteration 156/1000 | Loss: 0.00003216
Iteration 157/1000 | Loss: 0.00003216
Iteration 158/1000 | Loss: 0.00003216
Iteration 159/1000 | Loss: 0.00003216
Iteration 160/1000 | Loss: 0.00003216
Iteration 161/1000 | Loss: 0.00003216
Iteration 162/1000 | Loss: 0.00003216
Iteration 163/1000 | Loss: 0.00003216
Iteration 164/1000 | Loss: 0.00003216
Iteration 165/1000 | Loss: 0.00003215
Iteration 166/1000 | Loss: 0.00003215
Iteration 167/1000 | Loss: 0.00003215
Iteration 168/1000 | Loss: 0.00003215
Iteration 169/1000 | Loss: 0.00003215
Iteration 170/1000 | Loss: 0.00003215
Iteration 171/1000 | Loss: 0.00003215
Iteration 172/1000 | Loss: 0.00003215
Iteration 173/1000 | Loss: 0.00003215
Iteration 174/1000 | Loss: 0.00003215
Iteration 175/1000 | Loss: 0.00003215
Iteration 176/1000 | Loss: 0.00003214
Iteration 177/1000 | Loss: 0.00003214
Iteration 178/1000 | Loss: 0.00003214
Iteration 179/1000 | Loss: 0.00003214
Iteration 180/1000 | Loss: 0.00003214
Iteration 181/1000 | Loss: 0.00003214
Iteration 182/1000 | Loss: 0.00003214
Iteration 183/1000 | Loss: 0.00003214
Iteration 184/1000 | Loss: 0.00003214
Iteration 185/1000 | Loss: 0.00003214
Iteration 186/1000 | Loss: 0.00003214
Iteration 187/1000 | Loss: 0.00003214
Iteration 188/1000 | Loss: 0.00003214
Iteration 189/1000 | Loss: 0.00003214
Iteration 190/1000 | Loss: 0.00003214
Iteration 191/1000 | Loss: 0.00003214
Iteration 192/1000 | Loss: 0.00003214
Iteration 193/1000 | Loss: 0.00003214
Iteration 194/1000 | Loss: 0.00003214
Iteration 195/1000 | Loss: 0.00003214
Iteration 196/1000 | Loss: 0.00003213
Iteration 197/1000 | Loss: 0.00003213
Iteration 198/1000 | Loss: 0.00003213
Iteration 199/1000 | Loss: 0.00003213
Iteration 200/1000 | Loss: 0.00003213
Iteration 201/1000 | Loss: 0.00003213
Iteration 202/1000 | Loss: 0.00003213
Iteration 203/1000 | Loss: 0.00003213
Iteration 204/1000 | Loss: 0.00003213
Iteration 205/1000 | Loss: 0.00003213
Iteration 206/1000 | Loss: 0.00003213
Iteration 207/1000 | Loss: 0.00003213
Iteration 208/1000 | Loss: 0.00003213
Iteration 209/1000 | Loss: 0.00003213
Iteration 210/1000 | Loss: 0.00003213
Iteration 211/1000 | Loss: 0.00003213
Iteration 212/1000 | Loss: 0.00003213
Iteration 213/1000 | Loss: 0.00003213
Iteration 214/1000 | Loss: 0.00003213
Iteration 215/1000 | Loss: 0.00003213
Iteration 216/1000 | Loss: 0.00003213
Iteration 217/1000 | Loss: 0.00003213
Iteration 218/1000 | Loss: 0.00003213
Iteration 219/1000 | Loss: 0.00003213
Iteration 220/1000 | Loss: 0.00003213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [3.213420859538019e-05, 3.213420859538019e-05, 3.213420859538019e-05, 3.213420859538019e-05, 3.213420859538019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.213420859538019e-05

Optimization complete. Final v2v error: 4.61161994934082 mm

Highest mean error: 5.446691036224365 mm for frame 158

Lowest mean error: 3.2147576808929443 mm for frame 197

Saving results

Total time: 52.27832841873169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506885
Iteration 2/25 | Loss: 0.00147446
Iteration 3/25 | Loss: 0.00129266
Iteration 4/25 | Loss: 0.00126972
Iteration 5/25 | Loss: 0.00126684
Iteration 6/25 | Loss: 0.00126624
Iteration 7/25 | Loss: 0.00126624
Iteration 8/25 | Loss: 0.00126624
Iteration 9/25 | Loss: 0.00126624
Iteration 10/25 | Loss: 0.00126624
Iteration 11/25 | Loss: 0.00126624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012662375811487436, 0.0012662375811487436, 0.0012662375811487436, 0.0012662375811487436, 0.0012662375811487436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012662375811487436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45011938
Iteration 2/25 | Loss: 0.00082059
Iteration 3/25 | Loss: 0.00082059
Iteration 4/25 | Loss: 0.00082059
Iteration 5/25 | Loss: 0.00082059
Iteration 6/25 | Loss: 0.00082058
Iteration 7/25 | Loss: 0.00082058
Iteration 8/25 | Loss: 0.00082058
Iteration 9/25 | Loss: 0.00082058
Iteration 10/25 | Loss: 0.00082058
Iteration 11/25 | Loss: 0.00082058
Iteration 12/25 | Loss: 0.00082058
Iteration 13/25 | Loss: 0.00082058
Iteration 14/25 | Loss: 0.00082058
Iteration 15/25 | Loss: 0.00082058
Iteration 16/25 | Loss: 0.00082058
Iteration 17/25 | Loss: 0.00082058
Iteration 18/25 | Loss: 0.00082058
Iteration 19/25 | Loss: 0.00082058
Iteration 20/25 | Loss: 0.00082058
Iteration 21/25 | Loss: 0.00082058
Iteration 22/25 | Loss: 0.00082058
Iteration 23/25 | Loss: 0.00082058
Iteration 24/25 | Loss: 0.00082058
Iteration 25/25 | Loss: 0.00082058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082058
Iteration 2/1000 | Loss: 0.00003554
Iteration 3/1000 | Loss: 0.00002086
Iteration 4/1000 | Loss: 0.00001851
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001598
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001499
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001483
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001480
Iteration 23/1000 | Loss: 0.00001480
Iteration 24/1000 | Loss: 0.00001480
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001477
Iteration 30/1000 | Loss: 0.00001477
Iteration 31/1000 | Loss: 0.00001476
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001476
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001476
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001476
Iteration 40/1000 | Loss: 0.00001476
Iteration 41/1000 | Loss: 0.00001476
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001471
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001470
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001468
Iteration 68/1000 | Loss: 0.00001468
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001466
Iteration 75/1000 | Loss: 0.00001466
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001465
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001464
Iteration 81/1000 | Loss: 0.00001464
Iteration 82/1000 | Loss: 0.00001464
Iteration 83/1000 | Loss: 0.00001464
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001453
Iteration 123/1000 | Loss: 0.00001453
Iteration 124/1000 | Loss: 0.00001453
Iteration 125/1000 | Loss: 0.00001452
Iteration 126/1000 | Loss: 0.00001452
Iteration 127/1000 | Loss: 0.00001452
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001451
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001451
Iteration 132/1000 | Loss: 0.00001450
Iteration 133/1000 | Loss: 0.00001450
Iteration 134/1000 | Loss: 0.00001449
Iteration 135/1000 | Loss: 0.00001449
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001449
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001447
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001447
Iteration 149/1000 | Loss: 0.00001447
Iteration 150/1000 | Loss: 0.00001447
Iteration 151/1000 | Loss: 0.00001447
Iteration 152/1000 | Loss: 0.00001447
Iteration 153/1000 | Loss: 0.00001447
Iteration 154/1000 | Loss: 0.00001447
Iteration 155/1000 | Loss: 0.00001447
Iteration 156/1000 | Loss: 0.00001447
Iteration 157/1000 | Loss: 0.00001447
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001447
Iteration 167/1000 | Loss: 0.00001447
Iteration 168/1000 | Loss: 0.00001447
Iteration 169/1000 | Loss: 0.00001447
Iteration 170/1000 | Loss: 0.00001447
Iteration 171/1000 | Loss: 0.00001447
Iteration 172/1000 | Loss: 0.00001447
Iteration 173/1000 | Loss: 0.00001447
Iteration 174/1000 | Loss: 0.00001447
Iteration 175/1000 | Loss: 0.00001447
Iteration 176/1000 | Loss: 0.00001447
Iteration 177/1000 | Loss: 0.00001447
Iteration 178/1000 | Loss: 0.00001447
Iteration 179/1000 | Loss: 0.00001447
Iteration 180/1000 | Loss: 0.00001447
Iteration 181/1000 | Loss: 0.00001447
Iteration 182/1000 | Loss: 0.00001447
Iteration 183/1000 | Loss: 0.00001447
Iteration 184/1000 | Loss: 0.00001447
Iteration 185/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.4466129869106226e-05, 1.4466129869106226e-05, 1.4466129869106226e-05, 1.4466129869106226e-05, 1.4466129869106226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4466129869106226e-05

Optimization complete. Final v2v error: 3.1310842037200928 mm

Highest mean error: 4.684490203857422 mm for frame 83

Lowest mean error: 2.754287004470825 mm for frame 164

Saving results

Total time: 39.811793088912964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592805
Iteration 2/25 | Loss: 0.00181876
Iteration 3/25 | Loss: 0.00145223
Iteration 4/25 | Loss: 0.00142173
Iteration 5/25 | Loss: 0.00141582
Iteration 6/25 | Loss: 0.00141515
Iteration 7/25 | Loss: 0.00141515
Iteration 8/25 | Loss: 0.00141515
Iteration 9/25 | Loss: 0.00141515
Iteration 10/25 | Loss: 0.00141515
Iteration 11/25 | Loss: 0.00141515
Iteration 12/25 | Loss: 0.00141515
Iteration 13/25 | Loss: 0.00141515
Iteration 14/25 | Loss: 0.00141515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014151465147733688, 0.0014151465147733688, 0.0014151465147733688, 0.0014151465147733688, 0.0014151465147733688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014151465147733688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40900683
Iteration 2/25 | Loss: 0.00096381
Iteration 3/25 | Loss: 0.00096378
Iteration 4/25 | Loss: 0.00096378
Iteration 5/25 | Loss: 0.00096378
Iteration 6/25 | Loss: 0.00096378
Iteration 7/25 | Loss: 0.00096378
Iteration 8/25 | Loss: 0.00096378
Iteration 9/25 | Loss: 0.00096378
Iteration 10/25 | Loss: 0.00096378
Iteration 11/25 | Loss: 0.00096378
Iteration 12/25 | Loss: 0.00096378
Iteration 13/25 | Loss: 0.00096378
Iteration 14/25 | Loss: 0.00096378
Iteration 15/25 | Loss: 0.00096378
Iteration 16/25 | Loss: 0.00096378
Iteration 17/25 | Loss: 0.00096378
Iteration 18/25 | Loss: 0.00096378
Iteration 19/25 | Loss: 0.00096378
Iteration 20/25 | Loss: 0.00096378
Iteration 21/25 | Loss: 0.00096378
Iteration 22/25 | Loss: 0.00096378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009637795737944543, 0.0009637795737944543, 0.0009637795737944543, 0.0009637795737944543, 0.0009637795737944543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009637795737944543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096378
Iteration 2/1000 | Loss: 0.00006014
Iteration 3/1000 | Loss: 0.00003651
Iteration 4/1000 | Loss: 0.00003002
Iteration 5/1000 | Loss: 0.00002797
Iteration 6/1000 | Loss: 0.00002669
Iteration 7/1000 | Loss: 0.00002577
Iteration 8/1000 | Loss: 0.00002529
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002486
Iteration 11/1000 | Loss: 0.00002477
Iteration 12/1000 | Loss: 0.00002474
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002472
Iteration 15/1000 | Loss: 0.00002469
Iteration 16/1000 | Loss: 0.00002446
Iteration 17/1000 | Loss: 0.00002443
Iteration 18/1000 | Loss: 0.00002439
Iteration 19/1000 | Loss: 0.00002434
Iteration 20/1000 | Loss: 0.00002433
Iteration 21/1000 | Loss: 0.00002430
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002425
Iteration 24/1000 | Loss: 0.00002423
Iteration 25/1000 | Loss: 0.00002421
Iteration 26/1000 | Loss: 0.00002420
Iteration 27/1000 | Loss: 0.00002420
Iteration 28/1000 | Loss: 0.00002420
Iteration 29/1000 | Loss: 0.00002420
Iteration 30/1000 | Loss: 0.00002420
Iteration 31/1000 | Loss: 0.00002419
Iteration 32/1000 | Loss: 0.00002416
Iteration 33/1000 | Loss: 0.00002416
Iteration 34/1000 | Loss: 0.00002416
Iteration 35/1000 | Loss: 0.00002415
Iteration 36/1000 | Loss: 0.00002415
Iteration 37/1000 | Loss: 0.00002413
Iteration 38/1000 | Loss: 0.00002413
Iteration 39/1000 | Loss: 0.00002412
Iteration 40/1000 | Loss: 0.00002412
Iteration 41/1000 | Loss: 0.00002412
Iteration 42/1000 | Loss: 0.00002411
Iteration 43/1000 | Loss: 0.00002411
Iteration 44/1000 | Loss: 0.00002411
Iteration 45/1000 | Loss: 0.00002411
Iteration 46/1000 | Loss: 0.00002410
Iteration 47/1000 | Loss: 0.00002410
Iteration 48/1000 | Loss: 0.00002409
Iteration 49/1000 | Loss: 0.00002409
Iteration 50/1000 | Loss: 0.00002409
Iteration 51/1000 | Loss: 0.00002408
Iteration 52/1000 | Loss: 0.00002408
Iteration 53/1000 | Loss: 0.00002407
Iteration 54/1000 | Loss: 0.00002407
Iteration 55/1000 | Loss: 0.00002407
Iteration 56/1000 | Loss: 0.00002407
Iteration 57/1000 | Loss: 0.00002407
Iteration 58/1000 | Loss: 0.00002407
Iteration 59/1000 | Loss: 0.00002407
Iteration 60/1000 | Loss: 0.00002407
Iteration 61/1000 | Loss: 0.00002406
Iteration 62/1000 | Loss: 0.00002406
Iteration 63/1000 | Loss: 0.00002406
Iteration 64/1000 | Loss: 0.00002406
Iteration 65/1000 | Loss: 0.00002406
Iteration 66/1000 | Loss: 0.00002406
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002406
Iteration 69/1000 | Loss: 0.00002406
Iteration 70/1000 | Loss: 0.00002406
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002406
Iteration 73/1000 | Loss: 0.00002406
Iteration 74/1000 | Loss: 0.00002406
Iteration 75/1000 | Loss: 0.00002406
Iteration 76/1000 | Loss: 0.00002406
Iteration 77/1000 | Loss: 0.00002406
Iteration 78/1000 | Loss: 0.00002406
Iteration 79/1000 | Loss: 0.00002406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.406224848527927e-05, 2.406224848527927e-05, 2.406224848527927e-05, 2.406224848527927e-05, 2.406224848527927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.406224848527927e-05

Optimization complete. Final v2v error: 4.148846626281738 mm

Highest mean error: 4.288670539855957 mm for frame 83

Lowest mean error: 3.710211992263794 mm for frame 4

Saving results

Total time: 31.7729971408844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401590
Iteration 2/25 | Loss: 0.00129122
Iteration 3/25 | Loss: 0.00121839
Iteration 4/25 | Loss: 0.00120785
Iteration 5/25 | Loss: 0.00120397
Iteration 6/25 | Loss: 0.00120316
Iteration 7/25 | Loss: 0.00120316
Iteration 8/25 | Loss: 0.00120316
Iteration 9/25 | Loss: 0.00120316
Iteration 10/25 | Loss: 0.00120316
Iteration 11/25 | Loss: 0.00120316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012031609658151865, 0.0012031609658151865, 0.0012031609658151865, 0.0012031609658151865, 0.0012031609658151865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012031609658151865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66383910
Iteration 2/25 | Loss: 0.00080509
Iteration 3/25 | Loss: 0.00080508
Iteration 4/25 | Loss: 0.00080508
Iteration 5/25 | Loss: 0.00080508
Iteration 6/25 | Loss: 0.00080508
Iteration 7/25 | Loss: 0.00080508
Iteration 8/25 | Loss: 0.00080508
Iteration 9/25 | Loss: 0.00080508
Iteration 10/25 | Loss: 0.00080508
Iteration 11/25 | Loss: 0.00080508
Iteration 12/25 | Loss: 0.00080508
Iteration 13/25 | Loss: 0.00080508
Iteration 14/25 | Loss: 0.00080508
Iteration 15/25 | Loss: 0.00080508
Iteration 16/25 | Loss: 0.00080508
Iteration 17/25 | Loss: 0.00080508
Iteration 18/25 | Loss: 0.00080508
Iteration 19/25 | Loss: 0.00080508
Iteration 20/25 | Loss: 0.00080508
Iteration 21/25 | Loss: 0.00080508
Iteration 22/25 | Loss: 0.00080508
Iteration 23/25 | Loss: 0.00080508
Iteration 24/25 | Loss: 0.00080508
Iteration 25/25 | Loss: 0.00080508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080508
Iteration 2/1000 | Loss: 0.00002773
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001262
Iteration 8/1000 | Loss: 0.00001230
Iteration 9/1000 | Loss: 0.00001208
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001150
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001136
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001130
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001129
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001127
Iteration 47/1000 | Loss: 0.00001127
Iteration 48/1000 | Loss: 0.00001127
Iteration 49/1000 | Loss: 0.00001126
Iteration 50/1000 | Loss: 0.00001126
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001100
Iteration 159/1000 | Loss: 0.00001100
Iteration 160/1000 | Loss: 0.00001100
Iteration 161/1000 | Loss: 0.00001100
Iteration 162/1000 | Loss: 0.00001100
Iteration 163/1000 | Loss: 0.00001100
Iteration 164/1000 | Loss: 0.00001099
Iteration 165/1000 | Loss: 0.00001099
Iteration 166/1000 | Loss: 0.00001099
Iteration 167/1000 | Loss: 0.00001099
Iteration 168/1000 | Loss: 0.00001099
Iteration 169/1000 | Loss: 0.00001099
Iteration 170/1000 | Loss: 0.00001099
Iteration 171/1000 | Loss: 0.00001099
Iteration 172/1000 | Loss: 0.00001099
Iteration 173/1000 | Loss: 0.00001099
Iteration 174/1000 | Loss: 0.00001099
Iteration 175/1000 | Loss: 0.00001099
Iteration 176/1000 | Loss: 0.00001099
Iteration 177/1000 | Loss: 0.00001099
Iteration 178/1000 | Loss: 0.00001099
Iteration 179/1000 | Loss: 0.00001099
Iteration 180/1000 | Loss: 0.00001099
Iteration 181/1000 | Loss: 0.00001099
Iteration 182/1000 | Loss: 0.00001099
Iteration 183/1000 | Loss: 0.00001099
Iteration 184/1000 | Loss: 0.00001099
Iteration 185/1000 | Loss: 0.00001099
Iteration 186/1000 | Loss: 0.00001099
Iteration 187/1000 | Loss: 0.00001099
Iteration 188/1000 | Loss: 0.00001099
Iteration 189/1000 | Loss: 0.00001099
Iteration 190/1000 | Loss: 0.00001099
Iteration 191/1000 | Loss: 0.00001099
Iteration 192/1000 | Loss: 0.00001099
Iteration 193/1000 | Loss: 0.00001099
Iteration 194/1000 | Loss: 0.00001099
Iteration 195/1000 | Loss: 0.00001099
Iteration 196/1000 | Loss: 0.00001099
Iteration 197/1000 | Loss: 0.00001099
Iteration 198/1000 | Loss: 0.00001099
Iteration 199/1000 | Loss: 0.00001099
Iteration 200/1000 | Loss: 0.00001099
Iteration 201/1000 | Loss: 0.00001099
Iteration 202/1000 | Loss: 0.00001099
Iteration 203/1000 | Loss: 0.00001099
Iteration 204/1000 | Loss: 0.00001099
Iteration 205/1000 | Loss: 0.00001099
Iteration 206/1000 | Loss: 0.00001099
Iteration 207/1000 | Loss: 0.00001099
Iteration 208/1000 | Loss: 0.00001099
Iteration 209/1000 | Loss: 0.00001099
Iteration 210/1000 | Loss: 0.00001099
Iteration 211/1000 | Loss: 0.00001099
Iteration 212/1000 | Loss: 0.00001099
Iteration 213/1000 | Loss: 0.00001099
Iteration 214/1000 | Loss: 0.00001099
Iteration 215/1000 | Loss: 0.00001099
Iteration 216/1000 | Loss: 0.00001099
Iteration 217/1000 | Loss: 0.00001099
Iteration 218/1000 | Loss: 0.00001099
Iteration 219/1000 | Loss: 0.00001099
Iteration 220/1000 | Loss: 0.00001099
Iteration 221/1000 | Loss: 0.00001099
Iteration 222/1000 | Loss: 0.00001099
Iteration 223/1000 | Loss: 0.00001099
Iteration 224/1000 | Loss: 0.00001099
Iteration 225/1000 | Loss: 0.00001099
Iteration 226/1000 | Loss: 0.00001099
Iteration 227/1000 | Loss: 0.00001099
Iteration 228/1000 | Loss: 0.00001099
Iteration 229/1000 | Loss: 0.00001099
Iteration 230/1000 | Loss: 0.00001099
Iteration 231/1000 | Loss: 0.00001099
Iteration 232/1000 | Loss: 0.00001099
Iteration 233/1000 | Loss: 0.00001099
Iteration 234/1000 | Loss: 0.00001099
Iteration 235/1000 | Loss: 0.00001099
Iteration 236/1000 | Loss: 0.00001099
Iteration 237/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.0986816050717607e-05, 1.0986816050717607e-05, 1.0986816050717607e-05, 1.0986816050717607e-05, 1.0986816050717607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0986816050717607e-05

Optimization complete. Final v2v error: 2.857499837875366 mm

Highest mean error: 3.1364972591400146 mm for frame 99

Lowest mean error: 2.6468727588653564 mm for frame 9

Saving results

Total time: 40.863850355148315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907381
Iteration 2/25 | Loss: 0.00177097
Iteration 3/25 | Loss: 0.00141136
Iteration 4/25 | Loss: 0.00135028
Iteration 5/25 | Loss: 0.00133569
Iteration 6/25 | Loss: 0.00133292
Iteration 7/25 | Loss: 0.00133207
Iteration 8/25 | Loss: 0.00133207
Iteration 9/25 | Loss: 0.00133207
Iteration 10/25 | Loss: 0.00133207
Iteration 11/25 | Loss: 0.00133207
Iteration 12/25 | Loss: 0.00133207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013320698635652661, 0.0013320698635652661, 0.0013320698635652661, 0.0013320698635652661, 0.0013320698635652661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013320698635652661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52338982
Iteration 2/25 | Loss: 0.00088382
Iteration 3/25 | Loss: 0.00088381
Iteration 4/25 | Loss: 0.00088381
Iteration 5/25 | Loss: 0.00088381
Iteration 6/25 | Loss: 0.00088381
Iteration 7/25 | Loss: 0.00088381
Iteration 8/25 | Loss: 0.00088381
Iteration 9/25 | Loss: 0.00088381
Iteration 10/25 | Loss: 0.00088381
Iteration 11/25 | Loss: 0.00088381
Iteration 12/25 | Loss: 0.00088381
Iteration 13/25 | Loss: 0.00088381
Iteration 14/25 | Loss: 0.00088381
Iteration 15/25 | Loss: 0.00088381
Iteration 16/25 | Loss: 0.00088381
Iteration 17/25 | Loss: 0.00088381
Iteration 18/25 | Loss: 0.00088381
Iteration 19/25 | Loss: 0.00088381
Iteration 20/25 | Loss: 0.00088381
Iteration 21/25 | Loss: 0.00088381
Iteration 22/25 | Loss: 0.00088381
Iteration 23/25 | Loss: 0.00088381
Iteration 24/25 | Loss: 0.00088381
Iteration 25/25 | Loss: 0.00088381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088381
Iteration 2/1000 | Loss: 0.00007525
Iteration 3/1000 | Loss: 0.00005352
Iteration 4/1000 | Loss: 0.00004172
Iteration 5/1000 | Loss: 0.00003898
Iteration 6/1000 | Loss: 0.00003705
Iteration 7/1000 | Loss: 0.00003585
Iteration 8/1000 | Loss: 0.00003491
Iteration 9/1000 | Loss: 0.00003426
Iteration 10/1000 | Loss: 0.00003380
Iteration 11/1000 | Loss: 0.00003342
Iteration 12/1000 | Loss: 0.00003314
Iteration 13/1000 | Loss: 0.00003286
Iteration 14/1000 | Loss: 0.00003258
Iteration 15/1000 | Loss: 0.00003239
Iteration 16/1000 | Loss: 0.00003223
Iteration 17/1000 | Loss: 0.00003205
Iteration 18/1000 | Loss: 0.00003195
Iteration 19/1000 | Loss: 0.00003185
Iteration 20/1000 | Loss: 0.00003179
Iteration 21/1000 | Loss: 0.00003171
Iteration 22/1000 | Loss: 0.00003163
Iteration 23/1000 | Loss: 0.00003162
Iteration 24/1000 | Loss: 0.00003161
Iteration 25/1000 | Loss: 0.00003160
Iteration 26/1000 | Loss: 0.00003159
Iteration 27/1000 | Loss: 0.00003158
Iteration 28/1000 | Loss: 0.00003158
Iteration 29/1000 | Loss: 0.00003158
Iteration 30/1000 | Loss: 0.00003157
Iteration 31/1000 | Loss: 0.00003157
Iteration 32/1000 | Loss: 0.00003156
Iteration 33/1000 | Loss: 0.00003156
Iteration 34/1000 | Loss: 0.00003154
Iteration 35/1000 | Loss: 0.00003151
Iteration 36/1000 | Loss: 0.00003151
Iteration 37/1000 | Loss: 0.00003150
Iteration 38/1000 | Loss: 0.00003150
Iteration 39/1000 | Loss: 0.00003150
Iteration 40/1000 | Loss: 0.00003149
Iteration 41/1000 | Loss: 0.00003149
Iteration 42/1000 | Loss: 0.00003147
Iteration 43/1000 | Loss: 0.00003147
Iteration 44/1000 | Loss: 0.00003147
Iteration 45/1000 | Loss: 0.00003147
Iteration 46/1000 | Loss: 0.00003147
Iteration 47/1000 | Loss: 0.00003147
Iteration 48/1000 | Loss: 0.00003147
Iteration 49/1000 | Loss: 0.00003147
Iteration 50/1000 | Loss: 0.00003147
Iteration 51/1000 | Loss: 0.00003146
Iteration 52/1000 | Loss: 0.00003146
Iteration 53/1000 | Loss: 0.00003146
Iteration 54/1000 | Loss: 0.00003146
Iteration 55/1000 | Loss: 0.00003146
Iteration 56/1000 | Loss: 0.00003145
Iteration 57/1000 | Loss: 0.00003145
Iteration 58/1000 | Loss: 0.00003145
Iteration 59/1000 | Loss: 0.00003144
Iteration 60/1000 | Loss: 0.00003144
Iteration 61/1000 | Loss: 0.00003143
Iteration 62/1000 | Loss: 0.00003143
Iteration 63/1000 | Loss: 0.00003143
Iteration 64/1000 | Loss: 0.00003143
Iteration 65/1000 | Loss: 0.00003143
Iteration 66/1000 | Loss: 0.00003142
Iteration 67/1000 | Loss: 0.00003142
Iteration 68/1000 | Loss: 0.00003142
Iteration 69/1000 | Loss: 0.00003142
Iteration 70/1000 | Loss: 0.00003142
Iteration 71/1000 | Loss: 0.00003141
Iteration 72/1000 | Loss: 0.00003141
Iteration 73/1000 | Loss: 0.00003141
Iteration 74/1000 | Loss: 0.00003140
Iteration 75/1000 | Loss: 0.00003140
Iteration 76/1000 | Loss: 0.00003140
Iteration 77/1000 | Loss: 0.00003140
Iteration 78/1000 | Loss: 0.00003140
Iteration 79/1000 | Loss: 0.00003139
Iteration 80/1000 | Loss: 0.00003139
Iteration 81/1000 | Loss: 0.00003139
Iteration 82/1000 | Loss: 0.00003139
Iteration 83/1000 | Loss: 0.00003138
Iteration 84/1000 | Loss: 0.00003138
Iteration 85/1000 | Loss: 0.00003138
Iteration 86/1000 | Loss: 0.00003138
Iteration 87/1000 | Loss: 0.00003137
Iteration 88/1000 | Loss: 0.00003137
Iteration 89/1000 | Loss: 0.00003137
Iteration 90/1000 | Loss: 0.00003137
Iteration 91/1000 | Loss: 0.00003137
Iteration 92/1000 | Loss: 0.00003137
Iteration 93/1000 | Loss: 0.00003136
Iteration 94/1000 | Loss: 0.00003136
Iteration 95/1000 | Loss: 0.00003136
Iteration 96/1000 | Loss: 0.00003136
Iteration 97/1000 | Loss: 0.00003136
Iteration 98/1000 | Loss: 0.00003136
Iteration 99/1000 | Loss: 0.00003136
Iteration 100/1000 | Loss: 0.00003136
Iteration 101/1000 | Loss: 0.00003136
Iteration 102/1000 | Loss: 0.00003135
Iteration 103/1000 | Loss: 0.00003135
Iteration 104/1000 | Loss: 0.00003135
Iteration 105/1000 | Loss: 0.00003134
Iteration 106/1000 | Loss: 0.00003134
Iteration 107/1000 | Loss: 0.00003134
Iteration 108/1000 | Loss: 0.00003134
Iteration 109/1000 | Loss: 0.00003134
Iteration 110/1000 | Loss: 0.00003134
Iteration 111/1000 | Loss: 0.00003134
Iteration 112/1000 | Loss: 0.00003134
Iteration 113/1000 | Loss: 0.00003133
Iteration 114/1000 | Loss: 0.00003133
Iteration 115/1000 | Loss: 0.00003133
Iteration 116/1000 | Loss: 0.00003133
Iteration 117/1000 | Loss: 0.00003133
Iteration 118/1000 | Loss: 0.00003133
Iteration 119/1000 | Loss: 0.00003133
Iteration 120/1000 | Loss: 0.00003133
Iteration 121/1000 | Loss: 0.00003133
Iteration 122/1000 | Loss: 0.00003133
Iteration 123/1000 | Loss: 0.00003132
Iteration 124/1000 | Loss: 0.00003132
Iteration 125/1000 | Loss: 0.00003132
Iteration 126/1000 | Loss: 0.00003132
Iteration 127/1000 | Loss: 0.00003131
Iteration 128/1000 | Loss: 0.00003131
Iteration 129/1000 | Loss: 0.00003131
Iteration 130/1000 | Loss: 0.00003131
Iteration 131/1000 | Loss: 0.00003131
Iteration 132/1000 | Loss: 0.00003131
Iteration 133/1000 | Loss: 0.00003131
Iteration 134/1000 | Loss: 0.00003131
Iteration 135/1000 | Loss: 0.00003131
Iteration 136/1000 | Loss: 0.00003131
Iteration 137/1000 | Loss: 0.00003131
Iteration 138/1000 | Loss: 0.00003131
Iteration 139/1000 | Loss: 0.00003131
Iteration 140/1000 | Loss: 0.00003131
Iteration 141/1000 | Loss: 0.00003131
Iteration 142/1000 | Loss: 0.00003131
Iteration 143/1000 | Loss: 0.00003130
Iteration 144/1000 | Loss: 0.00003130
Iteration 145/1000 | Loss: 0.00003130
Iteration 146/1000 | Loss: 0.00003130
Iteration 147/1000 | Loss: 0.00003130
Iteration 148/1000 | Loss: 0.00003130
Iteration 149/1000 | Loss: 0.00003130
Iteration 150/1000 | Loss: 0.00003130
Iteration 151/1000 | Loss: 0.00003130
Iteration 152/1000 | Loss: 0.00003130
Iteration 153/1000 | Loss: 0.00003130
Iteration 154/1000 | Loss: 0.00003130
Iteration 155/1000 | Loss: 0.00003130
Iteration 156/1000 | Loss: 0.00003130
Iteration 157/1000 | Loss: 0.00003130
Iteration 158/1000 | Loss: 0.00003130
Iteration 159/1000 | Loss: 0.00003130
Iteration 160/1000 | Loss: 0.00003129
Iteration 161/1000 | Loss: 0.00003129
Iteration 162/1000 | Loss: 0.00003129
Iteration 163/1000 | Loss: 0.00003129
Iteration 164/1000 | Loss: 0.00003129
Iteration 165/1000 | Loss: 0.00003129
Iteration 166/1000 | Loss: 0.00003129
Iteration 167/1000 | Loss: 0.00003129
Iteration 168/1000 | Loss: 0.00003129
Iteration 169/1000 | Loss: 0.00003129
Iteration 170/1000 | Loss: 0.00003129
Iteration 171/1000 | Loss: 0.00003128
Iteration 172/1000 | Loss: 0.00003128
Iteration 173/1000 | Loss: 0.00003128
Iteration 174/1000 | Loss: 0.00003128
Iteration 175/1000 | Loss: 0.00003128
Iteration 176/1000 | Loss: 0.00003128
Iteration 177/1000 | Loss: 0.00003128
Iteration 178/1000 | Loss: 0.00003128
Iteration 179/1000 | Loss: 0.00003128
Iteration 180/1000 | Loss: 0.00003128
Iteration 181/1000 | Loss: 0.00003128
Iteration 182/1000 | Loss: 0.00003128
Iteration 183/1000 | Loss: 0.00003128
Iteration 184/1000 | Loss: 0.00003128
Iteration 185/1000 | Loss: 0.00003128
Iteration 186/1000 | Loss: 0.00003128
Iteration 187/1000 | Loss: 0.00003128
Iteration 188/1000 | Loss: 0.00003128
Iteration 189/1000 | Loss: 0.00003128
Iteration 190/1000 | Loss: 0.00003128
Iteration 191/1000 | Loss: 0.00003128
Iteration 192/1000 | Loss: 0.00003128
Iteration 193/1000 | Loss: 0.00003127
Iteration 194/1000 | Loss: 0.00003127
Iteration 195/1000 | Loss: 0.00003127
Iteration 196/1000 | Loss: 0.00003127
Iteration 197/1000 | Loss: 0.00003127
Iteration 198/1000 | Loss: 0.00003127
Iteration 199/1000 | Loss: 0.00003127
Iteration 200/1000 | Loss: 0.00003127
Iteration 201/1000 | Loss: 0.00003127
Iteration 202/1000 | Loss: 0.00003127
Iteration 203/1000 | Loss: 0.00003127
Iteration 204/1000 | Loss: 0.00003127
Iteration 205/1000 | Loss: 0.00003127
Iteration 206/1000 | Loss: 0.00003127
Iteration 207/1000 | Loss: 0.00003127
Iteration 208/1000 | Loss: 0.00003127
Iteration 209/1000 | Loss: 0.00003127
Iteration 210/1000 | Loss: 0.00003127
Iteration 211/1000 | Loss: 0.00003127
Iteration 212/1000 | Loss: 0.00003127
Iteration 213/1000 | Loss: 0.00003127
Iteration 214/1000 | Loss: 0.00003127
Iteration 215/1000 | Loss: 0.00003126
Iteration 216/1000 | Loss: 0.00003126
Iteration 217/1000 | Loss: 0.00003126
Iteration 218/1000 | Loss: 0.00003126
Iteration 219/1000 | Loss: 0.00003126
Iteration 220/1000 | Loss: 0.00003126
Iteration 221/1000 | Loss: 0.00003126
Iteration 222/1000 | Loss: 0.00003126
Iteration 223/1000 | Loss: 0.00003126
Iteration 224/1000 | Loss: 0.00003126
Iteration 225/1000 | Loss: 0.00003126
Iteration 226/1000 | Loss: 0.00003126
Iteration 227/1000 | Loss: 0.00003126
Iteration 228/1000 | Loss: 0.00003126
Iteration 229/1000 | Loss: 0.00003126
Iteration 230/1000 | Loss: 0.00003126
Iteration 231/1000 | Loss: 0.00003126
Iteration 232/1000 | Loss: 0.00003126
Iteration 233/1000 | Loss: 0.00003126
Iteration 234/1000 | Loss: 0.00003126
Iteration 235/1000 | Loss: 0.00003126
Iteration 236/1000 | Loss: 0.00003126
Iteration 237/1000 | Loss: 0.00003126
Iteration 238/1000 | Loss: 0.00003126
Iteration 239/1000 | Loss: 0.00003126
Iteration 240/1000 | Loss: 0.00003126
Iteration 241/1000 | Loss: 0.00003126
Iteration 242/1000 | Loss: 0.00003126
Iteration 243/1000 | Loss: 0.00003126
Iteration 244/1000 | Loss: 0.00003126
Iteration 245/1000 | Loss: 0.00003126
Iteration 246/1000 | Loss: 0.00003126
Iteration 247/1000 | Loss: 0.00003126
Iteration 248/1000 | Loss: 0.00003126
Iteration 249/1000 | Loss: 0.00003126
Iteration 250/1000 | Loss: 0.00003126
Iteration 251/1000 | Loss: 0.00003126
Iteration 252/1000 | Loss: 0.00003126
Iteration 253/1000 | Loss: 0.00003126
Iteration 254/1000 | Loss: 0.00003126
Iteration 255/1000 | Loss: 0.00003126
Iteration 256/1000 | Loss: 0.00003126
Iteration 257/1000 | Loss: 0.00003126
Iteration 258/1000 | Loss: 0.00003126
Iteration 259/1000 | Loss: 0.00003126
Iteration 260/1000 | Loss: 0.00003126
Iteration 261/1000 | Loss: 0.00003126
Iteration 262/1000 | Loss: 0.00003126
Iteration 263/1000 | Loss: 0.00003126
Iteration 264/1000 | Loss: 0.00003126
Iteration 265/1000 | Loss: 0.00003126
Iteration 266/1000 | Loss: 0.00003126
Iteration 267/1000 | Loss: 0.00003126
Iteration 268/1000 | Loss: 0.00003126
Iteration 269/1000 | Loss: 0.00003126
Iteration 270/1000 | Loss: 0.00003126
Iteration 271/1000 | Loss: 0.00003126
Iteration 272/1000 | Loss: 0.00003126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [3.126450610579923e-05, 3.126450610579923e-05, 3.126450610579923e-05, 3.126450610579923e-05, 3.126450610579923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.126450610579923e-05

Optimization complete. Final v2v error: 4.59422492980957 mm

Highest mean error: 6.831620693206787 mm for frame 83

Lowest mean error: 3.418644666671753 mm for frame 31

Saving results

Total time: 53.70599293708801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968481
Iteration 2/25 | Loss: 0.00308837
Iteration 3/25 | Loss: 0.00211275
Iteration 4/25 | Loss: 0.00195606
Iteration 5/25 | Loss: 0.00187616
Iteration 6/25 | Loss: 0.00174959
Iteration 7/25 | Loss: 0.00167286
Iteration 8/25 | Loss: 0.00164971
Iteration 9/25 | Loss: 0.00158759
Iteration 10/25 | Loss: 0.00158863
Iteration 11/25 | Loss: 0.00158199
Iteration 12/25 | Loss: 0.00156615
Iteration 13/25 | Loss: 0.00156023
Iteration 14/25 | Loss: 0.00155301
Iteration 15/25 | Loss: 0.00155105
Iteration 16/25 | Loss: 0.00155040
Iteration 17/25 | Loss: 0.00155008
Iteration 18/25 | Loss: 0.00154984
Iteration 19/25 | Loss: 0.00154962
Iteration 20/25 | Loss: 0.00154944
Iteration 21/25 | Loss: 0.00154933
Iteration 22/25 | Loss: 0.00154924
Iteration 23/25 | Loss: 0.00154916
Iteration 24/25 | Loss: 0.00154914
Iteration 25/25 | Loss: 0.00154906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41567349
Iteration 2/25 | Loss: 0.00339640
Iteration 3/25 | Loss: 0.00272087
Iteration 4/25 | Loss: 0.00272087
Iteration 5/25 | Loss: 0.00272087
Iteration 6/25 | Loss: 0.00272087
Iteration 7/25 | Loss: 0.00272087
Iteration 8/25 | Loss: 0.00272087
Iteration 9/25 | Loss: 0.00272087
Iteration 10/25 | Loss: 0.00272087
Iteration 11/25 | Loss: 0.00272087
Iteration 12/25 | Loss: 0.00272087
Iteration 13/25 | Loss: 0.00272087
Iteration 14/25 | Loss: 0.00272087
Iteration 15/25 | Loss: 0.00272087
Iteration 16/25 | Loss: 0.00272087
Iteration 17/25 | Loss: 0.00272087
Iteration 18/25 | Loss: 0.00272087
Iteration 19/25 | Loss: 0.00272087
Iteration 20/25 | Loss: 0.00272087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0027208703104406595, 0.0027208703104406595, 0.0027208703104406595, 0.0027208703104406595, 0.0027208703104406595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027208703104406595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272087
Iteration 2/1000 | Loss: 0.00293158
Iteration 3/1000 | Loss: 0.00229217
Iteration 4/1000 | Loss: 0.00056162
Iteration 5/1000 | Loss: 0.00048733
Iteration 6/1000 | Loss: 0.00106749
Iteration 7/1000 | Loss: 0.00190026
Iteration 8/1000 | Loss: 0.00194666
Iteration 9/1000 | Loss: 0.00058992
Iteration 10/1000 | Loss: 0.00019901
Iteration 11/1000 | Loss: 0.00067727
Iteration 12/1000 | Loss: 0.00194529
Iteration 13/1000 | Loss: 0.00033290
Iteration 14/1000 | Loss: 0.00094035
Iteration 15/1000 | Loss: 0.00095277
Iteration 16/1000 | Loss: 0.00223521
Iteration 17/1000 | Loss: 0.00024753
Iteration 18/1000 | Loss: 0.00029135
Iteration 19/1000 | Loss: 0.00066835
Iteration 20/1000 | Loss: 0.00012265
Iteration 21/1000 | Loss: 0.00039231
Iteration 22/1000 | Loss: 0.00150727
Iteration 23/1000 | Loss: 0.00021116
Iteration 24/1000 | Loss: 0.00010465
Iteration 25/1000 | Loss: 0.00015876
Iteration 26/1000 | Loss: 0.00011577
Iteration 27/1000 | Loss: 0.00050435
Iteration 28/1000 | Loss: 0.00005573
Iteration 29/1000 | Loss: 0.00046628
Iteration 30/1000 | Loss: 0.00071343
Iteration 31/1000 | Loss: 0.00011461
Iteration 32/1000 | Loss: 0.00005622
Iteration 33/1000 | Loss: 0.00015945
Iteration 34/1000 | Loss: 0.00018913
Iteration 35/1000 | Loss: 0.00005080
Iteration 36/1000 | Loss: 0.00010352
Iteration 37/1000 | Loss: 0.00005217
Iteration 38/1000 | Loss: 0.00005048
Iteration 39/1000 | Loss: 0.00021434
Iteration 40/1000 | Loss: 0.00004768
Iteration 41/1000 | Loss: 0.00004669
Iteration 42/1000 | Loss: 0.00004597
Iteration 43/1000 | Loss: 0.00023466
Iteration 44/1000 | Loss: 0.00120495
Iteration 45/1000 | Loss: 0.00226619
Iteration 46/1000 | Loss: 0.00029822
Iteration 47/1000 | Loss: 0.00009645
Iteration 48/1000 | Loss: 0.00043186
Iteration 49/1000 | Loss: 0.00045120
Iteration 50/1000 | Loss: 0.00005865
Iteration 51/1000 | Loss: 0.00004614
Iteration 52/1000 | Loss: 0.00003943
Iteration 53/1000 | Loss: 0.00024267
Iteration 54/1000 | Loss: 0.00008673
Iteration 55/1000 | Loss: 0.00003600
Iteration 56/1000 | Loss: 0.00003390
Iteration 57/1000 | Loss: 0.00003210
Iteration 58/1000 | Loss: 0.00047021
Iteration 59/1000 | Loss: 0.00021273
Iteration 60/1000 | Loss: 0.00029364
Iteration 61/1000 | Loss: 0.00016690
Iteration 62/1000 | Loss: 0.00020221
Iteration 63/1000 | Loss: 0.00019669
Iteration 64/1000 | Loss: 0.00015986
Iteration 65/1000 | Loss: 0.00043333
Iteration 66/1000 | Loss: 0.00045731
Iteration 67/1000 | Loss: 0.00005588
Iteration 68/1000 | Loss: 0.00004432
Iteration 69/1000 | Loss: 0.00043753
Iteration 70/1000 | Loss: 0.00005167
Iteration 71/1000 | Loss: 0.00023333
Iteration 72/1000 | Loss: 0.00013761
Iteration 73/1000 | Loss: 0.00016161
Iteration 74/1000 | Loss: 0.00061241
Iteration 75/1000 | Loss: 0.00062363
Iteration 76/1000 | Loss: 0.00076830
Iteration 77/1000 | Loss: 0.00034740
Iteration 78/1000 | Loss: 0.00088044
Iteration 79/1000 | Loss: 0.00060083
Iteration 80/1000 | Loss: 0.00013457
Iteration 81/1000 | Loss: 0.00007459
Iteration 82/1000 | Loss: 0.00019944
Iteration 83/1000 | Loss: 0.00009909
Iteration 84/1000 | Loss: 0.00009832
Iteration 85/1000 | Loss: 0.00020163
Iteration 86/1000 | Loss: 0.00022482
Iteration 87/1000 | Loss: 0.00007781
Iteration 88/1000 | Loss: 0.00006102
Iteration 89/1000 | Loss: 0.00003663
Iteration 90/1000 | Loss: 0.00003510
Iteration 91/1000 | Loss: 0.00016596
Iteration 92/1000 | Loss: 0.00012271
Iteration 93/1000 | Loss: 0.00030179
Iteration 94/1000 | Loss: 0.00004387
Iteration 95/1000 | Loss: 0.00008500
Iteration 96/1000 | Loss: 0.00003603
Iteration 97/1000 | Loss: 0.00003377
Iteration 98/1000 | Loss: 0.00003274
Iteration 99/1000 | Loss: 0.00003145
Iteration 100/1000 | Loss: 0.00003043
Iteration 101/1000 | Loss: 0.00002967
Iteration 102/1000 | Loss: 0.00014347
Iteration 103/1000 | Loss: 0.00002984
Iteration 104/1000 | Loss: 0.00002892
Iteration 105/1000 | Loss: 0.00002844
Iteration 106/1000 | Loss: 0.00002796
Iteration 107/1000 | Loss: 0.00020689
Iteration 108/1000 | Loss: 0.00028608
Iteration 109/1000 | Loss: 0.00056822
Iteration 110/1000 | Loss: 0.00015729
Iteration 111/1000 | Loss: 0.00008560
Iteration 112/1000 | Loss: 0.00014462
Iteration 113/1000 | Loss: 0.00012546
Iteration 114/1000 | Loss: 0.00003081
Iteration 115/1000 | Loss: 0.00017121
Iteration 116/1000 | Loss: 0.00003122
Iteration 117/1000 | Loss: 0.00002823
Iteration 118/1000 | Loss: 0.00002700
Iteration 119/1000 | Loss: 0.00002640
Iteration 120/1000 | Loss: 0.00002589
Iteration 121/1000 | Loss: 0.00002556
Iteration 122/1000 | Loss: 0.00002534
Iteration 123/1000 | Loss: 0.00009935
Iteration 124/1000 | Loss: 0.00009935
Iteration 125/1000 | Loss: 0.00036993
Iteration 126/1000 | Loss: 0.00005146
Iteration 127/1000 | Loss: 0.00002526
Iteration 128/1000 | Loss: 0.00002499
Iteration 129/1000 | Loss: 0.00002496
Iteration 130/1000 | Loss: 0.00002495
Iteration 131/1000 | Loss: 0.00002494
Iteration 132/1000 | Loss: 0.00002494
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002493
Iteration 135/1000 | Loss: 0.00002493
Iteration 136/1000 | Loss: 0.00002493
Iteration 137/1000 | Loss: 0.00002492
Iteration 138/1000 | Loss: 0.00002492
Iteration 139/1000 | Loss: 0.00002492
Iteration 140/1000 | Loss: 0.00002491
Iteration 141/1000 | Loss: 0.00002491
Iteration 142/1000 | Loss: 0.00002491
Iteration 143/1000 | Loss: 0.00002491
Iteration 144/1000 | Loss: 0.00002490
Iteration 145/1000 | Loss: 0.00002490
Iteration 146/1000 | Loss: 0.00002490
Iteration 147/1000 | Loss: 0.00002490
Iteration 148/1000 | Loss: 0.00002490
Iteration 149/1000 | Loss: 0.00002490
Iteration 150/1000 | Loss: 0.00002490
Iteration 151/1000 | Loss: 0.00002489
Iteration 152/1000 | Loss: 0.00002489
Iteration 153/1000 | Loss: 0.00008429
Iteration 154/1000 | Loss: 0.00003412
Iteration 155/1000 | Loss: 0.00004146
Iteration 156/1000 | Loss: 0.00002487
Iteration 157/1000 | Loss: 0.00002487
Iteration 158/1000 | Loss: 0.00002486
Iteration 159/1000 | Loss: 0.00002486
Iteration 160/1000 | Loss: 0.00002486
Iteration 161/1000 | Loss: 0.00002486
Iteration 162/1000 | Loss: 0.00002486
Iteration 163/1000 | Loss: 0.00002486
Iteration 164/1000 | Loss: 0.00002486
Iteration 165/1000 | Loss: 0.00002486
Iteration 166/1000 | Loss: 0.00002486
Iteration 167/1000 | Loss: 0.00002485
Iteration 168/1000 | Loss: 0.00002485
Iteration 169/1000 | Loss: 0.00002485
Iteration 170/1000 | Loss: 0.00002484
Iteration 171/1000 | Loss: 0.00002484
Iteration 172/1000 | Loss: 0.00002484
Iteration 173/1000 | Loss: 0.00002483
Iteration 174/1000 | Loss: 0.00002483
Iteration 175/1000 | Loss: 0.00002483
Iteration 176/1000 | Loss: 0.00002482
Iteration 177/1000 | Loss: 0.00002482
Iteration 178/1000 | Loss: 0.00002482
Iteration 179/1000 | Loss: 0.00002482
Iteration 180/1000 | Loss: 0.00003826
Iteration 181/1000 | Loss: 0.00014427
Iteration 182/1000 | Loss: 0.00043918
Iteration 183/1000 | Loss: 0.00008947
Iteration 184/1000 | Loss: 0.00002963
Iteration 185/1000 | Loss: 0.00002488
Iteration 186/1000 | Loss: 0.00003731
Iteration 187/1000 | Loss: 0.00039531
Iteration 188/1000 | Loss: 0.00008609
Iteration 189/1000 | Loss: 0.00003309
Iteration 190/1000 | Loss: 0.00002514
Iteration 191/1000 | Loss: 0.00003650
Iteration 192/1000 | Loss: 0.00010139
Iteration 193/1000 | Loss: 0.00003390
Iteration 194/1000 | Loss: 0.00004504
Iteration 195/1000 | Loss: 0.00004219
Iteration 196/1000 | Loss: 0.00003197
Iteration 197/1000 | Loss: 0.00002571
Iteration 198/1000 | Loss: 0.00014185
Iteration 199/1000 | Loss: 0.00003488
Iteration 200/1000 | Loss: 0.00002508
Iteration 201/1000 | Loss: 0.00002491
Iteration 202/1000 | Loss: 0.00002489
Iteration 203/1000 | Loss: 0.00002474
Iteration 204/1000 | Loss: 0.00010178
Iteration 205/1000 | Loss: 0.00002791
Iteration 206/1000 | Loss: 0.00002469
Iteration 207/1000 | Loss: 0.00003795
Iteration 208/1000 | Loss: 0.00002642
Iteration 209/1000 | Loss: 0.00002547
Iteration 210/1000 | Loss: 0.00002457
Iteration 211/1000 | Loss: 0.00002453
Iteration 212/1000 | Loss: 0.00002453
Iteration 213/1000 | Loss: 0.00002452
Iteration 214/1000 | Loss: 0.00002452
Iteration 215/1000 | Loss: 0.00002452
Iteration 216/1000 | Loss: 0.00002449
Iteration 217/1000 | Loss: 0.00002447
Iteration 218/1000 | Loss: 0.00002447
Iteration 219/1000 | Loss: 0.00002445
Iteration 220/1000 | Loss: 0.00002444
Iteration 221/1000 | Loss: 0.00002443
Iteration 222/1000 | Loss: 0.00002442
Iteration 223/1000 | Loss: 0.00002442
Iteration 224/1000 | Loss: 0.00002441
Iteration 225/1000 | Loss: 0.00002441
Iteration 226/1000 | Loss: 0.00002441
Iteration 227/1000 | Loss: 0.00002441
Iteration 228/1000 | Loss: 0.00002440
Iteration 229/1000 | Loss: 0.00002438
Iteration 230/1000 | Loss: 0.00002438
Iteration 231/1000 | Loss: 0.00002438
Iteration 232/1000 | Loss: 0.00002438
Iteration 233/1000 | Loss: 0.00002437
Iteration 234/1000 | Loss: 0.00002437
Iteration 235/1000 | Loss: 0.00002437
Iteration 236/1000 | Loss: 0.00002437
Iteration 237/1000 | Loss: 0.00002437
Iteration 238/1000 | Loss: 0.00002437
Iteration 239/1000 | Loss: 0.00002436
Iteration 240/1000 | Loss: 0.00002436
Iteration 241/1000 | Loss: 0.00002436
Iteration 242/1000 | Loss: 0.00002436
Iteration 243/1000 | Loss: 0.00002435
Iteration 244/1000 | Loss: 0.00002435
Iteration 245/1000 | Loss: 0.00002435
Iteration 246/1000 | Loss: 0.00002435
Iteration 247/1000 | Loss: 0.00002435
Iteration 248/1000 | Loss: 0.00002435
Iteration 249/1000 | Loss: 0.00002435
Iteration 250/1000 | Loss: 0.00002435
Iteration 251/1000 | Loss: 0.00002435
Iteration 252/1000 | Loss: 0.00002434
Iteration 253/1000 | Loss: 0.00002434
Iteration 254/1000 | Loss: 0.00002434
Iteration 255/1000 | Loss: 0.00002434
Iteration 256/1000 | Loss: 0.00002434
Iteration 257/1000 | Loss: 0.00002434
Iteration 258/1000 | Loss: 0.00002434
Iteration 259/1000 | Loss: 0.00002434
Iteration 260/1000 | Loss: 0.00002434
Iteration 261/1000 | Loss: 0.00002434
Iteration 262/1000 | Loss: 0.00002434
Iteration 263/1000 | Loss: 0.00002434
Iteration 264/1000 | Loss: 0.00002434
Iteration 265/1000 | Loss: 0.00002434
Iteration 266/1000 | Loss: 0.00002433
Iteration 267/1000 | Loss: 0.00002433
Iteration 268/1000 | Loss: 0.00002433
Iteration 269/1000 | Loss: 0.00002433
Iteration 270/1000 | Loss: 0.00002433
Iteration 271/1000 | Loss: 0.00002433
Iteration 272/1000 | Loss: 0.00002433
Iteration 273/1000 | Loss: 0.00002433
Iteration 274/1000 | Loss: 0.00002433
Iteration 275/1000 | Loss: 0.00002433
Iteration 276/1000 | Loss: 0.00002433
Iteration 277/1000 | Loss: 0.00002433
Iteration 278/1000 | Loss: 0.00002433
Iteration 279/1000 | Loss: 0.00002433
Iteration 280/1000 | Loss: 0.00002433
Iteration 281/1000 | Loss: 0.00002433
Iteration 282/1000 | Loss: 0.00002433
Iteration 283/1000 | Loss: 0.00002433
Iteration 284/1000 | Loss: 0.00002433
Iteration 285/1000 | Loss: 0.00002433
Iteration 286/1000 | Loss: 0.00002433
Iteration 287/1000 | Loss: 0.00002433
Iteration 288/1000 | Loss: 0.00002433
Iteration 289/1000 | Loss: 0.00002433
Iteration 290/1000 | Loss: 0.00002433
Iteration 291/1000 | Loss: 0.00002433
Iteration 292/1000 | Loss: 0.00002433
Iteration 293/1000 | Loss: 0.00002433
Iteration 294/1000 | Loss: 0.00002433
Iteration 295/1000 | Loss: 0.00002433
Iteration 296/1000 | Loss: 0.00002433
Iteration 297/1000 | Loss: 0.00002433
Iteration 298/1000 | Loss: 0.00002433
Iteration 299/1000 | Loss: 0.00002433
Iteration 300/1000 | Loss: 0.00002433
Iteration 301/1000 | Loss: 0.00002433
Iteration 302/1000 | Loss: 0.00002433
Iteration 303/1000 | Loss: 0.00002433
Iteration 304/1000 | Loss: 0.00002433
Iteration 305/1000 | Loss: 0.00002433
Iteration 306/1000 | Loss: 0.00002433
Iteration 307/1000 | Loss: 0.00002433
Iteration 308/1000 | Loss: 0.00002433
Iteration 309/1000 | Loss: 0.00002433
Iteration 310/1000 | Loss: 0.00002433
Iteration 311/1000 | Loss: 0.00002433
Iteration 312/1000 | Loss: 0.00002433
Iteration 313/1000 | Loss: 0.00002433
Iteration 314/1000 | Loss: 0.00002433
Iteration 315/1000 | Loss: 0.00002433
Iteration 316/1000 | Loss: 0.00002433
Iteration 317/1000 | Loss: 0.00002433
Iteration 318/1000 | Loss: 0.00002433
Iteration 319/1000 | Loss: 0.00002433
Iteration 320/1000 | Loss: 0.00002433
Iteration 321/1000 | Loss: 0.00002433
Iteration 322/1000 | Loss: 0.00002433
Iteration 323/1000 | Loss: 0.00002433
Iteration 324/1000 | Loss: 0.00002433
Iteration 325/1000 | Loss: 0.00002433
Iteration 326/1000 | Loss: 0.00002433
Iteration 327/1000 | Loss: 0.00002433
Iteration 328/1000 | Loss: 0.00002433
Iteration 329/1000 | Loss: 0.00002433
Iteration 330/1000 | Loss: 0.00002433
Iteration 331/1000 | Loss: 0.00002433
Iteration 332/1000 | Loss: 0.00002433
Iteration 333/1000 | Loss: 0.00002433
Iteration 334/1000 | Loss: 0.00002433
Iteration 335/1000 | Loss: 0.00002433
Iteration 336/1000 | Loss: 0.00002433
Iteration 337/1000 | Loss: 0.00002433
Iteration 338/1000 | Loss: 0.00002433
Iteration 339/1000 | Loss: 0.00002433
Iteration 340/1000 | Loss: 0.00002433
Iteration 341/1000 | Loss: 0.00002433
Iteration 342/1000 | Loss: 0.00002433
Iteration 343/1000 | Loss: 0.00002433
Iteration 344/1000 | Loss: 0.00002433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 344. Stopping optimization.
Last 5 losses: [2.4331719032488763e-05, 2.4331719032488763e-05, 2.4331719032488763e-05, 2.4331719032488763e-05, 2.4331719032488763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4331719032488763e-05

Optimization complete. Final v2v error: 3.8357596397399902 mm

Highest mean error: 10.784032821655273 mm for frame 43

Lowest mean error: 3.4325509071350098 mm for frame 132

Saving results

Total time: 301.6113021373749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970383
Iteration 2/25 | Loss: 0.00388507
Iteration 3/25 | Loss: 0.00232333
Iteration 4/25 | Loss: 0.00206842
Iteration 5/25 | Loss: 0.00193825
Iteration 6/25 | Loss: 0.00188383
Iteration 7/25 | Loss: 0.00185404
Iteration 8/25 | Loss: 0.00182276
Iteration 9/25 | Loss: 0.00181456
Iteration 10/25 | Loss: 0.00181551
Iteration 11/25 | Loss: 0.00180515
Iteration 12/25 | Loss: 0.00179541
Iteration 13/25 | Loss: 0.00178925
Iteration 14/25 | Loss: 0.00178428
Iteration 15/25 | Loss: 0.00178670
Iteration 16/25 | Loss: 0.00177931
Iteration 17/25 | Loss: 0.00177195
Iteration 18/25 | Loss: 0.00177113
Iteration 19/25 | Loss: 0.00180717
Iteration 20/25 | Loss: 0.00177442
Iteration 21/25 | Loss: 0.00170195
Iteration 22/25 | Loss: 0.00164255
Iteration 23/25 | Loss: 0.00161440
Iteration 24/25 | Loss: 0.00161183
Iteration 25/25 | Loss: 0.00160191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43048322
Iteration 2/25 | Loss: 0.00493628
Iteration 3/25 | Loss: 0.00466343
Iteration 4/25 | Loss: 0.00466341
Iteration 5/25 | Loss: 0.00466341
Iteration 6/25 | Loss: 0.00466341
Iteration 7/25 | Loss: 0.00466341
Iteration 8/25 | Loss: 0.00466341
Iteration 9/25 | Loss: 0.00466341
Iteration 10/25 | Loss: 0.00466341
Iteration 11/25 | Loss: 0.00466341
Iteration 12/25 | Loss: 0.00466341
Iteration 13/25 | Loss: 0.00466341
Iteration 14/25 | Loss: 0.00466341
Iteration 15/25 | Loss: 0.00466341
Iteration 16/25 | Loss: 0.00466341
Iteration 17/25 | Loss: 0.00466341
Iteration 18/25 | Loss: 0.00466341
Iteration 19/25 | Loss: 0.00466341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004663410596549511, 0.004663410596549511, 0.004663410596549511, 0.004663410596549511, 0.004663410596549511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004663410596549511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00466341
Iteration 2/1000 | Loss: 0.00172770
Iteration 3/1000 | Loss: 0.00090653
Iteration 4/1000 | Loss: 0.00047896
Iteration 5/1000 | Loss: 0.00079378
Iteration 6/1000 | Loss: 0.00120068
Iteration 7/1000 | Loss: 0.00067243
Iteration 8/1000 | Loss: 0.00108302
Iteration 9/1000 | Loss: 0.00070752
Iteration 10/1000 | Loss: 0.00038000
Iteration 11/1000 | Loss: 0.00084906
Iteration 12/1000 | Loss: 0.00032431
Iteration 13/1000 | Loss: 0.00092328
Iteration 14/1000 | Loss: 0.00166874
Iteration 15/1000 | Loss: 0.00071007
Iteration 16/1000 | Loss: 0.00049597
Iteration 17/1000 | Loss: 0.00034292
Iteration 18/1000 | Loss: 0.00056869
Iteration 19/1000 | Loss: 0.00125477
Iteration 20/1000 | Loss: 0.00068374
Iteration 21/1000 | Loss: 0.00057660
Iteration 22/1000 | Loss: 0.00080723
Iteration 23/1000 | Loss: 0.00060696
Iteration 24/1000 | Loss: 0.00033697
Iteration 25/1000 | Loss: 0.00041941
Iteration 26/1000 | Loss: 0.00086037
Iteration 27/1000 | Loss: 0.00037488
Iteration 28/1000 | Loss: 0.00034961
Iteration 29/1000 | Loss: 0.00055035
Iteration 30/1000 | Loss: 0.00044463
Iteration 31/1000 | Loss: 0.00115653
Iteration 32/1000 | Loss: 0.00136211
Iteration 33/1000 | Loss: 0.00102234
Iteration 34/1000 | Loss: 0.00036726
Iteration 35/1000 | Loss: 0.00030783
Iteration 36/1000 | Loss: 0.00021920
Iteration 37/1000 | Loss: 0.00067937
Iteration 38/1000 | Loss: 0.00034029
Iteration 39/1000 | Loss: 0.00086604
Iteration 40/1000 | Loss: 0.00059103
Iteration 41/1000 | Loss: 0.00138117
Iteration 42/1000 | Loss: 0.00098591
Iteration 43/1000 | Loss: 0.00022717
Iteration 44/1000 | Loss: 0.00027962
Iteration 45/1000 | Loss: 0.00059881
Iteration 46/1000 | Loss: 0.00055244
Iteration 47/1000 | Loss: 0.00132230
Iteration 48/1000 | Loss: 0.00064438
Iteration 49/1000 | Loss: 0.00044565
Iteration 50/1000 | Loss: 0.00024661
Iteration 51/1000 | Loss: 0.00022387
Iteration 52/1000 | Loss: 0.00018997
Iteration 53/1000 | Loss: 0.00113327
Iteration 54/1000 | Loss: 0.00038732
Iteration 55/1000 | Loss: 0.00041230
Iteration 56/1000 | Loss: 0.00027861
Iteration 57/1000 | Loss: 0.00082504
Iteration 58/1000 | Loss: 0.00068452
Iteration 59/1000 | Loss: 0.00030077
Iteration 60/1000 | Loss: 0.00091767
Iteration 61/1000 | Loss: 0.00023435
Iteration 62/1000 | Loss: 0.00062050
Iteration 63/1000 | Loss: 0.00028764
Iteration 64/1000 | Loss: 0.00024100
Iteration 65/1000 | Loss: 0.00052350
Iteration 66/1000 | Loss: 0.00019306
Iteration 67/1000 | Loss: 0.00016034
Iteration 68/1000 | Loss: 0.00040375
Iteration 69/1000 | Loss: 0.00078788
Iteration 70/1000 | Loss: 0.00031996
Iteration 71/1000 | Loss: 0.00015365
Iteration 72/1000 | Loss: 0.00017744
Iteration 73/1000 | Loss: 0.00055832
Iteration 74/1000 | Loss: 0.00014944
Iteration 75/1000 | Loss: 0.00097473
Iteration 76/1000 | Loss: 0.00053096
Iteration 77/1000 | Loss: 0.00039276
Iteration 78/1000 | Loss: 0.00059668
Iteration 79/1000 | Loss: 0.00014972
Iteration 80/1000 | Loss: 0.00023443
Iteration 81/1000 | Loss: 0.00044819
Iteration 82/1000 | Loss: 0.00037034
Iteration 83/1000 | Loss: 0.00024089
Iteration 84/1000 | Loss: 0.00021553
Iteration 85/1000 | Loss: 0.00021100
Iteration 86/1000 | Loss: 0.00089715
Iteration 87/1000 | Loss: 0.00016198
Iteration 88/1000 | Loss: 0.00050933
Iteration 89/1000 | Loss: 0.00039830
Iteration 90/1000 | Loss: 0.00019968
Iteration 91/1000 | Loss: 0.00060025
Iteration 92/1000 | Loss: 0.00022927
Iteration 93/1000 | Loss: 0.00032646
Iteration 94/1000 | Loss: 0.00020819
Iteration 95/1000 | Loss: 0.00014510
Iteration 96/1000 | Loss: 0.00021434
Iteration 97/1000 | Loss: 0.00018039
Iteration 98/1000 | Loss: 0.00017765
Iteration 99/1000 | Loss: 0.00043291
Iteration 100/1000 | Loss: 0.00024643
Iteration 101/1000 | Loss: 0.00024074
Iteration 102/1000 | Loss: 0.00029496
Iteration 103/1000 | Loss: 0.00047536
Iteration 104/1000 | Loss: 0.00047549
Iteration 105/1000 | Loss: 0.00013922
Iteration 106/1000 | Loss: 0.00012296
Iteration 107/1000 | Loss: 0.00040164
Iteration 108/1000 | Loss: 0.00013127
Iteration 109/1000 | Loss: 0.00032963
Iteration 110/1000 | Loss: 0.00029321
Iteration 111/1000 | Loss: 0.00012052
Iteration 112/1000 | Loss: 0.00011899
Iteration 113/1000 | Loss: 0.00034254
Iteration 114/1000 | Loss: 0.00024327
Iteration 115/1000 | Loss: 0.00035028
Iteration 116/1000 | Loss: 0.00024642
Iteration 117/1000 | Loss: 0.00012269
Iteration 118/1000 | Loss: 0.00011712
Iteration 119/1000 | Loss: 0.00011588
Iteration 120/1000 | Loss: 0.00025738
Iteration 121/1000 | Loss: 0.00092302
Iteration 122/1000 | Loss: 0.00073377
Iteration 123/1000 | Loss: 0.00014335
Iteration 124/1000 | Loss: 0.00057252
Iteration 125/1000 | Loss: 0.00038288
Iteration 126/1000 | Loss: 0.00019054
Iteration 127/1000 | Loss: 0.00011462
Iteration 128/1000 | Loss: 0.00034014
Iteration 129/1000 | Loss: 0.00026862
Iteration 130/1000 | Loss: 0.00011121
Iteration 131/1000 | Loss: 0.00033606
Iteration 132/1000 | Loss: 0.00014524
Iteration 133/1000 | Loss: 0.00013128
Iteration 134/1000 | Loss: 0.00011377
Iteration 135/1000 | Loss: 0.00012568
Iteration 136/1000 | Loss: 0.00026340
Iteration 137/1000 | Loss: 0.00010767
Iteration 138/1000 | Loss: 0.00010666
Iteration 139/1000 | Loss: 0.00010617
Iteration 140/1000 | Loss: 0.00026579
Iteration 141/1000 | Loss: 0.00011138
Iteration 142/1000 | Loss: 0.00010674
Iteration 143/1000 | Loss: 0.00010460
Iteration 144/1000 | Loss: 0.00010321
Iteration 145/1000 | Loss: 0.00010250
Iteration 146/1000 | Loss: 0.00010608
Iteration 147/1000 | Loss: 0.00010290
Iteration 148/1000 | Loss: 0.00010145
Iteration 149/1000 | Loss: 0.00010082
Iteration 150/1000 | Loss: 0.00010040
Iteration 151/1000 | Loss: 0.00010000
Iteration 152/1000 | Loss: 0.00010691
Iteration 153/1000 | Loss: 0.00028258
Iteration 154/1000 | Loss: 0.00010372
Iteration 155/1000 | Loss: 0.00013073
Iteration 156/1000 | Loss: 0.00131856
Iteration 157/1000 | Loss: 0.00086486
Iteration 158/1000 | Loss: 0.00080614
Iteration 159/1000 | Loss: 0.00011572
Iteration 160/1000 | Loss: 0.00066571
Iteration 161/1000 | Loss: 0.00011085
Iteration 162/1000 | Loss: 0.00020625
Iteration 163/1000 | Loss: 0.00017397
Iteration 164/1000 | Loss: 0.00016506
Iteration 165/1000 | Loss: 0.00018433
Iteration 166/1000 | Loss: 0.00009322
Iteration 167/1000 | Loss: 0.00009097
Iteration 168/1000 | Loss: 0.00008909
Iteration 169/1000 | Loss: 0.00008806
Iteration 170/1000 | Loss: 0.00010423
Iteration 171/1000 | Loss: 0.00008669
Iteration 172/1000 | Loss: 0.00008615
Iteration 173/1000 | Loss: 0.00057599
Iteration 174/1000 | Loss: 0.00035719
Iteration 175/1000 | Loss: 0.00034864
Iteration 176/1000 | Loss: 0.00066448
Iteration 177/1000 | Loss: 0.00106463
Iteration 178/1000 | Loss: 0.00046165
Iteration 179/1000 | Loss: 0.00012935
Iteration 180/1000 | Loss: 0.00010073
Iteration 181/1000 | Loss: 0.00017991
Iteration 182/1000 | Loss: 0.00009014
Iteration 183/1000 | Loss: 0.00008662
Iteration 184/1000 | Loss: 0.00008628
Iteration 185/1000 | Loss: 0.00035467
Iteration 186/1000 | Loss: 0.00076722
Iteration 187/1000 | Loss: 0.00041781
Iteration 188/1000 | Loss: 0.00051466
Iteration 189/1000 | Loss: 0.00034608
Iteration 190/1000 | Loss: 0.00036993
Iteration 191/1000 | Loss: 0.00008744
Iteration 192/1000 | Loss: 0.00033126
Iteration 193/1000 | Loss: 0.00012395
Iteration 194/1000 | Loss: 0.00027519
Iteration 195/1000 | Loss: 0.00035911
Iteration 196/1000 | Loss: 0.00046635
Iteration 197/1000 | Loss: 0.00086359
Iteration 198/1000 | Loss: 0.00067529
Iteration 199/1000 | Loss: 0.00060534
Iteration 200/1000 | Loss: 0.00009917
Iteration 201/1000 | Loss: 0.00008800
Iteration 202/1000 | Loss: 0.00008402
Iteration 203/1000 | Loss: 0.00013841
Iteration 204/1000 | Loss: 0.00024288
Iteration 205/1000 | Loss: 0.00018737
Iteration 206/1000 | Loss: 0.00024390
Iteration 207/1000 | Loss: 0.00022851
Iteration 208/1000 | Loss: 0.00018176
Iteration 209/1000 | Loss: 0.00020256
Iteration 210/1000 | Loss: 0.00019840
Iteration 211/1000 | Loss: 0.00021146
Iteration 212/1000 | Loss: 0.00018903
Iteration 213/1000 | Loss: 0.00011100
Iteration 214/1000 | Loss: 0.00009309
Iteration 215/1000 | Loss: 0.00009343
Iteration 216/1000 | Loss: 0.00036611
Iteration 217/1000 | Loss: 0.00107729
Iteration 218/1000 | Loss: 0.00069257
Iteration 219/1000 | Loss: 0.00048628
Iteration 220/1000 | Loss: 0.00021108
Iteration 221/1000 | Loss: 0.00022995
Iteration 222/1000 | Loss: 0.00019767
Iteration 223/1000 | Loss: 0.00025931
Iteration 224/1000 | Loss: 0.00011317
Iteration 225/1000 | Loss: 0.00008338
Iteration 226/1000 | Loss: 0.00010137
Iteration 227/1000 | Loss: 0.00008050
Iteration 228/1000 | Loss: 0.00011354
Iteration 229/1000 | Loss: 0.00009307
Iteration 230/1000 | Loss: 0.00007553
Iteration 231/1000 | Loss: 0.00007615
Iteration 232/1000 | Loss: 0.00113810
Iteration 233/1000 | Loss: 0.00051109
Iteration 234/1000 | Loss: 0.00061872
Iteration 235/1000 | Loss: 0.00025412
Iteration 236/1000 | Loss: 0.00055449
Iteration 237/1000 | Loss: 0.00032562
Iteration 238/1000 | Loss: 0.00008075
Iteration 239/1000 | Loss: 0.00048932
Iteration 240/1000 | Loss: 0.00019410
Iteration 241/1000 | Loss: 0.00008804
Iteration 242/1000 | Loss: 0.00008190
Iteration 243/1000 | Loss: 0.00012008
Iteration 244/1000 | Loss: 0.00036160
Iteration 245/1000 | Loss: 0.00046705
Iteration 246/1000 | Loss: 0.00034146
Iteration 247/1000 | Loss: 0.00025122
Iteration 248/1000 | Loss: 0.00029826
Iteration 249/1000 | Loss: 0.00009782
Iteration 250/1000 | Loss: 0.00007830
Iteration 251/1000 | Loss: 0.00007052
Iteration 252/1000 | Loss: 0.00011052
Iteration 253/1000 | Loss: 0.00006811
Iteration 254/1000 | Loss: 0.00007754
Iteration 255/1000 | Loss: 0.00009757
Iteration 256/1000 | Loss: 0.00006599
Iteration 257/1000 | Loss: 0.00008770
Iteration 258/1000 | Loss: 0.00006505
Iteration 259/1000 | Loss: 0.00006748
Iteration 260/1000 | Loss: 0.00006543
Iteration 261/1000 | Loss: 0.00006393
Iteration 262/1000 | Loss: 0.00006325
Iteration 263/1000 | Loss: 0.00006328
Iteration 264/1000 | Loss: 0.00006304
Iteration 265/1000 | Loss: 0.00006292
Iteration 266/1000 | Loss: 0.00006292
Iteration 267/1000 | Loss: 0.00015010
Iteration 268/1000 | Loss: 0.00006865
Iteration 269/1000 | Loss: 0.00006567
Iteration 270/1000 | Loss: 0.00006423
Iteration 271/1000 | Loss: 0.00008513
Iteration 272/1000 | Loss: 0.00007310
Iteration 273/1000 | Loss: 0.00006314
Iteration 274/1000 | Loss: 0.00006184
Iteration 275/1000 | Loss: 0.00006159
Iteration 276/1000 | Loss: 0.00006643
Iteration 277/1000 | Loss: 0.00006132
Iteration 278/1000 | Loss: 0.00006130
Iteration 279/1000 | Loss: 0.00006130
Iteration 280/1000 | Loss: 0.00006129
Iteration 281/1000 | Loss: 0.00006128
Iteration 282/1000 | Loss: 0.00006128
Iteration 283/1000 | Loss: 0.00006128
Iteration 284/1000 | Loss: 0.00006127
Iteration 285/1000 | Loss: 0.00006127
Iteration 286/1000 | Loss: 0.00006126
Iteration 287/1000 | Loss: 0.00006112
Iteration 288/1000 | Loss: 0.00006112
Iteration 289/1000 | Loss: 0.00006108
Iteration 290/1000 | Loss: 0.00006106
Iteration 291/1000 | Loss: 0.00006106
Iteration 292/1000 | Loss: 0.00006105
Iteration 293/1000 | Loss: 0.00006105
Iteration 294/1000 | Loss: 0.00006105
Iteration 295/1000 | Loss: 0.00006105
Iteration 296/1000 | Loss: 0.00006104
Iteration 297/1000 | Loss: 0.00006104
Iteration 298/1000 | Loss: 0.00006104
Iteration 299/1000 | Loss: 0.00006104
Iteration 300/1000 | Loss: 0.00006104
Iteration 301/1000 | Loss: 0.00006142
Iteration 302/1000 | Loss: 0.00006102
Iteration 303/1000 | Loss: 0.00006102
Iteration 304/1000 | Loss: 0.00006101
Iteration 305/1000 | Loss: 0.00006101
Iteration 306/1000 | Loss: 0.00006101
Iteration 307/1000 | Loss: 0.00006101
Iteration 308/1000 | Loss: 0.00006100
Iteration 309/1000 | Loss: 0.00006100
Iteration 310/1000 | Loss: 0.00006100
Iteration 311/1000 | Loss: 0.00006099
Iteration 312/1000 | Loss: 0.00006099
Iteration 313/1000 | Loss: 0.00006099
Iteration 314/1000 | Loss: 0.00006099
Iteration 315/1000 | Loss: 0.00006099
Iteration 316/1000 | Loss: 0.00006098
Iteration 317/1000 | Loss: 0.00006098
Iteration 318/1000 | Loss: 0.00006098
Iteration 319/1000 | Loss: 0.00006097
Iteration 320/1000 | Loss: 0.00006097
Iteration 321/1000 | Loss: 0.00006097
Iteration 322/1000 | Loss: 0.00008823
Iteration 323/1000 | Loss: 0.00006240
Iteration 324/1000 | Loss: 0.00009778
Iteration 325/1000 | Loss: 0.00006095
Iteration 326/1000 | Loss: 0.00006545
Iteration 327/1000 | Loss: 0.00006087
Iteration 328/1000 | Loss: 0.00006087
Iteration 329/1000 | Loss: 0.00006087
Iteration 330/1000 | Loss: 0.00006086
Iteration 331/1000 | Loss: 0.00006086
Iteration 332/1000 | Loss: 0.00006086
Iteration 333/1000 | Loss: 0.00006086
Iteration 334/1000 | Loss: 0.00006086
Iteration 335/1000 | Loss: 0.00006086
Iteration 336/1000 | Loss: 0.00006086
Iteration 337/1000 | Loss: 0.00006086
Iteration 338/1000 | Loss: 0.00006086
Iteration 339/1000 | Loss: 0.00006086
Iteration 340/1000 | Loss: 0.00006406
Iteration 341/1000 | Loss: 0.00006085
Iteration 342/1000 | Loss: 0.00006085
Iteration 343/1000 | Loss: 0.00006085
Iteration 344/1000 | Loss: 0.00006085
Iteration 345/1000 | Loss: 0.00006085
Iteration 346/1000 | Loss: 0.00006085
Iteration 347/1000 | Loss: 0.00006085
Iteration 348/1000 | Loss: 0.00006085
Iteration 349/1000 | Loss: 0.00006085
Iteration 350/1000 | Loss: 0.00006084
Iteration 351/1000 | Loss: 0.00006084
Iteration 352/1000 | Loss: 0.00006084
Iteration 353/1000 | Loss: 0.00006084
Iteration 354/1000 | Loss: 0.00006083
Iteration 355/1000 | Loss: 0.00006083
Iteration 356/1000 | Loss: 0.00006083
Iteration 357/1000 | Loss: 0.00006082
Iteration 358/1000 | Loss: 0.00006082
Iteration 359/1000 | Loss: 0.00006082
Iteration 360/1000 | Loss: 0.00006082
Iteration 361/1000 | Loss: 0.00006082
Iteration 362/1000 | Loss: 0.00006081
Iteration 363/1000 | Loss: 0.00006081
Iteration 364/1000 | Loss: 0.00006081
Iteration 365/1000 | Loss: 0.00006081
Iteration 366/1000 | Loss: 0.00006081
Iteration 367/1000 | Loss: 0.00006081
Iteration 368/1000 | Loss: 0.00006080
Iteration 369/1000 | Loss: 0.00006395
Iteration 370/1000 | Loss: 0.00006078
Iteration 371/1000 | Loss: 0.00006078
Iteration 372/1000 | Loss: 0.00006078
Iteration 373/1000 | Loss: 0.00006078
Iteration 374/1000 | Loss: 0.00006078
Iteration 375/1000 | Loss: 0.00006078
Iteration 376/1000 | Loss: 0.00006077
Iteration 377/1000 | Loss: 0.00006077
Iteration 378/1000 | Loss: 0.00006077
Iteration 379/1000 | Loss: 0.00006077
Iteration 380/1000 | Loss: 0.00006076
Iteration 381/1000 | Loss: 0.00006109
Iteration 382/1000 | Loss: 0.00006072
Iteration 383/1000 | Loss: 0.00006072
Iteration 384/1000 | Loss: 0.00006072
Iteration 385/1000 | Loss: 0.00006072
Iteration 386/1000 | Loss: 0.00006072
Iteration 387/1000 | Loss: 0.00006072
Iteration 388/1000 | Loss: 0.00006072
Iteration 389/1000 | Loss: 0.00006072
Iteration 390/1000 | Loss: 0.00006071
Iteration 391/1000 | Loss: 0.00006071
Iteration 392/1000 | Loss: 0.00006071
Iteration 393/1000 | Loss: 0.00006071
Iteration 394/1000 | Loss: 0.00006071
Iteration 395/1000 | Loss: 0.00006092
Iteration 396/1000 | Loss: 0.00006092
Iteration 397/1000 | Loss: 0.00006092
Iteration 398/1000 | Loss: 0.00006092
Iteration 399/1000 | Loss: 0.00006070
Iteration 400/1000 | Loss: 0.00006055
Iteration 401/1000 | Loss: 0.00007069
Iteration 402/1000 | Loss: 0.00006058
Iteration 403/1000 | Loss: 0.00006038
Iteration 404/1000 | Loss: 0.00006022
Iteration 405/1000 | Loss: 0.00013759
Iteration 406/1000 | Loss: 0.00033461
Iteration 407/1000 | Loss: 0.00027668
Iteration 408/1000 | Loss: 0.00008743
Iteration 409/1000 | Loss: 0.00010270
Iteration 410/1000 | Loss: 0.00006569
Iteration 411/1000 | Loss: 0.00006219
Iteration 412/1000 | Loss: 0.00006487
Iteration 413/1000 | Loss: 0.00006164
Iteration 414/1000 | Loss: 0.00005893
Iteration 415/1000 | Loss: 0.00020601
Iteration 416/1000 | Loss: 0.00015798
Iteration 417/1000 | Loss: 0.00016707
Iteration 418/1000 | Loss: 0.00017475
Iteration 419/1000 | Loss: 0.00007725
Iteration 420/1000 | Loss: 0.00006771
Iteration 421/1000 | Loss: 0.00008973
Iteration 422/1000 | Loss: 0.00006284
Iteration 423/1000 | Loss: 0.00006743
Iteration 424/1000 | Loss: 0.00035346
Iteration 425/1000 | Loss: 0.00021650
Iteration 426/1000 | Loss: 0.00005793
Iteration 427/1000 | Loss: 0.00007561
Iteration 428/1000 | Loss: 0.00005721
Iteration 429/1000 | Loss: 0.00026738
Iteration 430/1000 | Loss: 0.00022998
Iteration 431/1000 | Loss: 0.00019238
Iteration 432/1000 | Loss: 0.00010242
Iteration 433/1000 | Loss: 0.00024407
Iteration 434/1000 | Loss: 0.00013484
Iteration 435/1000 | Loss: 0.00005892
Iteration 436/1000 | Loss: 0.00007327
Iteration 437/1000 | Loss: 0.00005983
Iteration 438/1000 | Loss: 0.00026764
Iteration 439/1000 | Loss: 0.00033192
Iteration 440/1000 | Loss: 0.00010682
Iteration 441/1000 | Loss: 0.00005744
Iteration 442/1000 | Loss: 0.00026694
Iteration 443/1000 | Loss: 0.00020297
Iteration 444/1000 | Loss: 0.00023347
Iteration 445/1000 | Loss: 0.00020258
Iteration 446/1000 | Loss: 0.00076045
Iteration 447/1000 | Loss: 0.00055139
Iteration 448/1000 | Loss: 0.00060608
Iteration 449/1000 | Loss: 0.00031574
Iteration 450/1000 | Loss: 0.00009433
Iteration 451/1000 | Loss: 0.00034264
Iteration 452/1000 | Loss: 0.00033313
Iteration 453/1000 | Loss: 0.00007839
Iteration 454/1000 | Loss: 0.00007584
Iteration 455/1000 | Loss: 0.00006187
Iteration 456/1000 | Loss: 0.00032228
Iteration 457/1000 | Loss: 0.00018785
Iteration 458/1000 | Loss: 0.00008435
Iteration 459/1000 | Loss: 0.00007383
Iteration 460/1000 | Loss: 0.00035332
Iteration 461/1000 | Loss: 0.00045682
Iteration 462/1000 | Loss: 0.00031686
Iteration 463/1000 | Loss: 0.00024397
Iteration 464/1000 | Loss: 0.00007783
Iteration 465/1000 | Loss: 0.00006050
Iteration 466/1000 | Loss: 0.00015080
Iteration 467/1000 | Loss: 0.00022318
Iteration 468/1000 | Loss: 0.00015131
Iteration 469/1000 | Loss: 0.00018224
Iteration 470/1000 | Loss: 0.00019188
Iteration 471/1000 | Loss: 0.00018243
Iteration 472/1000 | Loss: 0.00018529
Iteration 473/1000 | Loss: 0.00019625
Iteration 474/1000 | Loss: 0.00025730
Iteration 475/1000 | Loss: 0.00026549
Iteration 476/1000 | Loss: 0.00020046
Iteration 477/1000 | Loss: 0.00009713
Iteration 478/1000 | Loss: 0.00007221
Iteration 479/1000 | Loss: 0.00005784
Iteration 480/1000 | Loss: 0.00005512
Iteration 481/1000 | Loss: 0.00027147
Iteration 482/1000 | Loss: 0.00015280
Iteration 483/1000 | Loss: 0.00021893
Iteration 484/1000 | Loss: 0.00021036
Iteration 485/1000 | Loss: 0.00020173
Iteration 486/1000 | Loss: 0.00007448
Iteration 487/1000 | Loss: 0.00019442
Iteration 488/1000 | Loss: 0.00013923
Iteration 489/1000 | Loss: 0.00016898
Iteration 490/1000 | Loss: 0.00018035
Iteration 491/1000 | Loss: 0.00005593
Iteration 492/1000 | Loss: 0.00005463
Iteration 493/1000 | Loss: 0.00005361
Iteration 494/1000 | Loss: 0.00007185
Iteration 495/1000 | Loss: 0.00022304
Iteration 496/1000 | Loss: 0.00005816
Iteration 497/1000 | Loss: 0.00009337
Iteration 498/1000 | Loss: 0.00011272
Iteration 499/1000 | Loss: 0.00006176
Iteration 500/1000 | Loss: 0.00013692
Iteration 501/1000 | Loss: 0.00060330
Iteration 502/1000 | Loss: 0.00027892
Iteration 503/1000 | Loss: 0.00015666
Iteration 504/1000 | Loss: 0.00005093
Iteration 505/1000 | Loss: 0.00004878
Iteration 506/1000 | Loss: 0.00025739
Iteration 507/1000 | Loss: 0.00005447
Iteration 508/1000 | Loss: 0.00005111
Iteration 509/1000 | Loss: 0.00004897
Iteration 510/1000 | Loss: 0.00011189
Iteration 511/1000 | Loss: 0.00005106
Iteration 512/1000 | Loss: 0.00005200
Iteration 513/1000 | Loss: 0.00004755
Iteration 514/1000 | Loss: 0.00025648
Iteration 515/1000 | Loss: 0.00015239
Iteration 516/1000 | Loss: 0.00012999
Iteration 517/1000 | Loss: 0.00005352
Iteration 518/1000 | Loss: 0.00029015
Iteration 519/1000 | Loss: 0.00043631
Iteration 520/1000 | Loss: 0.00087506
Iteration 521/1000 | Loss: 0.00025516
Iteration 522/1000 | Loss: 0.00043547
Iteration 523/1000 | Loss: 0.00030046
Iteration 524/1000 | Loss: 0.00016007
Iteration 525/1000 | Loss: 0.00012911
Iteration 526/1000 | Loss: 0.00007468
Iteration 527/1000 | Loss: 0.00006179
Iteration 528/1000 | Loss: 0.00037393
Iteration 529/1000 | Loss: 0.00013239
Iteration 530/1000 | Loss: 0.00026266
Iteration 531/1000 | Loss: 0.00024543
Iteration 532/1000 | Loss: 0.00018978
Iteration 533/1000 | Loss: 0.00015077
Iteration 534/1000 | Loss: 0.00014452
Iteration 535/1000 | Loss: 0.00013168
Iteration 536/1000 | Loss: 0.00005386
Iteration 537/1000 | Loss: 0.00015182
Iteration 538/1000 | Loss: 0.00025427
Iteration 539/1000 | Loss: 0.00022243
Iteration 540/1000 | Loss: 0.00022838
Iteration 541/1000 | Loss: 0.00012299
Iteration 542/1000 | Loss: 0.00004517
Iteration 543/1000 | Loss: 0.00011802
Iteration 544/1000 | Loss: 0.00013478
Iteration 545/1000 | Loss: 0.00013255
Iteration 546/1000 | Loss: 0.00009584
Iteration 547/1000 | Loss: 0.00004351
Iteration 548/1000 | Loss: 0.00005066
Iteration 549/1000 | Loss: 0.00005775
Iteration 550/1000 | Loss: 0.00004159
Iteration 551/1000 | Loss: 0.00004142
Iteration 552/1000 | Loss: 0.00014445
Iteration 553/1000 | Loss: 0.00009211
Iteration 554/1000 | Loss: 0.00013387
Iteration 555/1000 | Loss: 0.00013925
Iteration 556/1000 | Loss: 0.00009330
Iteration 557/1000 | Loss: 0.00015161
Iteration 558/1000 | Loss: 0.00009189
Iteration 559/1000 | Loss: 0.00016281
Iteration 560/1000 | Loss: 0.00005078
Iteration 561/1000 | Loss: 0.00012263
Iteration 562/1000 | Loss: 0.00018424
Iteration 563/1000 | Loss: 0.00012584
Iteration 564/1000 | Loss: 0.00004308
Iteration 565/1000 | Loss: 0.00009482
Iteration 566/1000 | Loss: 0.00013878
Iteration 567/1000 | Loss: 0.00009558
Iteration 568/1000 | Loss: 0.00005192
Iteration 569/1000 | Loss: 0.00005334
Iteration 570/1000 | Loss: 0.00009142
Iteration 571/1000 | Loss: 0.00003830
Iteration 572/1000 | Loss: 0.00004926
Iteration 573/1000 | Loss: 0.00004037
Iteration 574/1000 | Loss: 0.00003554
Iteration 575/1000 | Loss: 0.00003494
Iteration 576/1000 | Loss: 0.00003572
Iteration 577/1000 | Loss: 0.00003588
Iteration 578/1000 | Loss: 0.00003540
Iteration 579/1000 | Loss: 0.00004632
Iteration 580/1000 | Loss: 0.00021934
Iteration 581/1000 | Loss: 0.00018081
Iteration 582/1000 | Loss: 0.00042135
Iteration 583/1000 | Loss: 0.00031701
Iteration 584/1000 | Loss: 0.00005440
Iteration 585/1000 | Loss: 0.00032608
Iteration 586/1000 | Loss: 0.00006925
Iteration 587/1000 | Loss: 0.00028700
Iteration 588/1000 | Loss: 0.00020838
Iteration 589/1000 | Loss: 0.00041509
Iteration 590/1000 | Loss: 0.00020056
Iteration 591/1000 | Loss: 0.00004078
Iteration 592/1000 | Loss: 0.00003924
Iteration 593/1000 | Loss: 0.00003512
Iteration 594/1000 | Loss: 0.00003412
Iteration 595/1000 | Loss: 0.00003354
Iteration 596/1000 | Loss: 0.00024750
Iteration 597/1000 | Loss: 0.00018515
Iteration 598/1000 | Loss: 0.00014260
Iteration 599/1000 | Loss: 0.00003883
Iteration 600/1000 | Loss: 0.00003505
Iteration 601/1000 | Loss: 0.00023680
Iteration 602/1000 | Loss: 0.00020428
Iteration 603/1000 | Loss: 0.00014135
Iteration 604/1000 | Loss: 0.00004016
Iteration 605/1000 | Loss: 0.00003826
Iteration 606/1000 | Loss: 0.00006254
Iteration 607/1000 | Loss: 0.00003919
Iteration 608/1000 | Loss: 0.00003938
Iteration 609/1000 | Loss: 0.00003571
Iteration 610/1000 | Loss: 0.00003484
Iteration 611/1000 | Loss: 0.00003410
Iteration 612/1000 | Loss: 0.00003558
Iteration 613/1000 | Loss: 0.00003219
Iteration 614/1000 | Loss: 0.00003256
Iteration 615/1000 | Loss: 0.00057619
Iteration 616/1000 | Loss: 0.00040442
Iteration 617/1000 | Loss: 0.00013565
Iteration 618/1000 | Loss: 0.00049152
Iteration 619/1000 | Loss: 0.00033013
Iteration 620/1000 | Loss: 0.00047359
Iteration 621/1000 | Loss: 0.00027522
Iteration 622/1000 | Loss: 0.00024794
Iteration 623/1000 | Loss: 0.00025826
Iteration 624/1000 | Loss: 0.00021987
Iteration 625/1000 | Loss: 0.00024048
Iteration 626/1000 | Loss: 0.00018129
Iteration 627/1000 | Loss: 0.00008209
Iteration 628/1000 | Loss: 0.00006400
Iteration 629/1000 | Loss: 0.00003607
Iteration 630/1000 | Loss: 0.00006603
Iteration 631/1000 | Loss: 0.00005958
Iteration 632/1000 | Loss: 0.00004911
Iteration 633/1000 | Loss: 0.00003682
Iteration 634/1000 | Loss: 0.00003291
Iteration 635/1000 | Loss: 0.00006216
Iteration 636/1000 | Loss: 0.00004713
Iteration 637/1000 | Loss: 0.00026382
Iteration 638/1000 | Loss: 0.00037702
Iteration 639/1000 | Loss: 0.00014900
Iteration 640/1000 | Loss: 0.00003418
Iteration 641/1000 | Loss: 0.00003251
Iteration 642/1000 | Loss: 0.00003037
Iteration 643/1000 | Loss: 0.00003003
Iteration 644/1000 | Loss: 0.00003035
Iteration 645/1000 | Loss: 0.00003076
Iteration 646/1000 | Loss: 0.00024101
Iteration 647/1000 | Loss: 0.00015518
Iteration 648/1000 | Loss: 0.00003475
Iteration 649/1000 | Loss: 0.00003626
Iteration 650/1000 | Loss: 0.00003002
Iteration 651/1000 | Loss: 0.00004889
Iteration 652/1000 | Loss: 0.00024222
Iteration 653/1000 | Loss: 0.00006412
Iteration 654/1000 | Loss: 0.00003734
Iteration 655/1000 | Loss: 0.00009610
Iteration 656/1000 | Loss: 0.00004132
Iteration 657/1000 | Loss: 0.00003435
Iteration 658/1000 | Loss: 0.00007132
Iteration 659/1000 | Loss: 0.00003098
Iteration 660/1000 | Loss: 0.00003066
Iteration 661/1000 | Loss: 0.00002809
Iteration 662/1000 | Loss: 0.00003190
Iteration 663/1000 | Loss: 0.00002958
Iteration 664/1000 | Loss: 0.00022871
Iteration 665/1000 | Loss: 0.00020290
Iteration 666/1000 | Loss: 0.00037245
Iteration 667/1000 | Loss: 0.00068887
Iteration 668/1000 | Loss: 0.00003604
Iteration 669/1000 | Loss: 0.00003158
Iteration 670/1000 | Loss: 0.00003671
Iteration 671/1000 | Loss: 0.00002799
Iteration 672/1000 | Loss: 0.00002735
Iteration 673/1000 | Loss: 0.00002740
Iteration 674/1000 | Loss: 0.00002691
Iteration 675/1000 | Loss: 0.00002689
Iteration 676/1000 | Loss: 0.00002689
Iteration 677/1000 | Loss: 0.00002689
Iteration 678/1000 | Loss: 0.00002689
Iteration 679/1000 | Loss: 0.00002689
Iteration 680/1000 | Loss: 0.00002688
Iteration 681/1000 | Loss: 0.00023637
Iteration 682/1000 | Loss: 0.00052846
Iteration 683/1000 | Loss: 0.00028425
Iteration 684/1000 | Loss: 0.00006351
Iteration 685/1000 | Loss: 0.00009252
Iteration 686/1000 | Loss: 0.00006551
Iteration 687/1000 | Loss: 0.00002854
Iteration 688/1000 | Loss: 0.00003030
Iteration 689/1000 | Loss: 0.00004937
Iteration 690/1000 | Loss: 0.00002785
Iteration 691/1000 | Loss: 0.00003179
Iteration 692/1000 | Loss: 0.00002797
Iteration 693/1000 | Loss: 0.00002824
Iteration 694/1000 | Loss: 0.00002402
Iteration 695/1000 | Loss: 0.00002501
Iteration 696/1000 | Loss: 0.00002381
Iteration 697/1000 | Loss: 0.00002380
Iteration 698/1000 | Loss: 0.00002434
Iteration 699/1000 | Loss: 0.00002434
Iteration 700/1000 | Loss: 0.00002358
Iteration 701/1000 | Loss: 0.00002358
Iteration 702/1000 | Loss: 0.00002965
Iteration 703/1000 | Loss: 0.00002331
Iteration 704/1000 | Loss: 0.00002415
Iteration 705/1000 | Loss: 0.00002326
Iteration 706/1000 | Loss: 0.00002887
Iteration 707/1000 | Loss: 0.00002305
Iteration 708/1000 | Loss: 0.00002304
Iteration 709/1000 | Loss: 0.00002304
Iteration 710/1000 | Loss: 0.00002304
Iteration 711/1000 | Loss: 0.00002304
Iteration 712/1000 | Loss: 0.00002304
Iteration 713/1000 | Loss: 0.00002304
Iteration 714/1000 | Loss: 0.00002303
Iteration 715/1000 | Loss: 0.00002303
Iteration 716/1000 | Loss: 0.00002303
Iteration 717/1000 | Loss: 0.00002301
Iteration 718/1000 | Loss: 0.00002300
Iteration 719/1000 | Loss: 0.00002298
Iteration 720/1000 | Loss: 0.00002296
Iteration 721/1000 | Loss: 0.00002295
Iteration 722/1000 | Loss: 0.00002294
Iteration 723/1000 | Loss: 0.00002293
Iteration 724/1000 | Loss: 0.00002293
Iteration 725/1000 | Loss: 0.00002292
Iteration 726/1000 | Loss: 0.00002292
Iteration 727/1000 | Loss: 0.00002292
Iteration 728/1000 | Loss: 0.00002291
Iteration 729/1000 | Loss: 0.00002291
Iteration 730/1000 | Loss: 0.00002290
Iteration 731/1000 | Loss: 0.00002290
Iteration 732/1000 | Loss: 0.00002289
Iteration 733/1000 | Loss: 0.00002289
Iteration 734/1000 | Loss: 0.00002288
Iteration 735/1000 | Loss: 0.00002684
Iteration 736/1000 | Loss: 0.00021690
Iteration 737/1000 | Loss: 0.00013327
Iteration 738/1000 | Loss: 0.00010923
Iteration 739/1000 | Loss: 0.00006660
Iteration 740/1000 | Loss: 0.00002570
Iteration 741/1000 | Loss: 0.00002489
Iteration 742/1000 | Loss: 0.00002730
Iteration 743/1000 | Loss: 0.00002965
Iteration 744/1000 | Loss: 0.00002287
Iteration 745/1000 | Loss: 0.00002286
Iteration 746/1000 | Loss: 0.00002285
Iteration 747/1000 | Loss: 0.00002284
Iteration 748/1000 | Loss: 0.00002284
Iteration 749/1000 | Loss: 0.00002284
Iteration 750/1000 | Loss: 0.00002284
Iteration 751/1000 | Loss: 0.00002284
Iteration 752/1000 | Loss: 0.00002284
Iteration 753/1000 | Loss: 0.00002284
Iteration 754/1000 | Loss: 0.00002284
Iteration 755/1000 | Loss: 0.00002283
Iteration 756/1000 | Loss: 0.00002283
Iteration 757/1000 | Loss: 0.00002283
Iteration 758/1000 | Loss: 0.00002283
Iteration 759/1000 | Loss: 0.00002283
Iteration 760/1000 | Loss: 0.00002283
Iteration 761/1000 | Loss: 0.00002283
Iteration 762/1000 | Loss: 0.00002283
Iteration 763/1000 | Loss: 0.00002283
Iteration 764/1000 | Loss: 0.00002283
Iteration 765/1000 | Loss: 0.00002282
Iteration 766/1000 | Loss: 0.00002282
Iteration 767/1000 | Loss: 0.00002282
Iteration 768/1000 | Loss: 0.00002282
Iteration 769/1000 | Loss: 0.00002282
Iteration 770/1000 | Loss: 0.00002282
Iteration 771/1000 | Loss: 0.00002282
Iteration 772/1000 | Loss: 0.00002282
Iteration 773/1000 | Loss: 0.00002282
Iteration 774/1000 | Loss: 0.00002281
Iteration 775/1000 | Loss: 0.00002281
Iteration 776/1000 | Loss: 0.00002281
Iteration 777/1000 | Loss: 0.00002281
Iteration 778/1000 | Loss: 0.00002281
Iteration 779/1000 | Loss: 0.00002280
Iteration 780/1000 | Loss: 0.00002280
Iteration 781/1000 | Loss: 0.00002280
Iteration 782/1000 | Loss: 0.00002279
Iteration 783/1000 | Loss: 0.00002279
Iteration 784/1000 | Loss: 0.00002279
Iteration 785/1000 | Loss: 0.00002279
Iteration 786/1000 | Loss: 0.00002672
Iteration 787/1000 | Loss: 0.00002277
Iteration 788/1000 | Loss: 0.00002276
Iteration 789/1000 | Loss: 0.00002276
Iteration 790/1000 | Loss: 0.00002276
Iteration 791/1000 | Loss: 0.00002276
Iteration 792/1000 | Loss: 0.00002275
Iteration 793/1000 | Loss: 0.00002275
Iteration 794/1000 | Loss: 0.00002275
Iteration 795/1000 | Loss: 0.00002277
Iteration 796/1000 | Loss: 0.00002275
Iteration 797/1000 | Loss: 0.00002275
Iteration 798/1000 | Loss: 0.00002275
Iteration 799/1000 | Loss: 0.00002275
Iteration 800/1000 | Loss: 0.00002275
Iteration 801/1000 | Loss: 0.00002275
Iteration 802/1000 | Loss: 0.00002275
Iteration 803/1000 | Loss: 0.00002275
Iteration 804/1000 | Loss: 0.00002275
Iteration 805/1000 | Loss: 0.00002275
Iteration 806/1000 | Loss: 0.00002275
Iteration 807/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 807. Stopping optimization.
Last 5 losses: [2.2748739866074175e-05, 2.2748739866074175e-05, 2.2748739866074175e-05, 2.2748739866074175e-05, 2.2748739866074175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2748739866074175e-05

Optimization complete. Final v2v error: 3.582331418991089 mm

Highest mean error: 11.317832946777344 mm for frame 14

Lowest mean error: 2.9466209411621094 mm for frame 1

Saving results

Total time: 1026.651617050171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842569
Iteration 2/25 | Loss: 0.00139635
Iteration 3/25 | Loss: 0.00131771
Iteration 4/25 | Loss: 0.00129589
Iteration 5/25 | Loss: 0.00128961
Iteration 6/25 | Loss: 0.00128961
Iteration 7/25 | Loss: 0.00128961
Iteration 8/25 | Loss: 0.00128961
Iteration 9/25 | Loss: 0.00128961
Iteration 10/25 | Loss: 0.00128961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012896107509732246, 0.0012896107509732246, 0.0012896107509732246, 0.0012896107509732246, 0.0012896107509732246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012896107509732246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42133975
Iteration 2/25 | Loss: 0.00092436
Iteration 3/25 | Loss: 0.00092436
Iteration 4/25 | Loss: 0.00092436
Iteration 5/25 | Loss: 0.00092436
Iteration 6/25 | Loss: 0.00092435
Iteration 7/25 | Loss: 0.00092435
Iteration 8/25 | Loss: 0.00092435
Iteration 9/25 | Loss: 0.00092435
Iteration 10/25 | Loss: 0.00092435
Iteration 11/25 | Loss: 0.00092435
Iteration 12/25 | Loss: 0.00092435
Iteration 13/25 | Loss: 0.00092435
Iteration 14/25 | Loss: 0.00092435
Iteration 15/25 | Loss: 0.00092435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009243539534509182, 0.0009243539534509182, 0.0009243539534509182, 0.0009243539534509182, 0.0009243539534509182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009243539534509182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092435
Iteration 2/1000 | Loss: 0.00003127
Iteration 3/1000 | Loss: 0.00002516
Iteration 4/1000 | Loss: 0.00002421
Iteration 5/1000 | Loss: 0.00002350
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00002270
Iteration 8/1000 | Loss: 0.00002233
Iteration 9/1000 | Loss: 0.00002225
Iteration 10/1000 | Loss: 0.00002196
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002178
Iteration 13/1000 | Loss: 0.00002178
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002159
Iteration 16/1000 | Loss: 0.00002158
Iteration 17/1000 | Loss: 0.00002151
Iteration 18/1000 | Loss: 0.00002147
Iteration 19/1000 | Loss: 0.00002144
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002140
Iteration 23/1000 | Loss: 0.00002139
Iteration 24/1000 | Loss: 0.00002139
Iteration 25/1000 | Loss: 0.00002139
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00002138
Iteration 28/1000 | Loss: 0.00002138
Iteration 29/1000 | Loss: 0.00002137
Iteration 30/1000 | Loss: 0.00002137
Iteration 31/1000 | Loss: 0.00002137
Iteration 32/1000 | Loss: 0.00002136
Iteration 33/1000 | Loss: 0.00002136
Iteration 34/1000 | Loss: 0.00002136
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002129
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002127
Iteration 49/1000 | Loss: 0.00002127
Iteration 50/1000 | Loss: 0.00002127
Iteration 51/1000 | Loss: 0.00002126
Iteration 52/1000 | Loss: 0.00002126
Iteration 53/1000 | Loss: 0.00002126
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002124
Iteration 56/1000 | Loss: 0.00002124
Iteration 57/1000 | Loss: 0.00002124
Iteration 58/1000 | Loss: 0.00002124
Iteration 59/1000 | Loss: 0.00002124
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002122
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002122
Iteration 71/1000 | Loss: 0.00002122
Iteration 72/1000 | Loss: 0.00002122
Iteration 73/1000 | Loss: 0.00002122
Iteration 74/1000 | Loss: 0.00002122
Iteration 75/1000 | Loss: 0.00002122
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002121
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002120
Iteration 81/1000 | Loss: 0.00002120
Iteration 82/1000 | Loss: 0.00002120
Iteration 83/1000 | Loss: 0.00002120
Iteration 84/1000 | Loss: 0.00002120
Iteration 85/1000 | Loss: 0.00002119
Iteration 86/1000 | Loss: 0.00002118
Iteration 87/1000 | Loss: 0.00002118
Iteration 88/1000 | Loss: 0.00002118
Iteration 89/1000 | Loss: 0.00002118
Iteration 90/1000 | Loss: 0.00002117
Iteration 91/1000 | Loss: 0.00002117
Iteration 92/1000 | Loss: 0.00002117
Iteration 93/1000 | Loss: 0.00002117
Iteration 94/1000 | Loss: 0.00002117
Iteration 95/1000 | Loss: 0.00002117
Iteration 96/1000 | Loss: 0.00002117
Iteration 97/1000 | Loss: 0.00002116
Iteration 98/1000 | Loss: 0.00002116
Iteration 99/1000 | Loss: 0.00002116
Iteration 100/1000 | Loss: 0.00002115
Iteration 101/1000 | Loss: 0.00002115
Iteration 102/1000 | Loss: 0.00002115
Iteration 103/1000 | Loss: 0.00002115
Iteration 104/1000 | Loss: 0.00002114
Iteration 105/1000 | Loss: 0.00002114
Iteration 106/1000 | Loss: 0.00002114
Iteration 107/1000 | Loss: 0.00002114
Iteration 108/1000 | Loss: 0.00002114
Iteration 109/1000 | Loss: 0.00002114
Iteration 110/1000 | Loss: 0.00002114
Iteration 111/1000 | Loss: 0.00002114
Iteration 112/1000 | Loss: 0.00002114
Iteration 113/1000 | Loss: 0.00002114
Iteration 114/1000 | Loss: 0.00002114
Iteration 115/1000 | Loss: 0.00002114
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002113
Iteration 119/1000 | Loss: 0.00002113
Iteration 120/1000 | Loss: 0.00002113
Iteration 121/1000 | Loss: 0.00002113
Iteration 122/1000 | Loss: 0.00002113
Iteration 123/1000 | Loss: 0.00002113
Iteration 124/1000 | Loss: 0.00002113
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002113
Iteration 129/1000 | Loss: 0.00002113
Iteration 130/1000 | Loss: 0.00002113
Iteration 131/1000 | Loss: 0.00002113
Iteration 132/1000 | Loss: 0.00002113
Iteration 133/1000 | Loss: 0.00002113
Iteration 134/1000 | Loss: 0.00002113
Iteration 135/1000 | Loss: 0.00002112
Iteration 136/1000 | Loss: 0.00002112
Iteration 137/1000 | Loss: 0.00002112
Iteration 138/1000 | Loss: 0.00002112
Iteration 139/1000 | Loss: 0.00002111
Iteration 140/1000 | Loss: 0.00002111
Iteration 141/1000 | Loss: 0.00002111
Iteration 142/1000 | Loss: 0.00002110
Iteration 143/1000 | Loss: 0.00002110
Iteration 144/1000 | Loss: 0.00002110
Iteration 145/1000 | Loss: 0.00002109
Iteration 146/1000 | Loss: 0.00002109
Iteration 147/1000 | Loss: 0.00002109
Iteration 148/1000 | Loss: 0.00002108
Iteration 149/1000 | Loss: 0.00002108
Iteration 150/1000 | Loss: 0.00002108
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002107
Iteration 155/1000 | Loss: 0.00002107
Iteration 156/1000 | Loss: 0.00002106
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002106
Iteration 160/1000 | Loss: 0.00002106
Iteration 161/1000 | Loss: 0.00002106
Iteration 162/1000 | Loss: 0.00002106
Iteration 163/1000 | Loss: 0.00002106
Iteration 164/1000 | Loss: 0.00002106
Iteration 165/1000 | Loss: 0.00002106
Iteration 166/1000 | Loss: 0.00002106
Iteration 167/1000 | Loss: 0.00002106
Iteration 168/1000 | Loss: 0.00002106
Iteration 169/1000 | Loss: 0.00002106
Iteration 170/1000 | Loss: 0.00002106
Iteration 171/1000 | Loss: 0.00002106
Iteration 172/1000 | Loss: 0.00002106
Iteration 173/1000 | Loss: 0.00002106
Iteration 174/1000 | Loss: 0.00002106
Iteration 175/1000 | Loss: 0.00002106
Iteration 176/1000 | Loss: 0.00002106
Iteration 177/1000 | Loss: 0.00002106
Iteration 178/1000 | Loss: 0.00002106
Iteration 179/1000 | Loss: 0.00002106
Iteration 180/1000 | Loss: 0.00002106
Iteration 181/1000 | Loss: 0.00002106
Iteration 182/1000 | Loss: 0.00002106
Iteration 183/1000 | Loss: 0.00002106
Iteration 184/1000 | Loss: 0.00002106
Iteration 185/1000 | Loss: 0.00002106
Iteration 186/1000 | Loss: 0.00002106
Iteration 187/1000 | Loss: 0.00002106
Iteration 188/1000 | Loss: 0.00002106
Iteration 189/1000 | Loss: 0.00002106
Iteration 190/1000 | Loss: 0.00002106
Iteration 191/1000 | Loss: 0.00002106
Iteration 192/1000 | Loss: 0.00002106
Iteration 193/1000 | Loss: 0.00002106
Iteration 194/1000 | Loss: 0.00002106
Iteration 195/1000 | Loss: 0.00002106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.1058523998362944e-05, 2.1058523998362944e-05, 2.1058523998362944e-05, 2.1058523998362944e-05, 2.1058523998362944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1058523998362944e-05

Optimization complete. Final v2v error: 3.862051010131836 mm

Highest mean error: 3.938847303390503 mm for frame 195

Lowest mean error: 3.811128854751587 mm for frame 171

Saving results

Total time: 42.84164595603943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019834
Iteration 2/25 | Loss: 0.00261537
Iteration 3/25 | Loss: 0.00213760
Iteration 4/25 | Loss: 0.00143971
Iteration 5/25 | Loss: 0.00132974
Iteration 6/25 | Loss: 0.00131974
Iteration 7/25 | Loss: 0.00131825
Iteration 8/25 | Loss: 0.00132207
Iteration 9/25 | Loss: 0.00132475
Iteration 10/25 | Loss: 0.00132212
Iteration 11/25 | Loss: 0.00131242
Iteration 12/25 | Loss: 0.00130888
Iteration 13/25 | Loss: 0.00130751
Iteration 14/25 | Loss: 0.00130712
Iteration 15/25 | Loss: 0.00130697
Iteration 16/25 | Loss: 0.00130886
Iteration 17/25 | Loss: 0.00130684
Iteration 18/25 | Loss: 0.00130530
Iteration 19/25 | Loss: 0.00130761
Iteration 20/25 | Loss: 0.00130442
Iteration 21/25 | Loss: 0.00130325
Iteration 22/25 | Loss: 0.00130240
Iteration 23/25 | Loss: 0.00130213
Iteration 24/25 | Loss: 0.00130206
Iteration 25/25 | Loss: 0.00130202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64013684
Iteration 2/25 | Loss: 0.00123495
Iteration 3/25 | Loss: 0.00123495
Iteration 4/25 | Loss: 0.00119124
Iteration 5/25 | Loss: 0.00119124
Iteration 6/25 | Loss: 0.00119124
Iteration 7/25 | Loss: 0.00119124
Iteration 8/25 | Loss: 0.00119124
Iteration 9/25 | Loss: 0.00119124
Iteration 10/25 | Loss: 0.00119124
Iteration 11/25 | Loss: 0.00119124
Iteration 12/25 | Loss: 0.00119124
Iteration 13/25 | Loss: 0.00119124
Iteration 14/25 | Loss: 0.00119124
Iteration 15/25 | Loss: 0.00119124
Iteration 16/25 | Loss: 0.00119124
Iteration 17/25 | Loss: 0.00119124
Iteration 18/25 | Loss: 0.00119124
Iteration 19/25 | Loss: 0.00119124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011912358459085226, 0.0011912358459085226, 0.0011912358459085226, 0.0011912358459085226, 0.0011912358459085226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011912358459085226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119124
Iteration 2/1000 | Loss: 0.00010185
Iteration 3/1000 | Loss: 0.00004408
Iteration 4/1000 | Loss: 0.00003528
Iteration 5/1000 | Loss: 0.00003053
Iteration 6/1000 | Loss: 0.00002710
Iteration 7/1000 | Loss: 0.00002565
Iteration 8/1000 | Loss: 0.00025939
Iteration 9/1000 | Loss: 0.00016954
Iteration 10/1000 | Loss: 0.00003874
Iteration 11/1000 | Loss: 0.00016920
Iteration 12/1000 | Loss: 0.00017988
Iteration 13/1000 | Loss: 0.00014089
Iteration 14/1000 | Loss: 0.00021535
Iteration 15/1000 | Loss: 0.00002777
Iteration 16/1000 | Loss: 0.00024217
Iteration 17/1000 | Loss: 0.00020613
Iteration 18/1000 | Loss: 0.00013552
Iteration 19/1000 | Loss: 0.00013712
Iteration 20/1000 | Loss: 0.00014002
Iteration 21/1000 | Loss: 0.00011107
Iteration 22/1000 | Loss: 0.00039818
Iteration 23/1000 | Loss: 0.00046727
Iteration 24/1000 | Loss: 0.00014151
Iteration 25/1000 | Loss: 0.00014156
Iteration 26/1000 | Loss: 0.00014176
Iteration 27/1000 | Loss: 0.00016403
Iteration 28/1000 | Loss: 0.00025823
Iteration 29/1000 | Loss: 0.00040732
Iteration 30/1000 | Loss: 0.00024926
Iteration 31/1000 | Loss: 0.00041655
Iteration 32/1000 | Loss: 0.00005172
Iteration 33/1000 | Loss: 0.00003407
Iteration 34/1000 | Loss: 0.00002779
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002382
Iteration 37/1000 | Loss: 0.00049414
Iteration 38/1000 | Loss: 0.00025904
Iteration 39/1000 | Loss: 0.00002985
Iteration 40/1000 | Loss: 0.00002315
Iteration 41/1000 | Loss: 0.00025614
Iteration 42/1000 | Loss: 0.00030685
Iteration 43/1000 | Loss: 0.00025051
Iteration 44/1000 | Loss: 0.00002465
Iteration 45/1000 | Loss: 0.00048909
Iteration 46/1000 | Loss: 0.00019943
Iteration 47/1000 | Loss: 0.00003819
Iteration 48/1000 | Loss: 0.00003098
Iteration 49/1000 | Loss: 0.00002842
Iteration 50/1000 | Loss: 0.00002615
Iteration 51/1000 | Loss: 0.00005050
Iteration 52/1000 | Loss: 0.00005118
Iteration 53/1000 | Loss: 0.00004435
Iteration 54/1000 | Loss: 0.00004422
Iteration 55/1000 | Loss: 0.00004362
Iteration 56/1000 | Loss: 0.00005231
Iteration 57/1000 | Loss: 0.00004713
Iteration 58/1000 | Loss: 0.00004216
Iteration 59/1000 | Loss: 0.00003297
Iteration 60/1000 | Loss: 0.00003795
Iteration 61/1000 | Loss: 0.00003382
Iteration 62/1000 | Loss: 0.00003126
Iteration 63/1000 | Loss: 0.00002804
Iteration 64/1000 | Loss: 0.00003213
Iteration 65/1000 | Loss: 0.00003381
Iteration 66/1000 | Loss: 0.00003122
Iteration 67/1000 | Loss: 0.00003386
Iteration 68/1000 | Loss: 0.00003013
Iteration 69/1000 | Loss: 0.00003873
Iteration 70/1000 | Loss: 0.00003441
Iteration 71/1000 | Loss: 0.00003334
Iteration 72/1000 | Loss: 0.00003212
Iteration 73/1000 | Loss: 0.00003376
Iteration 74/1000 | Loss: 0.00003885
Iteration 75/1000 | Loss: 0.00002960
Iteration 76/1000 | Loss: 0.00002560
Iteration 77/1000 | Loss: 0.00002614
Iteration 78/1000 | Loss: 0.00002474
Iteration 79/1000 | Loss: 0.00002952
Iteration 80/1000 | Loss: 0.00004043
Iteration 81/1000 | Loss: 0.00005416
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002142
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00004836
Iteration 87/1000 | Loss: 0.00004147
Iteration 88/1000 | Loss: 0.00004515
Iteration 89/1000 | Loss: 0.00003911
Iteration 90/1000 | Loss: 0.00002141
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002002
Iteration 93/1000 | Loss: 0.00001999
Iteration 94/1000 | Loss: 0.00001999
Iteration 95/1000 | Loss: 0.00001999
Iteration 96/1000 | Loss: 0.00001998
Iteration 97/1000 | Loss: 0.00001998
Iteration 98/1000 | Loss: 0.00001998
Iteration 99/1000 | Loss: 0.00001998
Iteration 100/1000 | Loss: 0.00001998
Iteration 101/1000 | Loss: 0.00001998
Iteration 102/1000 | Loss: 0.00001998
Iteration 103/1000 | Loss: 0.00001998
Iteration 104/1000 | Loss: 0.00001998
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001997
Iteration 108/1000 | Loss: 0.00001997
Iteration 109/1000 | Loss: 0.00001997
Iteration 110/1000 | Loss: 0.00001996
Iteration 111/1000 | Loss: 0.00001995
Iteration 112/1000 | Loss: 0.00004564
Iteration 113/1000 | Loss: 0.00004419
Iteration 114/1000 | Loss: 0.00004528
Iteration 115/1000 | Loss: 0.00002535
Iteration 116/1000 | Loss: 0.00003989
Iteration 117/1000 | Loss: 0.00004542
Iteration 118/1000 | Loss: 0.00003846
Iteration 119/1000 | Loss: 0.00004565
Iteration 120/1000 | Loss: 0.00003760
Iteration 121/1000 | Loss: 0.00002438
Iteration 122/1000 | Loss: 0.00003296
Iteration 123/1000 | Loss: 0.00003606
Iteration 124/1000 | Loss: 0.00004644
Iteration 125/1000 | Loss: 0.00003617
Iteration 126/1000 | Loss: 0.00004654
Iteration 127/1000 | Loss: 0.00003528
Iteration 128/1000 | Loss: 0.00003365
Iteration 129/1000 | Loss: 0.00003541
Iteration 130/1000 | Loss: 0.00003209
Iteration 131/1000 | Loss: 0.00002509
Iteration 132/1000 | Loss: 0.00003054
Iteration 133/1000 | Loss: 0.00002324
Iteration 134/1000 | Loss: 0.00002115
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00001976
Iteration 137/1000 | Loss: 0.00001943
Iteration 138/1000 | Loss: 0.00001926
Iteration 139/1000 | Loss: 0.00001906
Iteration 140/1000 | Loss: 0.00001901
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001887
Iteration 147/1000 | Loss: 0.00001886
Iteration 148/1000 | Loss: 0.00001886
Iteration 149/1000 | Loss: 0.00001883
Iteration 150/1000 | Loss: 0.00001883
Iteration 151/1000 | Loss: 0.00001883
Iteration 152/1000 | Loss: 0.00001883
Iteration 153/1000 | Loss: 0.00001883
Iteration 154/1000 | Loss: 0.00001882
Iteration 155/1000 | Loss: 0.00001882
Iteration 156/1000 | Loss: 0.00001882
Iteration 157/1000 | Loss: 0.00001881
Iteration 158/1000 | Loss: 0.00001881
Iteration 159/1000 | Loss: 0.00001880
Iteration 160/1000 | Loss: 0.00001880
Iteration 161/1000 | Loss: 0.00001880
Iteration 162/1000 | Loss: 0.00001879
Iteration 163/1000 | Loss: 0.00001879
Iteration 164/1000 | Loss: 0.00001879
Iteration 165/1000 | Loss: 0.00001879
Iteration 166/1000 | Loss: 0.00001878
Iteration 167/1000 | Loss: 0.00001877
Iteration 168/1000 | Loss: 0.00001877
Iteration 169/1000 | Loss: 0.00001877
Iteration 170/1000 | Loss: 0.00001877
Iteration 171/1000 | Loss: 0.00001877
Iteration 172/1000 | Loss: 0.00001877
Iteration 173/1000 | Loss: 0.00001877
Iteration 174/1000 | Loss: 0.00001876
Iteration 175/1000 | Loss: 0.00001876
Iteration 176/1000 | Loss: 0.00001876
Iteration 177/1000 | Loss: 0.00001876
Iteration 178/1000 | Loss: 0.00001876
Iteration 179/1000 | Loss: 0.00001876
Iteration 180/1000 | Loss: 0.00001876
Iteration 181/1000 | Loss: 0.00001875
Iteration 182/1000 | Loss: 0.00001875
Iteration 183/1000 | Loss: 0.00001875
Iteration 184/1000 | Loss: 0.00001875
Iteration 185/1000 | Loss: 0.00001875
Iteration 186/1000 | Loss: 0.00001875
Iteration 187/1000 | Loss: 0.00001874
Iteration 188/1000 | Loss: 0.00001874
Iteration 189/1000 | Loss: 0.00001874
Iteration 190/1000 | Loss: 0.00001874
Iteration 191/1000 | Loss: 0.00001874
Iteration 192/1000 | Loss: 0.00001873
Iteration 193/1000 | Loss: 0.00001873
Iteration 194/1000 | Loss: 0.00001873
Iteration 195/1000 | Loss: 0.00001873
Iteration 196/1000 | Loss: 0.00001873
Iteration 197/1000 | Loss: 0.00001873
Iteration 198/1000 | Loss: 0.00001872
Iteration 199/1000 | Loss: 0.00001872
Iteration 200/1000 | Loss: 0.00001872
Iteration 201/1000 | Loss: 0.00001871
Iteration 202/1000 | Loss: 0.00001871
Iteration 203/1000 | Loss: 0.00001871
Iteration 204/1000 | Loss: 0.00001871
Iteration 205/1000 | Loss: 0.00001871
Iteration 206/1000 | Loss: 0.00001871
Iteration 207/1000 | Loss: 0.00001871
Iteration 208/1000 | Loss: 0.00001871
Iteration 209/1000 | Loss: 0.00001870
Iteration 210/1000 | Loss: 0.00001870
Iteration 211/1000 | Loss: 0.00001870
Iteration 212/1000 | Loss: 0.00001870
Iteration 213/1000 | Loss: 0.00001869
Iteration 214/1000 | Loss: 0.00001869
Iteration 215/1000 | Loss: 0.00001869
Iteration 216/1000 | Loss: 0.00001869
Iteration 217/1000 | Loss: 0.00001869
Iteration 218/1000 | Loss: 0.00001869
Iteration 219/1000 | Loss: 0.00001868
Iteration 220/1000 | Loss: 0.00001868
Iteration 221/1000 | Loss: 0.00001868
Iteration 222/1000 | Loss: 0.00001868
Iteration 223/1000 | Loss: 0.00001868
Iteration 224/1000 | Loss: 0.00001867
Iteration 225/1000 | Loss: 0.00001867
Iteration 226/1000 | Loss: 0.00001867
Iteration 227/1000 | Loss: 0.00001867
Iteration 228/1000 | Loss: 0.00001867
Iteration 229/1000 | Loss: 0.00001867
Iteration 230/1000 | Loss: 0.00001867
Iteration 231/1000 | Loss: 0.00001866
Iteration 232/1000 | Loss: 0.00001866
Iteration 233/1000 | Loss: 0.00001866
Iteration 234/1000 | Loss: 0.00001866
Iteration 235/1000 | Loss: 0.00001866
Iteration 236/1000 | Loss: 0.00001866
Iteration 237/1000 | Loss: 0.00001866
Iteration 238/1000 | Loss: 0.00001866
Iteration 239/1000 | Loss: 0.00001866
Iteration 240/1000 | Loss: 0.00001866
Iteration 241/1000 | Loss: 0.00001866
Iteration 242/1000 | Loss: 0.00001866
Iteration 243/1000 | Loss: 0.00001866
Iteration 244/1000 | Loss: 0.00001866
Iteration 245/1000 | Loss: 0.00001866
Iteration 246/1000 | Loss: 0.00001866
Iteration 247/1000 | Loss: 0.00001866
Iteration 248/1000 | Loss: 0.00001866
Iteration 249/1000 | Loss: 0.00001866
Iteration 250/1000 | Loss: 0.00001866
Iteration 251/1000 | Loss: 0.00001866
Iteration 252/1000 | Loss: 0.00001866
Iteration 253/1000 | Loss: 0.00001866
Iteration 254/1000 | Loss: 0.00001866
Iteration 255/1000 | Loss: 0.00001866
Iteration 256/1000 | Loss: 0.00001866
Iteration 257/1000 | Loss: 0.00001866
Iteration 258/1000 | Loss: 0.00001866
Iteration 259/1000 | Loss: 0.00001866
Iteration 260/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.865574267867487e-05, 1.865574267867487e-05, 1.865574267867487e-05, 1.865574267867487e-05, 1.865574267867487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.865574267867487e-05

Optimization complete. Final v2v error: 3.6901605129241943 mm

Highest mean error: 4.808419704437256 mm for frame 14

Lowest mean error: 3.072347640991211 mm for frame 169

Saving results

Total time: 227.01926398277283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389238
Iteration 2/25 | Loss: 0.00136045
Iteration 3/25 | Loss: 0.00125359
Iteration 4/25 | Loss: 0.00123405
Iteration 5/25 | Loss: 0.00122772
Iteration 6/25 | Loss: 0.00122651
Iteration 7/25 | Loss: 0.00122646
Iteration 8/25 | Loss: 0.00122646
Iteration 9/25 | Loss: 0.00122646
Iteration 10/25 | Loss: 0.00122646
Iteration 11/25 | Loss: 0.00122646
Iteration 12/25 | Loss: 0.00122646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012264635879546404, 0.0012264635879546404, 0.0012264635879546404, 0.0012264635879546404, 0.0012264635879546404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012264635879546404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40058517
Iteration 2/25 | Loss: 0.00068467
Iteration 3/25 | Loss: 0.00068467
Iteration 4/25 | Loss: 0.00068467
Iteration 5/25 | Loss: 0.00068466
Iteration 6/25 | Loss: 0.00068466
Iteration 7/25 | Loss: 0.00068466
Iteration 8/25 | Loss: 0.00068466
Iteration 9/25 | Loss: 0.00068466
Iteration 10/25 | Loss: 0.00068466
Iteration 11/25 | Loss: 0.00068466
Iteration 12/25 | Loss: 0.00068466
Iteration 13/25 | Loss: 0.00068466
Iteration 14/25 | Loss: 0.00068466
Iteration 15/25 | Loss: 0.00068466
Iteration 16/25 | Loss: 0.00068466
Iteration 17/25 | Loss: 0.00068466
Iteration 18/25 | Loss: 0.00068466
Iteration 19/25 | Loss: 0.00068466
Iteration 20/25 | Loss: 0.00068466
Iteration 21/25 | Loss: 0.00068466
Iteration 22/25 | Loss: 0.00068466
Iteration 23/25 | Loss: 0.00068466
Iteration 24/25 | Loss: 0.00068466
Iteration 25/25 | Loss: 0.00068466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068466
Iteration 2/1000 | Loss: 0.00004398
Iteration 3/1000 | Loss: 0.00003208
Iteration 4/1000 | Loss: 0.00002684
Iteration 5/1000 | Loss: 0.00002473
Iteration 6/1000 | Loss: 0.00002349
Iteration 7/1000 | Loss: 0.00002207
Iteration 8/1000 | Loss: 0.00002115
Iteration 9/1000 | Loss: 0.00002067
Iteration 10/1000 | Loss: 0.00002031
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00001978
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001925
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001921
Iteration 20/1000 | Loss: 0.00001916
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001902
Iteration 25/1000 | Loss: 0.00001902
Iteration 26/1000 | Loss: 0.00001901
Iteration 27/1000 | Loss: 0.00001901
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001895
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001894
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001893
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001892
Iteration 39/1000 | Loss: 0.00001891
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001890
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001889
Iteration 52/1000 | Loss: 0.00001889
Iteration 53/1000 | Loss: 0.00001889
Iteration 54/1000 | Loss: 0.00001889
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001888
Iteration 58/1000 | Loss: 0.00001888
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001886
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.886297650344204e-05, 1.886297650344204e-05, 1.886297650344204e-05, 1.886297650344204e-05, 1.886297650344204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.886297650344204e-05

Optimization complete. Final v2v error: 3.669570207595825 mm

Highest mean error: 4.334158897399902 mm for frame 69

Lowest mean error: 3.225780725479126 mm for frame 83

Saving results

Total time: 36.4723424911499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777335
Iteration 2/25 | Loss: 0.00130248
Iteration 3/25 | Loss: 0.00122336
Iteration 4/25 | Loss: 0.00121580
Iteration 5/25 | Loss: 0.00121345
Iteration 6/25 | Loss: 0.00121345
Iteration 7/25 | Loss: 0.00121345
Iteration 8/25 | Loss: 0.00121345
Iteration 9/25 | Loss: 0.00121345
Iteration 10/25 | Loss: 0.00121345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001213453128002584, 0.001213453128002584, 0.001213453128002584, 0.001213453128002584, 0.001213453128002584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001213453128002584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43581808
Iteration 2/25 | Loss: 0.00073280
Iteration 3/25 | Loss: 0.00073280
Iteration 4/25 | Loss: 0.00073280
Iteration 5/25 | Loss: 0.00073280
Iteration 6/25 | Loss: 0.00073280
Iteration 7/25 | Loss: 0.00073280
Iteration 8/25 | Loss: 0.00073280
Iteration 9/25 | Loss: 0.00073280
Iteration 10/25 | Loss: 0.00073280
Iteration 11/25 | Loss: 0.00073280
Iteration 12/25 | Loss: 0.00073280
Iteration 13/25 | Loss: 0.00073280
Iteration 14/25 | Loss: 0.00073280
Iteration 15/25 | Loss: 0.00073280
Iteration 16/25 | Loss: 0.00073280
Iteration 17/25 | Loss: 0.00073280
Iteration 18/25 | Loss: 0.00073280
Iteration 19/25 | Loss: 0.00073280
Iteration 20/25 | Loss: 0.00073280
Iteration 21/25 | Loss: 0.00073280
Iteration 22/25 | Loss: 0.00073280
Iteration 23/25 | Loss: 0.00073280
Iteration 24/25 | Loss: 0.00073280
Iteration 25/25 | Loss: 0.00073280

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073280
Iteration 2/1000 | Loss: 0.00002678
Iteration 3/1000 | Loss: 0.00001735
Iteration 4/1000 | Loss: 0.00001523
Iteration 5/1000 | Loss: 0.00001420
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001182
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001182
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001180
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001179
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001166
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001161
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001157
Iteration 107/1000 | Loss: 0.00001157
Iteration 108/1000 | Loss: 0.00001157
Iteration 109/1000 | Loss: 0.00001157
Iteration 110/1000 | Loss: 0.00001156
Iteration 111/1000 | Loss: 0.00001156
Iteration 112/1000 | Loss: 0.00001156
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001155
Iteration 116/1000 | Loss: 0.00001155
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001149
Iteration 140/1000 | Loss: 0.00001149
Iteration 141/1000 | Loss: 0.00001149
Iteration 142/1000 | Loss: 0.00001148
Iteration 143/1000 | Loss: 0.00001148
Iteration 144/1000 | Loss: 0.00001148
Iteration 145/1000 | Loss: 0.00001148
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001147
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001147
Iteration 150/1000 | Loss: 0.00001147
Iteration 151/1000 | Loss: 0.00001147
Iteration 152/1000 | Loss: 0.00001147
Iteration 153/1000 | Loss: 0.00001147
Iteration 154/1000 | Loss: 0.00001147
Iteration 155/1000 | Loss: 0.00001147
Iteration 156/1000 | Loss: 0.00001147
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Iteration 159/1000 | Loss: 0.00001146
Iteration 160/1000 | Loss: 0.00001146
Iteration 161/1000 | Loss: 0.00001145
Iteration 162/1000 | Loss: 0.00001145
Iteration 163/1000 | Loss: 0.00001145
Iteration 164/1000 | Loss: 0.00001145
Iteration 165/1000 | Loss: 0.00001145
Iteration 166/1000 | Loss: 0.00001145
Iteration 167/1000 | Loss: 0.00001145
Iteration 168/1000 | Loss: 0.00001145
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001144
Iteration 172/1000 | Loss: 0.00001144
Iteration 173/1000 | Loss: 0.00001144
Iteration 174/1000 | Loss: 0.00001144
Iteration 175/1000 | Loss: 0.00001144
Iteration 176/1000 | Loss: 0.00001144
Iteration 177/1000 | Loss: 0.00001143
Iteration 178/1000 | Loss: 0.00001143
Iteration 179/1000 | Loss: 0.00001143
Iteration 180/1000 | Loss: 0.00001143
Iteration 181/1000 | Loss: 0.00001143
Iteration 182/1000 | Loss: 0.00001143
Iteration 183/1000 | Loss: 0.00001143
Iteration 184/1000 | Loss: 0.00001143
Iteration 185/1000 | Loss: 0.00001143
Iteration 186/1000 | Loss: 0.00001143
Iteration 187/1000 | Loss: 0.00001142
Iteration 188/1000 | Loss: 0.00001142
Iteration 189/1000 | Loss: 0.00001142
Iteration 190/1000 | Loss: 0.00001142
Iteration 191/1000 | Loss: 0.00001142
Iteration 192/1000 | Loss: 0.00001142
Iteration 193/1000 | Loss: 0.00001142
Iteration 194/1000 | Loss: 0.00001142
Iteration 195/1000 | Loss: 0.00001142
Iteration 196/1000 | Loss: 0.00001142
Iteration 197/1000 | Loss: 0.00001142
Iteration 198/1000 | Loss: 0.00001142
Iteration 199/1000 | Loss: 0.00001142
Iteration 200/1000 | Loss: 0.00001142
Iteration 201/1000 | Loss: 0.00001142
Iteration 202/1000 | Loss: 0.00001142
Iteration 203/1000 | Loss: 0.00001142
Iteration 204/1000 | Loss: 0.00001142
Iteration 205/1000 | Loss: 0.00001142
Iteration 206/1000 | Loss: 0.00001142
Iteration 207/1000 | Loss: 0.00001142
Iteration 208/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.141717530117603e-05, 1.141717530117603e-05, 1.141717530117603e-05, 1.141717530117603e-05, 1.141717530117603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.141717530117603e-05

Optimization complete. Final v2v error: 2.8870742321014404 mm

Highest mean error: 3.0645270347595215 mm for frame 79

Lowest mean error: 2.7526392936706543 mm for frame 6

Saving results

Total time: 39.254042863845825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999899
Iteration 2/25 | Loss: 0.00999899
Iteration 3/25 | Loss: 0.00999899
Iteration 4/25 | Loss: 0.00999899
Iteration 5/25 | Loss: 0.00999899
Iteration 6/25 | Loss: 0.00999899
Iteration 7/25 | Loss: 0.00999899
Iteration 8/25 | Loss: 0.00999898
Iteration 9/25 | Loss: 0.00999898
Iteration 10/25 | Loss: 0.00999898
Iteration 11/25 | Loss: 0.00999898
Iteration 12/25 | Loss: 0.00999898
Iteration 13/25 | Loss: 0.00999898
Iteration 14/25 | Loss: 0.00999898
Iteration 15/25 | Loss: 0.00999898
Iteration 16/25 | Loss: 0.00999898
Iteration 17/25 | Loss: 0.00999898
Iteration 18/25 | Loss: 0.00999897
Iteration 19/25 | Loss: 0.00999897
Iteration 20/25 | Loss: 0.00999897
Iteration 21/25 | Loss: 0.00999897
Iteration 22/25 | Loss: 0.00999897
Iteration 23/25 | Loss: 0.00999897
Iteration 24/25 | Loss: 0.00999897
Iteration 25/25 | Loss: 0.00999897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67695296
Iteration 2/25 | Loss: 0.11804292
Iteration 3/25 | Loss: 0.11783981
Iteration 4/25 | Loss: 0.11783980
Iteration 5/25 | Loss: 0.11783978
Iteration 6/25 | Loss: 0.11783978
Iteration 7/25 | Loss: 0.11783978
Iteration 8/25 | Loss: 0.11783978
Iteration 9/25 | Loss: 0.11783978
Iteration 10/25 | Loss: 0.11783978
Iteration 11/25 | Loss: 0.11783978
Iteration 12/25 | Loss: 0.11783978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.11783977597951889, 0.11783977597951889, 0.11783977597951889, 0.11783977597951889, 0.11783977597951889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11783977597951889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11783978
Iteration 2/1000 | Loss: 0.00348850
Iteration 3/1000 | Loss: 0.00472987
Iteration 4/1000 | Loss: 0.00235751
Iteration 5/1000 | Loss: 0.00054300
Iteration 6/1000 | Loss: 0.00026717
Iteration 7/1000 | Loss: 0.00010532
Iteration 8/1000 | Loss: 0.00007010
Iteration 9/1000 | Loss: 0.00005590
Iteration 10/1000 | Loss: 0.00037379
Iteration 11/1000 | Loss: 0.00006302
Iteration 12/1000 | Loss: 0.00006874
Iteration 13/1000 | Loss: 0.00046837
Iteration 14/1000 | Loss: 0.00004268
Iteration 15/1000 | Loss: 0.00004694
Iteration 16/1000 | Loss: 0.00002897
Iteration 17/1000 | Loss: 0.00004113
Iteration 18/1000 | Loss: 0.00004334
Iteration 19/1000 | Loss: 0.00013491
Iteration 20/1000 | Loss: 0.00152736
Iteration 21/1000 | Loss: 0.00009916
Iteration 22/1000 | Loss: 0.00071342
Iteration 23/1000 | Loss: 0.00022338
Iteration 24/1000 | Loss: 0.00006043
Iteration 25/1000 | Loss: 0.00005450
Iteration 26/1000 | Loss: 0.00005604
Iteration 27/1000 | Loss: 0.00002119
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00005490
Iteration 30/1000 | Loss: 0.00079851
Iteration 31/1000 | Loss: 0.00002895
Iteration 32/1000 | Loss: 0.00007857
Iteration 33/1000 | Loss: 0.00004314
Iteration 34/1000 | Loss: 0.00003893
Iteration 35/1000 | Loss: 0.00004960
Iteration 36/1000 | Loss: 0.00004078
Iteration 37/1000 | Loss: 0.00003390
Iteration 38/1000 | Loss: 0.00002103
Iteration 39/1000 | Loss: 0.00005824
Iteration 40/1000 | Loss: 0.00005556
Iteration 41/1000 | Loss: 0.00017084
Iteration 42/1000 | Loss: 0.00005580
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00004803
Iteration 45/1000 | Loss: 0.00003497
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00003661
Iteration 48/1000 | Loss: 0.00001914
Iteration 49/1000 | Loss: 0.00001894
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00006113
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001749
Iteration 58/1000 | Loss: 0.00002947
Iteration 59/1000 | Loss: 0.00002205
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001749
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001748
Iteration 67/1000 | Loss: 0.00001748
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001748
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002159
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00002230
Iteration 74/1000 | Loss: 0.00001785
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001780
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00002312
Iteration 81/1000 | Loss: 0.00002011
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00003783
Iteration 90/1000 | Loss: 0.00015548
Iteration 91/1000 | Loss: 0.00003295
Iteration 92/1000 | Loss: 0.00006296
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001857
Iteration 95/1000 | Loss: 0.00002126
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001740
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001740
Iteration 117/1000 | Loss: 0.00001740
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.7395719623891637e-05, 1.7395719623891637e-05, 1.7395719623891637e-05, 1.7395719623891637e-05, 1.7395719623891637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7395719623891637e-05

Optimization complete. Final v2v error: 3.529411792755127 mm

Highest mean error: 3.850898027420044 mm for frame 231

Lowest mean error: 3.444422960281372 mm for frame 234

Saving results

Total time: 120.01913952827454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980642
Iteration 2/25 | Loss: 0.00246521
Iteration 3/25 | Loss: 0.00204472
Iteration 4/25 | Loss: 0.00195023
Iteration 5/25 | Loss: 0.00173575
Iteration 6/25 | Loss: 0.00157649
Iteration 7/25 | Loss: 0.00152060
Iteration 8/25 | Loss: 0.00147600
Iteration 9/25 | Loss: 0.00144091
Iteration 10/25 | Loss: 0.00141882
Iteration 11/25 | Loss: 0.00140147
Iteration 12/25 | Loss: 0.00139031
Iteration 13/25 | Loss: 0.00137563
Iteration 14/25 | Loss: 0.00137080
Iteration 15/25 | Loss: 0.00136796
Iteration 16/25 | Loss: 0.00136485
Iteration 17/25 | Loss: 0.00136900
Iteration 18/25 | Loss: 0.00136516
Iteration 19/25 | Loss: 0.00136392
Iteration 20/25 | Loss: 0.00136296
Iteration 21/25 | Loss: 0.00136320
Iteration 22/25 | Loss: 0.00136176
Iteration 23/25 | Loss: 0.00136033
Iteration 24/25 | Loss: 0.00136009
Iteration 25/25 | Loss: 0.00136001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42197227
Iteration 2/25 | Loss: 0.00082702
Iteration 3/25 | Loss: 0.00082702
Iteration 4/25 | Loss: 0.00082702
Iteration 5/25 | Loss: 0.00082702
Iteration 6/25 | Loss: 0.00082702
Iteration 7/25 | Loss: 0.00082702
Iteration 8/25 | Loss: 0.00082702
Iteration 9/25 | Loss: 0.00082702
Iteration 10/25 | Loss: 0.00082702
Iteration 11/25 | Loss: 0.00082702
Iteration 12/25 | Loss: 0.00082702
Iteration 13/25 | Loss: 0.00082702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008270190446637571, 0.0008270190446637571, 0.0008270190446637571, 0.0008270190446637571, 0.0008270190446637571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008270190446637571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082702
Iteration 2/1000 | Loss: 0.00025798
Iteration 3/1000 | Loss: 0.00005137
Iteration 4/1000 | Loss: 0.00004252
Iteration 5/1000 | Loss: 0.00003682
Iteration 6/1000 | Loss: 0.00003480
Iteration 7/1000 | Loss: 0.00003308
Iteration 8/1000 | Loss: 0.00003188
Iteration 9/1000 | Loss: 0.00003094
Iteration 10/1000 | Loss: 0.00003006
Iteration 11/1000 | Loss: 0.00002948
Iteration 12/1000 | Loss: 0.00002900
Iteration 13/1000 | Loss: 0.00002855
Iteration 14/1000 | Loss: 0.00002808
Iteration 15/1000 | Loss: 0.00043257
Iteration 16/1000 | Loss: 0.00003541
Iteration 17/1000 | Loss: 0.00003129
Iteration 18/1000 | Loss: 0.00002759
Iteration 19/1000 | Loss: 0.00002469
Iteration 20/1000 | Loss: 0.00002237
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002131
Iteration 23/1000 | Loss: 0.00002098
Iteration 24/1000 | Loss: 0.00002076
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002048
Iteration 27/1000 | Loss: 0.00002046
Iteration 28/1000 | Loss: 0.00002046
Iteration 29/1000 | Loss: 0.00002045
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002044
Iteration 32/1000 | Loss: 0.00002044
Iteration 33/1000 | Loss: 0.00002044
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002044
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002043
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002042
Iteration 43/1000 | Loss: 0.00002042
Iteration 44/1000 | Loss: 0.00002042
Iteration 45/1000 | Loss: 0.00002041
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002040
Iteration 50/1000 | Loss: 0.00002040
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002038
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002036
Iteration 58/1000 | Loss: 0.00002036
Iteration 59/1000 | Loss: 0.00002036
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002035
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002033
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002033
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002033
Iteration 77/1000 | Loss: 0.00002033
Iteration 78/1000 | Loss: 0.00002033
Iteration 79/1000 | Loss: 0.00002033
Iteration 80/1000 | Loss: 0.00002033
Iteration 81/1000 | Loss: 0.00002033
Iteration 82/1000 | Loss: 0.00002032
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002031
Iteration 89/1000 | Loss: 0.00002031
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002030
Iteration 95/1000 | Loss: 0.00002030
Iteration 96/1000 | Loss: 0.00002030
Iteration 97/1000 | Loss: 0.00002030
Iteration 98/1000 | Loss: 0.00002030
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00002029
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002028
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002027
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002026
Iteration 121/1000 | Loss: 0.00002026
Iteration 122/1000 | Loss: 0.00002026
Iteration 123/1000 | Loss: 0.00002026
Iteration 124/1000 | Loss: 0.00002026
Iteration 125/1000 | Loss: 0.00002026
Iteration 126/1000 | Loss: 0.00002026
Iteration 127/1000 | Loss: 0.00002026
Iteration 128/1000 | Loss: 0.00002026
Iteration 129/1000 | Loss: 0.00002026
Iteration 130/1000 | Loss: 0.00002026
Iteration 131/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.0260957171558402e-05, 2.0260957171558402e-05, 2.0260957171558402e-05, 2.0260957171558402e-05, 2.0260957171558402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0260957171558402e-05

Optimization complete. Final v2v error: 3.7616448402404785 mm

Highest mean error: 4.021023273468018 mm for frame 15

Lowest mean error: 3.562689781188965 mm for frame 24

Saving results

Total time: 93.75434446334839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842381
Iteration 2/25 | Loss: 0.00131804
Iteration 3/25 | Loss: 0.00123768
Iteration 4/25 | Loss: 0.00122344
Iteration 5/25 | Loss: 0.00121815
Iteration 6/25 | Loss: 0.00121707
Iteration 7/25 | Loss: 0.00121702
Iteration 8/25 | Loss: 0.00121702
Iteration 9/25 | Loss: 0.00121702
Iteration 10/25 | Loss: 0.00121702
Iteration 11/25 | Loss: 0.00121702
Iteration 12/25 | Loss: 0.00121702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012170170666649938, 0.0012170170666649938, 0.0012170170666649938, 0.0012170170666649938, 0.0012170170666649938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012170170666649938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05920577
Iteration 2/25 | Loss: 0.00079490
Iteration 3/25 | Loss: 0.00079490
Iteration 4/25 | Loss: 0.00079490
Iteration 5/25 | Loss: 0.00079490
Iteration 6/25 | Loss: 0.00079490
Iteration 7/25 | Loss: 0.00079490
Iteration 8/25 | Loss: 0.00079490
Iteration 9/25 | Loss: 0.00079490
Iteration 10/25 | Loss: 0.00079489
Iteration 11/25 | Loss: 0.00079489
Iteration 12/25 | Loss: 0.00079489
Iteration 13/25 | Loss: 0.00079489
Iteration 14/25 | Loss: 0.00079489
Iteration 15/25 | Loss: 0.00079489
Iteration 16/25 | Loss: 0.00079489
Iteration 17/25 | Loss: 0.00079489
Iteration 18/25 | Loss: 0.00079489
Iteration 19/25 | Loss: 0.00079489
Iteration 20/25 | Loss: 0.00079489
Iteration 21/25 | Loss: 0.00079489
Iteration 22/25 | Loss: 0.00079489
Iteration 23/25 | Loss: 0.00079489
Iteration 24/25 | Loss: 0.00079489
Iteration 25/25 | Loss: 0.00079489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079489
Iteration 2/1000 | Loss: 0.00002845
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001661
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001491
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001412
Iteration 11/1000 | Loss: 0.00001408
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001384
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001351
Iteration 20/1000 | Loss: 0.00001350
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001345
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001341
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001329
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001321
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001320
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001312
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001306
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001305
Iteration 102/1000 | Loss: 0.00001305
Iteration 103/1000 | Loss: 0.00001305
Iteration 104/1000 | Loss: 0.00001305
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001299
Iteration 134/1000 | Loss: 0.00001299
Iteration 135/1000 | Loss: 0.00001299
Iteration 136/1000 | Loss: 0.00001299
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001298
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001298
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001298
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001297
Iteration 155/1000 | Loss: 0.00001297
Iteration 156/1000 | Loss: 0.00001297
Iteration 157/1000 | Loss: 0.00001297
Iteration 158/1000 | Loss: 0.00001297
Iteration 159/1000 | Loss: 0.00001297
Iteration 160/1000 | Loss: 0.00001297
Iteration 161/1000 | Loss: 0.00001297
Iteration 162/1000 | Loss: 0.00001297
Iteration 163/1000 | Loss: 0.00001297
Iteration 164/1000 | Loss: 0.00001297
Iteration 165/1000 | Loss: 0.00001297
Iteration 166/1000 | Loss: 0.00001297
Iteration 167/1000 | Loss: 0.00001297
Iteration 168/1000 | Loss: 0.00001297
Iteration 169/1000 | Loss: 0.00001297
Iteration 170/1000 | Loss: 0.00001297
Iteration 171/1000 | Loss: 0.00001297
Iteration 172/1000 | Loss: 0.00001297
Iteration 173/1000 | Loss: 0.00001297
Iteration 174/1000 | Loss: 0.00001297
Iteration 175/1000 | Loss: 0.00001297
Iteration 176/1000 | Loss: 0.00001297
Iteration 177/1000 | Loss: 0.00001297
Iteration 178/1000 | Loss: 0.00001297
Iteration 179/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.2970634998055175e-05, 1.2970634998055175e-05, 1.2970634998055175e-05, 1.2970634998055175e-05, 1.2970634998055175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2970634998055175e-05

Optimization complete. Final v2v error: 3.041516065597534 mm

Highest mean error: 3.551409959793091 mm for frame 59

Lowest mean error: 2.815478563308716 mm for frame 98

Saving results

Total time: 40.66645169258118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016025
Iteration 2/25 | Loss: 0.00277076
Iteration 3/25 | Loss: 0.00175723
Iteration 4/25 | Loss: 0.00166362
Iteration 5/25 | Loss: 0.00211595
Iteration 6/25 | Loss: 0.00219250
Iteration 7/25 | Loss: 0.00192694
Iteration 8/25 | Loss: 0.00217200
Iteration 9/25 | Loss: 0.00215027
Iteration 10/25 | Loss: 0.00189005
Iteration 11/25 | Loss: 0.00154234
Iteration 12/25 | Loss: 0.00140785
Iteration 13/25 | Loss: 0.00137798
Iteration 14/25 | Loss: 0.00135719
Iteration 15/25 | Loss: 0.00134273
Iteration 16/25 | Loss: 0.00133545
Iteration 17/25 | Loss: 0.00132467
Iteration 18/25 | Loss: 0.00132006
Iteration 19/25 | Loss: 0.00131525
Iteration 20/25 | Loss: 0.00130927
Iteration 21/25 | Loss: 0.00128946
Iteration 22/25 | Loss: 0.00129347
Iteration 23/25 | Loss: 0.00128316
Iteration 24/25 | Loss: 0.00129289
Iteration 25/25 | Loss: 0.00128540

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40022612
Iteration 2/25 | Loss: 0.00135770
Iteration 3/25 | Loss: 0.00128913
Iteration 4/25 | Loss: 0.00128913
Iteration 5/25 | Loss: 0.00128913
Iteration 6/25 | Loss: 0.00128913
Iteration 7/25 | Loss: 0.00128913
Iteration 8/25 | Loss: 0.00128913
Iteration 9/25 | Loss: 0.00128913
Iteration 10/25 | Loss: 0.00128913
Iteration 11/25 | Loss: 0.00128913
Iteration 12/25 | Loss: 0.00128913
Iteration 13/25 | Loss: 0.00128913
Iteration 14/25 | Loss: 0.00128913
Iteration 15/25 | Loss: 0.00128913
Iteration 16/25 | Loss: 0.00128913
Iteration 17/25 | Loss: 0.00128913
Iteration 18/25 | Loss: 0.00128913
Iteration 19/25 | Loss: 0.00128913
Iteration 20/25 | Loss: 0.00128913
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012891261139884591, 0.0012891261139884591, 0.0012891261139884591, 0.0012891261139884591, 0.0012891261139884591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012891261139884591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128913
Iteration 2/1000 | Loss: 0.00039258
Iteration 3/1000 | Loss: 0.00021015
Iteration 4/1000 | Loss: 0.00021735
Iteration 5/1000 | Loss: 0.00018856
Iteration 6/1000 | Loss: 0.00016840
Iteration 7/1000 | Loss: 0.00030712
Iteration 8/1000 | Loss: 0.00025507
Iteration 9/1000 | Loss: 0.00017915
Iteration 10/1000 | Loss: 0.00040100
Iteration 11/1000 | Loss: 0.00017531
Iteration 12/1000 | Loss: 0.00018438
Iteration 13/1000 | Loss: 0.00034203
Iteration 14/1000 | Loss: 0.00037551
Iteration 15/1000 | Loss: 0.00014061
Iteration 16/1000 | Loss: 0.00016859
Iteration 17/1000 | Loss: 0.00007583
Iteration 18/1000 | Loss: 0.00012793
Iteration 19/1000 | Loss: 0.00020135
Iteration 20/1000 | Loss: 0.00030162
Iteration 21/1000 | Loss: 0.00021710
Iteration 22/1000 | Loss: 0.00007292
Iteration 23/1000 | Loss: 0.00007945
Iteration 24/1000 | Loss: 0.00006360
Iteration 25/1000 | Loss: 0.00029990
Iteration 26/1000 | Loss: 0.00010230
Iteration 27/1000 | Loss: 0.00008756
Iteration 28/1000 | Loss: 0.00008789
Iteration 29/1000 | Loss: 0.00009140
Iteration 30/1000 | Loss: 0.00009439
Iteration 31/1000 | Loss: 0.00008319
Iteration 32/1000 | Loss: 0.00009616
Iteration 33/1000 | Loss: 0.00008994
Iteration 34/1000 | Loss: 0.00009524
Iteration 35/1000 | Loss: 0.00008708
Iteration 36/1000 | Loss: 0.00009031
Iteration 37/1000 | Loss: 0.00008874
Iteration 38/1000 | Loss: 0.00008981
Iteration 39/1000 | Loss: 0.00009079
Iteration 40/1000 | Loss: 0.00005548
Iteration 41/1000 | Loss: 0.00004921
Iteration 42/1000 | Loss: 0.00009567
Iteration 43/1000 | Loss: 0.00008172
Iteration 44/1000 | Loss: 0.00006042
Iteration 45/1000 | Loss: 0.00003451
Iteration 46/1000 | Loss: 0.00004708
Iteration 47/1000 | Loss: 0.00005307
Iteration 48/1000 | Loss: 0.00007772
Iteration 49/1000 | Loss: 0.00006858
Iteration 50/1000 | Loss: 0.00006866
Iteration 51/1000 | Loss: 0.00006993
Iteration 52/1000 | Loss: 0.00008396
Iteration 53/1000 | Loss: 0.00007886
Iteration 54/1000 | Loss: 0.00008020
Iteration 55/1000 | Loss: 0.00007657
Iteration 56/1000 | Loss: 0.00009208
Iteration 57/1000 | Loss: 0.00007595
Iteration 58/1000 | Loss: 0.00009004
Iteration 59/1000 | Loss: 0.00007707
Iteration 60/1000 | Loss: 0.00008319
Iteration 61/1000 | Loss: 0.00006443
Iteration 62/1000 | Loss: 0.00009097
Iteration 63/1000 | Loss: 0.00007786
Iteration 64/1000 | Loss: 0.00007106
Iteration 65/1000 | Loss: 0.00006204
Iteration 66/1000 | Loss: 0.00008158
Iteration 67/1000 | Loss: 0.00006545
Iteration 68/1000 | Loss: 0.00008426
Iteration 69/1000 | Loss: 0.00006861
Iteration 70/1000 | Loss: 0.00008365
Iteration 71/1000 | Loss: 0.00005219
Iteration 72/1000 | Loss: 0.00007748
Iteration 73/1000 | Loss: 0.00006793
Iteration 74/1000 | Loss: 0.00008455
Iteration 75/1000 | Loss: 0.00006543
Iteration 76/1000 | Loss: 0.00009390
Iteration 77/1000 | Loss: 0.00006863
Iteration 78/1000 | Loss: 0.00010338
Iteration 79/1000 | Loss: 0.00006901
Iteration 80/1000 | Loss: 0.00009608
Iteration 81/1000 | Loss: 0.00006717
Iteration 82/1000 | Loss: 0.00008511
Iteration 83/1000 | Loss: 0.00009370
Iteration 84/1000 | Loss: 0.00009111
Iteration 85/1000 | Loss: 0.00006966
Iteration 86/1000 | Loss: 0.00009458
Iteration 87/1000 | Loss: 0.00006118
Iteration 88/1000 | Loss: 0.00009477
Iteration 89/1000 | Loss: 0.00006955
Iteration 90/1000 | Loss: 0.00008488
Iteration 91/1000 | Loss: 0.00006997
Iteration 92/1000 | Loss: 0.00008732
Iteration 93/1000 | Loss: 0.00006506
Iteration 94/1000 | Loss: 0.00003390
Iteration 95/1000 | Loss: 0.00004370
Iteration 96/1000 | Loss: 0.00008347
Iteration 97/1000 | Loss: 0.00006925
Iteration 98/1000 | Loss: 0.00009741
Iteration 99/1000 | Loss: 0.00006889
Iteration 100/1000 | Loss: 0.00009307
Iteration 101/1000 | Loss: 0.00007028
Iteration 102/1000 | Loss: 0.00009020
Iteration 103/1000 | Loss: 0.00008295
Iteration 104/1000 | Loss: 0.00007843
Iteration 105/1000 | Loss: 0.00008100
Iteration 106/1000 | Loss: 0.00007790
Iteration 107/1000 | Loss: 0.00006668
Iteration 108/1000 | Loss: 0.00007647
Iteration 109/1000 | Loss: 0.00008257
Iteration 110/1000 | Loss: 0.00008163
Iteration 111/1000 | Loss: 0.00007685
Iteration 112/1000 | Loss: 0.00008099
Iteration 113/1000 | Loss: 0.00007429
Iteration 114/1000 | Loss: 0.00008252
Iteration 115/1000 | Loss: 0.00007139
Iteration 116/1000 | Loss: 0.00008266
Iteration 117/1000 | Loss: 0.00005053
Iteration 118/1000 | Loss: 0.00006349
Iteration 119/1000 | Loss: 0.00006938
Iteration 120/1000 | Loss: 0.00008996
Iteration 121/1000 | Loss: 0.00005740
Iteration 122/1000 | Loss: 0.00007575
Iteration 123/1000 | Loss: 0.00004437
Iteration 124/1000 | Loss: 0.00005456
Iteration 125/1000 | Loss: 0.00005829
Iteration 126/1000 | Loss: 0.00006774
Iteration 127/1000 | Loss: 0.00007071
Iteration 128/1000 | Loss: 0.00005579
Iteration 129/1000 | Loss: 0.00006674
Iteration 130/1000 | Loss: 0.00007502
Iteration 131/1000 | Loss: 0.00006242
Iteration 132/1000 | Loss: 0.00006731
Iteration 133/1000 | Loss: 0.00007950
Iteration 134/1000 | Loss: 0.00008639
Iteration 135/1000 | Loss: 0.00007834
Iteration 136/1000 | Loss: 0.00006392
Iteration 137/1000 | Loss: 0.00005950
Iteration 138/1000 | Loss: 0.00006252
Iteration 139/1000 | Loss: 0.00006169
Iteration 140/1000 | Loss: 0.00005661
Iteration 141/1000 | Loss: 0.00006050
Iteration 142/1000 | Loss: 0.00006001
Iteration 143/1000 | Loss: 0.00006005
Iteration 144/1000 | Loss: 0.00006203
Iteration 145/1000 | Loss: 0.00005967
Iteration 146/1000 | Loss: 0.00005876
Iteration 147/1000 | Loss: 0.00005865
Iteration 148/1000 | Loss: 0.00004429
Iteration 149/1000 | Loss: 0.00006068
Iteration 150/1000 | Loss: 0.00006188
Iteration 151/1000 | Loss: 0.00007078
Iteration 152/1000 | Loss: 0.00006536
Iteration 153/1000 | Loss: 0.00007044
Iteration 154/1000 | Loss: 0.00006983
Iteration 155/1000 | Loss: 0.00005163
Iteration 156/1000 | Loss: 0.00004963
Iteration 157/1000 | Loss: 0.00004003
Iteration 158/1000 | Loss: 0.00004705
Iteration 159/1000 | Loss: 0.00005450
Iteration 160/1000 | Loss: 0.00006189
Iteration 161/1000 | Loss: 0.00006325
Iteration 162/1000 | Loss: 0.00006120
Iteration 163/1000 | Loss: 0.00006513
Iteration 164/1000 | Loss: 0.00005207
Iteration 165/1000 | Loss: 0.00005705
Iteration 166/1000 | Loss: 0.00005893
Iteration 167/1000 | Loss: 0.00005885
Iteration 168/1000 | Loss: 0.00005439
Iteration 169/1000 | Loss: 0.00005723
Iteration 170/1000 | Loss: 0.00005793
Iteration 171/1000 | Loss: 0.00005914
Iteration 172/1000 | Loss: 0.00006039
Iteration 173/1000 | Loss: 0.00005887
Iteration 174/1000 | Loss: 0.00006108
Iteration 175/1000 | Loss: 0.00006123
Iteration 176/1000 | Loss: 0.00005835
Iteration 177/1000 | Loss: 0.00006057
Iteration 178/1000 | Loss: 0.00005852
Iteration 179/1000 | Loss: 0.00005614
Iteration 180/1000 | Loss: 0.00005441
Iteration 181/1000 | Loss: 0.00006280
Iteration 182/1000 | Loss: 0.00008977
Iteration 183/1000 | Loss: 0.00003292
Iteration 184/1000 | Loss: 0.00005951
Iteration 185/1000 | Loss: 0.00005243
Iteration 186/1000 | Loss: 0.00006241
Iteration 187/1000 | Loss: 0.00006016
Iteration 188/1000 | Loss: 0.00005969
Iteration 189/1000 | Loss: 0.00006150
Iteration 190/1000 | Loss: 0.00005937
Iteration 191/1000 | Loss: 0.00005793
Iteration 192/1000 | Loss: 0.00006070
Iteration 193/1000 | Loss: 0.00005601
Iteration 194/1000 | Loss: 0.00005662
Iteration 195/1000 | Loss: 0.00005638
Iteration 196/1000 | Loss: 0.00003676
Iteration 197/1000 | Loss: 0.00005643
Iteration 198/1000 | Loss: 0.00006151
Iteration 199/1000 | Loss: 0.00005286
Iteration 200/1000 | Loss: 0.00005731
Iteration 201/1000 | Loss: 0.00004798
Iteration 202/1000 | Loss: 0.00006318
Iteration 203/1000 | Loss: 0.00004391
Iteration 204/1000 | Loss: 0.00004511
Iteration 205/1000 | Loss: 0.00005467
Iteration 206/1000 | Loss: 0.00006521
Iteration 207/1000 | Loss: 0.00006281
Iteration 208/1000 | Loss: 0.00005944
Iteration 209/1000 | Loss: 0.00007056
Iteration 210/1000 | Loss: 0.00003135
Iteration 211/1000 | Loss: 0.00002542
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002023
Iteration 214/1000 | Loss: 0.00001918
Iteration 215/1000 | Loss: 0.00001846
Iteration 216/1000 | Loss: 0.00001792
Iteration 217/1000 | Loss: 0.00001734
Iteration 218/1000 | Loss: 0.00001697
Iteration 219/1000 | Loss: 0.00001670
Iteration 220/1000 | Loss: 0.00001659
Iteration 221/1000 | Loss: 0.00001632
Iteration 222/1000 | Loss: 0.00001608
Iteration 223/1000 | Loss: 0.00001597
Iteration 224/1000 | Loss: 0.00001596
Iteration 225/1000 | Loss: 0.00001596
Iteration 226/1000 | Loss: 0.00001595
Iteration 227/1000 | Loss: 0.00001593
Iteration 228/1000 | Loss: 0.00001593
Iteration 229/1000 | Loss: 0.00001593
Iteration 230/1000 | Loss: 0.00001592
Iteration 231/1000 | Loss: 0.00001592
Iteration 232/1000 | Loss: 0.00001592
Iteration 233/1000 | Loss: 0.00001591
Iteration 234/1000 | Loss: 0.00001591
Iteration 235/1000 | Loss: 0.00001590
Iteration 236/1000 | Loss: 0.00001590
Iteration 237/1000 | Loss: 0.00001590
Iteration 238/1000 | Loss: 0.00001588
Iteration 239/1000 | Loss: 0.00001588
Iteration 240/1000 | Loss: 0.00001588
Iteration 241/1000 | Loss: 0.00001588
Iteration 242/1000 | Loss: 0.00001588
Iteration 243/1000 | Loss: 0.00001587
Iteration 244/1000 | Loss: 0.00001587
Iteration 245/1000 | Loss: 0.00001586
Iteration 246/1000 | Loss: 0.00001586
Iteration 247/1000 | Loss: 0.00001586
Iteration 248/1000 | Loss: 0.00001585
Iteration 249/1000 | Loss: 0.00001584
Iteration 250/1000 | Loss: 0.00001584
Iteration 251/1000 | Loss: 0.00001584
Iteration 252/1000 | Loss: 0.00001584
Iteration 253/1000 | Loss: 0.00001584
Iteration 254/1000 | Loss: 0.00001584
Iteration 255/1000 | Loss: 0.00001584
Iteration 256/1000 | Loss: 0.00001584
Iteration 257/1000 | Loss: 0.00001583
Iteration 258/1000 | Loss: 0.00001583
Iteration 259/1000 | Loss: 0.00001583
Iteration 260/1000 | Loss: 0.00001583
Iteration 261/1000 | Loss: 0.00001583
Iteration 262/1000 | Loss: 0.00001583
Iteration 263/1000 | Loss: 0.00001583
Iteration 264/1000 | Loss: 0.00001582
Iteration 265/1000 | Loss: 0.00001582
Iteration 266/1000 | Loss: 0.00001581
Iteration 267/1000 | Loss: 0.00001581
Iteration 268/1000 | Loss: 0.00001581
Iteration 269/1000 | Loss: 0.00001580
Iteration 270/1000 | Loss: 0.00001580
Iteration 271/1000 | Loss: 0.00001580
Iteration 272/1000 | Loss: 0.00001580
Iteration 273/1000 | Loss: 0.00001580
Iteration 274/1000 | Loss: 0.00001579
Iteration 275/1000 | Loss: 0.00001579
Iteration 276/1000 | Loss: 0.00001579
Iteration 277/1000 | Loss: 0.00001579
Iteration 278/1000 | Loss: 0.00001578
Iteration 279/1000 | Loss: 0.00001578
Iteration 280/1000 | Loss: 0.00001578
Iteration 281/1000 | Loss: 0.00001578
Iteration 282/1000 | Loss: 0.00001578
Iteration 283/1000 | Loss: 0.00001578
Iteration 284/1000 | Loss: 0.00001577
Iteration 285/1000 | Loss: 0.00001577
Iteration 286/1000 | Loss: 0.00001577
Iteration 287/1000 | Loss: 0.00001577
Iteration 288/1000 | Loss: 0.00001577
Iteration 289/1000 | Loss: 0.00001577
Iteration 290/1000 | Loss: 0.00001577
Iteration 291/1000 | Loss: 0.00001577
Iteration 292/1000 | Loss: 0.00001577
Iteration 293/1000 | Loss: 0.00001576
Iteration 294/1000 | Loss: 0.00001576
Iteration 295/1000 | Loss: 0.00001575
Iteration 296/1000 | Loss: 0.00001575
Iteration 297/1000 | Loss: 0.00001575
Iteration 298/1000 | Loss: 0.00001574
Iteration 299/1000 | Loss: 0.00001574
Iteration 300/1000 | Loss: 0.00001574
Iteration 301/1000 | Loss: 0.00001573
Iteration 302/1000 | Loss: 0.00001573
Iteration 303/1000 | Loss: 0.00001573
Iteration 304/1000 | Loss: 0.00001573
Iteration 305/1000 | Loss: 0.00001573
Iteration 306/1000 | Loss: 0.00001573
Iteration 307/1000 | Loss: 0.00001573
Iteration 308/1000 | Loss: 0.00001573
Iteration 309/1000 | Loss: 0.00001572
Iteration 310/1000 | Loss: 0.00001572
Iteration 311/1000 | Loss: 0.00001571
Iteration 312/1000 | Loss: 0.00001571
Iteration 313/1000 | Loss: 0.00001571
Iteration 314/1000 | Loss: 0.00001571
Iteration 315/1000 | Loss: 0.00001571
Iteration 316/1000 | Loss: 0.00001571
Iteration 317/1000 | Loss: 0.00001571
Iteration 318/1000 | Loss: 0.00001571
Iteration 319/1000 | Loss: 0.00001571
Iteration 320/1000 | Loss: 0.00001571
Iteration 321/1000 | Loss: 0.00001571
Iteration 322/1000 | Loss: 0.00001571
Iteration 323/1000 | Loss: 0.00001571
Iteration 324/1000 | Loss: 0.00001571
Iteration 325/1000 | Loss: 0.00001571
Iteration 326/1000 | Loss: 0.00001571
Iteration 327/1000 | Loss: 0.00001570
Iteration 328/1000 | Loss: 0.00001570
Iteration 329/1000 | Loss: 0.00001570
Iteration 330/1000 | Loss: 0.00001570
Iteration 331/1000 | Loss: 0.00001570
Iteration 332/1000 | Loss: 0.00001570
Iteration 333/1000 | Loss: 0.00001570
Iteration 334/1000 | Loss: 0.00001570
Iteration 335/1000 | Loss: 0.00001570
Iteration 336/1000 | Loss: 0.00001569
Iteration 337/1000 | Loss: 0.00001569
Iteration 338/1000 | Loss: 0.00001569
Iteration 339/1000 | Loss: 0.00001569
Iteration 340/1000 | Loss: 0.00001568
Iteration 341/1000 | Loss: 0.00001568
Iteration 342/1000 | Loss: 0.00001568
Iteration 343/1000 | Loss: 0.00001568
Iteration 344/1000 | Loss: 0.00001568
Iteration 345/1000 | Loss: 0.00001568
Iteration 346/1000 | Loss: 0.00001568
Iteration 347/1000 | Loss: 0.00001568
Iteration 348/1000 | Loss: 0.00001567
Iteration 349/1000 | Loss: 0.00001567
Iteration 350/1000 | Loss: 0.00001567
Iteration 351/1000 | Loss: 0.00001567
Iteration 352/1000 | Loss: 0.00001567
Iteration 353/1000 | Loss: 0.00001567
Iteration 354/1000 | Loss: 0.00001567
Iteration 355/1000 | Loss: 0.00001567
Iteration 356/1000 | Loss: 0.00001567
Iteration 357/1000 | Loss: 0.00001567
Iteration 358/1000 | Loss: 0.00001567
Iteration 359/1000 | Loss: 0.00001567
Iteration 360/1000 | Loss: 0.00001567
Iteration 361/1000 | Loss: 0.00001567
Iteration 362/1000 | Loss: 0.00001567
Iteration 363/1000 | Loss: 0.00001567
Iteration 364/1000 | Loss: 0.00001567
Iteration 365/1000 | Loss: 0.00001567
Iteration 366/1000 | Loss: 0.00001567
Iteration 367/1000 | Loss: 0.00001567
Iteration 368/1000 | Loss: 0.00001567
Iteration 369/1000 | Loss: 0.00001567
Iteration 370/1000 | Loss: 0.00001567
Iteration 371/1000 | Loss: 0.00001567
Iteration 372/1000 | Loss: 0.00001567
Iteration 373/1000 | Loss: 0.00001567
Iteration 374/1000 | Loss: 0.00001567
Iteration 375/1000 | Loss: 0.00001567
Iteration 376/1000 | Loss: 0.00001567
Iteration 377/1000 | Loss: 0.00001567
Iteration 378/1000 | Loss: 0.00001567
Iteration 379/1000 | Loss: 0.00001567
Iteration 380/1000 | Loss: 0.00001567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 380. Stopping optimization.
Last 5 losses: [1.5669851563870907e-05, 1.5669851563870907e-05, 1.5669851563870907e-05, 1.5669851563870907e-05, 1.5669851563870907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5669851563870907e-05

Optimization complete. Final v2v error: 3.2101962566375732 mm

Highest mean error: 5.754561901092529 mm for frame 37

Lowest mean error: 3.0501949787139893 mm for frame 133

Saving results

Total time: 390.39644265174866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793117
Iteration 2/25 | Loss: 0.00195942
Iteration 3/25 | Loss: 0.00144529
Iteration 4/25 | Loss: 0.00138013
Iteration 5/25 | Loss: 0.00137164
Iteration 6/25 | Loss: 0.00137021
Iteration 7/25 | Loss: 0.00136975
Iteration 8/25 | Loss: 0.00136975
Iteration 9/25 | Loss: 0.00136975
Iteration 10/25 | Loss: 0.00136975
Iteration 11/25 | Loss: 0.00136975
Iteration 12/25 | Loss: 0.00136975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00136974873021245, 0.00136974873021245, 0.00136974873021245, 0.00136974873021245, 0.00136974873021245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00136974873021245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29718375
Iteration 2/25 | Loss: 0.00066723
Iteration 3/25 | Loss: 0.00066720
Iteration 4/25 | Loss: 0.00066720
Iteration 5/25 | Loss: 0.00066720
Iteration 6/25 | Loss: 0.00066720
Iteration 7/25 | Loss: 0.00066720
Iteration 8/25 | Loss: 0.00066720
Iteration 9/25 | Loss: 0.00066719
Iteration 10/25 | Loss: 0.00066719
Iteration 11/25 | Loss: 0.00066719
Iteration 12/25 | Loss: 0.00066719
Iteration 13/25 | Loss: 0.00066719
Iteration 14/25 | Loss: 0.00066719
Iteration 15/25 | Loss: 0.00066719
Iteration 16/25 | Loss: 0.00066719
Iteration 17/25 | Loss: 0.00066719
Iteration 18/25 | Loss: 0.00066719
Iteration 19/25 | Loss: 0.00066719
Iteration 20/25 | Loss: 0.00066719
Iteration 21/25 | Loss: 0.00066719
Iteration 22/25 | Loss: 0.00066719
Iteration 23/25 | Loss: 0.00066719
Iteration 24/25 | Loss: 0.00066719
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006671947194263339, 0.0006671947194263339, 0.0006671947194263339, 0.0006671947194263339, 0.0006671947194263339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006671947194263339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066719
Iteration 2/1000 | Loss: 0.00004984
Iteration 3/1000 | Loss: 0.00003375
Iteration 4/1000 | Loss: 0.00002954
Iteration 5/1000 | Loss: 0.00002775
Iteration 6/1000 | Loss: 0.00002667
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002560
Iteration 9/1000 | Loss: 0.00002516
Iteration 10/1000 | Loss: 0.00002481
Iteration 11/1000 | Loss: 0.00002459
Iteration 12/1000 | Loss: 0.00002437
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002412
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002404
Iteration 17/1000 | Loss: 0.00002403
Iteration 18/1000 | Loss: 0.00002402
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002401
Iteration 21/1000 | Loss: 0.00002401
Iteration 22/1000 | Loss: 0.00002401
Iteration 23/1000 | Loss: 0.00002401
Iteration 24/1000 | Loss: 0.00002401
Iteration 25/1000 | Loss: 0.00002401
Iteration 26/1000 | Loss: 0.00002401
Iteration 27/1000 | Loss: 0.00002401
Iteration 28/1000 | Loss: 0.00002400
Iteration 29/1000 | Loss: 0.00002398
Iteration 30/1000 | Loss: 0.00002398
Iteration 31/1000 | Loss: 0.00002398
Iteration 32/1000 | Loss: 0.00002398
Iteration 33/1000 | Loss: 0.00002398
Iteration 34/1000 | Loss: 0.00002398
Iteration 35/1000 | Loss: 0.00002398
Iteration 36/1000 | Loss: 0.00002398
Iteration 37/1000 | Loss: 0.00002397
Iteration 38/1000 | Loss: 0.00002397
Iteration 39/1000 | Loss: 0.00002397
Iteration 40/1000 | Loss: 0.00002397
Iteration 41/1000 | Loss: 0.00002397
Iteration 42/1000 | Loss: 0.00002397
Iteration 43/1000 | Loss: 0.00002397
Iteration 44/1000 | Loss: 0.00002397
Iteration 45/1000 | Loss: 0.00002397
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002396
Iteration 49/1000 | Loss: 0.00002396
Iteration 50/1000 | Loss: 0.00002396
Iteration 51/1000 | Loss: 0.00002396
Iteration 52/1000 | Loss: 0.00002395
Iteration 53/1000 | Loss: 0.00002395
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002395
Iteration 56/1000 | Loss: 0.00002395
Iteration 57/1000 | Loss: 0.00002395
Iteration 58/1000 | Loss: 0.00002395
Iteration 59/1000 | Loss: 0.00002395
Iteration 60/1000 | Loss: 0.00002394
Iteration 61/1000 | Loss: 0.00002394
Iteration 62/1000 | Loss: 0.00002394
Iteration 63/1000 | Loss: 0.00002393
Iteration 64/1000 | Loss: 0.00002393
Iteration 65/1000 | Loss: 0.00002393
Iteration 66/1000 | Loss: 0.00002393
Iteration 67/1000 | Loss: 0.00002393
Iteration 68/1000 | Loss: 0.00002393
Iteration 69/1000 | Loss: 0.00002393
Iteration 70/1000 | Loss: 0.00002393
Iteration 71/1000 | Loss: 0.00002393
Iteration 72/1000 | Loss: 0.00002393
Iteration 73/1000 | Loss: 0.00002393
Iteration 74/1000 | Loss: 0.00002393
Iteration 75/1000 | Loss: 0.00002393
Iteration 76/1000 | Loss: 0.00002393
Iteration 77/1000 | Loss: 0.00002393
Iteration 78/1000 | Loss: 0.00002393
Iteration 79/1000 | Loss: 0.00002393
Iteration 80/1000 | Loss: 0.00002393
Iteration 81/1000 | Loss: 0.00002393
Iteration 82/1000 | Loss: 0.00002393
Iteration 83/1000 | Loss: 0.00002393
Iteration 84/1000 | Loss: 0.00002393
Iteration 85/1000 | Loss: 0.00002393
Iteration 86/1000 | Loss: 0.00002393
Iteration 87/1000 | Loss: 0.00002393
Iteration 88/1000 | Loss: 0.00002393
Iteration 89/1000 | Loss: 0.00002393
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002393
Iteration 92/1000 | Loss: 0.00002393
Iteration 93/1000 | Loss: 0.00002393
Iteration 94/1000 | Loss: 0.00002393
Iteration 95/1000 | Loss: 0.00002393
Iteration 96/1000 | Loss: 0.00002393
Iteration 97/1000 | Loss: 0.00002393
Iteration 98/1000 | Loss: 0.00002393
Iteration 99/1000 | Loss: 0.00002393
Iteration 100/1000 | Loss: 0.00002393
Iteration 101/1000 | Loss: 0.00002393
Iteration 102/1000 | Loss: 0.00002393
Iteration 103/1000 | Loss: 0.00002393
Iteration 104/1000 | Loss: 0.00002393
Iteration 105/1000 | Loss: 0.00002393
Iteration 106/1000 | Loss: 0.00002393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.3926739231683314e-05, 2.3926739231683314e-05, 2.3926739231683314e-05, 2.3926739231683314e-05, 2.3926739231683314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3926739231683314e-05

Optimization complete. Final v2v error: 4.076854228973389 mm

Highest mean error: 4.380671501159668 mm for frame 117

Lowest mean error: 3.816148281097412 mm for frame 45

Saving results

Total time: 35.80871343612671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771123
Iteration 2/25 | Loss: 0.00140298
Iteration 3/25 | Loss: 0.00125057
Iteration 4/25 | Loss: 0.00123793
Iteration 5/25 | Loss: 0.00123559
Iteration 6/25 | Loss: 0.00123532
Iteration 7/25 | Loss: 0.00123532
Iteration 8/25 | Loss: 0.00123532
Iteration 9/25 | Loss: 0.00123532
Iteration 10/25 | Loss: 0.00123532
Iteration 11/25 | Loss: 0.00123532
Iteration 12/25 | Loss: 0.00123532
Iteration 13/25 | Loss: 0.00123532
Iteration 14/25 | Loss: 0.00123532
Iteration 15/25 | Loss: 0.00123532
Iteration 16/25 | Loss: 0.00123532
Iteration 17/25 | Loss: 0.00123532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012353244237601757, 0.0012353244237601757, 0.0012353244237601757, 0.0012353244237601757, 0.0012353244237601757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012353244237601757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44052601
Iteration 2/25 | Loss: 0.00078923
Iteration 3/25 | Loss: 0.00078923
Iteration 4/25 | Loss: 0.00078923
Iteration 5/25 | Loss: 0.00078923
Iteration 6/25 | Loss: 0.00078923
Iteration 7/25 | Loss: 0.00078923
Iteration 8/25 | Loss: 0.00078923
Iteration 9/25 | Loss: 0.00078923
Iteration 10/25 | Loss: 0.00078923
Iteration 11/25 | Loss: 0.00078923
Iteration 12/25 | Loss: 0.00078923
Iteration 13/25 | Loss: 0.00078923
Iteration 14/25 | Loss: 0.00078923
Iteration 15/25 | Loss: 0.00078923
Iteration 16/25 | Loss: 0.00078923
Iteration 17/25 | Loss: 0.00078923
Iteration 18/25 | Loss: 0.00078923
Iteration 19/25 | Loss: 0.00078923
Iteration 20/25 | Loss: 0.00078923
Iteration 21/25 | Loss: 0.00078923
Iteration 22/25 | Loss: 0.00078923
Iteration 23/25 | Loss: 0.00078923
Iteration 24/25 | Loss: 0.00078923
Iteration 25/25 | Loss: 0.00078923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078923
Iteration 2/1000 | Loss: 0.00002668
Iteration 3/1000 | Loss: 0.00001744
Iteration 4/1000 | Loss: 0.00001568
Iteration 5/1000 | Loss: 0.00001449
Iteration 6/1000 | Loss: 0.00001374
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001291
Iteration 9/1000 | Loss: 0.00001281
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001211
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001204
Iteration 21/1000 | Loss: 0.00001204
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001188
Iteration 94/1000 | Loss: 0.00001188
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001183
Iteration 118/1000 | Loss: 0.00001183
Iteration 119/1000 | Loss: 0.00001183
Iteration 120/1000 | Loss: 0.00001182
Iteration 121/1000 | Loss: 0.00001182
Iteration 122/1000 | Loss: 0.00001182
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001182
Iteration 126/1000 | Loss: 0.00001181
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001181
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001180
Iteration 137/1000 | Loss: 0.00001180
Iteration 138/1000 | Loss: 0.00001180
Iteration 139/1000 | Loss: 0.00001180
Iteration 140/1000 | Loss: 0.00001180
Iteration 141/1000 | Loss: 0.00001179
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001179
Iteration 144/1000 | Loss: 0.00001179
Iteration 145/1000 | Loss: 0.00001179
Iteration 146/1000 | Loss: 0.00001178
Iteration 147/1000 | Loss: 0.00001178
Iteration 148/1000 | Loss: 0.00001178
Iteration 149/1000 | Loss: 0.00001178
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001178
Iteration 152/1000 | Loss: 0.00001177
Iteration 153/1000 | Loss: 0.00001177
Iteration 154/1000 | Loss: 0.00001177
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001175
Iteration 168/1000 | Loss: 0.00001175
Iteration 169/1000 | Loss: 0.00001175
Iteration 170/1000 | Loss: 0.00001175
Iteration 171/1000 | Loss: 0.00001175
Iteration 172/1000 | Loss: 0.00001175
Iteration 173/1000 | Loss: 0.00001175
Iteration 174/1000 | Loss: 0.00001175
Iteration 175/1000 | Loss: 0.00001175
Iteration 176/1000 | Loss: 0.00001175
Iteration 177/1000 | Loss: 0.00001175
Iteration 178/1000 | Loss: 0.00001175
Iteration 179/1000 | Loss: 0.00001175
Iteration 180/1000 | Loss: 0.00001175
Iteration 181/1000 | Loss: 0.00001175
Iteration 182/1000 | Loss: 0.00001175
Iteration 183/1000 | Loss: 0.00001175
Iteration 184/1000 | Loss: 0.00001174
Iteration 185/1000 | Loss: 0.00001174
Iteration 186/1000 | Loss: 0.00001174
Iteration 187/1000 | Loss: 0.00001174
Iteration 188/1000 | Loss: 0.00001174
Iteration 189/1000 | Loss: 0.00001174
Iteration 190/1000 | Loss: 0.00001174
Iteration 191/1000 | Loss: 0.00001174
Iteration 192/1000 | Loss: 0.00001174
Iteration 193/1000 | Loss: 0.00001174
Iteration 194/1000 | Loss: 0.00001174
Iteration 195/1000 | Loss: 0.00001174
Iteration 196/1000 | Loss: 0.00001174
Iteration 197/1000 | Loss: 0.00001174
Iteration 198/1000 | Loss: 0.00001174
Iteration 199/1000 | Loss: 0.00001174
Iteration 200/1000 | Loss: 0.00001174
Iteration 201/1000 | Loss: 0.00001174
Iteration 202/1000 | Loss: 0.00001173
Iteration 203/1000 | Loss: 0.00001173
Iteration 204/1000 | Loss: 0.00001173
Iteration 205/1000 | Loss: 0.00001173
Iteration 206/1000 | Loss: 0.00001173
Iteration 207/1000 | Loss: 0.00001173
Iteration 208/1000 | Loss: 0.00001172
Iteration 209/1000 | Loss: 0.00001172
Iteration 210/1000 | Loss: 0.00001172
Iteration 211/1000 | Loss: 0.00001172
Iteration 212/1000 | Loss: 0.00001172
Iteration 213/1000 | Loss: 0.00001172
Iteration 214/1000 | Loss: 0.00001172
Iteration 215/1000 | Loss: 0.00001172
Iteration 216/1000 | Loss: 0.00001172
Iteration 217/1000 | Loss: 0.00001172
Iteration 218/1000 | Loss: 0.00001172
Iteration 219/1000 | Loss: 0.00001171
Iteration 220/1000 | Loss: 0.00001171
Iteration 221/1000 | Loss: 0.00001171
Iteration 222/1000 | Loss: 0.00001171
Iteration 223/1000 | Loss: 0.00001171
Iteration 224/1000 | Loss: 0.00001171
Iteration 225/1000 | Loss: 0.00001171
Iteration 226/1000 | Loss: 0.00001171
Iteration 227/1000 | Loss: 0.00001171
Iteration 228/1000 | Loss: 0.00001171
Iteration 229/1000 | Loss: 0.00001171
Iteration 230/1000 | Loss: 0.00001171
Iteration 231/1000 | Loss: 0.00001171
Iteration 232/1000 | Loss: 0.00001171
Iteration 233/1000 | Loss: 0.00001171
Iteration 234/1000 | Loss: 0.00001171
Iteration 235/1000 | Loss: 0.00001171
Iteration 236/1000 | Loss: 0.00001171
Iteration 237/1000 | Loss: 0.00001171
Iteration 238/1000 | Loss: 0.00001171
Iteration 239/1000 | Loss: 0.00001171
Iteration 240/1000 | Loss: 0.00001171
Iteration 241/1000 | Loss: 0.00001171
Iteration 242/1000 | Loss: 0.00001171
Iteration 243/1000 | Loss: 0.00001171
Iteration 244/1000 | Loss: 0.00001171
Iteration 245/1000 | Loss: 0.00001171
Iteration 246/1000 | Loss: 0.00001171
Iteration 247/1000 | Loss: 0.00001171
Iteration 248/1000 | Loss: 0.00001171
Iteration 249/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.1706402801792137e-05, 1.1706402801792137e-05, 1.1706402801792137e-05, 1.1706402801792137e-05, 1.1706402801792137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1706402801792137e-05

Optimization complete. Final v2v error: 2.9304373264312744 mm

Highest mean error: 3.2784793376922607 mm for frame 84

Lowest mean error: 2.8356897830963135 mm for frame 174

Saving results

Total time: 41.50691843032837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371838
Iteration 2/25 | Loss: 0.00124710
Iteration 3/25 | Loss: 0.00119067
Iteration 4/25 | Loss: 0.00118202
Iteration 5/25 | Loss: 0.00117917
Iteration 6/25 | Loss: 0.00117860
Iteration 7/25 | Loss: 0.00117860
Iteration 8/25 | Loss: 0.00117860
Iteration 9/25 | Loss: 0.00117860
Iteration 10/25 | Loss: 0.00117860
Iteration 11/25 | Loss: 0.00117860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011786038521677256, 0.0011786038521677256, 0.0011786038521677256, 0.0011786038521677256, 0.0011786038521677256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011786038521677256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51406932
Iteration 2/25 | Loss: 0.00073259
Iteration 3/25 | Loss: 0.00073259
Iteration 4/25 | Loss: 0.00073259
Iteration 5/25 | Loss: 0.00073259
Iteration 6/25 | Loss: 0.00073258
Iteration 7/25 | Loss: 0.00073258
Iteration 8/25 | Loss: 0.00073258
Iteration 9/25 | Loss: 0.00073258
Iteration 10/25 | Loss: 0.00073258
Iteration 11/25 | Loss: 0.00073258
Iteration 12/25 | Loss: 0.00073258
Iteration 13/25 | Loss: 0.00073258
Iteration 14/25 | Loss: 0.00073258
Iteration 15/25 | Loss: 0.00073258
Iteration 16/25 | Loss: 0.00073258
Iteration 17/25 | Loss: 0.00073258
Iteration 18/25 | Loss: 0.00073258
Iteration 19/25 | Loss: 0.00073258
Iteration 20/25 | Loss: 0.00073258
Iteration 21/25 | Loss: 0.00073258
Iteration 22/25 | Loss: 0.00073258
Iteration 23/25 | Loss: 0.00073258
Iteration 24/25 | Loss: 0.00073258
Iteration 25/25 | Loss: 0.00073258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073258
Iteration 2/1000 | Loss: 0.00002530
Iteration 3/1000 | Loss: 0.00001644
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001177
Iteration 7/1000 | Loss: 0.00001131
Iteration 8/1000 | Loss: 0.00001112
Iteration 9/1000 | Loss: 0.00001110
Iteration 10/1000 | Loss: 0.00001110
Iteration 11/1000 | Loss: 0.00001110
Iteration 12/1000 | Loss: 0.00001110
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001097
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001077
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001067
Iteration 20/1000 | Loss: 0.00001067
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001052
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00001041
Iteration 37/1000 | Loss: 0.00001040
Iteration 38/1000 | Loss: 0.00001040
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001037
Iteration 41/1000 | Loss: 0.00001037
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001037
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001037
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001036
Iteration 51/1000 | Loss: 0.00001036
Iteration 52/1000 | Loss: 0.00001036
Iteration 53/1000 | Loss: 0.00001036
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001036
Iteration 58/1000 | Loss: 0.00001036
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001035
Iteration 62/1000 | Loss: 0.00001035
Iteration 63/1000 | Loss: 0.00001034
Iteration 64/1000 | Loss: 0.00001034
Iteration 65/1000 | Loss: 0.00001034
Iteration 66/1000 | Loss: 0.00001034
Iteration 67/1000 | Loss: 0.00001034
Iteration 68/1000 | Loss: 0.00001034
Iteration 69/1000 | Loss: 0.00001034
Iteration 70/1000 | Loss: 0.00001034
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00001034
Iteration 73/1000 | Loss: 0.00001033
Iteration 74/1000 | Loss: 0.00001033
Iteration 75/1000 | Loss: 0.00001033
Iteration 76/1000 | Loss: 0.00001033
Iteration 77/1000 | Loss: 0.00001033
Iteration 78/1000 | Loss: 0.00001033
Iteration 79/1000 | Loss: 0.00001033
Iteration 80/1000 | Loss: 0.00001033
Iteration 81/1000 | Loss: 0.00001032
Iteration 82/1000 | Loss: 0.00001032
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001031
Iteration 85/1000 | Loss: 0.00001031
Iteration 86/1000 | Loss: 0.00001030
Iteration 87/1000 | Loss: 0.00001030
Iteration 88/1000 | Loss: 0.00001030
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001028
Iteration 92/1000 | Loss: 0.00001028
Iteration 93/1000 | Loss: 0.00001028
Iteration 94/1000 | Loss: 0.00001028
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001028
Iteration 99/1000 | Loss: 0.00001028
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001026
Iteration 104/1000 | Loss: 0.00001026
Iteration 105/1000 | Loss: 0.00001026
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001025
Iteration 108/1000 | Loss: 0.00001025
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001023
Iteration 112/1000 | Loss: 0.00001023
Iteration 113/1000 | Loss: 0.00001023
Iteration 114/1000 | Loss: 0.00001023
Iteration 115/1000 | Loss: 0.00001022
Iteration 116/1000 | Loss: 0.00001022
Iteration 117/1000 | Loss: 0.00001022
Iteration 118/1000 | Loss: 0.00001021
Iteration 119/1000 | Loss: 0.00001021
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001019
Iteration 124/1000 | Loss: 0.00001019
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001019
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001019
Iteration 129/1000 | Loss: 0.00001019
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001018
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001017
Iteration 137/1000 | Loss: 0.00001017
Iteration 138/1000 | Loss: 0.00001017
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001017
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001015
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001014
Iteration 154/1000 | Loss: 0.00001014
Iteration 155/1000 | Loss: 0.00001014
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001012
Iteration 163/1000 | Loss: 0.00001011
Iteration 164/1000 | Loss: 0.00001011
Iteration 165/1000 | Loss: 0.00001011
Iteration 166/1000 | Loss: 0.00001011
Iteration 167/1000 | Loss: 0.00001011
Iteration 168/1000 | Loss: 0.00001011
Iteration 169/1000 | Loss: 0.00001011
Iteration 170/1000 | Loss: 0.00001011
Iteration 171/1000 | Loss: 0.00001011
Iteration 172/1000 | Loss: 0.00001011
Iteration 173/1000 | Loss: 0.00001011
Iteration 174/1000 | Loss: 0.00001011
Iteration 175/1000 | Loss: 0.00001011
Iteration 176/1000 | Loss: 0.00001011
Iteration 177/1000 | Loss: 0.00001011
Iteration 178/1000 | Loss: 0.00001011
Iteration 179/1000 | Loss: 0.00001010
Iteration 180/1000 | Loss: 0.00001010
Iteration 181/1000 | Loss: 0.00001010
Iteration 182/1000 | Loss: 0.00001010
Iteration 183/1000 | Loss: 0.00001010
Iteration 184/1000 | Loss: 0.00001010
Iteration 185/1000 | Loss: 0.00001010
Iteration 186/1000 | Loss: 0.00001010
Iteration 187/1000 | Loss: 0.00001009
Iteration 188/1000 | Loss: 0.00001009
Iteration 189/1000 | Loss: 0.00001009
Iteration 190/1000 | Loss: 0.00001009
Iteration 191/1000 | Loss: 0.00001009
Iteration 192/1000 | Loss: 0.00001009
Iteration 193/1000 | Loss: 0.00001009
Iteration 194/1000 | Loss: 0.00001009
Iteration 195/1000 | Loss: 0.00001009
Iteration 196/1000 | Loss: 0.00001009
Iteration 197/1000 | Loss: 0.00001009
Iteration 198/1000 | Loss: 0.00001009
Iteration 199/1000 | Loss: 0.00001009
Iteration 200/1000 | Loss: 0.00001009
Iteration 201/1000 | Loss: 0.00001009
Iteration 202/1000 | Loss: 0.00001009
Iteration 203/1000 | Loss: 0.00001009
Iteration 204/1000 | Loss: 0.00001009
Iteration 205/1000 | Loss: 0.00001009
Iteration 206/1000 | Loss: 0.00001009
Iteration 207/1000 | Loss: 0.00001009
Iteration 208/1000 | Loss: 0.00001009
Iteration 209/1000 | Loss: 0.00001009
Iteration 210/1000 | Loss: 0.00001009
Iteration 211/1000 | Loss: 0.00001009
Iteration 212/1000 | Loss: 0.00001009
Iteration 213/1000 | Loss: 0.00001009
Iteration 214/1000 | Loss: 0.00001009
Iteration 215/1000 | Loss: 0.00001009
Iteration 216/1000 | Loss: 0.00001009
Iteration 217/1000 | Loss: 0.00001009
Iteration 218/1000 | Loss: 0.00001009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.0090414434671402e-05, 1.0090414434671402e-05, 1.0090414434671402e-05, 1.0090414434671402e-05, 1.0090414434671402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0090414434671402e-05

Optimization complete. Final v2v error: 2.7354788780212402 mm

Highest mean error: 2.879821300506592 mm for frame 107

Lowest mean error: 2.6819076538085938 mm for frame 34

Saving results

Total time: 40.75291705131531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820636
Iteration 2/25 | Loss: 0.00178491
Iteration 3/25 | Loss: 0.00151266
Iteration 4/25 | Loss: 0.00147031
Iteration 5/25 | Loss: 0.00144069
Iteration 6/25 | Loss: 0.00139877
Iteration 7/25 | Loss: 0.00138446
Iteration 8/25 | Loss: 0.00137772
Iteration 9/25 | Loss: 0.00136893
Iteration 10/25 | Loss: 0.00136106
Iteration 11/25 | Loss: 0.00136450
Iteration 12/25 | Loss: 0.00135904
Iteration 13/25 | Loss: 0.00135228
Iteration 14/25 | Loss: 0.00135046
Iteration 15/25 | Loss: 0.00135001
Iteration 16/25 | Loss: 0.00134983
Iteration 17/25 | Loss: 0.00134972
Iteration 18/25 | Loss: 0.00134956
Iteration 19/25 | Loss: 0.00135297
Iteration 20/25 | Loss: 0.00134738
Iteration 21/25 | Loss: 0.00134821
Iteration 22/25 | Loss: 0.00134523
Iteration 23/25 | Loss: 0.00134377
Iteration 24/25 | Loss: 0.00134335
Iteration 25/25 | Loss: 0.00134325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54833937
Iteration 2/25 | Loss: 0.00085241
Iteration 3/25 | Loss: 0.00085241
Iteration 4/25 | Loss: 0.00085241
Iteration 5/25 | Loss: 0.00085240
Iteration 6/25 | Loss: 0.00085240
Iteration 7/25 | Loss: 0.00085240
Iteration 8/25 | Loss: 0.00085240
Iteration 9/25 | Loss: 0.00085240
Iteration 10/25 | Loss: 0.00085240
Iteration 11/25 | Loss: 0.00085240
Iteration 12/25 | Loss: 0.00085240
Iteration 13/25 | Loss: 0.00085240
Iteration 14/25 | Loss: 0.00085240
Iteration 15/25 | Loss: 0.00085240
Iteration 16/25 | Loss: 0.00085240
Iteration 17/25 | Loss: 0.00085240
Iteration 18/25 | Loss: 0.00085240
Iteration 19/25 | Loss: 0.00085240
Iteration 20/25 | Loss: 0.00085240
Iteration 21/25 | Loss: 0.00085240
Iteration 22/25 | Loss: 0.00085240
Iteration 23/25 | Loss: 0.00085240
Iteration 24/25 | Loss: 0.00085240
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008524037548340857, 0.0008524037548340857, 0.0008524037548340857, 0.0008524037548340857, 0.0008524037548340857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008524037548340857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085240
Iteration 2/1000 | Loss: 0.00007672
Iteration 3/1000 | Loss: 0.00005497
Iteration 4/1000 | Loss: 0.00014518
Iteration 5/1000 | Loss: 0.00004840
Iteration 6/1000 | Loss: 0.00004354
Iteration 7/1000 | Loss: 0.00004019
Iteration 8/1000 | Loss: 0.00047343
Iteration 9/1000 | Loss: 0.00005634
Iteration 10/1000 | Loss: 0.00004322
Iteration 11/1000 | Loss: 0.00003840
Iteration 12/1000 | Loss: 0.00003534
Iteration 13/1000 | Loss: 0.00003434
Iteration 14/1000 | Loss: 0.00003381
Iteration 15/1000 | Loss: 0.00003330
Iteration 16/1000 | Loss: 0.00003290
Iteration 17/1000 | Loss: 0.00003263
Iteration 18/1000 | Loss: 0.00003231
Iteration 19/1000 | Loss: 0.00003202
Iteration 20/1000 | Loss: 0.00003181
Iteration 21/1000 | Loss: 0.00003161
Iteration 22/1000 | Loss: 0.00003160
Iteration 23/1000 | Loss: 0.00003145
Iteration 24/1000 | Loss: 0.00003143
Iteration 25/1000 | Loss: 0.00003136
Iteration 26/1000 | Loss: 0.00003130
Iteration 27/1000 | Loss: 0.00003128
Iteration 28/1000 | Loss: 0.00003128
Iteration 29/1000 | Loss: 0.00003127
Iteration 30/1000 | Loss: 0.00003124
Iteration 31/1000 | Loss: 0.00003123
Iteration 32/1000 | Loss: 0.00003122
Iteration 33/1000 | Loss: 0.00003122
Iteration 34/1000 | Loss: 0.00003121
Iteration 35/1000 | Loss: 0.00003121
Iteration 36/1000 | Loss: 0.00003120
Iteration 37/1000 | Loss: 0.00003120
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003119
Iteration 40/1000 | Loss: 0.00003117
Iteration 41/1000 | Loss: 0.00003116
Iteration 42/1000 | Loss: 0.00003115
Iteration 43/1000 | Loss: 0.00003115
Iteration 44/1000 | Loss: 0.00003114
Iteration 45/1000 | Loss: 0.00003114
Iteration 46/1000 | Loss: 0.00003113
Iteration 47/1000 | Loss: 0.00003112
Iteration 48/1000 | Loss: 0.00003111
Iteration 49/1000 | Loss: 0.00003111
Iteration 50/1000 | Loss: 0.00003110
Iteration 51/1000 | Loss: 0.00003110
Iteration 52/1000 | Loss: 0.00003110
Iteration 53/1000 | Loss: 0.00003110
Iteration 54/1000 | Loss: 0.00003110
Iteration 55/1000 | Loss: 0.00003110
Iteration 56/1000 | Loss: 0.00003110
Iteration 57/1000 | Loss: 0.00003110
Iteration 58/1000 | Loss: 0.00003110
Iteration 59/1000 | Loss: 0.00003110
Iteration 60/1000 | Loss: 0.00003110
Iteration 61/1000 | Loss: 0.00003110
Iteration 62/1000 | Loss: 0.00003109
Iteration 63/1000 | Loss: 0.00003109
Iteration 64/1000 | Loss: 0.00003109
Iteration 65/1000 | Loss: 0.00003109
Iteration 66/1000 | Loss: 0.00003109
Iteration 67/1000 | Loss: 0.00003109
Iteration 68/1000 | Loss: 0.00003108
Iteration 69/1000 | Loss: 0.00003108
Iteration 70/1000 | Loss: 0.00003107
Iteration 71/1000 | Loss: 0.00003107
Iteration 72/1000 | Loss: 0.00003107
Iteration 73/1000 | Loss: 0.00003106
Iteration 74/1000 | Loss: 0.00003106
Iteration 75/1000 | Loss: 0.00003106
Iteration 76/1000 | Loss: 0.00003106
Iteration 77/1000 | Loss: 0.00003106
Iteration 78/1000 | Loss: 0.00003106
Iteration 79/1000 | Loss: 0.00003106
Iteration 80/1000 | Loss: 0.00003106
Iteration 81/1000 | Loss: 0.00003106
Iteration 82/1000 | Loss: 0.00003106
Iteration 83/1000 | Loss: 0.00003105
Iteration 84/1000 | Loss: 0.00003105
Iteration 85/1000 | Loss: 0.00003105
Iteration 86/1000 | Loss: 0.00003105
Iteration 87/1000 | Loss: 0.00003105
Iteration 88/1000 | Loss: 0.00003104
Iteration 89/1000 | Loss: 0.00003104
Iteration 90/1000 | Loss: 0.00003104
Iteration 91/1000 | Loss: 0.00003104
Iteration 92/1000 | Loss: 0.00003104
Iteration 93/1000 | Loss: 0.00003104
Iteration 94/1000 | Loss: 0.00003103
Iteration 95/1000 | Loss: 0.00003103
Iteration 96/1000 | Loss: 0.00003102
Iteration 97/1000 | Loss: 0.00003102
Iteration 98/1000 | Loss: 0.00003102
Iteration 99/1000 | Loss: 0.00003102
Iteration 100/1000 | Loss: 0.00003102
Iteration 101/1000 | Loss: 0.00003102
Iteration 102/1000 | Loss: 0.00003102
Iteration 103/1000 | Loss: 0.00003102
Iteration 104/1000 | Loss: 0.00003102
Iteration 105/1000 | Loss: 0.00003102
Iteration 106/1000 | Loss: 0.00003101
Iteration 107/1000 | Loss: 0.00003101
Iteration 108/1000 | Loss: 0.00003101
Iteration 109/1000 | Loss: 0.00003101
Iteration 110/1000 | Loss: 0.00003101
Iteration 111/1000 | Loss: 0.00003101
Iteration 112/1000 | Loss: 0.00003101
Iteration 113/1000 | Loss: 0.00003101
Iteration 114/1000 | Loss: 0.00003101
Iteration 115/1000 | Loss: 0.00003101
Iteration 116/1000 | Loss: 0.00003101
Iteration 117/1000 | Loss: 0.00003101
Iteration 118/1000 | Loss: 0.00003101
Iteration 119/1000 | Loss: 0.00003100
Iteration 120/1000 | Loss: 0.00003100
Iteration 121/1000 | Loss: 0.00003100
Iteration 122/1000 | Loss: 0.00003100
Iteration 123/1000 | Loss: 0.00003100
Iteration 124/1000 | Loss: 0.00003100
Iteration 125/1000 | Loss: 0.00003100
Iteration 126/1000 | Loss: 0.00003099
Iteration 127/1000 | Loss: 0.00003099
Iteration 128/1000 | Loss: 0.00003099
Iteration 129/1000 | Loss: 0.00003099
Iteration 130/1000 | Loss: 0.00003099
Iteration 131/1000 | Loss: 0.00003099
Iteration 132/1000 | Loss: 0.00003099
Iteration 133/1000 | Loss: 0.00003099
Iteration 134/1000 | Loss: 0.00003099
Iteration 135/1000 | Loss: 0.00003099
Iteration 136/1000 | Loss: 0.00003099
Iteration 137/1000 | Loss: 0.00003099
Iteration 138/1000 | Loss: 0.00003099
Iteration 139/1000 | Loss: 0.00003099
Iteration 140/1000 | Loss: 0.00003099
Iteration 141/1000 | Loss: 0.00003099
Iteration 142/1000 | Loss: 0.00003099
Iteration 143/1000 | Loss: 0.00003099
Iteration 144/1000 | Loss: 0.00003098
Iteration 145/1000 | Loss: 0.00003098
Iteration 146/1000 | Loss: 0.00003098
Iteration 147/1000 | Loss: 0.00003098
Iteration 148/1000 | Loss: 0.00003098
Iteration 149/1000 | Loss: 0.00003098
Iteration 150/1000 | Loss: 0.00003098
Iteration 151/1000 | Loss: 0.00003098
Iteration 152/1000 | Loss: 0.00003098
Iteration 153/1000 | Loss: 0.00003098
Iteration 154/1000 | Loss: 0.00003097
Iteration 155/1000 | Loss: 0.00003097
Iteration 156/1000 | Loss: 0.00003097
Iteration 157/1000 | Loss: 0.00003097
Iteration 158/1000 | Loss: 0.00003097
Iteration 159/1000 | Loss: 0.00003096
Iteration 160/1000 | Loss: 0.00003096
Iteration 161/1000 | Loss: 0.00003096
Iteration 162/1000 | Loss: 0.00003096
Iteration 163/1000 | Loss: 0.00003096
Iteration 164/1000 | Loss: 0.00003096
Iteration 165/1000 | Loss: 0.00003096
Iteration 166/1000 | Loss: 0.00003096
Iteration 167/1000 | Loss: 0.00003096
Iteration 168/1000 | Loss: 0.00003096
Iteration 169/1000 | Loss: 0.00003096
Iteration 170/1000 | Loss: 0.00003096
Iteration 171/1000 | Loss: 0.00003096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.0959963623899966e-05, 3.0959963623899966e-05, 3.0959963623899966e-05, 3.0959963623899966e-05, 3.0959963623899966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0959963623899966e-05

Optimization complete. Final v2v error: 4.639314651489258 mm

Highest mean error: 6.062351703643799 mm for frame 139

Lowest mean error: 3.659949541091919 mm for frame 82

Saving results

Total time: 100.60579133033752
