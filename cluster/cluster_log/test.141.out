Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=141, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7896-7951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1098
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053681
Iteration 2/25 | Loss: 0.00253021
Iteration 3/25 | Loss: 0.00165802
Iteration 4/25 | Loss: 0.00149345
Iteration 5/25 | Loss: 0.00148066
Iteration 6/25 | Loss: 0.00153401
Iteration 7/25 | Loss: 0.00144513
Iteration 8/25 | Loss: 0.00133595
Iteration 9/25 | Loss: 0.00125480
Iteration 10/25 | Loss: 0.00119909
Iteration 11/25 | Loss: 0.00118848
Iteration 12/25 | Loss: 0.00117444
Iteration 13/25 | Loss: 0.00115770
Iteration 14/25 | Loss: 0.00113916
Iteration 15/25 | Loss: 0.00112975
Iteration 16/25 | Loss: 0.00112848
Iteration 17/25 | Loss: 0.00112808
Iteration 18/25 | Loss: 0.00112251
Iteration 19/25 | Loss: 0.00111509
Iteration 20/25 | Loss: 0.00111132
Iteration 21/25 | Loss: 0.00111163
Iteration 22/25 | Loss: 0.00110971
Iteration 23/25 | Loss: 0.00111102
Iteration 24/25 | Loss: 0.00110937
Iteration 25/25 | Loss: 0.00110534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34533989
Iteration 2/25 | Loss: 0.00226171
Iteration 3/25 | Loss: 0.00200961
Iteration 4/25 | Loss: 0.00200958
Iteration 5/25 | Loss: 0.00200958
Iteration 6/25 | Loss: 0.00200958
Iteration 7/25 | Loss: 0.00200958
Iteration 8/25 | Loss: 0.00200958
Iteration 9/25 | Loss: 0.00200958
Iteration 10/25 | Loss: 0.00200958
Iteration 11/25 | Loss: 0.00200958
Iteration 12/25 | Loss: 0.00200958
Iteration 13/25 | Loss: 0.00200958
Iteration 14/25 | Loss: 0.00200958
Iteration 15/25 | Loss: 0.00200958
Iteration 16/25 | Loss: 0.00200958
Iteration 17/25 | Loss: 0.00200958
Iteration 18/25 | Loss: 0.00200958
Iteration 19/25 | Loss: 0.00200958
Iteration 20/25 | Loss: 0.00200958
Iteration 21/25 | Loss: 0.00200958
Iteration 22/25 | Loss: 0.00200958
Iteration 23/25 | Loss: 0.00200958
Iteration 24/25 | Loss: 0.00200958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020095810759812593, 0.0020095810759812593, 0.0020095810759812593, 0.0020095810759812593, 0.0020095810759812593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020095810759812593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200958
Iteration 2/1000 | Loss: 0.00065252
Iteration 3/1000 | Loss: 0.00035360
Iteration 4/1000 | Loss: 0.00047147
Iteration 5/1000 | Loss: 0.00075171
Iteration 6/1000 | Loss: 0.00084758
Iteration 7/1000 | Loss: 0.00073839
Iteration 8/1000 | Loss: 0.00072887
Iteration 9/1000 | Loss: 0.00128538
Iteration 10/1000 | Loss: 0.00084582
Iteration 11/1000 | Loss: 0.00108103
Iteration 12/1000 | Loss: 0.00065285
Iteration 13/1000 | Loss: 0.00131326
Iteration 14/1000 | Loss: 0.00068176
Iteration 15/1000 | Loss: 0.00052582
Iteration 16/1000 | Loss: 0.00194087
Iteration 17/1000 | Loss: 0.00075145
Iteration 18/1000 | Loss: 0.00047723
Iteration 19/1000 | Loss: 0.00109930
Iteration 20/1000 | Loss: 0.00019335
Iteration 21/1000 | Loss: 0.00067723
Iteration 22/1000 | Loss: 0.00066433
Iteration 23/1000 | Loss: 0.00042928
Iteration 24/1000 | Loss: 0.00114391
Iteration 25/1000 | Loss: 0.00058121
Iteration 26/1000 | Loss: 0.00033374
Iteration 27/1000 | Loss: 0.00058186
Iteration 28/1000 | Loss: 0.00062615
Iteration 29/1000 | Loss: 0.00084139
Iteration 30/1000 | Loss: 0.00062802
Iteration 31/1000 | Loss: 0.00012457
Iteration 32/1000 | Loss: 0.00017136
Iteration 33/1000 | Loss: 0.00038609
Iteration 34/1000 | Loss: 0.00099008
Iteration 35/1000 | Loss: 0.00018030
Iteration 36/1000 | Loss: 0.00020556
Iteration 37/1000 | Loss: 0.00011050
Iteration 38/1000 | Loss: 0.00062948
Iteration 39/1000 | Loss: 0.00025653
Iteration 40/1000 | Loss: 0.00030552
Iteration 41/1000 | Loss: 0.00012887
Iteration 42/1000 | Loss: 0.00025144
Iteration 43/1000 | Loss: 0.00018338
Iteration 44/1000 | Loss: 0.00035388
Iteration 45/1000 | Loss: 0.00030247
Iteration 46/1000 | Loss: 0.00107244
Iteration 47/1000 | Loss: 0.00091585
Iteration 48/1000 | Loss: 0.00058242
Iteration 49/1000 | Loss: 0.00085850
Iteration 50/1000 | Loss: 0.00063570
Iteration 51/1000 | Loss: 0.00073768
Iteration 52/1000 | Loss: 0.00027606
Iteration 53/1000 | Loss: 0.00021420
Iteration 54/1000 | Loss: 0.00020421
Iteration 55/1000 | Loss: 0.00010019
Iteration 56/1000 | Loss: 0.00019521
Iteration 57/1000 | Loss: 0.00018600
Iteration 58/1000 | Loss: 0.00016684
Iteration 59/1000 | Loss: 0.00015492
Iteration 60/1000 | Loss: 0.00028624
Iteration 61/1000 | Loss: 0.00044954
Iteration 62/1000 | Loss: 0.00040919
Iteration 63/1000 | Loss: 0.00062061
Iteration 64/1000 | Loss: 0.00031166
Iteration 65/1000 | Loss: 0.00049910
Iteration 66/1000 | Loss: 0.00021930
Iteration 67/1000 | Loss: 0.00027966
Iteration 68/1000 | Loss: 0.00018231
Iteration 69/1000 | Loss: 0.00020680
Iteration 70/1000 | Loss: 0.00029906
Iteration 71/1000 | Loss: 0.00027941
Iteration 72/1000 | Loss: 0.00039668
Iteration 73/1000 | Loss: 0.00048808
Iteration 74/1000 | Loss: 0.00081848
Iteration 75/1000 | Loss: 0.00031498
Iteration 76/1000 | Loss: 0.00085219
Iteration 77/1000 | Loss: 0.00037101
Iteration 78/1000 | Loss: 0.00052096
Iteration 79/1000 | Loss: 0.00057312
Iteration 80/1000 | Loss: 0.00050323
Iteration 81/1000 | Loss: 0.00047948
Iteration 82/1000 | Loss: 0.00038131
Iteration 83/1000 | Loss: 0.00020894
Iteration 84/1000 | Loss: 0.00018371
Iteration 85/1000 | Loss: 0.00016441
Iteration 86/1000 | Loss: 0.00020269
Iteration 87/1000 | Loss: 0.00044050
Iteration 88/1000 | Loss: 0.00027772
Iteration 89/1000 | Loss: 0.00023621
Iteration 90/1000 | Loss: 0.00018140
Iteration 91/1000 | Loss: 0.00016153
Iteration 92/1000 | Loss: 0.00016831
Iteration 93/1000 | Loss: 0.00010119
Iteration 94/1000 | Loss: 0.00011788
Iteration 95/1000 | Loss: 0.00037343
Iteration 96/1000 | Loss: 0.00048610
Iteration 97/1000 | Loss: 0.00067465
Iteration 98/1000 | Loss: 0.00052367
Iteration 99/1000 | Loss: 0.00053600
Iteration 100/1000 | Loss: 0.00025925
Iteration 101/1000 | Loss: 0.00074218
Iteration 102/1000 | Loss: 0.00035427
Iteration 103/1000 | Loss: 0.00007656
Iteration 104/1000 | Loss: 0.00016423
Iteration 105/1000 | Loss: 0.00010114
Iteration 106/1000 | Loss: 0.00020048
Iteration 107/1000 | Loss: 0.00020101
Iteration 108/1000 | Loss: 0.00017424
Iteration 109/1000 | Loss: 0.00024184
Iteration 110/1000 | Loss: 0.00039791
Iteration 111/1000 | Loss: 0.00036431
Iteration 112/1000 | Loss: 0.00029685
Iteration 113/1000 | Loss: 0.00038914
Iteration 114/1000 | Loss: 0.00035776
Iteration 115/1000 | Loss: 0.00010096
Iteration 116/1000 | Loss: 0.00019074
Iteration 117/1000 | Loss: 0.00028619
Iteration 118/1000 | Loss: 0.00026804
Iteration 119/1000 | Loss: 0.00023811
Iteration 120/1000 | Loss: 0.00014900
Iteration 121/1000 | Loss: 0.00016040
Iteration 122/1000 | Loss: 0.00016973
Iteration 123/1000 | Loss: 0.00016833
Iteration 124/1000 | Loss: 0.00008549
Iteration 125/1000 | Loss: 0.00049288
Iteration 126/1000 | Loss: 0.00133729
Iteration 127/1000 | Loss: 0.00057433
Iteration 128/1000 | Loss: 0.00021628
Iteration 129/1000 | Loss: 0.00012058
Iteration 130/1000 | Loss: 0.00014009
Iteration 131/1000 | Loss: 0.00012124
Iteration 132/1000 | Loss: 0.00010236
Iteration 133/1000 | Loss: 0.00066885
Iteration 134/1000 | Loss: 0.00074069
Iteration 135/1000 | Loss: 0.00077910
Iteration 136/1000 | Loss: 0.00057641
Iteration 137/1000 | Loss: 0.00068745
Iteration 138/1000 | Loss: 0.00006950
Iteration 139/1000 | Loss: 0.00029160
Iteration 140/1000 | Loss: 0.00012217
Iteration 141/1000 | Loss: 0.00007299
Iteration 142/1000 | Loss: 0.00007557
Iteration 143/1000 | Loss: 0.00006532
Iteration 144/1000 | Loss: 0.00006046
Iteration 145/1000 | Loss: 0.00005024
Iteration 146/1000 | Loss: 0.00005828
Iteration 147/1000 | Loss: 0.00016071
Iteration 148/1000 | Loss: 0.00020642
Iteration 149/1000 | Loss: 0.00021716
Iteration 150/1000 | Loss: 0.00028347
Iteration 151/1000 | Loss: 0.00008550
Iteration 152/1000 | Loss: 0.00016236
Iteration 153/1000 | Loss: 0.00021396
Iteration 154/1000 | Loss: 0.00019923
Iteration 155/1000 | Loss: 0.00023776
Iteration 156/1000 | Loss: 0.00035499
Iteration 157/1000 | Loss: 0.00022857
Iteration 158/1000 | Loss: 0.00028505
Iteration 159/1000 | Loss: 0.00011089
Iteration 160/1000 | Loss: 0.00021291
Iteration 161/1000 | Loss: 0.00012850
Iteration 162/1000 | Loss: 0.00048460
Iteration 163/1000 | Loss: 0.00026534
Iteration 164/1000 | Loss: 0.00009467
Iteration 165/1000 | Loss: 0.00028770
Iteration 166/1000 | Loss: 0.00024033
Iteration 167/1000 | Loss: 0.00009483
Iteration 168/1000 | Loss: 0.00006296
Iteration 169/1000 | Loss: 0.00004077
Iteration 170/1000 | Loss: 0.00021673
Iteration 171/1000 | Loss: 0.00005201
Iteration 172/1000 | Loss: 0.00003506
Iteration 173/1000 | Loss: 0.00005335
Iteration 174/1000 | Loss: 0.00005026
Iteration 175/1000 | Loss: 0.00004549
Iteration 176/1000 | Loss: 0.00006943
Iteration 177/1000 | Loss: 0.00005326
Iteration 178/1000 | Loss: 0.00004253
Iteration 179/1000 | Loss: 0.00005230
Iteration 180/1000 | Loss: 0.00004384
Iteration 181/1000 | Loss: 0.00005465
Iteration 182/1000 | Loss: 0.00073739
Iteration 183/1000 | Loss: 0.00028582
Iteration 184/1000 | Loss: 0.00052232
Iteration 185/1000 | Loss: 0.00020452
Iteration 186/1000 | Loss: 0.00048825
Iteration 187/1000 | Loss: 0.00008817
Iteration 188/1000 | Loss: 0.00007050
Iteration 189/1000 | Loss: 0.00017084
Iteration 190/1000 | Loss: 0.00013047
Iteration 191/1000 | Loss: 0.00025028
Iteration 192/1000 | Loss: 0.00018743
Iteration 193/1000 | Loss: 0.00010342
Iteration 194/1000 | Loss: 0.00012783
Iteration 195/1000 | Loss: 0.00011805
Iteration 196/1000 | Loss: 0.00007839
Iteration 197/1000 | Loss: 0.00005388
Iteration 198/1000 | Loss: 0.00004245
Iteration 199/1000 | Loss: 0.00004151
Iteration 200/1000 | Loss: 0.00014297
Iteration 201/1000 | Loss: 0.00010590
Iteration 202/1000 | Loss: 0.00013459
Iteration 203/1000 | Loss: 0.00004576
Iteration 204/1000 | Loss: 0.00006614
Iteration 205/1000 | Loss: 0.00005960
Iteration 206/1000 | Loss: 0.00007361
Iteration 207/1000 | Loss: 0.00005028
Iteration 208/1000 | Loss: 0.00004908
Iteration 209/1000 | Loss: 0.00004853
Iteration 210/1000 | Loss: 0.00005366
Iteration 211/1000 | Loss: 0.00006137
Iteration 212/1000 | Loss: 0.00005049
Iteration 213/1000 | Loss: 0.00005822
Iteration 214/1000 | Loss: 0.00005854
Iteration 215/1000 | Loss: 0.00005078
Iteration 216/1000 | Loss: 0.00006641
Iteration 217/1000 | Loss: 0.00004634
Iteration 218/1000 | Loss: 0.00004586
Iteration 219/1000 | Loss: 0.00005015
Iteration 220/1000 | Loss: 0.00006296
Iteration 221/1000 | Loss: 0.00004682
Iteration 222/1000 | Loss: 0.00004993
Iteration 223/1000 | Loss: 0.00004583
Iteration 224/1000 | Loss: 0.00005874
Iteration 225/1000 | Loss: 0.00004383
Iteration 226/1000 | Loss: 0.00004742
Iteration 227/1000 | Loss: 0.00004628
Iteration 228/1000 | Loss: 0.00004709
Iteration 229/1000 | Loss: 0.00004423
Iteration 230/1000 | Loss: 0.00005833
Iteration 231/1000 | Loss: 0.00004510
Iteration 232/1000 | Loss: 0.00004993
Iteration 233/1000 | Loss: 0.00004737
Iteration 234/1000 | Loss: 0.00004794
Iteration 235/1000 | Loss: 0.00005854
Iteration 236/1000 | Loss: 0.00033073
Iteration 237/1000 | Loss: 0.00011595
Iteration 238/1000 | Loss: 0.00005266
Iteration 239/1000 | Loss: 0.00005129
Iteration 240/1000 | Loss: 0.00004232
Iteration 241/1000 | Loss: 0.00026446
Iteration 242/1000 | Loss: 0.00018871
Iteration 243/1000 | Loss: 0.00037144
Iteration 244/1000 | Loss: 0.00005007
Iteration 245/1000 | Loss: 0.00044279
Iteration 246/1000 | Loss: 0.00015357
Iteration 247/1000 | Loss: 0.00013233
Iteration 248/1000 | Loss: 0.00010974
Iteration 249/1000 | Loss: 0.00010406
Iteration 250/1000 | Loss: 0.00004010
Iteration 251/1000 | Loss: 0.00033268
Iteration 252/1000 | Loss: 0.00015375
Iteration 253/1000 | Loss: 0.00014914
Iteration 254/1000 | Loss: 0.00015697
Iteration 255/1000 | Loss: 0.00007036
Iteration 256/1000 | Loss: 0.00012117
Iteration 257/1000 | Loss: 0.00002721
Iteration 258/1000 | Loss: 0.00002750
Iteration 259/1000 | Loss: 0.00001984
Iteration 260/1000 | Loss: 0.00002010
Iteration 261/1000 | Loss: 0.00005474
Iteration 262/1000 | Loss: 0.00002234
Iteration 263/1000 | Loss: 0.00002175
Iteration 264/1000 | Loss: 0.00001454
Iteration 265/1000 | Loss: 0.00001545
Iteration 266/1000 | Loss: 0.00001914
Iteration 267/1000 | Loss: 0.00001885
Iteration 268/1000 | Loss: 0.00002006
Iteration 269/1000 | Loss: 0.00002302
Iteration 270/1000 | Loss: 0.00002492
Iteration 271/1000 | Loss: 0.00002606
Iteration 272/1000 | Loss: 0.00002361
Iteration 273/1000 | Loss: 0.00002963
Iteration 274/1000 | Loss: 0.00002455
Iteration 275/1000 | Loss: 0.00002534
Iteration 276/1000 | Loss: 0.00002513
Iteration 277/1000 | Loss: 0.00001813
Iteration 278/1000 | Loss: 0.00001700
Iteration 279/1000 | Loss: 0.00002499
Iteration 280/1000 | Loss: 0.00003773
Iteration 281/1000 | Loss: 0.00002503
Iteration 282/1000 | Loss: 0.00002488
Iteration 283/1000 | Loss: 0.00002298
Iteration 284/1000 | Loss: 0.00002399
Iteration 285/1000 | Loss: 0.00002476
Iteration 286/1000 | Loss: 0.00003735
Iteration 287/1000 | Loss: 0.00007835
Iteration 288/1000 | Loss: 0.00003450
Iteration 289/1000 | Loss: 0.00002508
Iteration 290/1000 | Loss: 0.00002551
Iteration 291/1000 | Loss: 0.00002615
Iteration 292/1000 | Loss: 0.00002642
Iteration 293/1000 | Loss: 0.00002101
Iteration 294/1000 | Loss: 0.00002560
Iteration 295/1000 | Loss: 0.00002361
Iteration 296/1000 | Loss: 0.00002496
Iteration 297/1000 | Loss: 0.00002449
Iteration 298/1000 | Loss: 0.00002493
Iteration 299/1000 | Loss: 0.00002249
Iteration 300/1000 | Loss: 0.00002839
Iteration 301/1000 | Loss: 0.00001751
Iteration 302/1000 | Loss: 0.00001452
Iteration 303/1000 | Loss: 0.00001220
Iteration 304/1000 | Loss: 0.00001109
Iteration 305/1000 | Loss: 0.00001075
Iteration 306/1000 | Loss: 0.00001046
Iteration 307/1000 | Loss: 0.00003060
Iteration 308/1000 | Loss: 0.00001009
Iteration 309/1000 | Loss: 0.00000998
Iteration 310/1000 | Loss: 0.00000997
Iteration 311/1000 | Loss: 0.00000997
Iteration 312/1000 | Loss: 0.00000995
Iteration 313/1000 | Loss: 0.00000992
Iteration 314/1000 | Loss: 0.00000991
Iteration 315/1000 | Loss: 0.00000988
Iteration 316/1000 | Loss: 0.00000988
Iteration 317/1000 | Loss: 0.00000987
Iteration 318/1000 | Loss: 0.00000986
Iteration 319/1000 | Loss: 0.00000986
Iteration 320/1000 | Loss: 0.00000985
Iteration 321/1000 | Loss: 0.00000985
Iteration 322/1000 | Loss: 0.00000985
Iteration 323/1000 | Loss: 0.00000984
Iteration 324/1000 | Loss: 0.00000984
Iteration 325/1000 | Loss: 0.00000983
Iteration 326/1000 | Loss: 0.00000983
Iteration 327/1000 | Loss: 0.00000983
Iteration 328/1000 | Loss: 0.00000982
Iteration 329/1000 | Loss: 0.00000982
Iteration 330/1000 | Loss: 0.00000982
Iteration 331/1000 | Loss: 0.00000982
Iteration 332/1000 | Loss: 0.00000982
Iteration 333/1000 | Loss: 0.00000982
Iteration 334/1000 | Loss: 0.00000981
Iteration 335/1000 | Loss: 0.00000981
Iteration 336/1000 | Loss: 0.00000981
Iteration 337/1000 | Loss: 0.00000981
Iteration 338/1000 | Loss: 0.00000980
Iteration 339/1000 | Loss: 0.00000980
Iteration 340/1000 | Loss: 0.00000980
Iteration 341/1000 | Loss: 0.00000980
Iteration 342/1000 | Loss: 0.00000980
Iteration 343/1000 | Loss: 0.00000980
Iteration 344/1000 | Loss: 0.00000979
Iteration 345/1000 | Loss: 0.00000979
Iteration 346/1000 | Loss: 0.00000979
Iteration 347/1000 | Loss: 0.00000979
Iteration 348/1000 | Loss: 0.00000978
Iteration 349/1000 | Loss: 0.00000978
Iteration 350/1000 | Loss: 0.00000978
Iteration 351/1000 | Loss: 0.00000978
Iteration 352/1000 | Loss: 0.00000978
Iteration 353/1000 | Loss: 0.00000978
Iteration 354/1000 | Loss: 0.00000977
Iteration 355/1000 | Loss: 0.00000977
Iteration 356/1000 | Loss: 0.00000977
Iteration 357/1000 | Loss: 0.00000977
Iteration 358/1000 | Loss: 0.00000977
Iteration 359/1000 | Loss: 0.00000977
Iteration 360/1000 | Loss: 0.00000977
Iteration 361/1000 | Loss: 0.00000977
Iteration 362/1000 | Loss: 0.00000977
Iteration 363/1000 | Loss: 0.00000977
Iteration 364/1000 | Loss: 0.00000977
Iteration 365/1000 | Loss: 0.00000976
Iteration 366/1000 | Loss: 0.00000976
Iteration 367/1000 | Loss: 0.00000976
Iteration 368/1000 | Loss: 0.00000976
Iteration 369/1000 | Loss: 0.00000976
Iteration 370/1000 | Loss: 0.00000976
Iteration 371/1000 | Loss: 0.00000976
Iteration 372/1000 | Loss: 0.00000975
Iteration 373/1000 | Loss: 0.00000975
Iteration 374/1000 | Loss: 0.00000975
Iteration 375/1000 | Loss: 0.00000975
Iteration 376/1000 | Loss: 0.00000974
Iteration 377/1000 | Loss: 0.00000974
Iteration 378/1000 | Loss: 0.00000974
Iteration 379/1000 | Loss: 0.00000973
Iteration 380/1000 | Loss: 0.00000973
Iteration 381/1000 | Loss: 0.00000973
Iteration 382/1000 | Loss: 0.00000972
Iteration 383/1000 | Loss: 0.00000972
Iteration 384/1000 | Loss: 0.00000972
Iteration 385/1000 | Loss: 0.00000971
Iteration 386/1000 | Loss: 0.00000971
Iteration 387/1000 | Loss: 0.00000971
Iteration 388/1000 | Loss: 0.00000971
Iteration 389/1000 | Loss: 0.00000971
Iteration 390/1000 | Loss: 0.00000971
Iteration 391/1000 | Loss: 0.00000971
Iteration 392/1000 | Loss: 0.00000971
Iteration 393/1000 | Loss: 0.00000971
Iteration 394/1000 | Loss: 0.00000970
Iteration 395/1000 | Loss: 0.00000970
Iteration 396/1000 | Loss: 0.00000970
Iteration 397/1000 | Loss: 0.00000970
Iteration 398/1000 | Loss: 0.00000970
Iteration 399/1000 | Loss: 0.00000970
Iteration 400/1000 | Loss: 0.00000970
Iteration 401/1000 | Loss: 0.00000970
Iteration 402/1000 | Loss: 0.00000970
Iteration 403/1000 | Loss: 0.00000970
Iteration 404/1000 | Loss: 0.00000969
Iteration 405/1000 | Loss: 0.00000969
Iteration 406/1000 | Loss: 0.00000969
Iteration 407/1000 | Loss: 0.00000969
Iteration 408/1000 | Loss: 0.00000969
Iteration 409/1000 | Loss: 0.00000969
Iteration 410/1000 | Loss: 0.00000969
Iteration 411/1000 | Loss: 0.00000969
Iteration 412/1000 | Loss: 0.00000969
Iteration 413/1000 | Loss: 0.00000969
Iteration 414/1000 | Loss: 0.00000969
Iteration 415/1000 | Loss: 0.00000969
Iteration 416/1000 | Loss: 0.00000969
Iteration 417/1000 | Loss: 0.00000969
Iteration 418/1000 | Loss: 0.00000969
Iteration 419/1000 | Loss: 0.00000969
Iteration 420/1000 | Loss: 0.00000969
Iteration 421/1000 | Loss: 0.00000969
Iteration 422/1000 | Loss: 0.00000969
Iteration 423/1000 | Loss: 0.00000969
Iteration 424/1000 | Loss: 0.00000969
Iteration 425/1000 | Loss: 0.00000969
Iteration 426/1000 | Loss: 0.00000968
Iteration 427/1000 | Loss: 0.00000968
Iteration 428/1000 | Loss: 0.00000968
Iteration 429/1000 | Loss: 0.00000968
Iteration 430/1000 | Loss: 0.00000968
Iteration 431/1000 | Loss: 0.00000968
Iteration 432/1000 | Loss: 0.00000968
Iteration 433/1000 | Loss: 0.00000968
Iteration 434/1000 | Loss: 0.00000968
Iteration 435/1000 | Loss: 0.00000968
Iteration 436/1000 | Loss: 0.00000968
Iteration 437/1000 | Loss: 0.00000967
Iteration 438/1000 | Loss: 0.00000967
Iteration 439/1000 | Loss: 0.00000967
Iteration 440/1000 | Loss: 0.00000967
Iteration 441/1000 | Loss: 0.00000967
Iteration 442/1000 | Loss: 0.00000967
Iteration 443/1000 | Loss: 0.00000967
Iteration 444/1000 | Loss: 0.00000966
Iteration 445/1000 | Loss: 0.00000966
Iteration 446/1000 | Loss: 0.00000966
Iteration 447/1000 | Loss: 0.00000965
Iteration 448/1000 | Loss: 0.00000965
Iteration 449/1000 | Loss: 0.00000965
Iteration 450/1000 | Loss: 0.00000965
Iteration 451/1000 | Loss: 0.00000965
Iteration 452/1000 | Loss: 0.00000965
Iteration 453/1000 | Loss: 0.00000965
Iteration 454/1000 | Loss: 0.00000965
Iteration 455/1000 | Loss: 0.00000964
Iteration 456/1000 | Loss: 0.00000964
Iteration 457/1000 | Loss: 0.00000964
Iteration 458/1000 | Loss: 0.00000964
Iteration 459/1000 | Loss: 0.00000964
Iteration 460/1000 | Loss: 0.00000964
Iteration 461/1000 | Loss: 0.00000964
Iteration 462/1000 | Loss: 0.00000964
Iteration 463/1000 | Loss: 0.00000964
Iteration 464/1000 | Loss: 0.00000964
Iteration 465/1000 | Loss: 0.00000964
Iteration 466/1000 | Loss: 0.00000964
Iteration 467/1000 | Loss: 0.00000963
Iteration 468/1000 | Loss: 0.00000963
Iteration 469/1000 | Loss: 0.00000963
Iteration 470/1000 | Loss: 0.00000963
Iteration 471/1000 | Loss: 0.00000963
Iteration 472/1000 | Loss: 0.00000963
Iteration 473/1000 | Loss: 0.00000963
Iteration 474/1000 | Loss: 0.00000963
Iteration 475/1000 | Loss: 0.00000963
Iteration 476/1000 | Loss: 0.00000963
Iteration 477/1000 | Loss: 0.00000963
Iteration 478/1000 | Loss: 0.00000963
Iteration 479/1000 | Loss: 0.00000963
Iteration 480/1000 | Loss: 0.00000963
Iteration 481/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 481. Stopping optimization.
Last 5 losses: [9.6310304797953e-06, 9.6310304797953e-06, 9.6310304797953e-06, 9.6310304797953e-06, 9.6310304797953e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.6310304797953e-06

Optimization complete. Final v2v error: 2.4650096893310547 mm

Highest mean error: 9.929521560668945 mm for frame 53

Lowest mean error: 1.93706476688385 mm for frame 109

Saving results

Total time: 14993.525665521622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1074
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980979
Iteration 2/25 | Loss: 0.00124128
Iteration 3/25 | Loss: 0.00103904
Iteration 4/25 | Loss: 0.00100761
Iteration 5/25 | Loss: 0.00100026
Iteration 6/25 | Loss: 0.00099955
Iteration 7/25 | Loss: 0.00099955
Iteration 8/25 | Loss: 0.00099955
Iteration 9/25 | Loss: 0.00099955
Iteration 10/25 | Loss: 0.00099955
Iteration 11/25 | Loss: 0.00099955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000999546144157648, 0.000999546144157648, 0.000999546144157648, 0.000999546144157648, 0.000999546144157648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000999546144157648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44142044
Iteration 2/25 | Loss: 0.00090083
Iteration 3/25 | Loss: 0.00090083
Iteration 4/25 | Loss: 0.00090083
Iteration 5/25 | Loss: 0.00090083
Iteration 6/25 | Loss: 0.00090083
Iteration 7/25 | Loss: 0.00090083
Iteration 8/25 | Loss: 0.00090083
Iteration 9/25 | Loss: 0.00090083
Iteration 10/25 | Loss: 0.00090083
Iteration 11/25 | Loss: 0.00090083
Iteration 12/25 | Loss: 0.00090083
Iteration 13/25 | Loss: 0.00090083
Iteration 14/25 | Loss: 0.00090083
Iteration 15/25 | Loss: 0.00090083
Iteration 16/25 | Loss: 0.00090083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009008260094560683, 0.0009008260094560683, 0.0009008260094560683, 0.0009008260094560683, 0.0009008260094560683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009008260094560683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090083
Iteration 2/1000 | Loss: 0.00004649
Iteration 3/1000 | Loss: 0.00002396
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001504
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001317
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001305
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001296
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001277
Iteration 21/1000 | Loss: 0.00001276
Iteration 22/1000 | Loss: 0.00001276
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001275
Iteration 25/1000 | Loss: 0.00001274
Iteration 26/1000 | Loss: 0.00001274
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001272
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001267
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001265
Iteration 42/1000 | Loss: 0.00001265
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001257
Iteration 97/1000 | Loss: 0.00001257
Iteration 98/1000 | Loss: 0.00001257
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001257
Iteration 107/1000 | Loss: 0.00001257
Iteration 108/1000 | Loss: 0.00001257
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001257
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001257
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001257
Iteration 131/1000 | Loss: 0.00001257
Iteration 132/1000 | Loss: 0.00001257
Iteration 133/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.257469648408005e-05, 1.257469648408005e-05, 1.257469648408005e-05, 1.257469648408005e-05, 1.257469648408005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.257469648408005e-05

Optimization complete. Final v2v error: 3.0058658123016357 mm

Highest mean error: 3.2388906478881836 mm for frame 9

Lowest mean error: 2.751354455947876 mm for frame 134

Saving results

Total time: 761.2537016868591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1023
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872391
Iteration 2/25 | Loss: 0.00125026
Iteration 3/25 | Loss: 0.00106392
Iteration 4/25 | Loss: 0.00102386
Iteration 5/25 | Loss: 0.00100637
Iteration 6/25 | Loss: 0.00101042
Iteration 7/25 | Loss: 0.00102158
Iteration 8/25 | Loss: 0.00102527
Iteration 9/25 | Loss: 0.00102120
Iteration 10/25 | Loss: 0.00101360
Iteration 11/25 | Loss: 0.00100732
Iteration 12/25 | Loss: 0.00100352
Iteration 13/25 | Loss: 0.00100005
Iteration 14/25 | Loss: 0.00100359
Iteration 15/25 | Loss: 0.00100413
Iteration 16/25 | Loss: 0.00100223
Iteration 17/25 | Loss: 0.00100100
Iteration 18/25 | Loss: 0.00099917
Iteration 19/25 | Loss: 0.00099783
Iteration 20/25 | Loss: 0.00099425
Iteration 21/25 | Loss: 0.00099482
Iteration 22/25 | Loss: 0.00099478
Iteration 23/25 | Loss: 0.00099426
Iteration 24/25 | Loss: 0.00099465
Iteration 25/25 | Loss: 0.00099600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33282506
Iteration 2/25 | Loss: 0.00107490
Iteration 3/25 | Loss: 0.00107490
Iteration 4/25 | Loss: 0.00107490
Iteration 5/25 | Loss: 0.00107490
Iteration 6/25 | Loss: 0.00107490
Iteration 7/25 | Loss: 0.00107490
Iteration 8/25 | Loss: 0.00107490
Iteration 9/25 | Loss: 0.00107490
Iteration 10/25 | Loss: 0.00107490
Iteration 11/25 | Loss: 0.00107490
Iteration 12/25 | Loss: 0.00107490
Iteration 13/25 | Loss: 0.00107490
Iteration 14/25 | Loss: 0.00107490
Iteration 15/25 | Loss: 0.00107490
Iteration 16/25 | Loss: 0.00107490
Iteration 17/25 | Loss: 0.00107490
Iteration 18/25 | Loss: 0.00107490
Iteration 19/25 | Loss: 0.00107490
Iteration 20/25 | Loss: 0.00107490
Iteration 21/25 | Loss: 0.00107490
Iteration 22/25 | Loss: 0.00107490
Iteration 23/25 | Loss: 0.00107490
Iteration 24/25 | Loss: 0.00107490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010749015491455793, 0.0010749015491455793, 0.0010749015491455793, 0.0010749015491455793, 0.0010749015491455793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010749015491455793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107490
Iteration 2/1000 | Loss: 0.00006704
Iteration 3/1000 | Loss: 0.00004123
Iteration 4/1000 | Loss: 0.00007564
Iteration 5/1000 | Loss: 0.00013593
Iteration 6/1000 | Loss: 0.00005537
Iteration 7/1000 | Loss: 0.00004071
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00002109
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00017538
Iteration 13/1000 | Loss: 0.00009357
Iteration 14/1000 | Loss: 0.00007034
Iteration 15/1000 | Loss: 0.00002405
Iteration 16/1000 | Loss: 0.00004976
Iteration 17/1000 | Loss: 0.00002735
Iteration 18/1000 | Loss: 0.00002640
Iteration 19/1000 | Loss: 0.00002634
Iteration 20/1000 | Loss: 0.00002837
Iteration 21/1000 | Loss: 0.00002186
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00002264
Iteration 24/1000 | Loss: 0.00002035
Iteration 25/1000 | Loss: 0.00002959
Iteration 26/1000 | Loss: 0.00002763
Iteration 27/1000 | Loss: 0.00003949
Iteration 28/1000 | Loss: 0.00003054
Iteration 29/1000 | Loss: 0.00003669
Iteration 30/1000 | Loss: 0.00017844
Iteration 31/1000 | Loss: 0.00004433
Iteration 32/1000 | Loss: 0.00004466
Iteration 33/1000 | Loss: 0.00002090
Iteration 34/1000 | Loss: 0.00003228
Iteration 35/1000 | Loss: 0.00003336
Iteration 36/1000 | Loss: 0.00002809
Iteration 37/1000 | Loss: 0.00002395
Iteration 38/1000 | Loss: 0.00002221
Iteration 39/1000 | Loss: 0.00001823
Iteration 40/1000 | Loss: 0.00002472
Iteration 41/1000 | Loss: 0.00002751
Iteration 42/1000 | Loss: 0.00004128
Iteration 43/1000 | Loss: 0.00003557
Iteration 44/1000 | Loss: 0.00003710
Iteration 45/1000 | Loss: 0.00002147
Iteration 46/1000 | Loss: 0.00003152
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002384
Iteration 50/1000 | Loss: 0.00002754
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00002537
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002189
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002829
Iteration 58/1000 | Loss: 0.00002904
Iteration 59/1000 | Loss: 0.00002794
Iteration 60/1000 | Loss: 0.00003450
Iteration 61/1000 | Loss: 0.00002383
Iteration 62/1000 | Loss: 0.00002648
Iteration 63/1000 | Loss: 0.00002498
Iteration 64/1000 | Loss: 0.00003136
Iteration 65/1000 | Loss: 0.00002468
Iteration 66/1000 | Loss: 0.00003116
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002842
Iteration 69/1000 | Loss: 0.00002727
Iteration 70/1000 | Loss: 0.00001665
Iteration 71/1000 | Loss: 0.00002766
Iteration 72/1000 | Loss: 0.00001967
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00002606
Iteration 75/1000 | Loss: 0.00001812
Iteration 76/1000 | Loss: 0.00002433
Iteration 77/1000 | Loss: 0.00002757
Iteration 78/1000 | Loss: 0.00002506
Iteration 79/1000 | Loss: 0.00002556
Iteration 80/1000 | Loss: 0.00002537
Iteration 81/1000 | Loss: 0.00002902
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002811
Iteration 84/1000 | Loss: 0.00002453
Iteration 85/1000 | Loss: 0.00002800
Iteration 86/1000 | Loss: 0.00002443
Iteration 87/1000 | Loss: 0.00002795
Iteration 88/1000 | Loss: 0.00002443
Iteration 89/1000 | Loss: 0.00002766
Iteration 90/1000 | Loss: 0.00003796
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002343
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002348
Iteration 95/1000 | Loss: 0.00003105
Iteration 96/1000 | Loss: 0.00002507
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002919
Iteration 99/1000 | Loss: 0.00002368
Iteration 100/1000 | Loss: 0.00003097
Iteration 101/1000 | Loss: 0.00002813
Iteration 102/1000 | Loss: 0.00003069
Iteration 103/1000 | Loss: 0.00002758
Iteration 104/1000 | Loss: 0.00002366
Iteration 105/1000 | Loss: 0.00002485
Iteration 106/1000 | Loss: 0.00002326
Iteration 107/1000 | Loss: 0.00002876
Iteration 108/1000 | Loss: 0.00003073
Iteration 109/1000 | Loss: 0.00002804
Iteration 110/1000 | Loss: 0.00002961
Iteration 111/1000 | Loss: 0.00002634
Iteration 112/1000 | Loss: 0.00002666
Iteration 113/1000 | Loss: 0.00002765
Iteration 114/1000 | Loss: 0.00002917
Iteration 115/1000 | Loss: 0.00002767
Iteration 116/1000 | Loss: 0.00003331
Iteration 117/1000 | Loss: 0.00003181
Iteration 118/1000 | Loss: 0.00002536
Iteration 119/1000 | Loss: 0.00003151
Iteration 120/1000 | Loss: 0.00002502
Iteration 121/1000 | Loss: 0.00002947
Iteration 122/1000 | Loss: 0.00002451
Iteration 123/1000 | Loss: 0.00002912
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00003177
Iteration 126/1000 | Loss: 0.00002909
Iteration 127/1000 | Loss: 0.00002579
Iteration 128/1000 | Loss: 0.00002832
Iteration 129/1000 | Loss: 0.00003000
Iteration 130/1000 | Loss: 0.00002523
Iteration 131/1000 | Loss: 0.00002848
Iteration 132/1000 | Loss: 0.00002756
Iteration 133/1000 | Loss: 0.00002888
Iteration 134/1000 | Loss: 0.00003779
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002354
Iteration 137/1000 | Loss: 0.00002552
Iteration 138/1000 | Loss: 0.00002874
Iteration 139/1000 | Loss: 0.00002827
Iteration 140/1000 | Loss: 0.00003279
Iteration 141/1000 | Loss: 0.00002726
Iteration 142/1000 | Loss: 0.00001932
Iteration 143/1000 | Loss: 0.00002708
Iteration 144/1000 | Loss: 0.00002669
Iteration 145/1000 | Loss: 0.00002541
Iteration 146/1000 | Loss: 0.00002713
Iteration 147/1000 | Loss: 0.00002467
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00002158
Iteration 150/1000 | Loss: 0.00001970
Iteration 151/1000 | Loss: 0.00001919
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00002627
Iteration 154/1000 | Loss: 0.00001468
Iteration 155/1000 | Loss: 0.00002525
Iteration 156/1000 | Loss: 0.00002763
Iteration 157/1000 | Loss: 0.00002449
Iteration 158/1000 | Loss: 0.00001583
Iteration 159/1000 | Loss: 0.00002648
Iteration 160/1000 | Loss: 0.00002347
Iteration 161/1000 | Loss: 0.00002653
Iteration 162/1000 | Loss: 0.00002674
Iteration 163/1000 | Loss: 0.00001947
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00001811
Iteration 166/1000 | Loss: 0.00002536
Iteration 167/1000 | Loss: 0.00002550
Iteration 168/1000 | Loss: 0.00002796
Iteration 169/1000 | Loss: 0.00002634
Iteration 170/1000 | Loss: 0.00002655
Iteration 171/1000 | Loss: 0.00002863
Iteration 172/1000 | Loss: 0.00002542
Iteration 173/1000 | Loss: 0.00002706
Iteration 174/1000 | Loss: 0.00002496
Iteration 175/1000 | Loss: 0.00002610
Iteration 176/1000 | Loss: 0.00002463
Iteration 177/1000 | Loss: 0.00002606
Iteration 178/1000 | Loss: 0.00002422
Iteration 179/1000 | Loss: 0.00002595
Iteration 180/1000 | Loss: 0.00001627
Iteration 181/1000 | Loss: 0.00003972
Iteration 182/1000 | Loss: 0.00002357
Iteration 183/1000 | Loss: 0.00002511
Iteration 184/1000 | Loss: 0.00002497
Iteration 185/1000 | Loss: 0.00001792
Iteration 186/1000 | Loss: 0.00003777
Iteration 187/1000 | Loss: 0.00001493
Iteration 188/1000 | Loss: 0.00001420
Iteration 189/1000 | Loss: 0.00001387
Iteration 190/1000 | Loss: 0.00001376
Iteration 191/1000 | Loss: 0.00001376
Iteration 192/1000 | Loss: 0.00001374
Iteration 193/1000 | Loss: 0.00001373
Iteration 194/1000 | Loss: 0.00001358
Iteration 195/1000 | Loss: 0.00001354
Iteration 196/1000 | Loss: 0.00001349
Iteration 197/1000 | Loss: 0.00001346
Iteration 198/1000 | Loss: 0.00001325
Iteration 199/1000 | Loss: 0.00001314
Iteration 200/1000 | Loss: 0.00001300
Iteration 201/1000 | Loss: 0.00001291
Iteration 202/1000 | Loss: 0.00001288
Iteration 203/1000 | Loss: 0.00001286
Iteration 204/1000 | Loss: 0.00001282
Iteration 205/1000 | Loss: 0.00001277
Iteration 206/1000 | Loss: 0.00001275
Iteration 207/1000 | Loss: 0.00001273
Iteration 208/1000 | Loss: 0.00001272
Iteration 209/1000 | Loss: 0.00001271
Iteration 210/1000 | Loss: 0.00001271
Iteration 211/1000 | Loss: 0.00001270
Iteration 212/1000 | Loss: 0.00001269
Iteration 213/1000 | Loss: 0.00001263
Iteration 214/1000 | Loss: 0.00001262
Iteration 215/1000 | Loss: 0.00001262
Iteration 216/1000 | Loss: 0.00001261
Iteration 217/1000 | Loss: 0.00001261
Iteration 218/1000 | Loss: 0.00001260
Iteration 219/1000 | Loss: 0.00001260
Iteration 220/1000 | Loss: 0.00001259
Iteration 221/1000 | Loss: 0.00001259
Iteration 222/1000 | Loss: 0.00001259
Iteration 223/1000 | Loss: 0.00001258
Iteration 224/1000 | Loss: 0.00001258
Iteration 225/1000 | Loss: 0.00001257
Iteration 226/1000 | Loss: 0.00001257
Iteration 227/1000 | Loss: 0.00001257
Iteration 228/1000 | Loss: 0.00001254
Iteration 229/1000 | Loss: 0.00001253
Iteration 230/1000 | Loss: 0.00001253
Iteration 231/1000 | Loss: 0.00001253
Iteration 232/1000 | Loss: 0.00001252
Iteration 233/1000 | Loss: 0.00001252
Iteration 234/1000 | Loss: 0.00001252
Iteration 235/1000 | Loss: 0.00001252
Iteration 236/1000 | Loss: 0.00001251
Iteration 237/1000 | Loss: 0.00001251
Iteration 238/1000 | Loss: 0.00001251
Iteration 239/1000 | Loss: 0.00001250
Iteration 240/1000 | Loss: 0.00001250
Iteration 241/1000 | Loss: 0.00001250
Iteration 242/1000 | Loss: 0.00001250
Iteration 243/1000 | Loss: 0.00001250
Iteration 244/1000 | Loss: 0.00001250
Iteration 245/1000 | Loss: 0.00001249
Iteration 246/1000 | Loss: 0.00001249
Iteration 247/1000 | Loss: 0.00001249
Iteration 248/1000 | Loss: 0.00001249
Iteration 249/1000 | Loss: 0.00001249
Iteration 250/1000 | Loss: 0.00001249
Iteration 251/1000 | Loss: 0.00001249
Iteration 252/1000 | Loss: 0.00001249
Iteration 253/1000 | Loss: 0.00001249
Iteration 254/1000 | Loss: 0.00001249
Iteration 255/1000 | Loss: 0.00001249
Iteration 256/1000 | Loss: 0.00001249
Iteration 257/1000 | Loss: 0.00001249
Iteration 258/1000 | Loss: 0.00001249
Iteration 259/1000 | Loss: 0.00001249
Iteration 260/1000 | Loss: 0.00001249
Iteration 261/1000 | Loss: 0.00001249
Iteration 262/1000 | Loss: 0.00001249
Iteration 263/1000 | Loss: 0.00001249
Iteration 264/1000 | Loss: 0.00001249
Iteration 265/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.2488307220337447e-05, 1.2488307220337447e-05, 1.2488307220337447e-05, 1.2488307220337447e-05, 1.2488307220337447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2488307220337447e-05

Optimization complete. Final v2v error: 2.922593593597412 mm

Highest mean error: 5.024902820587158 mm for frame 221

Lowest mean error: 2.304960012435913 mm for frame 12

Saving results

Total time: 9924.487133026123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1061
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00717604
Iteration 2/25 | Loss: 0.00148152
Iteration 3/25 | Loss: 0.00116819
Iteration 4/25 | Loss: 0.00110084
Iteration 5/25 | Loss: 0.00107688
Iteration 6/25 | Loss: 0.00106616
Iteration 7/25 | Loss: 0.00106685
Iteration 8/25 | Loss: 0.00106241
Iteration 9/25 | Loss: 0.00106219
Iteration 10/25 | Loss: 0.00106212
Iteration 11/25 | Loss: 0.00106212
Iteration 12/25 | Loss: 0.00106212
Iteration 13/25 | Loss: 0.00106212
Iteration 14/25 | Loss: 0.00106212
Iteration 15/25 | Loss: 0.00106212
Iteration 16/25 | Loss: 0.00106212
Iteration 17/25 | Loss: 0.00106212
Iteration 18/25 | Loss: 0.00106212
Iteration 19/25 | Loss: 0.00106212
Iteration 20/25 | Loss: 0.00106212
Iteration 21/25 | Loss: 0.00106211
Iteration 22/25 | Loss: 0.00106211
Iteration 23/25 | Loss: 0.00106211
Iteration 24/25 | Loss: 0.00106211
Iteration 25/25 | Loss: 0.00106211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38935041
Iteration 2/25 | Loss: 0.00101378
Iteration 3/25 | Loss: 0.00101378
Iteration 4/25 | Loss: 0.00101378
Iteration 5/25 | Loss: 0.00101378
Iteration 6/25 | Loss: 0.00101378
Iteration 7/25 | Loss: 0.00101378
Iteration 8/25 | Loss: 0.00101378
Iteration 9/25 | Loss: 0.00101378
Iteration 10/25 | Loss: 0.00101378
Iteration 11/25 | Loss: 0.00101377
Iteration 12/25 | Loss: 0.00101378
Iteration 13/25 | Loss: 0.00101378
Iteration 14/25 | Loss: 0.00101378
Iteration 15/25 | Loss: 0.00101377
Iteration 16/25 | Loss: 0.00101377
Iteration 17/25 | Loss: 0.00101377
Iteration 18/25 | Loss: 0.00101378
Iteration 19/25 | Loss: 0.00101377
Iteration 20/25 | Loss: 0.00101377
Iteration 21/25 | Loss: 0.00101377
Iteration 22/25 | Loss: 0.00101377
Iteration 23/25 | Loss: 0.00101377
Iteration 24/25 | Loss: 0.00101377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010137748904526234, 0.0010137748904526234, 0.0010137748904526234, 0.0010137748904526234, 0.0010137748904526234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010137748904526234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101377
Iteration 2/1000 | Loss: 0.00004940
Iteration 3/1000 | Loss: 0.00002921
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00002171
Iteration 6/1000 | Loss: 0.00002069
Iteration 7/1000 | Loss: 0.00002001
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001868
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001856
Iteration 17/1000 | Loss: 0.00001852
Iteration 18/1000 | Loss: 0.00001852
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001850
Iteration 21/1000 | Loss: 0.00001850
Iteration 22/1000 | Loss: 0.00001850
Iteration 23/1000 | Loss: 0.00001850
Iteration 24/1000 | Loss: 0.00001849
Iteration 25/1000 | Loss: 0.00001849
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001848
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001845
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001841
Iteration 64/1000 | Loss: 0.00001841
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001840
Iteration 69/1000 | Loss: 0.00001840
Iteration 70/1000 | Loss: 0.00001840
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001840
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001839
Iteration 75/1000 | Loss: 0.00001839
Iteration 76/1000 | Loss: 0.00001839
Iteration 77/1000 | Loss: 0.00001839
Iteration 78/1000 | Loss: 0.00001839
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001838
Iteration 81/1000 | Loss: 0.00001838
Iteration 82/1000 | Loss: 0.00001838
Iteration 83/1000 | Loss: 0.00001838
Iteration 84/1000 | Loss: 0.00001838
Iteration 85/1000 | Loss: 0.00001838
Iteration 86/1000 | Loss: 0.00001838
Iteration 87/1000 | Loss: 0.00001837
Iteration 88/1000 | Loss: 0.00001837
Iteration 89/1000 | Loss: 0.00001837
Iteration 90/1000 | Loss: 0.00001837
Iteration 91/1000 | Loss: 0.00001837
Iteration 92/1000 | Loss: 0.00001836
Iteration 93/1000 | Loss: 0.00001836
Iteration 94/1000 | Loss: 0.00001836
Iteration 95/1000 | Loss: 0.00001836
Iteration 96/1000 | Loss: 0.00001836
Iteration 97/1000 | Loss: 0.00001836
Iteration 98/1000 | Loss: 0.00001835
Iteration 99/1000 | Loss: 0.00001835
Iteration 100/1000 | Loss: 0.00001835
Iteration 101/1000 | Loss: 0.00001835
Iteration 102/1000 | Loss: 0.00001834
Iteration 103/1000 | Loss: 0.00001834
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00001834
Iteration 106/1000 | Loss: 0.00001834
Iteration 107/1000 | Loss: 0.00001833
Iteration 108/1000 | Loss: 0.00001833
Iteration 109/1000 | Loss: 0.00001833
Iteration 110/1000 | Loss: 0.00001833
Iteration 111/1000 | Loss: 0.00001833
Iteration 112/1000 | Loss: 0.00001833
Iteration 113/1000 | Loss: 0.00001833
Iteration 114/1000 | Loss: 0.00001833
Iteration 115/1000 | Loss: 0.00001833
Iteration 116/1000 | Loss: 0.00001833
Iteration 117/1000 | Loss: 0.00001832
Iteration 118/1000 | Loss: 0.00001832
Iteration 119/1000 | Loss: 0.00001832
Iteration 120/1000 | Loss: 0.00001832
Iteration 121/1000 | Loss: 0.00001832
Iteration 122/1000 | Loss: 0.00001832
Iteration 123/1000 | Loss: 0.00001832
Iteration 124/1000 | Loss: 0.00001832
Iteration 125/1000 | Loss: 0.00001832
Iteration 126/1000 | Loss: 0.00001832
Iteration 127/1000 | Loss: 0.00001832
Iteration 128/1000 | Loss: 0.00001832
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001832
Iteration 132/1000 | Loss: 0.00001832
Iteration 133/1000 | Loss: 0.00001832
Iteration 134/1000 | Loss: 0.00001832
Iteration 135/1000 | Loss: 0.00001831
Iteration 136/1000 | Loss: 0.00001831
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Iteration 145/1000 | Loss: 0.00001830
Iteration 146/1000 | Loss: 0.00001830
Iteration 147/1000 | Loss: 0.00001830
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001830
Iteration 152/1000 | Loss: 0.00001830
Iteration 153/1000 | Loss: 0.00001830
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001830
Iteration 156/1000 | Loss: 0.00001830
Iteration 157/1000 | Loss: 0.00001830
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001829
Iteration 165/1000 | Loss: 0.00001829
Iteration 166/1000 | Loss: 0.00001829
Iteration 167/1000 | Loss: 0.00001829
Iteration 168/1000 | Loss: 0.00001829
Iteration 169/1000 | Loss: 0.00001829
Iteration 170/1000 | Loss: 0.00001829
Iteration 171/1000 | Loss: 0.00001829
Iteration 172/1000 | Loss: 0.00001829
Iteration 173/1000 | Loss: 0.00001829
Iteration 174/1000 | Loss: 0.00001829
Iteration 175/1000 | Loss: 0.00001829
Iteration 176/1000 | Loss: 0.00001829
Iteration 177/1000 | Loss: 0.00001829
Iteration 178/1000 | Loss: 0.00001829
Iteration 179/1000 | Loss: 0.00001829
Iteration 180/1000 | Loss: 0.00001829
Iteration 181/1000 | Loss: 0.00001829
Iteration 182/1000 | Loss: 0.00001829
Iteration 183/1000 | Loss: 0.00001829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.8294333131052554e-05, 1.8294333131052554e-05, 1.8294333131052554e-05, 1.8294333131052554e-05, 1.8294333131052554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8294333131052554e-05

Optimization complete. Final v2v error: 3.626237154006958 mm

Highest mean error: 4.2267937660217285 mm for frame 34

Lowest mean error: 2.5607659816741943 mm for frame 3

Saving results

Total time: 1254.0771524906158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1014
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026662
Iteration 2/25 | Loss: 0.00176194
Iteration 3/25 | Loss: 0.00153165
Iteration 4/25 | Loss: 0.00119342
Iteration 5/25 | Loss: 0.00113789
Iteration 6/25 | Loss: 0.00113149
Iteration 7/25 | Loss: 0.00112877
Iteration 8/25 | Loss: 0.00112910
Iteration 9/25 | Loss: 0.00112859
Iteration 10/25 | Loss: 0.00112917
Iteration 11/25 | Loss: 0.00113024
Iteration 12/25 | Loss: 0.00112882
Iteration 13/25 | Loss: 0.00112563
Iteration 14/25 | Loss: 0.00112348
Iteration 15/25 | Loss: 0.00112236
Iteration 16/25 | Loss: 0.00112131
Iteration 17/25 | Loss: 0.00112100
Iteration 18/25 | Loss: 0.00112089
Iteration 19/25 | Loss: 0.00112080
Iteration 20/25 | Loss: 0.00112073
Iteration 21/25 | Loss: 0.00112065
Iteration 22/25 | Loss: 0.00112065
Iteration 23/25 | Loss: 0.00112065
Iteration 24/25 | Loss: 0.00112064
Iteration 25/25 | Loss: 0.00112064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01257455
Iteration 2/25 | Loss: 0.00060418
Iteration 3/25 | Loss: 0.00060414
Iteration 4/25 | Loss: 0.00060414
Iteration 5/25 | Loss: 0.00060414
Iteration 6/25 | Loss: 0.00060414
Iteration 7/25 | Loss: 0.00060414
Iteration 8/25 | Loss: 0.00060414
Iteration 9/25 | Loss: 0.00060414
Iteration 10/25 | Loss: 0.00060414
Iteration 11/25 | Loss: 0.00060414
Iteration 12/25 | Loss: 0.00060414
Iteration 13/25 | Loss: 0.00060414
Iteration 14/25 | Loss: 0.00060414
Iteration 15/25 | Loss: 0.00060414
Iteration 16/25 | Loss: 0.00060414
Iteration 17/25 | Loss: 0.00060414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006041419692337513, 0.0006041419692337513, 0.0006041419692337513, 0.0006041419692337513, 0.0006041419692337513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006041419692337513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060414
Iteration 2/1000 | Loss: 0.00005970
Iteration 3/1000 | Loss: 0.00004448
Iteration 4/1000 | Loss: 0.00003967
Iteration 5/1000 | Loss: 0.00003799
Iteration 6/1000 | Loss: 0.00003714
Iteration 7/1000 | Loss: 0.00003625
Iteration 8/1000 | Loss: 0.00003534
Iteration 9/1000 | Loss: 0.00003474
Iteration 10/1000 | Loss: 0.00003435
Iteration 11/1000 | Loss: 0.00003401
Iteration 12/1000 | Loss: 0.00003375
Iteration 13/1000 | Loss: 0.00003352
Iteration 14/1000 | Loss: 0.00003342
Iteration 15/1000 | Loss: 0.00003328
Iteration 16/1000 | Loss: 0.00003317
Iteration 17/1000 | Loss: 0.00003302
Iteration 18/1000 | Loss: 0.00003302
Iteration 19/1000 | Loss: 0.00003291
Iteration 20/1000 | Loss: 0.00003290
Iteration 21/1000 | Loss: 0.00003281
Iteration 22/1000 | Loss: 0.00003276
Iteration 23/1000 | Loss: 0.00003275
Iteration 24/1000 | Loss: 0.00003274
Iteration 25/1000 | Loss: 0.00003273
Iteration 26/1000 | Loss: 0.00003273
Iteration 27/1000 | Loss: 0.00003273
Iteration 28/1000 | Loss: 0.00003273
Iteration 29/1000 | Loss: 0.00003273
Iteration 30/1000 | Loss: 0.00003269
Iteration 31/1000 | Loss: 0.00003269
Iteration 32/1000 | Loss: 0.00003269
Iteration 33/1000 | Loss: 0.00003269
Iteration 34/1000 | Loss: 0.00003269
Iteration 35/1000 | Loss: 0.00003269
Iteration 36/1000 | Loss: 0.00003268
Iteration 37/1000 | Loss: 0.00003265
Iteration 38/1000 | Loss: 0.00003265
Iteration 39/1000 | Loss: 0.00003265
Iteration 40/1000 | Loss: 0.00003265
Iteration 41/1000 | Loss: 0.00003265
Iteration 42/1000 | Loss: 0.00003264
Iteration 43/1000 | Loss: 0.00003264
Iteration 44/1000 | Loss: 0.00003264
Iteration 45/1000 | Loss: 0.00003264
Iteration 46/1000 | Loss: 0.00003264
Iteration 47/1000 | Loss: 0.00003261
Iteration 48/1000 | Loss: 0.00003260
Iteration 49/1000 | Loss: 0.00003260
Iteration 50/1000 | Loss: 0.00003260
Iteration 51/1000 | Loss: 0.00003260
Iteration 52/1000 | Loss: 0.00003260
Iteration 53/1000 | Loss: 0.00003260
Iteration 54/1000 | Loss: 0.00003260
Iteration 55/1000 | Loss: 0.00003260
Iteration 56/1000 | Loss: 0.00003260
Iteration 57/1000 | Loss: 0.00003260
Iteration 58/1000 | Loss: 0.00003258
Iteration 59/1000 | Loss: 0.00003258
Iteration 60/1000 | Loss: 0.00003258
Iteration 61/1000 | Loss: 0.00003258
Iteration 62/1000 | Loss: 0.00003258
Iteration 63/1000 | Loss: 0.00003257
Iteration 64/1000 | Loss: 0.00003257
Iteration 65/1000 | Loss: 0.00003257
Iteration 66/1000 | Loss: 0.00003256
Iteration 67/1000 | Loss: 0.00003256
Iteration 68/1000 | Loss: 0.00003256
Iteration 69/1000 | Loss: 0.00003256
Iteration 70/1000 | Loss: 0.00003255
Iteration 71/1000 | Loss: 0.00003254
Iteration 72/1000 | Loss: 0.00003254
Iteration 73/1000 | Loss: 0.00003254
Iteration 74/1000 | Loss: 0.00003254
Iteration 75/1000 | Loss: 0.00003254
Iteration 76/1000 | Loss: 0.00003254
Iteration 77/1000 | Loss: 0.00003254
Iteration 78/1000 | Loss: 0.00003254
Iteration 79/1000 | Loss: 0.00003254
Iteration 80/1000 | Loss: 0.00003253
Iteration 81/1000 | Loss: 0.00003253
Iteration 82/1000 | Loss: 0.00003253
Iteration 83/1000 | Loss: 0.00003253
Iteration 84/1000 | Loss: 0.00003253
Iteration 85/1000 | Loss: 0.00003252
Iteration 86/1000 | Loss: 0.00003252
Iteration 87/1000 | Loss: 0.00003252
Iteration 88/1000 | Loss: 0.00003252
Iteration 89/1000 | Loss: 0.00003252
Iteration 90/1000 | Loss: 0.00003252
Iteration 91/1000 | Loss: 0.00003252
Iteration 92/1000 | Loss: 0.00003252
Iteration 93/1000 | Loss: 0.00003252
Iteration 94/1000 | Loss: 0.00003252
Iteration 95/1000 | Loss: 0.00003252
Iteration 96/1000 | Loss: 0.00003252
Iteration 97/1000 | Loss: 0.00003251
Iteration 98/1000 | Loss: 0.00003251
Iteration 99/1000 | Loss: 0.00003251
Iteration 100/1000 | Loss: 0.00003251
Iteration 101/1000 | Loss: 0.00003251
Iteration 102/1000 | Loss: 0.00003251
Iteration 103/1000 | Loss: 0.00003251
Iteration 104/1000 | Loss: 0.00003251
Iteration 105/1000 | Loss: 0.00003251
Iteration 106/1000 | Loss: 0.00003251
Iteration 107/1000 | Loss: 0.00003251
Iteration 108/1000 | Loss: 0.00003250
Iteration 109/1000 | Loss: 0.00003250
Iteration 110/1000 | Loss: 0.00003250
Iteration 111/1000 | Loss: 0.00003250
Iteration 112/1000 | Loss: 0.00003250
Iteration 113/1000 | Loss: 0.00003250
Iteration 114/1000 | Loss: 0.00003249
Iteration 115/1000 | Loss: 0.00003249
Iteration 116/1000 | Loss: 0.00003249
Iteration 117/1000 | Loss: 0.00003249
Iteration 118/1000 | Loss: 0.00003249
Iteration 119/1000 | Loss: 0.00003249
Iteration 120/1000 | Loss: 0.00003248
Iteration 121/1000 | Loss: 0.00003248
Iteration 122/1000 | Loss: 0.00003248
Iteration 123/1000 | Loss: 0.00003247
Iteration 124/1000 | Loss: 0.00003247
Iteration 125/1000 | Loss: 0.00003247
Iteration 126/1000 | Loss: 0.00003247
Iteration 127/1000 | Loss: 0.00003246
Iteration 128/1000 | Loss: 0.00003246
Iteration 129/1000 | Loss: 0.00003246
Iteration 130/1000 | Loss: 0.00003246
Iteration 131/1000 | Loss: 0.00003246
Iteration 132/1000 | Loss: 0.00003246
Iteration 133/1000 | Loss: 0.00003246
Iteration 134/1000 | Loss: 0.00003246
Iteration 135/1000 | Loss: 0.00003246
Iteration 136/1000 | Loss: 0.00003246
Iteration 137/1000 | Loss: 0.00003246
Iteration 138/1000 | Loss: 0.00003246
Iteration 139/1000 | Loss: 0.00003246
Iteration 140/1000 | Loss: 0.00003245
Iteration 141/1000 | Loss: 0.00003245
Iteration 142/1000 | Loss: 0.00003245
Iteration 143/1000 | Loss: 0.00003245
Iteration 144/1000 | Loss: 0.00003245
Iteration 145/1000 | Loss: 0.00003245
Iteration 146/1000 | Loss: 0.00003245
Iteration 147/1000 | Loss: 0.00003245
Iteration 148/1000 | Loss: 0.00003245
Iteration 149/1000 | Loss: 0.00003244
Iteration 150/1000 | Loss: 0.00003244
Iteration 151/1000 | Loss: 0.00003244
Iteration 152/1000 | Loss: 0.00003243
Iteration 153/1000 | Loss: 0.00003243
Iteration 154/1000 | Loss: 0.00003243
Iteration 155/1000 | Loss: 0.00003243
Iteration 156/1000 | Loss: 0.00003242
Iteration 157/1000 | Loss: 0.00003242
Iteration 158/1000 | Loss: 0.00003242
Iteration 159/1000 | Loss: 0.00003242
Iteration 160/1000 | Loss: 0.00003242
Iteration 161/1000 | Loss: 0.00003242
Iteration 162/1000 | Loss: 0.00003242
Iteration 163/1000 | Loss: 0.00003242
Iteration 164/1000 | Loss: 0.00003241
Iteration 165/1000 | Loss: 0.00003241
Iteration 166/1000 | Loss: 0.00003241
Iteration 167/1000 | Loss: 0.00003241
Iteration 168/1000 | Loss: 0.00003241
Iteration 169/1000 | Loss: 0.00003241
Iteration 170/1000 | Loss: 0.00003241
Iteration 171/1000 | Loss: 0.00003241
Iteration 172/1000 | Loss: 0.00003240
Iteration 173/1000 | Loss: 0.00003240
Iteration 174/1000 | Loss: 0.00003240
Iteration 175/1000 | Loss: 0.00003240
Iteration 176/1000 | Loss: 0.00003240
Iteration 177/1000 | Loss: 0.00003240
Iteration 178/1000 | Loss: 0.00003239
Iteration 179/1000 | Loss: 0.00003239
Iteration 180/1000 | Loss: 0.00003239
Iteration 181/1000 | Loss: 0.00003239
Iteration 182/1000 | Loss: 0.00003239
Iteration 183/1000 | Loss: 0.00003238
Iteration 184/1000 | Loss: 0.00003238
Iteration 185/1000 | Loss: 0.00003238
Iteration 186/1000 | Loss: 0.00003238
Iteration 187/1000 | Loss: 0.00003238
Iteration 188/1000 | Loss: 0.00003238
Iteration 189/1000 | Loss: 0.00003238
Iteration 190/1000 | Loss: 0.00003238
Iteration 191/1000 | Loss: 0.00003238
Iteration 192/1000 | Loss: 0.00003238
Iteration 193/1000 | Loss: 0.00003237
Iteration 194/1000 | Loss: 0.00003237
Iteration 195/1000 | Loss: 0.00003237
Iteration 196/1000 | Loss: 0.00003237
Iteration 197/1000 | Loss: 0.00003237
Iteration 198/1000 | Loss: 0.00003237
Iteration 199/1000 | Loss: 0.00003237
Iteration 200/1000 | Loss: 0.00003237
Iteration 201/1000 | Loss: 0.00003237
Iteration 202/1000 | Loss: 0.00003237
Iteration 203/1000 | Loss: 0.00003236
Iteration 204/1000 | Loss: 0.00003236
Iteration 205/1000 | Loss: 0.00003236
Iteration 206/1000 | Loss: 0.00003236
Iteration 207/1000 | Loss: 0.00003236
Iteration 208/1000 | Loss: 0.00003236
Iteration 209/1000 | Loss: 0.00003236
Iteration 210/1000 | Loss: 0.00003236
Iteration 211/1000 | Loss: 0.00003236
Iteration 212/1000 | Loss: 0.00003236
Iteration 213/1000 | Loss: 0.00003235
Iteration 214/1000 | Loss: 0.00003235
Iteration 215/1000 | Loss: 0.00003235
Iteration 216/1000 | Loss: 0.00003235
Iteration 217/1000 | Loss: 0.00003235
Iteration 218/1000 | Loss: 0.00003235
Iteration 219/1000 | Loss: 0.00003235
Iteration 220/1000 | Loss: 0.00003235
Iteration 221/1000 | Loss: 0.00003235
Iteration 222/1000 | Loss: 0.00003235
Iteration 223/1000 | Loss: 0.00003235
Iteration 224/1000 | Loss: 0.00003235
Iteration 225/1000 | Loss: 0.00003235
Iteration 226/1000 | Loss: 0.00003235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [3.2349104003515095e-05, 3.2349104003515095e-05, 3.2349104003515095e-05, 3.2349104003515095e-05, 3.2349104003515095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2349104003515095e-05

Optimization complete. Final v2v error: 4.600391387939453 mm

Highest mean error: 5.783638954162598 mm for frame 162

Lowest mean error: 3.5669822692871094 mm for frame 218

Saving results

Total time: 2679.397185087204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1082
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549395
Iteration 2/25 | Loss: 0.00129649
Iteration 3/25 | Loss: 0.00101641
Iteration 4/25 | Loss: 0.00098537
Iteration 5/25 | Loss: 0.00097862
Iteration 6/25 | Loss: 0.00098194
Iteration 7/25 | Loss: 0.00097842
Iteration 8/25 | Loss: 0.00097639
Iteration 9/25 | Loss: 0.00097414
Iteration 10/25 | Loss: 0.00097341
Iteration 11/25 | Loss: 0.00097745
Iteration 12/25 | Loss: 0.00097570
Iteration 13/25 | Loss: 0.00097447
Iteration 14/25 | Loss: 0.00097107
Iteration 15/25 | Loss: 0.00097069
Iteration 16/25 | Loss: 0.00097058
Iteration 17/25 | Loss: 0.00097058
Iteration 18/25 | Loss: 0.00097058
Iteration 19/25 | Loss: 0.00097058
Iteration 20/25 | Loss: 0.00097058
Iteration 21/25 | Loss: 0.00097058
Iteration 22/25 | Loss: 0.00097058
Iteration 23/25 | Loss: 0.00097058
Iteration 24/25 | Loss: 0.00097058
Iteration 25/25 | Loss: 0.00097057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61418259
Iteration 2/25 | Loss: 0.00068824
Iteration 3/25 | Loss: 0.00068824
Iteration 4/25 | Loss: 0.00068823
Iteration 5/25 | Loss: 0.00068823
Iteration 6/25 | Loss: 0.00068823
Iteration 7/25 | Loss: 0.00068823
Iteration 8/25 | Loss: 0.00068823
Iteration 9/25 | Loss: 0.00068823
Iteration 10/25 | Loss: 0.00068823
Iteration 11/25 | Loss: 0.00068823
Iteration 12/25 | Loss: 0.00068823
Iteration 13/25 | Loss: 0.00068823
Iteration 14/25 | Loss: 0.00068823
Iteration 15/25 | Loss: 0.00068823
Iteration 16/25 | Loss: 0.00068823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006882321904413402, 0.0006882321904413402, 0.0006882321904413402, 0.0006882321904413402, 0.0006882321904413402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006882321904413402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068823
Iteration 2/1000 | Loss: 0.00022439
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001316
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001096
Iteration 15/1000 | Loss: 0.00001093
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001082
Iteration 18/1000 | Loss: 0.00001082
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001072
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001063
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001059
Iteration 38/1000 | Loss: 0.00001059
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001059
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001059
Iteration 44/1000 | Loss: 0.00001059
Iteration 45/1000 | Loss: 0.00001059
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001057
Iteration 48/1000 | Loss: 0.00001057
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001056
Iteration 51/1000 | Loss: 0.00001055
Iteration 52/1000 | Loss: 0.00001055
Iteration 53/1000 | Loss: 0.00001055
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001052
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001051
Iteration 64/1000 | Loss: 0.00001051
Iteration 65/1000 | Loss: 0.00001051
Iteration 66/1000 | Loss: 0.00001051
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001050
Iteration 70/1000 | Loss: 0.00001049
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001046
Iteration 74/1000 | Loss: 0.00001046
Iteration 75/1000 | Loss: 0.00001046
Iteration 76/1000 | Loss: 0.00001046
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001045
Iteration 86/1000 | Loss: 0.00001045
Iteration 87/1000 | Loss: 0.00001045
Iteration 88/1000 | Loss: 0.00001045
Iteration 89/1000 | Loss: 0.00001045
Iteration 90/1000 | Loss: 0.00001045
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001045
Iteration 93/1000 | Loss: 0.00001045
Iteration 94/1000 | Loss: 0.00001045
Iteration 95/1000 | Loss: 0.00001045
Iteration 96/1000 | Loss: 0.00001045
Iteration 97/1000 | Loss: 0.00001045
Iteration 98/1000 | Loss: 0.00001045
Iteration 99/1000 | Loss: 0.00001045
Iteration 100/1000 | Loss: 0.00001045
Iteration 101/1000 | Loss: 0.00001045
Iteration 102/1000 | Loss: 0.00001044
Iteration 103/1000 | Loss: 0.00001044
Iteration 104/1000 | Loss: 0.00001044
Iteration 105/1000 | Loss: 0.00001044
Iteration 106/1000 | Loss: 0.00001044
Iteration 107/1000 | Loss: 0.00001044
Iteration 108/1000 | Loss: 0.00001044
Iteration 109/1000 | Loss: 0.00001044
Iteration 110/1000 | Loss: 0.00001044
Iteration 111/1000 | Loss: 0.00001044
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001043
Iteration 114/1000 | Loss: 0.00001043
Iteration 115/1000 | Loss: 0.00001043
Iteration 116/1000 | Loss: 0.00001043
Iteration 117/1000 | Loss: 0.00001043
Iteration 118/1000 | Loss: 0.00001043
Iteration 119/1000 | Loss: 0.00001043
Iteration 120/1000 | Loss: 0.00001043
Iteration 121/1000 | Loss: 0.00001042
Iteration 122/1000 | Loss: 0.00001042
Iteration 123/1000 | Loss: 0.00001042
Iteration 124/1000 | Loss: 0.00001042
Iteration 125/1000 | Loss: 0.00001042
Iteration 126/1000 | Loss: 0.00001042
Iteration 127/1000 | Loss: 0.00001042
Iteration 128/1000 | Loss: 0.00001042
Iteration 129/1000 | Loss: 0.00001042
Iteration 130/1000 | Loss: 0.00001042
Iteration 131/1000 | Loss: 0.00001042
Iteration 132/1000 | Loss: 0.00001042
Iteration 133/1000 | Loss: 0.00001042
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001042
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001042
Iteration 142/1000 | Loss: 0.00001042
Iteration 143/1000 | Loss: 0.00001042
Iteration 144/1000 | Loss: 0.00001042
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001042
Iteration 147/1000 | Loss: 0.00001042
Iteration 148/1000 | Loss: 0.00001042
Iteration 149/1000 | Loss: 0.00001042
Iteration 150/1000 | Loss: 0.00001042
Iteration 151/1000 | Loss: 0.00001042
Iteration 152/1000 | Loss: 0.00001042
Iteration 153/1000 | Loss: 0.00001042
Iteration 154/1000 | Loss: 0.00001042
Iteration 155/1000 | Loss: 0.00001042
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.0416114491818007e-05, 1.0416114491818007e-05, 1.0416114491818007e-05, 1.0416114491818007e-05, 1.0416114491818007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0416114491818007e-05

Optimization complete. Final v2v error: 2.754650592803955 mm

Highest mean error: 4.274398326873779 mm for frame 130

Lowest mean error: 2.3424675464630127 mm for frame 29

Saving results

Total time: 2281.983597278595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1063
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024565
Iteration 2/25 | Loss: 0.01024565
Iteration 3/25 | Loss: 0.00423418
Iteration 4/25 | Loss: 0.00253297
Iteration 5/25 | Loss: 0.00193407
Iteration 6/25 | Loss: 0.00155943
Iteration 7/25 | Loss: 0.00141818
Iteration 8/25 | Loss: 0.00132205
Iteration 9/25 | Loss: 0.00131697
Iteration 10/25 | Loss: 0.00126569
Iteration 11/25 | Loss: 0.00124906
Iteration 12/25 | Loss: 0.00122119
Iteration 13/25 | Loss: 0.00120088
Iteration 14/25 | Loss: 0.00118587
Iteration 15/25 | Loss: 0.00119362
Iteration 16/25 | Loss: 0.00118685
Iteration 17/25 | Loss: 0.00118256
Iteration 18/25 | Loss: 0.00118590
Iteration 19/25 | Loss: 0.00118364
Iteration 20/25 | Loss: 0.00117611
Iteration 21/25 | Loss: 0.00117704
Iteration 22/25 | Loss: 0.00117538
Iteration 23/25 | Loss: 0.00117467
Iteration 24/25 | Loss: 0.00116959
Iteration 25/25 | Loss: 0.00116931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29635525
Iteration 2/25 | Loss: 0.00173913
Iteration 3/25 | Loss: 0.00154840
Iteration 4/25 | Loss: 0.00154840
Iteration 5/25 | Loss: 0.00154840
Iteration 6/25 | Loss: 0.00154840
Iteration 7/25 | Loss: 0.00154840
Iteration 8/25 | Loss: 0.00154840
Iteration 9/25 | Loss: 0.00154840
Iteration 10/25 | Loss: 0.00154840
Iteration 11/25 | Loss: 0.00154840
Iteration 12/25 | Loss: 0.00154840
Iteration 13/25 | Loss: 0.00154840
Iteration 14/25 | Loss: 0.00154840
Iteration 15/25 | Loss: 0.00154840
Iteration 16/25 | Loss: 0.00154840
Iteration 17/25 | Loss: 0.00154840
Iteration 18/25 | Loss: 0.00154840
Iteration 19/25 | Loss: 0.00154840
Iteration 20/25 | Loss: 0.00154840
Iteration 21/25 | Loss: 0.00154840
Iteration 22/25 | Loss: 0.00154840
Iteration 23/25 | Loss: 0.00154840
Iteration 24/25 | Loss: 0.00154840
Iteration 25/25 | Loss: 0.00154840

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154840
Iteration 2/1000 | Loss: 0.00048920
Iteration 3/1000 | Loss: 0.00030158
Iteration 4/1000 | Loss: 0.00022576
Iteration 5/1000 | Loss: 0.00024332
Iteration 6/1000 | Loss: 0.00038666
Iteration 7/1000 | Loss: 0.00021602
Iteration 8/1000 | Loss: 0.00013481
Iteration 9/1000 | Loss: 0.00020526
Iteration 10/1000 | Loss: 0.00034183
Iteration 11/1000 | Loss: 0.00012668
Iteration 12/1000 | Loss: 0.00010778
Iteration 13/1000 | Loss: 0.00009991
Iteration 14/1000 | Loss: 0.00009434
Iteration 15/1000 | Loss: 0.00059534
Iteration 16/1000 | Loss: 0.00219900
Iteration 17/1000 | Loss: 0.00159033
Iteration 18/1000 | Loss: 0.00036678
Iteration 19/1000 | Loss: 0.00033743
Iteration 20/1000 | Loss: 0.00017237
Iteration 21/1000 | Loss: 0.00034947
Iteration 22/1000 | Loss: 0.00010094
Iteration 23/1000 | Loss: 0.00011153
Iteration 24/1000 | Loss: 0.00006637
Iteration 25/1000 | Loss: 0.00007469
Iteration 26/1000 | Loss: 0.00005363
Iteration 27/1000 | Loss: 0.00024284
Iteration 28/1000 | Loss: 0.00003718
Iteration 29/1000 | Loss: 0.00003476
Iteration 30/1000 | Loss: 0.00003257
Iteration 31/1000 | Loss: 0.00003155
Iteration 32/1000 | Loss: 0.00003022
Iteration 33/1000 | Loss: 0.00002946
Iteration 34/1000 | Loss: 0.00002871
Iteration 35/1000 | Loss: 0.00002806
Iteration 36/1000 | Loss: 0.00002772
Iteration 37/1000 | Loss: 0.00002749
Iteration 38/1000 | Loss: 0.00002726
Iteration 39/1000 | Loss: 0.00002725
Iteration 40/1000 | Loss: 0.00002708
Iteration 41/1000 | Loss: 0.00002708
Iteration 42/1000 | Loss: 0.00002703
Iteration 43/1000 | Loss: 0.00002695
Iteration 44/1000 | Loss: 0.00002691
Iteration 45/1000 | Loss: 0.00002691
Iteration 46/1000 | Loss: 0.00002686
Iteration 47/1000 | Loss: 0.00002686
Iteration 48/1000 | Loss: 0.00002686
Iteration 49/1000 | Loss: 0.00002686
Iteration 50/1000 | Loss: 0.00002685
Iteration 51/1000 | Loss: 0.00002685
Iteration 52/1000 | Loss: 0.00002684
Iteration 53/1000 | Loss: 0.00002684
Iteration 54/1000 | Loss: 0.00002684
Iteration 55/1000 | Loss: 0.00002684
Iteration 56/1000 | Loss: 0.00002683
Iteration 57/1000 | Loss: 0.00002683
Iteration 58/1000 | Loss: 0.00002683
Iteration 59/1000 | Loss: 0.00002683
Iteration 60/1000 | Loss: 0.00002683
Iteration 61/1000 | Loss: 0.00002683
Iteration 62/1000 | Loss: 0.00002682
Iteration 63/1000 | Loss: 0.00002682
Iteration 64/1000 | Loss: 0.00002682
Iteration 65/1000 | Loss: 0.00002681
Iteration 66/1000 | Loss: 0.00002681
Iteration 67/1000 | Loss: 0.00002681
Iteration 68/1000 | Loss: 0.00002680
Iteration 69/1000 | Loss: 0.00002680
Iteration 70/1000 | Loss: 0.00002680
Iteration 71/1000 | Loss: 0.00002680
Iteration 72/1000 | Loss: 0.00002680
Iteration 73/1000 | Loss: 0.00002680
Iteration 74/1000 | Loss: 0.00002679
Iteration 75/1000 | Loss: 0.00002679
Iteration 76/1000 | Loss: 0.00002679
Iteration 77/1000 | Loss: 0.00002679
Iteration 78/1000 | Loss: 0.00002679
Iteration 79/1000 | Loss: 0.00002679
Iteration 80/1000 | Loss: 0.00002678
Iteration 81/1000 | Loss: 0.00002678
Iteration 82/1000 | Loss: 0.00002678
Iteration 83/1000 | Loss: 0.00002678
Iteration 84/1000 | Loss: 0.00002678
Iteration 85/1000 | Loss: 0.00002677
Iteration 86/1000 | Loss: 0.00002677
Iteration 87/1000 | Loss: 0.00002677
Iteration 88/1000 | Loss: 0.00002677
Iteration 89/1000 | Loss: 0.00002677
Iteration 90/1000 | Loss: 0.00002677
Iteration 91/1000 | Loss: 0.00002677
Iteration 92/1000 | Loss: 0.00002676
Iteration 93/1000 | Loss: 0.00002676
Iteration 94/1000 | Loss: 0.00002676
Iteration 95/1000 | Loss: 0.00002676
Iteration 96/1000 | Loss: 0.00002676
Iteration 97/1000 | Loss: 0.00002676
Iteration 98/1000 | Loss: 0.00002676
Iteration 99/1000 | Loss: 0.00002676
Iteration 100/1000 | Loss: 0.00002676
Iteration 101/1000 | Loss: 0.00002676
Iteration 102/1000 | Loss: 0.00002676
Iteration 103/1000 | Loss: 0.00002676
Iteration 104/1000 | Loss: 0.00002675
Iteration 105/1000 | Loss: 0.00002675
Iteration 106/1000 | Loss: 0.00002675
Iteration 107/1000 | Loss: 0.00002675
Iteration 108/1000 | Loss: 0.00002675
Iteration 109/1000 | Loss: 0.00002675
Iteration 110/1000 | Loss: 0.00002675
Iteration 111/1000 | Loss: 0.00002675
Iteration 112/1000 | Loss: 0.00002675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.6754776627058163e-05, 2.6754776627058163e-05, 2.6754776627058163e-05, 2.6754776627058163e-05, 2.6754776627058163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6754776627058163e-05

Optimization complete. Final v2v error: 3.2137644290924072 mm

Highest mean error: 21.82362174987793 mm for frame 172

Lowest mean error: 2.682295799255371 mm for frame 189

Saving results

Total time: 4018.193238258362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1076
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621150
Iteration 2/25 | Loss: 0.00108348
Iteration 3/25 | Loss: 0.00096519
Iteration 4/25 | Loss: 0.00094560
Iteration 5/25 | Loss: 0.00093703
Iteration 6/25 | Loss: 0.00093441
Iteration 7/25 | Loss: 0.00093396
Iteration 8/25 | Loss: 0.00093396
Iteration 9/25 | Loss: 0.00093396
Iteration 10/25 | Loss: 0.00093396
Iteration 11/25 | Loss: 0.00093396
Iteration 12/25 | Loss: 0.00093396
Iteration 13/25 | Loss: 0.00093396
Iteration 14/25 | Loss: 0.00093396
Iteration 15/25 | Loss: 0.00093396
Iteration 16/25 | Loss: 0.00093396
Iteration 17/25 | Loss: 0.00093396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009339579846709967, 0.0009339579846709967, 0.0009339579846709967, 0.0009339579846709967, 0.0009339579846709967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009339579846709967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45527720
Iteration 2/25 | Loss: 0.00070148
Iteration 3/25 | Loss: 0.00070148
Iteration 4/25 | Loss: 0.00070148
Iteration 5/25 | Loss: 0.00070148
Iteration 6/25 | Loss: 0.00070148
Iteration 7/25 | Loss: 0.00070148
Iteration 8/25 | Loss: 0.00070148
Iteration 9/25 | Loss: 0.00070148
Iteration 10/25 | Loss: 0.00070148
Iteration 11/25 | Loss: 0.00070148
Iteration 12/25 | Loss: 0.00070148
Iteration 13/25 | Loss: 0.00070148
Iteration 14/25 | Loss: 0.00070148
Iteration 15/25 | Loss: 0.00070148
Iteration 16/25 | Loss: 0.00070148
Iteration 17/25 | Loss: 0.00070148
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007014753646217287, 0.0007014753646217287, 0.0007014753646217287, 0.0007014753646217287, 0.0007014753646217287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007014753646217287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070148
Iteration 2/1000 | Loss: 0.00001892
Iteration 3/1000 | Loss: 0.00001334
Iteration 4/1000 | Loss: 0.00001245
Iteration 5/1000 | Loss: 0.00001198
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001107
Iteration 10/1000 | Loss: 0.00001100
Iteration 11/1000 | Loss: 0.00001088
Iteration 12/1000 | Loss: 0.00001080
Iteration 13/1000 | Loss: 0.00001080
Iteration 14/1000 | Loss: 0.00001079
Iteration 15/1000 | Loss: 0.00001078
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001074
Iteration 20/1000 | Loss: 0.00001070
Iteration 21/1000 | Loss: 0.00001070
Iteration 22/1000 | Loss: 0.00001069
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001069
Iteration 26/1000 | Loss: 0.00001068
Iteration 27/1000 | Loss: 0.00001067
Iteration 28/1000 | Loss: 0.00001067
Iteration 29/1000 | Loss: 0.00001067
Iteration 30/1000 | Loss: 0.00001067
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001066
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001066
Iteration 35/1000 | Loss: 0.00001065
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001065
Iteration 38/1000 | Loss: 0.00001065
Iteration 39/1000 | Loss: 0.00001065
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001065
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001064
Iteration 49/1000 | Loss: 0.00001064
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001064
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001063
Iteration 56/1000 | Loss: 0.00001063
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001063
Iteration 63/1000 | Loss: 0.00001063
Iteration 64/1000 | Loss: 0.00001063
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001062
Iteration 85/1000 | Loss: 0.00001062
Iteration 86/1000 | Loss: 0.00001062
Iteration 87/1000 | Loss: 0.00001062
Iteration 88/1000 | Loss: 0.00001062
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001062
Iteration 93/1000 | Loss: 0.00001062
Iteration 94/1000 | Loss: 0.00001062
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.0618705346132629e-05, 1.0618705346132629e-05, 1.0618705346132629e-05, 1.0618705346132629e-05, 1.0618705346132629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0618705346132629e-05

Optimization complete. Final v2v error: 2.7827322483062744 mm

Highest mean error: 3.2114949226379395 mm for frame 89

Lowest mean error: 2.5393364429473877 mm for frame 27

Saving results

Total time: 705.5953433513641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1031
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380481
Iteration 2/25 | Loss: 0.00103248
Iteration 3/25 | Loss: 0.00092934
Iteration 4/25 | Loss: 0.00091885
Iteration 5/25 | Loss: 0.00091565
Iteration 6/25 | Loss: 0.00091469
Iteration 7/25 | Loss: 0.00091469
Iteration 8/25 | Loss: 0.00091469
Iteration 9/25 | Loss: 0.00091469
Iteration 10/25 | Loss: 0.00091469
Iteration 11/25 | Loss: 0.00091469
Iteration 12/25 | Loss: 0.00091469
Iteration 13/25 | Loss: 0.00091469
Iteration 14/25 | Loss: 0.00091469
Iteration 15/25 | Loss: 0.00091469
Iteration 16/25 | Loss: 0.00091469
Iteration 17/25 | Loss: 0.00091469
Iteration 18/25 | Loss: 0.00091469
Iteration 19/25 | Loss: 0.00091469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009146906668320298, 0.0009146906668320298, 0.0009146906668320298, 0.0009146906668320298, 0.0009146906668320298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009146906668320298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34888637
Iteration 2/25 | Loss: 0.00065001
Iteration 3/25 | Loss: 0.00065001
Iteration 4/25 | Loss: 0.00065001
Iteration 5/25 | Loss: 0.00065001
Iteration 6/25 | Loss: 0.00065001
Iteration 7/25 | Loss: 0.00065001
Iteration 8/25 | Loss: 0.00065001
Iteration 9/25 | Loss: 0.00065001
Iteration 10/25 | Loss: 0.00065001
Iteration 11/25 | Loss: 0.00065001
Iteration 12/25 | Loss: 0.00065001
Iteration 13/25 | Loss: 0.00065001
Iteration 14/25 | Loss: 0.00065001
Iteration 15/25 | Loss: 0.00065001
Iteration 16/25 | Loss: 0.00065001
Iteration 17/25 | Loss: 0.00065001
Iteration 18/25 | Loss: 0.00065001
Iteration 19/25 | Loss: 0.00065001
Iteration 20/25 | Loss: 0.00065001
Iteration 21/25 | Loss: 0.00065001
Iteration 22/25 | Loss: 0.00065001
Iteration 23/25 | Loss: 0.00065001
Iteration 24/25 | Loss: 0.00065001
Iteration 25/25 | Loss: 0.00065001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065001
Iteration 2/1000 | Loss: 0.00001752
Iteration 3/1000 | Loss: 0.00001223
Iteration 4/1000 | Loss: 0.00001121
Iteration 5/1000 | Loss: 0.00001087
Iteration 6/1000 | Loss: 0.00001050
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00001013
Iteration 9/1000 | Loss: 0.00001013
Iteration 10/1000 | Loss: 0.00001011
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00000999
Iteration 13/1000 | Loss: 0.00000996
Iteration 14/1000 | Loss: 0.00000989
Iteration 15/1000 | Loss: 0.00000984
Iteration 16/1000 | Loss: 0.00000982
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000975
Iteration 19/1000 | Loss: 0.00000974
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000971
Iteration 22/1000 | Loss: 0.00000970
Iteration 23/1000 | Loss: 0.00000970
Iteration 24/1000 | Loss: 0.00000970
Iteration 25/1000 | Loss: 0.00000970
Iteration 26/1000 | Loss: 0.00000970
Iteration 27/1000 | Loss: 0.00000970
Iteration 28/1000 | Loss: 0.00000969
Iteration 29/1000 | Loss: 0.00000969
Iteration 30/1000 | Loss: 0.00000969
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000967
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000964
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000964
Iteration 42/1000 | Loss: 0.00000963
Iteration 43/1000 | Loss: 0.00000963
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000960
Iteration 46/1000 | Loss: 0.00000960
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000960
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000958
Iteration 53/1000 | Loss: 0.00000958
Iteration 54/1000 | Loss: 0.00000958
Iteration 55/1000 | Loss: 0.00000958
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000957
Iteration 58/1000 | Loss: 0.00000957
Iteration 59/1000 | Loss: 0.00000957
Iteration 60/1000 | Loss: 0.00000957
Iteration 61/1000 | Loss: 0.00000957
Iteration 62/1000 | Loss: 0.00000957
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000956
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000956
Iteration 67/1000 | Loss: 0.00000956
Iteration 68/1000 | Loss: 0.00000955
Iteration 69/1000 | Loss: 0.00000955
Iteration 70/1000 | Loss: 0.00000955
Iteration 71/1000 | Loss: 0.00000955
Iteration 72/1000 | Loss: 0.00000955
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000954
Iteration 75/1000 | Loss: 0.00000954
Iteration 76/1000 | Loss: 0.00000954
Iteration 77/1000 | Loss: 0.00000953
Iteration 78/1000 | Loss: 0.00000953
Iteration 79/1000 | Loss: 0.00000952
Iteration 80/1000 | Loss: 0.00000952
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000952
Iteration 83/1000 | Loss: 0.00000952
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000951
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000951
Iteration 97/1000 | Loss: 0.00000951
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000950
Iteration 100/1000 | Loss: 0.00000950
Iteration 101/1000 | Loss: 0.00000950
Iteration 102/1000 | Loss: 0.00000950
Iteration 103/1000 | Loss: 0.00000950
Iteration 104/1000 | Loss: 0.00000950
Iteration 105/1000 | Loss: 0.00000950
Iteration 106/1000 | Loss: 0.00000950
Iteration 107/1000 | Loss: 0.00000950
Iteration 108/1000 | Loss: 0.00000950
Iteration 109/1000 | Loss: 0.00000950
Iteration 110/1000 | Loss: 0.00000950
Iteration 111/1000 | Loss: 0.00000949
Iteration 112/1000 | Loss: 0.00000949
Iteration 113/1000 | Loss: 0.00000949
Iteration 114/1000 | Loss: 0.00000949
Iteration 115/1000 | Loss: 0.00000949
Iteration 116/1000 | Loss: 0.00000949
Iteration 117/1000 | Loss: 0.00000949
Iteration 118/1000 | Loss: 0.00000949
Iteration 119/1000 | Loss: 0.00000949
Iteration 120/1000 | Loss: 0.00000949
Iteration 121/1000 | Loss: 0.00000949
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000946
Iteration 133/1000 | Loss: 0.00000946
Iteration 134/1000 | Loss: 0.00000946
Iteration 135/1000 | Loss: 0.00000946
Iteration 136/1000 | Loss: 0.00000946
Iteration 137/1000 | Loss: 0.00000946
Iteration 138/1000 | Loss: 0.00000946
Iteration 139/1000 | Loss: 0.00000945
Iteration 140/1000 | Loss: 0.00000945
Iteration 141/1000 | Loss: 0.00000945
Iteration 142/1000 | Loss: 0.00000945
Iteration 143/1000 | Loss: 0.00000945
Iteration 144/1000 | Loss: 0.00000945
Iteration 145/1000 | Loss: 0.00000945
Iteration 146/1000 | Loss: 0.00000944
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000944
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000944
Iteration 152/1000 | Loss: 0.00000944
Iteration 153/1000 | Loss: 0.00000944
Iteration 154/1000 | Loss: 0.00000944
Iteration 155/1000 | Loss: 0.00000944
Iteration 156/1000 | Loss: 0.00000944
Iteration 157/1000 | Loss: 0.00000944
Iteration 158/1000 | Loss: 0.00000943
Iteration 159/1000 | Loss: 0.00000943
Iteration 160/1000 | Loss: 0.00000943
Iteration 161/1000 | Loss: 0.00000943
Iteration 162/1000 | Loss: 0.00000943
Iteration 163/1000 | Loss: 0.00000943
Iteration 164/1000 | Loss: 0.00000943
Iteration 165/1000 | Loss: 0.00000943
Iteration 166/1000 | Loss: 0.00000943
Iteration 167/1000 | Loss: 0.00000943
Iteration 168/1000 | Loss: 0.00000942
Iteration 169/1000 | Loss: 0.00000942
Iteration 170/1000 | Loss: 0.00000942
Iteration 171/1000 | Loss: 0.00000942
Iteration 172/1000 | Loss: 0.00000942
Iteration 173/1000 | Loss: 0.00000942
Iteration 174/1000 | Loss: 0.00000942
Iteration 175/1000 | Loss: 0.00000942
Iteration 176/1000 | Loss: 0.00000942
Iteration 177/1000 | Loss: 0.00000942
Iteration 178/1000 | Loss: 0.00000942
Iteration 179/1000 | Loss: 0.00000942
Iteration 180/1000 | Loss: 0.00000942
Iteration 181/1000 | Loss: 0.00000942
Iteration 182/1000 | Loss: 0.00000942
Iteration 183/1000 | Loss: 0.00000942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.416633474756964e-06, 9.416633474756964e-06, 9.416633474756964e-06, 9.416633474756964e-06, 9.416633474756964e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.416633474756964e-06

Optimization complete. Final v2v error: 2.6035473346710205 mm

Highest mean error: 2.8018581867218018 mm for frame 8

Lowest mean error: 2.4099538326263428 mm for frame 41

Saving results

Total time: 892.7719578742981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1068
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622197
Iteration 2/25 | Loss: 0.00101879
Iteration 3/25 | Loss: 0.00092187
Iteration 4/25 | Loss: 0.00090964
Iteration 5/25 | Loss: 0.00090515
Iteration 6/25 | Loss: 0.00090419
Iteration 7/25 | Loss: 0.00090419
Iteration 8/25 | Loss: 0.00090419
Iteration 9/25 | Loss: 0.00090419
Iteration 10/25 | Loss: 0.00090419
Iteration 11/25 | Loss: 0.00090419
Iteration 12/25 | Loss: 0.00090419
Iteration 13/25 | Loss: 0.00090419
Iteration 14/25 | Loss: 0.00090419
Iteration 15/25 | Loss: 0.00090419
Iteration 16/25 | Loss: 0.00090419
Iteration 17/25 | Loss: 0.00090419
Iteration 18/25 | Loss: 0.00090419
Iteration 19/25 | Loss: 0.00090419
Iteration 20/25 | Loss: 0.00090419
Iteration 21/25 | Loss: 0.00090419
Iteration 22/25 | Loss: 0.00090419
Iteration 23/25 | Loss: 0.00090419
Iteration 24/25 | Loss: 0.00090419
Iteration 25/25 | Loss: 0.00090419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89221740
Iteration 2/25 | Loss: 0.00061206
Iteration 3/25 | Loss: 0.00061206
Iteration 4/25 | Loss: 0.00061206
Iteration 5/25 | Loss: 0.00061206
Iteration 6/25 | Loss: 0.00061206
Iteration 7/25 | Loss: 0.00061206
Iteration 8/25 | Loss: 0.00061206
Iteration 9/25 | Loss: 0.00061206
Iteration 10/25 | Loss: 0.00061206
Iteration 11/25 | Loss: 0.00061206
Iteration 12/25 | Loss: 0.00061206
Iteration 13/25 | Loss: 0.00061206
Iteration 14/25 | Loss: 0.00061206
Iteration 15/25 | Loss: 0.00061206
Iteration 16/25 | Loss: 0.00061206
Iteration 17/25 | Loss: 0.00061206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006120596081018448, 0.0006120596081018448, 0.0006120596081018448, 0.0006120596081018448, 0.0006120596081018448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006120596081018448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061206
Iteration 2/1000 | Loss: 0.00002294
Iteration 3/1000 | Loss: 0.00001327
Iteration 4/1000 | Loss: 0.00001074
Iteration 5/1000 | Loss: 0.00000992
Iteration 6/1000 | Loss: 0.00000949
Iteration 7/1000 | Loss: 0.00000921
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000906
Iteration 10/1000 | Loss: 0.00000905
Iteration 11/1000 | Loss: 0.00000899
Iteration 12/1000 | Loss: 0.00000899
Iteration 13/1000 | Loss: 0.00000899
Iteration 14/1000 | Loss: 0.00000895
Iteration 15/1000 | Loss: 0.00000895
Iteration 16/1000 | Loss: 0.00000893
Iteration 17/1000 | Loss: 0.00000893
Iteration 18/1000 | Loss: 0.00000891
Iteration 19/1000 | Loss: 0.00000891
Iteration 20/1000 | Loss: 0.00000890
Iteration 21/1000 | Loss: 0.00000889
Iteration 22/1000 | Loss: 0.00000889
Iteration 23/1000 | Loss: 0.00000887
Iteration 24/1000 | Loss: 0.00000887
Iteration 25/1000 | Loss: 0.00000886
Iteration 26/1000 | Loss: 0.00000881
Iteration 27/1000 | Loss: 0.00000881
Iteration 28/1000 | Loss: 0.00000881
Iteration 29/1000 | Loss: 0.00000881
Iteration 30/1000 | Loss: 0.00000881
Iteration 31/1000 | Loss: 0.00000880
Iteration 32/1000 | Loss: 0.00000880
Iteration 33/1000 | Loss: 0.00000880
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000877
Iteration 36/1000 | Loss: 0.00000877
Iteration 37/1000 | Loss: 0.00000877
Iteration 38/1000 | Loss: 0.00000876
Iteration 39/1000 | Loss: 0.00000876
Iteration 40/1000 | Loss: 0.00000876
Iteration 41/1000 | Loss: 0.00000876
Iteration 42/1000 | Loss: 0.00000876
Iteration 43/1000 | Loss: 0.00000876
Iteration 44/1000 | Loss: 0.00000876
Iteration 45/1000 | Loss: 0.00000876
Iteration 46/1000 | Loss: 0.00000875
Iteration 47/1000 | Loss: 0.00000875
Iteration 48/1000 | Loss: 0.00000875
Iteration 49/1000 | Loss: 0.00000875
Iteration 50/1000 | Loss: 0.00000874
Iteration 51/1000 | Loss: 0.00000874
Iteration 52/1000 | Loss: 0.00000874
Iteration 53/1000 | Loss: 0.00000874
Iteration 54/1000 | Loss: 0.00000874
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000873
Iteration 58/1000 | Loss: 0.00000873
Iteration 59/1000 | Loss: 0.00000872
Iteration 60/1000 | Loss: 0.00000872
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000871
Iteration 63/1000 | Loss: 0.00000871
Iteration 64/1000 | Loss: 0.00000870
Iteration 65/1000 | Loss: 0.00000870
Iteration 66/1000 | Loss: 0.00000870
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000869
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000869
Iteration 72/1000 | Loss: 0.00000869
Iteration 73/1000 | Loss: 0.00000869
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000869
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000868
Iteration 80/1000 | Loss: 0.00000868
Iteration 81/1000 | Loss: 0.00000868
Iteration 82/1000 | Loss: 0.00000868
Iteration 83/1000 | Loss: 0.00000868
Iteration 84/1000 | Loss: 0.00000868
Iteration 85/1000 | Loss: 0.00000868
Iteration 86/1000 | Loss: 0.00000868
Iteration 87/1000 | Loss: 0.00000868
Iteration 88/1000 | Loss: 0.00000868
Iteration 89/1000 | Loss: 0.00000867
Iteration 90/1000 | Loss: 0.00000867
Iteration 91/1000 | Loss: 0.00000867
Iteration 92/1000 | Loss: 0.00000867
Iteration 93/1000 | Loss: 0.00000867
Iteration 94/1000 | Loss: 0.00000867
Iteration 95/1000 | Loss: 0.00000867
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000866
Iteration 101/1000 | Loss: 0.00000866
Iteration 102/1000 | Loss: 0.00000866
Iteration 103/1000 | Loss: 0.00000866
Iteration 104/1000 | Loss: 0.00000866
Iteration 105/1000 | Loss: 0.00000866
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000866
Iteration 108/1000 | Loss: 0.00000866
Iteration 109/1000 | Loss: 0.00000866
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000865
Iteration 112/1000 | Loss: 0.00000865
Iteration 113/1000 | Loss: 0.00000865
Iteration 114/1000 | Loss: 0.00000865
Iteration 115/1000 | Loss: 0.00000865
Iteration 116/1000 | Loss: 0.00000865
Iteration 117/1000 | Loss: 0.00000865
Iteration 118/1000 | Loss: 0.00000865
Iteration 119/1000 | Loss: 0.00000865
Iteration 120/1000 | Loss: 0.00000865
Iteration 121/1000 | Loss: 0.00000865
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000865
Iteration 124/1000 | Loss: 0.00000865
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000865
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000864
Iteration 130/1000 | Loss: 0.00000864
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000864
Iteration 134/1000 | Loss: 0.00000864
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000864
Iteration 138/1000 | Loss: 0.00000864
Iteration 139/1000 | Loss: 0.00000863
Iteration 140/1000 | Loss: 0.00000863
Iteration 141/1000 | Loss: 0.00000863
Iteration 142/1000 | Loss: 0.00000863
Iteration 143/1000 | Loss: 0.00000863
Iteration 144/1000 | Loss: 0.00000863
Iteration 145/1000 | Loss: 0.00000863
Iteration 146/1000 | Loss: 0.00000863
Iteration 147/1000 | Loss: 0.00000863
Iteration 148/1000 | Loss: 0.00000863
Iteration 149/1000 | Loss: 0.00000862
Iteration 150/1000 | Loss: 0.00000862
Iteration 151/1000 | Loss: 0.00000862
Iteration 152/1000 | Loss: 0.00000862
Iteration 153/1000 | Loss: 0.00000862
Iteration 154/1000 | Loss: 0.00000862
Iteration 155/1000 | Loss: 0.00000862
Iteration 156/1000 | Loss: 0.00000862
Iteration 157/1000 | Loss: 0.00000862
Iteration 158/1000 | Loss: 0.00000862
Iteration 159/1000 | Loss: 0.00000862
Iteration 160/1000 | Loss: 0.00000862
Iteration 161/1000 | Loss: 0.00000862
Iteration 162/1000 | Loss: 0.00000862
Iteration 163/1000 | Loss: 0.00000862
Iteration 164/1000 | Loss: 0.00000862
Iteration 165/1000 | Loss: 0.00000862
Iteration 166/1000 | Loss: 0.00000862
Iteration 167/1000 | Loss: 0.00000862
Iteration 168/1000 | Loss: 0.00000861
Iteration 169/1000 | Loss: 0.00000861
Iteration 170/1000 | Loss: 0.00000861
Iteration 171/1000 | Loss: 0.00000861
Iteration 172/1000 | Loss: 0.00000860
Iteration 173/1000 | Loss: 0.00000860
Iteration 174/1000 | Loss: 0.00000860
Iteration 175/1000 | Loss: 0.00000860
Iteration 176/1000 | Loss: 0.00000860
Iteration 177/1000 | Loss: 0.00000860
Iteration 178/1000 | Loss: 0.00000860
Iteration 179/1000 | Loss: 0.00000860
Iteration 180/1000 | Loss: 0.00000860
Iteration 181/1000 | Loss: 0.00000860
Iteration 182/1000 | Loss: 0.00000860
Iteration 183/1000 | Loss: 0.00000860
Iteration 184/1000 | Loss: 0.00000859
Iteration 185/1000 | Loss: 0.00000859
Iteration 186/1000 | Loss: 0.00000859
Iteration 187/1000 | Loss: 0.00000859
Iteration 188/1000 | Loss: 0.00000859
Iteration 189/1000 | Loss: 0.00000859
Iteration 190/1000 | Loss: 0.00000859
Iteration 191/1000 | Loss: 0.00000858
Iteration 192/1000 | Loss: 0.00000858
Iteration 193/1000 | Loss: 0.00000858
Iteration 194/1000 | Loss: 0.00000858
Iteration 195/1000 | Loss: 0.00000858
Iteration 196/1000 | Loss: 0.00000858
Iteration 197/1000 | Loss: 0.00000858
Iteration 198/1000 | Loss: 0.00000858
Iteration 199/1000 | Loss: 0.00000858
Iteration 200/1000 | Loss: 0.00000858
Iteration 201/1000 | Loss: 0.00000858
Iteration 202/1000 | Loss: 0.00000858
Iteration 203/1000 | Loss: 0.00000858
Iteration 204/1000 | Loss: 0.00000858
Iteration 205/1000 | Loss: 0.00000858
Iteration 206/1000 | Loss: 0.00000858
Iteration 207/1000 | Loss: 0.00000858
Iteration 208/1000 | Loss: 0.00000858
Iteration 209/1000 | Loss: 0.00000858
Iteration 210/1000 | Loss: 0.00000858
Iteration 211/1000 | Loss: 0.00000858
Iteration 212/1000 | Loss: 0.00000858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [8.578915185353253e-06, 8.578915185353253e-06, 8.578915185353253e-06, 8.578915185353253e-06, 8.578915185353253e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.578915185353253e-06

Optimization complete. Final v2v error: 2.504173994064331 mm

Highest mean error: 2.8883659839630127 mm for frame 130

Lowest mean error: 2.24342679977417 mm for frame 0

Saving results

Total time: 856.5228867530823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1081
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491155
Iteration 2/25 | Loss: 0.00109078
Iteration 3/25 | Loss: 0.00095317
Iteration 4/25 | Loss: 0.00094382
Iteration 5/25 | Loss: 0.00094115
Iteration 6/25 | Loss: 0.00094050
Iteration 7/25 | Loss: 0.00094050
Iteration 8/25 | Loss: 0.00094050
Iteration 9/25 | Loss: 0.00094050
Iteration 10/25 | Loss: 0.00094050
Iteration 11/25 | Loss: 0.00094050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000940496101975441, 0.000940496101975441, 0.000940496101975441, 0.000940496101975441, 0.000940496101975441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000940496101975441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63406372
Iteration 2/25 | Loss: 0.00056350
Iteration 3/25 | Loss: 0.00056349
Iteration 4/25 | Loss: 0.00056349
Iteration 5/25 | Loss: 0.00056349
Iteration 6/25 | Loss: 0.00056349
Iteration 7/25 | Loss: 0.00056349
Iteration 8/25 | Loss: 0.00056349
Iteration 9/25 | Loss: 0.00056349
Iteration 10/25 | Loss: 0.00056349
Iteration 11/25 | Loss: 0.00056349
Iteration 12/25 | Loss: 0.00056349
Iteration 13/25 | Loss: 0.00056349
Iteration 14/25 | Loss: 0.00056349
Iteration 15/25 | Loss: 0.00056349
Iteration 16/25 | Loss: 0.00056349
Iteration 17/25 | Loss: 0.00056349
Iteration 18/25 | Loss: 0.00056349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005634893896058202, 0.0005634893896058202, 0.0005634893896058202, 0.0005634893896058202, 0.0005634893896058202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005634893896058202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056349
Iteration 2/1000 | Loss: 0.00002283
Iteration 3/1000 | Loss: 0.00001273
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001090
Iteration 6/1000 | Loss: 0.00001063
Iteration 7/1000 | Loss: 0.00001048
Iteration 8/1000 | Loss: 0.00001033
Iteration 9/1000 | Loss: 0.00001022
Iteration 10/1000 | Loss: 0.00001019
Iteration 11/1000 | Loss: 0.00001015
Iteration 12/1000 | Loss: 0.00001015
Iteration 13/1000 | Loss: 0.00001014
Iteration 14/1000 | Loss: 0.00001005
Iteration 15/1000 | Loss: 0.00001005
Iteration 16/1000 | Loss: 0.00001005
Iteration 17/1000 | Loss: 0.00001005
Iteration 18/1000 | Loss: 0.00001004
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001004
Iteration 21/1000 | Loss: 0.00001002
Iteration 22/1000 | Loss: 0.00001002
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00001002
Iteration 25/1000 | Loss: 0.00001001
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000998
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000997
Iteration 34/1000 | Loss: 0.00000997
Iteration 35/1000 | Loss: 0.00000997
Iteration 36/1000 | Loss: 0.00000997
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000996
Iteration 39/1000 | Loss: 0.00000995
Iteration 40/1000 | Loss: 0.00000995
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000994
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000993
Iteration 45/1000 | Loss: 0.00000993
Iteration 46/1000 | Loss: 0.00000993
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000992
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000991
Iteration 52/1000 | Loss: 0.00000991
Iteration 53/1000 | Loss: 0.00000991
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000991
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000990
Iteration 64/1000 | Loss: 0.00000990
Iteration 65/1000 | Loss: 0.00000990
Iteration 66/1000 | Loss: 0.00000989
Iteration 67/1000 | Loss: 0.00000989
Iteration 68/1000 | Loss: 0.00000989
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000988
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000988
Iteration 74/1000 | Loss: 0.00000988
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000987
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000987
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000987
Iteration 87/1000 | Loss: 0.00000987
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000986
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000984
Iteration 101/1000 | Loss: 0.00000984
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000983
Iteration 110/1000 | Loss: 0.00000983
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000983
Iteration 113/1000 | Loss: 0.00000983
Iteration 114/1000 | Loss: 0.00000983
Iteration 115/1000 | Loss: 0.00000983
Iteration 116/1000 | Loss: 0.00000983
Iteration 117/1000 | Loss: 0.00000983
Iteration 118/1000 | Loss: 0.00000983
Iteration 119/1000 | Loss: 0.00000983
Iteration 120/1000 | Loss: 0.00000983
Iteration 121/1000 | Loss: 0.00000983
Iteration 122/1000 | Loss: 0.00000983
Iteration 123/1000 | Loss: 0.00000983
Iteration 124/1000 | Loss: 0.00000983
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000983
Iteration 127/1000 | Loss: 0.00000983
Iteration 128/1000 | Loss: 0.00000983
Iteration 129/1000 | Loss: 0.00000983
Iteration 130/1000 | Loss: 0.00000983
Iteration 131/1000 | Loss: 0.00000983
Iteration 132/1000 | Loss: 0.00000983
Iteration 133/1000 | Loss: 0.00000983
Iteration 134/1000 | Loss: 0.00000983
Iteration 135/1000 | Loss: 0.00000983
Iteration 136/1000 | Loss: 0.00000983
Iteration 137/1000 | Loss: 0.00000983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [9.829076589085162e-06, 9.829076589085162e-06, 9.829076589085162e-06, 9.829076589085162e-06, 9.829076589085162e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.829076589085162e-06

Optimization complete. Final v2v error: 2.672055959701538 mm

Highest mean error: 3.1202590465545654 mm for frame 53

Lowest mean error: 2.4446377754211426 mm for frame 27

Saving results

Total time: 950.6636741161346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1035
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874982
Iteration 2/25 | Loss: 0.00105422
Iteration 3/25 | Loss: 0.00092323
Iteration 4/25 | Loss: 0.00090911
Iteration 5/25 | Loss: 0.00090554
Iteration 6/25 | Loss: 0.00090478
Iteration 7/25 | Loss: 0.00090478
Iteration 8/25 | Loss: 0.00090478
Iteration 9/25 | Loss: 0.00090478
Iteration 10/25 | Loss: 0.00090478
Iteration 11/25 | Loss: 0.00090478
Iteration 12/25 | Loss: 0.00090478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009047788335010409, 0.0009047788335010409, 0.0009047788335010409, 0.0009047788335010409, 0.0009047788335010409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009047788335010409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34291816
Iteration 2/25 | Loss: 0.00050162
Iteration 3/25 | Loss: 0.00050160
Iteration 4/25 | Loss: 0.00050160
Iteration 5/25 | Loss: 0.00050160
Iteration 6/25 | Loss: 0.00050160
Iteration 7/25 | Loss: 0.00050160
Iteration 8/25 | Loss: 0.00050160
Iteration 9/25 | Loss: 0.00050160
Iteration 10/25 | Loss: 0.00050160
Iteration 11/25 | Loss: 0.00050160
Iteration 12/25 | Loss: 0.00050160
Iteration 13/25 | Loss: 0.00050160
Iteration 14/25 | Loss: 0.00050160
Iteration 15/25 | Loss: 0.00050160
Iteration 16/25 | Loss: 0.00050160
Iteration 17/25 | Loss: 0.00050160
Iteration 18/25 | Loss: 0.00050160
Iteration 19/25 | Loss: 0.00050160
Iteration 20/25 | Loss: 0.00050160
Iteration 21/25 | Loss: 0.00050160
Iteration 22/25 | Loss: 0.00050160
Iteration 23/25 | Loss: 0.00050160
Iteration 24/25 | Loss: 0.00050160
Iteration 25/25 | Loss: 0.00050160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050160
Iteration 2/1000 | Loss: 0.00002004
Iteration 3/1000 | Loss: 0.00001145
Iteration 4/1000 | Loss: 0.00001031
Iteration 5/1000 | Loss: 0.00000966
Iteration 6/1000 | Loss: 0.00000894
Iteration 7/1000 | Loss: 0.00000861
Iteration 8/1000 | Loss: 0.00000833
Iteration 9/1000 | Loss: 0.00000821
Iteration 10/1000 | Loss: 0.00000818
Iteration 11/1000 | Loss: 0.00000817
Iteration 12/1000 | Loss: 0.00000815
Iteration 13/1000 | Loss: 0.00000814
Iteration 14/1000 | Loss: 0.00000811
Iteration 15/1000 | Loss: 0.00000810
Iteration 16/1000 | Loss: 0.00000809
Iteration 17/1000 | Loss: 0.00000807
Iteration 18/1000 | Loss: 0.00000806
Iteration 19/1000 | Loss: 0.00000803
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000802
Iteration 22/1000 | Loss: 0.00000802
Iteration 23/1000 | Loss: 0.00000802
Iteration 24/1000 | Loss: 0.00000802
Iteration 25/1000 | Loss: 0.00000801
Iteration 26/1000 | Loss: 0.00000800
Iteration 27/1000 | Loss: 0.00000795
Iteration 28/1000 | Loss: 0.00000794
Iteration 29/1000 | Loss: 0.00000794
Iteration 30/1000 | Loss: 0.00000793
Iteration 31/1000 | Loss: 0.00000790
Iteration 32/1000 | Loss: 0.00000790
Iteration 33/1000 | Loss: 0.00000789
Iteration 34/1000 | Loss: 0.00000788
Iteration 35/1000 | Loss: 0.00000788
Iteration 36/1000 | Loss: 0.00000786
Iteration 37/1000 | Loss: 0.00000786
Iteration 38/1000 | Loss: 0.00000785
Iteration 39/1000 | Loss: 0.00000785
Iteration 40/1000 | Loss: 0.00000785
Iteration 41/1000 | Loss: 0.00000784
Iteration 42/1000 | Loss: 0.00000784
Iteration 43/1000 | Loss: 0.00000784
Iteration 44/1000 | Loss: 0.00000783
Iteration 45/1000 | Loss: 0.00000782
Iteration 46/1000 | Loss: 0.00000782
Iteration 47/1000 | Loss: 0.00000781
Iteration 48/1000 | Loss: 0.00000781
Iteration 49/1000 | Loss: 0.00000781
Iteration 50/1000 | Loss: 0.00000780
Iteration 51/1000 | Loss: 0.00000780
Iteration 52/1000 | Loss: 0.00000780
Iteration 53/1000 | Loss: 0.00000779
Iteration 54/1000 | Loss: 0.00000779
Iteration 55/1000 | Loss: 0.00000779
Iteration 56/1000 | Loss: 0.00000778
Iteration 57/1000 | Loss: 0.00000778
Iteration 58/1000 | Loss: 0.00000778
Iteration 59/1000 | Loss: 0.00000778
Iteration 60/1000 | Loss: 0.00000778
Iteration 61/1000 | Loss: 0.00000778
Iteration 62/1000 | Loss: 0.00000778
Iteration 63/1000 | Loss: 0.00000778
Iteration 64/1000 | Loss: 0.00000778
Iteration 65/1000 | Loss: 0.00000777
Iteration 66/1000 | Loss: 0.00000777
Iteration 67/1000 | Loss: 0.00000777
Iteration 68/1000 | Loss: 0.00000777
Iteration 69/1000 | Loss: 0.00000777
Iteration 70/1000 | Loss: 0.00000777
Iteration 71/1000 | Loss: 0.00000777
Iteration 72/1000 | Loss: 0.00000777
Iteration 73/1000 | Loss: 0.00000777
Iteration 74/1000 | Loss: 0.00000777
Iteration 75/1000 | Loss: 0.00000777
Iteration 76/1000 | Loss: 0.00000777
Iteration 77/1000 | Loss: 0.00000777
Iteration 78/1000 | Loss: 0.00000777
Iteration 79/1000 | Loss: 0.00000777
Iteration 80/1000 | Loss: 0.00000777
Iteration 81/1000 | Loss: 0.00000777
Iteration 82/1000 | Loss: 0.00000777
Iteration 83/1000 | Loss: 0.00000777
Iteration 84/1000 | Loss: 0.00000777
Iteration 85/1000 | Loss: 0.00000777
Iteration 86/1000 | Loss: 0.00000777
Iteration 87/1000 | Loss: 0.00000777
Iteration 88/1000 | Loss: 0.00000777
Iteration 89/1000 | Loss: 0.00000777
Iteration 90/1000 | Loss: 0.00000777
Iteration 91/1000 | Loss: 0.00000777
Iteration 92/1000 | Loss: 0.00000777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [7.765011105220765e-06, 7.765011105220765e-06, 7.765011105220765e-06, 7.765011105220765e-06, 7.765011105220765e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.765011105220765e-06

Optimization complete. Final v2v error: 2.392153263092041 mm

Highest mean error: 2.5502514839172363 mm for frame 72

Lowest mean error: 2.272158622741699 mm for frame 45

Saving results

Total time: 511.69184255599976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1026
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825762
Iteration 2/25 | Loss: 0.00129111
Iteration 3/25 | Loss: 0.00100987
Iteration 4/25 | Loss: 0.00098283
Iteration 5/25 | Loss: 0.00098328
Iteration 6/25 | Loss: 0.00097584
Iteration 7/25 | Loss: 0.00096989
Iteration 8/25 | Loss: 0.00096864
Iteration 9/25 | Loss: 0.00096800
Iteration 10/25 | Loss: 0.00097324
Iteration 11/25 | Loss: 0.00096484
Iteration 12/25 | Loss: 0.00096302
Iteration 13/25 | Loss: 0.00096249
Iteration 14/25 | Loss: 0.00096229
Iteration 15/25 | Loss: 0.00096229
Iteration 16/25 | Loss: 0.00096229
Iteration 17/25 | Loss: 0.00096229
Iteration 18/25 | Loss: 0.00096229
Iteration 19/25 | Loss: 0.00096229
Iteration 20/25 | Loss: 0.00096229
Iteration 21/25 | Loss: 0.00096228
Iteration 22/25 | Loss: 0.00096228
Iteration 23/25 | Loss: 0.00096228
Iteration 24/25 | Loss: 0.00096228
Iteration 25/25 | Loss: 0.00096228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.52140617
Iteration 2/25 | Loss: 0.00050853
Iteration 3/25 | Loss: 0.00050846
Iteration 4/25 | Loss: 0.00050846
Iteration 5/25 | Loss: 0.00050846
Iteration 6/25 | Loss: 0.00050846
Iteration 7/25 | Loss: 0.00050846
Iteration 8/25 | Loss: 0.00050846
Iteration 9/25 | Loss: 0.00050846
Iteration 10/25 | Loss: 0.00050846
Iteration 11/25 | Loss: 0.00050846
Iteration 12/25 | Loss: 0.00050846
Iteration 13/25 | Loss: 0.00050846
Iteration 14/25 | Loss: 0.00050846
Iteration 15/25 | Loss: 0.00050846
Iteration 16/25 | Loss: 0.00050846
Iteration 17/25 | Loss: 0.00050846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005084624863229692, 0.0005084624863229692, 0.0005084624863229692, 0.0005084624863229692, 0.0005084624863229692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005084624863229692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050846
Iteration 2/1000 | Loss: 0.00002056
Iteration 3/1000 | Loss: 0.00037990
Iteration 4/1000 | Loss: 0.00001277
Iteration 5/1000 | Loss: 0.00001184
Iteration 6/1000 | Loss: 0.00001144
Iteration 7/1000 | Loss: 0.00001113
Iteration 8/1000 | Loss: 0.00001101
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001088
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001079
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001071
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001065
Iteration 17/1000 | Loss: 0.00001064
Iteration 18/1000 | Loss: 0.00001064
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001063
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001060
Iteration 28/1000 | Loss: 0.00001060
Iteration 29/1000 | Loss: 0.00001060
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001059
Iteration 33/1000 | Loss: 0.00001058
Iteration 34/1000 | Loss: 0.00001058
Iteration 35/1000 | Loss: 0.00001057
Iteration 36/1000 | Loss: 0.00001057
Iteration 37/1000 | Loss: 0.00001057
Iteration 38/1000 | Loss: 0.00001056
Iteration 39/1000 | Loss: 0.00001056
Iteration 40/1000 | Loss: 0.00001056
Iteration 41/1000 | Loss: 0.00001056
Iteration 42/1000 | Loss: 0.00001056
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001055
Iteration 45/1000 | Loss: 0.00001055
Iteration 46/1000 | Loss: 0.00001055
Iteration 47/1000 | Loss: 0.00001055
Iteration 48/1000 | Loss: 0.00001055
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001054
Iteration 51/1000 | Loss: 0.00001054
Iteration 52/1000 | Loss: 0.00001054
Iteration 53/1000 | Loss: 0.00001054
Iteration 54/1000 | Loss: 0.00001054
Iteration 55/1000 | Loss: 0.00001054
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001054
Iteration 58/1000 | Loss: 0.00001054
Iteration 59/1000 | Loss: 0.00001054
Iteration 60/1000 | Loss: 0.00001054
Iteration 61/1000 | Loss: 0.00001053
Iteration 62/1000 | Loss: 0.00001053
Iteration 63/1000 | Loss: 0.00001053
Iteration 64/1000 | Loss: 0.00001053
Iteration 65/1000 | Loss: 0.00001053
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001050
Iteration 76/1000 | Loss: 0.00001050
Iteration 77/1000 | Loss: 0.00001050
Iteration 78/1000 | Loss: 0.00001050
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001049
Iteration 83/1000 | Loss: 0.00001048
Iteration 84/1000 | Loss: 0.00001048
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001048
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001048
Iteration 94/1000 | Loss: 0.00001048
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001046
Iteration 100/1000 | Loss: 0.00001046
Iteration 101/1000 | Loss: 0.00001046
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001045
Iteration 109/1000 | Loss: 0.00001045
Iteration 110/1000 | Loss: 0.00001045
Iteration 111/1000 | Loss: 0.00001045
Iteration 112/1000 | Loss: 0.00001044
Iteration 113/1000 | Loss: 0.00001044
Iteration 114/1000 | Loss: 0.00001044
Iteration 115/1000 | Loss: 0.00001044
Iteration 116/1000 | Loss: 0.00001044
Iteration 117/1000 | Loss: 0.00001044
Iteration 118/1000 | Loss: 0.00001044
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001043
Iteration 126/1000 | Loss: 0.00001043
Iteration 127/1000 | Loss: 0.00001043
Iteration 128/1000 | Loss: 0.00001043
Iteration 129/1000 | Loss: 0.00001043
Iteration 130/1000 | Loss: 0.00001043
Iteration 131/1000 | Loss: 0.00001043
Iteration 132/1000 | Loss: 0.00001043
Iteration 133/1000 | Loss: 0.00001043
Iteration 134/1000 | Loss: 0.00001043
Iteration 135/1000 | Loss: 0.00001043
Iteration 136/1000 | Loss: 0.00001043
Iteration 137/1000 | Loss: 0.00001043
Iteration 138/1000 | Loss: 0.00001043
Iteration 139/1000 | Loss: 0.00001043
Iteration 140/1000 | Loss: 0.00001043
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001043
Iteration 159/1000 | Loss: 0.00001043
Iteration 160/1000 | Loss: 0.00001043
Iteration 161/1000 | Loss: 0.00001043
Iteration 162/1000 | Loss: 0.00001043
Iteration 163/1000 | Loss: 0.00001043
Iteration 164/1000 | Loss: 0.00001043
Iteration 165/1000 | Loss: 0.00001043
Iteration 166/1000 | Loss: 0.00001043
Iteration 167/1000 | Loss: 0.00001043
Iteration 168/1000 | Loss: 0.00001043
Iteration 169/1000 | Loss: 0.00001043
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001043
Iteration 172/1000 | Loss: 0.00001043
Iteration 173/1000 | Loss: 0.00001043
Iteration 174/1000 | Loss: 0.00001043
Iteration 175/1000 | Loss: 0.00001043
Iteration 176/1000 | Loss: 0.00001043
Iteration 177/1000 | Loss: 0.00001043
Iteration 178/1000 | Loss: 0.00001043
Iteration 179/1000 | Loss: 0.00001043
Iteration 180/1000 | Loss: 0.00001043
Iteration 181/1000 | Loss: 0.00001043
Iteration 182/1000 | Loss: 0.00001043
Iteration 183/1000 | Loss: 0.00001043
Iteration 184/1000 | Loss: 0.00001043
Iteration 185/1000 | Loss: 0.00001043
Iteration 186/1000 | Loss: 0.00001043
Iteration 187/1000 | Loss: 0.00001043
Iteration 188/1000 | Loss: 0.00001043
Iteration 189/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.0428926543681882e-05, 1.0428926543681882e-05, 1.0428926543681882e-05, 1.0428926543681882e-05, 1.0428926543681882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0428926543681882e-05

Optimization complete. Final v2v error: 2.7207977771759033 mm

Highest mean error: 3.2627336978912354 mm for frame 174

Lowest mean error: 2.3564414978027344 mm for frame 136

Saving results

Total time: 1532.3624396324158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1083
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357722
Iteration 2/25 | Loss: 0.00103193
Iteration 3/25 | Loss: 0.00092003
Iteration 4/25 | Loss: 0.00091065
Iteration 5/25 | Loss: 0.00090774
Iteration 6/25 | Loss: 0.00090727
Iteration 7/25 | Loss: 0.00090727
Iteration 8/25 | Loss: 0.00090727
Iteration 9/25 | Loss: 0.00090727
Iteration 10/25 | Loss: 0.00090727
Iteration 11/25 | Loss: 0.00090727
Iteration 12/25 | Loss: 0.00090727
Iteration 13/25 | Loss: 0.00090727
Iteration 14/25 | Loss: 0.00090727
Iteration 15/25 | Loss: 0.00090727
Iteration 16/25 | Loss: 0.00090727
Iteration 17/25 | Loss: 0.00090727
Iteration 18/25 | Loss: 0.00090727
Iteration 19/25 | Loss: 0.00090727
Iteration 20/25 | Loss: 0.00090727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009072698885574937, 0.0009072698885574937, 0.0009072698885574937, 0.0009072698885574937, 0.0009072698885574937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009072698885574937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58897948
Iteration 2/25 | Loss: 0.00066921
Iteration 3/25 | Loss: 0.00066921
Iteration 4/25 | Loss: 0.00066921
Iteration 5/25 | Loss: 0.00066921
Iteration 6/25 | Loss: 0.00066921
Iteration 7/25 | Loss: 0.00066921
Iteration 8/25 | Loss: 0.00066920
Iteration 9/25 | Loss: 0.00066920
Iteration 10/25 | Loss: 0.00066920
Iteration 11/25 | Loss: 0.00066920
Iteration 12/25 | Loss: 0.00066920
Iteration 13/25 | Loss: 0.00066920
Iteration 14/25 | Loss: 0.00066920
Iteration 15/25 | Loss: 0.00066920
Iteration 16/25 | Loss: 0.00066920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006692044553346932, 0.0006692044553346932, 0.0006692044553346932, 0.0006692044553346932, 0.0006692044553346932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006692044553346932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066920
Iteration 2/1000 | Loss: 0.00001318
Iteration 3/1000 | Loss: 0.00001012
Iteration 4/1000 | Loss: 0.00000948
Iteration 5/1000 | Loss: 0.00000939
Iteration 6/1000 | Loss: 0.00000921
Iteration 7/1000 | Loss: 0.00000903
Iteration 8/1000 | Loss: 0.00000881
Iteration 9/1000 | Loss: 0.00000877
Iteration 10/1000 | Loss: 0.00000877
Iteration 11/1000 | Loss: 0.00000876
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000870
Iteration 14/1000 | Loss: 0.00000870
Iteration 15/1000 | Loss: 0.00000869
Iteration 16/1000 | Loss: 0.00000869
Iteration 17/1000 | Loss: 0.00000869
Iteration 18/1000 | Loss: 0.00000868
Iteration 19/1000 | Loss: 0.00000868
Iteration 20/1000 | Loss: 0.00000864
Iteration 21/1000 | Loss: 0.00000863
Iteration 22/1000 | Loss: 0.00000862
Iteration 23/1000 | Loss: 0.00000861
Iteration 24/1000 | Loss: 0.00000861
Iteration 25/1000 | Loss: 0.00000860
Iteration 26/1000 | Loss: 0.00000860
Iteration 27/1000 | Loss: 0.00000859
Iteration 28/1000 | Loss: 0.00000859
Iteration 29/1000 | Loss: 0.00000858
Iteration 30/1000 | Loss: 0.00000854
Iteration 31/1000 | Loss: 0.00000852
Iteration 32/1000 | Loss: 0.00000851
Iteration 33/1000 | Loss: 0.00000851
Iteration 34/1000 | Loss: 0.00000850
Iteration 35/1000 | Loss: 0.00000850
Iteration 36/1000 | Loss: 0.00000850
Iteration 37/1000 | Loss: 0.00000849
Iteration 38/1000 | Loss: 0.00000849
Iteration 39/1000 | Loss: 0.00000849
Iteration 40/1000 | Loss: 0.00000849
Iteration 41/1000 | Loss: 0.00000849
Iteration 42/1000 | Loss: 0.00000848
Iteration 43/1000 | Loss: 0.00000847
Iteration 44/1000 | Loss: 0.00000847
Iteration 45/1000 | Loss: 0.00000847
Iteration 46/1000 | Loss: 0.00000847
Iteration 47/1000 | Loss: 0.00000847
Iteration 48/1000 | Loss: 0.00000847
Iteration 49/1000 | Loss: 0.00000846
Iteration 50/1000 | Loss: 0.00000846
Iteration 51/1000 | Loss: 0.00000846
Iteration 52/1000 | Loss: 0.00000846
Iteration 53/1000 | Loss: 0.00000846
Iteration 54/1000 | Loss: 0.00000846
Iteration 55/1000 | Loss: 0.00000846
Iteration 56/1000 | Loss: 0.00000846
Iteration 57/1000 | Loss: 0.00000846
Iteration 58/1000 | Loss: 0.00000846
Iteration 59/1000 | Loss: 0.00000846
Iteration 60/1000 | Loss: 0.00000846
Iteration 61/1000 | Loss: 0.00000846
Iteration 62/1000 | Loss: 0.00000846
Iteration 63/1000 | Loss: 0.00000846
Iteration 64/1000 | Loss: 0.00000846
Iteration 65/1000 | Loss: 0.00000846
Iteration 66/1000 | Loss: 0.00000846
Iteration 67/1000 | Loss: 0.00000846
Iteration 68/1000 | Loss: 0.00000846
Iteration 69/1000 | Loss: 0.00000846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [8.455174793198239e-06, 8.455174793198239e-06, 8.455174793198239e-06, 8.455174793198239e-06, 8.455174793198239e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.455174793198239e-06

Optimization complete. Final v2v error: 2.4663476943969727 mm

Highest mean error: 3.1276464462280273 mm for frame 131

Lowest mean error: 2.3015146255493164 mm for frame 119

Saving results

Total time: 986.0958487987518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1069
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064240
Iteration 2/25 | Loss: 0.00121652
Iteration 3/25 | Loss: 0.00098654
Iteration 4/25 | Loss: 0.00096786
Iteration 5/25 | Loss: 0.00096288
Iteration 6/25 | Loss: 0.00096221
Iteration 7/25 | Loss: 0.00096221
Iteration 8/25 | Loss: 0.00096221
Iteration 9/25 | Loss: 0.00096221
Iteration 10/25 | Loss: 0.00096221
Iteration 11/25 | Loss: 0.00096221
Iteration 12/25 | Loss: 0.00096221
Iteration 13/25 | Loss: 0.00096221
Iteration 14/25 | Loss: 0.00096221
Iteration 15/25 | Loss: 0.00096221
Iteration 16/25 | Loss: 0.00096221
Iteration 17/25 | Loss: 0.00096221
Iteration 18/25 | Loss: 0.00096221
Iteration 19/25 | Loss: 0.00096221
Iteration 20/25 | Loss: 0.00096221
Iteration 21/25 | Loss: 0.00096221
Iteration 22/25 | Loss: 0.00096221
Iteration 23/25 | Loss: 0.00096221
Iteration 24/25 | Loss: 0.00096221
Iteration 25/25 | Loss: 0.00096221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31491959
Iteration 2/25 | Loss: 0.00051193
Iteration 3/25 | Loss: 0.00051193
Iteration 4/25 | Loss: 0.00051193
Iteration 5/25 | Loss: 0.00051193
Iteration 6/25 | Loss: 0.00051192
Iteration 7/25 | Loss: 0.00051192
Iteration 8/25 | Loss: 0.00051192
Iteration 9/25 | Loss: 0.00051192
Iteration 10/25 | Loss: 0.00051192
Iteration 11/25 | Loss: 0.00051192
Iteration 12/25 | Loss: 0.00051192
Iteration 13/25 | Loss: 0.00051192
Iteration 14/25 | Loss: 0.00051192
Iteration 15/25 | Loss: 0.00051192
Iteration 16/25 | Loss: 0.00051192
Iteration 17/25 | Loss: 0.00051192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000511924154125154, 0.000511924154125154, 0.000511924154125154, 0.000511924154125154, 0.000511924154125154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000511924154125154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051192
Iteration 2/1000 | Loss: 0.00001885
Iteration 3/1000 | Loss: 0.00001368
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001211
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001126
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001103
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001069
Iteration 15/1000 | Loss: 0.00001066
Iteration 16/1000 | Loss: 0.00001065
Iteration 17/1000 | Loss: 0.00001065
Iteration 18/1000 | Loss: 0.00001065
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001062
Iteration 25/1000 | Loss: 0.00001062
Iteration 26/1000 | Loss: 0.00001062
Iteration 27/1000 | Loss: 0.00001061
Iteration 28/1000 | Loss: 0.00001061
Iteration 29/1000 | Loss: 0.00001061
Iteration 30/1000 | Loss: 0.00001061
Iteration 31/1000 | Loss: 0.00001060
Iteration 32/1000 | Loss: 0.00001060
Iteration 33/1000 | Loss: 0.00001060
Iteration 34/1000 | Loss: 0.00001059
Iteration 35/1000 | Loss: 0.00001059
Iteration 36/1000 | Loss: 0.00001059
Iteration 37/1000 | Loss: 0.00001057
Iteration 38/1000 | Loss: 0.00001056
Iteration 39/1000 | Loss: 0.00001056
Iteration 40/1000 | Loss: 0.00001056
Iteration 41/1000 | Loss: 0.00001056
Iteration 42/1000 | Loss: 0.00001056
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001055
Iteration 45/1000 | Loss: 0.00001055
Iteration 46/1000 | Loss: 0.00001055
Iteration 47/1000 | Loss: 0.00001055
Iteration 48/1000 | Loss: 0.00001054
Iteration 49/1000 | Loss: 0.00001054
Iteration 50/1000 | Loss: 0.00001053
Iteration 51/1000 | Loss: 0.00001053
Iteration 52/1000 | Loss: 0.00001053
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001051
Iteration 59/1000 | Loss: 0.00001051
Iteration 60/1000 | Loss: 0.00001051
Iteration 61/1000 | Loss: 0.00001051
Iteration 62/1000 | Loss: 0.00001051
Iteration 63/1000 | Loss: 0.00001051
Iteration 64/1000 | Loss: 0.00001051
Iteration 65/1000 | Loss: 0.00001051
Iteration 66/1000 | Loss: 0.00001051
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001050
Iteration 70/1000 | Loss: 0.00001050
Iteration 71/1000 | Loss: 0.00001050
Iteration 72/1000 | Loss: 0.00001050
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001050
Iteration 76/1000 | Loss: 0.00001050
Iteration 77/1000 | Loss: 0.00001050
Iteration 78/1000 | Loss: 0.00001050
Iteration 79/1000 | Loss: 0.00001050
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001049
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001049
Iteration 86/1000 | Loss: 0.00001049
Iteration 87/1000 | Loss: 0.00001049
Iteration 88/1000 | Loss: 0.00001049
Iteration 89/1000 | Loss: 0.00001049
Iteration 90/1000 | Loss: 0.00001049
Iteration 91/1000 | Loss: 0.00001049
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001048
Iteration 94/1000 | Loss: 0.00001048
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001048
Iteration 102/1000 | Loss: 0.00001048
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001047
Iteration 110/1000 | Loss: 0.00001047
Iteration 111/1000 | Loss: 0.00001047
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001047
Iteration 116/1000 | Loss: 0.00001047
Iteration 117/1000 | Loss: 0.00001047
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001047
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001046
Iteration 124/1000 | Loss: 0.00001046
Iteration 125/1000 | Loss: 0.00001046
Iteration 126/1000 | Loss: 0.00001046
Iteration 127/1000 | Loss: 0.00001046
Iteration 128/1000 | Loss: 0.00001046
Iteration 129/1000 | Loss: 0.00001046
Iteration 130/1000 | Loss: 0.00001046
Iteration 131/1000 | Loss: 0.00001045
Iteration 132/1000 | Loss: 0.00001045
Iteration 133/1000 | Loss: 0.00001045
Iteration 134/1000 | Loss: 0.00001045
Iteration 135/1000 | Loss: 0.00001045
Iteration 136/1000 | Loss: 0.00001045
Iteration 137/1000 | Loss: 0.00001045
Iteration 138/1000 | Loss: 0.00001045
Iteration 139/1000 | Loss: 0.00001045
Iteration 140/1000 | Loss: 0.00001045
Iteration 141/1000 | Loss: 0.00001045
Iteration 142/1000 | Loss: 0.00001045
Iteration 143/1000 | Loss: 0.00001045
Iteration 144/1000 | Loss: 0.00001045
Iteration 145/1000 | Loss: 0.00001045
Iteration 146/1000 | Loss: 0.00001045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.044880900735734e-05, 1.044880900735734e-05, 1.044880900735734e-05, 1.044880900735734e-05, 1.044880900735734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.044880900735734e-05

Optimization complete. Final v2v error: 2.7540833950042725 mm

Highest mean error: 3.077535390853882 mm for frame 181

Lowest mean error: 2.581578493118286 mm for frame 138

Saving results

Total time: 861.7413856983185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1034
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906977
Iteration 2/25 | Loss: 0.00133838
Iteration 3/25 | Loss: 0.00112864
Iteration 4/25 | Loss: 0.00110275
Iteration 5/25 | Loss: 0.00109755
Iteration 6/25 | Loss: 0.00109635
Iteration 7/25 | Loss: 0.00109619
Iteration 8/25 | Loss: 0.00109619
Iteration 9/25 | Loss: 0.00109619
Iteration 10/25 | Loss: 0.00109619
Iteration 11/25 | Loss: 0.00109619
Iteration 12/25 | Loss: 0.00109619
Iteration 13/25 | Loss: 0.00109619
Iteration 14/25 | Loss: 0.00109619
Iteration 15/25 | Loss: 0.00109619
Iteration 16/25 | Loss: 0.00109619
Iteration 17/25 | Loss: 0.00109619
Iteration 18/25 | Loss: 0.00109619
Iteration 19/25 | Loss: 0.00109619
Iteration 20/25 | Loss: 0.00109619
Iteration 21/25 | Loss: 0.00109619
Iteration 22/25 | Loss: 0.00109619
Iteration 23/25 | Loss: 0.00109619
Iteration 24/25 | Loss: 0.00109619
Iteration 25/25 | Loss: 0.00109619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27908492
Iteration 2/25 | Loss: 0.00066597
Iteration 3/25 | Loss: 0.00066596
Iteration 4/25 | Loss: 0.00066596
Iteration 5/25 | Loss: 0.00066596
Iteration 6/25 | Loss: 0.00066596
Iteration 7/25 | Loss: 0.00066596
Iteration 8/25 | Loss: 0.00066596
Iteration 9/25 | Loss: 0.00066596
Iteration 10/25 | Loss: 0.00066596
Iteration 11/25 | Loss: 0.00066596
Iteration 12/25 | Loss: 0.00066596
Iteration 13/25 | Loss: 0.00066596
Iteration 14/25 | Loss: 0.00066596
Iteration 15/25 | Loss: 0.00066596
Iteration 16/25 | Loss: 0.00066596
Iteration 17/25 | Loss: 0.00066596
Iteration 18/25 | Loss: 0.00066596
Iteration 19/25 | Loss: 0.00066596
Iteration 20/25 | Loss: 0.00066596
Iteration 21/25 | Loss: 0.00066596
Iteration 22/25 | Loss: 0.00066596
Iteration 23/25 | Loss: 0.00066596
Iteration 24/25 | Loss: 0.00066596
Iteration 25/25 | Loss: 0.00066596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066596
Iteration 2/1000 | Loss: 0.00005259
Iteration 3/1000 | Loss: 0.00003728
Iteration 4/1000 | Loss: 0.00003156
Iteration 5/1000 | Loss: 0.00002930
Iteration 6/1000 | Loss: 0.00002810
Iteration 7/1000 | Loss: 0.00002714
Iteration 8/1000 | Loss: 0.00002659
Iteration 9/1000 | Loss: 0.00002615
Iteration 10/1000 | Loss: 0.00002588
Iteration 11/1000 | Loss: 0.00002573
Iteration 12/1000 | Loss: 0.00002572
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002543
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002542
Iteration 17/1000 | Loss: 0.00002542
Iteration 18/1000 | Loss: 0.00002541
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00002536
Iteration 22/1000 | Loss: 0.00002535
Iteration 23/1000 | Loss: 0.00002535
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002534
Iteration 26/1000 | Loss: 0.00002533
Iteration 27/1000 | Loss: 0.00002526
Iteration 28/1000 | Loss: 0.00002521
Iteration 29/1000 | Loss: 0.00002521
Iteration 30/1000 | Loss: 0.00002521
Iteration 31/1000 | Loss: 0.00002517
Iteration 32/1000 | Loss: 0.00002517
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002515
Iteration 35/1000 | Loss: 0.00002511
Iteration 36/1000 | Loss: 0.00002510
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002510
Iteration 41/1000 | Loss: 0.00002510
Iteration 42/1000 | Loss: 0.00002510
Iteration 43/1000 | Loss: 0.00002509
Iteration 44/1000 | Loss: 0.00002509
Iteration 45/1000 | Loss: 0.00002506
Iteration 46/1000 | Loss: 0.00002506
Iteration 47/1000 | Loss: 0.00002506
Iteration 48/1000 | Loss: 0.00002506
Iteration 49/1000 | Loss: 0.00002506
Iteration 50/1000 | Loss: 0.00002506
Iteration 51/1000 | Loss: 0.00002506
Iteration 52/1000 | Loss: 0.00002506
Iteration 53/1000 | Loss: 0.00002506
Iteration 54/1000 | Loss: 0.00002506
Iteration 55/1000 | Loss: 0.00002506
Iteration 56/1000 | Loss: 0.00002505
Iteration 57/1000 | Loss: 0.00002505
Iteration 58/1000 | Loss: 0.00002505
Iteration 59/1000 | Loss: 0.00002505
Iteration 60/1000 | Loss: 0.00002505
Iteration 61/1000 | Loss: 0.00002505
Iteration 62/1000 | Loss: 0.00002505
Iteration 63/1000 | Loss: 0.00002504
Iteration 64/1000 | Loss: 0.00002504
Iteration 65/1000 | Loss: 0.00002503
Iteration 66/1000 | Loss: 0.00002502
Iteration 67/1000 | Loss: 0.00002502
Iteration 68/1000 | Loss: 0.00002501
Iteration 69/1000 | Loss: 0.00002501
Iteration 70/1000 | Loss: 0.00002501
Iteration 71/1000 | Loss: 0.00002501
Iteration 72/1000 | Loss: 0.00002500
Iteration 73/1000 | Loss: 0.00002500
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002498
Iteration 77/1000 | Loss: 0.00002498
Iteration 78/1000 | Loss: 0.00002498
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002497
Iteration 82/1000 | Loss: 0.00002497
Iteration 83/1000 | Loss: 0.00002497
Iteration 84/1000 | Loss: 0.00002497
Iteration 85/1000 | Loss: 0.00002496
Iteration 86/1000 | Loss: 0.00002496
Iteration 87/1000 | Loss: 0.00002496
Iteration 88/1000 | Loss: 0.00002496
Iteration 89/1000 | Loss: 0.00002496
Iteration 90/1000 | Loss: 0.00002496
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002495
Iteration 94/1000 | Loss: 0.00002495
Iteration 95/1000 | Loss: 0.00002495
Iteration 96/1000 | Loss: 0.00002495
Iteration 97/1000 | Loss: 0.00002495
Iteration 98/1000 | Loss: 0.00002495
Iteration 99/1000 | Loss: 0.00002495
Iteration 100/1000 | Loss: 0.00002495
Iteration 101/1000 | Loss: 0.00002495
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002494
Iteration 106/1000 | Loss: 0.00002494
Iteration 107/1000 | Loss: 0.00002494
Iteration 108/1000 | Loss: 0.00002494
Iteration 109/1000 | Loss: 0.00002494
Iteration 110/1000 | Loss: 0.00002494
Iteration 111/1000 | Loss: 0.00002494
Iteration 112/1000 | Loss: 0.00002493
Iteration 113/1000 | Loss: 0.00002493
Iteration 114/1000 | Loss: 0.00002493
Iteration 115/1000 | Loss: 0.00002493
Iteration 116/1000 | Loss: 0.00002493
Iteration 117/1000 | Loss: 0.00002493
Iteration 118/1000 | Loss: 0.00002493
Iteration 119/1000 | Loss: 0.00002493
Iteration 120/1000 | Loss: 0.00002493
Iteration 121/1000 | Loss: 0.00002493
Iteration 122/1000 | Loss: 0.00002493
Iteration 123/1000 | Loss: 0.00002493
Iteration 124/1000 | Loss: 0.00002493
Iteration 125/1000 | Loss: 0.00002493
Iteration 126/1000 | Loss: 0.00002493
Iteration 127/1000 | Loss: 0.00002493
Iteration 128/1000 | Loss: 0.00002493
Iteration 129/1000 | Loss: 0.00002493
Iteration 130/1000 | Loss: 0.00002493
Iteration 131/1000 | Loss: 0.00002493
Iteration 132/1000 | Loss: 0.00002493
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.4929373466875404e-05, 2.4929373466875404e-05, 2.4929373466875404e-05, 2.4929373466875404e-05, 2.4929373466875404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4929373466875404e-05

Optimization complete. Final v2v error: 4.126003742218018 mm

Highest mean error: 4.445782661437988 mm for frame 122

Lowest mean error: 3.7519967555999756 mm for frame 85

Saving results

Total time: 678.5778996944427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1067
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00738068
Iteration 2/25 | Loss: 0.00142609
Iteration 3/25 | Loss: 0.00107553
Iteration 4/25 | Loss: 0.00103413
Iteration 5/25 | Loss: 0.00102490
Iteration 6/25 | Loss: 0.00101706
Iteration 7/25 | Loss: 0.00101240
Iteration 8/25 | Loss: 0.00100839
Iteration 9/25 | Loss: 0.00100586
Iteration 10/25 | Loss: 0.00100508
Iteration 11/25 | Loss: 0.00100460
Iteration 12/25 | Loss: 0.00100639
Iteration 13/25 | Loss: 0.00100304
Iteration 14/25 | Loss: 0.00100186
Iteration 15/25 | Loss: 0.00100149
Iteration 16/25 | Loss: 0.00100137
Iteration 17/25 | Loss: 0.00100134
Iteration 18/25 | Loss: 0.00100134
Iteration 19/25 | Loss: 0.00100134
Iteration 20/25 | Loss: 0.00100133
Iteration 21/25 | Loss: 0.00100133
Iteration 22/25 | Loss: 0.00100133
Iteration 23/25 | Loss: 0.00100132
Iteration 24/25 | Loss: 0.00100132
Iteration 25/25 | Loss: 0.00100132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34259295
Iteration 2/25 | Loss: 0.00046520
Iteration 3/25 | Loss: 0.00045688
Iteration 4/25 | Loss: 0.00045688
Iteration 5/25 | Loss: 0.00045688
Iteration 6/25 | Loss: 0.00045688
Iteration 7/25 | Loss: 0.00045688
Iteration 8/25 | Loss: 0.00045688
Iteration 9/25 | Loss: 0.00045688
Iteration 10/25 | Loss: 0.00045688
Iteration 11/25 | Loss: 0.00045688
Iteration 12/25 | Loss: 0.00045688
Iteration 13/25 | Loss: 0.00045688
Iteration 14/25 | Loss: 0.00045688
Iteration 15/25 | Loss: 0.00045688
Iteration 16/25 | Loss: 0.00045688
Iteration 17/25 | Loss: 0.00045688
Iteration 18/25 | Loss: 0.00045688
Iteration 19/25 | Loss: 0.00045688
Iteration 20/25 | Loss: 0.00045688
Iteration 21/25 | Loss: 0.00045688
Iteration 22/25 | Loss: 0.00045688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00045688083628192544, 0.00045688083628192544, 0.00045688083628192544, 0.00045688083628192544, 0.00045688083628192544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045688083628192544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045688
Iteration 2/1000 | Loss: 0.00004671
Iteration 3/1000 | Loss: 0.00002374
Iteration 4/1000 | Loss: 0.00002140
Iteration 5/1000 | Loss: 0.00002035
Iteration 6/1000 | Loss: 0.00001975
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001874
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001853
Iteration 13/1000 | Loss: 0.00001847
Iteration 14/1000 | Loss: 0.00001838
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001814
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001808
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001808
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001800
Iteration 30/1000 | Loss: 0.00001798
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001795
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001791
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001791
Iteration 45/1000 | Loss: 0.00001790
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001786
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001770
Iteration 50/1000 | Loss: 0.00001770
Iteration 51/1000 | Loss: 0.00001765
Iteration 52/1000 | Loss: 0.00001765
Iteration 53/1000 | Loss: 0.00001765
Iteration 54/1000 | Loss: 0.00001765
Iteration 55/1000 | Loss: 0.00001765
Iteration 56/1000 | Loss: 0.00001765
Iteration 57/1000 | Loss: 0.00001764
Iteration 58/1000 | Loss: 0.00001763
Iteration 59/1000 | Loss: 0.00001762
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001758
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00001756
Iteration 82/1000 | Loss: 0.00001756
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00001755
Iteration 86/1000 | Loss: 0.00001755
Iteration 87/1000 | Loss: 0.00001754
Iteration 88/1000 | Loss: 0.00001754
Iteration 89/1000 | Loss: 0.00001754
Iteration 90/1000 | Loss: 0.00001754
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001748
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00001748
Iteration 136/1000 | Loss: 0.00001748
Iteration 137/1000 | Loss: 0.00001748
Iteration 138/1000 | Loss: 0.00001748
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001748
Iteration 147/1000 | Loss: 0.00001748
Iteration 148/1000 | Loss: 0.00001748
Iteration 149/1000 | Loss: 0.00001748
Iteration 150/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.7478796507930383e-05, 1.7478796507930383e-05, 1.7478796507930383e-05, 1.7478796507930383e-05, 1.7478796507930383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7478796507930383e-05

Optimization complete. Final v2v error: 3.329550266265869 mm

Highest mean error: 3.872190237045288 mm for frame 122

Lowest mean error: 2.6806514263153076 mm for frame 81

Saving results

Total time: 2438.1258573532104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1080
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01236738
Iteration 2/25 | Loss: 0.00362317
Iteration 3/25 | Loss: 0.00228449
Iteration 4/25 | Loss: 0.00201588
Iteration 5/25 | Loss: 0.00187218
Iteration 6/25 | Loss: 0.00158015
Iteration 7/25 | Loss: 0.00153912
Iteration 8/25 | Loss: 0.00142690
Iteration 9/25 | Loss: 0.00137147
Iteration 10/25 | Loss: 0.00135950
Iteration 11/25 | Loss: 0.00135069
Iteration 12/25 | Loss: 0.00134961
Iteration 13/25 | Loss: 0.00132821
Iteration 14/25 | Loss: 0.00132146
Iteration 15/25 | Loss: 0.00131236
Iteration 16/25 | Loss: 0.00130359
Iteration 17/25 | Loss: 0.00130292
Iteration 18/25 | Loss: 0.00130265
Iteration 19/25 | Loss: 0.00130232
Iteration 20/25 | Loss: 0.00130081
Iteration 21/25 | Loss: 0.00129942
Iteration 22/25 | Loss: 0.00129748
Iteration 23/25 | Loss: 0.00129283
Iteration 24/25 | Loss: 0.00129238
Iteration 25/25 | Loss: 0.00129009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64879161
Iteration 2/25 | Loss: 0.00102277
Iteration 3/25 | Loss: 0.00102276
Iteration 4/25 | Loss: 0.00102276
Iteration 5/25 | Loss: 0.00102276
Iteration 6/25 | Loss: 0.00102276
Iteration 7/25 | Loss: 0.00102276
Iteration 8/25 | Loss: 0.00102276
Iteration 9/25 | Loss: 0.00102276
Iteration 10/25 | Loss: 0.00102276
Iteration 11/25 | Loss: 0.00102276
Iteration 12/25 | Loss: 0.00102276
Iteration 13/25 | Loss: 0.00102276
Iteration 14/25 | Loss: 0.00102276
Iteration 15/25 | Loss: 0.00102276
Iteration 16/25 | Loss: 0.00102276
Iteration 17/25 | Loss: 0.00102276
Iteration 18/25 | Loss: 0.00102276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001022758544422686, 0.001022758544422686, 0.001022758544422686, 0.001022758544422686, 0.001022758544422686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001022758544422686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102276
Iteration 2/1000 | Loss: 0.00021221
Iteration 3/1000 | Loss: 0.00009972
Iteration 4/1000 | Loss: 0.00024660
Iteration 5/1000 | Loss: 0.00007736
Iteration 6/1000 | Loss: 0.00014736
Iteration 7/1000 | Loss: 0.00014388
Iteration 8/1000 | Loss: 0.00017872
Iteration 9/1000 | Loss: 0.00007144
Iteration 10/1000 | Loss: 0.00005728
Iteration 11/1000 | Loss: 0.00005284
Iteration 12/1000 | Loss: 0.00005008
Iteration 13/1000 | Loss: 0.00004724
Iteration 14/1000 | Loss: 0.00004517
Iteration 15/1000 | Loss: 0.00004423
Iteration 16/1000 | Loss: 0.00004325
Iteration 17/1000 | Loss: 0.00004264
Iteration 18/1000 | Loss: 0.00004218
Iteration 19/1000 | Loss: 0.00004173
Iteration 20/1000 | Loss: 0.00004126
Iteration 21/1000 | Loss: 0.00004092
Iteration 22/1000 | Loss: 0.00004076
Iteration 23/1000 | Loss: 0.00004060
Iteration 24/1000 | Loss: 0.00004058
Iteration 25/1000 | Loss: 0.00004045
Iteration 26/1000 | Loss: 0.00004039
Iteration 27/1000 | Loss: 0.00004033
Iteration 28/1000 | Loss: 0.00004033
Iteration 29/1000 | Loss: 0.00004029
Iteration 30/1000 | Loss: 0.00004021
Iteration 31/1000 | Loss: 0.00004018
Iteration 32/1000 | Loss: 0.00004016
Iteration 33/1000 | Loss: 0.00004016
Iteration 34/1000 | Loss: 0.00004016
Iteration 35/1000 | Loss: 0.00004015
Iteration 36/1000 | Loss: 0.00004014
Iteration 37/1000 | Loss: 0.00004009
Iteration 38/1000 | Loss: 0.00004006
Iteration 39/1000 | Loss: 0.00004003
Iteration 40/1000 | Loss: 0.00004002
Iteration 41/1000 | Loss: 0.00004002
Iteration 42/1000 | Loss: 0.00004002
Iteration 43/1000 | Loss: 0.00004002
Iteration 44/1000 | Loss: 0.00004002
Iteration 45/1000 | Loss: 0.00004002
Iteration 46/1000 | Loss: 0.00004001
Iteration 47/1000 | Loss: 0.00004001
Iteration 48/1000 | Loss: 0.00004001
Iteration 49/1000 | Loss: 0.00004000
Iteration 50/1000 | Loss: 0.00004000
Iteration 51/1000 | Loss: 0.00003999
Iteration 52/1000 | Loss: 0.00003999
Iteration 53/1000 | Loss: 0.00003999
Iteration 54/1000 | Loss: 0.00003999
Iteration 55/1000 | Loss: 0.00003999
Iteration 56/1000 | Loss: 0.00003998
Iteration 57/1000 | Loss: 0.00003998
Iteration 58/1000 | Loss: 0.00003998
Iteration 59/1000 | Loss: 0.00003998
Iteration 60/1000 | Loss: 0.00003998
Iteration 61/1000 | Loss: 0.00003998
Iteration 62/1000 | Loss: 0.00003998
Iteration 63/1000 | Loss: 0.00003998
Iteration 64/1000 | Loss: 0.00003998
Iteration 65/1000 | Loss: 0.00003998
Iteration 66/1000 | Loss: 0.00003998
Iteration 67/1000 | Loss: 0.00003998
Iteration 68/1000 | Loss: 0.00003998
Iteration 69/1000 | Loss: 0.00003997
Iteration 70/1000 | Loss: 0.00003997
Iteration 71/1000 | Loss: 0.00003997
Iteration 72/1000 | Loss: 0.00003997
Iteration 73/1000 | Loss: 0.00003997
Iteration 74/1000 | Loss: 0.00003997
Iteration 75/1000 | Loss: 0.00003997
Iteration 76/1000 | Loss: 0.00003996
Iteration 77/1000 | Loss: 0.00003996
Iteration 78/1000 | Loss: 0.00003996
Iteration 79/1000 | Loss: 0.00003995
Iteration 80/1000 | Loss: 0.00003995
Iteration 81/1000 | Loss: 0.00003995
Iteration 82/1000 | Loss: 0.00003994
Iteration 83/1000 | Loss: 0.00003994
Iteration 84/1000 | Loss: 0.00003994
Iteration 85/1000 | Loss: 0.00003994
Iteration 86/1000 | Loss: 0.00003993
Iteration 87/1000 | Loss: 0.00003993
Iteration 88/1000 | Loss: 0.00003993
Iteration 89/1000 | Loss: 0.00003993
Iteration 90/1000 | Loss: 0.00003993
Iteration 91/1000 | Loss: 0.00003993
Iteration 92/1000 | Loss: 0.00003993
Iteration 93/1000 | Loss: 0.00003992
Iteration 94/1000 | Loss: 0.00003992
Iteration 95/1000 | Loss: 0.00003992
Iteration 96/1000 | Loss: 0.00003992
Iteration 97/1000 | Loss: 0.00003992
Iteration 98/1000 | Loss: 0.00003992
Iteration 99/1000 | Loss: 0.00003992
Iteration 100/1000 | Loss: 0.00003991
Iteration 101/1000 | Loss: 0.00003991
Iteration 102/1000 | Loss: 0.00003991
Iteration 103/1000 | Loss: 0.00003991
Iteration 104/1000 | Loss: 0.00003991
Iteration 105/1000 | Loss: 0.00003991
Iteration 106/1000 | Loss: 0.00003991
Iteration 107/1000 | Loss: 0.00003991
Iteration 108/1000 | Loss: 0.00003991
Iteration 109/1000 | Loss: 0.00003991
Iteration 110/1000 | Loss: 0.00003990
Iteration 111/1000 | Loss: 0.00003990
Iteration 112/1000 | Loss: 0.00003990
Iteration 113/1000 | Loss: 0.00003990
Iteration 114/1000 | Loss: 0.00003990
Iteration 115/1000 | Loss: 0.00003989
Iteration 116/1000 | Loss: 0.00003989
Iteration 117/1000 | Loss: 0.00003989
Iteration 118/1000 | Loss: 0.00003989
Iteration 119/1000 | Loss: 0.00003989
Iteration 120/1000 | Loss: 0.00003989
Iteration 121/1000 | Loss: 0.00003989
Iteration 122/1000 | Loss: 0.00003989
Iteration 123/1000 | Loss: 0.00003989
Iteration 124/1000 | Loss: 0.00003989
Iteration 125/1000 | Loss: 0.00003989
Iteration 126/1000 | Loss: 0.00003988
Iteration 127/1000 | Loss: 0.00003988
Iteration 128/1000 | Loss: 0.00003988
Iteration 129/1000 | Loss: 0.00003988
Iteration 130/1000 | Loss: 0.00003988
Iteration 131/1000 | Loss: 0.00003988
Iteration 132/1000 | Loss: 0.00003988
Iteration 133/1000 | Loss: 0.00003988
Iteration 134/1000 | Loss: 0.00003988
Iteration 135/1000 | Loss: 0.00003987
Iteration 136/1000 | Loss: 0.00003987
Iteration 137/1000 | Loss: 0.00003987
Iteration 138/1000 | Loss: 0.00003987
Iteration 139/1000 | Loss: 0.00003987
Iteration 140/1000 | Loss: 0.00003987
Iteration 141/1000 | Loss: 0.00003987
Iteration 142/1000 | Loss: 0.00003987
Iteration 143/1000 | Loss: 0.00003987
Iteration 144/1000 | Loss: 0.00003987
Iteration 145/1000 | Loss: 0.00003987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.9868944440968335e-05, 3.9868944440968335e-05, 3.9868944440968335e-05, 3.9868944440968335e-05, 3.9868944440968335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9868944440968335e-05

Optimization complete. Final v2v error: 5.262807369232178 mm

Highest mean error: 6.034434795379639 mm for frame 4

Lowest mean error: 4.657581806182861 mm for frame 20

Saving results

Total time: 1668.37841963768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1071
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00669392
Iteration 2/25 | Loss: 0.00135135
Iteration 3/25 | Loss: 0.00105337
Iteration 4/25 | Loss: 0.00099138
Iteration 5/25 | Loss: 0.00098025
Iteration 6/25 | Loss: 0.00097423
Iteration 7/25 | Loss: 0.00097809
Iteration 8/25 | Loss: 0.00097216
Iteration 9/25 | Loss: 0.00096620
Iteration 10/25 | Loss: 0.00096164
Iteration 11/25 | Loss: 0.00095939
Iteration 12/25 | Loss: 0.00095847
Iteration 13/25 | Loss: 0.00095738
Iteration 14/25 | Loss: 0.00095692
Iteration 15/25 | Loss: 0.00095677
Iteration 16/25 | Loss: 0.00095674
Iteration 17/25 | Loss: 0.00095674
Iteration 18/25 | Loss: 0.00095674
Iteration 19/25 | Loss: 0.00095674
Iteration 20/25 | Loss: 0.00095674
Iteration 21/25 | Loss: 0.00095674
Iteration 22/25 | Loss: 0.00095673
Iteration 23/25 | Loss: 0.00095673
Iteration 24/25 | Loss: 0.00095673
Iteration 25/25 | Loss: 0.00095673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70789456
Iteration 2/25 | Loss: 0.00058437
Iteration 3/25 | Loss: 0.00058437
Iteration 4/25 | Loss: 0.00058437
Iteration 5/25 | Loss: 0.00058437
Iteration 6/25 | Loss: 0.00058437
Iteration 7/25 | Loss: 0.00058437
Iteration 8/25 | Loss: 0.00058437
Iteration 9/25 | Loss: 0.00058437
Iteration 10/25 | Loss: 0.00058437
Iteration 11/25 | Loss: 0.00058437
Iteration 12/25 | Loss: 0.00058437
Iteration 13/25 | Loss: 0.00058436
Iteration 14/25 | Loss: 0.00058436
Iteration 15/25 | Loss: 0.00058436
Iteration 16/25 | Loss: 0.00058436
Iteration 17/25 | Loss: 0.00058436
Iteration 18/25 | Loss: 0.00058436
Iteration 19/25 | Loss: 0.00058436
Iteration 20/25 | Loss: 0.00058436
Iteration 21/25 | Loss: 0.00058436
Iteration 22/25 | Loss: 0.00058436
Iteration 23/25 | Loss: 0.00058436
Iteration 24/25 | Loss: 0.00058436
Iteration 25/25 | Loss: 0.00058436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058436
Iteration 2/1000 | Loss: 0.00002539
Iteration 3/1000 | Loss: 0.00001555
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001381
Iteration 6/1000 | Loss: 0.00001195
Iteration 7/1000 | Loss: 0.00001155
Iteration 8/1000 | Loss: 0.00001909
Iteration 9/1000 | Loss: 0.00001226
Iteration 10/1000 | Loss: 0.00001112
Iteration 11/1000 | Loss: 0.00001103
Iteration 12/1000 | Loss: 0.00001103
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001178
Iteration 16/1000 | Loss: 0.00001086
Iteration 17/1000 | Loss: 0.00001085
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001085
Iteration 20/1000 | Loss: 0.00001085
Iteration 21/1000 | Loss: 0.00001084
Iteration 22/1000 | Loss: 0.00001084
Iteration 23/1000 | Loss: 0.00001084
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001083
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001082
Iteration 33/1000 | Loss: 0.00001082
Iteration 34/1000 | Loss: 0.00001075
Iteration 35/1000 | Loss: 0.00001075
Iteration 36/1000 | Loss: 0.00001074
Iteration 37/1000 | Loss: 0.00001074
Iteration 38/1000 | Loss: 0.00001073
Iteration 39/1000 | Loss: 0.00001072
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001070
Iteration 44/1000 | Loss: 0.00001070
Iteration 45/1000 | Loss: 0.00001069
Iteration 46/1000 | Loss: 0.00001069
Iteration 47/1000 | Loss: 0.00001069
Iteration 48/1000 | Loss: 0.00001068
Iteration 49/1000 | Loss: 0.00001068
Iteration 50/1000 | Loss: 0.00001068
Iteration 51/1000 | Loss: 0.00001068
Iteration 52/1000 | Loss: 0.00001068
Iteration 53/1000 | Loss: 0.00001068
Iteration 54/1000 | Loss: 0.00001067
Iteration 55/1000 | Loss: 0.00001067
Iteration 56/1000 | Loss: 0.00001067
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001065
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001064
Iteration 71/1000 | Loss: 0.00001064
Iteration 72/1000 | Loss: 0.00001064
Iteration 73/1000 | Loss: 0.00001064
Iteration 74/1000 | Loss: 0.00001064
Iteration 75/1000 | Loss: 0.00001064
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001060
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001058
Iteration 98/1000 | Loss: 0.00001058
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001057
Iteration 101/1000 | Loss: 0.00001057
Iteration 102/1000 | Loss: 0.00001057
Iteration 103/1000 | Loss: 0.00001057
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001053
Iteration 123/1000 | Loss: 0.00001053
Iteration 124/1000 | Loss: 0.00001053
Iteration 125/1000 | Loss: 0.00001053
Iteration 126/1000 | Loss: 0.00001053
Iteration 127/1000 | Loss: 0.00001053
Iteration 128/1000 | Loss: 0.00001053
Iteration 129/1000 | Loss: 0.00001053
Iteration 130/1000 | Loss: 0.00001053
Iteration 131/1000 | Loss: 0.00001053
Iteration 132/1000 | Loss: 0.00001053
Iteration 133/1000 | Loss: 0.00001053
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001052
Iteration 136/1000 | Loss: 0.00001052
Iteration 137/1000 | Loss: 0.00001052
Iteration 138/1000 | Loss: 0.00001052
Iteration 139/1000 | Loss: 0.00001052
Iteration 140/1000 | Loss: 0.00001052
Iteration 141/1000 | Loss: 0.00001052
Iteration 142/1000 | Loss: 0.00001052
Iteration 143/1000 | Loss: 0.00001052
Iteration 144/1000 | Loss: 0.00001052
Iteration 145/1000 | Loss: 0.00001052
Iteration 146/1000 | Loss: 0.00001052
Iteration 147/1000 | Loss: 0.00001051
Iteration 148/1000 | Loss: 0.00001051
Iteration 149/1000 | Loss: 0.00001051
Iteration 150/1000 | Loss: 0.00001051
Iteration 151/1000 | Loss: 0.00001051
Iteration 152/1000 | Loss: 0.00001051
Iteration 153/1000 | Loss: 0.00001051
Iteration 154/1000 | Loss: 0.00001051
Iteration 155/1000 | Loss: 0.00001051
Iteration 156/1000 | Loss: 0.00001051
Iteration 157/1000 | Loss: 0.00001051
Iteration 158/1000 | Loss: 0.00001051
Iteration 159/1000 | Loss: 0.00001051
Iteration 160/1000 | Loss: 0.00001051
Iteration 161/1000 | Loss: 0.00001050
Iteration 162/1000 | Loss: 0.00001050
Iteration 163/1000 | Loss: 0.00001050
Iteration 164/1000 | Loss: 0.00001050
Iteration 165/1000 | Loss: 0.00001050
Iteration 166/1000 | Loss: 0.00001050
Iteration 167/1000 | Loss: 0.00001050
Iteration 168/1000 | Loss: 0.00001050
Iteration 169/1000 | Loss: 0.00001050
Iteration 170/1000 | Loss: 0.00001050
Iteration 171/1000 | Loss: 0.00001050
Iteration 172/1000 | Loss: 0.00001050
Iteration 173/1000 | Loss: 0.00001050
Iteration 174/1000 | Loss: 0.00001050
Iteration 175/1000 | Loss: 0.00001050
Iteration 176/1000 | Loss: 0.00001050
Iteration 177/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.0501187716727145e-05, 1.0501187716727145e-05, 1.0501187716727145e-05, 1.0501187716727145e-05, 1.0501187716727145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0501187716727145e-05

Optimization complete. Final v2v error: 2.7050395011901855 mm

Highest mean error: 3.7280354499816895 mm for frame 74

Lowest mean error: 2.331247329711914 mm for frame 49

Saving results

Total time: 2160.4385414123535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1054
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833074
Iteration 2/25 | Loss: 0.00111972
Iteration 3/25 | Loss: 0.00099569
Iteration 4/25 | Loss: 0.00097611
Iteration 5/25 | Loss: 0.00096940
Iteration 6/25 | Loss: 0.00096811
Iteration 7/25 | Loss: 0.00096801
Iteration 8/25 | Loss: 0.00096801
Iteration 9/25 | Loss: 0.00096801
Iteration 10/25 | Loss: 0.00096801
Iteration 11/25 | Loss: 0.00096801
Iteration 12/25 | Loss: 0.00096801
Iteration 13/25 | Loss: 0.00096801
Iteration 14/25 | Loss: 0.00096801
Iteration 15/25 | Loss: 0.00096801
Iteration 16/25 | Loss: 0.00096801
Iteration 17/25 | Loss: 0.00096801
Iteration 18/25 | Loss: 0.00096801
Iteration 19/25 | Loss: 0.00096801
Iteration 20/25 | Loss: 0.00096801
Iteration 21/25 | Loss: 0.00096801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009680145303718746, 0.0009680145303718746, 0.0009680145303718746, 0.0009680145303718746, 0.0009680145303718746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009680145303718746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29325652
Iteration 2/25 | Loss: 0.00080098
Iteration 3/25 | Loss: 0.00080097
Iteration 4/25 | Loss: 0.00080097
Iteration 5/25 | Loss: 0.00080097
Iteration 6/25 | Loss: 0.00080097
Iteration 7/25 | Loss: 0.00080097
Iteration 8/25 | Loss: 0.00080097
Iteration 9/25 | Loss: 0.00080097
Iteration 10/25 | Loss: 0.00080097
Iteration 11/25 | Loss: 0.00080097
Iteration 12/25 | Loss: 0.00080097
Iteration 13/25 | Loss: 0.00080097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008009722805581987, 0.0008009722805581987, 0.0008009722805581987, 0.0008009722805581987, 0.0008009722805581987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008009722805581987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080097
Iteration 2/1000 | Loss: 0.00003136
Iteration 3/1000 | Loss: 0.00001822
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001323
Iteration 6/1000 | Loss: 0.00001271
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001189
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001151
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001147
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001146
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001145
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001144
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001143
Iteration 34/1000 | Loss: 0.00001143
Iteration 35/1000 | Loss: 0.00001142
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001141
Iteration 38/1000 | Loss: 0.00001141
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001137
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001135
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001133
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001129
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001129
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001127
Iteration 106/1000 | Loss: 0.00001127
Iteration 107/1000 | Loss: 0.00001127
Iteration 108/1000 | Loss: 0.00001127
Iteration 109/1000 | Loss: 0.00001127
Iteration 110/1000 | Loss: 0.00001127
Iteration 111/1000 | Loss: 0.00001127
Iteration 112/1000 | Loss: 0.00001127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.1267758054600563e-05, 1.1267758054600563e-05, 1.1267758054600563e-05, 1.1267758054600563e-05, 1.1267758054600563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1267758054600563e-05

Optimization complete. Final v2v error: 2.899747610092163 mm

Highest mean error: 3.450727939605713 mm for frame 239

Lowest mean error: 2.615182876586914 mm for frame 41

Saving results

Total time: 1131.3030529022217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1087
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065844
Iteration 2/25 | Loss: 0.00172024
Iteration 3/25 | Loss: 0.00129826
Iteration 4/25 | Loss: 0.00128084
Iteration 5/25 | Loss: 0.00122930
Iteration 6/25 | Loss: 0.00121081
Iteration 7/25 | Loss: 0.00120062
Iteration 8/25 | Loss: 0.00118577
Iteration 9/25 | Loss: 0.00118775
Iteration 10/25 | Loss: 0.00118709
Iteration 11/25 | Loss: 0.00117892
Iteration 12/25 | Loss: 0.00117836
Iteration 13/25 | Loss: 0.00117677
Iteration 14/25 | Loss: 0.00118623
Iteration 15/25 | Loss: 0.00120863
Iteration 16/25 | Loss: 0.00122022
Iteration 17/25 | Loss: 0.00121041
Iteration 18/25 | Loss: 0.00117554
Iteration 19/25 | Loss: 0.00114715
Iteration 20/25 | Loss: 0.00113467
Iteration 21/25 | Loss: 0.00112292
Iteration 22/25 | Loss: 0.00112669
Iteration 23/25 | Loss: 0.00111971
Iteration 24/25 | Loss: 0.00111441
Iteration 25/25 | Loss: 0.00111454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94938397
Iteration 2/25 | Loss: 0.00124807
Iteration 3/25 | Loss: 0.00124807
Iteration 4/25 | Loss: 0.00124807
Iteration 5/25 | Loss: 0.00124807
Iteration 6/25 | Loss: 0.00124807
Iteration 7/25 | Loss: 0.00124807
Iteration 8/25 | Loss: 0.00124807
Iteration 9/25 | Loss: 0.00124807
Iteration 10/25 | Loss: 0.00124807
Iteration 11/25 | Loss: 0.00124807
Iteration 12/25 | Loss: 0.00124807
Iteration 13/25 | Loss: 0.00124807
Iteration 14/25 | Loss: 0.00124807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012480715522542596, 0.0012480715522542596, 0.0012480715522542596, 0.0012480715522542596, 0.0012480715522542596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012480715522542596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124807
Iteration 2/1000 | Loss: 0.00047841
Iteration 3/1000 | Loss: 0.00025877
Iteration 4/1000 | Loss: 0.00030739
Iteration 5/1000 | Loss: 0.00124419
Iteration 6/1000 | Loss: 0.00057635
Iteration 7/1000 | Loss: 0.00031623
Iteration 8/1000 | Loss: 0.00030528
Iteration 9/1000 | Loss: 0.00018647
Iteration 10/1000 | Loss: 0.00015345
Iteration 11/1000 | Loss: 0.00023497
Iteration 12/1000 | Loss: 0.00029835
Iteration 13/1000 | Loss: 0.00027499
Iteration 14/1000 | Loss: 0.00027398
Iteration 15/1000 | Loss: 0.00079043
Iteration 16/1000 | Loss: 0.00033458
Iteration 17/1000 | Loss: 0.00039747
Iteration 18/1000 | Loss: 0.00032450
Iteration 19/1000 | Loss: 0.00025938
Iteration 20/1000 | Loss: 0.00042393
Iteration 21/1000 | Loss: 0.00040237
Iteration 22/1000 | Loss: 0.00035056
Iteration 23/1000 | Loss: 0.00033385
Iteration 24/1000 | Loss: 0.00029628
Iteration 25/1000 | Loss: 0.00024313
Iteration 26/1000 | Loss: 0.00025847
Iteration 27/1000 | Loss: 0.00031342
Iteration 28/1000 | Loss: 0.00043943
Iteration 29/1000 | Loss: 0.00040442
Iteration 30/1000 | Loss: 0.00037273
Iteration 31/1000 | Loss: 0.00037306
Iteration 32/1000 | Loss: 0.00042090
Iteration 33/1000 | Loss: 0.00045838
Iteration 34/1000 | Loss: 0.00039702
Iteration 35/1000 | Loss: 0.00018213
Iteration 36/1000 | Loss: 0.00046154
Iteration 37/1000 | Loss: 0.00026333
Iteration 38/1000 | Loss: 0.00035572
Iteration 39/1000 | Loss: 0.00030202
Iteration 40/1000 | Loss: 0.00030637
Iteration 41/1000 | Loss: 0.00028100
Iteration 42/1000 | Loss: 0.00025933
Iteration 43/1000 | Loss: 0.00032230
Iteration 44/1000 | Loss: 0.00037846
Iteration 45/1000 | Loss: 0.00035700
Iteration 46/1000 | Loss: 0.00033442
Iteration 47/1000 | Loss: 0.00032887
Iteration 48/1000 | Loss: 0.00029477
Iteration 49/1000 | Loss: 0.00043927
Iteration 50/1000 | Loss: 0.00045330
Iteration 51/1000 | Loss: 0.00034299
Iteration 52/1000 | Loss: 0.00030608
Iteration 53/1000 | Loss: 0.00027407
Iteration 54/1000 | Loss: 0.00025651
Iteration 55/1000 | Loss: 0.00026939
Iteration 56/1000 | Loss: 0.00035501
Iteration 57/1000 | Loss: 0.00027963
Iteration 58/1000 | Loss: 0.00016940
Iteration 59/1000 | Loss: 0.00024815
Iteration 60/1000 | Loss: 0.00016582
Iteration 61/1000 | Loss: 0.00012687
Iteration 62/1000 | Loss: 0.00020236
Iteration 63/1000 | Loss: 0.00021002
Iteration 64/1000 | Loss: 0.00012675
Iteration 65/1000 | Loss: 0.00023556
Iteration 66/1000 | Loss: 0.00038209
Iteration 67/1000 | Loss: 0.00046544
Iteration 68/1000 | Loss: 0.00035891
Iteration 69/1000 | Loss: 0.00028887
Iteration 70/1000 | Loss: 0.00023603
Iteration 71/1000 | Loss: 0.00033335
Iteration 72/1000 | Loss: 0.00027946
Iteration 73/1000 | Loss: 0.00028746
Iteration 74/1000 | Loss: 0.00028886
Iteration 75/1000 | Loss: 0.00027460
Iteration 76/1000 | Loss: 0.00011828
Iteration 77/1000 | Loss: 0.00095737
Iteration 78/1000 | Loss: 0.00032762
Iteration 79/1000 | Loss: 0.00024351
Iteration 80/1000 | Loss: 0.00008442
Iteration 81/1000 | Loss: 0.00053587
Iteration 82/1000 | Loss: 0.00012092
Iteration 83/1000 | Loss: 0.00012783
Iteration 84/1000 | Loss: 0.00012759
Iteration 85/1000 | Loss: 0.00006085
Iteration 86/1000 | Loss: 0.00019058
Iteration 87/1000 | Loss: 0.00022113
Iteration 88/1000 | Loss: 0.00018270
Iteration 89/1000 | Loss: 0.00009676
Iteration 90/1000 | Loss: 0.00007211
Iteration 91/1000 | Loss: 0.00006863
Iteration 92/1000 | Loss: 0.00005752
Iteration 93/1000 | Loss: 0.00006082
Iteration 94/1000 | Loss: 0.00021595
Iteration 95/1000 | Loss: 0.00014493
Iteration 96/1000 | Loss: 0.00012854
Iteration 97/1000 | Loss: 0.00004828
Iteration 98/1000 | Loss: 0.00006139
Iteration 99/1000 | Loss: 0.00004270
Iteration 100/1000 | Loss: 0.00004953
Iteration 101/1000 | Loss: 0.00003807
Iteration 102/1000 | Loss: 0.00014066
Iteration 103/1000 | Loss: 0.00122987
Iteration 104/1000 | Loss: 0.00030886
Iteration 105/1000 | Loss: 0.00022055
Iteration 106/1000 | Loss: 0.00040892
Iteration 107/1000 | Loss: 0.00021207
Iteration 108/1000 | Loss: 0.00017383
Iteration 109/1000 | Loss: 0.00005280
Iteration 110/1000 | Loss: 0.00005175
Iteration 111/1000 | Loss: 0.00004697
Iteration 112/1000 | Loss: 0.00005253
Iteration 113/1000 | Loss: 0.00003007
Iteration 114/1000 | Loss: 0.00026718
Iteration 115/1000 | Loss: 0.00020954
Iteration 116/1000 | Loss: 0.00015519
Iteration 117/1000 | Loss: 0.00019462
Iteration 118/1000 | Loss: 0.00025663
Iteration 119/1000 | Loss: 0.00010809
Iteration 120/1000 | Loss: 0.00015835
Iteration 121/1000 | Loss: 0.00027164
Iteration 122/1000 | Loss: 0.00013632
Iteration 123/1000 | Loss: 0.00018220
Iteration 124/1000 | Loss: 0.00028385
Iteration 125/1000 | Loss: 0.00022277
Iteration 126/1000 | Loss: 0.00025556
Iteration 127/1000 | Loss: 0.00032577
Iteration 128/1000 | Loss: 0.00027844
Iteration 129/1000 | Loss: 0.00011398
Iteration 130/1000 | Loss: 0.00010505
Iteration 131/1000 | Loss: 0.00009793
Iteration 132/1000 | Loss: 0.00018065
Iteration 133/1000 | Loss: 0.00035369
Iteration 134/1000 | Loss: 0.00024724
Iteration 135/1000 | Loss: 0.00023456
Iteration 136/1000 | Loss: 0.00013498
Iteration 137/1000 | Loss: 0.00012084
Iteration 138/1000 | Loss: 0.00012885
Iteration 139/1000 | Loss: 0.00067807
Iteration 140/1000 | Loss: 0.00105425
Iteration 141/1000 | Loss: 0.00044025
Iteration 142/1000 | Loss: 0.00022519
Iteration 143/1000 | Loss: 0.00033106
Iteration 144/1000 | Loss: 0.00023408
Iteration 145/1000 | Loss: 0.00024652
Iteration 146/1000 | Loss: 0.00017140
Iteration 147/1000 | Loss: 0.00127267
Iteration 148/1000 | Loss: 0.00028454
Iteration 149/1000 | Loss: 0.00022622
Iteration 150/1000 | Loss: 0.00018339
Iteration 151/1000 | Loss: 0.00014042
Iteration 152/1000 | Loss: 0.00029275
Iteration 153/1000 | Loss: 0.00018517
Iteration 154/1000 | Loss: 0.00024559
Iteration 155/1000 | Loss: 0.00032651
Iteration 156/1000 | Loss: 0.00016579
Iteration 157/1000 | Loss: 0.00005361
Iteration 158/1000 | Loss: 0.00003394
Iteration 159/1000 | Loss: 0.00007025
Iteration 160/1000 | Loss: 0.00006105
Iteration 161/1000 | Loss: 0.00004640
Iteration 162/1000 | Loss: 0.00029395
Iteration 163/1000 | Loss: 0.00021084
Iteration 164/1000 | Loss: 0.00175525
Iteration 165/1000 | Loss: 0.00023457
Iteration 166/1000 | Loss: 0.00022620
Iteration 167/1000 | Loss: 0.00077860
Iteration 168/1000 | Loss: 0.00012160
Iteration 169/1000 | Loss: 0.00075648
Iteration 170/1000 | Loss: 0.00028413
Iteration 171/1000 | Loss: 0.00019448
Iteration 172/1000 | Loss: 0.00027948
Iteration 173/1000 | Loss: 0.00017481
Iteration 174/1000 | Loss: 0.00025986
Iteration 175/1000 | Loss: 0.00005489
Iteration 176/1000 | Loss: 0.00003569
Iteration 177/1000 | Loss: 0.00009126
Iteration 178/1000 | Loss: 0.00090443
Iteration 179/1000 | Loss: 0.00010931
Iteration 180/1000 | Loss: 0.00015758
Iteration 181/1000 | Loss: 0.00004369
Iteration 182/1000 | Loss: 0.00010837
Iteration 183/1000 | Loss: 0.00008941
Iteration 184/1000 | Loss: 0.00007454
Iteration 185/1000 | Loss: 0.00007608
Iteration 186/1000 | Loss: 0.00008031
Iteration 187/1000 | Loss: 0.00009116
Iteration 188/1000 | Loss: 0.00092416
Iteration 189/1000 | Loss: 0.00005688
Iteration 190/1000 | Loss: 0.00003916
Iteration 191/1000 | Loss: 0.00044083
Iteration 192/1000 | Loss: 0.00015316
Iteration 193/1000 | Loss: 0.00011428
Iteration 194/1000 | Loss: 0.00003557
Iteration 195/1000 | Loss: 0.00003152
Iteration 196/1000 | Loss: 0.00002927
Iteration 197/1000 | Loss: 0.00002853
Iteration 198/1000 | Loss: 0.00002819
Iteration 199/1000 | Loss: 0.00002791
Iteration 200/1000 | Loss: 0.00002790
Iteration 201/1000 | Loss: 0.00002768
Iteration 202/1000 | Loss: 0.00002746
Iteration 203/1000 | Loss: 0.00002725
Iteration 204/1000 | Loss: 0.00002706
Iteration 205/1000 | Loss: 0.00006264
Iteration 206/1000 | Loss: 0.00002933
Iteration 207/1000 | Loss: 0.00011095
Iteration 208/1000 | Loss: 0.00013266
Iteration 209/1000 | Loss: 0.00075824
Iteration 210/1000 | Loss: 0.00066128
Iteration 211/1000 | Loss: 0.00060485
Iteration 212/1000 | Loss: 0.00044165
Iteration 213/1000 | Loss: 0.00067959
Iteration 214/1000 | Loss: 0.00018853
Iteration 215/1000 | Loss: 0.00011296
Iteration 216/1000 | Loss: 0.00005592
Iteration 217/1000 | Loss: 0.00003379
Iteration 218/1000 | Loss: 0.00016403
Iteration 219/1000 | Loss: 0.00008375
Iteration 220/1000 | Loss: 0.00017513
Iteration 221/1000 | Loss: 0.00004486
Iteration 222/1000 | Loss: 0.00016922
Iteration 223/1000 | Loss: 0.00003761
Iteration 224/1000 | Loss: 0.00007541
Iteration 225/1000 | Loss: 0.00003505
Iteration 226/1000 | Loss: 0.00018624
Iteration 227/1000 | Loss: 0.00009371
Iteration 228/1000 | Loss: 0.00007833
Iteration 229/1000 | Loss: 0.00006615
Iteration 230/1000 | Loss: 0.00019799
Iteration 231/1000 | Loss: 0.00140823
Iteration 232/1000 | Loss: 0.00010674
Iteration 233/1000 | Loss: 0.00048544
Iteration 234/1000 | Loss: 0.00020454
Iteration 235/1000 | Loss: 0.00014710
Iteration 236/1000 | Loss: 0.00010067
Iteration 237/1000 | Loss: 0.00048978
Iteration 238/1000 | Loss: 0.00020590
Iteration 239/1000 | Loss: 0.00011775
Iteration 240/1000 | Loss: 0.00009807
Iteration 241/1000 | Loss: 0.00012045
Iteration 242/1000 | Loss: 0.00084358
Iteration 243/1000 | Loss: 0.00022064
Iteration 244/1000 | Loss: 0.00018190
Iteration 245/1000 | Loss: 0.00013697
Iteration 246/1000 | Loss: 0.00010818
Iteration 247/1000 | Loss: 0.00003918
Iteration 248/1000 | Loss: 0.00013613
Iteration 249/1000 | Loss: 0.00067901
Iteration 250/1000 | Loss: 0.00016984
Iteration 251/1000 | Loss: 0.00022590
Iteration 252/1000 | Loss: 0.00015169
Iteration 253/1000 | Loss: 0.00013306
Iteration 254/1000 | Loss: 0.00013764
Iteration 255/1000 | Loss: 0.00031735
Iteration 256/1000 | Loss: 0.00015672
Iteration 257/1000 | Loss: 0.00013862
Iteration 258/1000 | Loss: 0.00023069
Iteration 259/1000 | Loss: 0.00018201
Iteration 260/1000 | Loss: 0.00013791
Iteration 261/1000 | Loss: 0.00015866
Iteration 262/1000 | Loss: 0.00023042
Iteration 263/1000 | Loss: 0.00017908
Iteration 264/1000 | Loss: 0.00016533
Iteration 265/1000 | Loss: 0.00012435
Iteration 266/1000 | Loss: 0.00043797
Iteration 267/1000 | Loss: 0.00019926
Iteration 268/1000 | Loss: 0.00017786
Iteration 269/1000 | Loss: 0.00012157
Iteration 270/1000 | Loss: 0.00020244
Iteration 271/1000 | Loss: 0.00017223
Iteration 272/1000 | Loss: 0.00017891
Iteration 273/1000 | Loss: 0.00012365
Iteration 274/1000 | Loss: 0.00008579
Iteration 275/1000 | Loss: 0.00015903
Iteration 276/1000 | Loss: 0.00018139
Iteration 277/1000 | Loss: 0.00018147
Iteration 278/1000 | Loss: 0.00015873
Iteration 279/1000 | Loss: 0.00017570
Iteration 280/1000 | Loss: 0.00011653
Iteration 281/1000 | Loss: 0.00015638
Iteration 282/1000 | Loss: 0.00004712
Iteration 283/1000 | Loss: 0.00014420
Iteration 284/1000 | Loss: 0.00016009
Iteration 285/1000 | Loss: 0.00020389
Iteration 286/1000 | Loss: 0.00012650
Iteration 287/1000 | Loss: 0.00025529
Iteration 288/1000 | Loss: 0.00016666
Iteration 289/1000 | Loss: 0.00017088
Iteration 290/1000 | Loss: 0.00016164
Iteration 291/1000 | Loss: 0.00055690
Iteration 292/1000 | Loss: 0.00033236
Iteration 293/1000 | Loss: 0.00017565
Iteration 294/1000 | Loss: 0.00015146
Iteration 295/1000 | Loss: 0.00008912
Iteration 296/1000 | Loss: 0.00007952
Iteration 297/1000 | Loss: 0.00008566
Iteration 298/1000 | Loss: 0.00012745
Iteration 299/1000 | Loss: 0.00012948
Iteration 300/1000 | Loss: 0.00014990
Iteration 301/1000 | Loss: 0.00032091
Iteration 302/1000 | Loss: 0.00020203
Iteration 303/1000 | Loss: 0.00017981
Iteration 304/1000 | Loss: 0.00012429
Iteration 305/1000 | Loss: 0.00035907
Iteration 306/1000 | Loss: 0.00019816
Iteration 307/1000 | Loss: 0.00008512
Iteration 308/1000 | Loss: 0.00012887
Iteration 309/1000 | Loss: 0.00029409
Iteration 310/1000 | Loss: 0.00027109
Iteration 311/1000 | Loss: 0.00013500
Iteration 312/1000 | Loss: 0.00021134
Iteration 313/1000 | Loss: 0.00018397
Iteration 314/1000 | Loss: 0.00020346
Iteration 315/1000 | Loss: 0.00019177
Iteration 316/1000 | Loss: 0.00019516
Iteration 317/1000 | Loss: 0.00021336
Iteration 318/1000 | Loss: 0.00144936
Iteration 319/1000 | Loss: 0.00028811
Iteration 320/1000 | Loss: 0.00043830
Iteration 321/1000 | Loss: 0.00035577
Iteration 322/1000 | Loss: 0.00026363
Iteration 323/1000 | Loss: 0.00078634
Iteration 324/1000 | Loss: 0.00102416
Iteration 325/1000 | Loss: 0.00098568
Iteration 326/1000 | Loss: 0.00058057
Iteration 327/1000 | Loss: 0.00040799
Iteration 328/1000 | Loss: 0.00003847
Iteration 329/1000 | Loss: 0.00045537
Iteration 330/1000 | Loss: 0.00040263
Iteration 331/1000 | Loss: 0.00041933
Iteration 332/1000 | Loss: 0.00043088
Iteration 333/1000 | Loss: 0.00062567
Iteration 334/1000 | Loss: 0.00060375
Iteration 335/1000 | Loss: 0.00003619
Iteration 336/1000 | Loss: 0.00003240
Iteration 337/1000 | Loss: 0.00051091
Iteration 338/1000 | Loss: 0.00003357
Iteration 339/1000 | Loss: 0.00003081
Iteration 340/1000 | Loss: 0.00002884
Iteration 341/1000 | Loss: 0.00002803
Iteration 342/1000 | Loss: 0.00002741
Iteration 343/1000 | Loss: 0.00033667
Iteration 344/1000 | Loss: 0.00014885
Iteration 345/1000 | Loss: 0.00030138
Iteration 346/1000 | Loss: 0.00022254
Iteration 347/1000 | Loss: 0.00002845
Iteration 348/1000 | Loss: 0.00002678
Iteration 349/1000 | Loss: 0.00002618
Iteration 350/1000 | Loss: 0.00037946
Iteration 351/1000 | Loss: 0.00034973
Iteration 352/1000 | Loss: 0.00002727
Iteration 353/1000 | Loss: 0.00035250
Iteration 354/1000 | Loss: 0.00010356
Iteration 355/1000 | Loss: 0.00028452
Iteration 356/1000 | Loss: 0.00008075
Iteration 357/1000 | Loss: 0.00026335
Iteration 358/1000 | Loss: 0.00008391
Iteration 359/1000 | Loss: 0.00021958
Iteration 360/1000 | Loss: 0.00021164
Iteration 361/1000 | Loss: 0.00017350
Iteration 362/1000 | Loss: 0.00018032
Iteration 363/1000 | Loss: 0.00002943
Iteration 364/1000 | Loss: 0.00032579
Iteration 365/1000 | Loss: 0.00045103
Iteration 366/1000 | Loss: 0.00004205
Iteration 367/1000 | Loss: 0.00003269
Iteration 368/1000 | Loss: 0.00002817
Iteration 369/1000 | Loss: 0.00002480
Iteration 370/1000 | Loss: 0.00002372
Iteration 371/1000 | Loss: 0.00002320
Iteration 372/1000 | Loss: 0.00002296
Iteration 373/1000 | Loss: 0.00002289
Iteration 374/1000 | Loss: 0.00002276
Iteration 375/1000 | Loss: 0.00002275
Iteration 376/1000 | Loss: 0.00002271
Iteration 377/1000 | Loss: 0.00002257
Iteration 378/1000 | Loss: 0.00002252
Iteration 379/1000 | Loss: 0.00002252
Iteration 380/1000 | Loss: 0.00002248
Iteration 381/1000 | Loss: 0.00002244
Iteration 382/1000 | Loss: 0.00002242
Iteration 383/1000 | Loss: 0.00002242
Iteration 384/1000 | Loss: 0.00002241
Iteration 385/1000 | Loss: 0.00002241
Iteration 386/1000 | Loss: 0.00002240
Iteration 387/1000 | Loss: 0.00002240
Iteration 388/1000 | Loss: 0.00002236
Iteration 389/1000 | Loss: 0.00002236
Iteration 390/1000 | Loss: 0.00002236
Iteration 391/1000 | Loss: 0.00002235
Iteration 392/1000 | Loss: 0.00002235
Iteration 393/1000 | Loss: 0.00002234
Iteration 394/1000 | Loss: 0.00002234
Iteration 395/1000 | Loss: 0.00002234
Iteration 396/1000 | Loss: 0.00002233
Iteration 397/1000 | Loss: 0.00002230
Iteration 398/1000 | Loss: 0.00002230
Iteration 399/1000 | Loss: 0.00002230
Iteration 400/1000 | Loss: 0.00002230
Iteration 401/1000 | Loss: 0.00002230
Iteration 402/1000 | Loss: 0.00002230
Iteration 403/1000 | Loss: 0.00002230
Iteration 404/1000 | Loss: 0.00002230
Iteration 405/1000 | Loss: 0.00002230
Iteration 406/1000 | Loss: 0.00002229
Iteration 407/1000 | Loss: 0.00002229
Iteration 408/1000 | Loss: 0.00002228
Iteration 409/1000 | Loss: 0.00002228
Iteration 410/1000 | Loss: 0.00002228
Iteration 411/1000 | Loss: 0.00002227
Iteration 412/1000 | Loss: 0.00002227
Iteration 413/1000 | Loss: 0.00002227
Iteration 414/1000 | Loss: 0.00002227
Iteration 415/1000 | Loss: 0.00002226
Iteration 416/1000 | Loss: 0.00002226
Iteration 417/1000 | Loss: 0.00002225
Iteration 418/1000 | Loss: 0.00002222
Iteration 419/1000 | Loss: 0.00002221
Iteration 420/1000 | Loss: 0.00002221
Iteration 421/1000 | Loss: 0.00002221
Iteration 422/1000 | Loss: 0.00002220
Iteration 423/1000 | Loss: 0.00002220
Iteration 424/1000 | Loss: 0.00002219
Iteration 425/1000 | Loss: 0.00002219
Iteration 426/1000 | Loss: 0.00002219
Iteration 427/1000 | Loss: 0.00002218
Iteration 428/1000 | Loss: 0.00002218
Iteration 429/1000 | Loss: 0.00002218
Iteration 430/1000 | Loss: 0.00002218
Iteration 431/1000 | Loss: 0.00002217
Iteration 432/1000 | Loss: 0.00002217
Iteration 433/1000 | Loss: 0.00002217
Iteration 434/1000 | Loss: 0.00002217
Iteration 435/1000 | Loss: 0.00002217
Iteration 436/1000 | Loss: 0.00002217
Iteration 437/1000 | Loss: 0.00002216
Iteration 438/1000 | Loss: 0.00002216
Iteration 439/1000 | Loss: 0.00002216
Iteration 440/1000 | Loss: 0.00002216
Iteration 441/1000 | Loss: 0.00002216
Iteration 442/1000 | Loss: 0.00002215
Iteration 443/1000 | Loss: 0.00002215
Iteration 444/1000 | Loss: 0.00002215
Iteration 445/1000 | Loss: 0.00002215
Iteration 446/1000 | Loss: 0.00002215
Iteration 447/1000 | Loss: 0.00002215
Iteration 448/1000 | Loss: 0.00002215
Iteration 449/1000 | Loss: 0.00002214
Iteration 450/1000 | Loss: 0.00002214
Iteration 451/1000 | Loss: 0.00002214
Iteration 452/1000 | Loss: 0.00002214
Iteration 453/1000 | Loss: 0.00002214
Iteration 454/1000 | Loss: 0.00002213
Iteration 455/1000 | Loss: 0.00002213
Iteration 456/1000 | Loss: 0.00002213
Iteration 457/1000 | Loss: 0.00002213
Iteration 458/1000 | Loss: 0.00002212
Iteration 459/1000 | Loss: 0.00002212
Iteration 460/1000 | Loss: 0.00002212
Iteration 461/1000 | Loss: 0.00002212
Iteration 462/1000 | Loss: 0.00002212
Iteration 463/1000 | Loss: 0.00002212
Iteration 464/1000 | Loss: 0.00002212
Iteration 465/1000 | Loss: 0.00002211
Iteration 466/1000 | Loss: 0.00002211
Iteration 467/1000 | Loss: 0.00002211
Iteration 468/1000 | Loss: 0.00002211
Iteration 469/1000 | Loss: 0.00002210
Iteration 470/1000 | Loss: 0.00002210
Iteration 471/1000 | Loss: 0.00002210
Iteration 472/1000 | Loss: 0.00002210
Iteration 473/1000 | Loss: 0.00002210
Iteration 474/1000 | Loss: 0.00002210
Iteration 475/1000 | Loss: 0.00002209
Iteration 476/1000 | Loss: 0.00002209
Iteration 477/1000 | Loss: 0.00002209
Iteration 478/1000 | Loss: 0.00002209
Iteration 479/1000 | Loss: 0.00002209
Iteration 480/1000 | Loss: 0.00002209
Iteration 481/1000 | Loss: 0.00002209
Iteration 482/1000 | Loss: 0.00002209
Iteration 483/1000 | Loss: 0.00002208
Iteration 484/1000 | Loss: 0.00002208
Iteration 485/1000 | Loss: 0.00002208
Iteration 486/1000 | Loss: 0.00002208
Iteration 487/1000 | Loss: 0.00002208
Iteration 488/1000 | Loss: 0.00002208
Iteration 489/1000 | Loss: 0.00002208
Iteration 490/1000 | Loss: 0.00002208
Iteration 491/1000 | Loss: 0.00002208
Iteration 492/1000 | Loss: 0.00002208
Iteration 493/1000 | Loss: 0.00002208
Iteration 494/1000 | Loss: 0.00002208
Iteration 495/1000 | Loss: 0.00002208
Iteration 496/1000 | Loss: 0.00002208
Iteration 497/1000 | Loss: 0.00002208
Iteration 498/1000 | Loss: 0.00002208
Iteration 499/1000 | Loss: 0.00002207
Iteration 500/1000 | Loss: 0.00002207
Iteration 501/1000 | Loss: 0.00002207
Iteration 502/1000 | Loss: 0.00002207
Iteration 503/1000 | Loss: 0.00002207
Iteration 504/1000 | Loss: 0.00002207
Iteration 505/1000 | Loss: 0.00002207
Iteration 506/1000 | Loss: 0.00002207
Iteration 507/1000 | Loss: 0.00002207
Iteration 508/1000 | Loss: 0.00002207
Iteration 509/1000 | Loss: 0.00002207
Iteration 510/1000 | Loss: 0.00002207
Iteration 511/1000 | Loss: 0.00002207
Iteration 512/1000 | Loss: 0.00002207
Iteration 513/1000 | Loss: 0.00002206
Iteration 514/1000 | Loss: 0.00002206
Iteration 515/1000 | Loss: 0.00002206
Iteration 516/1000 | Loss: 0.00002206
Iteration 517/1000 | Loss: 0.00002206
Iteration 518/1000 | Loss: 0.00002206
Iteration 519/1000 | Loss: 0.00002206
Iteration 520/1000 | Loss: 0.00002205
Iteration 521/1000 | Loss: 0.00002205
Iteration 522/1000 | Loss: 0.00002205
Iteration 523/1000 | Loss: 0.00002205
Iteration 524/1000 | Loss: 0.00002205
Iteration 525/1000 | Loss: 0.00002205
Iteration 526/1000 | Loss: 0.00002205
Iteration 527/1000 | Loss: 0.00002205
Iteration 528/1000 | Loss: 0.00002205
Iteration 529/1000 | Loss: 0.00002205
Iteration 530/1000 | Loss: 0.00002205
Iteration 531/1000 | Loss: 0.00002205
Iteration 532/1000 | Loss: 0.00002205
Iteration 533/1000 | Loss: 0.00002205
Iteration 534/1000 | Loss: 0.00002205
Iteration 535/1000 | Loss: 0.00002205
Iteration 536/1000 | Loss: 0.00002205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 536. Stopping optimization.
Last 5 losses: [2.20452893699985e-05, 2.20452893699985e-05, 2.20452893699985e-05, 2.20452893699985e-05, 2.20452893699985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.20452893699985e-05

Optimization complete. Final v2v error: 3.803532361984253 mm

Highest mean error: 12.725027084350586 mm for frame 193

Lowest mean error: 3.368839979171753 mm for frame 0

Saving results

Total time: 16637.340972185135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1018
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473266
Iteration 2/25 | Loss: 0.00126942
Iteration 3/25 | Loss: 0.00104064
Iteration 4/25 | Loss: 0.00101696
Iteration 5/25 | Loss: 0.00101471
Iteration 6/25 | Loss: 0.00101455
Iteration 7/25 | Loss: 0.00101455
Iteration 8/25 | Loss: 0.00101455
Iteration 9/25 | Loss: 0.00101455
Iteration 10/25 | Loss: 0.00101455
Iteration 11/25 | Loss: 0.00101455
Iteration 12/25 | Loss: 0.00101455
Iteration 13/25 | Loss: 0.00101455
Iteration 14/25 | Loss: 0.00101455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010145489359274507, 0.0010145489359274507, 0.0010145489359274507, 0.0010145489359274507, 0.0010145489359274507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010145489359274507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33495951
Iteration 2/25 | Loss: 0.00056210
Iteration 3/25 | Loss: 0.00056208
Iteration 4/25 | Loss: 0.00056208
Iteration 5/25 | Loss: 0.00056208
Iteration 6/25 | Loss: 0.00056208
Iteration 7/25 | Loss: 0.00056208
Iteration 8/25 | Loss: 0.00056208
Iteration 9/25 | Loss: 0.00056208
Iteration 10/25 | Loss: 0.00056208
Iteration 11/25 | Loss: 0.00056208
Iteration 12/25 | Loss: 0.00056208
Iteration 13/25 | Loss: 0.00056208
Iteration 14/25 | Loss: 0.00056208
Iteration 15/25 | Loss: 0.00056208
Iteration 16/25 | Loss: 0.00056208
Iteration 17/25 | Loss: 0.00056208
Iteration 18/25 | Loss: 0.00056208
Iteration 19/25 | Loss: 0.00056208
Iteration 20/25 | Loss: 0.00056208
Iteration 21/25 | Loss: 0.00056208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005620821611955762, 0.0005620821611955762, 0.0005620821611955762, 0.0005620821611955762, 0.0005620821611955762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005620821611955762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056208
Iteration 2/1000 | Loss: 0.00003923
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001729
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001566
Iteration 7/1000 | Loss: 0.00001533
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001487
Iteration 10/1000 | Loss: 0.00001467
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001451
Iteration 15/1000 | Loss: 0.00001449
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001446
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001446
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001442
Iteration 28/1000 | Loss: 0.00001441
Iteration 29/1000 | Loss: 0.00001441
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001440
Iteration 32/1000 | Loss: 0.00001439
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001432
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001423
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001421
Iteration 78/1000 | Loss: 0.00001421
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001420
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001419
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001417
Iteration 94/1000 | Loss: 0.00001417
Iteration 95/1000 | Loss: 0.00001417
Iteration 96/1000 | Loss: 0.00001417
Iteration 97/1000 | Loss: 0.00001417
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001416
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001415
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001415
Iteration 112/1000 | Loss: 0.00001415
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001415
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001414
Iteration 117/1000 | Loss: 0.00001414
Iteration 118/1000 | Loss: 0.00001414
Iteration 119/1000 | Loss: 0.00001414
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001413
Iteration 131/1000 | Loss: 0.00001413
Iteration 132/1000 | Loss: 0.00001413
Iteration 133/1000 | Loss: 0.00001413
Iteration 134/1000 | Loss: 0.00001413
Iteration 135/1000 | Loss: 0.00001413
Iteration 136/1000 | Loss: 0.00001413
Iteration 137/1000 | Loss: 0.00001413
Iteration 138/1000 | Loss: 0.00001413
Iteration 139/1000 | Loss: 0.00001412
Iteration 140/1000 | Loss: 0.00001412
Iteration 141/1000 | Loss: 0.00001412
Iteration 142/1000 | Loss: 0.00001412
Iteration 143/1000 | Loss: 0.00001412
Iteration 144/1000 | Loss: 0.00001412
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001411
Iteration 150/1000 | Loss: 0.00001411
Iteration 151/1000 | Loss: 0.00001411
Iteration 152/1000 | Loss: 0.00001411
Iteration 153/1000 | Loss: 0.00001411
Iteration 154/1000 | Loss: 0.00001411
Iteration 155/1000 | Loss: 0.00001411
Iteration 156/1000 | Loss: 0.00001411
Iteration 157/1000 | Loss: 0.00001411
Iteration 158/1000 | Loss: 0.00001411
Iteration 159/1000 | Loss: 0.00001411
Iteration 160/1000 | Loss: 0.00001410
Iteration 161/1000 | Loss: 0.00001410
Iteration 162/1000 | Loss: 0.00001410
Iteration 163/1000 | Loss: 0.00001410
Iteration 164/1000 | Loss: 0.00001410
Iteration 165/1000 | Loss: 0.00001410
Iteration 166/1000 | Loss: 0.00001410
Iteration 167/1000 | Loss: 0.00001410
Iteration 168/1000 | Loss: 0.00001410
Iteration 169/1000 | Loss: 0.00001410
Iteration 170/1000 | Loss: 0.00001410
Iteration 171/1000 | Loss: 0.00001410
Iteration 172/1000 | Loss: 0.00001409
Iteration 173/1000 | Loss: 0.00001409
Iteration 174/1000 | Loss: 0.00001409
Iteration 175/1000 | Loss: 0.00001409
Iteration 176/1000 | Loss: 0.00001409
Iteration 177/1000 | Loss: 0.00001408
Iteration 178/1000 | Loss: 0.00001408
Iteration 179/1000 | Loss: 0.00001408
Iteration 180/1000 | Loss: 0.00001408
Iteration 181/1000 | Loss: 0.00001408
Iteration 182/1000 | Loss: 0.00001408
Iteration 183/1000 | Loss: 0.00001408
Iteration 184/1000 | Loss: 0.00001408
Iteration 185/1000 | Loss: 0.00001408
Iteration 186/1000 | Loss: 0.00001408
Iteration 187/1000 | Loss: 0.00001408
Iteration 188/1000 | Loss: 0.00001408
Iteration 189/1000 | Loss: 0.00001407
Iteration 190/1000 | Loss: 0.00001407
Iteration 191/1000 | Loss: 0.00001407
Iteration 192/1000 | Loss: 0.00001407
Iteration 193/1000 | Loss: 0.00001407
Iteration 194/1000 | Loss: 0.00001407
Iteration 195/1000 | Loss: 0.00001407
Iteration 196/1000 | Loss: 0.00001407
Iteration 197/1000 | Loss: 0.00001406
Iteration 198/1000 | Loss: 0.00001406
Iteration 199/1000 | Loss: 0.00001406
Iteration 200/1000 | Loss: 0.00001406
Iteration 201/1000 | Loss: 0.00001406
Iteration 202/1000 | Loss: 0.00001406
Iteration 203/1000 | Loss: 0.00001406
Iteration 204/1000 | Loss: 0.00001406
Iteration 205/1000 | Loss: 0.00001406
Iteration 206/1000 | Loss: 0.00001406
Iteration 207/1000 | Loss: 0.00001406
Iteration 208/1000 | Loss: 0.00001406
Iteration 209/1000 | Loss: 0.00001406
Iteration 210/1000 | Loss: 0.00001406
Iteration 211/1000 | Loss: 0.00001406
Iteration 212/1000 | Loss: 0.00001406
Iteration 213/1000 | Loss: 0.00001406
Iteration 214/1000 | Loss: 0.00001406
Iteration 215/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.405578132107621e-05, 1.405578132107621e-05, 1.405578132107621e-05, 1.405578132107621e-05, 1.405578132107621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.405578132107621e-05

Optimization complete. Final v2v error: 3.119249105453491 mm

Highest mean error: 3.4604434967041016 mm for frame 186

Lowest mean error: 2.598568916320801 mm for frame 239

Saving results

Total time: 1290.152803182602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1003
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102846
Iteration 2/25 | Loss: 0.00284172
Iteration 3/25 | Loss: 0.00175906
Iteration 4/25 | Loss: 0.00152907
Iteration 5/25 | Loss: 0.00148273
Iteration 6/25 | Loss: 0.00143826
Iteration 7/25 | Loss: 0.00139185
Iteration 8/25 | Loss: 0.00137528
Iteration 9/25 | Loss: 0.00135727
Iteration 10/25 | Loss: 0.00134772
Iteration 11/25 | Loss: 0.00134959
Iteration 12/25 | Loss: 0.00134352
Iteration 13/25 | Loss: 0.00134128
Iteration 14/25 | Loss: 0.00133574
Iteration 15/25 | Loss: 0.00133765
Iteration 16/25 | Loss: 0.00133667
Iteration 17/25 | Loss: 0.00133075
Iteration 18/25 | Loss: 0.00133637
Iteration 19/25 | Loss: 0.00133516
Iteration 20/25 | Loss: 0.00133624
Iteration 21/25 | Loss: 0.00133429
Iteration 22/25 | Loss: 0.00133544
Iteration 23/25 | Loss: 0.00133364
Iteration 24/25 | Loss: 0.00133438
Iteration 25/25 | Loss: 0.00133372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15756845
Iteration 2/25 | Loss: 0.00372527
Iteration 3/25 | Loss: 0.00372527
Iteration 4/25 | Loss: 0.00372527
Iteration 5/25 | Loss: 0.00372527
Iteration 6/25 | Loss: 0.00372527
Iteration 7/25 | Loss: 0.00372527
Iteration 8/25 | Loss: 0.00372527
Iteration 9/25 | Loss: 0.00372527
Iteration 10/25 | Loss: 0.00372527
Iteration 11/25 | Loss: 0.00372527
Iteration 12/25 | Loss: 0.00372527
Iteration 13/25 | Loss: 0.00372527
Iteration 14/25 | Loss: 0.00372527
Iteration 15/25 | Loss: 0.00372527
Iteration 16/25 | Loss: 0.00372527
Iteration 17/25 | Loss: 0.00372527
Iteration 18/25 | Loss: 0.00372527
Iteration 19/25 | Loss: 0.00372527
Iteration 20/25 | Loss: 0.00372527
Iteration 21/25 | Loss: 0.00372527
Iteration 22/25 | Loss: 0.00372527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0037252691108733416, 0.0037252691108733416, 0.0037252691108733416, 0.0037252691108733416, 0.0037252691108733416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037252691108733416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00372527
Iteration 2/1000 | Loss: 0.00087063
Iteration 3/1000 | Loss: 0.00156945
Iteration 4/1000 | Loss: 0.00577484
Iteration 5/1000 | Loss: 0.00167705
Iteration 6/1000 | Loss: 0.00357894
Iteration 7/1000 | Loss: 0.00335953
Iteration 8/1000 | Loss: 0.00173906
Iteration 9/1000 | Loss: 0.00063406
Iteration 10/1000 | Loss: 0.00050057
Iteration 11/1000 | Loss: 0.00034318
Iteration 12/1000 | Loss: 0.00082763
Iteration 13/1000 | Loss: 0.00223916
Iteration 14/1000 | Loss: 0.00028231
Iteration 15/1000 | Loss: 0.00055463
Iteration 16/1000 | Loss: 0.00032835
Iteration 17/1000 | Loss: 0.00126013
Iteration 18/1000 | Loss: 0.00075162
Iteration 19/1000 | Loss: 0.00019109
Iteration 20/1000 | Loss: 0.00095369
Iteration 21/1000 | Loss: 0.00078808
Iteration 22/1000 | Loss: 0.00018921
Iteration 23/1000 | Loss: 0.00016861
Iteration 24/1000 | Loss: 0.00015807
Iteration 25/1000 | Loss: 0.00024378
Iteration 26/1000 | Loss: 0.00078383
Iteration 27/1000 | Loss: 0.00082257
Iteration 28/1000 | Loss: 0.00157045
Iteration 29/1000 | Loss: 0.00437990
Iteration 30/1000 | Loss: 0.00270268
Iteration 31/1000 | Loss: 0.00128320
Iteration 32/1000 | Loss: 0.00300714
Iteration 33/1000 | Loss: 0.00149126
Iteration 34/1000 | Loss: 0.00557537
Iteration 35/1000 | Loss: 0.00440190
Iteration 36/1000 | Loss: 0.01017653
Iteration 37/1000 | Loss: 0.00456825
Iteration 38/1000 | Loss: 0.00366062
Iteration 39/1000 | Loss: 0.00677887
Iteration 40/1000 | Loss: 0.00272083
Iteration 41/1000 | Loss: 0.00171038
Iteration 42/1000 | Loss: 0.00279938
Iteration 43/1000 | Loss: 0.00109833
Iteration 44/1000 | Loss: 0.00296008
Iteration 45/1000 | Loss: 0.00121829
Iteration 46/1000 | Loss: 0.00317395
Iteration 47/1000 | Loss: 0.00295894
Iteration 48/1000 | Loss: 0.00125050
Iteration 49/1000 | Loss: 0.00130481
Iteration 50/1000 | Loss: 0.00157635
Iteration 51/1000 | Loss: 0.00237391
Iteration 52/1000 | Loss: 0.00191097
Iteration 53/1000 | Loss: 0.00167731
Iteration 54/1000 | Loss: 0.00274672
Iteration 55/1000 | Loss: 0.00220519
Iteration 56/1000 | Loss: 0.00252638
Iteration 57/1000 | Loss: 0.00243142
Iteration 58/1000 | Loss: 0.00117799
Iteration 59/1000 | Loss: 0.00095037
Iteration 60/1000 | Loss: 0.00096329
Iteration 61/1000 | Loss: 0.00127133
Iteration 62/1000 | Loss: 0.00105982
Iteration 63/1000 | Loss: 0.00154730
Iteration 64/1000 | Loss: 0.00100801
Iteration 65/1000 | Loss: 0.00148906
Iteration 66/1000 | Loss: 0.00135745
Iteration 67/1000 | Loss: 0.00119657
Iteration 68/1000 | Loss: 0.00128989
Iteration 69/1000 | Loss: 0.00221079
Iteration 70/1000 | Loss: 0.00164851
Iteration 71/1000 | Loss: 0.00274168
Iteration 72/1000 | Loss: 0.00172310
Iteration 73/1000 | Loss: 0.00120860
Iteration 74/1000 | Loss: 0.00213434
Iteration 75/1000 | Loss: 0.00116702
Iteration 76/1000 | Loss: 0.00225586
Iteration 77/1000 | Loss: 0.00193753
Iteration 78/1000 | Loss: 0.00171000
Iteration 79/1000 | Loss: 0.00432499
Iteration 80/1000 | Loss: 0.00431147
Iteration 81/1000 | Loss: 0.00302989
Iteration 82/1000 | Loss: 0.00425248
Iteration 83/1000 | Loss: 0.00418098
Iteration 84/1000 | Loss: 0.00230617
Iteration 85/1000 | Loss: 0.00211774
Iteration 86/1000 | Loss: 0.00271701
Iteration 87/1000 | Loss: 0.00176633
Iteration 88/1000 | Loss: 0.00310057
Iteration 89/1000 | Loss: 0.00429673
Iteration 90/1000 | Loss: 0.00301237
Iteration 91/1000 | Loss: 0.00442772
Iteration 92/1000 | Loss: 0.00160760
Iteration 93/1000 | Loss: 0.00290536
Iteration 94/1000 | Loss: 0.00372137
Iteration 95/1000 | Loss: 0.00132731
Iteration 96/1000 | Loss: 0.00210239
Iteration 97/1000 | Loss: 0.00225923
Iteration 98/1000 | Loss: 0.00202183
Iteration 99/1000 | Loss: 0.00249834
Iteration 100/1000 | Loss: 0.00375132
Iteration 101/1000 | Loss: 0.00394240
Iteration 102/1000 | Loss: 0.00370385
Iteration 103/1000 | Loss: 0.00325917
Iteration 104/1000 | Loss: 0.00444158
Iteration 105/1000 | Loss: 0.00377124
Iteration 106/1000 | Loss: 0.00366888
Iteration 107/1000 | Loss: 0.00238927
Iteration 108/1000 | Loss: 0.00299754
Iteration 109/1000 | Loss: 0.00195112
Iteration 110/1000 | Loss: 0.00272262
Iteration 111/1000 | Loss: 0.00116032
Iteration 112/1000 | Loss: 0.00115585
Iteration 113/1000 | Loss: 0.00167937
Iteration 114/1000 | Loss: 0.00171988
Iteration 115/1000 | Loss: 0.00317161
Iteration 116/1000 | Loss: 0.00301571
Iteration 117/1000 | Loss: 0.00434277
Iteration 118/1000 | Loss: 0.00186833
Iteration 119/1000 | Loss: 0.00107044
Iteration 120/1000 | Loss: 0.00153568
Iteration 121/1000 | Loss: 0.00149644
Iteration 122/1000 | Loss: 0.00225109
Iteration 123/1000 | Loss: 0.00159694
Iteration 124/1000 | Loss: 0.00259256
Iteration 125/1000 | Loss: 0.00170668
Iteration 126/1000 | Loss: 0.00302111
Iteration 127/1000 | Loss: 0.00104387
Iteration 128/1000 | Loss: 0.00065450
Iteration 129/1000 | Loss: 0.00414043
Iteration 130/1000 | Loss: 0.00184313
Iteration 131/1000 | Loss: 0.00112534
Iteration 132/1000 | Loss: 0.00095264
Iteration 133/1000 | Loss: 0.00123118
Iteration 134/1000 | Loss: 0.00082961
Iteration 135/1000 | Loss: 0.00096556
Iteration 136/1000 | Loss: 0.00077303
Iteration 137/1000 | Loss: 0.00113913
Iteration 138/1000 | Loss: 0.00146183
Iteration 139/1000 | Loss: 0.00087109
Iteration 140/1000 | Loss: 0.00029160
Iteration 141/1000 | Loss: 0.00139223
Iteration 142/1000 | Loss: 0.00082754
Iteration 143/1000 | Loss: 0.00062417
Iteration 144/1000 | Loss: 0.00110520
Iteration 145/1000 | Loss: 0.00137803
Iteration 146/1000 | Loss: 0.00080940
Iteration 147/1000 | Loss: 0.00077832
Iteration 148/1000 | Loss: 0.00086303
Iteration 149/1000 | Loss: 0.00154541
Iteration 150/1000 | Loss: 0.00084359
Iteration 151/1000 | Loss: 0.00121565
Iteration 152/1000 | Loss: 0.00114829
Iteration 153/1000 | Loss: 0.00093292
Iteration 154/1000 | Loss: 0.00093701
Iteration 155/1000 | Loss: 0.00161243
Iteration 156/1000 | Loss: 0.00168190
Iteration 157/1000 | Loss: 0.00140189
Iteration 158/1000 | Loss: 0.00097802
Iteration 159/1000 | Loss: 0.00121705
Iteration 160/1000 | Loss: 0.00126069
Iteration 161/1000 | Loss: 0.00088729
Iteration 162/1000 | Loss: 0.00040612
Iteration 163/1000 | Loss: 0.00071925
Iteration 164/1000 | Loss: 0.00156682
Iteration 165/1000 | Loss: 0.00043269
Iteration 166/1000 | Loss: 0.00090719
Iteration 167/1000 | Loss: 0.00093329
Iteration 168/1000 | Loss: 0.00059144
Iteration 169/1000 | Loss: 0.00101420
Iteration 170/1000 | Loss: 0.00166775
Iteration 171/1000 | Loss: 0.00136160
Iteration 172/1000 | Loss: 0.00078858
Iteration 173/1000 | Loss: 0.00161406
Iteration 174/1000 | Loss: 0.00152193
Iteration 175/1000 | Loss: 0.00120360
Iteration 176/1000 | Loss: 0.00096526
Iteration 177/1000 | Loss: 0.00072443
Iteration 178/1000 | Loss: 0.00104591
Iteration 179/1000 | Loss: 0.00074174
Iteration 180/1000 | Loss: 0.00066982
Iteration 181/1000 | Loss: 0.00074867
Iteration 182/1000 | Loss: 0.00087375
Iteration 183/1000 | Loss: 0.00073185
Iteration 184/1000 | Loss: 0.00093430
Iteration 185/1000 | Loss: 0.00078496
Iteration 186/1000 | Loss: 0.00093210
Iteration 187/1000 | Loss: 0.00157713
Iteration 188/1000 | Loss: 0.00092094
Iteration 189/1000 | Loss: 0.00112876
Iteration 190/1000 | Loss: 0.00095743
Iteration 191/1000 | Loss: 0.00076414
Iteration 192/1000 | Loss: 0.00057764
Iteration 193/1000 | Loss: 0.00046673
Iteration 194/1000 | Loss: 0.00016689
Iteration 195/1000 | Loss: 0.00008257
Iteration 196/1000 | Loss: 0.00068567
Iteration 197/1000 | Loss: 0.00125231
Iteration 198/1000 | Loss: 0.00066631
Iteration 199/1000 | Loss: 0.00099907
Iteration 200/1000 | Loss: 0.00092278
Iteration 201/1000 | Loss: 0.00058177
Iteration 202/1000 | Loss: 0.00048895
Iteration 203/1000 | Loss: 0.00106270
Iteration 204/1000 | Loss: 0.00038555
Iteration 205/1000 | Loss: 0.00046846
Iteration 206/1000 | Loss: 0.00045625
Iteration 207/1000 | Loss: 0.00046775
Iteration 208/1000 | Loss: 0.00061659
Iteration 209/1000 | Loss: 0.00137661
Iteration 210/1000 | Loss: 0.00055921
Iteration 211/1000 | Loss: 0.00067720
Iteration 212/1000 | Loss: 0.00045412
Iteration 213/1000 | Loss: 0.00073921
Iteration 214/1000 | Loss: 0.00090597
Iteration 215/1000 | Loss: 0.00051377
Iteration 216/1000 | Loss: 0.00039634
Iteration 217/1000 | Loss: 0.00045604
Iteration 218/1000 | Loss: 0.00050435
Iteration 219/1000 | Loss: 0.00082700
Iteration 220/1000 | Loss: 0.00155307
Iteration 221/1000 | Loss: 0.00134971
Iteration 222/1000 | Loss: 0.00178016
Iteration 223/1000 | Loss: 0.00149762
Iteration 224/1000 | Loss: 0.00156823
Iteration 225/1000 | Loss: 0.00078060
Iteration 226/1000 | Loss: 0.00188506
Iteration 227/1000 | Loss: 0.00131201
Iteration 228/1000 | Loss: 0.00108689
Iteration 229/1000 | Loss: 0.00142572
Iteration 230/1000 | Loss: 0.00113279
Iteration 231/1000 | Loss: 0.00172826
Iteration 232/1000 | Loss: 0.00053578
Iteration 233/1000 | Loss: 0.00085928
Iteration 234/1000 | Loss: 0.00081968
Iteration 235/1000 | Loss: 0.00037437
Iteration 236/1000 | Loss: 0.00110455
Iteration 237/1000 | Loss: 0.00041779
Iteration 238/1000 | Loss: 0.00022577
Iteration 239/1000 | Loss: 0.00043364
Iteration 240/1000 | Loss: 0.00015163
Iteration 241/1000 | Loss: 0.00139285
Iteration 242/1000 | Loss: 0.00089579
Iteration 243/1000 | Loss: 0.00108757
Iteration 244/1000 | Loss: 0.00043627
Iteration 245/1000 | Loss: 0.00013156
Iteration 246/1000 | Loss: 0.00035133
Iteration 247/1000 | Loss: 0.00044852
Iteration 248/1000 | Loss: 0.00023468
Iteration 249/1000 | Loss: 0.00019060
Iteration 250/1000 | Loss: 0.00016441
Iteration 251/1000 | Loss: 0.00035202
Iteration 252/1000 | Loss: 0.00057121
Iteration 253/1000 | Loss: 0.00048773
Iteration 254/1000 | Loss: 0.00044779
Iteration 255/1000 | Loss: 0.00063812
Iteration 256/1000 | Loss: 0.00023015
Iteration 257/1000 | Loss: 0.00020605
Iteration 258/1000 | Loss: 0.00026923
Iteration 259/1000 | Loss: 0.00041245
Iteration 260/1000 | Loss: 0.00061148
Iteration 261/1000 | Loss: 0.00070837
Iteration 262/1000 | Loss: 0.00062107
Iteration 263/1000 | Loss: 0.00060169
Iteration 264/1000 | Loss: 0.00070713
Iteration 265/1000 | Loss: 0.00025633
Iteration 266/1000 | Loss: 0.00031322
Iteration 267/1000 | Loss: 0.00015567
Iteration 268/1000 | Loss: 0.00022180
Iteration 269/1000 | Loss: 0.00030933
Iteration 270/1000 | Loss: 0.00074439
Iteration 271/1000 | Loss: 0.00009534
Iteration 272/1000 | Loss: 0.00057961
Iteration 273/1000 | Loss: 0.00006367
Iteration 274/1000 | Loss: 0.00054077
Iteration 275/1000 | Loss: 0.00005812
Iteration 276/1000 | Loss: 0.00005504
Iteration 277/1000 | Loss: 0.00102347
Iteration 278/1000 | Loss: 0.00036840
Iteration 279/1000 | Loss: 0.00076080
Iteration 280/1000 | Loss: 0.00088275
Iteration 281/1000 | Loss: 0.00051037
Iteration 282/1000 | Loss: 0.00008945
Iteration 283/1000 | Loss: 0.00056060
Iteration 284/1000 | Loss: 0.00006858
Iteration 285/1000 | Loss: 0.00050605
Iteration 286/1000 | Loss: 0.00029285
Iteration 287/1000 | Loss: 0.00021141
Iteration 288/1000 | Loss: 0.00080820
Iteration 289/1000 | Loss: 0.00009545
Iteration 290/1000 | Loss: 0.00009877
Iteration 291/1000 | Loss: 0.00041885
Iteration 292/1000 | Loss: 0.00028989
Iteration 293/1000 | Loss: 0.00006456
Iteration 294/1000 | Loss: 0.00005779
Iteration 295/1000 | Loss: 0.00005436
Iteration 296/1000 | Loss: 0.00047513
Iteration 297/1000 | Loss: 0.00018275
Iteration 298/1000 | Loss: 0.00033303
Iteration 299/1000 | Loss: 0.00048454
Iteration 300/1000 | Loss: 0.00046758
Iteration 301/1000 | Loss: 0.00035108
Iteration 302/1000 | Loss: 0.00155447
Iteration 303/1000 | Loss: 0.00063539
Iteration 304/1000 | Loss: 0.00141223
Iteration 305/1000 | Loss: 0.00056262
Iteration 306/1000 | Loss: 0.00006117
Iteration 307/1000 | Loss: 0.00079576
Iteration 308/1000 | Loss: 0.00085448
Iteration 309/1000 | Loss: 0.00063015
Iteration 310/1000 | Loss: 0.00104418
Iteration 311/1000 | Loss: 0.00041775
Iteration 312/1000 | Loss: 0.00016983
Iteration 313/1000 | Loss: 0.00005305
Iteration 314/1000 | Loss: 0.00048527
Iteration 315/1000 | Loss: 0.00011547
Iteration 316/1000 | Loss: 0.00004702
Iteration 317/1000 | Loss: 0.00093397
Iteration 318/1000 | Loss: 0.00005405
Iteration 319/1000 | Loss: 0.00004763
Iteration 320/1000 | Loss: 0.00004515
Iteration 321/1000 | Loss: 0.00016984
Iteration 322/1000 | Loss: 0.00007015
Iteration 323/1000 | Loss: 0.00004299
Iteration 324/1000 | Loss: 0.00027009
Iteration 325/1000 | Loss: 0.00009854
Iteration 326/1000 | Loss: 0.00004116
Iteration 327/1000 | Loss: 0.00013195
Iteration 328/1000 | Loss: 0.00010118
Iteration 329/1000 | Loss: 0.00026588
Iteration 330/1000 | Loss: 0.00017773
Iteration 331/1000 | Loss: 0.00052203
Iteration 332/1000 | Loss: 0.00013850
Iteration 333/1000 | Loss: 0.00032529
Iteration 334/1000 | Loss: 0.00059654
Iteration 335/1000 | Loss: 0.00048972
Iteration 336/1000 | Loss: 0.00087667
Iteration 337/1000 | Loss: 0.00049820
Iteration 338/1000 | Loss: 0.00041587
Iteration 339/1000 | Loss: 0.00005107
Iteration 340/1000 | Loss: 0.00004144
Iteration 341/1000 | Loss: 0.00003758
Iteration 342/1000 | Loss: 0.00003627
Iteration 343/1000 | Loss: 0.00003526
Iteration 344/1000 | Loss: 0.00003449
Iteration 345/1000 | Loss: 0.00003414
Iteration 346/1000 | Loss: 0.00003397
Iteration 347/1000 | Loss: 0.00003378
Iteration 348/1000 | Loss: 0.00003364
Iteration 349/1000 | Loss: 0.00003363
Iteration 350/1000 | Loss: 0.00003363
Iteration 351/1000 | Loss: 0.00003360
Iteration 352/1000 | Loss: 0.00003360
Iteration 353/1000 | Loss: 0.00003360
Iteration 354/1000 | Loss: 0.00003360
Iteration 355/1000 | Loss: 0.00003360
Iteration 356/1000 | Loss: 0.00003360
Iteration 357/1000 | Loss: 0.00003360
Iteration 358/1000 | Loss: 0.00003360
Iteration 359/1000 | Loss: 0.00003360
Iteration 360/1000 | Loss: 0.00003360
Iteration 361/1000 | Loss: 0.00003360
Iteration 362/1000 | Loss: 0.00003360
Iteration 363/1000 | Loss: 0.00003360
Iteration 364/1000 | Loss: 0.00003359
Iteration 365/1000 | Loss: 0.00003359
Iteration 366/1000 | Loss: 0.00003358
Iteration 367/1000 | Loss: 0.00003357
Iteration 368/1000 | Loss: 0.00003357
Iteration 369/1000 | Loss: 0.00003356
Iteration 370/1000 | Loss: 0.00003356
Iteration 371/1000 | Loss: 0.00003356
Iteration 372/1000 | Loss: 0.00003356
Iteration 373/1000 | Loss: 0.00003354
Iteration 374/1000 | Loss: 0.00003354
Iteration 375/1000 | Loss: 0.00003354
Iteration 376/1000 | Loss: 0.00003354
Iteration 377/1000 | Loss: 0.00003354
Iteration 378/1000 | Loss: 0.00003354
Iteration 379/1000 | Loss: 0.00003354
Iteration 380/1000 | Loss: 0.00003354
Iteration 381/1000 | Loss: 0.00003354
Iteration 382/1000 | Loss: 0.00003354
Iteration 383/1000 | Loss: 0.00003353
Iteration 384/1000 | Loss: 0.00003353
Iteration 385/1000 | Loss: 0.00003353
Iteration 386/1000 | Loss: 0.00003352
Iteration 387/1000 | Loss: 0.00003352
Iteration 388/1000 | Loss: 0.00003351
Iteration 389/1000 | Loss: 0.00003351
Iteration 390/1000 | Loss: 0.00003351
Iteration 391/1000 | Loss: 0.00003351
Iteration 392/1000 | Loss: 0.00003350
Iteration 393/1000 | Loss: 0.00003349
Iteration 394/1000 | Loss: 0.00003349
Iteration 395/1000 | Loss: 0.00003349
Iteration 396/1000 | Loss: 0.00003348
Iteration 397/1000 | Loss: 0.00003348
Iteration 398/1000 | Loss: 0.00003347
Iteration 399/1000 | Loss: 0.00003347
Iteration 400/1000 | Loss: 0.00003347
Iteration 401/1000 | Loss: 0.00003347
Iteration 402/1000 | Loss: 0.00003346
Iteration 403/1000 | Loss: 0.00003346
Iteration 404/1000 | Loss: 0.00003345
Iteration 405/1000 | Loss: 0.00003345
Iteration 406/1000 | Loss: 0.00003344
Iteration 407/1000 | Loss: 0.00003344
Iteration 408/1000 | Loss: 0.00003344
Iteration 409/1000 | Loss: 0.00003344
Iteration 410/1000 | Loss: 0.00003343
Iteration 411/1000 | Loss: 0.00003343
Iteration 412/1000 | Loss: 0.00003342
Iteration 413/1000 | Loss: 0.00003342
Iteration 414/1000 | Loss: 0.00003342
Iteration 415/1000 | Loss: 0.00003341
Iteration 416/1000 | Loss: 0.00003339
Iteration 417/1000 | Loss: 0.00003337
Iteration 418/1000 | Loss: 0.00003337
Iteration 419/1000 | Loss: 0.00003337
Iteration 420/1000 | Loss: 0.00003337
Iteration 421/1000 | Loss: 0.00003337
Iteration 422/1000 | Loss: 0.00003337
Iteration 423/1000 | Loss: 0.00003337
Iteration 424/1000 | Loss: 0.00003337
Iteration 425/1000 | Loss: 0.00003336
Iteration 426/1000 | Loss: 0.00003336
Iteration 427/1000 | Loss: 0.00003336
Iteration 428/1000 | Loss: 0.00003335
Iteration 429/1000 | Loss: 0.00003335
Iteration 430/1000 | Loss: 0.00003334
Iteration 431/1000 | Loss: 0.00003334
Iteration 432/1000 | Loss: 0.00003334
Iteration 433/1000 | Loss: 0.00003334
Iteration 434/1000 | Loss: 0.00003334
Iteration 435/1000 | Loss: 0.00003334
Iteration 436/1000 | Loss: 0.00003334
Iteration 437/1000 | Loss: 0.00003334
Iteration 438/1000 | Loss: 0.00003334
Iteration 439/1000 | Loss: 0.00003334
Iteration 440/1000 | Loss: 0.00003334
Iteration 441/1000 | Loss: 0.00003334
Iteration 442/1000 | Loss: 0.00003334
Iteration 443/1000 | Loss: 0.00003334
Iteration 444/1000 | Loss: 0.00003334
Iteration 445/1000 | Loss: 0.00003333
Iteration 446/1000 | Loss: 0.00003333
Iteration 447/1000 | Loss: 0.00003333
Iteration 448/1000 | Loss: 0.00003333
Iteration 449/1000 | Loss: 0.00003333
Iteration 450/1000 | Loss: 0.00003333
Iteration 451/1000 | Loss: 0.00003333
Iteration 452/1000 | Loss: 0.00003333
Iteration 453/1000 | Loss: 0.00003333
Iteration 454/1000 | Loss: 0.00003333
Iteration 455/1000 | Loss: 0.00003333
Iteration 456/1000 | Loss: 0.00003333
Iteration 457/1000 | Loss: 0.00003332
Iteration 458/1000 | Loss: 0.00003332
Iteration 459/1000 | Loss: 0.00003332
Iteration 460/1000 | Loss: 0.00003332
Iteration 461/1000 | Loss: 0.00003332
Iteration 462/1000 | Loss: 0.00003332
Iteration 463/1000 | Loss: 0.00003332
Iteration 464/1000 | Loss: 0.00003332
Iteration 465/1000 | Loss: 0.00003332
Iteration 466/1000 | Loss: 0.00003332
Iteration 467/1000 | Loss: 0.00003332
Iteration 468/1000 | Loss: 0.00003332
Iteration 469/1000 | Loss: 0.00003332
Iteration 470/1000 | Loss: 0.00003332
Iteration 471/1000 | Loss: 0.00003332
Iteration 472/1000 | Loss: 0.00003332
Iteration 473/1000 | Loss: 0.00003332
Iteration 474/1000 | Loss: 0.00003331
Iteration 475/1000 | Loss: 0.00003331
Iteration 476/1000 | Loss: 0.00003331
Iteration 477/1000 | Loss: 0.00003331
Iteration 478/1000 | Loss: 0.00003331
Iteration 479/1000 | Loss: 0.00003331
Iteration 480/1000 | Loss: 0.00003331
Iteration 481/1000 | Loss: 0.00003331
Iteration 482/1000 | Loss: 0.00003331
Iteration 483/1000 | Loss: 0.00003331
Iteration 484/1000 | Loss: 0.00003331
Iteration 485/1000 | Loss: 0.00003331
Iteration 486/1000 | Loss: 0.00003331
Iteration 487/1000 | Loss: 0.00003331
Iteration 488/1000 | Loss: 0.00003331
Iteration 489/1000 | Loss: 0.00003331
Iteration 490/1000 | Loss: 0.00003331
Iteration 491/1000 | Loss: 0.00003331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 491. Stopping optimization.
Last 5 losses: [3.33125026372727e-05, 3.33125026372727e-05, 3.33125026372727e-05, 3.33125026372727e-05, 3.33125026372727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.33125026372727e-05

Optimization complete. Final v2v error: 3.790005683898926 mm

Highest mean error: 13.44020938873291 mm for frame 23

Lowest mean error: 2.588179588317871 mm for frame 133

Saving results

Total time: 8580.445486783981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1047
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440769
Iteration 2/25 | Loss: 0.00113224
Iteration 3/25 | Loss: 0.00095285
Iteration 4/25 | Loss: 0.00093990
Iteration 5/25 | Loss: 0.00093555
Iteration 6/25 | Loss: 0.00093385
Iteration 7/25 | Loss: 0.00093385
Iteration 8/25 | Loss: 0.00093385
Iteration 9/25 | Loss: 0.00093385
Iteration 10/25 | Loss: 0.00093385
Iteration 11/25 | Loss: 0.00093385
Iteration 12/25 | Loss: 0.00093385
Iteration 13/25 | Loss: 0.00093385
Iteration 14/25 | Loss: 0.00093385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009338473901152611, 0.0009338473901152611, 0.0009338473901152611, 0.0009338473901152611, 0.0009338473901152611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009338473901152611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35362601
Iteration 2/25 | Loss: 0.00063243
Iteration 3/25 | Loss: 0.00063243
Iteration 4/25 | Loss: 0.00063243
Iteration 5/25 | Loss: 0.00063243
Iteration 6/25 | Loss: 0.00063243
Iteration 7/25 | Loss: 0.00063243
Iteration 8/25 | Loss: 0.00063243
Iteration 9/25 | Loss: 0.00063243
Iteration 10/25 | Loss: 0.00063243
Iteration 11/25 | Loss: 0.00063243
Iteration 12/25 | Loss: 0.00063243
Iteration 13/25 | Loss: 0.00063243
Iteration 14/25 | Loss: 0.00063243
Iteration 15/25 | Loss: 0.00063243
Iteration 16/25 | Loss: 0.00063243
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006324294372461736, 0.0006324294372461736, 0.0006324294372461736, 0.0006324294372461736, 0.0006324294372461736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006324294372461736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063243
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001482
Iteration 4/1000 | Loss: 0.00001398
Iteration 5/1000 | Loss: 0.00001364
Iteration 6/1000 | Loss: 0.00001331
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001289
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001264
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001245
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001245
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.2284832337172702e-05, 1.2284832337172702e-05, 1.2284832337172702e-05, 1.2284832337172702e-05, 1.2284832337172702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2284832337172702e-05

Optimization complete. Final v2v error: 2.8750717639923096 mm

Highest mean error: 3.0209081172943115 mm for frame 135

Lowest mean error: 2.5338447093963623 mm for frame 232

Saving results

Total time: 1051.521291732788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1016
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841899
Iteration 2/25 | Loss: 0.00140222
Iteration 3/25 | Loss: 0.00103291
Iteration 4/25 | Loss: 0.00097932
Iteration 5/25 | Loss: 0.00096473
Iteration 6/25 | Loss: 0.00096164
Iteration 7/25 | Loss: 0.00096059
Iteration 8/25 | Loss: 0.00096041
Iteration 9/25 | Loss: 0.00095782
Iteration 10/25 | Loss: 0.00095759
Iteration 11/25 | Loss: 0.00095710
Iteration 12/25 | Loss: 0.00095672
Iteration 13/25 | Loss: 0.00095665
Iteration 14/25 | Loss: 0.00095662
Iteration 15/25 | Loss: 0.00095662
Iteration 16/25 | Loss: 0.00095662
Iteration 17/25 | Loss: 0.00095661
Iteration 18/25 | Loss: 0.00095661
Iteration 19/25 | Loss: 0.00095661
Iteration 20/25 | Loss: 0.00095661
Iteration 21/25 | Loss: 0.00095661
Iteration 22/25 | Loss: 0.00095661
Iteration 23/25 | Loss: 0.00095660
Iteration 24/25 | Loss: 0.00095660
Iteration 25/25 | Loss: 0.00095660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09294748
Iteration 2/25 | Loss: 0.00065672
Iteration 3/25 | Loss: 0.00065670
Iteration 4/25 | Loss: 0.00065670
Iteration 5/25 | Loss: 0.00065670
Iteration 6/25 | Loss: 0.00065670
Iteration 7/25 | Loss: 0.00065670
Iteration 8/25 | Loss: 0.00065670
Iteration 9/25 | Loss: 0.00065670
Iteration 10/25 | Loss: 0.00065670
Iteration 11/25 | Loss: 0.00065670
Iteration 12/25 | Loss: 0.00065670
Iteration 13/25 | Loss: 0.00065670
Iteration 14/25 | Loss: 0.00065670
Iteration 15/25 | Loss: 0.00065670
Iteration 16/25 | Loss: 0.00065670
Iteration 17/25 | Loss: 0.00065670
Iteration 18/25 | Loss: 0.00065670
Iteration 19/25 | Loss: 0.00065670
Iteration 20/25 | Loss: 0.00065670
Iteration 21/25 | Loss: 0.00065670
Iteration 22/25 | Loss: 0.00065670
Iteration 23/25 | Loss: 0.00065670
Iteration 24/25 | Loss: 0.00065670
Iteration 25/25 | Loss: 0.00065670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006566967349499464, 0.0006566967349499464, 0.0006566967349499464, 0.0006566967349499464, 0.0006566967349499464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006566967349499464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065670
Iteration 2/1000 | Loss: 0.00002031
Iteration 3/1000 | Loss: 0.00002577
Iteration 4/1000 | Loss: 0.00001386
Iteration 5/1000 | Loss: 0.00002603
Iteration 6/1000 | Loss: 0.00001371
Iteration 7/1000 | Loss: 0.00002549
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001234
Iteration 10/1000 | Loss: 0.00002187
Iteration 11/1000 | Loss: 0.00001207
Iteration 12/1000 | Loss: 0.00001202
Iteration 13/1000 | Loss: 0.00001202
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001193
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001187
Iteration 19/1000 | Loss: 0.00001184
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001178
Iteration 24/1000 | Loss: 0.00001174
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001164
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001158
Iteration 106/1000 | Loss: 0.00001158
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1584794265218079e-05, 1.1584794265218079e-05, 1.1584794265218079e-05, 1.1584794265218079e-05, 1.1584794265218079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1584794265218079e-05

Optimization complete. Final v2v error: 2.8106744289398193 mm

Highest mean error: 9.440967559814453 mm for frame 177

Lowest mean error: 2.478654623031616 mm for frame 192

Saving results

Total time: 1749.1217749118805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_014/1065/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_014/1065. Skipping.
