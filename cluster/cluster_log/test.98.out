Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=98, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5488-5543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085825
Iteration 2/25 | Loss: 0.00190184
Iteration 3/25 | Loss: 0.00151856
Iteration 4/25 | Loss: 0.00144166
Iteration 5/25 | Loss: 0.00145902
Iteration 6/25 | Loss: 0.00143777
Iteration 7/25 | Loss: 0.00136415
Iteration 8/25 | Loss: 0.00131281
Iteration 9/25 | Loss: 0.00130466
Iteration 10/25 | Loss: 0.00129719
Iteration 11/25 | Loss: 0.00129415
Iteration 12/25 | Loss: 0.00127846
Iteration 13/25 | Loss: 0.00127317
Iteration 14/25 | Loss: 0.00127565
Iteration 15/25 | Loss: 0.00127532
Iteration 16/25 | Loss: 0.00127061
Iteration 17/25 | Loss: 0.00126761
Iteration 18/25 | Loss: 0.00126556
Iteration 19/25 | Loss: 0.00126462
Iteration 20/25 | Loss: 0.00126938
Iteration 21/25 | Loss: 0.00126995
Iteration 22/25 | Loss: 0.00126894
Iteration 23/25 | Loss: 0.00126745
Iteration 24/25 | Loss: 0.00126700
Iteration 25/25 | Loss: 0.00126321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78297675
Iteration 2/25 | Loss: 0.00103341
Iteration 3/25 | Loss: 0.00103341
Iteration 4/25 | Loss: 0.00103341
Iteration 5/25 | Loss: 0.00103341
Iteration 6/25 | Loss: 0.00103341
Iteration 7/25 | Loss: 0.00103341
Iteration 8/25 | Loss: 0.00103341
Iteration 9/25 | Loss: 0.00103341
Iteration 10/25 | Loss: 0.00103341
Iteration 11/25 | Loss: 0.00103341
Iteration 12/25 | Loss: 0.00103341
Iteration 13/25 | Loss: 0.00103341
Iteration 14/25 | Loss: 0.00103341
Iteration 15/25 | Loss: 0.00103341
Iteration 16/25 | Loss: 0.00103341
Iteration 17/25 | Loss: 0.00103341
Iteration 18/25 | Loss: 0.00103341
Iteration 19/25 | Loss: 0.00103341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001033410313539207, 0.001033410313539207, 0.001033410313539207, 0.001033410313539207, 0.001033410313539207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001033410313539207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103341
Iteration 2/1000 | Loss: 0.00004662
Iteration 3/1000 | Loss: 0.00003375
Iteration 4/1000 | Loss: 0.00022851
Iteration 5/1000 | Loss: 0.00013809
Iteration 6/1000 | Loss: 0.00002761
Iteration 7/1000 | Loss: 0.00021013
Iteration 8/1000 | Loss: 0.00003549
Iteration 9/1000 | Loss: 0.00002916
Iteration 10/1000 | Loss: 0.00002530
Iteration 11/1000 | Loss: 0.00002434
Iteration 12/1000 | Loss: 0.00002370
Iteration 13/1000 | Loss: 0.00002292
Iteration 14/1000 | Loss: 0.00002249
Iteration 15/1000 | Loss: 0.00002220
Iteration 16/1000 | Loss: 0.00002195
Iteration 17/1000 | Loss: 0.00002188
Iteration 18/1000 | Loss: 0.00002177
Iteration 19/1000 | Loss: 0.00002173
Iteration 20/1000 | Loss: 0.00002170
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00002152
Iteration 27/1000 | Loss: 0.00002152
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002148
Iteration 30/1000 | Loss: 0.00002148
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002146
Iteration 33/1000 | Loss: 0.00002145
Iteration 34/1000 | Loss: 0.00002145
Iteration 35/1000 | Loss: 0.00002145
Iteration 36/1000 | Loss: 0.00002145
Iteration 37/1000 | Loss: 0.00002145
Iteration 38/1000 | Loss: 0.00002145
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002145
Iteration 41/1000 | Loss: 0.00002145
Iteration 42/1000 | Loss: 0.00002144
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002144
Iteration 45/1000 | Loss: 0.00002144
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002143
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00002142
Iteration 50/1000 | Loss: 0.00002142
Iteration 51/1000 | Loss: 0.00002142
Iteration 52/1000 | Loss: 0.00002142
Iteration 53/1000 | Loss: 0.00002142
Iteration 54/1000 | Loss: 0.00002141
Iteration 55/1000 | Loss: 0.00002141
Iteration 56/1000 | Loss: 0.00002141
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00002141
Iteration 59/1000 | Loss: 0.00002140
Iteration 60/1000 | Loss: 0.00002140
Iteration 61/1000 | Loss: 0.00002140
Iteration 62/1000 | Loss: 0.00002140
Iteration 63/1000 | Loss: 0.00002140
Iteration 64/1000 | Loss: 0.00002139
Iteration 65/1000 | Loss: 0.00002139
Iteration 66/1000 | Loss: 0.00002139
Iteration 67/1000 | Loss: 0.00002139
Iteration 68/1000 | Loss: 0.00002139
Iteration 69/1000 | Loss: 0.00002139
Iteration 70/1000 | Loss: 0.00002138
Iteration 71/1000 | Loss: 0.00002138
Iteration 72/1000 | Loss: 0.00002138
Iteration 73/1000 | Loss: 0.00002138
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002138
Iteration 76/1000 | Loss: 0.00002138
Iteration 77/1000 | Loss: 0.00002138
Iteration 78/1000 | Loss: 0.00002137
Iteration 79/1000 | Loss: 0.00002137
Iteration 80/1000 | Loss: 0.00002137
Iteration 81/1000 | Loss: 0.00002137
Iteration 82/1000 | Loss: 0.00002136
Iteration 83/1000 | Loss: 0.00002136
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002136
Iteration 86/1000 | Loss: 0.00002135
Iteration 87/1000 | Loss: 0.00002135
Iteration 88/1000 | Loss: 0.00002135
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002134
Iteration 91/1000 | Loss: 0.00002134
Iteration 92/1000 | Loss: 0.00002133
Iteration 93/1000 | Loss: 0.00002133
Iteration 94/1000 | Loss: 0.00002133
Iteration 95/1000 | Loss: 0.00002132
Iteration 96/1000 | Loss: 0.00002132
Iteration 97/1000 | Loss: 0.00002132
Iteration 98/1000 | Loss: 0.00002132
Iteration 99/1000 | Loss: 0.00002132
Iteration 100/1000 | Loss: 0.00002132
Iteration 101/1000 | Loss: 0.00002131
Iteration 102/1000 | Loss: 0.00002131
Iteration 103/1000 | Loss: 0.00002131
Iteration 104/1000 | Loss: 0.00002131
Iteration 105/1000 | Loss: 0.00002131
Iteration 106/1000 | Loss: 0.00002131
Iteration 107/1000 | Loss: 0.00002131
Iteration 108/1000 | Loss: 0.00002131
Iteration 109/1000 | Loss: 0.00002131
Iteration 110/1000 | Loss: 0.00002130
Iteration 111/1000 | Loss: 0.00002129
Iteration 112/1000 | Loss: 0.00002129
Iteration 113/1000 | Loss: 0.00002129
Iteration 114/1000 | Loss: 0.00002129
Iteration 115/1000 | Loss: 0.00002129
Iteration 116/1000 | Loss: 0.00002129
Iteration 117/1000 | Loss: 0.00002129
Iteration 118/1000 | Loss: 0.00002129
Iteration 119/1000 | Loss: 0.00002129
Iteration 120/1000 | Loss: 0.00002129
Iteration 121/1000 | Loss: 0.00002129
Iteration 122/1000 | Loss: 0.00002129
Iteration 123/1000 | Loss: 0.00002129
Iteration 124/1000 | Loss: 0.00002129
Iteration 125/1000 | Loss: 0.00002129
Iteration 126/1000 | Loss: 0.00002129
Iteration 127/1000 | Loss: 0.00002129
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Iteration 130/1000 | Loss: 0.00002129
Iteration 131/1000 | Loss: 0.00002129
Iteration 132/1000 | Loss: 0.00002129
Iteration 133/1000 | Loss: 0.00002129
Iteration 134/1000 | Loss: 0.00002129
Iteration 135/1000 | Loss: 0.00002129
Iteration 136/1000 | Loss: 0.00002129
Iteration 137/1000 | Loss: 0.00002129
Iteration 138/1000 | Loss: 0.00002129
Iteration 139/1000 | Loss: 0.00002129
Iteration 140/1000 | Loss: 0.00002129
Iteration 141/1000 | Loss: 0.00002129
Iteration 142/1000 | Loss: 0.00002129
Iteration 143/1000 | Loss: 0.00002129
Iteration 144/1000 | Loss: 0.00002129
Iteration 145/1000 | Loss: 0.00002129
Iteration 146/1000 | Loss: 0.00002129
Iteration 147/1000 | Loss: 0.00002129
Iteration 148/1000 | Loss: 0.00002129
Iteration 149/1000 | Loss: 0.00002129
Iteration 150/1000 | Loss: 0.00002129
Iteration 151/1000 | Loss: 0.00002129
Iteration 152/1000 | Loss: 0.00002129
Iteration 153/1000 | Loss: 0.00002129
Iteration 154/1000 | Loss: 0.00002129
Iteration 155/1000 | Loss: 0.00002129
Iteration 156/1000 | Loss: 0.00002129
Iteration 157/1000 | Loss: 0.00002129
Iteration 158/1000 | Loss: 0.00002129
Iteration 159/1000 | Loss: 0.00002129
Iteration 160/1000 | Loss: 0.00002129
Iteration 161/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.1290970835252665e-05, 2.1290970835252665e-05, 2.1290970835252665e-05, 2.1290970835252665e-05, 2.1290970835252665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1290970835252665e-05

Optimization complete. Final v2v error: 3.7553298473358154 mm

Highest mean error: 5.470663070678711 mm for frame 117

Lowest mean error: 3.3676865100860596 mm for frame 1

Saving results

Total time: 84.75907182693481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436969
Iteration 2/25 | Loss: 0.00120006
Iteration 3/25 | Loss: 0.00113259
Iteration 4/25 | Loss: 0.00112599
Iteration 5/25 | Loss: 0.00112382
Iteration 6/25 | Loss: 0.00112339
Iteration 7/25 | Loss: 0.00112339
Iteration 8/25 | Loss: 0.00112339
Iteration 9/25 | Loss: 0.00112339
Iteration 10/25 | Loss: 0.00112339
Iteration 11/25 | Loss: 0.00112339
Iteration 12/25 | Loss: 0.00112339
Iteration 13/25 | Loss: 0.00112339
Iteration 14/25 | Loss: 0.00112339
Iteration 15/25 | Loss: 0.00112339
Iteration 16/25 | Loss: 0.00112339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011233888799324632, 0.0011233888799324632, 0.0011233888799324632, 0.0011233888799324632, 0.0011233888799324632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011233888799324632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94289064
Iteration 2/25 | Loss: 0.00115128
Iteration 3/25 | Loss: 0.00115128
Iteration 4/25 | Loss: 0.00115128
Iteration 5/25 | Loss: 0.00115128
Iteration 6/25 | Loss: 0.00115128
Iteration 7/25 | Loss: 0.00115128
Iteration 8/25 | Loss: 0.00115128
Iteration 9/25 | Loss: 0.00115128
Iteration 10/25 | Loss: 0.00115128
Iteration 11/25 | Loss: 0.00115128
Iteration 12/25 | Loss: 0.00115128
Iteration 13/25 | Loss: 0.00115128
Iteration 14/25 | Loss: 0.00115128
Iteration 15/25 | Loss: 0.00115128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011512793134897947, 0.0011512793134897947, 0.0011512793134897947, 0.0011512793134897947, 0.0011512793134897947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011512793134897947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115128
Iteration 2/1000 | Loss: 0.00002216
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001560
Iteration 5/1000 | Loss: 0.00001451
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00001323
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001320
Iteration 29/1000 | Loss: 0.00001319
Iteration 30/1000 | Loss: 0.00001319
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001317
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001316
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001314
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001313
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001303
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001302
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001301
Iteration 82/1000 | Loss: 0.00001301
Iteration 83/1000 | Loss: 0.00001301
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001300
Iteration 86/1000 | Loss: 0.00001300
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001297
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001295
Iteration 120/1000 | Loss: 0.00001295
Iteration 121/1000 | Loss: 0.00001295
Iteration 122/1000 | Loss: 0.00001295
Iteration 123/1000 | Loss: 0.00001295
Iteration 124/1000 | Loss: 0.00001295
Iteration 125/1000 | Loss: 0.00001295
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001295
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001293
Iteration 137/1000 | Loss: 0.00001293
Iteration 138/1000 | Loss: 0.00001293
Iteration 139/1000 | Loss: 0.00001293
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001293
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001292
Iteration 146/1000 | Loss: 0.00001292
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001292
Iteration 149/1000 | Loss: 0.00001292
Iteration 150/1000 | Loss: 0.00001292
Iteration 151/1000 | Loss: 0.00001292
Iteration 152/1000 | Loss: 0.00001292
Iteration 153/1000 | Loss: 0.00001292
Iteration 154/1000 | Loss: 0.00001292
Iteration 155/1000 | Loss: 0.00001292
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001291
Iteration 160/1000 | Loss: 0.00001291
Iteration 161/1000 | Loss: 0.00001291
Iteration 162/1000 | Loss: 0.00001291
Iteration 163/1000 | Loss: 0.00001291
Iteration 164/1000 | Loss: 0.00001291
Iteration 165/1000 | Loss: 0.00001291
Iteration 166/1000 | Loss: 0.00001290
Iteration 167/1000 | Loss: 0.00001290
Iteration 168/1000 | Loss: 0.00001290
Iteration 169/1000 | Loss: 0.00001290
Iteration 170/1000 | Loss: 0.00001290
Iteration 171/1000 | Loss: 0.00001290
Iteration 172/1000 | Loss: 0.00001290
Iteration 173/1000 | Loss: 0.00001290
Iteration 174/1000 | Loss: 0.00001290
Iteration 175/1000 | Loss: 0.00001290
Iteration 176/1000 | Loss: 0.00001290
Iteration 177/1000 | Loss: 0.00001290
Iteration 178/1000 | Loss: 0.00001290
Iteration 179/1000 | Loss: 0.00001290
Iteration 180/1000 | Loss: 0.00001290
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001290
Iteration 183/1000 | Loss: 0.00001290
Iteration 184/1000 | Loss: 0.00001290
Iteration 185/1000 | Loss: 0.00001290
Iteration 186/1000 | Loss: 0.00001290
Iteration 187/1000 | Loss: 0.00001290
Iteration 188/1000 | Loss: 0.00001290
Iteration 189/1000 | Loss: 0.00001290
Iteration 190/1000 | Loss: 0.00001290
Iteration 191/1000 | Loss: 0.00001290
Iteration 192/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.289747524424456e-05, 1.289747524424456e-05, 1.289747524424456e-05, 1.289747524424456e-05, 1.289747524424456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.289747524424456e-05

Optimization complete. Final v2v error: 3.1110055446624756 mm

Highest mean error: 3.7696239948272705 mm for frame 94

Lowest mean error: 2.6952624320983887 mm for frame 52

Saving results

Total time: 35.201908111572266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760652
Iteration 2/25 | Loss: 0.00144716
Iteration 3/25 | Loss: 0.00126962
Iteration 4/25 | Loss: 0.00125085
Iteration 5/25 | Loss: 0.00124860
Iteration 6/25 | Loss: 0.00124860
Iteration 7/25 | Loss: 0.00124860
Iteration 8/25 | Loss: 0.00124860
Iteration 9/25 | Loss: 0.00124860
Iteration 10/25 | Loss: 0.00124860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012485996121540666, 0.0012485996121540666, 0.0012485996121540666, 0.0012485996121540666, 0.0012485996121540666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012485996121540666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.98250484
Iteration 2/25 | Loss: 0.00118539
Iteration 3/25 | Loss: 0.00118537
Iteration 4/25 | Loss: 0.00118537
Iteration 5/25 | Loss: 0.00118537
Iteration 6/25 | Loss: 0.00118537
Iteration 7/25 | Loss: 0.00118537
Iteration 8/25 | Loss: 0.00118537
Iteration 9/25 | Loss: 0.00118537
Iteration 10/25 | Loss: 0.00118537
Iteration 11/25 | Loss: 0.00118537
Iteration 12/25 | Loss: 0.00118537
Iteration 13/25 | Loss: 0.00118537
Iteration 14/25 | Loss: 0.00118537
Iteration 15/25 | Loss: 0.00118537
Iteration 16/25 | Loss: 0.00118537
Iteration 17/25 | Loss: 0.00118537
Iteration 18/25 | Loss: 0.00118537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011853673495352268, 0.0011853673495352268, 0.0011853673495352268, 0.0011853673495352268, 0.0011853673495352268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011853673495352268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118537
Iteration 2/1000 | Loss: 0.00006030
Iteration 3/1000 | Loss: 0.00003740
Iteration 4/1000 | Loss: 0.00003129
Iteration 5/1000 | Loss: 0.00002817
Iteration 6/1000 | Loss: 0.00002620
Iteration 7/1000 | Loss: 0.00002504
Iteration 8/1000 | Loss: 0.00002429
Iteration 9/1000 | Loss: 0.00002375
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002318
Iteration 12/1000 | Loss: 0.00002302
Iteration 13/1000 | Loss: 0.00002295
Iteration 14/1000 | Loss: 0.00002291
Iteration 15/1000 | Loss: 0.00002291
Iteration 16/1000 | Loss: 0.00002291
Iteration 17/1000 | Loss: 0.00002291
Iteration 18/1000 | Loss: 0.00002290
Iteration 19/1000 | Loss: 0.00002290
Iteration 20/1000 | Loss: 0.00002290
Iteration 21/1000 | Loss: 0.00002290
Iteration 22/1000 | Loss: 0.00002290
Iteration 23/1000 | Loss: 0.00002289
Iteration 24/1000 | Loss: 0.00002288
Iteration 25/1000 | Loss: 0.00002288
Iteration 26/1000 | Loss: 0.00002287
Iteration 27/1000 | Loss: 0.00002287
Iteration 28/1000 | Loss: 0.00002287
Iteration 29/1000 | Loss: 0.00002287
Iteration 30/1000 | Loss: 0.00002287
Iteration 31/1000 | Loss: 0.00002287
Iteration 32/1000 | Loss: 0.00002287
Iteration 33/1000 | Loss: 0.00002287
Iteration 34/1000 | Loss: 0.00002287
Iteration 35/1000 | Loss: 0.00002287
Iteration 36/1000 | Loss: 0.00002287
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002287
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00002286
Iteration 42/1000 | Loss: 0.00002286
Iteration 43/1000 | Loss: 0.00002286
Iteration 44/1000 | Loss: 0.00002285
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002285
Iteration 47/1000 | Loss: 0.00002285
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002284
Iteration 51/1000 | Loss: 0.00002284
Iteration 52/1000 | Loss: 0.00002284
Iteration 53/1000 | Loss: 0.00002283
Iteration 54/1000 | Loss: 0.00002283
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002283
Iteration 57/1000 | Loss: 0.00002283
Iteration 58/1000 | Loss: 0.00002283
Iteration 59/1000 | Loss: 0.00002283
Iteration 60/1000 | Loss: 0.00002282
Iteration 61/1000 | Loss: 0.00002282
Iteration 62/1000 | Loss: 0.00002282
Iteration 63/1000 | Loss: 0.00002282
Iteration 64/1000 | Loss: 0.00002282
Iteration 65/1000 | Loss: 0.00002282
Iteration 66/1000 | Loss: 0.00002282
Iteration 67/1000 | Loss: 0.00002282
Iteration 68/1000 | Loss: 0.00002282
Iteration 69/1000 | Loss: 0.00002282
Iteration 70/1000 | Loss: 0.00002282
Iteration 71/1000 | Loss: 0.00002282
Iteration 72/1000 | Loss: 0.00002281
Iteration 73/1000 | Loss: 0.00002281
Iteration 74/1000 | Loss: 0.00002280
Iteration 75/1000 | Loss: 0.00002280
Iteration 76/1000 | Loss: 0.00002280
Iteration 77/1000 | Loss: 0.00002280
Iteration 78/1000 | Loss: 0.00002280
Iteration 79/1000 | Loss: 0.00002280
Iteration 80/1000 | Loss: 0.00002280
Iteration 81/1000 | Loss: 0.00002280
Iteration 82/1000 | Loss: 0.00002279
Iteration 83/1000 | Loss: 0.00002279
Iteration 84/1000 | Loss: 0.00002279
Iteration 85/1000 | Loss: 0.00002279
Iteration 86/1000 | Loss: 0.00002279
Iteration 87/1000 | Loss: 0.00002279
Iteration 88/1000 | Loss: 0.00002279
Iteration 89/1000 | Loss: 0.00002279
Iteration 90/1000 | Loss: 0.00002279
Iteration 91/1000 | Loss: 0.00002279
Iteration 92/1000 | Loss: 0.00002278
Iteration 93/1000 | Loss: 0.00002278
Iteration 94/1000 | Loss: 0.00002278
Iteration 95/1000 | Loss: 0.00002278
Iteration 96/1000 | Loss: 0.00002278
Iteration 97/1000 | Loss: 0.00002278
Iteration 98/1000 | Loss: 0.00002278
Iteration 99/1000 | Loss: 0.00002277
Iteration 100/1000 | Loss: 0.00002277
Iteration 101/1000 | Loss: 0.00002277
Iteration 102/1000 | Loss: 0.00002277
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002277
Iteration 107/1000 | Loss: 0.00002277
Iteration 108/1000 | Loss: 0.00002277
Iteration 109/1000 | Loss: 0.00002277
Iteration 110/1000 | Loss: 0.00002277
Iteration 111/1000 | Loss: 0.00002277
Iteration 112/1000 | Loss: 0.00002277
Iteration 113/1000 | Loss: 0.00002276
Iteration 114/1000 | Loss: 0.00002276
Iteration 115/1000 | Loss: 0.00002276
Iteration 116/1000 | Loss: 0.00002276
Iteration 117/1000 | Loss: 0.00002276
Iteration 118/1000 | Loss: 0.00002275
Iteration 119/1000 | Loss: 0.00002275
Iteration 120/1000 | Loss: 0.00002275
Iteration 121/1000 | Loss: 0.00002275
Iteration 122/1000 | Loss: 0.00002275
Iteration 123/1000 | Loss: 0.00002275
Iteration 124/1000 | Loss: 0.00002275
Iteration 125/1000 | Loss: 0.00002275
Iteration 126/1000 | Loss: 0.00002275
Iteration 127/1000 | Loss: 0.00002275
Iteration 128/1000 | Loss: 0.00002275
Iteration 129/1000 | Loss: 0.00002275
Iteration 130/1000 | Loss: 0.00002275
Iteration 131/1000 | Loss: 0.00002275
Iteration 132/1000 | Loss: 0.00002275
Iteration 133/1000 | Loss: 0.00002275
Iteration 134/1000 | Loss: 0.00002275
Iteration 135/1000 | Loss: 0.00002275
Iteration 136/1000 | Loss: 0.00002275
Iteration 137/1000 | Loss: 0.00002275
Iteration 138/1000 | Loss: 0.00002275
Iteration 139/1000 | Loss: 0.00002275
Iteration 140/1000 | Loss: 0.00002275
Iteration 141/1000 | Loss: 0.00002275
Iteration 142/1000 | Loss: 0.00002275
Iteration 143/1000 | Loss: 0.00002275
Iteration 144/1000 | Loss: 0.00002275
Iteration 145/1000 | Loss: 0.00002275
Iteration 146/1000 | Loss: 0.00002275
Iteration 147/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.275203041790519e-05, 2.275203041790519e-05, 2.275203041790519e-05, 2.275203041790519e-05, 2.275203041790519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.275203041790519e-05

Optimization complete. Final v2v error: 4.024885177612305 mm

Highest mean error: 4.459428787231445 mm for frame 19

Lowest mean error: 3.717311382293701 mm for frame 175

Saving results

Total time: 37.25524401664734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444277
Iteration 2/25 | Loss: 0.00131341
Iteration 3/25 | Loss: 0.00119794
Iteration 4/25 | Loss: 0.00118694
Iteration 5/25 | Loss: 0.00118462
Iteration 6/25 | Loss: 0.00118350
Iteration 7/25 | Loss: 0.00118348
Iteration 8/25 | Loss: 0.00118347
Iteration 9/25 | Loss: 0.00118347
Iteration 10/25 | Loss: 0.00118347
Iteration 11/25 | Loss: 0.00118347
Iteration 12/25 | Loss: 0.00118347
Iteration 13/25 | Loss: 0.00118347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011834698962047696, 0.0011834698962047696, 0.0011834698962047696, 0.0011834698962047696, 0.0011834698962047696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011834698962047696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30114710
Iteration 2/25 | Loss: 0.00105139
Iteration 3/25 | Loss: 0.00105138
Iteration 4/25 | Loss: 0.00105137
Iteration 5/25 | Loss: 0.00105137
Iteration 6/25 | Loss: 0.00105137
Iteration 7/25 | Loss: 0.00105137
Iteration 8/25 | Loss: 0.00105137
Iteration 9/25 | Loss: 0.00105137
Iteration 10/25 | Loss: 0.00105137
Iteration 11/25 | Loss: 0.00105137
Iteration 12/25 | Loss: 0.00105137
Iteration 13/25 | Loss: 0.00105137
Iteration 14/25 | Loss: 0.00105137
Iteration 15/25 | Loss: 0.00105137
Iteration 16/25 | Loss: 0.00105137
Iteration 17/25 | Loss: 0.00105137
Iteration 18/25 | Loss: 0.00105137
Iteration 19/25 | Loss: 0.00105137
Iteration 20/25 | Loss: 0.00105137
Iteration 21/25 | Loss: 0.00105137
Iteration 22/25 | Loss: 0.00105137
Iteration 23/25 | Loss: 0.00105137
Iteration 24/25 | Loss: 0.00105137
Iteration 25/25 | Loss: 0.00105137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105137
Iteration 2/1000 | Loss: 0.00005761
Iteration 3/1000 | Loss: 0.00003127
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001932
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001853
Iteration 10/1000 | Loss: 0.00001823
Iteration 11/1000 | Loss: 0.00001801
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001768
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001761
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001757
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001757
Iteration 27/1000 | Loss: 0.00001757
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001756
Iteration 30/1000 | Loss: 0.00001756
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00001756
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001756
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001753
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001753
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001753
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001745
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001742
Iteration 93/1000 | Loss: 0.00001742
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001740
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001739
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001738
Iteration 125/1000 | Loss: 0.00001738
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001735
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001735
Iteration 148/1000 | Loss: 0.00001735
Iteration 149/1000 | Loss: 0.00001735
Iteration 150/1000 | Loss: 0.00001735
Iteration 151/1000 | Loss: 0.00001735
Iteration 152/1000 | Loss: 0.00001735
Iteration 153/1000 | Loss: 0.00001735
Iteration 154/1000 | Loss: 0.00001735
Iteration 155/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7349091649521142e-05, 1.7349091649521142e-05, 1.7349091649521142e-05, 1.7349091649521142e-05, 1.7349091649521142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7349091649521142e-05

Optimization complete. Final v2v error: 3.4937589168548584 mm

Highest mean error: 4.053295612335205 mm for frame 37

Lowest mean error: 2.951939344406128 mm for frame 120

Saving results

Total time: 37.011040687561035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785384
Iteration 2/25 | Loss: 0.00129894
Iteration 3/25 | Loss: 0.00119673
Iteration 4/25 | Loss: 0.00118703
Iteration 5/25 | Loss: 0.00118395
Iteration 6/25 | Loss: 0.00118335
Iteration 7/25 | Loss: 0.00118335
Iteration 8/25 | Loss: 0.00118335
Iteration 9/25 | Loss: 0.00118335
Iteration 10/25 | Loss: 0.00118335
Iteration 11/25 | Loss: 0.00118335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011833544122055173, 0.0011833544122055173, 0.0011833544122055173, 0.0011833544122055173, 0.0011833544122055173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011833544122055173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11565876
Iteration 2/25 | Loss: 0.00079758
Iteration 3/25 | Loss: 0.00079757
Iteration 4/25 | Loss: 0.00079757
Iteration 5/25 | Loss: 0.00079757
Iteration 6/25 | Loss: 0.00079757
Iteration 7/25 | Loss: 0.00079757
Iteration 8/25 | Loss: 0.00079757
Iteration 9/25 | Loss: 0.00079757
Iteration 10/25 | Loss: 0.00079757
Iteration 11/25 | Loss: 0.00079757
Iteration 12/25 | Loss: 0.00079757
Iteration 13/25 | Loss: 0.00079757
Iteration 14/25 | Loss: 0.00079757
Iteration 15/25 | Loss: 0.00079757
Iteration 16/25 | Loss: 0.00079757
Iteration 17/25 | Loss: 0.00079757
Iteration 18/25 | Loss: 0.00079757
Iteration 19/25 | Loss: 0.00079757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007975681219249964, 0.0007975681219249964, 0.0007975681219249964, 0.0007975681219249964, 0.0007975681219249964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007975681219249964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079757
Iteration 2/1000 | Loss: 0.00004239
Iteration 3/1000 | Loss: 0.00002358
Iteration 4/1000 | Loss: 0.00002008
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001768
Iteration 7/1000 | Loss: 0.00001735
Iteration 8/1000 | Loss: 0.00001715
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001680
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001675
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001663
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001663
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001662
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001659
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001659
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001659
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001659
Iteration 53/1000 | Loss: 0.00001659
Iteration 54/1000 | Loss: 0.00001659
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001659
Iteration 59/1000 | Loss: 0.00001659
Iteration 60/1000 | Loss: 0.00001659
Iteration 61/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.6588492144364864e-05, 1.6588492144364864e-05, 1.6588492144364864e-05, 1.6588492144364864e-05, 1.6588492144364864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6588492144364864e-05

Optimization complete. Final v2v error: 3.5254290103912354 mm

Highest mean error: 3.876509428024292 mm for frame 10

Lowest mean error: 3.1522879600524902 mm for frame 133

Saving results

Total time: 25.94697070121765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121865
Iteration 2/25 | Loss: 0.01121865
Iteration 3/25 | Loss: 0.00231754
Iteration 4/25 | Loss: 0.00158928
Iteration 5/25 | Loss: 0.00136181
Iteration 6/25 | Loss: 0.00123034
Iteration 7/25 | Loss: 0.00117700
Iteration 8/25 | Loss: 0.00118309
Iteration 9/25 | Loss: 0.00114506
Iteration 10/25 | Loss: 0.00111677
Iteration 11/25 | Loss: 0.00110350
Iteration 12/25 | Loss: 0.00110615
Iteration 13/25 | Loss: 0.00109558
Iteration 14/25 | Loss: 0.00109491
Iteration 15/25 | Loss: 0.00109335
Iteration 16/25 | Loss: 0.00109215
Iteration 17/25 | Loss: 0.00109153
Iteration 18/25 | Loss: 0.00109539
Iteration 19/25 | Loss: 0.00108938
Iteration 20/25 | Loss: 0.00108922
Iteration 21/25 | Loss: 0.00109415
Iteration 22/25 | Loss: 0.00109048
Iteration 23/25 | Loss: 0.00108832
Iteration 24/25 | Loss: 0.00109282
Iteration 25/25 | Loss: 0.00108921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33946717
Iteration 2/25 | Loss: 0.00133805
Iteration 3/25 | Loss: 0.00133805
Iteration 4/25 | Loss: 0.00133805
Iteration 5/25 | Loss: 0.00133805
Iteration 6/25 | Loss: 0.00133805
Iteration 7/25 | Loss: 0.00133805
Iteration 8/25 | Loss: 0.00133805
Iteration 9/25 | Loss: 0.00133805
Iteration 10/25 | Loss: 0.00133805
Iteration 11/25 | Loss: 0.00133805
Iteration 12/25 | Loss: 0.00133805
Iteration 13/25 | Loss: 0.00133805
Iteration 14/25 | Loss: 0.00133805
Iteration 15/25 | Loss: 0.00133805
Iteration 16/25 | Loss: 0.00133805
Iteration 17/25 | Loss: 0.00133805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013380475575104356, 0.0013380475575104356, 0.0013380475575104356, 0.0013380475575104356, 0.0013380475575104356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013380475575104356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133805
Iteration 2/1000 | Loss: 0.00014648
Iteration 3/1000 | Loss: 0.00013151
Iteration 4/1000 | Loss: 0.00053881
Iteration 5/1000 | Loss: 0.00013397
Iteration 6/1000 | Loss: 0.00011898
Iteration 7/1000 | Loss: 0.00012781
Iteration 8/1000 | Loss: 0.00011446
Iteration 9/1000 | Loss: 0.00008870
Iteration 10/1000 | Loss: 0.00011359
Iteration 11/1000 | Loss: 0.00013172
Iteration 12/1000 | Loss: 0.00013554
Iteration 13/1000 | Loss: 0.00011406
Iteration 14/1000 | Loss: 0.00012201
Iteration 15/1000 | Loss: 0.00011169
Iteration 16/1000 | Loss: 0.00004511
Iteration 17/1000 | Loss: 0.00003629
Iteration 18/1000 | Loss: 0.00003428
Iteration 19/1000 | Loss: 0.00017330
Iteration 20/1000 | Loss: 0.00056886
Iteration 21/1000 | Loss: 0.00011405
Iteration 22/1000 | Loss: 0.00004900
Iteration 23/1000 | Loss: 0.00007577
Iteration 24/1000 | Loss: 0.00005258
Iteration 25/1000 | Loss: 0.00013382
Iteration 26/1000 | Loss: 0.00095989
Iteration 27/1000 | Loss: 0.00003653
Iteration 28/1000 | Loss: 0.00003103
Iteration 29/1000 | Loss: 0.00002899
Iteration 30/1000 | Loss: 0.00033662
Iteration 31/1000 | Loss: 0.00005033
Iteration 32/1000 | Loss: 0.00007945
Iteration 33/1000 | Loss: 0.00004601
Iteration 34/1000 | Loss: 0.00011094
Iteration 35/1000 | Loss: 0.00002812
Iteration 36/1000 | Loss: 0.00002681
Iteration 37/1000 | Loss: 0.00002643
Iteration 38/1000 | Loss: 0.00002618
Iteration 39/1000 | Loss: 0.00011188
Iteration 40/1000 | Loss: 0.00009976
Iteration 41/1000 | Loss: 0.00064031
Iteration 42/1000 | Loss: 0.00007555
Iteration 43/1000 | Loss: 0.00006729
Iteration 44/1000 | Loss: 0.00008858
Iteration 45/1000 | Loss: 0.00110054
Iteration 46/1000 | Loss: 0.00008873
Iteration 47/1000 | Loss: 0.00010716
Iteration 48/1000 | Loss: 0.00010576
Iteration 49/1000 | Loss: 0.00008016
Iteration 50/1000 | Loss: 0.00012925
Iteration 51/1000 | Loss: 0.00005103
Iteration 52/1000 | Loss: 0.00003563
Iteration 53/1000 | Loss: 0.00002662
Iteration 54/1000 | Loss: 0.00002608
Iteration 55/1000 | Loss: 0.00002583
Iteration 56/1000 | Loss: 0.00002574
Iteration 57/1000 | Loss: 0.00002573
Iteration 58/1000 | Loss: 0.00002571
Iteration 59/1000 | Loss: 0.00002571
Iteration 60/1000 | Loss: 0.00002571
Iteration 61/1000 | Loss: 0.00002571
Iteration 62/1000 | Loss: 0.00002570
Iteration 63/1000 | Loss: 0.00002570
Iteration 64/1000 | Loss: 0.00002570
Iteration 65/1000 | Loss: 0.00002570
Iteration 66/1000 | Loss: 0.00002570
Iteration 67/1000 | Loss: 0.00002564
Iteration 68/1000 | Loss: 0.00002564
Iteration 69/1000 | Loss: 0.00002564
Iteration 70/1000 | Loss: 0.00002564
Iteration 71/1000 | Loss: 0.00002564
Iteration 72/1000 | Loss: 0.00002564
Iteration 73/1000 | Loss: 0.00002564
Iteration 74/1000 | Loss: 0.00002563
Iteration 75/1000 | Loss: 0.00002562
Iteration 76/1000 | Loss: 0.00002562
Iteration 77/1000 | Loss: 0.00002561
Iteration 78/1000 | Loss: 0.00002560
Iteration 79/1000 | Loss: 0.00002560
Iteration 80/1000 | Loss: 0.00002560
Iteration 81/1000 | Loss: 0.00002560
Iteration 82/1000 | Loss: 0.00002560
Iteration 83/1000 | Loss: 0.00002560
Iteration 84/1000 | Loss: 0.00002560
Iteration 85/1000 | Loss: 0.00002560
Iteration 86/1000 | Loss: 0.00002559
Iteration 87/1000 | Loss: 0.00002559
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002559
Iteration 90/1000 | Loss: 0.00002559
Iteration 91/1000 | Loss: 0.00002559
Iteration 92/1000 | Loss: 0.00002559
Iteration 93/1000 | Loss: 0.00002559
Iteration 94/1000 | Loss: 0.00002559
Iteration 95/1000 | Loss: 0.00002559
Iteration 96/1000 | Loss: 0.00002556
Iteration 97/1000 | Loss: 0.00002556
Iteration 98/1000 | Loss: 0.00002555
Iteration 99/1000 | Loss: 0.00002555
Iteration 100/1000 | Loss: 0.00002555
Iteration 101/1000 | Loss: 0.00002555
Iteration 102/1000 | Loss: 0.00002554
Iteration 103/1000 | Loss: 0.00002554
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00002553
Iteration 106/1000 | Loss: 0.00002549
Iteration 107/1000 | Loss: 0.00002548
Iteration 108/1000 | Loss: 0.00002548
Iteration 109/1000 | Loss: 0.00002548
Iteration 110/1000 | Loss: 0.00002548
Iteration 111/1000 | Loss: 0.00002548
Iteration 112/1000 | Loss: 0.00002548
Iteration 113/1000 | Loss: 0.00002547
Iteration 114/1000 | Loss: 0.00002547
Iteration 115/1000 | Loss: 0.00002546
Iteration 116/1000 | Loss: 0.00002546
Iteration 117/1000 | Loss: 0.00002546
Iteration 118/1000 | Loss: 0.00002546
Iteration 119/1000 | Loss: 0.00002546
Iteration 120/1000 | Loss: 0.00002546
Iteration 121/1000 | Loss: 0.00002546
Iteration 122/1000 | Loss: 0.00002546
Iteration 123/1000 | Loss: 0.00002546
Iteration 124/1000 | Loss: 0.00002546
Iteration 125/1000 | Loss: 0.00002546
Iteration 126/1000 | Loss: 0.00002546
Iteration 127/1000 | Loss: 0.00002545
Iteration 128/1000 | Loss: 0.00002545
Iteration 129/1000 | Loss: 0.00002545
Iteration 130/1000 | Loss: 0.00002545
Iteration 131/1000 | Loss: 0.00002544
Iteration 132/1000 | Loss: 0.00002544
Iteration 133/1000 | Loss: 0.00002544
Iteration 134/1000 | Loss: 0.00002544
Iteration 135/1000 | Loss: 0.00002544
Iteration 136/1000 | Loss: 0.00002544
Iteration 137/1000 | Loss: 0.00002544
Iteration 138/1000 | Loss: 0.00002544
Iteration 139/1000 | Loss: 0.00002544
Iteration 140/1000 | Loss: 0.00002544
Iteration 141/1000 | Loss: 0.00002544
Iteration 142/1000 | Loss: 0.00002544
Iteration 143/1000 | Loss: 0.00002544
Iteration 144/1000 | Loss: 0.00002543
Iteration 145/1000 | Loss: 0.00002543
Iteration 146/1000 | Loss: 0.00002543
Iteration 147/1000 | Loss: 0.00002543
Iteration 148/1000 | Loss: 0.00002543
Iteration 149/1000 | Loss: 0.00002543
Iteration 150/1000 | Loss: 0.00002543
Iteration 151/1000 | Loss: 0.00002543
Iteration 152/1000 | Loss: 0.00002543
Iteration 153/1000 | Loss: 0.00002543
Iteration 154/1000 | Loss: 0.00002543
Iteration 155/1000 | Loss: 0.00002543
Iteration 156/1000 | Loss: 0.00002543
Iteration 157/1000 | Loss: 0.00002543
Iteration 158/1000 | Loss: 0.00002543
Iteration 159/1000 | Loss: 0.00002543
Iteration 160/1000 | Loss: 0.00002542
Iteration 161/1000 | Loss: 0.00002542
Iteration 162/1000 | Loss: 0.00002542
Iteration 163/1000 | Loss: 0.00002542
Iteration 164/1000 | Loss: 0.00002542
Iteration 165/1000 | Loss: 0.00002542
Iteration 166/1000 | Loss: 0.00002542
Iteration 167/1000 | Loss: 0.00002542
Iteration 168/1000 | Loss: 0.00002542
Iteration 169/1000 | Loss: 0.00002542
Iteration 170/1000 | Loss: 0.00002542
Iteration 171/1000 | Loss: 0.00002542
Iteration 172/1000 | Loss: 0.00002542
Iteration 173/1000 | Loss: 0.00002542
Iteration 174/1000 | Loss: 0.00002542
Iteration 175/1000 | Loss: 0.00002542
Iteration 176/1000 | Loss: 0.00002542
Iteration 177/1000 | Loss: 0.00002542
Iteration 178/1000 | Loss: 0.00002542
Iteration 179/1000 | Loss: 0.00002542
Iteration 180/1000 | Loss: 0.00002542
Iteration 181/1000 | Loss: 0.00002542
Iteration 182/1000 | Loss: 0.00002542
Iteration 183/1000 | Loss: 0.00002542
Iteration 184/1000 | Loss: 0.00002542
Iteration 185/1000 | Loss: 0.00002542
Iteration 186/1000 | Loss: 0.00002542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.542367292335257e-05, 2.542367292335257e-05, 2.542367292335257e-05, 2.542367292335257e-05, 2.542367292335257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.542367292335257e-05

Optimization complete. Final v2v error: 3.4370346069335938 mm

Highest mean error: 22.625307083129883 mm for frame 39

Lowest mean error: 2.875760078430176 mm for frame 90

Saving results

Total time: 126.06656384468079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566199
Iteration 2/25 | Loss: 0.00138168
Iteration 3/25 | Loss: 0.00125646
Iteration 4/25 | Loss: 0.00124714
Iteration 5/25 | Loss: 0.00124428
Iteration 6/25 | Loss: 0.00124422
Iteration 7/25 | Loss: 0.00124422
Iteration 8/25 | Loss: 0.00124422
Iteration 9/25 | Loss: 0.00124422
Iteration 10/25 | Loss: 0.00124422
Iteration 11/25 | Loss: 0.00124422
Iteration 12/25 | Loss: 0.00124422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012442154111340642, 0.0012442154111340642, 0.0012442154111340642, 0.0012442154111340642, 0.0012442154111340642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012442154111340642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76973033
Iteration 2/25 | Loss: 0.00090264
Iteration 3/25 | Loss: 0.00090263
Iteration 4/25 | Loss: 0.00090263
Iteration 5/25 | Loss: 0.00090263
Iteration 6/25 | Loss: 0.00090263
Iteration 7/25 | Loss: 0.00090262
Iteration 8/25 | Loss: 0.00090262
Iteration 9/25 | Loss: 0.00090262
Iteration 10/25 | Loss: 0.00090262
Iteration 11/25 | Loss: 0.00090262
Iteration 12/25 | Loss: 0.00090262
Iteration 13/25 | Loss: 0.00090262
Iteration 14/25 | Loss: 0.00090262
Iteration 15/25 | Loss: 0.00090262
Iteration 16/25 | Loss: 0.00090262
Iteration 17/25 | Loss: 0.00090262
Iteration 18/25 | Loss: 0.00090262
Iteration 19/25 | Loss: 0.00090262
Iteration 20/25 | Loss: 0.00090262
Iteration 21/25 | Loss: 0.00090262
Iteration 22/25 | Loss: 0.00090262
Iteration 23/25 | Loss: 0.00090262
Iteration 24/25 | Loss: 0.00090262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009026236948557198, 0.0009026236948557198, 0.0009026236948557198, 0.0009026236948557198, 0.0009026236948557198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009026236948557198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090262
Iteration 2/1000 | Loss: 0.00004572
Iteration 3/1000 | Loss: 0.00002755
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002010
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001947
Iteration 13/1000 | Loss: 0.00001946
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001944
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001943
Iteration 24/1000 | Loss: 0.00001943
Iteration 25/1000 | Loss: 0.00001942
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001935
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001934
Iteration 34/1000 | Loss: 0.00001934
Iteration 35/1000 | Loss: 0.00001934
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001920
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001920
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001920
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001920
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001920
Iteration 106/1000 | Loss: 0.00001920
Iteration 107/1000 | Loss: 0.00001920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.9199844246031716e-05, 1.9199844246031716e-05, 1.9199844246031716e-05, 1.9199844246031716e-05, 1.9199844246031716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9199844246031716e-05

Optimization complete. Final v2v error: 3.6926233768463135 mm

Highest mean error: 3.981049060821533 mm for frame 132

Lowest mean error: 3.4243361949920654 mm for frame 0

Saving results

Total time: 35.668084383010864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471325
Iteration 2/25 | Loss: 0.00131944
Iteration 3/25 | Loss: 0.00121924
Iteration 4/25 | Loss: 0.00120438
Iteration 5/25 | Loss: 0.00119856
Iteration 6/25 | Loss: 0.00119729
Iteration 7/25 | Loss: 0.00119723
Iteration 8/25 | Loss: 0.00119723
Iteration 9/25 | Loss: 0.00119723
Iteration 10/25 | Loss: 0.00119723
Iteration 11/25 | Loss: 0.00119723
Iteration 12/25 | Loss: 0.00119723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011972286738455296, 0.0011972286738455296, 0.0011972286738455296, 0.0011972286738455296, 0.0011972286738455296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011972286738455296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25798130
Iteration 2/25 | Loss: 0.00111243
Iteration 3/25 | Loss: 0.00111242
Iteration 4/25 | Loss: 0.00111242
Iteration 5/25 | Loss: 0.00111242
Iteration 6/25 | Loss: 0.00111242
Iteration 7/25 | Loss: 0.00111242
Iteration 8/25 | Loss: 0.00111242
Iteration 9/25 | Loss: 0.00111242
Iteration 10/25 | Loss: 0.00111242
Iteration 11/25 | Loss: 0.00111242
Iteration 12/25 | Loss: 0.00111242
Iteration 13/25 | Loss: 0.00111242
Iteration 14/25 | Loss: 0.00111242
Iteration 15/25 | Loss: 0.00111242
Iteration 16/25 | Loss: 0.00111242
Iteration 17/25 | Loss: 0.00111242
Iteration 18/25 | Loss: 0.00111242
Iteration 19/25 | Loss: 0.00111242
Iteration 20/25 | Loss: 0.00111242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011124223237857223, 0.0011124223237857223, 0.0011124223237857223, 0.0011124223237857223, 0.0011124223237857223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011124223237857223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111242
Iteration 2/1000 | Loss: 0.00003309
Iteration 3/1000 | Loss: 0.00002627
Iteration 4/1000 | Loss: 0.00002397
Iteration 5/1000 | Loss: 0.00002245
Iteration 6/1000 | Loss: 0.00002156
Iteration 7/1000 | Loss: 0.00002102
Iteration 8/1000 | Loss: 0.00002060
Iteration 9/1000 | Loss: 0.00002027
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001999
Iteration 13/1000 | Loss: 0.00001996
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001993
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001980
Iteration 26/1000 | Loss: 0.00001980
Iteration 27/1000 | Loss: 0.00001979
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001976
Iteration 32/1000 | Loss: 0.00001976
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001971
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001971
Iteration 43/1000 | Loss: 0.00001971
Iteration 44/1000 | Loss: 0.00001971
Iteration 45/1000 | Loss: 0.00001970
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001969
Iteration 50/1000 | Loss: 0.00001969
Iteration 51/1000 | Loss: 0.00001968
Iteration 52/1000 | Loss: 0.00001968
Iteration 53/1000 | Loss: 0.00001968
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001967
Iteration 61/1000 | Loss: 0.00001967
Iteration 62/1000 | Loss: 0.00001967
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001966
Iteration 67/1000 | Loss: 0.00001966
Iteration 68/1000 | Loss: 0.00001966
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001964
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001964
Iteration 86/1000 | Loss: 0.00001964
Iteration 87/1000 | Loss: 0.00001964
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001961
Iteration 104/1000 | Loss: 0.00001961
Iteration 105/1000 | Loss: 0.00001961
Iteration 106/1000 | Loss: 0.00001961
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001961
Iteration 112/1000 | Loss: 0.00001961
Iteration 113/1000 | Loss: 0.00001961
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001961
Iteration 118/1000 | Loss: 0.00001961
Iteration 119/1000 | Loss: 0.00001961
Iteration 120/1000 | Loss: 0.00001961
Iteration 121/1000 | Loss: 0.00001961
Iteration 122/1000 | Loss: 0.00001961
Iteration 123/1000 | Loss: 0.00001961
Iteration 124/1000 | Loss: 0.00001961
Iteration 125/1000 | Loss: 0.00001961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.960834561032243e-05, 1.960834561032243e-05, 1.960834561032243e-05, 1.960834561032243e-05, 1.960834561032243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.960834561032243e-05

Optimization complete. Final v2v error: 3.809101104736328 mm

Highest mean error: 4.021824359893799 mm for frame 44

Lowest mean error: 3.2915570735931396 mm for frame 1

Saving results

Total time: 31.13825249671936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443599
Iteration 2/25 | Loss: 0.00137269
Iteration 3/25 | Loss: 0.00118482
Iteration 4/25 | Loss: 0.00116532
Iteration 5/25 | Loss: 0.00116076
Iteration 6/25 | Loss: 0.00115919
Iteration 7/25 | Loss: 0.00115913
Iteration 8/25 | Loss: 0.00115913
Iteration 9/25 | Loss: 0.00115913
Iteration 10/25 | Loss: 0.00115913
Iteration 11/25 | Loss: 0.00115913
Iteration 12/25 | Loss: 0.00115913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011591289658099413, 0.0011591289658099413, 0.0011591289658099413, 0.0011591289658099413, 0.0011591289658099413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011591289658099413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48718691
Iteration 2/25 | Loss: 0.00110440
Iteration 3/25 | Loss: 0.00110440
Iteration 4/25 | Loss: 0.00110440
Iteration 5/25 | Loss: 0.00110440
Iteration 6/25 | Loss: 0.00110440
Iteration 7/25 | Loss: 0.00110440
Iteration 8/25 | Loss: 0.00110439
Iteration 9/25 | Loss: 0.00110439
Iteration 10/25 | Loss: 0.00110439
Iteration 11/25 | Loss: 0.00110439
Iteration 12/25 | Loss: 0.00110439
Iteration 13/25 | Loss: 0.00110439
Iteration 14/25 | Loss: 0.00110439
Iteration 15/25 | Loss: 0.00110439
Iteration 16/25 | Loss: 0.00110439
Iteration 17/25 | Loss: 0.00110439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001104394206777215, 0.001104394206777215, 0.001104394206777215, 0.001104394206777215, 0.001104394206777215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001104394206777215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110439
Iteration 2/1000 | Loss: 0.00003371
Iteration 3/1000 | Loss: 0.00002038
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001604
Iteration 6/1000 | Loss: 0.00001543
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001477
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001411
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001397
Iteration 16/1000 | Loss: 0.00001395
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001394
Iteration 20/1000 | Loss: 0.00001393
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001393
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001387
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001386
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001384
Iteration 50/1000 | Loss: 0.00001384
Iteration 51/1000 | Loss: 0.00001383
Iteration 52/1000 | Loss: 0.00001383
Iteration 53/1000 | Loss: 0.00001383
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001381
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.3781350389763247e-05, 1.3781350389763247e-05, 1.3781350389763247e-05, 1.3781350389763247e-05, 1.3781350389763247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3781350389763247e-05

Optimization complete. Final v2v error: 3.1597046852111816 mm

Highest mean error: 3.7788989543914795 mm for frame 117

Lowest mean error: 2.6528239250183105 mm for frame 239

Saving results

Total time: 35.15089654922485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908543
Iteration 2/25 | Loss: 0.00137900
Iteration 3/25 | Loss: 0.00127764
Iteration 4/25 | Loss: 0.00125232
Iteration 5/25 | Loss: 0.00124034
Iteration 6/25 | Loss: 0.00123723
Iteration 7/25 | Loss: 0.00123691
Iteration 8/25 | Loss: 0.00123691
Iteration 9/25 | Loss: 0.00123691
Iteration 10/25 | Loss: 0.00123691
Iteration 11/25 | Loss: 0.00123691
Iteration 12/25 | Loss: 0.00123691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012369058094918728, 0.0012369058094918728, 0.0012369058094918728, 0.0012369058094918728, 0.0012369058094918728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012369058094918728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23109484
Iteration 2/25 | Loss: 0.00131770
Iteration 3/25 | Loss: 0.00131770
Iteration 4/25 | Loss: 0.00131770
Iteration 5/25 | Loss: 0.00131769
Iteration 6/25 | Loss: 0.00131769
Iteration 7/25 | Loss: 0.00131769
Iteration 8/25 | Loss: 0.00131769
Iteration 9/25 | Loss: 0.00131769
Iteration 10/25 | Loss: 0.00131769
Iteration 11/25 | Loss: 0.00131769
Iteration 12/25 | Loss: 0.00131769
Iteration 13/25 | Loss: 0.00131769
Iteration 14/25 | Loss: 0.00131769
Iteration 15/25 | Loss: 0.00131769
Iteration 16/25 | Loss: 0.00131769
Iteration 17/25 | Loss: 0.00131769
Iteration 18/25 | Loss: 0.00131769
Iteration 19/25 | Loss: 0.00131769
Iteration 20/25 | Loss: 0.00131769
Iteration 21/25 | Loss: 0.00131769
Iteration 22/25 | Loss: 0.00131769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013176947832107544, 0.0013176947832107544, 0.0013176947832107544, 0.0013176947832107544, 0.0013176947832107544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013176947832107544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131769
Iteration 2/1000 | Loss: 0.00003711
Iteration 3/1000 | Loss: 0.00002948
Iteration 4/1000 | Loss: 0.00002697
Iteration 5/1000 | Loss: 0.00002529
Iteration 6/1000 | Loss: 0.00002424
Iteration 7/1000 | Loss: 0.00002355
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002288
Iteration 10/1000 | Loss: 0.00002271
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002271
Iteration 13/1000 | Loss: 0.00002270
Iteration 14/1000 | Loss: 0.00002265
Iteration 15/1000 | Loss: 0.00002258
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002253
Iteration 18/1000 | Loss: 0.00002253
Iteration 19/1000 | Loss: 0.00002253
Iteration 20/1000 | Loss: 0.00002252
Iteration 21/1000 | Loss: 0.00002251
Iteration 22/1000 | Loss: 0.00002251
Iteration 23/1000 | Loss: 0.00002250
Iteration 24/1000 | Loss: 0.00002250
Iteration 25/1000 | Loss: 0.00002249
Iteration 26/1000 | Loss: 0.00002249
Iteration 27/1000 | Loss: 0.00002249
Iteration 28/1000 | Loss: 0.00002248
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002248
Iteration 31/1000 | Loss: 0.00002247
Iteration 32/1000 | Loss: 0.00002246
Iteration 33/1000 | Loss: 0.00002246
Iteration 34/1000 | Loss: 0.00002246
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00002246
Iteration 37/1000 | Loss: 0.00002246
Iteration 38/1000 | Loss: 0.00002246
Iteration 39/1000 | Loss: 0.00002245
Iteration 40/1000 | Loss: 0.00002245
Iteration 41/1000 | Loss: 0.00002245
Iteration 42/1000 | Loss: 0.00002245
Iteration 43/1000 | Loss: 0.00002245
Iteration 44/1000 | Loss: 0.00002245
Iteration 45/1000 | Loss: 0.00002245
Iteration 46/1000 | Loss: 0.00002245
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00002244
Iteration 51/1000 | Loss: 0.00002244
Iteration 52/1000 | Loss: 0.00002244
Iteration 53/1000 | Loss: 0.00002244
Iteration 54/1000 | Loss: 0.00002244
Iteration 55/1000 | Loss: 0.00002244
Iteration 56/1000 | Loss: 0.00002244
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00002244
Iteration 59/1000 | Loss: 0.00002244
Iteration 60/1000 | Loss: 0.00002244
Iteration 61/1000 | Loss: 0.00002244
Iteration 62/1000 | Loss: 0.00002244
Iteration 63/1000 | Loss: 0.00002244
Iteration 64/1000 | Loss: 0.00002244
Iteration 65/1000 | Loss: 0.00002243
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002243
Iteration 70/1000 | Loss: 0.00002243
Iteration 71/1000 | Loss: 0.00002243
Iteration 72/1000 | Loss: 0.00002243
Iteration 73/1000 | Loss: 0.00002242
Iteration 74/1000 | Loss: 0.00002242
Iteration 75/1000 | Loss: 0.00002242
Iteration 76/1000 | Loss: 0.00002242
Iteration 77/1000 | Loss: 0.00002242
Iteration 78/1000 | Loss: 0.00002242
Iteration 79/1000 | Loss: 0.00002242
Iteration 80/1000 | Loss: 0.00002242
Iteration 81/1000 | Loss: 0.00002242
Iteration 82/1000 | Loss: 0.00002242
Iteration 83/1000 | Loss: 0.00002242
Iteration 84/1000 | Loss: 0.00002242
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002241
Iteration 87/1000 | Loss: 0.00002241
Iteration 88/1000 | Loss: 0.00002241
Iteration 89/1000 | Loss: 0.00002241
Iteration 90/1000 | Loss: 0.00002241
Iteration 91/1000 | Loss: 0.00002241
Iteration 92/1000 | Loss: 0.00002241
Iteration 93/1000 | Loss: 0.00002241
Iteration 94/1000 | Loss: 0.00002241
Iteration 95/1000 | Loss: 0.00002241
Iteration 96/1000 | Loss: 0.00002241
Iteration 97/1000 | Loss: 0.00002241
Iteration 98/1000 | Loss: 0.00002241
Iteration 99/1000 | Loss: 0.00002241
Iteration 100/1000 | Loss: 0.00002240
Iteration 101/1000 | Loss: 0.00002240
Iteration 102/1000 | Loss: 0.00002240
Iteration 103/1000 | Loss: 0.00002240
Iteration 104/1000 | Loss: 0.00002240
Iteration 105/1000 | Loss: 0.00002240
Iteration 106/1000 | Loss: 0.00002240
Iteration 107/1000 | Loss: 0.00002240
Iteration 108/1000 | Loss: 0.00002240
Iteration 109/1000 | Loss: 0.00002240
Iteration 110/1000 | Loss: 0.00002240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.2398071450879797e-05, 2.2398071450879797e-05, 2.2398071450879797e-05, 2.2398071450879797e-05, 2.2398071450879797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2398071450879797e-05

Optimization complete. Final v2v error: 4.066377639770508 mm

Highest mean error: 4.234365940093994 mm for frame 126

Lowest mean error: 3.7110509872436523 mm for frame 264

Saving results

Total time: 35.04330897331238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872195
Iteration 2/25 | Loss: 0.00141295
Iteration 3/25 | Loss: 0.00124710
Iteration 4/25 | Loss: 0.00122299
Iteration 5/25 | Loss: 0.00121870
Iteration 6/25 | Loss: 0.00121803
Iteration 7/25 | Loss: 0.00121803
Iteration 8/25 | Loss: 0.00121803
Iteration 9/25 | Loss: 0.00121803
Iteration 10/25 | Loss: 0.00121803
Iteration 11/25 | Loss: 0.00121803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012180263875052333, 0.0012180263875052333, 0.0012180263875052333, 0.0012180263875052333, 0.0012180263875052333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012180263875052333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14185822
Iteration 2/25 | Loss: 0.00110609
Iteration 3/25 | Loss: 0.00110607
Iteration 4/25 | Loss: 0.00110606
Iteration 5/25 | Loss: 0.00110606
Iteration 6/25 | Loss: 0.00110606
Iteration 7/25 | Loss: 0.00110606
Iteration 8/25 | Loss: 0.00110606
Iteration 9/25 | Loss: 0.00110606
Iteration 10/25 | Loss: 0.00110606
Iteration 11/25 | Loss: 0.00110606
Iteration 12/25 | Loss: 0.00110606
Iteration 13/25 | Loss: 0.00110606
Iteration 14/25 | Loss: 0.00110606
Iteration 15/25 | Loss: 0.00110606
Iteration 16/25 | Loss: 0.00110606
Iteration 17/25 | Loss: 0.00110606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011060629040002823, 0.0011060629040002823, 0.0011060629040002823, 0.0011060629040002823, 0.0011060629040002823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011060629040002823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110606
Iteration 2/1000 | Loss: 0.00004447
Iteration 3/1000 | Loss: 0.00002927
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002139
Iteration 6/1000 | Loss: 0.00002006
Iteration 7/1000 | Loss: 0.00001918
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001713
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001685
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001679
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001671
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001664
Iteration 27/1000 | Loss: 0.00001664
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001664
Iteration 34/1000 | Loss: 0.00001664
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001662
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001660
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001660
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001659
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001659
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001658
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001657
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001657
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001657
Iteration 64/1000 | Loss: 0.00001657
Iteration 65/1000 | Loss: 0.00001657
Iteration 66/1000 | Loss: 0.00001657
Iteration 67/1000 | Loss: 0.00001657
Iteration 68/1000 | Loss: 0.00001656
Iteration 69/1000 | Loss: 0.00001656
Iteration 70/1000 | Loss: 0.00001656
Iteration 71/1000 | Loss: 0.00001656
Iteration 72/1000 | Loss: 0.00001656
Iteration 73/1000 | Loss: 0.00001656
Iteration 74/1000 | Loss: 0.00001656
Iteration 75/1000 | Loss: 0.00001656
Iteration 76/1000 | Loss: 0.00001656
Iteration 77/1000 | Loss: 0.00001656
Iteration 78/1000 | Loss: 0.00001655
Iteration 79/1000 | Loss: 0.00001655
Iteration 80/1000 | Loss: 0.00001655
Iteration 81/1000 | Loss: 0.00001655
Iteration 82/1000 | Loss: 0.00001655
Iteration 83/1000 | Loss: 0.00001655
Iteration 84/1000 | Loss: 0.00001655
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00001654
Iteration 89/1000 | Loss: 0.00001654
Iteration 90/1000 | Loss: 0.00001654
Iteration 91/1000 | Loss: 0.00001654
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00001654
Iteration 94/1000 | Loss: 0.00001654
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001654
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001653
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001652
Iteration 116/1000 | Loss: 0.00001652
Iteration 117/1000 | Loss: 0.00001652
Iteration 118/1000 | Loss: 0.00001652
Iteration 119/1000 | Loss: 0.00001652
Iteration 120/1000 | Loss: 0.00001652
Iteration 121/1000 | Loss: 0.00001652
Iteration 122/1000 | Loss: 0.00001652
Iteration 123/1000 | Loss: 0.00001652
Iteration 124/1000 | Loss: 0.00001652
Iteration 125/1000 | Loss: 0.00001652
Iteration 126/1000 | Loss: 0.00001652
Iteration 127/1000 | Loss: 0.00001652
Iteration 128/1000 | Loss: 0.00001651
Iteration 129/1000 | Loss: 0.00001651
Iteration 130/1000 | Loss: 0.00001651
Iteration 131/1000 | Loss: 0.00001651
Iteration 132/1000 | Loss: 0.00001651
Iteration 133/1000 | Loss: 0.00001651
Iteration 134/1000 | Loss: 0.00001651
Iteration 135/1000 | Loss: 0.00001651
Iteration 136/1000 | Loss: 0.00001651
Iteration 137/1000 | Loss: 0.00001651
Iteration 138/1000 | Loss: 0.00001651
Iteration 139/1000 | Loss: 0.00001651
Iteration 140/1000 | Loss: 0.00001651
Iteration 141/1000 | Loss: 0.00001651
Iteration 142/1000 | Loss: 0.00001651
Iteration 143/1000 | Loss: 0.00001651
Iteration 144/1000 | Loss: 0.00001650
Iteration 145/1000 | Loss: 0.00001650
Iteration 146/1000 | Loss: 0.00001650
Iteration 147/1000 | Loss: 0.00001650
Iteration 148/1000 | Loss: 0.00001650
Iteration 149/1000 | Loss: 0.00001650
Iteration 150/1000 | Loss: 0.00001650
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001650
Iteration 153/1000 | Loss: 0.00001650
Iteration 154/1000 | Loss: 0.00001650
Iteration 155/1000 | Loss: 0.00001650
Iteration 156/1000 | Loss: 0.00001650
Iteration 157/1000 | Loss: 0.00001650
Iteration 158/1000 | Loss: 0.00001650
Iteration 159/1000 | Loss: 0.00001650
Iteration 160/1000 | Loss: 0.00001650
Iteration 161/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.6496320313308388e-05, 1.6496320313308388e-05, 1.6496320313308388e-05, 1.6496320313308388e-05, 1.6496320313308388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6496320313308388e-05

Optimization complete. Final v2v error: 3.543856382369995 mm

Highest mean error: 3.8519904613494873 mm for frame 135

Lowest mean error: 3.1417064666748047 mm for frame 224

Saving results

Total time: 43.514097690582275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036337
Iteration 2/25 | Loss: 0.01036335
Iteration 3/25 | Loss: 0.00408486
Iteration 4/25 | Loss: 0.00251965
Iteration 5/25 | Loss: 0.00232889
Iteration 6/25 | Loss: 0.00217055
Iteration 7/25 | Loss: 0.00209754
Iteration 8/25 | Loss: 0.00195542
Iteration 9/25 | Loss: 0.00169614
Iteration 10/25 | Loss: 0.00159445
Iteration 11/25 | Loss: 0.00155626
Iteration 12/25 | Loss: 0.00152962
Iteration 13/25 | Loss: 0.00150477
Iteration 14/25 | Loss: 0.00149604
Iteration 15/25 | Loss: 0.00149083
Iteration 16/25 | Loss: 0.00148459
Iteration 17/25 | Loss: 0.00147917
Iteration 18/25 | Loss: 0.00147692
Iteration 19/25 | Loss: 0.00147263
Iteration 20/25 | Loss: 0.00147305
Iteration 21/25 | Loss: 0.00147678
Iteration 22/25 | Loss: 0.00147484
Iteration 23/25 | Loss: 0.00147233
Iteration 24/25 | Loss: 0.00147255
Iteration 25/25 | Loss: 0.00147331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21251631
Iteration 2/25 | Loss: 0.00480096
Iteration 3/25 | Loss: 0.00480096
Iteration 4/25 | Loss: 0.00480096
Iteration 5/25 | Loss: 0.00480096
Iteration 6/25 | Loss: 0.00480096
Iteration 7/25 | Loss: 0.00480096
Iteration 8/25 | Loss: 0.00480096
Iteration 9/25 | Loss: 0.00480096
Iteration 10/25 | Loss: 0.00480096
Iteration 11/25 | Loss: 0.00480096
Iteration 12/25 | Loss: 0.00480096
Iteration 13/25 | Loss: 0.00480096
Iteration 14/25 | Loss: 0.00480096
Iteration 15/25 | Loss: 0.00480096
Iteration 16/25 | Loss: 0.00480096
Iteration 17/25 | Loss: 0.00480096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004800957627594471, 0.004800957627594471, 0.004800957627594471, 0.004800957627594471, 0.004800957627594471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004800957627594471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00480096
Iteration 2/1000 | Loss: 0.00053215
Iteration 3/1000 | Loss: 0.00048868
Iteration 4/1000 | Loss: 0.00043783
Iteration 5/1000 | Loss: 0.00042038
Iteration 6/1000 | Loss: 0.00045371
Iteration 7/1000 | Loss: 0.00029737
Iteration 8/1000 | Loss: 0.00033067
Iteration 9/1000 | Loss: 0.00035417
Iteration 10/1000 | Loss: 0.00034849
Iteration 11/1000 | Loss: 0.00036203
Iteration 12/1000 | Loss: 0.00030293
Iteration 13/1000 | Loss: 0.00025193
Iteration 14/1000 | Loss: 0.00023504
Iteration 15/1000 | Loss: 0.00049454
Iteration 16/1000 | Loss: 0.00037787
Iteration 17/1000 | Loss: 0.00039499
Iteration 18/1000 | Loss: 0.00036441
Iteration 19/1000 | Loss: 0.00038142
Iteration 20/1000 | Loss: 0.00035454
Iteration 21/1000 | Loss: 0.00029740
Iteration 22/1000 | Loss: 0.00022101
Iteration 23/1000 | Loss: 0.00022615
Iteration 24/1000 | Loss: 0.00021288
Iteration 25/1000 | Loss: 0.00068964
Iteration 26/1000 | Loss: 0.00077850
Iteration 27/1000 | Loss: 0.00055003
Iteration 28/1000 | Loss: 0.00040641
Iteration 29/1000 | Loss: 0.00036792
Iteration 30/1000 | Loss: 0.00087189
Iteration 31/1000 | Loss: 0.00054909
Iteration 32/1000 | Loss: 0.00056247
Iteration 33/1000 | Loss: 0.00069619
Iteration 34/1000 | Loss: 0.00022170
Iteration 35/1000 | Loss: 0.00020130
Iteration 36/1000 | Loss: 0.00048015
Iteration 37/1000 | Loss: 0.00044453
Iteration 38/1000 | Loss: 0.00024050
Iteration 39/1000 | Loss: 0.00019625
Iteration 40/1000 | Loss: 0.00019236
Iteration 41/1000 | Loss: 0.00020357
Iteration 42/1000 | Loss: 0.00018784
Iteration 43/1000 | Loss: 0.00019375
Iteration 44/1000 | Loss: 0.00020036
Iteration 45/1000 | Loss: 0.00019215
Iteration 46/1000 | Loss: 0.00020469
Iteration 47/1000 | Loss: 0.00018859
Iteration 48/1000 | Loss: 0.00019595
Iteration 49/1000 | Loss: 0.00020232
Iteration 50/1000 | Loss: 0.00019208
Iteration 51/1000 | Loss: 0.00020283
Iteration 52/1000 | Loss: 0.00031226
Iteration 53/1000 | Loss: 0.00020573
Iteration 54/1000 | Loss: 0.00018200
Iteration 55/1000 | Loss: 0.00017812
Iteration 56/1000 | Loss: 0.00017667
Iteration 57/1000 | Loss: 0.00017561
Iteration 58/1000 | Loss: 0.00017445
Iteration 59/1000 | Loss: 0.00017398
Iteration 60/1000 | Loss: 0.00017352
Iteration 61/1000 | Loss: 0.00017312
Iteration 62/1000 | Loss: 0.00017286
Iteration 63/1000 | Loss: 0.00017278
Iteration 64/1000 | Loss: 0.00017269
Iteration 65/1000 | Loss: 0.00017268
Iteration 66/1000 | Loss: 0.00017261
Iteration 67/1000 | Loss: 0.00017250
Iteration 68/1000 | Loss: 0.00017238
Iteration 69/1000 | Loss: 0.00017236
Iteration 70/1000 | Loss: 0.00017233
Iteration 71/1000 | Loss: 0.00017232
Iteration 72/1000 | Loss: 0.00017232
Iteration 73/1000 | Loss: 0.00017230
Iteration 74/1000 | Loss: 0.00017226
Iteration 75/1000 | Loss: 0.00017224
Iteration 76/1000 | Loss: 0.00017224
Iteration 77/1000 | Loss: 0.00017222
Iteration 78/1000 | Loss: 0.00017222
Iteration 79/1000 | Loss: 0.00017222
Iteration 80/1000 | Loss: 0.00017222
Iteration 81/1000 | Loss: 0.00017222
Iteration 82/1000 | Loss: 0.00017221
Iteration 83/1000 | Loss: 0.00017221
Iteration 84/1000 | Loss: 0.00017221
Iteration 85/1000 | Loss: 0.00017221
Iteration 86/1000 | Loss: 0.00017221
Iteration 87/1000 | Loss: 0.00017221
Iteration 88/1000 | Loss: 0.00017221
Iteration 89/1000 | Loss: 0.00030606
Iteration 90/1000 | Loss: 0.00030606
Iteration 91/1000 | Loss: 0.00020579
Iteration 92/1000 | Loss: 0.00018922
Iteration 93/1000 | Loss: 0.00047428
Iteration 94/1000 | Loss: 0.00020663
Iteration 95/1000 | Loss: 0.00018332
Iteration 96/1000 | Loss: 0.00018052
Iteration 97/1000 | Loss: 0.00034599
Iteration 98/1000 | Loss: 0.00025751
Iteration 99/1000 | Loss: 0.00017982
Iteration 100/1000 | Loss: 0.00017705
Iteration 101/1000 | Loss: 0.00018658
Iteration 102/1000 | Loss: 0.00018851
Iteration 103/1000 | Loss: 0.00018665
Iteration 104/1000 | Loss: 0.00039990
Iteration 105/1000 | Loss: 0.00022351
Iteration 106/1000 | Loss: 0.00020980
Iteration 107/1000 | Loss: 0.00048572
Iteration 108/1000 | Loss: 0.00021457
Iteration 109/1000 | Loss: 0.00017311
Iteration 110/1000 | Loss: 0.00017182
Iteration 111/1000 | Loss: 0.00017131
Iteration 112/1000 | Loss: 0.00017092
Iteration 113/1000 | Loss: 0.00017070
Iteration 114/1000 | Loss: 0.00017049
Iteration 115/1000 | Loss: 0.00017045
Iteration 116/1000 | Loss: 0.00017044
Iteration 117/1000 | Loss: 0.00017044
Iteration 118/1000 | Loss: 0.00017043
Iteration 119/1000 | Loss: 0.00017042
Iteration 120/1000 | Loss: 0.00017041
Iteration 121/1000 | Loss: 0.00017041
Iteration 122/1000 | Loss: 0.00017040
Iteration 123/1000 | Loss: 0.00017040
Iteration 124/1000 | Loss: 0.00017040
Iteration 125/1000 | Loss: 0.00017039
Iteration 126/1000 | Loss: 0.00017039
Iteration 127/1000 | Loss: 0.00017039
Iteration 128/1000 | Loss: 0.00017039
Iteration 129/1000 | Loss: 0.00017038
Iteration 130/1000 | Loss: 0.00017038
Iteration 131/1000 | Loss: 0.00017038
Iteration 132/1000 | Loss: 0.00017038
Iteration 133/1000 | Loss: 0.00017038
Iteration 134/1000 | Loss: 0.00017038
Iteration 135/1000 | Loss: 0.00017038
Iteration 136/1000 | Loss: 0.00017038
Iteration 137/1000 | Loss: 0.00017038
Iteration 138/1000 | Loss: 0.00017038
Iteration 139/1000 | Loss: 0.00017038
Iteration 140/1000 | Loss: 0.00017038
Iteration 141/1000 | Loss: 0.00017037
Iteration 142/1000 | Loss: 0.00017037
Iteration 143/1000 | Loss: 0.00017037
Iteration 144/1000 | Loss: 0.00017037
Iteration 145/1000 | Loss: 0.00017037
Iteration 146/1000 | Loss: 0.00017037
Iteration 147/1000 | Loss: 0.00017037
Iteration 148/1000 | Loss: 0.00017037
Iteration 149/1000 | Loss: 0.00017037
Iteration 150/1000 | Loss: 0.00017037
Iteration 151/1000 | Loss: 0.00017036
Iteration 152/1000 | Loss: 0.00017036
Iteration 153/1000 | Loss: 0.00017036
Iteration 154/1000 | Loss: 0.00017036
Iteration 155/1000 | Loss: 0.00017035
Iteration 156/1000 | Loss: 0.00017035
Iteration 157/1000 | Loss: 0.00017035
Iteration 158/1000 | Loss: 0.00017035
Iteration 159/1000 | Loss: 0.00017035
Iteration 160/1000 | Loss: 0.00017034
Iteration 161/1000 | Loss: 0.00017034
Iteration 162/1000 | Loss: 0.00017034
Iteration 163/1000 | Loss: 0.00017034
Iteration 164/1000 | Loss: 0.00017034
Iteration 165/1000 | Loss: 0.00017034
Iteration 166/1000 | Loss: 0.00017034
Iteration 167/1000 | Loss: 0.00017034
Iteration 168/1000 | Loss: 0.00017034
Iteration 169/1000 | Loss: 0.00017034
Iteration 170/1000 | Loss: 0.00017034
Iteration 171/1000 | Loss: 0.00017033
Iteration 172/1000 | Loss: 0.00017033
Iteration 173/1000 | Loss: 0.00017033
Iteration 174/1000 | Loss: 0.00017033
Iteration 175/1000 | Loss: 0.00017033
Iteration 176/1000 | Loss: 0.00017033
Iteration 177/1000 | Loss: 0.00017033
Iteration 178/1000 | Loss: 0.00017032
Iteration 179/1000 | Loss: 0.00017032
Iteration 180/1000 | Loss: 0.00017032
Iteration 181/1000 | Loss: 0.00017032
Iteration 182/1000 | Loss: 0.00017032
Iteration 183/1000 | Loss: 0.00017032
Iteration 184/1000 | Loss: 0.00017032
Iteration 185/1000 | Loss: 0.00017031
Iteration 186/1000 | Loss: 0.00017031
Iteration 187/1000 | Loss: 0.00017031
Iteration 188/1000 | Loss: 0.00017031
Iteration 189/1000 | Loss: 0.00017031
Iteration 190/1000 | Loss: 0.00017031
Iteration 191/1000 | Loss: 0.00017031
Iteration 192/1000 | Loss: 0.00017031
Iteration 193/1000 | Loss: 0.00017031
Iteration 194/1000 | Loss: 0.00017031
Iteration 195/1000 | Loss: 0.00017031
Iteration 196/1000 | Loss: 0.00017031
Iteration 197/1000 | Loss: 0.00017030
Iteration 198/1000 | Loss: 0.00017030
Iteration 199/1000 | Loss: 0.00017030
Iteration 200/1000 | Loss: 0.00017030
Iteration 201/1000 | Loss: 0.00017030
Iteration 202/1000 | Loss: 0.00017030
Iteration 203/1000 | Loss: 0.00017030
Iteration 204/1000 | Loss: 0.00017030
Iteration 205/1000 | Loss: 0.00017030
Iteration 206/1000 | Loss: 0.00017030
Iteration 207/1000 | Loss: 0.00017030
Iteration 208/1000 | Loss: 0.00017029
Iteration 209/1000 | Loss: 0.00017029
Iteration 210/1000 | Loss: 0.00017029
Iteration 211/1000 | Loss: 0.00017029
Iteration 212/1000 | Loss: 0.00017029
Iteration 213/1000 | Loss: 0.00017029
Iteration 214/1000 | Loss: 0.00017029
Iteration 215/1000 | Loss: 0.00017029
Iteration 216/1000 | Loss: 0.00017029
Iteration 217/1000 | Loss: 0.00017029
Iteration 218/1000 | Loss: 0.00017028
Iteration 219/1000 | Loss: 0.00017028
Iteration 220/1000 | Loss: 0.00017028
Iteration 221/1000 | Loss: 0.00017028
Iteration 222/1000 | Loss: 0.00017027
Iteration 223/1000 | Loss: 0.00017027
Iteration 224/1000 | Loss: 0.00017027
Iteration 225/1000 | Loss: 0.00017027
Iteration 226/1000 | Loss: 0.00017027
Iteration 227/1000 | Loss: 0.00017027
Iteration 228/1000 | Loss: 0.00017027
Iteration 229/1000 | Loss: 0.00017027
Iteration 230/1000 | Loss: 0.00017027
Iteration 231/1000 | Loss: 0.00017026
Iteration 232/1000 | Loss: 0.00017026
Iteration 233/1000 | Loss: 0.00017026
Iteration 234/1000 | Loss: 0.00017026
Iteration 235/1000 | Loss: 0.00017026
Iteration 236/1000 | Loss: 0.00017026
Iteration 237/1000 | Loss: 0.00017026
Iteration 238/1000 | Loss: 0.00017025
Iteration 239/1000 | Loss: 0.00017025
Iteration 240/1000 | Loss: 0.00017025
Iteration 241/1000 | Loss: 0.00017024
Iteration 242/1000 | Loss: 0.00017024
Iteration 243/1000 | Loss: 0.00017024
Iteration 244/1000 | Loss: 0.00017023
Iteration 245/1000 | Loss: 0.00017023
Iteration 246/1000 | Loss: 0.00017023
Iteration 247/1000 | Loss: 0.00017023
Iteration 248/1000 | Loss: 0.00017023
Iteration 249/1000 | Loss: 0.00017023
Iteration 250/1000 | Loss: 0.00017023
Iteration 251/1000 | Loss: 0.00017023
Iteration 252/1000 | Loss: 0.00017023
Iteration 253/1000 | Loss: 0.00017023
Iteration 254/1000 | Loss: 0.00017023
Iteration 255/1000 | Loss: 0.00017023
Iteration 256/1000 | Loss: 0.00017023
Iteration 257/1000 | Loss: 0.00017023
Iteration 258/1000 | Loss: 0.00017023
Iteration 259/1000 | Loss: 0.00017023
Iteration 260/1000 | Loss: 0.00017023
Iteration 261/1000 | Loss: 0.00017023
Iteration 262/1000 | Loss: 0.00017023
Iteration 263/1000 | Loss: 0.00017023
Iteration 264/1000 | Loss: 0.00017023
Iteration 265/1000 | Loss: 0.00017023
Iteration 266/1000 | Loss: 0.00017023
Iteration 267/1000 | Loss: 0.00017023
Iteration 268/1000 | Loss: 0.00017023
Iteration 269/1000 | Loss: 0.00017023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [0.00017022706742864102, 0.00017022706742864102, 0.00017022706742864102, 0.00017022706742864102, 0.00017022706742864102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017022706742864102

Optimization complete. Final v2v error: 7.148100852966309 mm

Highest mean error: 11.32525634765625 mm for frame 21

Lowest mean error: 4.319842338562012 mm for frame 4

Saving results

Total time: 204.70458889007568
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763813
Iteration 2/25 | Loss: 0.00153930
Iteration 3/25 | Loss: 0.00137966
Iteration 4/25 | Loss: 0.00136366
Iteration 5/25 | Loss: 0.00135795
Iteration 6/25 | Loss: 0.00135654
Iteration 7/25 | Loss: 0.00135613
Iteration 8/25 | Loss: 0.00135598
Iteration 9/25 | Loss: 0.00135598
Iteration 10/25 | Loss: 0.00135598
Iteration 11/25 | Loss: 0.00135598
Iteration 12/25 | Loss: 0.00135598
Iteration 13/25 | Loss: 0.00135598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013559795916080475, 0.0013559795916080475, 0.0013559795916080475, 0.0013559795916080475, 0.0013559795916080475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013559795916080475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.81584072
Iteration 2/25 | Loss: 0.00140123
Iteration 3/25 | Loss: 0.00140104
Iteration 4/25 | Loss: 0.00140104
Iteration 5/25 | Loss: 0.00140104
Iteration 6/25 | Loss: 0.00140104
Iteration 7/25 | Loss: 0.00140104
Iteration 8/25 | Loss: 0.00140104
Iteration 9/25 | Loss: 0.00140104
Iteration 10/25 | Loss: 0.00140104
Iteration 11/25 | Loss: 0.00140104
Iteration 12/25 | Loss: 0.00140104
Iteration 13/25 | Loss: 0.00140104
Iteration 14/25 | Loss: 0.00140104
Iteration 15/25 | Loss: 0.00140104
Iteration 16/25 | Loss: 0.00140104
Iteration 17/25 | Loss: 0.00140104
Iteration 18/25 | Loss: 0.00140104
Iteration 19/25 | Loss: 0.00140104
Iteration 20/25 | Loss: 0.00140104
Iteration 21/25 | Loss: 0.00140104
Iteration 22/25 | Loss: 0.00140104
Iteration 23/25 | Loss: 0.00140104
Iteration 24/25 | Loss: 0.00140104
Iteration 25/25 | Loss: 0.00140104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140104
Iteration 2/1000 | Loss: 0.00008855
Iteration 3/1000 | Loss: 0.00015827
Iteration 4/1000 | Loss: 0.00011645
Iteration 5/1000 | Loss: 0.00014159
Iteration 6/1000 | Loss: 0.00010516
Iteration 7/1000 | Loss: 0.00004139
Iteration 8/1000 | Loss: 0.00003960
Iteration 9/1000 | Loss: 0.00003831
Iteration 10/1000 | Loss: 0.00003734
Iteration 11/1000 | Loss: 0.00003675
Iteration 12/1000 | Loss: 0.00003624
Iteration 13/1000 | Loss: 0.00003591
Iteration 14/1000 | Loss: 0.00003557
Iteration 15/1000 | Loss: 0.00003534
Iteration 16/1000 | Loss: 0.00003507
Iteration 17/1000 | Loss: 0.00003489
Iteration 18/1000 | Loss: 0.00003470
Iteration 19/1000 | Loss: 0.00003453
Iteration 20/1000 | Loss: 0.00003452
Iteration 21/1000 | Loss: 0.00003438
Iteration 22/1000 | Loss: 0.00003433
Iteration 23/1000 | Loss: 0.00003428
Iteration 24/1000 | Loss: 0.00003426
Iteration 25/1000 | Loss: 0.00003426
Iteration 26/1000 | Loss: 0.00003425
Iteration 27/1000 | Loss: 0.00003424
Iteration 28/1000 | Loss: 0.00003424
Iteration 29/1000 | Loss: 0.00003424
Iteration 30/1000 | Loss: 0.00003424
Iteration 31/1000 | Loss: 0.00003424
Iteration 32/1000 | Loss: 0.00003423
Iteration 33/1000 | Loss: 0.00003423
Iteration 34/1000 | Loss: 0.00003423
Iteration 35/1000 | Loss: 0.00003423
Iteration 36/1000 | Loss: 0.00003422
Iteration 37/1000 | Loss: 0.00003421
Iteration 38/1000 | Loss: 0.00003421
Iteration 39/1000 | Loss: 0.00003420
Iteration 40/1000 | Loss: 0.00003420
Iteration 41/1000 | Loss: 0.00003420
Iteration 42/1000 | Loss: 0.00003419
Iteration 43/1000 | Loss: 0.00003419
Iteration 44/1000 | Loss: 0.00003419
Iteration 45/1000 | Loss: 0.00003418
Iteration 46/1000 | Loss: 0.00003417
Iteration 47/1000 | Loss: 0.00003416
Iteration 48/1000 | Loss: 0.00003416
Iteration 49/1000 | Loss: 0.00003416
Iteration 50/1000 | Loss: 0.00003415
Iteration 51/1000 | Loss: 0.00003415
Iteration 52/1000 | Loss: 0.00003415
Iteration 53/1000 | Loss: 0.00003414
Iteration 54/1000 | Loss: 0.00003414
Iteration 55/1000 | Loss: 0.00003414
Iteration 56/1000 | Loss: 0.00003414
Iteration 57/1000 | Loss: 0.00003413
Iteration 58/1000 | Loss: 0.00003412
Iteration 59/1000 | Loss: 0.00003412
Iteration 60/1000 | Loss: 0.00003411
Iteration 61/1000 | Loss: 0.00003411
Iteration 62/1000 | Loss: 0.00003411
Iteration 63/1000 | Loss: 0.00003411
Iteration 64/1000 | Loss: 0.00003410
Iteration 65/1000 | Loss: 0.00003410
Iteration 66/1000 | Loss: 0.00003410
Iteration 67/1000 | Loss: 0.00003410
Iteration 68/1000 | Loss: 0.00003410
Iteration 69/1000 | Loss: 0.00003409
Iteration 70/1000 | Loss: 0.00003409
Iteration 71/1000 | Loss: 0.00003409
Iteration 72/1000 | Loss: 0.00003409
Iteration 73/1000 | Loss: 0.00003408
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00003408
Iteration 76/1000 | Loss: 0.00003407
Iteration 77/1000 | Loss: 0.00003407
Iteration 78/1000 | Loss: 0.00003407
Iteration 79/1000 | Loss: 0.00003407
Iteration 80/1000 | Loss: 0.00003406
Iteration 81/1000 | Loss: 0.00003406
Iteration 82/1000 | Loss: 0.00003405
Iteration 83/1000 | Loss: 0.00003405
Iteration 84/1000 | Loss: 0.00003405
Iteration 85/1000 | Loss: 0.00003404
Iteration 86/1000 | Loss: 0.00003404
Iteration 87/1000 | Loss: 0.00003404
Iteration 88/1000 | Loss: 0.00003404
Iteration 89/1000 | Loss: 0.00003403
Iteration 90/1000 | Loss: 0.00003403
Iteration 91/1000 | Loss: 0.00003403
Iteration 92/1000 | Loss: 0.00003403
Iteration 93/1000 | Loss: 0.00003403
Iteration 94/1000 | Loss: 0.00003403
Iteration 95/1000 | Loss: 0.00003403
Iteration 96/1000 | Loss: 0.00003403
Iteration 97/1000 | Loss: 0.00003403
Iteration 98/1000 | Loss: 0.00003403
Iteration 99/1000 | Loss: 0.00003403
Iteration 100/1000 | Loss: 0.00003402
Iteration 101/1000 | Loss: 0.00003402
Iteration 102/1000 | Loss: 0.00003402
Iteration 103/1000 | Loss: 0.00003402
Iteration 104/1000 | Loss: 0.00003402
Iteration 105/1000 | Loss: 0.00003402
Iteration 106/1000 | Loss: 0.00003402
Iteration 107/1000 | Loss: 0.00003402
Iteration 108/1000 | Loss: 0.00003401
Iteration 109/1000 | Loss: 0.00003401
Iteration 110/1000 | Loss: 0.00003401
Iteration 111/1000 | Loss: 0.00003401
Iteration 112/1000 | Loss: 0.00003401
Iteration 113/1000 | Loss: 0.00003400
Iteration 114/1000 | Loss: 0.00003400
Iteration 115/1000 | Loss: 0.00003399
Iteration 116/1000 | Loss: 0.00003399
Iteration 117/1000 | Loss: 0.00003399
Iteration 118/1000 | Loss: 0.00003399
Iteration 119/1000 | Loss: 0.00003399
Iteration 120/1000 | Loss: 0.00003399
Iteration 121/1000 | Loss: 0.00003399
Iteration 122/1000 | Loss: 0.00003399
Iteration 123/1000 | Loss: 0.00003398
Iteration 124/1000 | Loss: 0.00003398
Iteration 125/1000 | Loss: 0.00003398
Iteration 126/1000 | Loss: 0.00003398
Iteration 127/1000 | Loss: 0.00003397
Iteration 128/1000 | Loss: 0.00003397
Iteration 129/1000 | Loss: 0.00003397
Iteration 130/1000 | Loss: 0.00003397
Iteration 131/1000 | Loss: 0.00003397
Iteration 132/1000 | Loss: 0.00003397
Iteration 133/1000 | Loss: 0.00003397
Iteration 134/1000 | Loss: 0.00003397
Iteration 135/1000 | Loss: 0.00003397
Iteration 136/1000 | Loss: 0.00003397
Iteration 137/1000 | Loss: 0.00003397
Iteration 138/1000 | Loss: 0.00003396
Iteration 139/1000 | Loss: 0.00003396
Iteration 140/1000 | Loss: 0.00003396
Iteration 141/1000 | Loss: 0.00003396
Iteration 142/1000 | Loss: 0.00003396
Iteration 143/1000 | Loss: 0.00003396
Iteration 144/1000 | Loss: 0.00003395
Iteration 145/1000 | Loss: 0.00003395
Iteration 146/1000 | Loss: 0.00003395
Iteration 147/1000 | Loss: 0.00003395
Iteration 148/1000 | Loss: 0.00003395
Iteration 149/1000 | Loss: 0.00003394
Iteration 150/1000 | Loss: 0.00003394
Iteration 151/1000 | Loss: 0.00003394
Iteration 152/1000 | Loss: 0.00003394
Iteration 153/1000 | Loss: 0.00003394
Iteration 154/1000 | Loss: 0.00003394
Iteration 155/1000 | Loss: 0.00003394
Iteration 156/1000 | Loss: 0.00003394
Iteration 157/1000 | Loss: 0.00003394
Iteration 158/1000 | Loss: 0.00003394
Iteration 159/1000 | Loss: 0.00003394
Iteration 160/1000 | Loss: 0.00003394
Iteration 161/1000 | Loss: 0.00003394
Iteration 162/1000 | Loss: 0.00003393
Iteration 163/1000 | Loss: 0.00003393
Iteration 164/1000 | Loss: 0.00003393
Iteration 165/1000 | Loss: 0.00003393
Iteration 166/1000 | Loss: 0.00003393
Iteration 167/1000 | Loss: 0.00003393
Iteration 168/1000 | Loss: 0.00003393
Iteration 169/1000 | Loss: 0.00003393
Iteration 170/1000 | Loss: 0.00003393
Iteration 171/1000 | Loss: 0.00003393
Iteration 172/1000 | Loss: 0.00003393
Iteration 173/1000 | Loss: 0.00003393
Iteration 174/1000 | Loss: 0.00003393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.3926600735867396e-05, 3.3926600735867396e-05, 3.3926600735867396e-05, 3.3926600735867396e-05, 3.3926600735867396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3926600735867396e-05

Optimization complete. Final v2v error: 4.658341407775879 mm

Highest mean error: 7.3263630867004395 mm for frame 145

Lowest mean error: 3.509997844696045 mm for frame 15

Saving results

Total time: 61.10300254821777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836932
Iteration 2/25 | Loss: 0.00124120
Iteration 3/25 | Loss: 0.00116694
Iteration 4/25 | Loss: 0.00116154
Iteration 5/25 | Loss: 0.00116122
Iteration 6/25 | Loss: 0.00116122
Iteration 7/25 | Loss: 0.00116122
Iteration 8/25 | Loss: 0.00116122
Iteration 9/25 | Loss: 0.00116122
Iteration 10/25 | Loss: 0.00116122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011612230446189642, 0.0011612230446189642, 0.0011612230446189642, 0.0011612230446189642, 0.0011612230446189642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011612230446189642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.73734951
Iteration 2/25 | Loss: 0.00126411
Iteration 3/25 | Loss: 0.00126411
Iteration 4/25 | Loss: 0.00126411
Iteration 5/25 | Loss: 0.00126411
Iteration 6/25 | Loss: 0.00126411
Iteration 7/25 | Loss: 0.00126411
Iteration 8/25 | Loss: 0.00126411
Iteration 9/25 | Loss: 0.00126411
Iteration 10/25 | Loss: 0.00126411
Iteration 11/25 | Loss: 0.00126411
Iteration 12/25 | Loss: 0.00126411
Iteration 13/25 | Loss: 0.00126411
Iteration 14/25 | Loss: 0.00126411
Iteration 15/25 | Loss: 0.00126411
Iteration 16/25 | Loss: 0.00126411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012641084613278508, 0.0012641084613278508, 0.0012641084613278508, 0.0012641084613278508, 0.0012641084613278508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012641084613278508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126411
Iteration 2/1000 | Loss: 0.00001755
Iteration 3/1000 | Loss: 0.00001293
Iteration 4/1000 | Loss: 0.00001174
Iteration 5/1000 | Loss: 0.00001095
Iteration 6/1000 | Loss: 0.00001052
Iteration 7/1000 | Loss: 0.00001026
Iteration 8/1000 | Loss: 0.00000994
Iteration 9/1000 | Loss: 0.00000966
Iteration 10/1000 | Loss: 0.00000953
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000945
Iteration 13/1000 | Loss: 0.00000937
Iteration 14/1000 | Loss: 0.00000936
Iteration 15/1000 | Loss: 0.00000933
Iteration 16/1000 | Loss: 0.00000919
Iteration 17/1000 | Loss: 0.00000919
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000915
Iteration 20/1000 | Loss: 0.00000915
Iteration 21/1000 | Loss: 0.00000914
Iteration 22/1000 | Loss: 0.00000913
Iteration 23/1000 | Loss: 0.00000906
Iteration 24/1000 | Loss: 0.00000905
Iteration 25/1000 | Loss: 0.00000905
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000894
Iteration 28/1000 | Loss: 0.00000889
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000889
Iteration 31/1000 | Loss: 0.00000888
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000887
Iteration 34/1000 | Loss: 0.00000886
Iteration 35/1000 | Loss: 0.00000885
Iteration 36/1000 | Loss: 0.00000884
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000883
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000881
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000880
Iteration 45/1000 | Loss: 0.00000880
Iteration 46/1000 | Loss: 0.00000879
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000879
Iteration 52/1000 | Loss: 0.00000879
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000875
Iteration 56/1000 | Loss: 0.00000875
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000875
Iteration 59/1000 | Loss: 0.00000875
Iteration 60/1000 | Loss: 0.00000874
Iteration 61/1000 | Loss: 0.00000874
Iteration 62/1000 | Loss: 0.00000874
Iteration 63/1000 | Loss: 0.00000874
Iteration 64/1000 | Loss: 0.00000874
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000873
Iteration 67/1000 | Loss: 0.00000871
Iteration 68/1000 | Loss: 0.00000870
Iteration 69/1000 | Loss: 0.00000870
Iteration 70/1000 | Loss: 0.00000870
Iteration 71/1000 | Loss: 0.00000870
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000868
Iteration 76/1000 | Loss: 0.00000868
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000866
Iteration 79/1000 | Loss: 0.00000865
Iteration 80/1000 | Loss: 0.00000865
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000864
Iteration 83/1000 | Loss: 0.00000864
Iteration 84/1000 | Loss: 0.00000863
Iteration 85/1000 | Loss: 0.00000863
Iteration 86/1000 | Loss: 0.00000862
Iteration 87/1000 | Loss: 0.00000862
Iteration 88/1000 | Loss: 0.00000861
Iteration 89/1000 | Loss: 0.00000861
Iteration 90/1000 | Loss: 0.00000861
Iteration 91/1000 | Loss: 0.00000861
Iteration 92/1000 | Loss: 0.00000861
Iteration 93/1000 | Loss: 0.00000860
Iteration 94/1000 | Loss: 0.00000859
Iteration 95/1000 | Loss: 0.00000859
Iteration 96/1000 | Loss: 0.00000859
Iteration 97/1000 | Loss: 0.00000858
Iteration 98/1000 | Loss: 0.00000858
Iteration 99/1000 | Loss: 0.00000858
Iteration 100/1000 | Loss: 0.00000857
Iteration 101/1000 | Loss: 0.00000856
Iteration 102/1000 | Loss: 0.00000856
Iteration 103/1000 | Loss: 0.00000855
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000854
Iteration 107/1000 | Loss: 0.00000853
Iteration 108/1000 | Loss: 0.00000853
Iteration 109/1000 | Loss: 0.00000853
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000853
Iteration 112/1000 | Loss: 0.00000853
Iteration 113/1000 | Loss: 0.00000853
Iteration 114/1000 | Loss: 0.00000853
Iteration 115/1000 | Loss: 0.00000853
Iteration 116/1000 | Loss: 0.00000853
Iteration 117/1000 | Loss: 0.00000853
Iteration 118/1000 | Loss: 0.00000853
Iteration 119/1000 | Loss: 0.00000853
Iteration 120/1000 | Loss: 0.00000853
Iteration 121/1000 | Loss: 0.00000852
Iteration 122/1000 | Loss: 0.00000852
Iteration 123/1000 | Loss: 0.00000852
Iteration 124/1000 | Loss: 0.00000852
Iteration 125/1000 | Loss: 0.00000852
Iteration 126/1000 | Loss: 0.00000852
Iteration 127/1000 | Loss: 0.00000852
Iteration 128/1000 | Loss: 0.00000852
Iteration 129/1000 | Loss: 0.00000852
Iteration 130/1000 | Loss: 0.00000852
Iteration 131/1000 | Loss: 0.00000852
Iteration 132/1000 | Loss: 0.00000851
Iteration 133/1000 | Loss: 0.00000851
Iteration 134/1000 | Loss: 0.00000851
Iteration 135/1000 | Loss: 0.00000851
Iteration 136/1000 | Loss: 0.00000851
Iteration 137/1000 | Loss: 0.00000851
Iteration 138/1000 | Loss: 0.00000851
Iteration 139/1000 | Loss: 0.00000850
Iteration 140/1000 | Loss: 0.00000850
Iteration 141/1000 | Loss: 0.00000850
Iteration 142/1000 | Loss: 0.00000850
Iteration 143/1000 | Loss: 0.00000850
Iteration 144/1000 | Loss: 0.00000850
Iteration 145/1000 | Loss: 0.00000850
Iteration 146/1000 | Loss: 0.00000850
Iteration 147/1000 | Loss: 0.00000850
Iteration 148/1000 | Loss: 0.00000850
Iteration 149/1000 | Loss: 0.00000850
Iteration 150/1000 | Loss: 0.00000849
Iteration 151/1000 | Loss: 0.00000849
Iteration 152/1000 | Loss: 0.00000849
Iteration 153/1000 | Loss: 0.00000849
Iteration 154/1000 | Loss: 0.00000849
Iteration 155/1000 | Loss: 0.00000849
Iteration 156/1000 | Loss: 0.00000849
Iteration 157/1000 | Loss: 0.00000849
Iteration 158/1000 | Loss: 0.00000849
Iteration 159/1000 | Loss: 0.00000849
Iteration 160/1000 | Loss: 0.00000849
Iteration 161/1000 | Loss: 0.00000848
Iteration 162/1000 | Loss: 0.00000848
Iteration 163/1000 | Loss: 0.00000848
Iteration 164/1000 | Loss: 0.00000848
Iteration 165/1000 | Loss: 0.00000848
Iteration 166/1000 | Loss: 0.00000848
Iteration 167/1000 | Loss: 0.00000848
Iteration 168/1000 | Loss: 0.00000848
Iteration 169/1000 | Loss: 0.00000848
Iteration 170/1000 | Loss: 0.00000848
Iteration 171/1000 | Loss: 0.00000848
Iteration 172/1000 | Loss: 0.00000848
Iteration 173/1000 | Loss: 0.00000847
Iteration 174/1000 | Loss: 0.00000847
Iteration 175/1000 | Loss: 0.00000847
Iteration 176/1000 | Loss: 0.00000846
Iteration 177/1000 | Loss: 0.00000846
Iteration 178/1000 | Loss: 0.00000846
Iteration 179/1000 | Loss: 0.00000846
Iteration 180/1000 | Loss: 0.00000846
Iteration 181/1000 | Loss: 0.00000846
Iteration 182/1000 | Loss: 0.00000846
Iteration 183/1000 | Loss: 0.00000845
Iteration 184/1000 | Loss: 0.00000845
Iteration 185/1000 | Loss: 0.00000845
Iteration 186/1000 | Loss: 0.00000845
Iteration 187/1000 | Loss: 0.00000845
Iteration 188/1000 | Loss: 0.00000845
Iteration 189/1000 | Loss: 0.00000845
Iteration 190/1000 | Loss: 0.00000844
Iteration 191/1000 | Loss: 0.00000844
Iteration 192/1000 | Loss: 0.00000844
Iteration 193/1000 | Loss: 0.00000844
Iteration 194/1000 | Loss: 0.00000844
Iteration 195/1000 | Loss: 0.00000844
Iteration 196/1000 | Loss: 0.00000844
Iteration 197/1000 | Loss: 0.00000844
Iteration 198/1000 | Loss: 0.00000844
Iteration 199/1000 | Loss: 0.00000844
Iteration 200/1000 | Loss: 0.00000844
Iteration 201/1000 | Loss: 0.00000844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [8.444225386483595e-06, 8.444225386483595e-06, 8.444225386483595e-06, 8.444225386483595e-06, 8.444225386483595e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.444225386483595e-06

Optimization complete. Final v2v error: 2.5150153636932373 mm

Highest mean error: 2.746192455291748 mm for frame 238

Lowest mean error: 2.3236138820648193 mm for frame 3

Saving results

Total time: 45.98899722099304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002174
Iteration 2/25 | Loss: 0.01002173
Iteration 3/25 | Loss: 0.01002173
Iteration 4/25 | Loss: 0.01002172
Iteration 5/25 | Loss: 0.00278282
Iteration 6/25 | Loss: 0.00198165
Iteration 7/25 | Loss: 0.00172110
Iteration 8/25 | Loss: 0.00165462
Iteration 9/25 | Loss: 0.00154516
Iteration 10/25 | Loss: 0.00146966
Iteration 11/25 | Loss: 0.00143082
Iteration 12/25 | Loss: 0.00141516
Iteration 13/25 | Loss: 0.00137577
Iteration 14/25 | Loss: 0.00137082
Iteration 15/25 | Loss: 0.00136838
Iteration 16/25 | Loss: 0.00136436
Iteration 17/25 | Loss: 0.00135734
Iteration 18/25 | Loss: 0.00134804
Iteration 19/25 | Loss: 0.00134339
Iteration 20/25 | Loss: 0.00134070
Iteration 21/25 | Loss: 0.00134246
Iteration 22/25 | Loss: 0.00133742
Iteration 23/25 | Loss: 0.00133001
Iteration 24/25 | Loss: 0.00133309
Iteration 25/25 | Loss: 0.00133069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30514824
Iteration 2/25 | Loss: 0.00341016
Iteration 3/25 | Loss: 0.00234130
Iteration 4/25 | Loss: 0.00234130
Iteration 5/25 | Loss: 0.00234130
Iteration 6/25 | Loss: 0.00234130
Iteration 7/25 | Loss: 0.00234130
Iteration 8/25 | Loss: 0.00234129
Iteration 9/25 | Loss: 0.00234129
Iteration 10/25 | Loss: 0.00234129
Iteration 11/25 | Loss: 0.00234129
Iteration 12/25 | Loss: 0.00234129
Iteration 13/25 | Loss: 0.00234129
Iteration 14/25 | Loss: 0.00234129
Iteration 15/25 | Loss: 0.00234129
Iteration 16/25 | Loss: 0.00234129
Iteration 17/25 | Loss: 0.00234129
Iteration 18/25 | Loss: 0.00234129
Iteration 19/25 | Loss: 0.00234129
Iteration 20/25 | Loss: 0.00234129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023412941955029964, 0.0023412941955029964, 0.0023412941955029964, 0.0023412941955029964, 0.0023412941955029964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023412941955029964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234129
Iteration 2/1000 | Loss: 0.00137315
Iteration 3/1000 | Loss: 0.00039358
Iteration 4/1000 | Loss: 0.00017710
Iteration 5/1000 | Loss: 0.00024362
Iteration 6/1000 | Loss: 0.00027250
Iteration 7/1000 | Loss: 0.00017818
Iteration 8/1000 | Loss: 0.00011140
Iteration 9/1000 | Loss: 0.00010888
Iteration 10/1000 | Loss: 0.00055827
Iteration 11/1000 | Loss: 0.00009836
Iteration 12/1000 | Loss: 0.00008414
Iteration 13/1000 | Loss: 0.00055896
Iteration 14/1000 | Loss: 0.00059516
Iteration 15/1000 | Loss: 0.00012109
Iteration 16/1000 | Loss: 0.00008840
Iteration 17/1000 | Loss: 0.00050970
Iteration 18/1000 | Loss: 0.00130581
Iteration 19/1000 | Loss: 0.00192234
Iteration 20/1000 | Loss: 0.00087036
Iteration 21/1000 | Loss: 0.00090215
Iteration 22/1000 | Loss: 0.00096243
Iteration 23/1000 | Loss: 0.00026794
Iteration 24/1000 | Loss: 0.00078019
Iteration 25/1000 | Loss: 0.00047870
Iteration 26/1000 | Loss: 0.00043717
Iteration 27/1000 | Loss: 0.00029410
Iteration 28/1000 | Loss: 0.00020583
Iteration 29/1000 | Loss: 0.00060720
Iteration 30/1000 | Loss: 0.00043816
Iteration 31/1000 | Loss: 0.00058710
Iteration 32/1000 | Loss: 0.00066123
Iteration 33/1000 | Loss: 0.00010074
Iteration 34/1000 | Loss: 0.00009748
Iteration 35/1000 | Loss: 0.00008130
Iteration 36/1000 | Loss: 0.00006959
Iteration 37/1000 | Loss: 0.00007433
Iteration 38/1000 | Loss: 0.00006760
Iteration 39/1000 | Loss: 0.00006294
Iteration 40/1000 | Loss: 0.00034147
Iteration 41/1000 | Loss: 0.00006294
Iteration 42/1000 | Loss: 0.00006940
Iteration 43/1000 | Loss: 0.00005988
Iteration 44/1000 | Loss: 0.00006989
Iteration 45/1000 | Loss: 0.00018331
Iteration 46/1000 | Loss: 0.00012440
Iteration 47/1000 | Loss: 0.00021832
Iteration 48/1000 | Loss: 0.00026178
Iteration 49/1000 | Loss: 0.00013230
Iteration 50/1000 | Loss: 0.00023139
Iteration 51/1000 | Loss: 0.00025029
Iteration 52/1000 | Loss: 0.00009670
Iteration 53/1000 | Loss: 0.00012730
Iteration 54/1000 | Loss: 0.00022979
Iteration 55/1000 | Loss: 0.00022727
Iteration 56/1000 | Loss: 0.00022368
Iteration 57/1000 | Loss: 0.00023051
Iteration 58/1000 | Loss: 0.00077619
Iteration 59/1000 | Loss: 0.00009711
Iteration 60/1000 | Loss: 0.00060264
Iteration 61/1000 | Loss: 0.00006125
Iteration 62/1000 | Loss: 0.00005632
Iteration 63/1000 | Loss: 0.00006617
Iteration 64/1000 | Loss: 0.00006666
Iteration 65/1000 | Loss: 0.00005832
Iteration 66/1000 | Loss: 0.00014763
Iteration 67/1000 | Loss: 0.00011822
Iteration 68/1000 | Loss: 0.00015450
Iteration 69/1000 | Loss: 0.00012190
Iteration 70/1000 | Loss: 0.00017103
Iteration 71/1000 | Loss: 0.00006771
Iteration 72/1000 | Loss: 0.00006091
Iteration 73/1000 | Loss: 0.00006526
Iteration 74/1000 | Loss: 0.00006612
Iteration 75/1000 | Loss: 0.00006508
Iteration 76/1000 | Loss: 0.00006097
Iteration 77/1000 | Loss: 0.00006423
Iteration 78/1000 | Loss: 0.00013931
Iteration 79/1000 | Loss: 0.00005946
Iteration 80/1000 | Loss: 0.00050809
Iteration 81/1000 | Loss: 0.00043761
Iteration 82/1000 | Loss: 0.00005640
Iteration 83/1000 | Loss: 0.00005250
Iteration 84/1000 | Loss: 0.00005104
Iteration 85/1000 | Loss: 0.00005009
Iteration 86/1000 | Loss: 0.00004945
Iteration 87/1000 | Loss: 0.00007614
Iteration 88/1000 | Loss: 0.00018279
Iteration 89/1000 | Loss: 0.00007433
Iteration 90/1000 | Loss: 0.00014422
Iteration 91/1000 | Loss: 0.00014848
Iteration 92/1000 | Loss: 0.00005263
Iteration 93/1000 | Loss: 0.00004815
Iteration 94/1000 | Loss: 0.00023663
Iteration 95/1000 | Loss: 0.00024525
Iteration 96/1000 | Loss: 0.00034441
Iteration 97/1000 | Loss: 0.00032548
Iteration 98/1000 | Loss: 0.00016505
Iteration 99/1000 | Loss: 0.00051982
Iteration 100/1000 | Loss: 0.00022065
Iteration 101/1000 | Loss: 0.00024657
Iteration 102/1000 | Loss: 0.00051941
Iteration 103/1000 | Loss: 0.00030098
Iteration 104/1000 | Loss: 0.00008478
Iteration 105/1000 | Loss: 0.00012568
Iteration 106/1000 | Loss: 0.00005393
Iteration 107/1000 | Loss: 0.00019693
Iteration 108/1000 | Loss: 0.00005121
Iteration 109/1000 | Loss: 0.00005026
Iteration 110/1000 | Loss: 0.00055689
Iteration 111/1000 | Loss: 0.00045229
Iteration 112/1000 | Loss: 0.00030479
Iteration 113/1000 | Loss: 0.00034313
Iteration 114/1000 | Loss: 0.00048315
Iteration 115/1000 | Loss: 0.00025839
Iteration 116/1000 | Loss: 0.00005115
Iteration 117/1000 | Loss: 0.00039587
Iteration 118/1000 | Loss: 0.00020818
Iteration 119/1000 | Loss: 0.00023440
Iteration 120/1000 | Loss: 0.00005770
Iteration 121/1000 | Loss: 0.00005362
Iteration 122/1000 | Loss: 0.00006526
Iteration 123/1000 | Loss: 0.00040155
Iteration 124/1000 | Loss: 0.00025568
Iteration 125/1000 | Loss: 0.00005144
Iteration 126/1000 | Loss: 0.00039829
Iteration 127/1000 | Loss: 0.00005182
Iteration 128/1000 | Loss: 0.00004916
Iteration 129/1000 | Loss: 0.00073246
Iteration 130/1000 | Loss: 0.00078257
Iteration 131/1000 | Loss: 0.00080845
Iteration 132/1000 | Loss: 0.00019136
Iteration 133/1000 | Loss: 0.00032908
Iteration 134/1000 | Loss: 0.00032668
Iteration 135/1000 | Loss: 0.00020669
Iteration 136/1000 | Loss: 0.00021228
Iteration 137/1000 | Loss: 0.00020212
Iteration 138/1000 | Loss: 0.00052119
Iteration 139/1000 | Loss: 0.00157242
Iteration 140/1000 | Loss: 0.00011819
Iteration 141/1000 | Loss: 0.00012859
Iteration 142/1000 | Loss: 0.00005406
Iteration 143/1000 | Loss: 0.00004833
Iteration 144/1000 | Loss: 0.00004629
Iteration 145/1000 | Loss: 0.00004400
Iteration 146/1000 | Loss: 0.00021719
Iteration 147/1000 | Loss: 0.00004227
Iteration 148/1000 | Loss: 0.00004094
Iteration 149/1000 | Loss: 0.00004027
Iteration 150/1000 | Loss: 0.00003971
Iteration 151/1000 | Loss: 0.00003933
Iteration 152/1000 | Loss: 0.00003909
Iteration 153/1000 | Loss: 0.00003905
Iteration 154/1000 | Loss: 0.00072656
Iteration 155/1000 | Loss: 0.00047831
Iteration 156/1000 | Loss: 0.00040531
Iteration 157/1000 | Loss: 0.00003888
Iteration 158/1000 | Loss: 0.00040713
Iteration 159/1000 | Loss: 0.00031090
Iteration 160/1000 | Loss: 0.00028209
Iteration 161/1000 | Loss: 0.00035928
Iteration 162/1000 | Loss: 0.00007416
Iteration 163/1000 | Loss: 0.00004543
Iteration 164/1000 | Loss: 0.00003848
Iteration 165/1000 | Loss: 0.00003693
Iteration 166/1000 | Loss: 0.00028491
Iteration 167/1000 | Loss: 0.00017837
Iteration 168/1000 | Loss: 0.00033590
Iteration 169/1000 | Loss: 0.00023389
Iteration 170/1000 | Loss: 0.00031733
Iteration 171/1000 | Loss: 0.00004603
Iteration 172/1000 | Loss: 0.00003805
Iteration 173/1000 | Loss: 0.00006434
Iteration 174/1000 | Loss: 0.00003654
Iteration 175/1000 | Loss: 0.00005484
Iteration 176/1000 | Loss: 0.00003602
Iteration 177/1000 | Loss: 0.00036791
Iteration 178/1000 | Loss: 0.00003744
Iteration 179/1000 | Loss: 0.00003556
Iteration 180/1000 | Loss: 0.00003462
Iteration 181/1000 | Loss: 0.00047307
Iteration 182/1000 | Loss: 0.00029987
Iteration 183/1000 | Loss: 0.00003393
Iteration 184/1000 | Loss: 0.00037001
Iteration 185/1000 | Loss: 0.00070787
Iteration 186/1000 | Loss: 0.00153138
Iteration 187/1000 | Loss: 0.00060704
Iteration 188/1000 | Loss: 0.00086674
Iteration 189/1000 | Loss: 0.00004103
Iteration 190/1000 | Loss: 0.00003481
Iteration 191/1000 | Loss: 0.00003522
Iteration 192/1000 | Loss: 0.00003157
Iteration 193/1000 | Loss: 0.00025219
Iteration 194/1000 | Loss: 0.00018947
Iteration 195/1000 | Loss: 0.00017299
Iteration 196/1000 | Loss: 0.00016804
Iteration 197/1000 | Loss: 0.00017641
Iteration 198/1000 | Loss: 0.00024068
Iteration 199/1000 | Loss: 0.00017414
Iteration 200/1000 | Loss: 0.00022434
Iteration 201/1000 | Loss: 0.00014748
Iteration 202/1000 | Loss: 0.00012601
Iteration 203/1000 | Loss: 0.00014236
Iteration 204/1000 | Loss: 0.00013416
Iteration 205/1000 | Loss: 0.00012426
Iteration 206/1000 | Loss: 0.00012260
Iteration 207/1000 | Loss: 0.00012129
Iteration 208/1000 | Loss: 0.00003510
Iteration 209/1000 | Loss: 0.00004428
Iteration 210/1000 | Loss: 0.00003077
Iteration 211/1000 | Loss: 0.00003771
Iteration 212/1000 | Loss: 0.00002951
Iteration 213/1000 | Loss: 0.00019395
Iteration 214/1000 | Loss: 0.00196569
Iteration 215/1000 | Loss: 0.00099389
Iteration 216/1000 | Loss: 0.00057744
Iteration 217/1000 | Loss: 0.00015702
Iteration 218/1000 | Loss: 0.00020938
Iteration 219/1000 | Loss: 0.00004326
Iteration 220/1000 | Loss: 0.00003413
Iteration 221/1000 | Loss: 0.00036153
Iteration 222/1000 | Loss: 0.00004418
Iteration 223/1000 | Loss: 0.00048820
Iteration 224/1000 | Loss: 0.00002593
Iteration 225/1000 | Loss: 0.00002225
Iteration 226/1000 | Loss: 0.00002047
Iteration 227/1000 | Loss: 0.00003548
Iteration 228/1000 | Loss: 0.00003901
Iteration 229/1000 | Loss: 0.00001844
Iteration 230/1000 | Loss: 0.00001717
Iteration 231/1000 | Loss: 0.00001646
Iteration 232/1000 | Loss: 0.00002076
Iteration 233/1000 | Loss: 0.00001649
Iteration 234/1000 | Loss: 0.00003979
Iteration 235/1000 | Loss: 0.00001574
Iteration 236/1000 | Loss: 0.00002698
Iteration 237/1000 | Loss: 0.00001526
Iteration 238/1000 | Loss: 0.00001746
Iteration 239/1000 | Loss: 0.00001491
Iteration 240/1000 | Loss: 0.00001488
Iteration 241/1000 | Loss: 0.00001815
Iteration 242/1000 | Loss: 0.00001465
Iteration 243/1000 | Loss: 0.00001464
Iteration 244/1000 | Loss: 0.00001464
Iteration 245/1000 | Loss: 0.00001463
Iteration 246/1000 | Loss: 0.00001463
Iteration 247/1000 | Loss: 0.00001463
Iteration 248/1000 | Loss: 0.00001463
Iteration 249/1000 | Loss: 0.00001462
Iteration 250/1000 | Loss: 0.00001462
Iteration 251/1000 | Loss: 0.00001462
Iteration 252/1000 | Loss: 0.00001462
Iteration 253/1000 | Loss: 0.00001462
Iteration 254/1000 | Loss: 0.00001461
Iteration 255/1000 | Loss: 0.00001458
Iteration 256/1000 | Loss: 0.00001458
Iteration 257/1000 | Loss: 0.00001458
Iteration 258/1000 | Loss: 0.00001457
Iteration 259/1000 | Loss: 0.00001457
Iteration 260/1000 | Loss: 0.00001457
Iteration 261/1000 | Loss: 0.00001457
Iteration 262/1000 | Loss: 0.00001457
Iteration 263/1000 | Loss: 0.00001457
Iteration 264/1000 | Loss: 0.00001457
Iteration 265/1000 | Loss: 0.00001457
Iteration 266/1000 | Loss: 0.00001457
Iteration 267/1000 | Loss: 0.00001457
Iteration 268/1000 | Loss: 0.00001457
Iteration 269/1000 | Loss: 0.00001456
Iteration 270/1000 | Loss: 0.00001456
Iteration 271/1000 | Loss: 0.00001456
Iteration 272/1000 | Loss: 0.00001455
Iteration 273/1000 | Loss: 0.00001455
Iteration 274/1000 | Loss: 0.00001455
Iteration 275/1000 | Loss: 0.00001455
Iteration 276/1000 | Loss: 0.00001454
Iteration 277/1000 | Loss: 0.00001454
Iteration 278/1000 | Loss: 0.00001454
Iteration 279/1000 | Loss: 0.00001454
Iteration 280/1000 | Loss: 0.00001454
Iteration 281/1000 | Loss: 0.00001454
Iteration 282/1000 | Loss: 0.00001454
Iteration 283/1000 | Loss: 0.00001454
Iteration 284/1000 | Loss: 0.00001454
Iteration 285/1000 | Loss: 0.00001454
Iteration 286/1000 | Loss: 0.00001453
Iteration 287/1000 | Loss: 0.00001453
Iteration 288/1000 | Loss: 0.00001453
Iteration 289/1000 | Loss: 0.00001453
Iteration 290/1000 | Loss: 0.00001453
Iteration 291/1000 | Loss: 0.00001453
Iteration 292/1000 | Loss: 0.00001452
Iteration 293/1000 | Loss: 0.00001452
Iteration 294/1000 | Loss: 0.00001452
Iteration 295/1000 | Loss: 0.00001451
Iteration 296/1000 | Loss: 0.00001451
Iteration 297/1000 | Loss: 0.00001451
Iteration 298/1000 | Loss: 0.00001451
Iteration 299/1000 | Loss: 0.00001450
Iteration 300/1000 | Loss: 0.00001450
Iteration 301/1000 | Loss: 0.00001450
Iteration 302/1000 | Loss: 0.00001450
Iteration 303/1000 | Loss: 0.00002537
Iteration 304/1000 | Loss: 0.00001452
Iteration 305/1000 | Loss: 0.00001824
Iteration 306/1000 | Loss: 0.00002555
Iteration 307/1000 | Loss: 0.00001714
Iteration 308/1000 | Loss: 0.00001701
Iteration 309/1000 | Loss: 0.00001446
Iteration 310/1000 | Loss: 0.00001446
Iteration 311/1000 | Loss: 0.00001446
Iteration 312/1000 | Loss: 0.00001446
Iteration 313/1000 | Loss: 0.00001446
Iteration 314/1000 | Loss: 0.00001446
Iteration 315/1000 | Loss: 0.00001446
Iteration 316/1000 | Loss: 0.00001446
Iteration 317/1000 | Loss: 0.00001446
Iteration 318/1000 | Loss: 0.00001446
Iteration 319/1000 | Loss: 0.00001446
Iteration 320/1000 | Loss: 0.00001446
Iteration 321/1000 | Loss: 0.00001446
Iteration 322/1000 | Loss: 0.00001446
Iteration 323/1000 | Loss: 0.00001446
Iteration 324/1000 | Loss: 0.00001446
Iteration 325/1000 | Loss: 0.00001446
Iteration 326/1000 | Loss: 0.00001446
Iteration 327/1000 | Loss: 0.00001446
Iteration 328/1000 | Loss: 0.00001446
Iteration 329/1000 | Loss: 0.00001446
Iteration 330/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [1.4458415535045788e-05, 1.4458415535045788e-05, 1.4458415535045788e-05, 1.4458415535045788e-05, 1.4458415535045788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4458415535045788e-05

Optimization complete. Final v2v error: 3.2580814361572266 mm

Highest mean error: 4.930509567260742 mm for frame 211

Lowest mean error: 2.996833324432373 mm for frame 133

Saving results

Total time: 443.193692445755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393542
Iteration 2/25 | Loss: 0.00122847
Iteration 3/25 | Loss: 0.00115505
Iteration 4/25 | Loss: 0.00114657
Iteration 5/25 | Loss: 0.00114434
Iteration 6/25 | Loss: 0.00114401
Iteration 7/25 | Loss: 0.00114401
Iteration 8/25 | Loss: 0.00114401
Iteration 9/25 | Loss: 0.00114401
Iteration 10/25 | Loss: 0.00114401
Iteration 11/25 | Loss: 0.00114401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011440146481618285, 0.0011440146481618285, 0.0011440146481618285, 0.0011440146481618285, 0.0011440146481618285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011440146481618285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30778193
Iteration 2/25 | Loss: 0.00145033
Iteration 3/25 | Loss: 0.00145032
Iteration 4/25 | Loss: 0.00145032
Iteration 5/25 | Loss: 0.00145032
Iteration 6/25 | Loss: 0.00145032
Iteration 7/25 | Loss: 0.00145032
Iteration 8/25 | Loss: 0.00145032
Iteration 9/25 | Loss: 0.00145032
Iteration 10/25 | Loss: 0.00145032
Iteration 11/25 | Loss: 0.00145032
Iteration 12/25 | Loss: 0.00145032
Iteration 13/25 | Loss: 0.00145032
Iteration 14/25 | Loss: 0.00145032
Iteration 15/25 | Loss: 0.00145032
Iteration 16/25 | Loss: 0.00145032
Iteration 17/25 | Loss: 0.00145032
Iteration 18/25 | Loss: 0.00145032
Iteration 19/25 | Loss: 0.00145032
Iteration 20/25 | Loss: 0.00145032
Iteration 21/25 | Loss: 0.00145032
Iteration 22/25 | Loss: 0.00145032
Iteration 23/25 | Loss: 0.00145032
Iteration 24/25 | Loss: 0.00145032
Iteration 25/25 | Loss: 0.00145032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145032
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001525
Iteration 4/1000 | Loss: 0.00001261
Iteration 5/1000 | Loss: 0.00001144
Iteration 6/1000 | Loss: 0.00001063
Iteration 7/1000 | Loss: 0.00001014
Iteration 8/1000 | Loss: 0.00000976
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000948
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000909
Iteration 13/1000 | Loss: 0.00000906
Iteration 14/1000 | Loss: 0.00000901
Iteration 15/1000 | Loss: 0.00000901
Iteration 16/1000 | Loss: 0.00000899
Iteration 17/1000 | Loss: 0.00000897
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000887
Iteration 21/1000 | Loss: 0.00000887
Iteration 22/1000 | Loss: 0.00000885
Iteration 23/1000 | Loss: 0.00000884
Iteration 24/1000 | Loss: 0.00000883
Iteration 25/1000 | Loss: 0.00000882
Iteration 26/1000 | Loss: 0.00000881
Iteration 27/1000 | Loss: 0.00000880
Iteration 28/1000 | Loss: 0.00000879
Iteration 29/1000 | Loss: 0.00000878
Iteration 30/1000 | Loss: 0.00000878
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000875
Iteration 33/1000 | Loss: 0.00000874
Iteration 34/1000 | Loss: 0.00000872
Iteration 35/1000 | Loss: 0.00000871
Iteration 36/1000 | Loss: 0.00000871
Iteration 37/1000 | Loss: 0.00000870
Iteration 38/1000 | Loss: 0.00000866
Iteration 39/1000 | Loss: 0.00000866
Iteration 40/1000 | Loss: 0.00000863
Iteration 41/1000 | Loss: 0.00000863
Iteration 42/1000 | Loss: 0.00000863
Iteration 43/1000 | Loss: 0.00000863
Iteration 44/1000 | Loss: 0.00000863
Iteration 45/1000 | Loss: 0.00000863
Iteration 46/1000 | Loss: 0.00000863
Iteration 47/1000 | Loss: 0.00000863
Iteration 48/1000 | Loss: 0.00000861
Iteration 49/1000 | Loss: 0.00000859
Iteration 50/1000 | Loss: 0.00000858
Iteration 51/1000 | Loss: 0.00000857
Iteration 52/1000 | Loss: 0.00000856
Iteration 53/1000 | Loss: 0.00000855
Iteration 54/1000 | Loss: 0.00000855
Iteration 55/1000 | Loss: 0.00000854
Iteration 56/1000 | Loss: 0.00000854
Iteration 57/1000 | Loss: 0.00000853
Iteration 58/1000 | Loss: 0.00000853
Iteration 59/1000 | Loss: 0.00000853
Iteration 60/1000 | Loss: 0.00000852
Iteration 61/1000 | Loss: 0.00000852
Iteration 62/1000 | Loss: 0.00000852
Iteration 63/1000 | Loss: 0.00000852
Iteration 64/1000 | Loss: 0.00000851
Iteration 65/1000 | Loss: 0.00000851
Iteration 66/1000 | Loss: 0.00000850
Iteration 67/1000 | Loss: 0.00000850
Iteration 68/1000 | Loss: 0.00000849
Iteration 69/1000 | Loss: 0.00000849
Iteration 70/1000 | Loss: 0.00000849
Iteration 71/1000 | Loss: 0.00000848
Iteration 72/1000 | Loss: 0.00000848
Iteration 73/1000 | Loss: 0.00000848
Iteration 74/1000 | Loss: 0.00000848
Iteration 75/1000 | Loss: 0.00000847
Iteration 76/1000 | Loss: 0.00000847
Iteration 77/1000 | Loss: 0.00000847
Iteration 78/1000 | Loss: 0.00000846
Iteration 79/1000 | Loss: 0.00000846
Iteration 80/1000 | Loss: 0.00000846
Iteration 81/1000 | Loss: 0.00000846
Iteration 82/1000 | Loss: 0.00000846
Iteration 83/1000 | Loss: 0.00000846
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000845
Iteration 86/1000 | Loss: 0.00000845
Iteration 87/1000 | Loss: 0.00000845
Iteration 88/1000 | Loss: 0.00000845
Iteration 89/1000 | Loss: 0.00000845
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000842
Iteration 94/1000 | Loss: 0.00000842
Iteration 95/1000 | Loss: 0.00000842
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000842
Iteration 101/1000 | Loss: 0.00000842
Iteration 102/1000 | Loss: 0.00000842
Iteration 103/1000 | Loss: 0.00000842
Iteration 104/1000 | Loss: 0.00000842
Iteration 105/1000 | Loss: 0.00000842
Iteration 106/1000 | Loss: 0.00000841
Iteration 107/1000 | Loss: 0.00000841
Iteration 108/1000 | Loss: 0.00000841
Iteration 109/1000 | Loss: 0.00000841
Iteration 110/1000 | Loss: 0.00000841
Iteration 111/1000 | Loss: 0.00000840
Iteration 112/1000 | Loss: 0.00000840
Iteration 113/1000 | Loss: 0.00000840
Iteration 114/1000 | Loss: 0.00000840
Iteration 115/1000 | Loss: 0.00000840
Iteration 116/1000 | Loss: 0.00000840
Iteration 117/1000 | Loss: 0.00000840
Iteration 118/1000 | Loss: 0.00000840
Iteration 119/1000 | Loss: 0.00000840
Iteration 120/1000 | Loss: 0.00000839
Iteration 121/1000 | Loss: 0.00000839
Iteration 122/1000 | Loss: 0.00000839
Iteration 123/1000 | Loss: 0.00000839
Iteration 124/1000 | Loss: 0.00000839
Iteration 125/1000 | Loss: 0.00000839
Iteration 126/1000 | Loss: 0.00000839
Iteration 127/1000 | Loss: 0.00000839
Iteration 128/1000 | Loss: 0.00000839
Iteration 129/1000 | Loss: 0.00000839
Iteration 130/1000 | Loss: 0.00000839
Iteration 131/1000 | Loss: 0.00000838
Iteration 132/1000 | Loss: 0.00000838
Iteration 133/1000 | Loss: 0.00000838
Iteration 134/1000 | Loss: 0.00000838
Iteration 135/1000 | Loss: 0.00000838
Iteration 136/1000 | Loss: 0.00000838
Iteration 137/1000 | Loss: 0.00000838
Iteration 138/1000 | Loss: 0.00000837
Iteration 139/1000 | Loss: 0.00000837
Iteration 140/1000 | Loss: 0.00000837
Iteration 141/1000 | Loss: 0.00000837
Iteration 142/1000 | Loss: 0.00000837
Iteration 143/1000 | Loss: 0.00000837
Iteration 144/1000 | Loss: 0.00000837
Iteration 145/1000 | Loss: 0.00000837
Iteration 146/1000 | Loss: 0.00000837
Iteration 147/1000 | Loss: 0.00000836
Iteration 148/1000 | Loss: 0.00000836
Iteration 149/1000 | Loss: 0.00000836
Iteration 150/1000 | Loss: 0.00000836
Iteration 151/1000 | Loss: 0.00000836
Iteration 152/1000 | Loss: 0.00000836
Iteration 153/1000 | Loss: 0.00000835
Iteration 154/1000 | Loss: 0.00000835
Iteration 155/1000 | Loss: 0.00000835
Iteration 156/1000 | Loss: 0.00000835
Iteration 157/1000 | Loss: 0.00000835
Iteration 158/1000 | Loss: 0.00000834
Iteration 159/1000 | Loss: 0.00000834
Iteration 160/1000 | Loss: 0.00000834
Iteration 161/1000 | Loss: 0.00000834
Iteration 162/1000 | Loss: 0.00000834
Iteration 163/1000 | Loss: 0.00000834
Iteration 164/1000 | Loss: 0.00000834
Iteration 165/1000 | Loss: 0.00000834
Iteration 166/1000 | Loss: 0.00000834
Iteration 167/1000 | Loss: 0.00000834
Iteration 168/1000 | Loss: 0.00000834
Iteration 169/1000 | Loss: 0.00000834
Iteration 170/1000 | Loss: 0.00000834
Iteration 171/1000 | Loss: 0.00000834
Iteration 172/1000 | Loss: 0.00000833
Iteration 173/1000 | Loss: 0.00000833
Iteration 174/1000 | Loss: 0.00000833
Iteration 175/1000 | Loss: 0.00000833
Iteration 176/1000 | Loss: 0.00000833
Iteration 177/1000 | Loss: 0.00000833
Iteration 178/1000 | Loss: 0.00000833
Iteration 179/1000 | Loss: 0.00000833
Iteration 180/1000 | Loss: 0.00000833
Iteration 181/1000 | Loss: 0.00000833
Iteration 182/1000 | Loss: 0.00000833
Iteration 183/1000 | Loss: 0.00000833
Iteration 184/1000 | Loss: 0.00000833
Iteration 185/1000 | Loss: 0.00000833
Iteration 186/1000 | Loss: 0.00000833
Iteration 187/1000 | Loss: 0.00000833
Iteration 188/1000 | Loss: 0.00000832
Iteration 189/1000 | Loss: 0.00000832
Iteration 190/1000 | Loss: 0.00000832
Iteration 191/1000 | Loss: 0.00000832
Iteration 192/1000 | Loss: 0.00000832
Iteration 193/1000 | Loss: 0.00000832
Iteration 194/1000 | Loss: 0.00000832
Iteration 195/1000 | Loss: 0.00000832
Iteration 196/1000 | Loss: 0.00000832
Iteration 197/1000 | Loss: 0.00000832
Iteration 198/1000 | Loss: 0.00000832
Iteration 199/1000 | Loss: 0.00000832
Iteration 200/1000 | Loss: 0.00000832
Iteration 201/1000 | Loss: 0.00000832
Iteration 202/1000 | Loss: 0.00000832
Iteration 203/1000 | Loss: 0.00000831
Iteration 204/1000 | Loss: 0.00000831
Iteration 205/1000 | Loss: 0.00000831
Iteration 206/1000 | Loss: 0.00000831
Iteration 207/1000 | Loss: 0.00000831
Iteration 208/1000 | Loss: 0.00000831
Iteration 209/1000 | Loss: 0.00000831
Iteration 210/1000 | Loss: 0.00000831
Iteration 211/1000 | Loss: 0.00000831
Iteration 212/1000 | Loss: 0.00000831
Iteration 213/1000 | Loss: 0.00000831
Iteration 214/1000 | Loss: 0.00000831
Iteration 215/1000 | Loss: 0.00000831
Iteration 216/1000 | Loss: 0.00000831
Iteration 217/1000 | Loss: 0.00000831
Iteration 218/1000 | Loss: 0.00000831
Iteration 219/1000 | Loss: 0.00000831
Iteration 220/1000 | Loss: 0.00000831
Iteration 221/1000 | Loss: 0.00000831
Iteration 222/1000 | Loss: 0.00000831
Iteration 223/1000 | Loss: 0.00000831
Iteration 224/1000 | Loss: 0.00000831
Iteration 225/1000 | Loss: 0.00000831
Iteration 226/1000 | Loss: 0.00000831
Iteration 227/1000 | Loss: 0.00000831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [8.306942618219182e-06, 8.306942618219182e-06, 8.306942618219182e-06, 8.306942618219182e-06, 8.306942618219182e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.306942618219182e-06

Optimization complete. Final v2v error: 2.4710991382598877 mm

Highest mean error: 3.4186441898345947 mm for frame 72

Lowest mean error: 2.2858662605285645 mm for frame 160

Saving results

Total time: 41.5298490524292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388576
Iteration 2/25 | Loss: 0.00137017
Iteration 3/25 | Loss: 0.00119764
Iteration 4/25 | Loss: 0.00117537
Iteration 5/25 | Loss: 0.00116779
Iteration 6/25 | Loss: 0.00116605
Iteration 7/25 | Loss: 0.00116550
Iteration 8/25 | Loss: 0.00116550
Iteration 9/25 | Loss: 0.00116550
Iteration 10/25 | Loss: 0.00116550
Iteration 11/25 | Loss: 0.00116550
Iteration 12/25 | Loss: 0.00116550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001165497349575162, 0.001165497349575162, 0.001165497349575162, 0.001165497349575162, 0.001165497349575162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001165497349575162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38524187
Iteration 2/25 | Loss: 0.00177192
Iteration 3/25 | Loss: 0.00177192
Iteration 4/25 | Loss: 0.00177192
Iteration 5/25 | Loss: 0.00177192
Iteration 6/25 | Loss: 0.00177192
Iteration 7/25 | Loss: 0.00177192
Iteration 8/25 | Loss: 0.00177192
Iteration 9/25 | Loss: 0.00177192
Iteration 10/25 | Loss: 0.00177192
Iteration 11/25 | Loss: 0.00177192
Iteration 12/25 | Loss: 0.00177192
Iteration 13/25 | Loss: 0.00177192
Iteration 14/25 | Loss: 0.00177192
Iteration 15/25 | Loss: 0.00177192
Iteration 16/25 | Loss: 0.00177192
Iteration 17/25 | Loss: 0.00177192
Iteration 18/25 | Loss: 0.00177192
Iteration 19/25 | Loss: 0.00177192
Iteration 20/25 | Loss: 0.00177192
Iteration 21/25 | Loss: 0.00177192
Iteration 22/25 | Loss: 0.00177192
Iteration 23/25 | Loss: 0.00177192
Iteration 24/25 | Loss: 0.00177192
Iteration 25/25 | Loss: 0.00177192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177192
Iteration 2/1000 | Loss: 0.00002767
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001819
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001699
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001638
Iteration 10/1000 | Loss: 0.00001636
Iteration 11/1000 | Loss: 0.00001618
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001597
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001595
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001571
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001571
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001570
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00001569
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001569
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001568
Iteration 68/1000 | Loss: 0.00001568
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001567
Iteration 76/1000 | Loss: 0.00001567
Iteration 77/1000 | Loss: 0.00001567
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001561
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001561
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001557
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001554
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001552
Iteration 128/1000 | Loss: 0.00001552
Iteration 129/1000 | Loss: 0.00001551
Iteration 130/1000 | Loss: 0.00001551
Iteration 131/1000 | Loss: 0.00001550
Iteration 132/1000 | Loss: 0.00001550
Iteration 133/1000 | Loss: 0.00001550
Iteration 134/1000 | Loss: 0.00001550
Iteration 135/1000 | Loss: 0.00001550
Iteration 136/1000 | Loss: 0.00001550
Iteration 137/1000 | Loss: 0.00001550
Iteration 138/1000 | Loss: 0.00001550
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001549
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001549
Iteration 143/1000 | Loss: 0.00001549
Iteration 144/1000 | Loss: 0.00001549
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001546
Iteration 156/1000 | Loss: 0.00001545
Iteration 157/1000 | Loss: 0.00001545
Iteration 158/1000 | Loss: 0.00001545
Iteration 159/1000 | Loss: 0.00001545
Iteration 160/1000 | Loss: 0.00001545
Iteration 161/1000 | Loss: 0.00001545
Iteration 162/1000 | Loss: 0.00001545
Iteration 163/1000 | Loss: 0.00001545
Iteration 164/1000 | Loss: 0.00001545
Iteration 165/1000 | Loss: 0.00001545
Iteration 166/1000 | Loss: 0.00001545
Iteration 167/1000 | Loss: 0.00001545
Iteration 168/1000 | Loss: 0.00001545
Iteration 169/1000 | Loss: 0.00001545
Iteration 170/1000 | Loss: 0.00001545
Iteration 171/1000 | Loss: 0.00001545
Iteration 172/1000 | Loss: 0.00001545
Iteration 173/1000 | Loss: 0.00001544
Iteration 174/1000 | Loss: 0.00001544
Iteration 175/1000 | Loss: 0.00001544
Iteration 176/1000 | Loss: 0.00001544
Iteration 177/1000 | Loss: 0.00001544
Iteration 178/1000 | Loss: 0.00001544
Iteration 179/1000 | Loss: 0.00001544
Iteration 180/1000 | Loss: 0.00001544
Iteration 181/1000 | Loss: 0.00001544
Iteration 182/1000 | Loss: 0.00001544
Iteration 183/1000 | Loss: 0.00001544
Iteration 184/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.5444833479705267e-05, 1.5444833479705267e-05, 1.5444833479705267e-05, 1.5444833479705267e-05, 1.5444833479705267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5444833479705267e-05

Optimization complete. Final v2v error: 3.2934718132019043 mm

Highest mean error: 3.6647589206695557 mm for frame 110

Lowest mean error: 2.7128183841705322 mm for frame 8

Saving results

Total time: 38.67601776123047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037974
Iteration 2/25 | Loss: 0.00305406
Iteration 3/25 | Loss: 0.00199046
Iteration 4/25 | Loss: 0.00196560
Iteration 5/25 | Loss: 0.00182047
Iteration 6/25 | Loss: 0.00170325
Iteration 7/25 | Loss: 0.00170056
Iteration 8/25 | Loss: 0.00164863
Iteration 9/25 | Loss: 0.00160516
Iteration 10/25 | Loss: 0.00157479
Iteration 11/25 | Loss: 0.00155938
Iteration 12/25 | Loss: 0.00153299
Iteration 13/25 | Loss: 0.00152201
Iteration 14/25 | Loss: 0.00151950
Iteration 15/25 | Loss: 0.00151088
Iteration 16/25 | Loss: 0.00150249
Iteration 17/25 | Loss: 0.00150081
Iteration 18/25 | Loss: 0.00149356
Iteration 19/25 | Loss: 0.00148826
Iteration 20/25 | Loss: 0.00148828
Iteration 21/25 | Loss: 0.00148725
Iteration 22/25 | Loss: 0.00148511
Iteration 23/25 | Loss: 0.00148502
Iteration 24/25 | Loss: 0.00147814
Iteration 25/25 | Loss: 0.00147845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27022922
Iteration 2/25 | Loss: 0.00389800
Iteration 3/25 | Loss: 0.00346434
Iteration 4/25 | Loss: 0.00346434
Iteration 5/25 | Loss: 0.00346434
Iteration 6/25 | Loss: 0.00346434
Iteration 7/25 | Loss: 0.00346434
Iteration 8/25 | Loss: 0.00346434
Iteration 9/25 | Loss: 0.00346434
Iteration 10/25 | Loss: 0.00346434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0034643379040062428, 0.0034643379040062428, 0.0034643379040062428, 0.0034643379040062428, 0.0034643379040062428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034643379040062428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00346434
Iteration 2/1000 | Loss: 0.00057107
Iteration 3/1000 | Loss: 0.00174250
Iteration 4/1000 | Loss: 0.00038633
Iteration 5/1000 | Loss: 0.00043783
Iteration 6/1000 | Loss: 0.00221582
Iteration 7/1000 | Loss: 0.00435650
Iteration 8/1000 | Loss: 0.00205347
Iteration 9/1000 | Loss: 0.00081867
Iteration 10/1000 | Loss: 0.00397722
Iteration 11/1000 | Loss: 0.00024269
Iteration 12/1000 | Loss: 0.00065788
Iteration 13/1000 | Loss: 0.00325277
Iteration 14/1000 | Loss: 0.00042135
Iteration 15/1000 | Loss: 0.00020459
Iteration 16/1000 | Loss: 0.00034666
Iteration 17/1000 | Loss: 0.00017668
Iteration 18/1000 | Loss: 0.00019998
Iteration 19/1000 | Loss: 0.00016673
Iteration 20/1000 | Loss: 0.00310497
Iteration 21/1000 | Loss: 0.01014968
Iteration 22/1000 | Loss: 0.00516637
Iteration 23/1000 | Loss: 0.01042712
Iteration 24/1000 | Loss: 0.00577137
Iteration 25/1000 | Loss: 0.00611547
Iteration 26/1000 | Loss: 0.00450344
Iteration 27/1000 | Loss: 0.00558249
Iteration 28/1000 | Loss: 0.00459619
Iteration 29/1000 | Loss: 0.00345842
Iteration 30/1000 | Loss: 0.00444504
Iteration 31/1000 | Loss: 0.00757380
Iteration 32/1000 | Loss: 0.00310211
Iteration 33/1000 | Loss: 0.00214227
Iteration 34/1000 | Loss: 0.00082398
Iteration 35/1000 | Loss: 0.00059431
Iteration 36/1000 | Loss: 0.00227463
Iteration 37/1000 | Loss: 0.00143715
Iteration 38/1000 | Loss: 0.00192312
Iteration 39/1000 | Loss: 0.00127375
Iteration 40/1000 | Loss: 0.00108929
Iteration 41/1000 | Loss: 0.00047832
Iteration 42/1000 | Loss: 0.00054037
Iteration 43/1000 | Loss: 0.00059613
Iteration 44/1000 | Loss: 0.00087399
Iteration 45/1000 | Loss: 0.00061177
Iteration 46/1000 | Loss: 0.00062698
Iteration 47/1000 | Loss: 0.00059414
Iteration 48/1000 | Loss: 0.00058244
Iteration 49/1000 | Loss: 0.00030349
Iteration 50/1000 | Loss: 0.00026976
Iteration 51/1000 | Loss: 0.00062218
Iteration 52/1000 | Loss: 0.00050059
Iteration 53/1000 | Loss: 0.00087544
Iteration 54/1000 | Loss: 0.00126836
Iteration 55/1000 | Loss: 0.00086047
Iteration 56/1000 | Loss: 0.00059352
Iteration 57/1000 | Loss: 0.00110458
Iteration 58/1000 | Loss: 0.00042072
Iteration 59/1000 | Loss: 0.00066836
Iteration 60/1000 | Loss: 0.00043961
Iteration 61/1000 | Loss: 0.00072231
Iteration 62/1000 | Loss: 0.00075481
Iteration 63/1000 | Loss: 0.00046337
Iteration 64/1000 | Loss: 0.00085599
Iteration 65/1000 | Loss: 0.00050943
Iteration 66/1000 | Loss: 0.00071537
Iteration 67/1000 | Loss: 0.00049711
Iteration 68/1000 | Loss: 0.00066605
Iteration 69/1000 | Loss: 0.00120548
Iteration 70/1000 | Loss: 0.00082649
Iteration 71/1000 | Loss: 0.00166455
Iteration 72/1000 | Loss: 0.00039952
Iteration 73/1000 | Loss: 0.00070861
Iteration 74/1000 | Loss: 0.00067153
Iteration 75/1000 | Loss: 0.00040375
Iteration 76/1000 | Loss: 0.00036284
Iteration 77/1000 | Loss: 0.00016060
Iteration 78/1000 | Loss: 0.00032714
Iteration 79/1000 | Loss: 0.00020831
Iteration 80/1000 | Loss: 0.00042198
Iteration 81/1000 | Loss: 0.00036824
Iteration 82/1000 | Loss: 0.00072032
Iteration 83/1000 | Loss: 0.00021808
Iteration 84/1000 | Loss: 0.00015623
Iteration 85/1000 | Loss: 0.00022886
Iteration 86/1000 | Loss: 0.00009760
Iteration 87/1000 | Loss: 0.00006577
Iteration 88/1000 | Loss: 0.00004419
Iteration 89/1000 | Loss: 0.00003338
Iteration 90/1000 | Loss: 0.00003708
Iteration 91/1000 | Loss: 0.00003174
Iteration 92/1000 | Loss: 0.00002981
Iteration 93/1000 | Loss: 0.00012494
Iteration 94/1000 | Loss: 0.00003438
Iteration 95/1000 | Loss: 0.00011350
Iteration 96/1000 | Loss: 0.00038460
Iteration 97/1000 | Loss: 0.00022944
Iteration 98/1000 | Loss: 0.00007846
Iteration 99/1000 | Loss: 0.00006686
Iteration 100/1000 | Loss: 0.00009858
Iteration 101/1000 | Loss: 0.00009407
Iteration 102/1000 | Loss: 0.00002961
Iteration 103/1000 | Loss: 0.00010062
Iteration 104/1000 | Loss: 0.00007369
Iteration 105/1000 | Loss: 0.00003323
Iteration 106/1000 | Loss: 0.00003903
Iteration 107/1000 | Loss: 0.00003185
Iteration 108/1000 | Loss: 0.00005455
Iteration 109/1000 | Loss: 0.00005176
Iteration 110/1000 | Loss: 0.00003133
Iteration 111/1000 | Loss: 0.00003031
Iteration 112/1000 | Loss: 0.00038159
Iteration 113/1000 | Loss: 0.00022040
Iteration 114/1000 | Loss: 0.00032345
Iteration 115/1000 | Loss: 0.00050571
Iteration 116/1000 | Loss: 0.00028951
Iteration 117/1000 | Loss: 0.00002997
Iteration 118/1000 | Loss: 0.00002230
Iteration 119/1000 | Loss: 0.00001951
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00002111
Iteration 123/1000 | Loss: 0.00002275
Iteration 124/1000 | Loss: 0.00007032
Iteration 125/1000 | Loss: 0.00007884
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001569
Iteration 128/1000 | Loss: 0.00001869
Iteration 129/1000 | Loss: 0.00001869
Iteration 130/1000 | Loss: 0.00002912
Iteration 131/1000 | Loss: 0.00001524
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001490
Iteration 135/1000 | Loss: 0.00001481
Iteration 136/1000 | Loss: 0.00001479
Iteration 137/1000 | Loss: 0.00001478
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001476
Iteration 140/1000 | Loss: 0.00001844
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001463
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001456
Iteration 153/1000 | Loss: 0.00001455
Iteration 154/1000 | Loss: 0.00001454
Iteration 155/1000 | Loss: 0.00001454
Iteration 156/1000 | Loss: 0.00001453
Iteration 157/1000 | Loss: 0.00001453
Iteration 158/1000 | Loss: 0.00001453
Iteration 159/1000 | Loss: 0.00001452
Iteration 160/1000 | Loss: 0.00001451
Iteration 161/1000 | Loss: 0.00001451
Iteration 162/1000 | Loss: 0.00001451
Iteration 163/1000 | Loss: 0.00001451
Iteration 164/1000 | Loss: 0.00001451
Iteration 165/1000 | Loss: 0.00001450
Iteration 166/1000 | Loss: 0.00001450
Iteration 167/1000 | Loss: 0.00001450
Iteration 168/1000 | Loss: 0.00001450
Iteration 169/1000 | Loss: 0.00001450
Iteration 170/1000 | Loss: 0.00001450
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001449
Iteration 173/1000 | Loss: 0.00001909
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001447
Iteration 176/1000 | Loss: 0.00001447
Iteration 177/1000 | Loss: 0.00001447
Iteration 178/1000 | Loss: 0.00001447
Iteration 179/1000 | Loss: 0.00001447
Iteration 180/1000 | Loss: 0.00001447
Iteration 181/1000 | Loss: 0.00001447
Iteration 182/1000 | Loss: 0.00001447
Iteration 183/1000 | Loss: 0.00001447
Iteration 184/1000 | Loss: 0.00001447
Iteration 185/1000 | Loss: 0.00001447
Iteration 186/1000 | Loss: 0.00001446
Iteration 187/1000 | Loss: 0.00001446
Iteration 188/1000 | Loss: 0.00001446
Iteration 189/1000 | Loss: 0.00001446
Iteration 190/1000 | Loss: 0.00001446
Iteration 191/1000 | Loss: 0.00001446
Iteration 192/1000 | Loss: 0.00001446
Iteration 193/1000 | Loss: 0.00001565
Iteration 194/1000 | Loss: 0.00001444
Iteration 195/1000 | Loss: 0.00001444
Iteration 196/1000 | Loss: 0.00001444
Iteration 197/1000 | Loss: 0.00001444
Iteration 198/1000 | Loss: 0.00001444
Iteration 199/1000 | Loss: 0.00001444
Iteration 200/1000 | Loss: 0.00001444
Iteration 201/1000 | Loss: 0.00001444
Iteration 202/1000 | Loss: 0.00001444
Iteration 203/1000 | Loss: 0.00001444
Iteration 204/1000 | Loss: 0.00001444
Iteration 205/1000 | Loss: 0.00001444
Iteration 206/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4437308891501743e-05, 1.4437308891501743e-05, 1.4437308891501743e-05, 1.4437308891501743e-05, 1.4437308891501743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4437308891501743e-05

Optimization complete. Final v2v error: 3.108267068862915 mm

Highest mean error: 5.668363571166992 mm for frame 63

Lowest mean error: 2.5786666870117188 mm for frame 117

Saving results

Total time: 269.0428183078766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056949
Iteration 2/25 | Loss: 0.01056949
Iteration 3/25 | Loss: 0.01056949
Iteration 4/25 | Loss: 0.01056949
Iteration 5/25 | Loss: 0.01056948
Iteration 6/25 | Loss: 0.00154866
Iteration 7/25 | Loss: 0.00133880
Iteration 8/25 | Loss: 0.00123200
Iteration 9/25 | Loss: 0.00121885
Iteration 10/25 | Loss: 0.00120397
Iteration 11/25 | Loss: 0.00120127
Iteration 12/25 | Loss: 0.00119720
Iteration 13/25 | Loss: 0.00119651
Iteration 14/25 | Loss: 0.00119611
Iteration 15/25 | Loss: 0.00119601
Iteration 16/25 | Loss: 0.00119601
Iteration 17/25 | Loss: 0.00119600
Iteration 18/25 | Loss: 0.00119600
Iteration 19/25 | Loss: 0.00119600
Iteration 20/25 | Loss: 0.00119597
Iteration 21/25 | Loss: 0.00119597
Iteration 22/25 | Loss: 0.00119597
Iteration 23/25 | Loss: 0.00119597
Iteration 24/25 | Loss: 0.00119597
Iteration 25/25 | Loss: 0.00119597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.37946320
Iteration 2/25 | Loss: 0.00133291
Iteration 3/25 | Loss: 0.00133290
Iteration 4/25 | Loss: 0.00133290
Iteration 5/25 | Loss: 0.00133290
Iteration 6/25 | Loss: 0.00133290
Iteration 7/25 | Loss: 0.00133290
Iteration 8/25 | Loss: 0.00133290
Iteration 9/25 | Loss: 0.00133290
Iteration 10/25 | Loss: 0.00133290
Iteration 11/25 | Loss: 0.00133290
Iteration 12/25 | Loss: 0.00133290
Iteration 13/25 | Loss: 0.00133290
Iteration 14/25 | Loss: 0.00133290
Iteration 15/25 | Loss: 0.00133290
Iteration 16/25 | Loss: 0.00133290
Iteration 17/25 | Loss: 0.00133290
Iteration 18/25 | Loss: 0.00133290
Iteration 19/25 | Loss: 0.00133290
Iteration 20/25 | Loss: 0.00133290
Iteration 21/25 | Loss: 0.00133290
Iteration 22/25 | Loss: 0.00133290
Iteration 23/25 | Loss: 0.00133290
Iteration 24/25 | Loss: 0.00133290
Iteration 25/25 | Loss: 0.00133290

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133290
Iteration 2/1000 | Loss: 0.00001982
Iteration 3/1000 | Loss: 0.00001380
Iteration 4/1000 | Loss: 0.00006604
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00012363
Iteration 7/1000 | Loss: 0.00012363
Iteration 8/1000 | Loss: 0.00015789
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00003652
Iteration 14/1000 | Loss: 0.00018928
Iteration 15/1000 | Loss: 0.00004284
Iteration 16/1000 | Loss: 0.00006084
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00006288
Iteration 19/1000 | Loss: 0.00009263
Iteration 20/1000 | Loss: 0.00008423
Iteration 21/1000 | Loss: 0.00003052
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00002223
Iteration 24/1000 | Loss: 0.00005715
Iteration 25/1000 | Loss: 0.00001828
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00003279
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001114
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001056
Iteration 34/1000 | Loss: 0.00001053
Iteration 35/1000 | Loss: 0.00001053
Iteration 36/1000 | Loss: 0.00001053
Iteration 37/1000 | Loss: 0.00001053
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001052
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001052
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001052
Iteration 53/1000 | Loss: 0.00001052
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001052
Iteration 57/1000 | Loss: 0.00001052
Iteration 58/1000 | Loss: 0.00001052
Iteration 59/1000 | Loss: 0.00001052
Iteration 60/1000 | Loss: 0.00001052
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001052
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001052
Iteration 70/1000 | Loss: 0.00001052
Iteration 71/1000 | Loss: 0.00001052
Iteration 72/1000 | Loss: 0.00001052
Iteration 73/1000 | Loss: 0.00001052
Iteration 74/1000 | Loss: 0.00001052
Iteration 75/1000 | Loss: 0.00001052
Iteration 76/1000 | Loss: 0.00001052
Iteration 77/1000 | Loss: 0.00001052
Iteration 78/1000 | Loss: 0.00001052
Iteration 79/1000 | Loss: 0.00001052
Iteration 80/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.0518746421439573e-05, 1.0518746421439573e-05, 1.0518746421439573e-05, 1.0518746421439573e-05, 1.0518746421439573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0518746421439573e-05

Optimization complete. Final v2v error: 2.77728271484375 mm

Highest mean error: 3.466998338699341 mm for frame 100

Lowest mean error: 2.583559989929199 mm for frame 160

Saving results

Total time: 66.94499564170837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996296
Iteration 2/25 | Loss: 0.00348038
Iteration 3/25 | Loss: 0.00256796
Iteration 4/25 | Loss: 0.00216321
Iteration 5/25 | Loss: 0.00219632
Iteration 6/25 | Loss: 0.00190715
Iteration 7/25 | Loss: 0.00167791
Iteration 8/25 | Loss: 0.00165500
Iteration 9/25 | Loss: 0.00165234
Iteration 10/25 | Loss: 0.00154511
Iteration 11/25 | Loss: 0.00149636
Iteration 12/25 | Loss: 0.00150636
Iteration 13/25 | Loss: 0.00145193
Iteration 14/25 | Loss: 0.00142765
Iteration 15/25 | Loss: 0.00142883
Iteration 16/25 | Loss: 0.00140513
Iteration 17/25 | Loss: 0.00140063
Iteration 18/25 | Loss: 0.00138759
Iteration 19/25 | Loss: 0.00138388
Iteration 20/25 | Loss: 0.00139388
Iteration 21/25 | Loss: 0.00138306
Iteration 22/25 | Loss: 0.00138048
Iteration 23/25 | Loss: 0.00138034
Iteration 24/25 | Loss: 0.00138027
Iteration 25/25 | Loss: 0.00138021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41691470
Iteration 2/25 | Loss: 0.00264932
Iteration 3/25 | Loss: 0.00191462
Iteration 4/25 | Loss: 0.00191462
Iteration 5/25 | Loss: 0.00191462
Iteration 6/25 | Loss: 0.00191462
Iteration 7/25 | Loss: 0.00191462
Iteration 8/25 | Loss: 0.00191462
Iteration 9/25 | Loss: 0.00191462
Iteration 10/25 | Loss: 0.00191462
Iteration 11/25 | Loss: 0.00191462
Iteration 12/25 | Loss: 0.00191462
Iteration 13/25 | Loss: 0.00191462
Iteration 14/25 | Loss: 0.00191462
Iteration 15/25 | Loss: 0.00191462
Iteration 16/25 | Loss: 0.00191462
Iteration 17/25 | Loss: 0.00191462
Iteration 18/25 | Loss: 0.00191462
Iteration 19/25 | Loss: 0.00191462
Iteration 20/25 | Loss: 0.00191462
Iteration 21/25 | Loss: 0.00191462
Iteration 22/25 | Loss: 0.00191462
Iteration 23/25 | Loss: 0.00191462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019146163249388337, 0.0019146163249388337, 0.0019146163249388337, 0.0019146163249388337, 0.0019146163249388337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019146163249388337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191462
Iteration 2/1000 | Loss: 0.00066491
Iteration 3/1000 | Loss: 0.00050156
Iteration 4/1000 | Loss: 0.00020610
Iteration 5/1000 | Loss: 0.00013314
Iteration 6/1000 | Loss: 0.00064641
Iteration 7/1000 | Loss: 0.00077765
Iteration 8/1000 | Loss: 0.00076638
Iteration 9/1000 | Loss: 0.00044689
Iteration 10/1000 | Loss: 0.00013692
Iteration 11/1000 | Loss: 0.00111901
Iteration 12/1000 | Loss: 0.00010053
Iteration 13/1000 | Loss: 0.00009751
Iteration 14/1000 | Loss: 0.00035562
Iteration 15/1000 | Loss: 0.00034475
Iteration 16/1000 | Loss: 0.00014683
Iteration 17/1000 | Loss: 0.00010905
Iteration 18/1000 | Loss: 0.00008189
Iteration 19/1000 | Loss: 0.00009107
Iteration 20/1000 | Loss: 0.00009419
Iteration 21/1000 | Loss: 0.00007677
Iteration 22/1000 | Loss: 0.00027144
Iteration 23/1000 | Loss: 0.00008002
Iteration 24/1000 | Loss: 0.00007388
Iteration 25/1000 | Loss: 0.00025408
Iteration 26/1000 | Loss: 0.00024146
Iteration 27/1000 | Loss: 0.00051732
Iteration 28/1000 | Loss: 0.00052968
Iteration 29/1000 | Loss: 0.00198418
Iteration 30/1000 | Loss: 0.00174349
Iteration 31/1000 | Loss: 0.00436273
Iteration 32/1000 | Loss: 0.00028064
Iteration 33/1000 | Loss: 0.00229776
Iteration 34/1000 | Loss: 0.00009983
Iteration 35/1000 | Loss: 0.00007851
Iteration 36/1000 | Loss: 0.00021280
Iteration 37/1000 | Loss: 0.00019433
Iteration 38/1000 | Loss: 0.00131217
Iteration 39/1000 | Loss: 0.00043944
Iteration 40/1000 | Loss: 0.00054197
Iteration 41/1000 | Loss: 0.00005273
Iteration 42/1000 | Loss: 0.00005207
Iteration 43/1000 | Loss: 0.00004140
Iteration 44/1000 | Loss: 0.00004366
Iteration 45/1000 | Loss: 0.00004882
Iteration 46/1000 | Loss: 0.00004878
Iteration 47/1000 | Loss: 0.00003451
Iteration 48/1000 | Loss: 0.00005030
Iteration 49/1000 | Loss: 0.00026403
Iteration 50/1000 | Loss: 0.00017791
Iteration 51/1000 | Loss: 0.00004945
Iteration 52/1000 | Loss: 0.00011497
Iteration 53/1000 | Loss: 0.00047488
Iteration 54/1000 | Loss: 0.00007951
Iteration 55/1000 | Loss: 0.00024291
Iteration 56/1000 | Loss: 0.00005214
Iteration 57/1000 | Loss: 0.00003737
Iteration 58/1000 | Loss: 0.00012056
Iteration 59/1000 | Loss: 0.00002895
Iteration 60/1000 | Loss: 0.00006796
Iteration 61/1000 | Loss: 0.00012986
Iteration 62/1000 | Loss: 0.00002771
Iteration 63/1000 | Loss: 0.00002714
Iteration 64/1000 | Loss: 0.00002654
Iteration 65/1000 | Loss: 0.00043052
Iteration 66/1000 | Loss: 0.00036634
Iteration 67/1000 | Loss: 0.00006490
Iteration 68/1000 | Loss: 0.00002644
Iteration 69/1000 | Loss: 0.00002585
Iteration 70/1000 | Loss: 0.00002565
Iteration 71/1000 | Loss: 0.00002561
Iteration 72/1000 | Loss: 0.00002556
Iteration 73/1000 | Loss: 0.00002555
Iteration 74/1000 | Loss: 0.00002555
Iteration 75/1000 | Loss: 0.00002554
Iteration 76/1000 | Loss: 0.00002554
Iteration 77/1000 | Loss: 0.00020949
Iteration 78/1000 | Loss: 0.00014891
Iteration 79/1000 | Loss: 0.00002854
Iteration 80/1000 | Loss: 0.00005651
Iteration 81/1000 | Loss: 0.00020536
Iteration 82/1000 | Loss: 0.00141025
Iteration 83/1000 | Loss: 0.00029393
Iteration 84/1000 | Loss: 0.00005226
Iteration 85/1000 | Loss: 0.00002579
Iteration 86/1000 | Loss: 0.00005461
Iteration 87/1000 | Loss: 0.00012933
Iteration 88/1000 | Loss: 0.00003133
Iteration 89/1000 | Loss: 0.00007007
Iteration 90/1000 | Loss: 0.00002554
Iteration 91/1000 | Loss: 0.00002547
Iteration 92/1000 | Loss: 0.00002542
Iteration 93/1000 | Loss: 0.00002540
Iteration 94/1000 | Loss: 0.00002539
Iteration 95/1000 | Loss: 0.00002539
Iteration 96/1000 | Loss: 0.00002538
Iteration 97/1000 | Loss: 0.00002538
Iteration 98/1000 | Loss: 0.00002538
Iteration 99/1000 | Loss: 0.00002538
Iteration 100/1000 | Loss: 0.00002538
Iteration 101/1000 | Loss: 0.00002537
Iteration 102/1000 | Loss: 0.00002537
Iteration 103/1000 | Loss: 0.00002537
Iteration 104/1000 | Loss: 0.00002537
Iteration 105/1000 | Loss: 0.00002536
Iteration 106/1000 | Loss: 0.00002536
Iteration 107/1000 | Loss: 0.00002536
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002536
Iteration 110/1000 | Loss: 0.00002536
Iteration 111/1000 | Loss: 0.00002536
Iteration 112/1000 | Loss: 0.00002536
Iteration 113/1000 | Loss: 0.00002535
Iteration 114/1000 | Loss: 0.00002535
Iteration 115/1000 | Loss: 0.00002535
Iteration 116/1000 | Loss: 0.00002535
Iteration 117/1000 | Loss: 0.00002534
Iteration 118/1000 | Loss: 0.00002534
Iteration 119/1000 | Loss: 0.00002534
Iteration 120/1000 | Loss: 0.00002534
Iteration 121/1000 | Loss: 0.00002534
Iteration 122/1000 | Loss: 0.00002534
Iteration 123/1000 | Loss: 0.00002534
Iteration 124/1000 | Loss: 0.00002534
Iteration 125/1000 | Loss: 0.00002534
Iteration 126/1000 | Loss: 0.00002534
Iteration 127/1000 | Loss: 0.00002534
Iteration 128/1000 | Loss: 0.00002534
Iteration 129/1000 | Loss: 0.00002534
Iteration 130/1000 | Loss: 0.00002534
Iteration 131/1000 | Loss: 0.00002534
Iteration 132/1000 | Loss: 0.00002534
Iteration 133/1000 | Loss: 0.00002534
Iteration 134/1000 | Loss: 0.00002534
Iteration 135/1000 | Loss: 0.00002534
Iteration 136/1000 | Loss: 0.00002534
Iteration 137/1000 | Loss: 0.00002534
Iteration 138/1000 | Loss: 0.00002534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.533543920435477e-05, 2.533543920435477e-05, 2.533543920435477e-05, 2.533543920435477e-05, 2.533543920435477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.533543920435477e-05

Optimization complete. Final v2v error: 3.6404004096984863 mm

Highest mean error: 11.197185516357422 mm for frame 54

Lowest mean error: 3.154334545135498 mm for frame 28

Saving results

Total time: 160.45019817352295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824046
Iteration 2/25 | Loss: 0.00149420
Iteration 3/25 | Loss: 0.00126793
Iteration 4/25 | Loss: 0.00124555
Iteration 5/25 | Loss: 0.00124369
Iteration 6/25 | Loss: 0.00124369
Iteration 7/25 | Loss: 0.00124369
Iteration 8/25 | Loss: 0.00124369
Iteration 9/25 | Loss: 0.00124369
Iteration 10/25 | Loss: 0.00124369
Iteration 11/25 | Loss: 0.00124369
Iteration 12/25 | Loss: 0.00124369
Iteration 13/25 | Loss: 0.00124369
Iteration 14/25 | Loss: 0.00124369
Iteration 15/25 | Loss: 0.00124369
Iteration 16/25 | Loss: 0.00124369
Iteration 17/25 | Loss: 0.00124369
Iteration 18/25 | Loss: 0.00124369
Iteration 19/25 | Loss: 0.00124369
Iteration 20/25 | Loss: 0.00124369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001243685488589108, 0.001243685488589108, 0.001243685488589108, 0.001243685488589108, 0.001243685488589108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243685488589108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92220271
Iteration 2/25 | Loss: 0.00072968
Iteration 3/25 | Loss: 0.00072967
Iteration 4/25 | Loss: 0.00072967
Iteration 5/25 | Loss: 0.00072967
Iteration 6/25 | Loss: 0.00072967
Iteration 7/25 | Loss: 0.00072967
Iteration 8/25 | Loss: 0.00072967
Iteration 9/25 | Loss: 0.00072967
Iteration 10/25 | Loss: 0.00072967
Iteration 11/25 | Loss: 0.00072967
Iteration 12/25 | Loss: 0.00072967
Iteration 13/25 | Loss: 0.00072967
Iteration 14/25 | Loss: 0.00072967
Iteration 15/25 | Loss: 0.00072967
Iteration 16/25 | Loss: 0.00072967
Iteration 17/25 | Loss: 0.00072967
Iteration 18/25 | Loss: 0.00072967
Iteration 19/25 | Loss: 0.00072967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007296690018847585, 0.0007296690018847585, 0.0007296690018847585, 0.0007296690018847585, 0.0007296690018847585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007296690018847585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072967
Iteration 2/1000 | Loss: 0.00003224
Iteration 3/1000 | Loss: 0.00002418
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00001991
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001864
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00001832
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001813
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001792
Iteration 23/1000 | Loss: 0.00001788
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001775
Iteration 35/1000 | Loss: 0.00001775
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001774
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001774
Iteration 44/1000 | Loss: 0.00001774
Iteration 45/1000 | Loss: 0.00001772
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001771
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001771
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001771
Iteration 53/1000 | Loss: 0.00001771
Iteration 54/1000 | Loss: 0.00001770
Iteration 55/1000 | Loss: 0.00001770
Iteration 56/1000 | Loss: 0.00001770
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001769
Iteration 59/1000 | Loss: 0.00001769
Iteration 60/1000 | Loss: 0.00001768
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001765
Iteration 63/1000 | Loss: 0.00001765
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001759
Iteration 67/1000 | Loss: 0.00001759
Iteration 68/1000 | Loss: 0.00001759
Iteration 69/1000 | Loss: 0.00001759
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001759
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001759
Iteration 77/1000 | Loss: 0.00001759
Iteration 78/1000 | Loss: 0.00001759
Iteration 79/1000 | Loss: 0.00001759
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001757
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001756
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001750
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001749
Iteration 141/1000 | Loss: 0.00001749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.7486261640442535e-05, 1.7486261640442535e-05, 1.7486261640442535e-05, 1.7486261640442535e-05, 1.7486261640442535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7486261640442535e-05

Optimization complete. Final v2v error: 3.5314371585845947 mm

Highest mean error: 3.7348825931549072 mm for frame 18

Lowest mean error: 3.362495183944702 mm for frame 37

Saving results

Total time: 35.23608684539795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499270
Iteration 2/25 | Loss: 0.00131297
Iteration 3/25 | Loss: 0.00122758
Iteration 4/25 | Loss: 0.00121499
Iteration 5/25 | Loss: 0.00121070
Iteration 6/25 | Loss: 0.00121070
Iteration 7/25 | Loss: 0.00121070
Iteration 8/25 | Loss: 0.00121070
Iteration 9/25 | Loss: 0.00121070
Iteration 10/25 | Loss: 0.00121070
Iteration 11/25 | Loss: 0.00121070
Iteration 12/25 | Loss: 0.00121070
Iteration 13/25 | Loss: 0.00121070
Iteration 14/25 | Loss: 0.00121070
Iteration 15/25 | Loss: 0.00121070
Iteration 16/25 | Loss: 0.00121070
Iteration 17/25 | Loss: 0.00121070
Iteration 18/25 | Loss: 0.00121070
Iteration 19/25 | Loss: 0.00121070
Iteration 20/25 | Loss: 0.00121070
Iteration 21/25 | Loss: 0.00121070
Iteration 22/25 | Loss: 0.00121070
Iteration 23/25 | Loss: 0.00121070
Iteration 24/25 | Loss: 0.00121070
Iteration 25/25 | Loss: 0.00121070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78915262
Iteration 2/25 | Loss: 0.00110521
Iteration 3/25 | Loss: 0.00110521
Iteration 4/25 | Loss: 0.00110521
Iteration 5/25 | Loss: 0.00110521
Iteration 6/25 | Loss: 0.00110521
Iteration 7/25 | Loss: 0.00110521
Iteration 8/25 | Loss: 0.00110521
Iteration 9/25 | Loss: 0.00110521
Iteration 10/25 | Loss: 0.00110521
Iteration 11/25 | Loss: 0.00110521
Iteration 12/25 | Loss: 0.00110521
Iteration 13/25 | Loss: 0.00110521
Iteration 14/25 | Loss: 0.00110521
Iteration 15/25 | Loss: 0.00110521
Iteration 16/25 | Loss: 0.00110521
Iteration 17/25 | Loss: 0.00110521
Iteration 18/25 | Loss: 0.00110521
Iteration 19/25 | Loss: 0.00110521
Iteration 20/25 | Loss: 0.00110521
Iteration 21/25 | Loss: 0.00110521
Iteration 22/25 | Loss: 0.00110521
Iteration 23/25 | Loss: 0.00110521
Iteration 24/25 | Loss: 0.00110521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001105206785723567, 0.001105206785723567, 0.001105206785723567, 0.001105206785723567, 0.001105206785723567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105206785723567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110521
Iteration 2/1000 | Loss: 0.00002948
Iteration 3/1000 | Loss: 0.00002079
Iteration 4/1000 | Loss: 0.00001892
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001731
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001629
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001541
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001489
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001442
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001441
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001440
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001439
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001437
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001434
Iteration 49/1000 | Loss: 0.00001434
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001431
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001431
Iteration 60/1000 | Loss: 0.00001431
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001429
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001429
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001428
Iteration 73/1000 | Loss: 0.00001428
Iteration 74/1000 | Loss: 0.00001428
Iteration 75/1000 | Loss: 0.00001428
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001428
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001425
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001424
Iteration 94/1000 | Loss: 0.00001424
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001423
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001422
Iteration 113/1000 | Loss: 0.00001422
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001421
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001421
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001420
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001419
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001418
Iteration 157/1000 | Loss: 0.00001418
Iteration 158/1000 | Loss: 0.00001418
Iteration 159/1000 | Loss: 0.00001418
Iteration 160/1000 | Loss: 0.00001418
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001417
Iteration 175/1000 | Loss: 0.00001417
Iteration 176/1000 | Loss: 0.00001417
Iteration 177/1000 | Loss: 0.00001417
Iteration 178/1000 | Loss: 0.00001417
Iteration 179/1000 | Loss: 0.00001417
Iteration 180/1000 | Loss: 0.00001417
Iteration 181/1000 | Loss: 0.00001417
Iteration 182/1000 | Loss: 0.00001416
Iteration 183/1000 | Loss: 0.00001416
Iteration 184/1000 | Loss: 0.00001416
Iteration 185/1000 | Loss: 0.00001416
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001416
Iteration 188/1000 | Loss: 0.00001416
Iteration 189/1000 | Loss: 0.00001416
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001416
Iteration 193/1000 | Loss: 0.00001416
Iteration 194/1000 | Loss: 0.00001416
Iteration 195/1000 | Loss: 0.00001416
Iteration 196/1000 | Loss: 0.00001416
Iteration 197/1000 | Loss: 0.00001416
Iteration 198/1000 | Loss: 0.00001416
Iteration 199/1000 | Loss: 0.00001416
Iteration 200/1000 | Loss: 0.00001416
Iteration 201/1000 | Loss: 0.00001416
Iteration 202/1000 | Loss: 0.00001416
Iteration 203/1000 | Loss: 0.00001416
Iteration 204/1000 | Loss: 0.00001416
Iteration 205/1000 | Loss: 0.00001416
Iteration 206/1000 | Loss: 0.00001416
Iteration 207/1000 | Loss: 0.00001416
Iteration 208/1000 | Loss: 0.00001416
Iteration 209/1000 | Loss: 0.00001416
Iteration 210/1000 | Loss: 0.00001416
Iteration 211/1000 | Loss: 0.00001416
Iteration 212/1000 | Loss: 0.00001416
Iteration 213/1000 | Loss: 0.00001416
Iteration 214/1000 | Loss: 0.00001416
Iteration 215/1000 | Loss: 0.00001416
Iteration 216/1000 | Loss: 0.00001416
Iteration 217/1000 | Loss: 0.00001416
Iteration 218/1000 | Loss: 0.00001416
Iteration 219/1000 | Loss: 0.00001416
Iteration 220/1000 | Loss: 0.00001416
Iteration 221/1000 | Loss: 0.00001416
Iteration 222/1000 | Loss: 0.00001416
Iteration 223/1000 | Loss: 0.00001416
Iteration 224/1000 | Loss: 0.00001416
Iteration 225/1000 | Loss: 0.00001416
Iteration 226/1000 | Loss: 0.00001416
Iteration 227/1000 | Loss: 0.00001416
Iteration 228/1000 | Loss: 0.00001416
Iteration 229/1000 | Loss: 0.00001416
Iteration 230/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.4155363714962732e-05, 1.4155363714962732e-05, 1.4155363714962732e-05, 1.4155363714962732e-05, 1.4155363714962732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4155363714962732e-05

Optimization complete. Final v2v error: 3.1458024978637695 mm

Highest mean error: 3.55728816986084 mm for frame 0

Lowest mean error: 3.1052279472351074 mm for frame 236

Saving results

Total time: 54.86619424819946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018247
Iteration 2/25 | Loss: 0.01018247
Iteration 3/25 | Loss: 0.00185131
Iteration 4/25 | Loss: 0.00168094
Iteration 5/25 | Loss: 0.00176062
Iteration 6/25 | Loss: 0.00164779
Iteration 7/25 | Loss: 0.00121336
Iteration 8/25 | Loss: 0.00120209
Iteration 9/25 | Loss: 0.00119693
Iteration 10/25 | Loss: 0.00119620
Iteration 11/25 | Loss: 0.00119601
Iteration 12/25 | Loss: 0.00119593
Iteration 13/25 | Loss: 0.00119593
Iteration 14/25 | Loss: 0.00119592
Iteration 15/25 | Loss: 0.00119592
Iteration 16/25 | Loss: 0.00119592
Iteration 17/25 | Loss: 0.00119592
Iteration 18/25 | Loss: 0.00119592
Iteration 19/25 | Loss: 0.00119592
Iteration 20/25 | Loss: 0.00119592
Iteration 21/25 | Loss: 0.00119592
Iteration 22/25 | Loss: 0.00119592
Iteration 23/25 | Loss: 0.00119592
Iteration 24/25 | Loss: 0.00119591
Iteration 25/25 | Loss: 0.00119591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25185239
Iteration 2/25 | Loss: 0.00142110
Iteration 3/25 | Loss: 0.00142110
Iteration 4/25 | Loss: 0.00142110
Iteration 5/25 | Loss: 0.00142110
Iteration 6/25 | Loss: 0.00142110
Iteration 7/25 | Loss: 0.00142110
Iteration 8/25 | Loss: 0.00142110
Iteration 9/25 | Loss: 0.00142110
Iteration 10/25 | Loss: 0.00142110
Iteration 11/25 | Loss: 0.00142110
Iteration 12/25 | Loss: 0.00142110
Iteration 13/25 | Loss: 0.00142110
Iteration 14/25 | Loss: 0.00142110
Iteration 15/25 | Loss: 0.00142110
Iteration 16/25 | Loss: 0.00142110
Iteration 17/25 | Loss: 0.00142110
Iteration 18/25 | Loss: 0.00142109
Iteration 19/25 | Loss: 0.00142109
Iteration 20/25 | Loss: 0.00142109
Iteration 21/25 | Loss: 0.00142109
Iteration 22/25 | Loss: 0.00142109
Iteration 23/25 | Loss: 0.00142109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001421094755642116, 0.001421094755642116, 0.001421094755642116, 0.001421094755642116, 0.001421094755642116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001421094755642116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142109
Iteration 2/1000 | Loss: 0.00003529
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001666
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00008762
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00007770
Iteration 10/1000 | Loss: 0.00007044
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00010976
Iteration 17/1000 | Loss: 0.00001940
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00004145
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001222
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001221
Iteration 40/1000 | Loss: 0.00001221
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00005435
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Iteration 104/1000 | Loss: 0.00001206
Iteration 105/1000 | Loss: 0.00001206
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001206
Iteration 110/1000 | Loss: 0.00001206
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001205
Iteration 115/1000 | Loss: 0.00001205
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001204
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001204
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001203
Iteration 143/1000 | Loss: 0.00001203
Iteration 144/1000 | Loss: 0.00001203
Iteration 145/1000 | Loss: 0.00001203
Iteration 146/1000 | Loss: 0.00001203
Iteration 147/1000 | Loss: 0.00001203
Iteration 148/1000 | Loss: 0.00001203
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Iteration 151/1000 | Loss: 0.00001203
Iteration 152/1000 | Loss: 0.00001203
Iteration 153/1000 | Loss: 0.00001203
Iteration 154/1000 | Loss: 0.00001203
Iteration 155/1000 | Loss: 0.00001203
Iteration 156/1000 | Loss: 0.00001203
Iteration 157/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2034335668431595e-05, 1.2034335668431595e-05, 1.2034335668431595e-05, 1.2034335668431595e-05, 1.2034335668431595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2034335668431595e-05

Optimization complete. Final v2v error: 2.9636118412017822 mm

Highest mean error: 3.2981295585632324 mm for frame 109

Lowest mean error: 2.717900514602661 mm for frame 4

Saving results

Total time: 54.34693384170532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800103
Iteration 2/25 | Loss: 0.00135080
Iteration 3/25 | Loss: 0.00120290
Iteration 4/25 | Loss: 0.00118559
Iteration 5/25 | Loss: 0.00118185
Iteration 6/25 | Loss: 0.00118098
Iteration 7/25 | Loss: 0.00118095
Iteration 8/25 | Loss: 0.00118095
Iteration 9/25 | Loss: 0.00118095
Iteration 10/25 | Loss: 0.00118095
Iteration 11/25 | Loss: 0.00118095
Iteration 12/25 | Loss: 0.00118095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011809461284428835, 0.0011809461284428835, 0.0011809461284428835, 0.0011809461284428835, 0.0011809461284428835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011809461284428835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21150684
Iteration 2/25 | Loss: 0.00107453
Iteration 3/25 | Loss: 0.00107450
Iteration 4/25 | Loss: 0.00107449
Iteration 5/25 | Loss: 0.00107449
Iteration 6/25 | Loss: 0.00107449
Iteration 7/25 | Loss: 0.00107449
Iteration 8/25 | Loss: 0.00107449
Iteration 9/25 | Loss: 0.00107449
Iteration 10/25 | Loss: 0.00107449
Iteration 11/25 | Loss: 0.00107449
Iteration 12/25 | Loss: 0.00107449
Iteration 13/25 | Loss: 0.00107449
Iteration 14/25 | Loss: 0.00107449
Iteration 15/25 | Loss: 0.00107449
Iteration 16/25 | Loss: 0.00107449
Iteration 17/25 | Loss: 0.00107449
Iteration 18/25 | Loss: 0.00107449
Iteration 19/25 | Loss: 0.00107449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010744924657046795, 0.0010744924657046795, 0.0010744924657046795, 0.0010744924657046795, 0.0010744924657046795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010744924657046795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107449
Iteration 2/1000 | Loss: 0.00003707
Iteration 3/1000 | Loss: 0.00002380
Iteration 4/1000 | Loss: 0.00001904
Iteration 5/1000 | Loss: 0.00001761
Iteration 6/1000 | Loss: 0.00001667
Iteration 7/1000 | Loss: 0.00001587
Iteration 8/1000 | Loss: 0.00001535
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001466
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001369
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001337
Iteration 53/1000 | Loss: 0.00001337
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001336
Iteration 57/1000 | Loss: 0.00001336
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001328
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001326
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001324
Iteration 115/1000 | Loss: 0.00001324
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001322
Iteration 138/1000 | Loss: 0.00001322
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001319
Iteration 159/1000 | Loss: 0.00001319
Iteration 160/1000 | Loss: 0.00001319
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001317
Iteration 165/1000 | Loss: 0.00001317
Iteration 166/1000 | Loss: 0.00001317
Iteration 167/1000 | Loss: 0.00001317
Iteration 168/1000 | Loss: 0.00001317
Iteration 169/1000 | Loss: 0.00001317
Iteration 170/1000 | Loss: 0.00001317
Iteration 171/1000 | Loss: 0.00001317
Iteration 172/1000 | Loss: 0.00001317
Iteration 173/1000 | Loss: 0.00001317
Iteration 174/1000 | Loss: 0.00001317
Iteration 175/1000 | Loss: 0.00001316
Iteration 176/1000 | Loss: 0.00001316
Iteration 177/1000 | Loss: 0.00001316
Iteration 178/1000 | Loss: 0.00001316
Iteration 179/1000 | Loss: 0.00001316
Iteration 180/1000 | Loss: 0.00001316
Iteration 181/1000 | Loss: 0.00001316
Iteration 182/1000 | Loss: 0.00001316
Iteration 183/1000 | Loss: 0.00001316
Iteration 184/1000 | Loss: 0.00001316
Iteration 185/1000 | Loss: 0.00001316
Iteration 186/1000 | Loss: 0.00001316
Iteration 187/1000 | Loss: 0.00001316
Iteration 188/1000 | Loss: 0.00001316
Iteration 189/1000 | Loss: 0.00001316
Iteration 190/1000 | Loss: 0.00001316
Iteration 191/1000 | Loss: 0.00001316
Iteration 192/1000 | Loss: 0.00001316
Iteration 193/1000 | Loss: 0.00001316
Iteration 194/1000 | Loss: 0.00001316
Iteration 195/1000 | Loss: 0.00001316
Iteration 196/1000 | Loss: 0.00001316
Iteration 197/1000 | Loss: 0.00001316
Iteration 198/1000 | Loss: 0.00001316
Iteration 199/1000 | Loss: 0.00001316
Iteration 200/1000 | Loss: 0.00001316
Iteration 201/1000 | Loss: 0.00001316
Iteration 202/1000 | Loss: 0.00001316
Iteration 203/1000 | Loss: 0.00001316
Iteration 204/1000 | Loss: 0.00001316
Iteration 205/1000 | Loss: 0.00001316
Iteration 206/1000 | Loss: 0.00001316
Iteration 207/1000 | Loss: 0.00001316
Iteration 208/1000 | Loss: 0.00001316
Iteration 209/1000 | Loss: 0.00001316
Iteration 210/1000 | Loss: 0.00001316
Iteration 211/1000 | Loss: 0.00001316
Iteration 212/1000 | Loss: 0.00001316
Iteration 213/1000 | Loss: 0.00001316
Iteration 214/1000 | Loss: 0.00001316
Iteration 215/1000 | Loss: 0.00001316
Iteration 216/1000 | Loss: 0.00001316
Iteration 217/1000 | Loss: 0.00001316
Iteration 218/1000 | Loss: 0.00001316
Iteration 219/1000 | Loss: 0.00001316
Iteration 220/1000 | Loss: 0.00001316
Iteration 221/1000 | Loss: 0.00001316
Iteration 222/1000 | Loss: 0.00001316
Iteration 223/1000 | Loss: 0.00001316
Iteration 224/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.3164503798179794e-05, 1.3164503798179794e-05, 1.3164503798179794e-05, 1.3164503798179794e-05, 1.3164503798179794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3164503798179794e-05

Optimization complete. Final v2v error: 3.0719990730285645 mm

Highest mean error: 3.5407614707946777 mm for frame 92

Lowest mean error: 2.7371602058410645 mm for frame 171

Saving results

Total time: 44.56599998474121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838956
Iteration 2/25 | Loss: 0.00144433
Iteration 3/25 | Loss: 0.00128619
Iteration 4/25 | Loss: 0.00126034
Iteration 5/25 | Loss: 0.00125250
Iteration 6/25 | Loss: 0.00125039
Iteration 7/25 | Loss: 0.00125039
Iteration 8/25 | Loss: 0.00125039
Iteration 9/25 | Loss: 0.00125039
Iteration 10/25 | Loss: 0.00125039
Iteration 11/25 | Loss: 0.00125039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012503908947110176, 0.0012503908947110176, 0.0012503908947110176, 0.0012503908947110176, 0.0012503908947110176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012503908947110176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.90402126
Iteration 2/25 | Loss: 0.00168899
Iteration 3/25 | Loss: 0.00168879
Iteration 4/25 | Loss: 0.00168879
Iteration 5/25 | Loss: 0.00168879
Iteration 6/25 | Loss: 0.00168879
Iteration 7/25 | Loss: 0.00168879
Iteration 8/25 | Loss: 0.00168879
Iteration 9/25 | Loss: 0.00168879
Iteration 10/25 | Loss: 0.00168879
Iteration 11/25 | Loss: 0.00168879
Iteration 12/25 | Loss: 0.00168879
Iteration 13/25 | Loss: 0.00168879
Iteration 14/25 | Loss: 0.00168879
Iteration 15/25 | Loss: 0.00168879
Iteration 16/25 | Loss: 0.00168879
Iteration 17/25 | Loss: 0.00168879
Iteration 18/25 | Loss: 0.00168879
Iteration 19/25 | Loss: 0.00168879
Iteration 20/25 | Loss: 0.00168879
Iteration 21/25 | Loss: 0.00168879
Iteration 22/25 | Loss: 0.00168879
Iteration 23/25 | Loss: 0.00168879
Iteration 24/25 | Loss: 0.00168879
Iteration 25/25 | Loss: 0.00168879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168879
Iteration 2/1000 | Loss: 0.00005917
Iteration 3/1000 | Loss: 0.00003995
Iteration 4/1000 | Loss: 0.00003396
Iteration 5/1000 | Loss: 0.00003193
Iteration 6/1000 | Loss: 0.00003010
Iteration 7/1000 | Loss: 0.00002910
Iteration 8/1000 | Loss: 0.00002829
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002719
Iteration 11/1000 | Loss: 0.00002683
Iteration 12/1000 | Loss: 0.00002641
Iteration 13/1000 | Loss: 0.00002613
Iteration 14/1000 | Loss: 0.00002587
Iteration 15/1000 | Loss: 0.00002567
Iteration 16/1000 | Loss: 0.00002561
Iteration 17/1000 | Loss: 0.00002546
Iteration 18/1000 | Loss: 0.00002539
Iteration 19/1000 | Loss: 0.00002532
Iteration 20/1000 | Loss: 0.00002531
Iteration 21/1000 | Loss: 0.00002530
Iteration 22/1000 | Loss: 0.00002525
Iteration 23/1000 | Loss: 0.00002525
Iteration 24/1000 | Loss: 0.00002524
Iteration 25/1000 | Loss: 0.00002523
Iteration 26/1000 | Loss: 0.00002523
Iteration 27/1000 | Loss: 0.00002519
Iteration 28/1000 | Loss: 0.00002519
Iteration 29/1000 | Loss: 0.00002517
Iteration 30/1000 | Loss: 0.00002516
Iteration 31/1000 | Loss: 0.00002516
Iteration 32/1000 | Loss: 0.00002516
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002515
Iteration 35/1000 | Loss: 0.00002515
Iteration 36/1000 | Loss: 0.00002514
Iteration 37/1000 | Loss: 0.00002514
Iteration 38/1000 | Loss: 0.00002513
Iteration 39/1000 | Loss: 0.00002512
Iteration 40/1000 | Loss: 0.00002512
Iteration 41/1000 | Loss: 0.00002511
Iteration 42/1000 | Loss: 0.00002511
Iteration 43/1000 | Loss: 0.00002510
Iteration 44/1000 | Loss: 0.00002510
Iteration 45/1000 | Loss: 0.00002509
Iteration 46/1000 | Loss: 0.00002509
Iteration 47/1000 | Loss: 0.00002508
Iteration 48/1000 | Loss: 0.00002508
Iteration 49/1000 | Loss: 0.00002507
Iteration 50/1000 | Loss: 0.00002507
Iteration 51/1000 | Loss: 0.00002507
Iteration 52/1000 | Loss: 0.00002506
Iteration 53/1000 | Loss: 0.00002505
Iteration 54/1000 | Loss: 0.00002505
Iteration 55/1000 | Loss: 0.00002504
Iteration 56/1000 | Loss: 0.00002504
Iteration 57/1000 | Loss: 0.00002504
Iteration 58/1000 | Loss: 0.00002503
Iteration 59/1000 | Loss: 0.00002503
Iteration 60/1000 | Loss: 0.00002503
Iteration 61/1000 | Loss: 0.00002502
Iteration 62/1000 | Loss: 0.00002502
Iteration 63/1000 | Loss: 0.00002501
Iteration 64/1000 | Loss: 0.00002501
Iteration 65/1000 | Loss: 0.00002501
Iteration 66/1000 | Loss: 0.00002500
Iteration 67/1000 | Loss: 0.00002500
Iteration 68/1000 | Loss: 0.00002500
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002499
Iteration 71/1000 | Loss: 0.00002499
Iteration 72/1000 | Loss: 0.00002498
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00002498
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002497
Iteration 77/1000 | Loss: 0.00002497
Iteration 78/1000 | Loss: 0.00002496
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002496
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002496
Iteration 85/1000 | Loss: 0.00002496
Iteration 86/1000 | Loss: 0.00002496
Iteration 87/1000 | Loss: 0.00002495
Iteration 88/1000 | Loss: 0.00002495
Iteration 89/1000 | Loss: 0.00002495
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002494
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002493
Iteration 95/1000 | Loss: 0.00002493
Iteration 96/1000 | Loss: 0.00002493
Iteration 97/1000 | Loss: 0.00002493
Iteration 98/1000 | Loss: 0.00002492
Iteration 99/1000 | Loss: 0.00002492
Iteration 100/1000 | Loss: 0.00002492
Iteration 101/1000 | Loss: 0.00002491
Iteration 102/1000 | Loss: 0.00002491
Iteration 103/1000 | Loss: 0.00002491
Iteration 104/1000 | Loss: 0.00002491
Iteration 105/1000 | Loss: 0.00002491
Iteration 106/1000 | Loss: 0.00002490
Iteration 107/1000 | Loss: 0.00002490
Iteration 108/1000 | Loss: 0.00002489
Iteration 109/1000 | Loss: 0.00002489
Iteration 110/1000 | Loss: 0.00002489
Iteration 111/1000 | Loss: 0.00002488
Iteration 112/1000 | Loss: 0.00002488
Iteration 113/1000 | Loss: 0.00002488
Iteration 114/1000 | Loss: 0.00002488
Iteration 115/1000 | Loss: 0.00002488
Iteration 116/1000 | Loss: 0.00002488
Iteration 117/1000 | Loss: 0.00002487
Iteration 118/1000 | Loss: 0.00002487
Iteration 119/1000 | Loss: 0.00002487
Iteration 120/1000 | Loss: 0.00002487
Iteration 121/1000 | Loss: 0.00002486
Iteration 122/1000 | Loss: 0.00002486
Iteration 123/1000 | Loss: 0.00002486
Iteration 124/1000 | Loss: 0.00002485
Iteration 125/1000 | Loss: 0.00002485
Iteration 126/1000 | Loss: 0.00002485
Iteration 127/1000 | Loss: 0.00002485
Iteration 128/1000 | Loss: 0.00002484
Iteration 129/1000 | Loss: 0.00002484
Iteration 130/1000 | Loss: 0.00002484
Iteration 131/1000 | Loss: 0.00002484
Iteration 132/1000 | Loss: 0.00002484
Iteration 133/1000 | Loss: 0.00002484
Iteration 134/1000 | Loss: 0.00002483
Iteration 135/1000 | Loss: 0.00002483
Iteration 136/1000 | Loss: 0.00002483
Iteration 137/1000 | Loss: 0.00002483
Iteration 138/1000 | Loss: 0.00002483
Iteration 139/1000 | Loss: 0.00002482
Iteration 140/1000 | Loss: 0.00002482
Iteration 141/1000 | Loss: 0.00002482
Iteration 142/1000 | Loss: 0.00002482
Iteration 143/1000 | Loss: 0.00002482
Iteration 144/1000 | Loss: 0.00002481
Iteration 145/1000 | Loss: 0.00002481
Iteration 146/1000 | Loss: 0.00002481
Iteration 147/1000 | Loss: 0.00002481
Iteration 148/1000 | Loss: 0.00002481
Iteration 149/1000 | Loss: 0.00002480
Iteration 150/1000 | Loss: 0.00002480
Iteration 151/1000 | Loss: 0.00002480
Iteration 152/1000 | Loss: 0.00002480
Iteration 153/1000 | Loss: 0.00002480
Iteration 154/1000 | Loss: 0.00002480
Iteration 155/1000 | Loss: 0.00002479
Iteration 156/1000 | Loss: 0.00002479
Iteration 157/1000 | Loss: 0.00002479
Iteration 158/1000 | Loss: 0.00002479
Iteration 159/1000 | Loss: 0.00002479
Iteration 160/1000 | Loss: 0.00002478
Iteration 161/1000 | Loss: 0.00002478
Iteration 162/1000 | Loss: 0.00002478
Iteration 163/1000 | Loss: 0.00002478
Iteration 164/1000 | Loss: 0.00002478
Iteration 165/1000 | Loss: 0.00002477
Iteration 166/1000 | Loss: 0.00002477
Iteration 167/1000 | Loss: 0.00002477
Iteration 168/1000 | Loss: 0.00002477
Iteration 169/1000 | Loss: 0.00002476
Iteration 170/1000 | Loss: 0.00002476
Iteration 171/1000 | Loss: 0.00002476
Iteration 172/1000 | Loss: 0.00002476
Iteration 173/1000 | Loss: 0.00002476
Iteration 174/1000 | Loss: 0.00002476
Iteration 175/1000 | Loss: 0.00002476
Iteration 176/1000 | Loss: 0.00002476
Iteration 177/1000 | Loss: 0.00002476
Iteration 178/1000 | Loss: 0.00002476
Iteration 179/1000 | Loss: 0.00002476
Iteration 180/1000 | Loss: 0.00002476
Iteration 181/1000 | Loss: 0.00002476
Iteration 182/1000 | Loss: 0.00002476
Iteration 183/1000 | Loss: 0.00002476
Iteration 184/1000 | Loss: 0.00002475
Iteration 185/1000 | Loss: 0.00002475
Iteration 186/1000 | Loss: 0.00002475
Iteration 187/1000 | Loss: 0.00002475
Iteration 188/1000 | Loss: 0.00002475
Iteration 189/1000 | Loss: 0.00002475
Iteration 190/1000 | Loss: 0.00002475
Iteration 191/1000 | Loss: 0.00002474
Iteration 192/1000 | Loss: 0.00002474
Iteration 193/1000 | Loss: 0.00002474
Iteration 194/1000 | Loss: 0.00002474
Iteration 195/1000 | Loss: 0.00002474
Iteration 196/1000 | Loss: 0.00002473
Iteration 197/1000 | Loss: 0.00002473
Iteration 198/1000 | Loss: 0.00002473
Iteration 199/1000 | Loss: 0.00002473
Iteration 200/1000 | Loss: 0.00002473
Iteration 201/1000 | Loss: 0.00002473
Iteration 202/1000 | Loss: 0.00002473
Iteration 203/1000 | Loss: 0.00002473
Iteration 204/1000 | Loss: 0.00002473
Iteration 205/1000 | Loss: 0.00002473
Iteration 206/1000 | Loss: 0.00002473
Iteration 207/1000 | Loss: 0.00002473
Iteration 208/1000 | Loss: 0.00002473
Iteration 209/1000 | Loss: 0.00002472
Iteration 210/1000 | Loss: 0.00002472
Iteration 211/1000 | Loss: 0.00002472
Iteration 212/1000 | Loss: 0.00002472
Iteration 213/1000 | Loss: 0.00002472
Iteration 214/1000 | Loss: 0.00002472
Iteration 215/1000 | Loss: 0.00002472
Iteration 216/1000 | Loss: 0.00002472
Iteration 217/1000 | Loss: 0.00002472
Iteration 218/1000 | Loss: 0.00002471
Iteration 219/1000 | Loss: 0.00002471
Iteration 220/1000 | Loss: 0.00002471
Iteration 221/1000 | Loss: 0.00002471
Iteration 222/1000 | Loss: 0.00002471
Iteration 223/1000 | Loss: 0.00002471
Iteration 224/1000 | Loss: 0.00002471
Iteration 225/1000 | Loss: 0.00002471
Iteration 226/1000 | Loss: 0.00002471
Iteration 227/1000 | Loss: 0.00002471
Iteration 228/1000 | Loss: 0.00002471
Iteration 229/1000 | Loss: 0.00002470
Iteration 230/1000 | Loss: 0.00002470
Iteration 231/1000 | Loss: 0.00002470
Iteration 232/1000 | Loss: 0.00002470
Iteration 233/1000 | Loss: 0.00002470
Iteration 234/1000 | Loss: 0.00002470
Iteration 235/1000 | Loss: 0.00002470
Iteration 236/1000 | Loss: 0.00002470
Iteration 237/1000 | Loss: 0.00002470
Iteration 238/1000 | Loss: 0.00002470
Iteration 239/1000 | Loss: 0.00002470
Iteration 240/1000 | Loss: 0.00002470
Iteration 241/1000 | Loss: 0.00002469
Iteration 242/1000 | Loss: 0.00002469
Iteration 243/1000 | Loss: 0.00002469
Iteration 244/1000 | Loss: 0.00002469
Iteration 245/1000 | Loss: 0.00002469
Iteration 246/1000 | Loss: 0.00002469
Iteration 247/1000 | Loss: 0.00002469
Iteration 248/1000 | Loss: 0.00002469
Iteration 249/1000 | Loss: 0.00002469
Iteration 250/1000 | Loss: 0.00002469
Iteration 251/1000 | Loss: 0.00002469
Iteration 252/1000 | Loss: 0.00002468
Iteration 253/1000 | Loss: 0.00002468
Iteration 254/1000 | Loss: 0.00002468
Iteration 255/1000 | Loss: 0.00002468
Iteration 256/1000 | Loss: 0.00002468
Iteration 257/1000 | Loss: 0.00002468
Iteration 258/1000 | Loss: 0.00002468
Iteration 259/1000 | Loss: 0.00002468
Iteration 260/1000 | Loss: 0.00002468
Iteration 261/1000 | Loss: 0.00002468
Iteration 262/1000 | Loss: 0.00002468
Iteration 263/1000 | Loss: 0.00002468
Iteration 264/1000 | Loss: 0.00002468
Iteration 265/1000 | Loss: 0.00002468
Iteration 266/1000 | Loss: 0.00002468
Iteration 267/1000 | Loss: 0.00002468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.4679762645973824e-05, 2.4679762645973824e-05, 2.4679762645973824e-05, 2.4679762645973824e-05, 2.4679762645973824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4679762645973824e-05

Optimization complete. Final v2v error: 4.096031665802002 mm

Highest mean error: 6.3340654373168945 mm for frame 130

Lowest mean error: 2.961883068084717 mm for frame 153

Saving results

Total time: 60.647029399871826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815095
Iteration 2/25 | Loss: 0.00125425
Iteration 3/25 | Loss: 0.00117516
Iteration 4/25 | Loss: 0.00116081
Iteration 5/25 | Loss: 0.00115654
Iteration 6/25 | Loss: 0.00115582
Iteration 7/25 | Loss: 0.00115578
Iteration 8/25 | Loss: 0.00115578
Iteration 9/25 | Loss: 0.00115578
Iteration 10/25 | Loss: 0.00115578
Iteration 11/25 | Loss: 0.00115578
Iteration 12/25 | Loss: 0.00115578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001155779347755015, 0.001155779347755015, 0.001155779347755015, 0.001155779347755015, 0.001155779347755015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001155779347755015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32134044
Iteration 2/25 | Loss: 0.00122720
Iteration 3/25 | Loss: 0.00122720
Iteration 4/25 | Loss: 0.00122720
Iteration 5/25 | Loss: 0.00122719
Iteration 6/25 | Loss: 0.00122719
Iteration 7/25 | Loss: 0.00122719
Iteration 8/25 | Loss: 0.00122719
Iteration 9/25 | Loss: 0.00122719
Iteration 10/25 | Loss: 0.00122719
Iteration 11/25 | Loss: 0.00122719
Iteration 12/25 | Loss: 0.00122719
Iteration 13/25 | Loss: 0.00122719
Iteration 14/25 | Loss: 0.00122719
Iteration 15/25 | Loss: 0.00122719
Iteration 16/25 | Loss: 0.00122719
Iteration 17/25 | Loss: 0.00122719
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012271928135305643, 0.0012271928135305643, 0.0012271928135305643, 0.0012271928135305643, 0.0012271928135305643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012271928135305643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122719
Iteration 2/1000 | Loss: 0.00002341
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001230
Iteration 9/1000 | Loss: 0.00001200
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001176
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001126
Iteration 24/1000 | Loss: 0.00001126
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001125
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001118
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001115
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001114
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001100
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001098
Iteration 88/1000 | Loss: 0.00001098
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001097
Iteration 96/1000 | Loss: 0.00001097
Iteration 97/1000 | Loss: 0.00001097
Iteration 98/1000 | Loss: 0.00001097
Iteration 99/1000 | Loss: 0.00001097
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001096
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001094
Iteration 108/1000 | Loss: 0.00001094
Iteration 109/1000 | Loss: 0.00001094
Iteration 110/1000 | Loss: 0.00001094
Iteration 111/1000 | Loss: 0.00001094
Iteration 112/1000 | Loss: 0.00001094
Iteration 113/1000 | Loss: 0.00001094
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001094
Iteration 116/1000 | Loss: 0.00001094
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001093
Iteration 119/1000 | Loss: 0.00001093
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001093
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001092
Iteration 134/1000 | Loss: 0.00001092
Iteration 135/1000 | Loss: 0.00001092
Iteration 136/1000 | Loss: 0.00001092
Iteration 137/1000 | Loss: 0.00001092
Iteration 138/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.0918221050815191e-05, 1.0918221050815191e-05, 1.0918221050815191e-05, 1.0918221050815191e-05, 1.0918221050815191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0918221050815191e-05

Optimization complete. Final v2v error: 2.881810188293457 mm

Highest mean error: 2.9212605953216553 mm for frame 65

Lowest mean error: 2.8221116065979004 mm for frame 103

Saving results

Total time: 37.111101388931274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549434
Iteration 2/25 | Loss: 0.00137480
Iteration 3/25 | Loss: 0.00126736
Iteration 4/25 | Loss: 0.00125512
Iteration 5/25 | Loss: 0.00125118
Iteration 6/25 | Loss: 0.00125077
Iteration 7/25 | Loss: 0.00125077
Iteration 8/25 | Loss: 0.00125077
Iteration 9/25 | Loss: 0.00125077
Iteration 10/25 | Loss: 0.00125077
Iteration 11/25 | Loss: 0.00125077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012507737847045064, 0.0012507737847045064, 0.0012507737847045064, 0.0012507737847045064, 0.0012507737847045064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012507737847045064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31335640
Iteration 2/25 | Loss: 0.00145389
Iteration 3/25 | Loss: 0.00145387
Iteration 4/25 | Loss: 0.00145387
Iteration 5/25 | Loss: 0.00145387
Iteration 6/25 | Loss: 0.00145387
Iteration 7/25 | Loss: 0.00145387
Iteration 8/25 | Loss: 0.00145387
Iteration 9/25 | Loss: 0.00145387
Iteration 10/25 | Loss: 0.00145387
Iteration 11/25 | Loss: 0.00145387
Iteration 12/25 | Loss: 0.00145387
Iteration 13/25 | Loss: 0.00145387
Iteration 14/25 | Loss: 0.00145387
Iteration 15/25 | Loss: 0.00145387
Iteration 16/25 | Loss: 0.00145387
Iteration 17/25 | Loss: 0.00145387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001453868462704122, 0.001453868462704122, 0.001453868462704122, 0.001453868462704122, 0.001453868462704122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001453868462704122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145387
Iteration 2/1000 | Loss: 0.00003492
Iteration 3/1000 | Loss: 0.00002787
Iteration 4/1000 | Loss: 0.00002625
Iteration 5/1000 | Loss: 0.00002538
Iteration 6/1000 | Loss: 0.00002480
Iteration 7/1000 | Loss: 0.00002442
Iteration 8/1000 | Loss: 0.00002412
Iteration 9/1000 | Loss: 0.00002381
Iteration 10/1000 | Loss: 0.00002357
Iteration 11/1000 | Loss: 0.00002337
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002294
Iteration 14/1000 | Loss: 0.00002280
Iteration 15/1000 | Loss: 0.00002267
Iteration 16/1000 | Loss: 0.00002261
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002255
Iteration 19/1000 | Loss: 0.00002255
Iteration 20/1000 | Loss: 0.00002252
Iteration 21/1000 | Loss: 0.00002245
Iteration 22/1000 | Loss: 0.00002245
Iteration 23/1000 | Loss: 0.00002243
Iteration 24/1000 | Loss: 0.00002243
Iteration 25/1000 | Loss: 0.00002243
Iteration 26/1000 | Loss: 0.00002243
Iteration 27/1000 | Loss: 0.00002243
Iteration 28/1000 | Loss: 0.00002243
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00002243
Iteration 31/1000 | Loss: 0.00002243
Iteration 32/1000 | Loss: 0.00002243
Iteration 33/1000 | Loss: 0.00002242
Iteration 34/1000 | Loss: 0.00002242
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002242
Iteration 40/1000 | Loss: 0.00002242
Iteration 41/1000 | Loss: 0.00002242
Iteration 42/1000 | Loss: 0.00002242
Iteration 43/1000 | Loss: 0.00002241
Iteration 44/1000 | Loss: 0.00002241
Iteration 45/1000 | Loss: 0.00002241
Iteration 46/1000 | Loss: 0.00002240
Iteration 47/1000 | Loss: 0.00002240
Iteration 48/1000 | Loss: 0.00002235
Iteration 49/1000 | Loss: 0.00002232
Iteration 50/1000 | Loss: 0.00002232
Iteration 51/1000 | Loss: 0.00002231
Iteration 52/1000 | Loss: 0.00002231
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002229
Iteration 55/1000 | Loss: 0.00002229
Iteration 56/1000 | Loss: 0.00002228
Iteration 57/1000 | Loss: 0.00002228
Iteration 58/1000 | Loss: 0.00002228
Iteration 59/1000 | Loss: 0.00002228
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002228
Iteration 62/1000 | Loss: 0.00002227
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002227
Iteration 66/1000 | Loss: 0.00002227
Iteration 67/1000 | Loss: 0.00002227
Iteration 68/1000 | Loss: 0.00002226
Iteration 69/1000 | Loss: 0.00002225
Iteration 70/1000 | Loss: 0.00002225
Iteration 71/1000 | Loss: 0.00002224
Iteration 72/1000 | Loss: 0.00002223
Iteration 73/1000 | Loss: 0.00002222
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002222
Iteration 79/1000 | Loss: 0.00002222
Iteration 80/1000 | Loss: 0.00002222
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00002221
Iteration 83/1000 | Loss: 0.00002221
Iteration 84/1000 | Loss: 0.00002221
Iteration 85/1000 | Loss: 0.00002221
Iteration 86/1000 | Loss: 0.00002221
Iteration 87/1000 | Loss: 0.00002221
Iteration 88/1000 | Loss: 0.00002221
Iteration 89/1000 | Loss: 0.00002221
Iteration 90/1000 | Loss: 0.00002221
Iteration 91/1000 | Loss: 0.00002220
Iteration 92/1000 | Loss: 0.00002220
Iteration 93/1000 | Loss: 0.00002220
Iteration 94/1000 | Loss: 0.00002220
Iteration 95/1000 | Loss: 0.00002220
Iteration 96/1000 | Loss: 0.00002220
Iteration 97/1000 | Loss: 0.00002220
Iteration 98/1000 | Loss: 0.00002220
Iteration 99/1000 | Loss: 0.00002220
Iteration 100/1000 | Loss: 0.00002220
Iteration 101/1000 | Loss: 0.00002220
Iteration 102/1000 | Loss: 0.00002220
Iteration 103/1000 | Loss: 0.00002220
Iteration 104/1000 | Loss: 0.00002220
Iteration 105/1000 | Loss: 0.00002220
Iteration 106/1000 | Loss: 0.00002220
Iteration 107/1000 | Loss: 0.00002220
Iteration 108/1000 | Loss: 0.00002220
Iteration 109/1000 | Loss: 0.00002220
Iteration 110/1000 | Loss: 0.00002220
Iteration 111/1000 | Loss: 0.00002220
Iteration 112/1000 | Loss: 0.00002220
Iteration 113/1000 | Loss: 0.00002220
Iteration 114/1000 | Loss: 0.00002220
Iteration 115/1000 | Loss: 0.00002220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.2200098101166077e-05, 2.2200098101166077e-05, 2.2200098101166077e-05, 2.2200098101166077e-05, 2.2200098101166077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2200098101166077e-05

Optimization complete. Final v2v error: 3.812821865081787 mm

Highest mean error: 4.2279486656188965 mm for frame 132

Lowest mean error: 3.373035430908203 mm for frame 80

Saving results

Total time: 44.629905700683594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627878
Iteration 2/25 | Loss: 0.00143738
Iteration 3/25 | Loss: 0.00127797
Iteration 4/25 | Loss: 0.00126600
Iteration 5/25 | Loss: 0.00124896
Iteration 6/25 | Loss: 0.00123489
Iteration 7/25 | Loss: 0.00124235
Iteration 8/25 | Loss: 0.00123300
Iteration 9/25 | Loss: 0.00123248
Iteration 10/25 | Loss: 0.00123916
Iteration 11/25 | Loss: 0.00123133
Iteration 12/25 | Loss: 0.00124120
Iteration 13/25 | Loss: 0.00122996
Iteration 14/25 | Loss: 0.00122954
Iteration 15/25 | Loss: 0.00123553
Iteration 16/25 | Loss: 0.00122865
Iteration 17/25 | Loss: 0.00122718
Iteration 18/25 | Loss: 0.00122471
Iteration 19/25 | Loss: 0.00122338
Iteration 20/25 | Loss: 0.00122317
Iteration 21/25 | Loss: 0.00122310
Iteration 22/25 | Loss: 0.00122310
Iteration 23/25 | Loss: 0.00122310
Iteration 24/25 | Loss: 0.00122310
Iteration 25/25 | Loss: 0.00122310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59623384
Iteration 2/25 | Loss: 0.00207649
Iteration 3/25 | Loss: 0.00207649
Iteration 4/25 | Loss: 0.00207649
Iteration 5/25 | Loss: 0.00207648
Iteration 6/25 | Loss: 0.00207648
Iteration 7/25 | Loss: 0.00207648
Iteration 8/25 | Loss: 0.00207648
Iteration 9/25 | Loss: 0.00207648
Iteration 10/25 | Loss: 0.00207648
Iteration 11/25 | Loss: 0.00207648
Iteration 12/25 | Loss: 0.00207648
Iteration 13/25 | Loss: 0.00207648
Iteration 14/25 | Loss: 0.00207648
Iteration 15/25 | Loss: 0.00207648
Iteration 16/25 | Loss: 0.00207648
Iteration 17/25 | Loss: 0.00207648
Iteration 18/25 | Loss: 0.00207648
Iteration 19/25 | Loss: 0.00207648
Iteration 20/25 | Loss: 0.00207648
Iteration 21/25 | Loss: 0.00207648
Iteration 22/25 | Loss: 0.00207648
Iteration 23/25 | Loss: 0.00207648
Iteration 24/25 | Loss: 0.00207648
Iteration 25/25 | Loss: 0.00207648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207648
Iteration 2/1000 | Loss: 0.00006120
Iteration 3/1000 | Loss: 0.00003696
Iteration 4/1000 | Loss: 0.00003051
Iteration 5/1000 | Loss: 0.00002817
Iteration 6/1000 | Loss: 0.00002696
Iteration 7/1000 | Loss: 0.00002568
Iteration 8/1000 | Loss: 0.00002508
Iteration 9/1000 | Loss: 0.00020960
Iteration 10/1000 | Loss: 0.00004201
Iteration 11/1000 | Loss: 0.00003127
Iteration 12/1000 | Loss: 0.00007120
Iteration 13/1000 | Loss: 0.00002418
Iteration 14/1000 | Loss: 0.00002372
Iteration 15/1000 | Loss: 0.00002342
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002287
Iteration 18/1000 | Loss: 0.00029995
Iteration 19/1000 | Loss: 0.00008460
Iteration 20/1000 | Loss: 0.00003169
Iteration 21/1000 | Loss: 0.00002512
Iteration 22/1000 | Loss: 0.00002281
Iteration 23/1000 | Loss: 0.00002245
Iteration 24/1000 | Loss: 0.00021517
Iteration 25/1000 | Loss: 0.00013299
Iteration 26/1000 | Loss: 0.00026694
Iteration 27/1000 | Loss: 0.00018303
Iteration 28/1000 | Loss: 0.00026195
Iteration 29/1000 | Loss: 0.00015351
Iteration 30/1000 | Loss: 0.00006866
Iteration 31/1000 | Loss: 0.00002767
Iteration 32/1000 | Loss: 0.00002359
Iteration 33/1000 | Loss: 0.00002259
Iteration 34/1000 | Loss: 0.00002238
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002225
Iteration 37/1000 | Loss: 0.00002225
Iteration 38/1000 | Loss: 0.00002223
Iteration 39/1000 | Loss: 0.00002223
Iteration 40/1000 | Loss: 0.00002222
Iteration 41/1000 | Loss: 0.00002221
Iteration 42/1000 | Loss: 0.00002221
Iteration 43/1000 | Loss: 0.00002218
Iteration 44/1000 | Loss: 0.00002215
Iteration 45/1000 | Loss: 0.00002215
Iteration 46/1000 | Loss: 0.00002214
Iteration 47/1000 | Loss: 0.00002214
Iteration 48/1000 | Loss: 0.00002214
Iteration 49/1000 | Loss: 0.00002213
Iteration 50/1000 | Loss: 0.00002213
Iteration 51/1000 | Loss: 0.00002212
Iteration 52/1000 | Loss: 0.00002212
Iteration 53/1000 | Loss: 0.00002211
Iteration 54/1000 | Loss: 0.00002211
Iteration 55/1000 | Loss: 0.00002211
Iteration 56/1000 | Loss: 0.00002211
Iteration 57/1000 | Loss: 0.00002210
Iteration 58/1000 | Loss: 0.00002210
Iteration 59/1000 | Loss: 0.00002210
Iteration 60/1000 | Loss: 0.00002210
Iteration 61/1000 | Loss: 0.00002210
Iteration 62/1000 | Loss: 0.00002210
Iteration 63/1000 | Loss: 0.00002210
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002210
Iteration 66/1000 | Loss: 0.00002209
Iteration 67/1000 | Loss: 0.00002209
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002197
Iteration 73/1000 | Loss: 0.00002197
Iteration 74/1000 | Loss: 0.00002196
Iteration 75/1000 | Loss: 0.00002196
Iteration 76/1000 | Loss: 0.00002195
Iteration 77/1000 | Loss: 0.00002195
Iteration 78/1000 | Loss: 0.00002195
Iteration 79/1000 | Loss: 0.00002194
Iteration 80/1000 | Loss: 0.00002194
Iteration 81/1000 | Loss: 0.00002194
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002194
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002194
Iteration 86/1000 | Loss: 0.00002194
Iteration 87/1000 | Loss: 0.00002194
Iteration 88/1000 | Loss: 0.00002194
Iteration 89/1000 | Loss: 0.00002193
Iteration 90/1000 | Loss: 0.00002193
Iteration 91/1000 | Loss: 0.00002193
Iteration 92/1000 | Loss: 0.00002192
Iteration 93/1000 | Loss: 0.00002192
Iteration 94/1000 | Loss: 0.00002191
Iteration 95/1000 | Loss: 0.00002190
Iteration 96/1000 | Loss: 0.00002188
Iteration 97/1000 | Loss: 0.00002188
Iteration 98/1000 | Loss: 0.00002187
Iteration 99/1000 | Loss: 0.00002187
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002186
Iteration 102/1000 | Loss: 0.00002186
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002184
Iteration 105/1000 | Loss: 0.00002183
Iteration 106/1000 | Loss: 0.00002183
Iteration 107/1000 | Loss: 0.00002182
Iteration 108/1000 | Loss: 0.00002182
Iteration 109/1000 | Loss: 0.00002181
Iteration 110/1000 | Loss: 0.00002181
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002181
Iteration 113/1000 | Loss: 0.00002181
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002178
Iteration 117/1000 | Loss: 0.00002178
Iteration 118/1000 | Loss: 0.00002178
Iteration 119/1000 | Loss: 0.00002178
Iteration 120/1000 | Loss: 0.00002177
Iteration 121/1000 | Loss: 0.00002177
Iteration 122/1000 | Loss: 0.00002177
Iteration 123/1000 | Loss: 0.00002177
Iteration 124/1000 | Loss: 0.00002176
Iteration 125/1000 | Loss: 0.00002176
Iteration 126/1000 | Loss: 0.00002176
Iteration 127/1000 | Loss: 0.00002176
Iteration 128/1000 | Loss: 0.00002176
Iteration 129/1000 | Loss: 0.00002175
Iteration 130/1000 | Loss: 0.00002175
Iteration 131/1000 | Loss: 0.00002175
Iteration 132/1000 | Loss: 0.00002175
Iteration 133/1000 | Loss: 0.00002175
Iteration 134/1000 | Loss: 0.00002174
Iteration 135/1000 | Loss: 0.00002174
Iteration 136/1000 | Loss: 0.00002174
Iteration 137/1000 | Loss: 0.00002173
Iteration 138/1000 | Loss: 0.00002173
Iteration 139/1000 | Loss: 0.00002173
Iteration 140/1000 | Loss: 0.00002172
Iteration 141/1000 | Loss: 0.00002172
Iteration 142/1000 | Loss: 0.00002172
Iteration 143/1000 | Loss: 0.00002171
Iteration 144/1000 | Loss: 0.00002171
Iteration 145/1000 | Loss: 0.00002170
Iteration 146/1000 | Loss: 0.00002170
Iteration 147/1000 | Loss: 0.00002170
Iteration 148/1000 | Loss: 0.00002170
Iteration 149/1000 | Loss: 0.00002170
Iteration 150/1000 | Loss: 0.00002170
Iteration 151/1000 | Loss: 0.00002170
Iteration 152/1000 | Loss: 0.00002169
Iteration 153/1000 | Loss: 0.00002169
Iteration 154/1000 | Loss: 0.00002169
Iteration 155/1000 | Loss: 0.00002169
Iteration 156/1000 | Loss: 0.00002169
Iteration 157/1000 | Loss: 0.00002168
Iteration 158/1000 | Loss: 0.00002168
Iteration 159/1000 | Loss: 0.00002168
Iteration 160/1000 | Loss: 0.00002168
Iteration 161/1000 | Loss: 0.00002168
Iteration 162/1000 | Loss: 0.00002168
Iteration 163/1000 | Loss: 0.00002168
Iteration 164/1000 | Loss: 0.00002167
Iteration 165/1000 | Loss: 0.00002167
Iteration 166/1000 | Loss: 0.00002167
Iteration 167/1000 | Loss: 0.00002167
Iteration 168/1000 | Loss: 0.00002167
Iteration 169/1000 | Loss: 0.00002167
Iteration 170/1000 | Loss: 0.00002167
Iteration 171/1000 | Loss: 0.00002167
Iteration 172/1000 | Loss: 0.00002167
Iteration 173/1000 | Loss: 0.00002167
Iteration 174/1000 | Loss: 0.00002167
Iteration 175/1000 | Loss: 0.00002167
Iteration 176/1000 | Loss: 0.00002167
Iteration 177/1000 | Loss: 0.00002166
Iteration 178/1000 | Loss: 0.00002166
Iteration 179/1000 | Loss: 0.00002166
Iteration 180/1000 | Loss: 0.00002166
Iteration 181/1000 | Loss: 0.00002166
Iteration 182/1000 | Loss: 0.00002166
Iteration 183/1000 | Loss: 0.00002165
Iteration 184/1000 | Loss: 0.00002165
Iteration 185/1000 | Loss: 0.00002165
Iteration 186/1000 | Loss: 0.00002165
Iteration 187/1000 | Loss: 0.00002165
Iteration 188/1000 | Loss: 0.00002165
Iteration 189/1000 | Loss: 0.00002165
Iteration 190/1000 | Loss: 0.00002164
Iteration 191/1000 | Loss: 0.00002164
Iteration 192/1000 | Loss: 0.00002164
Iteration 193/1000 | Loss: 0.00002164
Iteration 194/1000 | Loss: 0.00002163
Iteration 195/1000 | Loss: 0.00002163
Iteration 196/1000 | Loss: 0.00002163
Iteration 197/1000 | Loss: 0.00002163
Iteration 198/1000 | Loss: 0.00002163
Iteration 199/1000 | Loss: 0.00002163
Iteration 200/1000 | Loss: 0.00002163
Iteration 201/1000 | Loss: 0.00002163
Iteration 202/1000 | Loss: 0.00002163
Iteration 203/1000 | Loss: 0.00002163
Iteration 204/1000 | Loss: 0.00002162
Iteration 205/1000 | Loss: 0.00002162
Iteration 206/1000 | Loss: 0.00002162
Iteration 207/1000 | Loss: 0.00002162
Iteration 208/1000 | Loss: 0.00002162
Iteration 209/1000 | Loss: 0.00002162
Iteration 210/1000 | Loss: 0.00002161
Iteration 211/1000 | Loss: 0.00002161
Iteration 212/1000 | Loss: 0.00002161
Iteration 213/1000 | Loss: 0.00002160
Iteration 214/1000 | Loss: 0.00002160
Iteration 215/1000 | Loss: 0.00002160
Iteration 216/1000 | Loss: 0.00002160
Iteration 217/1000 | Loss: 0.00002160
Iteration 218/1000 | Loss: 0.00002160
Iteration 219/1000 | Loss: 0.00002159
Iteration 220/1000 | Loss: 0.00002159
Iteration 221/1000 | Loss: 0.00002159
Iteration 222/1000 | Loss: 0.00002159
Iteration 223/1000 | Loss: 0.00002159
Iteration 224/1000 | Loss: 0.00002159
Iteration 225/1000 | Loss: 0.00002158
Iteration 226/1000 | Loss: 0.00002158
Iteration 227/1000 | Loss: 0.00002158
Iteration 228/1000 | Loss: 0.00002157
Iteration 229/1000 | Loss: 0.00002157
Iteration 230/1000 | Loss: 0.00002157
Iteration 231/1000 | Loss: 0.00002157
Iteration 232/1000 | Loss: 0.00002156
Iteration 233/1000 | Loss: 0.00002156
Iteration 234/1000 | Loss: 0.00002156
Iteration 235/1000 | Loss: 0.00002156
Iteration 236/1000 | Loss: 0.00002155
Iteration 237/1000 | Loss: 0.00002155
Iteration 238/1000 | Loss: 0.00002155
Iteration 239/1000 | Loss: 0.00002155
Iteration 240/1000 | Loss: 0.00002155
Iteration 241/1000 | Loss: 0.00002154
Iteration 242/1000 | Loss: 0.00002154
Iteration 243/1000 | Loss: 0.00002154
Iteration 244/1000 | Loss: 0.00002154
Iteration 245/1000 | Loss: 0.00002153
Iteration 246/1000 | Loss: 0.00002153
Iteration 247/1000 | Loss: 0.00002153
Iteration 248/1000 | Loss: 0.00002153
Iteration 249/1000 | Loss: 0.00002153
Iteration 250/1000 | Loss: 0.00002153
Iteration 251/1000 | Loss: 0.00002153
Iteration 252/1000 | Loss: 0.00002153
Iteration 253/1000 | Loss: 0.00002153
Iteration 254/1000 | Loss: 0.00002152
Iteration 255/1000 | Loss: 0.00002152
Iteration 256/1000 | Loss: 0.00002152
Iteration 257/1000 | Loss: 0.00002152
Iteration 258/1000 | Loss: 0.00002152
Iteration 259/1000 | Loss: 0.00002151
Iteration 260/1000 | Loss: 0.00002151
Iteration 261/1000 | Loss: 0.00002151
Iteration 262/1000 | Loss: 0.00002151
Iteration 263/1000 | Loss: 0.00002151
Iteration 264/1000 | Loss: 0.00002151
Iteration 265/1000 | Loss: 0.00002151
Iteration 266/1000 | Loss: 0.00002150
Iteration 267/1000 | Loss: 0.00002150
Iteration 268/1000 | Loss: 0.00002150
Iteration 269/1000 | Loss: 0.00002149
Iteration 270/1000 | Loss: 0.00002149
Iteration 271/1000 | Loss: 0.00002148
Iteration 272/1000 | Loss: 0.00002148
Iteration 273/1000 | Loss: 0.00002148
Iteration 274/1000 | Loss: 0.00002148
Iteration 275/1000 | Loss: 0.00002147
Iteration 276/1000 | Loss: 0.00002147
Iteration 277/1000 | Loss: 0.00002147
Iteration 278/1000 | Loss: 0.00002147
Iteration 279/1000 | Loss: 0.00002147
Iteration 280/1000 | Loss: 0.00002147
Iteration 281/1000 | Loss: 0.00002146
Iteration 282/1000 | Loss: 0.00002146
Iteration 283/1000 | Loss: 0.00002146
Iteration 284/1000 | Loss: 0.00002146
Iteration 285/1000 | Loss: 0.00002146
Iteration 286/1000 | Loss: 0.00002145
Iteration 287/1000 | Loss: 0.00002145
Iteration 288/1000 | Loss: 0.00002145
Iteration 289/1000 | Loss: 0.00002145
Iteration 290/1000 | Loss: 0.00002145
Iteration 291/1000 | Loss: 0.00002145
Iteration 292/1000 | Loss: 0.00002145
Iteration 293/1000 | Loss: 0.00002145
Iteration 294/1000 | Loss: 0.00002145
Iteration 295/1000 | Loss: 0.00002145
Iteration 296/1000 | Loss: 0.00002145
Iteration 297/1000 | Loss: 0.00002145
Iteration 298/1000 | Loss: 0.00002145
Iteration 299/1000 | Loss: 0.00002144
Iteration 300/1000 | Loss: 0.00002144
Iteration 301/1000 | Loss: 0.00002144
Iteration 302/1000 | Loss: 0.00002144
Iteration 303/1000 | Loss: 0.00002144
Iteration 304/1000 | Loss: 0.00002144
Iteration 305/1000 | Loss: 0.00002144
Iteration 306/1000 | Loss: 0.00002144
Iteration 307/1000 | Loss: 0.00002144
Iteration 308/1000 | Loss: 0.00002144
Iteration 309/1000 | Loss: 0.00002144
Iteration 310/1000 | Loss: 0.00002144
Iteration 311/1000 | Loss: 0.00002144
Iteration 312/1000 | Loss: 0.00002144
Iteration 313/1000 | Loss: 0.00002144
Iteration 314/1000 | Loss: 0.00002144
Iteration 315/1000 | Loss: 0.00002144
Iteration 316/1000 | Loss: 0.00002144
Iteration 317/1000 | Loss: 0.00002144
Iteration 318/1000 | Loss: 0.00002144
Iteration 319/1000 | Loss: 0.00002144
Iteration 320/1000 | Loss: 0.00002144
Iteration 321/1000 | Loss: 0.00002144
Iteration 322/1000 | Loss: 0.00002144
Iteration 323/1000 | Loss: 0.00002144
Iteration 324/1000 | Loss: 0.00002144
Iteration 325/1000 | Loss: 0.00002144
Iteration 326/1000 | Loss: 0.00002144
Iteration 327/1000 | Loss: 0.00002144
Iteration 328/1000 | Loss: 0.00002144
Iteration 329/1000 | Loss: 0.00002144
Iteration 330/1000 | Loss: 0.00002144
Iteration 331/1000 | Loss: 0.00002144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [2.1435396774904802e-05, 2.1435396774904802e-05, 2.1435396774904802e-05, 2.1435396774904802e-05, 2.1435396774904802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1435396774904802e-05

Optimization complete. Final v2v error: 3.8516647815704346 mm

Highest mean error: 4.789094924926758 mm for frame 59

Lowest mean error: 3.156196355819702 mm for frame 81

Saving results

Total time: 104.62926602363586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691003
Iteration 2/25 | Loss: 0.00158134
Iteration 3/25 | Loss: 0.00129764
Iteration 4/25 | Loss: 0.00127413
Iteration 5/25 | Loss: 0.00127880
Iteration 6/25 | Loss: 0.00127897
Iteration 7/25 | Loss: 0.00124979
Iteration 8/25 | Loss: 0.00124090
Iteration 9/25 | Loss: 0.00123989
Iteration 10/25 | Loss: 0.00125319
Iteration 11/25 | Loss: 0.00123892
Iteration 12/25 | Loss: 0.00123502
Iteration 13/25 | Loss: 0.00123444
Iteration 14/25 | Loss: 0.00123442
Iteration 15/25 | Loss: 0.00123442
Iteration 16/25 | Loss: 0.00123442
Iteration 17/25 | Loss: 0.00123442
Iteration 18/25 | Loss: 0.00123442
Iteration 19/25 | Loss: 0.00123441
Iteration 20/25 | Loss: 0.00123441
Iteration 21/25 | Loss: 0.00123441
Iteration 22/25 | Loss: 0.00123441
Iteration 23/25 | Loss: 0.00123441
Iteration 24/25 | Loss: 0.00123441
Iteration 25/25 | Loss: 0.00123441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.57010078
Iteration 2/25 | Loss: 0.00095612
Iteration 3/25 | Loss: 0.00095612
Iteration 4/25 | Loss: 0.00095612
Iteration 5/25 | Loss: 0.00095612
Iteration 6/25 | Loss: 0.00095612
Iteration 7/25 | Loss: 0.00095612
Iteration 8/25 | Loss: 0.00095611
Iteration 9/25 | Loss: 0.00095611
Iteration 10/25 | Loss: 0.00095611
Iteration 11/25 | Loss: 0.00095611
Iteration 12/25 | Loss: 0.00095611
Iteration 13/25 | Loss: 0.00095611
Iteration 14/25 | Loss: 0.00095611
Iteration 15/25 | Loss: 0.00095611
Iteration 16/25 | Loss: 0.00095611
Iteration 17/25 | Loss: 0.00095611
Iteration 18/25 | Loss: 0.00095611
Iteration 19/25 | Loss: 0.00095611
Iteration 20/25 | Loss: 0.00095611
Iteration 21/25 | Loss: 0.00095611
Iteration 22/25 | Loss: 0.00095611
Iteration 23/25 | Loss: 0.00095611
Iteration 24/25 | Loss: 0.00095611
Iteration 25/25 | Loss: 0.00095611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095611
Iteration 2/1000 | Loss: 0.00002898
Iteration 3/1000 | Loss: 0.00001833
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001547
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001444
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001357
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001321
Iteration 20/1000 | Loss: 0.00001319
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001309
Iteration 23/1000 | Loss: 0.00001309
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001298
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001297
Iteration 46/1000 | Loss: 0.00001297
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001297
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001296
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001288
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001288
Iteration 78/1000 | Loss: 0.00001288
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001285
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001279
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001276
Iteration 113/1000 | Loss: 0.00001276
Iteration 114/1000 | Loss: 0.00001276
Iteration 115/1000 | Loss: 0.00001276
Iteration 116/1000 | Loss: 0.00001275
Iteration 117/1000 | Loss: 0.00001275
Iteration 118/1000 | Loss: 0.00001275
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001274
Iteration 122/1000 | Loss: 0.00001274
Iteration 123/1000 | Loss: 0.00001274
Iteration 124/1000 | Loss: 0.00001273
Iteration 125/1000 | Loss: 0.00001273
Iteration 126/1000 | Loss: 0.00001273
Iteration 127/1000 | Loss: 0.00001273
Iteration 128/1000 | Loss: 0.00001273
Iteration 129/1000 | Loss: 0.00001273
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001273
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001273
Iteration 141/1000 | Loss: 0.00001273
Iteration 142/1000 | Loss: 0.00001273
Iteration 143/1000 | Loss: 0.00001273
Iteration 144/1000 | Loss: 0.00001273
Iteration 145/1000 | Loss: 0.00001273
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001273
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2728791261906736e-05, 1.2728791261906736e-05, 1.2728791261906736e-05, 1.2728791261906736e-05, 1.2728791261906736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2728791261906736e-05

Optimization complete. Final v2v error: 3.0311472415924072 mm

Highest mean error: 3.8629372119903564 mm for frame 3

Lowest mean error: 2.762998104095459 mm for frame 94

Saving results

Total time: 50.630982637405396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370447
Iteration 2/25 | Loss: 0.00124484
Iteration 3/25 | Loss: 0.00115769
Iteration 4/25 | Loss: 0.00114660
Iteration 5/25 | Loss: 0.00114345
Iteration 6/25 | Loss: 0.00114284
Iteration 7/25 | Loss: 0.00114284
Iteration 8/25 | Loss: 0.00114284
Iteration 9/25 | Loss: 0.00114284
Iteration 10/25 | Loss: 0.00114284
Iteration 11/25 | Loss: 0.00114284
Iteration 12/25 | Loss: 0.00114284
Iteration 13/25 | Loss: 0.00114284
Iteration 14/25 | Loss: 0.00114284
Iteration 15/25 | Loss: 0.00114284
Iteration 16/25 | Loss: 0.00114284
Iteration 17/25 | Loss: 0.00114284
Iteration 18/25 | Loss: 0.00114284
Iteration 19/25 | Loss: 0.00114284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011428439756855369, 0.0011428439756855369, 0.0011428439756855369, 0.0011428439756855369, 0.0011428439756855369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011428439756855369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38963819
Iteration 2/25 | Loss: 0.00135984
Iteration 3/25 | Loss: 0.00135984
Iteration 4/25 | Loss: 0.00135984
Iteration 5/25 | Loss: 0.00135984
Iteration 6/25 | Loss: 0.00135984
Iteration 7/25 | Loss: 0.00135984
Iteration 8/25 | Loss: 0.00135984
Iteration 9/25 | Loss: 0.00135984
Iteration 10/25 | Loss: 0.00135984
Iteration 11/25 | Loss: 0.00135984
Iteration 12/25 | Loss: 0.00135984
Iteration 13/25 | Loss: 0.00135984
Iteration 14/25 | Loss: 0.00135984
Iteration 15/25 | Loss: 0.00135984
Iteration 16/25 | Loss: 0.00135984
Iteration 17/25 | Loss: 0.00135984
Iteration 18/25 | Loss: 0.00135984
Iteration 19/25 | Loss: 0.00135984
Iteration 20/25 | Loss: 0.00135984
Iteration 21/25 | Loss: 0.00135984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013598400400951505, 0.0013598400400951505, 0.0013598400400951505, 0.0013598400400951505, 0.0013598400400951505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013598400400951505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135984
Iteration 2/1000 | Loss: 0.00002411
Iteration 3/1000 | Loss: 0.00001492
Iteration 4/1000 | Loss: 0.00001223
Iteration 5/1000 | Loss: 0.00001086
Iteration 6/1000 | Loss: 0.00001023
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000945
Iteration 9/1000 | Loss: 0.00000926
Iteration 10/1000 | Loss: 0.00000901
Iteration 11/1000 | Loss: 0.00000884
Iteration 12/1000 | Loss: 0.00000882
Iteration 13/1000 | Loss: 0.00000879
Iteration 14/1000 | Loss: 0.00000877
Iteration 15/1000 | Loss: 0.00000873
Iteration 16/1000 | Loss: 0.00000869
Iteration 17/1000 | Loss: 0.00000869
Iteration 18/1000 | Loss: 0.00000867
Iteration 19/1000 | Loss: 0.00000862
Iteration 20/1000 | Loss: 0.00000862
Iteration 21/1000 | Loss: 0.00000862
Iteration 22/1000 | Loss: 0.00000861
Iteration 23/1000 | Loss: 0.00000861
Iteration 24/1000 | Loss: 0.00000861
Iteration 25/1000 | Loss: 0.00000861
Iteration 26/1000 | Loss: 0.00000860
Iteration 27/1000 | Loss: 0.00000860
Iteration 28/1000 | Loss: 0.00000858
Iteration 29/1000 | Loss: 0.00000857
Iteration 30/1000 | Loss: 0.00000856
Iteration 31/1000 | Loss: 0.00000855
Iteration 32/1000 | Loss: 0.00000854
Iteration 33/1000 | Loss: 0.00000850
Iteration 34/1000 | Loss: 0.00000847
Iteration 35/1000 | Loss: 0.00000846
Iteration 36/1000 | Loss: 0.00000846
Iteration 37/1000 | Loss: 0.00000841
Iteration 38/1000 | Loss: 0.00000838
Iteration 39/1000 | Loss: 0.00000838
Iteration 40/1000 | Loss: 0.00000837
Iteration 41/1000 | Loss: 0.00000837
Iteration 42/1000 | Loss: 0.00000836
Iteration 43/1000 | Loss: 0.00000836
Iteration 44/1000 | Loss: 0.00000835
Iteration 45/1000 | Loss: 0.00000835
Iteration 46/1000 | Loss: 0.00000831
Iteration 47/1000 | Loss: 0.00000830
Iteration 48/1000 | Loss: 0.00000830
Iteration 49/1000 | Loss: 0.00000830
Iteration 50/1000 | Loss: 0.00000830
Iteration 51/1000 | Loss: 0.00000829
Iteration 52/1000 | Loss: 0.00000829
Iteration 53/1000 | Loss: 0.00000829
Iteration 54/1000 | Loss: 0.00000828
Iteration 55/1000 | Loss: 0.00000828
Iteration 56/1000 | Loss: 0.00000828
Iteration 57/1000 | Loss: 0.00000828
Iteration 58/1000 | Loss: 0.00000828
Iteration 59/1000 | Loss: 0.00000827
Iteration 60/1000 | Loss: 0.00000827
Iteration 61/1000 | Loss: 0.00000827
Iteration 62/1000 | Loss: 0.00000827
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000826
Iteration 65/1000 | Loss: 0.00000826
Iteration 66/1000 | Loss: 0.00000826
Iteration 67/1000 | Loss: 0.00000826
Iteration 68/1000 | Loss: 0.00000826
Iteration 69/1000 | Loss: 0.00000826
Iteration 70/1000 | Loss: 0.00000825
Iteration 71/1000 | Loss: 0.00000825
Iteration 72/1000 | Loss: 0.00000825
Iteration 73/1000 | Loss: 0.00000825
Iteration 74/1000 | Loss: 0.00000825
Iteration 75/1000 | Loss: 0.00000825
Iteration 76/1000 | Loss: 0.00000825
Iteration 77/1000 | Loss: 0.00000824
Iteration 78/1000 | Loss: 0.00000824
Iteration 79/1000 | Loss: 0.00000824
Iteration 80/1000 | Loss: 0.00000824
Iteration 81/1000 | Loss: 0.00000824
Iteration 82/1000 | Loss: 0.00000823
Iteration 83/1000 | Loss: 0.00000823
Iteration 84/1000 | Loss: 0.00000823
Iteration 85/1000 | Loss: 0.00000823
Iteration 86/1000 | Loss: 0.00000823
Iteration 87/1000 | Loss: 0.00000822
Iteration 88/1000 | Loss: 0.00000822
Iteration 89/1000 | Loss: 0.00000822
Iteration 90/1000 | Loss: 0.00000822
Iteration 91/1000 | Loss: 0.00000822
Iteration 92/1000 | Loss: 0.00000822
Iteration 93/1000 | Loss: 0.00000821
Iteration 94/1000 | Loss: 0.00000821
Iteration 95/1000 | Loss: 0.00000821
Iteration 96/1000 | Loss: 0.00000821
Iteration 97/1000 | Loss: 0.00000821
Iteration 98/1000 | Loss: 0.00000821
Iteration 99/1000 | Loss: 0.00000821
Iteration 100/1000 | Loss: 0.00000820
Iteration 101/1000 | Loss: 0.00000820
Iteration 102/1000 | Loss: 0.00000820
Iteration 103/1000 | Loss: 0.00000820
Iteration 104/1000 | Loss: 0.00000820
Iteration 105/1000 | Loss: 0.00000820
Iteration 106/1000 | Loss: 0.00000820
Iteration 107/1000 | Loss: 0.00000820
Iteration 108/1000 | Loss: 0.00000820
Iteration 109/1000 | Loss: 0.00000820
Iteration 110/1000 | Loss: 0.00000820
Iteration 111/1000 | Loss: 0.00000819
Iteration 112/1000 | Loss: 0.00000819
Iteration 113/1000 | Loss: 0.00000819
Iteration 114/1000 | Loss: 0.00000819
Iteration 115/1000 | Loss: 0.00000819
Iteration 116/1000 | Loss: 0.00000819
Iteration 117/1000 | Loss: 0.00000819
Iteration 118/1000 | Loss: 0.00000819
Iteration 119/1000 | Loss: 0.00000819
Iteration 120/1000 | Loss: 0.00000819
Iteration 121/1000 | Loss: 0.00000818
Iteration 122/1000 | Loss: 0.00000818
Iteration 123/1000 | Loss: 0.00000818
Iteration 124/1000 | Loss: 0.00000817
Iteration 125/1000 | Loss: 0.00000817
Iteration 126/1000 | Loss: 0.00000817
Iteration 127/1000 | Loss: 0.00000817
Iteration 128/1000 | Loss: 0.00000817
Iteration 129/1000 | Loss: 0.00000817
Iteration 130/1000 | Loss: 0.00000817
Iteration 131/1000 | Loss: 0.00000817
Iteration 132/1000 | Loss: 0.00000817
Iteration 133/1000 | Loss: 0.00000817
Iteration 134/1000 | Loss: 0.00000817
Iteration 135/1000 | Loss: 0.00000817
Iteration 136/1000 | Loss: 0.00000817
Iteration 137/1000 | Loss: 0.00000817
Iteration 138/1000 | Loss: 0.00000816
Iteration 139/1000 | Loss: 0.00000816
Iteration 140/1000 | Loss: 0.00000816
Iteration 141/1000 | Loss: 0.00000816
Iteration 142/1000 | Loss: 0.00000816
Iteration 143/1000 | Loss: 0.00000816
Iteration 144/1000 | Loss: 0.00000816
Iteration 145/1000 | Loss: 0.00000815
Iteration 146/1000 | Loss: 0.00000815
Iteration 147/1000 | Loss: 0.00000815
Iteration 148/1000 | Loss: 0.00000815
Iteration 149/1000 | Loss: 0.00000815
Iteration 150/1000 | Loss: 0.00000815
Iteration 151/1000 | Loss: 0.00000815
Iteration 152/1000 | Loss: 0.00000815
Iteration 153/1000 | Loss: 0.00000814
Iteration 154/1000 | Loss: 0.00000814
Iteration 155/1000 | Loss: 0.00000814
Iteration 156/1000 | Loss: 0.00000814
Iteration 157/1000 | Loss: 0.00000814
Iteration 158/1000 | Loss: 0.00000814
Iteration 159/1000 | Loss: 0.00000814
Iteration 160/1000 | Loss: 0.00000814
Iteration 161/1000 | Loss: 0.00000814
Iteration 162/1000 | Loss: 0.00000814
Iteration 163/1000 | Loss: 0.00000814
Iteration 164/1000 | Loss: 0.00000814
Iteration 165/1000 | Loss: 0.00000814
Iteration 166/1000 | Loss: 0.00000814
Iteration 167/1000 | Loss: 0.00000813
Iteration 168/1000 | Loss: 0.00000813
Iteration 169/1000 | Loss: 0.00000813
Iteration 170/1000 | Loss: 0.00000813
Iteration 171/1000 | Loss: 0.00000813
Iteration 172/1000 | Loss: 0.00000813
Iteration 173/1000 | Loss: 0.00000813
Iteration 174/1000 | Loss: 0.00000813
Iteration 175/1000 | Loss: 0.00000813
Iteration 176/1000 | Loss: 0.00000813
Iteration 177/1000 | Loss: 0.00000813
Iteration 178/1000 | Loss: 0.00000813
Iteration 179/1000 | Loss: 0.00000813
Iteration 180/1000 | Loss: 0.00000813
Iteration 181/1000 | Loss: 0.00000813
Iteration 182/1000 | Loss: 0.00000813
Iteration 183/1000 | Loss: 0.00000812
Iteration 184/1000 | Loss: 0.00000812
Iteration 185/1000 | Loss: 0.00000812
Iteration 186/1000 | Loss: 0.00000812
Iteration 187/1000 | Loss: 0.00000812
Iteration 188/1000 | Loss: 0.00000812
Iteration 189/1000 | Loss: 0.00000812
Iteration 190/1000 | Loss: 0.00000812
Iteration 191/1000 | Loss: 0.00000812
Iteration 192/1000 | Loss: 0.00000812
Iteration 193/1000 | Loss: 0.00000812
Iteration 194/1000 | Loss: 0.00000812
Iteration 195/1000 | Loss: 0.00000812
Iteration 196/1000 | Loss: 0.00000812
Iteration 197/1000 | Loss: 0.00000812
Iteration 198/1000 | Loss: 0.00000812
Iteration 199/1000 | Loss: 0.00000812
Iteration 200/1000 | Loss: 0.00000812
Iteration 201/1000 | Loss: 0.00000811
Iteration 202/1000 | Loss: 0.00000811
Iteration 203/1000 | Loss: 0.00000811
Iteration 204/1000 | Loss: 0.00000811
Iteration 205/1000 | Loss: 0.00000811
Iteration 206/1000 | Loss: 0.00000811
Iteration 207/1000 | Loss: 0.00000811
Iteration 208/1000 | Loss: 0.00000810
Iteration 209/1000 | Loss: 0.00000810
Iteration 210/1000 | Loss: 0.00000810
Iteration 211/1000 | Loss: 0.00000810
Iteration 212/1000 | Loss: 0.00000810
Iteration 213/1000 | Loss: 0.00000810
Iteration 214/1000 | Loss: 0.00000810
Iteration 215/1000 | Loss: 0.00000810
Iteration 216/1000 | Loss: 0.00000809
Iteration 217/1000 | Loss: 0.00000809
Iteration 218/1000 | Loss: 0.00000809
Iteration 219/1000 | Loss: 0.00000809
Iteration 220/1000 | Loss: 0.00000809
Iteration 221/1000 | Loss: 0.00000809
Iteration 222/1000 | Loss: 0.00000809
Iteration 223/1000 | Loss: 0.00000809
Iteration 224/1000 | Loss: 0.00000809
Iteration 225/1000 | Loss: 0.00000809
Iteration 226/1000 | Loss: 0.00000809
Iteration 227/1000 | Loss: 0.00000809
Iteration 228/1000 | Loss: 0.00000809
Iteration 229/1000 | Loss: 0.00000809
Iteration 230/1000 | Loss: 0.00000809
Iteration 231/1000 | Loss: 0.00000809
Iteration 232/1000 | Loss: 0.00000809
Iteration 233/1000 | Loss: 0.00000809
Iteration 234/1000 | Loss: 0.00000809
Iteration 235/1000 | Loss: 0.00000809
Iteration 236/1000 | Loss: 0.00000808
Iteration 237/1000 | Loss: 0.00000808
Iteration 238/1000 | Loss: 0.00000808
Iteration 239/1000 | Loss: 0.00000808
Iteration 240/1000 | Loss: 0.00000807
Iteration 241/1000 | Loss: 0.00000807
Iteration 242/1000 | Loss: 0.00000807
Iteration 243/1000 | Loss: 0.00000807
Iteration 244/1000 | Loss: 0.00000807
Iteration 245/1000 | Loss: 0.00000807
Iteration 246/1000 | Loss: 0.00000807
Iteration 247/1000 | Loss: 0.00000807
Iteration 248/1000 | Loss: 0.00000807
Iteration 249/1000 | Loss: 0.00000807
Iteration 250/1000 | Loss: 0.00000807
Iteration 251/1000 | Loss: 0.00000807
Iteration 252/1000 | Loss: 0.00000807
Iteration 253/1000 | Loss: 0.00000807
Iteration 254/1000 | Loss: 0.00000807
Iteration 255/1000 | Loss: 0.00000807
Iteration 256/1000 | Loss: 0.00000807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [8.065168913162779e-06, 8.065168913162779e-06, 8.065168913162779e-06, 8.065168913162779e-06, 8.065168913162779e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.065168913162779e-06

Optimization complete. Final v2v error: 2.443916082382202 mm

Highest mean error: 2.941680431365967 mm for frame 98

Lowest mean error: 2.328728675842285 mm for frame 45

Saving results

Total time: 45.60383176803589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861439
Iteration 2/25 | Loss: 0.00158089
Iteration 3/25 | Loss: 0.00124275
Iteration 4/25 | Loss: 0.00121847
Iteration 5/25 | Loss: 0.00121415
Iteration 6/25 | Loss: 0.00121399
Iteration 7/25 | Loss: 0.00121399
Iteration 8/25 | Loss: 0.00121399
Iteration 9/25 | Loss: 0.00121399
Iteration 10/25 | Loss: 0.00121399
Iteration 11/25 | Loss: 0.00121399
Iteration 12/25 | Loss: 0.00121399
Iteration 13/25 | Loss: 0.00121399
Iteration 14/25 | Loss: 0.00121399
Iteration 15/25 | Loss: 0.00121399
Iteration 16/25 | Loss: 0.00121399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012139889877289534, 0.0012139889877289534, 0.0012139889877289534, 0.0012139889877289534, 0.0012139889877289534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012139889877289534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80951285
Iteration 2/25 | Loss: 0.00104301
Iteration 3/25 | Loss: 0.00104301
Iteration 4/25 | Loss: 0.00104301
Iteration 5/25 | Loss: 0.00104301
Iteration 6/25 | Loss: 0.00104301
Iteration 7/25 | Loss: 0.00104300
Iteration 8/25 | Loss: 0.00104300
Iteration 9/25 | Loss: 0.00104300
Iteration 10/25 | Loss: 0.00104300
Iteration 11/25 | Loss: 0.00104300
Iteration 12/25 | Loss: 0.00104300
Iteration 13/25 | Loss: 0.00104300
Iteration 14/25 | Loss: 0.00104300
Iteration 15/25 | Loss: 0.00104300
Iteration 16/25 | Loss: 0.00104300
Iteration 17/25 | Loss: 0.00104300
Iteration 18/25 | Loss: 0.00104300
Iteration 19/25 | Loss: 0.00104300
Iteration 20/25 | Loss: 0.00104300
Iteration 21/25 | Loss: 0.00104300
Iteration 22/25 | Loss: 0.00104300
Iteration 23/25 | Loss: 0.00104300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001043003867380321, 0.001043003867380321, 0.001043003867380321, 0.001043003867380321, 0.001043003867380321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043003867380321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104300
Iteration 2/1000 | Loss: 0.00002440
Iteration 3/1000 | Loss: 0.00001766
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001465
Iteration 7/1000 | Loss: 0.00001437
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001325
Iteration 16/1000 | Loss: 0.00001318
Iteration 17/1000 | Loss: 0.00001316
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001285
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001284
Iteration 62/1000 | Loss: 0.00001284
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001280
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001279
Iteration 77/1000 | Loss: 0.00001279
Iteration 78/1000 | Loss: 0.00001279
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00001278
Iteration 82/1000 | Loss: 0.00001278
Iteration 83/1000 | Loss: 0.00001278
Iteration 84/1000 | Loss: 0.00001278
Iteration 85/1000 | Loss: 0.00001278
Iteration 86/1000 | Loss: 0.00001278
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001277
Iteration 89/1000 | Loss: 0.00001277
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001275
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.2748954759445041e-05, 1.2748954759445041e-05, 1.2748954759445041e-05, 1.2748954759445041e-05, 1.2748954759445041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2748954759445041e-05

Optimization complete. Final v2v error: 3.0355005264282227 mm

Highest mean error: 3.417452335357666 mm for frame 7

Lowest mean error: 2.712080240249634 mm for frame 61

Saving results

Total time: 38.568520069122314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594528
Iteration 2/25 | Loss: 0.00123506
Iteration 3/25 | Loss: 0.00116731
Iteration 4/25 | Loss: 0.00115645
Iteration 5/25 | Loss: 0.00115272
Iteration 6/25 | Loss: 0.00115191
Iteration 7/25 | Loss: 0.00115191
Iteration 8/25 | Loss: 0.00115191
Iteration 9/25 | Loss: 0.00115191
Iteration 10/25 | Loss: 0.00115191
Iteration 11/25 | Loss: 0.00115191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011519059771671891, 0.0011519059771671891, 0.0011519059771671891, 0.0011519059771671891, 0.0011519059771671891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011519059771671891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04008102
Iteration 2/25 | Loss: 0.00137566
Iteration 3/25 | Loss: 0.00137566
Iteration 4/25 | Loss: 0.00137566
Iteration 5/25 | Loss: 0.00137566
Iteration 6/25 | Loss: 0.00137566
Iteration 7/25 | Loss: 0.00137566
Iteration 8/25 | Loss: 0.00137566
Iteration 9/25 | Loss: 0.00137566
Iteration 10/25 | Loss: 0.00137566
Iteration 11/25 | Loss: 0.00137566
Iteration 12/25 | Loss: 0.00137566
Iteration 13/25 | Loss: 0.00137566
Iteration 14/25 | Loss: 0.00137566
Iteration 15/25 | Loss: 0.00137566
Iteration 16/25 | Loss: 0.00137565
Iteration 17/25 | Loss: 0.00137566
Iteration 18/25 | Loss: 0.00137566
Iteration 19/25 | Loss: 0.00137566
Iteration 20/25 | Loss: 0.00137565
Iteration 21/25 | Loss: 0.00137565
Iteration 22/25 | Loss: 0.00137565
Iteration 23/25 | Loss: 0.00137565
Iteration 24/25 | Loss: 0.00137565
Iteration 25/25 | Loss: 0.00137565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013756549451500177, 0.0013756549451500177, 0.0013756549451500177, 0.0013756549451500177, 0.0013756549451500177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013756549451500177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137565
Iteration 2/1000 | Loss: 0.00001949
Iteration 3/1000 | Loss: 0.00001419
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001204
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001109
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001035
Iteration 11/1000 | Loss: 0.00001031
Iteration 12/1000 | Loss: 0.00001027
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001015
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00000999
Iteration 20/1000 | Loss: 0.00000999
Iteration 21/1000 | Loss: 0.00000998
Iteration 22/1000 | Loss: 0.00000995
Iteration 23/1000 | Loss: 0.00000994
Iteration 24/1000 | Loss: 0.00000993
Iteration 25/1000 | Loss: 0.00000993
Iteration 26/1000 | Loss: 0.00000993
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000992
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000985
Iteration 32/1000 | Loss: 0.00000984
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000980
Iteration 38/1000 | Loss: 0.00000980
Iteration 39/1000 | Loss: 0.00000980
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000976
Iteration 46/1000 | Loss: 0.00000976
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000974
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000973
Iteration 54/1000 | Loss: 0.00000973
Iteration 55/1000 | Loss: 0.00000973
Iteration 56/1000 | Loss: 0.00000972
Iteration 57/1000 | Loss: 0.00000972
Iteration 58/1000 | Loss: 0.00000972
Iteration 59/1000 | Loss: 0.00000971
Iteration 60/1000 | Loss: 0.00000971
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000968
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000966
Iteration 70/1000 | Loss: 0.00000966
Iteration 71/1000 | Loss: 0.00000966
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000965
Iteration 74/1000 | Loss: 0.00000965
Iteration 75/1000 | Loss: 0.00000965
Iteration 76/1000 | Loss: 0.00000964
Iteration 77/1000 | Loss: 0.00000964
Iteration 78/1000 | Loss: 0.00000964
Iteration 79/1000 | Loss: 0.00000964
Iteration 80/1000 | Loss: 0.00000964
Iteration 81/1000 | Loss: 0.00000964
Iteration 82/1000 | Loss: 0.00000964
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000963
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000963
Iteration 88/1000 | Loss: 0.00000963
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000962
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000958
Iteration 112/1000 | Loss: 0.00000958
Iteration 113/1000 | Loss: 0.00000958
Iteration 114/1000 | Loss: 0.00000958
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000958
Iteration 120/1000 | Loss: 0.00000958
Iteration 121/1000 | Loss: 0.00000958
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000957
Iteration 124/1000 | Loss: 0.00000957
Iteration 125/1000 | Loss: 0.00000957
Iteration 126/1000 | Loss: 0.00000957
Iteration 127/1000 | Loss: 0.00000957
Iteration 128/1000 | Loss: 0.00000957
Iteration 129/1000 | Loss: 0.00000957
Iteration 130/1000 | Loss: 0.00000957
Iteration 131/1000 | Loss: 0.00000957
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000956
Iteration 138/1000 | Loss: 0.00000956
Iteration 139/1000 | Loss: 0.00000956
Iteration 140/1000 | Loss: 0.00000956
Iteration 141/1000 | Loss: 0.00000956
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000956
Iteration 147/1000 | Loss: 0.00000956
Iteration 148/1000 | Loss: 0.00000956
Iteration 149/1000 | Loss: 0.00000956
Iteration 150/1000 | Loss: 0.00000956
Iteration 151/1000 | Loss: 0.00000955
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000955
Iteration 155/1000 | Loss: 0.00000955
Iteration 156/1000 | Loss: 0.00000955
Iteration 157/1000 | Loss: 0.00000955
Iteration 158/1000 | Loss: 0.00000955
Iteration 159/1000 | Loss: 0.00000955
Iteration 160/1000 | Loss: 0.00000955
Iteration 161/1000 | Loss: 0.00000955
Iteration 162/1000 | Loss: 0.00000955
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000955
Iteration 168/1000 | Loss: 0.00000955
Iteration 169/1000 | Loss: 0.00000955
Iteration 170/1000 | Loss: 0.00000955
Iteration 171/1000 | Loss: 0.00000955
Iteration 172/1000 | Loss: 0.00000955
Iteration 173/1000 | Loss: 0.00000955
Iteration 174/1000 | Loss: 0.00000955
Iteration 175/1000 | Loss: 0.00000955
Iteration 176/1000 | Loss: 0.00000955
Iteration 177/1000 | Loss: 0.00000955
Iteration 178/1000 | Loss: 0.00000955
Iteration 179/1000 | Loss: 0.00000955
Iteration 180/1000 | Loss: 0.00000955
Iteration 181/1000 | Loss: 0.00000955
Iteration 182/1000 | Loss: 0.00000955
Iteration 183/1000 | Loss: 0.00000955
Iteration 184/1000 | Loss: 0.00000955
Iteration 185/1000 | Loss: 0.00000955
Iteration 186/1000 | Loss: 0.00000955
Iteration 187/1000 | Loss: 0.00000955
Iteration 188/1000 | Loss: 0.00000955
Iteration 189/1000 | Loss: 0.00000955
Iteration 190/1000 | Loss: 0.00000955
Iteration 191/1000 | Loss: 0.00000955
Iteration 192/1000 | Loss: 0.00000955
Iteration 193/1000 | Loss: 0.00000955
Iteration 194/1000 | Loss: 0.00000955
Iteration 195/1000 | Loss: 0.00000955
Iteration 196/1000 | Loss: 0.00000955
Iteration 197/1000 | Loss: 0.00000955
Iteration 198/1000 | Loss: 0.00000955
Iteration 199/1000 | Loss: 0.00000955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [9.546750334266108e-06, 9.546750334266108e-06, 9.546750334266108e-06, 9.546750334266108e-06, 9.546750334266108e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.546750334266108e-06

Optimization complete. Final v2v error: 2.68084454536438 mm

Highest mean error: 2.9373538494110107 mm for frame 61

Lowest mean error: 2.513721466064453 mm for frame 119

Saving results

Total time: 38.27731490135193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047620
Iteration 2/25 | Loss: 0.01047620
Iteration 3/25 | Loss: 0.00558471
Iteration 4/25 | Loss: 0.00385847
Iteration 5/25 | Loss: 0.00402017
Iteration 6/25 | Loss: 0.00309577
Iteration 7/25 | Loss: 0.00262188
Iteration 8/25 | Loss: 0.00249724
Iteration 9/25 | Loss: 0.00226432
Iteration 10/25 | Loss: 0.00224211
Iteration 11/25 | Loss: 0.00240829
Iteration 12/25 | Loss: 0.00227764
Iteration 13/25 | Loss: 0.00208558
Iteration 14/25 | Loss: 0.00202776
Iteration 15/25 | Loss: 0.00198011
Iteration 16/25 | Loss: 0.00192638
Iteration 17/25 | Loss: 0.00189564
Iteration 18/25 | Loss: 0.00188049
Iteration 19/25 | Loss: 0.00185326
Iteration 20/25 | Loss: 0.00183158
Iteration 21/25 | Loss: 0.00182356
Iteration 22/25 | Loss: 0.00208163
Iteration 23/25 | Loss: 0.00199367
Iteration 24/25 | Loss: 0.00175694
Iteration 25/25 | Loss: 0.00171599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39370304
Iteration 2/25 | Loss: 0.00192970
Iteration 3/25 | Loss: 0.00192970
Iteration 4/25 | Loss: 0.00192969
Iteration 5/25 | Loss: 0.00192969
Iteration 6/25 | Loss: 0.00192969
Iteration 7/25 | Loss: 0.00192969
Iteration 8/25 | Loss: 0.00192969
Iteration 9/25 | Loss: 0.00192969
Iteration 10/25 | Loss: 0.00192969
Iteration 11/25 | Loss: 0.00192969
Iteration 12/25 | Loss: 0.00192969
Iteration 13/25 | Loss: 0.00192969
Iteration 14/25 | Loss: 0.00192969
Iteration 15/25 | Loss: 0.00192969
Iteration 16/25 | Loss: 0.00192969
Iteration 17/25 | Loss: 0.00192969
Iteration 18/25 | Loss: 0.00192969
Iteration 19/25 | Loss: 0.00192969
Iteration 20/25 | Loss: 0.00192969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019296921091154218, 0.0019296921091154218, 0.0019296921091154218, 0.0019296921091154218, 0.0019296921091154218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019296921091154218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192969
Iteration 2/1000 | Loss: 0.00022575
Iteration 3/1000 | Loss: 0.00015536
Iteration 4/1000 | Loss: 0.00013162
Iteration 5/1000 | Loss: 0.00012241
Iteration 6/1000 | Loss: 0.00020345
Iteration 7/1000 | Loss: 0.00020932
Iteration 8/1000 | Loss: 0.00017536
Iteration 9/1000 | Loss: 0.00016455
Iteration 10/1000 | Loss: 0.00011921
Iteration 11/1000 | Loss: 0.00011837
Iteration 12/1000 | Loss: 0.00011106
Iteration 13/1000 | Loss: 0.00011444
Iteration 14/1000 | Loss: 0.00012914
Iteration 15/1000 | Loss: 0.00011611
Iteration 16/1000 | Loss: 0.00010641
Iteration 17/1000 | Loss: 0.00026332
Iteration 18/1000 | Loss: 0.00027090
Iteration 19/1000 | Loss: 0.00012159
Iteration 20/1000 | Loss: 0.00011092
Iteration 21/1000 | Loss: 0.00059168
Iteration 22/1000 | Loss: 0.00056629
Iteration 23/1000 | Loss: 0.00021925
Iteration 24/1000 | Loss: 0.00028461
Iteration 25/1000 | Loss: 0.00019035
Iteration 26/1000 | Loss: 0.00015148
Iteration 27/1000 | Loss: 0.00012759
Iteration 28/1000 | Loss: 0.00011149
Iteration 29/1000 | Loss: 0.00029129
Iteration 30/1000 | Loss: 0.00110130
Iteration 31/1000 | Loss: 0.00027501
Iteration 32/1000 | Loss: 0.00040609
Iteration 33/1000 | Loss: 0.00027684
Iteration 34/1000 | Loss: 0.00017057
Iteration 35/1000 | Loss: 0.00045859
Iteration 36/1000 | Loss: 0.00031078
Iteration 37/1000 | Loss: 0.00039447
Iteration 38/1000 | Loss: 0.00012585
Iteration 39/1000 | Loss: 0.00023388
Iteration 40/1000 | Loss: 0.00015403
Iteration 41/1000 | Loss: 0.00016581
Iteration 42/1000 | Loss: 0.00016090
Iteration 43/1000 | Loss: 0.00010665
Iteration 44/1000 | Loss: 0.00027312
Iteration 45/1000 | Loss: 0.00010683
Iteration 46/1000 | Loss: 0.00013974
Iteration 47/1000 | Loss: 0.00011230
Iteration 48/1000 | Loss: 0.00011394
Iteration 49/1000 | Loss: 0.00010903
Iteration 50/1000 | Loss: 0.00059082
Iteration 51/1000 | Loss: 0.00022342
Iteration 52/1000 | Loss: 0.00025738
Iteration 53/1000 | Loss: 0.00049491
Iteration 54/1000 | Loss: 0.00049974
Iteration 55/1000 | Loss: 0.00013600
Iteration 56/1000 | Loss: 0.00011074
Iteration 57/1000 | Loss: 0.00015503
Iteration 58/1000 | Loss: 0.00017489
Iteration 59/1000 | Loss: 0.00010236
Iteration 60/1000 | Loss: 0.00009452
Iteration 61/1000 | Loss: 0.00009849
Iteration 62/1000 | Loss: 0.00010777
Iteration 63/1000 | Loss: 0.00009539
Iteration 64/1000 | Loss: 0.00009570
Iteration 65/1000 | Loss: 0.00009165
Iteration 66/1000 | Loss: 0.00009041
Iteration 67/1000 | Loss: 0.00032214
Iteration 68/1000 | Loss: 0.00097630
Iteration 69/1000 | Loss: 0.00045807
Iteration 70/1000 | Loss: 0.00033155
Iteration 71/1000 | Loss: 0.00021216
Iteration 72/1000 | Loss: 0.00012396
Iteration 73/1000 | Loss: 0.00017038
Iteration 74/1000 | Loss: 0.00009873
Iteration 75/1000 | Loss: 0.00009242
Iteration 76/1000 | Loss: 0.00008842
Iteration 77/1000 | Loss: 0.00008582
Iteration 78/1000 | Loss: 0.00008469
Iteration 79/1000 | Loss: 0.00008355
Iteration 80/1000 | Loss: 0.00008293
Iteration 81/1000 | Loss: 0.00010619
Iteration 82/1000 | Loss: 0.00008530
Iteration 83/1000 | Loss: 0.00008352
Iteration 84/1000 | Loss: 0.00008231
Iteration 85/1000 | Loss: 0.00008167
Iteration 86/1000 | Loss: 0.00008131
Iteration 87/1000 | Loss: 0.00008099
Iteration 88/1000 | Loss: 0.00008067
Iteration 89/1000 | Loss: 0.00008039
Iteration 90/1000 | Loss: 0.00008025
Iteration 91/1000 | Loss: 0.00008016
Iteration 92/1000 | Loss: 0.00008016
Iteration 93/1000 | Loss: 0.00008016
Iteration 94/1000 | Loss: 0.00008016
Iteration 95/1000 | Loss: 0.00008016
Iteration 96/1000 | Loss: 0.00008016
Iteration 97/1000 | Loss: 0.00008016
Iteration 98/1000 | Loss: 0.00008015
Iteration 99/1000 | Loss: 0.00008015
Iteration 100/1000 | Loss: 0.00008009
Iteration 101/1000 | Loss: 0.00008009
Iteration 102/1000 | Loss: 0.00008008
Iteration 103/1000 | Loss: 0.00008006
Iteration 104/1000 | Loss: 0.00008005
Iteration 105/1000 | Loss: 0.00008004
Iteration 106/1000 | Loss: 0.00008003
Iteration 107/1000 | Loss: 0.00008003
Iteration 108/1000 | Loss: 0.00008003
Iteration 109/1000 | Loss: 0.00008002
Iteration 110/1000 | Loss: 0.00008002
Iteration 111/1000 | Loss: 0.00008002
Iteration 112/1000 | Loss: 0.00008001
Iteration 113/1000 | Loss: 0.00008000
Iteration 114/1000 | Loss: 0.00008000
Iteration 115/1000 | Loss: 0.00008000
Iteration 116/1000 | Loss: 0.00007999
Iteration 117/1000 | Loss: 0.00007999
Iteration 118/1000 | Loss: 0.00007998
Iteration 119/1000 | Loss: 0.00007998
Iteration 120/1000 | Loss: 0.00007997
Iteration 121/1000 | Loss: 0.00007997
Iteration 122/1000 | Loss: 0.00007997
Iteration 123/1000 | Loss: 0.00007997
Iteration 124/1000 | Loss: 0.00007997
Iteration 125/1000 | Loss: 0.00007996
Iteration 126/1000 | Loss: 0.00007996
Iteration 127/1000 | Loss: 0.00007996
Iteration 128/1000 | Loss: 0.00007996
Iteration 129/1000 | Loss: 0.00007996
Iteration 130/1000 | Loss: 0.00007996
Iteration 131/1000 | Loss: 0.00007996
Iteration 132/1000 | Loss: 0.00007996
Iteration 133/1000 | Loss: 0.00007996
Iteration 134/1000 | Loss: 0.00007996
Iteration 135/1000 | Loss: 0.00007996
Iteration 136/1000 | Loss: 0.00007996
Iteration 137/1000 | Loss: 0.00007996
Iteration 138/1000 | Loss: 0.00007996
Iteration 139/1000 | Loss: 0.00007996
Iteration 140/1000 | Loss: 0.00007996
Iteration 141/1000 | Loss: 0.00007996
Iteration 142/1000 | Loss: 0.00007996
Iteration 143/1000 | Loss: 0.00007996
Iteration 144/1000 | Loss: 0.00007996
Iteration 145/1000 | Loss: 0.00007995
Iteration 146/1000 | Loss: 0.00007995
Iteration 147/1000 | Loss: 0.00007995
Iteration 148/1000 | Loss: 0.00007995
Iteration 149/1000 | Loss: 0.00007995
Iteration 150/1000 | Loss: 0.00007994
Iteration 151/1000 | Loss: 0.00007994
Iteration 152/1000 | Loss: 0.00007994
Iteration 153/1000 | Loss: 0.00007993
Iteration 154/1000 | Loss: 0.00007993
Iteration 155/1000 | Loss: 0.00007993
Iteration 156/1000 | Loss: 0.00007993
Iteration 157/1000 | Loss: 0.00007993
Iteration 158/1000 | Loss: 0.00007993
Iteration 159/1000 | Loss: 0.00007993
Iteration 160/1000 | Loss: 0.00007993
Iteration 161/1000 | Loss: 0.00007993
Iteration 162/1000 | Loss: 0.00007993
Iteration 163/1000 | Loss: 0.00007992
Iteration 164/1000 | Loss: 0.00007992
Iteration 165/1000 | Loss: 0.00007992
Iteration 166/1000 | Loss: 0.00007992
Iteration 167/1000 | Loss: 0.00007992
Iteration 168/1000 | Loss: 0.00007992
Iteration 169/1000 | Loss: 0.00007992
Iteration 170/1000 | Loss: 0.00007992
Iteration 171/1000 | Loss: 0.00007992
Iteration 172/1000 | Loss: 0.00007992
Iteration 173/1000 | Loss: 0.00007991
Iteration 174/1000 | Loss: 0.00007991
Iteration 175/1000 | Loss: 0.00007991
Iteration 176/1000 | Loss: 0.00007991
Iteration 177/1000 | Loss: 0.00007991
Iteration 178/1000 | Loss: 0.00007990
Iteration 179/1000 | Loss: 0.00007990
Iteration 180/1000 | Loss: 0.00007990
Iteration 181/1000 | Loss: 0.00007990
Iteration 182/1000 | Loss: 0.00007989
Iteration 183/1000 | Loss: 0.00007989
Iteration 184/1000 | Loss: 0.00007988
Iteration 185/1000 | Loss: 0.00007988
Iteration 186/1000 | Loss: 0.00007988
Iteration 187/1000 | Loss: 0.00007988
Iteration 188/1000 | Loss: 0.00007988
Iteration 189/1000 | Loss: 0.00007987
Iteration 190/1000 | Loss: 0.00007987
Iteration 191/1000 | Loss: 0.00007987
Iteration 192/1000 | Loss: 0.00007987
Iteration 193/1000 | Loss: 0.00007987
Iteration 194/1000 | Loss: 0.00007987
Iteration 195/1000 | Loss: 0.00007987
Iteration 196/1000 | Loss: 0.00007986
Iteration 197/1000 | Loss: 0.00007986
Iteration 198/1000 | Loss: 0.00007986
Iteration 199/1000 | Loss: 0.00007986
Iteration 200/1000 | Loss: 0.00007985
Iteration 201/1000 | Loss: 0.00007985
Iteration 202/1000 | Loss: 0.00007984
Iteration 203/1000 | Loss: 0.00007984
Iteration 204/1000 | Loss: 0.00007983
Iteration 205/1000 | Loss: 0.00007983
Iteration 206/1000 | Loss: 0.00007983
Iteration 207/1000 | Loss: 0.00007983
Iteration 208/1000 | Loss: 0.00007983
Iteration 209/1000 | Loss: 0.00007982
Iteration 210/1000 | Loss: 0.00007982
Iteration 211/1000 | Loss: 0.00007982
Iteration 212/1000 | Loss: 0.00007982
Iteration 213/1000 | Loss: 0.00007982
Iteration 214/1000 | Loss: 0.00007982
Iteration 215/1000 | Loss: 0.00007982
Iteration 216/1000 | Loss: 0.00007981
Iteration 217/1000 | Loss: 0.00007981
Iteration 218/1000 | Loss: 0.00007981
Iteration 219/1000 | Loss: 0.00007980
Iteration 220/1000 | Loss: 0.00007980
Iteration 221/1000 | Loss: 0.00007980
Iteration 222/1000 | Loss: 0.00007979
Iteration 223/1000 | Loss: 0.00007979
Iteration 224/1000 | Loss: 0.00007979
Iteration 225/1000 | Loss: 0.00007978
Iteration 226/1000 | Loss: 0.00007978
Iteration 227/1000 | Loss: 0.00007978
Iteration 228/1000 | Loss: 0.00007978
Iteration 229/1000 | Loss: 0.00007978
Iteration 230/1000 | Loss: 0.00007978
Iteration 231/1000 | Loss: 0.00007978
Iteration 232/1000 | Loss: 0.00007978
Iteration 233/1000 | Loss: 0.00007978
Iteration 234/1000 | Loss: 0.00007978
Iteration 235/1000 | Loss: 0.00007978
Iteration 236/1000 | Loss: 0.00007978
Iteration 237/1000 | Loss: 0.00007978
Iteration 238/1000 | Loss: 0.00007977
Iteration 239/1000 | Loss: 0.00007977
Iteration 240/1000 | Loss: 0.00007977
Iteration 241/1000 | Loss: 0.00007977
Iteration 242/1000 | Loss: 0.00007977
Iteration 243/1000 | Loss: 0.00007976
Iteration 244/1000 | Loss: 0.00007976
Iteration 245/1000 | Loss: 0.00007976
Iteration 246/1000 | Loss: 0.00007975
Iteration 247/1000 | Loss: 0.00007973
Iteration 248/1000 | Loss: 0.00007973
Iteration 249/1000 | Loss: 0.00007973
Iteration 250/1000 | Loss: 0.00007973
Iteration 251/1000 | Loss: 0.00007973
Iteration 252/1000 | Loss: 0.00007973
Iteration 253/1000 | Loss: 0.00007973
Iteration 254/1000 | Loss: 0.00007973
Iteration 255/1000 | Loss: 0.00007972
Iteration 256/1000 | Loss: 0.00007972
Iteration 257/1000 | Loss: 0.00007972
Iteration 258/1000 | Loss: 0.00007971
Iteration 259/1000 | Loss: 0.00007971
Iteration 260/1000 | Loss: 0.00007971
Iteration 261/1000 | Loss: 0.00007971
Iteration 262/1000 | Loss: 0.00007970
Iteration 263/1000 | Loss: 0.00007970
Iteration 264/1000 | Loss: 0.00007970
Iteration 265/1000 | Loss: 0.00007970
Iteration 266/1000 | Loss: 0.00007970
Iteration 267/1000 | Loss: 0.00007970
Iteration 268/1000 | Loss: 0.00007969
Iteration 269/1000 | Loss: 0.00007969
Iteration 270/1000 | Loss: 0.00007969
Iteration 271/1000 | Loss: 0.00007969
Iteration 272/1000 | Loss: 0.00007969
Iteration 273/1000 | Loss: 0.00007969
Iteration 274/1000 | Loss: 0.00007968
Iteration 275/1000 | Loss: 0.00007968
Iteration 276/1000 | Loss: 0.00007968
Iteration 277/1000 | Loss: 0.00007968
Iteration 278/1000 | Loss: 0.00007968
Iteration 279/1000 | Loss: 0.00007968
Iteration 280/1000 | Loss: 0.00007968
Iteration 281/1000 | Loss: 0.00007968
Iteration 282/1000 | Loss: 0.00007968
Iteration 283/1000 | Loss: 0.00007967
Iteration 284/1000 | Loss: 0.00007967
Iteration 285/1000 | Loss: 0.00007967
Iteration 286/1000 | Loss: 0.00007967
Iteration 287/1000 | Loss: 0.00007967
Iteration 288/1000 | Loss: 0.00007967
Iteration 289/1000 | Loss: 0.00007967
Iteration 290/1000 | Loss: 0.00007967
Iteration 291/1000 | Loss: 0.00007967
Iteration 292/1000 | Loss: 0.00007967
Iteration 293/1000 | Loss: 0.00007967
Iteration 294/1000 | Loss: 0.00007966
Iteration 295/1000 | Loss: 0.00007966
Iteration 296/1000 | Loss: 0.00007966
Iteration 297/1000 | Loss: 0.00007966
Iteration 298/1000 | Loss: 0.00007966
Iteration 299/1000 | Loss: 0.00007966
Iteration 300/1000 | Loss: 0.00007965
Iteration 301/1000 | Loss: 0.00007965
Iteration 302/1000 | Loss: 0.00007965
Iteration 303/1000 | Loss: 0.00007965
Iteration 304/1000 | Loss: 0.00007965
Iteration 305/1000 | Loss: 0.00007965
Iteration 306/1000 | Loss: 0.00007965
Iteration 307/1000 | Loss: 0.00007965
Iteration 308/1000 | Loss: 0.00007965
Iteration 309/1000 | Loss: 0.00007964
Iteration 310/1000 | Loss: 0.00007964
Iteration 311/1000 | Loss: 0.00007964
Iteration 312/1000 | Loss: 0.00007964
Iteration 313/1000 | Loss: 0.00007964
Iteration 314/1000 | Loss: 0.00007964
Iteration 315/1000 | Loss: 0.00007964
Iteration 316/1000 | Loss: 0.00009237
Iteration 317/1000 | Loss: 0.00008029
Iteration 318/1000 | Loss: 0.00008318
Iteration 319/1000 | Loss: 0.00007961
Iteration 320/1000 | Loss: 0.00007961
Iteration 321/1000 | Loss: 0.00007960
Iteration 322/1000 | Loss: 0.00007960
Iteration 323/1000 | Loss: 0.00007960
Iteration 324/1000 | Loss: 0.00007960
Iteration 325/1000 | Loss: 0.00007960
Iteration 326/1000 | Loss: 0.00007960
Iteration 327/1000 | Loss: 0.00007960
Iteration 328/1000 | Loss: 0.00007960
Iteration 329/1000 | Loss: 0.00007960
Iteration 330/1000 | Loss: 0.00007960
Iteration 331/1000 | Loss: 0.00007960
Iteration 332/1000 | Loss: 0.00007960
Iteration 333/1000 | Loss: 0.00007960
Iteration 334/1000 | Loss: 0.00007960
Iteration 335/1000 | Loss: 0.00007959
Iteration 336/1000 | Loss: 0.00007959
Iteration 337/1000 | Loss: 0.00007959
Iteration 338/1000 | Loss: 0.00007959
Iteration 339/1000 | Loss: 0.00007959
Iteration 340/1000 | Loss: 0.00007959
Iteration 341/1000 | Loss: 0.00007959
Iteration 342/1000 | Loss: 0.00007959
Iteration 343/1000 | Loss: 0.00007959
Iteration 344/1000 | Loss: 0.00007959
Iteration 345/1000 | Loss: 0.00007959
Iteration 346/1000 | Loss: 0.00007959
Iteration 347/1000 | Loss: 0.00007959
Iteration 348/1000 | Loss: 0.00007959
Iteration 349/1000 | Loss: 0.00007959
Iteration 350/1000 | Loss: 0.00007959
Iteration 351/1000 | Loss: 0.00007959
Iteration 352/1000 | Loss: 0.00007959
Iteration 353/1000 | Loss: 0.00007959
Iteration 354/1000 | Loss: 0.00007959
Iteration 355/1000 | Loss: 0.00007959
Iteration 356/1000 | Loss: 0.00007959
Iteration 357/1000 | Loss: 0.00007959
Iteration 358/1000 | Loss: 0.00007959
Iteration 359/1000 | Loss: 0.00007959
Iteration 360/1000 | Loss: 0.00007959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 360. Stopping optimization.
Last 5 losses: [7.959285721881315e-05, 7.959285721881315e-05, 7.959285721881315e-05, 7.959285721881315e-05, 7.959285721881315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.959285721881315e-05

Optimization complete. Final v2v error: 6.159908771514893 mm

Highest mean error: 12.069670677185059 mm for frame 46

Lowest mean error: 4.129909038543701 mm for frame 10

Saving results

Total time: 205.46207928657532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00290487
Iteration 2/25 | Loss: 0.00136974
Iteration 3/25 | Loss: 0.00118764
Iteration 4/25 | Loss: 0.00116432
Iteration 5/25 | Loss: 0.00115748
Iteration 6/25 | Loss: 0.00115453
Iteration 7/25 | Loss: 0.00115352
Iteration 8/25 | Loss: 0.00115285
Iteration 9/25 | Loss: 0.00115267
Iteration 10/25 | Loss: 0.00115267
Iteration 11/25 | Loss: 0.00115267
Iteration 12/25 | Loss: 0.00115267
Iteration 13/25 | Loss: 0.00115267
Iteration 14/25 | Loss: 0.00115267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011526655871421099, 0.0011526655871421099, 0.0011526655871421099, 0.0011526655871421099, 0.0011526655871421099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011526655871421099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24745119
Iteration 2/25 | Loss: 0.00186162
Iteration 3/25 | Loss: 0.00186161
Iteration 4/25 | Loss: 0.00186161
Iteration 5/25 | Loss: 0.00186161
Iteration 6/25 | Loss: 0.00186161
Iteration 7/25 | Loss: 0.00186161
Iteration 8/25 | Loss: 0.00186161
Iteration 9/25 | Loss: 0.00186161
Iteration 10/25 | Loss: 0.00186161
Iteration 11/25 | Loss: 0.00186161
Iteration 12/25 | Loss: 0.00186161
Iteration 13/25 | Loss: 0.00186161
Iteration 14/25 | Loss: 0.00186161
Iteration 15/25 | Loss: 0.00186161
Iteration 16/25 | Loss: 0.00186161
Iteration 17/25 | Loss: 0.00186161
Iteration 18/25 | Loss: 0.00186161
Iteration 19/25 | Loss: 0.00186161
Iteration 20/25 | Loss: 0.00186161
Iteration 21/25 | Loss: 0.00186161
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018616107990965247, 0.0018616107990965247, 0.0018616107990965247, 0.0018616107990965247, 0.0018616107990965247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018616107990965247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186161
Iteration 2/1000 | Loss: 0.00004444
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001591
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001431
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001322
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001258
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001248
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001239
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001237
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001234
Iteration 51/1000 | Loss: 0.00001234
Iteration 52/1000 | Loss: 0.00001233
Iteration 53/1000 | Loss: 0.00001233
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001224
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001222
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001222
Iteration 92/1000 | Loss: 0.00001222
Iteration 93/1000 | Loss: 0.00001222
Iteration 94/1000 | Loss: 0.00001222
Iteration 95/1000 | Loss: 0.00001222
Iteration 96/1000 | Loss: 0.00001222
Iteration 97/1000 | Loss: 0.00001222
Iteration 98/1000 | Loss: 0.00001221
Iteration 99/1000 | Loss: 0.00001221
Iteration 100/1000 | Loss: 0.00001221
Iteration 101/1000 | Loss: 0.00001221
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001218
Iteration 118/1000 | Loss: 0.00001217
Iteration 119/1000 | Loss: 0.00001217
Iteration 120/1000 | Loss: 0.00001217
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001216
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001215
Iteration 129/1000 | Loss: 0.00001215
Iteration 130/1000 | Loss: 0.00001215
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001214
Iteration 135/1000 | Loss: 0.00001214
Iteration 136/1000 | Loss: 0.00001214
Iteration 137/1000 | Loss: 0.00001213
Iteration 138/1000 | Loss: 0.00001213
Iteration 139/1000 | Loss: 0.00001213
Iteration 140/1000 | Loss: 0.00001213
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001212
Iteration 143/1000 | Loss: 0.00001212
Iteration 144/1000 | Loss: 0.00001212
Iteration 145/1000 | Loss: 0.00001212
Iteration 146/1000 | Loss: 0.00001212
Iteration 147/1000 | Loss: 0.00001211
Iteration 148/1000 | Loss: 0.00001211
Iteration 149/1000 | Loss: 0.00001211
Iteration 150/1000 | Loss: 0.00001211
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001211
Iteration 155/1000 | Loss: 0.00001211
Iteration 156/1000 | Loss: 0.00001211
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001210
Iteration 159/1000 | Loss: 0.00001210
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001209
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001209
Iteration 172/1000 | Loss: 0.00001209
Iteration 173/1000 | Loss: 0.00001209
Iteration 174/1000 | Loss: 0.00001209
Iteration 175/1000 | Loss: 0.00001209
Iteration 176/1000 | Loss: 0.00001209
Iteration 177/1000 | Loss: 0.00001209
Iteration 178/1000 | Loss: 0.00001208
Iteration 179/1000 | Loss: 0.00001208
Iteration 180/1000 | Loss: 0.00001208
Iteration 181/1000 | Loss: 0.00001208
Iteration 182/1000 | Loss: 0.00001207
Iteration 183/1000 | Loss: 0.00001207
Iteration 184/1000 | Loss: 0.00001207
Iteration 185/1000 | Loss: 0.00001207
Iteration 186/1000 | Loss: 0.00001207
Iteration 187/1000 | Loss: 0.00001206
Iteration 188/1000 | Loss: 0.00001206
Iteration 189/1000 | Loss: 0.00001206
Iteration 190/1000 | Loss: 0.00001206
Iteration 191/1000 | Loss: 0.00001206
Iteration 192/1000 | Loss: 0.00001206
Iteration 193/1000 | Loss: 0.00001206
Iteration 194/1000 | Loss: 0.00001206
Iteration 195/1000 | Loss: 0.00001206
Iteration 196/1000 | Loss: 0.00001206
Iteration 197/1000 | Loss: 0.00001206
Iteration 198/1000 | Loss: 0.00001206
Iteration 199/1000 | Loss: 0.00001205
Iteration 200/1000 | Loss: 0.00001205
Iteration 201/1000 | Loss: 0.00001205
Iteration 202/1000 | Loss: 0.00001205
Iteration 203/1000 | Loss: 0.00001205
Iteration 204/1000 | Loss: 0.00001205
Iteration 205/1000 | Loss: 0.00001205
Iteration 206/1000 | Loss: 0.00001205
Iteration 207/1000 | Loss: 0.00001205
Iteration 208/1000 | Loss: 0.00001205
Iteration 209/1000 | Loss: 0.00001205
Iteration 210/1000 | Loss: 0.00001205
Iteration 211/1000 | Loss: 0.00001205
Iteration 212/1000 | Loss: 0.00001205
Iteration 213/1000 | Loss: 0.00001205
Iteration 214/1000 | Loss: 0.00001205
Iteration 215/1000 | Loss: 0.00001205
Iteration 216/1000 | Loss: 0.00001205
Iteration 217/1000 | Loss: 0.00001205
Iteration 218/1000 | Loss: 0.00001205
Iteration 219/1000 | Loss: 0.00001205
Iteration 220/1000 | Loss: 0.00001204
Iteration 221/1000 | Loss: 0.00001204
Iteration 222/1000 | Loss: 0.00001204
Iteration 223/1000 | Loss: 0.00001204
Iteration 224/1000 | Loss: 0.00001204
Iteration 225/1000 | Loss: 0.00001204
Iteration 226/1000 | Loss: 0.00001204
Iteration 227/1000 | Loss: 0.00001204
Iteration 228/1000 | Loss: 0.00001204
Iteration 229/1000 | Loss: 0.00001204
Iteration 230/1000 | Loss: 0.00001204
Iteration 231/1000 | Loss: 0.00001204
Iteration 232/1000 | Loss: 0.00001204
Iteration 233/1000 | Loss: 0.00001204
Iteration 234/1000 | Loss: 0.00001204
Iteration 235/1000 | Loss: 0.00001203
Iteration 236/1000 | Loss: 0.00001203
Iteration 237/1000 | Loss: 0.00001203
Iteration 238/1000 | Loss: 0.00001203
Iteration 239/1000 | Loss: 0.00001203
Iteration 240/1000 | Loss: 0.00001203
Iteration 241/1000 | Loss: 0.00001202
Iteration 242/1000 | Loss: 0.00001202
Iteration 243/1000 | Loss: 0.00001202
Iteration 244/1000 | Loss: 0.00001202
Iteration 245/1000 | Loss: 0.00001202
Iteration 246/1000 | Loss: 0.00001202
Iteration 247/1000 | Loss: 0.00001202
Iteration 248/1000 | Loss: 0.00001202
Iteration 249/1000 | Loss: 0.00001202
Iteration 250/1000 | Loss: 0.00001202
Iteration 251/1000 | Loss: 0.00001202
Iteration 252/1000 | Loss: 0.00001201
Iteration 253/1000 | Loss: 0.00001201
Iteration 254/1000 | Loss: 0.00001201
Iteration 255/1000 | Loss: 0.00001201
Iteration 256/1000 | Loss: 0.00001201
Iteration 257/1000 | Loss: 0.00001201
Iteration 258/1000 | Loss: 0.00001201
Iteration 259/1000 | Loss: 0.00001201
Iteration 260/1000 | Loss: 0.00001200
Iteration 261/1000 | Loss: 0.00001200
Iteration 262/1000 | Loss: 0.00001200
Iteration 263/1000 | Loss: 0.00001200
Iteration 264/1000 | Loss: 0.00001200
Iteration 265/1000 | Loss: 0.00001200
Iteration 266/1000 | Loss: 0.00001200
Iteration 267/1000 | Loss: 0.00001200
Iteration 268/1000 | Loss: 0.00001200
Iteration 269/1000 | Loss: 0.00001200
Iteration 270/1000 | Loss: 0.00001200
Iteration 271/1000 | Loss: 0.00001200
Iteration 272/1000 | Loss: 0.00001200
Iteration 273/1000 | Loss: 0.00001200
Iteration 274/1000 | Loss: 0.00001200
Iteration 275/1000 | Loss: 0.00001200
Iteration 276/1000 | Loss: 0.00001200
Iteration 277/1000 | Loss: 0.00001200
Iteration 278/1000 | Loss: 0.00001200
Iteration 279/1000 | Loss: 0.00001200
Iteration 280/1000 | Loss: 0.00001199
Iteration 281/1000 | Loss: 0.00001199
Iteration 282/1000 | Loss: 0.00001199
Iteration 283/1000 | Loss: 0.00001199
Iteration 284/1000 | Loss: 0.00001199
Iteration 285/1000 | Loss: 0.00001199
Iteration 286/1000 | Loss: 0.00001199
Iteration 287/1000 | Loss: 0.00001199
Iteration 288/1000 | Loss: 0.00001199
Iteration 289/1000 | Loss: 0.00001199
Iteration 290/1000 | Loss: 0.00001199
Iteration 291/1000 | Loss: 0.00001199
Iteration 292/1000 | Loss: 0.00001199
Iteration 293/1000 | Loss: 0.00001199
Iteration 294/1000 | Loss: 0.00001199
Iteration 295/1000 | Loss: 0.00001199
Iteration 296/1000 | Loss: 0.00001199
Iteration 297/1000 | Loss: 0.00001198
Iteration 298/1000 | Loss: 0.00001198
Iteration 299/1000 | Loss: 0.00001198
Iteration 300/1000 | Loss: 0.00001198
Iteration 301/1000 | Loss: 0.00001198
Iteration 302/1000 | Loss: 0.00001198
Iteration 303/1000 | Loss: 0.00001198
Iteration 304/1000 | Loss: 0.00001198
Iteration 305/1000 | Loss: 0.00001198
Iteration 306/1000 | Loss: 0.00001198
Iteration 307/1000 | Loss: 0.00001198
Iteration 308/1000 | Loss: 0.00001198
Iteration 309/1000 | Loss: 0.00001198
Iteration 310/1000 | Loss: 0.00001198
Iteration 311/1000 | Loss: 0.00001198
Iteration 312/1000 | Loss: 0.00001198
Iteration 313/1000 | Loss: 0.00001198
Iteration 314/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [1.1983666809101123e-05, 1.1983666809101123e-05, 1.1983666809101123e-05, 1.1983666809101123e-05, 1.1983666809101123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1983666809101123e-05

Optimization complete. Final v2v error: 2.912191390991211 mm

Highest mean error: 3.6788856983184814 mm for frame 106

Lowest mean error: 2.5385959148406982 mm for frame 13

Saving results

Total time: 52.540409326553345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425433
Iteration 2/25 | Loss: 0.00128922
Iteration 3/25 | Loss: 0.00120782
Iteration 4/25 | Loss: 0.00119320
Iteration 5/25 | Loss: 0.00118856
Iteration 6/25 | Loss: 0.00118774
Iteration 7/25 | Loss: 0.00118774
Iteration 8/25 | Loss: 0.00118774
Iteration 9/25 | Loss: 0.00118774
Iteration 10/25 | Loss: 0.00118774
Iteration 11/25 | Loss: 0.00118774
Iteration 12/25 | Loss: 0.00118774
Iteration 13/25 | Loss: 0.00118774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011877442011609674, 0.0011877442011609674, 0.0011877442011609674, 0.0011877442011609674, 0.0011877442011609674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011877442011609674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84351826
Iteration 2/25 | Loss: 0.00130010
Iteration 3/25 | Loss: 0.00130010
Iteration 4/25 | Loss: 0.00130009
Iteration 5/25 | Loss: 0.00130009
Iteration 6/25 | Loss: 0.00130009
Iteration 7/25 | Loss: 0.00130009
Iteration 8/25 | Loss: 0.00130009
Iteration 9/25 | Loss: 0.00130009
Iteration 10/25 | Loss: 0.00130009
Iteration 11/25 | Loss: 0.00130009
Iteration 12/25 | Loss: 0.00130009
Iteration 13/25 | Loss: 0.00130009
Iteration 14/25 | Loss: 0.00130009
Iteration 15/25 | Loss: 0.00130009
Iteration 16/25 | Loss: 0.00130009
Iteration 17/25 | Loss: 0.00130009
Iteration 18/25 | Loss: 0.00130009
Iteration 19/25 | Loss: 0.00130009
Iteration 20/25 | Loss: 0.00130009
Iteration 21/25 | Loss: 0.00130009
Iteration 22/25 | Loss: 0.00130009
Iteration 23/25 | Loss: 0.00130009
Iteration 24/25 | Loss: 0.00130009
Iteration 25/25 | Loss: 0.00130009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130009
Iteration 2/1000 | Loss: 0.00002921
Iteration 3/1000 | Loss: 0.00002213
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001715
Iteration 8/1000 | Loss: 0.00001670
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001605
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001558
Iteration 14/1000 | Loss: 0.00001538
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001524
Iteration 19/1000 | Loss: 0.00001523
Iteration 20/1000 | Loss: 0.00001523
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001521
Iteration 23/1000 | Loss: 0.00001520
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001513
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001505
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001492
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001482
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001482
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001481
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001476
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001476
Iteration 103/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.4758704310224857e-05, 1.4758704310224857e-05, 1.4758704310224857e-05, 1.4758704310224857e-05, 1.4758704310224857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4758704310224857e-05

Optimization complete. Final v2v error: 3.295156717300415 mm

Highest mean error: 3.8647475242614746 mm for frame 21

Lowest mean error: 3.0073676109313965 mm for frame 42

Saving results

Total time: 37.32302689552307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843460
Iteration 2/25 | Loss: 0.00140051
Iteration 3/25 | Loss: 0.00124979
Iteration 4/25 | Loss: 0.00122935
Iteration 5/25 | Loss: 0.00122221
Iteration 6/25 | Loss: 0.00122017
Iteration 7/25 | Loss: 0.00121959
Iteration 8/25 | Loss: 0.00121959
Iteration 9/25 | Loss: 0.00121959
Iteration 10/25 | Loss: 0.00121959
Iteration 11/25 | Loss: 0.00121959
Iteration 12/25 | Loss: 0.00121959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012195889139547944, 0.0012195889139547944, 0.0012195889139547944, 0.0012195889139547944, 0.0012195889139547944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012195889139547944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31378603
Iteration 2/25 | Loss: 0.00141451
Iteration 3/25 | Loss: 0.00141451
Iteration 4/25 | Loss: 0.00141451
Iteration 5/25 | Loss: 0.00141451
Iteration 6/25 | Loss: 0.00141451
Iteration 7/25 | Loss: 0.00141451
Iteration 8/25 | Loss: 0.00141451
Iteration 9/25 | Loss: 0.00141451
Iteration 10/25 | Loss: 0.00141451
Iteration 11/25 | Loss: 0.00141451
Iteration 12/25 | Loss: 0.00141451
Iteration 13/25 | Loss: 0.00141451
Iteration 14/25 | Loss: 0.00141451
Iteration 15/25 | Loss: 0.00141451
Iteration 16/25 | Loss: 0.00141451
Iteration 17/25 | Loss: 0.00141451
Iteration 18/25 | Loss: 0.00141451
Iteration 19/25 | Loss: 0.00141451
Iteration 20/25 | Loss: 0.00141451
Iteration 21/25 | Loss: 0.00141451
Iteration 22/25 | Loss: 0.00141451
Iteration 23/25 | Loss: 0.00141451
Iteration 24/25 | Loss: 0.00141451
Iteration 25/25 | Loss: 0.00141451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141451
Iteration 2/1000 | Loss: 0.00005295
Iteration 3/1000 | Loss: 0.00003239
Iteration 4/1000 | Loss: 0.00002345
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00001970
Iteration 7/1000 | Loss: 0.00001878
Iteration 8/1000 | Loss: 0.00001829
Iteration 9/1000 | Loss: 0.00001788
Iteration 10/1000 | Loss: 0.00001742
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001674
Iteration 14/1000 | Loss: 0.00001653
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001639
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001628
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001622
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001618
Iteration 29/1000 | Loss: 0.00001617
Iteration 30/1000 | Loss: 0.00001617
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001604
Iteration 41/1000 | Loss: 0.00001604
Iteration 42/1000 | Loss: 0.00001604
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001603
Iteration 45/1000 | Loss: 0.00001603
Iteration 46/1000 | Loss: 0.00001603
Iteration 47/1000 | Loss: 0.00001603
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001601
Iteration 63/1000 | Loss: 0.00001601
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001600
Iteration 70/1000 | Loss: 0.00001600
Iteration 71/1000 | Loss: 0.00001600
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001599
Iteration 74/1000 | Loss: 0.00001599
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001599
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001598
Iteration 79/1000 | Loss: 0.00001598
Iteration 80/1000 | Loss: 0.00001598
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001597
Iteration 85/1000 | Loss: 0.00001597
Iteration 86/1000 | Loss: 0.00001597
Iteration 87/1000 | Loss: 0.00001597
Iteration 88/1000 | Loss: 0.00001597
Iteration 89/1000 | Loss: 0.00001597
Iteration 90/1000 | Loss: 0.00001597
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001596
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001596
Iteration 97/1000 | Loss: 0.00001596
Iteration 98/1000 | Loss: 0.00001596
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001593
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001592
Iteration 122/1000 | Loss: 0.00001592
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001591
Iteration 131/1000 | Loss: 0.00001591
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001590
Iteration 138/1000 | Loss: 0.00001590
Iteration 139/1000 | Loss: 0.00001590
Iteration 140/1000 | Loss: 0.00001590
Iteration 141/1000 | Loss: 0.00001590
Iteration 142/1000 | Loss: 0.00001590
Iteration 143/1000 | Loss: 0.00001589
Iteration 144/1000 | Loss: 0.00001589
Iteration 145/1000 | Loss: 0.00001589
Iteration 146/1000 | Loss: 0.00001589
Iteration 147/1000 | Loss: 0.00001589
Iteration 148/1000 | Loss: 0.00001589
Iteration 149/1000 | Loss: 0.00001589
Iteration 150/1000 | Loss: 0.00001589
Iteration 151/1000 | Loss: 0.00001588
Iteration 152/1000 | Loss: 0.00001588
Iteration 153/1000 | Loss: 0.00001588
Iteration 154/1000 | Loss: 0.00001588
Iteration 155/1000 | Loss: 0.00001588
Iteration 156/1000 | Loss: 0.00001588
Iteration 157/1000 | Loss: 0.00001587
Iteration 158/1000 | Loss: 0.00001587
Iteration 159/1000 | Loss: 0.00001587
Iteration 160/1000 | Loss: 0.00001587
Iteration 161/1000 | Loss: 0.00001587
Iteration 162/1000 | Loss: 0.00001587
Iteration 163/1000 | Loss: 0.00001587
Iteration 164/1000 | Loss: 0.00001586
Iteration 165/1000 | Loss: 0.00001586
Iteration 166/1000 | Loss: 0.00001586
Iteration 167/1000 | Loss: 0.00001586
Iteration 168/1000 | Loss: 0.00001586
Iteration 169/1000 | Loss: 0.00001586
Iteration 170/1000 | Loss: 0.00001586
Iteration 171/1000 | Loss: 0.00001586
Iteration 172/1000 | Loss: 0.00001586
Iteration 173/1000 | Loss: 0.00001585
Iteration 174/1000 | Loss: 0.00001585
Iteration 175/1000 | Loss: 0.00001585
Iteration 176/1000 | Loss: 0.00001585
Iteration 177/1000 | Loss: 0.00001585
Iteration 178/1000 | Loss: 0.00001585
Iteration 179/1000 | Loss: 0.00001585
Iteration 180/1000 | Loss: 0.00001585
Iteration 181/1000 | Loss: 0.00001585
Iteration 182/1000 | Loss: 0.00001585
Iteration 183/1000 | Loss: 0.00001585
Iteration 184/1000 | Loss: 0.00001584
Iteration 185/1000 | Loss: 0.00001584
Iteration 186/1000 | Loss: 0.00001584
Iteration 187/1000 | Loss: 0.00001584
Iteration 188/1000 | Loss: 0.00001584
Iteration 189/1000 | Loss: 0.00001584
Iteration 190/1000 | Loss: 0.00001584
Iteration 191/1000 | Loss: 0.00001584
Iteration 192/1000 | Loss: 0.00001584
Iteration 193/1000 | Loss: 0.00001583
Iteration 194/1000 | Loss: 0.00001583
Iteration 195/1000 | Loss: 0.00001583
Iteration 196/1000 | Loss: 0.00001583
Iteration 197/1000 | Loss: 0.00001583
Iteration 198/1000 | Loss: 0.00001583
Iteration 199/1000 | Loss: 0.00001582
Iteration 200/1000 | Loss: 0.00001582
Iteration 201/1000 | Loss: 0.00001582
Iteration 202/1000 | Loss: 0.00001582
Iteration 203/1000 | Loss: 0.00001582
Iteration 204/1000 | Loss: 0.00001582
Iteration 205/1000 | Loss: 0.00001581
Iteration 206/1000 | Loss: 0.00001581
Iteration 207/1000 | Loss: 0.00001581
Iteration 208/1000 | Loss: 0.00001581
Iteration 209/1000 | Loss: 0.00001581
Iteration 210/1000 | Loss: 0.00001581
Iteration 211/1000 | Loss: 0.00001581
Iteration 212/1000 | Loss: 0.00001581
Iteration 213/1000 | Loss: 0.00001580
Iteration 214/1000 | Loss: 0.00001580
Iteration 215/1000 | Loss: 0.00001580
Iteration 216/1000 | Loss: 0.00001580
Iteration 217/1000 | Loss: 0.00001580
Iteration 218/1000 | Loss: 0.00001580
Iteration 219/1000 | Loss: 0.00001579
Iteration 220/1000 | Loss: 0.00001579
Iteration 221/1000 | Loss: 0.00001579
Iteration 222/1000 | Loss: 0.00001579
Iteration 223/1000 | Loss: 0.00001579
Iteration 224/1000 | Loss: 0.00001579
Iteration 225/1000 | Loss: 0.00001579
Iteration 226/1000 | Loss: 0.00001579
Iteration 227/1000 | Loss: 0.00001579
Iteration 228/1000 | Loss: 0.00001579
Iteration 229/1000 | Loss: 0.00001579
Iteration 230/1000 | Loss: 0.00001579
Iteration 231/1000 | Loss: 0.00001579
Iteration 232/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.5790230463608168e-05, 1.5790230463608168e-05, 1.5790230463608168e-05, 1.5790230463608168e-05, 1.5790230463608168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5790230463608168e-05

Optimization complete. Final v2v error: 3.2866759300231934 mm

Highest mean error: 5.525434494018555 mm for frame 70

Lowest mean error: 2.5179598331451416 mm for frame 127

Saving results

Total time: 47.47443604469299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491611
Iteration 2/25 | Loss: 0.00136693
Iteration 3/25 | Loss: 0.00126141
Iteration 4/25 | Loss: 0.00123913
Iteration 5/25 | Loss: 0.00123150
Iteration 6/25 | Loss: 0.00122985
Iteration 7/25 | Loss: 0.00122985
Iteration 8/25 | Loss: 0.00122985
Iteration 9/25 | Loss: 0.00122985
Iteration 10/25 | Loss: 0.00122985
Iteration 11/25 | Loss: 0.00122985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001229850109666586, 0.001229850109666586, 0.001229850109666586, 0.001229850109666586, 0.001229850109666586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001229850109666586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46127784
Iteration 2/25 | Loss: 0.00190072
Iteration 3/25 | Loss: 0.00190072
Iteration 4/25 | Loss: 0.00190072
Iteration 5/25 | Loss: 0.00190072
Iteration 6/25 | Loss: 0.00190071
Iteration 7/25 | Loss: 0.00190071
Iteration 8/25 | Loss: 0.00190071
Iteration 9/25 | Loss: 0.00190071
Iteration 10/25 | Loss: 0.00190071
Iteration 11/25 | Loss: 0.00190071
Iteration 12/25 | Loss: 0.00190071
Iteration 13/25 | Loss: 0.00190071
Iteration 14/25 | Loss: 0.00190071
Iteration 15/25 | Loss: 0.00190071
Iteration 16/25 | Loss: 0.00190071
Iteration 17/25 | Loss: 0.00190071
Iteration 18/25 | Loss: 0.00190071
Iteration 19/25 | Loss: 0.00190071
Iteration 20/25 | Loss: 0.00190071
Iteration 21/25 | Loss: 0.00190071
Iteration 22/25 | Loss: 0.00190071
Iteration 23/25 | Loss: 0.00190071
Iteration 24/25 | Loss: 0.00190071
Iteration 25/25 | Loss: 0.00190071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190071
Iteration 2/1000 | Loss: 0.00005816
Iteration 3/1000 | Loss: 0.00004166
Iteration 4/1000 | Loss: 0.00003434
Iteration 5/1000 | Loss: 0.00003161
Iteration 6/1000 | Loss: 0.00002979
Iteration 7/1000 | Loss: 0.00002846
Iteration 8/1000 | Loss: 0.00002771
Iteration 9/1000 | Loss: 0.00002714
Iteration 10/1000 | Loss: 0.00002669
Iteration 11/1000 | Loss: 0.00002634
Iteration 12/1000 | Loss: 0.00002605
Iteration 13/1000 | Loss: 0.00002580
Iteration 14/1000 | Loss: 0.00002559
Iteration 15/1000 | Loss: 0.00002555
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002532
Iteration 18/1000 | Loss: 0.00002525
Iteration 19/1000 | Loss: 0.00002520
Iteration 20/1000 | Loss: 0.00002520
Iteration 21/1000 | Loss: 0.00002520
Iteration 22/1000 | Loss: 0.00002519
Iteration 23/1000 | Loss: 0.00002518
Iteration 24/1000 | Loss: 0.00002517
Iteration 25/1000 | Loss: 0.00002516
Iteration 26/1000 | Loss: 0.00002515
Iteration 27/1000 | Loss: 0.00002515
Iteration 28/1000 | Loss: 0.00002514
Iteration 29/1000 | Loss: 0.00002514
Iteration 30/1000 | Loss: 0.00002512
Iteration 31/1000 | Loss: 0.00002511
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002510
Iteration 34/1000 | Loss: 0.00002510
Iteration 35/1000 | Loss: 0.00002509
Iteration 36/1000 | Loss: 0.00002509
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002508
Iteration 39/1000 | Loss: 0.00002506
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002506
Iteration 44/1000 | Loss: 0.00002506
Iteration 45/1000 | Loss: 0.00002506
Iteration 46/1000 | Loss: 0.00002506
Iteration 47/1000 | Loss: 0.00002505
Iteration 48/1000 | Loss: 0.00002505
Iteration 49/1000 | Loss: 0.00002505
Iteration 50/1000 | Loss: 0.00002505
Iteration 51/1000 | Loss: 0.00002504
Iteration 52/1000 | Loss: 0.00002503
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002502
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00002501
Iteration 60/1000 | Loss: 0.00002501
Iteration 61/1000 | Loss: 0.00002501
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002500
Iteration 65/1000 | Loss: 0.00002499
Iteration 66/1000 | Loss: 0.00002499
Iteration 67/1000 | Loss: 0.00002498
Iteration 68/1000 | Loss: 0.00002498
Iteration 69/1000 | Loss: 0.00002498
Iteration 70/1000 | Loss: 0.00002498
Iteration 71/1000 | Loss: 0.00002497
Iteration 72/1000 | Loss: 0.00002497
Iteration 73/1000 | Loss: 0.00002497
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002496
Iteration 77/1000 | Loss: 0.00002496
Iteration 78/1000 | Loss: 0.00002495
Iteration 79/1000 | Loss: 0.00002495
Iteration 80/1000 | Loss: 0.00002495
Iteration 81/1000 | Loss: 0.00002494
Iteration 82/1000 | Loss: 0.00002494
Iteration 83/1000 | Loss: 0.00002494
Iteration 84/1000 | Loss: 0.00002493
Iteration 85/1000 | Loss: 0.00002493
Iteration 86/1000 | Loss: 0.00002493
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002492
Iteration 89/1000 | Loss: 0.00002492
Iteration 90/1000 | Loss: 0.00002492
Iteration 91/1000 | Loss: 0.00002492
Iteration 92/1000 | Loss: 0.00002491
Iteration 93/1000 | Loss: 0.00002491
Iteration 94/1000 | Loss: 0.00002491
Iteration 95/1000 | Loss: 0.00002491
Iteration 96/1000 | Loss: 0.00002490
Iteration 97/1000 | Loss: 0.00002490
Iteration 98/1000 | Loss: 0.00002490
Iteration 99/1000 | Loss: 0.00002490
Iteration 100/1000 | Loss: 0.00002489
Iteration 101/1000 | Loss: 0.00002489
Iteration 102/1000 | Loss: 0.00002489
Iteration 103/1000 | Loss: 0.00002489
Iteration 104/1000 | Loss: 0.00002488
Iteration 105/1000 | Loss: 0.00002488
Iteration 106/1000 | Loss: 0.00002488
Iteration 107/1000 | Loss: 0.00002488
Iteration 108/1000 | Loss: 0.00002488
Iteration 109/1000 | Loss: 0.00002488
Iteration 110/1000 | Loss: 0.00002488
Iteration 111/1000 | Loss: 0.00002488
Iteration 112/1000 | Loss: 0.00002488
Iteration 113/1000 | Loss: 0.00002488
Iteration 114/1000 | Loss: 0.00002488
Iteration 115/1000 | Loss: 0.00002488
Iteration 116/1000 | Loss: 0.00002488
Iteration 117/1000 | Loss: 0.00002488
Iteration 118/1000 | Loss: 0.00002488
Iteration 119/1000 | Loss: 0.00002488
Iteration 120/1000 | Loss: 0.00002488
Iteration 121/1000 | Loss: 0.00002488
Iteration 122/1000 | Loss: 0.00002488
Iteration 123/1000 | Loss: 0.00002488
Iteration 124/1000 | Loss: 0.00002488
Iteration 125/1000 | Loss: 0.00002488
Iteration 126/1000 | Loss: 0.00002488
Iteration 127/1000 | Loss: 0.00002487
Iteration 128/1000 | Loss: 0.00002487
Iteration 129/1000 | Loss: 0.00002487
Iteration 130/1000 | Loss: 0.00002487
Iteration 131/1000 | Loss: 0.00002487
Iteration 132/1000 | Loss: 0.00002487
Iteration 133/1000 | Loss: 0.00002487
Iteration 134/1000 | Loss: 0.00002487
Iteration 135/1000 | Loss: 0.00002487
Iteration 136/1000 | Loss: 0.00002487
Iteration 137/1000 | Loss: 0.00002487
Iteration 138/1000 | Loss: 0.00002487
Iteration 139/1000 | Loss: 0.00002487
Iteration 140/1000 | Loss: 0.00002487
Iteration 141/1000 | Loss: 0.00002487
Iteration 142/1000 | Loss: 0.00002487
Iteration 143/1000 | Loss: 0.00002487
Iteration 144/1000 | Loss: 0.00002487
Iteration 145/1000 | Loss: 0.00002487
Iteration 146/1000 | Loss: 0.00002487
Iteration 147/1000 | Loss: 0.00002487
Iteration 148/1000 | Loss: 0.00002487
Iteration 149/1000 | Loss: 0.00002487
Iteration 150/1000 | Loss: 0.00002487
Iteration 151/1000 | Loss: 0.00002487
Iteration 152/1000 | Loss: 0.00002487
Iteration 153/1000 | Loss: 0.00002487
Iteration 154/1000 | Loss: 0.00002487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.4874441805877723e-05, 2.4874441805877723e-05, 2.4874441805877723e-05, 2.4874441805877723e-05, 2.4874441805877723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4874441805877723e-05

Optimization complete. Final v2v error: 4.128716945648193 mm

Highest mean error: 5.234228610992432 mm for frame 40

Lowest mean error: 3.3034353256225586 mm for frame 146

Saving results

Total time: 47.81660795211792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008004
Iteration 2/25 | Loss: 0.00181135
Iteration 3/25 | Loss: 0.00151362
Iteration 4/25 | Loss: 0.00139157
Iteration 5/25 | Loss: 0.00134112
Iteration 6/25 | Loss: 0.00134144
Iteration 7/25 | Loss: 0.00134922
Iteration 8/25 | Loss: 0.00132557
Iteration 9/25 | Loss: 0.00130287
Iteration 10/25 | Loss: 0.00129148
Iteration 11/25 | Loss: 0.00127862
Iteration 12/25 | Loss: 0.00127686
Iteration 13/25 | Loss: 0.00129096
Iteration 14/25 | Loss: 0.00130026
Iteration 15/25 | Loss: 0.00128533
Iteration 16/25 | Loss: 0.00127272
Iteration 17/25 | Loss: 0.00126986
Iteration 18/25 | Loss: 0.00126662
Iteration 19/25 | Loss: 0.00127130
Iteration 20/25 | Loss: 0.00126698
Iteration 21/25 | Loss: 0.00126007
Iteration 22/25 | Loss: 0.00125478
Iteration 23/25 | Loss: 0.00124965
Iteration 24/25 | Loss: 0.00125169
Iteration 25/25 | Loss: 0.00124418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31151116
Iteration 2/25 | Loss: 0.00172658
Iteration 3/25 | Loss: 0.00172657
Iteration 4/25 | Loss: 0.00172657
Iteration 5/25 | Loss: 0.00172657
Iteration 6/25 | Loss: 0.00172657
Iteration 7/25 | Loss: 0.00172657
Iteration 8/25 | Loss: 0.00172657
Iteration 9/25 | Loss: 0.00172657
Iteration 10/25 | Loss: 0.00172657
Iteration 11/25 | Loss: 0.00172657
Iteration 12/25 | Loss: 0.00172657
Iteration 13/25 | Loss: 0.00172657
Iteration 14/25 | Loss: 0.00172657
Iteration 15/25 | Loss: 0.00172657
Iteration 16/25 | Loss: 0.00172657
Iteration 17/25 | Loss: 0.00172657
Iteration 18/25 | Loss: 0.00172657
Iteration 19/25 | Loss: 0.00172657
Iteration 20/25 | Loss: 0.00172657
Iteration 21/25 | Loss: 0.00172657
Iteration 22/25 | Loss: 0.00172657
Iteration 23/25 | Loss: 0.00172657
Iteration 24/25 | Loss: 0.00172657
Iteration 25/25 | Loss: 0.00172657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172657
Iteration 2/1000 | Loss: 0.00052282
Iteration 3/1000 | Loss: 0.00065486
Iteration 4/1000 | Loss: 0.00045354
Iteration 5/1000 | Loss: 0.00022517
Iteration 6/1000 | Loss: 0.00048590
Iteration 7/1000 | Loss: 0.00054756
Iteration 8/1000 | Loss: 0.00112508
Iteration 9/1000 | Loss: 0.00050035
Iteration 10/1000 | Loss: 0.00029370
Iteration 11/1000 | Loss: 0.00040440
Iteration 12/1000 | Loss: 0.00170951
Iteration 13/1000 | Loss: 0.00061077
Iteration 14/1000 | Loss: 0.00046962
Iteration 15/1000 | Loss: 0.00043107
Iteration 16/1000 | Loss: 0.00039963
Iteration 17/1000 | Loss: 0.00060504
Iteration 18/1000 | Loss: 0.00048006
Iteration 19/1000 | Loss: 0.00020563
Iteration 20/1000 | Loss: 0.00032590
Iteration 21/1000 | Loss: 0.00083752
Iteration 22/1000 | Loss: 0.00040765
Iteration 23/1000 | Loss: 0.00027397
Iteration 24/1000 | Loss: 0.00037184
Iteration 25/1000 | Loss: 0.00041894
Iteration 26/1000 | Loss: 0.00040227
Iteration 27/1000 | Loss: 0.00042844
Iteration 28/1000 | Loss: 0.00034985
Iteration 29/1000 | Loss: 0.00057580
Iteration 30/1000 | Loss: 0.00043291
Iteration 31/1000 | Loss: 0.00052531
Iteration 32/1000 | Loss: 0.00034412
Iteration 33/1000 | Loss: 0.00022614
Iteration 34/1000 | Loss: 0.00029860
Iteration 35/1000 | Loss: 0.00031742
Iteration 36/1000 | Loss: 0.00042186
Iteration 37/1000 | Loss: 0.00038568
Iteration 38/1000 | Loss: 0.00049746
Iteration 39/1000 | Loss: 0.00043138
Iteration 40/1000 | Loss: 0.00045053
Iteration 41/1000 | Loss: 0.00051701
Iteration 42/1000 | Loss: 0.00036325
Iteration 43/1000 | Loss: 0.00038170
Iteration 44/1000 | Loss: 0.00036663
Iteration 45/1000 | Loss: 0.00038587
Iteration 46/1000 | Loss: 0.00040213
Iteration 47/1000 | Loss: 0.00018176
Iteration 48/1000 | Loss: 0.00058327
Iteration 49/1000 | Loss: 0.00043677
Iteration 50/1000 | Loss: 0.00034786
Iteration 51/1000 | Loss: 0.00025327
Iteration 52/1000 | Loss: 0.00024816
Iteration 53/1000 | Loss: 0.00044425
Iteration 54/1000 | Loss: 0.00042326
Iteration 55/1000 | Loss: 0.00028570
Iteration 56/1000 | Loss: 0.00032926
Iteration 57/1000 | Loss: 0.00042101
Iteration 58/1000 | Loss: 0.00040660
Iteration 59/1000 | Loss: 0.00035138
Iteration 60/1000 | Loss: 0.00040897
Iteration 61/1000 | Loss: 0.00041388
Iteration 62/1000 | Loss: 0.00041068
Iteration 63/1000 | Loss: 0.00043056
Iteration 64/1000 | Loss: 0.00047356
Iteration 65/1000 | Loss: 0.00034172
Iteration 66/1000 | Loss: 0.00094057
Iteration 67/1000 | Loss: 0.00049530
Iteration 68/1000 | Loss: 0.00052464
Iteration 69/1000 | Loss: 0.00037083
Iteration 70/1000 | Loss: 0.00039885
Iteration 71/1000 | Loss: 0.00023776
Iteration 72/1000 | Loss: 0.00025573
Iteration 73/1000 | Loss: 0.00042256
Iteration 74/1000 | Loss: 0.00045087
Iteration 75/1000 | Loss: 0.00048311
Iteration 76/1000 | Loss: 0.00045841
Iteration 77/1000 | Loss: 0.00038605
Iteration 78/1000 | Loss: 0.00039747
Iteration 79/1000 | Loss: 0.00041557
Iteration 80/1000 | Loss: 0.00049119
Iteration 81/1000 | Loss: 0.00016261
Iteration 82/1000 | Loss: 0.00025527
Iteration 83/1000 | Loss: 0.00032324
Iteration 84/1000 | Loss: 0.00039823
Iteration 85/1000 | Loss: 0.00028362
Iteration 86/1000 | Loss: 0.00035350
Iteration 87/1000 | Loss: 0.00034276
Iteration 88/1000 | Loss: 0.00034765
Iteration 89/1000 | Loss: 0.00020381
Iteration 90/1000 | Loss: 0.00036718
Iteration 91/1000 | Loss: 0.00017654
Iteration 92/1000 | Loss: 0.00035887
Iteration 93/1000 | Loss: 0.00025711
Iteration 94/1000 | Loss: 0.00007650
Iteration 95/1000 | Loss: 0.00029503
Iteration 96/1000 | Loss: 0.00020164
Iteration 97/1000 | Loss: 0.00025518
Iteration 98/1000 | Loss: 0.00029900
Iteration 99/1000 | Loss: 0.00024976
Iteration 100/1000 | Loss: 0.00030148
Iteration 101/1000 | Loss: 0.00025113
Iteration 102/1000 | Loss: 0.00029283
Iteration 103/1000 | Loss: 0.00024868
Iteration 104/1000 | Loss: 0.00030603
Iteration 105/1000 | Loss: 0.00098325
Iteration 106/1000 | Loss: 0.00040750
Iteration 107/1000 | Loss: 0.00027358
Iteration 108/1000 | Loss: 0.00034785
Iteration 109/1000 | Loss: 0.00031722
Iteration 110/1000 | Loss: 0.00033684
Iteration 111/1000 | Loss: 0.00018488
Iteration 112/1000 | Loss: 0.00019629
Iteration 113/1000 | Loss: 0.00038716
Iteration 114/1000 | Loss: 0.00021880
Iteration 115/1000 | Loss: 0.00030139
Iteration 116/1000 | Loss: 0.00034603
Iteration 117/1000 | Loss: 0.00029805
Iteration 118/1000 | Loss: 0.00029564
Iteration 119/1000 | Loss: 0.00017386
Iteration 120/1000 | Loss: 0.00028493
Iteration 121/1000 | Loss: 0.00031853
Iteration 122/1000 | Loss: 0.00027210
Iteration 123/1000 | Loss: 0.00034910
Iteration 124/1000 | Loss: 0.00034337
Iteration 125/1000 | Loss: 0.00029155
Iteration 126/1000 | Loss: 0.00024070
Iteration 127/1000 | Loss: 0.00040023
Iteration 128/1000 | Loss: 0.00028300
Iteration 129/1000 | Loss: 0.00036817
Iteration 130/1000 | Loss: 0.00031678
Iteration 131/1000 | Loss: 0.00049154
Iteration 132/1000 | Loss: 0.00031436
Iteration 133/1000 | Loss: 0.00040652
Iteration 134/1000 | Loss: 0.00038841
Iteration 135/1000 | Loss: 0.00032046
Iteration 136/1000 | Loss: 0.00034798
Iteration 137/1000 | Loss: 0.00031782
Iteration 138/1000 | Loss: 0.00030456
Iteration 139/1000 | Loss: 0.00033965
Iteration 140/1000 | Loss: 0.00022282
Iteration 141/1000 | Loss: 0.00034122
Iteration 142/1000 | Loss: 0.00026895
Iteration 143/1000 | Loss: 0.00027356
Iteration 144/1000 | Loss: 0.00027175
Iteration 145/1000 | Loss: 0.00031146
Iteration 146/1000 | Loss: 0.00029115
Iteration 147/1000 | Loss: 0.00020093
Iteration 148/1000 | Loss: 0.00017741
Iteration 149/1000 | Loss: 0.00015338
Iteration 150/1000 | Loss: 0.00016767
Iteration 151/1000 | Loss: 0.00014753
Iteration 152/1000 | Loss: 0.00016496
Iteration 153/1000 | Loss: 0.00016882
Iteration 154/1000 | Loss: 0.00011016
Iteration 155/1000 | Loss: 0.00007288
Iteration 156/1000 | Loss: 0.00010435
Iteration 157/1000 | Loss: 0.00016721
Iteration 158/1000 | Loss: 0.00021047
Iteration 159/1000 | Loss: 0.00014498
Iteration 160/1000 | Loss: 0.00017625
Iteration 161/1000 | Loss: 0.00016181
Iteration 162/1000 | Loss: 0.00017579
Iteration 163/1000 | Loss: 0.00009491
Iteration 164/1000 | Loss: 0.00015269
Iteration 165/1000 | Loss: 0.00023133
Iteration 166/1000 | Loss: 0.00011375
Iteration 167/1000 | Loss: 0.00008300
Iteration 168/1000 | Loss: 0.00018463
Iteration 169/1000 | Loss: 0.00015649
Iteration 170/1000 | Loss: 0.00015733
Iteration 171/1000 | Loss: 0.00012840
Iteration 172/1000 | Loss: 0.00014827
Iteration 173/1000 | Loss: 0.00006406
Iteration 174/1000 | Loss: 0.00003763
Iteration 175/1000 | Loss: 0.00026654
Iteration 176/1000 | Loss: 0.00005844
Iteration 177/1000 | Loss: 0.00014865
Iteration 178/1000 | Loss: 0.00010631
Iteration 179/1000 | Loss: 0.00014517
Iteration 180/1000 | Loss: 0.00009910
Iteration 181/1000 | Loss: 0.00014126
Iteration 182/1000 | Loss: 0.00009890
Iteration 183/1000 | Loss: 0.00013210
Iteration 184/1000 | Loss: 0.00010006
Iteration 185/1000 | Loss: 0.00012628
Iteration 186/1000 | Loss: 0.00029134
Iteration 187/1000 | Loss: 0.00020707
Iteration 188/1000 | Loss: 0.00024687
Iteration 189/1000 | Loss: 0.00020683
Iteration 190/1000 | Loss: 0.00019094
Iteration 191/1000 | Loss: 0.00015485
Iteration 192/1000 | Loss: 0.00002386
Iteration 193/1000 | Loss: 0.00002496
Iteration 194/1000 | Loss: 0.00001867
Iteration 195/1000 | Loss: 0.00002716
Iteration 196/1000 | Loss: 0.00002497
Iteration 197/1000 | Loss: 0.00004338
Iteration 198/1000 | Loss: 0.00002888
Iteration 199/1000 | Loss: 0.00018665
Iteration 200/1000 | Loss: 0.00012616
Iteration 201/1000 | Loss: 0.00016676
Iteration 202/1000 | Loss: 0.00004766
Iteration 203/1000 | Loss: 0.00007736
Iteration 204/1000 | Loss: 0.00004013
Iteration 205/1000 | Loss: 0.00004163
Iteration 206/1000 | Loss: 0.00003164
Iteration 207/1000 | Loss: 0.00004610
Iteration 208/1000 | Loss: 0.00003004
Iteration 209/1000 | Loss: 0.00003608
Iteration 210/1000 | Loss: 0.00004178
Iteration 211/1000 | Loss: 0.00005370
Iteration 212/1000 | Loss: 0.00002874
Iteration 213/1000 | Loss: 0.00002942
Iteration 214/1000 | Loss: 0.00002777
Iteration 215/1000 | Loss: 0.00003432
Iteration 216/1000 | Loss: 0.00003166
Iteration 217/1000 | Loss: 0.00006646
Iteration 218/1000 | Loss: 0.00002695
Iteration 219/1000 | Loss: 0.00002848
Iteration 220/1000 | Loss: 0.00003562
Iteration 221/1000 | Loss: 0.00002827
Iteration 222/1000 | Loss: 0.00003338
Iteration 223/1000 | Loss: 0.00005564
Iteration 224/1000 | Loss: 0.00002936
Iteration 225/1000 | Loss: 0.00005139
Iteration 226/1000 | Loss: 0.00002939
Iteration 227/1000 | Loss: 0.00003011
Iteration 228/1000 | Loss: 0.00014833
Iteration 229/1000 | Loss: 0.00002901
Iteration 230/1000 | Loss: 0.00002777
Iteration 231/1000 | Loss: 0.00003258
Iteration 232/1000 | Loss: 0.00003778
Iteration 233/1000 | Loss: 0.00005085
Iteration 234/1000 | Loss: 0.00002857
Iteration 235/1000 | Loss: 0.00002383
Iteration 236/1000 | Loss: 0.00002675
Iteration 237/1000 | Loss: 0.00003069
Iteration 238/1000 | Loss: 0.00003388
Iteration 239/1000 | Loss: 0.00002313
Iteration 240/1000 | Loss: 0.00004556
Iteration 241/1000 | Loss: 0.00002809
Iteration 242/1000 | Loss: 0.00002975
Iteration 243/1000 | Loss: 0.00003375
Iteration 244/1000 | Loss: 0.00002941
Iteration 245/1000 | Loss: 0.00002711
Iteration 246/1000 | Loss: 0.00003809
Iteration 247/1000 | Loss: 0.00003238
Iteration 248/1000 | Loss: 0.00003700
Iteration 249/1000 | Loss: 0.00002846
Iteration 250/1000 | Loss: 0.00003076
Iteration 251/1000 | Loss: 0.00002837
Iteration 252/1000 | Loss: 0.00002837
Iteration 253/1000 | Loss: 0.00005729
Iteration 254/1000 | Loss: 0.00003323
Iteration 255/1000 | Loss: 0.00002648
Iteration 256/1000 | Loss: 0.00004118
Iteration 257/1000 | Loss: 0.00002718
Iteration 258/1000 | Loss: 0.00003150
Iteration 259/1000 | Loss: 0.00002565
Iteration 260/1000 | Loss: 0.00002838
Iteration 261/1000 | Loss: 0.00003393
Iteration 262/1000 | Loss: 0.00003416
Iteration 263/1000 | Loss: 0.00002898
Iteration 264/1000 | Loss: 0.00002958
Iteration 265/1000 | Loss: 0.00002787
Iteration 266/1000 | Loss: 0.00003470
Iteration 267/1000 | Loss: 0.00002841
Iteration 268/1000 | Loss: 0.00002844
Iteration 269/1000 | Loss: 0.00002734
Iteration 270/1000 | Loss: 0.00003466
Iteration 271/1000 | Loss: 0.00003061
Iteration 272/1000 | Loss: 0.00004156
Iteration 273/1000 | Loss: 0.00002544
Iteration 274/1000 | Loss: 0.00004208
Iteration 275/1000 | Loss: 0.00002233
Iteration 276/1000 | Loss: 0.00003420
Iteration 277/1000 | Loss: 0.00002747
Iteration 278/1000 | Loss: 0.00003245
Iteration 279/1000 | Loss: 0.00002606
Iteration 280/1000 | Loss: 0.00003203
Iteration 281/1000 | Loss: 0.00003656
Iteration 282/1000 | Loss: 0.00003530
Iteration 283/1000 | Loss: 0.00004055
Iteration 284/1000 | Loss: 0.00005350
Iteration 285/1000 | Loss: 0.00003247
Iteration 286/1000 | Loss: 0.00001753
Iteration 287/1000 | Loss: 0.00004122
Iteration 288/1000 | Loss: 0.00001550
Iteration 289/1000 | Loss: 0.00001450
Iteration 290/1000 | Loss: 0.00001462
Iteration 291/1000 | Loss: 0.00001410
Iteration 292/1000 | Loss: 0.00001410
Iteration 293/1000 | Loss: 0.00001410
Iteration 294/1000 | Loss: 0.00001410
Iteration 295/1000 | Loss: 0.00001404
Iteration 296/1000 | Loss: 0.00001446
Iteration 297/1000 | Loss: 0.00001395
Iteration 298/1000 | Loss: 0.00001395
Iteration 299/1000 | Loss: 0.00001395
Iteration 300/1000 | Loss: 0.00001394
Iteration 301/1000 | Loss: 0.00001393
Iteration 302/1000 | Loss: 0.00001389
Iteration 303/1000 | Loss: 0.00001389
Iteration 304/1000 | Loss: 0.00001388
Iteration 305/1000 | Loss: 0.00001387
Iteration 306/1000 | Loss: 0.00001387
Iteration 307/1000 | Loss: 0.00001387
Iteration 308/1000 | Loss: 0.00001386
Iteration 309/1000 | Loss: 0.00001386
Iteration 310/1000 | Loss: 0.00001386
Iteration 311/1000 | Loss: 0.00001385
Iteration 312/1000 | Loss: 0.00001401
Iteration 313/1000 | Loss: 0.00001378
Iteration 314/1000 | Loss: 0.00001377
Iteration 315/1000 | Loss: 0.00001377
Iteration 316/1000 | Loss: 0.00001377
Iteration 317/1000 | Loss: 0.00001377
Iteration 318/1000 | Loss: 0.00001377
Iteration 319/1000 | Loss: 0.00001377
Iteration 320/1000 | Loss: 0.00001377
Iteration 321/1000 | Loss: 0.00001377
Iteration 322/1000 | Loss: 0.00001376
Iteration 323/1000 | Loss: 0.00001376
Iteration 324/1000 | Loss: 0.00001376
Iteration 325/1000 | Loss: 0.00001376
Iteration 326/1000 | Loss: 0.00001376
Iteration 327/1000 | Loss: 0.00001376
Iteration 328/1000 | Loss: 0.00001376
Iteration 329/1000 | Loss: 0.00001375
Iteration 330/1000 | Loss: 0.00001375
Iteration 331/1000 | Loss: 0.00001375
Iteration 332/1000 | Loss: 0.00001375
Iteration 333/1000 | Loss: 0.00001375
Iteration 334/1000 | Loss: 0.00001375
Iteration 335/1000 | Loss: 0.00001375
Iteration 336/1000 | Loss: 0.00001375
Iteration 337/1000 | Loss: 0.00001457
Iteration 338/1000 | Loss: 0.00001374
Iteration 339/1000 | Loss: 0.00001374
Iteration 340/1000 | Loss: 0.00001374
Iteration 341/1000 | Loss: 0.00001373
Iteration 342/1000 | Loss: 0.00001373
Iteration 343/1000 | Loss: 0.00001373
Iteration 344/1000 | Loss: 0.00001373
Iteration 345/1000 | Loss: 0.00001373
Iteration 346/1000 | Loss: 0.00001373
Iteration 347/1000 | Loss: 0.00001373
Iteration 348/1000 | Loss: 0.00001372
Iteration 349/1000 | Loss: 0.00001372
Iteration 350/1000 | Loss: 0.00001372
Iteration 351/1000 | Loss: 0.00001371
Iteration 352/1000 | Loss: 0.00001371
Iteration 353/1000 | Loss: 0.00001371
Iteration 354/1000 | Loss: 0.00001371
Iteration 355/1000 | Loss: 0.00001371
Iteration 356/1000 | Loss: 0.00001370
Iteration 357/1000 | Loss: 0.00001370
Iteration 358/1000 | Loss: 0.00001370
Iteration 359/1000 | Loss: 0.00001369
Iteration 360/1000 | Loss: 0.00001369
Iteration 361/1000 | Loss: 0.00001369
Iteration 362/1000 | Loss: 0.00001407
Iteration 363/1000 | Loss: 0.00001369
Iteration 364/1000 | Loss: 0.00001368
Iteration 365/1000 | Loss: 0.00001368
Iteration 366/1000 | Loss: 0.00001368
Iteration 367/1000 | Loss: 0.00001368
Iteration 368/1000 | Loss: 0.00001367
Iteration 369/1000 | Loss: 0.00001367
Iteration 370/1000 | Loss: 0.00001367
Iteration 371/1000 | Loss: 0.00001367
Iteration 372/1000 | Loss: 0.00001367
Iteration 373/1000 | Loss: 0.00001367
Iteration 374/1000 | Loss: 0.00001367
Iteration 375/1000 | Loss: 0.00001367
Iteration 376/1000 | Loss: 0.00001367
Iteration 377/1000 | Loss: 0.00001367
Iteration 378/1000 | Loss: 0.00001367
Iteration 379/1000 | Loss: 0.00001366
Iteration 380/1000 | Loss: 0.00001366
Iteration 381/1000 | Loss: 0.00001366
Iteration 382/1000 | Loss: 0.00001366
Iteration 383/1000 | Loss: 0.00001366
Iteration 384/1000 | Loss: 0.00001366
Iteration 385/1000 | Loss: 0.00001366
Iteration 386/1000 | Loss: 0.00001366
Iteration 387/1000 | Loss: 0.00001366
Iteration 388/1000 | Loss: 0.00001365
Iteration 389/1000 | Loss: 0.00001365
Iteration 390/1000 | Loss: 0.00001365
Iteration 391/1000 | Loss: 0.00001365
Iteration 392/1000 | Loss: 0.00001365
Iteration 393/1000 | Loss: 0.00001364
Iteration 394/1000 | Loss: 0.00001364
Iteration 395/1000 | Loss: 0.00001364
Iteration 396/1000 | Loss: 0.00001364
Iteration 397/1000 | Loss: 0.00001364
Iteration 398/1000 | Loss: 0.00001363
Iteration 399/1000 | Loss: 0.00001363
Iteration 400/1000 | Loss: 0.00001363
Iteration 401/1000 | Loss: 0.00001363
Iteration 402/1000 | Loss: 0.00001362
Iteration 403/1000 | Loss: 0.00001362
Iteration 404/1000 | Loss: 0.00001362
Iteration 405/1000 | Loss: 0.00001362
Iteration 406/1000 | Loss: 0.00001362
Iteration 407/1000 | Loss: 0.00001362
Iteration 408/1000 | Loss: 0.00001361
Iteration 409/1000 | Loss: 0.00001361
Iteration 410/1000 | Loss: 0.00001361
Iteration 411/1000 | Loss: 0.00001361
Iteration 412/1000 | Loss: 0.00001361
Iteration 413/1000 | Loss: 0.00001361
Iteration 414/1000 | Loss: 0.00001361
Iteration 415/1000 | Loss: 0.00001361
Iteration 416/1000 | Loss: 0.00001361
Iteration 417/1000 | Loss: 0.00001361
Iteration 418/1000 | Loss: 0.00001360
Iteration 419/1000 | Loss: 0.00001360
Iteration 420/1000 | Loss: 0.00001360
Iteration 421/1000 | Loss: 0.00001360
Iteration 422/1000 | Loss: 0.00001360
Iteration 423/1000 | Loss: 0.00001360
Iteration 424/1000 | Loss: 0.00001360
Iteration 425/1000 | Loss: 0.00001360
Iteration 426/1000 | Loss: 0.00001360
Iteration 427/1000 | Loss: 0.00001359
Iteration 428/1000 | Loss: 0.00001359
Iteration 429/1000 | Loss: 0.00001359
Iteration 430/1000 | Loss: 0.00001359
Iteration 431/1000 | Loss: 0.00001359
Iteration 432/1000 | Loss: 0.00001359
Iteration 433/1000 | Loss: 0.00001359
Iteration 434/1000 | Loss: 0.00001359
Iteration 435/1000 | Loss: 0.00001359
Iteration 436/1000 | Loss: 0.00001359
Iteration 437/1000 | Loss: 0.00001359
Iteration 438/1000 | Loss: 0.00001359
Iteration 439/1000 | Loss: 0.00001358
Iteration 440/1000 | Loss: 0.00001358
Iteration 441/1000 | Loss: 0.00001358
Iteration 442/1000 | Loss: 0.00001358
Iteration 443/1000 | Loss: 0.00001358
Iteration 444/1000 | Loss: 0.00001357
Iteration 445/1000 | Loss: 0.00001357
Iteration 446/1000 | Loss: 0.00001357
Iteration 447/1000 | Loss: 0.00001357
Iteration 448/1000 | Loss: 0.00001357
Iteration 449/1000 | Loss: 0.00001357
Iteration 450/1000 | Loss: 0.00001357
Iteration 451/1000 | Loss: 0.00001357
Iteration 452/1000 | Loss: 0.00001357
Iteration 453/1000 | Loss: 0.00001356
Iteration 454/1000 | Loss: 0.00001356
Iteration 455/1000 | Loss: 0.00001356
Iteration 456/1000 | Loss: 0.00001356
Iteration 457/1000 | Loss: 0.00001356
Iteration 458/1000 | Loss: 0.00001356
Iteration 459/1000 | Loss: 0.00001356
Iteration 460/1000 | Loss: 0.00001356
Iteration 461/1000 | Loss: 0.00001356
Iteration 462/1000 | Loss: 0.00001356
Iteration 463/1000 | Loss: 0.00001356
Iteration 464/1000 | Loss: 0.00001356
Iteration 465/1000 | Loss: 0.00001356
Iteration 466/1000 | Loss: 0.00001356
Iteration 467/1000 | Loss: 0.00001356
Iteration 468/1000 | Loss: 0.00001356
Iteration 469/1000 | Loss: 0.00001356
Iteration 470/1000 | Loss: 0.00001356
Iteration 471/1000 | Loss: 0.00001356
Iteration 472/1000 | Loss: 0.00001356
Iteration 473/1000 | Loss: 0.00001356
Iteration 474/1000 | Loss: 0.00001356
Iteration 475/1000 | Loss: 0.00001356
Iteration 476/1000 | Loss: 0.00001356
Iteration 477/1000 | Loss: 0.00001356
Iteration 478/1000 | Loss: 0.00001356
Iteration 479/1000 | Loss: 0.00001356
Iteration 480/1000 | Loss: 0.00001356
Iteration 481/1000 | Loss: 0.00001356
Iteration 482/1000 | Loss: 0.00001356
Iteration 483/1000 | Loss: 0.00001356
Iteration 484/1000 | Loss: 0.00001356
Iteration 485/1000 | Loss: 0.00001356
Iteration 486/1000 | Loss: 0.00001356
Iteration 487/1000 | Loss: 0.00001356
Iteration 488/1000 | Loss: 0.00001356
Iteration 489/1000 | Loss: 0.00001356
Iteration 490/1000 | Loss: 0.00001356
Iteration 491/1000 | Loss: 0.00001356
Iteration 492/1000 | Loss: 0.00001356
Iteration 493/1000 | Loss: 0.00001356
Iteration 494/1000 | Loss: 0.00001356
Iteration 495/1000 | Loss: 0.00001356
Iteration 496/1000 | Loss: 0.00001356
Iteration 497/1000 | Loss: 0.00001356
Iteration 498/1000 | Loss: 0.00001356
Iteration 499/1000 | Loss: 0.00001356
Iteration 500/1000 | Loss: 0.00001356
Iteration 501/1000 | Loss: 0.00001356
Iteration 502/1000 | Loss: 0.00001356
Iteration 503/1000 | Loss: 0.00001356
Iteration 504/1000 | Loss: 0.00001356
Iteration 505/1000 | Loss: 0.00001356
Iteration 506/1000 | Loss: 0.00001356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 506. Stopping optimization.
Last 5 losses: [1.3556602425524034e-05, 1.3556602425524034e-05, 1.3556602425524034e-05, 1.3556602425524034e-05, 1.3556602425524034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3556602425524034e-05

Optimization complete. Final v2v error: 3.048933506011963 mm

Highest mean error: 4.72632360458374 mm for frame 67

Lowest mean error: 2.6050477027893066 mm for frame 4

Saving results

Total time: 466.7451157569885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008178
Iteration 2/25 | Loss: 0.00261557
Iteration 3/25 | Loss: 0.00331440
Iteration 4/25 | Loss: 0.00152489
Iteration 5/25 | Loss: 0.00142518
Iteration 6/25 | Loss: 0.00135830
Iteration 7/25 | Loss: 0.00131470
Iteration 8/25 | Loss: 0.00129298
Iteration 9/25 | Loss: 0.00127690
Iteration 10/25 | Loss: 0.00124428
Iteration 11/25 | Loss: 0.00123752
Iteration 12/25 | Loss: 0.00123408
Iteration 13/25 | Loss: 0.00123554
Iteration 14/25 | Loss: 0.00122964
Iteration 15/25 | Loss: 0.00122857
Iteration 16/25 | Loss: 0.00122808
Iteration 17/25 | Loss: 0.00122549
Iteration 18/25 | Loss: 0.00123305
Iteration 19/25 | Loss: 0.00121972
Iteration 20/25 | Loss: 0.00121867
Iteration 21/25 | Loss: 0.00121294
Iteration 22/25 | Loss: 0.00120883
Iteration 23/25 | Loss: 0.00120447
Iteration 24/25 | Loss: 0.00120347
Iteration 25/25 | Loss: 0.00120319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40465391
Iteration 2/25 | Loss: 0.00140241
Iteration 3/25 | Loss: 0.00140241
Iteration 4/25 | Loss: 0.00140241
Iteration 5/25 | Loss: 0.00140241
Iteration 6/25 | Loss: 0.00140241
Iteration 7/25 | Loss: 0.00140241
Iteration 8/25 | Loss: 0.00140241
Iteration 9/25 | Loss: 0.00140241
Iteration 10/25 | Loss: 0.00140241
Iteration 11/25 | Loss: 0.00140241
Iteration 12/25 | Loss: 0.00140241
Iteration 13/25 | Loss: 0.00140241
Iteration 14/25 | Loss: 0.00140241
Iteration 15/25 | Loss: 0.00140241
Iteration 16/25 | Loss: 0.00140241
Iteration 17/25 | Loss: 0.00140241
Iteration 18/25 | Loss: 0.00140241
Iteration 19/25 | Loss: 0.00140241
Iteration 20/25 | Loss: 0.00140241
Iteration 21/25 | Loss: 0.00140241
Iteration 22/25 | Loss: 0.00140241
Iteration 23/25 | Loss: 0.00140241
Iteration 24/25 | Loss: 0.00140241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014024062547832727, 0.0014024062547832727, 0.0014024062547832727, 0.0014024062547832727, 0.0014024062547832727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014024062547832727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140241
Iteration 2/1000 | Loss: 0.00091822
Iteration 3/1000 | Loss: 0.00005333
Iteration 4/1000 | Loss: 0.00003804
Iteration 5/1000 | Loss: 0.00003026
Iteration 6/1000 | Loss: 0.00002624
Iteration 7/1000 | Loss: 0.00002399
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002035
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001753
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001691
Iteration 15/1000 | Loss: 0.00001678
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001645
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001635
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001621
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001620
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001619
Iteration 41/1000 | Loss: 0.00001619
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00019611
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00025617
Iteration 54/1000 | Loss: 0.00019218
Iteration 55/1000 | Loss: 0.00008465
Iteration 56/1000 | Loss: 0.00052984
Iteration 57/1000 | Loss: 0.00034237
Iteration 58/1000 | Loss: 0.00025096
Iteration 59/1000 | Loss: 0.00052825
Iteration 60/1000 | Loss: 0.00040524
Iteration 61/1000 | Loss: 0.00003006
Iteration 62/1000 | Loss: 0.00022181
Iteration 63/1000 | Loss: 0.00013664
Iteration 64/1000 | Loss: 0.00058530
Iteration 65/1000 | Loss: 0.00030584
Iteration 66/1000 | Loss: 0.00028237
Iteration 67/1000 | Loss: 0.00003059
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002138
Iteration 70/1000 | Loss: 0.00001961
Iteration 71/1000 | Loss: 0.00026979
Iteration 72/1000 | Loss: 0.00045184
Iteration 73/1000 | Loss: 0.00036841
Iteration 74/1000 | Loss: 0.00020007
Iteration 75/1000 | Loss: 0.00020146
Iteration 76/1000 | Loss: 0.00013426
Iteration 77/1000 | Loss: 0.00011884
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00028890
Iteration 81/1000 | Loss: 0.00012469
Iteration 82/1000 | Loss: 0.00002716
Iteration 83/1000 | Loss: 0.00002014
Iteration 84/1000 | Loss: 0.00040247
Iteration 85/1000 | Loss: 0.00021807
Iteration 86/1000 | Loss: 0.00019652
Iteration 87/1000 | Loss: 0.00043284
Iteration 88/1000 | Loss: 0.00039244
Iteration 89/1000 | Loss: 0.00044013
Iteration 90/1000 | Loss: 0.00003456
Iteration 91/1000 | Loss: 0.00038354
Iteration 92/1000 | Loss: 0.00039568
Iteration 93/1000 | Loss: 0.00039715
Iteration 94/1000 | Loss: 0.00023498
Iteration 95/1000 | Loss: 0.00013992
Iteration 96/1000 | Loss: 0.00027294
Iteration 97/1000 | Loss: 0.00023798
Iteration 98/1000 | Loss: 0.00024698
Iteration 99/1000 | Loss: 0.00010684
Iteration 100/1000 | Loss: 0.00003208
Iteration 101/1000 | Loss: 0.00002770
Iteration 102/1000 | Loss: 0.00033216
Iteration 103/1000 | Loss: 0.00035191
Iteration 104/1000 | Loss: 0.00004292
Iteration 105/1000 | Loss: 0.00003007
Iteration 106/1000 | Loss: 0.00034793
Iteration 107/1000 | Loss: 0.00021181
Iteration 108/1000 | Loss: 0.00002627
Iteration 109/1000 | Loss: 0.00002083
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001764
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001571
Iteration 115/1000 | Loss: 0.00003566
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00003310
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001562
Iteration 120/1000 | Loss: 0.00001511
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001458
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001779
Iteration 135/1000 | Loss: 0.00001506
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001364
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001362
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001350
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001338
Iteration 148/1000 | Loss: 0.00001338
Iteration 149/1000 | Loss: 0.00001337
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001336
Iteration 154/1000 | Loss: 0.00001336
Iteration 155/1000 | Loss: 0.00001334
Iteration 156/1000 | Loss: 0.00001333
Iteration 157/1000 | Loss: 0.00001333
Iteration 158/1000 | Loss: 0.00001333
Iteration 159/1000 | Loss: 0.00001331
Iteration 160/1000 | Loss: 0.00001331
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001321
Iteration 170/1000 | Loss: 0.00001321
Iteration 171/1000 | Loss: 0.00001321
Iteration 172/1000 | Loss: 0.00001321
Iteration 173/1000 | Loss: 0.00001321
Iteration 174/1000 | Loss: 0.00001320
Iteration 175/1000 | Loss: 0.00001320
Iteration 176/1000 | Loss: 0.00001320
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001320
Iteration 179/1000 | Loss: 0.00001320
Iteration 180/1000 | Loss: 0.00001319
Iteration 181/1000 | Loss: 0.00001319
Iteration 182/1000 | Loss: 0.00001318
Iteration 183/1000 | Loss: 0.00001318
Iteration 184/1000 | Loss: 0.00001318
Iteration 185/1000 | Loss: 0.00001318
Iteration 186/1000 | Loss: 0.00001317
Iteration 187/1000 | Loss: 0.00001317
Iteration 188/1000 | Loss: 0.00001316
Iteration 189/1000 | Loss: 0.00001316
Iteration 190/1000 | Loss: 0.00001316
Iteration 191/1000 | Loss: 0.00001316
Iteration 192/1000 | Loss: 0.00001315
Iteration 193/1000 | Loss: 0.00001315
Iteration 194/1000 | Loss: 0.00001315
Iteration 195/1000 | Loss: 0.00001315
Iteration 196/1000 | Loss: 0.00001315
Iteration 197/1000 | Loss: 0.00001314
Iteration 198/1000 | Loss: 0.00001314
Iteration 199/1000 | Loss: 0.00001314
Iteration 200/1000 | Loss: 0.00001314
Iteration 201/1000 | Loss: 0.00001314
Iteration 202/1000 | Loss: 0.00001313
Iteration 203/1000 | Loss: 0.00001313
Iteration 204/1000 | Loss: 0.00001313
Iteration 205/1000 | Loss: 0.00001313
Iteration 206/1000 | Loss: 0.00001313
Iteration 207/1000 | Loss: 0.00001313
Iteration 208/1000 | Loss: 0.00001313
Iteration 209/1000 | Loss: 0.00001313
Iteration 210/1000 | Loss: 0.00001312
Iteration 211/1000 | Loss: 0.00001312
Iteration 212/1000 | Loss: 0.00001312
Iteration 213/1000 | Loss: 0.00001312
Iteration 214/1000 | Loss: 0.00001312
Iteration 215/1000 | Loss: 0.00001312
Iteration 216/1000 | Loss: 0.00001312
Iteration 217/1000 | Loss: 0.00001312
Iteration 218/1000 | Loss: 0.00001312
Iteration 219/1000 | Loss: 0.00001312
Iteration 220/1000 | Loss: 0.00001312
Iteration 221/1000 | Loss: 0.00001312
Iteration 222/1000 | Loss: 0.00001312
Iteration 223/1000 | Loss: 0.00001312
Iteration 224/1000 | Loss: 0.00001312
Iteration 225/1000 | Loss: 0.00001312
Iteration 226/1000 | Loss: 0.00001312
Iteration 227/1000 | Loss: 0.00001311
Iteration 228/1000 | Loss: 0.00001311
Iteration 229/1000 | Loss: 0.00001311
Iteration 230/1000 | Loss: 0.00001311
Iteration 231/1000 | Loss: 0.00001311
Iteration 232/1000 | Loss: 0.00001311
Iteration 233/1000 | Loss: 0.00001311
Iteration 234/1000 | Loss: 0.00001311
Iteration 235/1000 | Loss: 0.00001311
Iteration 236/1000 | Loss: 0.00001311
Iteration 237/1000 | Loss: 0.00001311
Iteration 238/1000 | Loss: 0.00001311
Iteration 239/1000 | Loss: 0.00001311
Iteration 240/1000 | Loss: 0.00001311
Iteration 241/1000 | Loss: 0.00001311
Iteration 242/1000 | Loss: 0.00001310
Iteration 243/1000 | Loss: 0.00001310
Iteration 244/1000 | Loss: 0.00001310
Iteration 245/1000 | Loss: 0.00001310
Iteration 246/1000 | Loss: 0.00001310
Iteration 247/1000 | Loss: 0.00001310
Iteration 248/1000 | Loss: 0.00001310
Iteration 249/1000 | Loss: 0.00001310
Iteration 250/1000 | Loss: 0.00001310
Iteration 251/1000 | Loss: 0.00001310
Iteration 252/1000 | Loss: 0.00001310
Iteration 253/1000 | Loss: 0.00001310
Iteration 254/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.310401512455428e-05, 1.310401512455428e-05, 1.310401512455428e-05, 1.310401512455428e-05, 1.310401512455428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.310401512455428e-05

Optimization complete. Final v2v error: 2.9347243309020996 mm

Highest mean error: 5.07564115524292 mm for frame 101

Lowest mean error: 2.516505241394043 mm for frame 124

Saving results

Total time: 202.66769123077393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871308
Iteration 2/25 | Loss: 0.00126718
Iteration 3/25 | Loss: 0.00118849
Iteration 4/25 | Loss: 0.00117600
Iteration 5/25 | Loss: 0.00117211
Iteration 6/25 | Loss: 0.00117147
Iteration 7/25 | Loss: 0.00117147
Iteration 8/25 | Loss: 0.00117147
Iteration 9/25 | Loss: 0.00117147
Iteration 10/25 | Loss: 0.00117147
Iteration 11/25 | Loss: 0.00117147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011714728316292167, 0.0011714728316292167, 0.0011714728316292167, 0.0011714728316292167, 0.0011714728316292167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011714728316292167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37158310
Iteration 2/25 | Loss: 0.00126366
Iteration 3/25 | Loss: 0.00126366
Iteration 4/25 | Loss: 0.00126366
Iteration 5/25 | Loss: 0.00126366
Iteration 6/25 | Loss: 0.00126365
Iteration 7/25 | Loss: 0.00126365
Iteration 8/25 | Loss: 0.00126365
Iteration 9/25 | Loss: 0.00126365
Iteration 10/25 | Loss: 0.00126365
Iteration 11/25 | Loss: 0.00126365
Iteration 12/25 | Loss: 0.00126365
Iteration 13/25 | Loss: 0.00126365
Iteration 14/25 | Loss: 0.00126365
Iteration 15/25 | Loss: 0.00126365
Iteration 16/25 | Loss: 0.00126365
Iteration 17/25 | Loss: 0.00126365
Iteration 18/25 | Loss: 0.00126365
Iteration 19/25 | Loss: 0.00126365
Iteration 20/25 | Loss: 0.00126365
Iteration 21/25 | Loss: 0.00126365
Iteration 22/25 | Loss: 0.00126365
Iteration 23/25 | Loss: 0.00126365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012636533938348293, 0.0012636533938348293, 0.0012636533938348293, 0.0012636533938348293, 0.0012636533938348293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012636533938348293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126365
Iteration 2/1000 | Loss: 0.00002328
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001432
Iteration 6/1000 | Loss: 0.00001391
Iteration 7/1000 | Loss: 0.00001351
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001293
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001243
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001238
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001213
Iteration 53/1000 | Loss: 0.00001213
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001209
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001197
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001196
Iteration 98/1000 | Loss: 0.00001196
Iteration 99/1000 | Loss: 0.00001196
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001195
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001194
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001193
Iteration 115/1000 | Loss: 0.00001193
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001189
Iteration 136/1000 | Loss: 0.00001189
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001187
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001186
Iteration 157/1000 | Loss: 0.00001186
Iteration 158/1000 | Loss: 0.00001186
Iteration 159/1000 | Loss: 0.00001186
Iteration 160/1000 | Loss: 0.00001186
Iteration 161/1000 | Loss: 0.00001186
Iteration 162/1000 | Loss: 0.00001186
Iteration 163/1000 | Loss: 0.00001186
Iteration 164/1000 | Loss: 0.00001186
Iteration 165/1000 | Loss: 0.00001186
Iteration 166/1000 | Loss: 0.00001186
Iteration 167/1000 | Loss: 0.00001186
Iteration 168/1000 | Loss: 0.00001186
Iteration 169/1000 | Loss: 0.00001186
Iteration 170/1000 | Loss: 0.00001185
Iteration 171/1000 | Loss: 0.00001185
Iteration 172/1000 | Loss: 0.00001185
Iteration 173/1000 | Loss: 0.00001185
Iteration 174/1000 | Loss: 0.00001185
Iteration 175/1000 | Loss: 0.00001185
Iteration 176/1000 | Loss: 0.00001185
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Iteration 182/1000 | Loss: 0.00001185
Iteration 183/1000 | Loss: 0.00001185
Iteration 184/1000 | Loss: 0.00001185
Iteration 185/1000 | Loss: 0.00001185
Iteration 186/1000 | Loss: 0.00001185
Iteration 187/1000 | Loss: 0.00001185
Iteration 188/1000 | Loss: 0.00001185
Iteration 189/1000 | Loss: 0.00001185
Iteration 190/1000 | Loss: 0.00001185
Iteration 191/1000 | Loss: 0.00001185
Iteration 192/1000 | Loss: 0.00001185
Iteration 193/1000 | Loss: 0.00001185
Iteration 194/1000 | Loss: 0.00001185
Iteration 195/1000 | Loss: 0.00001185
Iteration 196/1000 | Loss: 0.00001185
Iteration 197/1000 | Loss: 0.00001185
Iteration 198/1000 | Loss: 0.00001185
Iteration 199/1000 | Loss: 0.00001185
Iteration 200/1000 | Loss: 0.00001185
Iteration 201/1000 | Loss: 0.00001185
Iteration 202/1000 | Loss: 0.00001185
Iteration 203/1000 | Loss: 0.00001185
Iteration 204/1000 | Loss: 0.00001185
Iteration 205/1000 | Loss: 0.00001185
Iteration 206/1000 | Loss: 0.00001185
Iteration 207/1000 | Loss: 0.00001185
Iteration 208/1000 | Loss: 0.00001185
Iteration 209/1000 | Loss: 0.00001185
Iteration 210/1000 | Loss: 0.00001185
Iteration 211/1000 | Loss: 0.00001185
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001185
Iteration 215/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.1851180715893861e-05, 1.1851180715893861e-05, 1.1851180715893861e-05, 1.1851180715893861e-05, 1.1851180715893861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1851180715893861e-05

Optimization complete. Final v2v error: 2.957184314727783 mm

Highest mean error: 3.18156361579895 mm for frame 135

Lowest mean error: 2.7258756160736084 mm for frame 60

Saving results

Total time: 39.756892919540405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00644386
Iteration 2/25 | Loss: 0.00156347
Iteration 3/25 | Loss: 0.00128376
Iteration 4/25 | Loss: 0.00124348
Iteration 5/25 | Loss: 0.00123239
Iteration 6/25 | Loss: 0.00123279
Iteration 7/25 | Loss: 0.00122766
Iteration 8/25 | Loss: 0.00121983
Iteration 9/25 | Loss: 0.00121622
Iteration 10/25 | Loss: 0.00121402
Iteration 11/25 | Loss: 0.00121385
Iteration 12/25 | Loss: 0.00121385
Iteration 13/25 | Loss: 0.00121385
Iteration 14/25 | Loss: 0.00121385
Iteration 15/25 | Loss: 0.00121385
Iteration 16/25 | Loss: 0.00121385
Iteration 17/25 | Loss: 0.00121385
Iteration 18/25 | Loss: 0.00121384
Iteration 19/25 | Loss: 0.00121384
Iteration 20/25 | Loss: 0.00121384
Iteration 21/25 | Loss: 0.00121384
Iteration 22/25 | Loss: 0.00121384
Iteration 23/25 | Loss: 0.00121384
Iteration 24/25 | Loss: 0.00121384
Iteration 25/25 | Loss: 0.00121384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96099377
Iteration 2/25 | Loss: 0.00171929
Iteration 3/25 | Loss: 0.00171910
Iteration 4/25 | Loss: 0.00171910
Iteration 5/25 | Loss: 0.00171910
Iteration 6/25 | Loss: 0.00171910
Iteration 7/25 | Loss: 0.00171910
Iteration 8/25 | Loss: 0.00171910
Iteration 9/25 | Loss: 0.00171910
Iteration 10/25 | Loss: 0.00171909
Iteration 11/25 | Loss: 0.00171909
Iteration 12/25 | Loss: 0.00171909
Iteration 13/25 | Loss: 0.00171909
Iteration 14/25 | Loss: 0.00171909
Iteration 15/25 | Loss: 0.00171909
Iteration 16/25 | Loss: 0.00171909
Iteration 17/25 | Loss: 0.00171909
Iteration 18/25 | Loss: 0.00171909
Iteration 19/25 | Loss: 0.00171909
Iteration 20/25 | Loss: 0.00171909
Iteration 21/25 | Loss: 0.00171909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00171909318305552, 0.00171909318305552, 0.00171909318305552, 0.00171909318305552, 0.00171909318305552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00171909318305552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171909
Iteration 2/1000 | Loss: 0.00007157
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00003207
Iteration 5/1000 | Loss: 0.00002888
Iteration 6/1000 | Loss: 0.00002611
Iteration 7/1000 | Loss: 0.00002467
Iteration 8/1000 | Loss: 0.00002356
Iteration 9/1000 | Loss: 0.00002266
Iteration 10/1000 | Loss: 0.00002208
Iteration 11/1000 | Loss: 0.00002171
Iteration 12/1000 | Loss: 0.00002135
Iteration 13/1000 | Loss: 0.00002108
Iteration 14/1000 | Loss: 0.00002089
Iteration 15/1000 | Loss: 0.00002085
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002047
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002021
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002018
Iteration 23/1000 | Loss: 0.00002018
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00002017
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002012
Iteration 29/1000 | Loss: 0.00002011
Iteration 30/1000 | Loss: 0.00002011
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002005
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00002005
Iteration 37/1000 | Loss: 0.00002004
Iteration 38/1000 | Loss: 0.00002003
Iteration 39/1000 | Loss: 0.00002002
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00002000
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001998
Iteration 45/1000 | Loss: 0.00001998
Iteration 46/1000 | Loss: 0.00001997
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001995
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001992
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001992
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001991
Iteration 69/1000 | Loss: 0.00001991
Iteration 70/1000 | Loss: 0.00001990
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001989
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001988
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001984
Iteration 88/1000 | Loss: 0.00001984
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001981
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001980
Iteration 102/1000 | Loss: 0.00001980
Iteration 103/1000 | Loss: 0.00001978
Iteration 104/1000 | Loss: 0.00001978
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001976
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001975
Iteration 116/1000 | Loss: 0.00001975
Iteration 117/1000 | Loss: 0.00001975
Iteration 118/1000 | Loss: 0.00001975
Iteration 119/1000 | Loss: 0.00001975
Iteration 120/1000 | Loss: 0.00001975
Iteration 121/1000 | Loss: 0.00001974
Iteration 122/1000 | Loss: 0.00001974
Iteration 123/1000 | Loss: 0.00001973
Iteration 124/1000 | Loss: 0.00001973
Iteration 125/1000 | Loss: 0.00001973
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001971
Iteration 128/1000 | Loss: 0.00001971
Iteration 129/1000 | Loss: 0.00001971
Iteration 130/1000 | Loss: 0.00001971
Iteration 131/1000 | Loss: 0.00001970
Iteration 132/1000 | Loss: 0.00001970
Iteration 133/1000 | Loss: 0.00001970
Iteration 134/1000 | Loss: 0.00001970
Iteration 135/1000 | Loss: 0.00001969
Iteration 136/1000 | Loss: 0.00001969
Iteration 137/1000 | Loss: 0.00001968
Iteration 138/1000 | Loss: 0.00001968
Iteration 139/1000 | Loss: 0.00001968
Iteration 140/1000 | Loss: 0.00001968
Iteration 141/1000 | Loss: 0.00001968
Iteration 142/1000 | Loss: 0.00001967
Iteration 143/1000 | Loss: 0.00001967
Iteration 144/1000 | Loss: 0.00001966
Iteration 145/1000 | Loss: 0.00001966
Iteration 146/1000 | Loss: 0.00001966
Iteration 147/1000 | Loss: 0.00001966
Iteration 148/1000 | Loss: 0.00001966
Iteration 149/1000 | Loss: 0.00001966
Iteration 150/1000 | Loss: 0.00001966
Iteration 151/1000 | Loss: 0.00001965
Iteration 152/1000 | Loss: 0.00001965
Iteration 153/1000 | Loss: 0.00001965
Iteration 154/1000 | Loss: 0.00001965
Iteration 155/1000 | Loss: 0.00001965
Iteration 156/1000 | Loss: 0.00001965
Iteration 157/1000 | Loss: 0.00001965
Iteration 158/1000 | Loss: 0.00001965
Iteration 159/1000 | Loss: 0.00001965
Iteration 160/1000 | Loss: 0.00001964
Iteration 161/1000 | Loss: 0.00001964
Iteration 162/1000 | Loss: 0.00001964
Iteration 163/1000 | Loss: 0.00001964
Iteration 164/1000 | Loss: 0.00001964
Iteration 165/1000 | Loss: 0.00001964
Iteration 166/1000 | Loss: 0.00001964
Iteration 167/1000 | Loss: 0.00001963
Iteration 168/1000 | Loss: 0.00001963
Iteration 169/1000 | Loss: 0.00001963
Iteration 170/1000 | Loss: 0.00001963
Iteration 171/1000 | Loss: 0.00001963
Iteration 172/1000 | Loss: 0.00001963
Iteration 173/1000 | Loss: 0.00001963
Iteration 174/1000 | Loss: 0.00001963
Iteration 175/1000 | Loss: 0.00001963
Iteration 176/1000 | Loss: 0.00001963
Iteration 177/1000 | Loss: 0.00001963
Iteration 178/1000 | Loss: 0.00001963
Iteration 179/1000 | Loss: 0.00001962
Iteration 180/1000 | Loss: 0.00001962
Iteration 181/1000 | Loss: 0.00001962
Iteration 182/1000 | Loss: 0.00001962
Iteration 183/1000 | Loss: 0.00001962
Iteration 184/1000 | Loss: 0.00001962
Iteration 185/1000 | Loss: 0.00001962
Iteration 186/1000 | Loss: 0.00001962
Iteration 187/1000 | Loss: 0.00001962
Iteration 188/1000 | Loss: 0.00001962
Iteration 189/1000 | Loss: 0.00001961
Iteration 190/1000 | Loss: 0.00001961
Iteration 191/1000 | Loss: 0.00001961
Iteration 192/1000 | Loss: 0.00001961
Iteration 193/1000 | Loss: 0.00001961
Iteration 194/1000 | Loss: 0.00001961
Iteration 195/1000 | Loss: 0.00001960
Iteration 196/1000 | Loss: 0.00001960
Iteration 197/1000 | Loss: 0.00001960
Iteration 198/1000 | Loss: 0.00001959
Iteration 199/1000 | Loss: 0.00001959
Iteration 200/1000 | Loss: 0.00001959
Iteration 201/1000 | Loss: 0.00001959
Iteration 202/1000 | Loss: 0.00001959
Iteration 203/1000 | Loss: 0.00001959
Iteration 204/1000 | Loss: 0.00001959
Iteration 205/1000 | Loss: 0.00001959
Iteration 206/1000 | Loss: 0.00001958
Iteration 207/1000 | Loss: 0.00001958
Iteration 208/1000 | Loss: 0.00001958
Iteration 209/1000 | Loss: 0.00001958
Iteration 210/1000 | Loss: 0.00001958
Iteration 211/1000 | Loss: 0.00001958
Iteration 212/1000 | Loss: 0.00001957
Iteration 213/1000 | Loss: 0.00001957
Iteration 214/1000 | Loss: 0.00001957
Iteration 215/1000 | Loss: 0.00001957
Iteration 216/1000 | Loss: 0.00001957
Iteration 217/1000 | Loss: 0.00001957
Iteration 218/1000 | Loss: 0.00001957
Iteration 219/1000 | Loss: 0.00001957
Iteration 220/1000 | Loss: 0.00001957
Iteration 221/1000 | Loss: 0.00001957
Iteration 222/1000 | Loss: 0.00001957
Iteration 223/1000 | Loss: 0.00001956
Iteration 224/1000 | Loss: 0.00001956
Iteration 225/1000 | Loss: 0.00001956
Iteration 226/1000 | Loss: 0.00001956
Iteration 227/1000 | Loss: 0.00001956
Iteration 228/1000 | Loss: 0.00001956
Iteration 229/1000 | Loss: 0.00001956
Iteration 230/1000 | Loss: 0.00001956
Iteration 231/1000 | Loss: 0.00001956
Iteration 232/1000 | Loss: 0.00001956
Iteration 233/1000 | Loss: 0.00001956
Iteration 234/1000 | Loss: 0.00001956
Iteration 235/1000 | Loss: 0.00001956
Iteration 236/1000 | Loss: 0.00001955
Iteration 237/1000 | Loss: 0.00001955
Iteration 238/1000 | Loss: 0.00001955
Iteration 239/1000 | Loss: 0.00001955
Iteration 240/1000 | Loss: 0.00001955
Iteration 241/1000 | Loss: 0.00001955
Iteration 242/1000 | Loss: 0.00001955
Iteration 243/1000 | Loss: 0.00001955
Iteration 244/1000 | Loss: 0.00001955
Iteration 245/1000 | Loss: 0.00001955
Iteration 246/1000 | Loss: 0.00001955
Iteration 247/1000 | Loss: 0.00001955
Iteration 248/1000 | Loss: 0.00001955
Iteration 249/1000 | Loss: 0.00001955
Iteration 250/1000 | Loss: 0.00001955
Iteration 251/1000 | Loss: 0.00001955
Iteration 252/1000 | Loss: 0.00001954
Iteration 253/1000 | Loss: 0.00001954
Iteration 254/1000 | Loss: 0.00001954
Iteration 255/1000 | Loss: 0.00001954
Iteration 256/1000 | Loss: 0.00001954
Iteration 257/1000 | Loss: 0.00001953
Iteration 258/1000 | Loss: 0.00001953
Iteration 259/1000 | Loss: 0.00001953
Iteration 260/1000 | Loss: 0.00001953
Iteration 261/1000 | Loss: 0.00001953
Iteration 262/1000 | Loss: 0.00001953
Iteration 263/1000 | Loss: 0.00001953
Iteration 264/1000 | Loss: 0.00001953
Iteration 265/1000 | Loss: 0.00001953
Iteration 266/1000 | Loss: 0.00001953
Iteration 267/1000 | Loss: 0.00001953
Iteration 268/1000 | Loss: 0.00001953
Iteration 269/1000 | Loss: 0.00001953
Iteration 270/1000 | Loss: 0.00001953
Iteration 271/1000 | Loss: 0.00001953
Iteration 272/1000 | Loss: 0.00001953
Iteration 273/1000 | Loss: 0.00001953
Iteration 274/1000 | Loss: 0.00001953
Iteration 275/1000 | Loss: 0.00001953
Iteration 276/1000 | Loss: 0.00001953
Iteration 277/1000 | Loss: 0.00001953
Iteration 278/1000 | Loss: 0.00001953
Iteration 279/1000 | Loss: 0.00001953
Iteration 280/1000 | Loss: 0.00001953
Iteration 281/1000 | Loss: 0.00001953
Iteration 282/1000 | Loss: 0.00001953
Iteration 283/1000 | Loss: 0.00001953
Iteration 284/1000 | Loss: 0.00001953
Iteration 285/1000 | Loss: 0.00001953
Iteration 286/1000 | Loss: 0.00001953
Iteration 287/1000 | Loss: 0.00001953
Iteration 288/1000 | Loss: 0.00001953
Iteration 289/1000 | Loss: 0.00001953
Iteration 290/1000 | Loss: 0.00001953
Iteration 291/1000 | Loss: 0.00001953
Iteration 292/1000 | Loss: 0.00001953
Iteration 293/1000 | Loss: 0.00001953
Iteration 294/1000 | Loss: 0.00001953
Iteration 295/1000 | Loss: 0.00001953
Iteration 296/1000 | Loss: 0.00001953
Iteration 297/1000 | Loss: 0.00001953
Iteration 298/1000 | Loss: 0.00001953
Iteration 299/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [1.9533526938175783e-05, 1.9533526938175783e-05, 1.9533526938175783e-05, 1.9533526938175783e-05, 1.9533526938175783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9533526938175783e-05

Optimization complete. Final v2v error: 3.552232503890991 mm

Highest mean error: 5.988487720489502 mm for frame 120

Lowest mean error: 2.680759906768799 mm for frame 10

Saving results

Total time: 74.38246774673462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826010
Iteration 2/25 | Loss: 0.00150845
Iteration 3/25 | Loss: 0.00130911
Iteration 4/25 | Loss: 0.00129070
Iteration 5/25 | Loss: 0.00128702
Iteration 6/25 | Loss: 0.00128628
Iteration 7/25 | Loss: 0.00128628
Iteration 8/25 | Loss: 0.00128628
Iteration 9/25 | Loss: 0.00128628
Iteration 10/25 | Loss: 0.00128628
Iteration 11/25 | Loss: 0.00128628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012862763833254576, 0.0012862763833254576, 0.0012862763833254576, 0.0012862763833254576, 0.0012862763833254576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012862763833254576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30389488
Iteration 2/25 | Loss: 0.00076505
Iteration 3/25 | Loss: 0.00076505
Iteration 4/25 | Loss: 0.00076505
Iteration 5/25 | Loss: 0.00076505
Iteration 6/25 | Loss: 0.00076505
Iteration 7/25 | Loss: 0.00076505
Iteration 8/25 | Loss: 0.00076505
Iteration 9/25 | Loss: 0.00076505
Iteration 10/25 | Loss: 0.00076505
Iteration 11/25 | Loss: 0.00076505
Iteration 12/25 | Loss: 0.00076505
Iteration 13/25 | Loss: 0.00076505
Iteration 14/25 | Loss: 0.00076505
Iteration 15/25 | Loss: 0.00076505
Iteration 16/25 | Loss: 0.00076504
Iteration 17/25 | Loss: 0.00076504
Iteration 18/25 | Loss: 0.00076504
Iteration 19/25 | Loss: 0.00076504
Iteration 20/25 | Loss: 0.00076504
Iteration 21/25 | Loss: 0.00076504
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007650449988432229, 0.0007650449988432229, 0.0007650449988432229, 0.0007650449988432229, 0.0007650449988432229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007650449988432229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076504
Iteration 2/1000 | Loss: 0.00005986
Iteration 3/1000 | Loss: 0.00004397
Iteration 4/1000 | Loss: 0.00004160
Iteration 5/1000 | Loss: 0.00004024
Iteration 6/1000 | Loss: 0.00003897
Iteration 7/1000 | Loss: 0.00003821
Iteration 8/1000 | Loss: 0.00003764
Iteration 9/1000 | Loss: 0.00003730
Iteration 10/1000 | Loss: 0.00003696
Iteration 11/1000 | Loss: 0.00003677
Iteration 12/1000 | Loss: 0.00003659
Iteration 13/1000 | Loss: 0.00003655
Iteration 14/1000 | Loss: 0.00003641
Iteration 15/1000 | Loss: 0.00003633
Iteration 16/1000 | Loss: 0.00003632
Iteration 17/1000 | Loss: 0.00003631
Iteration 18/1000 | Loss: 0.00003631
Iteration 19/1000 | Loss: 0.00003630
Iteration 20/1000 | Loss: 0.00003630
Iteration 21/1000 | Loss: 0.00003630
Iteration 22/1000 | Loss: 0.00003629
Iteration 23/1000 | Loss: 0.00003629
Iteration 24/1000 | Loss: 0.00003628
Iteration 25/1000 | Loss: 0.00003628
Iteration 26/1000 | Loss: 0.00003627
Iteration 27/1000 | Loss: 0.00003627
Iteration 28/1000 | Loss: 0.00003626
Iteration 29/1000 | Loss: 0.00003626
Iteration 30/1000 | Loss: 0.00003625
Iteration 31/1000 | Loss: 0.00003625
Iteration 32/1000 | Loss: 0.00003625
Iteration 33/1000 | Loss: 0.00003623
Iteration 34/1000 | Loss: 0.00003623
Iteration 35/1000 | Loss: 0.00003623
Iteration 36/1000 | Loss: 0.00003623
Iteration 37/1000 | Loss: 0.00003623
Iteration 38/1000 | Loss: 0.00003623
Iteration 39/1000 | Loss: 0.00003623
Iteration 40/1000 | Loss: 0.00003623
Iteration 41/1000 | Loss: 0.00003623
Iteration 42/1000 | Loss: 0.00003622
Iteration 43/1000 | Loss: 0.00003622
Iteration 44/1000 | Loss: 0.00003621
Iteration 45/1000 | Loss: 0.00003621
Iteration 46/1000 | Loss: 0.00003621
Iteration 47/1000 | Loss: 0.00003621
Iteration 48/1000 | Loss: 0.00003620
Iteration 49/1000 | Loss: 0.00003620
Iteration 50/1000 | Loss: 0.00003620
Iteration 51/1000 | Loss: 0.00003619
Iteration 52/1000 | Loss: 0.00003619
Iteration 53/1000 | Loss: 0.00003619
Iteration 54/1000 | Loss: 0.00003619
Iteration 55/1000 | Loss: 0.00003618
Iteration 56/1000 | Loss: 0.00003618
Iteration 57/1000 | Loss: 0.00003618
Iteration 58/1000 | Loss: 0.00003618
Iteration 59/1000 | Loss: 0.00003618
Iteration 60/1000 | Loss: 0.00003617
Iteration 61/1000 | Loss: 0.00003617
Iteration 62/1000 | Loss: 0.00003616
Iteration 63/1000 | Loss: 0.00003616
Iteration 64/1000 | Loss: 0.00003615
Iteration 65/1000 | Loss: 0.00003615
Iteration 66/1000 | Loss: 0.00003615
Iteration 67/1000 | Loss: 0.00003615
Iteration 68/1000 | Loss: 0.00003614
Iteration 69/1000 | Loss: 0.00003614
Iteration 70/1000 | Loss: 0.00003614
Iteration 71/1000 | Loss: 0.00003614
Iteration 72/1000 | Loss: 0.00003613
Iteration 73/1000 | Loss: 0.00003613
Iteration 74/1000 | Loss: 0.00003613
Iteration 75/1000 | Loss: 0.00003613
Iteration 76/1000 | Loss: 0.00003613
Iteration 77/1000 | Loss: 0.00003612
Iteration 78/1000 | Loss: 0.00003612
Iteration 79/1000 | Loss: 0.00003612
Iteration 80/1000 | Loss: 0.00003612
Iteration 81/1000 | Loss: 0.00003612
Iteration 82/1000 | Loss: 0.00003612
Iteration 83/1000 | Loss: 0.00003612
Iteration 84/1000 | Loss: 0.00003612
Iteration 85/1000 | Loss: 0.00003611
Iteration 86/1000 | Loss: 0.00003611
Iteration 87/1000 | Loss: 0.00003611
Iteration 88/1000 | Loss: 0.00003611
Iteration 89/1000 | Loss: 0.00003611
Iteration 90/1000 | Loss: 0.00003611
Iteration 91/1000 | Loss: 0.00003611
Iteration 92/1000 | Loss: 0.00003611
Iteration 93/1000 | Loss: 0.00003611
Iteration 94/1000 | Loss: 0.00003610
Iteration 95/1000 | Loss: 0.00003610
Iteration 96/1000 | Loss: 0.00003610
Iteration 97/1000 | Loss: 0.00003610
Iteration 98/1000 | Loss: 0.00003610
Iteration 99/1000 | Loss: 0.00003610
Iteration 100/1000 | Loss: 0.00003610
Iteration 101/1000 | Loss: 0.00003610
Iteration 102/1000 | Loss: 0.00003610
Iteration 103/1000 | Loss: 0.00003610
Iteration 104/1000 | Loss: 0.00003610
Iteration 105/1000 | Loss: 0.00003609
Iteration 106/1000 | Loss: 0.00003609
Iteration 107/1000 | Loss: 0.00003609
Iteration 108/1000 | Loss: 0.00003609
Iteration 109/1000 | Loss: 0.00003609
Iteration 110/1000 | Loss: 0.00003608
Iteration 111/1000 | Loss: 0.00003608
Iteration 112/1000 | Loss: 0.00003608
Iteration 113/1000 | Loss: 0.00003608
Iteration 114/1000 | Loss: 0.00003608
Iteration 115/1000 | Loss: 0.00003607
Iteration 116/1000 | Loss: 0.00003607
Iteration 117/1000 | Loss: 0.00003607
Iteration 118/1000 | Loss: 0.00003606
Iteration 119/1000 | Loss: 0.00003606
Iteration 120/1000 | Loss: 0.00003606
Iteration 121/1000 | Loss: 0.00003606
Iteration 122/1000 | Loss: 0.00003606
Iteration 123/1000 | Loss: 0.00003605
Iteration 124/1000 | Loss: 0.00003605
Iteration 125/1000 | Loss: 0.00003605
Iteration 126/1000 | Loss: 0.00003605
Iteration 127/1000 | Loss: 0.00003605
Iteration 128/1000 | Loss: 0.00003605
Iteration 129/1000 | Loss: 0.00003605
Iteration 130/1000 | Loss: 0.00003605
Iteration 131/1000 | Loss: 0.00003605
Iteration 132/1000 | Loss: 0.00003605
Iteration 133/1000 | Loss: 0.00003605
Iteration 134/1000 | Loss: 0.00003604
Iteration 135/1000 | Loss: 0.00003604
Iteration 136/1000 | Loss: 0.00003604
Iteration 137/1000 | Loss: 0.00003604
Iteration 138/1000 | Loss: 0.00003604
Iteration 139/1000 | Loss: 0.00003604
Iteration 140/1000 | Loss: 0.00003604
Iteration 141/1000 | Loss: 0.00003604
Iteration 142/1000 | Loss: 0.00003604
Iteration 143/1000 | Loss: 0.00003604
Iteration 144/1000 | Loss: 0.00003604
Iteration 145/1000 | Loss: 0.00003604
Iteration 146/1000 | Loss: 0.00003604
Iteration 147/1000 | Loss: 0.00003604
Iteration 148/1000 | Loss: 0.00003604
Iteration 149/1000 | Loss: 0.00003604
Iteration 150/1000 | Loss: 0.00003604
Iteration 151/1000 | Loss: 0.00003604
Iteration 152/1000 | Loss: 0.00003603
Iteration 153/1000 | Loss: 0.00003603
Iteration 154/1000 | Loss: 0.00003603
Iteration 155/1000 | Loss: 0.00003603
Iteration 156/1000 | Loss: 0.00003603
Iteration 157/1000 | Loss: 0.00003603
Iteration 158/1000 | Loss: 0.00003603
Iteration 159/1000 | Loss: 0.00003603
Iteration 160/1000 | Loss: 0.00003603
Iteration 161/1000 | Loss: 0.00003603
Iteration 162/1000 | Loss: 0.00003603
Iteration 163/1000 | Loss: 0.00003603
Iteration 164/1000 | Loss: 0.00003603
Iteration 165/1000 | Loss: 0.00003603
Iteration 166/1000 | Loss: 0.00003603
Iteration 167/1000 | Loss: 0.00003603
Iteration 168/1000 | Loss: 0.00003603
Iteration 169/1000 | Loss: 0.00003603
Iteration 170/1000 | Loss: 0.00003603
Iteration 171/1000 | Loss: 0.00003603
Iteration 172/1000 | Loss: 0.00003603
Iteration 173/1000 | Loss: 0.00003603
Iteration 174/1000 | Loss: 0.00003603
Iteration 175/1000 | Loss: 0.00003603
Iteration 176/1000 | Loss: 0.00003603
Iteration 177/1000 | Loss: 0.00003603
Iteration 178/1000 | Loss: 0.00003603
Iteration 179/1000 | Loss: 0.00003603
Iteration 180/1000 | Loss: 0.00003603
Iteration 181/1000 | Loss: 0.00003603
Iteration 182/1000 | Loss: 0.00003603
Iteration 183/1000 | Loss: 0.00003603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.6026140151079744e-05, 3.6026140151079744e-05, 3.6026140151079744e-05, 3.6026140151079744e-05, 3.6026140151079744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6026140151079744e-05

Optimization complete. Final v2v error: 4.985251426696777 mm

Highest mean error: 5.407992839813232 mm for frame 18

Lowest mean error: 4.504662036895752 mm for frame 31

Saving results

Total time: 38.69430947303772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955802
Iteration 2/25 | Loss: 0.00160448
Iteration 3/25 | Loss: 0.00136902
Iteration 4/25 | Loss: 0.00135156
Iteration 5/25 | Loss: 0.00134743
Iteration 6/25 | Loss: 0.00134743
Iteration 7/25 | Loss: 0.00134743
Iteration 8/25 | Loss: 0.00134743
Iteration 9/25 | Loss: 0.00134743
Iteration 10/25 | Loss: 0.00134743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013474338920786977, 0.0013474338920786977, 0.0013474338920786977, 0.0013474338920786977, 0.0013474338920786977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013474338920786977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81732452
Iteration 2/25 | Loss: 0.00140569
Iteration 3/25 | Loss: 0.00140569
Iteration 4/25 | Loss: 0.00140569
Iteration 5/25 | Loss: 0.00140569
Iteration 6/25 | Loss: 0.00140569
Iteration 7/25 | Loss: 0.00140568
Iteration 8/25 | Loss: 0.00140568
Iteration 9/25 | Loss: 0.00140568
Iteration 10/25 | Loss: 0.00140568
Iteration 11/25 | Loss: 0.00140568
Iteration 12/25 | Loss: 0.00140568
Iteration 13/25 | Loss: 0.00140568
Iteration 14/25 | Loss: 0.00140568
Iteration 15/25 | Loss: 0.00140568
Iteration 16/25 | Loss: 0.00140568
Iteration 17/25 | Loss: 0.00140568
Iteration 18/25 | Loss: 0.00140568
Iteration 19/25 | Loss: 0.00140568
Iteration 20/25 | Loss: 0.00140568
Iteration 21/25 | Loss: 0.00140568
Iteration 22/25 | Loss: 0.00140568
Iteration 23/25 | Loss: 0.00140568
Iteration 24/25 | Loss: 0.00140568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014056834625080228, 0.0014056834625080228, 0.0014056834625080228, 0.0014056834625080228, 0.0014056834625080228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014056834625080228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140568
Iteration 2/1000 | Loss: 0.00005642
Iteration 3/1000 | Loss: 0.00004347
Iteration 4/1000 | Loss: 0.00004091
Iteration 5/1000 | Loss: 0.00003863
Iteration 6/1000 | Loss: 0.00003758
Iteration 7/1000 | Loss: 0.00003657
Iteration 8/1000 | Loss: 0.00003605
Iteration 9/1000 | Loss: 0.00003562
Iteration 10/1000 | Loss: 0.00003519
Iteration 11/1000 | Loss: 0.00003481
Iteration 12/1000 | Loss: 0.00003446
Iteration 13/1000 | Loss: 0.00003418
Iteration 14/1000 | Loss: 0.00003388
Iteration 15/1000 | Loss: 0.00003361
Iteration 16/1000 | Loss: 0.00003332
Iteration 17/1000 | Loss: 0.00003309
Iteration 18/1000 | Loss: 0.00003294
Iteration 19/1000 | Loss: 0.00003282
Iteration 20/1000 | Loss: 0.00003268
Iteration 21/1000 | Loss: 0.00003262
Iteration 22/1000 | Loss: 0.00003261
Iteration 23/1000 | Loss: 0.00003259
Iteration 24/1000 | Loss: 0.00003253
Iteration 25/1000 | Loss: 0.00003252
Iteration 26/1000 | Loss: 0.00003252
Iteration 27/1000 | Loss: 0.00003252
Iteration 28/1000 | Loss: 0.00003250
Iteration 29/1000 | Loss: 0.00003250
Iteration 30/1000 | Loss: 0.00003250
Iteration 31/1000 | Loss: 0.00003250
Iteration 32/1000 | Loss: 0.00003250
Iteration 33/1000 | Loss: 0.00003250
Iteration 34/1000 | Loss: 0.00003249
Iteration 35/1000 | Loss: 0.00003249
Iteration 36/1000 | Loss: 0.00003249
Iteration 37/1000 | Loss: 0.00003248
Iteration 38/1000 | Loss: 0.00003247
Iteration 39/1000 | Loss: 0.00003247
Iteration 40/1000 | Loss: 0.00003247
Iteration 41/1000 | Loss: 0.00003247
Iteration 42/1000 | Loss: 0.00003247
Iteration 43/1000 | Loss: 0.00003247
Iteration 44/1000 | Loss: 0.00003247
Iteration 45/1000 | Loss: 0.00003246
Iteration 46/1000 | Loss: 0.00003246
Iteration 47/1000 | Loss: 0.00003246
Iteration 48/1000 | Loss: 0.00003244
Iteration 49/1000 | Loss: 0.00003244
Iteration 50/1000 | Loss: 0.00003243
Iteration 51/1000 | Loss: 0.00003243
Iteration 52/1000 | Loss: 0.00003243
Iteration 53/1000 | Loss: 0.00003243
Iteration 54/1000 | Loss: 0.00003243
Iteration 55/1000 | Loss: 0.00003243
Iteration 56/1000 | Loss: 0.00003243
Iteration 57/1000 | Loss: 0.00003243
Iteration 58/1000 | Loss: 0.00003243
Iteration 59/1000 | Loss: 0.00003242
Iteration 60/1000 | Loss: 0.00003242
Iteration 61/1000 | Loss: 0.00003242
Iteration 62/1000 | Loss: 0.00003242
Iteration 63/1000 | Loss: 0.00003242
Iteration 64/1000 | Loss: 0.00003241
Iteration 65/1000 | Loss: 0.00003241
Iteration 66/1000 | Loss: 0.00003240
Iteration 67/1000 | Loss: 0.00003240
Iteration 68/1000 | Loss: 0.00003240
Iteration 69/1000 | Loss: 0.00003239
Iteration 70/1000 | Loss: 0.00003239
Iteration 71/1000 | Loss: 0.00003239
Iteration 72/1000 | Loss: 0.00003239
Iteration 73/1000 | Loss: 0.00003239
Iteration 74/1000 | Loss: 0.00003238
Iteration 75/1000 | Loss: 0.00003238
Iteration 76/1000 | Loss: 0.00003238
Iteration 77/1000 | Loss: 0.00003238
Iteration 78/1000 | Loss: 0.00003238
Iteration 79/1000 | Loss: 0.00003237
Iteration 80/1000 | Loss: 0.00003237
Iteration 81/1000 | Loss: 0.00003237
Iteration 82/1000 | Loss: 0.00003237
Iteration 83/1000 | Loss: 0.00003237
Iteration 84/1000 | Loss: 0.00003237
Iteration 85/1000 | Loss: 0.00003237
Iteration 86/1000 | Loss: 0.00003237
Iteration 87/1000 | Loss: 0.00003237
Iteration 88/1000 | Loss: 0.00003237
Iteration 89/1000 | Loss: 0.00003237
Iteration 90/1000 | Loss: 0.00003236
Iteration 91/1000 | Loss: 0.00003236
Iteration 92/1000 | Loss: 0.00003236
Iteration 93/1000 | Loss: 0.00003236
Iteration 94/1000 | Loss: 0.00003236
Iteration 95/1000 | Loss: 0.00003236
Iteration 96/1000 | Loss: 0.00003236
Iteration 97/1000 | Loss: 0.00003236
Iteration 98/1000 | Loss: 0.00003236
Iteration 99/1000 | Loss: 0.00003235
Iteration 100/1000 | Loss: 0.00003235
Iteration 101/1000 | Loss: 0.00003235
Iteration 102/1000 | Loss: 0.00003235
Iteration 103/1000 | Loss: 0.00003234
Iteration 104/1000 | Loss: 0.00003234
Iteration 105/1000 | Loss: 0.00003234
Iteration 106/1000 | Loss: 0.00003234
Iteration 107/1000 | Loss: 0.00003234
Iteration 108/1000 | Loss: 0.00003234
Iteration 109/1000 | Loss: 0.00003234
Iteration 110/1000 | Loss: 0.00003233
Iteration 111/1000 | Loss: 0.00003233
Iteration 112/1000 | Loss: 0.00003233
Iteration 113/1000 | Loss: 0.00003233
Iteration 114/1000 | Loss: 0.00003233
Iteration 115/1000 | Loss: 0.00003233
Iteration 116/1000 | Loss: 0.00003233
Iteration 117/1000 | Loss: 0.00003233
Iteration 118/1000 | Loss: 0.00003233
Iteration 119/1000 | Loss: 0.00003232
Iteration 120/1000 | Loss: 0.00003232
Iteration 121/1000 | Loss: 0.00003232
Iteration 122/1000 | Loss: 0.00003232
Iteration 123/1000 | Loss: 0.00003232
Iteration 124/1000 | Loss: 0.00003232
Iteration 125/1000 | Loss: 0.00003232
Iteration 126/1000 | Loss: 0.00003232
Iteration 127/1000 | Loss: 0.00003232
Iteration 128/1000 | Loss: 0.00003232
Iteration 129/1000 | Loss: 0.00003231
Iteration 130/1000 | Loss: 0.00003231
Iteration 131/1000 | Loss: 0.00003231
Iteration 132/1000 | Loss: 0.00003231
Iteration 133/1000 | Loss: 0.00003231
Iteration 134/1000 | Loss: 0.00003231
Iteration 135/1000 | Loss: 0.00003231
Iteration 136/1000 | Loss: 0.00003231
Iteration 137/1000 | Loss: 0.00003231
Iteration 138/1000 | Loss: 0.00003231
Iteration 139/1000 | Loss: 0.00003231
Iteration 140/1000 | Loss: 0.00003231
Iteration 141/1000 | Loss: 0.00003231
Iteration 142/1000 | Loss: 0.00003231
Iteration 143/1000 | Loss: 0.00003231
Iteration 144/1000 | Loss: 0.00003231
Iteration 145/1000 | Loss: 0.00003230
Iteration 146/1000 | Loss: 0.00003230
Iteration 147/1000 | Loss: 0.00003230
Iteration 148/1000 | Loss: 0.00003230
Iteration 149/1000 | Loss: 0.00003230
Iteration 150/1000 | Loss: 0.00003230
Iteration 151/1000 | Loss: 0.00003230
Iteration 152/1000 | Loss: 0.00003230
Iteration 153/1000 | Loss: 0.00003230
Iteration 154/1000 | Loss: 0.00003230
Iteration 155/1000 | Loss: 0.00003230
Iteration 156/1000 | Loss: 0.00003230
Iteration 157/1000 | Loss: 0.00003230
Iteration 158/1000 | Loss: 0.00003230
Iteration 159/1000 | Loss: 0.00003230
Iteration 160/1000 | Loss: 0.00003230
Iteration 161/1000 | Loss: 0.00003230
Iteration 162/1000 | Loss: 0.00003230
Iteration 163/1000 | Loss: 0.00003230
Iteration 164/1000 | Loss: 0.00003230
Iteration 165/1000 | Loss: 0.00003230
Iteration 166/1000 | Loss: 0.00003230
Iteration 167/1000 | Loss: 0.00003230
Iteration 168/1000 | Loss: 0.00003230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [3.230402944609523e-05, 3.230402944609523e-05, 3.230402944609523e-05, 3.230402944609523e-05, 3.230402944609523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.230402944609523e-05

Optimization complete. Final v2v error: 4.765172958374023 mm

Highest mean error: 5.328925609588623 mm for frame 95

Lowest mean error: 3.8817028999328613 mm for frame 48

Saving results

Total time: 54.26472473144531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878084
Iteration 2/25 | Loss: 0.00160940
Iteration 3/25 | Loss: 0.00135788
Iteration 4/25 | Loss: 0.00131702
Iteration 5/25 | Loss: 0.00131979
Iteration 6/25 | Loss: 0.00131794
Iteration 7/25 | Loss: 0.00129733
Iteration 8/25 | Loss: 0.00127166
Iteration 9/25 | Loss: 0.00126356
Iteration 10/25 | Loss: 0.00126876
Iteration 11/25 | Loss: 0.00125969
Iteration 12/25 | Loss: 0.00125647
Iteration 13/25 | Loss: 0.00126117
Iteration 14/25 | Loss: 0.00126231
Iteration 15/25 | Loss: 0.00125566
Iteration 16/25 | Loss: 0.00125012
Iteration 17/25 | Loss: 0.00124619
Iteration 18/25 | Loss: 0.00124529
Iteration 19/25 | Loss: 0.00124494
Iteration 20/25 | Loss: 0.00124485
Iteration 21/25 | Loss: 0.00124485
Iteration 22/25 | Loss: 0.00124485
Iteration 23/25 | Loss: 0.00124485
Iteration 24/25 | Loss: 0.00124485
Iteration 25/25 | Loss: 0.00124485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.23020601
Iteration 2/25 | Loss: 0.00153125
Iteration 3/25 | Loss: 0.00153124
Iteration 4/25 | Loss: 0.00153124
Iteration 5/25 | Loss: 0.00153124
Iteration 6/25 | Loss: 0.00153124
Iteration 7/25 | Loss: 0.00153124
Iteration 8/25 | Loss: 0.00153124
Iteration 9/25 | Loss: 0.00153124
Iteration 10/25 | Loss: 0.00153123
Iteration 11/25 | Loss: 0.00153123
Iteration 12/25 | Loss: 0.00153123
Iteration 13/25 | Loss: 0.00153123
Iteration 14/25 | Loss: 0.00153123
Iteration 15/25 | Loss: 0.00153123
Iteration 16/25 | Loss: 0.00153123
Iteration 17/25 | Loss: 0.00153123
Iteration 18/25 | Loss: 0.00153123
Iteration 19/25 | Loss: 0.00153123
Iteration 20/25 | Loss: 0.00153123
Iteration 21/25 | Loss: 0.00153123
Iteration 22/25 | Loss: 0.00153123
Iteration 23/25 | Loss: 0.00153123
Iteration 24/25 | Loss: 0.00153123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015312337782233953, 0.0015312337782233953, 0.0015312337782233953, 0.0015312337782233953, 0.0015312337782233953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015312337782233953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153123
Iteration 2/1000 | Loss: 0.00014017
Iteration 3/1000 | Loss: 0.00026140
Iteration 4/1000 | Loss: 0.00004177
Iteration 5/1000 | Loss: 0.00003065
Iteration 6/1000 | Loss: 0.00002562
Iteration 7/1000 | Loss: 0.00002410
Iteration 8/1000 | Loss: 0.00002323
Iteration 9/1000 | Loss: 0.00002222
Iteration 10/1000 | Loss: 0.00002155
Iteration 11/1000 | Loss: 0.00002099
Iteration 12/1000 | Loss: 0.00002073
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002017
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001975
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001959
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001944
Iteration 24/1000 | Loss: 0.00001943
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001942
Iteration 27/1000 | Loss: 0.00001942
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001940
Iteration 31/1000 | Loss: 0.00001939
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001926
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001925
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001919
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001916
Iteration 77/1000 | Loss: 0.00001916
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001914
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001913
Iteration 96/1000 | Loss: 0.00001913
Iteration 97/1000 | Loss: 0.00001913
Iteration 98/1000 | Loss: 0.00001913
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001910
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001909
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001908
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001906
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001905
Iteration 131/1000 | Loss: 0.00001905
Iteration 132/1000 | Loss: 0.00001905
Iteration 133/1000 | Loss: 0.00001905
Iteration 134/1000 | Loss: 0.00001905
Iteration 135/1000 | Loss: 0.00001905
Iteration 136/1000 | Loss: 0.00001904
Iteration 137/1000 | Loss: 0.00001904
Iteration 138/1000 | Loss: 0.00001904
Iteration 139/1000 | Loss: 0.00001904
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001904
Iteration 142/1000 | Loss: 0.00001904
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001902
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001901
Iteration 156/1000 | Loss: 0.00001901
Iteration 157/1000 | Loss: 0.00001901
Iteration 158/1000 | Loss: 0.00001901
Iteration 159/1000 | Loss: 0.00001901
Iteration 160/1000 | Loss: 0.00001901
Iteration 161/1000 | Loss: 0.00001901
Iteration 162/1000 | Loss: 0.00001901
Iteration 163/1000 | Loss: 0.00001901
Iteration 164/1000 | Loss: 0.00001901
Iteration 165/1000 | Loss: 0.00001901
Iteration 166/1000 | Loss: 0.00001900
Iteration 167/1000 | Loss: 0.00001900
Iteration 168/1000 | Loss: 0.00001900
Iteration 169/1000 | Loss: 0.00001900
Iteration 170/1000 | Loss: 0.00001899
Iteration 171/1000 | Loss: 0.00001899
Iteration 172/1000 | Loss: 0.00001899
Iteration 173/1000 | Loss: 0.00001899
Iteration 174/1000 | Loss: 0.00001899
Iteration 175/1000 | Loss: 0.00001899
Iteration 176/1000 | Loss: 0.00001899
Iteration 177/1000 | Loss: 0.00001899
Iteration 178/1000 | Loss: 0.00001899
Iteration 179/1000 | Loss: 0.00001899
Iteration 180/1000 | Loss: 0.00001899
Iteration 181/1000 | Loss: 0.00001899
Iteration 182/1000 | Loss: 0.00001899
Iteration 183/1000 | Loss: 0.00001899
Iteration 184/1000 | Loss: 0.00001899
Iteration 185/1000 | Loss: 0.00001899
Iteration 186/1000 | Loss: 0.00001899
Iteration 187/1000 | Loss: 0.00001899
Iteration 188/1000 | Loss: 0.00001899
Iteration 189/1000 | Loss: 0.00001899
Iteration 190/1000 | Loss: 0.00001899
Iteration 191/1000 | Loss: 0.00001898
Iteration 192/1000 | Loss: 0.00001898
Iteration 193/1000 | Loss: 0.00001898
Iteration 194/1000 | Loss: 0.00001898
Iteration 195/1000 | Loss: 0.00001898
Iteration 196/1000 | Loss: 0.00001898
Iteration 197/1000 | Loss: 0.00001898
Iteration 198/1000 | Loss: 0.00001898
Iteration 199/1000 | Loss: 0.00001898
Iteration 200/1000 | Loss: 0.00001898
Iteration 201/1000 | Loss: 0.00001898
Iteration 202/1000 | Loss: 0.00001898
Iteration 203/1000 | Loss: 0.00001898
Iteration 204/1000 | Loss: 0.00001898
Iteration 205/1000 | Loss: 0.00001898
Iteration 206/1000 | Loss: 0.00001898
Iteration 207/1000 | Loss: 0.00001898
Iteration 208/1000 | Loss: 0.00001898
Iteration 209/1000 | Loss: 0.00001898
Iteration 210/1000 | Loss: 0.00001898
Iteration 211/1000 | Loss: 0.00001898
Iteration 212/1000 | Loss: 0.00001898
Iteration 213/1000 | Loss: 0.00001898
Iteration 214/1000 | Loss: 0.00001898
Iteration 215/1000 | Loss: 0.00001898
Iteration 216/1000 | Loss: 0.00001898
Iteration 217/1000 | Loss: 0.00001898
Iteration 218/1000 | Loss: 0.00001898
Iteration 219/1000 | Loss: 0.00001898
Iteration 220/1000 | Loss: 0.00001898
Iteration 221/1000 | Loss: 0.00001898
Iteration 222/1000 | Loss: 0.00001898
Iteration 223/1000 | Loss: 0.00001898
Iteration 224/1000 | Loss: 0.00001898
Iteration 225/1000 | Loss: 0.00001898
Iteration 226/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.8984417692990974e-05, 1.8984417692990974e-05, 1.8984417692990974e-05, 1.8984417692990974e-05, 1.8984417692990974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8984417692990974e-05

Optimization complete. Final v2v error: 3.6297354698181152 mm

Highest mean error: 5.632394790649414 mm for frame 94

Lowest mean error: 3.08784556388855 mm for frame 68

Saving results

Total time: 79.27815365791321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035726
Iteration 2/25 | Loss: 0.01035726
Iteration 3/25 | Loss: 0.01035726
Iteration 4/25 | Loss: 0.01035726
Iteration 5/25 | Loss: 0.01035726
Iteration 6/25 | Loss: 0.01035726
Iteration 7/25 | Loss: 0.01035726
Iteration 8/25 | Loss: 0.01035726
Iteration 9/25 | Loss: 0.01035726
Iteration 10/25 | Loss: 0.01035725
Iteration 11/25 | Loss: 0.01035725
Iteration 12/25 | Loss: 0.01035725
Iteration 13/25 | Loss: 0.01035725
Iteration 14/25 | Loss: 0.01035725
Iteration 15/25 | Loss: 0.01035725
Iteration 16/25 | Loss: 0.01035725
Iteration 17/25 | Loss: 0.01035725
Iteration 18/25 | Loss: 0.01035725
Iteration 19/25 | Loss: 0.01035725
Iteration 20/25 | Loss: 0.01035724
Iteration 21/25 | Loss: 0.01035724
Iteration 22/25 | Loss: 0.01035724
Iteration 23/25 | Loss: 0.01035724
Iteration 24/25 | Loss: 0.01035724
Iteration 25/25 | Loss: 0.01035724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44455409
Iteration 2/25 | Loss: 0.10120570
Iteration 3/25 | Loss: 0.10116418
Iteration 4/25 | Loss: 0.10106336
Iteration 5/25 | Loss: 0.10106334
Iteration 6/25 | Loss: 0.10106333
Iteration 7/25 | Loss: 0.10106332
Iteration 8/25 | Loss: 0.10106331
Iteration 9/25 | Loss: 0.10106331
Iteration 10/25 | Loss: 0.10106331
Iteration 11/25 | Loss: 0.10106331
Iteration 12/25 | Loss: 0.10106331
Iteration 13/25 | Loss: 0.10106331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.1010633111000061, 0.1010633111000061, 0.1010633111000061, 0.1010633111000061, 0.1010633111000061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1010633111000061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10106330
Iteration 2/1000 | Loss: 0.00052826
Iteration 3/1000 | Loss: 0.00028708
Iteration 4/1000 | Loss: 0.00007729
Iteration 5/1000 | Loss: 0.00021660
Iteration 6/1000 | Loss: 0.00029334
Iteration 7/1000 | Loss: 0.00006726
Iteration 8/1000 | Loss: 0.00026522
Iteration 9/1000 | Loss: 0.00004907
Iteration 10/1000 | Loss: 0.00012002
Iteration 11/1000 | Loss: 0.00007785
Iteration 12/1000 | Loss: 0.00002828
Iteration 13/1000 | Loss: 0.00004210
Iteration 14/1000 | Loss: 0.00010093
Iteration 15/1000 | Loss: 0.00001859
Iteration 16/1000 | Loss: 0.00001746
Iteration 17/1000 | Loss: 0.00005733
Iteration 18/1000 | Loss: 0.00006842
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001547
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00006055
Iteration 24/1000 | Loss: 0.00001424
Iteration 25/1000 | Loss: 0.00003420
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00003067
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00005241
Iteration 35/1000 | Loss: 0.00003273
Iteration 36/1000 | Loss: 0.00005888
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00003510
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001056
Iteration 45/1000 | Loss: 0.00001056
Iteration 46/1000 | Loss: 0.00001054
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001047
Iteration 49/1000 | Loss: 0.00001047
Iteration 50/1000 | Loss: 0.00001045
Iteration 51/1000 | Loss: 0.00001045
Iteration 52/1000 | Loss: 0.00001044
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001039
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001038
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001037
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001034
Iteration 84/1000 | Loss: 0.00001033
Iteration 85/1000 | Loss: 0.00001033
Iteration 86/1000 | Loss: 0.00001033
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001033
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001032
Iteration 92/1000 | Loss: 0.00001032
Iteration 93/1000 | Loss: 0.00001032
Iteration 94/1000 | Loss: 0.00001032
Iteration 95/1000 | Loss: 0.00001031
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001030
Iteration 98/1000 | Loss: 0.00001030
Iteration 99/1000 | Loss: 0.00001030
Iteration 100/1000 | Loss: 0.00001030
Iteration 101/1000 | Loss: 0.00001030
Iteration 102/1000 | Loss: 0.00001030
Iteration 103/1000 | Loss: 0.00001029
Iteration 104/1000 | Loss: 0.00001029
Iteration 105/1000 | Loss: 0.00001029
Iteration 106/1000 | Loss: 0.00001029
Iteration 107/1000 | Loss: 0.00001028
Iteration 108/1000 | Loss: 0.00001028
Iteration 109/1000 | Loss: 0.00001028
Iteration 110/1000 | Loss: 0.00001027
Iteration 111/1000 | Loss: 0.00001027
Iteration 112/1000 | Loss: 0.00001027
Iteration 113/1000 | Loss: 0.00001027
Iteration 114/1000 | Loss: 0.00001027
Iteration 115/1000 | Loss: 0.00001027
Iteration 116/1000 | Loss: 0.00001026
Iteration 117/1000 | Loss: 0.00001026
Iteration 118/1000 | Loss: 0.00001026
Iteration 119/1000 | Loss: 0.00001026
Iteration 120/1000 | Loss: 0.00001026
Iteration 121/1000 | Loss: 0.00001026
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001026
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001026
Iteration 126/1000 | Loss: 0.00001026
Iteration 127/1000 | Loss: 0.00001025
Iteration 128/1000 | Loss: 0.00001025
Iteration 129/1000 | Loss: 0.00001025
Iteration 130/1000 | Loss: 0.00001024
Iteration 131/1000 | Loss: 0.00001024
Iteration 132/1000 | Loss: 0.00001024
Iteration 133/1000 | Loss: 0.00001024
Iteration 134/1000 | Loss: 0.00001024
Iteration 135/1000 | Loss: 0.00001023
Iteration 136/1000 | Loss: 0.00001023
Iteration 137/1000 | Loss: 0.00001023
Iteration 138/1000 | Loss: 0.00001023
Iteration 139/1000 | Loss: 0.00001023
Iteration 140/1000 | Loss: 0.00001023
Iteration 141/1000 | Loss: 0.00001023
Iteration 142/1000 | Loss: 0.00001022
Iteration 143/1000 | Loss: 0.00001022
Iteration 144/1000 | Loss: 0.00001022
Iteration 145/1000 | Loss: 0.00001022
Iteration 146/1000 | Loss: 0.00001022
Iteration 147/1000 | Loss: 0.00001021
Iteration 148/1000 | Loss: 0.00001021
Iteration 149/1000 | Loss: 0.00001021
Iteration 150/1000 | Loss: 0.00001021
Iteration 151/1000 | Loss: 0.00001021
Iteration 152/1000 | Loss: 0.00001021
Iteration 153/1000 | Loss: 0.00001021
Iteration 154/1000 | Loss: 0.00001020
Iteration 155/1000 | Loss: 0.00001020
Iteration 156/1000 | Loss: 0.00001020
Iteration 157/1000 | Loss: 0.00004385
Iteration 158/1000 | Loss: 0.00001028
Iteration 159/1000 | Loss: 0.00001020
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001018
Iteration 165/1000 | Loss: 0.00001018
Iteration 166/1000 | Loss: 0.00001018
Iteration 167/1000 | Loss: 0.00001018
Iteration 168/1000 | Loss: 0.00001018
Iteration 169/1000 | Loss: 0.00001018
Iteration 170/1000 | Loss: 0.00001018
Iteration 171/1000 | Loss: 0.00001018
Iteration 172/1000 | Loss: 0.00001018
Iteration 173/1000 | Loss: 0.00001017
Iteration 174/1000 | Loss: 0.00001017
Iteration 175/1000 | Loss: 0.00001017
Iteration 176/1000 | Loss: 0.00001017
Iteration 177/1000 | Loss: 0.00001017
Iteration 178/1000 | Loss: 0.00001017
Iteration 179/1000 | Loss: 0.00001017
Iteration 180/1000 | Loss: 0.00001017
Iteration 181/1000 | Loss: 0.00001017
Iteration 182/1000 | Loss: 0.00001017
Iteration 183/1000 | Loss: 0.00001017
Iteration 184/1000 | Loss: 0.00001017
Iteration 185/1000 | Loss: 0.00001017
Iteration 186/1000 | Loss: 0.00001017
Iteration 187/1000 | Loss: 0.00001017
Iteration 188/1000 | Loss: 0.00001017
Iteration 189/1000 | Loss: 0.00001017
Iteration 190/1000 | Loss: 0.00001017
Iteration 191/1000 | Loss: 0.00001017
Iteration 192/1000 | Loss: 0.00001017
Iteration 193/1000 | Loss: 0.00001017
Iteration 194/1000 | Loss: 0.00001017
Iteration 195/1000 | Loss: 0.00001017
Iteration 196/1000 | Loss: 0.00001017
Iteration 197/1000 | Loss: 0.00001017
Iteration 198/1000 | Loss: 0.00001017
Iteration 199/1000 | Loss: 0.00001017
Iteration 200/1000 | Loss: 0.00001017
Iteration 201/1000 | Loss: 0.00001017
Iteration 202/1000 | Loss: 0.00001017
Iteration 203/1000 | Loss: 0.00001017
Iteration 204/1000 | Loss: 0.00001017
Iteration 205/1000 | Loss: 0.00001017
Iteration 206/1000 | Loss: 0.00001017
Iteration 207/1000 | Loss: 0.00001017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0166983884118963e-05, 1.0166983884118963e-05, 1.0166983884118963e-05, 1.0166983884118963e-05, 1.0166983884118963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0166983884118963e-05

Optimization complete. Final v2v error: 2.722710371017456 mm

Highest mean error: 9.176751136779785 mm for frame 67

Lowest mean error: 2.5671920776367188 mm for frame 6

Saving results

Total time: 92.71651601791382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412040
Iteration 2/25 | Loss: 0.00131432
Iteration 3/25 | Loss: 0.00121441
Iteration 4/25 | Loss: 0.00119840
Iteration 5/25 | Loss: 0.00119407
Iteration 6/25 | Loss: 0.00119294
Iteration 7/25 | Loss: 0.00119260
Iteration 8/25 | Loss: 0.00119257
Iteration 9/25 | Loss: 0.00119257
Iteration 10/25 | Loss: 0.00119257
Iteration 11/25 | Loss: 0.00119257
Iteration 12/25 | Loss: 0.00119257
Iteration 13/25 | Loss: 0.00119257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011925705475732684, 0.0011925705475732684, 0.0011925705475732684, 0.0011925705475732684, 0.0011925705475732684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011925705475732684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34530282
Iteration 2/25 | Loss: 0.00123862
Iteration 3/25 | Loss: 0.00123861
Iteration 4/25 | Loss: 0.00123861
Iteration 5/25 | Loss: 0.00123861
Iteration 6/25 | Loss: 0.00123861
Iteration 7/25 | Loss: 0.00123861
Iteration 8/25 | Loss: 0.00123861
Iteration 9/25 | Loss: 0.00123861
Iteration 10/25 | Loss: 0.00123861
Iteration 11/25 | Loss: 0.00123861
Iteration 12/25 | Loss: 0.00123861
Iteration 13/25 | Loss: 0.00123861
Iteration 14/25 | Loss: 0.00123861
Iteration 15/25 | Loss: 0.00123861
Iteration 16/25 | Loss: 0.00123861
Iteration 17/25 | Loss: 0.00123861
Iteration 18/25 | Loss: 0.00123861
Iteration 19/25 | Loss: 0.00123861
Iteration 20/25 | Loss: 0.00123861
Iteration 21/25 | Loss: 0.00123861
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012386091984808445, 0.0012386091984808445, 0.0012386091984808445, 0.0012386091984808445, 0.0012386091984808445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012386091984808445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123861
Iteration 2/1000 | Loss: 0.00003646
Iteration 3/1000 | Loss: 0.00002428
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001428
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001351
Iteration 16/1000 | Loss: 0.00001349
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001335
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001331
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001329
Iteration 27/1000 | Loss: 0.00001329
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001329
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001328
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001324
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001323
Iteration 41/1000 | Loss: 0.00001323
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001323
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001322
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001321
Iteration 53/1000 | Loss: 0.00001321
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001320
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001319
Iteration 63/1000 | Loss: 0.00001319
Iteration 64/1000 | Loss: 0.00001319
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001319
Iteration 67/1000 | Loss: 0.00001319
Iteration 68/1000 | Loss: 0.00001319
Iteration 69/1000 | Loss: 0.00001319
Iteration 70/1000 | Loss: 0.00001319
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001319
Iteration 74/1000 | Loss: 0.00001319
Iteration 75/1000 | Loss: 0.00001319
Iteration 76/1000 | Loss: 0.00001319
Iteration 77/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.3187536751502194e-05, 1.3187536751502194e-05, 1.3187536751502194e-05, 1.3187536751502194e-05, 1.3187536751502194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3187536751502194e-05

Optimization complete. Final v2v error: 3.0455234050750732 mm

Highest mean error: 4.350017547607422 mm for frame 54

Lowest mean error: 2.529528856277466 mm for frame 112

Saving results

Total time: 35.06525230407715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784862
Iteration 2/25 | Loss: 0.00138753
Iteration 3/25 | Loss: 0.00122259
Iteration 4/25 | Loss: 0.00120084
Iteration 5/25 | Loss: 0.00119687
Iteration 6/25 | Loss: 0.00119590
Iteration 7/25 | Loss: 0.00119590
Iteration 8/25 | Loss: 0.00119590
Iteration 9/25 | Loss: 0.00119590
Iteration 10/25 | Loss: 0.00119590
Iteration 11/25 | Loss: 0.00119590
Iteration 12/25 | Loss: 0.00119590
Iteration 13/25 | Loss: 0.00119590
Iteration 14/25 | Loss: 0.00119590
Iteration 15/25 | Loss: 0.00119590
Iteration 16/25 | Loss: 0.00119590
Iteration 17/25 | Loss: 0.00119590
Iteration 18/25 | Loss: 0.00119590
Iteration 19/25 | Loss: 0.00119590
Iteration 20/25 | Loss: 0.00119590
Iteration 21/25 | Loss: 0.00119590
Iteration 22/25 | Loss: 0.00119590
Iteration 23/25 | Loss: 0.00119590
Iteration 24/25 | Loss: 0.00119590
Iteration 25/25 | Loss: 0.00119590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28874552
Iteration 2/25 | Loss: 0.00134509
Iteration 3/25 | Loss: 0.00134509
Iteration 4/25 | Loss: 0.00134508
Iteration 5/25 | Loss: 0.00134508
Iteration 6/25 | Loss: 0.00134508
Iteration 7/25 | Loss: 0.00134508
Iteration 8/25 | Loss: 0.00134508
Iteration 9/25 | Loss: 0.00134508
Iteration 10/25 | Loss: 0.00134508
Iteration 11/25 | Loss: 0.00134508
Iteration 12/25 | Loss: 0.00134508
Iteration 13/25 | Loss: 0.00134508
Iteration 14/25 | Loss: 0.00134508
Iteration 15/25 | Loss: 0.00134508
Iteration 16/25 | Loss: 0.00134508
Iteration 17/25 | Loss: 0.00134508
Iteration 18/25 | Loss: 0.00134508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013450821861624718, 0.0013450821861624718, 0.0013450821861624718, 0.0013450821861624718, 0.0013450821861624718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013450821861624718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134508
Iteration 2/1000 | Loss: 0.00003521
Iteration 3/1000 | Loss: 0.00002575
Iteration 4/1000 | Loss: 0.00002039
Iteration 5/1000 | Loss: 0.00001902
Iteration 6/1000 | Loss: 0.00001809
Iteration 7/1000 | Loss: 0.00001714
Iteration 8/1000 | Loss: 0.00001653
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001514
Iteration 13/1000 | Loss: 0.00001498
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001472
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001470
Iteration 31/1000 | Loss: 0.00001470
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001469
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001465
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001461
Iteration 55/1000 | Loss: 0.00001461
Iteration 56/1000 | Loss: 0.00001461
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001460
Iteration 60/1000 | Loss: 0.00001460
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001459
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001457
Iteration 67/1000 | Loss: 0.00001457
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001446
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001446
Iteration 127/1000 | Loss: 0.00001446
Iteration 128/1000 | Loss: 0.00001445
Iteration 129/1000 | Loss: 0.00001445
Iteration 130/1000 | Loss: 0.00001445
Iteration 131/1000 | Loss: 0.00001445
Iteration 132/1000 | Loss: 0.00001445
Iteration 133/1000 | Loss: 0.00001445
Iteration 134/1000 | Loss: 0.00001445
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001444
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001440
Iteration 159/1000 | Loss: 0.00001440
Iteration 160/1000 | Loss: 0.00001440
Iteration 161/1000 | Loss: 0.00001440
Iteration 162/1000 | Loss: 0.00001440
Iteration 163/1000 | Loss: 0.00001440
Iteration 164/1000 | Loss: 0.00001440
Iteration 165/1000 | Loss: 0.00001440
Iteration 166/1000 | Loss: 0.00001440
Iteration 167/1000 | Loss: 0.00001440
Iteration 168/1000 | Loss: 0.00001440
Iteration 169/1000 | Loss: 0.00001440
Iteration 170/1000 | Loss: 0.00001440
Iteration 171/1000 | Loss: 0.00001440
Iteration 172/1000 | Loss: 0.00001440
Iteration 173/1000 | Loss: 0.00001440
Iteration 174/1000 | Loss: 0.00001440
Iteration 175/1000 | Loss: 0.00001440
Iteration 176/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4401704902411439e-05, 1.4401704902411439e-05, 1.4401704902411439e-05, 1.4401704902411439e-05, 1.4401704902411439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4401704902411439e-05

Optimization complete. Final v2v error: 3.2100415229797363 mm

Highest mean error: 3.92435359954834 mm for frame 89

Lowest mean error: 2.804116725921631 mm for frame 57

Saving results

Total time: 39.70581674575806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003267
Iteration 2/25 | Loss: 0.00286548
Iteration 3/25 | Loss: 0.00206386
Iteration 4/25 | Loss: 0.00199346
Iteration 5/25 | Loss: 0.00191245
Iteration 6/25 | Loss: 0.00156772
Iteration 7/25 | Loss: 0.00136683
Iteration 8/25 | Loss: 0.00126717
Iteration 9/25 | Loss: 0.00121695
Iteration 10/25 | Loss: 0.00120769
Iteration 11/25 | Loss: 0.00120146
Iteration 12/25 | Loss: 0.00119797
Iteration 13/25 | Loss: 0.00121005
Iteration 14/25 | Loss: 0.00121004
Iteration 15/25 | Loss: 0.00120980
Iteration 16/25 | Loss: 0.00120861
Iteration 17/25 | Loss: 0.00120495
Iteration 18/25 | Loss: 0.00119431
Iteration 19/25 | Loss: 0.00119317
Iteration 20/25 | Loss: 0.00119303
Iteration 21/25 | Loss: 0.00119303
Iteration 22/25 | Loss: 0.00119303
Iteration 23/25 | Loss: 0.00119303
Iteration 24/25 | Loss: 0.00119303
Iteration 25/25 | Loss: 0.00119302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31711376
Iteration 2/25 | Loss: 0.00155012
Iteration 3/25 | Loss: 0.00128315
Iteration 4/25 | Loss: 0.00128315
Iteration 5/25 | Loss: 0.00128314
Iteration 6/25 | Loss: 0.00128314
Iteration 7/25 | Loss: 0.00128314
Iteration 8/25 | Loss: 0.00128314
Iteration 9/25 | Loss: 0.00128314
Iteration 10/25 | Loss: 0.00128314
Iteration 11/25 | Loss: 0.00128314
Iteration 12/25 | Loss: 0.00128314
Iteration 13/25 | Loss: 0.00128314
Iteration 14/25 | Loss: 0.00128314
Iteration 15/25 | Loss: 0.00128314
Iteration 16/25 | Loss: 0.00128314
Iteration 17/25 | Loss: 0.00128314
Iteration 18/25 | Loss: 0.00128314
Iteration 19/25 | Loss: 0.00128314
Iteration 20/25 | Loss: 0.00128314
Iteration 21/25 | Loss: 0.00128314
Iteration 22/25 | Loss: 0.00128314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012831423664465547, 0.0012831423664465547, 0.0012831423664465547, 0.0012831423664465547, 0.0012831423664465547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012831423664465547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128314
Iteration 2/1000 | Loss: 0.00025848
Iteration 3/1000 | Loss: 0.00003068
Iteration 4/1000 | Loss: 0.00002627
Iteration 5/1000 | Loss: 0.00002470
Iteration 6/1000 | Loss: 0.00002364
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002210
Iteration 9/1000 | Loss: 0.00002157
Iteration 10/1000 | Loss: 0.00002112
Iteration 11/1000 | Loss: 0.00149837
Iteration 12/1000 | Loss: 0.00096311
Iteration 13/1000 | Loss: 0.00156331
Iteration 14/1000 | Loss: 0.00047981
Iteration 15/1000 | Loss: 0.00009525
Iteration 16/1000 | Loss: 0.00003375
Iteration 17/1000 | Loss: 0.00002725
Iteration 18/1000 | Loss: 0.00002302
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00013907
Iteration 21/1000 | Loss: 0.00002007
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001537
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001172
Iteration 30/1000 | Loss: 0.00001146
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001114
Iteration 42/1000 | Loss: 0.00001109
Iteration 43/1000 | Loss: 0.00001109
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001104
Iteration 50/1000 | Loss: 0.00001104
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001103
Iteration 59/1000 | Loss: 0.00001103
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001102
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001101
Iteration 67/1000 | Loss: 0.00001101
Iteration 68/1000 | Loss: 0.00001101
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001095
Iteration 101/1000 | Loss: 0.00001095
Iteration 102/1000 | Loss: 0.00001095
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001094
Iteration 106/1000 | Loss: 0.00001094
Iteration 107/1000 | Loss: 0.00001094
Iteration 108/1000 | Loss: 0.00001094
Iteration 109/1000 | Loss: 0.00001094
Iteration 110/1000 | Loss: 0.00001094
Iteration 111/1000 | Loss: 0.00001093
Iteration 112/1000 | Loss: 0.00001093
Iteration 113/1000 | Loss: 0.00001093
Iteration 114/1000 | Loss: 0.00001093
Iteration 115/1000 | Loss: 0.00001093
Iteration 116/1000 | Loss: 0.00001093
Iteration 117/1000 | Loss: 0.00001093
Iteration 118/1000 | Loss: 0.00001092
Iteration 119/1000 | Loss: 0.00001092
Iteration 120/1000 | Loss: 0.00001092
Iteration 121/1000 | Loss: 0.00001092
Iteration 122/1000 | Loss: 0.00001092
Iteration 123/1000 | Loss: 0.00001092
Iteration 124/1000 | Loss: 0.00001092
Iteration 125/1000 | Loss: 0.00001092
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001092
Iteration 134/1000 | Loss: 0.00001091
Iteration 135/1000 | Loss: 0.00001091
Iteration 136/1000 | Loss: 0.00001091
Iteration 137/1000 | Loss: 0.00001091
Iteration 138/1000 | Loss: 0.00001091
Iteration 139/1000 | Loss: 0.00001091
Iteration 140/1000 | Loss: 0.00001091
Iteration 141/1000 | Loss: 0.00001090
Iteration 142/1000 | Loss: 0.00001090
Iteration 143/1000 | Loss: 0.00001090
Iteration 144/1000 | Loss: 0.00001090
Iteration 145/1000 | Loss: 0.00001090
Iteration 146/1000 | Loss: 0.00001090
Iteration 147/1000 | Loss: 0.00001090
Iteration 148/1000 | Loss: 0.00001090
Iteration 149/1000 | Loss: 0.00001090
Iteration 150/1000 | Loss: 0.00001090
Iteration 151/1000 | Loss: 0.00001090
Iteration 152/1000 | Loss: 0.00001090
Iteration 153/1000 | Loss: 0.00001090
Iteration 154/1000 | Loss: 0.00001090
Iteration 155/1000 | Loss: 0.00001089
Iteration 156/1000 | Loss: 0.00001089
Iteration 157/1000 | Loss: 0.00001089
Iteration 158/1000 | Loss: 0.00001089
Iteration 159/1000 | Loss: 0.00001089
Iteration 160/1000 | Loss: 0.00001089
Iteration 161/1000 | Loss: 0.00001089
Iteration 162/1000 | Loss: 0.00001089
Iteration 163/1000 | Loss: 0.00001089
Iteration 164/1000 | Loss: 0.00001089
Iteration 165/1000 | Loss: 0.00001089
Iteration 166/1000 | Loss: 0.00001088
Iteration 167/1000 | Loss: 0.00001088
Iteration 168/1000 | Loss: 0.00001088
Iteration 169/1000 | Loss: 0.00001088
Iteration 170/1000 | Loss: 0.00001088
Iteration 171/1000 | Loss: 0.00001088
Iteration 172/1000 | Loss: 0.00001088
Iteration 173/1000 | Loss: 0.00001088
Iteration 174/1000 | Loss: 0.00001088
Iteration 175/1000 | Loss: 0.00001088
Iteration 176/1000 | Loss: 0.00001088
Iteration 177/1000 | Loss: 0.00001088
Iteration 178/1000 | Loss: 0.00001088
Iteration 179/1000 | Loss: 0.00001088
Iteration 180/1000 | Loss: 0.00001088
Iteration 181/1000 | Loss: 0.00001088
Iteration 182/1000 | Loss: 0.00001088
Iteration 183/1000 | Loss: 0.00001088
Iteration 184/1000 | Loss: 0.00001088
Iteration 185/1000 | Loss: 0.00001088
Iteration 186/1000 | Loss: 0.00001088
Iteration 187/1000 | Loss: 0.00001088
Iteration 188/1000 | Loss: 0.00001088
Iteration 189/1000 | Loss: 0.00001088
Iteration 190/1000 | Loss: 0.00001088
Iteration 191/1000 | Loss: 0.00001088
Iteration 192/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.0884624316531699e-05, 1.0884624316531699e-05, 1.0884624316531699e-05, 1.0884624316531699e-05, 1.0884624316531699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0884624316531699e-05

Optimization complete. Final v2v error: 2.826617479324341 mm

Highest mean error: 3.542924165725708 mm for frame 76

Lowest mean error: 2.628582715988159 mm for frame 113

Saving results

Total time: 88.1272234916687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038910
Iteration 2/25 | Loss: 0.00250813
Iteration 3/25 | Loss: 0.00161792
Iteration 4/25 | Loss: 0.00134222
Iteration 5/25 | Loss: 0.00129349
Iteration 6/25 | Loss: 0.00130283
Iteration 7/25 | Loss: 0.00129259
Iteration 8/25 | Loss: 0.00126968
Iteration 9/25 | Loss: 0.00126150
Iteration 10/25 | Loss: 0.00125027
Iteration 11/25 | Loss: 0.00125456
Iteration 12/25 | Loss: 0.00125321
Iteration 13/25 | Loss: 0.00124410
Iteration 14/25 | Loss: 0.00123369
Iteration 15/25 | Loss: 0.00122096
Iteration 16/25 | Loss: 0.00121266
Iteration 17/25 | Loss: 0.00121710
Iteration 18/25 | Loss: 0.00121427
Iteration 19/25 | Loss: 0.00121339
Iteration 20/25 | Loss: 0.00121108
Iteration 21/25 | Loss: 0.00121029
Iteration 22/25 | Loss: 0.00120886
Iteration 23/25 | Loss: 0.00120672
Iteration 24/25 | Loss: 0.00121509
Iteration 25/25 | Loss: 0.00120773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71767282
Iteration 2/25 | Loss: 0.00162194
Iteration 3/25 | Loss: 0.00162043
Iteration 4/25 | Loss: 0.00153516
Iteration 5/25 | Loss: 0.00153473
Iteration 6/25 | Loss: 0.00153473
Iteration 7/25 | Loss: 0.00153473
Iteration 8/25 | Loss: 0.00153473
Iteration 9/25 | Loss: 0.00153473
Iteration 10/25 | Loss: 0.00153473
Iteration 11/25 | Loss: 0.00153473
Iteration 12/25 | Loss: 0.00153473
Iteration 13/25 | Loss: 0.00153473
Iteration 14/25 | Loss: 0.00153473
Iteration 15/25 | Loss: 0.00153473
Iteration 16/25 | Loss: 0.00153473
Iteration 17/25 | Loss: 0.00153473
Iteration 18/25 | Loss: 0.00153473
Iteration 19/25 | Loss: 0.00153473
Iteration 20/25 | Loss: 0.00153473
Iteration 21/25 | Loss: 0.00153473
Iteration 22/25 | Loss: 0.00153473
Iteration 23/25 | Loss: 0.00153473
Iteration 24/25 | Loss: 0.00153473
Iteration 25/25 | Loss: 0.00153473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153473
Iteration 2/1000 | Loss: 0.00032964
Iteration 3/1000 | Loss: 0.00028137
Iteration 4/1000 | Loss: 0.00021345
Iteration 5/1000 | Loss: 0.00017889
Iteration 6/1000 | Loss: 0.00010044
Iteration 7/1000 | Loss: 0.00028376
Iteration 8/1000 | Loss: 0.00013277
Iteration 9/1000 | Loss: 0.00011209
Iteration 10/1000 | Loss: 0.00016124
Iteration 11/1000 | Loss: 0.00132487
Iteration 12/1000 | Loss: 0.00025825
Iteration 13/1000 | Loss: 0.00011760
Iteration 14/1000 | Loss: 0.00009933
Iteration 15/1000 | Loss: 0.00012507
Iteration 16/1000 | Loss: 0.00017454
Iteration 17/1000 | Loss: 0.00010573
Iteration 18/1000 | Loss: 0.00009194
Iteration 19/1000 | Loss: 0.00009705
Iteration 20/1000 | Loss: 0.00020692
Iteration 21/1000 | Loss: 0.00020666
Iteration 22/1000 | Loss: 0.00010622
Iteration 23/1000 | Loss: 0.00011314
Iteration 24/1000 | Loss: 0.00012277
Iteration 25/1000 | Loss: 0.00012377
Iteration 26/1000 | Loss: 0.00012624
Iteration 27/1000 | Loss: 0.00012708
Iteration 28/1000 | Loss: 0.00013028
Iteration 29/1000 | Loss: 0.00008220
Iteration 30/1000 | Loss: 0.00009066
Iteration 31/1000 | Loss: 0.00010993
Iteration 32/1000 | Loss: 0.00011633
Iteration 33/1000 | Loss: 0.00026045
Iteration 34/1000 | Loss: 0.00022046
Iteration 35/1000 | Loss: 0.00011454
Iteration 36/1000 | Loss: 0.00012209
Iteration 37/1000 | Loss: 0.00014007
Iteration 38/1000 | Loss: 0.00009655
Iteration 39/1000 | Loss: 0.00008873
Iteration 40/1000 | Loss: 0.00011306
Iteration 41/1000 | Loss: 0.00010681
Iteration 42/1000 | Loss: 0.00022044
Iteration 43/1000 | Loss: 0.00030017
Iteration 44/1000 | Loss: 0.00021878
Iteration 45/1000 | Loss: 0.00012308
Iteration 46/1000 | Loss: 0.00011979
Iteration 47/1000 | Loss: 0.00010037
Iteration 48/1000 | Loss: 0.00012999
Iteration 49/1000 | Loss: 0.00011414
Iteration 50/1000 | Loss: 0.00022223
Iteration 51/1000 | Loss: 0.00011909
Iteration 52/1000 | Loss: 0.00013457
Iteration 53/1000 | Loss: 0.00011354
Iteration 54/1000 | Loss: 0.00018376
Iteration 55/1000 | Loss: 0.00008804
Iteration 56/1000 | Loss: 0.00012037
Iteration 57/1000 | Loss: 0.00010574
Iteration 58/1000 | Loss: 0.00012102
Iteration 59/1000 | Loss: 0.00012479
Iteration 60/1000 | Loss: 0.00011505
Iteration 61/1000 | Loss: 0.00011635
Iteration 62/1000 | Loss: 0.00011607
Iteration 63/1000 | Loss: 0.00012807
Iteration 64/1000 | Loss: 0.00014542
Iteration 65/1000 | Loss: 0.00012124
Iteration 66/1000 | Loss: 0.00012727
Iteration 67/1000 | Loss: 0.00014116
Iteration 68/1000 | Loss: 0.00007018
Iteration 69/1000 | Loss: 0.00024961
Iteration 70/1000 | Loss: 0.00015598
Iteration 71/1000 | Loss: 0.00014317
Iteration 72/1000 | Loss: 0.00011559
Iteration 73/1000 | Loss: 0.00013160
Iteration 74/1000 | Loss: 0.00011055
Iteration 75/1000 | Loss: 0.00012931
Iteration 76/1000 | Loss: 0.00010896
Iteration 77/1000 | Loss: 0.00011633
Iteration 78/1000 | Loss: 0.00010361
Iteration 79/1000 | Loss: 0.00010921
Iteration 80/1000 | Loss: 0.00027910
Iteration 81/1000 | Loss: 0.00021418
Iteration 82/1000 | Loss: 0.00020960
Iteration 83/1000 | Loss: 0.00009357
Iteration 84/1000 | Loss: 0.00006366
Iteration 85/1000 | Loss: 0.00009945
Iteration 86/1000 | Loss: 0.00010577
Iteration 87/1000 | Loss: 0.00009935
Iteration 88/1000 | Loss: 0.00011189
Iteration 89/1000 | Loss: 0.00009997
Iteration 90/1000 | Loss: 0.00013375
Iteration 91/1000 | Loss: 0.00013973
Iteration 92/1000 | Loss: 0.00010451
Iteration 93/1000 | Loss: 0.00013602
Iteration 94/1000 | Loss: 0.00028461
Iteration 95/1000 | Loss: 0.00018181
Iteration 96/1000 | Loss: 0.00013148
Iteration 97/1000 | Loss: 0.00009842
Iteration 98/1000 | Loss: 0.00007592
Iteration 99/1000 | Loss: 0.00010798
Iteration 100/1000 | Loss: 0.00009594
Iteration 101/1000 | Loss: 0.00010591
Iteration 102/1000 | Loss: 0.00012591
Iteration 103/1000 | Loss: 0.00008732
Iteration 104/1000 | Loss: 0.00013400
Iteration 105/1000 | Loss: 0.00004787
Iteration 106/1000 | Loss: 0.00007010
Iteration 107/1000 | Loss: 0.00005405
Iteration 108/1000 | Loss: 0.00008370
Iteration 109/1000 | Loss: 0.00009246
Iteration 110/1000 | Loss: 0.00009252
Iteration 111/1000 | Loss: 0.00009700
Iteration 112/1000 | Loss: 0.00015679
Iteration 113/1000 | Loss: 0.00014094
Iteration 114/1000 | Loss: 0.00030608
Iteration 115/1000 | Loss: 0.00004922
Iteration 116/1000 | Loss: 0.00009980
Iteration 117/1000 | Loss: 0.00002668
Iteration 118/1000 | Loss: 0.00008355
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00003079
Iteration 121/1000 | Loss: 0.00013631
Iteration 122/1000 | Loss: 0.00033493
Iteration 123/1000 | Loss: 0.00032381
Iteration 124/1000 | Loss: 0.00021818
Iteration 125/1000 | Loss: 0.00021829
Iteration 126/1000 | Loss: 0.00015386
Iteration 127/1000 | Loss: 0.00003066
Iteration 128/1000 | Loss: 0.00001902
Iteration 129/1000 | Loss: 0.00007488
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00002236
Iteration 132/1000 | Loss: 0.00003998
Iteration 133/1000 | Loss: 0.00001379
Iteration 134/1000 | Loss: 0.00004394
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00006191
Iteration 137/1000 | Loss: 0.00001285
Iteration 138/1000 | Loss: 0.00001432
Iteration 139/1000 | Loss: 0.00004828
Iteration 140/1000 | Loss: 0.00003256
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001240
Iteration 143/1000 | Loss: 0.00001743
Iteration 144/1000 | Loss: 0.00001232
Iteration 145/1000 | Loss: 0.00001232
Iteration 146/1000 | Loss: 0.00001232
Iteration 147/1000 | Loss: 0.00001231
Iteration 148/1000 | Loss: 0.00001231
Iteration 149/1000 | Loss: 0.00001231
Iteration 150/1000 | Loss: 0.00001231
Iteration 151/1000 | Loss: 0.00001231
Iteration 152/1000 | Loss: 0.00001231
Iteration 153/1000 | Loss: 0.00001231
Iteration 154/1000 | Loss: 0.00001231
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001231
Iteration 157/1000 | Loss: 0.00001231
Iteration 158/1000 | Loss: 0.00001230
Iteration 159/1000 | Loss: 0.00001230
Iteration 160/1000 | Loss: 0.00001230
Iteration 161/1000 | Loss: 0.00001230
Iteration 162/1000 | Loss: 0.00001230
Iteration 163/1000 | Loss: 0.00001229
Iteration 164/1000 | Loss: 0.00001229
Iteration 165/1000 | Loss: 0.00001228
Iteration 166/1000 | Loss: 0.00001228
Iteration 167/1000 | Loss: 0.00001228
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001226
Iteration 170/1000 | Loss: 0.00001223
Iteration 171/1000 | Loss: 0.00001223
Iteration 172/1000 | Loss: 0.00001223
Iteration 173/1000 | Loss: 0.00004966
Iteration 174/1000 | Loss: 0.00002530
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001209
Iteration 178/1000 | Loss: 0.00001209
Iteration 179/1000 | Loss: 0.00001208
Iteration 180/1000 | Loss: 0.00001208
Iteration 181/1000 | Loss: 0.00001208
Iteration 182/1000 | Loss: 0.00001208
Iteration 183/1000 | Loss: 0.00001208
Iteration 184/1000 | Loss: 0.00001208
Iteration 185/1000 | Loss: 0.00001208
Iteration 186/1000 | Loss: 0.00001208
Iteration 187/1000 | Loss: 0.00001208
Iteration 188/1000 | Loss: 0.00001208
Iteration 189/1000 | Loss: 0.00001208
Iteration 190/1000 | Loss: 0.00001207
Iteration 191/1000 | Loss: 0.00001207
Iteration 192/1000 | Loss: 0.00001206
Iteration 193/1000 | Loss: 0.00001206
Iteration 194/1000 | Loss: 0.00001206
Iteration 195/1000 | Loss: 0.00001206
Iteration 196/1000 | Loss: 0.00001205
Iteration 197/1000 | Loss: 0.00001223
Iteration 198/1000 | Loss: 0.00001203
Iteration 199/1000 | Loss: 0.00001202
Iteration 200/1000 | Loss: 0.00001202
Iteration 201/1000 | Loss: 0.00001202
Iteration 202/1000 | Loss: 0.00001202
Iteration 203/1000 | Loss: 0.00001202
Iteration 204/1000 | Loss: 0.00001202
Iteration 205/1000 | Loss: 0.00001202
Iteration 206/1000 | Loss: 0.00001202
Iteration 207/1000 | Loss: 0.00001202
Iteration 208/1000 | Loss: 0.00001201
Iteration 209/1000 | Loss: 0.00001201
Iteration 210/1000 | Loss: 0.00001199
Iteration 211/1000 | Loss: 0.00001230
Iteration 212/1000 | Loss: 0.00004566
Iteration 213/1000 | Loss: 0.00002248
Iteration 214/1000 | Loss: 0.00004985
Iteration 215/1000 | Loss: 0.00001197
Iteration 216/1000 | Loss: 0.00001195
Iteration 217/1000 | Loss: 0.00001195
Iteration 218/1000 | Loss: 0.00001195
Iteration 219/1000 | Loss: 0.00001195
Iteration 220/1000 | Loss: 0.00001195
Iteration 221/1000 | Loss: 0.00001195
Iteration 222/1000 | Loss: 0.00001194
Iteration 223/1000 | Loss: 0.00001194
Iteration 224/1000 | Loss: 0.00001194
Iteration 225/1000 | Loss: 0.00002223
Iteration 226/1000 | Loss: 0.00003503
Iteration 227/1000 | Loss: 0.00001212
Iteration 228/1000 | Loss: 0.00001195
Iteration 229/1000 | Loss: 0.00001195
Iteration 230/1000 | Loss: 0.00001195
Iteration 231/1000 | Loss: 0.00001195
Iteration 232/1000 | Loss: 0.00001193
Iteration 233/1000 | Loss: 0.00001193
Iteration 234/1000 | Loss: 0.00001192
Iteration 235/1000 | Loss: 0.00002299
Iteration 236/1000 | Loss: 0.00002119
Iteration 237/1000 | Loss: 0.00001194
Iteration 238/1000 | Loss: 0.00001193
Iteration 239/1000 | Loss: 0.00001193
Iteration 240/1000 | Loss: 0.00001193
Iteration 241/1000 | Loss: 0.00001193
Iteration 242/1000 | Loss: 0.00001192
Iteration 243/1000 | Loss: 0.00001192
Iteration 244/1000 | Loss: 0.00001192
Iteration 245/1000 | Loss: 0.00001192
Iteration 246/1000 | Loss: 0.00001192
Iteration 247/1000 | Loss: 0.00001192
Iteration 248/1000 | Loss: 0.00001192
Iteration 249/1000 | Loss: 0.00001192
Iteration 250/1000 | Loss: 0.00001192
Iteration 251/1000 | Loss: 0.00001192
Iteration 252/1000 | Loss: 0.00001191
Iteration 253/1000 | Loss: 0.00001191
Iteration 254/1000 | Loss: 0.00001191
Iteration 255/1000 | Loss: 0.00001191
Iteration 256/1000 | Loss: 0.00001191
Iteration 257/1000 | Loss: 0.00001191
Iteration 258/1000 | Loss: 0.00001191
Iteration 259/1000 | Loss: 0.00001191
Iteration 260/1000 | Loss: 0.00001191
Iteration 261/1000 | Loss: 0.00001191
Iteration 262/1000 | Loss: 0.00001191
Iteration 263/1000 | Loss: 0.00001191
Iteration 264/1000 | Loss: 0.00001191
Iteration 265/1000 | Loss: 0.00001191
Iteration 266/1000 | Loss: 0.00001191
Iteration 267/1000 | Loss: 0.00001191
Iteration 268/1000 | Loss: 0.00001191
Iteration 269/1000 | Loss: 0.00001191
Iteration 270/1000 | Loss: 0.00001191
Iteration 271/1000 | Loss: 0.00001191
Iteration 272/1000 | Loss: 0.00001191
Iteration 273/1000 | Loss: 0.00001191
Iteration 274/1000 | Loss: 0.00001191
Iteration 275/1000 | Loss: 0.00001191
Iteration 276/1000 | Loss: 0.00001191
Iteration 277/1000 | Loss: 0.00001191
Iteration 278/1000 | Loss: 0.00001191
Iteration 279/1000 | Loss: 0.00001191
Iteration 280/1000 | Loss: 0.00001191
Iteration 281/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.1905436622328125e-05, 1.1905436622328125e-05, 1.1905436622328125e-05, 1.1905436622328125e-05, 1.1905436622328125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1905436622328125e-05

Optimization complete. Final v2v error: 2.9602103233337402 mm

Highest mean error: 3.6328325271606445 mm for frame 85

Lowest mean error: 2.74528431892395 mm for frame 151

Saving results

Total time: 270.5113728046417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018610
Iteration 2/25 | Loss: 0.01018610
Iteration 3/25 | Loss: 0.01018610
Iteration 4/25 | Loss: 0.00185759
Iteration 5/25 | Loss: 0.00146662
Iteration 6/25 | Loss: 0.00137263
Iteration 7/25 | Loss: 0.00146504
Iteration 8/25 | Loss: 0.00140513
Iteration 9/25 | Loss: 0.00136628
Iteration 10/25 | Loss: 0.00133144
Iteration 11/25 | Loss: 0.00128188
Iteration 12/25 | Loss: 0.00125887
Iteration 13/25 | Loss: 0.00124053
Iteration 14/25 | Loss: 0.00122979
Iteration 15/25 | Loss: 0.00122893
Iteration 16/25 | Loss: 0.00122502
Iteration 17/25 | Loss: 0.00123116
Iteration 18/25 | Loss: 0.00123180
Iteration 19/25 | Loss: 0.00122782
Iteration 20/25 | Loss: 0.00122508
Iteration 21/25 | Loss: 0.00122611
Iteration 22/25 | Loss: 0.00121963
Iteration 23/25 | Loss: 0.00122572
Iteration 24/25 | Loss: 0.00122112
Iteration 25/25 | Loss: 0.00122170

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32285309
Iteration 2/25 | Loss: 0.00183271
Iteration 3/25 | Loss: 0.00183270
Iteration 4/25 | Loss: 0.00182768
Iteration 5/25 | Loss: 0.00182767
Iteration 6/25 | Loss: 0.00182767
Iteration 7/25 | Loss: 0.00182767
Iteration 8/25 | Loss: 0.00182767
Iteration 9/25 | Loss: 0.00182767
Iteration 10/25 | Loss: 0.00182767
Iteration 11/25 | Loss: 0.00182767
Iteration 12/25 | Loss: 0.00182767
Iteration 13/25 | Loss: 0.00182767
Iteration 14/25 | Loss: 0.00182767
Iteration 15/25 | Loss: 0.00182767
Iteration 16/25 | Loss: 0.00182767
Iteration 17/25 | Loss: 0.00182767
Iteration 18/25 | Loss: 0.00182767
Iteration 19/25 | Loss: 0.00182767
Iteration 20/25 | Loss: 0.00182767
Iteration 21/25 | Loss: 0.00182767
Iteration 22/25 | Loss: 0.00182767
Iteration 23/25 | Loss: 0.00182767
Iteration 24/25 | Loss: 0.00182767
Iteration 25/25 | Loss: 0.00182767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182767
Iteration 2/1000 | Loss: 0.00018424
Iteration 3/1000 | Loss: 0.00017076
Iteration 4/1000 | Loss: 0.00024401
Iteration 5/1000 | Loss: 0.00029521
Iteration 6/1000 | Loss: 0.00012705
Iteration 7/1000 | Loss: 0.00014190
Iteration 8/1000 | Loss: 0.00011929
Iteration 9/1000 | Loss: 0.00017532
Iteration 10/1000 | Loss: 0.00015175
Iteration 11/1000 | Loss: 0.00013962
Iteration 12/1000 | Loss: 0.00012449
Iteration 13/1000 | Loss: 0.00017289
Iteration 14/1000 | Loss: 0.00016600
Iteration 15/1000 | Loss: 0.00020430
Iteration 16/1000 | Loss: 0.00014785
Iteration 17/1000 | Loss: 0.00010973
Iteration 18/1000 | Loss: 0.00009081
Iteration 19/1000 | Loss: 0.00005053
Iteration 20/1000 | Loss: 0.00016116
Iteration 21/1000 | Loss: 0.00011498
Iteration 22/1000 | Loss: 0.00009437
Iteration 23/1000 | Loss: 0.00015769
Iteration 24/1000 | Loss: 0.00016810
Iteration 25/1000 | Loss: 0.00017641
Iteration 26/1000 | Loss: 0.00016692
Iteration 27/1000 | Loss: 0.00007218
Iteration 28/1000 | Loss: 0.00017545
Iteration 29/1000 | Loss: 0.00011242
Iteration 30/1000 | Loss: 0.00014379
Iteration 31/1000 | Loss: 0.00014658
Iteration 32/1000 | Loss: 0.00020008
Iteration 33/1000 | Loss: 0.00007980
Iteration 34/1000 | Loss: 0.00010687
Iteration 35/1000 | Loss: 0.00011341
Iteration 36/1000 | Loss: 0.00015397
Iteration 37/1000 | Loss: 0.00012810
Iteration 38/1000 | Loss: 0.00007337
Iteration 39/1000 | Loss: 0.00010250
Iteration 40/1000 | Loss: 0.00013582
Iteration 41/1000 | Loss: 0.00014296
Iteration 42/1000 | Loss: 0.00015243
Iteration 43/1000 | Loss: 0.00013310
Iteration 44/1000 | Loss: 0.00014912
Iteration 45/1000 | Loss: 0.00016970
Iteration 46/1000 | Loss: 0.00015286
Iteration 47/1000 | Loss: 0.00016685
Iteration 48/1000 | Loss: 0.00017197
Iteration 49/1000 | Loss: 0.00017028
Iteration 50/1000 | Loss: 0.00004445
Iteration 51/1000 | Loss: 0.00005970
Iteration 52/1000 | Loss: 0.00008002
Iteration 53/1000 | Loss: 0.00005492
Iteration 54/1000 | Loss: 0.00007157
Iteration 55/1000 | Loss: 0.00005761
Iteration 56/1000 | Loss: 0.00004172
Iteration 57/1000 | Loss: 0.00009640
Iteration 58/1000 | Loss: 0.00010566
Iteration 59/1000 | Loss: 0.00011207
Iteration 60/1000 | Loss: 0.00012316
Iteration 61/1000 | Loss: 0.00010760
Iteration 62/1000 | Loss: 0.00004950
Iteration 63/1000 | Loss: 0.00008309
Iteration 64/1000 | Loss: 0.00011477
Iteration 65/1000 | Loss: 0.00004432
Iteration 66/1000 | Loss: 0.00004224
Iteration 67/1000 | Loss: 0.00003516
Iteration 68/1000 | Loss: 0.00004301
Iteration 69/1000 | Loss: 0.00004520
Iteration 70/1000 | Loss: 0.00002948
Iteration 71/1000 | Loss: 0.00007113
Iteration 72/1000 | Loss: 0.00005462
Iteration 73/1000 | Loss: 0.00004343
Iteration 74/1000 | Loss: 0.00004165
Iteration 75/1000 | Loss: 0.00011136
Iteration 76/1000 | Loss: 0.00008647
Iteration 77/1000 | Loss: 0.00002324
Iteration 78/1000 | Loss: 0.00008162
Iteration 79/1000 | Loss: 0.00002436
Iteration 80/1000 | Loss: 0.00004528
Iteration 81/1000 | Loss: 0.00033747
Iteration 82/1000 | Loss: 0.00016551
Iteration 83/1000 | Loss: 0.00030202
Iteration 84/1000 | Loss: 0.00016482
Iteration 85/1000 | Loss: 0.00013504
Iteration 86/1000 | Loss: 0.00002490
Iteration 87/1000 | Loss: 0.00002090
Iteration 88/1000 | Loss: 0.00018746
Iteration 89/1000 | Loss: 0.00010726
Iteration 90/1000 | Loss: 0.00015444
Iteration 91/1000 | Loss: 0.00013098
Iteration 92/1000 | Loss: 0.00002659
Iteration 93/1000 | Loss: 0.00015823
Iteration 94/1000 | Loss: 0.00012477
Iteration 95/1000 | Loss: 0.00013437
Iteration 96/1000 | Loss: 0.00012914
Iteration 97/1000 | Loss: 0.00013865
Iteration 98/1000 | Loss: 0.00001885
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001540
Iteration 101/1000 | Loss: 0.00001471
Iteration 102/1000 | Loss: 0.00003937
Iteration 103/1000 | Loss: 0.00001556
Iteration 104/1000 | Loss: 0.00001406
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001336
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001315
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00005052
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001299
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001287
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001286
Iteration 163/1000 | Loss: 0.00001286
Iteration 164/1000 | Loss: 0.00001286
Iteration 165/1000 | Loss: 0.00001285
Iteration 166/1000 | Loss: 0.00001285
Iteration 167/1000 | Loss: 0.00001284
Iteration 168/1000 | Loss: 0.00001284
Iteration 169/1000 | Loss: 0.00001284
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001282
Iteration 174/1000 | Loss: 0.00001282
Iteration 175/1000 | Loss: 0.00001282
Iteration 176/1000 | Loss: 0.00001282
Iteration 177/1000 | Loss: 0.00009730
Iteration 178/1000 | Loss: 0.00008756
Iteration 179/1000 | Loss: 0.00008713
Iteration 180/1000 | Loss: 0.00008627
Iteration 181/1000 | Loss: 0.00016704
Iteration 182/1000 | Loss: 0.00008117
Iteration 183/1000 | Loss: 0.00012657
Iteration 184/1000 | Loss: 0.00011267
Iteration 185/1000 | Loss: 0.00010446
Iteration 186/1000 | Loss: 0.00012914
Iteration 187/1000 | Loss: 0.00007224
Iteration 188/1000 | Loss: 0.00005747
Iteration 189/1000 | Loss: 0.00001769
Iteration 190/1000 | Loss: 0.00001692
Iteration 191/1000 | Loss: 0.00001590
Iteration 192/1000 | Loss: 0.00001946
Iteration 193/1000 | Loss: 0.00001541
Iteration 194/1000 | Loss: 0.00012945
Iteration 195/1000 | Loss: 0.00002187
Iteration 196/1000 | Loss: 0.00002208
Iteration 197/1000 | Loss: 0.00001674
Iteration 198/1000 | Loss: 0.00010671
Iteration 199/1000 | Loss: 0.00030952
Iteration 200/1000 | Loss: 0.00012049
Iteration 201/1000 | Loss: 0.00002028
Iteration 202/1000 | Loss: 0.00008001
Iteration 203/1000 | Loss: 0.00017679
Iteration 204/1000 | Loss: 0.00008060
Iteration 205/1000 | Loss: 0.00044753
Iteration 206/1000 | Loss: 0.00026991
Iteration 207/1000 | Loss: 0.00003155
Iteration 208/1000 | Loss: 0.00002937
Iteration 209/1000 | Loss: 0.00002822
Iteration 210/1000 | Loss: 0.00002408
Iteration 211/1000 | Loss: 0.00002556
Iteration 212/1000 | Loss: 0.00001641
Iteration 213/1000 | Loss: 0.00012952
Iteration 214/1000 | Loss: 0.00009447
Iteration 215/1000 | Loss: 0.00010648
Iteration 216/1000 | Loss: 0.00001830
Iteration 217/1000 | Loss: 0.00007211
Iteration 218/1000 | Loss: 0.00007708
Iteration 219/1000 | Loss: 0.00013908
Iteration 220/1000 | Loss: 0.00010009
Iteration 221/1000 | Loss: 0.00010413
Iteration 222/1000 | Loss: 0.00010257
Iteration 223/1000 | Loss: 0.00009572
Iteration 224/1000 | Loss: 0.00008696
Iteration 225/1000 | Loss: 0.00011967
Iteration 226/1000 | Loss: 0.00010983
Iteration 227/1000 | Loss: 0.00007804
Iteration 228/1000 | Loss: 0.00003025
Iteration 229/1000 | Loss: 0.00016799
Iteration 230/1000 | Loss: 0.00015040
Iteration 231/1000 | Loss: 0.00002343
Iteration 232/1000 | Loss: 0.00001796
Iteration 233/1000 | Loss: 0.00004526
Iteration 234/1000 | Loss: 0.00001510
Iteration 235/1000 | Loss: 0.00001391
Iteration 236/1000 | Loss: 0.00001319
Iteration 237/1000 | Loss: 0.00001274
Iteration 238/1000 | Loss: 0.00001246
Iteration 239/1000 | Loss: 0.00001229
Iteration 240/1000 | Loss: 0.00001209
Iteration 241/1000 | Loss: 0.00001200
Iteration 242/1000 | Loss: 0.00001193
Iteration 243/1000 | Loss: 0.00001186
Iteration 244/1000 | Loss: 0.00001186
Iteration 245/1000 | Loss: 0.00001185
Iteration 246/1000 | Loss: 0.00001184
Iteration 247/1000 | Loss: 0.00001184
Iteration 248/1000 | Loss: 0.00001184
Iteration 249/1000 | Loss: 0.00001183
Iteration 250/1000 | Loss: 0.00001183
Iteration 251/1000 | Loss: 0.00001182
Iteration 252/1000 | Loss: 0.00001182
Iteration 253/1000 | Loss: 0.00001181
Iteration 254/1000 | Loss: 0.00001181
Iteration 255/1000 | Loss: 0.00001181
Iteration 256/1000 | Loss: 0.00001181
Iteration 257/1000 | Loss: 0.00001181
Iteration 258/1000 | Loss: 0.00001180
Iteration 259/1000 | Loss: 0.00001180
Iteration 260/1000 | Loss: 0.00001180
Iteration 261/1000 | Loss: 0.00001180
Iteration 262/1000 | Loss: 0.00001180
Iteration 263/1000 | Loss: 0.00001180
Iteration 264/1000 | Loss: 0.00001180
Iteration 265/1000 | Loss: 0.00001180
Iteration 266/1000 | Loss: 0.00001180
Iteration 267/1000 | Loss: 0.00001180
Iteration 268/1000 | Loss: 0.00001179
Iteration 269/1000 | Loss: 0.00001179
Iteration 270/1000 | Loss: 0.00001179
Iteration 271/1000 | Loss: 0.00001179
Iteration 272/1000 | Loss: 0.00001179
Iteration 273/1000 | Loss: 0.00001179
Iteration 274/1000 | Loss: 0.00001179
Iteration 275/1000 | Loss: 0.00001179
Iteration 276/1000 | Loss: 0.00001179
Iteration 277/1000 | Loss: 0.00001179
Iteration 278/1000 | Loss: 0.00001179
Iteration 279/1000 | Loss: 0.00001179
Iteration 280/1000 | Loss: 0.00001179
Iteration 281/1000 | Loss: 0.00001178
Iteration 282/1000 | Loss: 0.00001178
Iteration 283/1000 | Loss: 0.00001178
Iteration 284/1000 | Loss: 0.00001178
Iteration 285/1000 | Loss: 0.00001178
Iteration 286/1000 | Loss: 0.00001178
Iteration 287/1000 | Loss: 0.00001178
Iteration 288/1000 | Loss: 0.00001178
Iteration 289/1000 | Loss: 0.00001178
Iteration 290/1000 | Loss: 0.00001178
Iteration 291/1000 | Loss: 0.00001178
Iteration 292/1000 | Loss: 0.00001178
Iteration 293/1000 | Loss: 0.00001178
Iteration 294/1000 | Loss: 0.00001178
Iteration 295/1000 | Loss: 0.00001178
Iteration 296/1000 | Loss: 0.00001178
Iteration 297/1000 | Loss: 0.00001178
Iteration 298/1000 | Loss: 0.00001178
Iteration 299/1000 | Loss: 0.00001178
Iteration 300/1000 | Loss: 0.00001178
Iteration 301/1000 | Loss: 0.00001178
Iteration 302/1000 | Loss: 0.00001178
Iteration 303/1000 | Loss: 0.00001178
Iteration 304/1000 | Loss: 0.00001178
Iteration 305/1000 | Loss: 0.00001178
Iteration 306/1000 | Loss: 0.00001178
Iteration 307/1000 | Loss: 0.00001178
Iteration 308/1000 | Loss: 0.00001178
Iteration 309/1000 | Loss: 0.00001178
Iteration 310/1000 | Loss: 0.00001178
Iteration 311/1000 | Loss: 0.00001178
Iteration 312/1000 | Loss: 0.00001178
Iteration 313/1000 | Loss: 0.00001178
Iteration 314/1000 | Loss: 0.00001178
Iteration 315/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.1782959518313874e-05, 1.1782959518313874e-05, 1.1782959518313874e-05, 1.1782959518313874e-05, 1.1782959518313874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1782959518313874e-05

Optimization complete. Final v2v error: 2.904418706893921 mm

Highest mean error: 5.168043613433838 mm for frame 219

Lowest mean error: 2.61324405670166 mm for frame 233

Saving results

Total time: 344.8721351623535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902564
Iteration 2/25 | Loss: 0.00157754
Iteration 3/25 | Loss: 0.00138942
Iteration 4/25 | Loss: 0.00137655
Iteration 5/25 | Loss: 0.00137825
Iteration 6/25 | Loss: 0.00135904
Iteration 7/25 | Loss: 0.00134543
Iteration 8/25 | Loss: 0.00134468
Iteration 9/25 | Loss: 0.00133819
Iteration 10/25 | Loss: 0.00133116
Iteration 11/25 | Loss: 0.00133444
Iteration 12/25 | Loss: 0.00133254
Iteration 13/25 | Loss: 0.00133013
Iteration 14/25 | Loss: 0.00132928
Iteration 15/25 | Loss: 0.00132876
Iteration 16/25 | Loss: 0.00132825
Iteration 17/25 | Loss: 0.00132617
Iteration 18/25 | Loss: 0.00132602
Iteration 19/25 | Loss: 0.00132588
Iteration 20/25 | Loss: 0.00132586
Iteration 21/25 | Loss: 0.00132586
Iteration 22/25 | Loss: 0.00132586
Iteration 23/25 | Loss: 0.00132586
Iteration 24/25 | Loss: 0.00132586
Iteration 25/25 | Loss: 0.00132586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24917805
Iteration 2/25 | Loss: 0.00160802
Iteration 3/25 | Loss: 0.00160798
Iteration 4/25 | Loss: 0.00160797
Iteration 5/25 | Loss: 0.00160797
Iteration 6/25 | Loss: 0.00160797
Iteration 7/25 | Loss: 0.00160797
Iteration 8/25 | Loss: 0.00160797
Iteration 9/25 | Loss: 0.00160797
Iteration 10/25 | Loss: 0.00160797
Iteration 11/25 | Loss: 0.00160797
Iteration 12/25 | Loss: 0.00160797
Iteration 13/25 | Loss: 0.00160797
Iteration 14/25 | Loss: 0.00160797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016079721972346306, 0.0016079721972346306, 0.0016079721972346306, 0.0016079721972346306, 0.0016079721972346306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016079721972346306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160797
Iteration 2/1000 | Loss: 0.00045188
Iteration 3/1000 | Loss: 0.00005810
Iteration 4/1000 | Loss: 0.00004043
Iteration 5/1000 | Loss: 0.00003517
Iteration 6/1000 | Loss: 0.00003281
Iteration 7/1000 | Loss: 0.00020067
Iteration 8/1000 | Loss: 0.00003879
Iteration 9/1000 | Loss: 0.00003149
Iteration 10/1000 | Loss: 0.00002979
Iteration 11/1000 | Loss: 0.00002886
Iteration 12/1000 | Loss: 0.00010981
Iteration 13/1000 | Loss: 0.00008952
Iteration 14/1000 | Loss: 0.00016646
Iteration 15/1000 | Loss: 0.00009217
Iteration 16/1000 | Loss: 0.00011636
Iteration 17/1000 | Loss: 0.00002994
Iteration 18/1000 | Loss: 0.00002868
Iteration 19/1000 | Loss: 0.00002742
Iteration 20/1000 | Loss: 0.00002647
Iteration 21/1000 | Loss: 0.00002560
Iteration 22/1000 | Loss: 0.00002523
Iteration 23/1000 | Loss: 0.00002484
Iteration 24/1000 | Loss: 0.00002453
Iteration 25/1000 | Loss: 0.00002435
Iteration 26/1000 | Loss: 0.00002422
Iteration 27/1000 | Loss: 0.00002419
Iteration 28/1000 | Loss: 0.00002416
Iteration 29/1000 | Loss: 0.00002413
Iteration 30/1000 | Loss: 0.00002409
Iteration 31/1000 | Loss: 0.00002405
Iteration 32/1000 | Loss: 0.00002404
Iteration 33/1000 | Loss: 0.00002397
Iteration 34/1000 | Loss: 0.00002396
Iteration 35/1000 | Loss: 0.00002395
Iteration 36/1000 | Loss: 0.00002394
Iteration 37/1000 | Loss: 0.00002394
Iteration 38/1000 | Loss: 0.00002393
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002392
Iteration 41/1000 | Loss: 0.00002392
Iteration 42/1000 | Loss: 0.00002392
Iteration 43/1000 | Loss: 0.00002391
Iteration 44/1000 | Loss: 0.00002391
Iteration 45/1000 | Loss: 0.00002391
Iteration 46/1000 | Loss: 0.00002391
Iteration 47/1000 | Loss: 0.00002391
Iteration 48/1000 | Loss: 0.00002391
Iteration 49/1000 | Loss: 0.00002391
Iteration 50/1000 | Loss: 0.00002391
Iteration 51/1000 | Loss: 0.00002390
Iteration 52/1000 | Loss: 0.00002390
Iteration 53/1000 | Loss: 0.00002390
Iteration 54/1000 | Loss: 0.00002390
Iteration 55/1000 | Loss: 0.00002389
Iteration 56/1000 | Loss: 0.00002389
Iteration 57/1000 | Loss: 0.00002389
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002388
Iteration 60/1000 | Loss: 0.00002388
Iteration 61/1000 | Loss: 0.00002388
Iteration 62/1000 | Loss: 0.00002388
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002387
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002386
Iteration 69/1000 | Loss: 0.00002386
Iteration 70/1000 | Loss: 0.00002386
Iteration 71/1000 | Loss: 0.00002386
Iteration 72/1000 | Loss: 0.00002385
Iteration 73/1000 | Loss: 0.00002385
Iteration 74/1000 | Loss: 0.00002385
Iteration 75/1000 | Loss: 0.00002385
Iteration 76/1000 | Loss: 0.00002385
Iteration 77/1000 | Loss: 0.00002385
Iteration 78/1000 | Loss: 0.00002385
Iteration 79/1000 | Loss: 0.00002385
Iteration 80/1000 | Loss: 0.00002385
Iteration 81/1000 | Loss: 0.00002384
Iteration 82/1000 | Loss: 0.00002384
Iteration 83/1000 | Loss: 0.00002384
Iteration 84/1000 | Loss: 0.00002384
Iteration 85/1000 | Loss: 0.00002384
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002384
Iteration 88/1000 | Loss: 0.00002384
Iteration 89/1000 | Loss: 0.00002384
Iteration 90/1000 | Loss: 0.00002384
Iteration 91/1000 | Loss: 0.00002384
Iteration 92/1000 | Loss: 0.00002384
Iteration 93/1000 | Loss: 0.00002384
Iteration 94/1000 | Loss: 0.00002384
Iteration 95/1000 | Loss: 0.00002384
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002384
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002384
Iteration 103/1000 | Loss: 0.00002384
Iteration 104/1000 | Loss: 0.00002384
Iteration 105/1000 | Loss: 0.00002384
Iteration 106/1000 | Loss: 0.00002384
Iteration 107/1000 | Loss: 0.00002384
Iteration 108/1000 | Loss: 0.00002384
Iteration 109/1000 | Loss: 0.00002384
Iteration 110/1000 | Loss: 0.00002384
Iteration 111/1000 | Loss: 0.00002384
Iteration 112/1000 | Loss: 0.00002384
Iteration 113/1000 | Loss: 0.00002384
Iteration 114/1000 | Loss: 0.00002384
Iteration 115/1000 | Loss: 0.00002384
Iteration 116/1000 | Loss: 0.00002384
Iteration 117/1000 | Loss: 0.00002384
Iteration 118/1000 | Loss: 0.00002384
Iteration 119/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.38396769418614e-05, 2.38396769418614e-05, 2.38396769418614e-05, 2.38396769418614e-05, 2.38396769418614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.38396769418614e-05

Optimization complete. Final v2v error: 4.099412441253662 mm

Highest mean error: 4.788370132446289 mm for frame 139

Lowest mean error: 3.6040942668914795 mm for frame 83

Saving results

Total time: 88.93858623504639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429548
Iteration 2/25 | Loss: 0.00135545
Iteration 3/25 | Loss: 0.00121356
Iteration 4/25 | Loss: 0.00120496
Iteration 5/25 | Loss: 0.00120371
Iteration 6/25 | Loss: 0.00120371
Iteration 7/25 | Loss: 0.00120371
Iteration 8/25 | Loss: 0.00120371
Iteration 9/25 | Loss: 0.00120371
Iteration 10/25 | Loss: 0.00120371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012037084670737386, 0.0012037084670737386, 0.0012037084670737386, 0.0012037084670737386, 0.0012037084670737386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012037084670737386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24760675
Iteration 2/25 | Loss: 0.00090054
Iteration 3/25 | Loss: 0.00090052
Iteration 4/25 | Loss: 0.00090052
Iteration 5/25 | Loss: 0.00090052
Iteration 6/25 | Loss: 0.00090052
Iteration 7/25 | Loss: 0.00090052
Iteration 8/25 | Loss: 0.00090052
Iteration 9/25 | Loss: 0.00090052
Iteration 10/25 | Loss: 0.00090052
Iteration 11/25 | Loss: 0.00090052
Iteration 12/25 | Loss: 0.00090052
Iteration 13/25 | Loss: 0.00090052
Iteration 14/25 | Loss: 0.00090052
Iteration 15/25 | Loss: 0.00090052
Iteration 16/25 | Loss: 0.00090052
Iteration 17/25 | Loss: 0.00090052
Iteration 18/25 | Loss: 0.00090052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009005151223391294, 0.0009005151223391294, 0.0009005151223391294, 0.0009005151223391294, 0.0009005151223391294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009005151223391294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090052
Iteration 2/1000 | Loss: 0.00002473
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001843
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001661
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001536
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001477
Iteration 15/1000 | Loss: 0.00001475
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001472
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Iteration 79/1000 | Loss: 0.00001426
Iteration 80/1000 | Loss: 0.00001425
Iteration 81/1000 | Loss: 0.00001425
Iteration 82/1000 | Loss: 0.00001425
Iteration 83/1000 | Loss: 0.00001425
Iteration 84/1000 | Loss: 0.00001425
Iteration 85/1000 | Loss: 0.00001425
Iteration 86/1000 | Loss: 0.00001424
Iteration 87/1000 | Loss: 0.00001424
Iteration 88/1000 | Loss: 0.00001424
Iteration 89/1000 | Loss: 0.00001424
Iteration 90/1000 | Loss: 0.00001424
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001422
Iteration 97/1000 | Loss: 0.00001422
Iteration 98/1000 | Loss: 0.00001422
Iteration 99/1000 | Loss: 0.00001422
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001421
Iteration 111/1000 | Loss: 0.00001421
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001421
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001420
Iteration 126/1000 | Loss: 0.00001420
Iteration 127/1000 | Loss: 0.00001420
Iteration 128/1000 | Loss: 0.00001420
Iteration 129/1000 | Loss: 0.00001420
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001420
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.4201160411175806e-05, 1.4201160411175806e-05, 1.4201160411175806e-05, 1.4201160411175806e-05, 1.4201160411175806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4201160411175806e-05

Optimization complete. Final v2v error: 3.2283642292022705 mm

Highest mean error: 3.6244115829467773 mm for frame 101

Lowest mean error: 2.935168504714966 mm for frame 130

Saving results

Total time: 37.500211000442505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806746
Iteration 2/25 | Loss: 0.00123985
Iteration 3/25 | Loss: 0.00116311
Iteration 4/25 | Loss: 0.00114168
Iteration 5/25 | Loss: 0.00113339
Iteration 6/25 | Loss: 0.00113325
Iteration 7/25 | Loss: 0.00113325
Iteration 8/25 | Loss: 0.00113325
Iteration 9/25 | Loss: 0.00113325
Iteration 10/25 | Loss: 0.00113325
Iteration 11/25 | Loss: 0.00113325
Iteration 12/25 | Loss: 0.00113325
Iteration 13/25 | Loss: 0.00113325
Iteration 14/25 | Loss: 0.00113325
Iteration 15/25 | Loss: 0.00113325
Iteration 16/25 | Loss: 0.00113325
Iteration 17/25 | Loss: 0.00113325
Iteration 18/25 | Loss: 0.00113325
Iteration 19/25 | Loss: 0.00113325
Iteration 20/25 | Loss: 0.00113325
Iteration 21/25 | Loss: 0.00113325
Iteration 22/25 | Loss: 0.00113325
Iteration 23/25 | Loss: 0.00113325
Iteration 24/25 | Loss: 0.00113325
Iteration 25/25 | Loss: 0.00113325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40183353
Iteration 2/25 | Loss: 0.00198476
Iteration 3/25 | Loss: 0.00198471
Iteration 4/25 | Loss: 0.00198471
Iteration 5/25 | Loss: 0.00198471
Iteration 6/25 | Loss: 0.00198471
Iteration 7/25 | Loss: 0.00198471
Iteration 8/25 | Loss: 0.00198471
Iteration 9/25 | Loss: 0.00198471
Iteration 10/25 | Loss: 0.00198471
Iteration 11/25 | Loss: 0.00198471
Iteration 12/25 | Loss: 0.00198471
Iteration 13/25 | Loss: 0.00198471
Iteration 14/25 | Loss: 0.00198471
Iteration 15/25 | Loss: 0.00198471
Iteration 16/25 | Loss: 0.00198471
Iteration 17/25 | Loss: 0.00198471
Iteration 18/25 | Loss: 0.00198471
Iteration 19/25 | Loss: 0.00198471
Iteration 20/25 | Loss: 0.00198471
Iteration 21/25 | Loss: 0.00198471
Iteration 22/25 | Loss: 0.00198471
Iteration 23/25 | Loss: 0.00198471
Iteration 24/25 | Loss: 0.00198471
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001984707545489073, 0.001984707545489073, 0.001984707545489073, 0.001984707545489073, 0.001984707545489073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001984707545489073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198471
Iteration 2/1000 | Loss: 0.00003526
Iteration 3/1000 | Loss: 0.00002183
Iteration 4/1000 | Loss: 0.00001804
Iteration 5/1000 | Loss: 0.00001651
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001301
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001253
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001236
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001214
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001212
Iteration 45/1000 | Loss: 0.00001211
Iteration 46/1000 | Loss: 0.00001211
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001204
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001203
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001200
Iteration 85/1000 | Loss: 0.00001200
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001197
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001196
Iteration 120/1000 | Loss: 0.00001196
Iteration 121/1000 | Loss: 0.00001196
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001195
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001190
Iteration 141/1000 | Loss: 0.00001190
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001189
Iteration 144/1000 | Loss: 0.00001189
Iteration 145/1000 | Loss: 0.00001189
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001186
Iteration 158/1000 | Loss: 0.00001186
Iteration 159/1000 | Loss: 0.00001186
Iteration 160/1000 | Loss: 0.00001186
Iteration 161/1000 | Loss: 0.00001185
Iteration 162/1000 | Loss: 0.00001185
Iteration 163/1000 | Loss: 0.00001185
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001184
Iteration 168/1000 | Loss: 0.00001184
Iteration 169/1000 | Loss: 0.00001184
Iteration 170/1000 | Loss: 0.00001184
Iteration 171/1000 | Loss: 0.00001184
Iteration 172/1000 | Loss: 0.00001184
Iteration 173/1000 | Loss: 0.00001183
Iteration 174/1000 | Loss: 0.00001183
Iteration 175/1000 | Loss: 0.00001183
Iteration 176/1000 | Loss: 0.00001183
Iteration 177/1000 | Loss: 0.00001182
Iteration 178/1000 | Loss: 0.00001182
Iteration 179/1000 | Loss: 0.00001182
Iteration 180/1000 | Loss: 0.00001182
Iteration 181/1000 | Loss: 0.00001182
Iteration 182/1000 | Loss: 0.00001182
Iteration 183/1000 | Loss: 0.00001182
Iteration 184/1000 | Loss: 0.00001182
Iteration 185/1000 | Loss: 0.00001181
Iteration 186/1000 | Loss: 0.00001181
Iteration 187/1000 | Loss: 0.00001181
Iteration 188/1000 | Loss: 0.00001181
Iteration 189/1000 | Loss: 0.00001181
Iteration 190/1000 | Loss: 0.00001181
Iteration 191/1000 | Loss: 0.00001181
Iteration 192/1000 | Loss: 0.00001181
Iteration 193/1000 | Loss: 0.00001181
Iteration 194/1000 | Loss: 0.00001181
Iteration 195/1000 | Loss: 0.00001181
Iteration 196/1000 | Loss: 0.00001181
Iteration 197/1000 | Loss: 0.00001181
Iteration 198/1000 | Loss: 0.00001181
Iteration 199/1000 | Loss: 0.00001181
Iteration 200/1000 | Loss: 0.00001180
Iteration 201/1000 | Loss: 0.00001180
Iteration 202/1000 | Loss: 0.00001180
Iteration 203/1000 | Loss: 0.00001180
Iteration 204/1000 | Loss: 0.00001180
Iteration 205/1000 | Loss: 0.00001180
Iteration 206/1000 | Loss: 0.00001180
Iteration 207/1000 | Loss: 0.00001180
Iteration 208/1000 | Loss: 0.00001180
Iteration 209/1000 | Loss: 0.00001180
Iteration 210/1000 | Loss: 0.00001180
Iteration 211/1000 | Loss: 0.00001180
Iteration 212/1000 | Loss: 0.00001180
Iteration 213/1000 | Loss: 0.00001180
Iteration 214/1000 | Loss: 0.00001180
Iteration 215/1000 | Loss: 0.00001180
Iteration 216/1000 | Loss: 0.00001180
Iteration 217/1000 | Loss: 0.00001179
Iteration 218/1000 | Loss: 0.00001179
Iteration 219/1000 | Loss: 0.00001179
Iteration 220/1000 | Loss: 0.00001179
Iteration 221/1000 | Loss: 0.00001179
Iteration 222/1000 | Loss: 0.00001179
Iteration 223/1000 | Loss: 0.00001179
Iteration 224/1000 | Loss: 0.00001179
Iteration 225/1000 | Loss: 0.00001179
Iteration 226/1000 | Loss: 0.00001179
Iteration 227/1000 | Loss: 0.00001179
Iteration 228/1000 | Loss: 0.00001179
Iteration 229/1000 | Loss: 0.00001179
Iteration 230/1000 | Loss: 0.00001179
Iteration 231/1000 | Loss: 0.00001179
Iteration 232/1000 | Loss: 0.00001178
Iteration 233/1000 | Loss: 0.00001178
Iteration 234/1000 | Loss: 0.00001178
Iteration 235/1000 | Loss: 0.00001178
Iteration 236/1000 | Loss: 0.00001178
Iteration 237/1000 | Loss: 0.00001178
Iteration 238/1000 | Loss: 0.00001178
Iteration 239/1000 | Loss: 0.00001178
Iteration 240/1000 | Loss: 0.00001178
Iteration 241/1000 | Loss: 0.00001178
Iteration 242/1000 | Loss: 0.00001178
Iteration 243/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.1780385648307856e-05, 1.1780385648307856e-05, 1.1780385648307856e-05, 1.1780385648307856e-05, 1.1780385648307856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1780385648307856e-05

Optimization complete. Final v2v error: 2.9376115798950195 mm

Highest mean error: 3.439549684524536 mm for frame 47

Lowest mean error: 2.5609323978424072 mm for frame 215

Saving results

Total time: 53.23503041267395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042713
Iteration 2/25 | Loss: 0.00242060
Iteration 3/25 | Loss: 0.00172105
Iteration 4/25 | Loss: 0.00164410
Iteration 5/25 | Loss: 0.00145396
Iteration 6/25 | Loss: 0.00132396
Iteration 7/25 | Loss: 0.00126296
Iteration 8/25 | Loss: 0.00122855
Iteration 9/25 | Loss: 0.00120542
Iteration 10/25 | Loss: 0.00120458
Iteration 11/25 | Loss: 0.00119523
Iteration 12/25 | Loss: 0.00118428
Iteration 13/25 | Loss: 0.00119039
Iteration 14/25 | Loss: 0.00118323
Iteration 15/25 | Loss: 0.00118643
Iteration 16/25 | Loss: 0.00118474
Iteration 17/25 | Loss: 0.00118422
Iteration 18/25 | Loss: 0.00118276
Iteration 19/25 | Loss: 0.00118275
Iteration 20/25 | Loss: 0.00118275
Iteration 21/25 | Loss: 0.00118275
Iteration 22/25 | Loss: 0.00118275
Iteration 23/25 | Loss: 0.00118275
Iteration 24/25 | Loss: 0.00118275
Iteration 25/25 | Loss: 0.00118275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24725461
Iteration 2/25 | Loss: 0.00143848
Iteration 3/25 | Loss: 0.00126233
Iteration 4/25 | Loss: 0.00126233
Iteration 5/25 | Loss: 0.00126232
Iteration 6/25 | Loss: 0.00126232
Iteration 7/25 | Loss: 0.00126232
Iteration 8/25 | Loss: 0.00126232
Iteration 9/25 | Loss: 0.00126232
Iteration 10/25 | Loss: 0.00126232
Iteration 11/25 | Loss: 0.00126232
Iteration 12/25 | Loss: 0.00126232
Iteration 13/25 | Loss: 0.00126232
Iteration 14/25 | Loss: 0.00126232
Iteration 15/25 | Loss: 0.00126232
Iteration 16/25 | Loss: 0.00126232
Iteration 17/25 | Loss: 0.00126232
Iteration 18/25 | Loss: 0.00126232
Iteration 19/25 | Loss: 0.00126232
Iteration 20/25 | Loss: 0.00126232
Iteration 21/25 | Loss: 0.00126232
Iteration 22/25 | Loss: 0.00126232
Iteration 23/25 | Loss: 0.00126232
Iteration 24/25 | Loss: 0.00126232
Iteration 25/25 | Loss: 0.00126232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126232
Iteration 2/1000 | Loss: 0.00005050
Iteration 3/1000 | Loss: 0.00003847
Iteration 4/1000 | Loss: 0.00003347
Iteration 5/1000 | Loss: 0.00003066
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00002822
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002664
Iteration 10/1000 | Loss: 0.00002600
Iteration 11/1000 | Loss: 0.00002538
Iteration 12/1000 | Loss: 0.00002497
Iteration 13/1000 | Loss: 0.00002456
Iteration 14/1000 | Loss: 0.00002434
Iteration 15/1000 | Loss: 0.00002413
Iteration 16/1000 | Loss: 0.00056205
Iteration 17/1000 | Loss: 0.00336729
Iteration 18/1000 | Loss: 0.00025468
Iteration 19/1000 | Loss: 0.00004295
Iteration 20/1000 | Loss: 0.00021150
Iteration 21/1000 | Loss: 0.00018547
Iteration 22/1000 | Loss: 0.00002408
Iteration 23/1000 | Loss: 0.00001990
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001431
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00003798
Iteration 28/1000 | Loss: 0.00011338
Iteration 29/1000 | Loss: 0.00002260
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001006
Iteration 32/1000 | Loss: 0.00000979
Iteration 33/1000 | Loss: 0.00000944
Iteration 34/1000 | Loss: 0.00000910
Iteration 35/1000 | Loss: 0.00000873
Iteration 36/1000 | Loss: 0.00000846
Iteration 37/1000 | Loss: 0.00000842
Iteration 38/1000 | Loss: 0.00000836
Iteration 39/1000 | Loss: 0.00000831
Iteration 40/1000 | Loss: 0.00000827
Iteration 41/1000 | Loss: 0.00000827
Iteration 42/1000 | Loss: 0.00000826
Iteration 43/1000 | Loss: 0.00000826
Iteration 44/1000 | Loss: 0.00000826
Iteration 45/1000 | Loss: 0.00000826
Iteration 46/1000 | Loss: 0.00000826
Iteration 47/1000 | Loss: 0.00000825
Iteration 48/1000 | Loss: 0.00000825
Iteration 49/1000 | Loss: 0.00000825
Iteration 50/1000 | Loss: 0.00000825
Iteration 51/1000 | Loss: 0.00000825
Iteration 52/1000 | Loss: 0.00000825
Iteration 53/1000 | Loss: 0.00000824
Iteration 54/1000 | Loss: 0.00000824
Iteration 55/1000 | Loss: 0.00000821
Iteration 56/1000 | Loss: 0.00000820
Iteration 57/1000 | Loss: 0.00000820
Iteration 58/1000 | Loss: 0.00000820
Iteration 59/1000 | Loss: 0.00000820
Iteration 60/1000 | Loss: 0.00000819
Iteration 61/1000 | Loss: 0.00000819
Iteration 62/1000 | Loss: 0.00000819
Iteration 63/1000 | Loss: 0.00000818
Iteration 64/1000 | Loss: 0.00000817
Iteration 65/1000 | Loss: 0.00000817
Iteration 66/1000 | Loss: 0.00000817
Iteration 67/1000 | Loss: 0.00000817
Iteration 68/1000 | Loss: 0.00000817
Iteration 69/1000 | Loss: 0.00000817
Iteration 70/1000 | Loss: 0.00000817
Iteration 71/1000 | Loss: 0.00000817
Iteration 72/1000 | Loss: 0.00000817
Iteration 73/1000 | Loss: 0.00000817
Iteration 74/1000 | Loss: 0.00000817
Iteration 75/1000 | Loss: 0.00000817
Iteration 76/1000 | Loss: 0.00000816
Iteration 77/1000 | Loss: 0.00000816
Iteration 78/1000 | Loss: 0.00000816
Iteration 79/1000 | Loss: 0.00000816
Iteration 80/1000 | Loss: 0.00000815
Iteration 81/1000 | Loss: 0.00000815
Iteration 82/1000 | Loss: 0.00000815
Iteration 83/1000 | Loss: 0.00000814
Iteration 84/1000 | Loss: 0.00000814
Iteration 85/1000 | Loss: 0.00000814
Iteration 86/1000 | Loss: 0.00000814
Iteration 87/1000 | Loss: 0.00000814
Iteration 88/1000 | Loss: 0.00000814
Iteration 89/1000 | Loss: 0.00000813
Iteration 90/1000 | Loss: 0.00000813
Iteration 91/1000 | Loss: 0.00000813
Iteration 92/1000 | Loss: 0.00000813
Iteration 93/1000 | Loss: 0.00000812
Iteration 94/1000 | Loss: 0.00000812
Iteration 95/1000 | Loss: 0.00000812
Iteration 96/1000 | Loss: 0.00000812
Iteration 97/1000 | Loss: 0.00000812
Iteration 98/1000 | Loss: 0.00000811
Iteration 99/1000 | Loss: 0.00000811
Iteration 100/1000 | Loss: 0.00000811
Iteration 101/1000 | Loss: 0.00000811
Iteration 102/1000 | Loss: 0.00000811
Iteration 103/1000 | Loss: 0.00000811
Iteration 104/1000 | Loss: 0.00000811
Iteration 105/1000 | Loss: 0.00000811
Iteration 106/1000 | Loss: 0.00000811
Iteration 107/1000 | Loss: 0.00000810
Iteration 108/1000 | Loss: 0.00000810
Iteration 109/1000 | Loss: 0.00000810
Iteration 110/1000 | Loss: 0.00000810
Iteration 111/1000 | Loss: 0.00000810
Iteration 112/1000 | Loss: 0.00000810
Iteration 113/1000 | Loss: 0.00000810
Iteration 114/1000 | Loss: 0.00000810
Iteration 115/1000 | Loss: 0.00000810
Iteration 116/1000 | Loss: 0.00000810
Iteration 117/1000 | Loss: 0.00000810
Iteration 118/1000 | Loss: 0.00000810
Iteration 119/1000 | Loss: 0.00000810
Iteration 120/1000 | Loss: 0.00000810
Iteration 121/1000 | Loss: 0.00000810
Iteration 122/1000 | Loss: 0.00000810
Iteration 123/1000 | Loss: 0.00000810
Iteration 124/1000 | Loss: 0.00000810
Iteration 125/1000 | Loss: 0.00000810
Iteration 126/1000 | Loss: 0.00000810
Iteration 127/1000 | Loss: 0.00000810
Iteration 128/1000 | Loss: 0.00000810
Iteration 129/1000 | Loss: 0.00000810
Iteration 130/1000 | Loss: 0.00000810
Iteration 131/1000 | Loss: 0.00000810
Iteration 132/1000 | Loss: 0.00000810
Iteration 133/1000 | Loss: 0.00000810
Iteration 134/1000 | Loss: 0.00000810
Iteration 135/1000 | Loss: 0.00000810
Iteration 136/1000 | Loss: 0.00000810
Iteration 137/1000 | Loss: 0.00000810
Iteration 138/1000 | Loss: 0.00000810
Iteration 139/1000 | Loss: 0.00000810
Iteration 140/1000 | Loss: 0.00000810
Iteration 141/1000 | Loss: 0.00000810
Iteration 142/1000 | Loss: 0.00000810
Iteration 143/1000 | Loss: 0.00000810
Iteration 144/1000 | Loss: 0.00000810
Iteration 145/1000 | Loss: 0.00000810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [8.09845187177416e-06, 8.09845187177416e-06, 8.09845187177416e-06, 8.09845187177416e-06, 8.09845187177416e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.09845187177416e-06

Optimization complete. Final v2v error: 2.463832139968872 mm

Highest mean error: 2.8210036754608154 mm for frame 95

Lowest mean error: 2.3798632621765137 mm for frame 12

Saving results

Total time: 87.79357147216797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018426
Iteration 2/25 | Loss: 0.00179277
Iteration 3/25 | Loss: 0.00146145
Iteration 4/25 | Loss: 0.00143138
Iteration 5/25 | Loss: 0.00142222
Iteration 6/25 | Loss: 0.00135295
Iteration 7/25 | Loss: 0.00132514
Iteration 8/25 | Loss: 0.00133720
Iteration 9/25 | Loss: 0.00129627
Iteration 10/25 | Loss: 0.00129507
Iteration 11/25 | Loss: 0.00127211
Iteration 12/25 | Loss: 0.00127396
Iteration 13/25 | Loss: 0.00125683
Iteration 14/25 | Loss: 0.00124141
Iteration 15/25 | Loss: 0.00124056
Iteration 16/25 | Loss: 0.00123734
Iteration 17/25 | Loss: 0.00123619
Iteration 18/25 | Loss: 0.00123511
Iteration 19/25 | Loss: 0.00123645
Iteration 20/25 | Loss: 0.00123193
Iteration 21/25 | Loss: 0.00123087
Iteration 22/25 | Loss: 0.00123036
Iteration 23/25 | Loss: 0.00123023
Iteration 24/25 | Loss: 0.00123019
Iteration 25/25 | Loss: 0.00123018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30743349
Iteration 2/25 | Loss: 0.00170527
Iteration 3/25 | Loss: 0.00170527
Iteration 4/25 | Loss: 0.00170527
Iteration 5/25 | Loss: 0.00170527
Iteration 6/25 | Loss: 0.00170527
Iteration 7/25 | Loss: 0.00170527
Iteration 8/25 | Loss: 0.00170527
Iteration 9/25 | Loss: 0.00170527
Iteration 10/25 | Loss: 0.00170527
Iteration 11/25 | Loss: 0.00170527
Iteration 12/25 | Loss: 0.00170527
Iteration 13/25 | Loss: 0.00170527
Iteration 14/25 | Loss: 0.00170527
Iteration 15/25 | Loss: 0.00170527
Iteration 16/25 | Loss: 0.00170527
Iteration 17/25 | Loss: 0.00170527
Iteration 18/25 | Loss: 0.00170527
Iteration 19/25 | Loss: 0.00170527
Iteration 20/25 | Loss: 0.00170527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017052689800038934, 0.0017052689800038934, 0.0017052689800038934, 0.0017052689800038934, 0.0017052689800038934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017052689800038934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170527
Iteration 2/1000 | Loss: 0.00017478
Iteration 3/1000 | Loss: 0.00004547
Iteration 4/1000 | Loss: 0.00109127
Iteration 5/1000 | Loss: 0.00062605
Iteration 6/1000 | Loss: 0.00003582
Iteration 7/1000 | Loss: 0.00105088
Iteration 8/1000 | Loss: 0.00117610
Iteration 9/1000 | Loss: 0.00073469
Iteration 10/1000 | Loss: 0.00096102
Iteration 11/1000 | Loss: 0.00020454
Iteration 12/1000 | Loss: 0.00096771
Iteration 13/1000 | Loss: 0.00113747
Iteration 14/1000 | Loss: 0.00008364
Iteration 15/1000 | Loss: 0.00005215
Iteration 16/1000 | Loss: 0.00003696
Iteration 17/1000 | Loss: 0.00007987
Iteration 18/1000 | Loss: 0.00003099
Iteration 19/1000 | Loss: 0.00006968
Iteration 20/1000 | Loss: 0.00006458
Iteration 21/1000 | Loss: 0.00003573
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00003151
Iteration 24/1000 | Loss: 0.00002426
Iteration 25/1000 | Loss: 0.00002308
Iteration 26/1000 | Loss: 0.00002202
Iteration 27/1000 | Loss: 0.00002124
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00070057
Iteration 30/1000 | Loss: 0.00004161
Iteration 31/1000 | Loss: 0.00002311
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00015284
Iteration 35/1000 | Loss: 0.00017235
Iteration 36/1000 | Loss: 0.00021760
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001477
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001434
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001426
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001418
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001416
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001410
Iteration 111/1000 | Loss: 0.00001410
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001409
Iteration 117/1000 | Loss: 0.00001409
Iteration 118/1000 | Loss: 0.00001409
Iteration 119/1000 | Loss: 0.00001408
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001407
Iteration 122/1000 | Loss: 0.00001407
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001407
Iteration 128/1000 | Loss: 0.00001407
Iteration 129/1000 | Loss: 0.00001407
Iteration 130/1000 | Loss: 0.00001407
Iteration 131/1000 | Loss: 0.00001407
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001406
Iteration 137/1000 | Loss: 0.00001406
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001406
Iteration 140/1000 | Loss: 0.00001406
Iteration 141/1000 | Loss: 0.00001406
Iteration 142/1000 | Loss: 0.00001406
Iteration 143/1000 | Loss: 0.00001405
Iteration 144/1000 | Loss: 0.00001405
Iteration 145/1000 | Loss: 0.00001405
Iteration 146/1000 | Loss: 0.00001405
Iteration 147/1000 | Loss: 0.00001405
Iteration 148/1000 | Loss: 0.00001405
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001405
Iteration 151/1000 | Loss: 0.00001405
Iteration 152/1000 | Loss: 0.00001405
Iteration 153/1000 | Loss: 0.00001405
Iteration 154/1000 | Loss: 0.00001405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.404780414304696e-05, 1.404780414304696e-05, 1.404780414304696e-05, 1.404780414304696e-05, 1.404780414304696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.404780414304696e-05

Optimization complete. Final v2v error: 3.2481272220611572 mm

Highest mean error: 3.808439254760742 mm for frame 161

Lowest mean error: 2.964357376098633 mm for frame 34

Saving results

Total time: 113.15989279747009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811667
Iteration 2/25 | Loss: 0.00160013
Iteration 3/25 | Loss: 0.00124704
Iteration 4/25 | Loss: 0.00122058
Iteration 5/25 | Loss: 0.00121784
Iteration 6/25 | Loss: 0.00121784
Iteration 7/25 | Loss: 0.00121784
Iteration 8/25 | Loss: 0.00121784
Iteration 9/25 | Loss: 0.00121784
Iteration 10/25 | Loss: 0.00121784
Iteration 11/25 | Loss: 0.00121784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012178353499621153, 0.0012178353499621153, 0.0012178353499621153, 0.0012178353499621153, 0.0012178353499621153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012178353499621153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26602900
Iteration 2/25 | Loss: 0.00092170
Iteration 3/25 | Loss: 0.00092170
Iteration 4/25 | Loss: 0.00092170
Iteration 5/25 | Loss: 0.00092170
Iteration 6/25 | Loss: 0.00092170
Iteration 7/25 | Loss: 0.00092170
Iteration 8/25 | Loss: 0.00092169
Iteration 9/25 | Loss: 0.00092169
Iteration 10/25 | Loss: 0.00092169
Iteration 11/25 | Loss: 0.00092169
Iteration 12/25 | Loss: 0.00092169
Iteration 13/25 | Loss: 0.00092169
Iteration 14/25 | Loss: 0.00092169
Iteration 15/25 | Loss: 0.00092169
Iteration 16/25 | Loss: 0.00092169
Iteration 17/25 | Loss: 0.00092169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00092169432900846, 0.00092169432900846, 0.00092169432900846, 0.00092169432900846, 0.00092169432900846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00092169432900846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092169
Iteration 2/1000 | Loss: 0.00002631
Iteration 3/1000 | Loss: 0.00001903
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00001627
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001334
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001313
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001312
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001307
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001306
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001304
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001303
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001296
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.2942467947141267e-05, 1.2942467947141267e-05, 1.2942467947141267e-05, 1.2942467947141267e-05, 1.2942467947141267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2942467947141267e-05

Optimization complete. Final v2v error: 3.0245609283447266 mm

Highest mean error: 3.2838454246520996 mm for frame 176

Lowest mean error: 2.876154899597168 mm for frame 142

Saving results

Total time: 39.735365867614746
