Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=32, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1792-1847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780807
Iteration 2/25 | Loss: 0.00156770
Iteration 3/25 | Loss: 0.00129212
Iteration 4/25 | Loss: 0.00126151
Iteration 5/25 | Loss: 0.00125626
Iteration 6/25 | Loss: 0.00125466
Iteration 7/25 | Loss: 0.00125425
Iteration 8/25 | Loss: 0.00125417
Iteration 9/25 | Loss: 0.00125417
Iteration 10/25 | Loss: 0.00125417
Iteration 11/25 | Loss: 0.00125417
Iteration 12/25 | Loss: 0.00125417
Iteration 13/25 | Loss: 0.00125417
Iteration 14/25 | Loss: 0.00125417
Iteration 15/25 | Loss: 0.00125417
Iteration 16/25 | Loss: 0.00125417
Iteration 17/25 | Loss: 0.00125417
Iteration 18/25 | Loss: 0.00125417
Iteration 19/25 | Loss: 0.00125417
Iteration 20/25 | Loss: 0.00125417
Iteration 21/25 | Loss: 0.00125417
Iteration 22/25 | Loss: 0.00125417
Iteration 23/25 | Loss: 0.00125417
Iteration 24/25 | Loss: 0.00125417
Iteration 25/25 | Loss: 0.00125417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59041023
Iteration 2/25 | Loss: 0.00112390
Iteration 3/25 | Loss: 0.00112390
Iteration 4/25 | Loss: 0.00112390
Iteration 5/25 | Loss: 0.00112390
Iteration 6/25 | Loss: 0.00112390
Iteration 7/25 | Loss: 0.00112390
Iteration 8/25 | Loss: 0.00112390
Iteration 9/25 | Loss: 0.00112390
Iteration 10/25 | Loss: 0.00112390
Iteration 11/25 | Loss: 0.00112390
Iteration 12/25 | Loss: 0.00112390
Iteration 13/25 | Loss: 0.00112390
Iteration 14/25 | Loss: 0.00112390
Iteration 15/25 | Loss: 0.00112390
Iteration 16/25 | Loss: 0.00112390
Iteration 17/25 | Loss: 0.00112390
Iteration 18/25 | Loss: 0.00112390
Iteration 19/25 | Loss: 0.00112390
Iteration 20/25 | Loss: 0.00112390
Iteration 21/25 | Loss: 0.00112390
Iteration 22/25 | Loss: 0.00112390
Iteration 23/25 | Loss: 0.00112390
Iteration 24/25 | Loss: 0.00112390
Iteration 25/25 | Loss: 0.00112390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001123899593949318, 0.001123899593949318, 0.001123899593949318, 0.001123899593949318, 0.001123899593949318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001123899593949318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112390
Iteration 2/1000 | Loss: 0.00002502
Iteration 3/1000 | Loss: 0.00002084
Iteration 4/1000 | Loss: 0.00001950
Iteration 5/1000 | Loss: 0.00016809
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002010
Iteration 8/1000 | Loss: 0.00001895
Iteration 9/1000 | Loss: 0.00015412
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00015413
Iteration 12/1000 | Loss: 0.00016665
Iteration 13/1000 | Loss: 0.00009248
Iteration 14/1000 | Loss: 0.00008273
Iteration 15/1000 | Loss: 0.00013624
Iteration 16/1000 | Loss: 0.00004440
Iteration 17/1000 | Loss: 0.00010847
Iteration 18/1000 | Loss: 0.00012239
Iteration 19/1000 | Loss: 0.00002328
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001567
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00001555
Iteration 66/1000 | Loss: 0.00001555
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001554
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001553
Iteration 78/1000 | Loss: 0.00001553
Iteration 79/1000 | Loss: 0.00001553
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001548
Iteration 139/1000 | Loss: 0.00001548
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001548
Iteration 150/1000 | Loss: 0.00001548
Iteration 151/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.5477653505513445e-05, 1.5477653505513445e-05, 1.5477653505513445e-05, 1.5477653505513445e-05, 1.5477653505513445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5477653505513445e-05

Optimization complete. Final v2v error: 3.2696404457092285 mm

Highest mean error: 5.1804680824279785 mm for frame 56

Lowest mean error: 2.9318203926086426 mm for frame 139

Saving results

Total time: 67.45544123649597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389088
Iteration 2/25 | Loss: 0.00123638
Iteration 3/25 | Loss: 0.00118396
Iteration 4/25 | Loss: 0.00117668
Iteration 5/25 | Loss: 0.00117464
Iteration 6/25 | Loss: 0.00117464
Iteration 7/25 | Loss: 0.00117464
Iteration 8/25 | Loss: 0.00117464
Iteration 9/25 | Loss: 0.00117464
Iteration 10/25 | Loss: 0.00117464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011746383970603347, 0.0011746383970603347, 0.0011746383970603347, 0.0011746383970603347, 0.0011746383970603347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011746383970603347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34333467
Iteration 2/25 | Loss: 0.00089384
Iteration 3/25 | Loss: 0.00089384
Iteration 4/25 | Loss: 0.00089384
Iteration 5/25 | Loss: 0.00089384
Iteration 6/25 | Loss: 0.00089384
Iteration 7/25 | Loss: 0.00089384
Iteration 8/25 | Loss: 0.00089384
Iteration 9/25 | Loss: 0.00089384
Iteration 10/25 | Loss: 0.00089384
Iteration 11/25 | Loss: 0.00089384
Iteration 12/25 | Loss: 0.00089384
Iteration 13/25 | Loss: 0.00089384
Iteration 14/25 | Loss: 0.00089384
Iteration 15/25 | Loss: 0.00089384
Iteration 16/25 | Loss: 0.00089384
Iteration 17/25 | Loss: 0.00089384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008938354440033436, 0.0008938354440033436, 0.0008938354440033436, 0.0008938354440033436, 0.0008938354440033436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008938354440033436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089384
Iteration 2/1000 | Loss: 0.00002261
Iteration 3/1000 | Loss: 0.00001791
Iteration 4/1000 | Loss: 0.00001557
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001366
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001212
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001161
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001138
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001134
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001133
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001132
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001118
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001104
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001103
Iteration 61/1000 | Loss: 0.00001100
Iteration 62/1000 | Loss: 0.00001100
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001095
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001094
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001087
Iteration 89/1000 | Loss: 0.00001086
Iteration 90/1000 | Loss: 0.00001086
Iteration 91/1000 | Loss: 0.00001085
Iteration 92/1000 | Loss: 0.00001085
Iteration 93/1000 | Loss: 0.00001085
Iteration 94/1000 | Loss: 0.00001084
Iteration 95/1000 | Loss: 0.00001084
Iteration 96/1000 | Loss: 0.00001084
Iteration 97/1000 | Loss: 0.00001083
Iteration 98/1000 | Loss: 0.00001083
Iteration 99/1000 | Loss: 0.00001083
Iteration 100/1000 | Loss: 0.00001083
Iteration 101/1000 | Loss: 0.00001083
Iteration 102/1000 | Loss: 0.00001082
Iteration 103/1000 | Loss: 0.00001082
Iteration 104/1000 | Loss: 0.00001082
Iteration 105/1000 | Loss: 0.00001082
Iteration 106/1000 | Loss: 0.00001082
Iteration 107/1000 | Loss: 0.00001082
Iteration 108/1000 | Loss: 0.00001082
Iteration 109/1000 | Loss: 0.00001082
Iteration 110/1000 | Loss: 0.00001081
Iteration 111/1000 | Loss: 0.00001081
Iteration 112/1000 | Loss: 0.00001081
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001079
Iteration 119/1000 | Loss: 0.00001079
Iteration 120/1000 | Loss: 0.00001079
Iteration 121/1000 | Loss: 0.00001079
Iteration 122/1000 | Loss: 0.00001079
Iteration 123/1000 | Loss: 0.00001079
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001078
Iteration 126/1000 | Loss: 0.00001078
Iteration 127/1000 | Loss: 0.00001078
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001078
Iteration 139/1000 | Loss: 0.00001078
Iteration 140/1000 | Loss: 0.00001078
Iteration 141/1000 | Loss: 0.00001078
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001077
Iteration 146/1000 | Loss: 0.00001077
Iteration 147/1000 | Loss: 0.00001077
Iteration 148/1000 | Loss: 0.00001077
Iteration 149/1000 | Loss: 0.00001077
Iteration 150/1000 | Loss: 0.00001077
Iteration 151/1000 | Loss: 0.00001077
Iteration 152/1000 | Loss: 0.00001077
Iteration 153/1000 | Loss: 0.00001077
Iteration 154/1000 | Loss: 0.00001077
Iteration 155/1000 | Loss: 0.00001077
Iteration 156/1000 | Loss: 0.00001077
Iteration 157/1000 | Loss: 0.00001077
Iteration 158/1000 | Loss: 0.00001077
Iteration 159/1000 | Loss: 0.00001077
Iteration 160/1000 | Loss: 0.00001077
Iteration 161/1000 | Loss: 0.00001077
Iteration 162/1000 | Loss: 0.00001077
Iteration 163/1000 | Loss: 0.00001077
Iteration 164/1000 | Loss: 0.00001077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.0774279871839099e-05, 1.0774279871839099e-05, 1.0774279871839099e-05, 1.0774279871839099e-05, 1.0774279871839099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0774279871839099e-05

Optimization complete. Final v2v error: 2.8346588611602783 mm

Highest mean error: 2.974585771560669 mm for frame 156

Lowest mean error: 2.7380430698394775 mm for frame 46

Saving results

Total time: 39.16749882698059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452355
Iteration 2/25 | Loss: 0.00147190
Iteration 3/25 | Loss: 0.00131996
Iteration 4/25 | Loss: 0.00131194
Iteration 5/25 | Loss: 0.00130997
Iteration 6/25 | Loss: 0.00130995
Iteration 7/25 | Loss: 0.00130995
Iteration 8/25 | Loss: 0.00130995
Iteration 9/25 | Loss: 0.00130995
Iteration 10/25 | Loss: 0.00130995
Iteration 11/25 | Loss: 0.00130995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013099528150632977, 0.0013099528150632977, 0.0013099528150632977, 0.0013099528150632977, 0.0013099528150632977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013099528150632977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32117522
Iteration 2/25 | Loss: 0.00097352
Iteration 3/25 | Loss: 0.00097351
Iteration 4/25 | Loss: 0.00097351
Iteration 5/25 | Loss: 0.00097351
Iteration 6/25 | Loss: 0.00097351
Iteration 7/25 | Loss: 0.00097350
Iteration 8/25 | Loss: 0.00097350
Iteration 9/25 | Loss: 0.00097350
Iteration 10/25 | Loss: 0.00097350
Iteration 11/25 | Loss: 0.00097350
Iteration 12/25 | Loss: 0.00097350
Iteration 13/25 | Loss: 0.00097350
Iteration 14/25 | Loss: 0.00097350
Iteration 15/25 | Loss: 0.00097350
Iteration 16/25 | Loss: 0.00097350
Iteration 17/25 | Loss: 0.00097350
Iteration 18/25 | Loss: 0.00097350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009735040366649628, 0.0009735040366649628, 0.0009735040366649628, 0.0009735040366649628, 0.0009735040366649628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009735040366649628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097350
Iteration 2/1000 | Loss: 0.00003194
Iteration 3/1000 | Loss: 0.00001818
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001449
Iteration 6/1000 | Loss: 0.00001396
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001329
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001325
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001313
Iteration 14/1000 | Loss: 0.00001312
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001306
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001284
Iteration 47/1000 | Loss: 0.00001284
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001274
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001269
Iteration 56/1000 | Loss: 0.00001269
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001269
Iteration 59/1000 | Loss: 0.00001269
Iteration 60/1000 | Loss: 0.00001269
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001269
Iteration 65/1000 | Loss: 0.00001269
Iteration 66/1000 | Loss: 0.00001269
Iteration 67/1000 | Loss: 0.00001269
Iteration 68/1000 | Loss: 0.00001269
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Iteration 71/1000 | Loss: 0.00001268
Iteration 72/1000 | Loss: 0.00001268
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001267
Iteration 86/1000 | Loss: 0.00001267
Iteration 87/1000 | Loss: 0.00001266
Iteration 88/1000 | Loss: 0.00001266
Iteration 89/1000 | Loss: 0.00001266
Iteration 90/1000 | Loss: 0.00001266
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001263
Iteration 103/1000 | Loss: 0.00001263
Iteration 104/1000 | Loss: 0.00001263
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001262
Iteration 122/1000 | Loss: 0.00001262
Iteration 123/1000 | Loss: 0.00001262
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001262
Iteration 126/1000 | Loss: 0.00001262
Iteration 127/1000 | Loss: 0.00001262
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001260
Iteration 138/1000 | Loss: 0.00001260
Iteration 139/1000 | Loss: 0.00001260
Iteration 140/1000 | Loss: 0.00001260
Iteration 141/1000 | Loss: 0.00001260
Iteration 142/1000 | Loss: 0.00001259
Iteration 143/1000 | Loss: 0.00001259
Iteration 144/1000 | Loss: 0.00001259
Iteration 145/1000 | Loss: 0.00001259
Iteration 146/1000 | Loss: 0.00001259
Iteration 147/1000 | Loss: 0.00001259
Iteration 148/1000 | Loss: 0.00001259
Iteration 149/1000 | Loss: 0.00001259
Iteration 150/1000 | Loss: 0.00001259
Iteration 151/1000 | Loss: 0.00001259
Iteration 152/1000 | Loss: 0.00001258
Iteration 153/1000 | Loss: 0.00001258
Iteration 154/1000 | Loss: 0.00001258
Iteration 155/1000 | Loss: 0.00001258
Iteration 156/1000 | Loss: 0.00001258
Iteration 157/1000 | Loss: 0.00001258
Iteration 158/1000 | Loss: 0.00001258
Iteration 159/1000 | Loss: 0.00001258
Iteration 160/1000 | Loss: 0.00001258
Iteration 161/1000 | Loss: 0.00001258
Iteration 162/1000 | Loss: 0.00001258
Iteration 163/1000 | Loss: 0.00001258
Iteration 164/1000 | Loss: 0.00001258
Iteration 165/1000 | Loss: 0.00001258
Iteration 166/1000 | Loss: 0.00001258
Iteration 167/1000 | Loss: 0.00001258
Iteration 168/1000 | Loss: 0.00001258
Iteration 169/1000 | Loss: 0.00001258
Iteration 170/1000 | Loss: 0.00001258
Iteration 171/1000 | Loss: 0.00001258
Iteration 172/1000 | Loss: 0.00001258
Iteration 173/1000 | Loss: 0.00001258
Iteration 174/1000 | Loss: 0.00001258
Iteration 175/1000 | Loss: 0.00001258
Iteration 176/1000 | Loss: 0.00001258
Iteration 177/1000 | Loss: 0.00001258
Iteration 178/1000 | Loss: 0.00001258
Iteration 179/1000 | Loss: 0.00001258
Iteration 180/1000 | Loss: 0.00001258
Iteration 181/1000 | Loss: 0.00001258
Iteration 182/1000 | Loss: 0.00001258
Iteration 183/1000 | Loss: 0.00001258
Iteration 184/1000 | Loss: 0.00001258
Iteration 185/1000 | Loss: 0.00001258
Iteration 186/1000 | Loss: 0.00001258
Iteration 187/1000 | Loss: 0.00001258
Iteration 188/1000 | Loss: 0.00001258
Iteration 189/1000 | Loss: 0.00001258
Iteration 190/1000 | Loss: 0.00001258
Iteration 191/1000 | Loss: 0.00001258
Iteration 192/1000 | Loss: 0.00001258
Iteration 193/1000 | Loss: 0.00001258
Iteration 194/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.2577269444591366e-05, 1.2577269444591366e-05, 1.2577269444591366e-05, 1.2577269444591366e-05, 1.2577269444591366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2577269444591366e-05

Optimization complete. Final v2v error: 3.0384225845336914 mm

Highest mean error: 3.2769837379455566 mm for frame 94

Lowest mean error: 2.9024009704589844 mm for frame 155

Saving results

Total time: 35.26394176483154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004920
Iteration 2/25 | Loss: 0.00225331
Iteration 3/25 | Loss: 0.00175005
Iteration 4/25 | Loss: 0.00168693
Iteration 5/25 | Loss: 0.00153359
Iteration 6/25 | Loss: 0.00145723
Iteration 7/25 | Loss: 0.00135270
Iteration 8/25 | Loss: 0.00135341
Iteration 9/25 | Loss: 0.00130430
Iteration 10/25 | Loss: 0.00129782
Iteration 11/25 | Loss: 0.00126867
Iteration 12/25 | Loss: 0.00127639
Iteration 13/25 | Loss: 0.00126093
Iteration 14/25 | Loss: 0.00126344
Iteration 15/25 | Loss: 0.00126169
Iteration 16/25 | Loss: 0.00125674
Iteration 17/25 | Loss: 0.00125435
Iteration 18/25 | Loss: 0.00125439
Iteration 19/25 | Loss: 0.00125350
Iteration 20/25 | Loss: 0.00125336
Iteration 21/25 | Loss: 0.00125336
Iteration 22/25 | Loss: 0.00125336
Iteration 23/25 | Loss: 0.00125336
Iteration 24/25 | Loss: 0.00125336
Iteration 25/25 | Loss: 0.00125336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53834879
Iteration 2/25 | Loss: 0.00165871
Iteration 3/25 | Loss: 0.00099700
Iteration 4/25 | Loss: 0.00091965
Iteration 5/25 | Loss: 0.00091964
Iteration 6/25 | Loss: 0.00091964
Iteration 7/25 | Loss: 0.00091964
Iteration 8/25 | Loss: 0.00091964
Iteration 9/25 | Loss: 0.00091964
Iteration 10/25 | Loss: 0.00091964
Iteration 11/25 | Loss: 0.00091964
Iteration 12/25 | Loss: 0.00091964
Iteration 13/25 | Loss: 0.00091964
Iteration 14/25 | Loss: 0.00091964
Iteration 15/25 | Loss: 0.00091964
Iteration 16/25 | Loss: 0.00091964
Iteration 17/25 | Loss: 0.00091964
Iteration 18/25 | Loss: 0.00091964
Iteration 19/25 | Loss: 0.00091964
Iteration 20/25 | Loss: 0.00091964
Iteration 21/25 | Loss: 0.00091964
Iteration 22/25 | Loss: 0.00091964
Iteration 23/25 | Loss: 0.00091964
Iteration 24/25 | Loss: 0.00091964
Iteration 25/25 | Loss: 0.00091964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091964
Iteration 2/1000 | Loss: 0.00025587
Iteration 3/1000 | Loss: 0.00025918
Iteration 4/1000 | Loss: 0.00005026
Iteration 5/1000 | Loss: 0.00004084
Iteration 6/1000 | Loss: 0.00003643
Iteration 7/1000 | Loss: 0.00016136
Iteration 8/1000 | Loss: 0.00069826
Iteration 9/1000 | Loss: 0.00057741
Iteration 10/1000 | Loss: 0.00003546
Iteration 11/1000 | Loss: 0.00003836
Iteration 12/1000 | Loss: 0.00012138
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00005495
Iteration 15/1000 | Loss: 0.00002541
Iteration 16/1000 | Loss: 0.00006158
Iteration 17/1000 | Loss: 0.00004122
Iteration 18/1000 | Loss: 0.00003245
Iteration 19/1000 | Loss: 0.00010521
Iteration 20/1000 | Loss: 0.00007139
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001656
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00002967
Iteration 25/1000 | Loss: 0.00007920
Iteration 26/1000 | Loss: 0.00003535
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00001618
Iteration 29/1000 | Loss: 0.00001617
Iteration 30/1000 | Loss: 0.00001617
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001614
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00002606
Iteration 35/1000 | Loss: 0.00005767
Iteration 36/1000 | Loss: 0.00001600
Iteration 37/1000 | Loss: 0.00001599
Iteration 38/1000 | Loss: 0.00001599
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001599
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00002630
Iteration 48/1000 | Loss: 0.00002766
Iteration 49/1000 | Loss: 0.00001598
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001594
Iteration 53/1000 | Loss: 0.00001594
Iteration 54/1000 | Loss: 0.00003508
Iteration 55/1000 | Loss: 0.00006855
Iteration 56/1000 | Loss: 0.00039269
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00002477
Iteration 59/1000 | Loss: 0.00001587
Iteration 60/1000 | Loss: 0.00001587
Iteration 61/1000 | Loss: 0.00002601
Iteration 62/1000 | Loss: 0.00001813
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001577
Iteration 65/1000 | Loss: 0.00001577
Iteration 66/1000 | Loss: 0.00001577
Iteration 67/1000 | Loss: 0.00001577
Iteration 68/1000 | Loss: 0.00001577
Iteration 69/1000 | Loss: 0.00002889
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001577
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00001577
Iteration 77/1000 | Loss: 0.00001576
Iteration 78/1000 | Loss: 0.00001576
Iteration 79/1000 | Loss: 0.00001576
Iteration 80/1000 | Loss: 0.00001576
Iteration 81/1000 | Loss: 0.00001576
Iteration 82/1000 | Loss: 0.00001576
Iteration 83/1000 | Loss: 0.00001575
Iteration 84/1000 | Loss: 0.00001575
Iteration 85/1000 | Loss: 0.00001575
Iteration 86/1000 | Loss: 0.00001575
Iteration 87/1000 | Loss: 0.00001574
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001574
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001574
Iteration 92/1000 | Loss: 0.00001574
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001574
Iteration 95/1000 | Loss: 0.00001573
Iteration 96/1000 | Loss: 0.00001573
Iteration 97/1000 | Loss: 0.00001573
Iteration 98/1000 | Loss: 0.00001573
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001574
Iteration 101/1000 | Loss: 0.00001574
Iteration 102/1000 | Loss: 0.00001574
Iteration 103/1000 | Loss: 0.00001574
Iteration 104/1000 | Loss: 0.00001574
Iteration 105/1000 | Loss: 0.00001574
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001574
Iteration 110/1000 | Loss: 0.00001574
Iteration 111/1000 | Loss: 0.00001574
Iteration 112/1000 | Loss: 0.00001574
Iteration 113/1000 | Loss: 0.00001574
Iteration 114/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.5737057765363716e-05, 1.5737057765363716e-05, 1.5737057765363716e-05, 1.5737057765363716e-05, 1.5737057765363716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5737057765363716e-05

Optimization complete. Final v2v error: 3.3348116874694824 mm

Highest mean error: 3.582468271255493 mm for frame 161

Lowest mean error: 3.037367820739746 mm for frame 247

Saving results

Total time: 112.7671263217926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400957
Iteration 2/25 | Loss: 0.00128554
Iteration 3/25 | Loss: 0.00121300
Iteration 4/25 | Loss: 0.00120091
Iteration 5/25 | Loss: 0.00119654
Iteration 6/25 | Loss: 0.00119572
Iteration 7/25 | Loss: 0.00119561
Iteration 8/25 | Loss: 0.00119561
Iteration 9/25 | Loss: 0.00119561
Iteration 10/25 | Loss: 0.00119561
Iteration 11/25 | Loss: 0.00119561
Iteration 12/25 | Loss: 0.00119561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011956134112551808, 0.0011956134112551808, 0.0011956134112551808, 0.0011956134112551808, 0.0011956134112551808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011956134112551808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85378897
Iteration 2/25 | Loss: 0.00096725
Iteration 3/25 | Loss: 0.00096725
Iteration 4/25 | Loss: 0.00096724
Iteration 5/25 | Loss: 0.00096724
Iteration 6/25 | Loss: 0.00096724
Iteration 7/25 | Loss: 0.00096724
Iteration 8/25 | Loss: 0.00096724
Iteration 9/25 | Loss: 0.00096724
Iteration 10/25 | Loss: 0.00096724
Iteration 11/25 | Loss: 0.00096724
Iteration 12/25 | Loss: 0.00096724
Iteration 13/25 | Loss: 0.00096724
Iteration 14/25 | Loss: 0.00096724
Iteration 15/25 | Loss: 0.00096724
Iteration 16/25 | Loss: 0.00096724
Iteration 17/25 | Loss: 0.00096724
Iteration 18/25 | Loss: 0.00096724
Iteration 19/25 | Loss: 0.00096724
Iteration 20/25 | Loss: 0.00096724
Iteration 21/25 | Loss: 0.00096724
Iteration 22/25 | Loss: 0.00096724
Iteration 23/25 | Loss: 0.00096724
Iteration 24/25 | Loss: 0.00096724
Iteration 25/25 | Loss: 0.00096724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096724
Iteration 2/1000 | Loss: 0.00002706
Iteration 3/1000 | Loss: 0.00001914
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001534
Iteration 6/1000 | Loss: 0.00001462
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001253
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001236
Iteration 48/1000 | Loss: 0.00001236
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001231
Iteration 65/1000 | Loss: 0.00001231
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001213
Iteration 101/1000 | Loss: 0.00001213
Iteration 102/1000 | Loss: 0.00001213
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001212
Iteration 106/1000 | Loss: 0.00001212
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001212
Iteration 112/1000 | Loss: 0.00001212
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001211
Iteration 120/1000 | Loss: 0.00001210
Iteration 121/1000 | Loss: 0.00001210
Iteration 122/1000 | Loss: 0.00001210
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001209
Iteration 130/1000 | Loss: 0.00001209
Iteration 131/1000 | Loss: 0.00001209
Iteration 132/1000 | Loss: 0.00001209
Iteration 133/1000 | Loss: 0.00001209
Iteration 134/1000 | Loss: 0.00001209
Iteration 135/1000 | Loss: 0.00001209
Iteration 136/1000 | Loss: 0.00001209
Iteration 137/1000 | Loss: 0.00001209
Iteration 138/1000 | Loss: 0.00001209
Iteration 139/1000 | Loss: 0.00001209
Iteration 140/1000 | Loss: 0.00001209
Iteration 141/1000 | Loss: 0.00001209
Iteration 142/1000 | Loss: 0.00001209
Iteration 143/1000 | Loss: 0.00001209
Iteration 144/1000 | Loss: 0.00001209
Iteration 145/1000 | Loss: 0.00001208
Iteration 146/1000 | Loss: 0.00001208
Iteration 147/1000 | Loss: 0.00001208
Iteration 148/1000 | Loss: 0.00001208
Iteration 149/1000 | Loss: 0.00001208
Iteration 150/1000 | Loss: 0.00001208
Iteration 151/1000 | Loss: 0.00001208
Iteration 152/1000 | Loss: 0.00001208
Iteration 153/1000 | Loss: 0.00001208
Iteration 154/1000 | Loss: 0.00001208
Iteration 155/1000 | Loss: 0.00001208
Iteration 156/1000 | Loss: 0.00001208
Iteration 157/1000 | Loss: 0.00001208
Iteration 158/1000 | Loss: 0.00001208
Iteration 159/1000 | Loss: 0.00001208
Iteration 160/1000 | Loss: 0.00001207
Iteration 161/1000 | Loss: 0.00001207
Iteration 162/1000 | Loss: 0.00001207
Iteration 163/1000 | Loss: 0.00001207
Iteration 164/1000 | Loss: 0.00001207
Iteration 165/1000 | Loss: 0.00001207
Iteration 166/1000 | Loss: 0.00001207
Iteration 167/1000 | Loss: 0.00001207
Iteration 168/1000 | Loss: 0.00001207
Iteration 169/1000 | Loss: 0.00001207
Iteration 170/1000 | Loss: 0.00001207
Iteration 171/1000 | Loss: 0.00001207
Iteration 172/1000 | Loss: 0.00001207
Iteration 173/1000 | Loss: 0.00001207
Iteration 174/1000 | Loss: 0.00001207
Iteration 175/1000 | Loss: 0.00001207
Iteration 176/1000 | Loss: 0.00001207
Iteration 177/1000 | Loss: 0.00001207
Iteration 178/1000 | Loss: 0.00001207
Iteration 179/1000 | Loss: 0.00001207
Iteration 180/1000 | Loss: 0.00001207
Iteration 181/1000 | Loss: 0.00001206
Iteration 182/1000 | Loss: 0.00001206
Iteration 183/1000 | Loss: 0.00001206
Iteration 184/1000 | Loss: 0.00001206
Iteration 185/1000 | Loss: 0.00001206
Iteration 186/1000 | Loss: 0.00001206
Iteration 187/1000 | Loss: 0.00001206
Iteration 188/1000 | Loss: 0.00001206
Iteration 189/1000 | Loss: 0.00001206
Iteration 190/1000 | Loss: 0.00001206
Iteration 191/1000 | Loss: 0.00001206
Iteration 192/1000 | Loss: 0.00001206
Iteration 193/1000 | Loss: 0.00001206
Iteration 194/1000 | Loss: 0.00001206
Iteration 195/1000 | Loss: 0.00001206
Iteration 196/1000 | Loss: 0.00001206
Iteration 197/1000 | Loss: 0.00001206
Iteration 198/1000 | Loss: 0.00001206
Iteration 199/1000 | Loss: 0.00001206
Iteration 200/1000 | Loss: 0.00001206
Iteration 201/1000 | Loss: 0.00001206
Iteration 202/1000 | Loss: 0.00001206
Iteration 203/1000 | Loss: 0.00001206
Iteration 204/1000 | Loss: 0.00001206
Iteration 205/1000 | Loss: 0.00001206
Iteration 206/1000 | Loss: 0.00001206
Iteration 207/1000 | Loss: 0.00001206
Iteration 208/1000 | Loss: 0.00001206
Iteration 209/1000 | Loss: 0.00001206
Iteration 210/1000 | Loss: 0.00001206
Iteration 211/1000 | Loss: 0.00001206
Iteration 212/1000 | Loss: 0.00001206
Iteration 213/1000 | Loss: 0.00001206
Iteration 214/1000 | Loss: 0.00001206
Iteration 215/1000 | Loss: 0.00001206
Iteration 216/1000 | Loss: 0.00001206
Iteration 217/1000 | Loss: 0.00001206
Iteration 218/1000 | Loss: 0.00001206
Iteration 219/1000 | Loss: 0.00001206
Iteration 220/1000 | Loss: 0.00001206
Iteration 221/1000 | Loss: 0.00001206
Iteration 222/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.206234264827799e-05, 1.206234264827799e-05, 1.206234264827799e-05, 1.206234264827799e-05, 1.206234264827799e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.206234264827799e-05

Optimization complete. Final v2v error: 2.9824020862579346 mm

Highest mean error: 3.543431043624878 mm for frame 73

Lowest mean error: 2.6952059268951416 mm for frame 95

Saving results

Total time: 41.859206438064575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841884
Iteration 2/25 | Loss: 0.00145890
Iteration 3/25 | Loss: 0.00130440
Iteration 4/25 | Loss: 0.00128415
Iteration 5/25 | Loss: 0.00127674
Iteration 6/25 | Loss: 0.00127473
Iteration 7/25 | Loss: 0.00127402
Iteration 8/25 | Loss: 0.00127402
Iteration 9/25 | Loss: 0.00127402
Iteration 10/25 | Loss: 0.00127402
Iteration 11/25 | Loss: 0.00127402
Iteration 12/25 | Loss: 0.00127402
Iteration 13/25 | Loss: 0.00127402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00127402285579592, 0.00127402285579592, 0.00127402285579592, 0.00127402285579592, 0.00127402285579592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127402285579592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36646938
Iteration 2/25 | Loss: 0.00113098
Iteration 3/25 | Loss: 0.00113097
Iteration 4/25 | Loss: 0.00113097
Iteration 5/25 | Loss: 0.00113097
Iteration 6/25 | Loss: 0.00113097
Iteration 7/25 | Loss: 0.00113097
Iteration 8/25 | Loss: 0.00113097
Iteration 9/25 | Loss: 0.00113097
Iteration 10/25 | Loss: 0.00113097
Iteration 11/25 | Loss: 0.00113097
Iteration 12/25 | Loss: 0.00113097
Iteration 13/25 | Loss: 0.00113097
Iteration 14/25 | Loss: 0.00113097
Iteration 15/25 | Loss: 0.00113097
Iteration 16/25 | Loss: 0.00113097
Iteration 17/25 | Loss: 0.00113097
Iteration 18/25 | Loss: 0.00113097
Iteration 19/25 | Loss: 0.00113097
Iteration 20/25 | Loss: 0.00113097
Iteration 21/25 | Loss: 0.00113097
Iteration 22/25 | Loss: 0.00113097
Iteration 23/25 | Loss: 0.00113097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011309696128591895, 0.0011309696128591895, 0.0011309696128591895, 0.0011309696128591895, 0.0011309696128591895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011309696128591895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113097
Iteration 2/1000 | Loss: 0.00005686
Iteration 3/1000 | Loss: 0.00003433
Iteration 4/1000 | Loss: 0.00002614
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002192
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001933
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001869
Iteration 14/1000 | Loss: 0.00001849
Iteration 15/1000 | Loss: 0.00001836
Iteration 16/1000 | Loss: 0.00001835
Iteration 17/1000 | Loss: 0.00001834
Iteration 18/1000 | Loss: 0.00001827
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001823
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001822
Iteration 23/1000 | Loss: 0.00001821
Iteration 24/1000 | Loss: 0.00001820
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001816
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001815
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001814
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001808
Iteration 35/1000 | Loss: 0.00001805
Iteration 36/1000 | Loss: 0.00001802
Iteration 37/1000 | Loss: 0.00001802
Iteration 38/1000 | Loss: 0.00001801
Iteration 39/1000 | Loss: 0.00001801
Iteration 40/1000 | Loss: 0.00001800
Iteration 41/1000 | Loss: 0.00001800
Iteration 42/1000 | Loss: 0.00001800
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001798
Iteration 45/1000 | Loss: 0.00001798
Iteration 46/1000 | Loss: 0.00001797
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001795
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001795
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001794
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001790
Iteration 73/1000 | Loss: 0.00001790
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001790
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001789
Iteration 80/1000 | Loss: 0.00001789
Iteration 81/1000 | Loss: 0.00001789
Iteration 82/1000 | Loss: 0.00001788
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001787
Iteration 85/1000 | Loss: 0.00001787
Iteration 86/1000 | Loss: 0.00001787
Iteration 87/1000 | Loss: 0.00001787
Iteration 88/1000 | Loss: 0.00001787
Iteration 89/1000 | Loss: 0.00001787
Iteration 90/1000 | Loss: 0.00001787
Iteration 91/1000 | Loss: 0.00001787
Iteration 92/1000 | Loss: 0.00001787
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001786
Iteration 99/1000 | Loss: 0.00001786
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001785
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001779
Iteration 137/1000 | Loss: 0.00001779
Iteration 138/1000 | Loss: 0.00001779
Iteration 139/1000 | Loss: 0.00001779
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001778
Iteration 143/1000 | Loss: 0.00001778
Iteration 144/1000 | Loss: 0.00001777
Iteration 145/1000 | Loss: 0.00001777
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001777
Iteration 148/1000 | Loss: 0.00001777
Iteration 149/1000 | Loss: 0.00001777
Iteration 150/1000 | Loss: 0.00001776
Iteration 151/1000 | Loss: 0.00001776
Iteration 152/1000 | Loss: 0.00001776
Iteration 153/1000 | Loss: 0.00001776
Iteration 154/1000 | Loss: 0.00001776
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001776
Iteration 162/1000 | Loss: 0.00001775
Iteration 163/1000 | Loss: 0.00001775
Iteration 164/1000 | Loss: 0.00001775
Iteration 165/1000 | Loss: 0.00001775
Iteration 166/1000 | Loss: 0.00001775
Iteration 167/1000 | Loss: 0.00001775
Iteration 168/1000 | Loss: 0.00001775
Iteration 169/1000 | Loss: 0.00001775
Iteration 170/1000 | Loss: 0.00001774
Iteration 171/1000 | Loss: 0.00001774
Iteration 172/1000 | Loss: 0.00001774
Iteration 173/1000 | Loss: 0.00001774
Iteration 174/1000 | Loss: 0.00001774
Iteration 175/1000 | Loss: 0.00001774
Iteration 176/1000 | Loss: 0.00001774
Iteration 177/1000 | Loss: 0.00001774
Iteration 178/1000 | Loss: 0.00001774
Iteration 179/1000 | Loss: 0.00001774
Iteration 180/1000 | Loss: 0.00001773
Iteration 181/1000 | Loss: 0.00001773
Iteration 182/1000 | Loss: 0.00001773
Iteration 183/1000 | Loss: 0.00001773
Iteration 184/1000 | Loss: 0.00001773
Iteration 185/1000 | Loss: 0.00001773
Iteration 186/1000 | Loss: 0.00001773
Iteration 187/1000 | Loss: 0.00001773
Iteration 188/1000 | Loss: 0.00001773
Iteration 189/1000 | Loss: 0.00001772
Iteration 190/1000 | Loss: 0.00001772
Iteration 191/1000 | Loss: 0.00001772
Iteration 192/1000 | Loss: 0.00001772
Iteration 193/1000 | Loss: 0.00001772
Iteration 194/1000 | Loss: 0.00001772
Iteration 195/1000 | Loss: 0.00001772
Iteration 196/1000 | Loss: 0.00001772
Iteration 197/1000 | Loss: 0.00001771
Iteration 198/1000 | Loss: 0.00001771
Iteration 199/1000 | Loss: 0.00001771
Iteration 200/1000 | Loss: 0.00001771
Iteration 201/1000 | Loss: 0.00001771
Iteration 202/1000 | Loss: 0.00001770
Iteration 203/1000 | Loss: 0.00001770
Iteration 204/1000 | Loss: 0.00001770
Iteration 205/1000 | Loss: 0.00001770
Iteration 206/1000 | Loss: 0.00001770
Iteration 207/1000 | Loss: 0.00001770
Iteration 208/1000 | Loss: 0.00001770
Iteration 209/1000 | Loss: 0.00001770
Iteration 210/1000 | Loss: 0.00001770
Iteration 211/1000 | Loss: 0.00001770
Iteration 212/1000 | Loss: 0.00001769
Iteration 213/1000 | Loss: 0.00001769
Iteration 214/1000 | Loss: 0.00001769
Iteration 215/1000 | Loss: 0.00001769
Iteration 216/1000 | Loss: 0.00001769
Iteration 217/1000 | Loss: 0.00001769
Iteration 218/1000 | Loss: 0.00001769
Iteration 219/1000 | Loss: 0.00001769
Iteration 220/1000 | Loss: 0.00001769
Iteration 221/1000 | Loss: 0.00001769
Iteration 222/1000 | Loss: 0.00001769
Iteration 223/1000 | Loss: 0.00001768
Iteration 224/1000 | Loss: 0.00001768
Iteration 225/1000 | Loss: 0.00001768
Iteration 226/1000 | Loss: 0.00001768
Iteration 227/1000 | Loss: 0.00001768
Iteration 228/1000 | Loss: 0.00001768
Iteration 229/1000 | Loss: 0.00001768
Iteration 230/1000 | Loss: 0.00001768
Iteration 231/1000 | Loss: 0.00001768
Iteration 232/1000 | Loss: 0.00001768
Iteration 233/1000 | Loss: 0.00001768
Iteration 234/1000 | Loss: 0.00001768
Iteration 235/1000 | Loss: 0.00001768
Iteration 236/1000 | Loss: 0.00001768
Iteration 237/1000 | Loss: 0.00001768
Iteration 238/1000 | Loss: 0.00001767
Iteration 239/1000 | Loss: 0.00001767
Iteration 240/1000 | Loss: 0.00001767
Iteration 241/1000 | Loss: 0.00001767
Iteration 242/1000 | Loss: 0.00001767
Iteration 243/1000 | Loss: 0.00001767
Iteration 244/1000 | Loss: 0.00001767
Iteration 245/1000 | Loss: 0.00001767
Iteration 246/1000 | Loss: 0.00001767
Iteration 247/1000 | Loss: 0.00001767
Iteration 248/1000 | Loss: 0.00001767
Iteration 249/1000 | Loss: 0.00001767
Iteration 250/1000 | Loss: 0.00001767
Iteration 251/1000 | Loss: 0.00001767
Iteration 252/1000 | Loss: 0.00001767
Iteration 253/1000 | Loss: 0.00001767
Iteration 254/1000 | Loss: 0.00001766
Iteration 255/1000 | Loss: 0.00001766
Iteration 256/1000 | Loss: 0.00001766
Iteration 257/1000 | Loss: 0.00001766
Iteration 258/1000 | Loss: 0.00001766
Iteration 259/1000 | Loss: 0.00001766
Iteration 260/1000 | Loss: 0.00001766
Iteration 261/1000 | Loss: 0.00001766
Iteration 262/1000 | Loss: 0.00001766
Iteration 263/1000 | Loss: 0.00001766
Iteration 264/1000 | Loss: 0.00001766
Iteration 265/1000 | Loss: 0.00001766
Iteration 266/1000 | Loss: 0.00001766
Iteration 267/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.766418426996097e-05, 1.766418426996097e-05, 1.766418426996097e-05, 1.766418426996097e-05, 1.766418426996097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.766418426996097e-05

Optimization complete. Final v2v error: 3.4765379428863525 mm

Highest mean error: 5.660864353179932 mm for frame 70

Lowest mean error: 2.7055397033691406 mm for frame 126

Saving results

Total time: 49.594658851623535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941798
Iteration 2/25 | Loss: 0.00219155
Iteration 3/25 | Loss: 0.00163147
Iteration 4/25 | Loss: 0.00156237
Iteration 5/25 | Loss: 0.00153442
Iteration 6/25 | Loss: 0.00148686
Iteration 7/25 | Loss: 0.00137319
Iteration 8/25 | Loss: 0.00134731
Iteration 9/25 | Loss: 0.00134565
Iteration 10/25 | Loss: 0.00136023
Iteration 11/25 | Loss: 0.00136776
Iteration 12/25 | Loss: 0.00135489
Iteration 13/25 | Loss: 0.00134696
Iteration 14/25 | Loss: 0.00133813
Iteration 15/25 | Loss: 0.00133599
Iteration 16/25 | Loss: 0.00133559
Iteration 17/25 | Loss: 0.00133533
Iteration 18/25 | Loss: 0.00133507
Iteration 19/25 | Loss: 0.00134006
Iteration 20/25 | Loss: 0.00133382
Iteration 21/25 | Loss: 0.00133018
Iteration 22/25 | Loss: 0.00134874
Iteration 23/25 | Loss: 0.00134252
Iteration 24/25 | Loss: 0.00134173
Iteration 25/25 | Loss: 0.00134980

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34375274
Iteration 2/25 | Loss: 0.00245064
Iteration 3/25 | Loss: 0.00245064
Iteration 4/25 | Loss: 0.00245064
Iteration 5/25 | Loss: 0.00245064
Iteration 6/25 | Loss: 0.00245064
Iteration 7/25 | Loss: 0.00245064
Iteration 8/25 | Loss: 0.00245064
Iteration 9/25 | Loss: 0.00245064
Iteration 10/25 | Loss: 0.00245064
Iteration 11/25 | Loss: 0.00245064
Iteration 12/25 | Loss: 0.00245064
Iteration 13/25 | Loss: 0.00245064
Iteration 14/25 | Loss: 0.00245064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002450639847666025, 0.002450639847666025, 0.002450639847666025, 0.002450639847666025, 0.002450639847666025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002450639847666025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245064
Iteration 2/1000 | Loss: 0.00053086
Iteration 3/1000 | Loss: 0.00050897
Iteration 4/1000 | Loss: 0.00051943
Iteration 5/1000 | Loss: 0.00015693
Iteration 6/1000 | Loss: 0.00105477
Iteration 7/1000 | Loss: 0.00076972
Iteration 8/1000 | Loss: 0.00078263
Iteration 9/1000 | Loss: 0.00056908
Iteration 10/1000 | Loss: 0.00019960
Iteration 11/1000 | Loss: 0.00030638
Iteration 12/1000 | Loss: 0.00060371
Iteration 13/1000 | Loss: 0.00054745
Iteration 14/1000 | Loss: 0.00050918
Iteration 15/1000 | Loss: 0.00036526
Iteration 16/1000 | Loss: 0.00120127
Iteration 17/1000 | Loss: 0.00097733
Iteration 18/1000 | Loss: 0.00074472
Iteration 19/1000 | Loss: 0.00043359
Iteration 20/1000 | Loss: 0.00026384
Iteration 21/1000 | Loss: 0.00069320
Iteration 22/1000 | Loss: 0.00038520
Iteration 23/1000 | Loss: 0.00057051
Iteration 24/1000 | Loss: 0.00062234
Iteration 25/1000 | Loss: 0.00028223
Iteration 26/1000 | Loss: 0.00007920
Iteration 27/1000 | Loss: 0.00006471
Iteration 28/1000 | Loss: 0.00064901
Iteration 29/1000 | Loss: 0.00031748
Iteration 30/1000 | Loss: 0.00057122
Iteration 31/1000 | Loss: 0.00045389
Iteration 32/1000 | Loss: 0.00025934
Iteration 33/1000 | Loss: 0.00020198
Iteration 34/1000 | Loss: 0.00035037
Iteration 35/1000 | Loss: 0.00015784
Iteration 36/1000 | Loss: 0.00006729
Iteration 37/1000 | Loss: 0.00005352
Iteration 38/1000 | Loss: 0.00005064
Iteration 39/1000 | Loss: 0.00034254
Iteration 40/1000 | Loss: 0.00053320
Iteration 41/1000 | Loss: 0.00025435
Iteration 42/1000 | Loss: 0.00016223
Iteration 43/1000 | Loss: 0.00004560
Iteration 44/1000 | Loss: 0.00020269
Iteration 45/1000 | Loss: 0.00012836
Iteration 46/1000 | Loss: 0.00004227
Iteration 47/1000 | Loss: 0.00023465
Iteration 48/1000 | Loss: 0.00013620
Iteration 49/1000 | Loss: 0.00004379
Iteration 50/1000 | Loss: 0.00003910
Iteration 51/1000 | Loss: 0.00003777
Iteration 52/1000 | Loss: 0.00023703
Iteration 53/1000 | Loss: 0.00019275
Iteration 54/1000 | Loss: 0.00023412
Iteration 55/1000 | Loss: 0.00023603
Iteration 56/1000 | Loss: 0.00006481
Iteration 57/1000 | Loss: 0.00005273
Iteration 58/1000 | Loss: 0.00004522
Iteration 59/1000 | Loss: 0.00049054
Iteration 60/1000 | Loss: 0.00034983
Iteration 61/1000 | Loss: 0.00032860
Iteration 62/1000 | Loss: 0.00008011
Iteration 63/1000 | Loss: 0.00008451
Iteration 64/1000 | Loss: 0.00010929
Iteration 65/1000 | Loss: 0.00005348
Iteration 66/1000 | Loss: 0.00004965
Iteration 67/1000 | Loss: 0.00004629
Iteration 68/1000 | Loss: 0.00004424
Iteration 69/1000 | Loss: 0.00004263
Iteration 70/1000 | Loss: 0.00031732
Iteration 71/1000 | Loss: 0.00006735
Iteration 72/1000 | Loss: 0.00005358
Iteration 73/1000 | Loss: 0.00011697
Iteration 74/1000 | Loss: 0.00025582
Iteration 75/1000 | Loss: 0.00005142
Iteration 76/1000 | Loss: 0.00004671
Iteration 77/1000 | Loss: 0.00004309
Iteration 78/1000 | Loss: 0.00035522
Iteration 79/1000 | Loss: 0.00144709
Iteration 80/1000 | Loss: 0.00041146
Iteration 81/1000 | Loss: 0.00005509
Iteration 82/1000 | Loss: 0.00015955
Iteration 83/1000 | Loss: 0.00005135
Iteration 84/1000 | Loss: 0.00004228
Iteration 85/1000 | Loss: 0.00003905
Iteration 86/1000 | Loss: 0.00040588
Iteration 87/1000 | Loss: 0.00026583
Iteration 88/1000 | Loss: 0.00018475
Iteration 89/1000 | Loss: 0.00025316
Iteration 90/1000 | Loss: 0.00004393
Iteration 91/1000 | Loss: 0.00003724
Iteration 92/1000 | Loss: 0.00040005
Iteration 93/1000 | Loss: 0.00033127
Iteration 94/1000 | Loss: 0.00014043
Iteration 95/1000 | Loss: 0.00029058
Iteration 96/1000 | Loss: 0.00008598
Iteration 97/1000 | Loss: 0.00018833
Iteration 98/1000 | Loss: 0.00003799
Iteration 99/1000 | Loss: 0.00029583
Iteration 100/1000 | Loss: 0.00016421
Iteration 101/1000 | Loss: 0.00025565
Iteration 102/1000 | Loss: 0.00015062
Iteration 103/1000 | Loss: 0.00023441
Iteration 104/1000 | Loss: 0.00004079
Iteration 105/1000 | Loss: 0.00003603
Iteration 106/1000 | Loss: 0.00031653
Iteration 107/1000 | Loss: 0.00003572
Iteration 108/1000 | Loss: 0.00003282
Iteration 109/1000 | Loss: 0.00004403
Iteration 110/1000 | Loss: 0.00003039
Iteration 111/1000 | Loss: 0.00004064
Iteration 112/1000 | Loss: 0.00002936
Iteration 113/1000 | Loss: 0.00003347
Iteration 114/1000 | Loss: 0.00002882
Iteration 115/1000 | Loss: 0.00002840
Iteration 116/1000 | Loss: 0.00002824
Iteration 117/1000 | Loss: 0.00002803
Iteration 118/1000 | Loss: 0.00002784
Iteration 119/1000 | Loss: 0.00002779
Iteration 120/1000 | Loss: 0.00002758
Iteration 121/1000 | Loss: 0.00002757
Iteration 122/1000 | Loss: 0.00002757
Iteration 123/1000 | Loss: 0.00002755
Iteration 124/1000 | Loss: 0.00002740
Iteration 125/1000 | Loss: 0.00002738
Iteration 126/1000 | Loss: 0.00002734
Iteration 127/1000 | Loss: 0.00002733
Iteration 128/1000 | Loss: 0.00002733
Iteration 129/1000 | Loss: 0.00002732
Iteration 130/1000 | Loss: 0.00002730
Iteration 131/1000 | Loss: 0.00002729
Iteration 132/1000 | Loss: 0.00002728
Iteration 133/1000 | Loss: 0.00002728
Iteration 134/1000 | Loss: 0.00002723
Iteration 135/1000 | Loss: 0.00002723
Iteration 136/1000 | Loss: 0.00002723
Iteration 137/1000 | Loss: 0.00002722
Iteration 138/1000 | Loss: 0.00002721
Iteration 139/1000 | Loss: 0.00002721
Iteration 140/1000 | Loss: 0.00002721
Iteration 141/1000 | Loss: 0.00002721
Iteration 142/1000 | Loss: 0.00002721
Iteration 143/1000 | Loss: 0.00002720
Iteration 144/1000 | Loss: 0.00002720
Iteration 145/1000 | Loss: 0.00002719
Iteration 146/1000 | Loss: 0.00002719
Iteration 147/1000 | Loss: 0.00002718
Iteration 148/1000 | Loss: 0.00002715
Iteration 149/1000 | Loss: 0.00002714
Iteration 150/1000 | Loss: 0.00002714
Iteration 151/1000 | Loss: 0.00002714
Iteration 152/1000 | Loss: 0.00002713
Iteration 153/1000 | Loss: 0.00002713
Iteration 154/1000 | Loss: 0.00002712
Iteration 155/1000 | Loss: 0.00002712
Iteration 156/1000 | Loss: 0.00002711
Iteration 157/1000 | Loss: 0.00002711
Iteration 158/1000 | Loss: 0.00002710
Iteration 159/1000 | Loss: 0.00002710
Iteration 160/1000 | Loss: 0.00002710
Iteration 161/1000 | Loss: 0.00002709
Iteration 162/1000 | Loss: 0.00002709
Iteration 163/1000 | Loss: 0.00002709
Iteration 164/1000 | Loss: 0.00002709
Iteration 165/1000 | Loss: 0.00002709
Iteration 166/1000 | Loss: 0.00002709
Iteration 167/1000 | Loss: 0.00002709
Iteration 168/1000 | Loss: 0.00002709
Iteration 169/1000 | Loss: 0.00002708
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002708
Iteration 172/1000 | Loss: 0.00002708
Iteration 173/1000 | Loss: 0.00002707
Iteration 174/1000 | Loss: 0.00002707
Iteration 175/1000 | Loss: 0.00002706
Iteration 176/1000 | Loss: 0.00002705
Iteration 177/1000 | Loss: 0.00002704
Iteration 178/1000 | Loss: 0.00002703
Iteration 179/1000 | Loss: 0.00002703
Iteration 180/1000 | Loss: 0.00002702
Iteration 181/1000 | Loss: 0.00002701
Iteration 182/1000 | Loss: 0.00002701
Iteration 183/1000 | Loss: 0.00002700
Iteration 184/1000 | Loss: 0.00002700
Iteration 185/1000 | Loss: 0.00002700
Iteration 186/1000 | Loss: 0.00002699
Iteration 187/1000 | Loss: 0.00002699
Iteration 188/1000 | Loss: 0.00067202
Iteration 189/1000 | Loss: 0.00067202
Iteration 190/1000 | Loss: 0.00036971
Iteration 191/1000 | Loss: 0.00002773
Iteration 192/1000 | Loss: 0.00067866
Iteration 193/1000 | Loss: 0.00005082
Iteration 194/1000 | Loss: 0.00003612
Iteration 195/1000 | Loss: 0.00004080
Iteration 196/1000 | Loss: 0.00003129
Iteration 197/1000 | Loss: 0.00003019
Iteration 198/1000 | Loss: 0.00002974
Iteration 199/1000 | Loss: 0.00007030
Iteration 200/1000 | Loss: 0.00003117
Iteration 201/1000 | Loss: 0.00002957
Iteration 202/1000 | Loss: 0.00002931
Iteration 203/1000 | Loss: 0.00002930
Iteration 204/1000 | Loss: 0.00002929
Iteration 205/1000 | Loss: 0.00002929
Iteration 206/1000 | Loss: 0.00002928
Iteration 207/1000 | Loss: 0.00002928
Iteration 208/1000 | Loss: 0.00002928
Iteration 209/1000 | Loss: 0.00002928
Iteration 210/1000 | Loss: 0.00002925
Iteration 211/1000 | Loss: 0.00002918
Iteration 212/1000 | Loss: 0.00002902
Iteration 213/1000 | Loss: 0.00002895
Iteration 214/1000 | Loss: 0.00002894
Iteration 215/1000 | Loss: 0.00002894
Iteration 216/1000 | Loss: 0.00002893
Iteration 217/1000 | Loss: 0.00002893
Iteration 218/1000 | Loss: 0.00002891
Iteration 219/1000 | Loss: 0.00002890
Iteration 220/1000 | Loss: 0.00002890
Iteration 221/1000 | Loss: 0.00002890
Iteration 222/1000 | Loss: 0.00002889
Iteration 223/1000 | Loss: 0.00002888
Iteration 224/1000 | Loss: 0.00002888
Iteration 225/1000 | Loss: 0.00002888
Iteration 226/1000 | Loss: 0.00002888
Iteration 227/1000 | Loss: 0.00002887
Iteration 228/1000 | Loss: 0.00002887
Iteration 229/1000 | Loss: 0.00002885
Iteration 230/1000 | Loss: 0.00002885
Iteration 231/1000 | Loss: 0.00002884
Iteration 232/1000 | Loss: 0.00002884
Iteration 233/1000 | Loss: 0.00023139
Iteration 234/1000 | Loss: 0.00128484
Iteration 235/1000 | Loss: 0.00062388
Iteration 236/1000 | Loss: 0.00002979
Iteration 237/1000 | Loss: 0.00006000
Iteration 238/1000 | Loss: 0.00002999
Iteration 239/1000 | Loss: 0.00002900
Iteration 240/1000 | Loss: 0.00002893
Iteration 241/1000 | Loss: 0.00002891
Iteration 242/1000 | Loss: 0.00002891
Iteration 243/1000 | Loss: 0.00002890
Iteration 244/1000 | Loss: 0.00002890
Iteration 245/1000 | Loss: 0.00002890
Iteration 246/1000 | Loss: 0.00002890
Iteration 247/1000 | Loss: 0.00002889
Iteration 248/1000 | Loss: 0.00002889
Iteration 249/1000 | Loss: 0.00002889
Iteration 250/1000 | Loss: 0.00002889
Iteration 251/1000 | Loss: 0.00002889
Iteration 252/1000 | Loss: 0.00002889
Iteration 253/1000 | Loss: 0.00002889
Iteration 254/1000 | Loss: 0.00002889
Iteration 255/1000 | Loss: 0.00002889
Iteration 256/1000 | Loss: 0.00002889
Iteration 257/1000 | Loss: 0.00002889
Iteration 258/1000 | Loss: 0.00002889
Iteration 259/1000 | Loss: 0.00002889
Iteration 260/1000 | Loss: 0.00002888
Iteration 261/1000 | Loss: 0.00002888
Iteration 262/1000 | Loss: 0.00002888
Iteration 263/1000 | Loss: 0.00002888
Iteration 264/1000 | Loss: 0.00002888
Iteration 265/1000 | Loss: 0.00002888
Iteration 266/1000 | Loss: 0.00002888
Iteration 267/1000 | Loss: 0.00002888
Iteration 268/1000 | Loss: 0.00002888
Iteration 269/1000 | Loss: 0.00002888
Iteration 270/1000 | Loss: 0.00002888
Iteration 271/1000 | Loss: 0.00002888
Iteration 272/1000 | Loss: 0.00002888
Iteration 273/1000 | Loss: 0.00002888
Iteration 274/1000 | Loss: 0.00002888
Iteration 275/1000 | Loss: 0.00002888
Iteration 276/1000 | Loss: 0.00002888
Iteration 277/1000 | Loss: 0.00002888
Iteration 278/1000 | Loss: 0.00002888
Iteration 279/1000 | Loss: 0.00002888
Iteration 280/1000 | Loss: 0.00002888
Iteration 281/1000 | Loss: 0.00002888
Iteration 282/1000 | Loss: 0.00002888
Iteration 283/1000 | Loss: 0.00002888
Iteration 284/1000 | Loss: 0.00002888
Iteration 285/1000 | Loss: 0.00002888
Iteration 286/1000 | Loss: 0.00002888
Iteration 287/1000 | Loss: 0.00002888
Iteration 288/1000 | Loss: 0.00002888
Iteration 289/1000 | Loss: 0.00002888
Iteration 290/1000 | Loss: 0.00002888
Iteration 291/1000 | Loss: 0.00002888
Iteration 292/1000 | Loss: 0.00002888
Iteration 293/1000 | Loss: 0.00002888
Iteration 294/1000 | Loss: 0.00002888
Iteration 295/1000 | Loss: 0.00002888
Iteration 296/1000 | Loss: 0.00002888
Iteration 297/1000 | Loss: 0.00002888
Iteration 298/1000 | Loss: 0.00002888
Iteration 299/1000 | Loss: 0.00002888
Iteration 300/1000 | Loss: 0.00002888
Iteration 301/1000 | Loss: 0.00002888
Iteration 302/1000 | Loss: 0.00002888
Iteration 303/1000 | Loss: 0.00002888
Iteration 304/1000 | Loss: 0.00002888
Iteration 305/1000 | Loss: 0.00002888
Iteration 306/1000 | Loss: 0.00002888
Iteration 307/1000 | Loss: 0.00002888
Iteration 308/1000 | Loss: 0.00002888
Iteration 309/1000 | Loss: 0.00002888
Iteration 310/1000 | Loss: 0.00002888
Iteration 311/1000 | Loss: 0.00002888
Iteration 312/1000 | Loss: 0.00002888
Iteration 313/1000 | Loss: 0.00002888
Iteration 314/1000 | Loss: 0.00002888
Iteration 315/1000 | Loss: 0.00002888
Iteration 316/1000 | Loss: 0.00002888
Iteration 317/1000 | Loss: 0.00002888
Iteration 318/1000 | Loss: 0.00002888
Iteration 319/1000 | Loss: 0.00002888
Iteration 320/1000 | Loss: 0.00002888
Iteration 321/1000 | Loss: 0.00002888
Iteration 322/1000 | Loss: 0.00002888
Iteration 323/1000 | Loss: 0.00002888
Iteration 324/1000 | Loss: 0.00002888
Iteration 325/1000 | Loss: 0.00002888
Iteration 326/1000 | Loss: 0.00002888
Iteration 327/1000 | Loss: 0.00002888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [2.8876058422611095e-05, 2.8876058422611095e-05, 2.8876058422611095e-05, 2.8876058422611095e-05, 2.8876058422611095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8876058422611095e-05

Optimization complete. Final v2v error: 4.037900924682617 mm

Highest mean error: 11.911477088928223 mm for frame 21

Lowest mean error: 2.7027745246887207 mm for frame 10

Saving results

Total time: 251.13265872001648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987408
Iteration 2/25 | Loss: 0.00194078
Iteration 3/25 | Loss: 0.00156434
Iteration 4/25 | Loss: 0.00149196
Iteration 5/25 | Loss: 0.00149856
Iteration 6/25 | Loss: 0.00146905
Iteration 7/25 | Loss: 0.00140734
Iteration 8/25 | Loss: 0.00137929
Iteration 9/25 | Loss: 0.00136424
Iteration 10/25 | Loss: 0.00135341
Iteration 11/25 | Loss: 0.00134821
Iteration 12/25 | Loss: 0.00134655
Iteration 13/25 | Loss: 0.00134561
Iteration 14/25 | Loss: 0.00134522
Iteration 15/25 | Loss: 0.00134507
Iteration 16/25 | Loss: 0.00134503
Iteration 17/25 | Loss: 0.00134502
Iteration 18/25 | Loss: 0.00134501
Iteration 19/25 | Loss: 0.00134500
Iteration 20/25 | Loss: 0.00134500
Iteration 21/25 | Loss: 0.00134500
Iteration 22/25 | Loss: 0.00134500
Iteration 23/25 | Loss: 0.00134499
Iteration 24/25 | Loss: 0.00134499
Iteration 25/25 | Loss: 0.00134499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89798903
Iteration 2/25 | Loss: 0.00104367
Iteration 3/25 | Loss: 0.00104366
Iteration 4/25 | Loss: 0.00104366
Iteration 5/25 | Loss: 0.00104366
Iteration 6/25 | Loss: 0.00104366
Iteration 7/25 | Loss: 0.00104366
Iteration 8/25 | Loss: 0.00104366
Iteration 9/25 | Loss: 0.00104366
Iteration 10/25 | Loss: 0.00104366
Iteration 11/25 | Loss: 0.00104366
Iteration 12/25 | Loss: 0.00104366
Iteration 13/25 | Loss: 0.00104366
Iteration 14/25 | Loss: 0.00104366
Iteration 15/25 | Loss: 0.00104366
Iteration 16/25 | Loss: 0.00104366
Iteration 17/25 | Loss: 0.00104366
Iteration 18/25 | Loss: 0.00104366
Iteration 19/25 | Loss: 0.00104366
Iteration 20/25 | Loss: 0.00104366
Iteration 21/25 | Loss: 0.00104366
Iteration 22/25 | Loss: 0.00104366
Iteration 23/25 | Loss: 0.00104366
Iteration 24/25 | Loss: 0.00104366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010436592856422067, 0.0010436592856422067, 0.0010436592856422067, 0.0010436592856422067, 0.0010436592856422067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010436592856422067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104366
Iteration 2/1000 | Loss: 0.00004253
Iteration 3/1000 | Loss: 0.00003252
Iteration 4/1000 | Loss: 0.00002921
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002752
Iteration 7/1000 | Loss: 0.00002703
Iteration 8/1000 | Loss: 0.00002648
Iteration 9/1000 | Loss: 0.00002612
Iteration 10/1000 | Loss: 0.00002574
Iteration 11/1000 | Loss: 0.00002537
Iteration 12/1000 | Loss: 0.00002512
Iteration 13/1000 | Loss: 0.00017503
Iteration 14/1000 | Loss: 0.00005664
Iteration 15/1000 | Loss: 0.00004481
Iteration 16/1000 | Loss: 0.00018013
Iteration 17/1000 | Loss: 0.00003409
Iteration 18/1000 | Loss: 0.00007671
Iteration 19/1000 | Loss: 0.00007996
Iteration 20/1000 | Loss: 0.00007397
Iteration 21/1000 | Loss: 0.00007101
Iteration 22/1000 | Loss: 0.00006458
Iteration 23/1000 | Loss: 0.00005567
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00013848
Iteration 26/1000 | Loss: 0.00012084
Iteration 27/1000 | Loss: 0.00007875
Iteration 28/1000 | Loss: 0.00003251
Iteration 29/1000 | Loss: 0.00015384
Iteration 30/1000 | Loss: 0.00005390
Iteration 31/1000 | Loss: 0.00014761
Iteration 32/1000 | Loss: 0.00003968
Iteration 33/1000 | Loss: 0.00003165
Iteration 34/1000 | Loss: 0.00002862
Iteration 35/1000 | Loss: 0.00007206
Iteration 36/1000 | Loss: 0.00011603
Iteration 37/1000 | Loss: 0.00010833
Iteration 38/1000 | Loss: 0.00010844
Iteration 39/1000 | Loss: 0.00003485
Iteration 40/1000 | Loss: 0.00003966
Iteration 41/1000 | Loss: 0.00008662
Iteration 42/1000 | Loss: 0.00010243
Iteration 43/1000 | Loss: 0.00004263
Iteration 44/1000 | Loss: 0.00003170
Iteration 45/1000 | Loss: 0.00002805
Iteration 46/1000 | Loss: 0.00002719
Iteration 47/1000 | Loss: 0.00002654
Iteration 48/1000 | Loss: 0.00002619
Iteration 49/1000 | Loss: 0.00002590
Iteration 50/1000 | Loss: 0.00002559
Iteration 51/1000 | Loss: 0.00002523
Iteration 52/1000 | Loss: 0.00004801
Iteration 53/1000 | Loss: 0.00003497
Iteration 54/1000 | Loss: 0.00004137
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00004428
Iteration 57/1000 | Loss: 0.00003776
Iteration 58/1000 | Loss: 0.00003292
Iteration 59/1000 | Loss: 0.00002665
Iteration 60/1000 | Loss: 0.00003735
Iteration 61/1000 | Loss: 0.00004794
Iteration 62/1000 | Loss: 0.00003883
Iteration 63/1000 | Loss: 0.00004445
Iteration 64/1000 | Loss: 0.00004329
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002507
Iteration 67/1000 | Loss: 0.00002425
Iteration 68/1000 | Loss: 0.00004544
Iteration 69/1000 | Loss: 0.00003900
Iteration 70/1000 | Loss: 0.00004236
Iteration 71/1000 | Loss: 0.00002888
Iteration 72/1000 | Loss: 0.00003620
Iteration 73/1000 | Loss: 0.00004521
Iteration 74/1000 | Loss: 0.00003881
Iteration 75/1000 | Loss: 0.00004509
Iteration 76/1000 | Loss: 0.00002902
Iteration 77/1000 | Loss: 0.00004414
Iteration 78/1000 | Loss: 0.00003766
Iteration 79/1000 | Loss: 0.00003607
Iteration 80/1000 | Loss: 0.00003772
Iteration 81/1000 | Loss: 0.00002481
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00004751
Iteration 84/1000 | Loss: 0.00002947
Iteration 85/1000 | Loss: 0.00002409
Iteration 86/1000 | Loss: 0.00004620
Iteration 87/1000 | Loss: 0.00002801
Iteration 88/1000 | Loss: 0.00004272
Iteration 89/1000 | Loss: 0.00002885
Iteration 90/1000 | Loss: 0.00004090
Iteration 91/1000 | Loss: 0.00002573
Iteration 92/1000 | Loss: 0.00004177
Iteration 93/1000 | Loss: 0.00002644
Iteration 94/1000 | Loss: 0.00004059
Iteration 95/1000 | Loss: 0.00002629
Iteration 96/1000 | Loss: 0.00003715
Iteration 97/1000 | Loss: 0.00002480
Iteration 98/1000 | Loss: 0.00002443
Iteration 99/1000 | Loss: 0.00002399
Iteration 100/1000 | Loss: 0.00002377
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002354
Iteration 103/1000 | Loss: 0.00002352
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002351
Iteration 106/1000 | Loss: 0.00002351
Iteration 107/1000 | Loss: 0.00002350
Iteration 108/1000 | Loss: 0.00002350
Iteration 109/1000 | Loss: 0.00002350
Iteration 110/1000 | Loss: 0.00002349
Iteration 111/1000 | Loss: 0.00002349
Iteration 112/1000 | Loss: 0.00002348
Iteration 113/1000 | Loss: 0.00002346
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002342
Iteration 116/1000 | Loss: 0.00002341
Iteration 117/1000 | Loss: 0.00002341
Iteration 118/1000 | Loss: 0.00002340
Iteration 119/1000 | Loss: 0.00002340
Iteration 120/1000 | Loss: 0.00002340
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00002339
Iteration 123/1000 | Loss: 0.00002339
Iteration 124/1000 | Loss: 0.00002339
Iteration 125/1000 | Loss: 0.00002339
Iteration 126/1000 | Loss: 0.00002339
Iteration 127/1000 | Loss: 0.00002339
Iteration 128/1000 | Loss: 0.00002339
Iteration 129/1000 | Loss: 0.00002339
Iteration 130/1000 | Loss: 0.00002339
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00002339
Iteration 133/1000 | Loss: 0.00002338
Iteration 134/1000 | Loss: 0.00002338
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00002338
Iteration 137/1000 | Loss: 0.00002338
Iteration 138/1000 | Loss: 0.00002337
Iteration 139/1000 | Loss: 0.00002337
Iteration 140/1000 | Loss: 0.00002337
Iteration 141/1000 | Loss: 0.00002337
Iteration 142/1000 | Loss: 0.00002337
Iteration 143/1000 | Loss: 0.00002337
Iteration 144/1000 | Loss: 0.00002337
Iteration 145/1000 | Loss: 0.00002337
Iteration 146/1000 | Loss: 0.00002337
Iteration 147/1000 | Loss: 0.00002336
Iteration 148/1000 | Loss: 0.00002336
Iteration 149/1000 | Loss: 0.00002336
Iteration 150/1000 | Loss: 0.00002336
Iteration 151/1000 | Loss: 0.00002336
Iteration 152/1000 | Loss: 0.00002336
Iteration 153/1000 | Loss: 0.00002336
Iteration 154/1000 | Loss: 0.00002336
Iteration 155/1000 | Loss: 0.00002336
Iteration 156/1000 | Loss: 0.00002336
Iteration 157/1000 | Loss: 0.00002336
Iteration 158/1000 | Loss: 0.00002336
Iteration 159/1000 | Loss: 0.00002336
Iteration 160/1000 | Loss: 0.00002336
Iteration 161/1000 | Loss: 0.00002336
Iteration 162/1000 | Loss: 0.00002336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.3359598344541155e-05, 2.3359598344541155e-05, 2.3359598344541155e-05, 2.3359598344541155e-05, 2.3359598344541155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3359598344541155e-05

Optimization complete. Final v2v error: 3.8475725650787354 mm

Highest mean error: 5.620564937591553 mm for frame 115

Lowest mean error: 3.3923258781433105 mm for frame 182

Saving results

Total time: 179.48000144958496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045749
Iteration 2/25 | Loss: 0.01045749
Iteration 3/25 | Loss: 0.01045749
Iteration 4/25 | Loss: 0.01045749
Iteration 5/25 | Loss: 0.01045749
Iteration 6/25 | Loss: 0.01045748
Iteration 7/25 | Loss: 0.01045748
Iteration 8/25 | Loss: 0.01045748
Iteration 9/25 | Loss: 0.01045748
Iteration 10/25 | Loss: 0.01045748
Iteration 11/25 | Loss: 0.01045748
Iteration 12/25 | Loss: 0.01045748
Iteration 13/25 | Loss: 0.01045747
Iteration 14/25 | Loss: 0.01045747
Iteration 15/25 | Loss: 0.01045747
Iteration 16/25 | Loss: 0.01045747
Iteration 17/25 | Loss: 0.01045747
Iteration 18/25 | Loss: 0.01045747
Iteration 19/25 | Loss: 0.01045746
Iteration 20/25 | Loss: 0.01045746
Iteration 21/25 | Loss: 0.01045746
Iteration 22/25 | Loss: 0.01045746
Iteration 23/25 | Loss: 0.01045746
Iteration 24/25 | Loss: 0.01045746
Iteration 25/25 | Loss: 0.01045745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81787252
Iteration 2/25 | Loss: 0.10135742
Iteration 3/25 | Loss: 0.10055558
Iteration 4/25 | Loss: 0.10244544
Iteration 5/25 | Loss: 0.10235899
Iteration 6/25 | Loss: 0.10196920
Iteration 7/25 | Loss: 0.10245020
Iteration 8/25 | Loss: 0.10109495
Iteration 9/25 | Loss: 0.10114940
Iteration 10/25 | Loss: 0.09918917
Iteration 11/25 | Loss: 0.09918152
Iteration 12/25 | Loss: 0.09902810
Iteration 13/25 | Loss: 0.09902602
Iteration 14/25 | Loss: 0.09902601
Iteration 15/25 | Loss: 0.09902599
Iteration 16/25 | Loss: 0.09902599
Iteration 17/25 | Loss: 0.09902598
Iteration 18/25 | Loss: 0.09902598
Iteration 19/25 | Loss: 0.09902598
Iteration 20/25 | Loss: 0.09902598
Iteration 21/25 | Loss: 0.09902598
Iteration 22/25 | Loss: 0.09902598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.09902597963809967, 0.09902597963809967, 0.09902597963809967, 0.09902597963809967, 0.09902597963809967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09902597963809967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09902598
Iteration 2/1000 | Loss: 0.00295653
Iteration 3/1000 | Loss: 0.00128867
Iteration 4/1000 | Loss: 0.00048861
Iteration 5/1000 | Loss: 0.00018544
Iteration 6/1000 | Loss: 0.00012288
Iteration 7/1000 | Loss: 0.00019212
Iteration 8/1000 | Loss: 0.00013675
Iteration 9/1000 | Loss: 0.00007475
Iteration 10/1000 | Loss: 0.00005601
Iteration 11/1000 | Loss: 0.00010994
Iteration 12/1000 | Loss: 0.00002826
Iteration 13/1000 | Loss: 0.00006371
Iteration 14/1000 | Loss: 0.00013928
Iteration 15/1000 | Loss: 0.00045505
Iteration 16/1000 | Loss: 0.00034381
Iteration 17/1000 | Loss: 0.00004997
Iteration 18/1000 | Loss: 0.00041934
Iteration 19/1000 | Loss: 0.00003062
Iteration 20/1000 | Loss: 0.00002487
Iteration 21/1000 | Loss: 0.00010548
Iteration 22/1000 | Loss: 0.00071431
Iteration 23/1000 | Loss: 0.00161294
Iteration 24/1000 | Loss: 0.00027204
Iteration 25/1000 | Loss: 0.00008498
Iteration 26/1000 | Loss: 0.00003282
Iteration 27/1000 | Loss: 0.00004322
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00004123
Iteration 30/1000 | Loss: 0.00001799
Iteration 31/1000 | Loss: 0.00003184
Iteration 32/1000 | Loss: 0.00003303
Iteration 33/1000 | Loss: 0.00008273
Iteration 34/1000 | Loss: 0.00002901
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00004808
Iteration 38/1000 | Loss: 0.00001547
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001427
Iteration 41/1000 | Loss: 0.00003275
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00002403
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00002642
Iteration 47/1000 | Loss: 0.00006303
Iteration 48/1000 | Loss: 0.00002727
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001321
Iteration 57/1000 | Loss: 0.00001321
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00003276
Iteration 62/1000 | Loss: 0.00003325
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001307
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00002979
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001293
Iteration 75/1000 | Loss: 0.00001292
Iteration 76/1000 | Loss: 0.00001292
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001292
Iteration 80/1000 | Loss: 0.00001292
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001292
Iteration 83/1000 | Loss: 0.00001292
Iteration 84/1000 | Loss: 0.00001292
Iteration 85/1000 | Loss: 0.00001292
Iteration 86/1000 | Loss: 0.00001292
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.291956959903473e-05, 1.291956959903473e-05, 1.291956959903473e-05, 1.291956959903473e-05, 1.291956959903473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.291956959903473e-05

Optimization complete. Final v2v error: 3.0942821502685547 mm

Highest mean error: 3.6753480434417725 mm for frame 92

Lowest mean error: 2.64534854888916 mm for frame 236

Saving results

Total time: 109.27141833305359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851264
Iteration 2/25 | Loss: 0.00132763
Iteration 3/25 | Loss: 0.00125318
Iteration 4/25 | Loss: 0.00123988
Iteration 5/25 | Loss: 0.00123668
Iteration 6/25 | Loss: 0.00123668
Iteration 7/25 | Loss: 0.00123668
Iteration 8/25 | Loss: 0.00123668
Iteration 9/25 | Loss: 0.00123668
Iteration 10/25 | Loss: 0.00123668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012366818264126778, 0.0012366818264126778, 0.0012366818264126778, 0.0012366818264126778, 0.0012366818264126778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012366818264126778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.01618767
Iteration 2/25 | Loss: 0.00094320
Iteration 3/25 | Loss: 0.00094319
Iteration 4/25 | Loss: 0.00094319
Iteration 5/25 | Loss: 0.00094319
Iteration 6/25 | Loss: 0.00094319
Iteration 7/25 | Loss: 0.00094319
Iteration 8/25 | Loss: 0.00094319
Iteration 9/25 | Loss: 0.00094319
Iteration 10/25 | Loss: 0.00094319
Iteration 11/25 | Loss: 0.00094319
Iteration 12/25 | Loss: 0.00094319
Iteration 13/25 | Loss: 0.00094319
Iteration 14/25 | Loss: 0.00094319
Iteration 15/25 | Loss: 0.00094319
Iteration 16/25 | Loss: 0.00094319
Iteration 17/25 | Loss: 0.00094319
Iteration 18/25 | Loss: 0.00094319
Iteration 19/25 | Loss: 0.00094319
Iteration 20/25 | Loss: 0.00094319
Iteration 21/25 | Loss: 0.00094319
Iteration 22/25 | Loss: 0.00094319
Iteration 23/25 | Loss: 0.00094319
Iteration 24/25 | Loss: 0.00094319
Iteration 25/25 | Loss: 0.00094319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094319
Iteration 2/1000 | Loss: 0.00002471
Iteration 3/1000 | Loss: 0.00001934
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001677
Iteration 7/1000 | Loss: 0.00001638
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001566
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001551
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001468
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001458
Iteration 29/1000 | Loss: 0.00001457
Iteration 30/1000 | Loss: 0.00001457
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001455
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001454
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001435
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001434
Iteration 64/1000 | Loss: 0.00001434
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001432
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001430
Iteration 94/1000 | Loss: 0.00001430
Iteration 95/1000 | Loss: 0.00001430
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.430340671504382e-05, 1.430340671504382e-05, 1.430340671504382e-05, 1.430340671504382e-05, 1.430340671504382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.430340671504382e-05

Optimization complete. Final v2v error: 3.2235090732574463 mm

Highest mean error: 3.518092155456543 mm for frame 199

Lowest mean error: 2.947674036026001 mm for frame 4

Saving results

Total time: 37.408748149871826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407912
Iteration 2/25 | Loss: 0.00126396
Iteration 3/25 | Loss: 0.00121473
Iteration 4/25 | Loss: 0.00120653
Iteration 5/25 | Loss: 0.00120449
Iteration 6/25 | Loss: 0.00120420
Iteration 7/25 | Loss: 0.00120420
Iteration 8/25 | Loss: 0.00120420
Iteration 9/25 | Loss: 0.00120420
Iteration 10/25 | Loss: 0.00120420
Iteration 11/25 | Loss: 0.00120420
Iteration 12/25 | Loss: 0.00120420
Iteration 13/25 | Loss: 0.00120420
Iteration 14/25 | Loss: 0.00120420
Iteration 15/25 | Loss: 0.00120420
Iteration 16/25 | Loss: 0.00120420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012041981099173427, 0.0012041981099173427, 0.0012041981099173427, 0.0012041981099173427, 0.0012041981099173427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012041981099173427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35403216
Iteration 2/25 | Loss: 0.00100490
Iteration 3/25 | Loss: 0.00100490
Iteration 4/25 | Loss: 0.00100490
Iteration 5/25 | Loss: 0.00100490
Iteration 6/25 | Loss: 0.00100490
Iteration 7/25 | Loss: 0.00100490
Iteration 8/25 | Loss: 0.00100490
Iteration 9/25 | Loss: 0.00100490
Iteration 10/25 | Loss: 0.00100490
Iteration 11/25 | Loss: 0.00100490
Iteration 12/25 | Loss: 0.00100490
Iteration 13/25 | Loss: 0.00100490
Iteration 14/25 | Loss: 0.00100490
Iteration 15/25 | Loss: 0.00100490
Iteration 16/25 | Loss: 0.00100490
Iteration 17/25 | Loss: 0.00100490
Iteration 18/25 | Loss: 0.00100490
Iteration 19/25 | Loss: 0.00100490
Iteration 20/25 | Loss: 0.00100490
Iteration 21/25 | Loss: 0.00100490
Iteration 22/25 | Loss: 0.00100490
Iteration 23/25 | Loss: 0.00100490
Iteration 24/25 | Loss: 0.00100490
Iteration 25/25 | Loss: 0.00100490

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100490
Iteration 2/1000 | Loss: 0.00002512
Iteration 3/1000 | Loss: 0.00001779
Iteration 4/1000 | Loss: 0.00001474
Iteration 5/1000 | Loss: 0.00001389
Iteration 6/1000 | Loss: 0.00001331
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001230
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001195
Iteration 16/1000 | Loss: 0.00001193
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001184
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001178
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001171
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001161
Iteration 38/1000 | Loss: 0.00001161
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001160
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001159
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001157
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001149
Iteration 64/1000 | Loss: 0.00001149
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001148
Iteration 68/1000 | Loss: 0.00001148
Iteration 69/1000 | Loss: 0.00001148
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001147
Iteration 74/1000 | Loss: 0.00001147
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001146
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001145
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001145
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001144
Iteration 89/1000 | Loss: 0.00001144
Iteration 90/1000 | Loss: 0.00001144
Iteration 91/1000 | Loss: 0.00001144
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001141
Iteration 97/1000 | Loss: 0.00001141
Iteration 98/1000 | Loss: 0.00001141
Iteration 99/1000 | Loss: 0.00001141
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001140
Iteration 102/1000 | Loss: 0.00001140
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001138
Iteration 110/1000 | Loss: 0.00001138
Iteration 111/1000 | Loss: 0.00001138
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001136
Iteration 124/1000 | Loss: 0.00001135
Iteration 125/1000 | Loss: 0.00001135
Iteration 126/1000 | Loss: 0.00001135
Iteration 127/1000 | Loss: 0.00001135
Iteration 128/1000 | Loss: 0.00001134
Iteration 129/1000 | Loss: 0.00001134
Iteration 130/1000 | Loss: 0.00001134
Iteration 131/1000 | Loss: 0.00001134
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001133
Iteration 134/1000 | Loss: 0.00001133
Iteration 135/1000 | Loss: 0.00001133
Iteration 136/1000 | Loss: 0.00001133
Iteration 137/1000 | Loss: 0.00001133
Iteration 138/1000 | Loss: 0.00001133
Iteration 139/1000 | Loss: 0.00001133
Iteration 140/1000 | Loss: 0.00001133
Iteration 141/1000 | Loss: 0.00001133
Iteration 142/1000 | Loss: 0.00001132
Iteration 143/1000 | Loss: 0.00001132
Iteration 144/1000 | Loss: 0.00001132
Iteration 145/1000 | Loss: 0.00001132
Iteration 146/1000 | Loss: 0.00001132
Iteration 147/1000 | Loss: 0.00001132
Iteration 148/1000 | Loss: 0.00001132
Iteration 149/1000 | Loss: 0.00001132
Iteration 150/1000 | Loss: 0.00001132
Iteration 151/1000 | Loss: 0.00001131
Iteration 152/1000 | Loss: 0.00001131
Iteration 153/1000 | Loss: 0.00001131
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001130
Iteration 157/1000 | Loss: 0.00001130
Iteration 158/1000 | Loss: 0.00001130
Iteration 159/1000 | Loss: 0.00001130
Iteration 160/1000 | Loss: 0.00001130
Iteration 161/1000 | Loss: 0.00001130
Iteration 162/1000 | Loss: 0.00001130
Iteration 163/1000 | Loss: 0.00001130
Iteration 164/1000 | Loss: 0.00001130
Iteration 165/1000 | Loss: 0.00001129
Iteration 166/1000 | Loss: 0.00001129
Iteration 167/1000 | Loss: 0.00001129
Iteration 168/1000 | Loss: 0.00001129
Iteration 169/1000 | Loss: 0.00001129
Iteration 170/1000 | Loss: 0.00001129
Iteration 171/1000 | Loss: 0.00001129
Iteration 172/1000 | Loss: 0.00001129
Iteration 173/1000 | Loss: 0.00001129
Iteration 174/1000 | Loss: 0.00001129
Iteration 175/1000 | Loss: 0.00001129
Iteration 176/1000 | Loss: 0.00001129
Iteration 177/1000 | Loss: 0.00001129
Iteration 178/1000 | Loss: 0.00001128
Iteration 179/1000 | Loss: 0.00001128
Iteration 180/1000 | Loss: 0.00001128
Iteration 181/1000 | Loss: 0.00001128
Iteration 182/1000 | Loss: 0.00001128
Iteration 183/1000 | Loss: 0.00001128
Iteration 184/1000 | Loss: 0.00001128
Iteration 185/1000 | Loss: 0.00001128
Iteration 186/1000 | Loss: 0.00001128
Iteration 187/1000 | Loss: 0.00001128
Iteration 188/1000 | Loss: 0.00001128
Iteration 189/1000 | Loss: 0.00001128
Iteration 190/1000 | Loss: 0.00001128
Iteration 191/1000 | Loss: 0.00001128
Iteration 192/1000 | Loss: 0.00001128
Iteration 193/1000 | Loss: 0.00001128
Iteration 194/1000 | Loss: 0.00001128
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001128
Iteration 199/1000 | Loss: 0.00001128
Iteration 200/1000 | Loss: 0.00001128
Iteration 201/1000 | Loss: 0.00001128
Iteration 202/1000 | Loss: 0.00001127
Iteration 203/1000 | Loss: 0.00001127
Iteration 204/1000 | Loss: 0.00001127
Iteration 205/1000 | Loss: 0.00001127
Iteration 206/1000 | Loss: 0.00001127
Iteration 207/1000 | Loss: 0.00001127
Iteration 208/1000 | Loss: 0.00001127
Iteration 209/1000 | Loss: 0.00001127
Iteration 210/1000 | Loss: 0.00001127
Iteration 211/1000 | Loss: 0.00001127
Iteration 212/1000 | Loss: 0.00001127
Iteration 213/1000 | Loss: 0.00001127
Iteration 214/1000 | Loss: 0.00001127
Iteration 215/1000 | Loss: 0.00001127
Iteration 216/1000 | Loss: 0.00001127
Iteration 217/1000 | Loss: 0.00001127
Iteration 218/1000 | Loss: 0.00001127
Iteration 219/1000 | Loss: 0.00001127
Iteration 220/1000 | Loss: 0.00001127
Iteration 221/1000 | Loss: 0.00001127
Iteration 222/1000 | Loss: 0.00001127
Iteration 223/1000 | Loss: 0.00001127
Iteration 224/1000 | Loss: 0.00001127
Iteration 225/1000 | Loss: 0.00001127
Iteration 226/1000 | Loss: 0.00001127
Iteration 227/1000 | Loss: 0.00001127
Iteration 228/1000 | Loss: 0.00001127
Iteration 229/1000 | Loss: 0.00001127
Iteration 230/1000 | Loss: 0.00001127
Iteration 231/1000 | Loss: 0.00001127
Iteration 232/1000 | Loss: 0.00001127
Iteration 233/1000 | Loss: 0.00001127
Iteration 234/1000 | Loss: 0.00001127
Iteration 235/1000 | Loss: 0.00001127
Iteration 236/1000 | Loss: 0.00001127
Iteration 237/1000 | Loss: 0.00001127
Iteration 238/1000 | Loss: 0.00001127
Iteration 239/1000 | Loss: 0.00001127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.126976712839678e-05, 1.126976712839678e-05, 1.126976712839678e-05, 1.126976712839678e-05, 1.126976712839678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.126976712839678e-05

Optimization complete. Final v2v error: 2.887017011642456 mm

Highest mean error: 3.1226806640625 mm for frame 104

Lowest mean error: 2.734755754470825 mm for frame 87

Saving results

Total time: 38.58070373535156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755222
Iteration 2/25 | Loss: 0.00216374
Iteration 3/25 | Loss: 0.00164320
Iteration 4/25 | Loss: 0.00164876
Iteration 5/25 | Loss: 0.00150448
Iteration 6/25 | Loss: 0.00143961
Iteration 7/25 | Loss: 0.00143371
Iteration 8/25 | Loss: 0.00139304
Iteration 9/25 | Loss: 0.00137153
Iteration 10/25 | Loss: 0.00136204
Iteration 11/25 | Loss: 0.00135664
Iteration 12/25 | Loss: 0.00135486
Iteration 13/25 | Loss: 0.00135597
Iteration 14/25 | Loss: 0.00135073
Iteration 15/25 | Loss: 0.00135225
Iteration 16/25 | Loss: 0.00134628
Iteration 17/25 | Loss: 0.00134849
Iteration 18/25 | Loss: 0.00134572
Iteration 19/25 | Loss: 0.00134411
Iteration 20/25 | Loss: 0.00133897
Iteration 21/25 | Loss: 0.00134134
Iteration 22/25 | Loss: 0.00133670
Iteration 23/25 | Loss: 0.00133576
Iteration 24/25 | Loss: 0.00133568
Iteration 25/25 | Loss: 0.00133568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25062537
Iteration 2/25 | Loss: 0.00096672
Iteration 3/25 | Loss: 0.00096670
Iteration 4/25 | Loss: 0.00096670
Iteration 5/25 | Loss: 0.00096670
Iteration 6/25 | Loss: 0.00096670
Iteration 7/25 | Loss: 0.00096670
Iteration 8/25 | Loss: 0.00096670
Iteration 9/25 | Loss: 0.00096670
Iteration 10/25 | Loss: 0.00096670
Iteration 11/25 | Loss: 0.00096670
Iteration 12/25 | Loss: 0.00096670
Iteration 13/25 | Loss: 0.00096670
Iteration 14/25 | Loss: 0.00096670
Iteration 15/25 | Loss: 0.00096670
Iteration 16/25 | Loss: 0.00096670
Iteration 17/25 | Loss: 0.00096670
Iteration 18/25 | Loss: 0.00096670
Iteration 19/25 | Loss: 0.00096670
Iteration 20/25 | Loss: 0.00096670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009667013073340058, 0.0009667013073340058, 0.0009667013073340058, 0.0009667013073340058, 0.0009667013073340058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009667013073340058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096670
Iteration 2/1000 | Loss: 0.00025662
Iteration 3/1000 | Loss: 0.00005306
Iteration 4/1000 | Loss: 0.00004191
Iteration 5/1000 | Loss: 0.00003662
Iteration 6/1000 | Loss: 0.00003372
Iteration 7/1000 | Loss: 0.00003213
Iteration 8/1000 | Loss: 0.00003117
Iteration 9/1000 | Loss: 0.00003014
Iteration 10/1000 | Loss: 0.00060976
Iteration 11/1000 | Loss: 0.00003424
Iteration 12/1000 | Loss: 0.00002995
Iteration 13/1000 | Loss: 0.00002648
Iteration 14/1000 | Loss: 0.00002395
Iteration 15/1000 | Loss: 0.00002268
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002097
Iteration 18/1000 | Loss: 0.00002050
Iteration 19/1000 | Loss: 0.00002008
Iteration 20/1000 | Loss: 0.00001975
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001927
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001902
Iteration 25/1000 | Loss: 0.00001902
Iteration 26/1000 | Loss: 0.00001901
Iteration 27/1000 | Loss: 0.00001901
Iteration 28/1000 | Loss: 0.00001900
Iteration 29/1000 | Loss: 0.00001899
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001897
Iteration 32/1000 | Loss: 0.00001893
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001891
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001886
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001884
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001883
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001879
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001878
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001878
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001876
Iteration 78/1000 | Loss: 0.00001876
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001876
Iteration 83/1000 | Loss: 0.00001876
Iteration 84/1000 | Loss: 0.00001876
Iteration 85/1000 | Loss: 0.00001876
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001875
Iteration 88/1000 | Loss: 0.00001875
Iteration 89/1000 | Loss: 0.00001875
Iteration 90/1000 | Loss: 0.00001875
Iteration 91/1000 | Loss: 0.00001875
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001874
Iteration 94/1000 | Loss: 0.00001874
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001873
Iteration 99/1000 | Loss: 0.00001873
Iteration 100/1000 | Loss: 0.00001873
Iteration 101/1000 | Loss: 0.00001873
Iteration 102/1000 | Loss: 0.00001873
Iteration 103/1000 | Loss: 0.00001873
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001869
Iteration 129/1000 | Loss: 0.00001869
Iteration 130/1000 | Loss: 0.00001869
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001868
Iteration 134/1000 | Loss: 0.00001868
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001868
Iteration 137/1000 | Loss: 0.00001868
Iteration 138/1000 | Loss: 0.00001868
Iteration 139/1000 | Loss: 0.00001868
Iteration 140/1000 | Loss: 0.00001868
Iteration 141/1000 | Loss: 0.00001868
Iteration 142/1000 | Loss: 0.00001868
Iteration 143/1000 | Loss: 0.00001868
Iteration 144/1000 | Loss: 0.00001867
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001867
Iteration 148/1000 | Loss: 0.00001867
Iteration 149/1000 | Loss: 0.00001867
Iteration 150/1000 | Loss: 0.00001867
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001866
Iteration 157/1000 | Loss: 0.00001865
Iteration 158/1000 | Loss: 0.00001865
Iteration 159/1000 | Loss: 0.00001865
Iteration 160/1000 | Loss: 0.00001865
Iteration 161/1000 | Loss: 0.00001865
Iteration 162/1000 | Loss: 0.00001865
Iteration 163/1000 | Loss: 0.00001865
Iteration 164/1000 | Loss: 0.00001865
Iteration 165/1000 | Loss: 0.00001865
Iteration 166/1000 | Loss: 0.00001865
Iteration 167/1000 | Loss: 0.00001865
Iteration 168/1000 | Loss: 0.00001865
Iteration 169/1000 | Loss: 0.00001864
Iteration 170/1000 | Loss: 0.00001864
Iteration 171/1000 | Loss: 0.00001864
Iteration 172/1000 | Loss: 0.00001864
Iteration 173/1000 | Loss: 0.00001864
Iteration 174/1000 | Loss: 0.00001864
Iteration 175/1000 | Loss: 0.00001863
Iteration 176/1000 | Loss: 0.00001863
Iteration 177/1000 | Loss: 0.00001863
Iteration 178/1000 | Loss: 0.00001863
Iteration 179/1000 | Loss: 0.00001863
Iteration 180/1000 | Loss: 0.00001863
Iteration 181/1000 | Loss: 0.00001863
Iteration 182/1000 | Loss: 0.00001863
Iteration 183/1000 | Loss: 0.00001863
Iteration 184/1000 | Loss: 0.00001863
Iteration 185/1000 | Loss: 0.00001863
Iteration 186/1000 | Loss: 0.00001863
Iteration 187/1000 | Loss: 0.00001862
Iteration 188/1000 | Loss: 0.00001862
Iteration 189/1000 | Loss: 0.00001862
Iteration 190/1000 | Loss: 0.00001862
Iteration 191/1000 | Loss: 0.00001862
Iteration 192/1000 | Loss: 0.00001862
Iteration 193/1000 | Loss: 0.00001862
Iteration 194/1000 | Loss: 0.00001862
Iteration 195/1000 | Loss: 0.00001862
Iteration 196/1000 | Loss: 0.00001862
Iteration 197/1000 | Loss: 0.00001862
Iteration 198/1000 | Loss: 0.00001862
Iteration 199/1000 | Loss: 0.00001862
Iteration 200/1000 | Loss: 0.00001861
Iteration 201/1000 | Loss: 0.00001861
Iteration 202/1000 | Loss: 0.00001861
Iteration 203/1000 | Loss: 0.00001861
Iteration 204/1000 | Loss: 0.00001861
Iteration 205/1000 | Loss: 0.00001861
Iteration 206/1000 | Loss: 0.00001861
Iteration 207/1000 | Loss: 0.00001861
Iteration 208/1000 | Loss: 0.00001861
Iteration 209/1000 | Loss: 0.00001861
Iteration 210/1000 | Loss: 0.00001861
Iteration 211/1000 | Loss: 0.00001860
Iteration 212/1000 | Loss: 0.00001860
Iteration 213/1000 | Loss: 0.00001860
Iteration 214/1000 | Loss: 0.00001860
Iteration 215/1000 | Loss: 0.00001860
Iteration 216/1000 | Loss: 0.00001860
Iteration 217/1000 | Loss: 0.00001860
Iteration 218/1000 | Loss: 0.00001860
Iteration 219/1000 | Loss: 0.00001860
Iteration 220/1000 | Loss: 0.00001860
Iteration 221/1000 | Loss: 0.00001860
Iteration 222/1000 | Loss: 0.00001860
Iteration 223/1000 | Loss: 0.00001860
Iteration 224/1000 | Loss: 0.00001859
Iteration 225/1000 | Loss: 0.00001859
Iteration 226/1000 | Loss: 0.00001859
Iteration 227/1000 | Loss: 0.00001859
Iteration 228/1000 | Loss: 0.00001859
Iteration 229/1000 | Loss: 0.00001859
Iteration 230/1000 | Loss: 0.00001859
Iteration 231/1000 | Loss: 0.00001859
Iteration 232/1000 | Loss: 0.00001859
Iteration 233/1000 | Loss: 0.00001859
Iteration 234/1000 | Loss: 0.00001859
Iteration 235/1000 | Loss: 0.00001858
Iteration 236/1000 | Loss: 0.00001858
Iteration 237/1000 | Loss: 0.00001858
Iteration 238/1000 | Loss: 0.00001858
Iteration 239/1000 | Loss: 0.00001858
Iteration 240/1000 | Loss: 0.00001858
Iteration 241/1000 | Loss: 0.00001858
Iteration 242/1000 | Loss: 0.00001858
Iteration 243/1000 | Loss: 0.00001858
Iteration 244/1000 | Loss: 0.00001858
Iteration 245/1000 | Loss: 0.00001858
Iteration 246/1000 | Loss: 0.00001857
Iteration 247/1000 | Loss: 0.00001857
Iteration 248/1000 | Loss: 0.00001857
Iteration 249/1000 | Loss: 0.00001857
Iteration 250/1000 | Loss: 0.00001857
Iteration 251/1000 | Loss: 0.00001857
Iteration 252/1000 | Loss: 0.00001857
Iteration 253/1000 | Loss: 0.00001857
Iteration 254/1000 | Loss: 0.00001857
Iteration 255/1000 | Loss: 0.00001857
Iteration 256/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.857088500401005e-05, 1.857088500401005e-05, 1.857088500401005e-05, 1.857088500401005e-05, 1.857088500401005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.857088500401005e-05

Optimization complete. Final v2v error: 3.604222536087036 mm

Highest mean error: 3.9252912998199463 mm for frame 4

Lowest mean error: 3.380052328109741 mm for frame 152

Saving results

Total time: 103.52391648292542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971652
Iteration 2/25 | Loss: 0.00262315
Iteration 3/25 | Loss: 0.00215749
Iteration 4/25 | Loss: 0.00203743
Iteration 5/25 | Loss: 0.00202840
Iteration 6/25 | Loss: 0.00196605
Iteration 7/25 | Loss: 0.00193768
Iteration 8/25 | Loss: 0.00165134
Iteration 9/25 | Loss: 0.00157768
Iteration 10/25 | Loss: 0.00154712
Iteration 11/25 | Loss: 0.00153552
Iteration 12/25 | Loss: 0.00151343
Iteration 13/25 | Loss: 0.00151060
Iteration 14/25 | Loss: 0.00151073
Iteration 15/25 | Loss: 0.00151070
Iteration 16/25 | Loss: 0.00151094
Iteration 17/25 | Loss: 0.00151101
Iteration 18/25 | Loss: 0.00151069
Iteration 19/25 | Loss: 0.00151084
Iteration 20/25 | Loss: 0.00151065
Iteration 21/25 | Loss: 0.00151068
Iteration 22/25 | Loss: 0.00151038
Iteration 23/25 | Loss: 0.00151079
Iteration 24/25 | Loss: 0.00151016
Iteration 25/25 | Loss: 0.00151148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33918989
Iteration 2/25 | Loss: 0.00177056
Iteration 3/25 | Loss: 0.00134821
Iteration 4/25 | Loss: 0.00134820
Iteration 5/25 | Loss: 0.00134820
Iteration 6/25 | Loss: 0.00134820
Iteration 7/25 | Loss: 0.00134820
Iteration 8/25 | Loss: 0.00134820
Iteration 9/25 | Loss: 0.00134820
Iteration 10/25 | Loss: 0.00134820
Iteration 11/25 | Loss: 0.00134820
Iteration 12/25 | Loss: 0.00134820
Iteration 13/25 | Loss: 0.00134820
Iteration 14/25 | Loss: 0.00134820
Iteration 15/25 | Loss: 0.00134820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001348202466033399, 0.001348202466033399, 0.001348202466033399, 0.001348202466033399, 0.001348202466033399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001348202466033399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134820
Iteration 2/1000 | Loss: 0.00234137
Iteration 3/1000 | Loss: 0.00040288
Iteration 4/1000 | Loss: 0.00016256
Iteration 5/1000 | Loss: 0.00014030
Iteration 6/1000 | Loss: 0.00011045
Iteration 7/1000 | Loss: 0.00010166
Iteration 8/1000 | Loss: 0.00008951
Iteration 9/1000 | Loss: 0.00032186
Iteration 10/1000 | Loss: 0.00009052
Iteration 11/1000 | Loss: 0.00009689
Iteration 12/1000 | Loss: 0.00009183
Iteration 13/1000 | Loss: 0.00009273
Iteration 14/1000 | Loss: 0.00009305
Iteration 15/1000 | Loss: 0.00008893
Iteration 16/1000 | Loss: 0.00008485
Iteration 17/1000 | Loss: 0.00007987
Iteration 18/1000 | Loss: 0.00008334
Iteration 19/1000 | Loss: 0.00008464
Iteration 20/1000 | Loss: 0.00008244
Iteration 21/1000 | Loss: 0.00007733
Iteration 22/1000 | Loss: 0.00007737
Iteration 23/1000 | Loss: 0.00008025
Iteration 24/1000 | Loss: 0.00008222
Iteration 25/1000 | Loss: 0.00008220
Iteration 26/1000 | Loss: 0.00008442
Iteration 27/1000 | Loss: 0.00020417
Iteration 28/1000 | Loss: 0.00010238
Iteration 29/1000 | Loss: 0.00008331
Iteration 30/1000 | Loss: 0.00007632
Iteration 31/1000 | Loss: 0.00035254
Iteration 32/1000 | Loss: 0.00026265
Iteration 33/1000 | Loss: 0.00035739
Iteration 34/1000 | Loss: 0.00008422
Iteration 35/1000 | Loss: 0.00007284
Iteration 36/1000 | Loss: 0.00016611
Iteration 37/1000 | Loss: 0.00015305
Iteration 38/1000 | Loss: 0.00012412
Iteration 39/1000 | Loss: 0.00062774
Iteration 40/1000 | Loss: 0.00007192
Iteration 41/1000 | Loss: 0.00007123
Iteration 42/1000 | Loss: 0.00007080
Iteration 43/1000 | Loss: 0.00021395
Iteration 44/1000 | Loss: 0.00025666
Iteration 45/1000 | Loss: 0.00010126
Iteration 46/1000 | Loss: 0.00007038
Iteration 47/1000 | Loss: 0.00007010
Iteration 48/1000 | Loss: 0.00006970
Iteration 49/1000 | Loss: 0.00033835
Iteration 50/1000 | Loss: 0.00009970
Iteration 51/1000 | Loss: 0.00011937
Iteration 52/1000 | Loss: 0.00019709
Iteration 53/1000 | Loss: 0.00155615
Iteration 54/1000 | Loss: 0.00108541
Iteration 55/1000 | Loss: 0.00063118
Iteration 56/1000 | Loss: 0.00021565
Iteration 57/1000 | Loss: 0.00048104
Iteration 58/1000 | Loss: 0.00009156
Iteration 59/1000 | Loss: 0.00027641
Iteration 60/1000 | Loss: 0.00034767
Iteration 61/1000 | Loss: 0.00006754
Iteration 62/1000 | Loss: 0.00006217
Iteration 63/1000 | Loss: 0.00082835
Iteration 64/1000 | Loss: 0.00084237
Iteration 65/1000 | Loss: 0.00407959
Iteration 66/1000 | Loss: 0.00013980
Iteration 67/1000 | Loss: 0.00019353
Iteration 68/1000 | Loss: 0.00102263
Iteration 69/1000 | Loss: 0.00006390
Iteration 70/1000 | Loss: 0.00005879
Iteration 71/1000 | Loss: 0.00005673
Iteration 72/1000 | Loss: 0.00025011
Iteration 73/1000 | Loss: 0.00027362
Iteration 74/1000 | Loss: 0.00005877
Iteration 75/1000 | Loss: 0.00053558
Iteration 76/1000 | Loss: 0.00032795
Iteration 77/1000 | Loss: 0.00005505
Iteration 78/1000 | Loss: 0.00021047
Iteration 79/1000 | Loss: 0.00030197
Iteration 80/1000 | Loss: 0.00073917
Iteration 81/1000 | Loss: 0.00015728
Iteration 82/1000 | Loss: 0.00005345
Iteration 83/1000 | Loss: 0.00026562
Iteration 84/1000 | Loss: 0.00005220
Iteration 85/1000 | Loss: 0.00019393
Iteration 86/1000 | Loss: 0.00005077
Iteration 87/1000 | Loss: 0.00027054
Iteration 88/1000 | Loss: 0.00004952
Iteration 89/1000 | Loss: 0.00004910
Iteration 90/1000 | Loss: 0.00004884
Iteration 91/1000 | Loss: 0.00004870
Iteration 92/1000 | Loss: 0.00017955
Iteration 93/1000 | Loss: 0.00015490
Iteration 94/1000 | Loss: 0.00008674
Iteration 95/1000 | Loss: 0.00014985
Iteration 96/1000 | Loss: 0.00004861
Iteration 97/1000 | Loss: 0.00004859
Iteration 98/1000 | Loss: 0.00004855
Iteration 99/1000 | Loss: 0.00004855
Iteration 100/1000 | Loss: 0.00004853
Iteration 101/1000 | Loss: 0.00004852
Iteration 102/1000 | Loss: 0.00004851
Iteration 103/1000 | Loss: 0.00004849
Iteration 104/1000 | Loss: 0.00004849
Iteration 105/1000 | Loss: 0.00004847
Iteration 106/1000 | Loss: 0.00004843
Iteration 107/1000 | Loss: 0.00004842
Iteration 108/1000 | Loss: 0.00004842
Iteration 109/1000 | Loss: 0.00004840
Iteration 110/1000 | Loss: 0.00004840
Iteration 111/1000 | Loss: 0.00004840
Iteration 112/1000 | Loss: 0.00004839
Iteration 113/1000 | Loss: 0.00004839
Iteration 114/1000 | Loss: 0.00004838
Iteration 115/1000 | Loss: 0.00004838
Iteration 116/1000 | Loss: 0.00004836
Iteration 117/1000 | Loss: 0.00004835
Iteration 118/1000 | Loss: 0.00004834
Iteration 119/1000 | Loss: 0.00004834
Iteration 120/1000 | Loss: 0.00004834
Iteration 121/1000 | Loss: 0.00004833
Iteration 122/1000 | Loss: 0.00004833
Iteration 123/1000 | Loss: 0.00004833
Iteration 124/1000 | Loss: 0.00004833
Iteration 125/1000 | Loss: 0.00004833
Iteration 126/1000 | Loss: 0.00004832
Iteration 127/1000 | Loss: 0.00004832
Iteration 128/1000 | Loss: 0.00004832
Iteration 129/1000 | Loss: 0.00004831
Iteration 130/1000 | Loss: 0.00004831
Iteration 131/1000 | Loss: 0.00004831
Iteration 132/1000 | Loss: 0.00004831
Iteration 133/1000 | Loss: 0.00004831
Iteration 134/1000 | Loss: 0.00004831
Iteration 135/1000 | Loss: 0.00004830
Iteration 136/1000 | Loss: 0.00004830
Iteration 137/1000 | Loss: 0.00004830
Iteration 138/1000 | Loss: 0.00004829
Iteration 139/1000 | Loss: 0.00004829
Iteration 140/1000 | Loss: 0.00004829
Iteration 141/1000 | Loss: 0.00004829
Iteration 142/1000 | Loss: 0.00004829
Iteration 143/1000 | Loss: 0.00004829
Iteration 144/1000 | Loss: 0.00004829
Iteration 145/1000 | Loss: 0.00004829
Iteration 146/1000 | Loss: 0.00004828
Iteration 147/1000 | Loss: 0.00004828
Iteration 148/1000 | Loss: 0.00004828
Iteration 149/1000 | Loss: 0.00004828
Iteration 150/1000 | Loss: 0.00004828
Iteration 151/1000 | Loss: 0.00004828
Iteration 152/1000 | Loss: 0.00004828
Iteration 153/1000 | Loss: 0.00004828
Iteration 154/1000 | Loss: 0.00004828
Iteration 155/1000 | Loss: 0.00004828
Iteration 156/1000 | Loss: 0.00004827
Iteration 157/1000 | Loss: 0.00004827
Iteration 158/1000 | Loss: 0.00004827
Iteration 159/1000 | Loss: 0.00004827
Iteration 160/1000 | Loss: 0.00004827
Iteration 161/1000 | Loss: 0.00004827
Iteration 162/1000 | Loss: 0.00004827
Iteration 163/1000 | Loss: 0.00004826
Iteration 164/1000 | Loss: 0.00004826
Iteration 165/1000 | Loss: 0.00004826
Iteration 166/1000 | Loss: 0.00004826
Iteration 167/1000 | Loss: 0.00004826
Iteration 168/1000 | Loss: 0.00004826
Iteration 169/1000 | Loss: 0.00004826
Iteration 170/1000 | Loss: 0.00004826
Iteration 171/1000 | Loss: 0.00004826
Iteration 172/1000 | Loss: 0.00004826
Iteration 173/1000 | Loss: 0.00004826
Iteration 174/1000 | Loss: 0.00004825
Iteration 175/1000 | Loss: 0.00004825
Iteration 176/1000 | Loss: 0.00004825
Iteration 177/1000 | Loss: 0.00004825
Iteration 178/1000 | Loss: 0.00004825
Iteration 179/1000 | Loss: 0.00004825
Iteration 180/1000 | Loss: 0.00004824
Iteration 181/1000 | Loss: 0.00004824
Iteration 182/1000 | Loss: 0.00004824
Iteration 183/1000 | Loss: 0.00004824
Iteration 184/1000 | Loss: 0.00004824
Iteration 185/1000 | Loss: 0.00004824
Iteration 186/1000 | Loss: 0.00004824
Iteration 187/1000 | Loss: 0.00004824
Iteration 188/1000 | Loss: 0.00004824
Iteration 189/1000 | Loss: 0.00004823
Iteration 190/1000 | Loss: 0.00004823
Iteration 191/1000 | Loss: 0.00004823
Iteration 192/1000 | Loss: 0.00004823
Iteration 193/1000 | Loss: 0.00004823
Iteration 194/1000 | Loss: 0.00004823
Iteration 195/1000 | Loss: 0.00004823
Iteration 196/1000 | Loss: 0.00004823
Iteration 197/1000 | Loss: 0.00004823
Iteration 198/1000 | Loss: 0.00004823
Iteration 199/1000 | Loss: 0.00004823
Iteration 200/1000 | Loss: 0.00004823
Iteration 201/1000 | Loss: 0.00004823
Iteration 202/1000 | Loss: 0.00004823
Iteration 203/1000 | Loss: 0.00004823
Iteration 204/1000 | Loss: 0.00004823
Iteration 205/1000 | Loss: 0.00004823
Iteration 206/1000 | Loss: 0.00004823
Iteration 207/1000 | Loss: 0.00004823
Iteration 208/1000 | Loss: 0.00004823
Iteration 209/1000 | Loss: 0.00004823
Iteration 210/1000 | Loss: 0.00004823
Iteration 211/1000 | Loss: 0.00004823
Iteration 212/1000 | Loss: 0.00004823
Iteration 213/1000 | Loss: 0.00004823
Iteration 214/1000 | Loss: 0.00004823
Iteration 215/1000 | Loss: 0.00004823
Iteration 216/1000 | Loss: 0.00004823
Iteration 217/1000 | Loss: 0.00004823
Iteration 218/1000 | Loss: 0.00004823
Iteration 219/1000 | Loss: 0.00004823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [4.8230063839582726e-05, 4.8230063839582726e-05, 4.8230063839582726e-05, 4.8230063839582726e-05, 4.8230063839582726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.8230063839582726e-05

Optimization complete. Final v2v error: 5.038271903991699 mm

Highest mean error: 11.418791770935059 mm for frame 74

Lowest mean error: 4.558189868927002 mm for frame 2

Saving results

Total time: 212.78414225578308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752740
Iteration 2/25 | Loss: 0.00153126
Iteration 3/25 | Loss: 0.00133285
Iteration 4/25 | Loss: 0.00129945
Iteration 5/25 | Loss: 0.00128863
Iteration 6/25 | Loss: 0.00128636
Iteration 7/25 | Loss: 0.00128239
Iteration 8/25 | Loss: 0.00127765
Iteration 9/25 | Loss: 0.00127847
Iteration 10/25 | Loss: 0.00127173
Iteration 11/25 | Loss: 0.00127032
Iteration 12/25 | Loss: 0.00127293
Iteration 13/25 | Loss: 0.00126978
Iteration 14/25 | Loss: 0.00126736
Iteration 15/25 | Loss: 0.00126770
Iteration 16/25 | Loss: 0.00126608
Iteration 17/25 | Loss: 0.00126582
Iteration 18/25 | Loss: 0.00126536
Iteration 19/25 | Loss: 0.00126454
Iteration 20/25 | Loss: 0.00126398
Iteration 21/25 | Loss: 0.00126386
Iteration 22/25 | Loss: 0.00126384
Iteration 23/25 | Loss: 0.00126383
Iteration 24/25 | Loss: 0.00126383
Iteration 25/25 | Loss: 0.00126383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97602403
Iteration 2/25 | Loss: 0.00112056
Iteration 3/25 | Loss: 0.00112056
Iteration 4/25 | Loss: 0.00109126
Iteration 5/25 | Loss: 0.00109126
Iteration 6/25 | Loss: 0.00109126
Iteration 7/25 | Loss: 0.00109126
Iteration 8/25 | Loss: 0.00109125
Iteration 9/25 | Loss: 0.00109125
Iteration 10/25 | Loss: 0.00109125
Iteration 11/25 | Loss: 0.00109125
Iteration 12/25 | Loss: 0.00109125
Iteration 13/25 | Loss: 0.00109125
Iteration 14/25 | Loss: 0.00109125
Iteration 15/25 | Loss: 0.00109125
Iteration 16/25 | Loss: 0.00109125
Iteration 17/25 | Loss: 0.00109125
Iteration 18/25 | Loss: 0.00109125
Iteration 19/25 | Loss: 0.00109125
Iteration 20/25 | Loss: 0.00109125
Iteration 21/25 | Loss: 0.00109125
Iteration 22/25 | Loss: 0.00109125
Iteration 23/25 | Loss: 0.00109125
Iteration 24/25 | Loss: 0.00109125
Iteration 25/25 | Loss: 0.00109125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109125
Iteration 2/1000 | Loss: 0.00006038
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001712
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001600
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001460
Iteration 21/1000 | Loss: 0.00001459
Iteration 22/1000 | Loss: 0.00001459
Iteration 23/1000 | Loss: 0.00001457
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001435
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001435
Iteration 57/1000 | Loss: 0.00001435
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001433
Iteration 61/1000 | Loss: 0.00001432
Iteration 62/1000 | Loss: 0.00001432
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001431
Iteration 65/1000 | Loss: 0.00001431
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001429
Iteration 72/1000 | Loss: 0.00001429
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001428
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001428
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001428
Iteration 88/1000 | Loss: 0.00001428
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001427
Iteration 95/1000 | Loss: 0.00001427
Iteration 96/1000 | Loss: 0.00001427
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001426
Iteration 101/1000 | Loss: 0.00001426
Iteration 102/1000 | Loss: 0.00001426
Iteration 103/1000 | Loss: 0.00001426
Iteration 104/1000 | Loss: 0.00001426
Iteration 105/1000 | Loss: 0.00001426
Iteration 106/1000 | Loss: 0.00001426
Iteration 107/1000 | Loss: 0.00001426
Iteration 108/1000 | Loss: 0.00001426
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001426
Iteration 113/1000 | Loss: 0.00001426
Iteration 114/1000 | Loss: 0.00001426
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001424
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001423
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001420
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001417
Iteration 176/1000 | Loss: 0.00001417
Iteration 177/1000 | Loss: 0.00001417
Iteration 178/1000 | Loss: 0.00001417
Iteration 179/1000 | Loss: 0.00001417
Iteration 180/1000 | Loss: 0.00001417
Iteration 181/1000 | Loss: 0.00001417
Iteration 182/1000 | Loss: 0.00001417
Iteration 183/1000 | Loss: 0.00001417
Iteration 184/1000 | Loss: 0.00001417
Iteration 185/1000 | Loss: 0.00001416
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001416
Iteration 188/1000 | Loss: 0.00001416
Iteration 189/1000 | Loss: 0.00001416
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001416
Iteration 193/1000 | Loss: 0.00001415
Iteration 194/1000 | Loss: 0.00001415
Iteration 195/1000 | Loss: 0.00001415
Iteration 196/1000 | Loss: 0.00001415
Iteration 197/1000 | Loss: 0.00001415
Iteration 198/1000 | Loss: 0.00001415
Iteration 199/1000 | Loss: 0.00001415
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Iteration 204/1000 | Loss: 0.00001415
Iteration 205/1000 | Loss: 0.00001415
Iteration 206/1000 | Loss: 0.00001415
Iteration 207/1000 | Loss: 0.00001415
Iteration 208/1000 | Loss: 0.00001415
Iteration 209/1000 | Loss: 0.00001415
Iteration 210/1000 | Loss: 0.00001415
Iteration 211/1000 | Loss: 0.00001415
Iteration 212/1000 | Loss: 0.00001415
Iteration 213/1000 | Loss: 0.00001415
Iteration 214/1000 | Loss: 0.00001415
Iteration 215/1000 | Loss: 0.00001415
Iteration 216/1000 | Loss: 0.00001415
Iteration 217/1000 | Loss: 0.00001415
Iteration 218/1000 | Loss: 0.00001415
Iteration 219/1000 | Loss: 0.00001415
Iteration 220/1000 | Loss: 0.00001415
Iteration 221/1000 | Loss: 0.00001415
Iteration 222/1000 | Loss: 0.00001415
Iteration 223/1000 | Loss: 0.00001415
Iteration 224/1000 | Loss: 0.00001415
Iteration 225/1000 | Loss: 0.00001415
Iteration 226/1000 | Loss: 0.00001415
Iteration 227/1000 | Loss: 0.00001415
Iteration 228/1000 | Loss: 0.00001415
Iteration 229/1000 | Loss: 0.00001415
Iteration 230/1000 | Loss: 0.00001415
Iteration 231/1000 | Loss: 0.00001415
Iteration 232/1000 | Loss: 0.00001415
Iteration 233/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.4145296518108808e-05, 1.4145296518108808e-05, 1.4145296518108808e-05, 1.4145296518108808e-05, 1.4145296518108808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4145296518108808e-05

Optimization complete. Final v2v error: 3.183953046798706 mm

Highest mean error: 3.6601624488830566 mm for frame 99

Lowest mean error: 2.890068292617798 mm for frame 198

Saving results

Total time: 81.13779735565186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806699
Iteration 2/25 | Loss: 0.00132591
Iteration 3/25 | Loss: 0.00125450
Iteration 4/25 | Loss: 0.00124175
Iteration 5/25 | Loss: 0.00123836
Iteration 6/25 | Loss: 0.00123836
Iteration 7/25 | Loss: 0.00123836
Iteration 8/25 | Loss: 0.00123836
Iteration 9/25 | Loss: 0.00123836
Iteration 10/25 | Loss: 0.00123836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012383601861074567, 0.0012383601861074567, 0.0012383601861074567, 0.0012383601861074567, 0.0012383601861074567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012383601861074567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.79903793
Iteration 2/25 | Loss: 0.00101053
Iteration 3/25 | Loss: 0.00101052
Iteration 4/25 | Loss: 0.00101052
Iteration 5/25 | Loss: 0.00101052
Iteration 6/25 | Loss: 0.00101052
Iteration 7/25 | Loss: 0.00101052
Iteration 8/25 | Loss: 0.00101052
Iteration 9/25 | Loss: 0.00101052
Iteration 10/25 | Loss: 0.00101052
Iteration 11/25 | Loss: 0.00101052
Iteration 12/25 | Loss: 0.00101052
Iteration 13/25 | Loss: 0.00101052
Iteration 14/25 | Loss: 0.00101052
Iteration 15/25 | Loss: 0.00101052
Iteration 16/25 | Loss: 0.00101052
Iteration 17/25 | Loss: 0.00101052
Iteration 18/25 | Loss: 0.00101052
Iteration 19/25 | Loss: 0.00101052
Iteration 20/25 | Loss: 0.00101052
Iteration 21/25 | Loss: 0.00101052
Iteration 22/25 | Loss: 0.00101052
Iteration 23/25 | Loss: 0.00101052
Iteration 24/25 | Loss: 0.00101052
Iteration 25/25 | Loss: 0.00101052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101052
Iteration 2/1000 | Loss: 0.00003032
Iteration 3/1000 | Loss: 0.00002353
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00002021
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001823
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001720
Iteration 11/1000 | Loss: 0.00001695
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00001680
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001648
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001647
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001643
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001641
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001640
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001636
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001635
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001634
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001630
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001625
Iteration 58/1000 | Loss: 0.00001625
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001624
Iteration 61/1000 | Loss: 0.00001624
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001621
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001617
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001611
Iteration 81/1000 | Loss: 0.00001611
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001610
Iteration 84/1000 | Loss: 0.00001610
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001609
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001608
Iteration 89/1000 | Loss: 0.00001606
Iteration 90/1000 | Loss: 0.00001605
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001602
Iteration 99/1000 | Loss: 0.00001602
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001601
Iteration 103/1000 | Loss: 0.00001601
Iteration 104/1000 | Loss: 0.00001601
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001601
Iteration 107/1000 | Loss: 0.00001601
Iteration 108/1000 | Loss: 0.00001601
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001600
Iteration 114/1000 | Loss: 0.00001600
Iteration 115/1000 | Loss: 0.00001600
Iteration 116/1000 | Loss: 0.00001600
Iteration 117/1000 | Loss: 0.00001600
Iteration 118/1000 | Loss: 0.00001600
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001599
Iteration 124/1000 | Loss: 0.00001599
Iteration 125/1000 | Loss: 0.00001599
Iteration 126/1000 | Loss: 0.00001599
Iteration 127/1000 | Loss: 0.00001599
Iteration 128/1000 | Loss: 0.00001599
Iteration 129/1000 | Loss: 0.00001599
Iteration 130/1000 | Loss: 0.00001599
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001598
Iteration 135/1000 | Loss: 0.00001598
Iteration 136/1000 | Loss: 0.00001598
Iteration 137/1000 | Loss: 0.00001598
Iteration 138/1000 | Loss: 0.00001598
Iteration 139/1000 | Loss: 0.00001598
Iteration 140/1000 | Loss: 0.00001598
Iteration 141/1000 | Loss: 0.00001598
Iteration 142/1000 | Loss: 0.00001598
Iteration 143/1000 | Loss: 0.00001598
Iteration 144/1000 | Loss: 0.00001598
Iteration 145/1000 | Loss: 0.00001598
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001597
Iteration 150/1000 | Loss: 0.00001597
Iteration 151/1000 | Loss: 0.00001597
Iteration 152/1000 | Loss: 0.00001597
Iteration 153/1000 | Loss: 0.00001597
Iteration 154/1000 | Loss: 0.00001597
Iteration 155/1000 | Loss: 0.00001597
Iteration 156/1000 | Loss: 0.00001597
Iteration 157/1000 | Loss: 0.00001597
Iteration 158/1000 | Loss: 0.00001597
Iteration 159/1000 | Loss: 0.00001596
Iteration 160/1000 | Loss: 0.00001596
Iteration 161/1000 | Loss: 0.00001596
Iteration 162/1000 | Loss: 0.00001596
Iteration 163/1000 | Loss: 0.00001596
Iteration 164/1000 | Loss: 0.00001596
Iteration 165/1000 | Loss: 0.00001596
Iteration 166/1000 | Loss: 0.00001596
Iteration 167/1000 | Loss: 0.00001596
Iteration 168/1000 | Loss: 0.00001596
Iteration 169/1000 | Loss: 0.00001596
Iteration 170/1000 | Loss: 0.00001596
Iteration 171/1000 | Loss: 0.00001595
Iteration 172/1000 | Loss: 0.00001595
Iteration 173/1000 | Loss: 0.00001595
Iteration 174/1000 | Loss: 0.00001595
Iteration 175/1000 | Loss: 0.00001595
Iteration 176/1000 | Loss: 0.00001595
Iteration 177/1000 | Loss: 0.00001595
Iteration 178/1000 | Loss: 0.00001595
Iteration 179/1000 | Loss: 0.00001595
Iteration 180/1000 | Loss: 0.00001595
Iteration 181/1000 | Loss: 0.00001595
Iteration 182/1000 | Loss: 0.00001595
Iteration 183/1000 | Loss: 0.00001595
Iteration 184/1000 | Loss: 0.00001595
Iteration 185/1000 | Loss: 0.00001595
Iteration 186/1000 | Loss: 0.00001595
Iteration 187/1000 | Loss: 0.00001595
Iteration 188/1000 | Loss: 0.00001594
Iteration 189/1000 | Loss: 0.00001594
Iteration 190/1000 | Loss: 0.00001594
Iteration 191/1000 | Loss: 0.00001594
Iteration 192/1000 | Loss: 0.00001594
Iteration 193/1000 | Loss: 0.00001594
Iteration 194/1000 | Loss: 0.00001594
Iteration 195/1000 | Loss: 0.00001594
Iteration 196/1000 | Loss: 0.00001594
Iteration 197/1000 | Loss: 0.00001594
Iteration 198/1000 | Loss: 0.00001594
Iteration 199/1000 | Loss: 0.00001594
Iteration 200/1000 | Loss: 0.00001594
Iteration 201/1000 | Loss: 0.00001594
Iteration 202/1000 | Loss: 0.00001594
Iteration 203/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.594449713593349e-05, 1.594449713593349e-05, 1.594449713593349e-05, 1.594449713593349e-05, 1.594449713593349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.594449713593349e-05

Optimization complete. Final v2v error: 3.3686721324920654 mm

Highest mean error: 3.640380620956421 mm for frame 160

Lowest mean error: 3.189981460571289 mm for frame 202

Saving results

Total time: 47.59504532814026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786245
Iteration 2/25 | Loss: 0.00267973
Iteration 3/25 | Loss: 0.00186459
Iteration 4/25 | Loss: 0.00193379
Iteration 5/25 | Loss: 0.00179630
Iteration 6/25 | Loss: 0.00173698
Iteration 7/25 | Loss: 0.00175146
Iteration 8/25 | Loss: 0.00162757
Iteration 9/25 | Loss: 0.00165614
Iteration 10/25 | Loss: 0.00155536
Iteration 11/25 | Loss: 0.00147985
Iteration 12/25 | Loss: 0.00144233
Iteration 13/25 | Loss: 0.00143389
Iteration 14/25 | Loss: 0.00143127
Iteration 15/25 | Loss: 0.00142877
Iteration 16/25 | Loss: 0.00140187
Iteration 17/25 | Loss: 0.00142084
Iteration 18/25 | Loss: 0.00139037
Iteration 19/25 | Loss: 0.00140791
Iteration 20/25 | Loss: 0.00141103
Iteration 21/25 | Loss: 0.00138422
Iteration 22/25 | Loss: 0.00137844
Iteration 23/25 | Loss: 0.00136232
Iteration 24/25 | Loss: 0.00135777
Iteration 25/25 | Loss: 0.00134737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79659736
Iteration 2/25 | Loss: 0.00187709
Iteration 3/25 | Loss: 0.00160603
Iteration 4/25 | Loss: 0.00150858
Iteration 5/25 | Loss: 0.00150858
Iteration 6/25 | Loss: 0.00150858
Iteration 7/25 | Loss: 0.00150858
Iteration 8/25 | Loss: 0.00150858
Iteration 9/25 | Loss: 0.00150858
Iteration 10/25 | Loss: 0.00150858
Iteration 11/25 | Loss: 0.00150858
Iteration 12/25 | Loss: 0.00150858
Iteration 13/25 | Loss: 0.00150858
Iteration 14/25 | Loss: 0.00150858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0015085753984749317, 0.0015085753984749317, 0.0015085753984749317, 0.0015085753984749317, 0.0015085753984749317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015085753984749317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150858
Iteration 2/1000 | Loss: 0.00027324
Iteration 3/1000 | Loss: 0.00050917
Iteration 4/1000 | Loss: 0.00019791
Iteration 5/1000 | Loss: 0.00020006
Iteration 6/1000 | Loss: 0.00156147
Iteration 7/1000 | Loss: 0.00012680
Iteration 8/1000 | Loss: 0.00010874
Iteration 9/1000 | Loss: 0.00008070
Iteration 10/1000 | Loss: 0.00005265
Iteration 11/1000 | Loss: 0.00007430
Iteration 12/1000 | Loss: 0.00073116
Iteration 13/1000 | Loss: 0.00011166
Iteration 14/1000 | Loss: 0.00025054
Iteration 15/1000 | Loss: 0.00005076
Iteration 16/1000 | Loss: 0.00045267
Iteration 17/1000 | Loss: 0.00154165
Iteration 18/1000 | Loss: 0.00075948
Iteration 19/1000 | Loss: 0.00016076
Iteration 20/1000 | Loss: 0.00004856
Iteration 21/1000 | Loss: 0.00087349
Iteration 22/1000 | Loss: 0.00079218
Iteration 23/1000 | Loss: 0.00126835
Iteration 24/1000 | Loss: 0.00008131
Iteration 25/1000 | Loss: 0.00025359
Iteration 26/1000 | Loss: 0.00006458
Iteration 27/1000 | Loss: 0.00044625
Iteration 28/1000 | Loss: 0.00004122
Iteration 29/1000 | Loss: 0.00038005
Iteration 30/1000 | Loss: 0.00003953
Iteration 31/1000 | Loss: 0.00003817
Iteration 32/1000 | Loss: 0.00003738
Iteration 33/1000 | Loss: 0.00017958
Iteration 34/1000 | Loss: 0.00040691
Iteration 35/1000 | Loss: 0.00003788
Iteration 36/1000 | Loss: 0.00003622
Iteration 37/1000 | Loss: 0.00003588
Iteration 38/1000 | Loss: 0.00003567
Iteration 39/1000 | Loss: 0.00003548
Iteration 40/1000 | Loss: 0.00003546
Iteration 41/1000 | Loss: 0.00003540
Iteration 42/1000 | Loss: 0.00003540
Iteration 43/1000 | Loss: 0.00005788
Iteration 44/1000 | Loss: 0.00003531
Iteration 45/1000 | Loss: 0.00037833
Iteration 46/1000 | Loss: 0.00160126
Iteration 47/1000 | Loss: 0.00119963
Iteration 48/1000 | Loss: 0.00013551
Iteration 49/1000 | Loss: 0.00007538
Iteration 50/1000 | Loss: 0.00007185
Iteration 51/1000 | Loss: 0.00006038
Iteration 52/1000 | Loss: 0.00035974
Iteration 53/1000 | Loss: 0.00003013
Iteration 54/1000 | Loss: 0.00002625
Iteration 55/1000 | Loss: 0.00021541
Iteration 56/1000 | Loss: 0.00002433
Iteration 57/1000 | Loss: 0.00002327
Iteration 58/1000 | Loss: 0.00056494
Iteration 59/1000 | Loss: 0.00013119
Iteration 60/1000 | Loss: 0.00007880
Iteration 61/1000 | Loss: 0.00004898
Iteration 62/1000 | Loss: 0.00002318
Iteration 63/1000 | Loss: 0.00007915
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00028239
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00015937
Iteration 70/1000 | Loss: 0.00003074
Iteration 71/1000 | Loss: 0.00001836
Iteration 72/1000 | Loss: 0.00002669
Iteration 73/1000 | Loss: 0.00005507
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00005242
Iteration 81/1000 | Loss: 0.00001836
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001740
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001734
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001734
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001733
Iteration 103/1000 | Loss: 0.00001733
Iteration 104/1000 | Loss: 0.00001733
Iteration 105/1000 | Loss: 0.00001733
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001732
Iteration 109/1000 | Loss: 0.00001732
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001732
Iteration 114/1000 | Loss: 0.00001732
Iteration 115/1000 | Loss: 0.00001732
Iteration 116/1000 | Loss: 0.00001732
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001732
Iteration 127/1000 | Loss: 0.00001732
Iteration 128/1000 | Loss: 0.00001732
Iteration 129/1000 | Loss: 0.00001732
Iteration 130/1000 | Loss: 0.00001732
Iteration 131/1000 | Loss: 0.00001732
Iteration 132/1000 | Loss: 0.00001732
Iteration 133/1000 | Loss: 0.00001732
Iteration 134/1000 | Loss: 0.00001732
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001732
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001732
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Iteration 145/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.7316395314992405e-05, 1.7316395314992405e-05, 1.7316395314992405e-05, 1.7316395314992405e-05, 1.7316395314992405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7316395314992405e-05

Optimization complete. Final v2v error: 3.4832797050476074 mm

Highest mean error: 3.940863609313965 mm for frame 18

Lowest mean error: 3.1406214237213135 mm for frame 122

Saving results

Total time: 151.7341856956482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424763
Iteration 2/25 | Loss: 0.00134743
Iteration 3/25 | Loss: 0.00124075
Iteration 4/25 | Loss: 0.00122869
Iteration 5/25 | Loss: 0.00122535
Iteration 6/25 | Loss: 0.00122461
Iteration 7/25 | Loss: 0.00122461
Iteration 8/25 | Loss: 0.00122461
Iteration 9/25 | Loss: 0.00122461
Iteration 10/25 | Loss: 0.00122461
Iteration 11/25 | Loss: 0.00122461
Iteration 12/25 | Loss: 0.00122461
Iteration 13/25 | Loss: 0.00122461
Iteration 14/25 | Loss: 0.00122461
Iteration 15/25 | Loss: 0.00122461
Iteration 16/25 | Loss: 0.00122461
Iteration 17/25 | Loss: 0.00122461
Iteration 18/25 | Loss: 0.00122461
Iteration 19/25 | Loss: 0.00122461
Iteration 20/25 | Loss: 0.00122461
Iteration 21/25 | Loss: 0.00122461
Iteration 22/25 | Loss: 0.00122461
Iteration 23/25 | Loss: 0.00122461
Iteration 24/25 | Loss: 0.00122461
Iteration 25/25 | Loss: 0.00122461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34659803
Iteration 2/25 | Loss: 0.00109831
Iteration 3/25 | Loss: 0.00109831
Iteration 4/25 | Loss: 0.00109831
Iteration 5/25 | Loss: 0.00109831
Iteration 6/25 | Loss: 0.00109831
Iteration 7/25 | Loss: 0.00109831
Iteration 8/25 | Loss: 0.00109831
Iteration 9/25 | Loss: 0.00109831
Iteration 10/25 | Loss: 0.00109831
Iteration 11/25 | Loss: 0.00109831
Iteration 12/25 | Loss: 0.00109831
Iteration 13/25 | Loss: 0.00109831
Iteration 14/25 | Loss: 0.00109831
Iteration 15/25 | Loss: 0.00109831
Iteration 16/25 | Loss: 0.00109831
Iteration 17/25 | Loss: 0.00109831
Iteration 18/25 | Loss: 0.00109831
Iteration 19/25 | Loss: 0.00109831
Iteration 20/25 | Loss: 0.00109831
Iteration 21/25 | Loss: 0.00109831
Iteration 22/25 | Loss: 0.00109831
Iteration 23/25 | Loss: 0.00109831
Iteration 24/25 | Loss: 0.00109831
Iteration 25/25 | Loss: 0.00109831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109831
Iteration 2/1000 | Loss: 0.00002946
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001411
Iteration 7/1000 | Loss: 0.00001353
Iteration 8/1000 | Loss: 0.00001316
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001278
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001208
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001206
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001194
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001188
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001183
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001181
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001181
Iteration 122/1000 | Loss: 0.00001181
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001179
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001176
Iteration 142/1000 | Loss: 0.00001176
Iteration 143/1000 | Loss: 0.00001176
Iteration 144/1000 | Loss: 0.00001176
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001174
Iteration 156/1000 | Loss: 0.00001174
Iteration 157/1000 | Loss: 0.00001174
Iteration 158/1000 | Loss: 0.00001174
Iteration 159/1000 | Loss: 0.00001174
Iteration 160/1000 | Loss: 0.00001174
Iteration 161/1000 | Loss: 0.00001174
Iteration 162/1000 | Loss: 0.00001174
Iteration 163/1000 | Loss: 0.00001174
Iteration 164/1000 | Loss: 0.00001173
Iteration 165/1000 | Loss: 0.00001173
Iteration 166/1000 | Loss: 0.00001173
Iteration 167/1000 | Loss: 0.00001173
Iteration 168/1000 | Loss: 0.00001173
Iteration 169/1000 | Loss: 0.00001173
Iteration 170/1000 | Loss: 0.00001173
Iteration 171/1000 | Loss: 0.00001173
Iteration 172/1000 | Loss: 0.00001173
Iteration 173/1000 | Loss: 0.00001173
Iteration 174/1000 | Loss: 0.00001173
Iteration 175/1000 | Loss: 0.00001173
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1728528079402167e-05, 1.1728528079402167e-05, 1.1728528079402167e-05, 1.1728528079402167e-05, 1.1728528079402167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1728528079402167e-05

Optimization complete. Final v2v error: 2.9316866397857666 mm

Highest mean error: 3.9854278564453125 mm for frame 64

Lowest mean error: 2.647087574005127 mm for frame 183

Saving results

Total time: 44.895097732543945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010569
Iteration 2/25 | Loss: 0.00188413
Iteration 3/25 | Loss: 0.00155163
Iteration 4/25 | Loss: 0.00151281
Iteration 5/25 | Loss: 0.00159461
Iteration 6/25 | Loss: 0.00137735
Iteration 7/25 | Loss: 0.00132934
Iteration 8/25 | Loss: 0.00132189
Iteration 9/25 | Loss: 0.00132017
Iteration 10/25 | Loss: 0.00132017
Iteration 11/25 | Loss: 0.00132017
Iteration 12/25 | Loss: 0.00132017
Iteration 13/25 | Loss: 0.00132017
Iteration 14/25 | Loss: 0.00132017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013201743131503463, 0.0013201743131503463, 0.0013201743131503463, 0.0013201743131503463, 0.0013201743131503463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013201743131503463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29110563
Iteration 2/25 | Loss: 0.00120410
Iteration 3/25 | Loss: 0.00120409
Iteration 4/25 | Loss: 0.00120409
Iteration 5/25 | Loss: 0.00120409
Iteration 6/25 | Loss: 0.00120409
Iteration 7/25 | Loss: 0.00120409
Iteration 8/25 | Loss: 0.00120409
Iteration 9/25 | Loss: 0.00120409
Iteration 10/25 | Loss: 0.00120409
Iteration 11/25 | Loss: 0.00120409
Iteration 12/25 | Loss: 0.00120409
Iteration 13/25 | Loss: 0.00120409
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012040940346196294, 0.0012040940346196294, 0.0012040940346196294, 0.0012040940346196294, 0.0012040940346196294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012040940346196294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120409
Iteration 2/1000 | Loss: 0.00003593
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002391
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002246
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002181
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002126
Iteration 11/1000 | Loss: 0.00002113
Iteration 12/1000 | Loss: 0.00002096
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002031
Iteration 16/1000 | Loss: 0.00002026
Iteration 17/1000 | Loss: 0.00002025
Iteration 18/1000 | Loss: 0.00002025
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002021
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00002020
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002012
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00002011
Iteration 31/1000 | Loss: 0.00002010
Iteration 32/1000 | Loss: 0.00002010
Iteration 33/1000 | Loss: 0.00002009
Iteration 34/1000 | Loss: 0.00002009
Iteration 35/1000 | Loss: 0.00002009
Iteration 36/1000 | Loss: 0.00002008
Iteration 37/1000 | Loss: 0.00002008
Iteration 38/1000 | Loss: 0.00002007
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002006
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00002005
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00002004
Iteration 48/1000 | Loss: 0.00002004
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00002004
Iteration 51/1000 | Loss: 0.00002004
Iteration 52/1000 | Loss: 0.00002004
Iteration 53/1000 | Loss: 0.00002004
Iteration 54/1000 | Loss: 0.00002003
Iteration 55/1000 | Loss: 0.00002003
Iteration 56/1000 | Loss: 0.00002002
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002001
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002001
Iteration 72/1000 | Loss: 0.00002001
Iteration 73/1000 | Loss: 0.00002001
Iteration 74/1000 | Loss: 0.00002001
Iteration 75/1000 | Loss: 0.00002000
Iteration 76/1000 | Loss: 0.00002000
Iteration 77/1000 | Loss: 0.00002000
Iteration 78/1000 | Loss: 0.00002000
Iteration 79/1000 | Loss: 0.00002000
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001999
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00001998
Iteration 88/1000 | Loss: 0.00001998
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Iteration 91/1000 | Loss: 0.00001998
Iteration 92/1000 | Loss: 0.00001998
Iteration 93/1000 | Loss: 0.00001998
Iteration 94/1000 | Loss: 0.00001998
Iteration 95/1000 | Loss: 0.00001998
Iteration 96/1000 | Loss: 0.00001998
Iteration 97/1000 | Loss: 0.00001998
Iteration 98/1000 | Loss: 0.00001998
Iteration 99/1000 | Loss: 0.00001998
Iteration 100/1000 | Loss: 0.00001998
Iteration 101/1000 | Loss: 0.00001998
Iteration 102/1000 | Loss: 0.00001998
Iteration 103/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.9979521312052384e-05, 1.9979521312052384e-05, 1.9979521312052384e-05, 1.9979521312052384e-05, 1.9979521312052384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9979521312052384e-05

Optimization complete. Final v2v error: 3.7801434993743896 mm

Highest mean error: 3.8723461627960205 mm for frame 83

Lowest mean error: 3.586864948272705 mm for frame 173

Saving results

Total time: 40.122838735580444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00645928
Iteration 2/25 | Loss: 0.00139391
Iteration 3/25 | Loss: 0.00127548
Iteration 4/25 | Loss: 0.00126162
Iteration 5/25 | Loss: 0.00125854
Iteration 6/25 | Loss: 0.00125854
Iteration 7/25 | Loss: 0.00125854
Iteration 8/25 | Loss: 0.00125854
Iteration 9/25 | Loss: 0.00125854
Iteration 10/25 | Loss: 0.00125854
Iteration 11/25 | Loss: 0.00125854
Iteration 12/25 | Loss: 0.00125854
Iteration 13/25 | Loss: 0.00125854
Iteration 14/25 | Loss: 0.00125854
Iteration 15/25 | Loss: 0.00125854
Iteration 16/25 | Loss: 0.00125854
Iteration 17/25 | Loss: 0.00125854
Iteration 18/25 | Loss: 0.00125854
Iteration 19/25 | Loss: 0.00125854
Iteration 20/25 | Loss: 0.00125854
Iteration 21/25 | Loss: 0.00125854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012585407821461558, 0.0012585407821461558, 0.0012585407821461558, 0.0012585407821461558, 0.0012585407821461558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012585407821461558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06130028
Iteration 2/25 | Loss: 0.00102288
Iteration 3/25 | Loss: 0.00102288
Iteration 4/25 | Loss: 0.00102288
Iteration 5/25 | Loss: 0.00102288
Iteration 6/25 | Loss: 0.00102288
Iteration 7/25 | Loss: 0.00102288
Iteration 8/25 | Loss: 0.00102288
Iteration 9/25 | Loss: 0.00102288
Iteration 10/25 | Loss: 0.00102288
Iteration 11/25 | Loss: 0.00102288
Iteration 12/25 | Loss: 0.00102288
Iteration 13/25 | Loss: 0.00102288
Iteration 14/25 | Loss: 0.00102288
Iteration 15/25 | Loss: 0.00102288
Iteration 16/25 | Loss: 0.00102288
Iteration 17/25 | Loss: 0.00102288
Iteration 18/25 | Loss: 0.00102288
Iteration 19/25 | Loss: 0.00102288
Iteration 20/25 | Loss: 0.00102288
Iteration 21/25 | Loss: 0.00102288
Iteration 22/25 | Loss: 0.00102288
Iteration 23/25 | Loss: 0.00102288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010228798491880298, 0.0010228798491880298, 0.0010228798491880298, 0.0010228798491880298, 0.0010228798491880298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010228798491880298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102288
Iteration 2/1000 | Loss: 0.00004087
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002185
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001925
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001795
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001659
Iteration 14/1000 | Loss: 0.00001656
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001611
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001604
Iteration 23/1000 | Loss: 0.00001601
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001592
Iteration 32/1000 | Loss: 0.00001592
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001590
Iteration 37/1000 | Loss: 0.00001590
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001585
Iteration 43/1000 | Loss: 0.00001585
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001584
Iteration 51/1000 | Loss: 0.00001583
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001581
Iteration 55/1000 | Loss: 0.00001580
Iteration 56/1000 | Loss: 0.00001580
Iteration 57/1000 | Loss: 0.00001580
Iteration 58/1000 | Loss: 0.00001580
Iteration 59/1000 | Loss: 0.00001580
Iteration 60/1000 | Loss: 0.00001580
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001578
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001570
Iteration 82/1000 | Loss: 0.00001570
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001570
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001570
Iteration 101/1000 | Loss: 0.00001570
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.5695524780312553e-05, 1.5695524780312553e-05, 1.5695524780312553e-05, 1.5695524780312553e-05, 1.5695524780312553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5695524780312553e-05

Optimization complete. Final v2v error: 3.3055012226104736 mm

Highest mean error: 3.8388350009918213 mm for frame 195

Lowest mean error: 2.8620080947875977 mm for frame 87

Saving results

Total time: 41.1626398563385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353261
Iteration 2/25 | Loss: 0.00123745
Iteration 3/25 | Loss: 0.00117395
Iteration 4/25 | Loss: 0.00116545
Iteration 5/25 | Loss: 0.00116237
Iteration 6/25 | Loss: 0.00116194
Iteration 7/25 | Loss: 0.00116194
Iteration 8/25 | Loss: 0.00116194
Iteration 9/25 | Loss: 0.00116194
Iteration 10/25 | Loss: 0.00116194
Iteration 11/25 | Loss: 0.00116194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011619380675256252, 0.0011619380675256252, 0.0011619380675256252, 0.0011619380675256252, 0.0011619380675256252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011619380675256252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36185145
Iteration 2/25 | Loss: 0.00103553
Iteration 3/25 | Loss: 0.00103553
Iteration 4/25 | Loss: 0.00103552
Iteration 5/25 | Loss: 0.00103552
Iteration 6/25 | Loss: 0.00103552
Iteration 7/25 | Loss: 0.00103552
Iteration 8/25 | Loss: 0.00103552
Iteration 9/25 | Loss: 0.00103552
Iteration 10/25 | Loss: 0.00103552
Iteration 11/25 | Loss: 0.00103552
Iteration 12/25 | Loss: 0.00103552
Iteration 13/25 | Loss: 0.00103552
Iteration 14/25 | Loss: 0.00103552
Iteration 15/25 | Loss: 0.00103552
Iteration 16/25 | Loss: 0.00103552
Iteration 17/25 | Loss: 0.00103552
Iteration 18/25 | Loss: 0.00103552
Iteration 19/25 | Loss: 0.00103552
Iteration 20/25 | Loss: 0.00103552
Iteration 21/25 | Loss: 0.00103552
Iteration 22/25 | Loss: 0.00103552
Iteration 23/25 | Loss: 0.00103552
Iteration 24/25 | Loss: 0.00103552
Iteration 25/25 | Loss: 0.00103552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103552
Iteration 2/1000 | Loss: 0.00001666
Iteration 3/1000 | Loss: 0.00001205
Iteration 4/1000 | Loss: 0.00001098
Iteration 5/1000 | Loss: 0.00001031
Iteration 6/1000 | Loss: 0.00000982
Iteration 7/1000 | Loss: 0.00000958
Iteration 8/1000 | Loss: 0.00000935
Iteration 9/1000 | Loss: 0.00000916
Iteration 10/1000 | Loss: 0.00000913
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000913
Iteration 13/1000 | Loss: 0.00000912
Iteration 14/1000 | Loss: 0.00000909
Iteration 15/1000 | Loss: 0.00000909
Iteration 16/1000 | Loss: 0.00000908
Iteration 17/1000 | Loss: 0.00000908
Iteration 18/1000 | Loss: 0.00000907
Iteration 19/1000 | Loss: 0.00000907
Iteration 20/1000 | Loss: 0.00000906
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000905
Iteration 24/1000 | Loss: 0.00000904
Iteration 25/1000 | Loss: 0.00000903
Iteration 26/1000 | Loss: 0.00000899
Iteration 27/1000 | Loss: 0.00000898
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000896
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000895
Iteration 33/1000 | Loss: 0.00000895
Iteration 34/1000 | Loss: 0.00000893
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000886
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000884
Iteration 39/1000 | Loss: 0.00000884
Iteration 40/1000 | Loss: 0.00000884
Iteration 41/1000 | Loss: 0.00000884
Iteration 42/1000 | Loss: 0.00000884
Iteration 43/1000 | Loss: 0.00000883
Iteration 44/1000 | Loss: 0.00000883
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000880
Iteration 48/1000 | Loss: 0.00000880
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000878
Iteration 51/1000 | Loss: 0.00000878
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000877
Iteration 54/1000 | Loss: 0.00000876
Iteration 55/1000 | Loss: 0.00000876
Iteration 56/1000 | Loss: 0.00000875
Iteration 57/1000 | Loss: 0.00000874
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000873
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000872
Iteration 66/1000 | Loss: 0.00000872
Iteration 67/1000 | Loss: 0.00000872
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000866
Iteration 75/1000 | Loss: 0.00000865
Iteration 76/1000 | Loss: 0.00000865
Iteration 77/1000 | Loss: 0.00000864
Iteration 78/1000 | Loss: 0.00000864
Iteration 79/1000 | Loss: 0.00000864
Iteration 80/1000 | Loss: 0.00000861
Iteration 81/1000 | Loss: 0.00000861
Iteration 82/1000 | Loss: 0.00000860
Iteration 83/1000 | Loss: 0.00000860
Iteration 84/1000 | Loss: 0.00000860
Iteration 85/1000 | Loss: 0.00000859
Iteration 86/1000 | Loss: 0.00000858
Iteration 87/1000 | Loss: 0.00000858
Iteration 88/1000 | Loss: 0.00000857
Iteration 89/1000 | Loss: 0.00000857
Iteration 90/1000 | Loss: 0.00000857
Iteration 91/1000 | Loss: 0.00000856
Iteration 92/1000 | Loss: 0.00000856
Iteration 93/1000 | Loss: 0.00000855
Iteration 94/1000 | Loss: 0.00000853
Iteration 95/1000 | Loss: 0.00000852
Iteration 96/1000 | Loss: 0.00000852
Iteration 97/1000 | Loss: 0.00000852
Iteration 98/1000 | Loss: 0.00000852
Iteration 99/1000 | Loss: 0.00000852
Iteration 100/1000 | Loss: 0.00000852
Iteration 101/1000 | Loss: 0.00000851
Iteration 102/1000 | Loss: 0.00000851
Iteration 103/1000 | Loss: 0.00000851
Iteration 104/1000 | Loss: 0.00000851
Iteration 105/1000 | Loss: 0.00000850
Iteration 106/1000 | Loss: 0.00000850
Iteration 107/1000 | Loss: 0.00000850
Iteration 108/1000 | Loss: 0.00000850
Iteration 109/1000 | Loss: 0.00000850
Iteration 110/1000 | Loss: 0.00000850
Iteration 111/1000 | Loss: 0.00000849
Iteration 112/1000 | Loss: 0.00000849
Iteration 113/1000 | Loss: 0.00000849
Iteration 114/1000 | Loss: 0.00000849
Iteration 115/1000 | Loss: 0.00000849
Iteration 116/1000 | Loss: 0.00000849
Iteration 117/1000 | Loss: 0.00000849
Iteration 118/1000 | Loss: 0.00000849
Iteration 119/1000 | Loss: 0.00000849
Iteration 120/1000 | Loss: 0.00000849
Iteration 121/1000 | Loss: 0.00000848
Iteration 122/1000 | Loss: 0.00000848
Iteration 123/1000 | Loss: 0.00000848
Iteration 124/1000 | Loss: 0.00000848
Iteration 125/1000 | Loss: 0.00000847
Iteration 126/1000 | Loss: 0.00000847
Iteration 127/1000 | Loss: 0.00000847
Iteration 128/1000 | Loss: 0.00000847
Iteration 129/1000 | Loss: 0.00000846
Iteration 130/1000 | Loss: 0.00000846
Iteration 131/1000 | Loss: 0.00000846
Iteration 132/1000 | Loss: 0.00000846
Iteration 133/1000 | Loss: 0.00000845
Iteration 134/1000 | Loss: 0.00000845
Iteration 135/1000 | Loss: 0.00000845
Iteration 136/1000 | Loss: 0.00000845
Iteration 137/1000 | Loss: 0.00000845
Iteration 138/1000 | Loss: 0.00000845
Iteration 139/1000 | Loss: 0.00000844
Iteration 140/1000 | Loss: 0.00000844
Iteration 141/1000 | Loss: 0.00000844
Iteration 142/1000 | Loss: 0.00000844
Iteration 143/1000 | Loss: 0.00000844
Iteration 144/1000 | Loss: 0.00000844
Iteration 145/1000 | Loss: 0.00000844
Iteration 146/1000 | Loss: 0.00000844
Iteration 147/1000 | Loss: 0.00000844
Iteration 148/1000 | Loss: 0.00000844
Iteration 149/1000 | Loss: 0.00000843
Iteration 150/1000 | Loss: 0.00000843
Iteration 151/1000 | Loss: 0.00000843
Iteration 152/1000 | Loss: 0.00000843
Iteration 153/1000 | Loss: 0.00000843
Iteration 154/1000 | Loss: 0.00000843
Iteration 155/1000 | Loss: 0.00000843
Iteration 156/1000 | Loss: 0.00000843
Iteration 157/1000 | Loss: 0.00000843
Iteration 158/1000 | Loss: 0.00000843
Iteration 159/1000 | Loss: 0.00000843
Iteration 160/1000 | Loss: 0.00000843
Iteration 161/1000 | Loss: 0.00000843
Iteration 162/1000 | Loss: 0.00000843
Iteration 163/1000 | Loss: 0.00000843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.4285820776131e-06, 8.4285820776131e-06, 8.4285820776131e-06, 8.4285820776131e-06, 8.4285820776131e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.4285820776131e-06

Optimization complete. Final v2v error: 2.5160210132598877 mm

Highest mean error: 2.651553153991699 mm for frame 130

Lowest mean error: 2.4759232997894287 mm for frame 104

Saving results

Total time: 35.41928958892822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363914
Iteration 2/25 | Loss: 0.00130898
Iteration 3/25 | Loss: 0.00121904
Iteration 4/25 | Loss: 0.00120317
Iteration 5/25 | Loss: 0.00119870
Iteration 6/25 | Loss: 0.00119776
Iteration 7/25 | Loss: 0.00119776
Iteration 8/25 | Loss: 0.00119776
Iteration 9/25 | Loss: 0.00119776
Iteration 10/25 | Loss: 0.00119776
Iteration 11/25 | Loss: 0.00119776
Iteration 12/25 | Loss: 0.00119776
Iteration 13/25 | Loss: 0.00119776
Iteration 14/25 | Loss: 0.00119776
Iteration 15/25 | Loss: 0.00119776
Iteration 16/25 | Loss: 0.00119776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011977608082816005, 0.0011977608082816005, 0.0011977608082816005, 0.0011977608082816005, 0.0011977608082816005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011977608082816005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36174655
Iteration 2/25 | Loss: 0.00125705
Iteration 3/25 | Loss: 0.00125705
Iteration 4/25 | Loss: 0.00125705
Iteration 5/25 | Loss: 0.00125705
Iteration 6/25 | Loss: 0.00125705
Iteration 7/25 | Loss: 0.00125705
Iteration 8/25 | Loss: 0.00125705
Iteration 9/25 | Loss: 0.00125705
Iteration 10/25 | Loss: 0.00125705
Iteration 11/25 | Loss: 0.00125705
Iteration 12/25 | Loss: 0.00125705
Iteration 13/25 | Loss: 0.00125705
Iteration 14/25 | Loss: 0.00125705
Iteration 15/25 | Loss: 0.00125705
Iteration 16/25 | Loss: 0.00125705
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012570454273372889, 0.0012570454273372889, 0.0012570454273372889, 0.0012570454273372889, 0.0012570454273372889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012570454273372889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125705
Iteration 2/1000 | Loss: 0.00005108
Iteration 3/1000 | Loss: 0.00003430
Iteration 4/1000 | Loss: 0.00002495
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001741
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001703
Iteration 14/1000 | Loss: 0.00001690
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001675
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001657
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001644
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001642
Iteration 81/1000 | Loss: 0.00001642
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001641
Iteration 84/1000 | Loss: 0.00001641
Iteration 85/1000 | Loss: 0.00001640
Iteration 86/1000 | Loss: 0.00001640
Iteration 87/1000 | Loss: 0.00001640
Iteration 88/1000 | Loss: 0.00001640
Iteration 89/1000 | Loss: 0.00001639
Iteration 90/1000 | Loss: 0.00001639
Iteration 91/1000 | Loss: 0.00001639
Iteration 92/1000 | Loss: 0.00001639
Iteration 93/1000 | Loss: 0.00001639
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001638
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001638
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001637
Iteration 101/1000 | Loss: 0.00001637
Iteration 102/1000 | Loss: 0.00001637
Iteration 103/1000 | Loss: 0.00001637
Iteration 104/1000 | Loss: 0.00001637
Iteration 105/1000 | Loss: 0.00001637
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001635
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001635
Iteration 112/1000 | Loss: 0.00001635
Iteration 113/1000 | Loss: 0.00001634
Iteration 114/1000 | Loss: 0.00001634
Iteration 115/1000 | Loss: 0.00001634
Iteration 116/1000 | Loss: 0.00001634
Iteration 117/1000 | Loss: 0.00001633
Iteration 118/1000 | Loss: 0.00001633
Iteration 119/1000 | Loss: 0.00001633
Iteration 120/1000 | Loss: 0.00001633
Iteration 121/1000 | Loss: 0.00001633
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001632
Iteration 124/1000 | Loss: 0.00001632
Iteration 125/1000 | Loss: 0.00001632
Iteration 126/1000 | Loss: 0.00001632
Iteration 127/1000 | Loss: 0.00001632
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001631
Iteration 132/1000 | Loss: 0.00001630
Iteration 133/1000 | Loss: 0.00001630
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001629
Iteration 136/1000 | Loss: 0.00001629
Iteration 137/1000 | Loss: 0.00001629
Iteration 138/1000 | Loss: 0.00001629
Iteration 139/1000 | Loss: 0.00001629
Iteration 140/1000 | Loss: 0.00001629
Iteration 141/1000 | Loss: 0.00001628
Iteration 142/1000 | Loss: 0.00001628
Iteration 143/1000 | Loss: 0.00001628
Iteration 144/1000 | Loss: 0.00001628
Iteration 145/1000 | Loss: 0.00001628
Iteration 146/1000 | Loss: 0.00001628
Iteration 147/1000 | Loss: 0.00001628
Iteration 148/1000 | Loss: 0.00001628
Iteration 149/1000 | Loss: 0.00001627
Iteration 150/1000 | Loss: 0.00001627
Iteration 151/1000 | Loss: 0.00001627
Iteration 152/1000 | Loss: 0.00001627
Iteration 153/1000 | Loss: 0.00001627
Iteration 154/1000 | Loss: 0.00001627
Iteration 155/1000 | Loss: 0.00001627
Iteration 156/1000 | Loss: 0.00001627
Iteration 157/1000 | Loss: 0.00001627
Iteration 158/1000 | Loss: 0.00001627
Iteration 159/1000 | Loss: 0.00001627
Iteration 160/1000 | Loss: 0.00001627
Iteration 161/1000 | Loss: 0.00001627
Iteration 162/1000 | Loss: 0.00001626
Iteration 163/1000 | Loss: 0.00001626
Iteration 164/1000 | Loss: 0.00001626
Iteration 165/1000 | Loss: 0.00001626
Iteration 166/1000 | Loss: 0.00001626
Iteration 167/1000 | Loss: 0.00001626
Iteration 168/1000 | Loss: 0.00001626
Iteration 169/1000 | Loss: 0.00001626
Iteration 170/1000 | Loss: 0.00001626
Iteration 171/1000 | Loss: 0.00001626
Iteration 172/1000 | Loss: 0.00001626
Iteration 173/1000 | Loss: 0.00001626
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001626
Iteration 176/1000 | Loss: 0.00001626
Iteration 177/1000 | Loss: 0.00001626
Iteration 178/1000 | Loss: 0.00001626
Iteration 179/1000 | Loss: 0.00001626
Iteration 180/1000 | Loss: 0.00001626
Iteration 181/1000 | Loss: 0.00001626
Iteration 182/1000 | Loss: 0.00001626
Iteration 183/1000 | Loss: 0.00001626
Iteration 184/1000 | Loss: 0.00001626
Iteration 185/1000 | Loss: 0.00001626
Iteration 186/1000 | Loss: 0.00001626
Iteration 187/1000 | Loss: 0.00001626
Iteration 188/1000 | Loss: 0.00001626
Iteration 189/1000 | Loss: 0.00001626
Iteration 190/1000 | Loss: 0.00001626
Iteration 191/1000 | Loss: 0.00001626
Iteration 192/1000 | Loss: 0.00001626
Iteration 193/1000 | Loss: 0.00001626
Iteration 194/1000 | Loss: 0.00001626
Iteration 195/1000 | Loss: 0.00001626
Iteration 196/1000 | Loss: 0.00001626
Iteration 197/1000 | Loss: 0.00001626
Iteration 198/1000 | Loss: 0.00001626
Iteration 199/1000 | Loss: 0.00001626
Iteration 200/1000 | Loss: 0.00001626
Iteration 201/1000 | Loss: 0.00001626
Iteration 202/1000 | Loss: 0.00001626
Iteration 203/1000 | Loss: 0.00001626
Iteration 204/1000 | Loss: 0.00001626
Iteration 205/1000 | Loss: 0.00001626
Iteration 206/1000 | Loss: 0.00001626
Iteration 207/1000 | Loss: 0.00001626
Iteration 208/1000 | Loss: 0.00001626
Iteration 209/1000 | Loss: 0.00001626
Iteration 210/1000 | Loss: 0.00001626
Iteration 211/1000 | Loss: 0.00001626
Iteration 212/1000 | Loss: 0.00001626
Iteration 213/1000 | Loss: 0.00001626
Iteration 214/1000 | Loss: 0.00001626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.6264288206002675e-05, 1.6264288206002675e-05, 1.6264288206002675e-05, 1.6264288206002675e-05, 1.6264288206002675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6264288206002675e-05

Optimization complete. Final v2v error: 3.3785548210144043 mm

Highest mean error: 4.961087226867676 mm for frame 151

Lowest mean error: 2.4745161533355713 mm for frame 1

Saving results

Total time: 43.72980046272278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440097
Iteration 2/25 | Loss: 0.00133767
Iteration 3/25 | Loss: 0.00126271
Iteration 4/25 | Loss: 0.00125252
Iteration 5/25 | Loss: 0.00124827
Iteration 6/25 | Loss: 0.00124813
Iteration 7/25 | Loss: 0.00124813
Iteration 8/25 | Loss: 0.00124813
Iteration 9/25 | Loss: 0.00124813
Iteration 10/25 | Loss: 0.00124813
Iteration 11/25 | Loss: 0.00124813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012481301091611385, 0.0012481301091611385, 0.0012481301091611385, 0.0012481301091611385, 0.0012481301091611385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012481301091611385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33778918
Iteration 2/25 | Loss: 0.00125552
Iteration 3/25 | Loss: 0.00125552
Iteration 4/25 | Loss: 0.00125552
Iteration 5/25 | Loss: 0.00125552
Iteration 6/25 | Loss: 0.00125552
Iteration 7/25 | Loss: 0.00125552
Iteration 8/25 | Loss: 0.00125552
Iteration 9/25 | Loss: 0.00125552
Iteration 10/25 | Loss: 0.00125552
Iteration 11/25 | Loss: 0.00125552
Iteration 12/25 | Loss: 0.00125552
Iteration 13/25 | Loss: 0.00125552
Iteration 14/25 | Loss: 0.00125552
Iteration 15/25 | Loss: 0.00125552
Iteration 16/25 | Loss: 0.00125552
Iteration 17/25 | Loss: 0.00125552
Iteration 18/25 | Loss: 0.00125552
Iteration 19/25 | Loss: 0.00125552
Iteration 20/25 | Loss: 0.00125552
Iteration 21/25 | Loss: 0.00125552
Iteration 22/25 | Loss: 0.00125552
Iteration 23/25 | Loss: 0.00125552
Iteration 24/25 | Loss: 0.00125552
Iteration 25/25 | Loss: 0.00125552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125552
Iteration 2/1000 | Loss: 0.00004523
Iteration 3/1000 | Loss: 0.00002784
Iteration 4/1000 | Loss: 0.00002298
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00001963
Iteration 7/1000 | Loss: 0.00001873
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001727
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001665
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001634
Iteration 15/1000 | Loss: 0.00001631
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001624
Iteration 21/1000 | Loss: 0.00001623
Iteration 22/1000 | Loss: 0.00001619
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001600
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001599
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001588
Iteration 57/1000 | Loss: 0.00001588
Iteration 58/1000 | Loss: 0.00001588
Iteration 59/1000 | Loss: 0.00001587
Iteration 60/1000 | Loss: 0.00001587
Iteration 61/1000 | Loss: 0.00001587
Iteration 62/1000 | Loss: 0.00001586
Iteration 63/1000 | Loss: 0.00001586
Iteration 64/1000 | Loss: 0.00001586
Iteration 65/1000 | Loss: 0.00001585
Iteration 66/1000 | Loss: 0.00001585
Iteration 67/1000 | Loss: 0.00001585
Iteration 68/1000 | Loss: 0.00001585
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001584
Iteration 71/1000 | Loss: 0.00001584
Iteration 72/1000 | Loss: 0.00001584
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001583
Iteration 78/1000 | Loss: 0.00001583
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001581
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.5807952877366915e-05, 1.5807952877366915e-05, 1.5807952877366915e-05, 1.5807952877366915e-05, 1.5807952877366915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5807952877366915e-05

Optimization complete. Final v2v error: 3.3457889556884766 mm

Highest mean error: 4.145987510681152 mm for frame 12

Lowest mean error: 2.701362371444702 mm for frame 186

Saving results

Total time: 42.03400206565857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800203
Iteration 2/25 | Loss: 0.00151860
Iteration 3/25 | Loss: 0.00126067
Iteration 4/25 | Loss: 0.00123931
Iteration 5/25 | Loss: 0.00123667
Iteration 6/25 | Loss: 0.00123582
Iteration 7/25 | Loss: 0.00123540
Iteration 8/25 | Loss: 0.00123517
Iteration 9/25 | Loss: 0.00123501
Iteration 10/25 | Loss: 0.00123817
Iteration 11/25 | Loss: 0.00123775
Iteration 12/25 | Loss: 0.00123836
Iteration 13/25 | Loss: 0.00123787
Iteration 14/25 | Loss: 0.00123560
Iteration 15/25 | Loss: 0.00123800
Iteration 16/25 | Loss: 0.00123662
Iteration 17/25 | Loss: 0.00123777
Iteration 18/25 | Loss: 0.00123689
Iteration 19/25 | Loss: 0.00123628
Iteration 20/25 | Loss: 0.00123501
Iteration 21/25 | Loss: 0.00123602
Iteration 22/25 | Loss: 0.00123629
Iteration 23/25 | Loss: 0.00123710
Iteration 24/25 | Loss: 0.00123678
Iteration 25/25 | Loss: 0.00123696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35073483
Iteration 2/25 | Loss: 0.00100307
Iteration 3/25 | Loss: 0.00100307
Iteration 4/25 | Loss: 0.00100307
Iteration 5/25 | Loss: 0.00100307
Iteration 6/25 | Loss: 0.00100307
Iteration 7/25 | Loss: 0.00100307
Iteration 8/25 | Loss: 0.00100307
Iteration 9/25 | Loss: 0.00100307
Iteration 10/25 | Loss: 0.00100307
Iteration 11/25 | Loss: 0.00100307
Iteration 12/25 | Loss: 0.00100307
Iteration 13/25 | Loss: 0.00100307
Iteration 14/25 | Loss: 0.00100307
Iteration 15/25 | Loss: 0.00100307
Iteration 16/25 | Loss: 0.00100307
Iteration 17/25 | Loss: 0.00100307
Iteration 18/25 | Loss: 0.00100307
Iteration 19/25 | Loss: 0.00100307
Iteration 20/25 | Loss: 0.00100307
Iteration 21/25 | Loss: 0.00100307
Iteration 22/25 | Loss: 0.00100307
Iteration 23/25 | Loss: 0.00100307
Iteration 24/25 | Loss: 0.00100307
Iteration 25/25 | Loss: 0.00100307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100307
Iteration 2/1000 | Loss: 0.00002795
Iteration 3/1000 | Loss: 0.00004372
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00003611
Iteration 6/1000 | Loss: 0.00004168
Iteration 7/1000 | Loss: 0.00002817
Iteration 8/1000 | Loss: 0.00006288
Iteration 9/1000 | Loss: 0.00002930
Iteration 10/1000 | Loss: 0.00002195
Iteration 11/1000 | Loss: 0.00004364
Iteration 12/1000 | Loss: 0.00004647
Iteration 13/1000 | Loss: 0.00003024
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00003592
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001489
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001251
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001171
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001146
Iteration 31/1000 | Loss: 0.00001144
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001143
Iteration 34/1000 | Loss: 0.00001143
Iteration 35/1000 | Loss: 0.00001143
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001131
Iteration 48/1000 | Loss: 0.00001131
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001130
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001125
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001116
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001108
Iteration 117/1000 | Loss: 0.00001108
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001106
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001104
Iteration 140/1000 | Loss: 0.00001104
Iteration 141/1000 | Loss: 0.00001104
Iteration 142/1000 | Loss: 0.00001104
Iteration 143/1000 | Loss: 0.00001104
Iteration 144/1000 | Loss: 0.00001104
Iteration 145/1000 | Loss: 0.00001104
Iteration 146/1000 | Loss: 0.00001104
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001103
Iteration 149/1000 | Loss: 0.00001103
Iteration 150/1000 | Loss: 0.00001103
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001102
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001101
Iteration 200/1000 | Loss: 0.00001101
Iteration 201/1000 | Loss: 0.00001101
Iteration 202/1000 | Loss: 0.00001101
Iteration 203/1000 | Loss: 0.00001101
Iteration 204/1000 | Loss: 0.00001101
Iteration 205/1000 | Loss: 0.00001101
Iteration 206/1000 | Loss: 0.00001101
Iteration 207/1000 | Loss: 0.00001101
Iteration 208/1000 | Loss: 0.00001101
Iteration 209/1000 | Loss: 0.00001101
Iteration 210/1000 | Loss: 0.00001101
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Iteration 219/1000 | Loss: 0.00001101
Iteration 220/1000 | Loss: 0.00001101
Iteration 221/1000 | Loss: 0.00001101
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001101
Iteration 225/1000 | Loss: 0.00001101
Iteration 226/1000 | Loss: 0.00001101
Iteration 227/1000 | Loss: 0.00001101
Iteration 228/1000 | Loss: 0.00001101
Iteration 229/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.100859935831977e-05, 1.100859935831977e-05, 1.100859935831977e-05, 1.100859935831977e-05, 1.100859935831977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.100859935831977e-05

Optimization complete. Final v2v error: 2.85461163520813 mm

Highest mean error: 3.466507911682129 mm for frame 53

Lowest mean error: 2.7299861907958984 mm for frame 20

Saving results

Total time: 95.07739663124084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441491
Iteration 2/25 | Loss: 0.00130623
Iteration 3/25 | Loss: 0.00124641
Iteration 4/25 | Loss: 0.00123547
Iteration 5/25 | Loss: 0.00123184
Iteration 6/25 | Loss: 0.00123167
Iteration 7/25 | Loss: 0.00123167
Iteration 8/25 | Loss: 0.00123167
Iteration 9/25 | Loss: 0.00123167
Iteration 10/25 | Loss: 0.00123167
Iteration 11/25 | Loss: 0.00123167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001231667003594339, 0.001231667003594339, 0.001231667003594339, 0.001231667003594339, 0.001231667003594339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001231667003594339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34080875
Iteration 2/25 | Loss: 0.00097976
Iteration 3/25 | Loss: 0.00097975
Iteration 4/25 | Loss: 0.00097975
Iteration 5/25 | Loss: 0.00097975
Iteration 6/25 | Loss: 0.00097975
Iteration 7/25 | Loss: 0.00097975
Iteration 8/25 | Loss: 0.00097975
Iteration 9/25 | Loss: 0.00097975
Iteration 10/25 | Loss: 0.00097975
Iteration 11/25 | Loss: 0.00097975
Iteration 12/25 | Loss: 0.00097975
Iteration 13/25 | Loss: 0.00097975
Iteration 14/25 | Loss: 0.00097975
Iteration 15/25 | Loss: 0.00097975
Iteration 16/25 | Loss: 0.00097975
Iteration 17/25 | Loss: 0.00097975
Iteration 18/25 | Loss: 0.00097975
Iteration 19/25 | Loss: 0.00097975
Iteration 20/25 | Loss: 0.00097975
Iteration 21/25 | Loss: 0.00097975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009797472739592195, 0.0009797472739592195, 0.0009797472739592195, 0.0009797472739592195, 0.0009797472739592195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009797472739592195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097975
Iteration 2/1000 | Loss: 0.00003247
Iteration 3/1000 | Loss: 0.00002403
Iteration 4/1000 | Loss: 0.00002133
Iteration 5/1000 | Loss: 0.00002037
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001804
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001741
Iteration 16/1000 | Loss: 0.00001728
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001725
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001718
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001690
Iteration 40/1000 | Loss: 0.00001690
Iteration 41/1000 | Loss: 0.00001689
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001689
Iteration 44/1000 | Loss: 0.00001689
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001686
Iteration 51/1000 | Loss: 0.00001686
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001684
Iteration 56/1000 | Loss: 0.00001684
Iteration 57/1000 | Loss: 0.00001683
Iteration 58/1000 | Loss: 0.00001683
Iteration 59/1000 | Loss: 0.00001680
Iteration 60/1000 | Loss: 0.00001680
Iteration 61/1000 | Loss: 0.00001680
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001679
Iteration 65/1000 | Loss: 0.00001679
Iteration 66/1000 | Loss: 0.00001679
Iteration 67/1000 | Loss: 0.00001679
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001678
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001676
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001673
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001672
Iteration 93/1000 | Loss: 0.00001672
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001671
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001671
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001669
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001668
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001667
Iteration 130/1000 | Loss: 0.00001667
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001666
Iteration 135/1000 | Loss: 0.00001666
Iteration 136/1000 | Loss: 0.00001666
Iteration 137/1000 | Loss: 0.00001666
Iteration 138/1000 | Loss: 0.00001666
Iteration 139/1000 | Loss: 0.00001666
Iteration 140/1000 | Loss: 0.00001666
Iteration 141/1000 | Loss: 0.00001666
Iteration 142/1000 | Loss: 0.00001666
Iteration 143/1000 | Loss: 0.00001666
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001665
Iteration 149/1000 | Loss: 0.00001665
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001665
Iteration 153/1000 | Loss: 0.00001665
Iteration 154/1000 | Loss: 0.00001664
Iteration 155/1000 | Loss: 0.00001664
Iteration 156/1000 | Loss: 0.00001664
Iteration 157/1000 | Loss: 0.00001664
Iteration 158/1000 | Loss: 0.00001664
Iteration 159/1000 | Loss: 0.00001664
Iteration 160/1000 | Loss: 0.00001664
Iteration 161/1000 | Loss: 0.00001663
Iteration 162/1000 | Loss: 0.00001663
Iteration 163/1000 | Loss: 0.00001663
Iteration 164/1000 | Loss: 0.00001663
Iteration 165/1000 | Loss: 0.00001663
Iteration 166/1000 | Loss: 0.00001663
Iteration 167/1000 | Loss: 0.00001663
Iteration 168/1000 | Loss: 0.00001663
Iteration 169/1000 | Loss: 0.00001663
Iteration 170/1000 | Loss: 0.00001663
Iteration 171/1000 | Loss: 0.00001663
Iteration 172/1000 | Loss: 0.00001663
Iteration 173/1000 | Loss: 0.00001663
Iteration 174/1000 | Loss: 0.00001663
Iteration 175/1000 | Loss: 0.00001663
Iteration 176/1000 | Loss: 0.00001663
Iteration 177/1000 | Loss: 0.00001663
Iteration 178/1000 | Loss: 0.00001663
Iteration 179/1000 | Loss: 0.00001663
Iteration 180/1000 | Loss: 0.00001663
Iteration 181/1000 | Loss: 0.00001663
Iteration 182/1000 | Loss: 0.00001663
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001663
Iteration 187/1000 | Loss: 0.00001663
Iteration 188/1000 | Loss: 0.00001663
Iteration 189/1000 | Loss: 0.00001663
Iteration 190/1000 | Loss: 0.00001663
Iteration 191/1000 | Loss: 0.00001663
Iteration 192/1000 | Loss: 0.00001663
Iteration 193/1000 | Loss: 0.00001663
Iteration 194/1000 | Loss: 0.00001663
Iteration 195/1000 | Loss: 0.00001663
Iteration 196/1000 | Loss: 0.00001663
Iteration 197/1000 | Loss: 0.00001663
Iteration 198/1000 | Loss: 0.00001663
Iteration 199/1000 | Loss: 0.00001663
Iteration 200/1000 | Loss: 0.00001663
Iteration 201/1000 | Loss: 0.00001663
Iteration 202/1000 | Loss: 0.00001663
Iteration 203/1000 | Loss: 0.00001663
Iteration 204/1000 | Loss: 0.00001663
Iteration 205/1000 | Loss: 0.00001663
Iteration 206/1000 | Loss: 0.00001663
Iteration 207/1000 | Loss: 0.00001663
Iteration 208/1000 | Loss: 0.00001663
Iteration 209/1000 | Loss: 0.00001663
Iteration 210/1000 | Loss: 0.00001663
Iteration 211/1000 | Loss: 0.00001663
Iteration 212/1000 | Loss: 0.00001663
Iteration 213/1000 | Loss: 0.00001663
Iteration 214/1000 | Loss: 0.00001663
Iteration 215/1000 | Loss: 0.00001663
Iteration 216/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.662924660195131e-05, 1.662924660195131e-05, 1.662924660195131e-05, 1.662924660195131e-05, 1.662924660195131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.662924660195131e-05

Optimization complete. Final v2v error: 3.4788753986358643 mm

Highest mean error: 3.671121120452881 mm for frame 114

Lowest mean error: 3.2902920246124268 mm for frame 40

Saving results

Total time: 41.15229868888855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490341
Iteration 2/25 | Loss: 0.00135355
Iteration 3/25 | Loss: 0.00125771
Iteration 4/25 | Loss: 0.00124268
Iteration 5/25 | Loss: 0.00123796
Iteration 6/25 | Loss: 0.00123768
Iteration 7/25 | Loss: 0.00123768
Iteration 8/25 | Loss: 0.00123768
Iteration 9/25 | Loss: 0.00123768
Iteration 10/25 | Loss: 0.00123768
Iteration 11/25 | Loss: 0.00123768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012376788072288036, 0.0012376788072288036, 0.0012376788072288036, 0.0012376788072288036, 0.0012376788072288036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012376788072288036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32826447
Iteration 2/25 | Loss: 0.00094972
Iteration 3/25 | Loss: 0.00094970
Iteration 4/25 | Loss: 0.00094970
Iteration 5/25 | Loss: 0.00094970
Iteration 6/25 | Loss: 0.00094970
Iteration 7/25 | Loss: 0.00094970
Iteration 8/25 | Loss: 0.00094970
Iteration 9/25 | Loss: 0.00094970
Iteration 10/25 | Loss: 0.00094970
Iteration 11/25 | Loss: 0.00094970
Iteration 12/25 | Loss: 0.00094970
Iteration 13/25 | Loss: 0.00094970
Iteration 14/25 | Loss: 0.00094970
Iteration 15/25 | Loss: 0.00094970
Iteration 16/25 | Loss: 0.00094970
Iteration 17/25 | Loss: 0.00094970
Iteration 18/25 | Loss: 0.00094970
Iteration 19/25 | Loss: 0.00094970
Iteration 20/25 | Loss: 0.00094970
Iteration 21/25 | Loss: 0.00094970
Iteration 22/25 | Loss: 0.00094970
Iteration 23/25 | Loss: 0.00094970
Iteration 24/25 | Loss: 0.00094970
Iteration 25/25 | Loss: 0.00094970

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094970
Iteration 2/1000 | Loss: 0.00003527
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002144
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001837
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001745
Iteration 10/1000 | Loss: 0.00001722
Iteration 11/1000 | Loss: 0.00001694
Iteration 12/1000 | Loss: 0.00001673
Iteration 13/1000 | Loss: 0.00001670
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001628
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001624
Iteration 21/1000 | Loss: 0.00001624
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001622
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001611
Iteration 32/1000 | Loss: 0.00001604
Iteration 33/1000 | Loss: 0.00001603
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001601
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001600
Iteration 39/1000 | Loss: 0.00001600
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001599
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001597
Iteration 48/1000 | Loss: 0.00001594
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001593
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001589
Iteration 61/1000 | Loss: 0.00001589
Iteration 62/1000 | Loss: 0.00001589
Iteration 63/1000 | Loss: 0.00001589
Iteration 64/1000 | Loss: 0.00001589
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001588
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001583
Iteration 82/1000 | Loss: 0.00001583
Iteration 83/1000 | Loss: 0.00001583
Iteration 84/1000 | Loss: 0.00001583
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001578
Iteration 94/1000 | Loss: 0.00001577
Iteration 95/1000 | Loss: 0.00001577
Iteration 96/1000 | Loss: 0.00001577
Iteration 97/1000 | Loss: 0.00001577
Iteration 98/1000 | Loss: 0.00001577
Iteration 99/1000 | Loss: 0.00001576
Iteration 100/1000 | Loss: 0.00001576
Iteration 101/1000 | Loss: 0.00001576
Iteration 102/1000 | Loss: 0.00001576
Iteration 103/1000 | Loss: 0.00001576
Iteration 104/1000 | Loss: 0.00001576
Iteration 105/1000 | Loss: 0.00001575
Iteration 106/1000 | Loss: 0.00001575
Iteration 107/1000 | Loss: 0.00001575
Iteration 108/1000 | Loss: 0.00001575
Iteration 109/1000 | Loss: 0.00001575
Iteration 110/1000 | Loss: 0.00001575
Iteration 111/1000 | Loss: 0.00001574
Iteration 112/1000 | Loss: 0.00001574
Iteration 113/1000 | Loss: 0.00001573
Iteration 114/1000 | Loss: 0.00001573
Iteration 115/1000 | Loss: 0.00001573
Iteration 116/1000 | Loss: 0.00001573
Iteration 117/1000 | Loss: 0.00001572
Iteration 118/1000 | Loss: 0.00001572
Iteration 119/1000 | Loss: 0.00001572
Iteration 120/1000 | Loss: 0.00001571
Iteration 121/1000 | Loss: 0.00001571
Iteration 122/1000 | Loss: 0.00001571
Iteration 123/1000 | Loss: 0.00001571
Iteration 124/1000 | Loss: 0.00001571
Iteration 125/1000 | Loss: 0.00001571
Iteration 126/1000 | Loss: 0.00001571
Iteration 127/1000 | Loss: 0.00001571
Iteration 128/1000 | Loss: 0.00001571
Iteration 129/1000 | Loss: 0.00001570
Iteration 130/1000 | Loss: 0.00001570
Iteration 131/1000 | Loss: 0.00001570
Iteration 132/1000 | Loss: 0.00001570
Iteration 133/1000 | Loss: 0.00001570
Iteration 134/1000 | Loss: 0.00001570
Iteration 135/1000 | Loss: 0.00001569
Iteration 136/1000 | Loss: 0.00001569
Iteration 137/1000 | Loss: 0.00001569
Iteration 138/1000 | Loss: 0.00001569
Iteration 139/1000 | Loss: 0.00001569
Iteration 140/1000 | Loss: 0.00001569
Iteration 141/1000 | Loss: 0.00001569
Iteration 142/1000 | Loss: 0.00001569
Iteration 143/1000 | Loss: 0.00001569
Iteration 144/1000 | Loss: 0.00001569
Iteration 145/1000 | Loss: 0.00001569
Iteration 146/1000 | Loss: 0.00001568
Iteration 147/1000 | Loss: 0.00001568
Iteration 148/1000 | Loss: 0.00001568
Iteration 149/1000 | Loss: 0.00001568
Iteration 150/1000 | Loss: 0.00001568
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00001568
Iteration 153/1000 | Loss: 0.00001568
Iteration 154/1000 | Loss: 0.00001568
Iteration 155/1000 | Loss: 0.00001568
Iteration 156/1000 | Loss: 0.00001568
Iteration 157/1000 | Loss: 0.00001568
Iteration 158/1000 | Loss: 0.00001568
Iteration 159/1000 | Loss: 0.00001568
Iteration 160/1000 | Loss: 0.00001568
Iteration 161/1000 | Loss: 0.00001568
Iteration 162/1000 | Loss: 0.00001568
Iteration 163/1000 | Loss: 0.00001568
Iteration 164/1000 | Loss: 0.00001568
Iteration 165/1000 | Loss: 0.00001568
Iteration 166/1000 | Loss: 0.00001568
Iteration 167/1000 | Loss: 0.00001568
Iteration 168/1000 | Loss: 0.00001568
Iteration 169/1000 | Loss: 0.00001568
Iteration 170/1000 | Loss: 0.00001568
Iteration 171/1000 | Loss: 0.00001568
Iteration 172/1000 | Loss: 0.00001568
Iteration 173/1000 | Loss: 0.00001568
Iteration 174/1000 | Loss: 0.00001568
Iteration 175/1000 | Loss: 0.00001568
Iteration 176/1000 | Loss: 0.00001568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.56795940711163e-05, 1.56795940711163e-05, 1.56795940711163e-05, 1.56795940711163e-05, 1.56795940711163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.56795940711163e-05

Optimization complete. Final v2v error: 3.354249954223633 mm

Highest mean error: 3.6640429496765137 mm for frame 110

Lowest mean error: 3.067673921585083 mm for frame 18

Saving results

Total time: 42.03347063064575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379356
Iteration 2/25 | Loss: 0.00127768
Iteration 3/25 | Loss: 0.00122537
Iteration 4/25 | Loss: 0.00121985
Iteration 5/25 | Loss: 0.00121687
Iteration 6/25 | Loss: 0.00121687
Iteration 7/25 | Loss: 0.00121687
Iteration 8/25 | Loss: 0.00121687
Iteration 9/25 | Loss: 0.00121687
Iteration 10/25 | Loss: 0.00121687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012168670073151588, 0.0012168670073151588, 0.0012168670073151588, 0.0012168670073151588, 0.0012168670073151588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012168670073151588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49572146
Iteration 2/25 | Loss: 0.00096842
Iteration 3/25 | Loss: 0.00096842
Iteration 4/25 | Loss: 0.00096842
Iteration 5/25 | Loss: 0.00096842
Iteration 6/25 | Loss: 0.00096842
Iteration 7/25 | Loss: 0.00096842
Iteration 8/25 | Loss: 0.00096842
Iteration 9/25 | Loss: 0.00096842
Iteration 10/25 | Loss: 0.00096842
Iteration 11/25 | Loss: 0.00096842
Iteration 12/25 | Loss: 0.00096842
Iteration 13/25 | Loss: 0.00096842
Iteration 14/25 | Loss: 0.00096842
Iteration 15/25 | Loss: 0.00096842
Iteration 16/25 | Loss: 0.00096842
Iteration 17/25 | Loss: 0.00096842
Iteration 18/25 | Loss: 0.00096842
Iteration 19/25 | Loss: 0.00096842
Iteration 20/25 | Loss: 0.00096842
Iteration 21/25 | Loss: 0.00096842
Iteration 22/25 | Loss: 0.00096842
Iteration 23/25 | Loss: 0.00096842
Iteration 24/25 | Loss: 0.00096842
Iteration 25/25 | Loss: 0.00096842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096842
Iteration 2/1000 | Loss: 0.00002430
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001290
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001221
Iteration 12/1000 | Loss: 0.00001208
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001193
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001168
Iteration 76/1000 | Loss: 0.00001168
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001168
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001166
Iteration 88/1000 | Loss: 0.00001166
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001165
Iteration 92/1000 | Loss: 0.00001165
Iteration 93/1000 | Loss: 0.00001165
Iteration 94/1000 | Loss: 0.00001165
Iteration 95/1000 | Loss: 0.00001165
Iteration 96/1000 | Loss: 0.00001165
Iteration 97/1000 | Loss: 0.00001165
Iteration 98/1000 | Loss: 0.00001165
Iteration 99/1000 | Loss: 0.00001165
Iteration 100/1000 | Loss: 0.00001165
Iteration 101/1000 | Loss: 0.00001165
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001165
Iteration 106/1000 | Loss: 0.00001165
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.1648080544546247e-05, 1.1648080544546247e-05, 1.1648080544546247e-05, 1.1648080544546247e-05, 1.1648080544546247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1648080544546247e-05

Optimization complete. Final v2v error: 2.92706561088562 mm

Highest mean error: 3.341271162033081 mm for frame 193

Lowest mean error: 2.596296787261963 mm for frame 219

Saving results

Total time: 37.86367201805115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508195
Iteration 2/25 | Loss: 0.00134539
Iteration 3/25 | Loss: 0.00125860
Iteration 4/25 | Loss: 0.00125088
Iteration 5/25 | Loss: 0.00124970
Iteration 6/25 | Loss: 0.00124970
Iteration 7/25 | Loss: 0.00124970
Iteration 8/25 | Loss: 0.00124970
Iteration 9/25 | Loss: 0.00124970
Iteration 10/25 | Loss: 0.00124970
Iteration 11/25 | Loss: 0.00124970
Iteration 12/25 | Loss: 0.00124970
Iteration 13/25 | Loss: 0.00124970
Iteration 14/25 | Loss: 0.00124970
Iteration 15/25 | Loss: 0.00124970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012497041607275605, 0.0012497041607275605, 0.0012497041607275605, 0.0012497041607275605, 0.0012497041607275605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012497041607275605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32402670
Iteration 2/25 | Loss: 0.00098586
Iteration 3/25 | Loss: 0.00098583
Iteration 4/25 | Loss: 0.00098583
Iteration 5/25 | Loss: 0.00098582
Iteration 6/25 | Loss: 0.00098582
Iteration 7/25 | Loss: 0.00098582
Iteration 8/25 | Loss: 0.00098582
Iteration 9/25 | Loss: 0.00098582
Iteration 10/25 | Loss: 0.00098582
Iteration 11/25 | Loss: 0.00098582
Iteration 12/25 | Loss: 0.00098582
Iteration 13/25 | Loss: 0.00098582
Iteration 14/25 | Loss: 0.00098582
Iteration 15/25 | Loss: 0.00098582
Iteration 16/25 | Loss: 0.00098582
Iteration 17/25 | Loss: 0.00098582
Iteration 18/25 | Loss: 0.00098582
Iteration 19/25 | Loss: 0.00098582
Iteration 20/25 | Loss: 0.00098582
Iteration 21/25 | Loss: 0.00098582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009858228731900454, 0.0009858228731900454, 0.0009858228731900454, 0.0009858228731900454, 0.0009858228731900454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009858228731900454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098582
Iteration 2/1000 | Loss: 0.00002732
Iteration 3/1000 | Loss: 0.00001869
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001424
Iteration 7/1000 | Loss: 0.00001386
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001304
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001271
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001271
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001269
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001268
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001267
Iteration 51/1000 | Loss: 0.00001267
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001260
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001260
Iteration 92/1000 | Loss: 0.00001260
Iteration 93/1000 | Loss: 0.00001260
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001259
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001255
Iteration 128/1000 | Loss: 0.00001255
Iteration 129/1000 | Loss: 0.00001255
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.2552953194244765e-05, 1.2552953194244765e-05, 1.2552953194244765e-05, 1.2552953194244765e-05, 1.2552953194244765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2552953194244765e-05

Optimization complete. Final v2v error: 3.039048433303833 mm

Highest mean error: 3.229979991912842 mm for frame 138

Lowest mean error: 2.915479898452759 mm for frame 220

Saving results

Total time: 38.25007677078247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077947
Iteration 2/25 | Loss: 0.00235632
Iteration 3/25 | Loss: 0.00164993
Iteration 4/25 | Loss: 0.00153161
Iteration 5/25 | Loss: 0.00150150
Iteration 6/25 | Loss: 0.00148741
Iteration 7/25 | Loss: 0.00147436
Iteration 8/25 | Loss: 0.00146582
Iteration 9/25 | Loss: 0.00146122
Iteration 10/25 | Loss: 0.00145937
Iteration 11/25 | Loss: 0.00145887
Iteration 12/25 | Loss: 0.00145828
Iteration 13/25 | Loss: 0.00145804
Iteration 14/25 | Loss: 0.00145701
Iteration 15/25 | Loss: 0.00145687
Iteration 16/25 | Loss: 0.00145686
Iteration 17/25 | Loss: 0.00145686
Iteration 18/25 | Loss: 0.00145685
Iteration 19/25 | Loss: 0.00145685
Iteration 20/25 | Loss: 0.00145685
Iteration 21/25 | Loss: 0.00145685
Iteration 22/25 | Loss: 0.00145685
Iteration 23/25 | Loss: 0.00145685
Iteration 24/25 | Loss: 0.00145685
Iteration 25/25 | Loss: 0.00145685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.59767532
Iteration 2/25 | Loss: 0.00167859
Iteration 3/25 | Loss: 0.00167845
Iteration 4/25 | Loss: 0.00167845
Iteration 5/25 | Loss: 0.00167845
Iteration 6/25 | Loss: 0.00167845
Iteration 7/25 | Loss: 0.00167845
Iteration 8/25 | Loss: 0.00167845
Iteration 9/25 | Loss: 0.00167845
Iteration 10/25 | Loss: 0.00167845
Iteration 11/25 | Loss: 0.00167845
Iteration 12/25 | Loss: 0.00167845
Iteration 13/25 | Loss: 0.00167845
Iteration 14/25 | Loss: 0.00167845
Iteration 15/25 | Loss: 0.00167845
Iteration 16/25 | Loss: 0.00167845
Iteration 17/25 | Loss: 0.00167845
Iteration 18/25 | Loss: 0.00167845
Iteration 19/25 | Loss: 0.00167845
Iteration 20/25 | Loss: 0.00167845
Iteration 21/25 | Loss: 0.00167845
Iteration 22/25 | Loss: 0.00167845
Iteration 23/25 | Loss: 0.00167845
Iteration 24/25 | Loss: 0.00167845
Iteration 25/25 | Loss: 0.00167845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167845
Iteration 2/1000 | Loss: 0.00017831
Iteration 3/1000 | Loss: 0.00017652
Iteration 4/1000 | Loss: 0.00017131
Iteration 5/1000 | Loss: 0.00012073
Iteration 6/1000 | Loss: 0.00018135
Iteration 7/1000 | Loss: 0.00020241
Iteration 8/1000 | Loss: 0.00008065
Iteration 9/1000 | Loss: 0.00005740
Iteration 10/1000 | Loss: 0.00005908
Iteration 11/1000 | Loss: 0.00004850
Iteration 12/1000 | Loss: 0.00005396
Iteration 13/1000 | Loss: 0.00006647
Iteration 14/1000 | Loss: 0.00005102
Iteration 15/1000 | Loss: 0.00006751
Iteration 16/1000 | Loss: 0.00005041
Iteration 17/1000 | Loss: 0.00004798
Iteration 18/1000 | Loss: 0.00004584
Iteration 19/1000 | Loss: 0.00004403
Iteration 20/1000 | Loss: 0.00005872
Iteration 21/1000 | Loss: 0.00007766
Iteration 22/1000 | Loss: 0.00004698
Iteration 23/1000 | Loss: 0.00005647
Iteration 24/1000 | Loss: 0.00006307
Iteration 25/1000 | Loss: 0.00004817
Iteration 26/1000 | Loss: 0.00004650
Iteration 27/1000 | Loss: 0.00005136
Iteration 28/1000 | Loss: 0.00005491
Iteration 29/1000 | Loss: 0.00005365
Iteration 30/1000 | Loss: 0.00005479
Iteration 31/1000 | Loss: 0.00006823
Iteration 32/1000 | Loss: 0.00005803
Iteration 33/1000 | Loss: 0.00010504
Iteration 34/1000 | Loss: 0.00006370
Iteration 35/1000 | Loss: 0.00005780
Iteration 36/1000 | Loss: 0.00005208
Iteration 37/1000 | Loss: 0.00011651
Iteration 38/1000 | Loss: 0.00005256
Iteration 39/1000 | Loss: 0.00005725
Iteration 40/1000 | Loss: 0.00005550
Iteration 41/1000 | Loss: 0.00005921
Iteration 42/1000 | Loss: 0.00005477
Iteration 43/1000 | Loss: 0.00006116
Iteration 44/1000 | Loss: 0.00006910
Iteration 45/1000 | Loss: 0.00009266
Iteration 46/1000 | Loss: 0.00012085
Iteration 47/1000 | Loss: 0.00006545
Iteration 48/1000 | Loss: 0.00007663
Iteration 49/1000 | Loss: 0.00006252
Iteration 50/1000 | Loss: 0.00004554
Iteration 51/1000 | Loss: 0.00006631
Iteration 52/1000 | Loss: 0.00006489
Iteration 53/1000 | Loss: 0.00006299
Iteration 54/1000 | Loss: 0.00005795
Iteration 55/1000 | Loss: 0.00005131
Iteration 56/1000 | Loss: 0.00005872
Iteration 57/1000 | Loss: 0.00005504
Iteration 58/1000 | Loss: 0.00005529
Iteration 59/1000 | Loss: 0.00005797
Iteration 60/1000 | Loss: 0.00006869
Iteration 61/1000 | Loss: 0.00006771
Iteration 62/1000 | Loss: 0.00006399
Iteration 63/1000 | Loss: 0.00006395
Iteration 64/1000 | Loss: 0.00006822
Iteration 65/1000 | Loss: 0.00004813
Iteration 66/1000 | Loss: 0.00007213
Iteration 67/1000 | Loss: 0.00006436
Iteration 68/1000 | Loss: 0.00006446
Iteration 69/1000 | Loss: 0.00004440
Iteration 70/1000 | Loss: 0.00004807
Iteration 71/1000 | Loss: 0.00004297
Iteration 72/1000 | Loss: 0.00005066
Iteration 73/1000 | Loss: 0.00004271
Iteration 74/1000 | Loss: 0.00004713
Iteration 75/1000 | Loss: 0.00004370
Iteration 76/1000 | Loss: 0.00004541
Iteration 77/1000 | Loss: 0.00004321
Iteration 78/1000 | Loss: 0.00004416
Iteration 79/1000 | Loss: 0.00004298
Iteration 80/1000 | Loss: 0.00004482
Iteration 81/1000 | Loss: 0.00004308
Iteration 82/1000 | Loss: 0.00004440
Iteration 83/1000 | Loss: 0.00004286
Iteration 84/1000 | Loss: 0.00004406
Iteration 85/1000 | Loss: 0.00004286
Iteration 86/1000 | Loss: 0.00004392
Iteration 87/1000 | Loss: 0.00004224
Iteration 88/1000 | Loss: 0.00004317
Iteration 89/1000 | Loss: 0.00004490
Iteration 90/1000 | Loss: 0.00004944
Iteration 91/1000 | Loss: 0.00005490
Iteration 92/1000 | Loss: 0.00005060
Iteration 93/1000 | Loss: 0.00005839
Iteration 94/1000 | Loss: 0.00005646
Iteration 95/1000 | Loss: 0.00005201
Iteration 96/1000 | Loss: 0.00004889
Iteration 97/1000 | Loss: 0.00006638
Iteration 98/1000 | Loss: 0.00006275
Iteration 99/1000 | Loss: 0.00006298
Iteration 100/1000 | Loss: 0.00005899
Iteration 101/1000 | Loss: 0.00004163
Iteration 102/1000 | Loss: 0.00006679
Iteration 103/1000 | Loss: 0.00006224
Iteration 104/1000 | Loss: 0.00006428
Iteration 105/1000 | Loss: 0.00006312
Iteration 106/1000 | Loss: 0.00006324
Iteration 107/1000 | Loss: 0.00006237
Iteration 108/1000 | Loss: 0.00004294
Iteration 109/1000 | Loss: 0.00004644
Iteration 110/1000 | Loss: 0.00005911
Iteration 111/1000 | Loss: 0.00005169
Iteration 112/1000 | Loss: 0.00004508
Iteration 113/1000 | Loss: 0.00004314
Iteration 114/1000 | Loss: 0.00005167
Iteration 115/1000 | Loss: 0.00004148
Iteration 116/1000 | Loss: 0.00005640
Iteration 117/1000 | Loss: 0.00005022
Iteration 118/1000 | Loss: 0.00004188
Iteration 119/1000 | Loss: 0.00005568
Iteration 120/1000 | Loss: 0.00005731
Iteration 121/1000 | Loss: 0.00004049
Iteration 122/1000 | Loss: 0.00003947
Iteration 123/1000 | Loss: 0.00003881
Iteration 124/1000 | Loss: 0.00003838
Iteration 125/1000 | Loss: 0.00003811
Iteration 126/1000 | Loss: 0.00003795
Iteration 127/1000 | Loss: 0.00003788
Iteration 128/1000 | Loss: 0.00003788
Iteration 129/1000 | Loss: 0.00003788
Iteration 130/1000 | Loss: 0.00003788
Iteration 131/1000 | Loss: 0.00003788
Iteration 132/1000 | Loss: 0.00003788
Iteration 133/1000 | Loss: 0.00003788
Iteration 134/1000 | Loss: 0.00003788
Iteration 135/1000 | Loss: 0.00003787
Iteration 136/1000 | Loss: 0.00003787
Iteration 137/1000 | Loss: 0.00003787
Iteration 138/1000 | Loss: 0.00003787
Iteration 139/1000 | Loss: 0.00003786
Iteration 140/1000 | Loss: 0.00003786
Iteration 141/1000 | Loss: 0.00003785
Iteration 142/1000 | Loss: 0.00003785
Iteration 143/1000 | Loss: 0.00003785
Iteration 144/1000 | Loss: 0.00003785
Iteration 145/1000 | Loss: 0.00003785
Iteration 146/1000 | Loss: 0.00003785
Iteration 147/1000 | Loss: 0.00003784
Iteration 148/1000 | Loss: 0.00003784
Iteration 149/1000 | Loss: 0.00003784
Iteration 150/1000 | Loss: 0.00003783
Iteration 151/1000 | Loss: 0.00003783
Iteration 152/1000 | Loss: 0.00003783
Iteration 153/1000 | Loss: 0.00003783
Iteration 154/1000 | Loss: 0.00003783
Iteration 155/1000 | Loss: 0.00003783
Iteration 156/1000 | Loss: 0.00003783
Iteration 157/1000 | Loss: 0.00003783
Iteration 158/1000 | Loss: 0.00003783
Iteration 159/1000 | Loss: 0.00003783
Iteration 160/1000 | Loss: 0.00003783
Iteration 161/1000 | Loss: 0.00003783
Iteration 162/1000 | Loss: 0.00003782
Iteration 163/1000 | Loss: 0.00003782
Iteration 164/1000 | Loss: 0.00003782
Iteration 165/1000 | Loss: 0.00003781
Iteration 166/1000 | Loss: 0.00003781
Iteration 167/1000 | Loss: 0.00003781
Iteration 168/1000 | Loss: 0.00003780
Iteration 169/1000 | Loss: 0.00003780
Iteration 170/1000 | Loss: 0.00003780
Iteration 171/1000 | Loss: 0.00003780
Iteration 172/1000 | Loss: 0.00003779
Iteration 173/1000 | Loss: 0.00003779
Iteration 174/1000 | Loss: 0.00003779
Iteration 175/1000 | Loss: 0.00003779
Iteration 176/1000 | Loss: 0.00003779
Iteration 177/1000 | Loss: 0.00003779
Iteration 178/1000 | Loss: 0.00003779
Iteration 179/1000 | Loss: 0.00003779
Iteration 180/1000 | Loss: 0.00003778
Iteration 181/1000 | Loss: 0.00003778
Iteration 182/1000 | Loss: 0.00003778
Iteration 183/1000 | Loss: 0.00003778
Iteration 184/1000 | Loss: 0.00003778
Iteration 185/1000 | Loss: 0.00003778
Iteration 186/1000 | Loss: 0.00003778
Iteration 187/1000 | Loss: 0.00003778
Iteration 188/1000 | Loss: 0.00003778
Iteration 189/1000 | Loss: 0.00003778
Iteration 190/1000 | Loss: 0.00003778
Iteration 191/1000 | Loss: 0.00003778
Iteration 192/1000 | Loss: 0.00003778
Iteration 193/1000 | Loss: 0.00003777
Iteration 194/1000 | Loss: 0.00003777
Iteration 195/1000 | Loss: 0.00003777
Iteration 196/1000 | Loss: 0.00003777
Iteration 197/1000 | Loss: 0.00003776
Iteration 198/1000 | Loss: 0.00003776
Iteration 199/1000 | Loss: 0.00003776
Iteration 200/1000 | Loss: 0.00003776
Iteration 201/1000 | Loss: 0.00003776
Iteration 202/1000 | Loss: 0.00003776
Iteration 203/1000 | Loss: 0.00003776
Iteration 204/1000 | Loss: 0.00003776
Iteration 205/1000 | Loss: 0.00003776
Iteration 206/1000 | Loss: 0.00003775
Iteration 207/1000 | Loss: 0.00003775
Iteration 208/1000 | Loss: 0.00003775
Iteration 209/1000 | Loss: 0.00003775
Iteration 210/1000 | Loss: 0.00003775
Iteration 211/1000 | Loss: 0.00003775
Iteration 212/1000 | Loss: 0.00003774
Iteration 213/1000 | Loss: 0.00003774
Iteration 214/1000 | Loss: 0.00003774
Iteration 215/1000 | Loss: 0.00003774
Iteration 216/1000 | Loss: 0.00003774
Iteration 217/1000 | Loss: 0.00003774
Iteration 218/1000 | Loss: 0.00003773
Iteration 219/1000 | Loss: 0.00003773
Iteration 220/1000 | Loss: 0.00003773
Iteration 221/1000 | Loss: 0.00003773
Iteration 222/1000 | Loss: 0.00003773
Iteration 223/1000 | Loss: 0.00003773
Iteration 224/1000 | Loss: 0.00003773
Iteration 225/1000 | Loss: 0.00003773
Iteration 226/1000 | Loss: 0.00003773
Iteration 227/1000 | Loss: 0.00003773
Iteration 228/1000 | Loss: 0.00003773
Iteration 229/1000 | Loss: 0.00003773
Iteration 230/1000 | Loss: 0.00003773
Iteration 231/1000 | Loss: 0.00003773
Iteration 232/1000 | Loss: 0.00003773
Iteration 233/1000 | Loss: 0.00003773
Iteration 234/1000 | Loss: 0.00003773
Iteration 235/1000 | Loss: 0.00003773
Iteration 236/1000 | Loss: 0.00003773
Iteration 237/1000 | Loss: 0.00003773
Iteration 238/1000 | Loss: 0.00003773
Iteration 239/1000 | Loss: 0.00003773
Iteration 240/1000 | Loss: 0.00003773
Iteration 241/1000 | Loss: 0.00003773
Iteration 242/1000 | Loss: 0.00003773
Iteration 243/1000 | Loss: 0.00003773
Iteration 244/1000 | Loss: 0.00003773
Iteration 245/1000 | Loss: 0.00003773
Iteration 246/1000 | Loss: 0.00003773
Iteration 247/1000 | Loss: 0.00003773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [3.772702257265337e-05, 3.772702257265337e-05, 3.772702257265337e-05, 3.772702257265337e-05, 3.772702257265337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.772702257265337e-05

Optimization complete. Final v2v error: 4.990901947021484 mm

Highest mean error: 6.24088716506958 mm for frame 207

Lowest mean error: 3.835709571838379 mm for frame 21

Saving results

Total time: 241.69542288780212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475960
Iteration 2/25 | Loss: 0.00128619
Iteration 3/25 | Loss: 0.00122551
Iteration 4/25 | Loss: 0.00121352
Iteration 5/25 | Loss: 0.00120934
Iteration 6/25 | Loss: 0.00120851
Iteration 7/25 | Loss: 0.00120851
Iteration 8/25 | Loss: 0.00120851
Iteration 9/25 | Loss: 0.00120851
Iteration 10/25 | Loss: 0.00120851
Iteration 11/25 | Loss: 0.00120851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012085147900506854, 0.0012085147900506854, 0.0012085147900506854, 0.0012085147900506854, 0.0012085147900506854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012085147900506854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.29397964
Iteration 2/25 | Loss: 0.00095788
Iteration 3/25 | Loss: 0.00095788
Iteration 4/25 | Loss: 0.00095788
Iteration 5/25 | Loss: 0.00095788
Iteration 6/25 | Loss: 0.00095788
Iteration 7/25 | Loss: 0.00095788
Iteration 8/25 | Loss: 0.00095788
Iteration 9/25 | Loss: 0.00095788
Iteration 10/25 | Loss: 0.00095788
Iteration 11/25 | Loss: 0.00095788
Iteration 12/25 | Loss: 0.00095788
Iteration 13/25 | Loss: 0.00095788
Iteration 14/25 | Loss: 0.00095788
Iteration 15/25 | Loss: 0.00095788
Iteration 16/25 | Loss: 0.00095788
Iteration 17/25 | Loss: 0.00095788
Iteration 18/25 | Loss: 0.00095788
Iteration 19/25 | Loss: 0.00095788
Iteration 20/25 | Loss: 0.00095788
Iteration 21/25 | Loss: 0.00095788
Iteration 22/25 | Loss: 0.00095788
Iteration 23/25 | Loss: 0.00095788
Iteration 24/25 | Loss: 0.00095788
Iteration 25/25 | Loss: 0.00095788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095788
Iteration 2/1000 | Loss: 0.00003141
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001555
Iteration 6/1000 | Loss: 0.00001484
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001389
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001322
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001317
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001281
Iteration 18/1000 | Loss: 0.00001279
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001278
Iteration 21/1000 | Loss: 0.00001277
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001245
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001238
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001226
Iteration 142/1000 | Loss: 0.00001226
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001226
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Iteration 147/1000 | Loss: 0.00001226
Iteration 148/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2257600246812217e-05, 1.2257600246812217e-05, 1.2257600246812217e-05, 1.2257600246812217e-05, 1.2257600246812217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2257600246812217e-05

Optimization complete. Final v2v error: 3.035299301147461 mm

Highest mean error: 3.3390753269195557 mm for frame 109

Lowest mean error: 2.8472816944122314 mm for frame 1

Saving results

Total time: 38.533729553222656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442689
Iteration 2/25 | Loss: 0.00130700
Iteration 3/25 | Loss: 0.00124246
Iteration 4/25 | Loss: 0.00123258
Iteration 5/25 | Loss: 0.00122895
Iteration 6/25 | Loss: 0.00122827
Iteration 7/25 | Loss: 0.00122827
Iteration 8/25 | Loss: 0.00122827
Iteration 9/25 | Loss: 0.00122827
Iteration 10/25 | Loss: 0.00122827
Iteration 11/25 | Loss: 0.00122827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001228267326951027, 0.001228267326951027, 0.001228267326951027, 0.001228267326951027, 0.001228267326951027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001228267326951027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36763608
Iteration 2/25 | Loss: 0.00127347
Iteration 3/25 | Loss: 0.00127347
Iteration 4/25 | Loss: 0.00127347
Iteration 5/25 | Loss: 0.00127347
Iteration 6/25 | Loss: 0.00127347
Iteration 7/25 | Loss: 0.00127347
Iteration 8/25 | Loss: 0.00127347
Iteration 9/25 | Loss: 0.00127347
Iteration 10/25 | Loss: 0.00127347
Iteration 11/25 | Loss: 0.00127347
Iteration 12/25 | Loss: 0.00127347
Iteration 13/25 | Loss: 0.00127347
Iteration 14/25 | Loss: 0.00127347
Iteration 15/25 | Loss: 0.00127347
Iteration 16/25 | Loss: 0.00127347
Iteration 17/25 | Loss: 0.00127347
Iteration 18/25 | Loss: 0.00127347
Iteration 19/25 | Loss: 0.00127347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012734659248962998, 0.0012734659248962998, 0.0012734659248962998, 0.0012734659248962998, 0.0012734659248962998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012734659248962998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127347
Iteration 2/1000 | Loss: 0.00003467
Iteration 3/1000 | Loss: 0.00002299
Iteration 4/1000 | Loss: 0.00001809
Iteration 5/1000 | Loss: 0.00001666
Iteration 6/1000 | Loss: 0.00001552
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001323
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001319
Iteration 35/1000 | Loss: 0.00001319
Iteration 36/1000 | Loss: 0.00001319
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001317
Iteration 41/1000 | Loss: 0.00001317
Iteration 42/1000 | Loss: 0.00001317
Iteration 43/1000 | Loss: 0.00001316
Iteration 44/1000 | Loss: 0.00001316
Iteration 45/1000 | Loss: 0.00001316
Iteration 46/1000 | Loss: 0.00001316
Iteration 47/1000 | Loss: 0.00001315
Iteration 48/1000 | Loss: 0.00001315
Iteration 49/1000 | Loss: 0.00001315
Iteration 50/1000 | Loss: 0.00001313
Iteration 51/1000 | Loss: 0.00001313
Iteration 52/1000 | Loss: 0.00001313
Iteration 53/1000 | Loss: 0.00001312
Iteration 54/1000 | Loss: 0.00001312
Iteration 55/1000 | Loss: 0.00001312
Iteration 56/1000 | Loss: 0.00001312
Iteration 57/1000 | Loss: 0.00001312
Iteration 58/1000 | Loss: 0.00001312
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001311
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001310
Iteration 65/1000 | Loss: 0.00001310
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001309
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001309
Iteration 74/1000 | Loss: 0.00001309
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001307
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001307
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001300
Iteration 123/1000 | Loss: 0.00001300
Iteration 124/1000 | Loss: 0.00001300
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001299
Iteration 128/1000 | Loss: 0.00001299
Iteration 129/1000 | Loss: 0.00001299
Iteration 130/1000 | Loss: 0.00001299
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001297
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001296
Iteration 144/1000 | Loss: 0.00001296
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001296
Iteration 147/1000 | Loss: 0.00001296
Iteration 148/1000 | Loss: 0.00001296
Iteration 149/1000 | Loss: 0.00001296
Iteration 150/1000 | Loss: 0.00001296
Iteration 151/1000 | Loss: 0.00001296
Iteration 152/1000 | Loss: 0.00001295
Iteration 153/1000 | Loss: 0.00001295
Iteration 154/1000 | Loss: 0.00001295
Iteration 155/1000 | Loss: 0.00001295
Iteration 156/1000 | Loss: 0.00001295
Iteration 157/1000 | Loss: 0.00001295
Iteration 158/1000 | Loss: 0.00001295
Iteration 159/1000 | Loss: 0.00001295
Iteration 160/1000 | Loss: 0.00001294
Iteration 161/1000 | Loss: 0.00001294
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001294
Iteration 164/1000 | Loss: 0.00001294
Iteration 165/1000 | Loss: 0.00001293
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001293
Iteration 171/1000 | Loss: 0.00001293
Iteration 172/1000 | Loss: 0.00001293
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001292
Iteration 176/1000 | Loss: 0.00001292
Iteration 177/1000 | Loss: 0.00001292
Iteration 178/1000 | Loss: 0.00001292
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001291
Iteration 181/1000 | Loss: 0.00001291
Iteration 182/1000 | Loss: 0.00001291
Iteration 183/1000 | Loss: 0.00001291
Iteration 184/1000 | Loss: 0.00001291
Iteration 185/1000 | Loss: 0.00001291
Iteration 186/1000 | Loss: 0.00001291
Iteration 187/1000 | Loss: 0.00001290
Iteration 188/1000 | Loss: 0.00001290
Iteration 189/1000 | Loss: 0.00001290
Iteration 190/1000 | Loss: 0.00001290
Iteration 191/1000 | Loss: 0.00001290
Iteration 192/1000 | Loss: 0.00001290
Iteration 193/1000 | Loss: 0.00001290
Iteration 194/1000 | Loss: 0.00001290
Iteration 195/1000 | Loss: 0.00001290
Iteration 196/1000 | Loss: 0.00001290
Iteration 197/1000 | Loss: 0.00001290
Iteration 198/1000 | Loss: 0.00001290
Iteration 199/1000 | Loss: 0.00001290
Iteration 200/1000 | Loss: 0.00001290
Iteration 201/1000 | Loss: 0.00001290
Iteration 202/1000 | Loss: 0.00001289
Iteration 203/1000 | Loss: 0.00001289
Iteration 204/1000 | Loss: 0.00001289
Iteration 205/1000 | Loss: 0.00001289
Iteration 206/1000 | Loss: 0.00001289
Iteration 207/1000 | Loss: 0.00001289
Iteration 208/1000 | Loss: 0.00001289
Iteration 209/1000 | Loss: 0.00001289
Iteration 210/1000 | Loss: 0.00001289
Iteration 211/1000 | Loss: 0.00001289
Iteration 212/1000 | Loss: 0.00001289
Iteration 213/1000 | Loss: 0.00001289
Iteration 214/1000 | Loss: 0.00001289
Iteration 215/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.288857947656652e-05, 1.288857947656652e-05, 1.288857947656652e-05, 1.288857947656652e-05, 1.288857947656652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.288857947656652e-05

Optimization complete. Final v2v error: 3.0087430477142334 mm

Highest mean error: 3.5954766273498535 mm for frame 160

Lowest mean error: 2.6273083686828613 mm for frame 168

Saving results

Total time: 41.66748881340027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555470
Iteration 2/25 | Loss: 0.00169277
Iteration 3/25 | Loss: 0.00140295
Iteration 4/25 | Loss: 0.00138247
Iteration 5/25 | Loss: 0.00137956
Iteration 6/25 | Loss: 0.00137909
Iteration 7/25 | Loss: 0.00137909
Iteration 8/25 | Loss: 0.00137909
Iteration 9/25 | Loss: 0.00137909
Iteration 10/25 | Loss: 0.00137909
Iteration 11/25 | Loss: 0.00137909
Iteration 12/25 | Loss: 0.00137909
Iteration 13/25 | Loss: 0.00137909
Iteration 14/25 | Loss: 0.00137909
Iteration 15/25 | Loss: 0.00137909
Iteration 16/25 | Loss: 0.00137909
Iteration 17/25 | Loss: 0.00137909
Iteration 18/25 | Loss: 0.00137909
Iteration 19/25 | Loss: 0.00137909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001379086752422154, 0.001379086752422154, 0.001379086752422154, 0.001379086752422154, 0.001379086752422154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001379086752422154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99036086
Iteration 2/25 | Loss: 0.00112338
Iteration 3/25 | Loss: 0.00112336
Iteration 4/25 | Loss: 0.00112336
Iteration 5/25 | Loss: 0.00112336
Iteration 6/25 | Loss: 0.00112336
Iteration 7/25 | Loss: 0.00112336
Iteration 8/25 | Loss: 0.00112336
Iteration 9/25 | Loss: 0.00112336
Iteration 10/25 | Loss: 0.00112336
Iteration 11/25 | Loss: 0.00112336
Iteration 12/25 | Loss: 0.00112336
Iteration 13/25 | Loss: 0.00112336
Iteration 14/25 | Loss: 0.00112336
Iteration 15/25 | Loss: 0.00112336
Iteration 16/25 | Loss: 0.00112336
Iteration 17/25 | Loss: 0.00112336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011233570985496044, 0.0011233570985496044, 0.0011233570985496044, 0.0011233570985496044, 0.0011233570985496044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011233570985496044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112336
Iteration 2/1000 | Loss: 0.00004716
Iteration 3/1000 | Loss: 0.00003092
Iteration 4/1000 | Loss: 0.00002729
Iteration 5/1000 | Loss: 0.00002600
Iteration 6/1000 | Loss: 0.00002533
Iteration 7/1000 | Loss: 0.00002474
Iteration 8/1000 | Loss: 0.00002423
Iteration 9/1000 | Loss: 0.00002384
Iteration 10/1000 | Loss: 0.00002352
Iteration 11/1000 | Loss: 0.00002323
Iteration 12/1000 | Loss: 0.00002299
Iteration 13/1000 | Loss: 0.00002278
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002235
Iteration 17/1000 | Loss: 0.00002228
Iteration 18/1000 | Loss: 0.00002218
Iteration 19/1000 | Loss: 0.00002209
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002202
Iteration 22/1000 | Loss: 0.00002195
Iteration 23/1000 | Loss: 0.00002195
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002191
Iteration 26/1000 | Loss: 0.00002189
Iteration 27/1000 | Loss: 0.00002189
Iteration 28/1000 | Loss: 0.00002188
Iteration 29/1000 | Loss: 0.00002187
Iteration 30/1000 | Loss: 0.00002187
Iteration 31/1000 | Loss: 0.00002186
Iteration 32/1000 | Loss: 0.00002186
Iteration 33/1000 | Loss: 0.00002186
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00002186
Iteration 37/1000 | Loss: 0.00002186
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002186
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002186
Iteration 42/1000 | Loss: 0.00002186
Iteration 43/1000 | Loss: 0.00002186
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002186
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [2.1855013983440585e-05, 2.1855013983440585e-05, 2.1855013983440585e-05, 2.1855013983440585e-05, 2.1855013983440585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1855013983440585e-05

Optimization complete. Final v2v error: 3.755146026611328 mm

Highest mean error: 4.777750015258789 mm for frame 58

Lowest mean error: 2.927658796310425 mm for frame 137

Saving results

Total time: 38.07607173919678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465491
Iteration 2/25 | Loss: 0.00131844
Iteration 3/25 | Loss: 0.00124207
Iteration 4/25 | Loss: 0.00123256
Iteration 5/25 | Loss: 0.00123089
Iteration 6/25 | Loss: 0.00123089
Iteration 7/25 | Loss: 0.00123089
Iteration 8/25 | Loss: 0.00123089
Iteration 9/25 | Loss: 0.00123089
Iteration 10/25 | Loss: 0.00123089
Iteration 11/25 | Loss: 0.00123089
Iteration 12/25 | Loss: 0.00123089
Iteration 13/25 | Loss: 0.00123089
Iteration 14/25 | Loss: 0.00123089
Iteration 15/25 | Loss: 0.00123089
Iteration 16/25 | Loss: 0.00123089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001230889349244535, 0.001230889349244535, 0.001230889349244535, 0.001230889349244535, 0.001230889349244535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001230889349244535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41435862
Iteration 2/25 | Loss: 0.00094810
Iteration 3/25 | Loss: 0.00094810
Iteration 4/25 | Loss: 0.00094810
Iteration 5/25 | Loss: 0.00094809
Iteration 6/25 | Loss: 0.00094809
Iteration 7/25 | Loss: 0.00094809
Iteration 8/25 | Loss: 0.00094809
Iteration 9/25 | Loss: 0.00094809
Iteration 10/25 | Loss: 0.00094809
Iteration 11/25 | Loss: 0.00094809
Iteration 12/25 | Loss: 0.00094809
Iteration 13/25 | Loss: 0.00094809
Iteration 14/25 | Loss: 0.00094809
Iteration 15/25 | Loss: 0.00094809
Iteration 16/25 | Loss: 0.00094809
Iteration 17/25 | Loss: 0.00094809
Iteration 18/25 | Loss: 0.00094809
Iteration 19/25 | Loss: 0.00094809
Iteration 20/25 | Loss: 0.00094809
Iteration 21/25 | Loss: 0.00094809
Iteration 22/25 | Loss: 0.00094809
Iteration 23/25 | Loss: 0.00094809
Iteration 24/25 | Loss: 0.00094809
Iteration 25/25 | Loss: 0.00094809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094809
Iteration 2/1000 | Loss: 0.00002210
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001489
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001418
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001347
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001343
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001303
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001303
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001295
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001284
Iteration 68/1000 | Loss: 0.00001284
Iteration 69/1000 | Loss: 0.00001284
Iteration 70/1000 | Loss: 0.00001284
Iteration 71/1000 | Loss: 0.00001284
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.2806107406504452e-05, 1.2806107406504452e-05, 1.2806107406504452e-05, 1.2806107406504452e-05, 1.2806107406504452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2806107406504452e-05

Optimization complete. Final v2v error: 3.0753464698791504 mm

Highest mean error: 3.224468946456909 mm for frame 249

Lowest mean error: 2.912863254547119 mm for frame 222

Saving results

Total time: 38.16942548751831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533711
Iteration 2/25 | Loss: 0.00158583
Iteration 3/25 | Loss: 0.00130784
Iteration 4/25 | Loss: 0.00127881
Iteration 5/25 | Loss: 0.00126857
Iteration 6/25 | Loss: 0.00126551
Iteration 7/25 | Loss: 0.00126250
Iteration 8/25 | Loss: 0.00126082
Iteration 9/25 | Loss: 0.00125997
Iteration 10/25 | Loss: 0.00126790
Iteration 11/25 | Loss: 0.00125976
Iteration 12/25 | Loss: 0.00125532
Iteration 13/25 | Loss: 0.00125421
Iteration 14/25 | Loss: 0.00125424
Iteration 15/25 | Loss: 0.00125423
Iteration 16/25 | Loss: 0.00125402
Iteration 17/25 | Loss: 0.00125402
Iteration 18/25 | Loss: 0.00125402
Iteration 19/25 | Loss: 0.00125402
Iteration 20/25 | Loss: 0.00125402
Iteration 21/25 | Loss: 0.00125402
Iteration 22/25 | Loss: 0.00125402
Iteration 23/25 | Loss: 0.00125402
Iteration 24/25 | Loss: 0.00125402
Iteration 25/25 | Loss: 0.00125402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57393897
Iteration 2/25 | Loss: 0.00113870
Iteration 3/25 | Loss: 0.00113870
Iteration 4/25 | Loss: 0.00113870
Iteration 5/25 | Loss: 0.00113869
Iteration 6/25 | Loss: 0.00113869
Iteration 7/25 | Loss: 0.00113869
Iteration 8/25 | Loss: 0.00113869
Iteration 9/25 | Loss: 0.00113869
Iteration 10/25 | Loss: 0.00113869
Iteration 11/25 | Loss: 0.00113869
Iteration 12/25 | Loss: 0.00113869
Iteration 13/25 | Loss: 0.00113869
Iteration 14/25 | Loss: 0.00113869
Iteration 15/25 | Loss: 0.00113869
Iteration 16/25 | Loss: 0.00113869
Iteration 17/25 | Loss: 0.00113869
Iteration 18/25 | Loss: 0.00113869
Iteration 19/25 | Loss: 0.00113869
Iteration 20/25 | Loss: 0.00113869
Iteration 21/25 | Loss: 0.00113869
Iteration 22/25 | Loss: 0.00113869
Iteration 23/25 | Loss: 0.00113869
Iteration 24/25 | Loss: 0.00113869
Iteration 25/25 | Loss: 0.00113869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113869
Iteration 2/1000 | Loss: 0.00003361
Iteration 3/1000 | Loss: 0.00002375
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001793
Iteration 6/1000 | Loss: 0.00001713
Iteration 7/1000 | Loss: 0.00001662
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001567
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001538
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001531
Iteration 16/1000 | Loss: 0.00001530
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00001520
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001516
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001515
Iteration 31/1000 | Loss: 0.00001515
Iteration 32/1000 | Loss: 0.00001514
Iteration 33/1000 | Loss: 0.00001514
Iteration 34/1000 | Loss: 0.00001514
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001513
Iteration 38/1000 | Loss: 0.00001513
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001511
Iteration 43/1000 | Loss: 0.00001510
Iteration 44/1000 | Loss: 0.00001510
Iteration 45/1000 | Loss: 0.00001510
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001508
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001506
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001505
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001505
Iteration 73/1000 | Loss: 0.00001505
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001504
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001504
Iteration 80/1000 | Loss: 0.00001504
Iteration 81/1000 | Loss: 0.00001503
Iteration 82/1000 | Loss: 0.00001503
Iteration 83/1000 | Loss: 0.00001503
Iteration 84/1000 | Loss: 0.00001503
Iteration 85/1000 | Loss: 0.00001503
Iteration 86/1000 | Loss: 0.00001503
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001502
Iteration 92/1000 | Loss: 0.00001502
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001501
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001501
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001500
Iteration 107/1000 | Loss: 0.00001500
Iteration 108/1000 | Loss: 0.00001500
Iteration 109/1000 | Loss: 0.00001500
Iteration 110/1000 | Loss: 0.00001500
Iteration 111/1000 | Loss: 0.00001500
Iteration 112/1000 | Loss: 0.00001500
Iteration 113/1000 | Loss: 0.00001500
Iteration 114/1000 | Loss: 0.00001500
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001499
Iteration 118/1000 | Loss: 0.00001499
Iteration 119/1000 | Loss: 0.00001499
Iteration 120/1000 | Loss: 0.00001499
Iteration 121/1000 | Loss: 0.00001499
Iteration 122/1000 | Loss: 0.00001499
Iteration 123/1000 | Loss: 0.00001499
Iteration 124/1000 | Loss: 0.00001499
Iteration 125/1000 | Loss: 0.00001499
Iteration 126/1000 | Loss: 0.00001499
Iteration 127/1000 | Loss: 0.00001499
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001498
Iteration 137/1000 | Loss: 0.00001498
Iteration 138/1000 | Loss: 0.00001498
Iteration 139/1000 | Loss: 0.00001498
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4975946214690339e-05, 1.4975946214690339e-05, 1.4975946214690339e-05, 1.4975946214690339e-05, 1.4975946214690339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4975946214690339e-05

Optimization complete. Final v2v error: 3.262089729309082 mm

Highest mean error: 3.99715256690979 mm for frame 67

Lowest mean error: 2.990198850631714 mm for frame 239

Saving results

Total time: 61.34779715538025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513343
Iteration 2/25 | Loss: 0.00139560
Iteration 3/25 | Loss: 0.00128341
Iteration 4/25 | Loss: 0.00127792
Iteration 5/25 | Loss: 0.00127783
Iteration 6/25 | Loss: 0.00127783
Iteration 7/25 | Loss: 0.00127783
Iteration 8/25 | Loss: 0.00127783
Iteration 9/25 | Loss: 0.00127783
Iteration 10/25 | Loss: 0.00127783
Iteration 11/25 | Loss: 0.00127783
Iteration 12/25 | Loss: 0.00127783
Iteration 13/25 | Loss: 0.00127783
Iteration 14/25 | Loss: 0.00127783
Iteration 15/25 | Loss: 0.00127783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001277825445868075, 0.001277825445868075, 0.001277825445868075, 0.001277825445868075, 0.001277825445868075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277825445868075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81560045
Iteration 2/25 | Loss: 0.00079627
Iteration 3/25 | Loss: 0.00079627
Iteration 4/25 | Loss: 0.00079626
Iteration 5/25 | Loss: 0.00079626
Iteration 6/25 | Loss: 0.00079626
Iteration 7/25 | Loss: 0.00079626
Iteration 8/25 | Loss: 0.00079626
Iteration 9/25 | Loss: 0.00079626
Iteration 10/25 | Loss: 0.00079626
Iteration 11/25 | Loss: 0.00079626
Iteration 12/25 | Loss: 0.00079626
Iteration 13/25 | Loss: 0.00079626
Iteration 14/25 | Loss: 0.00079626
Iteration 15/25 | Loss: 0.00079626
Iteration 16/25 | Loss: 0.00079626
Iteration 17/25 | Loss: 0.00079626
Iteration 18/25 | Loss: 0.00079626
Iteration 19/25 | Loss: 0.00079626
Iteration 20/25 | Loss: 0.00079626
Iteration 21/25 | Loss: 0.00079626
Iteration 22/25 | Loss: 0.00079626
Iteration 23/25 | Loss: 0.00079626
Iteration 24/25 | Loss: 0.00079626
Iteration 25/25 | Loss: 0.00079626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079626
Iteration 2/1000 | Loss: 0.00002360
Iteration 3/1000 | Loss: 0.00001848
Iteration 4/1000 | Loss: 0.00001709
Iteration 5/1000 | Loss: 0.00001643
Iteration 6/1000 | Loss: 0.00001608
Iteration 7/1000 | Loss: 0.00001578
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001451
Iteration 19/1000 | Loss: 0.00001451
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001435
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001435
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001434
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001432
Iteration 35/1000 | Loss: 0.00001432
Iteration 36/1000 | Loss: 0.00001432
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001421
Iteration 44/1000 | Loss: 0.00001421
Iteration 45/1000 | Loss: 0.00001421
Iteration 46/1000 | Loss: 0.00001420
Iteration 47/1000 | Loss: 0.00001420
Iteration 48/1000 | Loss: 0.00001420
Iteration 49/1000 | Loss: 0.00001419
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001408
Iteration 66/1000 | Loss: 0.00001408
Iteration 67/1000 | Loss: 0.00001408
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001407
Iteration 71/1000 | Loss: 0.00001407
Iteration 72/1000 | Loss: 0.00001407
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001404
Iteration 98/1000 | Loss: 0.00001404
Iteration 99/1000 | Loss: 0.00001404
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001402
Iteration 108/1000 | Loss: 0.00001402
Iteration 109/1000 | Loss: 0.00001402
Iteration 110/1000 | Loss: 0.00001402
Iteration 111/1000 | Loss: 0.00001402
Iteration 112/1000 | Loss: 0.00001402
Iteration 113/1000 | Loss: 0.00001402
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001402
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001402
Iteration 141/1000 | Loss: 0.00001402
Iteration 142/1000 | Loss: 0.00001402
Iteration 143/1000 | Loss: 0.00001402
Iteration 144/1000 | Loss: 0.00001402
Iteration 145/1000 | Loss: 0.00001402
Iteration 146/1000 | Loss: 0.00001402
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.402137513650814e-05, 1.402137513650814e-05, 1.402137513650814e-05, 1.402137513650814e-05, 1.402137513650814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.402137513650814e-05

Optimization complete. Final v2v error: 3.1592984199523926 mm

Highest mean error: 3.206746816635132 mm for frame 3

Lowest mean error: 3.1110360622406006 mm for frame 143

Saving results

Total time: 40.18873405456543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958964
Iteration 2/25 | Loss: 0.00165835
Iteration 3/25 | Loss: 0.00128075
Iteration 4/25 | Loss: 0.00124178
Iteration 5/25 | Loss: 0.00123192
Iteration 6/25 | Loss: 0.00122517
Iteration 7/25 | Loss: 0.00122131
Iteration 8/25 | Loss: 0.00122262
Iteration 9/25 | Loss: 0.00122012
Iteration 10/25 | Loss: 0.00122134
Iteration 11/25 | Loss: 0.00122047
Iteration 12/25 | Loss: 0.00122075
Iteration 13/25 | Loss: 0.00121918
Iteration 14/25 | Loss: 0.00121863
Iteration 15/25 | Loss: 0.00121843
Iteration 16/25 | Loss: 0.00121839
Iteration 17/25 | Loss: 0.00121839
Iteration 18/25 | Loss: 0.00121838
Iteration 19/25 | Loss: 0.00121838
Iteration 20/25 | Loss: 0.00121838
Iteration 21/25 | Loss: 0.00121838
Iteration 22/25 | Loss: 0.00121838
Iteration 23/25 | Loss: 0.00121838
Iteration 24/25 | Loss: 0.00121838
Iteration 25/25 | Loss: 0.00121838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.84832168
Iteration 2/25 | Loss: 0.00111401
Iteration 3/25 | Loss: 0.00111392
Iteration 4/25 | Loss: 0.00111392
Iteration 5/25 | Loss: 0.00111392
Iteration 6/25 | Loss: 0.00111392
Iteration 7/25 | Loss: 0.00111392
Iteration 8/25 | Loss: 0.00111392
Iteration 9/25 | Loss: 0.00111392
Iteration 10/25 | Loss: 0.00111392
Iteration 11/25 | Loss: 0.00111392
Iteration 12/25 | Loss: 0.00111392
Iteration 13/25 | Loss: 0.00111392
Iteration 14/25 | Loss: 0.00111392
Iteration 15/25 | Loss: 0.00111392
Iteration 16/25 | Loss: 0.00111392
Iteration 17/25 | Loss: 0.00111392
Iteration 18/25 | Loss: 0.00111392
Iteration 19/25 | Loss: 0.00111392
Iteration 20/25 | Loss: 0.00111392
Iteration 21/25 | Loss: 0.00111392
Iteration 22/25 | Loss: 0.00111392
Iteration 23/25 | Loss: 0.00111392
Iteration 24/25 | Loss: 0.00111392
Iteration 25/25 | Loss: 0.00111392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111392
Iteration 2/1000 | Loss: 0.00003750
Iteration 3/1000 | Loss: 0.00002606
Iteration 4/1000 | Loss: 0.00003945
Iteration 5/1000 | Loss: 0.00002440
Iteration 6/1000 | Loss: 0.00002802
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00003221
Iteration 9/1000 | Loss: 0.00001887
Iteration 10/1000 | Loss: 0.00001733
Iteration 11/1000 | Loss: 0.00001707
Iteration 12/1000 | Loss: 0.00001698
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001654
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001635
Iteration 17/1000 | Loss: 0.00001635
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001632
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001626
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001617
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001615
Iteration 32/1000 | Loss: 0.00001614
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001613
Iteration 35/1000 | Loss: 0.00001613
Iteration 36/1000 | Loss: 0.00001613
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001610
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001609
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00001607
Iteration 50/1000 | Loss: 0.00001607
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001604
Iteration 56/1000 | Loss: 0.00001603
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001602
Iteration 59/1000 | Loss: 0.00001602
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001601
Iteration 63/1000 | Loss: 0.00001601
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001599
Iteration 68/1000 | Loss: 0.00001599
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00003630
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001599
Iteration 74/1000 | Loss: 0.00001598
Iteration 75/1000 | Loss: 0.00001598
Iteration 76/1000 | Loss: 0.00001598
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001598
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001596
Iteration 81/1000 | Loss: 0.00001596
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00002538
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001592
Iteration 107/1000 | Loss: 0.00001592
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001592
Iteration 110/1000 | Loss: 0.00001592
Iteration 111/1000 | Loss: 0.00001592
Iteration 112/1000 | Loss: 0.00001592
Iteration 113/1000 | Loss: 0.00001592
Iteration 114/1000 | Loss: 0.00001592
Iteration 115/1000 | Loss: 0.00001592
Iteration 116/1000 | Loss: 0.00001592
Iteration 117/1000 | Loss: 0.00001592
Iteration 118/1000 | Loss: 0.00001592
Iteration 119/1000 | Loss: 0.00001592
Iteration 120/1000 | Loss: 0.00001592
Iteration 121/1000 | Loss: 0.00001592
Iteration 122/1000 | Loss: 0.00001592
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.5916552001726814e-05, 1.5916552001726814e-05, 1.5916552001726814e-05, 1.5916552001726814e-05, 1.5916552001726814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5916552001726814e-05

Optimization complete. Final v2v error: 3.3645527362823486 mm

Highest mean error: 4.3149542808532715 mm for frame 239

Lowest mean error: 2.7960121631622314 mm for frame 135

Saving results

Total time: 68.28412222862244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999047
Iteration 2/25 | Loss: 0.00999047
Iteration 3/25 | Loss: 0.00211440
Iteration 4/25 | Loss: 0.00146694
Iteration 5/25 | Loss: 0.00141085
Iteration 6/25 | Loss: 0.00137941
Iteration 7/25 | Loss: 0.00136667
Iteration 8/25 | Loss: 0.00133947
Iteration 9/25 | Loss: 0.00133673
Iteration 10/25 | Loss: 0.00132850
Iteration 11/25 | Loss: 0.00133967
Iteration 12/25 | Loss: 0.00132201
Iteration 13/25 | Loss: 0.00131819
Iteration 14/25 | Loss: 0.00131358
Iteration 15/25 | Loss: 0.00131200
Iteration 16/25 | Loss: 0.00131534
Iteration 17/25 | Loss: 0.00131337
Iteration 18/25 | Loss: 0.00131606
Iteration 19/25 | Loss: 0.00130970
Iteration 20/25 | Loss: 0.00130710
Iteration 21/25 | Loss: 0.00130653
Iteration 22/25 | Loss: 0.00130677
Iteration 23/25 | Loss: 0.00130644
Iteration 24/25 | Loss: 0.00130609
Iteration 25/25 | Loss: 0.00130609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34817195
Iteration 2/25 | Loss: 0.00189093
Iteration 3/25 | Loss: 0.00175317
Iteration 4/25 | Loss: 0.00175316
Iteration 5/25 | Loss: 0.00175315
Iteration 6/25 | Loss: 0.00175315
Iteration 7/25 | Loss: 0.00175315
Iteration 8/25 | Loss: 0.00175315
Iteration 9/25 | Loss: 0.00175315
Iteration 10/25 | Loss: 0.00175315
Iteration 11/25 | Loss: 0.00175315
Iteration 12/25 | Loss: 0.00175315
Iteration 13/25 | Loss: 0.00175315
Iteration 14/25 | Loss: 0.00175315
Iteration 15/25 | Loss: 0.00175315
Iteration 16/25 | Loss: 0.00175315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017531521152704954, 0.0017531521152704954, 0.0017531521152704954, 0.0017531521152704954, 0.0017531521152704954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017531521152704954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175315
Iteration 2/1000 | Loss: 0.00158167
Iteration 3/1000 | Loss: 0.00115787
Iteration 4/1000 | Loss: 0.00016440
Iteration 5/1000 | Loss: 0.00014258
Iteration 6/1000 | Loss: 0.00007453
Iteration 7/1000 | Loss: 0.00015912
Iteration 8/1000 | Loss: 0.00047766
Iteration 9/1000 | Loss: 0.00007910
Iteration 10/1000 | Loss: 0.00013090
Iteration 11/1000 | Loss: 0.00009162
Iteration 12/1000 | Loss: 0.00005516
Iteration 13/1000 | Loss: 0.00136049
Iteration 14/1000 | Loss: 0.00038532
Iteration 15/1000 | Loss: 0.00026008
Iteration 16/1000 | Loss: 0.00105417
Iteration 17/1000 | Loss: 0.00092915
Iteration 18/1000 | Loss: 0.00021205
Iteration 19/1000 | Loss: 0.00062033
Iteration 20/1000 | Loss: 0.00144408
Iteration 21/1000 | Loss: 0.00027466
Iteration 22/1000 | Loss: 0.00015189
Iteration 23/1000 | Loss: 0.00102918
Iteration 24/1000 | Loss: 0.00018648
Iteration 25/1000 | Loss: 0.00005449
Iteration 26/1000 | Loss: 0.00052795
Iteration 27/1000 | Loss: 0.00033703
Iteration 28/1000 | Loss: 0.00117741
Iteration 29/1000 | Loss: 0.00024761
Iteration 30/1000 | Loss: 0.00076674
Iteration 31/1000 | Loss: 0.00053238
Iteration 32/1000 | Loss: 0.00008534
Iteration 33/1000 | Loss: 0.00055172
Iteration 34/1000 | Loss: 0.00021250
Iteration 35/1000 | Loss: 0.00032545
Iteration 36/1000 | Loss: 0.00035454
Iteration 37/1000 | Loss: 0.00008320
Iteration 38/1000 | Loss: 0.00042436
Iteration 39/1000 | Loss: 0.00019184
Iteration 40/1000 | Loss: 0.00077943
Iteration 41/1000 | Loss: 0.00029202
Iteration 42/1000 | Loss: 0.00006433
Iteration 43/1000 | Loss: 0.00007292
Iteration 44/1000 | Loss: 0.00010693
Iteration 45/1000 | Loss: 0.00003871
Iteration 46/1000 | Loss: 0.00003778
Iteration 47/1000 | Loss: 0.00077284
Iteration 48/1000 | Loss: 0.00034826
Iteration 49/1000 | Loss: 0.00020042
Iteration 50/1000 | Loss: 0.00017275
Iteration 51/1000 | Loss: 0.00011673
Iteration 52/1000 | Loss: 0.00006350
Iteration 53/1000 | Loss: 0.00011542
Iteration 54/1000 | Loss: 0.00005969
Iteration 55/1000 | Loss: 0.00030935
Iteration 56/1000 | Loss: 0.00012138
Iteration 57/1000 | Loss: 0.00006170
Iteration 58/1000 | Loss: 0.00003215
Iteration 59/1000 | Loss: 0.00006776
Iteration 60/1000 | Loss: 0.00004339
Iteration 61/1000 | Loss: 0.00003906
Iteration 62/1000 | Loss: 0.00005512
Iteration 63/1000 | Loss: 0.00008540
Iteration 64/1000 | Loss: 0.00003811
Iteration 65/1000 | Loss: 0.00003838
Iteration 66/1000 | Loss: 0.00003288
Iteration 67/1000 | Loss: 0.00002783
Iteration 68/1000 | Loss: 0.00003607
Iteration 69/1000 | Loss: 0.00006195
Iteration 70/1000 | Loss: 0.00002726
Iteration 71/1000 | Loss: 0.00011006
Iteration 72/1000 | Loss: 0.00002991
Iteration 73/1000 | Loss: 0.00006905
Iteration 74/1000 | Loss: 0.00003613
Iteration 75/1000 | Loss: 0.00023474
Iteration 76/1000 | Loss: 0.00003075
Iteration 77/1000 | Loss: 0.00002775
Iteration 78/1000 | Loss: 0.00003715
Iteration 79/1000 | Loss: 0.00002657
Iteration 80/1000 | Loss: 0.00002646
Iteration 81/1000 | Loss: 0.00002636
Iteration 82/1000 | Loss: 0.00009255
Iteration 83/1000 | Loss: 0.00003260
Iteration 84/1000 | Loss: 0.00002601
Iteration 85/1000 | Loss: 0.00013824
Iteration 86/1000 | Loss: 0.00004525
Iteration 87/1000 | Loss: 0.00004044
Iteration 88/1000 | Loss: 0.00002577
Iteration 89/1000 | Loss: 0.00002558
Iteration 90/1000 | Loss: 0.00002542
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00004754
Iteration 93/1000 | Loss: 0.00005077
Iteration 94/1000 | Loss: 0.00003022
Iteration 95/1000 | Loss: 0.00002542
Iteration 96/1000 | Loss: 0.00002509
Iteration 97/1000 | Loss: 0.00002500
Iteration 98/1000 | Loss: 0.00002481
Iteration 99/1000 | Loss: 0.00002479
Iteration 100/1000 | Loss: 0.00012074
Iteration 101/1000 | Loss: 0.00009323
Iteration 102/1000 | Loss: 0.00002799
Iteration 103/1000 | Loss: 0.00011363
Iteration 104/1000 | Loss: 0.00002853
Iteration 105/1000 | Loss: 0.00014690
Iteration 106/1000 | Loss: 0.00012754
Iteration 107/1000 | Loss: 0.00003344
Iteration 108/1000 | Loss: 0.00002459
Iteration 109/1000 | Loss: 0.00002398
Iteration 110/1000 | Loss: 0.00002363
Iteration 111/1000 | Loss: 0.00004800
Iteration 112/1000 | Loss: 0.00003907
Iteration 113/1000 | Loss: 0.00020579
Iteration 114/1000 | Loss: 0.00002608
Iteration 115/1000 | Loss: 0.00002328
Iteration 116/1000 | Loss: 0.00002643
Iteration 117/1000 | Loss: 0.00002322
Iteration 118/1000 | Loss: 0.00002321
Iteration 119/1000 | Loss: 0.00002321
Iteration 120/1000 | Loss: 0.00002320
Iteration 121/1000 | Loss: 0.00002320
Iteration 122/1000 | Loss: 0.00002320
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002318
Iteration 128/1000 | Loss: 0.00002318
Iteration 129/1000 | Loss: 0.00002317
Iteration 130/1000 | Loss: 0.00004805
Iteration 131/1000 | Loss: 0.00002318
Iteration 132/1000 | Loss: 0.00002315
Iteration 133/1000 | Loss: 0.00002314
Iteration 134/1000 | Loss: 0.00002314
Iteration 135/1000 | Loss: 0.00002314
Iteration 136/1000 | Loss: 0.00002313
Iteration 137/1000 | Loss: 0.00002313
Iteration 138/1000 | Loss: 0.00002306
Iteration 139/1000 | Loss: 0.00002306
Iteration 140/1000 | Loss: 0.00002306
Iteration 141/1000 | Loss: 0.00002306
Iteration 142/1000 | Loss: 0.00002305
Iteration 143/1000 | Loss: 0.00002305
Iteration 144/1000 | Loss: 0.00002304
Iteration 145/1000 | Loss: 0.00002304
Iteration 146/1000 | Loss: 0.00002304
Iteration 147/1000 | Loss: 0.00002304
Iteration 148/1000 | Loss: 0.00002304
Iteration 149/1000 | Loss: 0.00002303
Iteration 150/1000 | Loss: 0.00002303
Iteration 151/1000 | Loss: 0.00002303
Iteration 152/1000 | Loss: 0.00002303
Iteration 153/1000 | Loss: 0.00002302
Iteration 154/1000 | Loss: 0.00002302
Iteration 155/1000 | Loss: 0.00004422
Iteration 156/1000 | Loss: 0.00002496
Iteration 157/1000 | Loss: 0.00002360
Iteration 158/1000 | Loss: 0.00002330
Iteration 159/1000 | Loss: 0.00002301
Iteration 160/1000 | Loss: 0.00002299
Iteration 161/1000 | Loss: 0.00002299
Iteration 162/1000 | Loss: 0.00002298
Iteration 163/1000 | Loss: 0.00002298
Iteration 164/1000 | Loss: 0.00002298
Iteration 165/1000 | Loss: 0.00002297
Iteration 166/1000 | Loss: 0.00002297
Iteration 167/1000 | Loss: 0.00002297
Iteration 168/1000 | Loss: 0.00002297
Iteration 169/1000 | Loss: 0.00002297
Iteration 170/1000 | Loss: 0.00002297
Iteration 171/1000 | Loss: 0.00003233
Iteration 172/1000 | Loss: 0.00004153
Iteration 173/1000 | Loss: 0.00002464
Iteration 174/1000 | Loss: 0.00002381
Iteration 175/1000 | Loss: 0.00002295
Iteration 176/1000 | Loss: 0.00002295
Iteration 177/1000 | Loss: 0.00002295
Iteration 178/1000 | Loss: 0.00002295
Iteration 179/1000 | Loss: 0.00002295
Iteration 180/1000 | Loss: 0.00002295
Iteration 181/1000 | Loss: 0.00002295
Iteration 182/1000 | Loss: 0.00002295
Iteration 183/1000 | Loss: 0.00002295
Iteration 184/1000 | Loss: 0.00002295
Iteration 185/1000 | Loss: 0.00002295
Iteration 186/1000 | Loss: 0.00002295
Iteration 187/1000 | Loss: 0.00002295
Iteration 188/1000 | Loss: 0.00002295
Iteration 189/1000 | Loss: 0.00002295
Iteration 190/1000 | Loss: 0.00002295
Iteration 191/1000 | Loss: 0.00002295
Iteration 192/1000 | Loss: 0.00002295
Iteration 193/1000 | Loss: 0.00002295
Iteration 194/1000 | Loss: 0.00002295
Iteration 195/1000 | Loss: 0.00002295
Iteration 196/1000 | Loss: 0.00002295
Iteration 197/1000 | Loss: 0.00002295
Iteration 198/1000 | Loss: 0.00002295
Iteration 199/1000 | Loss: 0.00002295
Iteration 200/1000 | Loss: 0.00002295
Iteration 201/1000 | Loss: 0.00002295
Iteration 202/1000 | Loss: 0.00002295
Iteration 203/1000 | Loss: 0.00002295
Iteration 204/1000 | Loss: 0.00002295
Iteration 205/1000 | Loss: 0.00002295
Iteration 206/1000 | Loss: 0.00002295
Iteration 207/1000 | Loss: 0.00002295
Iteration 208/1000 | Loss: 0.00002295
Iteration 209/1000 | Loss: 0.00002295
Iteration 210/1000 | Loss: 0.00002295
Iteration 211/1000 | Loss: 0.00002295
Iteration 212/1000 | Loss: 0.00002295
Iteration 213/1000 | Loss: 0.00002295
Iteration 214/1000 | Loss: 0.00002295
Iteration 215/1000 | Loss: 0.00002295
Iteration 216/1000 | Loss: 0.00002295
Iteration 217/1000 | Loss: 0.00002295
Iteration 218/1000 | Loss: 0.00002295
Iteration 219/1000 | Loss: 0.00002295
Iteration 220/1000 | Loss: 0.00002295
Iteration 221/1000 | Loss: 0.00002295
Iteration 222/1000 | Loss: 0.00002295
Iteration 223/1000 | Loss: 0.00002295
Iteration 224/1000 | Loss: 0.00002295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.2945327145862393e-05, 2.2945327145862393e-05, 2.2945327145862393e-05, 2.2945327145862393e-05, 2.2945327145862393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2945327145862393e-05

Optimization complete. Final v2v error: 3.2655065059661865 mm

Highest mean error: 11.334197998046875 mm for frame 4

Lowest mean error: 2.8165764808654785 mm for frame 231

Saving results

Total time: 244.43536710739136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004742
Iteration 2/25 | Loss: 0.00159751
Iteration 3/25 | Loss: 0.00138343
Iteration 4/25 | Loss: 0.00137221
Iteration 5/25 | Loss: 0.00131391
Iteration 6/25 | Loss: 0.00129102
Iteration 7/25 | Loss: 0.00128713
Iteration 8/25 | Loss: 0.00129092
Iteration 9/25 | Loss: 0.00128475
Iteration 10/25 | Loss: 0.00126953
Iteration 11/25 | Loss: 0.00126896
Iteration 12/25 | Loss: 0.00126497
Iteration 13/25 | Loss: 0.00125953
Iteration 14/25 | Loss: 0.00125770
Iteration 15/25 | Loss: 0.00125436
Iteration 16/25 | Loss: 0.00125309
Iteration 17/25 | Loss: 0.00125451
Iteration 18/25 | Loss: 0.00125402
Iteration 19/25 | Loss: 0.00125397
Iteration 20/25 | Loss: 0.00125384
Iteration 21/25 | Loss: 0.00125364
Iteration 22/25 | Loss: 0.00125387
Iteration 23/25 | Loss: 0.00125442
Iteration 24/25 | Loss: 0.00125442
Iteration 25/25 | Loss: 0.00125432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.94274139
Iteration 2/25 | Loss: 0.00129431
Iteration 3/25 | Loss: 0.00123026
Iteration 4/25 | Loss: 0.00123025
Iteration 5/25 | Loss: 0.00123025
Iteration 6/25 | Loss: 0.00123025
Iteration 7/25 | Loss: 0.00123025
Iteration 8/25 | Loss: 0.00123025
Iteration 9/25 | Loss: 0.00123025
Iteration 10/25 | Loss: 0.00123025
Iteration 11/25 | Loss: 0.00123025
Iteration 12/25 | Loss: 0.00123025
Iteration 13/25 | Loss: 0.00123025
Iteration 14/25 | Loss: 0.00123025
Iteration 15/25 | Loss: 0.00123025
Iteration 16/25 | Loss: 0.00123025
Iteration 17/25 | Loss: 0.00123025
Iteration 18/25 | Loss: 0.00123025
Iteration 19/25 | Loss: 0.00123025
Iteration 20/25 | Loss: 0.00123025
Iteration 21/25 | Loss: 0.00123025
Iteration 22/25 | Loss: 0.00123025
Iteration 23/25 | Loss: 0.00123025
Iteration 24/25 | Loss: 0.00123025
Iteration 25/25 | Loss: 0.00123025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123025
Iteration 2/1000 | Loss: 0.00010848
Iteration 3/1000 | Loss: 0.00002767
Iteration 4/1000 | Loss: 0.00009874
Iteration 5/1000 | Loss: 0.00001766
Iteration 6/1000 | Loss: 0.00011231
Iteration 7/1000 | Loss: 0.00001540
Iteration 8/1000 | Loss: 0.00004061
Iteration 9/1000 | Loss: 0.00001481
Iteration 10/1000 | Loss: 0.00005509
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001398
Iteration 14/1000 | Loss: 0.00001386
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001364
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001353
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001351
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001348
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001318
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001316
Iteration 145/1000 | Loss: 0.00001316
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001315
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001314
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001312
Iteration 169/1000 | Loss: 0.00001312
Iteration 170/1000 | Loss: 0.00001312
Iteration 171/1000 | Loss: 0.00001312
Iteration 172/1000 | Loss: 0.00001312
Iteration 173/1000 | Loss: 0.00001312
Iteration 174/1000 | Loss: 0.00001312
Iteration 175/1000 | Loss: 0.00001312
Iteration 176/1000 | Loss: 0.00001311
Iteration 177/1000 | Loss: 0.00001311
Iteration 178/1000 | Loss: 0.00001311
Iteration 179/1000 | Loss: 0.00001311
Iteration 180/1000 | Loss: 0.00001311
Iteration 181/1000 | Loss: 0.00001311
Iteration 182/1000 | Loss: 0.00001311
Iteration 183/1000 | Loss: 0.00001311
Iteration 184/1000 | Loss: 0.00001311
Iteration 185/1000 | Loss: 0.00001311
Iteration 186/1000 | Loss: 0.00001311
Iteration 187/1000 | Loss: 0.00001311
Iteration 188/1000 | Loss: 0.00001311
Iteration 189/1000 | Loss: 0.00001310
Iteration 190/1000 | Loss: 0.00001310
Iteration 191/1000 | Loss: 0.00001310
Iteration 192/1000 | Loss: 0.00001310
Iteration 193/1000 | Loss: 0.00001310
Iteration 194/1000 | Loss: 0.00001310
Iteration 195/1000 | Loss: 0.00001310
Iteration 196/1000 | Loss: 0.00001310
Iteration 197/1000 | Loss: 0.00001310
Iteration 198/1000 | Loss: 0.00001310
Iteration 199/1000 | Loss: 0.00001310
Iteration 200/1000 | Loss: 0.00001310
Iteration 201/1000 | Loss: 0.00001310
Iteration 202/1000 | Loss: 0.00001310
Iteration 203/1000 | Loss: 0.00001310
Iteration 204/1000 | Loss: 0.00001310
Iteration 205/1000 | Loss: 0.00001310
Iteration 206/1000 | Loss: 0.00001310
Iteration 207/1000 | Loss: 0.00001310
Iteration 208/1000 | Loss: 0.00001310
Iteration 209/1000 | Loss: 0.00001310
Iteration 210/1000 | Loss: 0.00001310
Iteration 211/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.3101443983032368e-05, 1.3101443983032368e-05, 1.3101443983032368e-05, 1.3101443983032368e-05, 1.3101443983032368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3101443983032368e-05

Optimization complete. Final v2v error: 3.0540404319763184 mm

Highest mean error: 4.393215179443359 mm for frame 135

Lowest mean error: 2.7573177814483643 mm for frame 73

Saving results

Total time: 97.69017958641052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479493
Iteration 2/25 | Loss: 0.00133270
Iteration 3/25 | Loss: 0.00126848
Iteration 4/25 | Loss: 0.00126228
Iteration 5/25 | Loss: 0.00126021
Iteration 6/25 | Loss: 0.00126021
Iteration 7/25 | Loss: 0.00126021
Iteration 8/25 | Loss: 0.00126021
Iteration 9/25 | Loss: 0.00126021
Iteration 10/25 | Loss: 0.00126021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012602120405063033, 0.0012602120405063033, 0.0012602120405063033, 0.0012602120405063033, 0.0012602120405063033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012602120405063033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37763870
Iteration 2/25 | Loss: 0.00107513
Iteration 3/25 | Loss: 0.00107512
Iteration 4/25 | Loss: 0.00107512
Iteration 5/25 | Loss: 0.00107512
Iteration 6/25 | Loss: 0.00107512
Iteration 7/25 | Loss: 0.00107512
Iteration 8/25 | Loss: 0.00107512
Iteration 9/25 | Loss: 0.00107512
Iteration 10/25 | Loss: 0.00107512
Iteration 11/25 | Loss: 0.00107512
Iteration 12/25 | Loss: 0.00107512
Iteration 13/25 | Loss: 0.00107512
Iteration 14/25 | Loss: 0.00107512
Iteration 15/25 | Loss: 0.00107512
Iteration 16/25 | Loss: 0.00107512
Iteration 17/25 | Loss: 0.00107512
Iteration 18/25 | Loss: 0.00107512
Iteration 19/25 | Loss: 0.00107512
Iteration 20/25 | Loss: 0.00107512
Iteration 21/25 | Loss: 0.00107512
Iteration 22/25 | Loss: 0.00107512
Iteration 23/25 | Loss: 0.00107512
Iteration 24/25 | Loss: 0.00107512
Iteration 25/25 | Loss: 0.00107512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010751168010756373, 0.0010751168010756373, 0.0010751168010756373, 0.0010751168010756373, 0.0010751168010756373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010751168010756373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107512
Iteration 2/1000 | Loss: 0.00002942
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001928
Iteration 5/1000 | Loss: 0.00001835
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001642
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001591
Iteration 11/1000 | Loss: 0.00001574
Iteration 12/1000 | Loss: 0.00001566
Iteration 13/1000 | Loss: 0.00001563
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001562
Iteration 16/1000 | Loss: 0.00001561
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001555
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001550
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001549
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001528
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001520
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001517
Iteration 60/1000 | Loss: 0.00001517
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001516
Iteration 63/1000 | Loss: 0.00001516
Iteration 64/1000 | Loss: 0.00001516
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001508
Iteration 77/1000 | Loss: 0.00001508
Iteration 78/1000 | Loss: 0.00001508
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001503
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001502
Iteration 105/1000 | Loss: 0.00001502
Iteration 106/1000 | Loss: 0.00001502
Iteration 107/1000 | Loss: 0.00001502
Iteration 108/1000 | Loss: 0.00001502
Iteration 109/1000 | Loss: 0.00001502
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001501
Iteration 113/1000 | Loss: 0.00001500
Iteration 114/1000 | Loss: 0.00001500
Iteration 115/1000 | Loss: 0.00001499
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001496
Iteration 131/1000 | Loss: 0.00001496
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001495
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001495
Iteration 144/1000 | Loss: 0.00001495
Iteration 145/1000 | Loss: 0.00001495
Iteration 146/1000 | Loss: 0.00001495
Iteration 147/1000 | Loss: 0.00001495
Iteration 148/1000 | Loss: 0.00001495
Iteration 149/1000 | Loss: 0.00001495
Iteration 150/1000 | Loss: 0.00001495
Iteration 151/1000 | Loss: 0.00001495
Iteration 152/1000 | Loss: 0.00001495
Iteration 153/1000 | Loss: 0.00001495
Iteration 154/1000 | Loss: 0.00001495
Iteration 155/1000 | Loss: 0.00001495
Iteration 156/1000 | Loss: 0.00001495
Iteration 157/1000 | Loss: 0.00001495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.4948649550206028e-05, 1.4948649550206028e-05, 1.4948649550206028e-05, 1.4948649550206028e-05, 1.4948649550206028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4948649550206028e-05

Optimization complete. Final v2v error: 3.2631995677948 mm

Highest mean error: 3.586466073989868 mm for frame 103

Lowest mean error: 3.0103049278259277 mm for frame 213

Saving results

Total time: 41.43832612037659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787684
Iteration 2/25 | Loss: 0.00134260
Iteration 3/25 | Loss: 0.00125203
Iteration 4/25 | Loss: 0.00123873
Iteration 5/25 | Loss: 0.00123550
Iteration 6/25 | Loss: 0.00123533
Iteration 7/25 | Loss: 0.00123533
Iteration 8/25 | Loss: 0.00123533
Iteration 9/25 | Loss: 0.00123533
Iteration 10/25 | Loss: 0.00123533
Iteration 11/25 | Loss: 0.00123533
Iteration 12/25 | Loss: 0.00123533
Iteration 13/25 | Loss: 0.00123533
Iteration 14/25 | Loss: 0.00123533
Iteration 15/25 | Loss: 0.00123533
Iteration 16/25 | Loss: 0.00123533
Iteration 17/25 | Loss: 0.00123533
Iteration 18/25 | Loss: 0.00123533
Iteration 19/25 | Loss: 0.00123533
Iteration 20/25 | Loss: 0.00123533
Iteration 21/25 | Loss: 0.00123533
Iteration 22/25 | Loss: 0.00123533
Iteration 23/25 | Loss: 0.00123533
Iteration 24/25 | Loss: 0.00123533
Iteration 25/25 | Loss: 0.00123533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31873107
Iteration 2/25 | Loss: 0.00086195
Iteration 3/25 | Loss: 0.00086192
Iteration 4/25 | Loss: 0.00086192
Iteration 5/25 | Loss: 0.00086192
Iteration 6/25 | Loss: 0.00086192
Iteration 7/25 | Loss: 0.00086192
Iteration 8/25 | Loss: 0.00086192
Iteration 9/25 | Loss: 0.00086192
Iteration 10/25 | Loss: 0.00086192
Iteration 11/25 | Loss: 0.00086192
Iteration 12/25 | Loss: 0.00086192
Iteration 13/25 | Loss: 0.00086192
Iteration 14/25 | Loss: 0.00086192
Iteration 15/25 | Loss: 0.00086192
Iteration 16/25 | Loss: 0.00086192
Iteration 17/25 | Loss: 0.00086192
Iteration 18/25 | Loss: 0.00086192
Iteration 19/25 | Loss: 0.00086192
Iteration 20/25 | Loss: 0.00086192
Iteration 21/25 | Loss: 0.00086192
Iteration 22/25 | Loss: 0.00086192
Iteration 23/25 | Loss: 0.00086192
Iteration 24/25 | Loss: 0.00086192
Iteration 25/25 | Loss: 0.00086192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086192
Iteration 2/1000 | Loss: 0.00002910
Iteration 3/1000 | Loss: 0.00002162
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001776
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001642
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001604
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00001588
Iteration 14/1000 | Loss: 0.00001573
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001552
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001547
Iteration 19/1000 | Loss: 0.00001546
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001540
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001537
Iteration 27/1000 | Loss: 0.00001537
Iteration 28/1000 | Loss: 0.00001534
Iteration 29/1000 | Loss: 0.00001532
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001531
Iteration 32/1000 | Loss: 0.00001530
Iteration 33/1000 | Loss: 0.00001529
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001520
Iteration 36/1000 | Loss: 0.00001520
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001519
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001513
Iteration 48/1000 | Loss: 0.00001513
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001512
Iteration 51/1000 | Loss: 0.00001512
Iteration 52/1000 | Loss: 0.00001512
Iteration 53/1000 | Loss: 0.00001512
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001511
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001503
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001502
Iteration 63/1000 | Loss: 0.00001502
Iteration 64/1000 | Loss: 0.00001501
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001499
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001499
Iteration 72/1000 | Loss: 0.00001499
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001498
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001497
Iteration 77/1000 | Loss: 0.00001497
Iteration 78/1000 | Loss: 0.00001497
Iteration 79/1000 | Loss: 0.00001497
Iteration 80/1000 | Loss: 0.00001497
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001497
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001497
Iteration 88/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.4968299183237832e-05, 1.4968299183237832e-05, 1.4968299183237832e-05, 1.4968299183237832e-05, 1.4968299183237832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4968299183237832e-05

Optimization complete. Final v2v error: 3.263200521469116 mm

Highest mean error: 3.4812769889831543 mm for frame 65

Lowest mean error: 3.042017698287964 mm for frame 0

Saving results

Total time: 37.29389309883118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409968
Iteration 2/25 | Loss: 0.00136059
Iteration 3/25 | Loss: 0.00124236
Iteration 4/25 | Loss: 0.00122477
Iteration 5/25 | Loss: 0.00121964
Iteration 6/25 | Loss: 0.00121816
Iteration 7/25 | Loss: 0.00121816
Iteration 8/25 | Loss: 0.00121816
Iteration 9/25 | Loss: 0.00121816
Iteration 10/25 | Loss: 0.00121816
Iteration 11/25 | Loss: 0.00121816
Iteration 12/25 | Loss: 0.00121816
Iteration 13/25 | Loss: 0.00121816
Iteration 14/25 | Loss: 0.00121816
Iteration 15/25 | Loss: 0.00121816
Iteration 16/25 | Loss: 0.00121816
Iteration 17/25 | Loss: 0.00121816
Iteration 18/25 | Loss: 0.00121816
Iteration 19/25 | Loss: 0.00121816
Iteration 20/25 | Loss: 0.00121816
Iteration 21/25 | Loss: 0.00121816
Iteration 22/25 | Loss: 0.00121816
Iteration 23/25 | Loss: 0.00121816
Iteration 24/25 | Loss: 0.00121816
Iteration 25/25 | Loss: 0.00121816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33831656
Iteration 2/25 | Loss: 0.00113663
Iteration 3/25 | Loss: 0.00113661
Iteration 4/25 | Loss: 0.00113661
Iteration 5/25 | Loss: 0.00113661
Iteration 6/25 | Loss: 0.00113661
Iteration 7/25 | Loss: 0.00113661
Iteration 8/25 | Loss: 0.00113661
Iteration 9/25 | Loss: 0.00113661
Iteration 10/25 | Loss: 0.00113661
Iteration 11/25 | Loss: 0.00113661
Iteration 12/25 | Loss: 0.00113661
Iteration 13/25 | Loss: 0.00113661
Iteration 14/25 | Loss: 0.00113661
Iteration 15/25 | Loss: 0.00113661
Iteration 16/25 | Loss: 0.00113661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001136608887463808, 0.001136608887463808, 0.001136608887463808, 0.001136608887463808, 0.001136608887463808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001136608887463808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113661
Iteration 2/1000 | Loss: 0.00005004
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001891
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001585
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001533
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001508
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001504
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001503
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001503
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001502
Iteration 27/1000 | Loss: 0.00001501
Iteration 28/1000 | Loss: 0.00001501
Iteration 29/1000 | Loss: 0.00001500
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001496
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001484
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001482
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001474
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001464
Iteration 80/1000 | Loss: 0.00001464
Iteration 81/1000 | Loss: 0.00001464
Iteration 82/1000 | Loss: 0.00001464
Iteration 83/1000 | Loss: 0.00001464
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001463
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001460
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001459
Iteration 101/1000 | Loss: 0.00001459
Iteration 102/1000 | Loss: 0.00001459
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001458
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001457
Iteration 118/1000 | Loss: 0.00001457
Iteration 119/1000 | Loss: 0.00001457
Iteration 120/1000 | Loss: 0.00001457
Iteration 121/1000 | Loss: 0.00001457
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001453
Iteration 138/1000 | Loss: 0.00001453
Iteration 139/1000 | Loss: 0.00001453
Iteration 140/1000 | Loss: 0.00001453
Iteration 141/1000 | Loss: 0.00001452
Iteration 142/1000 | Loss: 0.00001452
Iteration 143/1000 | Loss: 0.00001452
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001451
Iteration 148/1000 | Loss: 0.00001451
Iteration 149/1000 | Loss: 0.00001451
Iteration 150/1000 | Loss: 0.00001451
Iteration 151/1000 | Loss: 0.00001451
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001449
Iteration 159/1000 | Loss: 0.00001449
Iteration 160/1000 | Loss: 0.00001449
Iteration 161/1000 | Loss: 0.00001449
Iteration 162/1000 | Loss: 0.00001449
Iteration 163/1000 | Loss: 0.00001449
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001448
Iteration 168/1000 | Loss: 0.00001448
Iteration 169/1000 | Loss: 0.00001448
Iteration 170/1000 | Loss: 0.00001448
Iteration 171/1000 | Loss: 0.00001448
Iteration 172/1000 | Loss: 0.00001448
Iteration 173/1000 | Loss: 0.00001448
Iteration 174/1000 | Loss: 0.00001448
Iteration 175/1000 | Loss: 0.00001447
Iteration 176/1000 | Loss: 0.00001447
Iteration 177/1000 | Loss: 0.00001447
Iteration 178/1000 | Loss: 0.00001447
Iteration 179/1000 | Loss: 0.00001447
Iteration 180/1000 | Loss: 0.00001446
Iteration 181/1000 | Loss: 0.00001446
Iteration 182/1000 | Loss: 0.00001446
Iteration 183/1000 | Loss: 0.00001446
Iteration 184/1000 | Loss: 0.00001446
Iteration 185/1000 | Loss: 0.00001446
Iteration 186/1000 | Loss: 0.00001446
Iteration 187/1000 | Loss: 0.00001445
Iteration 188/1000 | Loss: 0.00001445
Iteration 189/1000 | Loss: 0.00001445
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Iteration 192/1000 | Loss: 0.00001444
Iteration 193/1000 | Loss: 0.00001444
Iteration 194/1000 | Loss: 0.00001443
Iteration 195/1000 | Loss: 0.00001443
Iteration 196/1000 | Loss: 0.00001443
Iteration 197/1000 | Loss: 0.00001442
Iteration 198/1000 | Loss: 0.00001442
Iteration 199/1000 | Loss: 0.00001442
Iteration 200/1000 | Loss: 0.00001442
Iteration 201/1000 | Loss: 0.00001442
Iteration 202/1000 | Loss: 0.00001441
Iteration 203/1000 | Loss: 0.00001441
Iteration 204/1000 | Loss: 0.00001441
Iteration 205/1000 | Loss: 0.00001441
Iteration 206/1000 | Loss: 0.00001441
Iteration 207/1000 | Loss: 0.00001440
Iteration 208/1000 | Loss: 0.00001440
Iteration 209/1000 | Loss: 0.00001440
Iteration 210/1000 | Loss: 0.00001440
Iteration 211/1000 | Loss: 0.00001439
Iteration 212/1000 | Loss: 0.00001439
Iteration 213/1000 | Loss: 0.00001439
Iteration 214/1000 | Loss: 0.00001439
Iteration 215/1000 | Loss: 0.00001439
Iteration 216/1000 | Loss: 0.00001439
Iteration 217/1000 | Loss: 0.00001439
Iteration 218/1000 | Loss: 0.00001439
Iteration 219/1000 | Loss: 0.00001438
Iteration 220/1000 | Loss: 0.00001438
Iteration 221/1000 | Loss: 0.00001438
Iteration 222/1000 | Loss: 0.00001438
Iteration 223/1000 | Loss: 0.00001438
Iteration 224/1000 | Loss: 0.00001438
Iteration 225/1000 | Loss: 0.00001438
Iteration 226/1000 | Loss: 0.00001438
Iteration 227/1000 | Loss: 0.00001438
Iteration 228/1000 | Loss: 0.00001438
Iteration 229/1000 | Loss: 0.00001438
Iteration 230/1000 | Loss: 0.00001438
Iteration 231/1000 | Loss: 0.00001437
Iteration 232/1000 | Loss: 0.00001437
Iteration 233/1000 | Loss: 0.00001437
Iteration 234/1000 | Loss: 0.00001437
Iteration 235/1000 | Loss: 0.00001437
Iteration 236/1000 | Loss: 0.00001437
Iteration 237/1000 | Loss: 0.00001437
Iteration 238/1000 | Loss: 0.00001437
Iteration 239/1000 | Loss: 0.00001437
Iteration 240/1000 | Loss: 0.00001437
Iteration 241/1000 | Loss: 0.00001437
Iteration 242/1000 | Loss: 0.00001437
Iteration 243/1000 | Loss: 0.00001437
Iteration 244/1000 | Loss: 0.00001437
Iteration 245/1000 | Loss: 0.00001436
Iteration 246/1000 | Loss: 0.00001436
Iteration 247/1000 | Loss: 0.00001436
Iteration 248/1000 | Loss: 0.00001436
Iteration 249/1000 | Loss: 0.00001436
Iteration 250/1000 | Loss: 0.00001436
Iteration 251/1000 | Loss: 0.00001436
Iteration 252/1000 | Loss: 0.00001436
Iteration 253/1000 | Loss: 0.00001436
Iteration 254/1000 | Loss: 0.00001436
Iteration 255/1000 | Loss: 0.00001436
Iteration 256/1000 | Loss: 0.00001436
Iteration 257/1000 | Loss: 0.00001436
Iteration 258/1000 | Loss: 0.00001436
Iteration 259/1000 | Loss: 0.00001436
Iteration 260/1000 | Loss: 0.00001436
Iteration 261/1000 | Loss: 0.00001435
Iteration 262/1000 | Loss: 0.00001435
Iteration 263/1000 | Loss: 0.00001435
Iteration 264/1000 | Loss: 0.00001435
Iteration 265/1000 | Loss: 0.00001435
Iteration 266/1000 | Loss: 0.00001435
Iteration 267/1000 | Loss: 0.00001435
Iteration 268/1000 | Loss: 0.00001435
Iteration 269/1000 | Loss: 0.00001435
Iteration 270/1000 | Loss: 0.00001435
Iteration 271/1000 | Loss: 0.00001435
Iteration 272/1000 | Loss: 0.00001435
Iteration 273/1000 | Loss: 0.00001434
Iteration 274/1000 | Loss: 0.00001434
Iteration 275/1000 | Loss: 0.00001434
Iteration 276/1000 | Loss: 0.00001434
Iteration 277/1000 | Loss: 0.00001434
Iteration 278/1000 | Loss: 0.00001434
Iteration 279/1000 | Loss: 0.00001434
Iteration 280/1000 | Loss: 0.00001433
Iteration 281/1000 | Loss: 0.00001433
Iteration 282/1000 | Loss: 0.00001433
Iteration 283/1000 | Loss: 0.00001433
Iteration 284/1000 | Loss: 0.00001433
Iteration 285/1000 | Loss: 0.00001433
Iteration 286/1000 | Loss: 0.00001433
Iteration 287/1000 | Loss: 0.00001433
Iteration 288/1000 | Loss: 0.00001433
Iteration 289/1000 | Loss: 0.00001433
Iteration 290/1000 | Loss: 0.00001433
Iteration 291/1000 | Loss: 0.00001433
Iteration 292/1000 | Loss: 0.00001433
Iteration 293/1000 | Loss: 0.00001433
Iteration 294/1000 | Loss: 0.00001433
Iteration 295/1000 | Loss: 0.00001433
Iteration 296/1000 | Loss: 0.00001433
Iteration 297/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.4330090380099136e-05, 1.4330090380099136e-05, 1.4330090380099136e-05, 1.4330090380099136e-05, 1.4330090380099136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4330090380099136e-05

Optimization complete. Final v2v error: 3.11666202545166 mm

Highest mean error: 5.058684349060059 mm for frame 84

Lowest mean error: 2.6213486194610596 mm for frame 174

Saving results

Total time: 50.016215085983276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048418
Iteration 2/25 | Loss: 0.00151328
Iteration 3/25 | Loss: 0.00124543
Iteration 4/25 | Loss: 0.00122233
Iteration 5/25 | Loss: 0.00121776
Iteration 6/25 | Loss: 0.00121746
Iteration 7/25 | Loss: 0.00121746
Iteration 8/25 | Loss: 0.00121746
Iteration 9/25 | Loss: 0.00121746
Iteration 10/25 | Loss: 0.00121746
Iteration 11/25 | Loss: 0.00121746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012174599105492234, 0.0012174599105492234, 0.0012174599105492234, 0.0012174599105492234, 0.0012174599105492234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012174599105492234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13726306
Iteration 2/25 | Loss: 0.00103526
Iteration 3/25 | Loss: 0.00103526
Iteration 4/25 | Loss: 0.00103526
Iteration 5/25 | Loss: 0.00103526
Iteration 6/25 | Loss: 0.00103526
Iteration 7/25 | Loss: 0.00103526
Iteration 8/25 | Loss: 0.00103526
Iteration 9/25 | Loss: 0.00103526
Iteration 10/25 | Loss: 0.00103526
Iteration 11/25 | Loss: 0.00103526
Iteration 12/25 | Loss: 0.00103526
Iteration 13/25 | Loss: 0.00103526
Iteration 14/25 | Loss: 0.00103526
Iteration 15/25 | Loss: 0.00103526
Iteration 16/25 | Loss: 0.00103526
Iteration 17/25 | Loss: 0.00103526
Iteration 18/25 | Loss: 0.00103526
Iteration 19/25 | Loss: 0.00103526
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010352602694183588, 0.0010352602694183588, 0.0010352602694183588, 0.0010352602694183588, 0.0010352602694183588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010352602694183588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103526
Iteration 2/1000 | Loss: 0.00003099
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001641
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001123
Iteration 21/1000 | Loss: 0.00001123
Iteration 22/1000 | Loss: 0.00001123
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001122
Iteration 26/1000 | Loss: 0.00001122
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001114
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001107
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001103
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001102
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001101
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001100
Iteration 81/1000 | Loss: 0.00001100
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001100
Iteration 85/1000 | Loss: 0.00001100
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001100
Iteration 89/1000 | Loss: 0.00001099
Iteration 90/1000 | Loss: 0.00001099
Iteration 91/1000 | Loss: 0.00001099
Iteration 92/1000 | Loss: 0.00001099
Iteration 93/1000 | Loss: 0.00001099
Iteration 94/1000 | Loss: 0.00001099
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001098
Iteration 97/1000 | Loss: 0.00001097
Iteration 98/1000 | Loss: 0.00001097
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001096
Iteration 102/1000 | Loss: 0.00001096
Iteration 103/1000 | Loss: 0.00001096
Iteration 104/1000 | Loss: 0.00001096
Iteration 105/1000 | Loss: 0.00001096
Iteration 106/1000 | Loss: 0.00001096
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001095
Iteration 109/1000 | Loss: 0.00001095
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001095
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001095
Iteration 115/1000 | Loss: 0.00001095
Iteration 116/1000 | Loss: 0.00001095
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001094
Iteration 119/1000 | Loss: 0.00001094
Iteration 120/1000 | Loss: 0.00001094
Iteration 121/1000 | Loss: 0.00001094
Iteration 122/1000 | Loss: 0.00001094
Iteration 123/1000 | Loss: 0.00001094
Iteration 124/1000 | Loss: 0.00001094
Iteration 125/1000 | Loss: 0.00001094
Iteration 126/1000 | Loss: 0.00001094
Iteration 127/1000 | Loss: 0.00001094
Iteration 128/1000 | Loss: 0.00001094
Iteration 129/1000 | Loss: 0.00001094
Iteration 130/1000 | Loss: 0.00001094
Iteration 131/1000 | Loss: 0.00001093
Iteration 132/1000 | Loss: 0.00001093
Iteration 133/1000 | Loss: 0.00001093
Iteration 134/1000 | Loss: 0.00001093
Iteration 135/1000 | Loss: 0.00001093
Iteration 136/1000 | Loss: 0.00001093
Iteration 137/1000 | Loss: 0.00001093
Iteration 138/1000 | Loss: 0.00001093
Iteration 139/1000 | Loss: 0.00001093
Iteration 140/1000 | Loss: 0.00001093
Iteration 141/1000 | Loss: 0.00001093
Iteration 142/1000 | Loss: 0.00001093
Iteration 143/1000 | Loss: 0.00001092
Iteration 144/1000 | Loss: 0.00001092
Iteration 145/1000 | Loss: 0.00001092
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001090
Iteration 155/1000 | Loss: 0.00001090
Iteration 156/1000 | Loss: 0.00001090
Iteration 157/1000 | Loss: 0.00001090
Iteration 158/1000 | Loss: 0.00001090
Iteration 159/1000 | Loss: 0.00001090
Iteration 160/1000 | Loss: 0.00001090
Iteration 161/1000 | Loss: 0.00001090
Iteration 162/1000 | Loss: 0.00001090
Iteration 163/1000 | Loss: 0.00001090
Iteration 164/1000 | Loss: 0.00001089
Iteration 165/1000 | Loss: 0.00001089
Iteration 166/1000 | Loss: 0.00001089
Iteration 167/1000 | Loss: 0.00001089
Iteration 168/1000 | Loss: 0.00001088
Iteration 169/1000 | Loss: 0.00001088
Iteration 170/1000 | Loss: 0.00001088
Iteration 171/1000 | Loss: 0.00001088
Iteration 172/1000 | Loss: 0.00001088
Iteration 173/1000 | Loss: 0.00001088
Iteration 174/1000 | Loss: 0.00001088
Iteration 175/1000 | Loss: 0.00001088
Iteration 176/1000 | Loss: 0.00001088
Iteration 177/1000 | Loss: 0.00001088
Iteration 178/1000 | Loss: 0.00001088
Iteration 179/1000 | Loss: 0.00001087
Iteration 180/1000 | Loss: 0.00001087
Iteration 181/1000 | Loss: 0.00001087
Iteration 182/1000 | Loss: 0.00001087
Iteration 183/1000 | Loss: 0.00001087
Iteration 184/1000 | Loss: 0.00001087
Iteration 185/1000 | Loss: 0.00001087
Iteration 186/1000 | Loss: 0.00001087
Iteration 187/1000 | Loss: 0.00001087
Iteration 188/1000 | Loss: 0.00001087
Iteration 189/1000 | Loss: 0.00001087
Iteration 190/1000 | Loss: 0.00001086
Iteration 191/1000 | Loss: 0.00001086
Iteration 192/1000 | Loss: 0.00001086
Iteration 193/1000 | Loss: 0.00001086
Iteration 194/1000 | Loss: 0.00001086
Iteration 195/1000 | Loss: 0.00001086
Iteration 196/1000 | Loss: 0.00001086
Iteration 197/1000 | Loss: 0.00001086
Iteration 198/1000 | Loss: 0.00001086
Iteration 199/1000 | Loss: 0.00001086
Iteration 200/1000 | Loss: 0.00001086
Iteration 201/1000 | Loss: 0.00001086
Iteration 202/1000 | Loss: 0.00001086
Iteration 203/1000 | Loss: 0.00001086
Iteration 204/1000 | Loss: 0.00001086
Iteration 205/1000 | Loss: 0.00001086
Iteration 206/1000 | Loss: 0.00001086
Iteration 207/1000 | Loss: 0.00001086
Iteration 208/1000 | Loss: 0.00001086
Iteration 209/1000 | Loss: 0.00001086
Iteration 210/1000 | Loss: 0.00001086
Iteration 211/1000 | Loss: 0.00001086
Iteration 212/1000 | Loss: 0.00001086
Iteration 213/1000 | Loss: 0.00001086
Iteration 214/1000 | Loss: 0.00001086
Iteration 215/1000 | Loss: 0.00001086
Iteration 216/1000 | Loss: 0.00001086
Iteration 217/1000 | Loss: 0.00001086
Iteration 218/1000 | Loss: 0.00001086
Iteration 219/1000 | Loss: 0.00001086
Iteration 220/1000 | Loss: 0.00001086
Iteration 221/1000 | Loss: 0.00001086
Iteration 222/1000 | Loss: 0.00001086
Iteration 223/1000 | Loss: 0.00001086
Iteration 224/1000 | Loss: 0.00001086
Iteration 225/1000 | Loss: 0.00001086
Iteration 226/1000 | Loss: 0.00001086
Iteration 227/1000 | Loss: 0.00001086
Iteration 228/1000 | Loss: 0.00001086
Iteration 229/1000 | Loss: 0.00001086
Iteration 230/1000 | Loss: 0.00001086
Iteration 231/1000 | Loss: 0.00001086
Iteration 232/1000 | Loss: 0.00001086
Iteration 233/1000 | Loss: 0.00001086
Iteration 234/1000 | Loss: 0.00001086
Iteration 235/1000 | Loss: 0.00001086
Iteration 236/1000 | Loss: 0.00001086
Iteration 237/1000 | Loss: 0.00001086
Iteration 238/1000 | Loss: 0.00001086
Iteration 239/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.0857312190637458e-05, 1.0857312190637458e-05, 1.0857312190637458e-05, 1.0857312190637458e-05, 1.0857312190637458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0857312190637458e-05

Optimization complete. Final v2v error: 2.8308918476104736 mm

Highest mean error: 3.0186078548431396 mm for frame 32

Lowest mean error: 2.703906774520874 mm for frame 153

Saving results

Total time: 45.553155183792114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456245
Iteration 2/25 | Loss: 0.00149139
Iteration 3/25 | Loss: 0.00130351
Iteration 4/25 | Loss: 0.00128590
Iteration 5/25 | Loss: 0.00128330
Iteration 6/25 | Loss: 0.00128294
Iteration 7/25 | Loss: 0.00128294
Iteration 8/25 | Loss: 0.00128294
Iteration 9/25 | Loss: 0.00128294
Iteration 10/25 | Loss: 0.00128294
Iteration 11/25 | Loss: 0.00128294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012829386396333575, 0.0012829386396333575, 0.0012829386396333575, 0.0012829386396333575, 0.0012829386396333575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012829386396333575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09887266
Iteration 2/25 | Loss: 0.00077097
Iteration 3/25 | Loss: 0.00077095
Iteration 4/25 | Loss: 0.00077095
Iteration 5/25 | Loss: 0.00077095
Iteration 6/25 | Loss: 0.00077095
Iteration 7/25 | Loss: 0.00077095
Iteration 8/25 | Loss: 0.00077095
Iteration 9/25 | Loss: 0.00077095
Iteration 10/25 | Loss: 0.00077094
Iteration 11/25 | Loss: 0.00077094
Iteration 12/25 | Loss: 0.00077094
Iteration 13/25 | Loss: 0.00077094
Iteration 14/25 | Loss: 0.00077094
Iteration 15/25 | Loss: 0.00077094
Iteration 16/25 | Loss: 0.00077094
Iteration 17/25 | Loss: 0.00077094
Iteration 18/25 | Loss: 0.00077094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007709445198997855, 0.0007709445198997855, 0.0007709445198997855, 0.0007709445198997855, 0.0007709445198997855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007709445198997855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077094
Iteration 2/1000 | Loss: 0.00003344
Iteration 3/1000 | Loss: 0.00002392
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001912
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001842
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001822
Iteration 13/1000 | Loss: 0.00001809
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001806
Iteration 16/1000 | Loss: 0.00001805
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001789
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001788
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001778
Iteration 36/1000 | Loss: 0.00001777
Iteration 37/1000 | Loss: 0.00001777
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001776
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001774
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001769
Iteration 60/1000 | Loss: 0.00001769
Iteration 61/1000 | Loss: 0.00001768
Iteration 62/1000 | Loss: 0.00001768
Iteration 63/1000 | Loss: 0.00001767
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001767
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001766
Iteration 68/1000 | Loss: 0.00001766
Iteration 69/1000 | Loss: 0.00001766
Iteration 70/1000 | Loss: 0.00001766
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001766
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001765
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001762
Iteration 84/1000 | Loss: 0.00001761
Iteration 85/1000 | Loss: 0.00001761
Iteration 86/1000 | Loss: 0.00001761
Iteration 87/1000 | Loss: 0.00001760
Iteration 88/1000 | Loss: 0.00001760
Iteration 89/1000 | Loss: 0.00001760
Iteration 90/1000 | Loss: 0.00001760
Iteration 91/1000 | Loss: 0.00001760
Iteration 92/1000 | Loss: 0.00001760
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00001760
Iteration 95/1000 | Loss: 0.00001760
Iteration 96/1000 | Loss: 0.00001760
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001759
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001759
Iteration 103/1000 | Loss: 0.00001759
Iteration 104/1000 | Loss: 0.00001759
Iteration 105/1000 | Loss: 0.00001759
Iteration 106/1000 | Loss: 0.00001759
Iteration 107/1000 | Loss: 0.00001759
Iteration 108/1000 | Loss: 0.00001759
Iteration 109/1000 | Loss: 0.00001759
Iteration 110/1000 | Loss: 0.00001759
Iteration 111/1000 | Loss: 0.00001759
Iteration 112/1000 | Loss: 0.00001759
Iteration 113/1000 | Loss: 0.00001759
Iteration 114/1000 | Loss: 0.00001759
Iteration 115/1000 | Loss: 0.00001759
Iteration 116/1000 | Loss: 0.00001759
Iteration 117/1000 | Loss: 0.00001759
Iteration 118/1000 | Loss: 0.00001759
Iteration 119/1000 | Loss: 0.00001759
Iteration 120/1000 | Loss: 0.00001759
Iteration 121/1000 | Loss: 0.00001759
Iteration 122/1000 | Loss: 0.00001759
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001759
Iteration 127/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.7592219592188485e-05, 1.7592219592188485e-05, 1.7592219592188485e-05, 1.7592219592188485e-05, 1.7592219592188485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7592219592188485e-05

Optimization complete. Final v2v error: 3.533966064453125 mm

Highest mean error: 4.137439250946045 mm for frame 62

Lowest mean error: 3.1415328979492188 mm for frame 0

Saving results

Total time: 35.02946496009827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479325
Iteration 2/25 | Loss: 0.00137985
Iteration 3/25 | Loss: 0.00128234
Iteration 4/25 | Loss: 0.00127425
Iteration 5/25 | Loss: 0.00127261
Iteration 6/25 | Loss: 0.00127261
Iteration 7/25 | Loss: 0.00127261
Iteration 8/25 | Loss: 0.00127261
Iteration 9/25 | Loss: 0.00127261
Iteration 10/25 | Loss: 0.00127261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012726130662485957, 0.0012726130662485957, 0.0012726130662485957, 0.0012726130662485957, 0.0012726130662485957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012726130662485957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35176492
Iteration 2/25 | Loss: 0.00098978
Iteration 3/25 | Loss: 0.00098976
Iteration 4/25 | Loss: 0.00098976
Iteration 5/25 | Loss: 0.00098976
Iteration 6/25 | Loss: 0.00098976
Iteration 7/25 | Loss: 0.00098976
Iteration 8/25 | Loss: 0.00098976
Iteration 9/25 | Loss: 0.00098976
Iteration 10/25 | Loss: 0.00098976
Iteration 11/25 | Loss: 0.00098976
Iteration 12/25 | Loss: 0.00098976
Iteration 13/25 | Loss: 0.00098976
Iteration 14/25 | Loss: 0.00098976
Iteration 15/25 | Loss: 0.00098976
Iteration 16/25 | Loss: 0.00098975
Iteration 17/25 | Loss: 0.00098975
Iteration 18/25 | Loss: 0.00098975
Iteration 19/25 | Loss: 0.00098975
Iteration 20/25 | Loss: 0.00098975
Iteration 21/25 | Loss: 0.00098975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000989754917100072, 0.000989754917100072, 0.000989754917100072, 0.000989754917100072, 0.000989754917100072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000989754917100072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098975
Iteration 2/1000 | Loss: 0.00003180
Iteration 3/1000 | Loss: 0.00002229
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001916
Iteration 6/1000 | Loss: 0.00001843
Iteration 7/1000 | Loss: 0.00001799
Iteration 8/1000 | Loss: 0.00001774
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001722
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001697
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001694
Iteration 19/1000 | Loss: 0.00001689
Iteration 20/1000 | Loss: 0.00001688
Iteration 21/1000 | Loss: 0.00001688
Iteration 22/1000 | Loss: 0.00001687
Iteration 23/1000 | Loss: 0.00001686
Iteration 24/1000 | Loss: 0.00001679
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001676
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001675
Iteration 29/1000 | Loss: 0.00001674
Iteration 30/1000 | Loss: 0.00001674
Iteration 31/1000 | Loss: 0.00001673
Iteration 32/1000 | Loss: 0.00001673
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001673
Iteration 35/1000 | Loss: 0.00001672
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001656
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001656
Iteration 52/1000 | Loss: 0.00001656
Iteration 53/1000 | Loss: 0.00001656
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001655
Iteration 57/1000 | Loss: 0.00001655
Iteration 58/1000 | Loss: 0.00001654
Iteration 59/1000 | Loss: 0.00001654
Iteration 60/1000 | Loss: 0.00001654
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001653
Iteration 63/1000 | Loss: 0.00001653
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001652
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001652
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00001651
Iteration 73/1000 | Loss: 0.00001651
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001644
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001637
Iteration 116/1000 | Loss: 0.00001637
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001636
Iteration 122/1000 | Loss: 0.00001636
Iteration 123/1000 | Loss: 0.00001636
Iteration 124/1000 | Loss: 0.00001636
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001635
Iteration 127/1000 | Loss: 0.00001635
Iteration 128/1000 | Loss: 0.00001634
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001633
Iteration 133/1000 | Loss: 0.00001633
Iteration 134/1000 | Loss: 0.00001633
Iteration 135/1000 | Loss: 0.00001632
Iteration 136/1000 | Loss: 0.00001632
Iteration 137/1000 | Loss: 0.00001632
Iteration 138/1000 | Loss: 0.00001632
Iteration 139/1000 | Loss: 0.00001632
Iteration 140/1000 | Loss: 0.00001632
Iteration 141/1000 | Loss: 0.00001632
Iteration 142/1000 | Loss: 0.00001632
Iteration 143/1000 | Loss: 0.00001632
Iteration 144/1000 | Loss: 0.00001632
Iteration 145/1000 | Loss: 0.00001632
Iteration 146/1000 | Loss: 0.00001631
Iteration 147/1000 | Loss: 0.00001631
Iteration 148/1000 | Loss: 0.00001631
Iteration 149/1000 | Loss: 0.00001631
Iteration 150/1000 | Loss: 0.00001631
Iteration 151/1000 | Loss: 0.00001631
Iteration 152/1000 | Loss: 0.00001630
Iteration 153/1000 | Loss: 0.00001630
Iteration 154/1000 | Loss: 0.00001630
Iteration 155/1000 | Loss: 0.00001630
Iteration 156/1000 | Loss: 0.00001630
Iteration 157/1000 | Loss: 0.00001630
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001629
Iteration 160/1000 | Loss: 0.00001629
Iteration 161/1000 | Loss: 0.00001629
Iteration 162/1000 | Loss: 0.00001629
Iteration 163/1000 | Loss: 0.00001629
Iteration 164/1000 | Loss: 0.00001629
Iteration 165/1000 | Loss: 0.00001629
Iteration 166/1000 | Loss: 0.00001629
Iteration 167/1000 | Loss: 0.00001629
Iteration 168/1000 | Loss: 0.00001628
Iteration 169/1000 | Loss: 0.00001628
Iteration 170/1000 | Loss: 0.00001627
Iteration 171/1000 | Loss: 0.00001627
Iteration 172/1000 | Loss: 0.00001627
Iteration 173/1000 | Loss: 0.00001627
Iteration 174/1000 | Loss: 0.00001627
Iteration 175/1000 | Loss: 0.00001627
Iteration 176/1000 | Loss: 0.00001627
Iteration 177/1000 | Loss: 0.00001627
Iteration 178/1000 | Loss: 0.00001627
Iteration 179/1000 | Loss: 0.00001627
Iteration 180/1000 | Loss: 0.00001627
Iteration 181/1000 | Loss: 0.00001627
Iteration 182/1000 | Loss: 0.00001627
Iteration 183/1000 | Loss: 0.00001627
Iteration 184/1000 | Loss: 0.00001627
Iteration 185/1000 | Loss: 0.00001627
Iteration 186/1000 | Loss: 0.00001627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.6273435903713107e-05, 1.6273435903713107e-05, 1.6273435903713107e-05, 1.6273435903713107e-05, 1.6273435903713107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6273435903713107e-05

Optimization complete. Final v2v error: 3.306657552719116 mm

Highest mean error: 3.9680590629577637 mm for frame 58

Lowest mean error: 3.0292465686798096 mm for frame 10

Saving results

Total time: 45.02811050415039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584672
Iteration 2/25 | Loss: 0.00134521
Iteration 3/25 | Loss: 0.00123952
Iteration 4/25 | Loss: 0.00121596
Iteration 5/25 | Loss: 0.00120972
Iteration 6/25 | Loss: 0.00120767
Iteration 7/25 | Loss: 0.00120767
Iteration 8/25 | Loss: 0.00120767
Iteration 9/25 | Loss: 0.00120767
Iteration 10/25 | Loss: 0.00120767
Iteration 11/25 | Loss: 0.00120767
Iteration 12/25 | Loss: 0.00120767
Iteration 13/25 | Loss: 0.00120767
Iteration 14/25 | Loss: 0.00120767
Iteration 15/25 | Loss: 0.00120767
Iteration 16/25 | Loss: 0.00120767
Iteration 17/25 | Loss: 0.00120767
Iteration 18/25 | Loss: 0.00120767
Iteration 19/25 | Loss: 0.00120767
Iteration 20/25 | Loss: 0.00120767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001207670196890831, 0.001207670196890831, 0.001207670196890831, 0.001207670196890831, 0.001207670196890831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001207670196890831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50010610
Iteration 2/25 | Loss: 0.00161972
Iteration 3/25 | Loss: 0.00161972
Iteration 4/25 | Loss: 0.00161972
Iteration 5/25 | Loss: 0.00161972
Iteration 6/25 | Loss: 0.00161972
Iteration 7/25 | Loss: 0.00161972
Iteration 8/25 | Loss: 0.00161972
Iteration 9/25 | Loss: 0.00161972
Iteration 10/25 | Loss: 0.00161972
Iteration 11/25 | Loss: 0.00161972
Iteration 12/25 | Loss: 0.00161972
Iteration 13/25 | Loss: 0.00161972
Iteration 14/25 | Loss: 0.00161972
Iteration 15/25 | Loss: 0.00161972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016197196673601866, 0.0016197196673601866, 0.0016197196673601866, 0.0016197196673601866, 0.0016197196673601866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016197196673601866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161972
Iteration 2/1000 | Loss: 0.00004008
Iteration 3/1000 | Loss: 0.00002646
Iteration 4/1000 | Loss: 0.00002371
Iteration 5/1000 | Loss: 0.00002208
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002039
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001917
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001891
Iteration 13/1000 | Loss: 0.00001889
Iteration 14/1000 | Loss: 0.00001888
Iteration 15/1000 | Loss: 0.00001887
Iteration 16/1000 | Loss: 0.00001887
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001886
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001880
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001873
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001850
Iteration 28/1000 | Loss: 0.00001846
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001844
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001830
Iteration 39/1000 | Loss: 0.00001829
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001828
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001824
Iteration 44/1000 | Loss: 0.00001824
Iteration 45/1000 | Loss: 0.00001824
Iteration 46/1000 | Loss: 0.00001823
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001821
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001817
Iteration 56/1000 | Loss: 0.00001817
Iteration 57/1000 | Loss: 0.00001816
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001815
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001813
Iteration 62/1000 | Loss: 0.00001810
Iteration 63/1000 | Loss: 0.00001810
Iteration 64/1000 | Loss: 0.00001809
Iteration 65/1000 | Loss: 0.00001809
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001807
Iteration 70/1000 | Loss: 0.00001807
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001807
Iteration 76/1000 | Loss: 0.00001807
Iteration 77/1000 | Loss: 0.00001807
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001805
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001804
Iteration 84/1000 | Loss: 0.00001804
Iteration 85/1000 | Loss: 0.00001804
Iteration 86/1000 | Loss: 0.00001803
Iteration 87/1000 | Loss: 0.00001803
Iteration 88/1000 | Loss: 0.00001803
Iteration 89/1000 | Loss: 0.00001803
Iteration 90/1000 | Loss: 0.00001803
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001801
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001801
Iteration 107/1000 | Loss: 0.00001801
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001801
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001800
Iteration 112/1000 | Loss: 0.00001800
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001800
Iteration 115/1000 | Loss: 0.00001800
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001800
Iteration 118/1000 | Loss: 0.00001800
Iteration 119/1000 | Loss: 0.00001800
Iteration 120/1000 | Loss: 0.00001800
Iteration 121/1000 | Loss: 0.00001800
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001800
Iteration 126/1000 | Loss: 0.00001800
Iteration 127/1000 | Loss: 0.00001800
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001797
Iteration 140/1000 | Loss: 0.00001797
Iteration 141/1000 | Loss: 0.00001797
Iteration 142/1000 | Loss: 0.00001797
Iteration 143/1000 | Loss: 0.00001797
Iteration 144/1000 | Loss: 0.00001797
Iteration 145/1000 | Loss: 0.00001797
Iteration 146/1000 | Loss: 0.00001797
Iteration 147/1000 | Loss: 0.00001797
Iteration 148/1000 | Loss: 0.00001797
Iteration 149/1000 | Loss: 0.00001796
Iteration 150/1000 | Loss: 0.00001796
Iteration 151/1000 | Loss: 0.00001796
Iteration 152/1000 | Loss: 0.00001796
Iteration 153/1000 | Loss: 0.00001796
Iteration 154/1000 | Loss: 0.00001796
Iteration 155/1000 | Loss: 0.00001796
Iteration 156/1000 | Loss: 0.00001795
Iteration 157/1000 | Loss: 0.00001795
Iteration 158/1000 | Loss: 0.00001795
Iteration 159/1000 | Loss: 0.00001795
Iteration 160/1000 | Loss: 0.00001795
Iteration 161/1000 | Loss: 0.00001795
Iteration 162/1000 | Loss: 0.00001795
Iteration 163/1000 | Loss: 0.00001795
Iteration 164/1000 | Loss: 0.00001795
Iteration 165/1000 | Loss: 0.00001795
Iteration 166/1000 | Loss: 0.00001795
Iteration 167/1000 | Loss: 0.00001795
Iteration 168/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.7951291738427244e-05, 1.7951291738427244e-05, 1.7951291738427244e-05, 1.7951291738427244e-05, 1.7951291738427244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7951291738427244e-05

Optimization complete. Final v2v error: 3.6657471656799316 mm

Highest mean error: 3.808056592941284 mm for frame 21

Lowest mean error: 3.494706630706787 mm for frame 239

Saving results

Total time: 46.96786451339722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893347
Iteration 2/25 | Loss: 0.00169750
Iteration 3/25 | Loss: 0.00146614
Iteration 4/25 | Loss: 0.00145418
Iteration 5/25 | Loss: 0.00145186
Iteration 6/25 | Loss: 0.00145186
Iteration 7/25 | Loss: 0.00145186
Iteration 8/25 | Loss: 0.00145186
Iteration 9/25 | Loss: 0.00145186
Iteration 10/25 | Loss: 0.00145186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014518630923703313, 0.0014518630923703313, 0.0014518630923703313, 0.0014518630923703313, 0.0014518630923703313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014518630923703313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54531604
Iteration 2/25 | Loss: 0.00097635
Iteration 3/25 | Loss: 0.00097635
Iteration 4/25 | Loss: 0.00097635
Iteration 5/25 | Loss: 0.00097635
Iteration 6/25 | Loss: 0.00097635
Iteration 7/25 | Loss: 0.00097635
Iteration 8/25 | Loss: 0.00097635
Iteration 9/25 | Loss: 0.00097635
Iteration 10/25 | Loss: 0.00097635
Iteration 11/25 | Loss: 0.00097635
Iteration 12/25 | Loss: 0.00097635
Iteration 13/25 | Loss: 0.00097635
Iteration 14/25 | Loss: 0.00097635
Iteration 15/25 | Loss: 0.00097635
Iteration 16/25 | Loss: 0.00097635
Iteration 17/25 | Loss: 0.00097635
Iteration 18/25 | Loss: 0.00097635
Iteration 19/25 | Loss: 0.00097635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009763503912836313, 0.0009763503912836313, 0.0009763503912836313, 0.0009763503912836313, 0.0009763503912836313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009763503912836313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097635
Iteration 2/1000 | Loss: 0.00006993
Iteration 3/1000 | Loss: 0.00004260
Iteration 4/1000 | Loss: 0.00003464
Iteration 5/1000 | Loss: 0.00003254
Iteration 6/1000 | Loss: 0.00003124
Iteration 7/1000 | Loss: 0.00003060
Iteration 8/1000 | Loss: 0.00002989
Iteration 9/1000 | Loss: 0.00002931
Iteration 10/1000 | Loss: 0.00002895
Iteration 11/1000 | Loss: 0.00002866
Iteration 12/1000 | Loss: 0.00002841
Iteration 13/1000 | Loss: 0.00002820
Iteration 14/1000 | Loss: 0.00002802
Iteration 15/1000 | Loss: 0.00002798
Iteration 16/1000 | Loss: 0.00002794
Iteration 17/1000 | Loss: 0.00002789
Iteration 18/1000 | Loss: 0.00002789
Iteration 19/1000 | Loss: 0.00002789
Iteration 20/1000 | Loss: 0.00002788
Iteration 21/1000 | Loss: 0.00002787
Iteration 22/1000 | Loss: 0.00002787
Iteration 23/1000 | Loss: 0.00002787
Iteration 24/1000 | Loss: 0.00002787
Iteration 25/1000 | Loss: 0.00002786
Iteration 26/1000 | Loss: 0.00002786
Iteration 27/1000 | Loss: 0.00002786
Iteration 28/1000 | Loss: 0.00002786
Iteration 29/1000 | Loss: 0.00002786
Iteration 30/1000 | Loss: 0.00002786
Iteration 31/1000 | Loss: 0.00002786
Iteration 32/1000 | Loss: 0.00002786
Iteration 33/1000 | Loss: 0.00002786
Iteration 34/1000 | Loss: 0.00002786
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002785
Iteration 37/1000 | Loss: 0.00002785
Iteration 38/1000 | Loss: 0.00002785
Iteration 39/1000 | Loss: 0.00002785
Iteration 40/1000 | Loss: 0.00002785
Iteration 41/1000 | Loss: 0.00002785
Iteration 42/1000 | Loss: 0.00002785
Iteration 43/1000 | Loss: 0.00002785
Iteration 44/1000 | Loss: 0.00002785
Iteration 45/1000 | Loss: 0.00002784
Iteration 46/1000 | Loss: 0.00002784
Iteration 47/1000 | Loss: 0.00002784
Iteration 48/1000 | Loss: 0.00002784
Iteration 49/1000 | Loss: 0.00002783
Iteration 50/1000 | Loss: 0.00002783
Iteration 51/1000 | Loss: 0.00002783
Iteration 52/1000 | Loss: 0.00002783
Iteration 53/1000 | Loss: 0.00002783
Iteration 54/1000 | Loss: 0.00002783
Iteration 55/1000 | Loss: 0.00002783
Iteration 56/1000 | Loss: 0.00002783
Iteration 57/1000 | Loss: 0.00002783
Iteration 58/1000 | Loss: 0.00002782
Iteration 59/1000 | Loss: 0.00002782
Iteration 60/1000 | Loss: 0.00002782
Iteration 61/1000 | Loss: 0.00002782
Iteration 62/1000 | Loss: 0.00002782
Iteration 63/1000 | Loss: 0.00002782
Iteration 64/1000 | Loss: 0.00002782
Iteration 65/1000 | Loss: 0.00002782
Iteration 66/1000 | Loss: 0.00002782
Iteration 67/1000 | Loss: 0.00002782
Iteration 68/1000 | Loss: 0.00002781
Iteration 69/1000 | Loss: 0.00002781
Iteration 70/1000 | Loss: 0.00002781
Iteration 71/1000 | Loss: 0.00002781
Iteration 72/1000 | Loss: 0.00002781
Iteration 73/1000 | Loss: 0.00002781
Iteration 74/1000 | Loss: 0.00002781
Iteration 75/1000 | Loss: 0.00002780
Iteration 76/1000 | Loss: 0.00002780
Iteration 77/1000 | Loss: 0.00002780
Iteration 78/1000 | Loss: 0.00002780
Iteration 79/1000 | Loss: 0.00002780
Iteration 80/1000 | Loss: 0.00002780
Iteration 81/1000 | Loss: 0.00002780
Iteration 82/1000 | Loss: 0.00002780
Iteration 83/1000 | Loss: 0.00002780
Iteration 84/1000 | Loss: 0.00002780
Iteration 85/1000 | Loss: 0.00002780
Iteration 86/1000 | Loss: 0.00002780
Iteration 87/1000 | Loss: 0.00002780
Iteration 88/1000 | Loss: 0.00002780
Iteration 89/1000 | Loss: 0.00002779
Iteration 90/1000 | Loss: 0.00002779
Iteration 91/1000 | Loss: 0.00002779
Iteration 92/1000 | Loss: 0.00002779
Iteration 93/1000 | Loss: 0.00002779
Iteration 94/1000 | Loss: 0.00002779
Iteration 95/1000 | Loss: 0.00002779
Iteration 96/1000 | Loss: 0.00002779
Iteration 97/1000 | Loss: 0.00002779
Iteration 98/1000 | Loss: 0.00002779
Iteration 99/1000 | Loss: 0.00002779
Iteration 100/1000 | Loss: 0.00002778
Iteration 101/1000 | Loss: 0.00002778
Iteration 102/1000 | Loss: 0.00002778
Iteration 103/1000 | Loss: 0.00002778
Iteration 104/1000 | Loss: 0.00002778
Iteration 105/1000 | Loss: 0.00002777
Iteration 106/1000 | Loss: 0.00002777
Iteration 107/1000 | Loss: 0.00002777
Iteration 108/1000 | Loss: 0.00002777
Iteration 109/1000 | Loss: 0.00002777
Iteration 110/1000 | Loss: 0.00002777
Iteration 111/1000 | Loss: 0.00002777
Iteration 112/1000 | Loss: 0.00002776
Iteration 113/1000 | Loss: 0.00002776
Iteration 114/1000 | Loss: 0.00002776
Iteration 115/1000 | Loss: 0.00002776
Iteration 116/1000 | Loss: 0.00002775
Iteration 117/1000 | Loss: 0.00002774
Iteration 118/1000 | Loss: 0.00002774
Iteration 119/1000 | Loss: 0.00002774
Iteration 120/1000 | Loss: 0.00002774
Iteration 121/1000 | Loss: 0.00002774
Iteration 122/1000 | Loss: 0.00002774
Iteration 123/1000 | Loss: 0.00002773
Iteration 124/1000 | Loss: 0.00002773
Iteration 125/1000 | Loss: 0.00002773
Iteration 126/1000 | Loss: 0.00002773
Iteration 127/1000 | Loss: 0.00002773
Iteration 128/1000 | Loss: 0.00002773
Iteration 129/1000 | Loss: 0.00002773
Iteration 130/1000 | Loss: 0.00002773
Iteration 131/1000 | Loss: 0.00002773
Iteration 132/1000 | Loss: 0.00002773
Iteration 133/1000 | Loss: 0.00002772
Iteration 134/1000 | Loss: 0.00002772
Iteration 135/1000 | Loss: 0.00002772
Iteration 136/1000 | Loss: 0.00002772
Iteration 137/1000 | Loss: 0.00002772
Iteration 138/1000 | Loss: 0.00002772
Iteration 139/1000 | Loss: 0.00002772
Iteration 140/1000 | Loss: 0.00002772
Iteration 141/1000 | Loss: 0.00002772
Iteration 142/1000 | Loss: 0.00002772
Iteration 143/1000 | Loss: 0.00002772
Iteration 144/1000 | Loss: 0.00002772
Iteration 145/1000 | Loss: 0.00002772
Iteration 146/1000 | Loss: 0.00002772
Iteration 147/1000 | Loss: 0.00002772
Iteration 148/1000 | Loss: 0.00002772
Iteration 149/1000 | Loss: 0.00002772
Iteration 150/1000 | Loss: 0.00002772
Iteration 151/1000 | Loss: 0.00002772
Iteration 152/1000 | Loss: 0.00002772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.771859180938918e-05, 2.771859180938918e-05, 2.771859180938918e-05, 2.771859180938918e-05, 2.771859180938918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.771859180938918e-05

Optimization complete. Final v2v error: 4.383355140686035 mm

Highest mean error: 4.986663341522217 mm for frame 17

Lowest mean error: 4.13195276260376 mm for frame 33

Saving results

Total time: 38.10005593299866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482730
Iteration 2/25 | Loss: 0.00129996
Iteration 3/25 | Loss: 0.00123445
Iteration 4/25 | Loss: 0.00122645
Iteration 5/25 | Loss: 0.00122428
Iteration 6/25 | Loss: 0.00122428
Iteration 7/25 | Loss: 0.00122428
Iteration 8/25 | Loss: 0.00122428
Iteration 9/25 | Loss: 0.00122428
Iteration 10/25 | Loss: 0.00122428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012242808006703854, 0.0012242808006703854, 0.0012242808006703854, 0.0012242808006703854, 0.0012242808006703854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012242808006703854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71964931
Iteration 2/25 | Loss: 0.00096896
Iteration 3/25 | Loss: 0.00096896
Iteration 4/25 | Loss: 0.00096895
Iteration 5/25 | Loss: 0.00096895
Iteration 6/25 | Loss: 0.00096895
Iteration 7/25 | Loss: 0.00096895
Iteration 8/25 | Loss: 0.00096895
Iteration 9/25 | Loss: 0.00096895
Iteration 10/25 | Loss: 0.00096895
Iteration 11/25 | Loss: 0.00096895
Iteration 12/25 | Loss: 0.00096895
Iteration 13/25 | Loss: 0.00096895
Iteration 14/25 | Loss: 0.00096895
Iteration 15/25 | Loss: 0.00096895
Iteration 16/25 | Loss: 0.00096895
Iteration 17/25 | Loss: 0.00096895
Iteration 18/25 | Loss: 0.00096895
Iteration 19/25 | Loss: 0.00096895
Iteration 20/25 | Loss: 0.00096895
Iteration 21/25 | Loss: 0.00096895
Iteration 22/25 | Loss: 0.00096895
Iteration 23/25 | Loss: 0.00096895
Iteration 24/25 | Loss: 0.00096895
Iteration 25/25 | Loss: 0.00096895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096895
Iteration 2/1000 | Loss: 0.00002463
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001719
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001559
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001477
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001422
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001414
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001397
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001372
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001364
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001353
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001321
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001321
Iteration 103/1000 | Loss: 0.00001321
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001320
Iteration 106/1000 | Loss: 0.00001320
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001319
Iteration 117/1000 | Loss: 0.00001319
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001316
Iteration 125/1000 | Loss: 0.00001316
Iteration 126/1000 | Loss: 0.00001316
Iteration 127/1000 | Loss: 0.00001316
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001315
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001314
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001310
Iteration 151/1000 | Loss: 0.00001310
Iteration 152/1000 | Loss: 0.00001310
Iteration 153/1000 | Loss: 0.00001310
Iteration 154/1000 | Loss: 0.00001310
Iteration 155/1000 | Loss: 0.00001310
Iteration 156/1000 | Loss: 0.00001310
Iteration 157/1000 | Loss: 0.00001310
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001309
Iteration 164/1000 | Loss: 0.00001309
Iteration 165/1000 | Loss: 0.00001309
Iteration 166/1000 | Loss: 0.00001309
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Iteration 171/1000 | Loss: 0.00001308
Iteration 172/1000 | Loss: 0.00001308
Iteration 173/1000 | Loss: 0.00001308
Iteration 174/1000 | Loss: 0.00001308
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001307
Iteration 184/1000 | Loss: 0.00001307
Iteration 185/1000 | Loss: 0.00001307
Iteration 186/1000 | Loss: 0.00001307
Iteration 187/1000 | Loss: 0.00001307
Iteration 188/1000 | Loss: 0.00001307
Iteration 189/1000 | Loss: 0.00001307
Iteration 190/1000 | Loss: 0.00001307
Iteration 191/1000 | Loss: 0.00001306
Iteration 192/1000 | Loss: 0.00001306
Iteration 193/1000 | Loss: 0.00001306
Iteration 194/1000 | Loss: 0.00001306
Iteration 195/1000 | Loss: 0.00001306
Iteration 196/1000 | Loss: 0.00001306
Iteration 197/1000 | Loss: 0.00001306
Iteration 198/1000 | Loss: 0.00001306
Iteration 199/1000 | Loss: 0.00001306
Iteration 200/1000 | Loss: 0.00001306
Iteration 201/1000 | Loss: 0.00001306
Iteration 202/1000 | Loss: 0.00001306
Iteration 203/1000 | Loss: 0.00001306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.3060134733677842e-05, 1.3060134733677842e-05, 1.3060134733677842e-05, 1.3060134733677842e-05, 1.3060134733677842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3060134733677842e-05

Optimization complete. Final v2v error: 3.0625834465026855 mm

Highest mean error: 3.4898622035980225 mm for frame 191

Lowest mean error: 2.779426097869873 mm for frame 213

Saving results

Total time: 46.64086413383484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957932
Iteration 2/25 | Loss: 0.00160428
Iteration 3/25 | Loss: 0.00138398
Iteration 4/25 | Loss: 0.00130868
Iteration 5/25 | Loss: 0.00129746
Iteration 6/25 | Loss: 0.00127772
Iteration 7/25 | Loss: 0.00125885
Iteration 8/25 | Loss: 0.00125203
Iteration 9/25 | Loss: 0.00124692
Iteration 10/25 | Loss: 0.00124454
Iteration 11/25 | Loss: 0.00124316
Iteration 12/25 | Loss: 0.00124093
Iteration 13/25 | Loss: 0.00123996
Iteration 14/25 | Loss: 0.00124074
Iteration 15/25 | Loss: 0.00124022
Iteration 16/25 | Loss: 0.00123943
Iteration 17/25 | Loss: 0.00123878
Iteration 18/25 | Loss: 0.00123859
Iteration 19/25 | Loss: 0.00123854
Iteration 20/25 | Loss: 0.00123853
Iteration 21/25 | Loss: 0.00123853
Iteration 22/25 | Loss: 0.00123853
Iteration 23/25 | Loss: 0.00123853
Iteration 24/25 | Loss: 0.00123853
Iteration 25/25 | Loss: 0.00123853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21656418
Iteration 2/25 | Loss: 0.00095079
Iteration 3/25 | Loss: 0.00095078
Iteration 4/25 | Loss: 0.00095078
Iteration 5/25 | Loss: 0.00095078
Iteration 6/25 | Loss: 0.00095078
Iteration 7/25 | Loss: 0.00095078
Iteration 8/25 | Loss: 0.00095078
Iteration 9/25 | Loss: 0.00095078
Iteration 10/25 | Loss: 0.00095078
Iteration 11/25 | Loss: 0.00095078
Iteration 12/25 | Loss: 0.00095078
Iteration 13/25 | Loss: 0.00095078
Iteration 14/25 | Loss: 0.00095078
Iteration 15/25 | Loss: 0.00095078
Iteration 16/25 | Loss: 0.00095078
Iteration 17/25 | Loss: 0.00095078
Iteration 18/25 | Loss: 0.00095078
Iteration 19/25 | Loss: 0.00095078
Iteration 20/25 | Loss: 0.00095078
Iteration 21/25 | Loss: 0.00095078
Iteration 22/25 | Loss: 0.00095078
Iteration 23/25 | Loss: 0.00095078
Iteration 24/25 | Loss: 0.00095078
Iteration 25/25 | Loss: 0.00095078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095078
Iteration 2/1000 | Loss: 0.00003401
Iteration 3/1000 | Loss: 0.00002249
Iteration 4/1000 | Loss: 0.00002022
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001863
Iteration 7/1000 | Loss: 0.00001817
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001732
Iteration 11/1000 | Loss: 0.00001731
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001696
Iteration 14/1000 | Loss: 0.00001693
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001675
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001674
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001666
Iteration 26/1000 | Loss: 0.00001664
Iteration 27/1000 | Loss: 0.00001664
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001656
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001652
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001648
Iteration 49/1000 | Loss: 0.00001648
Iteration 50/1000 | Loss: 0.00001648
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001647
Iteration 56/1000 | Loss: 0.00001647
Iteration 57/1000 | Loss: 0.00001647
Iteration 58/1000 | Loss: 0.00001647
Iteration 59/1000 | Loss: 0.00001647
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001647
Iteration 75/1000 | Loss: 0.00001647
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001647
Iteration 79/1000 | Loss: 0.00001647
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.6468340618303046e-05, 1.6468340618303046e-05, 1.6468340618303046e-05, 1.6468340618303046e-05, 1.6468340618303046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6468340618303046e-05

Optimization complete. Final v2v error: 3.4837563037872314 mm

Highest mean error: 4.062172889709473 mm for frame 0

Lowest mean error: 2.8919098377227783 mm for frame 61

Saving results

Total time: 63.92971181869507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795865
Iteration 2/25 | Loss: 0.00136856
Iteration 3/25 | Loss: 0.00127398
Iteration 4/25 | Loss: 0.00126276
Iteration 5/25 | Loss: 0.00125974
Iteration 6/25 | Loss: 0.00125941
Iteration 7/25 | Loss: 0.00125941
Iteration 8/25 | Loss: 0.00125941
Iteration 9/25 | Loss: 0.00125941
Iteration 10/25 | Loss: 0.00125941
Iteration 11/25 | Loss: 0.00125941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012594078434631228, 0.0012594078434631228, 0.0012594078434631228, 0.0012594078434631228, 0.0012594078434631228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012594078434631228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40888333
Iteration 2/25 | Loss: 0.00093751
Iteration 3/25 | Loss: 0.00093751
Iteration 4/25 | Loss: 0.00093751
Iteration 5/25 | Loss: 0.00093751
Iteration 6/25 | Loss: 0.00093751
Iteration 7/25 | Loss: 0.00093751
Iteration 8/25 | Loss: 0.00093751
Iteration 9/25 | Loss: 0.00093751
Iteration 10/25 | Loss: 0.00093751
Iteration 11/25 | Loss: 0.00093751
Iteration 12/25 | Loss: 0.00093751
Iteration 13/25 | Loss: 0.00093751
Iteration 14/25 | Loss: 0.00093751
Iteration 15/25 | Loss: 0.00093751
Iteration 16/25 | Loss: 0.00093751
Iteration 17/25 | Loss: 0.00093751
Iteration 18/25 | Loss: 0.00093751
Iteration 19/25 | Loss: 0.00093751
Iteration 20/25 | Loss: 0.00093751
Iteration 21/25 | Loss: 0.00093751
Iteration 22/25 | Loss: 0.00093751
Iteration 23/25 | Loss: 0.00093751
Iteration 24/25 | Loss: 0.00093751
Iteration 25/25 | Loss: 0.00093751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093751
Iteration 2/1000 | Loss: 0.00004126
Iteration 3/1000 | Loss: 0.00002648
Iteration 4/1000 | Loss: 0.00002167
Iteration 5/1000 | Loss: 0.00002049
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001903
Iteration 8/1000 | Loss: 0.00001859
Iteration 9/1000 | Loss: 0.00001830
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001786
Iteration 12/1000 | Loss: 0.00001769
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001750
Iteration 16/1000 | Loss: 0.00001749
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001733
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001708
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001707
Iteration 28/1000 | Loss: 0.00001707
Iteration 29/1000 | Loss: 0.00001706
Iteration 30/1000 | Loss: 0.00001706
Iteration 31/1000 | Loss: 0.00001705
Iteration 32/1000 | Loss: 0.00001704
Iteration 33/1000 | Loss: 0.00001704
Iteration 34/1000 | Loss: 0.00001703
Iteration 35/1000 | Loss: 0.00001703
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001696
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001695
Iteration 56/1000 | Loss: 0.00001694
Iteration 57/1000 | Loss: 0.00001694
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00001693
Iteration 60/1000 | Loss: 0.00001693
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001691
Iteration 67/1000 | Loss: 0.00001690
Iteration 68/1000 | Loss: 0.00001690
Iteration 69/1000 | Loss: 0.00001690
Iteration 70/1000 | Loss: 0.00001690
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001690
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001688
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001683
Iteration 88/1000 | Loss: 0.00001683
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001676
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001675
Iteration 107/1000 | Loss: 0.00001675
Iteration 108/1000 | Loss: 0.00001675
Iteration 109/1000 | Loss: 0.00001675
Iteration 110/1000 | Loss: 0.00001675
Iteration 111/1000 | Loss: 0.00001675
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001674
Iteration 116/1000 | Loss: 0.00001674
Iteration 117/1000 | Loss: 0.00001674
Iteration 118/1000 | Loss: 0.00001674
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001672
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001672
Iteration 127/1000 | Loss: 0.00001672
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001672
Iteration 134/1000 | Loss: 0.00001672
Iteration 135/1000 | Loss: 0.00001672
Iteration 136/1000 | Loss: 0.00001672
Iteration 137/1000 | Loss: 0.00001672
Iteration 138/1000 | Loss: 0.00001672
Iteration 139/1000 | Loss: 0.00001672
Iteration 140/1000 | Loss: 0.00001672
Iteration 141/1000 | Loss: 0.00001672
Iteration 142/1000 | Loss: 0.00001672
Iteration 143/1000 | Loss: 0.00001672
Iteration 144/1000 | Loss: 0.00001672
Iteration 145/1000 | Loss: 0.00001672
Iteration 146/1000 | Loss: 0.00001672
Iteration 147/1000 | Loss: 0.00001672
Iteration 148/1000 | Loss: 0.00001671
Iteration 149/1000 | Loss: 0.00001671
Iteration 150/1000 | Loss: 0.00001671
Iteration 151/1000 | Loss: 0.00001671
Iteration 152/1000 | Loss: 0.00001671
Iteration 153/1000 | Loss: 0.00001671
Iteration 154/1000 | Loss: 0.00001671
Iteration 155/1000 | Loss: 0.00001671
Iteration 156/1000 | Loss: 0.00001671
Iteration 157/1000 | Loss: 0.00001671
Iteration 158/1000 | Loss: 0.00001671
Iteration 159/1000 | Loss: 0.00001671
Iteration 160/1000 | Loss: 0.00001671
Iteration 161/1000 | Loss: 0.00001671
Iteration 162/1000 | Loss: 0.00001671
Iteration 163/1000 | Loss: 0.00001671
Iteration 164/1000 | Loss: 0.00001671
Iteration 165/1000 | Loss: 0.00001671
Iteration 166/1000 | Loss: 0.00001671
Iteration 167/1000 | Loss: 0.00001671
Iteration 168/1000 | Loss: 0.00001671
Iteration 169/1000 | Loss: 0.00001671
Iteration 170/1000 | Loss: 0.00001671
Iteration 171/1000 | Loss: 0.00001671
Iteration 172/1000 | Loss: 0.00001671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.67141424753936e-05, 1.67141424753936e-05, 1.67141424753936e-05, 1.67141424753936e-05, 1.67141424753936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.67141424753936e-05

Optimization complete. Final v2v error: 3.4186041355133057 mm

Highest mean error: 3.8755414485931396 mm for frame 97

Lowest mean error: 2.76031231880188 mm for frame 1

Saving results

Total time: 40.69978213310242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792988
Iteration 2/25 | Loss: 0.00208386
Iteration 3/25 | Loss: 0.00138867
Iteration 4/25 | Loss: 0.00132445
Iteration 5/25 | Loss: 0.00131683
Iteration 6/25 | Loss: 0.00131584
Iteration 7/25 | Loss: 0.00131584
Iteration 8/25 | Loss: 0.00131584
Iteration 9/25 | Loss: 0.00131584
Iteration 10/25 | Loss: 0.00131584
Iteration 11/25 | Loss: 0.00131584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013158449437469244, 0.0013158449437469244, 0.0013158449437469244, 0.0013158449437469244, 0.0013158449437469244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013158449437469244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28027785
Iteration 2/25 | Loss: 0.00090404
Iteration 3/25 | Loss: 0.00090404
Iteration 4/25 | Loss: 0.00090404
Iteration 5/25 | Loss: 0.00090404
Iteration 6/25 | Loss: 0.00090404
Iteration 7/25 | Loss: 0.00090404
Iteration 8/25 | Loss: 0.00090404
Iteration 9/25 | Loss: 0.00090404
Iteration 10/25 | Loss: 0.00090404
Iteration 11/25 | Loss: 0.00090404
Iteration 12/25 | Loss: 0.00090404
Iteration 13/25 | Loss: 0.00090404
Iteration 14/25 | Loss: 0.00090404
Iteration 15/25 | Loss: 0.00090404
Iteration 16/25 | Loss: 0.00090404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009040375589393079, 0.0009040375589393079, 0.0009040375589393079, 0.0009040375589393079, 0.0009040375589393079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009040375589393079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090404
Iteration 2/1000 | Loss: 0.00004816
Iteration 3/1000 | Loss: 0.00002983
Iteration 4/1000 | Loss: 0.00002592
Iteration 5/1000 | Loss: 0.00002459
Iteration 6/1000 | Loss: 0.00002363
Iteration 7/1000 | Loss: 0.00002301
Iteration 8/1000 | Loss: 0.00002260
Iteration 9/1000 | Loss: 0.00002224
Iteration 10/1000 | Loss: 0.00002197
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002155
Iteration 13/1000 | Loss: 0.00002136
Iteration 14/1000 | Loss: 0.00002121
Iteration 15/1000 | Loss: 0.00002120
Iteration 16/1000 | Loss: 0.00002115
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002103
Iteration 19/1000 | Loss: 0.00002102
Iteration 20/1000 | Loss: 0.00002101
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00002088
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002088
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002085
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002082
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002080
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002080
Iteration 38/1000 | Loss: 0.00002080
Iteration 39/1000 | Loss: 0.00002080
Iteration 40/1000 | Loss: 0.00002080
Iteration 41/1000 | Loss: 0.00002080
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002079
Iteration 44/1000 | Loss: 0.00002079
Iteration 45/1000 | Loss: 0.00002079
Iteration 46/1000 | Loss: 0.00002077
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002077
Iteration 49/1000 | Loss: 0.00002077
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002076
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002074
Iteration 57/1000 | Loss: 0.00002074
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002072
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002070
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002069
Iteration 85/1000 | Loss: 0.00002069
Iteration 86/1000 | Loss: 0.00002069
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002067
Iteration 115/1000 | Loss: 0.00002067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.0669100194936618e-05, 2.0669100194936618e-05, 2.0669100194936618e-05, 2.0669100194936618e-05, 2.0669100194936618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0669100194936618e-05

Optimization complete. Final v2v error: 3.7657253742218018 mm

Highest mean error: 4.6731719970703125 mm for frame 111

Lowest mean error: 3.338175058364868 mm for frame 98

Saving results

Total time: 43.42552042007446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775279
Iteration 2/25 | Loss: 0.00159473
Iteration 3/25 | Loss: 0.00142720
Iteration 4/25 | Loss: 0.00140995
Iteration 5/25 | Loss: 0.00140561
Iteration 6/25 | Loss: 0.00140463
Iteration 7/25 | Loss: 0.00140463
Iteration 8/25 | Loss: 0.00140463
Iteration 9/25 | Loss: 0.00140463
Iteration 10/25 | Loss: 0.00140463
Iteration 11/25 | Loss: 0.00140463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001404634560458362, 0.001404634560458362, 0.001404634560458362, 0.001404634560458362, 0.001404634560458362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001404634560458362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35750985
Iteration 2/25 | Loss: 0.00110021
Iteration 3/25 | Loss: 0.00110018
Iteration 4/25 | Loss: 0.00110017
Iteration 5/25 | Loss: 0.00110017
Iteration 6/25 | Loss: 0.00110017
Iteration 7/25 | Loss: 0.00110017
Iteration 8/25 | Loss: 0.00110017
Iteration 9/25 | Loss: 0.00110017
Iteration 10/25 | Loss: 0.00110017
Iteration 11/25 | Loss: 0.00110017
Iteration 12/25 | Loss: 0.00110017
Iteration 13/25 | Loss: 0.00110017
Iteration 14/25 | Loss: 0.00110017
Iteration 15/25 | Loss: 0.00110017
Iteration 16/25 | Loss: 0.00110017
Iteration 17/25 | Loss: 0.00110017
Iteration 18/25 | Loss: 0.00110017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011001711245626211, 0.0011001711245626211, 0.0011001711245626211, 0.0011001711245626211, 0.0011001711245626211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011001711245626211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110017
Iteration 2/1000 | Loss: 0.00005797
Iteration 3/1000 | Loss: 0.00003679
Iteration 4/1000 | Loss: 0.00003104
Iteration 5/1000 | Loss: 0.00002934
Iteration 6/1000 | Loss: 0.00002884
Iteration 7/1000 | Loss: 0.00002846
Iteration 8/1000 | Loss: 0.00002811
Iteration 9/1000 | Loss: 0.00002781
Iteration 10/1000 | Loss: 0.00002763
Iteration 11/1000 | Loss: 0.00002755
Iteration 12/1000 | Loss: 0.00002737
Iteration 13/1000 | Loss: 0.00002722
Iteration 14/1000 | Loss: 0.00002720
Iteration 15/1000 | Loss: 0.00002720
Iteration 16/1000 | Loss: 0.00002715
Iteration 17/1000 | Loss: 0.00002715
Iteration 18/1000 | Loss: 0.00002713
Iteration 19/1000 | Loss: 0.00002713
Iteration 20/1000 | Loss: 0.00002711
Iteration 21/1000 | Loss: 0.00002709
Iteration 22/1000 | Loss: 0.00002707
Iteration 23/1000 | Loss: 0.00002707
Iteration 24/1000 | Loss: 0.00002706
Iteration 25/1000 | Loss: 0.00002703
Iteration 26/1000 | Loss: 0.00002701
Iteration 27/1000 | Loss: 0.00002700
Iteration 28/1000 | Loss: 0.00002700
Iteration 29/1000 | Loss: 0.00002699
Iteration 30/1000 | Loss: 0.00002698
Iteration 31/1000 | Loss: 0.00002696
Iteration 32/1000 | Loss: 0.00002687
Iteration 33/1000 | Loss: 0.00002687
Iteration 34/1000 | Loss: 0.00002686
Iteration 35/1000 | Loss: 0.00002686
Iteration 36/1000 | Loss: 0.00002682
Iteration 37/1000 | Loss: 0.00002678
Iteration 38/1000 | Loss: 0.00002676
Iteration 39/1000 | Loss: 0.00002676
Iteration 40/1000 | Loss: 0.00002675
Iteration 41/1000 | Loss: 0.00002675
Iteration 42/1000 | Loss: 0.00002672
Iteration 43/1000 | Loss: 0.00002671
Iteration 44/1000 | Loss: 0.00002671
Iteration 45/1000 | Loss: 0.00002671
Iteration 46/1000 | Loss: 0.00002671
Iteration 47/1000 | Loss: 0.00002670
Iteration 48/1000 | Loss: 0.00002670
Iteration 49/1000 | Loss: 0.00002669
Iteration 50/1000 | Loss: 0.00002668
Iteration 51/1000 | Loss: 0.00002666
Iteration 52/1000 | Loss: 0.00002666
Iteration 53/1000 | Loss: 0.00002665
Iteration 54/1000 | Loss: 0.00002665
Iteration 55/1000 | Loss: 0.00002665
Iteration 56/1000 | Loss: 0.00002664
Iteration 57/1000 | Loss: 0.00002664
Iteration 58/1000 | Loss: 0.00002664
Iteration 59/1000 | Loss: 0.00002664
Iteration 60/1000 | Loss: 0.00002663
Iteration 61/1000 | Loss: 0.00002662
Iteration 62/1000 | Loss: 0.00002661
Iteration 63/1000 | Loss: 0.00002661
Iteration 64/1000 | Loss: 0.00002660
Iteration 65/1000 | Loss: 0.00002660
Iteration 66/1000 | Loss: 0.00002660
Iteration 67/1000 | Loss: 0.00002660
Iteration 68/1000 | Loss: 0.00002659
Iteration 69/1000 | Loss: 0.00002659
Iteration 70/1000 | Loss: 0.00002659
Iteration 71/1000 | Loss: 0.00002659
Iteration 72/1000 | Loss: 0.00002658
Iteration 73/1000 | Loss: 0.00002658
Iteration 74/1000 | Loss: 0.00002657
Iteration 75/1000 | Loss: 0.00002657
Iteration 76/1000 | Loss: 0.00002657
Iteration 77/1000 | Loss: 0.00002656
Iteration 78/1000 | Loss: 0.00002656
Iteration 79/1000 | Loss: 0.00002655
Iteration 80/1000 | Loss: 0.00002655
Iteration 81/1000 | Loss: 0.00002655
Iteration 82/1000 | Loss: 0.00002655
Iteration 83/1000 | Loss: 0.00002655
Iteration 84/1000 | Loss: 0.00002655
Iteration 85/1000 | Loss: 0.00002655
Iteration 86/1000 | Loss: 0.00002655
Iteration 87/1000 | Loss: 0.00002655
Iteration 88/1000 | Loss: 0.00002654
Iteration 89/1000 | Loss: 0.00002654
Iteration 90/1000 | Loss: 0.00002654
Iteration 91/1000 | Loss: 0.00002654
Iteration 92/1000 | Loss: 0.00002653
Iteration 93/1000 | Loss: 0.00002653
Iteration 94/1000 | Loss: 0.00002653
Iteration 95/1000 | Loss: 0.00002653
Iteration 96/1000 | Loss: 0.00002653
Iteration 97/1000 | Loss: 0.00002653
Iteration 98/1000 | Loss: 0.00002653
Iteration 99/1000 | Loss: 0.00002653
Iteration 100/1000 | Loss: 0.00002652
Iteration 101/1000 | Loss: 0.00002652
Iteration 102/1000 | Loss: 0.00002652
Iteration 103/1000 | Loss: 0.00002652
Iteration 104/1000 | Loss: 0.00002652
Iteration 105/1000 | Loss: 0.00002652
Iteration 106/1000 | Loss: 0.00002652
Iteration 107/1000 | Loss: 0.00002652
Iteration 108/1000 | Loss: 0.00002652
Iteration 109/1000 | Loss: 0.00002652
Iteration 110/1000 | Loss: 0.00002652
Iteration 111/1000 | Loss: 0.00002652
Iteration 112/1000 | Loss: 0.00002652
Iteration 113/1000 | Loss: 0.00002652
Iteration 114/1000 | Loss: 0.00002652
Iteration 115/1000 | Loss: 0.00002652
Iteration 116/1000 | Loss: 0.00002652
Iteration 117/1000 | Loss: 0.00002652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.6524588975007646e-05, 2.6524588975007646e-05, 2.6524588975007646e-05, 2.6524588975007646e-05, 2.6524588975007646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6524588975007646e-05

Optimization complete. Final v2v error: 4.152426719665527 mm

Highest mean error: 4.512722969055176 mm for frame 72

Lowest mean error: 3.252514600753784 mm for frame 0

Saving results

Total time: 39.68902778625488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852651
Iteration 2/25 | Loss: 0.00280838
Iteration 3/25 | Loss: 0.00183586
Iteration 4/25 | Loss: 0.00166722
Iteration 5/25 | Loss: 0.00161013
Iteration 6/25 | Loss: 0.00155556
Iteration 7/25 | Loss: 0.00154373
Iteration 8/25 | Loss: 0.00150037
Iteration 9/25 | Loss: 0.00149345
Iteration 10/25 | Loss: 0.00148760
Iteration 11/25 | Loss: 0.00149134
Iteration 12/25 | Loss: 0.00148829
Iteration 13/25 | Loss: 0.00148866
Iteration 14/25 | Loss: 0.00148413
Iteration 15/25 | Loss: 0.00148355
Iteration 16/25 | Loss: 0.00148149
Iteration 17/25 | Loss: 0.00148117
Iteration 18/25 | Loss: 0.00148087
Iteration 19/25 | Loss: 0.00148069
Iteration 20/25 | Loss: 0.00148053
Iteration 21/25 | Loss: 0.00148038
Iteration 22/25 | Loss: 0.00148024
Iteration 23/25 | Loss: 0.00148012
Iteration 24/25 | Loss: 0.00148008
Iteration 25/25 | Loss: 0.00148007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02706623
Iteration 2/25 | Loss: 0.00509706
Iteration 3/25 | Loss: 0.00285615
Iteration 4/25 | Loss: 0.00285615
Iteration 5/25 | Loss: 0.00285615
Iteration 6/25 | Loss: 0.00285615
Iteration 7/25 | Loss: 0.00285614
Iteration 8/25 | Loss: 0.00285614
Iteration 9/25 | Loss: 0.00285614
Iteration 10/25 | Loss: 0.00285614
Iteration 11/25 | Loss: 0.00285614
Iteration 12/25 | Loss: 0.00285614
Iteration 13/25 | Loss: 0.00285614
Iteration 14/25 | Loss: 0.00285614
Iteration 15/25 | Loss: 0.00285614
Iteration 16/25 | Loss: 0.00285614
Iteration 17/25 | Loss: 0.00285614
Iteration 18/25 | Loss: 0.00285614
Iteration 19/25 | Loss: 0.00285614
Iteration 20/25 | Loss: 0.00285614
Iteration 21/25 | Loss: 0.00285614
Iteration 22/25 | Loss: 0.00285614
Iteration 23/25 | Loss: 0.00285614
Iteration 24/25 | Loss: 0.00285614
Iteration 25/25 | Loss: 0.00285614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285614
Iteration 2/1000 | Loss: 0.00409863
Iteration 3/1000 | Loss: 0.00263552
Iteration 4/1000 | Loss: 0.00068342
Iteration 5/1000 | Loss: 0.00023537
Iteration 6/1000 | Loss: 0.00140382
Iteration 7/1000 | Loss: 0.00127765
Iteration 8/1000 | Loss: 0.00063803
Iteration 9/1000 | Loss: 0.00465995
Iteration 10/1000 | Loss: 0.00121497
Iteration 11/1000 | Loss: 0.00126487
Iteration 12/1000 | Loss: 0.00117574
Iteration 13/1000 | Loss: 0.00019665
Iteration 14/1000 | Loss: 0.00078189
Iteration 15/1000 | Loss: 0.00012876
Iteration 16/1000 | Loss: 0.00011730
Iteration 17/1000 | Loss: 0.00011757
Iteration 18/1000 | Loss: 0.00043082
Iteration 19/1000 | Loss: 0.00306367
Iteration 20/1000 | Loss: 0.00261926
Iteration 21/1000 | Loss: 0.00162005
Iteration 22/1000 | Loss: 0.00679747
Iteration 23/1000 | Loss: 0.00369807
Iteration 24/1000 | Loss: 0.00182497
Iteration 25/1000 | Loss: 0.00333600
Iteration 26/1000 | Loss: 0.00467258
Iteration 27/1000 | Loss: 0.00206182
Iteration 28/1000 | Loss: 0.00219426
Iteration 29/1000 | Loss: 0.00359241
Iteration 30/1000 | Loss: 0.00325401
Iteration 31/1000 | Loss: 0.00249435
Iteration 32/1000 | Loss: 0.00345120
Iteration 33/1000 | Loss: 0.00114087
Iteration 34/1000 | Loss: 0.00192205
Iteration 35/1000 | Loss: 0.00111052
Iteration 36/1000 | Loss: 0.00120027
Iteration 37/1000 | Loss: 0.00221501
Iteration 38/1000 | Loss: 0.00022817
Iteration 39/1000 | Loss: 0.00017403
Iteration 40/1000 | Loss: 0.00015820
Iteration 41/1000 | Loss: 0.00011419
Iteration 42/1000 | Loss: 0.00047497
Iteration 43/1000 | Loss: 0.00112941
Iteration 44/1000 | Loss: 0.00023181
Iteration 45/1000 | Loss: 0.00034863
Iteration 46/1000 | Loss: 0.00022433
Iteration 47/1000 | Loss: 0.00142597
Iteration 48/1000 | Loss: 0.00024550
Iteration 49/1000 | Loss: 0.00037906
Iteration 50/1000 | Loss: 0.00020239
Iteration 51/1000 | Loss: 0.00068843
Iteration 52/1000 | Loss: 0.00044293
Iteration 53/1000 | Loss: 0.00008015
Iteration 54/1000 | Loss: 0.00062521
Iteration 55/1000 | Loss: 0.00028789
Iteration 56/1000 | Loss: 0.00044966
Iteration 57/1000 | Loss: 0.00034153
Iteration 58/1000 | Loss: 0.00076108
Iteration 59/1000 | Loss: 0.00025142
Iteration 60/1000 | Loss: 0.00016834
Iteration 61/1000 | Loss: 0.00015324
Iteration 62/1000 | Loss: 0.00027469
Iteration 63/1000 | Loss: 0.00016053
Iteration 64/1000 | Loss: 0.00016945
Iteration 65/1000 | Loss: 0.00017031
Iteration 66/1000 | Loss: 0.00029040
Iteration 67/1000 | Loss: 0.00030999
Iteration 68/1000 | Loss: 0.00008737
Iteration 69/1000 | Loss: 0.00032370
Iteration 70/1000 | Loss: 0.00024046
Iteration 71/1000 | Loss: 0.00021921
Iteration 72/1000 | Loss: 0.00103857
Iteration 73/1000 | Loss: 0.00035540
Iteration 74/1000 | Loss: 0.00007937
Iteration 75/1000 | Loss: 0.00047126
Iteration 76/1000 | Loss: 0.00026915
Iteration 77/1000 | Loss: 0.00048231
Iteration 78/1000 | Loss: 0.00206983
Iteration 79/1000 | Loss: 0.00090803
Iteration 80/1000 | Loss: 0.00072504
Iteration 81/1000 | Loss: 0.00078991
Iteration 82/1000 | Loss: 0.00014771
Iteration 83/1000 | Loss: 0.00007181
Iteration 84/1000 | Loss: 0.00059186
Iteration 85/1000 | Loss: 0.00244800
Iteration 86/1000 | Loss: 0.00124595
Iteration 87/1000 | Loss: 0.00092506
Iteration 88/1000 | Loss: 0.00060683
Iteration 89/1000 | Loss: 0.00021417
Iteration 90/1000 | Loss: 0.00045994
Iteration 91/1000 | Loss: 0.00019700
Iteration 92/1000 | Loss: 0.00042532
Iteration 93/1000 | Loss: 0.00063190
Iteration 94/1000 | Loss: 0.00021626
Iteration 95/1000 | Loss: 0.00057394
Iteration 96/1000 | Loss: 0.00034254
Iteration 97/1000 | Loss: 0.00055277
Iteration 98/1000 | Loss: 0.00006466
Iteration 99/1000 | Loss: 0.00050009
Iteration 100/1000 | Loss: 0.00018350
Iteration 101/1000 | Loss: 0.00032692
Iteration 102/1000 | Loss: 0.00122308
Iteration 103/1000 | Loss: 0.00012469
Iteration 104/1000 | Loss: 0.00057328
Iteration 105/1000 | Loss: 0.00005724
Iteration 106/1000 | Loss: 0.00038405
Iteration 107/1000 | Loss: 0.00041942
Iteration 108/1000 | Loss: 0.00003076
Iteration 109/1000 | Loss: 0.00017422
Iteration 110/1000 | Loss: 0.00002855
Iteration 111/1000 | Loss: 0.00002684
Iteration 112/1000 | Loss: 0.00002627
Iteration 113/1000 | Loss: 0.00002417
Iteration 114/1000 | Loss: 0.00061132
Iteration 115/1000 | Loss: 0.00068883
Iteration 116/1000 | Loss: 0.00068486
Iteration 117/1000 | Loss: 0.00036768
Iteration 118/1000 | Loss: 0.00191812
Iteration 119/1000 | Loss: 0.00064538
Iteration 120/1000 | Loss: 0.00016826
Iteration 121/1000 | Loss: 0.00120131
Iteration 122/1000 | Loss: 0.00020650
Iteration 123/1000 | Loss: 0.00015247
Iteration 124/1000 | Loss: 0.00023416
Iteration 125/1000 | Loss: 0.00004136
Iteration 126/1000 | Loss: 0.00005921
Iteration 127/1000 | Loss: 0.00006750
Iteration 128/1000 | Loss: 0.00003055
Iteration 129/1000 | Loss: 0.00002171
Iteration 130/1000 | Loss: 0.00002085
Iteration 131/1000 | Loss: 0.00014461
Iteration 132/1000 | Loss: 0.00002030
Iteration 133/1000 | Loss: 0.00008146
Iteration 134/1000 | Loss: 0.00004826
Iteration 135/1000 | Loss: 0.00003480
Iteration 136/1000 | Loss: 0.00116622
Iteration 137/1000 | Loss: 0.00004718
Iteration 138/1000 | Loss: 0.00002352
Iteration 139/1000 | Loss: 0.00007070
Iteration 140/1000 | Loss: 0.00004433
Iteration 141/1000 | Loss: 0.00003757
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00077970
Iteration 144/1000 | Loss: 0.00002331
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001797
Iteration 147/1000 | Loss: 0.00004740
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00016325
Iteration 150/1000 | Loss: 0.00004847
Iteration 151/1000 | Loss: 0.00015574
Iteration 152/1000 | Loss: 0.00006461
Iteration 153/1000 | Loss: 0.00001646
Iteration 154/1000 | Loss: 0.00001600
Iteration 155/1000 | Loss: 0.00012942
Iteration 156/1000 | Loss: 0.00001598
Iteration 157/1000 | Loss: 0.00001583
Iteration 158/1000 | Loss: 0.00001580
Iteration 159/1000 | Loss: 0.00001579
Iteration 160/1000 | Loss: 0.00001578
Iteration 161/1000 | Loss: 0.00001578
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001576
Iteration 164/1000 | Loss: 0.00001574
Iteration 165/1000 | Loss: 0.00001574
Iteration 166/1000 | Loss: 0.00001573
Iteration 167/1000 | Loss: 0.00001571
Iteration 168/1000 | Loss: 0.00001570
Iteration 169/1000 | Loss: 0.00005975
Iteration 170/1000 | Loss: 0.00002110
Iteration 171/1000 | Loss: 0.00002691
Iteration 172/1000 | Loss: 0.00011446
Iteration 173/1000 | Loss: 0.00001578
Iteration 174/1000 | Loss: 0.00001559
Iteration 175/1000 | Loss: 0.00001556
Iteration 176/1000 | Loss: 0.00001556
Iteration 177/1000 | Loss: 0.00001556
Iteration 178/1000 | Loss: 0.00001556
Iteration 179/1000 | Loss: 0.00001556
Iteration 180/1000 | Loss: 0.00001555
Iteration 181/1000 | Loss: 0.00001555
Iteration 182/1000 | Loss: 0.00001555
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001554
Iteration 185/1000 | Loss: 0.00001554
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001554
Iteration 188/1000 | Loss: 0.00001553
Iteration 189/1000 | Loss: 0.00001553
Iteration 190/1000 | Loss: 0.00001553
Iteration 191/1000 | Loss: 0.00001553
Iteration 192/1000 | Loss: 0.00001553
Iteration 193/1000 | Loss: 0.00001553
Iteration 194/1000 | Loss: 0.00001553
Iteration 195/1000 | Loss: 0.00001553
Iteration 196/1000 | Loss: 0.00001553
Iteration 197/1000 | Loss: 0.00001553
Iteration 198/1000 | Loss: 0.00001553
Iteration 199/1000 | Loss: 0.00001553
Iteration 200/1000 | Loss: 0.00001552
Iteration 201/1000 | Loss: 0.00001552
Iteration 202/1000 | Loss: 0.00001552
Iteration 203/1000 | Loss: 0.00001552
Iteration 204/1000 | Loss: 0.00001552
Iteration 205/1000 | Loss: 0.00001552
Iteration 206/1000 | Loss: 0.00001552
Iteration 207/1000 | Loss: 0.00001552
Iteration 208/1000 | Loss: 0.00001552
Iteration 209/1000 | Loss: 0.00001551
Iteration 210/1000 | Loss: 0.00001551
Iteration 211/1000 | Loss: 0.00001551
Iteration 212/1000 | Loss: 0.00001551
Iteration 213/1000 | Loss: 0.00001551
Iteration 214/1000 | Loss: 0.00001551
Iteration 215/1000 | Loss: 0.00001551
Iteration 216/1000 | Loss: 0.00001551
Iteration 217/1000 | Loss: 0.00001551
Iteration 218/1000 | Loss: 0.00001551
Iteration 219/1000 | Loss: 0.00001551
Iteration 220/1000 | Loss: 0.00001550
Iteration 221/1000 | Loss: 0.00001550
Iteration 222/1000 | Loss: 0.00001550
Iteration 223/1000 | Loss: 0.00001550
Iteration 224/1000 | Loss: 0.00001550
Iteration 225/1000 | Loss: 0.00001550
Iteration 226/1000 | Loss: 0.00001550
Iteration 227/1000 | Loss: 0.00001550
Iteration 228/1000 | Loss: 0.00001549
Iteration 229/1000 | Loss: 0.00001549
Iteration 230/1000 | Loss: 0.00001549
Iteration 231/1000 | Loss: 0.00001549
Iteration 232/1000 | Loss: 0.00001549
Iteration 233/1000 | Loss: 0.00001549
Iteration 234/1000 | Loss: 0.00001549
Iteration 235/1000 | Loss: 0.00001549
Iteration 236/1000 | Loss: 0.00001549
Iteration 237/1000 | Loss: 0.00001549
Iteration 238/1000 | Loss: 0.00001548
Iteration 239/1000 | Loss: 0.00001548
Iteration 240/1000 | Loss: 0.00001548
Iteration 241/1000 | Loss: 0.00001548
Iteration 242/1000 | Loss: 0.00001548
Iteration 243/1000 | Loss: 0.00001548
Iteration 244/1000 | Loss: 0.00001548
Iteration 245/1000 | Loss: 0.00001548
Iteration 246/1000 | Loss: 0.00001548
Iteration 247/1000 | Loss: 0.00001548
Iteration 248/1000 | Loss: 0.00001548
Iteration 249/1000 | Loss: 0.00001548
Iteration 250/1000 | Loss: 0.00001548
Iteration 251/1000 | Loss: 0.00001548
Iteration 252/1000 | Loss: 0.00001547
Iteration 253/1000 | Loss: 0.00001547
Iteration 254/1000 | Loss: 0.00001547
Iteration 255/1000 | Loss: 0.00001547
Iteration 256/1000 | Loss: 0.00001546
Iteration 257/1000 | Loss: 0.00001546
Iteration 258/1000 | Loss: 0.00001546
Iteration 259/1000 | Loss: 0.00004998
Iteration 260/1000 | Loss: 0.00001549
Iteration 261/1000 | Loss: 0.00001544
Iteration 262/1000 | Loss: 0.00001544
Iteration 263/1000 | Loss: 0.00001544
Iteration 264/1000 | Loss: 0.00001543
Iteration 265/1000 | Loss: 0.00001543
Iteration 266/1000 | Loss: 0.00001543
Iteration 267/1000 | Loss: 0.00001542
Iteration 268/1000 | Loss: 0.00001542
Iteration 269/1000 | Loss: 0.00001542
Iteration 270/1000 | Loss: 0.00001542
Iteration 271/1000 | Loss: 0.00001542
Iteration 272/1000 | Loss: 0.00001542
Iteration 273/1000 | Loss: 0.00001541
Iteration 274/1000 | Loss: 0.00001541
Iteration 275/1000 | Loss: 0.00001541
Iteration 276/1000 | Loss: 0.00001541
Iteration 277/1000 | Loss: 0.00001541
Iteration 278/1000 | Loss: 0.00001541
Iteration 279/1000 | Loss: 0.00001541
Iteration 280/1000 | Loss: 0.00001541
Iteration 281/1000 | Loss: 0.00001541
Iteration 282/1000 | Loss: 0.00001541
Iteration 283/1000 | Loss: 0.00001541
Iteration 284/1000 | Loss: 0.00001541
Iteration 285/1000 | Loss: 0.00001541
Iteration 286/1000 | Loss: 0.00001541
Iteration 287/1000 | Loss: 0.00001541
Iteration 288/1000 | Loss: 0.00001540
Iteration 289/1000 | Loss: 0.00001540
Iteration 290/1000 | Loss: 0.00001540
Iteration 291/1000 | Loss: 0.00001540
Iteration 292/1000 | Loss: 0.00001540
Iteration 293/1000 | Loss: 0.00001540
Iteration 294/1000 | Loss: 0.00001540
Iteration 295/1000 | Loss: 0.00001540
Iteration 296/1000 | Loss: 0.00001540
Iteration 297/1000 | Loss: 0.00001540
Iteration 298/1000 | Loss: 0.00001540
Iteration 299/1000 | Loss: 0.00001540
Iteration 300/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.5402001736219972e-05, 1.5402001736219972e-05, 1.5402001736219972e-05, 1.5402001736219972e-05, 1.5402001736219972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5402001736219972e-05

Optimization complete. Final v2v error: 3.240476131439209 mm

Highest mean error: 5.587307929992676 mm for frame 78

Lowest mean error: 2.6558773517608643 mm for frame 141

Saving results

Total time: 310.28451323509216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776654
Iteration 2/25 | Loss: 0.00144054
Iteration 3/25 | Loss: 0.00131654
Iteration 4/25 | Loss: 0.00129508
Iteration 5/25 | Loss: 0.00128411
Iteration 6/25 | Loss: 0.00129619
Iteration 7/25 | Loss: 0.00127720
Iteration 8/25 | Loss: 0.00127074
Iteration 9/25 | Loss: 0.00126730
Iteration 10/25 | Loss: 0.00126533
Iteration 11/25 | Loss: 0.00126461
Iteration 12/25 | Loss: 0.00126344
Iteration 13/25 | Loss: 0.00126310
Iteration 14/25 | Loss: 0.00126299
Iteration 15/25 | Loss: 0.00126296
Iteration 16/25 | Loss: 0.00126295
Iteration 17/25 | Loss: 0.00126295
Iteration 18/25 | Loss: 0.00126295
Iteration 19/25 | Loss: 0.00126294
Iteration 20/25 | Loss: 0.00126294
Iteration 21/25 | Loss: 0.00126294
Iteration 22/25 | Loss: 0.00126294
Iteration 23/25 | Loss: 0.00126293
Iteration 24/25 | Loss: 0.00126293
Iteration 25/25 | Loss: 0.00126293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63630784
Iteration 2/25 | Loss: 0.00091174
Iteration 3/25 | Loss: 0.00091173
Iteration 4/25 | Loss: 0.00091173
Iteration 5/25 | Loss: 0.00091173
Iteration 6/25 | Loss: 0.00091173
Iteration 7/25 | Loss: 0.00091173
Iteration 8/25 | Loss: 0.00091173
Iteration 9/25 | Loss: 0.00091173
Iteration 10/25 | Loss: 0.00091173
Iteration 11/25 | Loss: 0.00091173
Iteration 12/25 | Loss: 0.00091173
Iteration 13/25 | Loss: 0.00091173
Iteration 14/25 | Loss: 0.00091173
Iteration 15/25 | Loss: 0.00091173
Iteration 16/25 | Loss: 0.00091173
Iteration 17/25 | Loss: 0.00091173
Iteration 18/25 | Loss: 0.00091173
Iteration 19/25 | Loss: 0.00091173
Iteration 20/25 | Loss: 0.00091173
Iteration 21/25 | Loss: 0.00091173
Iteration 22/25 | Loss: 0.00091173
Iteration 23/25 | Loss: 0.00091173
Iteration 24/25 | Loss: 0.00091173
Iteration 25/25 | Loss: 0.00091173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091173
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00002099
Iteration 4/1000 | Loss: 0.00001961
Iteration 5/1000 | Loss: 0.00001878
Iteration 6/1000 | Loss: 0.00001842
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001745
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001727
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001720
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001711
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001706
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001704
Iteration 32/1000 | Loss: 0.00001704
Iteration 33/1000 | Loss: 0.00001702
Iteration 34/1000 | Loss: 0.00001702
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001702
Iteration 37/1000 | Loss: 0.00001702
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001699
Iteration 48/1000 | Loss: 0.00001699
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001698
Iteration 59/1000 | Loss: 0.00001698
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001690
Iteration 74/1000 | Loss: 0.00001690
Iteration 75/1000 | Loss: 0.00001690
Iteration 76/1000 | Loss: 0.00001690
Iteration 77/1000 | Loss: 0.00001690
Iteration 78/1000 | Loss: 0.00001690
Iteration 79/1000 | Loss: 0.00001690
Iteration 80/1000 | Loss: 0.00001689
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001687
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001685
Iteration 89/1000 | Loss: 0.00001685
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001680
Iteration 98/1000 | Loss: 0.00001680
Iteration 99/1000 | Loss: 0.00001680
Iteration 100/1000 | Loss: 0.00001680
Iteration 101/1000 | Loss: 0.00001680
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001679
Iteration 104/1000 | Loss: 0.00001679
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001675
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001675
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001674
Iteration 116/1000 | Loss: 0.00001674
Iteration 117/1000 | Loss: 0.00001674
Iteration 118/1000 | Loss: 0.00001674
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001672
Iteration 124/1000 | Loss: 0.00001672
Iteration 125/1000 | Loss: 0.00001672
Iteration 126/1000 | Loss: 0.00001671
Iteration 127/1000 | Loss: 0.00001671
Iteration 128/1000 | Loss: 0.00001671
Iteration 129/1000 | Loss: 0.00001671
Iteration 130/1000 | Loss: 0.00001671
Iteration 131/1000 | Loss: 0.00001671
Iteration 132/1000 | Loss: 0.00001671
Iteration 133/1000 | Loss: 0.00001671
Iteration 134/1000 | Loss: 0.00001671
Iteration 135/1000 | Loss: 0.00001671
Iteration 136/1000 | Loss: 0.00001671
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001670
Iteration 139/1000 | Loss: 0.00001670
Iteration 140/1000 | Loss: 0.00001670
Iteration 141/1000 | Loss: 0.00001670
Iteration 142/1000 | Loss: 0.00001670
Iteration 143/1000 | Loss: 0.00001670
Iteration 144/1000 | Loss: 0.00001670
Iteration 145/1000 | Loss: 0.00001670
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001669
Iteration 148/1000 | Loss: 0.00001669
Iteration 149/1000 | Loss: 0.00001669
Iteration 150/1000 | Loss: 0.00001669
Iteration 151/1000 | Loss: 0.00001669
Iteration 152/1000 | Loss: 0.00001669
Iteration 153/1000 | Loss: 0.00001669
Iteration 154/1000 | Loss: 0.00001669
Iteration 155/1000 | Loss: 0.00001669
Iteration 156/1000 | Loss: 0.00001669
Iteration 157/1000 | Loss: 0.00001669
Iteration 158/1000 | Loss: 0.00001669
Iteration 159/1000 | Loss: 0.00001669
Iteration 160/1000 | Loss: 0.00001669
Iteration 161/1000 | Loss: 0.00001669
Iteration 162/1000 | Loss: 0.00001669
Iteration 163/1000 | Loss: 0.00001669
Iteration 164/1000 | Loss: 0.00001669
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001669
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001669
Iteration 172/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.668534787313547e-05, 1.668534787313547e-05, 1.668534787313547e-05, 1.668534787313547e-05, 1.668534787313547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.668534787313547e-05

Optimization complete. Final v2v error: 3.340869426727295 mm

Highest mean error: 4.311117649078369 mm for frame 118

Lowest mean error: 2.9319376945495605 mm for frame 137

Saving results

Total time: 54.25686287879944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405933
Iteration 2/25 | Loss: 0.00142147
Iteration 3/25 | Loss: 0.00123631
Iteration 4/25 | Loss: 0.00122150
Iteration 5/25 | Loss: 0.00121945
Iteration 6/25 | Loss: 0.00121899
Iteration 7/25 | Loss: 0.00121899
Iteration 8/25 | Loss: 0.00121899
Iteration 9/25 | Loss: 0.00121899
Iteration 10/25 | Loss: 0.00121899
Iteration 11/25 | Loss: 0.00121899
Iteration 12/25 | Loss: 0.00121899
Iteration 13/25 | Loss: 0.00121899
Iteration 14/25 | Loss: 0.00121899
Iteration 15/25 | Loss: 0.00121899
Iteration 16/25 | Loss: 0.00121899
Iteration 17/25 | Loss: 0.00121899
Iteration 18/25 | Loss: 0.00121899
Iteration 19/25 | Loss: 0.00121899
Iteration 20/25 | Loss: 0.00121899
Iteration 21/25 | Loss: 0.00121899
Iteration 22/25 | Loss: 0.00121899
Iteration 23/25 | Loss: 0.00121899
Iteration 24/25 | Loss: 0.00121899
Iteration 25/25 | Loss: 0.00121899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34238219
Iteration 2/25 | Loss: 0.00082191
Iteration 3/25 | Loss: 0.00082191
Iteration 4/25 | Loss: 0.00082191
Iteration 5/25 | Loss: 0.00082190
Iteration 6/25 | Loss: 0.00082190
Iteration 7/25 | Loss: 0.00082190
Iteration 8/25 | Loss: 0.00082190
Iteration 9/25 | Loss: 0.00082190
Iteration 10/25 | Loss: 0.00082190
Iteration 11/25 | Loss: 0.00082190
Iteration 12/25 | Loss: 0.00082190
Iteration 13/25 | Loss: 0.00082190
Iteration 14/25 | Loss: 0.00082190
Iteration 15/25 | Loss: 0.00082190
Iteration 16/25 | Loss: 0.00082190
Iteration 17/25 | Loss: 0.00082190
Iteration 18/25 | Loss: 0.00082190
Iteration 19/25 | Loss: 0.00082190
Iteration 20/25 | Loss: 0.00082190
Iteration 21/25 | Loss: 0.00082190
Iteration 22/25 | Loss: 0.00082190
Iteration 23/25 | Loss: 0.00082190
Iteration 24/25 | Loss: 0.00082190
Iteration 25/25 | Loss: 0.00082190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082190
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001886
Iteration 4/1000 | Loss: 0.00001749
Iteration 5/1000 | Loss: 0.00001631
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001486
Iteration 8/1000 | Loss: 0.00001458
Iteration 9/1000 | Loss: 0.00001426
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001397
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001388
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001365
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001334
Iteration 47/1000 | Loss: 0.00001333
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001332
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001321
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001307
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001306
Iteration 90/1000 | Loss: 0.00001306
Iteration 91/1000 | Loss: 0.00001306
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001304
Iteration 99/1000 | Loss: 0.00001304
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001303
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001300
Iteration 128/1000 | Loss: 0.00001300
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001299
Iteration 135/1000 | Loss: 0.00001299
Iteration 136/1000 | Loss: 0.00001299
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001298
Iteration 140/1000 | Loss: 0.00001298
Iteration 141/1000 | Loss: 0.00001298
Iteration 142/1000 | Loss: 0.00001298
Iteration 143/1000 | Loss: 0.00001298
Iteration 144/1000 | Loss: 0.00001298
Iteration 145/1000 | Loss: 0.00001297
Iteration 146/1000 | Loss: 0.00001297
Iteration 147/1000 | Loss: 0.00001297
Iteration 148/1000 | Loss: 0.00001297
Iteration 149/1000 | Loss: 0.00001297
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001296
Iteration 155/1000 | Loss: 0.00001296
Iteration 156/1000 | Loss: 0.00001296
Iteration 157/1000 | Loss: 0.00001296
Iteration 158/1000 | Loss: 0.00001295
Iteration 159/1000 | Loss: 0.00001295
Iteration 160/1000 | Loss: 0.00001295
Iteration 161/1000 | Loss: 0.00001295
Iteration 162/1000 | Loss: 0.00001295
Iteration 163/1000 | Loss: 0.00001294
Iteration 164/1000 | Loss: 0.00001294
Iteration 165/1000 | Loss: 0.00001293
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001293
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001291
Iteration 176/1000 | Loss: 0.00001291
Iteration 177/1000 | Loss: 0.00001291
Iteration 178/1000 | Loss: 0.00001290
Iteration 179/1000 | Loss: 0.00001290
Iteration 180/1000 | Loss: 0.00001290
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001290
Iteration 183/1000 | Loss: 0.00001290
Iteration 184/1000 | Loss: 0.00001290
Iteration 185/1000 | Loss: 0.00001290
Iteration 186/1000 | Loss: 0.00001290
Iteration 187/1000 | Loss: 0.00001290
Iteration 188/1000 | Loss: 0.00001290
Iteration 189/1000 | Loss: 0.00001290
Iteration 190/1000 | Loss: 0.00001290
Iteration 191/1000 | Loss: 0.00001290
Iteration 192/1000 | Loss: 0.00001290
Iteration 193/1000 | Loss: 0.00001290
Iteration 194/1000 | Loss: 0.00001290
Iteration 195/1000 | Loss: 0.00001290
Iteration 196/1000 | Loss: 0.00001290
Iteration 197/1000 | Loss: 0.00001290
Iteration 198/1000 | Loss: 0.00001290
Iteration 199/1000 | Loss: 0.00001290
Iteration 200/1000 | Loss: 0.00001290
Iteration 201/1000 | Loss: 0.00001290
Iteration 202/1000 | Loss: 0.00001290
Iteration 203/1000 | Loss: 0.00001290
Iteration 204/1000 | Loss: 0.00001290
Iteration 205/1000 | Loss: 0.00001290
Iteration 206/1000 | Loss: 0.00001290
Iteration 207/1000 | Loss: 0.00001290
Iteration 208/1000 | Loss: 0.00001290
Iteration 209/1000 | Loss: 0.00001290
Iteration 210/1000 | Loss: 0.00001290
Iteration 211/1000 | Loss: 0.00001290
Iteration 212/1000 | Loss: 0.00001290
Iteration 213/1000 | Loss: 0.00001290
Iteration 214/1000 | Loss: 0.00001290
Iteration 215/1000 | Loss: 0.00001290
Iteration 216/1000 | Loss: 0.00001290
Iteration 217/1000 | Loss: 0.00001290
Iteration 218/1000 | Loss: 0.00001290
Iteration 219/1000 | Loss: 0.00001290
Iteration 220/1000 | Loss: 0.00001290
Iteration 221/1000 | Loss: 0.00001290
Iteration 222/1000 | Loss: 0.00001290
Iteration 223/1000 | Loss: 0.00001290
Iteration 224/1000 | Loss: 0.00001290
Iteration 225/1000 | Loss: 0.00001290
Iteration 226/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.2896916814497672e-05, 1.2896916814497672e-05, 1.2896916814497672e-05, 1.2896916814497672e-05, 1.2896916814497672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2896916814497672e-05

Optimization complete. Final v2v error: 3.0781190395355225 mm

Highest mean error: 3.2077784538269043 mm for frame 27

Lowest mean error: 2.9233551025390625 mm for frame 1

Saving results

Total time: 44.18238568305969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017151
Iteration 2/25 | Loss: 0.00215352
Iteration 3/25 | Loss: 0.00161022
Iteration 4/25 | Loss: 0.00153937
Iteration 5/25 | Loss: 0.00170990
Iteration 6/25 | Loss: 0.00158144
Iteration 7/25 | Loss: 0.00150398
Iteration 8/25 | Loss: 0.00142581
Iteration 9/25 | Loss: 0.00131888
Iteration 10/25 | Loss: 0.00131912
Iteration 11/25 | Loss: 0.00127988
Iteration 12/25 | Loss: 0.00127815
Iteration 13/25 | Loss: 0.00127552
Iteration 14/25 | Loss: 0.00126621
Iteration 15/25 | Loss: 0.00127009
Iteration 16/25 | Loss: 0.00126850
Iteration 17/25 | Loss: 0.00126165
Iteration 18/25 | Loss: 0.00126074
Iteration 19/25 | Loss: 0.00126046
Iteration 20/25 | Loss: 0.00126042
Iteration 21/25 | Loss: 0.00126042
Iteration 22/25 | Loss: 0.00126035
Iteration 23/25 | Loss: 0.00126035
Iteration 24/25 | Loss: 0.00126035
Iteration 25/25 | Loss: 0.00126035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40757418
Iteration 2/25 | Loss: 0.00161155
Iteration 3/25 | Loss: 0.00161155
Iteration 4/25 | Loss: 0.00161155
Iteration 5/25 | Loss: 0.00161155
Iteration 6/25 | Loss: 0.00161155
Iteration 7/25 | Loss: 0.00161155
Iteration 8/25 | Loss: 0.00161154
Iteration 9/25 | Loss: 0.00161154
Iteration 10/25 | Loss: 0.00161154
Iteration 11/25 | Loss: 0.00161154
Iteration 12/25 | Loss: 0.00161154
Iteration 13/25 | Loss: 0.00161154
Iteration 14/25 | Loss: 0.00161154
Iteration 15/25 | Loss: 0.00161154
Iteration 16/25 | Loss: 0.00161154
Iteration 17/25 | Loss: 0.00161154
Iteration 18/25 | Loss: 0.00161154
Iteration 19/25 | Loss: 0.00161154
Iteration 20/25 | Loss: 0.00161154
Iteration 21/25 | Loss: 0.00161154
Iteration 22/25 | Loss: 0.00161154
Iteration 23/25 | Loss: 0.00161154
Iteration 24/25 | Loss: 0.00161154
Iteration 25/25 | Loss: 0.00161154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161154
Iteration 2/1000 | Loss: 0.00010495
Iteration 3/1000 | Loss: 0.00006979
Iteration 4/1000 | Loss: 0.00005616
Iteration 5/1000 | Loss: 0.00015012
Iteration 6/1000 | Loss: 0.00004846
Iteration 7/1000 | Loss: 0.00010229
Iteration 8/1000 | Loss: 0.00004522
Iteration 9/1000 | Loss: 0.00004342
Iteration 10/1000 | Loss: 0.00004176
Iteration 11/1000 | Loss: 0.00015395
Iteration 12/1000 | Loss: 0.00003975
Iteration 13/1000 | Loss: 0.00009239
Iteration 14/1000 | Loss: 0.00003844
Iteration 15/1000 | Loss: 0.00003775
Iteration 16/1000 | Loss: 0.00011261
Iteration 17/1000 | Loss: 0.00003884
Iteration 18/1000 | Loss: 0.00003689
Iteration 19/1000 | Loss: 0.00003664
Iteration 20/1000 | Loss: 0.00003649
Iteration 21/1000 | Loss: 0.00003644
Iteration 22/1000 | Loss: 0.00003641
Iteration 23/1000 | Loss: 0.00003640
Iteration 24/1000 | Loss: 0.00003639
Iteration 25/1000 | Loss: 0.00003639
Iteration 26/1000 | Loss: 0.00003638
Iteration 27/1000 | Loss: 0.00003636
Iteration 28/1000 | Loss: 0.00003634
Iteration 29/1000 | Loss: 0.00003633
Iteration 30/1000 | Loss: 0.00003632
Iteration 31/1000 | Loss: 0.00003632
Iteration 32/1000 | Loss: 0.00003631
Iteration 33/1000 | Loss: 0.00003631
Iteration 34/1000 | Loss: 0.00003630
Iteration 35/1000 | Loss: 0.00003630
Iteration 36/1000 | Loss: 0.00019058
Iteration 37/1000 | Loss: 0.00003888
Iteration 38/1000 | Loss: 0.00003660
Iteration 39/1000 | Loss: 0.00003624
Iteration 40/1000 | Loss: 0.00003612
Iteration 41/1000 | Loss: 0.00003612
Iteration 42/1000 | Loss: 0.00003612
Iteration 43/1000 | Loss: 0.00003612
Iteration 44/1000 | Loss: 0.00003612
Iteration 45/1000 | Loss: 0.00003612
Iteration 46/1000 | Loss: 0.00003611
Iteration 47/1000 | Loss: 0.00003611
Iteration 48/1000 | Loss: 0.00003611
Iteration 49/1000 | Loss: 0.00003611
Iteration 50/1000 | Loss: 0.00003611
Iteration 51/1000 | Loss: 0.00003611
Iteration 52/1000 | Loss: 0.00003611
Iteration 53/1000 | Loss: 0.00003610
Iteration 54/1000 | Loss: 0.00003609
Iteration 55/1000 | Loss: 0.00003608
Iteration 56/1000 | Loss: 0.00003607
Iteration 57/1000 | Loss: 0.00015022
Iteration 58/1000 | Loss: 0.00005706
Iteration 59/1000 | Loss: 0.00003618
Iteration 60/1000 | Loss: 0.00008831
Iteration 61/1000 | Loss: 0.00005431
Iteration 62/1000 | Loss: 0.00004580
Iteration 63/1000 | Loss: 0.00009690
Iteration 64/1000 | Loss: 0.00003720
Iteration 65/1000 | Loss: 0.00003632
Iteration 66/1000 | Loss: 0.00008205
Iteration 67/1000 | Loss: 0.00003622
Iteration 68/1000 | Loss: 0.00003603
Iteration 69/1000 | Loss: 0.00003599
Iteration 70/1000 | Loss: 0.00003599
Iteration 71/1000 | Loss: 0.00003598
Iteration 72/1000 | Loss: 0.00003598
Iteration 73/1000 | Loss: 0.00003597
Iteration 74/1000 | Loss: 0.00003597
Iteration 75/1000 | Loss: 0.00003596
Iteration 76/1000 | Loss: 0.00003595
Iteration 77/1000 | Loss: 0.00003595
Iteration 78/1000 | Loss: 0.00003594
Iteration 79/1000 | Loss: 0.00003594
Iteration 80/1000 | Loss: 0.00003594
Iteration 81/1000 | Loss: 0.00003594
Iteration 82/1000 | Loss: 0.00003594
Iteration 83/1000 | Loss: 0.00003594
Iteration 84/1000 | Loss: 0.00003594
Iteration 85/1000 | Loss: 0.00003594
Iteration 86/1000 | Loss: 0.00003594
Iteration 87/1000 | Loss: 0.00003594
Iteration 88/1000 | Loss: 0.00003594
Iteration 89/1000 | Loss: 0.00003593
Iteration 90/1000 | Loss: 0.00003593
Iteration 91/1000 | Loss: 0.00003593
Iteration 92/1000 | Loss: 0.00003593
Iteration 93/1000 | Loss: 0.00003592
Iteration 94/1000 | Loss: 0.00003592
Iteration 95/1000 | Loss: 0.00003592
Iteration 96/1000 | Loss: 0.00003592
Iteration 97/1000 | Loss: 0.00003591
Iteration 98/1000 | Loss: 0.00003591
Iteration 99/1000 | Loss: 0.00003591
Iteration 100/1000 | Loss: 0.00003590
Iteration 101/1000 | Loss: 0.00003590
Iteration 102/1000 | Loss: 0.00003590
Iteration 103/1000 | Loss: 0.00003590
Iteration 104/1000 | Loss: 0.00003590
Iteration 105/1000 | Loss: 0.00003590
Iteration 106/1000 | Loss: 0.00003590
Iteration 107/1000 | Loss: 0.00003590
Iteration 108/1000 | Loss: 0.00003590
Iteration 109/1000 | Loss: 0.00003590
Iteration 110/1000 | Loss: 0.00003590
Iteration 111/1000 | Loss: 0.00003590
Iteration 112/1000 | Loss: 0.00003590
Iteration 113/1000 | Loss: 0.00003590
Iteration 114/1000 | Loss: 0.00003590
Iteration 115/1000 | Loss: 0.00003590
Iteration 116/1000 | Loss: 0.00003590
Iteration 117/1000 | Loss: 0.00003589
Iteration 118/1000 | Loss: 0.00003589
Iteration 119/1000 | Loss: 0.00003589
Iteration 120/1000 | Loss: 0.00003589
Iteration 121/1000 | Loss: 0.00003589
Iteration 122/1000 | Loss: 0.00003589
Iteration 123/1000 | Loss: 0.00003589
Iteration 124/1000 | Loss: 0.00003589
Iteration 125/1000 | Loss: 0.00003588
Iteration 126/1000 | Loss: 0.00003588
Iteration 127/1000 | Loss: 0.00003588
Iteration 128/1000 | Loss: 0.00003588
Iteration 129/1000 | Loss: 0.00003588
Iteration 130/1000 | Loss: 0.00003588
Iteration 131/1000 | Loss: 0.00003588
Iteration 132/1000 | Loss: 0.00003588
Iteration 133/1000 | Loss: 0.00003588
Iteration 134/1000 | Loss: 0.00003588
Iteration 135/1000 | Loss: 0.00003588
Iteration 136/1000 | Loss: 0.00003588
Iteration 137/1000 | Loss: 0.00003588
Iteration 138/1000 | Loss: 0.00003588
Iteration 139/1000 | Loss: 0.00003588
Iteration 140/1000 | Loss: 0.00003588
Iteration 141/1000 | Loss: 0.00003588
Iteration 142/1000 | Loss: 0.00003588
Iteration 143/1000 | Loss: 0.00003588
Iteration 144/1000 | Loss: 0.00003588
Iteration 145/1000 | Loss: 0.00003588
Iteration 146/1000 | Loss: 0.00003587
Iteration 147/1000 | Loss: 0.00003587
Iteration 148/1000 | Loss: 0.00003587
Iteration 149/1000 | Loss: 0.00003587
Iteration 150/1000 | Loss: 0.00003587
Iteration 151/1000 | Loss: 0.00003587
Iteration 152/1000 | Loss: 0.00003587
Iteration 153/1000 | Loss: 0.00003587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.5874116292688996e-05, 3.5874116292688996e-05, 3.5874116292688996e-05, 3.5874116292688996e-05, 3.5874116292688996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5874116292688996e-05

Optimization complete. Final v2v error: 3.501896619796753 mm

Highest mean error: 12.397322654724121 mm for frame 97

Lowest mean error: 2.638972520828247 mm for frame 28

Saving results

Total time: 93.5354516506195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874264
Iteration 2/25 | Loss: 0.00182539
Iteration 3/25 | Loss: 0.00142648
Iteration 4/25 | Loss: 0.00136223
Iteration 5/25 | Loss: 0.00135561
Iteration 6/25 | Loss: 0.00132521
Iteration 7/25 | Loss: 0.00132465
Iteration 8/25 | Loss: 0.00133291
Iteration 9/25 | Loss: 0.00132187
Iteration 10/25 | Loss: 0.00131026
Iteration 11/25 | Loss: 0.00131286
Iteration 12/25 | Loss: 0.00131165
Iteration 13/25 | Loss: 0.00130631
Iteration 14/25 | Loss: 0.00130491
Iteration 15/25 | Loss: 0.00130468
Iteration 16/25 | Loss: 0.00130460
Iteration 17/25 | Loss: 0.00130460
Iteration 18/25 | Loss: 0.00130460
Iteration 19/25 | Loss: 0.00130460
Iteration 20/25 | Loss: 0.00130460
Iteration 21/25 | Loss: 0.00130459
Iteration 22/25 | Loss: 0.00130459
Iteration 23/25 | Loss: 0.00130459
Iteration 24/25 | Loss: 0.00130459
Iteration 25/25 | Loss: 0.00130459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.70353127
Iteration 2/25 | Loss: 0.00092623
Iteration 3/25 | Loss: 0.00092602
Iteration 4/25 | Loss: 0.00092602
Iteration 5/25 | Loss: 0.00092602
Iteration 6/25 | Loss: 0.00092602
Iteration 7/25 | Loss: 0.00092601
Iteration 8/25 | Loss: 0.00092601
Iteration 9/25 | Loss: 0.00092601
Iteration 10/25 | Loss: 0.00092601
Iteration 11/25 | Loss: 0.00092601
Iteration 12/25 | Loss: 0.00092601
Iteration 13/25 | Loss: 0.00092601
Iteration 14/25 | Loss: 0.00092601
Iteration 15/25 | Loss: 0.00092601
Iteration 16/25 | Loss: 0.00092601
Iteration 17/25 | Loss: 0.00092601
Iteration 18/25 | Loss: 0.00092601
Iteration 19/25 | Loss: 0.00092601
Iteration 20/25 | Loss: 0.00092601
Iteration 21/25 | Loss: 0.00092601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009260133374482393, 0.0009260133374482393, 0.0009260133374482393, 0.0009260133374482393, 0.0009260133374482393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009260133374482393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092601
Iteration 2/1000 | Loss: 0.00005414
Iteration 3/1000 | Loss: 0.00027602
Iteration 4/1000 | Loss: 0.00023939
Iteration 5/1000 | Loss: 0.00026315
Iteration 6/1000 | Loss: 0.00019650
Iteration 7/1000 | Loss: 0.00007992
Iteration 8/1000 | Loss: 0.00014191
Iteration 9/1000 | Loss: 0.00015089
Iteration 10/1000 | Loss: 0.00009324
Iteration 11/1000 | Loss: 0.00003049
Iteration 12/1000 | Loss: 0.00016814
Iteration 13/1000 | Loss: 0.00002896
Iteration 14/1000 | Loss: 0.00002673
Iteration 15/1000 | Loss: 0.00002586
Iteration 16/1000 | Loss: 0.00002487
Iteration 17/1000 | Loss: 0.00015560
Iteration 18/1000 | Loss: 0.00002533
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002297
Iteration 21/1000 | Loss: 0.00002260
Iteration 22/1000 | Loss: 0.00002228
Iteration 23/1000 | Loss: 0.00029622
Iteration 24/1000 | Loss: 0.00024929
Iteration 25/1000 | Loss: 0.00002969
Iteration 26/1000 | Loss: 0.00006277
Iteration 27/1000 | Loss: 0.00002198
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002051
Iteration 30/1000 | Loss: 0.00002015
Iteration 31/1000 | Loss: 0.00012194
Iteration 32/1000 | Loss: 0.00001987
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001961
Iteration 35/1000 | Loss: 0.00001961
Iteration 36/1000 | Loss: 0.00001960
Iteration 37/1000 | Loss: 0.00001960
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001960
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001960
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001959
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001957
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001955
Iteration 55/1000 | Loss: 0.00001955
Iteration 56/1000 | Loss: 0.00001954
Iteration 57/1000 | Loss: 0.00001954
Iteration 58/1000 | Loss: 0.00001954
Iteration 59/1000 | Loss: 0.00001953
Iteration 60/1000 | Loss: 0.00001953
Iteration 61/1000 | Loss: 0.00001953
Iteration 62/1000 | Loss: 0.00001952
Iteration 63/1000 | Loss: 0.00001952
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001951
Iteration 67/1000 | Loss: 0.00001950
Iteration 68/1000 | Loss: 0.00001950
Iteration 69/1000 | Loss: 0.00001950
Iteration 70/1000 | Loss: 0.00001950
Iteration 71/1000 | Loss: 0.00001950
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001949
Iteration 74/1000 | Loss: 0.00001949
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001949
Iteration 77/1000 | Loss: 0.00001948
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001947
Iteration 84/1000 | Loss: 0.00001947
Iteration 85/1000 | Loss: 0.00001947
Iteration 86/1000 | Loss: 0.00001947
Iteration 87/1000 | Loss: 0.00001946
Iteration 88/1000 | Loss: 0.00001946
Iteration 89/1000 | Loss: 0.00001946
Iteration 90/1000 | Loss: 0.00001946
Iteration 91/1000 | Loss: 0.00001945
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00011896
Iteration 96/1000 | Loss: 0.00011896
Iteration 97/1000 | Loss: 0.00002169
Iteration 98/1000 | Loss: 0.00002008
Iteration 99/1000 | Loss: 0.00001952
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001940
Iteration 105/1000 | Loss: 0.00001939
Iteration 106/1000 | Loss: 0.00001939
Iteration 107/1000 | Loss: 0.00001939
Iteration 108/1000 | Loss: 0.00001938
Iteration 109/1000 | Loss: 0.00001938
Iteration 110/1000 | Loss: 0.00001938
Iteration 111/1000 | Loss: 0.00001938
Iteration 112/1000 | Loss: 0.00001938
Iteration 113/1000 | Loss: 0.00001938
Iteration 114/1000 | Loss: 0.00001938
Iteration 115/1000 | Loss: 0.00001938
Iteration 116/1000 | Loss: 0.00001938
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001936
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001935
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001934
Iteration 150/1000 | Loss: 0.00001934
Iteration 151/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.934009742399212e-05, 1.934009742399212e-05, 1.934009742399212e-05, 1.934009742399212e-05, 1.934009742399212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.934009742399212e-05

Optimization complete. Final v2v error: 3.66658616065979 mm

Highest mean error: 5.74611234664917 mm for frame 99

Lowest mean error: 3.014261484146118 mm for frame 17

Saving results

Total time: 88.38513040542603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364127
Iteration 2/25 | Loss: 0.00126428
Iteration 3/25 | Loss: 0.00119787
Iteration 4/25 | Loss: 0.00118792
Iteration 5/25 | Loss: 0.00118488
Iteration 6/25 | Loss: 0.00118488
Iteration 7/25 | Loss: 0.00118488
Iteration 8/25 | Loss: 0.00118488
Iteration 9/25 | Loss: 0.00118488
Iteration 10/25 | Loss: 0.00118488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011848825961351395, 0.0011848825961351395, 0.0011848825961351395, 0.0011848825961351395, 0.0011848825961351395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011848825961351395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58106172
Iteration 2/25 | Loss: 0.00101692
Iteration 3/25 | Loss: 0.00101692
Iteration 4/25 | Loss: 0.00101692
Iteration 5/25 | Loss: 0.00101692
Iteration 6/25 | Loss: 0.00101692
Iteration 7/25 | Loss: 0.00101692
Iteration 8/25 | Loss: 0.00101692
Iteration 9/25 | Loss: 0.00101692
Iteration 10/25 | Loss: 0.00101692
Iteration 11/25 | Loss: 0.00101692
Iteration 12/25 | Loss: 0.00101692
Iteration 13/25 | Loss: 0.00101692
Iteration 14/25 | Loss: 0.00101692
Iteration 15/25 | Loss: 0.00101692
Iteration 16/25 | Loss: 0.00101692
Iteration 17/25 | Loss: 0.00101692
Iteration 18/25 | Loss: 0.00101692
Iteration 19/25 | Loss: 0.00101692
Iteration 20/25 | Loss: 0.00101692
Iteration 21/25 | Loss: 0.00101692
Iteration 22/25 | Loss: 0.00101692
Iteration 23/25 | Loss: 0.00101692
Iteration 24/25 | Loss: 0.00101692
Iteration 25/25 | Loss: 0.00101692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101692
Iteration 2/1000 | Loss: 0.00001614
Iteration 3/1000 | Loss: 0.00001258
Iteration 4/1000 | Loss: 0.00001153
Iteration 5/1000 | Loss: 0.00001105
Iteration 6/1000 | Loss: 0.00001063
Iteration 7/1000 | Loss: 0.00001043
Iteration 8/1000 | Loss: 0.00001008
Iteration 9/1000 | Loss: 0.00000997
Iteration 10/1000 | Loss: 0.00000994
Iteration 11/1000 | Loss: 0.00000988
Iteration 12/1000 | Loss: 0.00000986
Iteration 13/1000 | Loss: 0.00000984
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000976
Iteration 17/1000 | Loss: 0.00000975
Iteration 18/1000 | Loss: 0.00000975
Iteration 19/1000 | Loss: 0.00000974
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000964
Iteration 22/1000 | Loss: 0.00000960
Iteration 23/1000 | Loss: 0.00000960
Iteration 24/1000 | Loss: 0.00000960
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000957
Iteration 28/1000 | Loss: 0.00000957
Iteration 29/1000 | Loss: 0.00000956
Iteration 30/1000 | Loss: 0.00000955
Iteration 31/1000 | Loss: 0.00000952
Iteration 32/1000 | Loss: 0.00000952
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000952
Iteration 36/1000 | Loss: 0.00000952
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000951
Iteration 39/1000 | Loss: 0.00000949
Iteration 40/1000 | Loss: 0.00000947
Iteration 41/1000 | Loss: 0.00000944
Iteration 42/1000 | Loss: 0.00000944
Iteration 43/1000 | Loss: 0.00000940
Iteration 44/1000 | Loss: 0.00000938
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000935
Iteration 47/1000 | Loss: 0.00000934
Iteration 48/1000 | Loss: 0.00000933
Iteration 49/1000 | Loss: 0.00000933
Iteration 50/1000 | Loss: 0.00000932
Iteration 51/1000 | Loss: 0.00000932
Iteration 52/1000 | Loss: 0.00000932
Iteration 53/1000 | Loss: 0.00000932
Iteration 54/1000 | Loss: 0.00000932
Iteration 55/1000 | Loss: 0.00000932
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000930
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000927
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000925
Iteration 66/1000 | Loss: 0.00000925
Iteration 67/1000 | Loss: 0.00000925
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000923
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000921
Iteration 85/1000 | Loss: 0.00000921
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000920
Iteration 94/1000 | Loss: 0.00000920
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000920
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000920
Iteration 100/1000 | Loss: 0.00000919
Iteration 101/1000 | Loss: 0.00000919
Iteration 102/1000 | Loss: 0.00000919
Iteration 103/1000 | Loss: 0.00000919
Iteration 104/1000 | Loss: 0.00000918
Iteration 105/1000 | Loss: 0.00000918
Iteration 106/1000 | Loss: 0.00000918
Iteration 107/1000 | Loss: 0.00000918
Iteration 108/1000 | Loss: 0.00000918
Iteration 109/1000 | Loss: 0.00000918
Iteration 110/1000 | Loss: 0.00000917
Iteration 111/1000 | Loss: 0.00000917
Iteration 112/1000 | Loss: 0.00000917
Iteration 113/1000 | Loss: 0.00000917
Iteration 114/1000 | Loss: 0.00000917
Iteration 115/1000 | Loss: 0.00000917
Iteration 116/1000 | Loss: 0.00000917
Iteration 117/1000 | Loss: 0.00000917
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000916
Iteration 124/1000 | Loss: 0.00000916
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000915
Iteration 127/1000 | Loss: 0.00000915
Iteration 128/1000 | Loss: 0.00000915
Iteration 129/1000 | Loss: 0.00000915
Iteration 130/1000 | Loss: 0.00000915
Iteration 131/1000 | Loss: 0.00000915
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000914
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000912
Iteration 140/1000 | Loss: 0.00000912
Iteration 141/1000 | Loss: 0.00000912
Iteration 142/1000 | Loss: 0.00000912
Iteration 143/1000 | Loss: 0.00000911
Iteration 144/1000 | Loss: 0.00000911
Iteration 145/1000 | Loss: 0.00000911
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000909
Iteration 151/1000 | Loss: 0.00000909
Iteration 152/1000 | Loss: 0.00000909
Iteration 153/1000 | Loss: 0.00000908
Iteration 154/1000 | Loss: 0.00000908
Iteration 155/1000 | Loss: 0.00000908
Iteration 156/1000 | Loss: 0.00000907
Iteration 157/1000 | Loss: 0.00000907
Iteration 158/1000 | Loss: 0.00000907
Iteration 159/1000 | Loss: 0.00000907
Iteration 160/1000 | Loss: 0.00000906
Iteration 161/1000 | Loss: 0.00000906
Iteration 162/1000 | Loss: 0.00000906
Iteration 163/1000 | Loss: 0.00000906
Iteration 164/1000 | Loss: 0.00000906
Iteration 165/1000 | Loss: 0.00000906
Iteration 166/1000 | Loss: 0.00000906
Iteration 167/1000 | Loss: 0.00000906
Iteration 168/1000 | Loss: 0.00000906
Iteration 169/1000 | Loss: 0.00000906
Iteration 170/1000 | Loss: 0.00000906
Iteration 171/1000 | Loss: 0.00000906
Iteration 172/1000 | Loss: 0.00000906
Iteration 173/1000 | Loss: 0.00000906
Iteration 174/1000 | Loss: 0.00000906
Iteration 175/1000 | Loss: 0.00000906
Iteration 176/1000 | Loss: 0.00000906
Iteration 177/1000 | Loss: 0.00000906
Iteration 178/1000 | Loss: 0.00000906
Iteration 179/1000 | Loss: 0.00000906
Iteration 180/1000 | Loss: 0.00000906
Iteration 181/1000 | Loss: 0.00000906
Iteration 182/1000 | Loss: 0.00000906
Iteration 183/1000 | Loss: 0.00000906
Iteration 184/1000 | Loss: 0.00000906
Iteration 185/1000 | Loss: 0.00000906
Iteration 186/1000 | Loss: 0.00000906
Iteration 187/1000 | Loss: 0.00000906
Iteration 188/1000 | Loss: 0.00000906
Iteration 189/1000 | Loss: 0.00000906
Iteration 190/1000 | Loss: 0.00000906
Iteration 191/1000 | Loss: 0.00000906
Iteration 192/1000 | Loss: 0.00000906
Iteration 193/1000 | Loss: 0.00000906
Iteration 194/1000 | Loss: 0.00000906
Iteration 195/1000 | Loss: 0.00000906
Iteration 196/1000 | Loss: 0.00000906
Iteration 197/1000 | Loss: 0.00000906
Iteration 198/1000 | Loss: 0.00000906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [9.06367404240882e-06, 9.06367404240882e-06, 9.06367404240882e-06, 9.06367404240882e-06, 9.06367404240882e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.06367404240882e-06

Optimization complete. Final v2v error: 2.6044769287109375 mm

Highest mean error: 3.0220870971679688 mm for frame 139

Lowest mean error: 2.5430338382720947 mm for frame 0

Saving results

Total time: 42.194448709487915
