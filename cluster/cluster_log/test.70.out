Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=70, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3920-3975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803184
Iteration 2/25 | Loss: 0.00183451
Iteration 3/25 | Loss: 0.00147670
Iteration 4/25 | Loss: 0.00144571
Iteration 5/25 | Loss: 0.00144172
Iteration 6/25 | Loss: 0.00144124
Iteration 7/25 | Loss: 0.00144124
Iteration 8/25 | Loss: 0.00144124
Iteration 9/25 | Loss: 0.00144124
Iteration 10/25 | Loss: 0.00144124
Iteration 11/25 | Loss: 0.00144124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014412415912374854, 0.0014412415912374854, 0.0014412415912374854, 0.0014412415912374854, 0.0014412415912374854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014412415912374854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22793150
Iteration 2/25 | Loss: 0.00161831
Iteration 3/25 | Loss: 0.00161831
Iteration 4/25 | Loss: 0.00161831
Iteration 5/25 | Loss: 0.00161831
Iteration 6/25 | Loss: 0.00161831
Iteration 7/25 | Loss: 0.00161830
Iteration 8/25 | Loss: 0.00161830
Iteration 9/25 | Loss: 0.00161830
Iteration 10/25 | Loss: 0.00161830
Iteration 11/25 | Loss: 0.00161830
Iteration 12/25 | Loss: 0.00161830
Iteration 13/25 | Loss: 0.00161830
Iteration 14/25 | Loss: 0.00161830
Iteration 15/25 | Loss: 0.00161830
Iteration 16/25 | Loss: 0.00161830
Iteration 17/25 | Loss: 0.00161830
Iteration 18/25 | Loss: 0.00161830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016183035913854837, 0.0016183035913854837, 0.0016183035913854837, 0.0016183035913854837, 0.0016183035913854837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016183035913854837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161830
Iteration 2/1000 | Loss: 0.00003472
Iteration 3/1000 | Loss: 0.00002638
Iteration 4/1000 | Loss: 0.00002430
Iteration 5/1000 | Loss: 0.00002250
Iteration 6/1000 | Loss: 0.00002138
Iteration 7/1000 | Loss: 0.00002067
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00001959
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001855
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001824
Iteration 23/1000 | Loss: 0.00001824
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001823
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00001822
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001806
Iteration 41/1000 | Loss: 0.00001806
Iteration 42/1000 | Loss: 0.00001806
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001805
Iteration 48/1000 | Loss: 0.00001804
Iteration 49/1000 | Loss: 0.00001804
Iteration 50/1000 | Loss: 0.00001804
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001804
Iteration 53/1000 | Loss: 0.00001804
Iteration 54/1000 | Loss: 0.00001804
Iteration 55/1000 | Loss: 0.00001804
Iteration 56/1000 | Loss: 0.00001804
Iteration 57/1000 | Loss: 0.00001804
Iteration 58/1000 | Loss: 0.00001804
Iteration 59/1000 | Loss: 0.00001804
Iteration 60/1000 | Loss: 0.00001804
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001802
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001801
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001799
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001798
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001791
Iteration 82/1000 | Loss: 0.00001791
Iteration 83/1000 | Loss: 0.00001791
Iteration 84/1000 | Loss: 0.00001791
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001790
Iteration 88/1000 | Loss: 0.00001790
Iteration 89/1000 | Loss: 0.00001790
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001789
Iteration 93/1000 | Loss: 0.00001789
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001789
Iteration 97/1000 | Loss: 0.00001789
Iteration 98/1000 | Loss: 0.00001788
Iteration 99/1000 | Loss: 0.00001788
Iteration 100/1000 | Loss: 0.00001788
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001788
Iteration 103/1000 | Loss: 0.00001787
Iteration 104/1000 | Loss: 0.00001787
Iteration 105/1000 | Loss: 0.00001787
Iteration 106/1000 | Loss: 0.00001786
Iteration 107/1000 | Loss: 0.00001786
Iteration 108/1000 | Loss: 0.00001786
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001785
Iteration 111/1000 | Loss: 0.00001785
Iteration 112/1000 | Loss: 0.00001785
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001785
Iteration 115/1000 | Loss: 0.00001785
Iteration 116/1000 | Loss: 0.00001784
Iteration 117/1000 | Loss: 0.00001784
Iteration 118/1000 | Loss: 0.00001784
Iteration 119/1000 | Loss: 0.00001784
Iteration 120/1000 | Loss: 0.00001784
Iteration 121/1000 | Loss: 0.00001784
Iteration 122/1000 | Loss: 0.00001784
Iteration 123/1000 | Loss: 0.00001784
Iteration 124/1000 | Loss: 0.00001784
Iteration 125/1000 | Loss: 0.00001784
Iteration 126/1000 | Loss: 0.00001783
Iteration 127/1000 | Loss: 0.00001783
Iteration 128/1000 | Loss: 0.00001783
Iteration 129/1000 | Loss: 0.00001783
Iteration 130/1000 | Loss: 0.00001783
Iteration 131/1000 | Loss: 0.00001783
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001783
Iteration 137/1000 | Loss: 0.00001783
Iteration 138/1000 | Loss: 0.00001783
Iteration 139/1000 | Loss: 0.00001783
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001782
Iteration 142/1000 | Loss: 0.00001782
Iteration 143/1000 | Loss: 0.00001782
Iteration 144/1000 | Loss: 0.00001782
Iteration 145/1000 | Loss: 0.00001782
Iteration 146/1000 | Loss: 0.00001782
Iteration 147/1000 | Loss: 0.00001782
Iteration 148/1000 | Loss: 0.00001782
Iteration 149/1000 | Loss: 0.00001782
Iteration 150/1000 | Loss: 0.00001782
Iteration 151/1000 | Loss: 0.00001782
Iteration 152/1000 | Loss: 0.00001782
Iteration 153/1000 | Loss: 0.00001782
Iteration 154/1000 | Loss: 0.00001782
Iteration 155/1000 | Loss: 0.00001782
Iteration 156/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.782187246135436e-05, 1.782187246135436e-05, 1.782187246135436e-05, 1.782187246135436e-05, 1.782187246135436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.782187246135436e-05

Optimization complete. Final v2v error: 3.6161606311798096 mm

Highest mean error: 4.171136379241943 mm for frame 224

Lowest mean error: 3.2432732582092285 mm for frame 63

Saving results

Total time: 48.11368417739868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989562
Iteration 2/25 | Loss: 0.00380252
Iteration 3/25 | Loss: 0.00232968
Iteration 4/25 | Loss: 0.00221179
Iteration 5/25 | Loss: 0.00181565
Iteration 6/25 | Loss: 0.00163238
Iteration 7/25 | Loss: 0.00155510
Iteration 8/25 | Loss: 0.00150784
Iteration 9/25 | Loss: 0.00146482
Iteration 10/25 | Loss: 0.00145182
Iteration 11/25 | Loss: 0.00142909
Iteration 12/25 | Loss: 0.00141261
Iteration 13/25 | Loss: 0.00141221
Iteration 14/25 | Loss: 0.00141000
Iteration 15/25 | Loss: 0.00140065
Iteration 16/25 | Loss: 0.00139744
Iteration 17/25 | Loss: 0.00139482
Iteration 18/25 | Loss: 0.00139899
Iteration 19/25 | Loss: 0.00139363
Iteration 20/25 | Loss: 0.00139129
Iteration 21/25 | Loss: 0.00138921
Iteration 22/25 | Loss: 0.00139049
Iteration 23/25 | Loss: 0.00138945
Iteration 24/25 | Loss: 0.00139029
Iteration 25/25 | Loss: 0.00138869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23322451
Iteration 2/25 | Loss: 0.00248494
Iteration 3/25 | Loss: 0.00248494
Iteration 4/25 | Loss: 0.00248493
Iteration 5/25 | Loss: 0.00248493
Iteration 6/25 | Loss: 0.00248493
Iteration 7/25 | Loss: 0.00248493
Iteration 8/25 | Loss: 0.00248493
Iteration 9/25 | Loss: 0.00248493
Iteration 10/25 | Loss: 0.00248493
Iteration 11/25 | Loss: 0.00248493
Iteration 12/25 | Loss: 0.00248493
Iteration 13/25 | Loss: 0.00248493
Iteration 14/25 | Loss: 0.00248493
Iteration 15/25 | Loss: 0.00248493
Iteration 16/25 | Loss: 0.00248493
Iteration 17/25 | Loss: 0.00248493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00248493324033916, 0.00248493324033916, 0.00248493324033916, 0.00248493324033916, 0.00248493324033916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00248493324033916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248493
Iteration 2/1000 | Loss: 0.00009950
Iteration 3/1000 | Loss: 0.00004851
Iteration 4/1000 | Loss: 0.00004268
Iteration 5/1000 | Loss: 0.00004007
Iteration 6/1000 | Loss: 0.00003859
Iteration 7/1000 | Loss: 0.00006320
Iteration 8/1000 | Loss: 0.00004302
Iteration 9/1000 | Loss: 0.00007216
Iteration 10/1000 | Loss: 0.00005704
Iteration 11/1000 | Loss: 0.00003831
Iteration 12/1000 | Loss: 0.00004059
Iteration 13/1000 | Loss: 0.00003363
Iteration 14/1000 | Loss: 0.00003468
Iteration 15/1000 | Loss: 0.00004437
Iteration 16/1000 | Loss: 0.00003296
Iteration 17/1000 | Loss: 0.00003448
Iteration 18/1000 | Loss: 0.00003629
Iteration 19/1000 | Loss: 0.00077291
Iteration 20/1000 | Loss: 0.00285488
Iteration 21/1000 | Loss: 0.00086444
Iteration 22/1000 | Loss: 0.00125446
Iteration 23/1000 | Loss: 0.00059958
Iteration 24/1000 | Loss: 0.00011540
Iteration 25/1000 | Loss: 0.00004038
Iteration 26/1000 | Loss: 0.00005902
Iteration 27/1000 | Loss: 0.00004960
Iteration 28/1000 | Loss: 0.00003293
Iteration 29/1000 | Loss: 0.00005322
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00002565
Iteration 33/1000 | Loss: 0.00002798
Iteration 34/1000 | Loss: 0.00002096
Iteration 35/1000 | Loss: 0.00002638
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00002994
Iteration 39/1000 | Loss: 0.00002764
Iteration 40/1000 | Loss: 0.00007889
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001807
Iteration 43/1000 | Loss: 0.00001329
Iteration 44/1000 | Loss: 0.00004288
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001240
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001235
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001232
Iteration 133/1000 | Loss: 0.00001232
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Iteration 137/1000 | Loss: 0.00001232
Iteration 138/1000 | Loss: 0.00001232
Iteration 139/1000 | Loss: 0.00001232
Iteration 140/1000 | Loss: 0.00001232
Iteration 141/1000 | Loss: 0.00001232
Iteration 142/1000 | Loss: 0.00001232
Iteration 143/1000 | Loss: 0.00001232
Iteration 144/1000 | Loss: 0.00001232
Iteration 145/1000 | Loss: 0.00001232
Iteration 146/1000 | Loss: 0.00001232
Iteration 147/1000 | Loss: 0.00001232
Iteration 148/1000 | Loss: 0.00001232
Iteration 149/1000 | Loss: 0.00001232
Iteration 150/1000 | Loss: 0.00001232
Iteration 151/1000 | Loss: 0.00001232
Iteration 152/1000 | Loss: 0.00001232
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001232
Iteration 155/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.2323388546064962e-05, 1.2323388546064962e-05, 1.2323388546064962e-05, 1.2323388546064962e-05, 1.2323388546064962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2323388546064962e-05

Optimization complete. Final v2v error: 2.995633125305176 mm

Highest mean error: 3.7485158443450928 mm for frame 87

Lowest mean error: 2.571192741394043 mm for frame 47

Saving results

Total time: 118.66002869606018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829326
Iteration 2/25 | Loss: 0.00143528
Iteration 3/25 | Loss: 0.00137041
Iteration 4/25 | Loss: 0.00135968
Iteration 5/25 | Loss: 0.00135761
Iteration 6/25 | Loss: 0.00135761
Iteration 7/25 | Loss: 0.00135761
Iteration 8/25 | Loss: 0.00135761
Iteration 9/25 | Loss: 0.00135761
Iteration 10/25 | Loss: 0.00135761
Iteration 11/25 | Loss: 0.00135761
Iteration 12/25 | Loss: 0.00135761
Iteration 13/25 | Loss: 0.00135761
Iteration 14/25 | Loss: 0.00135761
Iteration 15/25 | Loss: 0.00135761
Iteration 16/25 | Loss: 0.00135761
Iteration 17/25 | Loss: 0.00135761
Iteration 18/25 | Loss: 0.00135761
Iteration 19/25 | Loss: 0.00135761
Iteration 20/25 | Loss: 0.00135761
Iteration 21/25 | Loss: 0.00135761
Iteration 22/25 | Loss: 0.00135761
Iteration 23/25 | Loss: 0.00135761
Iteration 24/25 | Loss: 0.00135761
Iteration 25/25 | Loss: 0.00135761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26635301
Iteration 2/25 | Loss: 0.00204322
Iteration 3/25 | Loss: 0.00204321
Iteration 4/25 | Loss: 0.00204321
Iteration 5/25 | Loss: 0.00204321
Iteration 6/25 | Loss: 0.00204321
Iteration 7/25 | Loss: 0.00204321
Iteration 8/25 | Loss: 0.00204321
Iteration 9/25 | Loss: 0.00204321
Iteration 10/25 | Loss: 0.00204321
Iteration 11/25 | Loss: 0.00204321
Iteration 12/25 | Loss: 0.00204321
Iteration 13/25 | Loss: 0.00204321
Iteration 14/25 | Loss: 0.00204321
Iteration 15/25 | Loss: 0.00204321
Iteration 16/25 | Loss: 0.00204321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020432069431990385, 0.0020432069431990385, 0.0020432069431990385, 0.0020432069431990385, 0.0020432069431990385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020432069431990385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204321
Iteration 2/1000 | Loss: 0.00002415
Iteration 3/1000 | Loss: 0.00001949
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001598
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001422
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001356
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001336
Iteration 16/1000 | Loss: 0.00001327
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001310
Iteration 20/1000 | Loss: 0.00001309
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00001307
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001285
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001284
Iteration 44/1000 | Loss: 0.00001284
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001267
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001266
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001265
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001265
Iteration 71/1000 | Loss: 0.00001265
Iteration 72/1000 | Loss: 0.00001264
Iteration 73/1000 | Loss: 0.00001264
Iteration 74/1000 | Loss: 0.00001264
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001263
Iteration 83/1000 | Loss: 0.00001263
Iteration 84/1000 | Loss: 0.00001263
Iteration 85/1000 | Loss: 0.00001263
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001262
Iteration 88/1000 | Loss: 0.00001262
Iteration 89/1000 | Loss: 0.00001262
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001261
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001260
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001260
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001256
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001254
Iteration 145/1000 | Loss: 0.00001254
Iteration 146/1000 | Loss: 0.00001254
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001253
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001253
Iteration 159/1000 | Loss: 0.00001253
Iteration 160/1000 | Loss: 0.00001253
Iteration 161/1000 | Loss: 0.00001253
Iteration 162/1000 | Loss: 0.00001253
Iteration 163/1000 | Loss: 0.00001253
Iteration 164/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.2532780601759441e-05, 1.2532780601759441e-05, 1.2532780601759441e-05, 1.2532780601759441e-05, 1.2532780601759441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2532780601759441e-05

Optimization complete. Final v2v error: 3.0804810523986816 mm

Highest mean error: 3.3736519813537598 mm for frame 116

Lowest mean error: 2.967857837677002 mm for frame 196

Saving results

Total time: 50.45252323150635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795341
Iteration 2/25 | Loss: 0.00141610
Iteration 3/25 | Loss: 0.00134643
Iteration 4/25 | Loss: 0.00132910
Iteration 5/25 | Loss: 0.00132286
Iteration 6/25 | Loss: 0.00132286
Iteration 7/25 | Loss: 0.00132277
Iteration 8/25 | Loss: 0.00132277
Iteration 9/25 | Loss: 0.00132277
Iteration 10/25 | Loss: 0.00132277
Iteration 11/25 | Loss: 0.00132277
Iteration 12/25 | Loss: 0.00132277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013227707240730524, 0.0013227707240730524, 0.0013227707240730524, 0.0013227707240730524, 0.0013227707240730524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013227707240730524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45545912
Iteration 2/25 | Loss: 0.00330365
Iteration 3/25 | Loss: 0.00330365
Iteration 4/25 | Loss: 0.00330365
Iteration 5/25 | Loss: 0.00330365
Iteration 6/25 | Loss: 0.00330365
Iteration 7/25 | Loss: 0.00330365
Iteration 8/25 | Loss: 0.00330365
Iteration 9/25 | Loss: 0.00330365
Iteration 10/25 | Loss: 0.00330365
Iteration 11/25 | Loss: 0.00330365
Iteration 12/25 | Loss: 0.00330365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0033036477398127317, 0.0033036477398127317, 0.0033036477398127317, 0.0033036477398127317, 0.0033036477398127317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033036477398127317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00330365
Iteration 2/1000 | Loss: 0.00003554
Iteration 3/1000 | Loss: 0.00002281
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00001794
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001616
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001487
Iteration 10/1000 | Loss: 0.00001441
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001354
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001352
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001350
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001345
Iteration 26/1000 | Loss: 0.00001341
Iteration 27/1000 | Loss: 0.00001338
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001325
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001323
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001322
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001318
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001315
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001314
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001313
Iteration 72/1000 | Loss: 0.00001313
Iteration 73/1000 | Loss: 0.00001313
Iteration 74/1000 | Loss: 0.00001313
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001312
Iteration 79/1000 | Loss: 0.00001312
Iteration 80/1000 | Loss: 0.00001312
Iteration 81/1000 | Loss: 0.00001312
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001311
Iteration 85/1000 | Loss: 0.00001311
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001310
Iteration 89/1000 | Loss: 0.00001310
Iteration 90/1000 | Loss: 0.00001309
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001309
Iteration 93/1000 | Loss: 0.00001309
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001309
Iteration 98/1000 | Loss: 0.00001308
Iteration 99/1000 | Loss: 0.00001308
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001307
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001300
Iteration 120/1000 | Loss: 0.00001300
Iteration 121/1000 | Loss: 0.00001300
Iteration 122/1000 | Loss: 0.00001300
Iteration 123/1000 | Loss: 0.00001300
Iteration 124/1000 | Loss: 0.00001299
Iteration 125/1000 | Loss: 0.00001299
Iteration 126/1000 | Loss: 0.00001299
Iteration 127/1000 | Loss: 0.00001299
Iteration 128/1000 | Loss: 0.00001299
Iteration 129/1000 | Loss: 0.00001299
Iteration 130/1000 | Loss: 0.00001298
Iteration 131/1000 | Loss: 0.00001298
Iteration 132/1000 | Loss: 0.00001298
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001297
Iteration 138/1000 | Loss: 0.00001297
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001297
Iteration 144/1000 | Loss: 0.00001297
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001296
Iteration 147/1000 | Loss: 0.00001296
Iteration 148/1000 | Loss: 0.00001296
Iteration 149/1000 | Loss: 0.00001296
Iteration 150/1000 | Loss: 0.00001296
Iteration 151/1000 | Loss: 0.00001296
Iteration 152/1000 | Loss: 0.00001296
Iteration 153/1000 | Loss: 0.00001296
Iteration 154/1000 | Loss: 0.00001295
Iteration 155/1000 | Loss: 0.00001295
Iteration 156/1000 | Loss: 0.00001295
Iteration 157/1000 | Loss: 0.00001295
Iteration 158/1000 | Loss: 0.00001294
Iteration 159/1000 | Loss: 0.00001294
Iteration 160/1000 | Loss: 0.00001294
Iteration 161/1000 | Loss: 0.00001294
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001294
Iteration 164/1000 | Loss: 0.00001294
Iteration 165/1000 | Loss: 0.00001294
Iteration 166/1000 | Loss: 0.00001294
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001293
Iteration 171/1000 | Loss: 0.00001293
Iteration 172/1000 | Loss: 0.00001293
Iteration 173/1000 | Loss: 0.00001293
Iteration 174/1000 | Loss: 0.00001293
Iteration 175/1000 | Loss: 0.00001293
Iteration 176/1000 | Loss: 0.00001293
Iteration 177/1000 | Loss: 0.00001292
Iteration 178/1000 | Loss: 0.00001292
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001292
Iteration 181/1000 | Loss: 0.00001292
Iteration 182/1000 | Loss: 0.00001292
Iteration 183/1000 | Loss: 0.00001292
Iteration 184/1000 | Loss: 0.00001291
Iteration 185/1000 | Loss: 0.00001291
Iteration 186/1000 | Loss: 0.00001291
Iteration 187/1000 | Loss: 0.00001291
Iteration 188/1000 | Loss: 0.00001291
Iteration 189/1000 | Loss: 0.00001291
Iteration 190/1000 | Loss: 0.00001291
Iteration 191/1000 | Loss: 0.00001291
Iteration 192/1000 | Loss: 0.00001291
Iteration 193/1000 | Loss: 0.00001291
Iteration 194/1000 | Loss: 0.00001291
Iteration 195/1000 | Loss: 0.00001291
Iteration 196/1000 | Loss: 0.00001291
Iteration 197/1000 | Loss: 0.00001291
Iteration 198/1000 | Loss: 0.00001291
Iteration 199/1000 | Loss: 0.00001291
Iteration 200/1000 | Loss: 0.00001290
Iteration 201/1000 | Loss: 0.00001290
Iteration 202/1000 | Loss: 0.00001290
Iteration 203/1000 | Loss: 0.00001290
Iteration 204/1000 | Loss: 0.00001290
Iteration 205/1000 | Loss: 0.00001290
Iteration 206/1000 | Loss: 0.00001290
Iteration 207/1000 | Loss: 0.00001290
Iteration 208/1000 | Loss: 0.00001290
Iteration 209/1000 | Loss: 0.00001290
Iteration 210/1000 | Loss: 0.00001290
Iteration 211/1000 | Loss: 0.00001290
Iteration 212/1000 | Loss: 0.00001290
Iteration 213/1000 | Loss: 0.00001290
Iteration 214/1000 | Loss: 0.00001290
Iteration 215/1000 | Loss: 0.00001290
Iteration 216/1000 | Loss: 0.00001290
Iteration 217/1000 | Loss: 0.00001290
Iteration 218/1000 | Loss: 0.00001290
Iteration 219/1000 | Loss: 0.00001290
Iteration 220/1000 | Loss: 0.00001290
Iteration 221/1000 | Loss: 0.00001290
Iteration 222/1000 | Loss: 0.00001290
Iteration 223/1000 | Loss: 0.00001290
Iteration 224/1000 | Loss: 0.00001290
Iteration 225/1000 | Loss: 0.00001290
Iteration 226/1000 | Loss: 0.00001290
Iteration 227/1000 | Loss: 0.00001290
Iteration 228/1000 | Loss: 0.00001290
Iteration 229/1000 | Loss: 0.00001290
Iteration 230/1000 | Loss: 0.00001290
Iteration 231/1000 | Loss: 0.00001290
Iteration 232/1000 | Loss: 0.00001290
Iteration 233/1000 | Loss: 0.00001290
Iteration 234/1000 | Loss: 0.00001290
Iteration 235/1000 | Loss: 0.00001290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2895980034954846e-05, 1.2895980034954846e-05, 1.2895980034954846e-05, 1.2895980034954846e-05, 1.2895980034954846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2895980034954846e-05

Optimization complete. Final v2v error: 3.116072416305542 mm

Highest mean error: 3.4843335151672363 mm for frame 28

Lowest mean error: 2.8084516525268555 mm for frame 238

Saving results

Total time: 48.44845485687256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_003/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_003/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989140
Iteration 2/25 | Loss: 0.00294447
Iteration 3/25 | Loss: 0.00235895
Iteration 4/25 | Loss: 0.00222210
Iteration 5/25 | Loss: 0.00216302
Iteration 6/25 | Loss: 0.00212990
Iteration 7/25 | Loss: 0.00210011
Iteration 8/25 | Loss: 0.00208847
Iteration 9/25 | Loss: 0.00208412
Iteration 10/25 | Loss: 0.00208221
Iteration 11/25 | Loss: 0.00208151
Iteration 12/25 | Loss: 0.00208122
Iteration 13/25 | Loss: 0.00208112
Iteration 14/25 | Loss: 0.00208112
Iteration 15/25 | Loss: 0.00208111
Iteration 16/25 | Loss: 0.00208110
Iteration 17/25 | Loss: 0.00208110
Iteration 18/25 | Loss: 0.00208110
Iteration 19/25 | Loss: 0.00208110
Iteration 20/25 | Loss: 0.00208110
Iteration 21/25 | Loss: 0.00208110
Iteration 22/25 | Loss: 0.00208110
Iteration 23/25 | Loss: 0.00208110
Iteration 24/25 | Loss: 0.00208110
Iteration 25/25 | Loss: 0.00208110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23266578
Iteration 2/25 | Loss: 0.00466933
Iteration 3/25 | Loss: 0.00466933
Iteration 4/25 | Loss: 0.00466933
Iteration 5/25 | Loss: 0.00466933
Iteration 6/25 | Loss: 0.00466933
Iteration 7/25 | Loss: 0.00466933
Iteration 8/25 | Loss: 0.00466933
Iteration 9/25 | Loss: 0.00466933
Iteration 10/25 | Loss: 0.00466933
Iteration 11/25 | Loss: 0.00466933
Iteration 12/25 | Loss: 0.00466933
Iteration 13/25 | Loss: 0.00466933
Iteration 14/25 | Loss: 0.00466933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004669326823204756, 0.004669326823204756, 0.004669326823204756, 0.004669326823204756, 0.004669326823204756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004669326823204756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00466933
Iteration 2/1000 | Loss: 0.00054455
Iteration 3/1000 | Loss: 0.00042437
Iteration 4/1000 | Loss: 0.00038822
Iteration 5/1000 | Loss: 0.00036755
Iteration 6/1000 | Loss: 0.00034081
Iteration 7/1000 | Loss: 0.00032126
Iteration 8/1000 | Loss: 0.00030838
Iteration 9/1000 | Loss: 0.00029742
Iteration 10/1000 | Loss: 0.00067208
Iteration 11/1000 | Loss: 0.02329413
Iteration 12/1000 | Loss: 0.02504723
Iteration 13/1000 | Loss: 0.01725579
Iteration 14/1000 | Loss: 0.02333235
Iteration 15/1000 | Loss: 0.00645352
Iteration 16/1000 | Loss: 0.00120247
Iteration 17/1000 | Loss: 0.00691484
Iteration 18/1000 | Loss: 0.01293773
Iteration 19/1000 | Loss: 0.00286942
Iteration 20/1000 | Loss: 0.00133292
Iteration 21/1000 | Loss: 0.00095085
Iteration 22/1000 | Loss: 0.00068732
Iteration 23/1000 | Loss: 0.00024918
Iteration 24/1000 | Loss: 0.00058776
Iteration 25/1000 | Loss: 0.00069060
Iteration 26/1000 | Loss: 0.00029328
Iteration 27/1000 | Loss: 0.00044016
Iteration 28/1000 | Loss: 0.00044412
Iteration 29/1000 | Loss: 0.00010482
Iteration 30/1000 | Loss: 0.00009699
Iteration 31/1000 | Loss: 0.00007072
Iteration 32/1000 | Loss: 0.00005736
Iteration 33/1000 | Loss: 0.00004636
Iteration 34/1000 | Loss: 0.00003958
Iteration 35/1000 | Loss: 0.00003474
Iteration 36/1000 | Loss: 0.00005215
Iteration 37/1000 | Loss: 0.00003390
Iteration 38/1000 | Loss: 0.00003347
Iteration 39/1000 | Loss: 0.00002542
Iteration 40/1000 | Loss: 0.00004299
Iteration 41/1000 | Loss: 0.00002872
Iteration 42/1000 | Loss: 0.00003943
Iteration 43/1000 | Loss: 0.00003673
Iteration 44/1000 | Loss: 0.00004192
Iteration 45/1000 | Loss: 0.00003082
Iteration 46/1000 | Loss: 0.00003397
Iteration 47/1000 | Loss: 0.00003962
Iteration 48/1000 | Loss: 0.00004016
Iteration 49/1000 | Loss: 0.00003759
Iteration 50/1000 | Loss: 0.00002696
Iteration 51/1000 | Loss: 0.00002474
Iteration 52/1000 | Loss: 0.00003867
Iteration 53/1000 | Loss: 0.00004297
Iteration 54/1000 | Loss: 0.00003645
Iteration 55/1000 | Loss: 0.00004045
Iteration 56/1000 | Loss: 0.00003432
Iteration 57/1000 | Loss: 0.00005384
Iteration 58/1000 | Loss: 0.00004191
Iteration 59/1000 | Loss: 0.00002667
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001682
Iteration 75/1000 | Loss: 0.00001682
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001679
Iteration 82/1000 | Loss: 0.00001679
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001669
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001668
Iteration 110/1000 | Loss: 0.00001668
Iteration 111/1000 | Loss: 0.00001668
Iteration 112/1000 | Loss: 0.00001668
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001668
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001668
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001667
Iteration 128/1000 | Loss: 0.00001667
Iteration 129/1000 | Loss: 0.00001667
Iteration 130/1000 | Loss: 0.00001667
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001667
Iteration 135/1000 | Loss: 0.00001666
Iteration 136/1000 | Loss: 0.00001666
Iteration 137/1000 | Loss: 0.00001666
Iteration 138/1000 | Loss: 0.00001666
Iteration 139/1000 | Loss: 0.00001666
Iteration 140/1000 | Loss: 0.00001666
Iteration 141/1000 | Loss: 0.00001665
Iteration 142/1000 | Loss: 0.00001665
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001665
Iteration 149/1000 | Loss: 0.00001665
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001665
Iteration 153/1000 | Loss: 0.00001665
Iteration 154/1000 | Loss: 0.00001665
Iteration 155/1000 | Loss: 0.00001665
Iteration 156/1000 | Loss: 0.00001664
Iteration 157/1000 | Loss: 0.00001664
Iteration 158/1000 | Loss: 0.00001664
Iteration 159/1000 | Loss: 0.00001664
Iteration 160/1000 | Loss: 0.00001664
Iteration 161/1000 | Loss: 0.00001664
Iteration 162/1000 | Loss: 0.00001664
Iteration 163/1000 | Loss: 0.00001664
Iteration 164/1000 | Loss: 0.00001664
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001663
Iteration 168/1000 | Loss: 0.00001663
Iteration 169/1000 | Loss: 0.00001663
Iteration 170/1000 | Loss: 0.00001663
Iteration 171/1000 | Loss: 0.00001663
Iteration 172/1000 | Loss: 0.00001663
Iteration 173/1000 | Loss: 0.00001663
Iteration 174/1000 | Loss: 0.00001663
Iteration 175/1000 | Loss: 0.00001663
Iteration 176/1000 | Loss: 0.00001663
Iteration 177/1000 | Loss: 0.00001663
Iteration 178/1000 | Loss: 0.00001663
Iteration 179/1000 | Loss: 0.00001663
Iteration 180/1000 | Loss: 0.00001663
Iteration 181/1000 | Loss: 0.00001663
Iteration 182/1000 | Loss: 0.00001663
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001663
Iteration 187/1000 | Loss: 0.00001663
Iteration 188/1000 | Loss: 0.00001663
Iteration 189/1000 | Loss: 0.00001663
Iteration 190/1000 | Loss: 0.00001663
Iteration 191/1000 | Loss: 0.00001663
Iteration 192/1000 | Loss: 0.00001663
Iteration 193/1000 | Loss: 0.00001663
Iteration 194/1000 | Loss: 0.00001663
Iteration 195/1000 | Loss: 0.00001663
Iteration 196/1000 | Loss: 0.00001663
Iteration 197/1000 | Loss: 0.00001663
Iteration 198/1000 | Loss: 0.00001663
Iteration 199/1000 | Loss: 0.00001663
Iteration 200/1000 | Loss: 0.00001663
Iteration 201/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.6626099750283174e-05, 1.6626099750283174e-05, 1.6626099750283174e-05, 1.6626099750283174e-05, 1.6626099750283174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6626099750283174e-05

Optimization complete. Final v2v error: 3.501894474029541 mm

Highest mean error: 3.901357650756836 mm for frame 80

Lowest mean error: 3.433332920074463 mm for frame 135

Saving results

Total time: 122.96902871131897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767386
Iteration 2/25 | Loss: 0.00114775
Iteration 3/25 | Loss: 0.00102335
Iteration 4/25 | Loss: 0.00100445
Iteration 5/25 | Loss: 0.00099676
Iteration 6/25 | Loss: 0.00099474
Iteration 7/25 | Loss: 0.00099474
Iteration 8/25 | Loss: 0.00099474
Iteration 9/25 | Loss: 0.00099474
Iteration 10/25 | Loss: 0.00099474
Iteration 11/25 | Loss: 0.00099474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009947438957169652, 0.0009947438957169652, 0.0009947438957169652, 0.0009947438957169652, 0.0009947438957169652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009947438957169652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39346933
Iteration 2/25 | Loss: 0.00137806
Iteration 3/25 | Loss: 0.00137806
Iteration 4/25 | Loss: 0.00137806
Iteration 5/25 | Loss: 0.00137805
Iteration 6/25 | Loss: 0.00137805
Iteration 7/25 | Loss: 0.00137805
Iteration 8/25 | Loss: 0.00137805
Iteration 9/25 | Loss: 0.00137805
Iteration 10/25 | Loss: 0.00137805
Iteration 11/25 | Loss: 0.00137805
Iteration 12/25 | Loss: 0.00137805
Iteration 13/25 | Loss: 0.00137805
Iteration 14/25 | Loss: 0.00137805
Iteration 15/25 | Loss: 0.00137805
Iteration 16/25 | Loss: 0.00137805
Iteration 17/25 | Loss: 0.00137805
Iteration 18/25 | Loss: 0.00137805
Iteration 19/25 | Loss: 0.00137805
Iteration 20/25 | Loss: 0.00137805
Iteration 21/25 | Loss: 0.00137805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013780527515336871, 0.0013780527515336871, 0.0013780527515336871, 0.0013780527515336871, 0.0013780527515336871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013780527515336871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137805
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00001881
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001614
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001469
Iteration 10/1000 | Loss: 0.00001456
Iteration 11/1000 | Loss: 0.00001438
Iteration 12/1000 | Loss: 0.00001433
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001392
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001389
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001384
Iteration 32/1000 | Loss: 0.00001384
Iteration 33/1000 | Loss: 0.00001383
Iteration 34/1000 | Loss: 0.00001383
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001377
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001372
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001371
Iteration 72/1000 | Loss: 0.00001371
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001371
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001370
Iteration 86/1000 | Loss: 0.00001370
Iteration 87/1000 | Loss: 0.00001370
Iteration 88/1000 | Loss: 0.00001370
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001367
Iteration 110/1000 | Loss: 0.00001367
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001367
Iteration 113/1000 | Loss: 0.00001367
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001366
Iteration 121/1000 | Loss: 0.00001366
Iteration 122/1000 | Loss: 0.00001365
Iteration 123/1000 | Loss: 0.00001365
Iteration 124/1000 | Loss: 0.00001365
Iteration 125/1000 | Loss: 0.00001365
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Iteration 131/1000 | Loss: 0.00001364
Iteration 132/1000 | Loss: 0.00001364
Iteration 133/1000 | Loss: 0.00001364
Iteration 134/1000 | Loss: 0.00001364
Iteration 135/1000 | Loss: 0.00001364
Iteration 136/1000 | Loss: 0.00001364
Iteration 137/1000 | Loss: 0.00001364
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001363
Iteration 140/1000 | Loss: 0.00001363
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001363
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001363
Iteration 147/1000 | Loss: 0.00001362
Iteration 148/1000 | Loss: 0.00001362
Iteration 149/1000 | Loss: 0.00001362
Iteration 150/1000 | Loss: 0.00001362
Iteration 151/1000 | Loss: 0.00001362
Iteration 152/1000 | Loss: 0.00001362
Iteration 153/1000 | Loss: 0.00001362
Iteration 154/1000 | Loss: 0.00001362
Iteration 155/1000 | Loss: 0.00001362
Iteration 156/1000 | Loss: 0.00001362
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001361
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001360
Iteration 163/1000 | Loss: 0.00001360
Iteration 164/1000 | Loss: 0.00001360
Iteration 165/1000 | Loss: 0.00001360
Iteration 166/1000 | Loss: 0.00001360
Iteration 167/1000 | Loss: 0.00001360
Iteration 168/1000 | Loss: 0.00001360
Iteration 169/1000 | Loss: 0.00001360
Iteration 170/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.360362148261629e-05, 1.360362148261629e-05, 1.360362148261629e-05, 1.360362148261629e-05, 1.360362148261629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.360362148261629e-05

Optimization complete. Final v2v error: 3.152859926223755 mm

Highest mean error: 3.831523895263672 mm for frame 201

Lowest mean error: 2.5048320293426514 mm for frame 10

Saving results

Total time: 44.74017143249512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538429
Iteration 2/25 | Loss: 0.00112934
Iteration 3/25 | Loss: 0.00101202
Iteration 4/25 | Loss: 0.00099221
Iteration 5/25 | Loss: 0.00098706
Iteration 6/25 | Loss: 0.00098584
Iteration 7/25 | Loss: 0.00098584
Iteration 8/25 | Loss: 0.00098584
Iteration 9/25 | Loss: 0.00098584
Iteration 10/25 | Loss: 0.00098584
Iteration 11/25 | Loss: 0.00098584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000985843944363296, 0.000985843944363296, 0.000985843944363296, 0.000985843944363296, 0.000985843944363296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000985843944363296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88156199
Iteration 2/25 | Loss: 0.00118536
Iteration 3/25 | Loss: 0.00118535
Iteration 4/25 | Loss: 0.00118535
Iteration 5/25 | Loss: 0.00118535
Iteration 6/25 | Loss: 0.00118535
Iteration 7/25 | Loss: 0.00118535
Iteration 8/25 | Loss: 0.00118535
Iteration 9/25 | Loss: 0.00118535
Iteration 10/25 | Loss: 0.00118535
Iteration 11/25 | Loss: 0.00118535
Iteration 12/25 | Loss: 0.00118535
Iteration 13/25 | Loss: 0.00118535
Iteration 14/25 | Loss: 0.00118535
Iteration 15/25 | Loss: 0.00118535
Iteration 16/25 | Loss: 0.00118535
Iteration 17/25 | Loss: 0.00118535
Iteration 18/25 | Loss: 0.00118535
Iteration 19/25 | Loss: 0.00118535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001185351051390171, 0.001185351051390171, 0.001185351051390171, 0.001185351051390171, 0.001185351051390171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001185351051390171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118535
Iteration 2/1000 | Loss: 0.00003156
Iteration 3/1000 | Loss: 0.00002281
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001967
Iteration 6/1000 | Loss: 0.00001885
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001778
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001652
Iteration 24/1000 | Loss: 0.00001652
Iteration 25/1000 | Loss: 0.00001651
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001650
Iteration 28/1000 | Loss: 0.00001650
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001649
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001645
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001639
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001637
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001637
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001636
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001635
Iteration 92/1000 | Loss: 0.00001635
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001634
Iteration 96/1000 | Loss: 0.00001634
Iteration 97/1000 | Loss: 0.00001634
Iteration 98/1000 | Loss: 0.00001634
Iteration 99/1000 | Loss: 0.00001634
Iteration 100/1000 | Loss: 0.00001633
Iteration 101/1000 | Loss: 0.00001633
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001633
Iteration 105/1000 | Loss: 0.00001633
Iteration 106/1000 | Loss: 0.00001632
Iteration 107/1000 | Loss: 0.00001632
Iteration 108/1000 | Loss: 0.00001632
Iteration 109/1000 | Loss: 0.00001632
Iteration 110/1000 | Loss: 0.00001632
Iteration 111/1000 | Loss: 0.00001631
Iteration 112/1000 | Loss: 0.00001631
Iteration 113/1000 | Loss: 0.00001631
Iteration 114/1000 | Loss: 0.00001631
Iteration 115/1000 | Loss: 0.00001631
Iteration 116/1000 | Loss: 0.00001631
Iteration 117/1000 | Loss: 0.00001631
Iteration 118/1000 | Loss: 0.00001630
Iteration 119/1000 | Loss: 0.00001630
Iteration 120/1000 | Loss: 0.00001630
Iteration 121/1000 | Loss: 0.00001630
Iteration 122/1000 | Loss: 0.00001630
Iteration 123/1000 | Loss: 0.00001630
Iteration 124/1000 | Loss: 0.00001630
Iteration 125/1000 | Loss: 0.00001630
Iteration 126/1000 | Loss: 0.00001630
Iteration 127/1000 | Loss: 0.00001630
Iteration 128/1000 | Loss: 0.00001630
Iteration 129/1000 | Loss: 0.00001630
Iteration 130/1000 | Loss: 0.00001630
Iteration 131/1000 | Loss: 0.00001629
Iteration 132/1000 | Loss: 0.00001629
Iteration 133/1000 | Loss: 0.00001629
Iteration 134/1000 | Loss: 0.00001628
Iteration 135/1000 | Loss: 0.00001628
Iteration 136/1000 | Loss: 0.00001628
Iteration 137/1000 | Loss: 0.00001628
Iteration 138/1000 | Loss: 0.00001627
Iteration 139/1000 | Loss: 0.00001627
Iteration 140/1000 | Loss: 0.00001627
Iteration 141/1000 | Loss: 0.00001627
Iteration 142/1000 | Loss: 0.00001627
Iteration 143/1000 | Loss: 0.00001627
Iteration 144/1000 | Loss: 0.00001627
Iteration 145/1000 | Loss: 0.00001627
Iteration 146/1000 | Loss: 0.00001626
Iteration 147/1000 | Loss: 0.00001626
Iteration 148/1000 | Loss: 0.00001626
Iteration 149/1000 | Loss: 0.00001626
Iteration 150/1000 | Loss: 0.00001626
Iteration 151/1000 | Loss: 0.00001626
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001625
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001624
Iteration 172/1000 | Loss: 0.00001624
Iteration 173/1000 | Loss: 0.00001624
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001624
Iteration 176/1000 | Loss: 0.00001624
Iteration 177/1000 | Loss: 0.00001624
Iteration 178/1000 | Loss: 0.00001624
Iteration 179/1000 | Loss: 0.00001624
Iteration 180/1000 | Loss: 0.00001624
Iteration 181/1000 | Loss: 0.00001624
Iteration 182/1000 | Loss: 0.00001624
Iteration 183/1000 | Loss: 0.00001624
Iteration 184/1000 | Loss: 0.00001624
Iteration 185/1000 | Loss: 0.00001624
Iteration 186/1000 | Loss: 0.00001624
Iteration 187/1000 | Loss: 0.00001624
Iteration 188/1000 | Loss: 0.00001624
Iteration 189/1000 | Loss: 0.00001624
Iteration 190/1000 | Loss: 0.00001624
Iteration 191/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.6237238014582545e-05, 1.6237238014582545e-05, 1.6237238014582545e-05, 1.6237238014582545e-05, 1.6237238014582545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6237238014582545e-05

Optimization complete. Final v2v error: 3.4243547916412354 mm

Highest mean error: 3.9483230113983154 mm for frame 125

Lowest mean error: 2.8932316303253174 mm for frame 86

Saving results

Total time: 42.66356897354126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830248
Iteration 2/25 | Loss: 0.00122168
Iteration 3/25 | Loss: 0.00089856
Iteration 4/25 | Loss: 0.00086769
Iteration 5/25 | Loss: 0.00086339
Iteration 6/25 | Loss: 0.00086195
Iteration 7/25 | Loss: 0.00086195
Iteration 8/25 | Loss: 0.00086195
Iteration 9/25 | Loss: 0.00086195
Iteration 10/25 | Loss: 0.00086195
Iteration 11/25 | Loss: 0.00086195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008619514992460608, 0.0008619514992460608, 0.0008619514992460608, 0.0008619514992460608, 0.0008619514992460608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008619514992460608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63890839
Iteration 2/25 | Loss: 0.00113485
Iteration 3/25 | Loss: 0.00113484
Iteration 4/25 | Loss: 0.00113484
Iteration 5/25 | Loss: 0.00113484
Iteration 6/25 | Loss: 0.00113484
Iteration 7/25 | Loss: 0.00113484
Iteration 8/25 | Loss: 0.00113484
Iteration 9/25 | Loss: 0.00113484
Iteration 10/25 | Loss: 0.00113484
Iteration 11/25 | Loss: 0.00113484
Iteration 12/25 | Loss: 0.00113484
Iteration 13/25 | Loss: 0.00113484
Iteration 14/25 | Loss: 0.00113484
Iteration 15/25 | Loss: 0.00113484
Iteration 16/25 | Loss: 0.00113484
Iteration 17/25 | Loss: 0.00113484
Iteration 18/25 | Loss: 0.00113484
Iteration 19/25 | Loss: 0.00113484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001134841819293797, 0.001134841819293797, 0.001134841819293797, 0.001134841819293797, 0.001134841819293797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001134841819293797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113484
Iteration 2/1000 | Loss: 0.00001882
Iteration 3/1000 | Loss: 0.00001107
Iteration 4/1000 | Loss: 0.00000973
Iteration 5/1000 | Loss: 0.00000906
Iteration 6/1000 | Loss: 0.00000857
Iteration 7/1000 | Loss: 0.00000843
Iteration 8/1000 | Loss: 0.00000819
Iteration 9/1000 | Loss: 0.00000802
Iteration 10/1000 | Loss: 0.00000792
Iteration 11/1000 | Loss: 0.00000786
Iteration 12/1000 | Loss: 0.00000784
Iteration 13/1000 | Loss: 0.00000783
Iteration 14/1000 | Loss: 0.00000782
Iteration 15/1000 | Loss: 0.00000781
Iteration 16/1000 | Loss: 0.00000781
Iteration 17/1000 | Loss: 0.00000780
Iteration 18/1000 | Loss: 0.00000780
Iteration 19/1000 | Loss: 0.00000780
Iteration 20/1000 | Loss: 0.00000779
Iteration 21/1000 | Loss: 0.00000778
Iteration 22/1000 | Loss: 0.00000778
Iteration 23/1000 | Loss: 0.00000777
Iteration 24/1000 | Loss: 0.00000774
Iteration 25/1000 | Loss: 0.00000774
Iteration 26/1000 | Loss: 0.00000772
Iteration 27/1000 | Loss: 0.00000772
Iteration 28/1000 | Loss: 0.00000771
Iteration 29/1000 | Loss: 0.00000771
Iteration 30/1000 | Loss: 0.00000771
Iteration 31/1000 | Loss: 0.00000770
Iteration 32/1000 | Loss: 0.00000770
Iteration 33/1000 | Loss: 0.00000770
Iteration 34/1000 | Loss: 0.00000769
Iteration 35/1000 | Loss: 0.00000769
Iteration 36/1000 | Loss: 0.00000768
Iteration 37/1000 | Loss: 0.00000764
Iteration 38/1000 | Loss: 0.00000764
Iteration 39/1000 | Loss: 0.00000763
Iteration 40/1000 | Loss: 0.00000762
Iteration 41/1000 | Loss: 0.00000762
Iteration 42/1000 | Loss: 0.00000760
Iteration 43/1000 | Loss: 0.00000760
Iteration 44/1000 | Loss: 0.00000760
Iteration 45/1000 | Loss: 0.00000760
Iteration 46/1000 | Loss: 0.00000760
Iteration 47/1000 | Loss: 0.00000760
Iteration 48/1000 | Loss: 0.00000760
Iteration 49/1000 | Loss: 0.00000759
Iteration 50/1000 | Loss: 0.00000759
Iteration 51/1000 | Loss: 0.00000759
Iteration 52/1000 | Loss: 0.00000759
Iteration 53/1000 | Loss: 0.00000759
Iteration 54/1000 | Loss: 0.00000759
Iteration 55/1000 | Loss: 0.00000759
Iteration 56/1000 | Loss: 0.00000759
Iteration 57/1000 | Loss: 0.00000759
Iteration 58/1000 | Loss: 0.00000758
Iteration 59/1000 | Loss: 0.00000758
Iteration 60/1000 | Loss: 0.00000757
Iteration 61/1000 | Loss: 0.00000757
Iteration 62/1000 | Loss: 0.00000755
Iteration 63/1000 | Loss: 0.00000755
Iteration 64/1000 | Loss: 0.00000755
Iteration 65/1000 | Loss: 0.00000755
Iteration 66/1000 | Loss: 0.00000755
Iteration 67/1000 | Loss: 0.00000755
Iteration 68/1000 | Loss: 0.00000755
Iteration 69/1000 | Loss: 0.00000754
Iteration 70/1000 | Loss: 0.00000754
Iteration 71/1000 | Loss: 0.00000754
Iteration 72/1000 | Loss: 0.00000752
Iteration 73/1000 | Loss: 0.00000752
Iteration 74/1000 | Loss: 0.00000751
Iteration 75/1000 | Loss: 0.00000751
Iteration 76/1000 | Loss: 0.00000750
Iteration 77/1000 | Loss: 0.00000750
Iteration 78/1000 | Loss: 0.00000750
Iteration 79/1000 | Loss: 0.00000749
Iteration 80/1000 | Loss: 0.00000749
Iteration 81/1000 | Loss: 0.00000749
Iteration 82/1000 | Loss: 0.00000749
Iteration 83/1000 | Loss: 0.00000749
Iteration 84/1000 | Loss: 0.00000747
Iteration 85/1000 | Loss: 0.00000747
Iteration 86/1000 | Loss: 0.00000746
Iteration 87/1000 | Loss: 0.00000746
Iteration 88/1000 | Loss: 0.00000746
Iteration 89/1000 | Loss: 0.00000746
Iteration 90/1000 | Loss: 0.00000745
Iteration 91/1000 | Loss: 0.00000745
Iteration 92/1000 | Loss: 0.00000744
Iteration 93/1000 | Loss: 0.00000744
Iteration 94/1000 | Loss: 0.00000744
Iteration 95/1000 | Loss: 0.00000744
Iteration 96/1000 | Loss: 0.00000744
Iteration 97/1000 | Loss: 0.00000744
Iteration 98/1000 | Loss: 0.00000743
Iteration 99/1000 | Loss: 0.00000743
Iteration 100/1000 | Loss: 0.00000743
Iteration 101/1000 | Loss: 0.00000743
Iteration 102/1000 | Loss: 0.00000743
Iteration 103/1000 | Loss: 0.00000743
Iteration 104/1000 | Loss: 0.00000743
Iteration 105/1000 | Loss: 0.00000743
Iteration 106/1000 | Loss: 0.00000743
Iteration 107/1000 | Loss: 0.00000743
Iteration 108/1000 | Loss: 0.00000743
Iteration 109/1000 | Loss: 0.00000743
Iteration 110/1000 | Loss: 0.00000743
Iteration 111/1000 | Loss: 0.00000743
Iteration 112/1000 | Loss: 0.00000742
Iteration 113/1000 | Loss: 0.00000742
Iteration 114/1000 | Loss: 0.00000742
Iteration 115/1000 | Loss: 0.00000742
Iteration 116/1000 | Loss: 0.00000742
Iteration 117/1000 | Loss: 0.00000742
Iteration 118/1000 | Loss: 0.00000742
Iteration 119/1000 | Loss: 0.00000742
Iteration 120/1000 | Loss: 0.00000741
Iteration 121/1000 | Loss: 0.00000741
Iteration 122/1000 | Loss: 0.00000741
Iteration 123/1000 | Loss: 0.00000741
Iteration 124/1000 | Loss: 0.00000741
Iteration 125/1000 | Loss: 0.00000741
Iteration 126/1000 | Loss: 0.00000741
Iteration 127/1000 | Loss: 0.00000741
Iteration 128/1000 | Loss: 0.00000741
Iteration 129/1000 | Loss: 0.00000741
Iteration 130/1000 | Loss: 0.00000741
Iteration 131/1000 | Loss: 0.00000741
Iteration 132/1000 | Loss: 0.00000741
Iteration 133/1000 | Loss: 0.00000741
Iteration 134/1000 | Loss: 0.00000741
Iteration 135/1000 | Loss: 0.00000741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [7.406758868455654e-06, 7.406758868455654e-06, 7.406758868455654e-06, 7.406758868455654e-06, 7.406758868455654e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.406758868455654e-06

Optimization complete. Final v2v error: 2.3066790103912354 mm

Highest mean error: 2.7585220336914062 mm for frame 47

Lowest mean error: 1.9731223583221436 mm for frame 100

Saving results

Total time: 37.13737893104553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027577
Iteration 2/25 | Loss: 0.00156365
Iteration 3/25 | Loss: 0.00116415
Iteration 4/25 | Loss: 0.00107294
Iteration 5/25 | Loss: 0.00107484
Iteration 6/25 | Loss: 0.00108613
Iteration 7/25 | Loss: 0.00102945
Iteration 8/25 | Loss: 0.00102013
Iteration 9/25 | Loss: 0.00099161
Iteration 10/25 | Loss: 0.00098139
Iteration 11/25 | Loss: 0.00097769
Iteration 12/25 | Loss: 0.00096382
Iteration 13/25 | Loss: 0.00096109
Iteration 14/25 | Loss: 0.00096012
Iteration 15/25 | Loss: 0.00095997
Iteration 16/25 | Loss: 0.00095995
Iteration 17/25 | Loss: 0.00095995
Iteration 18/25 | Loss: 0.00095995
Iteration 19/25 | Loss: 0.00095995
Iteration 20/25 | Loss: 0.00095995
Iteration 21/25 | Loss: 0.00095994
Iteration 22/25 | Loss: 0.00095994
Iteration 23/25 | Loss: 0.00095994
Iteration 24/25 | Loss: 0.00095993
Iteration 25/25 | Loss: 0.00095993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36180377
Iteration 2/25 | Loss: 0.00123086
Iteration 3/25 | Loss: 0.00123085
Iteration 4/25 | Loss: 0.00112346
Iteration 5/25 | Loss: 0.00112346
Iteration 6/25 | Loss: 0.00112346
Iteration 7/25 | Loss: 0.00112346
Iteration 8/25 | Loss: 0.00112346
Iteration 9/25 | Loss: 0.00112346
Iteration 10/25 | Loss: 0.00112346
Iteration 11/25 | Loss: 0.00112346
Iteration 12/25 | Loss: 0.00112346
Iteration 13/25 | Loss: 0.00112346
Iteration 14/25 | Loss: 0.00112346
Iteration 15/25 | Loss: 0.00112346
Iteration 16/25 | Loss: 0.00112346
Iteration 17/25 | Loss: 0.00112346
Iteration 18/25 | Loss: 0.00112346
Iteration 19/25 | Loss: 0.00112346
Iteration 20/25 | Loss: 0.00112346
Iteration 21/25 | Loss: 0.00112346
Iteration 22/25 | Loss: 0.00112346
Iteration 23/25 | Loss: 0.00112346
Iteration 24/25 | Loss: 0.00112346
Iteration 25/25 | Loss: 0.00112346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011234551202505827, 0.0011234551202505827, 0.0011234551202505827, 0.0011234551202505827, 0.0011234551202505827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011234551202505827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112345
Iteration 2/1000 | Loss: 0.00012888
Iteration 3/1000 | Loss: 0.00001703
Iteration 4/1000 | Loss: 0.00001486
Iteration 5/1000 | Loss: 0.00001391
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00043687
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001100
Iteration 14/1000 | Loss: 0.00001046
Iteration 15/1000 | Loss: 0.00001005
Iteration 16/1000 | Loss: 0.00000984
Iteration 17/1000 | Loss: 0.00000969
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000954
Iteration 20/1000 | Loss: 0.00000953
Iteration 21/1000 | Loss: 0.00000952
Iteration 22/1000 | Loss: 0.00000942
Iteration 23/1000 | Loss: 0.00000936
Iteration 24/1000 | Loss: 0.00000932
Iteration 25/1000 | Loss: 0.00000931
Iteration 26/1000 | Loss: 0.00000931
Iteration 27/1000 | Loss: 0.00000930
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000928
Iteration 30/1000 | Loss: 0.00000928
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000927
Iteration 34/1000 | Loss: 0.00000927
Iteration 35/1000 | Loss: 0.00000927
Iteration 36/1000 | Loss: 0.00000927
Iteration 37/1000 | Loss: 0.00000926
Iteration 38/1000 | Loss: 0.00000926
Iteration 39/1000 | Loss: 0.00000926
Iteration 40/1000 | Loss: 0.00000926
Iteration 41/1000 | Loss: 0.00000926
Iteration 42/1000 | Loss: 0.00000926
Iteration 43/1000 | Loss: 0.00000925
Iteration 44/1000 | Loss: 0.00000925
Iteration 45/1000 | Loss: 0.00000924
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000923
Iteration 50/1000 | Loss: 0.00000923
Iteration 51/1000 | Loss: 0.00000922
Iteration 52/1000 | Loss: 0.00000922
Iteration 53/1000 | Loss: 0.00000922
Iteration 54/1000 | Loss: 0.00000921
Iteration 55/1000 | Loss: 0.00000920
Iteration 56/1000 | Loss: 0.00000920
Iteration 57/1000 | Loss: 0.00000920
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000918
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000917
Iteration 63/1000 | Loss: 0.00000917
Iteration 64/1000 | Loss: 0.00000917
Iteration 65/1000 | Loss: 0.00000917
Iteration 66/1000 | Loss: 0.00000917
Iteration 67/1000 | Loss: 0.00000916
Iteration 68/1000 | Loss: 0.00000916
Iteration 69/1000 | Loss: 0.00000916
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000916
Iteration 73/1000 | Loss: 0.00000916
Iteration 74/1000 | Loss: 0.00000915
Iteration 75/1000 | Loss: 0.00000915
Iteration 76/1000 | Loss: 0.00000915
Iteration 77/1000 | Loss: 0.00000915
Iteration 78/1000 | Loss: 0.00000915
Iteration 79/1000 | Loss: 0.00000915
Iteration 80/1000 | Loss: 0.00000915
Iteration 81/1000 | Loss: 0.00000915
Iteration 82/1000 | Loss: 0.00000915
Iteration 83/1000 | Loss: 0.00000915
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000914
Iteration 86/1000 | Loss: 0.00000914
Iteration 87/1000 | Loss: 0.00000914
Iteration 88/1000 | Loss: 0.00000914
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000914
Iteration 93/1000 | Loss: 0.00000914
Iteration 94/1000 | Loss: 0.00000914
Iteration 95/1000 | Loss: 0.00000914
Iteration 96/1000 | Loss: 0.00000914
Iteration 97/1000 | Loss: 0.00000914
Iteration 98/1000 | Loss: 0.00000914
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000914
Iteration 101/1000 | Loss: 0.00000914
Iteration 102/1000 | Loss: 0.00000913
Iteration 103/1000 | Loss: 0.00000913
Iteration 104/1000 | Loss: 0.00000913
Iteration 105/1000 | Loss: 0.00000913
Iteration 106/1000 | Loss: 0.00000912
Iteration 107/1000 | Loss: 0.00000912
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000912
Iteration 110/1000 | Loss: 0.00000912
Iteration 111/1000 | Loss: 0.00000912
Iteration 112/1000 | Loss: 0.00000912
Iteration 113/1000 | Loss: 0.00000912
Iteration 114/1000 | Loss: 0.00000912
Iteration 115/1000 | Loss: 0.00000912
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000911
Iteration 119/1000 | Loss: 0.00000911
Iteration 120/1000 | Loss: 0.00000911
Iteration 121/1000 | Loss: 0.00000911
Iteration 122/1000 | Loss: 0.00000911
Iteration 123/1000 | Loss: 0.00000911
Iteration 124/1000 | Loss: 0.00000911
Iteration 125/1000 | Loss: 0.00000911
Iteration 126/1000 | Loss: 0.00000911
Iteration 127/1000 | Loss: 0.00000911
Iteration 128/1000 | Loss: 0.00000911
Iteration 129/1000 | Loss: 0.00000911
Iteration 130/1000 | Loss: 0.00000911
Iteration 131/1000 | Loss: 0.00000911
Iteration 132/1000 | Loss: 0.00000911
Iteration 133/1000 | Loss: 0.00000911
Iteration 134/1000 | Loss: 0.00000911
Iteration 135/1000 | Loss: 0.00000911
Iteration 136/1000 | Loss: 0.00000910
Iteration 137/1000 | Loss: 0.00000910
Iteration 138/1000 | Loss: 0.00000910
Iteration 139/1000 | Loss: 0.00000910
Iteration 140/1000 | Loss: 0.00000910
Iteration 141/1000 | Loss: 0.00000910
Iteration 142/1000 | Loss: 0.00000910
Iteration 143/1000 | Loss: 0.00000910
Iteration 144/1000 | Loss: 0.00000910
Iteration 145/1000 | Loss: 0.00000910
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000910
Iteration 154/1000 | Loss: 0.00000910
Iteration 155/1000 | Loss: 0.00000910
Iteration 156/1000 | Loss: 0.00000910
Iteration 157/1000 | Loss: 0.00000910
Iteration 158/1000 | Loss: 0.00000910
Iteration 159/1000 | Loss: 0.00000910
Iteration 160/1000 | Loss: 0.00000910
Iteration 161/1000 | Loss: 0.00000910
Iteration 162/1000 | Loss: 0.00000910
Iteration 163/1000 | Loss: 0.00000910
Iteration 164/1000 | Loss: 0.00000910
Iteration 165/1000 | Loss: 0.00000910
Iteration 166/1000 | Loss: 0.00000910
Iteration 167/1000 | Loss: 0.00000910
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [9.097154361370485e-06, 9.097154361370485e-06, 9.097154361370485e-06, 9.097154361370485e-06, 9.097154361370485e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.097154361370485e-06

Optimization complete. Final v2v error: 2.594407081604004 mm

Highest mean error: 3.566809892654419 mm for frame 66

Lowest mean error: 2.1873135566711426 mm for frame 141

Saving results

Total time: 63.733412981033325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064857
Iteration 2/25 | Loss: 0.00373310
Iteration 3/25 | Loss: 0.00238963
Iteration 4/25 | Loss: 0.00206047
Iteration 5/25 | Loss: 0.00188449
Iteration 6/25 | Loss: 0.00173362
Iteration 7/25 | Loss: 0.00173031
Iteration 8/25 | Loss: 0.00158076
Iteration 9/25 | Loss: 0.00153715
Iteration 10/25 | Loss: 0.00145654
Iteration 11/25 | Loss: 0.00143728
Iteration 12/25 | Loss: 0.00139424
Iteration 13/25 | Loss: 0.00142408
Iteration 14/25 | Loss: 0.00139342
Iteration 15/25 | Loss: 0.00140109
Iteration 16/25 | Loss: 0.00139297
Iteration 17/25 | Loss: 0.00140893
Iteration 18/25 | Loss: 0.00136918
Iteration 19/25 | Loss: 0.00136253
Iteration 20/25 | Loss: 0.00136061
Iteration 21/25 | Loss: 0.00136151
Iteration 22/25 | Loss: 0.00136668
Iteration 23/25 | Loss: 0.00136420
Iteration 24/25 | Loss: 0.00135706
Iteration 25/25 | Loss: 0.00135272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24541795
Iteration 2/25 | Loss: 0.00513975
Iteration 3/25 | Loss: 0.00504369
Iteration 4/25 | Loss: 0.00504369
Iteration 5/25 | Loss: 0.00504369
Iteration 6/25 | Loss: 0.00504369
Iteration 7/25 | Loss: 0.00504369
Iteration 8/25 | Loss: 0.00504369
Iteration 9/25 | Loss: 0.00504369
Iteration 10/25 | Loss: 0.00504369
Iteration 11/25 | Loss: 0.00504369
Iteration 12/25 | Loss: 0.00504369
Iteration 13/25 | Loss: 0.00504369
Iteration 14/25 | Loss: 0.00504369
Iteration 15/25 | Loss: 0.00504369
Iteration 16/25 | Loss: 0.00504369
Iteration 17/25 | Loss: 0.00504369
Iteration 18/25 | Loss: 0.00504369
Iteration 19/25 | Loss: 0.00504369
Iteration 20/25 | Loss: 0.00504369
Iteration 21/25 | Loss: 0.00504369
Iteration 22/25 | Loss: 0.00504369
Iteration 23/25 | Loss: 0.00504369
Iteration 24/25 | Loss: 0.00504369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.005043687764555216, 0.005043687764555216, 0.005043687764555216, 0.005043687764555216, 0.005043687764555216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005043687764555216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00504369
Iteration 2/1000 | Loss: 0.00221859
Iteration 3/1000 | Loss: 0.00277199
Iteration 4/1000 | Loss: 0.00271104
Iteration 5/1000 | Loss: 0.00233489
Iteration 6/1000 | Loss: 0.00223345
Iteration 7/1000 | Loss: 0.00371641
Iteration 8/1000 | Loss: 0.00178792
Iteration 9/1000 | Loss: 0.00482514
Iteration 10/1000 | Loss: 0.00240854
Iteration 11/1000 | Loss: 0.00396028
Iteration 12/1000 | Loss: 0.00431497
Iteration 13/1000 | Loss: 0.00159374
Iteration 14/1000 | Loss: 0.00364666
Iteration 15/1000 | Loss: 0.00405602
Iteration 16/1000 | Loss: 0.00243527
Iteration 17/1000 | Loss: 0.00217565
Iteration 18/1000 | Loss: 0.00312445
Iteration 19/1000 | Loss: 0.00391670
Iteration 20/1000 | Loss: 0.00173728
Iteration 21/1000 | Loss: 0.00305001
Iteration 22/1000 | Loss: 0.00181230
Iteration 23/1000 | Loss: 0.00277113
Iteration 24/1000 | Loss: 0.00344687
Iteration 25/1000 | Loss: 0.00164135
Iteration 26/1000 | Loss: 0.00043182
Iteration 27/1000 | Loss: 0.00036521
Iteration 28/1000 | Loss: 0.00074321
Iteration 29/1000 | Loss: 0.00075462
Iteration 30/1000 | Loss: 0.00155518
Iteration 31/1000 | Loss: 0.00096014
Iteration 32/1000 | Loss: 0.00085389
Iteration 33/1000 | Loss: 0.00061040
Iteration 34/1000 | Loss: 0.00124746
Iteration 35/1000 | Loss: 0.00053237
Iteration 36/1000 | Loss: 0.00055933
Iteration 37/1000 | Loss: 0.00056274
Iteration 38/1000 | Loss: 0.00037478
Iteration 39/1000 | Loss: 0.00137322
Iteration 40/1000 | Loss: 0.00081861
Iteration 41/1000 | Loss: 0.00068573
Iteration 42/1000 | Loss: 0.00117793
Iteration 43/1000 | Loss: 0.00079437
Iteration 44/1000 | Loss: 0.00525517
Iteration 45/1000 | Loss: 0.00923482
Iteration 46/1000 | Loss: 0.00600526
Iteration 47/1000 | Loss: 0.00187410
Iteration 48/1000 | Loss: 0.00325380
Iteration 49/1000 | Loss: 0.00244497
Iteration 50/1000 | Loss: 0.00242934
Iteration 51/1000 | Loss: 0.00154863
Iteration 52/1000 | Loss: 0.00233625
Iteration 53/1000 | Loss: 0.00318779
Iteration 54/1000 | Loss: 0.00054875
Iteration 55/1000 | Loss: 0.00034536
Iteration 56/1000 | Loss: 0.00054627
Iteration 57/1000 | Loss: 0.00067093
Iteration 58/1000 | Loss: 0.00075645
Iteration 59/1000 | Loss: 0.00050829
Iteration 60/1000 | Loss: 0.00086174
Iteration 61/1000 | Loss: 0.00059896
Iteration 62/1000 | Loss: 0.00076205
Iteration 63/1000 | Loss: 0.00040778
Iteration 64/1000 | Loss: 0.00082073
Iteration 65/1000 | Loss: 0.00049104
Iteration 66/1000 | Loss: 0.00300419
Iteration 67/1000 | Loss: 0.00086835
Iteration 68/1000 | Loss: 0.00138608
Iteration 69/1000 | Loss: 0.00038709
Iteration 70/1000 | Loss: 0.00051137
Iteration 71/1000 | Loss: 0.00050657
Iteration 72/1000 | Loss: 0.00052316
Iteration 73/1000 | Loss: 0.00032776
Iteration 74/1000 | Loss: 0.00047127
Iteration 75/1000 | Loss: 0.00023843
Iteration 76/1000 | Loss: 0.00053650
Iteration 77/1000 | Loss: 0.00087821
Iteration 78/1000 | Loss: 0.00072674
Iteration 79/1000 | Loss: 0.00141154
Iteration 80/1000 | Loss: 0.00042137
Iteration 81/1000 | Loss: 0.00028281
Iteration 82/1000 | Loss: 0.00072003
Iteration 83/1000 | Loss: 0.00107797
Iteration 84/1000 | Loss: 0.00109325
Iteration 85/1000 | Loss: 0.00110803
Iteration 86/1000 | Loss: 0.00138333
Iteration 87/1000 | Loss: 0.00044678
Iteration 88/1000 | Loss: 0.00009084
Iteration 89/1000 | Loss: 0.00010639
Iteration 90/1000 | Loss: 0.00009527
Iteration 91/1000 | Loss: 0.00009246
Iteration 92/1000 | Loss: 0.00009652
Iteration 93/1000 | Loss: 0.00013632
Iteration 94/1000 | Loss: 0.00007981
Iteration 95/1000 | Loss: 0.00007903
Iteration 96/1000 | Loss: 0.00011807
Iteration 97/1000 | Loss: 0.00065512
Iteration 98/1000 | Loss: 0.00134098
Iteration 99/1000 | Loss: 0.00289761
Iteration 100/1000 | Loss: 0.00080423
Iteration 101/1000 | Loss: 0.00041988
Iteration 102/1000 | Loss: 0.00045215
Iteration 103/1000 | Loss: 0.00032534
Iteration 104/1000 | Loss: 0.00041793
Iteration 105/1000 | Loss: 0.00035361
Iteration 106/1000 | Loss: 0.00066743
Iteration 107/1000 | Loss: 0.00069088
Iteration 108/1000 | Loss: 0.00091961
Iteration 109/1000 | Loss: 0.00061422
Iteration 110/1000 | Loss: 0.00031637
Iteration 111/1000 | Loss: 0.00011933
Iteration 112/1000 | Loss: 0.00019985
Iteration 113/1000 | Loss: 0.00008848
Iteration 114/1000 | Loss: 0.00031587
Iteration 115/1000 | Loss: 0.00087815
Iteration 116/1000 | Loss: 0.00060725
Iteration 117/1000 | Loss: 0.00030318
Iteration 118/1000 | Loss: 0.00048870
Iteration 119/1000 | Loss: 0.00048818
Iteration 120/1000 | Loss: 0.00011581
Iteration 121/1000 | Loss: 0.00019350
Iteration 122/1000 | Loss: 0.00045092
Iteration 123/1000 | Loss: 0.00042550
Iteration 124/1000 | Loss: 0.00054773
Iteration 125/1000 | Loss: 0.00039963
Iteration 126/1000 | Loss: 0.00034312
Iteration 127/1000 | Loss: 0.00048412
Iteration 128/1000 | Loss: 0.00070528
Iteration 129/1000 | Loss: 0.00044250
Iteration 130/1000 | Loss: 0.00046468
Iteration 131/1000 | Loss: 0.00043409
Iteration 132/1000 | Loss: 0.00024253
Iteration 133/1000 | Loss: 0.00053644
Iteration 134/1000 | Loss: 0.00020914
Iteration 135/1000 | Loss: 0.00044204
Iteration 136/1000 | Loss: 0.00005411
Iteration 137/1000 | Loss: 0.00022940
Iteration 138/1000 | Loss: 0.00004291
Iteration 139/1000 | Loss: 0.00004188
Iteration 140/1000 | Loss: 0.00037828
Iteration 141/1000 | Loss: 0.00169625
Iteration 142/1000 | Loss: 0.00123605
Iteration 143/1000 | Loss: 0.00109788
Iteration 144/1000 | Loss: 0.00046007
Iteration 145/1000 | Loss: 0.00020845
Iteration 146/1000 | Loss: 0.00045494
Iteration 147/1000 | Loss: 0.00021060
Iteration 148/1000 | Loss: 0.00009183
Iteration 149/1000 | Loss: 0.00003886
Iteration 150/1000 | Loss: 0.00024590
Iteration 151/1000 | Loss: 0.00011828
Iteration 152/1000 | Loss: 0.00074332
Iteration 153/1000 | Loss: 0.00025306
Iteration 154/1000 | Loss: 0.00004157
Iteration 155/1000 | Loss: 0.00005785
Iteration 156/1000 | Loss: 0.00004300
Iteration 157/1000 | Loss: 0.00003895
Iteration 158/1000 | Loss: 0.00005141
Iteration 159/1000 | Loss: 0.00003684
Iteration 160/1000 | Loss: 0.00098559
Iteration 161/1000 | Loss: 0.00024999
Iteration 162/1000 | Loss: 0.00043339
Iteration 163/1000 | Loss: 0.00005678
Iteration 164/1000 | Loss: 0.00009036
Iteration 165/1000 | Loss: 0.00040563
Iteration 166/1000 | Loss: 0.00052101
Iteration 167/1000 | Loss: 0.00006746
Iteration 168/1000 | Loss: 0.00003462
Iteration 169/1000 | Loss: 0.00031340
Iteration 170/1000 | Loss: 0.00017873
Iteration 171/1000 | Loss: 0.00076936
Iteration 172/1000 | Loss: 0.00034330
Iteration 173/1000 | Loss: 0.00015849
Iteration 174/1000 | Loss: 0.00014142
Iteration 175/1000 | Loss: 0.00089216
Iteration 176/1000 | Loss: 0.00078548
Iteration 177/1000 | Loss: 0.00030797
Iteration 178/1000 | Loss: 0.00006521
Iteration 179/1000 | Loss: 0.00004047
Iteration 180/1000 | Loss: 0.00004553
Iteration 181/1000 | Loss: 0.00004849
Iteration 182/1000 | Loss: 0.00003367
Iteration 183/1000 | Loss: 0.00007602
Iteration 184/1000 | Loss: 0.00003692
Iteration 185/1000 | Loss: 0.00002903
Iteration 186/1000 | Loss: 0.00002805
Iteration 187/1000 | Loss: 0.00005388
Iteration 188/1000 | Loss: 0.00004552
Iteration 189/1000 | Loss: 0.00006437
Iteration 190/1000 | Loss: 0.00007368
Iteration 191/1000 | Loss: 0.00005286
Iteration 192/1000 | Loss: 0.00002798
Iteration 193/1000 | Loss: 0.00037214
Iteration 194/1000 | Loss: 0.00024458
Iteration 195/1000 | Loss: 0.00003327
Iteration 196/1000 | Loss: 0.00033195
Iteration 197/1000 | Loss: 0.00022990
Iteration 198/1000 | Loss: 0.00056564
Iteration 199/1000 | Loss: 0.00061176
Iteration 200/1000 | Loss: 0.00037984
Iteration 201/1000 | Loss: 0.00006260
Iteration 202/1000 | Loss: 0.00002977
Iteration 203/1000 | Loss: 0.00060930
Iteration 204/1000 | Loss: 0.00006402
Iteration 205/1000 | Loss: 0.00043231
Iteration 206/1000 | Loss: 0.00019144
Iteration 207/1000 | Loss: 0.00003203
Iteration 208/1000 | Loss: 0.00037898
Iteration 209/1000 | Loss: 0.00029691
Iteration 210/1000 | Loss: 0.00027087
Iteration 211/1000 | Loss: 0.00031802
Iteration 212/1000 | Loss: 0.00003247
Iteration 213/1000 | Loss: 0.00004057
Iteration 214/1000 | Loss: 0.00002399
Iteration 215/1000 | Loss: 0.00002260
Iteration 216/1000 | Loss: 0.00044523
Iteration 217/1000 | Loss: 0.00020396
Iteration 218/1000 | Loss: 0.00009153
Iteration 219/1000 | Loss: 0.00018814
Iteration 220/1000 | Loss: 0.00003889
Iteration 221/1000 | Loss: 0.00005927
Iteration 222/1000 | Loss: 0.00002693
Iteration 223/1000 | Loss: 0.00002557
Iteration 224/1000 | Loss: 0.00004541
Iteration 225/1000 | Loss: 0.00003209
Iteration 226/1000 | Loss: 0.00046376
Iteration 227/1000 | Loss: 0.00031733
Iteration 228/1000 | Loss: 0.00008070
Iteration 229/1000 | Loss: 0.00004541
Iteration 230/1000 | Loss: 0.00002733
Iteration 231/1000 | Loss: 0.00002611
Iteration 232/1000 | Loss: 0.00002666
Iteration 233/1000 | Loss: 0.00002666
Iteration 234/1000 | Loss: 0.00003325
Iteration 235/1000 | Loss: 0.00002609
Iteration 236/1000 | Loss: 0.00002829
Iteration 237/1000 | Loss: 0.00002632
Iteration 238/1000 | Loss: 0.00002559
Iteration 239/1000 | Loss: 0.00002560
Iteration 240/1000 | Loss: 0.00002472
Iteration 241/1000 | Loss: 0.00002515
Iteration 242/1000 | Loss: 0.00002452
Iteration 243/1000 | Loss: 0.00002520
Iteration 244/1000 | Loss: 0.00002459
Iteration 245/1000 | Loss: 0.00002533
Iteration 246/1000 | Loss: 0.00048549
Iteration 247/1000 | Loss: 0.00014022
Iteration 248/1000 | Loss: 0.00002672
Iteration 249/1000 | Loss: 0.00004209
Iteration 250/1000 | Loss: 0.00002482
Iteration 251/1000 | Loss: 0.00002641
Iteration 252/1000 | Loss: 0.00002528
Iteration 253/1000 | Loss: 0.00002497
Iteration 254/1000 | Loss: 0.00002500
Iteration 255/1000 | Loss: 0.00002536
Iteration 256/1000 | Loss: 0.00048350
Iteration 257/1000 | Loss: 0.00027114
Iteration 258/1000 | Loss: 0.00009250
Iteration 259/1000 | Loss: 0.00002945
Iteration 260/1000 | Loss: 0.00004558
Iteration 261/1000 | Loss: 0.00002490
Iteration 262/1000 | Loss: 0.00037854
Iteration 263/1000 | Loss: 0.00015485
Iteration 264/1000 | Loss: 0.00002718
Iteration 265/1000 | Loss: 0.00002428
Iteration 266/1000 | Loss: 0.00002312
Iteration 267/1000 | Loss: 0.00002352
Iteration 268/1000 | Loss: 0.00002300
Iteration 269/1000 | Loss: 0.00002236
Iteration 270/1000 | Loss: 0.00002305
Iteration 271/1000 | Loss: 0.00002337
Iteration 272/1000 | Loss: 0.00024467
Iteration 273/1000 | Loss: 0.00013049
Iteration 274/1000 | Loss: 0.00032412
Iteration 275/1000 | Loss: 0.00007199
Iteration 276/1000 | Loss: 0.00006748
Iteration 277/1000 | Loss: 0.00025151
Iteration 278/1000 | Loss: 0.00046027
Iteration 279/1000 | Loss: 0.00020830
Iteration 280/1000 | Loss: 0.00030538
Iteration 281/1000 | Loss: 0.00019732
Iteration 282/1000 | Loss: 0.00033855
Iteration 283/1000 | Loss: 0.00020965
Iteration 284/1000 | Loss: 0.00003135
Iteration 285/1000 | Loss: 0.00002115
Iteration 286/1000 | Loss: 0.00003267
Iteration 287/1000 | Loss: 0.00002031
Iteration 288/1000 | Loss: 0.00002444
Iteration 289/1000 | Loss: 0.00001961
Iteration 290/1000 | Loss: 0.00002121
Iteration 291/1000 | Loss: 0.00001894
Iteration 292/1000 | Loss: 0.00001868
Iteration 293/1000 | Loss: 0.00001848
Iteration 294/1000 | Loss: 0.00029970
Iteration 295/1000 | Loss: 0.00007560
Iteration 296/1000 | Loss: 0.00001935
Iteration 297/1000 | Loss: 0.00001845
Iteration 298/1000 | Loss: 0.00004083
Iteration 299/1000 | Loss: 0.00056285
Iteration 300/1000 | Loss: 0.00009444
Iteration 301/1000 | Loss: 0.00012632
Iteration 302/1000 | Loss: 0.00028895
Iteration 303/1000 | Loss: 0.00029437
Iteration 304/1000 | Loss: 0.00061951
Iteration 305/1000 | Loss: 0.00026370
Iteration 306/1000 | Loss: 0.00002721
Iteration 307/1000 | Loss: 0.00003325
Iteration 308/1000 | Loss: 0.00013950
Iteration 309/1000 | Loss: 0.00022465
Iteration 310/1000 | Loss: 0.00002219
Iteration 311/1000 | Loss: 0.00002424
Iteration 312/1000 | Loss: 0.00005532
Iteration 313/1000 | Loss: 0.00002450
Iteration 314/1000 | Loss: 0.00003823
Iteration 315/1000 | Loss: 0.00023605
Iteration 316/1000 | Loss: 0.00031325
Iteration 317/1000 | Loss: 0.00063863
Iteration 318/1000 | Loss: 0.00061852
Iteration 319/1000 | Loss: 0.00032610
Iteration 320/1000 | Loss: 0.00011667
Iteration 321/1000 | Loss: 0.00003898
Iteration 322/1000 | Loss: 0.00013086
Iteration 323/1000 | Loss: 0.00003418
Iteration 324/1000 | Loss: 0.00002437
Iteration 325/1000 | Loss: 0.00028702
Iteration 326/1000 | Loss: 0.00045326
Iteration 327/1000 | Loss: 0.00001905
Iteration 328/1000 | Loss: 0.00001749
Iteration 329/1000 | Loss: 0.00001684
Iteration 330/1000 | Loss: 0.00001654
Iteration 331/1000 | Loss: 0.00002620
Iteration 332/1000 | Loss: 0.00003563
Iteration 333/1000 | Loss: 0.00003227
Iteration 334/1000 | Loss: 0.00001623
Iteration 335/1000 | Loss: 0.00001617
Iteration 336/1000 | Loss: 0.00001617
Iteration 337/1000 | Loss: 0.00001616
Iteration 338/1000 | Loss: 0.00001610
Iteration 339/1000 | Loss: 0.00001610
Iteration 340/1000 | Loss: 0.00001610
Iteration 341/1000 | Loss: 0.00002212
Iteration 342/1000 | Loss: 0.00033307
Iteration 343/1000 | Loss: 0.00010253
Iteration 344/1000 | Loss: 0.00001783
Iteration 345/1000 | Loss: 0.00001612
Iteration 346/1000 | Loss: 0.00001606
Iteration 347/1000 | Loss: 0.00001605
Iteration 348/1000 | Loss: 0.00001604
Iteration 349/1000 | Loss: 0.00001604
Iteration 350/1000 | Loss: 0.00001604
Iteration 351/1000 | Loss: 0.00001604
Iteration 352/1000 | Loss: 0.00001604
Iteration 353/1000 | Loss: 0.00001604
Iteration 354/1000 | Loss: 0.00001604
Iteration 355/1000 | Loss: 0.00001604
Iteration 356/1000 | Loss: 0.00001604
Iteration 357/1000 | Loss: 0.00001604
Iteration 358/1000 | Loss: 0.00001604
Iteration 359/1000 | Loss: 0.00001604
Iteration 360/1000 | Loss: 0.00001604
Iteration 361/1000 | Loss: 0.00001604
Iteration 362/1000 | Loss: 0.00001604
Iteration 363/1000 | Loss: 0.00001604
Iteration 364/1000 | Loss: 0.00001604
Iteration 365/1000 | Loss: 0.00001604
Iteration 366/1000 | Loss: 0.00001604
Iteration 367/1000 | Loss: 0.00001604
Iteration 368/1000 | Loss: 0.00001604
Iteration 369/1000 | Loss: 0.00001604
Iteration 370/1000 | Loss: 0.00001604
Iteration 371/1000 | Loss: 0.00001604
Iteration 372/1000 | Loss: 0.00001604
Iteration 373/1000 | Loss: 0.00001604
Iteration 374/1000 | Loss: 0.00001604
Iteration 375/1000 | Loss: 0.00001604
Iteration 376/1000 | Loss: 0.00001604
Iteration 377/1000 | Loss: 0.00001604
Iteration 378/1000 | Loss: 0.00001604
Iteration 379/1000 | Loss: 0.00001604
Iteration 380/1000 | Loss: 0.00001604
Iteration 381/1000 | Loss: 0.00001604
Iteration 382/1000 | Loss: 0.00001604
Iteration 383/1000 | Loss: 0.00001604
Iteration 384/1000 | Loss: 0.00001604
Iteration 385/1000 | Loss: 0.00001604
Iteration 386/1000 | Loss: 0.00001604
Iteration 387/1000 | Loss: 0.00001604
Iteration 388/1000 | Loss: 0.00001604
Iteration 389/1000 | Loss: 0.00001604
Iteration 390/1000 | Loss: 0.00001604
Iteration 391/1000 | Loss: 0.00001604
Iteration 392/1000 | Loss: 0.00001604
Iteration 393/1000 | Loss: 0.00001604
Iteration 394/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 394. Stopping optimization.
Last 5 losses: [1.6036890883697197e-05, 1.6036890883697197e-05, 1.6036890883697197e-05, 1.6036890883697197e-05, 1.6036890883697197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6036890883697197e-05

Optimization complete. Final v2v error: 3.0736124515533447 mm

Highest mean error: 12.089463233947754 mm for frame 164

Lowest mean error: 2.486854314804077 mm for frame 233

Saving results

Total time: 592.0288779735565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402752
Iteration 2/25 | Loss: 0.00104538
Iteration 3/25 | Loss: 0.00094875
Iteration 4/25 | Loss: 0.00094031
Iteration 5/25 | Loss: 0.00093901
Iteration 6/25 | Loss: 0.00093901
Iteration 7/25 | Loss: 0.00093901
Iteration 8/25 | Loss: 0.00093901
Iteration 9/25 | Loss: 0.00093901
Iteration 10/25 | Loss: 0.00093901
Iteration 11/25 | Loss: 0.00093901
Iteration 12/25 | Loss: 0.00093901
Iteration 13/25 | Loss: 0.00093901
Iteration 14/25 | Loss: 0.00093901
Iteration 15/25 | Loss: 0.00093901
Iteration 16/25 | Loss: 0.00093901
Iteration 17/25 | Loss: 0.00093901
Iteration 18/25 | Loss: 0.00093901
Iteration 19/25 | Loss: 0.00093901
Iteration 20/25 | Loss: 0.00093901
Iteration 21/25 | Loss: 0.00093901
Iteration 22/25 | Loss: 0.00093901
Iteration 23/25 | Loss: 0.00093901
Iteration 24/25 | Loss: 0.00093901
Iteration 25/25 | Loss: 0.00093901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009390084305778146, 0.0009390084305778146, 0.0009390084305778146, 0.0009390084305778146, 0.0009390084305778146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009390084305778146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28734076
Iteration 2/25 | Loss: 0.00117490
Iteration 3/25 | Loss: 0.00117489
Iteration 4/25 | Loss: 0.00117489
Iteration 5/25 | Loss: 0.00117489
Iteration 6/25 | Loss: 0.00117489
Iteration 7/25 | Loss: 0.00117489
Iteration 8/25 | Loss: 0.00117489
Iteration 9/25 | Loss: 0.00117489
Iteration 10/25 | Loss: 0.00117489
Iteration 11/25 | Loss: 0.00117489
Iteration 12/25 | Loss: 0.00117489
Iteration 13/25 | Loss: 0.00117489
Iteration 14/25 | Loss: 0.00117489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011748899705708027, 0.0011748899705708027, 0.0011748899705708027, 0.0011748899705708027, 0.0011748899705708027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011748899705708027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117489
Iteration 2/1000 | Loss: 0.00001702
Iteration 3/1000 | Loss: 0.00001146
Iteration 4/1000 | Loss: 0.00000991
Iteration 5/1000 | Loss: 0.00000927
Iteration 6/1000 | Loss: 0.00000891
Iteration 7/1000 | Loss: 0.00000890
Iteration 8/1000 | Loss: 0.00000878
Iteration 9/1000 | Loss: 0.00000877
Iteration 10/1000 | Loss: 0.00000877
Iteration 11/1000 | Loss: 0.00000866
Iteration 12/1000 | Loss: 0.00000850
Iteration 13/1000 | Loss: 0.00000850
Iteration 14/1000 | Loss: 0.00000837
Iteration 15/1000 | Loss: 0.00000829
Iteration 16/1000 | Loss: 0.00000828
Iteration 17/1000 | Loss: 0.00000828
Iteration 18/1000 | Loss: 0.00000827
Iteration 19/1000 | Loss: 0.00000826
Iteration 20/1000 | Loss: 0.00000826
Iteration 21/1000 | Loss: 0.00000816
Iteration 22/1000 | Loss: 0.00000810
Iteration 23/1000 | Loss: 0.00000808
Iteration 24/1000 | Loss: 0.00000807
Iteration 25/1000 | Loss: 0.00000807
Iteration 26/1000 | Loss: 0.00000807
Iteration 27/1000 | Loss: 0.00000807
Iteration 28/1000 | Loss: 0.00000806
Iteration 29/1000 | Loss: 0.00000806
Iteration 30/1000 | Loss: 0.00000805
Iteration 31/1000 | Loss: 0.00000805
Iteration 32/1000 | Loss: 0.00000804
Iteration 33/1000 | Loss: 0.00000804
Iteration 34/1000 | Loss: 0.00000804
Iteration 35/1000 | Loss: 0.00000804
Iteration 36/1000 | Loss: 0.00000804
Iteration 37/1000 | Loss: 0.00000803
Iteration 38/1000 | Loss: 0.00000803
Iteration 39/1000 | Loss: 0.00000803
Iteration 40/1000 | Loss: 0.00000803
Iteration 41/1000 | Loss: 0.00000803
Iteration 42/1000 | Loss: 0.00000803
Iteration 43/1000 | Loss: 0.00000803
Iteration 44/1000 | Loss: 0.00000802
Iteration 45/1000 | Loss: 0.00000802
Iteration 46/1000 | Loss: 0.00000802
Iteration 47/1000 | Loss: 0.00000801
Iteration 48/1000 | Loss: 0.00000801
Iteration 49/1000 | Loss: 0.00000801
Iteration 50/1000 | Loss: 0.00000801
Iteration 51/1000 | Loss: 0.00000800
Iteration 52/1000 | Loss: 0.00000800
Iteration 53/1000 | Loss: 0.00000800
Iteration 54/1000 | Loss: 0.00000799
Iteration 55/1000 | Loss: 0.00000799
Iteration 56/1000 | Loss: 0.00000799
Iteration 57/1000 | Loss: 0.00000799
Iteration 58/1000 | Loss: 0.00000798
Iteration 59/1000 | Loss: 0.00000798
Iteration 60/1000 | Loss: 0.00000798
Iteration 61/1000 | Loss: 0.00000798
Iteration 62/1000 | Loss: 0.00000797
Iteration 63/1000 | Loss: 0.00000797
Iteration 64/1000 | Loss: 0.00000797
Iteration 65/1000 | Loss: 0.00000797
Iteration 66/1000 | Loss: 0.00000796
Iteration 67/1000 | Loss: 0.00000796
Iteration 68/1000 | Loss: 0.00000795
Iteration 69/1000 | Loss: 0.00000795
Iteration 70/1000 | Loss: 0.00000795
Iteration 71/1000 | Loss: 0.00000795
Iteration 72/1000 | Loss: 0.00000794
Iteration 73/1000 | Loss: 0.00000794
Iteration 74/1000 | Loss: 0.00000794
Iteration 75/1000 | Loss: 0.00000794
Iteration 76/1000 | Loss: 0.00000794
Iteration 77/1000 | Loss: 0.00000794
Iteration 78/1000 | Loss: 0.00000794
Iteration 79/1000 | Loss: 0.00000794
Iteration 80/1000 | Loss: 0.00000794
Iteration 81/1000 | Loss: 0.00000794
Iteration 82/1000 | Loss: 0.00000794
Iteration 83/1000 | Loss: 0.00000793
Iteration 84/1000 | Loss: 0.00000793
Iteration 85/1000 | Loss: 0.00000793
Iteration 86/1000 | Loss: 0.00000793
Iteration 87/1000 | Loss: 0.00000793
Iteration 88/1000 | Loss: 0.00000793
Iteration 89/1000 | Loss: 0.00000793
Iteration 90/1000 | Loss: 0.00000793
Iteration 91/1000 | Loss: 0.00000793
Iteration 92/1000 | Loss: 0.00000793
Iteration 93/1000 | Loss: 0.00000793
Iteration 94/1000 | Loss: 0.00000793
Iteration 95/1000 | Loss: 0.00000793
Iteration 96/1000 | Loss: 0.00000792
Iteration 97/1000 | Loss: 0.00000792
Iteration 98/1000 | Loss: 0.00000792
Iteration 99/1000 | Loss: 0.00000792
Iteration 100/1000 | Loss: 0.00000792
Iteration 101/1000 | Loss: 0.00000792
Iteration 102/1000 | Loss: 0.00000792
Iteration 103/1000 | Loss: 0.00000792
Iteration 104/1000 | Loss: 0.00000792
Iteration 105/1000 | Loss: 0.00000792
Iteration 106/1000 | Loss: 0.00000792
Iteration 107/1000 | Loss: 0.00000792
Iteration 108/1000 | Loss: 0.00000792
Iteration 109/1000 | Loss: 0.00000791
Iteration 110/1000 | Loss: 0.00000791
Iteration 111/1000 | Loss: 0.00000791
Iteration 112/1000 | Loss: 0.00000791
Iteration 113/1000 | Loss: 0.00000791
Iteration 114/1000 | Loss: 0.00000791
Iteration 115/1000 | Loss: 0.00000791
Iteration 116/1000 | Loss: 0.00000791
Iteration 117/1000 | Loss: 0.00000791
Iteration 118/1000 | Loss: 0.00000791
Iteration 119/1000 | Loss: 0.00000791
Iteration 120/1000 | Loss: 0.00000791
Iteration 121/1000 | Loss: 0.00000791
Iteration 122/1000 | Loss: 0.00000791
Iteration 123/1000 | Loss: 0.00000791
Iteration 124/1000 | Loss: 0.00000791
Iteration 125/1000 | Loss: 0.00000790
Iteration 126/1000 | Loss: 0.00000790
Iteration 127/1000 | Loss: 0.00000790
Iteration 128/1000 | Loss: 0.00000790
Iteration 129/1000 | Loss: 0.00000790
Iteration 130/1000 | Loss: 0.00000790
Iteration 131/1000 | Loss: 0.00000790
Iteration 132/1000 | Loss: 0.00000790
Iteration 133/1000 | Loss: 0.00000790
Iteration 134/1000 | Loss: 0.00000790
Iteration 135/1000 | Loss: 0.00000790
Iteration 136/1000 | Loss: 0.00000790
Iteration 137/1000 | Loss: 0.00000790
Iteration 138/1000 | Loss: 0.00000790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [7.90181275078794e-06, 7.90181275078794e-06, 7.90181275078794e-06, 7.90181275078794e-06, 7.90181275078794e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.90181275078794e-06

Optimization complete. Final v2v error: 2.4237258434295654 mm

Highest mean error: 2.6708874702453613 mm for frame 119

Lowest mean error: 2.0940539836883545 mm for frame 199

Saving results

Total time: 31.701858282089233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041409
Iteration 2/25 | Loss: 0.00202175
Iteration 3/25 | Loss: 0.00132585
Iteration 4/25 | Loss: 0.00119772
Iteration 5/25 | Loss: 0.00126899
Iteration 6/25 | Loss: 0.00122036
Iteration 7/25 | Loss: 0.00119979
Iteration 8/25 | Loss: 0.00116895
Iteration 9/25 | Loss: 0.00106792
Iteration 10/25 | Loss: 0.00110099
Iteration 11/25 | Loss: 0.00110260
Iteration 12/25 | Loss: 0.00102751
Iteration 13/25 | Loss: 0.00108085
Iteration 14/25 | Loss: 0.00104073
Iteration 15/25 | Loss: 0.00104024
Iteration 16/25 | Loss: 0.00102444
Iteration 17/25 | Loss: 0.00102023
Iteration 18/25 | Loss: 0.00102377
Iteration 19/25 | Loss: 0.00102151
Iteration 20/25 | Loss: 0.00102087
Iteration 21/25 | Loss: 0.00101896
Iteration 22/25 | Loss: 0.00101027
Iteration 23/25 | Loss: 0.00101472
Iteration 24/25 | Loss: 0.00101371
Iteration 25/25 | Loss: 0.00100818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33177090
Iteration 2/25 | Loss: 0.00147391
Iteration 3/25 | Loss: 0.00147391
Iteration 4/25 | Loss: 0.00147391
Iteration 5/25 | Loss: 0.00147391
Iteration 6/25 | Loss: 0.00147391
Iteration 7/25 | Loss: 0.00147391
Iteration 8/25 | Loss: 0.00147391
Iteration 9/25 | Loss: 0.00147391
Iteration 10/25 | Loss: 0.00147391
Iteration 11/25 | Loss: 0.00147391
Iteration 12/25 | Loss: 0.00147391
Iteration 13/25 | Loss: 0.00147391
Iteration 14/25 | Loss: 0.00147391
Iteration 15/25 | Loss: 0.00147391
Iteration 16/25 | Loss: 0.00147391
Iteration 17/25 | Loss: 0.00147391
Iteration 18/25 | Loss: 0.00147391
Iteration 19/25 | Loss: 0.00147391
Iteration 20/25 | Loss: 0.00147391
Iteration 21/25 | Loss: 0.00147391
Iteration 22/25 | Loss: 0.00147391
Iteration 23/25 | Loss: 0.00147391
Iteration 24/25 | Loss: 0.00147391
Iteration 25/25 | Loss: 0.00147391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147391
Iteration 2/1000 | Loss: 0.00012286
Iteration 3/1000 | Loss: 0.00020241
Iteration 4/1000 | Loss: 0.00009755
Iteration 5/1000 | Loss: 0.00008686
Iteration 6/1000 | Loss: 0.00015534
Iteration 7/1000 | Loss: 0.00012029
Iteration 8/1000 | Loss: 0.00013135
Iteration 9/1000 | Loss: 0.00020525
Iteration 10/1000 | Loss: 0.00024284
Iteration 11/1000 | Loss: 0.00017832
Iteration 12/1000 | Loss: 0.00003912
Iteration 13/1000 | Loss: 0.00024664
Iteration 14/1000 | Loss: 0.00015285
Iteration 15/1000 | Loss: 0.00020135
Iteration 16/1000 | Loss: 0.00011338
Iteration 17/1000 | Loss: 0.00025344
Iteration 18/1000 | Loss: 0.00027622
Iteration 19/1000 | Loss: 0.00008243
Iteration 20/1000 | Loss: 0.00025848
Iteration 21/1000 | Loss: 0.00024666
Iteration 22/1000 | Loss: 0.00025823
Iteration 23/1000 | Loss: 0.00014115
Iteration 24/1000 | Loss: 0.00024097
Iteration 25/1000 | Loss: 0.00021098
Iteration 26/1000 | Loss: 0.00016559
Iteration 27/1000 | Loss: 0.00008897
Iteration 28/1000 | Loss: 0.00021087
Iteration 29/1000 | Loss: 0.00011057
Iteration 30/1000 | Loss: 0.00010559
Iteration 31/1000 | Loss: 0.00010839
Iteration 32/1000 | Loss: 0.00007522
Iteration 33/1000 | Loss: 0.00010575
Iteration 34/1000 | Loss: 0.00011438
Iteration 35/1000 | Loss: 0.00003642
Iteration 36/1000 | Loss: 0.00004618
Iteration 37/1000 | Loss: 0.00002924
Iteration 38/1000 | Loss: 0.00003312
Iteration 39/1000 | Loss: 0.00003566
Iteration 40/1000 | Loss: 0.00003078
Iteration 41/1000 | Loss: 0.00002970
Iteration 42/1000 | Loss: 0.00003056
Iteration 43/1000 | Loss: 0.00045015
Iteration 44/1000 | Loss: 0.00004288
Iteration 45/1000 | Loss: 0.00003327
Iteration 46/1000 | Loss: 0.00003486
Iteration 47/1000 | Loss: 0.00003214
Iteration 48/1000 | Loss: 0.00003528
Iteration 49/1000 | Loss: 0.00003215
Iteration 50/1000 | Loss: 0.00002272
Iteration 51/1000 | Loss: 0.00002035
Iteration 52/1000 | Loss: 0.00004827
Iteration 53/1000 | Loss: 0.00005077
Iteration 54/1000 | Loss: 0.00003308
Iteration 55/1000 | Loss: 0.00003185
Iteration 56/1000 | Loss: 0.00002357
Iteration 57/1000 | Loss: 0.00002618
Iteration 58/1000 | Loss: 0.00002726
Iteration 59/1000 | Loss: 0.00003918
Iteration 60/1000 | Loss: 0.00048500
Iteration 61/1000 | Loss: 0.00024242
Iteration 62/1000 | Loss: 0.00036679
Iteration 63/1000 | Loss: 0.00004608
Iteration 64/1000 | Loss: 0.00003709
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00002622
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00001763
Iteration 69/1000 | Loss: 0.00001934
Iteration 70/1000 | Loss: 0.00002992
Iteration 71/1000 | Loss: 0.00002745
Iteration 72/1000 | Loss: 0.00002836
Iteration 73/1000 | Loss: 0.00002644
Iteration 74/1000 | Loss: 0.00002723
Iteration 75/1000 | Loss: 0.00002566
Iteration 76/1000 | Loss: 0.00002866
Iteration 77/1000 | Loss: 0.00002754
Iteration 78/1000 | Loss: 0.00002497
Iteration 79/1000 | Loss: 0.00002637
Iteration 80/1000 | Loss: 0.00002918
Iteration 81/1000 | Loss: 0.00002681
Iteration 82/1000 | Loss: 0.00003480
Iteration 83/1000 | Loss: 0.00003146
Iteration 84/1000 | Loss: 0.00002530
Iteration 85/1000 | Loss: 0.00002888
Iteration 86/1000 | Loss: 0.00002419
Iteration 87/1000 | Loss: 0.00002836
Iteration 88/1000 | Loss: 0.00002590
Iteration 89/1000 | Loss: 0.00003088
Iteration 90/1000 | Loss: 0.00002588
Iteration 91/1000 | Loss: 0.00002913
Iteration 92/1000 | Loss: 0.00003840
Iteration 93/1000 | Loss: 0.00002811
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002809
Iteration 96/1000 | Loss: 0.00002262
Iteration 97/1000 | Loss: 0.00002906
Iteration 98/1000 | Loss: 0.00002837
Iteration 99/1000 | Loss: 0.00002777
Iteration 100/1000 | Loss: 0.00002660
Iteration 101/1000 | Loss: 0.00002781
Iteration 102/1000 | Loss: 0.00002657
Iteration 103/1000 | Loss: 0.00002683
Iteration 104/1000 | Loss: 0.00002563
Iteration 105/1000 | Loss: 0.00002671
Iteration 106/1000 | Loss: 0.00002502
Iteration 107/1000 | Loss: 0.00002742
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00002257
Iteration 110/1000 | Loss: 0.00003055
Iteration 111/1000 | Loss: 0.00002644
Iteration 112/1000 | Loss: 0.00002494
Iteration 113/1000 | Loss: 0.00002626
Iteration 114/1000 | Loss: 0.00003012
Iteration 115/1000 | Loss: 0.00004228
Iteration 116/1000 | Loss: 0.00002745
Iteration 117/1000 | Loss: 0.00002982
Iteration 118/1000 | Loss: 0.00002555
Iteration 119/1000 | Loss: 0.00003083
Iteration 120/1000 | Loss: 0.00002640
Iteration 121/1000 | Loss: 0.00002756
Iteration 122/1000 | Loss: 0.00002585
Iteration 123/1000 | Loss: 0.00002925
Iteration 124/1000 | Loss: 0.00002552
Iteration 125/1000 | Loss: 0.00003452
Iteration 126/1000 | Loss: 0.00002910
Iteration 127/1000 | Loss: 0.00002775
Iteration 128/1000 | Loss: 0.00002642
Iteration 129/1000 | Loss: 0.00003101
Iteration 130/1000 | Loss: 0.00002412
Iteration 131/1000 | Loss: 0.00002395
Iteration 132/1000 | Loss: 0.00002421
Iteration 133/1000 | Loss: 0.00003489
Iteration 134/1000 | Loss: 0.00003336
Iteration 135/1000 | Loss: 0.00003205
Iteration 136/1000 | Loss: 0.00002765
Iteration 137/1000 | Loss: 0.00003226
Iteration 138/1000 | Loss: 0.00002762
Iteration 139/1000 | Loss: 0.00004131
Iteration 140/1000 | Loss: 0.00002274
Iteration 141/1000 | Loss: 0.00001807
Iteration 142/1000 | Loss: 0.00001417
Iteration 143/1000 | Loss: 0.00001311
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001189
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001136
Iteration 149/1000 | Loss: 0.00001136
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001135
Iteration 154/1000 | Loss: 0.00001134
Iteration 155/1000 | Loss: 0.00001130
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001125
Iteration 163/1000 | Loss: 0.00001123
Iteration 164/1000 | Loss: 0.00001120
Iteration 165/1000 | Loss: 0.00001119
Iteration 166/1000 | Loss: 0.00001119
Iteration 167/1000 | Loss: 0.00001118
Iteration 168/1000 | Loss: 0.00001117
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001109
Iteration 171/1000 | Loss: 0.00001109
Iteration 172/1000 | Loss: 0.00001109
Iteration 173/1000 | Loss: 0.00001109
Iteration 174/1000 | Loss: 0.00001109
Iteration 175/1000 | Loss: 0.00001109
Iteration 176/1000 | Loss: 0.00001109
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001099
Iteration 183/1000 | Loss: 0.00001099
Iteration 184/1000 | Loss: 0.00001099
Iteration 185/1000 | Loss: 0.00001099
Iteration 186/1000 | Loss: 0.00001099
Iteration 187/1000 | Loss: 0.00001098
Iteration 188/1000 | Loss: 0.00001098
Iteration 189/1000 | Loss: 0.00001098
Iteration 190/1000 | Loss: 0.00001098
Iteration 191/1000 | Loss: 0.00001098
Iteration 192/1000 | Loss: 0.00001098
Iteration 193/1000 | Loss: 0.00001098
Iteration 194/1000 | Loss: 0.00001098
Iteration 195/1000 | Loss: 0.00001098
Iteration 196/1000 | Loss: 0.00001097
Iteration 197/1000 | Loss: 0.00001097
Iteration 198/1000 | Loss: 0.00001097
Iteration 199/1000 | Loss: 0.00001097
Iteration 200/1000 | Loss: 0.00001097
Iteration 201/1000 | Loss: 0.00001097
Iteration 202/1000 | Loss: 0.00001097
Iteration 203/1000 | Loss: 0.00001097
Iteration 204/1000 | Loss: 0.00001097
Iteration 205/1000 | Loss: 0.00001097
Iteration 206/1000 | Loss: 0.00001097
Iteration 207/1000 | Loss: 0.00001096
Iteration 208/1000 | Loss: 0.00001096
Iteration 209/1000 | Loss: 0.00001096
Iteration 210/1000 | Loss: 0.00001096
Iteration 211/1000 | Loss: 0.00001096
Iteration 212/1000 | Loss: 0.00001096
Iteration 213/1000 | Loss: 0.00001096
Iteration 214/1000 | Loss: 0.00001096
Iteration 215/1000 | Loss: 0.00001096
Iteration 216/1000 | Loss: 0.00001096
Iteration 217/1000 | Loss: 0.00001096
Iteration 218/1000 | Loss: 0.00001096
Iteration 219/1000 | Loss: 0.00001096
Iteration 220/1000 | Loss: 0.00001096
Iteration 221/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.0958148777717724e-05, 1.0958148777717724e-05, 1.0958148777717724e-05, 1.0958148777717724e-05, 1.0958148777717724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0958148777717724e-05

Optimization complete. Final v2v error: 2.8161368370056152 mm

Highest mean error: 3.815500020980835 mm for frame 64

Lowest mean error: 2.402838945388794 mm for frame 0

Saving results

Total time: 256.3485891819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987454
Iteration 2/25 | Loss: 0.00106344
Iteration 3/25 | Loss: 0.00093979
Iteration 4/25 | Loss: 0.00092654
Iteration 5/25 | Loss: 0.00092302
Iteration 6/25 | Loss: 0.00092241
Iteration 7/25 | Loss: 0.00092241
Iteration 8/25 | Loss: 0.00092241
Iteration 9/25 | Loss: 0.00092241
Iteration 10/25 | Loss: 0.00092241
Iteration 11/25 | Loss: 0.00092241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000922406034078449, 0.000922406034078449, 0.000922406034078449, 0.000922406034078449, 0.000922406034078449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000922406034078449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26821566
Iteration 2/25 | Loss: 0.00139087
Iteration 3/25 | Loss: 0.00139087
Iteration 4/25 | Loss: 0.00139087
Iteration 5/25 | Loss: 0.00139087
Iteration 6/25 | Loss: 0.00139087
Iteration 7/25 | Loss: 0.00139087
Iteration 8/25 | Loss: 0.00139087
Iteration 9/25 | Loss: 0.00139087
Iteration 10/25 | Loss: 0.00139087
Iteration 11/25 | Loss: 0.00139087
Iteration 12/25 | Loss: 0.00139087
Iteration 13/25 | Loss: 0.00139087
Iteration 14/25 | Loss: 0.00139087
Iteration 15/25 | Loss: 0.00139087
Iteration 16/25 | Loss: 0.00139087
Iteration 17/25 | Loss: 0.00139087
Iteration 18/25 | Loss: 0.00139087
Iteration 19/25 | Loss: 0.00139087
Iteration 20/25 | Loss: 0.00139087
Iteration 21/25 | Loss: 0.00139087
Iteration 22/25 | Loss: 0.00139087
Iteration 23/25 | Loss: 0.00139087
Iteration 24/25 | Loss: 0.00139087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013908699620515108, 0.0013908699620515108, 0.0013908699620515108, 0.0013908699620515108, 0.0013908699620515108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013908699620515108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139087
Iteration 2/1000 | Loss: 0.00002457
Iteration 3/1000 | Loss: 0.00001498
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001087
Iteration 7/1000 | Loss: 0.00001042
Iteration 8/1000 | Loss: 0.00000999
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000952
Iteration 11/1000 | Loss: 0.00000948
Iteration 12/1000 | Loss: 0.00000943
Iteration 13/1000 | Loss: 0.00000943
Iteration 14/1000 | Loss: 0.00000943
Iteration 15/1000 | Loss: 0.00000943
Iteration 16/1000 | Loss: 0.00000943
Iteration 17/1000 | Loss: 0.00000942
Iteration 18/1000 | Loss: 0.00000942
Iteration 19/1000 | Loss: 0.00000942
Iteration 20/1000 | Loss: 0.00000941
Iteration 21/1000 | Loss: 0.00000939
Iteration 22/1000 | Loss: 0.00000931
Iteration 23/1000 | Loss: 0.00000927
Iteration 24/1000 | Loss: 0.00000926
Iteration 25/1000 | Loss: 0.00000926
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000925
Iteration 28/1000 | Loss: 0.00000925
Iteration 29/1000 | Loss: 0.00000925
Iteration 30/1000 | Loss: 0.00000924
Iteration 31/1000 | Loss: 0.00000923
Iteration 32/1000 | Loss: 0.00000922
Iteration 33/1000 | Loss: 0.00000922
Iteration 34/1000 | Loss: 0.00000922
Iteration 35/1000 | Loss: 0.00000921
Iteration 36/1000 | Loss: 0.00000921
Iteration 37/1000 | Loss: 0.00000920
Iteration 38/1000 | Loss: 0.00000919
Iteration 39/1000 | Loss: 0.00000919
Iteration 40/1000 | Loss: 0.00000919
Iteration 41/1000 | Loss: 0.00000918
Iteration 42/1000 | Loss: 0.00000918
Iteration 43/1000 | Loss: 0.00000918
Iteration 44/1000 | Loss: 0.00000918
Iteration 45/1000 | Loss: 0.00000918
Iteration 46/1000 | Loss: 0.00000918
Iteration 47/1000 | Loss: 0.00000918
Iteration 48/1000 | Loss: 0.00000918
Iteration 49/1000 | Loss: 0.00000917
Iteration 50/1000 | Loss: 0.00000917
Iteration 51/1000 | Loss: 0.00000916
Iteration 52/1000 | Loss: 0.00000916
Iteration 53/1000 | Loss: 0.00000916
Iteration 54/1000 | Loss: 0.00000915
Iteration 55/1000 | Loss: 0.00000915
Iteration 56/1000 | Loss: 0.00000915
Iteration 57/1000 | Loss: 0.00000914
Iteration 58/1000 | Loss: 0.00000914
Iteration 59/1000 | Loss: 0.00000914
Iteration 60/1000 | Loss: 0.00000913
Iteration 61/1000 | Loss: 0.00000913
Iteration 62/1000 | Loss: 0.00000912
Iteration 63/1000 | Loss: 0.00000912
Iteration 64/1000 | Loss: 0.00000912
Iteration 65/1000 | Loss: 0.00000912
Iteration 66/1000 | Loss: 0.00000911
Iteration 67/1000 | Loss: 0.00000910
Iteration 68/1000 | Loss: 0.00000910
Iteration 69/1000 | Loss: 0.00000909
Iteration 70/1000 | Loss: 0.00000909
Iteration 71/1000 | Loss: 0.00000908
Iteration 72/1000 | Loss: 0.00000907
Iteration 73/1000 | Loss: 0.00000907
Iteration 74/1000 | Loss: 0.00000905
Iteration 75/1000 | Loss: 0.00000905
Iteration 76/1000 | Loss: 0.00000904
Iteration 77/1000 | Loss: 0.00000904
Iteration 78/1000 | Loss: 0.00000904
Iteration 79/1000 | Loss: 0.00000904
Iteration 80/1000 | Loss: 0.00000904
Iteration 81/1000 | Loss: 0.00000904
Iteration 82/1000 | Loss: 0.00000904
Iteration 83/1000 | Loss: 0.00000903
Iteration 84/1000 | Loss: 0.00000903
Iteration 85/1000 | Loss: 0.00000902
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000901
Iteration 88/1000 | Loss: 0.00000901
Iteration 89/1000 | Loss: 0.00000901
Iteration 90/1000 | Loss: 0.00000901
Iteration 91/1000 | Loss: 0.00000901
Iteration 92/1000 | Loss: 0.00000900
Iteration 93/1000 | Loss: 0.00000900
Iteration 94/1000 | Loss: 0.00000900
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000898
Iteration 101/1000 | Loss: 0.00000898
Iteration 102/1000 | Loss: 0.00000898
Iteration 103/1000 | Loss: 0.00000898
Iteration 104/1000 | Loss: 0.00000897
Iteration 105/1000 | Loss: 0.00000897
Iteration 106/1000 | Loss: 0.00000897
Iteration 107/1000 | Loss: 0.00000897
Iteration 108/1000 | Loss: 0.00000897
Iteration 109/1000 | Loss: 0.00000896
Iteration 110/1000 | Loss: 0.00000896
Iteration 111/1000 | Loss: 0.00000896
Iteration 112/1000 | Loss: 0.00000896
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000895
Iteration 118/1000 | Loss: 0.00000895
Iteration 119/1000 | Loss: 0.00000895
Iteration 120/1000 | Loss: 0.00000895
Iteration 121/1000 | Loss: 0.00000894
Iteration 122/1000 | Loss: 0.00000894
Iteration 123/1000 | Loss: 0.00000894
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000893
Iteration 126/1000 | Loss: 0.00000893
Iteration 127/1000 | Loss: 0.00000893
Iteration 128/1000 | Loss: 0.00000892
Iteration 129/1000 | Loss: 0.00000892
Iteration 130/1000 | Loss: 0.00000892
Iteration 131/1000 | Loss: 0.00000892
Iteration 132/1000 | Loss: 0.00000892
Iteration 133/1000 | Loss: 0.00000891
Iteration 134/1000 | Loss: 0.00000891
Iteration 135/1000 | Loss: 0.00000891
Iteration 136/1000 | Loss: 0.00000891
Iteration 137/1000 | Loss: 0.00000891
Iteration 138/1000 | Loss: 0.00000891
Iteration 139/1000 | Loss: 0.00000891
Iteration 140/1000 | Loss: 0.00000891
Iteration 141/1000 | Loss: 0.00000890
Iteration 142/1000 | Loss: 0.00000890
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000889
Iteration 147/1000 | Loss: 0.00000889
Iteration 148/1000 | Loss: 0.00000889
Iteration 149/1000 | Loss: 0.00000889
Iteration 150/1000 | Loss: 0.00000888
Iteration 151/1000 | Loss: 0.00000888
Iteration 152/1000 | Loss: 0.00000888
Iteration 153/1000 | Loss: 0.00000888
Iteration 154/1000 | Loss: 0.00000887
Iteration 155/1000 | Loss: 0.00000887
Iteration 156/1000 | Loss: 0.00000887
Iteration 157/1000 | Loss: 0.00000887
Iteration 158/1000 | Loss: 0.00000887
Iteration 159/1000 | Loss: 0.00000887
Iteration 160/1000 | Loss: 0.00000887
Iteration 161/1000 | Loss: 0.00000887
Iteration 162/1000 | Loss: 0.00000886
Iteration 163/1000 | Loss: 0.00000886
Iteration 164/1000 | Loss: 0.00000886
Iteration 165/1000 | Loss: 0.00000886
Iteration 166/1000 | Loss: 0.00000886
Iteration 167/1000 | Loss: 0.00000886
Iteration 168/1000 | Loss: 0.00000886
Iteration 169/1000 | Loss: 0.00000885
Iteration 170/1000 | Loss: 0.00000885
Iteration 171/1000 | Loss: 0.00000885
Iteration 172/1000 | Loss: 0.00000885
Iteration 173/1000 | Loss: 0.00000885
Iteration 174/1000 | Loss: 0.00000884
Iteration 175/1000 | Loss: 0.00000884
Iteration 176/1000 | Loss: 0.00000884
Iteration 177/1000 | Loss: 0.00000884
Iteration 178/1000 | Loss: 0.00000884
Iteration 179/1000 | Loss: 0.00000884
Iteration 180/1000 | Loss: 0.00000884
Iteration 181/1000 | Loss: 0.00000884
Iteration 182/1000 | Loss: 0.00000884
Iteration 183/1000 | Loss: 0.00000884
Iteration 184/1000 | Loss: 0.00000884
Iteration 185/1000 | Loss: 0.00000884
Iteration 186/1000 | Loss: 0.00000884
Iteration 187/1000 | Loss: 0.00000884
Iteration 188/1000 | Loss: 0.00000883
Iteration 189/1000 | Loss: 0.00000883
Iteration 190/1000 | Loss: 0.00000883
Iteration 191/1000 | Loss: 0.00000883
Iteration 192/1000 | Loss: 0.00000883
Iteration 193/1000 | Loss: 0.00000883
Iteration 194/1000 | Loss: 0.00000883
Iteration 195/1000 | Loss: 0.00000883
Iteration 196/1000 | Loss: 0.00000883
Iteration 197/1000 | Loss: 0.00000883
Iteration 198/1000 | Loss: 0.00000883
Iteration 199/1000 | Loss: 0.00000883
Iteration 200/1000 | Loss: 0.00000883
Iteration 201/1000 | Loss: 0.00000882
Iteration 202/1000 | Loss: 0.00000882
Iteration 203/1000 | Loss: 0.00000882
Iteration 204/1000 | Loss: 0.00000882
Iteration 205/1000 | Loss: 0.00000882
Iteration 206/1000 | Loss: 0.00000882
Iteration 207/1000 | Loss: 0.00000882
Iteration 208/1000 | Loss: 0.00000882
Iteration 209/1000 | Loss: 0.00000882
Iteration 210/1000 | Loss: 0.00000882
Iteration 211/1000 | Loss: 0.00000882
Iteration 212/1000 | Loss: 0.00000882
Iteration 213/1000 | Loss: 0.00000882
Iteration 214/1000 | Loss: 0.00000882
Iteration 215/1000 | Loss: 0.00000882
Iteration 216/1000 | Loss: 0.00000882
Iteration 217/1000 | Loss: 0.00000882
Iteration 218/1000 | Loss: 0.00000882
Iteration 219/1000 | Loss: 0.00000882
Iteration 220/1000 | Loss: 0.00000882
Iteration 221/1000 | Loss: 0.00000882
Iteration 222/1000 | Loss: 0.00000882
Iteration 223/1000 | Loss: 0.00000882
Iteration 224/1000 | Loss: 0.00000882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [8.8189162852359e-06, 8.8189162852359e-06, 8.8189162852359e-06, 8.8189162852359e-06, 8.8189162852359e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.8189162852359e-06

Optimization complete. Final v2v error: 2.544062852859497 mm

Highest mean error: 2.9051315784454346 mm for frame 111

Lowest mean error: 2.269482374191284 mm for frame 0

Saving results

Total time: 46.440489530563354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860168
Iteration 2/25 | Loss: 0.00108367
Iteration 3/25 | Loss: 0.00091393
Iteration 4/25 | Loss: 0.00089505
Iteration 5/25 | Loss: 0.00088995
Iteration 6/25 | Loss: 0.00088853
Iteration 7/25 | Loss: 0.00088853
Iteration 8/25 | Loss: 0.00088853
Iteration 9/25 | Loss: 0.00088853
Iteration 10/25 | Loss: 0.00088853
Iteration 11/25 | Loss: 0.00088853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008885308634489775, 0.0008885308634489775, 0.0008885308634489775, 0.0008885308634489775, 0.0008885308634489775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008885308634489775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61605740
Iteration 2/25 | Loss: 0.00121258
Iteration 3/25 | Loss: 0.00121258
Iteration 4/25 | Loss: 0.00121258
Iteration 5/25 | Loss: 0.00121258
Iteration 6/25 | Loss: 0.00121258
Iteration 7/25 | Loss: 0.00121258
Iteration 8/25 | Loss: 0.00121258
Iteration 9/25 | Loss: 0.00121258
Iteration 10/25 | Loss: 0.00121258
Iteration 11/25 | Loss: 0.00121258
Iteration 12/25 | Loss: 0.00121258
Iteration 13/25 | Loss: 0.00121258
Iteration 14/25 | Loss: 0.00121258
Iteration 15/25 | Loss: 0.00121258
Iteration 16/25 | Loss: 0.00121258
Iteration 17/25 | Loss: 0.00121258
Iteration 18/25 | Loss: 0.00121258
Iteration 19/25 | Loss: 0.00121258
Iteration 20/25 | Loss: 0.00121258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012125788489356637, 0.0012125788489356637, 0.0012125788489356637, 0.0012125788489356637, 0.0012125788489356637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012125788489356637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121258
Iteration 2/1000 | Loss: 0.00002167
Iteration 3/1000 | Loss: 0.00001346
Iteration 4/1000 | Loss: 0.00001079
Iteration 5/1000 | Loss: 0.00000976
Iteration 6/1000 | Loss: 0.00000920
Iteration 7/1000 | Loss: 0.00000872
Iteration 8/1000 | Loss: 0.00000851
Iteration 9/1000 | Loss: 0.00000824
Iteration 10/1000 | Loss: 0.00000821
Iteration 11/1000 | Loss: 0.00000820
Iteration 12/1000 | Loss: 0.00000814
Iteration 13/1000 | Loss: 0.00000814
Iteration 14/1000 | Loss: 0.00000813
Iteration 15/1000 | Loss: 0.00000812
Iteration 16/1000 | Loss: 0.00000811
Iteration 17/1000 | Loss: 0.00000807
Iteration 18/1000 | Loss: 0.00000806
Iteration 19/1000 | Loss: 0.00000806
Iteration 20/1000 | Loss: 0.00000805
Iteration 21/1000 | Loss: 0.00000805
Iteration 22/1000 | Loss: 0.00000804
Iteration 23/1000 | Loss: 0.00000804
Iteration 24/1000 | Loss: 0.00000801
Iteration 25/1000 | Loss: 0.00000799
Iteration 26/1000 | Loss: 0.00000794
Iteration 27/1000 | Loss: 0.00000794
Iteration 28/1000 | Loss: 0.00000793
Iteration 29/1000 | Loss: 0.00000793
Iteration 30/1000 | Loss: 0.00000792
Iteration 31/1000 | Loss: 0.00000792
Iteration 32/1000 | Loss: 0.00000791
Iteration 33/1000 | Loss: 0.00000791
Iteration 34/1000 | Loss: 0.00000790
Iteration 35/1000 | Loss: 0.00000790
Iteration 36/1000 | Loss: 0.00000790
Iteration 37/1000 | Loss: 0.00000789
Iteration 38/1000 | Loss: 0.00000789
Iteration 39/1000 | Loss: 0.00000788
Iteration 40/1000 | Loss: 0.00000787
Iteration 41/1000 | Loss: 0.00000787
Iteration 42/1000 | Loss: 0.00000787
Iteration 43/1000 | Loss: 0.00000787
Iteration 44/1000 | Loss: 0.00000787
Iteration 45/1000 | Loss: 0.00000786
Iteration 46/1000 | Loss: 0.00000786
Iteration 47/1000 | Loss: 0.00000786
Iteration 48/1000 | Loss: 0.00000785
Iteration 49/1000 | Loss: 0.00000785
Iteration 50/1000 | Loss: 0.00000784
Iteration 51/1000 | Loss: 0.00000784
Iteration 52/1000 | Loss: 0.00000783
Iteration 53/1000 | Loss: 0.00000783
Iteration 54/1000 | Loss: 0.00000783
Iteration 55/1000 | Loss: 0.00000783
Iteration 56/1000 | Loss: 0.00000783
Iteration 57/1000 | Loss: 0.00000783
Iteration 58/1000 | Loss: 0.00000782
Iteration 59/1000 | Loss: 0.00000782
Iteration 60/1000 | Loss: 0.00000781
Iteration 61/1000 | Loss: 0.00000779
Iteration 62/1000 | Loss: 0.00000779
Iteration 63/1000 | Loss: 0.00000779
Iteration 64/1000 | Loss: 0.00000779
Iteration 65/1000 | Loss: 0.00000779
Iteration 66/1000 | Loss: 0.00000778
Iteration 67/1000 | Loss: 0.00000778
Iteration 68/1000 | Loss: 0.00000778
Iteration 69/1000 | Loss: 0.00000777
Iteration 70/1000 | Loss: 0.00000775
Iteration 71/1000 | Loss: 0.00000775
Iteration 72/1000 | Loss: 0.00000774
Iteration 73/1000 | Loss: 0.00000773
Iteration 74/1000 | Loss: 0.00000773
Iteration 75/1000 | Loss: 0.00000773
Iteration 76/1000 | Loss: 0.00000773
Iteration 77/1000 | Loss: 0.00000773
Iteration 78/1000 | Loss: 0.00000772
Iteration 79/1000 | Loss: 0.00000772
Iteration 80/1000 | Loss: 0.00000772
Iteration 81/1000 | Loss: 0.00000771
Iteration 82/1000 | Loss: 0.00000771
Iteration 83/1000 | Loss: 0.00000771
Iteration 84/1000 | Loss: 0.00000771
Iteration 85/1000 | Loss: 0.00000771
Iteration 86/1000 | Loss: 0.00000771
Iteration 87/1000 | Loss: 0.00000771
Iteration 88/1000 | Loss: 0.00000771
Iteration 89/1000 | Loss: 0.00000771
Iteration 90/1000 | Loss: 0.00000770
Iteration 91/1000 | Loss: 0.00000770
Iteration 92/1000 | Loss: 0.00000770
Iteration 93/1000 | Loss: 0.00000770
Iteration 94/1000 | Loss: 0.00000770
Iteration 95/1000 | Loss: 0.00000770
Iteration 96/1000 | Loss: 0.00000770
Iteration 97/1000 | Loss: 0.00000770
Iteration 98/1000 | Loss: 0.00000769
Iteration 99/1000 | Loss: 0.00000769
Iteration 100/1000 | Loss: 0.00000769
Iteration 101/1000 | Loss: 0.00000768
Iteration 102/1000 | Loss: 0.00000768
Iteration 103/1000 | Loss: 0.00000768
Iteration 104/1000 | Loss: 0.00000768
Iteration 105/1000 | Loss: 0.00000768
Iteration 106/1000 | Loss: 0.00000768
Iteration 107/1000 | Loss: 0.00000768
Iteration 108/1000 | Loss: 0.00000768
Iteration 109/1000 | Loss: 0.00000768
Iteration 110/1000 | Loss: 0.00000767
Iteration 111/1000 | Loss: 0.00000767
Iteration 112/1000 | Loss: 0.00000767
Iteration 113/1000 | Loss: 0.00000766
Iteration 114/1000 | Loss: 0.00000766
Iteration 115/1000 | Loss: 0.00000766
Iteration 116/1000 | Loss: 0.00000766
Iteration 117/1000 | Loss: 0.00000766
Iteration 118/1000 | Loss: 0.00000766
Iteration 119/1000 | Loss: 0.00000766
Iteration 120/1000 | Loss: 0.00000765
Iteration 121/1000 | Loss: 0.00000765
Iteration 122/1000 | Loss: 0.00000765
Iteration 123/1000 | Loss: 0.00000765
Iteration 124/1000 | Loss: 0.00000765
Iteration 125/1000 | Loss: 0.00000765
Iteration 126/1000 | Loss: 0.00000765
Iteration 127/1000 | Loss: 0.00000765
Iteration 128/1000 | Loss: 0.00000764
Iteration 129/1000 | Loss: 0.00000764
Iteration 130/1000 | Loss: 0.00000764
Iteration 131/1000 | Loss: 0.00000764
Iteration 132/1000 | Loss: 0.00000764
Iteration 133/1000 | Loss: 0.00000764
Iteration 134/1000 | Loss: 0.00000764
Iteration 135/1000 | Loss: 0.00000764
Iteration 136/1000 | Loss: 0.00000764
Iteration 137/1000 | Loss: 0.00000764
Iteration 138/1000 | Loss: 0.00000764
Iteration 139/1000 | Loss: 0.00000763
Iteration 140/1000 | Loss: 0.00000763
Iteration 141/1000 | Loss: 0.00000763
Iteration 142/1000 | Loss: 0.00000763
Iteration 143/1000 | Loss: 0.00000763
Iteration 144/1000 | Loss: 0.00000763
Iteration 145/1000 | Loss: 0.00000763
Iteration 146/1000 | Loss: 0.00000763
Iteration 147/1000 | Loss: 0.00000763
Iteration 148/1000 | Loss: 0.00000763
Iteration 149/1000 | Loss: 0.00000763
Iteration 150/1000 | Loss: 0.00000763
Iteration 151/1000 | Loss: 0.00000763
Iteration 152/1000 | Loss: 0.00000763
Iteration 153/1000 | Loss: 0.00000763
Iteration 154/1000 | Loss: 0.00000763
Iteration 155/1000 | Loss: 0.00000763
Iteration 156/1000 | Loss: 0.00000763
Iteration 157/1000 | Loss: 0.00000762
Iteration 158/1000 | Loss: 0.00000762
Iteration 159/1000 | Loss: 0.00000762
Iteration 160/1000 | Loss: 0.00000762
Iteration 161/1000 | Loss: 0.00000762
Iteration 162/1000 | Loss: 0.00000762
Iteration 163/1000 | Loss: 0.00000762
Iteration 164/1000 | Loss: 0.00000762
Iteration 165/1000 | Loss: 0.00000762
Iteration 166/1000 | Loss: 0.00000761
Iteration 167/1000 | Loss: 0.00000761
Iteration 168/1000 | Loss: 0.00000761
Iteration 169/1000 | Loss: 0.00000761
Iteration 170/1000 | Loss: 0.00000761
Iteration 171/1000 | Loss: 0.00000761
Iteration 172/1000 | Loss: 0.00000761
Iteration 173/1000 | Loss: 0.00000761
Iteration 174/1000 | Loss: 0.00000761
Iteration 175/1000 | Loss: 0.00000761
Iteration 176/1000 | Loss: 0.00000761
Iteration 177/1000 | Loss: 0.00000761
Iteration 178/1000 | Loss: 0.00000761
Iteration 179/1000 | Loss: 0.00000761
Iteration 180/1000 | Loss: 0.00000761
Iteration 181/1000 | Loss: 0.00000761
Iteration 182/1000 | Loss: 0.00000761
Iteration 183/1000 | Loss: 0.00000761
Iteration 184/1000 | Loss: 0.00000761
Iteration 185/1000 | Loss: 0.00000761
Iteration 186/1000 | Loss: 0.00000761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [7.613905836478807e-06, 7.613905836478807e-06, 7.613905836478807e-06, 7.613905836478807e-06, 7.613905836478807e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.613905836478807e-06

Optimization complete. Final v2v error: 2.3848471641540527 mm

Highest mean error: 2.861236095428467 mm for frame 98

Lowest mean error: 1.940136432647705 mm for frame 0

Saving results

Total time: 41.540913105010986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417846
Iteration 2/25 | Loss: 0.00102592
Iteration 3/25 | Loss: 0.00093137
Iteration 4/25 | Loss: 0.00091574
Iteration 5/25 | Loss: 0.00091085
Iteration 6/25 | Loss: 0.00090913
Iteration 7/25 | Loss: 0.00090906
Iteration 8/25 | Loss: 0.00090906
Iteration 9/25 | Loss: 0.00090906
Iteration 10/25 | Loss: 0.00090906
Iteration 11/25 | Loss: 0.00090906
Iteration 12/25 | Loss: 0.00090906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009090642561204731, 0.0009090642561204731, 0.0009090642561204731, 0.0009090642561204731, 0.0009090642561204731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009090642561204731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.49510360
Iteration 2/25 | Loss: 0.00130714
Iteration 3/25 | Loss: 0.00130714
Iteration 4/25 | Loss: 0.00130714
Iteration 5/25 | Loss: 0.00130714
Iteration 6/25 | Loss: 0.00130714
Iteration 7/25 | Loss: 0.00130714
Iteration 8/25 | Loss: 0.00130714
Iteration 9/25 | Loss: 0.00130714
Iteration 10/25 | Loss: 0.00130714
Iteration 11/25 | Loss: 0.00130713
Iteration 12/25 | Loss: 0.00130713
Iteration 13/25 | Loss: 0.00130713
Iteration 14/25 | Loss: 0.00130713
Iteration 15/25 | Loss: 0.00130713
Iteration 16/25 | Loss: 0.00130713
Iteration 17/25 | Loss: 0.00130713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013071346329525113, 0.0013071346329525113, 0.0013071346329525113, 0.0013071346329525113, 0.0013071346329525113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013071346329525113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130713
Iteration 2/1000 | Loss: 0.00003144
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001383
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001190
Iteration 7/1000 | Loss: 0.00001150
Iteration 8/1000 | Loss: 0.00001105
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001047
Iteration 13/1000 | Loss: 0.00001044
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001034
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001025
Iteration 18/1000 | Loss: 0.00001025
Iteration 19/1000 | Loss: 0.00001021
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001018
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001015
Iteration 25/1000 | Loss: 0.00001014
Iteration 26/1000 | Loss: 0.00001012
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001008
Iteration 30/1000 | Loss: 0.00001008
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001007
Iteration 35/1000 | Loss: 0.00001007
Iteration 36/1000 | Loss: 0.00001007
Iteration 37/1000 | Loss: 0.00001006
Iteration 38/1000 | Loss: 0.00001006
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001006
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001004
Iteration 43/1000 | Loss: 0.00001004
Iteration 44/1000 | Loss: 0.00001004
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001004
Iteration 49/1000 | Loss: 0.00001004
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000998
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000997
Iteration 78/1000 | Loss: 0.00000997
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000996
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000996
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000994
Iteration 101/1000 | Loss: 0.00000994
Iteration 102/1000 | Loss: 0.00000993
Iteration 103/1000 | Loss: 0.00000993
Iteration 104/1000 | Loss: 0.00000993
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000993
Iteration 107/1000 | Loss: 0.00000993
Iteration 108/1000 | Loss: 0.00000993
Iteration 109/1000 | Loss: 0.00000993
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000992
Iteration 112/1000 | Loss: 0.00000992
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000992
Iteration 115/1000 | Loss: 0.00000992
Iteration 116/1000 | Loss: 0.00000992
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000991
Iteration 120/1000 | Loss: 0.00000991
Iteration 121/1000 | Loss: 0.00000991
Iteration 122/1000 | Loss: 0.00000991
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000991
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00000990
Iteration 129/1000 | Loss: 0.00000990
Iteration 130/1000 | Loss: 0.00000989
Iteration 131/1000 | Loss: 0.00000989
Iteration 132/1000 | Loss: 0.00000989
Iteration 133/1000 | Loss: 0.00000989
Iteration 134/1000 | Loss: 0.00000989
Iteration 135/1000 | Loss: 0.00000989
Iteration 136/1000 | Loss: 0.00000988
Iteration 137/1000 | Loss: 0.00000988
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000988
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000987
Iteration 142/1000 | Loss: 0.00000987
Iteration 143/1000 | Loss: 0.00000987
Iteration 144/1000 | Loss: 0.00000987
Iteration 145/1000 | Loss: 0.00000987
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000987
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000986
Iteration 156/1000 | Loss: 0.00000986
Iteration 157/1000 | Loss: 0.00000986
Iteration 158/1000 | Loss: 0.00000986
Iteration 159/1000 | Loss: 0.00000986
Iteration 160/1000 | Loss: 0.00000986
Iteration 161/1000 | Loss: 0.00000986
Iteration 162/1000 | Loss: 0.00000986
Iteration 163/1000 | Loss: 0.00000986
Iteration 164/1000 | Loss: 0.00000986
Iteration 165/1000 | Loss: 0.00000986
Iteration 166/1000 | Loss: 0.00000986
Iteration 167/1000 | Loss: 0.00000986
Iteration 168/1000 | Loss: 0.00000986
Iteration 169/1000 | Loss: 0.00000986
Iteration 170/1000 | Loss: 0.00000986
Iteration 171/1000 | Loss: 0.00000986
Iteration 172/1000 | Loss: 0.00000986
Iteration 173/1000 | Loss: 0.00000986
Iteration 174/1000 | Loss: 0.00000986
Iteration 175/1000 | Loss: 0.00000986
Iteration 176/1000 | Loss: 0.00000986
Iteration 177/1000 | Loss: 0.00000986
Iteration 178/1000 | Loss: 0.00000986
Iteration 179/1000 | Loss: 0.00000986
Iteration 180/1000 | Loss: 0.00000986
Iteration 181/1000 | Loss: 0.00000986
Iteration 182/1000 | Loss: 0.00000986
Iteration 183/1000 | Loss: 0.00000986
Iteration 184/1000 | Loss: 0.00000986
Iteration 185/1000 | Loss: 0.00000986
Iteration 186/1000 | Loss: 0.00000986
Iteration 187/1000 | Loss: 0.00000986
Iteration 188/1000 | Loss: 0.00000986
Iteration 189/1000 | Loss: 0.00000986
Iteration 190/1000 | Loss: 0.00000986
Iteration 191/1000 | Loss: 0.00000986
Iteration 192/1000 | Loss: 0.00000986
Iteration 193/1000 | Loss: 0.00000986
Iteration 194/1000 | Loss: 0.00000986
Iteration 195/1000 | Loss: 0.00000986
Iteration 196/1000 | Loss: 0.00000986
Iteration 197/1000 | Loss: 0.00000986
Iteration 198/1000 | Loss: 0.00000986
Iteration 199/1000 | Loss: 0.00000986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [9.856703400146216e-06, 9.856703400146216e-06, 9.856703400146216e-06, 9.856703400146216e-06, 9.856703400146216e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.856703400146216e-06

Optimization complete. Final v2v error: 2.6956868171691895 mm

Highest mean error: 3.112406015396118 mm for frame 46

Lowest mean error: 2.2973036766052246 mm for frame 103

Saving results

Total time: 38.5795738697052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454769
Iteration 2/25 | Loss: 0.00144913
Iteration 3/25 | Loss: 0.00115828
Iteration 4/25 | Loss: 0.00108660
Iteration 5/25 | Loss: 0.00106057
Iteration 6/25 | Loss: 0.00105481
Iteration 7/25 | Loss: 0.00102861
Iteration 8/25 | Loss: 0.00102171
Iteration 9/25 | Loss: 0.00102013
Iteration 10/25 | Loss: 0.00101964
Iteration 11/25 | Loss: 0.00102098
Iteration 12/25 | Loss: 0.00102073
Iteration 13/25 | Loss: 0.00101974
Iteration 14/25 | Loss: 0.00101922
Iteration 15/25 | Loss: 0.00101900
Iteration 16/25 | Loss: 0.00101894
Iteration 17/25 | Loss: 0.00101894
Iteration 18/25 | Loss: 0.00101894
Iteration 19/25 | Loss: 0.00101894
Iteration 20/25 | Loss: 0.00101893
Iteration 21/25 | Loss: 0.00101893
Iteration 22/25 | Loss: 0.00101893
Iteration 23/25 | Loss: 0.00101893
Iteration 24/25 | Loss: 0.00101893
Iteration 25/25 | Loss: 0.00101893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28766334
Iteration 2/25 | Loss: 0.00133263
Iteration 3/25 | Loss: 0.00133261
Iteration 4/25 | Loss: 0.00133261
Iteration 5/25 | Loss: 0.00133261
Iteration 6/25 | Loss: 0.00133261
Iteration 7/25 | Loss: 0.00133261
Iteration 8/25 | Loss: 0.00133261
Iteration 9/25 | Loss: 0.00133261
Iteration 10/25 | Loss: 0.00133261
Iteration 11/25 | Loss: 0.00133261
Iteration 12/25 | Loss: 0.00133261
Iteration 13/25 | Loss: 0.00133261
Iteration 14/25 | Loss: 0.00133261
Iteration 15/25 | Loss: 0.00133261
Iteration 16/25 | Loss: 0.00133261
Iteration 17/25 | Loss: 0.00133261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013326099142432213, 0.0013326099142432213, 0.0013326099142432213, 0.0013326099142432213, 0.0013326099142432213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013326099142432213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133261
Iteration 2/1000 | Loss: 0.00009099
Iteration 3/1000 | Loss: 0.00006163
Iteration 4/1000 | Loss: 0.00006620
Iteration 5/1000 | Loss: 0.00006283
Iteration 6/1000 | Loss: 0.00004403
Iteration 7/1000 | Loss: 0.00005217
Iteration 8/1000 | Loss: 0.00006903
Iteration 9/1000 | Loss: 0.00003852
Iteration 10/1000 | Loss: 0.00003291
Iteration 11/1000 | Loss: 0.00002470
Iteration 12/1000 | Loss: 0.00002487
Iteration 13/1000 | Loss: 0.00002239
Iteration 14/1000 | Loss: 0.00002406
Iteration 15/1000 | Loss: 0.00002822
Iteration 16/1000 | Loss: 0.00003311
Iteration 17/1000 | Loss: 0.00003124
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002506
Iteration 20/1000 | Loss: 0.00002261
Iteration 21/1000 | Loss: 0.00002432
Iteration 22/1000 | Loss: 0.00002188
Iteration 23/1000 | Loss: 0.00002436
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002331
Iteration 26/1000 | Loss: 0.00004263
Iteration 27/1000 | Loss: 0.00006486
Iteration 28/1000 | Loss: 0.00007199
Iteration 29/1000 | Loss: 0.00004485
Iteration 30/1000 | Loss: 0.00002896
Iteration 31/1000 | Loss: 0.00002337
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00002247
Iteration 35/1000 | Loss: 0.00004467
Iteration 36/1000 | Loss: 0.00005151
Iteration 37/1000 | Loss: 0.00003449
Iteration 38/1000 | Loss: 0.00003809
Iteration 39/1000 | Loss: 0.00003144
Iteration 40/1000 | Loss: 0.00002162
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00003180
Iteration 44/1000 | Loss: 0.00004954
Iteration 45/1000 | Loss: 0.00004322
Iteration 46/1000 | Loss: 0.00003072
Iteration 47/1000 | Loss: 0.00002479
Iteration 48/1000 | Loss: 0.00002700
Iteration 49/1000 | Loss: 0.00002175
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002141
Iteration 53/1000 | Loss: 0.00002202
Iteration 54/1000 | Loss: 0.00002579
Iteration 55/1000 | Loss: 0.00002716
Iteration 56/1000 | Loss: 0.00002727
Iteration 57/1000 | Loss: 0.00002771
Iteration 58/1000 | Loss: 0.00002347
Iteration 59/1000 | Loss: 0.00002502
Iteration 60/1000 | Loss: 0.00002450
Iteration 61/1000 | Loss: 0.00003128
Iteration 62/1000 | Loss: 0.00002276
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00002803
Iteration 66/1000 | Loss: 0.00002779
Iteration 67/1000 | Loss: 0.00002752
Iteration 68/1000 | Loss: 0.00002535
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002352
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002350
Iteration 74/1000 | Loss: 0.00002562
Iteration 75/1000 | Loss: 0.00003311
Iteration 76/1000 | Loss: 0.00002736
Iteration 77/1000 | Loss: 0.00002320
Iteration 78/1000 | Loss: 0.00002210
Iteration 79/1000 | Loss: 0.00002492
Iteration 80/1000 | Loss: 0.00002531
Iteration 81/1000 | Loss: 0.00003312
Iteration 82/1000 | Loss: 0.00002864
Iteration 83/1000 | Loss: 0.00004169
Iteration 84/1000 | Loss: 0.00003407
Iteration 85/1000 | Loss: 0.00005118
Iteration 86/1000 | Loss: 0.00003700
Iteration 87/1000 | Loss: 0.00002949
Iteration 88/1000 | Loss: 0.00002275
Iteration 89/1000 | Loss: 0.00002592
Iteration 90/1000 | Loss: 0.00002530
Iteration 91/1000 | Loss: 0.00004154
Iteration 92/1000 | Loss: 0.00003437
Iteration 93/1000 | Loss: 0.00002670
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002503
Iteration 96/1000 | Loss: 0.00002304
Iteration 97/1000 | Loss: 0.00002378
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00004592
Iteration 100/1000 | Loss: 0.00003920
Iteration 101/1000 | Loss: 0.00002773
Iteration 102/1000 | Loss: 0.00002649
Iteration 103/1000 | Loss: 0.00002716
Iteration 104/1000 | Loss: 0.00002792
Iteration 105/1000 | Loss: 0.00002507
Iteration 106/1000 | Loss: 0.00002210
Iteration 107/1000 | Loss: 0.00002907
Iteration 108/1000 | Loss: 0.00003373
Iteration 109/1000 | Loss: 0.00002794
Iteration 110/1000 | Loss: 0.00002158
Iteration 111/1000 | Loss: 0.00002308
Iteration 112/1000 | Loss: 0.00003159
Iteration 113/1000 | Loss: 0.00005187
Iteration 114/1000 | Loss: 0.00004333
Iteration 115/1000 | Loss: 0.00004934
Iteration 116/1000 | Loss: 0.00004729
Iteration 117/1000 | Loss: 0.00003923
Iteration 118/1000 | Loss: 0.00004188
Iteration 119/1000 | Loss: 0.00003560
Iteration 120/1000 | Loss: 0.00002722
Iteration 121/1000 | Loss: 0.00002187
Iteration 122/1000 | Loss: 0.00002399
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002250
Iteration 125/1000 | Loss: 0.00002228
Iteration 126/1000 | Loss: 0.00003432
Iteration 127/1000 | Loss: 0.00003190
Iteration 128/1000 | Loss: 0.00003220
Iteration 129/1000 | Loss: 0.00003291
Iteration 130/1000 | Loss: 0.00003280
Iteration 131/1000 | Loss: 0.00002897
Iteration 132/1000 | Loss: 0.00002430
Iteration 133/1000 | Loss: 0.00001977
Iteration 134/1000 | Loss: 0.00001879
Iteration 135/1000 | Loss: 0.00001990
Iteration 136/1000 | Loss: 0.00004223
Iteration 137/1000 | Loss: 0.00002838
Iteration 138/1000 | Loss: 0.00002162
Iteration 139/1000 | Loss: 0.00003090
Iteration 140/1000 | Loss: 0.00002476
Iteration 141/1000 | Loss: 0.00004001
Iteration 142/1000 | Loss: 0.00002229
Iteration 143/1000 | Loss: 0.00002601
Iteration 144/1000 | Loss: 0.00002149
Iteration 145/1000 | Loss: 0.00003244
Iteration 146/1000 | Loss: 0.00002755
Iteration 147/1000 | Loss: 0.00004055
Iteration 148/1000 | Loss: 0.00003016
Iteration 149/1000 | Loss: 0.00004358
Iteration 150/1000 | Loss: 0.00003173
Iteration 151/1000 | Loss: 0.00002715
Iteration 152/1000 | Loss: 0.00002278
Iteration 153/1000 | Loss: 0.00004081
Iteration 154/1000 | Loss: 0.00003339
Iteration 155/1000 | Loss: 0.00004863
Iteration 156/1000 | Loss: 0.00003721
Iteration 157/1000 | Loss: 0.00005073
Iteration 158/1000 | Loss: 0.00003270
Iteration 159/1000 | Loss: 0.00004313
Iteration 160/1000 | Loss: 0.00004744
Iteration 161/1000 | Loss: 0.00003672
Iteration 162/1000 | Loss: 0.00002470
Iteration 163/1000 | Loss: 0.00005684
Iteration 164/1000 | Loss: 0.00003463
Iteration 165/1000 | Loss: 0.00003262
Iteration 166/1000 | Loss: 0.00002628
Iteration 167/1000 | Loss: 0.00006160
Iteration 168/1000 | Loss: 0.00004235
Iteration 169/1000 | Loss: 0.00006248
Iteration 170/1000 | Loss: 0.00004224
Iteration 171/1000 | Loss: 0.00005173
Iteration 172/1000 | Loss: 0.00003411
Iteration 173/1000 | Loss: 0.00005980
Iteration 174/1000 | Loss: 0.00004735
Iteration 175/1000 | Loss: 0.00007000
Iteration 176/1000 | Loss: 0.00005773
Iteration 177/1000 | Loss: 0.00007052
Iteration 178/1000 | Loss: 0.00005088
Iteration 179/1000 | Loss: 0.00005732
Iteration 180/1000 | Loss: 0.00005021
Iteration 181/1000 | Loss: 0.00006532
Iteration 182/1000 | Loss: 0.00005602
Iteration 183/1000 | Loss: 0.00005931
Iteration 184/1000 | Loss: 0.00005639
Iteration 185/1000 | Loss: 0.00006393
Iteration 186/1000 | Loss: 0.00005595
Iteration 187/1000 | Loss: 0.00005963
Iteration 188/1000 | Loss: 0.00005878
Iteration 189/1000 | Loss: 0.00003560
Iteration 190/1000 | Loss: 0.00003729
Iteration 191/1000 | Loss: 0.00002627
Iteration 192/1000 | Loss: 0.00004798
Iteration 193/1000 | Loss: 0.00002945
Iteration 194/1000 | Loss: 0.00001952
Iteration 195/1000 | Loss: 0.00004659
Iteration 196/1000 | Loss: 0.00005320
Iteration 197/1000 | Loss: 0.00002613
Iteration 198/1000 | Loss: 0.00002771
Iteration 199/1000 | Loss: 0.00003798
Iteration 200/1000 | Loss: 0.00002909
Iteration 201/1000 | Loss: 0.00002169
Iteration 202/1000 | Loss: 0.00002094
Iteration 203/1000 | Loss: 0.00002624
Iteration 204/1000 | Loss: 0.00002150
Iteration 205/1000 | Loss: 0.00001979
Iteration 206/1000 | Loss: 0.00002104
Iteration 207/1000 | Loss: 0.00003172
Iteration 208/1000 | Loss: 0.00003169
Iteration 209/1000 | Loss: 0.00002488
Iteration 210/1000 | Loss: 0.00003864
Iteration 211/1000 | Loss: 0.00002806
Iteration 212/1000 | Loss: 0.00003105
Iteration 213/1000 | Loss: 0.00002353
Iteration 214/1000 | Loss: 0.00001946
Iteration 215/1000 | Loss: 0.00002030
Iteration 216/1000 | Loss: 0.00001946
Iteration 217/1000 | Loss: 0.00003976
Iteration 218/1000 | Loss: 0.00003352
Iteration 219/1000 | Loss: 0.00004598
Iteration 220/1000 | Loss: 0.00003210
Iteration 221/1000 | Loss: 0.00004469
Iteration 222/1000 | Loss: 0.00003415
Iteration 223/1000 | Loss: 0.00004699
Iteration 224/1000 | Loss: 0.00004111
Iteration 225/1000 | Loss: 0.00004711
Iteration 226/1000 | Loss: 0.00003627
Iteration 227/1000 | Loss: 0.00004927
Iteration 228/1000 | Loss: 0.00004557
Iteration 229/1000 | Loss: 0.00005337
Iteration 230/1000 | Loss: 0.00003945
Iteration 231/1000 | Loss: 0.00002581
Iteration 232/1000 | Loss: 0.00002402
Iteration 233/1000 | Loss: 0.00002148
Iteration 234/1000 | Loss: 0.00002144
Iteration 235/1000 | Loss: 0.00001966
Iteration 236/1000 | Loss: 0.00001937
Iteration 237/1000 | Loss: 0.00001985
Iteration 238/1000 | Loss: 0.00002351
Iteration 239/1000 | Loss: 0.00002415
Iteration 240/1000 | Loss: 0.00002051
Iteration 241/1000 | Loss: 0.00002157
Iteration 242/1000 | Loss: 0.00002524
Iteration 243/1000 | Loss: 0.00002154
Iteration 244/1000 | Loss: 0.00002705
Iteration 245/1000 | Loss: 0.00002321
Iteration 246/1000 | Loss: 0.00001886
Iteration 247/1000 | Loss: 0.00002855
Iteration 248/1000 | Loss: 0.00002357
Iteration 249/1000 | Loss: 0.00002353
Iteration 250/1000 | Loss: 0.00002183
Iteration 251/1000 | Loss: 0.00004243
Iteration 252/1000 | Loss: 0.00002886
Iteration 253/1000 | Loss: 0.00001839
Iteration 254/1000 | Loss: 0.00003415
Iteration 255/1000 | Loss: 0.00002836
Iteration 256/1000 | Loss: 0.00004578
Iteration 257/1000 | Loss: 0.00003027
Iteration 258/1000 | Loss: 0.00002730
Iteration 259/1000 | Loss: 0.00002170
Iteration 260/1000 | Loss: 0.00002577
Iteration 261/1000 | Loss: 0.00002061
Iteration 262/1000 | Loss: 0.00003081
Iteration 263/1000 | Loss: 0.00002232
Iteration 264/1000 | Loss: 0.00002503
Iteration 265/1000 | Loss: 0.00002215
Iteration 266/1000 | Loss: 0.00002176
Iteration 267/1000 | Loss: 0.00002128
Iteration 268/1000 | Loss: 0.00005302
Iteration 269/1000 | Loss: 0.00002988
Iteration 270/1000 | Loss: 0.00001945
Iteration 271/1000 | Loss: 0.00003343
Iteration 272/1000 | Loss: 0.00002169
Iteration 273/1000 | Loss: 0.00002146
Iteration 274/1000 | Loss: 0.00003264
Iteration 275/1000 | Loss: 0.00002528
Iteration 276/1000 | Loss: 0.00002954
Iteration 277/1000 | Loss: 0.00002058
Iteration 278/1000 | Loss: 0.00001880
Iteration 279/1000 | Loss: 0.00002645
Iteration 280/1000 | Loss: 0.00002110
Iteration 281/1000 | Loss: 0.00003739
Iteration 282/1000 | Loss: 0.00002274
Iteration 283/1000 | Loss: 0.00003678
Iteration 284/1000 | Loss: 0.00002123
Iteration 285/1000 | Loss: 0.00003246
Iteration 286/1000 | Loss: 0.00004282
Iteration 287/1000 | Loss: 0.00002214
Iteration 288/1000 | Loss: 0.00003348
Iteration 289/1000 | Loss: 0.00002219
Iteration 290/1000 | Loss: 0.00002931
Iteration 291/1000 | Loss: 0.00002303
Iteration 292/1000 | Loss: 0.00003289
Iteration 293/1000 | Loss: 0.00002681
Iteration 294/1000 | Loss: 0.00003949
Iteration 295/1000 | Loss: 0.00002324
Iteration 296/1000 | Loss: 0.00004294
Iteration 297/1000 | Loss: 0.00002559
Iteration 298/1000 | Loss: 0.00004539
Iteration 299/1000 | Loss: 0.00002920
Iteration 300/1000 | Loss: 0.00005435
Iteration 301/1000 | Loss: 0.00002861
Iteration 302/1000 | Loss: 0.00004188
Iteration 303/1000 | Loss: 0.00002442
Iteration 304/1000 | Loss: 0.00005011
Iteration 305/1000 | Loss: 0.00003179
Iteration 306/1000 | Loss: 0.00005375
Iteration 307/1000 | Loss: 0.00003710
Iteration 308/1000 | Loss: 0.00004104
Iteration 309/1000 | Loss: 0.00003009
Iteration 310/1000 | Loss: 0.00004994
Iteration 311/1000 | Loss: 0.00003289
Iteration 312/1000 | Loss: 0.00004876
Iteration 313/1000 | Loss: 0.00003359
Iteration 314/1000 | Loss: 0.00002352
Iteration 315/1000 | Loss: 0.00002595
Iteration 316/1000 | Loss: 0.00002553
Iteration 317/1000 | Loss: 0.00002154
Iteration 318/1000 | Loss: 0.00004029
Iteration 319/1000 | Loss: 0.00002769
Iteration 320/1000 | Loss: 0.00002116
Iteration 321/1000 | Loss: 0.00001925
Iteration 322/1000 | Loss: 0.00002324
Iteration 323/1000 | Loss: 0.00002171
Iteration 324/1000 | Loss: 0.00002617
Iteration 325/1000 | Loss: 0.00004773
Iteration 326/1000 | Loss: 0.00002376
Iteration 327/1000 | Loss: 0.00001965
Iteration 328/1000 | Loss: 0.00002517
Iteration 329/1000 | Loss: 0.00002239
Iteration 330/1000 | Loss: 0.00002348
Iteration 331/1000 | Loss: 0.00002112
Iteration 332/1000 | Loss: 0.00001874
Iteration 333/1000 | Loss: 0.00002380
Iteration 334/1000 | Loss: 0.00003951
Iteration 335/1000 | Loss: 0.00002631
Iteration 336/1000 | Loss: 0.00002218
Iteration 337/1000 | Loss: 0.00003436
Iteration 338/1000 | Loss: 0.00002439
Iteration 339/1000 | Loss: 0.00002596
Iteration 340/1000 | Loss: 0.00002470
Iteration 341/1000 | Loss: 0.00002061
Iteration 342/1000 | Loss: 0.00003301
Iteration 343/1000 | Loss: 0.00002301
Iteration 344/1000 | Loss: 0.00003188
Iteration 345/1000 | Loss: 0.00002551
Iteration 346/1000 | Loss: 0.00003005
Iteration 347/1000 | Loss: 0.00002476
Iteration 348/1000 | Loss: 0.00002054
Iteration 349/1000 | Loss: 0.00002673
Iteration 350/1000 | Loss: 0.00002343
Iteration 351/1000 | Loss: 0.00002551
Iteration 352/1000 | Loss: 0.00002320
Iteration 353/1000 | Loss: 0.00001952
Iteration 354/1000 | Loss: 0.00001902
Iteration 355/1000 | Loss: 0.00002037
Iteration 356/1000 | Loss: 0.00002288
Iteration 357/1000 | Loss: 0.00002186
Iteration 358/1000 | Loss: 0.00002545
Iteration 359/1000 | Loss: 0.00002243
Iteration 360/1000 | Loss: 0.00001970
Iteration 361/1000 | Loss: 0.00002201
Iteration 362/1000 | Loss: 0.00001942
Iteration 363/1000 | Loss: 0.00002594
Iteration 364/1000 | Loss: 0.00002240
Iteration 365/1000 | Loss: 0.00003126
Iteration 366/1000 | Loss: 0.00002620
Iteration 367/1000 | Loss: 0.00002053
Iteration 368/1000 | Loss: 0.00002755
Iteration 369/1000 | Loss: 0.00002538
Iteration 370/1000 | Loss: 0.00003202
Iteration 371/1000 | Loss: 0.00002834
Iteration 372/1000 | Loss: 0.00002299
Iteration 373/1000 | Loss: 0.00001939
Iteration 374/1000 | Loss: 0.00002021
Iteration 375/1000 | Loss: 0.00001924
Iteration 376/1000 | Loss: 0.00002501
Iteration 377/1000 | Loss: 0.00002630
Iteration 378/1000 | Loss: 0.00004188
Iteration 379/1000 | Loss: 0.00003320
Iteration 380/1000 | Loss: 0.00003801
Iteration 381/1000 | Loss: 0.00002965
Iteration 382/1000 | Loss: 0.00004358
Iteration 383/1000 | Loss: 0.00003558
Iteration 384/1000 | Loss: 0.00003948
Iteration 385/1000 | Loss: 0.00003249
Iteration 386/1000 | Loss: 0.00004710
Iteration 387/1000 | Loss: 0.00003848
Iteration 388/1000 | Loss: 0.00005338
Iteration 389/1000 | Loss: 0.00004634
Iteration 390/1000 | Loss: 0.00003585
Iteration 391/1000 | Loss: 0.00003428
Iteration 392/1000 | Loss: 0.00004094
Iteration 393/1000 | Loss: 0.00003488
Iteration 394/1000 | Loss: 0.00004710
Iteration 395/1000 | Loss: 0.00004039
Iteration 396/1000 | Loss: 0.00005278
Iteration 397/1000 | Loss: 0.00004529
Iteration 398/1000 | Loss: 0.00003303
Iteration 399/1000 | Loss: 0.00003215
Iteration 400/1000 | Loss: 0.00004636
Iteration 401/1000 | Loss: 0.00003414
Iteration 402/1000 | Loss: 0.00005774
Iteration 403/1000 | Loss: 0.00004016
Iteration 404/1000 | Loss: 0.00005721
Iteration 405/1000 | Loss: 0.00004070
Iteration 406/1000 | Loss: 0.00003064
Iteration 407/1000 | Loss: 0.00002713
Iteration 408/1000 | Loss: 0.00005227
Iteration 409/1000 | Loss: 0.00004353
Iteration 410/1000 | Loss: 0.00002555
Iteration 411/1000 | Loss: 0.00002089
Iteration 412/1000 | Loss: 0.00002061
Iteration 413/1000 | Loss: 0.00002205
Iteration 414/1000 | Loss: 0.00001992
Iteration 415/1000 | Loss: 0.00003304
Iteration 416/1000 | Loss: 0.00003563
Iteration 417/1000 | Loss: 0.00005502
Iteration 418/1000 | Loss: 0.00004162
Iteration 419/1000 | Loss: 0.00005058
Iteration 420/1000 | Loss: 0.00004326
Iteration 421/1000 | Loss: 0.00004187
Iteration 422/1000 | Loss: 0.00004253
Iteration 423/1000 | Loss: 0.00005183
Iteration 424/1000 | Loss: 0.00003987
Iteration 425/1000 | Loss: 0.00003815
Iteration 426/1000 | Loss: 0.00002906
Iteration 427/1000 | Loss: 0.00002106
Iteration 428/1000 | Loss: 0.00001831
Iteration 429/1000 | Loss: 0.00002580
Iteration 430/1000 | Loss: 0.00002241
Iteration 431/1000 | Loss: 0.00002067
Iteration 432/1000 | Loss: 0.00002942
Iteration 433/1000 | Loss: 0.00002908
Iteration 434/1000 | Loss: 0.00002041
Iteration 435/1000 | Loss: 0.00004983
Iteration 436/1000 | Loss: 0.00002763
Iteration 437/1000 | Loss: 0.00002129
Iteration 438/1000 | Loss: 0.00001880
Iteration 439/1000 | Loss: 0.00003598
Iteration 440/1000 | Loss: 0.00002615
Iteration 441/1000 | Loss: 0.00002014
Iteration 442/1000 | Loss: 0.00002045
Iteration 443/1000 | Loss: 0.00003455
Iteration 444/1000 | Loss: 0.00002552
Iteration 445/1000 | Loss: 0.00002128
Iteration 446/1000 | Loss: 0.00002074
Iteration 447/1000 | Loss: 0.00002440
Iteration 448/1000 | Loss: 0.00002337
Iteration 449/1000 | Loss: 0.00004591
Iteration 450/1000 | Loss: 0.00003154
Iteration 451/1000 | Loss: 0.00002849
Iteration 452/1000 | Loss: 0.00002588
Iteration 453/1000 | Loss: 0.00003392
Iteration 454/1000 | Loss: 0.00003232
Iteration 455/1000 | Loss: 0.00005310
Iteration 456/1000 | Loss: 0.00003730
Iteration 457/1000 | Loss: 0.00005073
Iteration 458/1000 | Loss: 0.00003673
Iteration 459/1000 | Loss: 0.00003183
Iteration 460/1000 | Loss: 0.00002148
Iteration 461/1000 | Loss: 0.00001830
Iteration 462/1000 | Loss: 0.00002392
Iteration 463/1000 | Loss: 0.00001968
Iteration 464/1000 | Loss: 0.00001907
Iteration 465/1000 | Loss: 0.00001970
Iteration 466/1000 | Loss: 0.00001916
Iteration 467/1000 | Loss: 0.00002173
Iteration 468/1000 | Loss: 0.00004090
Iteration 469/1000 | Loss: 0.00003589
Iteration 470/1000 | Loss: 0.00002301
Iteration 471/1000 | Loss: 0.00004317
Iteration 472/1000 | Loss: 0.00003137
Iteration 473/1000 | Loss: 0.00002750
Iteration 474/1000 | Loss: 0.00002284
Iteration 475/1000 | Loss: 0.00002224
Iteration 476/1000 | Loss: 0.00002213
Iteration 477/1000 | Loss: 0.00002199
Iteration 478/1000 | Loss: 0.00004187
Iteration 479/1000 | Loss: 0.00003061
Iteration 480/1000 | Loss: 0.00002847
Iteration 481/1000 | Loss: 0.00002434
Iteration 482/1000 | Loss: 0.00002587
Iteration 483/1000 | Loss: 0.00002453
Iteration 484/1000 | Loss: 0.00004037
Iteration 485/1000 | Loss: 0.00003244
Iteration 486/1000 | Loss: 0.00002617
Iteration 487/1000 | Loss: 0.00002035
Iteration 488/1000 | Loss: 0.00001856
Iteration 489/1000 | Loss: 0.00002737
Iteration 490/1000 | Loss: 0.00002351
Iteration 491/1000 | Loss: 0.00003444
Iteration 492/1000 | Loss: 0.00002612
Iteration 493/1000 | Loss: 0.00001872
Iteration 494/1000 | Loss: 0.00003785
Iteration 495/1000 | Loss: 0.00002724
Iteration 496/1000 | Loss: 0.00002246
Iteration 497/1000 | Loss: 0.00002026
Iteration 498/1000 | Loss: 0.00001880
Iteration 499/1000 | Loss: 0.00002156
Iteration 500/1000 | Loss: 0.00003821
Iteration 501/1000 | Loss: 0.00003177
Iteration 502/1000 | Loss: 0.00002368
Iteration 503/1000 | Loss: 0.00002356
Iteration 504/1000 | Loss: 0.00002133
Iteration 505/1000 | Loss: 0.00004275
Iteration 506/1000 | Loss: 0.00002736
Iteration 507/1000 | Loss: 0.00004282
Iteration 508/1000 | Loss: 0.00003134
Iteration 509/1000 | Loss: 0.00003109
Iteration 510/1000 | Loss: 0.00002938
Iteration 511/1000 | Loss: 0.00003516
Iteration 512/1000 | Loss: 0.00002491
Iteration 513/1000 | Loss: 0.00001893
Iteration 514/1000 | Loss: 0.00002427
Iteration 515/1000 | Loss: 0.00002258
Iteration 516/1000 | Loss: 0.00004044
Iteration 517/1000 | Loss: 0.00002390
Iteration 518/1000 | Loss: 0.00003109
Iteration 519/1000 | Loss: 0.00002677
Iteration 520/1000 | Loss: 0.00003594
Iteration 521/1000 | Loss: 0.00002798
Iteration 522/1000 | Loss: 0.00002192
Iteration 523/1000 | Loss: 0.00002033
Iteration 524/1000 | Loss: 0.00001987
Iteration 525/1000 | Loss: 0.00001885
Iteration 526/1000 | Loss: 0.00001874
Iteration 527/1000 | Loss: 0.00002208
Iteration 528/1000 | Loss: 0.00002320
Iteration 529/1000 | Loss: 0.00002776
Iteration 530/1000 | Loss: 0.00002609
Iteration 531/1000 | Loss: 0.00001934
Iteration 532/1000 | Loss: 0.00003869
Iteration 533/1000 | Loss: 0.00002623
Iteration 534/1000 | Loss: 0.00002059
Iteration 535/1000 | Loss: 0.00001916
Iteration 536/1000 | Loss: 0.00002022
Iteration 537/1000 | Loss: 0.00002063
Iteration 538/1000 | Loss: 0.00003454
Iteration 539/1000 | Loss: 0.00002880
Iteration 540/1000 | Loss: 0.00002156
Iteration 541/1000 | Loss: 0.00002173
Iteration 542/1000 | Loss: 0.00004327
Iteration 543/1000 | Loss: 0.00003680
Iteration 544/1000 | Loss: 0.00002931
Iteration 545/1000 | Loss: 0.00002813
Iteration 546/1000 | Loss: 0.00002294
Iteration 547/1000 | Loss: 0.00005473
Iteration 548/1000 | Loss: 0.00003589
Iteration 549/1000 | Loss: 0.00002609
Iteration 550/1000 | Loss: 0.00002289
Iteration 551/1000 | Loss: 0.00001951
Iteration 552/1000 | Loss: 0.00003001
Iteration 553/1000 | Loss: 0.00002848
Iteration 554/1000 | Loss: 0.00003054
Iteration 555/1000 | Loss: 0.00002963
Iteration 556/1000 | Loss: 0.00003985
Iteration 557/1000 | Loss: 0.00003347
Iteration 558/1000 | Loss: 0.00005085
Iteration 559/1000 | Loss: 0.00004200
Iteration 560/1000 | Loss: 0.00004050
Iteration 561/1000 | Loss: 0.00003709
Iteration 562/1000 | Loss: 0.00003305
Iteration 563/1000 | Loss: 0.00002782
Iteration 564/1000 | Loss: 0.00003058
Iteration 565/1000 | Loss: 0.00002964
Iteration 566/1000 | Loss: 0.00002026
Iteration 567/1000 | Loss: 0.00002264
Iteration 568/1000 | Loss: 0.00004288
Iteration 569/1000 | Loss: 0.00003850
Iteration 570/1000 | Loss: 0.00002066
Iteration 571/1000 | Loss: 0.00001973
Iteration 572/1000 | Loss: 0.00002743
Iteration 573/1000 | Loss: 0.00003186
Iteration 574/1000 | Loss: 0.00004482
Iteration 575/1000 | Loss: 0.00003943
Iteration 576/1000 | Loss: 0.00002042
Iteration 577/1000 | Loss: 0.00001898
Iteration 578/1000 | Loss: 0.00002364
Iteration 579/1000 | Loss: 0.00002137
Iteration 580/1000 | Loss: 0.00002151
Iteration 581/1000 | Loss: 0.00003683
Iteration 582/1000 | Loss: 0.00003105
Iteration 583/1000 | Loss: 0.00003207
Iteration 584/1000 | Loss: 0.00003043
Iteration 585/1000 | Loss: 0.00004428
Iteration 586/1000 | Loss: 0.00004023
Iteration 587/1000 | Loss: 0.00004564
Iteration 588/1000 | Loss: 0.00003664
Iteration 589/1000 | Loss: 0.00002505
Iteration 590/1000 | Loss: 0.00002489
Iteration 591/1000 | Loss: 0.00003008
Iteration 592/1000 | Loss: 0.00003047
Iteration 593/1000 | Loss: 0.00004381
Iteration 594/1000 | Loss: 0.00003514
Iteration 595/1000 | Loss: 0.00006001
Iteration 596/1000 | Loss: 0.00003907
Iteration 597/1000 | Loss: 0.00004089
Iteration 598/1000 | Loss: 0.00003459
Iteration 599/1000 | Loss: 0.00003348
Iteration 600/1000 | Loss: 0.00002478
Iteration 601/1000 | Loss: 0.00003560
Iteration 602/1000 | Loss: 0.00003092
Iteration 603/1000 | Loss: 0.00004085
Iteration 604/1000 | Loss: 0.00003570
Iteration 605/1000 | Loss: 0.00005996
Iteration 606/1000 | Loss: 0.00003662
Iteration 607/1000 | Loss: 0.00003160
Iteration 608/1000 | Loss: 0.00003195
Iteration 609/1000 | Loss: 0.00002720
Iteration 610/1000 | Loss: 0.00002389
Iteration 611/1000 | Loss: 0.00002885
Iteration 612/1000 | Loss: 0.00002975
Iteration 613/1000 | Loss: 0.00002801
Iteration 614/1000 | Loss: 0.00007709
Iteration 615/1000 | Loss: 0.00003928
Iteration 616/1000 | Loss: 0.00002342
Iteration 617/1000 | Loss: 0.00001895
Iteration 618/1000 | Loss: 0.00002108
Iteration 619/1000 | Loss: 0.00002163
Iteration 620/1000 | Loss: 0.00003067
Iteration 621/1000 | Loss: 0.00002606
Iteration 622/1000 | Loss: 0.00003451
Iteration 623/1000 | Loss: 0.00002185
Iteration 624/1000 | Loss: 0.00001886
Iteration 625/1000 | Loss: 0.00001996
Iteration 626/1000 | Loss: 0.00001844
Iteration 627/1000 | Loss: 0.00002008
Iteration 628/1000 | Loss: 0.00003459
Iteration 629/1000 | Loss: 0.00002923
Iteration 630/1000 | Loss: 0.00003200
Iteration 631/1000 | Loss: 0.00002445
Iteration 632/1000 | Loss: 0.00002787
Iteration 633/1000 | Loss: 0.00002709
Iteration 634/1000 | Loss: 0.00002172
Iteration 635/1000 | Loss: 0.00002527
Iteration 636/1000 | Loss: 0.00002518
Iteration 637/1000 | Loss: 0.00002656
Iteration 638/1000 | Loss: 0.00002846
Iteration 639/1000 | Loss: 0.00003560
Iteration 640/1000 | Loss: 0.00003890
Iteration 641/1000 | Loss: 0.00003930
Iteration 642/1000 | Loss: 0.00004337
Iteration 643/1000 | Loss: 0.00002540
Iteration 644/1000 | Loss: 0.00002692
Iteration 645/1000 | Loss: 0.00003388
Iteration 646/1000 | Loss: 0.00002946
Iteration 647/1000 | Loss: 0.00002898
Iteration 648/1000 | Loss: 0.00002404
Iteration 649/1000 | Loss: 0.00002233
Iteration 650/1000 | Loss: 0.00001886
Iteration 651/1000 | Loss: 0.00001949
Iteration 652/1000 | Loss: 0.00002487
Iteration 653/1000 | Loss: 0.00003005
Iteration 654/1000 | Loss: 0.00002386
Iteration 655/1000 | Loss: 0.00002266
Iteration 656/1000 | Loss: 0.00003085
Iteration 657/1000 | Loss: 0.00003302
Iteration 658/1000 | Loss: 0.00002252
Iteration 659/1000 | Loss: 0.00002995
Iteration 660/1000 | Loss: 0.00002115
Iteration 661/1000 | Loss: 0.00002813
Iteration 662/1000 | Loss: 0.00002252
Iteration 663/1000 | Loss: 0.00003852
Iteration 664/1000 | Loss: 0.00002928
Iteration 665/1000 | Loss: 0.00004634
Iteration 666/1000 | Loss: 0.00002435
Iteration 667/1000 | Loss: 0.00002995
Iteration 668/1000 | Loss: 0.00002333
Iteration 669/1000 | Loss: 0.00003809
Iteration 670/1000 | Loss: 0.00002791
Iteration 671/1000 | Loss: 0.00004148
Iteration 672/1000 | Loss: 0.00003085
Iteration 673/1000 | Loss: 0.00004540
Iteration 674/1000 | Loss: 0.00003010
Iteration 675/1000 | Loss: 0.00003981
Iteration 676/1000 | Loss: 0.00003130
Iteration 677/1000 | Loss: 0.00004562
Iteration 678/1000 | Loss: 0.00004140
Iteration 679/1000 | Loss: 0.00005805
Iteration 680/1000 | Loss: 0.00002856
Iteration 681/1000 | Loss: 0.00002571
Iteration 682/1000 | Loss: 0.00002321
Iteration 683/1000 | Loss: 0.00002035
Iteration 684/1000 | Loss: 0.00002419
Iteration 685/1000 | Loss: 0.00002540
Iteration 686/1000 | Loss: 0.00004007
Iteration 687/1000 | Loss: 0.00003375
Iteration 688/1000 | Loss: 0.00004934
Iteration 689/1000 | Loss: 0.00004472
Iteration 690/1000 | Loss: 0.00002804
Iteration 691/1000 | Loss: 0.00002112
Iteration 692/1000 | Loss: 0.00001869
Iteration 693/1000 | Loss: 0.00001941
Iteration 694/1000 | Loss: 0.00001996
Iteration 695/1000 | Loss: 0.00002781
Iteration 696/1000 | Loss: 0.00004068
Iteration 697/1000 | Loss: 0.00004728
Iteration 698/1000 | Loss: 0.00004447
Iteration 699/1000 | Loss: 0.00005558
Iteration 700/1000 | Loss: 0.00003398
Iteration 701/1000 | Loss: 0.00004215
Iteration 702/1000 | Loss: 0.00003922
Iteration 703/1000 | Loss: 0.00004818
Iteration 704/1000 | Loss: 0.00003915
Iteration 705/1000 | Loss: 0.00005447
Iteration 706/1000 | Loss: 0.00003726
Iteration 707/1000 | Loss: 0.00004760
Iteration 708/1000 | Loss: 0.00003077
Iteration 709/1000 | Loss: 0.00002885
Iteration 710/1000 | Loss: 0.00002067
Iteration 711/1000 | Loss: 0.00002859
Iteration 712/1000 | Loss: 0.00003472
Iteration 713/1000 | Loss: 0.00004565
Iteration 714/1000 | Loss: 0.00004835
Iteration 715/1000 | Loss: 0.00005476
Iteration 716/1000 | Loss: 0.00004920
Iteration 717/1000 | Loss: 0.00006801
Iteration 718/1000 | Loss: 0.00002797
Iteration 719/1000 | Loss: 0.00001854
Iteration 720/1000 | Loss: 0.00002074
Iteration 721/1000 | Loss: 0.00003729
Iteration 722/1000 | Loss: 0.00006595
Iteration 723/1000 | Loss: 0.00003797
Iteration 724/1000 | Loss: 0.00004884
Iteration 725/1000 | Loss: 0.00004088
Iteration 726/1000 | Loss: 0.00005340
Iteration 727/1000 | Loss: 0.00005234
Iteration 728/1000 | Loss: 0.00006679
Iteration 729/1000 | Loss: 0.00005432
Iteration 730/1000 | Loss: 0.00006709
Iteration 731/1000 | Loss: 0.00003964
Iteration 732/1000 | Loss: 0.00002299
Iteration 733/1000 | Loss: 0.00001911
Iteration 734/1000 | Loss: 0.00001815
Iteration 735/1000 | Loss: 0.00002617
Iteration 736/1000 | Loss: 0.00002589
Iteration 737/1000 | Loss: 0.00002340
Iteration 738/1000 | Loss: 0.00002307
Iteration 739/1000 | Loss: 0.00002141
Iteration 740/1000 | Loss: 0.00001955
Iteration 741/1000 | Loss: 0.00002088
Iteration 742/1000 | Loss: 0.00003668
Iteration 743/1000 | Loss: 0.00002456
Iteration 744/1000 | Loss: 0.00001953
Iteration 745/1000 | Loss: 0.00001887
Iteration 746/1000 | Loss: 0.00001873
Iteration 747/1000 | Loss: 0.00002029
Iteration 748/1000 | Loss: 0.00001941
Iteration 749/1000 | Loss: 0.00002785
Iteration 750/1000 | Loss: 0.00002334
Iteration 751/1000 | Loss: 0.00003058
Iteration 752/1000 | Loss: 0.00002454
Iteration 753/1000 | Loss: 0.00003517
Iteration 754/1000 | Loss: 0.00002690
Iteration 755/1000 | Loss: 0.00001976
Iteration 756/1000 | Loss: 0.00002859
Iteration 757/1000 | Loss: 0.00002523
Iteration 758/1000 | Loss: 0.00002241
Iteration 759/1000 | Loss: 0.00002058
Iteration 760/1000 | Loss: 0.00002304
Iteration 761/1000 | Loss: 0.00002239
Iteration 762/1000 | Loss: 0.00003187
Iteration 763/1000 | Loss: 0.00002423
Iteration 764/1000 | Loss: 0.00002482
Iteration 765/1000 | Loss: 0.00002360
Iteration 766/1000 | Loss: 0.00004526
Iteration 767/1000 | Loss: 0.00003282
Iteration 768/1000 | Loss: 0.00005100
Iteration 769/1000 | Loss: 0.00003114
Iteration 770/1000 | Loss: 0.00004397
Iteration 771/1000 | Loss: 0.00003256
Iteration 772/1000 | Loss: 0.00001936
Iteration 773/1000 | Loss: 0.00001928
Iteration 774/1000 | Loss: 0.00001958
Iteration 775/1000 | Loss: 0.00004217
Iteration 776/1000 | Loss: 0.00002864
Iteration 777/1000 | Loss: 0.00004073
Iteration 778/1000 | Loss: 0.00003254
Iteration 779/1000 | Loss: 0.00001968
Iteration 780/1000 | Loss: 0.00001856
Iteration 781/1000 | Loss: 0.00002965
Iteration 782/1000 | Loss: 0.00002298
Iteration 783/1000 | Loss: 0.00004097
Iteration 784/1000 | Loss: 0.00003001
Iteration 785/1000 | Loss: 0.00005254
Iteration 786/1000 | Loss: 0.00003734
Iteration 787/1000 | Loss: 0.00005365
Iteration 788/1000 | Loss: 0.00003927
Iteration 789/1000 | Loss: 0.00005940
Iteration 790/1000 | Loss: 0.00004175
Iteration 791/1000 | Loss: 0.00004981
Iteration 792/1000 | Loss: 0.00003755
Iteration 793/1000 | Loss: 0.00002603
Iteration 794/1000 | Loss: 0.00004912
Iteration 795/1000 | Loss: 0.00002726
Iteration 796/1000 | Loss: 0.00002145
Iteration 797/1000 | Loss: 0.00002025
Iteration 798/1000 | Loss: 0.00001827
Iteration 799/1000 | Loss: 0.00001795
Iteration 800/1000 | Loss: 0.00003971
Iteration 801/1000 | Loss: 0.00002504
Iteration 802/1000 | Loss: 0.00004010
Iteration 803/1000 | Loss: 0.00002673
Iteration 804/1000 | Loss: 0.00005185
Iteration 805/1000 | Loss: 0.00003612
Iteration 806/1000 | Loss: 0.00001931
Iteration 807/1000 | Loss: 0.00001865
Iteration 808/1000 | Loss: 0.00001994
Iteration 809/1000 | Loss: 0.00004317
Iteration 810/1000 | Loss: 0.00002895
Iteration 811/1000 | Loss: 0.00002337
Iteration 812/1000 | Loss: 0.00001963
Iteration 813/1000 | Loss: 0.00001945
Iteration 814/1000 | Loss: 0.00002014
Iteration 815/1000 | Loss: 0.00002091
Iteration 816/1000 | Loss: 0.00001981
Iteration 817/1000 | Loss: 0.00004469
Iteration 818/1000 | Loss: 0.00002620
Iteration 819/1000 | Loss: 0.00004571
Iteration 820/1000 | Loss: 0.00002444
Iteration 821/1000 | Loss: 0.00001876
Iteration 822/1000 | Loss: 0.00003374
Iteration 823/1000 | Loss: 0.00002266
Iteration 824/1000 | Loss: 0.00001911
Iteration 825/1000 | Loss: 0.00002760
Iteration 826/1000 | Loss: 0.00002155
Iteration 827/1000 | Loss: 0.00002877
Iteration 828/1000 | Loss: 0.00002364
Iteration 829/1000 | Loss: 0.00002433
Iteration 830/1000 | Loss: 0.00001965
Iteration 831/1000 | Loss: 0.00001973
Iteration 832/1000 | Loss: 0.00001891
Iteration 833/1000 | Loss: 0.00002824
Iteration 834/1000 | Loss: 0.00002055
Iteration 835/1000 | Loss: 0.00002089
Iteration 836/1000 | Loss: 0.00001907
Iteration 837/1000 | Loss: 0.00004621
Iteration 838/1000 | Loss: 0.00002307
Iteration 839/1000 | Loss: 0.00001954
Iteration 840/1000 | Loss: 0.00003540
Iteration 841/1000 | Loss: 0.00002441
Iteration 842/1000 | Loss: 0.00002429
Iteration 843/1000 | Loss: 0.00002081
Iteration 844/1000 | Loss: 0.00002481
Iteration 845/1000 | Loss: 0.00002335
Iteration 846/1000 | Loss: 0.00002940
Iteration 847/1000 | Loss: 0.00002668
Iteration 848/1000 | Loss: 0.00005035
Iteration 849/1000 | Loss: 0.00003434
Iteration 850/1000 | Loss: 0.00004687
Iteration 851/1000 | Loss: 0.00003252
Iteration 852/1000 | Loss: 0.00005160
Iteration 853/1000 | Loss: 0.00002807
Iteration 854/1000 | Loss: 0.00004330
Iteration 855/1000 | Loss: 0.00002993
Iteration 856/1000 | Loss: 0.00002157
Iteration 857/1000 | Loss: 0.00002131
Iteration 858/1000 | Loss: 0.00002869
Iteration 859/1000 | Loss: 0.00002532
Iteration 860/1000 | Loss: 0.00004863
Iteration 861/1000 | Loss: 0.00003547
Iteration 862/1000 | Loss: 0.00002170
Iteration 863/1000 | Loss: 0.00001975
Iteration 864/1000 | Loss: 0.00002187
Iteration 865/1000 | Loss: 0.00002215
Iteration 866/1000 | Loss: 0.00003786
Iteration 867/1000 | Loss: 0.00002804
Iteration 868/1000 | Loss: 0.00004706
Iteration 869/1000 | Loss: 0.00002375
Iteration 870/1000 | Loss: 0.00001860
Iteration 871/1000 | Loss: 0.00001926
Iteration 872/1000 | Loss: 0.00003149
Iteration 873/1000 | Loss: 0.00002404
Iteration 874/1000 | Loss: 0.00002444
Iteration 875/1000 | Loss: 0.00002048
Iteration 876/1000 | Loss: 0.00002266
Iteration 877/1000 | Loss: 0.00003650
Iteration 878/1000 | Loss: 0.00002982
Iteration 879/1000 | Loss: 0.00002481
Iteration 880/1000 | Loss: 0.00002449
Iteration 881/1000 | Loss: 0.00004006
Iteration 882/1000 | Loss: 0.00003298
Iteration 883/1000 | Loss: 0.00003197
Iteration 884/1000 | Loss: 0.00003267
Iteration 885/1000 | Loss: 0.00002570
Iteration 886/1000 | Loss: 0.00002614
Iteration 887/1000 | Loss: 0.00003084
Iteration 888/1000 | Loss: 0.00003165
Iteration 889/1000 | Loss: 0.00005050
Iteration 890/1000 | Loss: 0.00003145
Iteration 891/1000 | Loss: 0.00005284
Iteration 892/1000 | Loss: 0.00003365
Iteration 893/1000 | Loss: 0.00004949
Iteration 894/1000 | Loss: 0.00002888
Iteration 895/1000 | Loss: 0.00002437
Iteration 896/1000 | Loss: 0.00002164
Iteration 897/1000 | Loss: 0.00002100
Iteration 898/1000 | Loss: 0.00004138
Iteration 899/1000 | Loss: 0.00002484
Iteration 900/1000 | Loss: 0.00001905
Iteration 901/1000 | Loss: 0.00001863
Iteration 902/1000 | Loss: 0.00001801
Iteration 903/1000 | Loss: 0.00001820
Iteration 904/1000 | Loss: 0.00002065
Iteration 905/1000 | Loss: 0.00005223
Iteration 906/1000 | Loss: 0.00002577
Iteration 907/1000 | Loss: 0.00004379
Iteration 908/1000 | Loss: 0.00002987
Iteration 909/1000 | Loss: 0.00001986
Iteration 910/1000 | Loss: 0.00003083
Iteration 911/1000 | Loss: 0.00002443
Iteration 912/1000 | Loss: 0.00002040
Iteration 913/1000 | Loss: 0.00002076
Iteration 914/1000 | Loss: 0.00002499
Iteration 915/1000 | Loss: 0.00002781
Iteration 916/1000 | Loss: 0.00002289
Iteration 917/1000 | Loss: 0.00003930
Iteration 918/1000 | Loss: 0.00002337
Iteration 919/1000 | Loss: 0.00002384
Iteration 920/1000 | Loss: 0.00002185
Iteration 921/1000 | Loss: 0.00003240
Iteration 922/1000 | Loss: 0.00002380
Iteration 923/1000 | Loss: 0.00001855
Iteration 924/1000 | Loss: 0.00002942
Iteration 925/1000 | Loss: 0.00002486
Iteration 926/1000 | Loss: 0.00004917
Iteration 927/1000 | Loss: 0.00003015
Iteration 928/1000 | Loss: 0.00005352
Iteration 929/1000 | Loss: 0.00003234
Iteration 930/1000 | Loss: 0.00002978
Iteration 931/1000 | Loss: 0.00002514
Iteration 932/1000 | Loss: 0.00004395
Iteration 933/1000 | Loss: 0.00003304
Iteration 934/1000 | Loss: 0.00003076
Iteration 935/1000 | Loss: 0.00002594
Iteration 936/1000 | Loss: 0.00004050
Iteration 937/1000 | Loss: 0.00002831
Iteration 938/1000 | Loss: 0.00005449
Iteration 939/1000 | Loss: 0.00003951
Iteration 940/1000 | Loss: 0.00005806
Iteration 941/1000 | Loss: 0.00004235
Iteration 942/1000 | Loss: 0.00003127
Iteration 943/1000 | Loss: 0.00002286
Iteration 944/1000 | Loss: 0.00005113
Iteration 945/1000 | Loss: 0.00003648
Iteration 946/1000 | Loss: 0.00005903
Iteration 947/1000 | Loss: 0.00004626
Iteration 948/1000 | Loss: 0.00003023
Iteration 949/1000 | Loss: 0.00002198
Iteration 950/1000 | Loss: 0.00003549
Iteration 951/1000 | Loss: 0.00002575
Iteration 952/1000 | Loss: 0.00003798
Iteration 953/1000 | Loss: 0.00003061
Iteration 954/1000 | Loss: 0.00005812
Iteration 955/1000 | Loss: 0.00004226
Iteration 956/1000 | Loss: 0.00004130
Iteration 957/1000 | Loss: 0.00003750
Iteration 958/1000 | Loss: 0.00005370
Iteration 959/1000 | Loss: 0.00003776
Iteration 960/1000 | Loss: 0.00002298
Iteration 961/1000 | Loss: 0.00001994
Iteration 962/1000 | Loss: 0.00005985
Iteration 963/1000 | Loss: 0.00004893
Iteration 964/1000 | Loss: 0.00006267
Iteration 965/1000 | Loss: 0.00004445
Iteration 966/1000 | Loss: 0.00006508
Iteration 967/1000 | Loss: 0.00005154
Iteration 968/1000 | Loss: 0.00004900
Iteration 969/1000 | Loss: 0.00004314
Iteration 970/1000 | Loss: 0.00004675
Iteration 971/1000 | Loss: 0.00004609
Iteration 972/1000 | Loss: 0.00004627
Iteration 973/1000 | Loss: 0.00003367
Iteration 974/1000 | Loss: 0.00004676
Iteration 975/1000 | Loss: 0.00003538
Iteration 976/1000 | Loss: 0.00002260
Iteration 977/1000 | Loss: 0.00001900
Iteration 978/1000 | Loss: 0.00002950
Iteration 979/1000 | Loss: 0.00003104
Iteration 980/1000 | Loss: 0.00002933
Iteration 981/1000 | Loss: 0.00001987
Iteration 982/1000 | Loss: 0.00003012
Iteration 983/1000 | Loss: 0.00002291
Iteration 984/1000 | Loss: 0.00002075
Iteration 985/1000 | Loss: 0.00001920
Iteration 986/1000 | Loss: 0.00004216
Iteration 987/1000 | Loss: 0.00003038
Iteration 988/1000 | Loss: 0.00002985
Iteration 989/1000 | Loss: 0.00002015
Iteration 990/1000 | Loss: 0.00003123
Iteration 991/1000 | Loss: 0.00002194
Iteration 992/1000 | Loss: 0.00002317
Iteration 993/1000 | Loss: 0.00001991
Iteration 994/1000 | Loss: 0.00004813
Iteration 995/1000 | Loss: 0.00002804
Iteration 996/1000 | Loss: 0.00003801
Iteration 997/1000 | Loss: 0.00002735
Iteration 998/1000 | Loss: 0.00005810
Iteration 999/1000 | Loss: 0.00003651
Iteration 1000/1000 | Loss: 0.00006873

Optimization complete. Final v2v error: 3.8145711421966553 mm

Highest mean error: 10.824923515319824 mm for frame 122

Lowest mean error: 2.288949489593506 mm for frame 9

Saving results

Total time: 1373.1368958950043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827937
Iteration 2/25 | Loss: 0.00115911
Iteration 3/25 | Loss: 0.00098771
Iteration 4/25 | Loss: 0.00097333
Iteration 5/25 | Loss: 0.00097054
Iteration 6/25 | Loss: 0.00097042
Iteration 7/25 | Loss: 0.00097042
Iteration 8/25 | Loss: 0.00097042
Iteration 9/25 | Loss: 0.00097042
Iteration 10/25 | Loss: 0.00097042
Iteration 11/25 | Loss: 0.00097042
Iteration 12/25 | Loss: 0.00097042
Iteration 13/25 | Loss: 0.00097042
Iteration 14/25 | Loss: 0.00097042
Iteration 15/25 | Loss: 0.00097042
Iteration 16/25 | Loss: 0.00097042
Iteration 17/25 | Loss: 0.00097042
Iteration 18/25 | Loss: 0.00097042
Iteration 19/25 | Loss: 0.00097042
Iteration 20/25 | Loss: 0.00097042
Iteration 21/25 | Loss: 0.00097042
Iteration 22/25 | Loss: 0.00097042
Iteration 23/25 | Loss: 0.00097042
Iteration 24/25 | Loss: 0.00097042
Iteration 25/25 | Loss: 0.00097042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.36384583
Iteration 2/25 | Loss: 0.00120309
Iteration 3/25 | Loss: 0.00120309
Iteration 4/25 | Loss: 0.00120309
Iteration 5/25 | Loss: 0.00120309
Iteration 6/25 | Loss: 0.00120309
Iteration 7/25 | Loss: 0.00120309
Iteration 8/25 | Loss: 0.00120309
Iteration 9/25 | Loss: 0.00120309
Iteration 10/25 | Loss: 0.00120309
Iteration 11/25 | Loss: 0.00120309
Iteration 12/25 | Loss: 0.00120309
Iteration 13/25 | Loss: 0.00120309
Iteration 14/25 | Loss: 0.00120309
Iteration 15/25 | Loss: 0.00120309
Iteration 16/25 | Loss: 0.00120309
Iteration 17/25 | Loss: 0.00120309
Iteration 18/25 | Loss: 0.00120309
Iteration 19/25 | Loss: 0.00120309
Iteration 20/25 | Loss: 0.00120309
Iteration 21/25 | Loss: 0.00120309
Iteration 22/25 | Loss: 0.00120309
Iteration 23/25 | Loss: 0.00120309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012030863435938954, 0.0012030863435938954, 0.0012030863435938954, 0.0012030863435938954, 0.0012030863435938954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012030863435938954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120309
Iteration 2/1000 | Loss: 0.00003754
Iteration 3/1000 | Loss: 0.00002504
Iteration 4/1000 | Loss: 0.00002220
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001901
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001786
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001715
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001694
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001671
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001651
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001643
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001632
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001632
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001630
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001628
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001627
Iteration 52/1000 | Loss: 0.00001626
Iteration 53/1000 | Loss: 0.00001625
Iteration 54/1000 | Loss: 0.00001625
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001618
Iteration 62/1000 | Loss: 0.00001615
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001614
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001613
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001611
Iteration 78/1000 | Loss: 0.00001611
Iteration 79/1000 | Loss: 0.00001611
Iteration 80/1000 | Loss: 0.00001611
Iteration 81/1000 | Loss: 0.00001611
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001610
Iteration 84/1000 | Loss: 0.00001610
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001609
Iteration 92/1000 | Loss: 0.00001609
Iteration 93/1000 | Loss: 0.00001609
Iteration 94/1000 | Loss: 0.00001609
Iteration 95/1000 | Loss: 0.00001609
Iteration 96/1000 | Loss: 0.00001609
Iteration 97/1000 | Loss: 0.00001609
Iteration 98/1000 | Loss: 0.00001609
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.607904050615616e-05, 1.607904050615616e-05, 1.607904050615616e-05, 1.607904050615616e-05, 1.607904050615616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.607904050615616e-05

Optimization complete. Final v2v error: 3.418757438659668 mm

Highest mean error: 4.63169527053833 mm for frame 71

Lowest mean error: 2.535750389099121 mm for frame 237

Saving results

Total time: 42.91228461265564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847964
Iteration 2/25 | Loss: 0.00114616
Iteration 3/25 | Loss: 0.00098668
Iteration 4/25 | Loss: 0.00095369
Iteration 5/25 | Loss: 0.00094091
Iteration 6/25 | Loss: 0.00093655
Iteration 7/25 | Loss: 0.00093540
Iteration 8/25 | Loss: 0.00093516
Iteration 9/25 | Loss: 0.00093516
Iteration 10/25 | Loss: 0.00093516
Iteration 11/25 | Loss: 0.00093516
Iteration 12/25 | Loss: 0.00093516
Iteration 13/25 | Loss: 0.00093516
Iteration 14/25 | Loss: 0.00093516
Iteration 15/25 | Loss: 0.00093516
Iteration 16/25 | Loss: 0.00093516
Iteration 17/25 | Loss: 0.00093516
Iteration 18/25 | Loss: 0.00093516
Iteration 19/25 | Loss: 0.00093516
Iteration 20/25 | Loss: 0.00093516
Iteration 21/25 | Loss: 0.00093516
Iteration 22/25 | Loss: 0.00093516
Iteration 23/25 | Loss: 0.00093516
Iteration 24/25 | Loss: 0.00093516
Iteration 25/25 | Loss: 0.00093516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42342949
Iteration 2/25 | Loss: 0.00170889
Iteration 3/25 | Loss: 0.00170889
Iteration 4/25 | Loss: 0.00170889
Iteration 5/25 | Loss: 0.00170888
Iteration 6/25 | Loss: 0.00170888
Iteration 7/25 | Loss: 0.00170888
Iteration 8/25 | Loss: 0.00170888
Iteration 9/25 | Loss: 0.00170888
Iteration 10/25 | Loss: 0.00170888
Iteration 11/25 | Loss: 0.00170888
Iteration 12/25 | Loss: 0.00170888
Iteration 13/25 | Loss: 0.00170888
Iteration 14/25 | Loss: 0.00170888
Iteration 15/25 | Loss: 0.00170888
Iteration 16/25 | Loss: 0.00170888
Iteration 17/25 | Loss: 0.00170888
Iteration 18/25 | Loss: 0.00170888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017088840249925852, 0.0017088840249925852, 0.0017088840249925852, 0.0017088840249925852, 0.0017088840249925852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017088840249925852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170888
Iteration 2/1000 | Loss: 0.00004595
Iteration 3/1000 | Loss: 0.00003311
Iteration 4/1000 | Loss: 0.00002547
Iteration 5/1000 | Loss: 0.00002362
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002156
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002057
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002001
Iteration 12/1000 | Loss: 0.00001979
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001944
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00001936
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001936
Iteration 27/1000 | Loss: 0.00001935
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001933
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001930
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001926
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001925
Iteration 53/1000 | Loss: 0.00001925
Iteration 54/1000 | Loss: 0.00001925
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001924
Iteration 63/1000 | Loss: 0.00001924
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001921
Iteration 74/1000 | Loss: 0.00001921
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001920
Iteration 80/1000 | Loss: 0.00001919
Iteration 81/1000 | Loss: 0.00001919
Iteration 82/1000 | Loss: 0.00001919
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001918
Iteration 85/1000 | Loss: 0.00001918
Iteration 86/1000 | Loss: 0.00001918
Iteration 87/1000 | Loss: 0.00001918
Iteration 88/1000 | Loss: 0.00001917
Iteration 89/1000 | Loss: 0.00001917
Iteration 90/1000 | Loss: 0.00001917
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001916
Iteration 98/1000 | Loss: 0.00001916
Iteration 99/1000 | Loss: 0.00001916
Iteration 100/1000 | Loss: 0.00001916
Iteration 101/1000 | Loss: 0.00001916
Iteration 102/1000 | Loss: 0.00001916
Iteration 103/1000 | Loss: 0.00001916
Iteration 104/1000 | Loss: 0.00001916
Iteration 105/1000 | Loss: 0.00001916
Iteration 106/1000 | Loss: 0.00001916
Iteration 107/1000 | Loss: 0.00001916
Iteration 108/1000 | Loss: 0.00001916
Iteration 109/1000 | Loss: 0.00001916
Iteration 110/1000 | Loss: 0.00001916
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001916
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001916
Iteration 119/1000 | Loss: 0.00001916
Iteration 120/1000 | Loss: 0.00001916
Iteration 121/1000 | Loss: 0.00001916
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001916
Iteration 124/1000 | Loss: 0.00001916
Iteration 125/1000 | Loss: 0.00001916
Iteration 126/1000 | Loss: 0.00001916
Iteration 127/1000 | Loss: 0.00001916
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001916
Iteration 140/1000 | Loss: 0.00001916
Iteration 141/1000 | Loss: 0.00001916
Iteration 142/1000 | Loss: 0.00001916
Iteration 143/1000 | Loss: 0.00001916
Iteration 144/1000 | Loss: 0.00001916
Iteration 145/1000 | Loss: 0.00001916
Iteration 146/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.9158807845087722e-05, 1.9158807845087722e-05, 1.9158807845087722e-05, 1.9158807845087722e-05, 1.9158807845087722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9158807845087722e-05

Optimization complete. Final v2v error: 3.5778706073760986 mm

Highest mean error: 5.413741588592529 mm for frame 136

Lowest mean error: 2.342893600463867 mm for frame 2

Saving results

Total time: 39.12414526939392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487432
Iteration 2/25 | Loss: 0.00100976
Iteration 3/25 | Loss: 0.00094212
Iteration 4/25 | Loss: 0.00092529
Iteration 5/25 | Loss: 0.00092023
Iteration 6/25 | Loss: 0.00091912
Iteration 7/25 | Loss: 0.00091912
Iteration 8/25 | Loss: 0.00091912
Iteration 9/25 | Loss: 0.00091912
Iteration 10/25 | Loss: 0.00091912
Iteration 11/25 | Loss: 0.00091912
Iteration 12/25 | Loss: 0.00091912
Iteration 13/25 | Loss: 0.00091912
Iteration 14/25 | Loss: 0.00091912
Iteration 15/25 | Loss: 0.00091912
Iteration 16/25 | Loss: 0.00091912
Iteration 17/25 | Loss: 0.00091912
Iteration 18/25 | Loss: 0.00091912
Iteration 19/25 | Loss: 0.00091912
Iteration 20/25 | Loss: 0.00091912
Iteration 21/25 | Loss: 0.00091912
Iteration 22/25 | Loss: 0.00091912
Iteration 23/25 | Loss: 0.00091912
Iteration 24/25 | Loss: 0.00091912
Iteration 25/25 | Loss: 0.00091912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.26290989
Iteration 2/25 | Loss: 0.00125162
Iteration 3/25 | Loss: 0.00125162
Iteration 4/25 | Loss: 0.00125162
Iteration 5/25 | Loss: 0.00125162
Iteration 6/25 | Loss: 0.00125162
Iteration 7/25 | Loss: 0.00125162
Iteration 8/25 | Loss: 0.00125162
Iteration 9/25 | Loss: 0.00125162
Iteration 10/25 | Loss: 0.00125162
Iteration 11/25 | Loss: 0.00125162
Iteration 12/25 | Loss: 0.00125162
Iteration 13/25 | Loss: 0.00125162
Iteration 14/25 | Loss: 0.00125162
Iteration 15/25 | Loss: 0.00125162
Iteration 16/25 | Loss: 0.00125162
Iteration 17/25 | Loss: 0.00125162
Iteration 18/25 | Loss: 0.00125162
Iteration 19/25 | Loss: 0.00125162
Iteration 20/25 | Loss: 0.00125162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012516211718320847, 0.0012516211718320847, 0.0012516211718320847, 0.0012516211718320847, 0.0012516211718320847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012516211718320847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125162
Iteration 2/1000 | Loss: 0.00001977
Iteration 3/1000 | Loss: 0.00001429
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001202
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001091
Iteration 8/1000 | Loss: 0.00001090
Iteration 9/1000 | Loss: 0.00001057
Iteration 10/1000 | Loss: 0.00001036
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001023
Iteration 14/1000 | Loss: 0.00001017
Iteration 15/1000 | Loss: 0.00001014
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001009
Iteration 18/1000 | Loss: 0.00001005
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00000997
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000989
Iteration 23/1000 | Loss: 0.00000979
Iteration 24/1000 | Loss: 0.00000977
Iteration 25/1000 | Loss: 0.00000974
Iteration 26/1000 | Loss: 0.00000973
Iteration 27/1000 | Loss: 0.00000970
Iteration 28/1000 | Loss: 0.00000966
Iteration 29/1000 | Loss: 0.00000965
Iteration 30/1000 | Loss: 0.00000965
Iteration 31/1000 | Loss: 0.00000964
Iteration 32/1000 | Loss: 0.00000963
Iteration 33/1000 | Loss: 0.00000963
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000962
Iteration 36/1000 | Loss: 0.00000962
Iteration 37/1000 | Loss: 0.00000962
Iteration 38/1000 | Loss: 0.00000961
Iteration 39/1000 | Loss: 0.00000961
Iteration 40/1000 | Loss: 0.00000960
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000959
Iteration 43/1000 | Loss: 0.00000959
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000958
Iteration 46/1000 | Loss: 0.00000958
Iteration 47/1000 | Loss: 0.00000957
Iteration 48/1000 | Loss: 0.00000957
Iteration 49/1000 | Loss: 0.00000957
Iteration 50/1000 | Loss: 0.00000957
Iteration 51/1000 | Loss: 0.00000957
Iteration 52/1000 | Loss: 0.00000956
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000955
Iteration 58/1000 | Loss: 0.00000955
Iteration 59/1000 | Loss: 0.00000955
Iteration 60/1000 | Loss: 0.00000955
Iteration 61/1000 | Loss: 0.00000955
Iteration 62/1000 | Loss: 0.00000955
Iteration 63/1000 | Loss: 0.00000954
Iteration 64/1000 | Loss: 0.00000954
Iteration 65/1000 | Loss: 0.00000954
Iteration 66/1000 | Loss: 0.00000954
Iteration 67/1000 | Loss: 0.00000954
Iteration 68/1000 | Loss: 0.00000953
Iteration 69/1000 | Loss: 0.00000953
Iteration 70/1000 | Loss: 0.00000953
Iteration 71/1000 | Loss: 0.00000953
Iteration 72/1000 | Loss: 0.00000953
Iteration 73/1000 | Loss: 0.00000953
Iteration 74/1000 | Loss: 0.00000952
Iteration 75/1000 | Loss: 0.00000952
Iteration 76/1000 | Loss: 0.00000952
Iteration 77/1000 | Loss: 0.00000952
Iteration 78/1000 | Loss: 0.00000952
Iteration 79/1000 | Loss: 0.00000952
Iteration 80/1000 | Loss: 0.00000952
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000952
Iteration 83/1000 | Loss: 0.00000952
Iteration 84/1000 | Loss: 0.00000951
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000951
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000950
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000949
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000948
Iteration 102/1000 | Loss: 0.00000947
Iteration 103/1000 | Loss: 0.00000947
Iteration 104/1000 | Loss: 0.00000947
Iteration 105/1000 | Loss: 0.00000947
Iteration 106/1000 | Loss: 0.00000947
Iteration 107/1000 | Loss: 0.00000947
Iteration 108/1000 | Loss: 0.00000947
Iteration 109/1000 | Loss: 0.00000947
Iteration 110/1000 | Loss: 0.00000947
Iteration 111/1000 | Loss: 0.00000947
Iteration 112/1000 | Loss: 0.00000947
Iteration 113/1000 | Loss: 0.00000946
Iteration 114/1000 | Loss: 0.00000946
Iteration 115/1000 | Loss: 0.00000946
Iteration 116/1000 | Loss: 0.00000945
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000944
Iteration 121/1000 | Loss: 0.00000944
Iteration 122/1000 | Loss: 0.00000944
Iteration 123/1000 | Loss: 0.00000944
Iteration 124/1000 | Loss: 0.00000944
Iteration 125/1000 | Loss: 0.00000944
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000943
Iteration 129/1000 | Loss: 0.00000943
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000941
Iteration 135/1000 | Loss: 0.00000941
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000941
Iteration 145/1000 | Loss: 0.00000941
Iteration 146/1000 | Loss: 0.00000941
Iteration 147/1000 | Loss: 0.00000941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [9.407714060216676e-06, 9.407714060216676e-06, 9.407714060216676e-06, 9.407714060216676e-06, 9.407714060216676e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.407714060216676e-06

Optimization complete. Final v2v error: 2.664900302886963 mm

Highest mean error: 2.884445905685425 mm for frame 170

Lowest mean error: 2.461331367492676 mm for frame 11

Saving results

Total time: 41.53827428817749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080161
Iteration 2/25 | Loss: 0.01080161
Iteration 3/25 | Loss: 0.00461457
Iteration 4/25 | Loss: 0.00512871
Iteration 5/25 | Loss: 0.00239934
Iteration 6/25 | Loss: 0.00213856
Iteration 7/25 | Loss: 0.00213776
Iteration 8/25 | Loss: 0.00199925
Iteration 9/25 | Loss: 0.00188675
Iteration 10/25 | Loss: 0.00180027
Iteration 11/25 | Loss: 0.00170532
Iteration 12/25 | Loss: 0.00168026
Iteration 13/25 | Loss: 0.00164314
Iteration 14/25 | Loss: 0.00161447
Iteration 15/25 | Loss: 0.00158035
Iteration 16/25 | Loss: 0.00156075
Iteration 17/25 | Loss: 0.00154016
Iteration 18/25 | Loss: 0.00153303
Iteration 19/25 | Loss: 0.00153357
Iteration 20/25 | Loss: 0.00152629
Iteration 21/25 | Loss: 0.00152876
Iteration 22/25 | Loss: 0.00152429
Iteration 23/25 | Loss: 0.00152229
Iteration 24/25 | Loss: 0.00152159
Iteration 25/25 | Loss: 0.00152135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52523887
Iteration 2/25 | Loss: 0.00684751
Iteration 3/25 | Loss: 0.00684751
Iteration 4/25 | Loss: 0.00684751
Iteration 5/25 | Loss: 0.00684751
Iteration 6/25 | Loss: 0.00684751
Iteration 7/25 | Loss: 0.00684751
Iteration 8/25 | Loss: 0.00684751
Iteration 9/25 | Loss: 0.00684751
Iteration 10/25 | Loss: 0.00684751
Iteration 11/25 | Loss: 0.00684751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.006847509182989597, 0.006847509182989597, 0.006847509182989597, 0.006847509182989597, 0.006847509182989597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006847509182989597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00684751
Iteration 2/1000 | Loss: 0.00084348
Iteration 3/1000 | Loss: 0.00056770
Iteration 4/1000 | Loss: 0.00043090
Iteration 5/1000 | Loss: 0.00035543
Iteration 6/1000 | Loss: 0.00033148
Iteration 7/1000 | Loss: 0.00047775
Iteration 8/1000 | Loss: 0.00042375
Iteration 9/1000 | Loss: 0.00038369
Iteration 10/1000 | Loss: 0.00030001
Iteration 11/1000 | Loss: 0.00029371
Iteration 12/1000 | Loss: 0.00028892
Iteration 13/1000 | Loss: 0.00028554
Iteration 14/1000 | Loss: 0.00028303
Iteration 15/1000 | Loss: 0.00028126
Iteration 16/1000 | Loss: 0.00048201
Iteration 17/1000 | Loss: 0.00028604
Iteration 18/1000 | Loss: 0.00028082
Iteration 19/1000 | Loss: 0.00027919
Iteration 20/1000 | Loss: 0.00027837
Iteration 21/1000 | Loss: 0.00027767
Iteration 22/1000 | Loss: 0.00041855
Iteration 23/1000 | Loss: 0.00066730
Iteration 24/1000 | Loss: 0.00032460
Iteration 25/1000 | Loss: 0.00028380
Iteration 26/1000 | Loss: 0.00027996
Iteration 27/1000 | Loss: 0.00027599
Iteration 28/1000 | Loss: 0.00027347
Iteration 29/1000 | Loss: 0.00027230
Iteration 30/1000 | Loss: 0.00027168
Iteration 31/1000 | Loss: 0.00027132
Iteration 32/1000 | Loss: 0.00027091
Iteration 33/1000 | Loss: 0.00027070
Iteration 34/1000 | Loss: 0.00027050
Iteration 35/1000 | Loss: 0.00027050
Iteration 36/1000 | Loss: 0.00027050
Iteration 37/1000 | Loss: 0.00027048
Iteration 38/1000 | Loss: 0.00027047
Iteration 39/1000 | Loss: 0.00027043
Iteration 40/1000 | Loss: 0.00027034
Iteration 41/1000 | Loss: 0.00027026
Iteration 42/1000 | Loss: 0.00027021
Iteration 43/1000 | Loss: 0.00027021
Iteration 44/1000 | Loss: 0.00027021
Iteration 45/1000 | Loss: 0.00027020
Iteration 46/1000 | Loss: 0.00027019
Iteration 47/1000 | Loss: 0.00027019
Iteration 48/1000 | Loss: 0.00027018
Iteration 49/1000 | Loss: 0.00027016
Iteration 50/1000 | Loss: 0.00027013
Iteration 51/1000 | Loss: 0.00027011
Iteration 52/1000 | Loss: 0.00027005
Iteration 53/1000 | Loss: 0.00026999
Iteration 54/1000 | Loss: 0.00026999
Iteration 55/1000 | Loss: 0.00026999
Iteration 56/1000 | Loss: 0.00026998
Iteration 57/1000 | Loss: 0.00039128
Iteration 58/1000 | Loss: 0.00027459
Iteration 59/1000 | Loss: 0.00027294
Iteration 60/1000 | Loss: 0.00027205
Iteration 61/1000 | Loss: 0.00027142
Iteration 62/1000 | Loss: 0.00046326
Iteration 63/1000 | Loss: 0.00040735
Iteration 64/1000 | Loss: 0.00030382
Iteration 65/1000 | Loss: 0.00027589
Iteration 66/1000 | Loss: 0.00027346
Iteration 67/1000 | Loss: 0.00027271
Iteration 68/1000 | Loss: 0.00027220
Iteration 69/1000 | Loss: 0.00027192
Iteration 70/1000 | Loss: 0.00027158
Iteration 71/1000 | Loss: 0.00050201
Iteration 72/1000 | Loss: 0.00047550
Iteration 73/1000 | Loss: 0.00045019
Iteration 74/1000 | Loss: 0.00041482
Iteration 75/1000 | Loss: 0.00033031
Iteration 76/1000 | Loss: 0.00027459
Iteration 77/1000 | Loss: 0.00027196
Iteration 78/1000 | Loss: 0.00027126
Iteration 79/1000 | Loss: 0.00027091
Iteration 80/1000 | Loss: 0.00027072
Iteration 81/1000 | Loss: 0.00027069
Iteration 82/1000 | Loss: 0.00027064
Iteration 83/1000 | Loss: 0.00027062
Iteration 84/1000 | Loss: 0.00027062
Iteration 85/1000 | Loss: 0.00027062
Iteration 86/1000 | Loss: 0.00028267
Iteration 87/1000 | Loss: 0.00027188
Iteration 88/1000 | Loss: 0.00027153
Iteration 89/1000 | Loss: 0.00027093
Iteration 90/1000 | Loss: 0.00027074
Iteration 91/1000 | Loss: 0.00027073
Iteration 92/1000 | Loss: 0.00027072
Iteration 93/1000 | Loss: 0.00027071
Iteration 94/1000 | Loss: 0.00027066
Iteration 95/1000 | Loss: 0.00027066
Iteration 96/1000 | Loss: 0.00027063
Iteration 97/1000 | Loss: 0.00027063
Iteration 98/1000 | Loss: 0.00027063
Iteration 99/1000 | Loss: 0.00027063
Iteration 100/1000 | Loss: 0.00027063
Iteration 101/1000 | Loss: 0.00027063
Iteration 102/1000 | Loss: 0.00027063
Iteration 103/1000 | Loss: 0.00027062
Iteration 104/1000 | Loss: 0.00027062
Iteration 105/1000 | Loss: 0.00027062
Iteration 106/1000 | Loss: 0.00027062
Iteration 107/1000 | Loss: 0.00027062
Iteration 108/1000 | Loss: 0.00027062
Iteration 109/1000 | Loss: 0.00027061
Iteration 110/1000 | Loss: 0.00027060
Iteration 111/1000 | Loss: 0.00027060
Iteration 112/1000 | Loss: 0.00027056
Iteration 113/1000 | Loss: 0.00027055
Iteration 114/1000 | Loss: 0.00027054
Iteration 115/1000 | Loss: 0.00027053
Iteration 116/1000 | Loss: 0.00027053
Iteration 117/1000 | Loss: 0.00027053
Iteration 118/1000 | Loss: 0.00027053
Iteration 119/1000 | Loss: 0.00027053
Iteration 120/1000 | Loss: 0.00027053
Iteration 121/1000 | Loss: 0.00027052
Iteration 122/1000 | Loss: 0.00027052
Iteration 123/1000 | Loss: 0.00027052
Iteration 124/1000 | Loss: 0.00027052
Iteration 125/1000 | Loss: 0.00027052
Iteration 126/1000 | Loss: 0.00027052
Iteration 127/1000 | Loss: 0.00027052
Iteration 128/1000 | Loss: 0.00027052
Iteration 129/1000 | Loss: 0.00027051
Iteration 130/1000 | Loss: 0.00027051
Iteration 131/1000 | Loss: 0.00027051
Iteration 132/1000 | Loss: 0.00027051
Iteration 133/1000 | Loss: 0.00027051
Iteration 134/1000 | Loss: 0.00027051
Iteration 135/1000 | Loss: 0.00027051
Iteration 136/1000 | Loss: 0.00027051
Iteration 137/1000 | Loss: 0.00027051
Iteration 138/1000 | Loss: 0.00027051
Iteration 139/1000 | Loss: 0.00027050
Iteration 140/1000 | Loss: 0.00027050
Iteration 141/1000 | Loss: 0.00027050
Iteration 142/1000 | Loss: 0.00027050
Iteration 143/1000 | Loss: 0.00027050
Iteration 144/1000 | Loss: 0.00027050
Iteration 145/1000 | Loss: 0.00027049
Iteration 146/1000 | Loss: 0.00027048
Iteration 147/1000 | Loss: 0.00027047
Iteration 148/1000 | Loss: 0.00027046
Iteration 149/1000 | Loss: 0.00027044
Iteration 150/1000 | Loss: 0.00027000
Iteration 151/1000 | Loss: 0.00026967
Iteration 152/1000 | Loss: 0.00026950
Iteration 153/1000 | Loss: 0.00026948
Iteration 154/1000 | Loss: 0.00026936
Iteration 155/1000 | Loss: 0.00026931
Iteration 156/1000 | Loss: 0.00026929
Iteration 157/1000 | Loss: 0.00026925
Iteration 158/1000 | Loss: 0.00026924
Iteration 159/1000 | Loss: 0.00026923
Iteration 160/1000 | Loss: 0.00026922
Iteration 161/1000 | Loss: 0.00026922
Iteration 162/1000 | Loss: 0.00026920
Iteration 163/1000 | Loss: 0.00026920
Iteration 164/1000 | Loss: 0.00026920
Iteration 165/1000 | Loss: 0.00026920
Iteration 166/1000 | Loss: 0.00026920
Iteration 167/1000 | Loss: 0.00026920
Iteration 168/1000 | Loss: 0.00026920
Iteration 169/1000 | Loss: 0.00026920
Iteration 170/1000 | Loss: 0.00026920
Iteration 171/1000 | Loss: 0.00026920
Iteration 172/1000 | Loss: 0.00026920
Iteration 173/1000 | Loss: 0.00026920
Iteration 174/1000 | Loss: 0.00026920
Iteration 175/1000 | Loss: 0.00026919
Iteration 176/1000 | Loss: 0.00026918
Iteration 177/1000 | Loss: 0.00026918
Iteration 178/1000 | Loss: 0.00026918
Iteration 179/1000 | Loss: 0.00026918
Iteration 180/1000 | Loss: 0.00026918
Iteration 181/1000 | Loss: 0.00026918
Iteration 182/1000 | Loss: 0.00026918
Iteration 183/1000 | Loss: 0.00026918
Iteration 184/1000 | Loss: 0.00026918
Iteration 185/1000 | Loss: 0.00026918
Iteration 186/1000 | Loss: 0.00026917
Iteration 187/1000 | Loss: 0.00026917
Iteration 188/1000 | Loss: 0.00026917
Iteration 189/1000 | Loss: 0.00026917
Iteration 190/1000 | Loss: 0.00026917
Iteration 191/1000 | Loss: 0.00026917
Iteration 192/1000 | Loss: 0.00026917
Iteration 193/1000 | Loss: 0.00026917
Iteration 194/1000 | Loss: 0.00026917
Iteration 195/1000 | Loss: 0.00026917
Iteration 196/1000 | Loss: 0.00026917
Iteration 197/1000 | Loss: 0.00026917
Iteration 198/1000 | Loss: 0.00026916
Iteration 199/1000 | Loss: 0.00026916
Iteration 200/1000 | Loss: 0.00026916
Iteration 201/1000 | Loss: 0.00026916
Iteration 202/1000 | Loss: 0.00026915
Iteration 203/1000 | Loss: 0.00026915
Iteration 204/1000 | Loss: 0.00026915
Iteration 205/1000 | Loss: 0.00026915
Iteration 206/1000 | Loss: 0.00026915
Iteration 207/1000 | Loss: 0.00026915
Iteration 208/1000 | Loss: 0.00026915
Iteration 209/1000 | Loss: 0.00026915
Iteration 210/1000 | Loss: 0.00026915
Iteration 211/1000 | Loss: 0.00026915
Iteration 212/1000 | Loss: 0.00026915
Iteration 213/1000 | Loss: 0.00026915
Iteration 214/1000 | Loss: 0.00026915
Iteration 215/1000 | Loss: 0.00026914
Iteration 216/1000 | Loss: 0.00026914
Iteration 217/1000 | Loss: 0.00026914
Iteration 218/1000 | Loss: 0.00026914
Iteration 219/1000 | Loss: 0.00026914
Iteration 220/1000 | Loss: 0.00026914
Iteration 221/1000 | Loss: 0.00026913
Iteration 222/1000 | Loss: 0.00026913
Iteration 223/1000 | Loss: 0.00026913
Iteration 224/1000 | Loss: 0.00026913
Iteration 225/1000 | Loss: 0.00026913
Iteration 226/1000 | Loss: 0.00026913
Iteration 227/1000 | Loss: 0.00026913
Iteration 228/1000 | Loss: 0.00026912
Iteration 229/1000 | Loss: 0.00026912
Iteration 230/1000 | Loss: 0.00026912
Iteration 231/1000 | Loss: 0.00026912
Iteration 232/1000 | Loss: 0.00026911
Iteration 233/1000 | Loss: 0.00026911
Iteration 234/1000 | Loss: 0.00026910
Iteration 235/1000 | Loss: 0.00026910
Iteration 236/1000 | Loss: 0.00026910
Iteration 237/1000 | Loss: 0.00026910
Iteration 238/1000 | Loss: 0.00026909
Iteration 239/1000 | Loss: 0.00026909
Iteration 240/1000 | Loss: 0.00026909
Iteration 241/1000 | Loss: 0.00026909
Iteration 242/1000 | Loss: 0.00026909
Iteration 243/1000 | Loss: 0.00026909
Iteration 244/1000 | Loss: 0.00026908
Iteration 245/1000 | Loss: 0.00026908
Iteration 246/1000 | Loss: 0.00026908
Iteration 247/1000 | Loss: 0.00026908
Iteration 248/1000 | Loss: 0.00026908
Iteration 249/1000 | Loss: 0.00026908
Iteration 250/1000 | Loss: 0.00026908
Iteration 251/1000 | Loss: 0.00026908
Iteration 252/1000 | Loss: 0.00026908
Iteration 253/1000 | Loss: 0.00026907
Iteration 254/1000 | Loss: 0.00026907
Iteration 255/1000 | Loss: 0.00026907
Iteration 256/1000 | Loss: 0.00026907
Iteration 257/1000 | Loss: 0.00026907
Iteration 258/1000 | Loss: 0.00026907
Iteration 259/1000 | Loss: 0.00026907
Iteration 260/1000 | Loss: 0.00026907
Iteration 261/1000 | Loss: 0.00026907
Iteration 262/1000 | Loss: 0.00026907
Iteration 263/1000 | Loss: 0.00026907
Iteration 264/1000 | Loss: 0.00026907
Iteration 265/1000 | Loss: 0.00026907
Iteration 266/1000 | Loss: 0.00026907
Iteration 267/1000 | Loss: 0.00026907
Iteration 268/1000 | Loss: 0.00026907
Iteration 269/1000 | Loss: 0.00026907
Iteration 270/1000 | Loss: 0.00026907
Iteration 271/1000 | Loss: 0.00026907
Iteration 272/1000 | Loss: 0.00026907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [0.00026906930725090206, 0.00026906930725090206, 0.00026906930725090206, 0.00026906930725090206, 0.00026906930725090206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026906930725090206

Optimization complete. Final v2v error: 8.931647300720215 mm

Highest mean error: 12.927400588989258 mm for frame 97

Lowest mean error: 4.994134426116943 mm for frame 55

Saving results

Total time: 160.12265801429749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022533
Iteration 2/25 | Loss: 0.01022533
Iteration 3/25 | Loss: 0.00239038
Iteration 4/25 | Loss: 0.00144988
Iteration 5/25 | Loss: 0.00128185
Iteration 6/25 | Loss: 0.00123418
Iteration 7/25 | Loss: 0.00122567
Iteration 8/25 | Loss: 0.00121061
Iteration 9/25 | Loss: 0.00117424
Iteration 10/25 | Loss: 0.00117054
Iteration 11/25 | Loss: 0.00115656
Iteration 12/25 | Loss: 0.00114962
Iteration 13/25 | Loss: 0.00114300
Iteration 14/25 | Loss: 0.00114691
Iteration 15/25 | Loss: 0.00114830
Iteration 16/25 | Loss: 0.00113883
Iteration 17/25 | Loss: 0.00113443
Iteration 18/25 | Loss: 0.00113601
Iteration 19/25 | Loss: 0.00114326
Iteration 20/25 | Loss: 0.00113872
Iteration 21/25 | Loss: 0.00114274
Iteration 22/25 | Loss: 0.00114030
Iteration 23/25 | Loss: 0.00113775
Iteration 24/25 | Loss: 0.00113757
Iteration 25/25 | Loss: 0.00113782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26056683
Iteration 2/25 | Loss: 0.00137269
Iteration 3/25 | Loss: 0.00137269
Iteration 4/25 | Loss: 0.00137269
Iteration 5/25 | Loss: 0.00137269
Iteration 6/25 | Loss: 0.00137269
Iteration 7/25 | Loss: 0.00137269
Iteration 8/25 | Loss: 0.00137269
Iteration 9/25 | Loss: 0.00137269
Iteration 10/25 | Loss: 0.00137269
Iteration 11/25 | Loss: 0.00137269
Iteration 12/25 | Loss: 0.00137269
Iteration 13/25 | Loss: 0.00137269
Iteration 14/25 | Loss: 0.00137269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001372691011056304, 0.001372691011056304, 0.001372691011056304, 0.001372691011056304, 0.001372691011056304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001372691011056304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137269
Iteration 2/1000 | Loss: 0.00046059
Iteration 3/1000 | Loss: 0.00034817
Iteration 4/1000 | Loss: 0.00026531
Iteration 5/1000 | Loss: 0.00024211
Iteration 6/1000 | Loss: 0.00024222
Iteration 7/1000 | Loss: 0.00006664
Iteration 8/1000 | Loss: 0.00005146
Iteration 9/1000 | Loss: 0.00007608
Iteration 10/1000 | Loss: 0.00013808
Iteration 11/1000 | Loss: 0.00017414
Iteration 12/1000 | Loss: 0.00018138
Iteration 13/1000 | Loss: 0.00018192
Iteration 14/1000 | Loss: 0.00018287
Iteration 15/1000 | Loss: 0.00030459
Iteration 16/1000 | Loss: 0.00023647
Iteration 17/1000 | Loss: 0.00060735
Iteration 18/1000 | Loss: 0.00016186
Iteration 19/1000 | Loss: 0.00018372
Iteration 20/1000 | Loss: 0.00022030
Iteration 21/1000 | Loss: 0.00015762
Iteration 22/1000 | Loss: 0.00026511
Iteration 23/1000 | Loss: 0.00018009
Iteration 24/1000 | Loss: 0.00021485
Iteration 25/1000 | Loss: 0.00032422
Iteration 26/1000 | Loss: 0.00012949
Iteration 27/1000 | Loss: 0.00011008
Iteration 28/1000 | Loss: 0.00005013
Iteration 29/1000 | Loss: 0.00015560
Iteration 30/1000 | Loss: 0.00024601
Iteration 31/1000 | Loss: 0.00017479
Iteration 32/1000 | Loss: 0.00017583
Iteration 33/1000 | Loss: 0.00016716
Iteration 34/1000 | Loss: 0.00018780
Iteration 35/1000 | Loss: 0.00043075
Iteration 36/1000 | Loss: 0.00018892
Iteration 37/1000 | Loss: 0.00037010
Iteration 38/1000 | Loss: 0.00028827
Iteration 39/1000 | Loss: 0.00012365
Iteration 40/1000 | Loss: 0.00028787
Iteration 41/1000 | Loss: 0.00024329
Iteration 42/1000 | Loss: 0.00007180
Iteration 43/1000 | Loss: 0.00013509
Iteration 44/1000 | Loss: 0.00013492
Iteration 45/1000 | Loss: 0.00003488
Iteration 46/1000 | Loss: 0.00011970
Iteration 47/1000 | Loss: 0.00004288
Iteration 48/1000 | Loss: 0.00003658
Iteration 49/1000 | Loss: 0.00003355
Iteration 50/1000 | Loss: 0.00003142
Iteration 51/1000 | Loss: 0.00022412
Iteration 52/1000 | Loss: 0.00012919
Iteration 53/1000 | Loss: 0.00039689
Iteration 54/1000 | Loss: 0.00028341
Iteration 55/1000 | Loss: 0.00045899
Iteration 56/1000 | Loss: 0.00016686
Iteration 57/1000 | Loss: 0.00006106
Iteration 58/1000 | Loss: 0.00004933
Iteration 59/1000 | Loss: 0.00003028
Iteration 60/1000 | Loss: 0.00005066
Iteration 61/1000 | Loss: 0.00004250
Iteration 62/1000 | Loss: 0.00004412
Iteration 63/1000 | Loss: 0.00003858
Iteration 64/1000 | Loss: 0.00004123
Iteration 65/1000 | Loss: 0.00003622
Iteration 66/1000 | Loss: 0.00002751
Iteration 67/1000 | Loss: 0.00003832
Iteration 68/1000 | Loss: 0.00002845
Iteration 69/1000 | Loss: 0.00002708
Iteration 70/1000 | Loss: 0.00002627
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00004292
Iteration 73/1000 | Loss: 0.00003197
Iteration 74/1000 | Loss: 0.00003804
Iteration 75/1000 | Loss: 0.00003932
Iteration 76/1000 | Loss: 0.00003342
Iteration 77/1000 | Loss: 0.00003416
Iteration 78/1000 | Loss: 0.00004382
Iteration 79/1000 | Loss: 0.00003355
Iteration 80/1000 | Loss: 0.00004391
Iteration 81/1000 | Loss: 0.00002884
Iteration 82/1000 | Loss: 0.00003013
Iteration 83/1000 | Loss: 0.00003235
Iteration 84/1000 | Loss: 0.00003154
Iteration 85/1000 | Loss: 0.00004458
Iteration 86/1000 | Loss: 0.00003863
Iteration 87/1000 | Loss: 0.00002847
Iteration 88/1000 | Loss: 0.00002658
Iteration 89/1000 | Loss: 0.00004557
Iteration 90/1000 | Loss: 0.00002602
Iteration 91/1000 | Loss: 0.00002534
Iteration 92/1000 | Loss: 0.00002587
Iteration 93/1000 | Loss: 0.00003532
Iteration 94/1000 | Loss: 0.00004066
Iteration 95/1000 | Loss: 0.00003312
Iteration 96/1000 | Loss: 0.00004141
Iteration 97/1000 | Loss: 0.00003793
Iteration 98/1000 | Loss: 0.00002609
Iteration 99/1000 | Loss: 0.00003024
Iteration 100/1000 | Loss: 0.00004079
Iteration 101/1000 | Loss: 0.00003353
Iteration 102/1000 | Loss: 0.00002649
Iteration 103/1000 | Loss: 0.00002569
Iteration 104/1000 | Loss: 0.00003939
Iteration 105/1000 | Loss: 0.00003288
Iteration 106/1000 | Loss: 0.00003939
Iteration 107/1000 | Loss: 0.00002620
Iteration 108/1000 | Loss: 0.00002769
Iteration 109/1000 | Loss: 0.00003271
Iteration 110/1000 | Loss: 0.00003786
Iteration 111/1000 | Loss: 0.00003827
Iteration 112/1000 | Loss: 0.00003721
Iteration 113/1000 | Loss: 0.00004222
Iteration 114/1000 | Loss: 0.00008559
Iteration 115/1000 | Loss: 0.00009013
Iteration 116/1000 | Loss: 0.00006395
Iteration 117/1000 | Loss: 0.00011955
Iteration 118/1000 | Loss: 0.00010316
Iteration 119/1000 | Loss: 0.00004932
Iteration 120/1000 | Loss: 0.00014337
Iteration 121/1000 | Loss: 0.00002962
Iteration 122/1000 | Loss: 0.00002677
Iteration 123/1000 | Loss: 0.00002579
Iteration 124/1000 | Loss: 0.00002508
Iteration 125/1000 | Loss: 0.00002476
Iteration 126/1000 | Loss: 0.00002443
Iteration 127/1000 | Loss: 0.00002416
Iteration 128/1000 | Loss: 0.00002415
Iteration 129/1000 | Loss: 0.00002397
Iteration 130/1000 | Loss: 0.00002392
Iteration 131/1000 | Loss: 0.00002384
Iteration 132/1000 | Loss: 0.00002383
Iteration 133/1000 | Loss: 0.00002383
Iteration 134/1000 | Loss: 0.00002383
Iteration 135/1000 | Loss: 0.00002382
Iteration 136/1000 | Loss: 0.00002382
Iteration 137/1000 | Loss: 0.00002382
Iteration 138/1000 | Loss: 0.00002381
Iteration 139/1000 | Loss: 0.00002381
Iteration 140/1000 | Loss: 0.00002381
Iteration 141/1000 | Loss: 0.00002381
Iteration 142/1000 | Loss: 0.00002380
Iteration 143/1000 | Loss: 0.00002380
Iteration 144/1000 | Loss: 0.00002380
Iteration 145/1000 | Loss: 0.00002380
Iteration 146/1000 | Loss: 0.00002379
Iteration 147/1000 | Loss: 0.00002379
Iteration 148/1000 | Loss: 0.00002378
Iteration 149/1000 | Loss: 0.00002377
Iteration 150/1000 | Loss: 0.00002377
Iteration 151/1000 | Loss: 0.00002376
Iteration 152/1000 | Loss: 0.00002374
Iteration 153/1000 | Loss: 0.00002367
Iteration 154/1000 | Loss: 0.00002367
Iteration 155/1000 | Loss: 0.00002367
Iteration 156/1000 | Loss: 0.00002367
Iteration 157/1000 | Loss: 0.00002367
Iteration 158/1000 | Loss: 0.00002367
Iteration 159/1000 | Loss: 0.00002366
Iteration 160/1000 | Loss: 0.00002366
Iteration 161/1000 | Loss: 0.00002366
Iteration 162/1000 | Loss: 0.00002364
Iteration 163/1000 | Loss: 0.00002364
Iteration 164/1000 | Loss: 0.00002363
Iteration 165/1000 | Loss: 0.00002360
Iteration 166/1000 | Loss: 0.00002360
Iteration 167/1000 | Loss: 0.00002358
Iteration 168/1000 | Loss: 0.00002358
Iteration 169/1000 | Loss: 0.00002358
Iteration 170/1000 | Loss: 0.00002358
Iteration 171/1000 | Loss: 0.00002358
Iteration 172/1000 | Loss: 0.00002358
Iteration 173/1000 | Loss: 0.00002358
Iteration 174/1000 | Loss: 0.00002358
Iteration 175/1000 | Loss: 0.00002358
Iteration 176/1000 | Loss: 0.00002358
Iteration 177/1000 | Loss: 0.00002357
Iteration 178/1000 | Loss: 0.00002357
Iteration 179/1000 | Loss: 0.00002356
Iteration 180/1000 | Loss: 0.00002356
Iteration 181/1000 | Loss: 0.00002356
Iteration 182/1000 | Loss: 0.00002355
Iteration 183/1000 | Loss: 0.00002355
Iteration 184/1000 | Loss: 0.00002355
Iteration 185/1000 | Loss: 0.00002355
Iteration 186/1000 | Loss: 0.00002355
Iteration 187/1000 | Loss: 0.00002355
Iteration 188/1000 | Loss: 0.00002354
Iteration 189/1000 | Loss: 0.00002354
Iteration 190/1000 | Loss: 0.00002354
Iteration 191/1000 | Loss: 0.00002354
Iteration 192/1000 | Loss: 0.00002354
Iteration 193/1000 | Loss: 0.00002354
Iteration 194/1000 | Loss: 0.00002354
Iteration 195/1000 | Loss: 0.00002353
Iteration 196/1000 | Loss: 0.00002353
Iteration 197/1000 | Loss: 0.00002352
Iteration 198/1000 | Loss: 0.00002352
Iteration 199/1000 | Loss: 0.00002351
Iteration 200/1000 | Loss: 0.00002351
Iteration 201/1000 | Loss: 0.00002351
Iteration 202/1000 | Loss: 0.00002351
Iteration 203/1000 | Loss: 0.00002351
Iteration 204/1000 | Loss: 0.00002350
Iteration 205/1000 | Loss: 0.00002350
Iteration 206/1000 | Loss: 0.00002350
Iteration 207/1000 | Loss: 0.00002350
Iteration 208/1000 | Loss: 0.00002350
Iteration 209/1000 | Loss: 0.00002349
Iteration 210/1000 | Loss: 0.00002349
Iteration 211/1000 | Loss: 0.00002348
Iteration 212/1000 | Loss: 0.00002348
Iteration 213/1000 | Loss: 0.00002348
Iteration 214/1000 | Loss: 0.00002348
Iteration 215/1000 | Loss: 0.00002348
Iteration 216/1000 | Loss: 0.00002348
Iteration 217/1000 | Loss: 0.00002347
Iteration 218/1000 | Loss: 0.00002347
Iteration 219/1000 | Loss: 0.00002347
Iteration 220/1000 | Loss: 0.00002347
Iteration 221/1000 | Loss: 0.00002347
Iteration 222/1000 | Loss: 0.00002347
Iteration 223/1000 | Loss: 0.00002346
Iteration 224/1000 | Loss: 0.00002346
Iteration 225/1000 | Loss: 0.00002346
Iteration 226/1000 | Loss: 0.00002346
Iteration 227/1000 | Loss: 0.00002345
Iteration 228/1000 | Loss: 0.00002345
Iteration 229/1000 | Loss: 0.00002345
Iteration 230/1000 | Loss: 0.00002344
Iteration 231/1000 | Loss: 0.00002344
Iteration 232/1000 | Loss: 0.00002344
Iteration 233/1000 | Loss: 0.00002343
Iteration 234/1000 | Loss: 0.00002342
Iteration 235/1000 | Loss: 0.00002342
Iteration 236/1000 | Loss: 0.00002342
Iteration 237/1000 | Loss: 0.00002342
Iteration 238/1000 | Loss: 0.00002342
Iteration 239/1000 | Loss: 0.00002342
Iteration 240/1000 | Loss: 0.00002342
Iteration 241/1000 | Loss: 0.00002342
Iteration 242/1000 | Loss: 0.00002341
Iteration 243/1000 | Loss: 0.00002341
Iteration 244/1000 | Loss: 0.00002341
Iteration 245/1000 | Loss: 0.00002341
Iteration 246/1000 | Loss: 0.00002341
Iteration 247/1000 | Loss: 0.00002341
Iteration 248/1000 | Loss: 0.00002341
Iteration 249/1000 | Loss: 0.00002341
Iteration 250/1000 | Loss: 0.00002340
Iteration 251/1000 | Loss: 0.00002340
Iteration 252/1000 | Loss: 0.00002340
Iteration 253/1000 | Loss: 0.00002340
Iteration 254/1000 | Loss: 0.00002339
Iteration 255/1000 | Loss: 0.00002339
Iteration 256/1000 | Loss: 0.00002339
Iteration 257/1000 | Loss: 0.00002339
Iteration 258/1000 | Loss: 0.00002339
Iteration 259/1000 | Loss: 0.00002339
Iteration 260/1000 | Loss: 0.00002339
Iteration 261/1000 | Loss: 0.00002339
Iteration 262/1000 | Loss: 0.00002338
Iteration 263/1000 | Loss: 0.00002338
Iteration 264/1000 | Loss: 0.00002338
Iteration 265/1000 | Loss: 0.00002338
Iteration 266/1000 | Loss: 0.00002338
Iteration 267/1000 | Loss: 0.00002338
Iteration 268/1000 | Loss: 0.00002338
Iteration 269/1000 | Loss: 0.00002338
Iteration 270/1000 | Loss: 0.00002338
Iteration 271/1000 | Loss: 0.00002338
Iteration 272/1000 | Loss: 0.00002338
Iteration 273/1000 | Loss: 0.00002338
Iteration 274/1000 | Loss: 0.00002338
Iteration 275/1000 | Loss: 0.00002338
Iteration 276/1000 | Loss: 0.00002338
Iteration 277/1000 | Loss: 0.00002338
Iteration 278/1000 | Loss: 0.00002338
Iteration 279/1000 | Loss: 0.00002338
Iteration 280/1000 | Loss: 0.00002338
Iteration 281/1000 | Loss: 0.00002338
Iteration 282/1000 | Loss: 0.00002338
Iteration 283/1000 | Loss: 0.00002338
Iteration 284/1000 | Loss: 0.00002338
Iteration 285/1000 | Loss: 0.00002338
Iteration 286/1000 | Loss: 0.00002338
Iteration 287/1000 | Loss: 0.00002338
Iteration 288/1000 | Loss: 0.00002338
Iteration 289/1000 | Loss: 0.00002338
Iteration 290/1000 | Loss: 0.00002338
Iteration 291/1000 | Loss: 0.00002338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [2.337650948902592e-05, 2.337650948902592e-05, 2.337650948902592e-05, 2.337650948902592e-05, 2.337650948902592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.337650948902592e-05

Optimization complete. Final v2v error: 4.031431198120117 mm

Highest mean error: 5.340578079223633 mm for frame 63

Lowest mean error: 3.5404419898986816 mm for frame 205

Saving results

Total time: 266.4881374835968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846382
Iteration 2/25 | Loss: 0.00132282
Iteration 3/25 | Loss: 0.00101362
Iteration 4/25 | Loss: 0.00097305
Iteration 5/25 | Loss: 0.00096610
Iteration 6/25 | Loss: 0.00096460
Iteration 7/25 | Loss: 0.00096460
Iteration 8/25 | Loss: 0.00096460
Iteration 9/25 | Loss: 0.00096460
Iteration 10/25 | Loss: 0.00096460
Iteration 11/25 | Loss: 0.00096460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009646001271903515, 0.0009646001271903515, 0.0009646001271903515, 0.0009646001271903515, 0.0009646001271903515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009646001271903515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26796651
Iteration 2/25 | Loss: 0.00116033
Iteration 3/25 | Loss: 0.00116031
Iteration 4/25 | Loss: 0.00116031
Iteration 5/25 | Loss: 0.00116031
Iteration 6/25 | Loss: 0.00116031
Iteration 7/25 | Loss: 0.00116031
Iteration 8/25 | Loss: 0.00116031
Iteration 9/25 | Loss: 0.00116031
Iteration 10/25 | Loss: 0.00116031
Iteration 11/25 | Loss: 0.00116031
Iteration 12/25 | Loss: 0.00116031
Iteration 13/25 | Loss: 0.00116031
Iteration 14/25 | Loss: 0.00116031
Iteration 15/25 | Loss: 0.00116031
Iteration 16/25 | Loss: 0.00116031
Iteration 17/25 | Loss: 0.00116031
Iteration 18/25 | Loss: 0.00116031
Iteration 19/25 | Loss: 0.00116031
Iteration 20/25 | Loss: 0.00116031
Iteration 21/25 | Loss: 0.00116031
Iteration 22/25 | Loss: 0.00116031
Iteration 23/25 | Loss: 0.00116031
Iteration 24/25 | Loss: 0.00116031
Iteration 25/25 | Loss: 0.00116031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116031
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00001767
Iteration 4/1000 | Loss: 0.00001581
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001284
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00001182
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001178
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001171
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001158
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001146
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001144
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001143
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001141
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001140
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001139
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001139
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001138
Iteration 76/1000 | Loss: 0.00001138
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001131
Iteration 114/1000 | Loss: 0.00001131
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.1304859071969986e-05, 1.1304859071969986e-05, 1.1304859071969986e-05, 1.1304859071969986e-05, 1.1304859071969986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1304859071969986e-05

Optimization complete. Final v2v error: 2.898475170135498 mm

Highest mean error: 3.4613072872161865 mm for frame 134

Lowest mean error: 2.3513023853302 mm for frame 36

Saving results

Total time: 41.39504337310791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00723258
Iteration 2/25 | Loss: 0.00123490
Iteration 3/25 | Loss: 0.00106700
Iteration 4/25 | Loss: 0.00105025
Iteration 5/25 | Loss: 0.00104839
Iteration 6/25 | Loss: 0.00104839
Iteration 7/25 | Loss: 0.00104839
Iteration 8/25 | Loss: 0.00104839
Iteration 9/25 | Loss: 0.00104839
Iteration 10/25 | Loss: 0.00104839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010483920341357589, 0.0010483920341357589, 0.0010483920341357589, 0.0010483920341357589, 0.0010483920341357589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010483920341357589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.03426170
Iteration 2/25 | Loss: 0.00119196
Iteration 3/25 | Loss: 0.00119194
Iteration 4/25 | Loss: 0.00119194
Iteration 5/25 | Loss: 0.00119194
Iteration 6/25 | Loss: 0.00119194
Iteration 7/25 | Loss: 0.00119194
Iteration 8/25 | Loss: 0.00119194
Iteration 9/25 | Loss: 0.00119194
Iteration 10/25 | Loss: 0.00119194
Iteration 11/25 | Loss: 0.00119194
Iteration 12/25 | Loss: 0.00119194
Iteration 13/25 | Loss: 0.00119194
Iteration 14/25 | Loss: 0.00119194
Iteration 15/25 | Loss: 0.00119194
Iteration 16/25 | Loss: 0.00119194
Iteration 17/25 | Loss: 0.00119194
Iteration 18/25 | Loss: 0.00119194
Iteration 19/25 | Loss: 0.00119194
Iteration 20/25 | Loss: 0.00119194
Iteration 21/25 | Loss: 0.00119194
Iteration 22/25 | Loss: 0.00119194
Iteration 23/25 | Loss: 0.00119194
Iteration 24/25 | Loss: 0.00119194
Iteration 25/25 | Loss: 0.00119194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119194
Iteration 2/1000 | Loss: 0.00003868
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00001969
Iteration 6/1000 | Loss: 0.00001892
Iteration 7/1000 | Loss: 0.00001830
Iteration 8/1000 | Loss: 0.00001790
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001727
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001670
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001665
Iteration 23/1000 | Loss: 0.00001665
Iteration 24/1000 | Loss: 0.00001665
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001665
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001664
Iteration 34/1000 | Loss: 0.00001664
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001660
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001660
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001660
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001657
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001657
Iteration 57/1000 | Loss: 0.00001657
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001656
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001655
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001653
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001652
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001651
Iteration 85/1000 | Loss: 0.00001651
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001650
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001646
Iteration 94/1000 | Loss: 0.00001646
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001643
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001643
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001643
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001642
Iteration 105/1000 | Loss: 0.00001642
Iteration 106/1000 | Loss: 0.00001641
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001638
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001637
Iteration 119/1000 | Loss: 0.00001637
Iteration 120/1000 | Loss: 0.00001637
Iteration 121/1000 | Loss: 0.00001637
Iteration 122/1000 | Loss: 0.00001637
Iteration 123/1000 | Loss: 0.00001636
Iteration 124/1000 | Loss: 0.00001636
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001636
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001635
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001634
Iteration 136/1000 | Loss: 0.00001634
Iteration 137/1000 | Loss: 0.00001634
Iteration 138/1000 | Loss: 0.00001634
Iteration 139/1000 | Loss: 0.00001633
Iteration 140/1000 | Loss: 0.00001633
Iteration 141/1000 | Loss: 0.00001633
Iteration 142/1000 | Loss: 0.00001633
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001633
Iteration 145/1000 | Loss: 0.00001633
Iteration 146/1000 | Loss: 0.00001633
Iteration 147/1000 | Loss: 0.00001633
Iteration 148/1000 | Loss: 0.00001633
Iteration 149/1000 | Loss: 0.00001632
Iteration 150/1000 | Loss: 0.00001632
Iteration 151/1000 | Loss: 0.00001632
Iteration 152/1000 | Loss: 0.00001632
Iteration 153/1000 | Loss: 0.00001632
Iteration 154/1000 | Loss: 0.00001632
Iteration 155/1000 | Loss: 0.00001632
Iteration 156/1000 | Loss: 0.00001632
Iteration 157/1000 | Loss: 0.00001632
Iteration 158/1000 | Loss: 0.00001631
Iteration 159/1000 | Loss: 0.00001631
Iteration 160/1000 | Loss: 0.00001630
Iteration 161/1000 | Loss: 0.00001630
Iteration 162/1000 | Loss: 0.00001630
Iteration 163/1000 | Loss: 0.00001630
Iteration 164/1000 | Loss: 0.00001630
Iteration 165/1000 | Loss: 0.00001630
Iteration 166/1000 | Loss: 0.00001630
Iteration 167/1000 | Loss: 0.00001630
Iteration 168/1000 | Loss: 0.00001630
Iteration 169/1000 | Loss: 0.00001630
Iteration 170/1000 | Loss: 0.00001630
Iteration 171/1000 | Loss: 0.00001630
Iteration 172/1000 | Loss: 0.00001630
Iteration 173/1000 | Loss: 0.00001629
Iteration 174/1000 | Loss: 0.00001629
Iteration 175/1000 | Loss: 0.00001629
Iteration 176/1000 | Loss: 0.00001629
Iteration 177/1000 | Loss: 0.00001629
Iteration 178/1000 | Loss: 0.00001629
Iteration 179/1000 | Loss: 0.00001629
Iteration 180/1000 | Loss: 0.00001629
Iteration 181/1000 | Loss: 0.00001629
Iteration 182/1000 | Loss: 0.00001629
Iteration 183/1000 | Loss: 0.00001629
Iteration 184/1000 | Loss: 0.00001629
Iteration 185/1000 | Loss: 0.00001629
Iteration 186/1000 | Loss: 0.00001629
Iteration 187/1000 | Loss: 0.00001628
Iteration 188/1000 | Loss: 0.00001628
Iteration 189/1000 | Loss: 0.00001628
Iteration 190/1000 | Loss: 0.00001628
Iteration 191/1000 | Loss: 0.00001628
Iteration 192/1000 | Loss: 0.00001627
Iteration 193/1000 | Loss: 0.00001627
Iteration 194/1000 | Loss: 0.00001627
Iteration 195/1000 | Loss: 0.00001627
Iteration 196/1000 | Loss: 0.00001627
Iteration 197/1000 | Loss: 0.00001627
Iteration 198/1000 | Loss: 0.00001627
Iteration 199/1000 | Loss: 0.00001627
Iteration 200/1000 | Loss: 0.00001627
Iteration 201/1000 | Loss: 0.00001627
Iteration 202/1000 | Loss: 0.00001627
Iteration 203/1000 | Loss: 0.00001627
Iteration 204/1000 | Loss: 0.00001626
Iteration 205/1000 | Loss: 0.00001626
Iteration 206/1000 | Loss: 0.00001626
Iteration 207/1000 | Loss: 0.00001626
Iteration 208/1000 | Loss: 0.00001626
Iteration 209/1000 | Loss: 0.00001626
Iteration 210/1000 | Loss: 0.00001626
Iteration 211/1000 | Loss: 0.00001626
Iteration 212/1000 | Loss: 0.00001625
Iteration 213/1000 | Loss: 0.00001625
Iteration 214/1000 | Loss: 0.00001625
Iteration 215/1000 | Loss: 0.00001625
Iteration 216/1000 | Loss: 0.00001625
Iteration 217/1000 | Loss: 0.00001625
Iteration 218/1000 | Loss: 0.00001625
Iteration 219/1000 | Loss: 0.00001625
Iteration 220/1000 | Loss: 0.00001625
Iteration 221/1000 | Loss: 0.00001625
Iteration 222/1000 | Loss: 0.00001625
Iteration 223/1000 | Loss: 0.00001625
Iteration 224/1000 | Loss: 0.00001625
Iteration 225/1000 | Loss: 0.00001625
Iteration 226/1000 | Loss: 0.00001625
Iteration 227/1000 | Loss: 0.00001625
Iteration 228/1000 | Loss: 0.00001625
Iteration 229/1000 | Loss: 0.00001625
Iteration 230/1000 | Loss: 0.00001625
Iteration 231/1000 | Loss: 0.00001625
Iteration 232/1000 | Loss: 0.00001625
Iteration 233/1000 | Loss: 0.00001625
Iteration 234/1000 | Loss: 0.00001625
Iteration 235/1000 | Loss: 0.00001625
Iteration 236/1000 | Loss: 0.00001625
Iteration 237/1000 | Loss: 0.00001625
Iteration 238/1000 | Loss: 0.00001625
Iteration 239/1000 | Loss: 0.00001625
Iteration 240/1000 | Loss: 0.00001625
Iteration 241/1000 | Loss: 0.00001625
Iteration 242/1000 | Loss: 0.00001625
Iteration 243/1000 | Loss: 0.00001625
Iteration 244/1000 | Loss: 0.00001625
Iteration 245/1000 | Loss: 0.00001625
Iteration 246/1000 | Loss: 0.00001625
Iteration 247/1000 | Loss: 0.00001625
Iteration 248/1000 | Loss: 0.00001625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.6246129234787077e-05, 1.6246129234787077e-05, 1.6246129234787077e-05, 1.6246129234787077e-05, 1.6246129234787077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6246129234787077e-05

Optimization complete. Final v2v error: 3.3245203495025635 mm

Highest mean error: 3.698216199874878 mm for frame 6

Lowest mean error: 2.7334647178649902 mm for frame 239

Saving results

Total time: 47.53460955619812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850882
Iteration 2/25 | Loss: 0.00143934
Iteration 3/25 | Loss: 0.00108744
Iteration 4/25 | Loss: 0.00103230
Iteration 5/25 | Loss: 0.00102196
Iteration 6/25 | Loss: 0.00101849
Iteration 7/25 | Loss: 0.00101821
Iteration 8/25 | Loss: 0.00101821
Iteration 9/25 | Loss: 0.00101821
Iteration 10/25 | Loss: 0.00101821
Iteration 11/25 | Loss: 0.00101821
Iteration 12/25 | Loss: 0.00101821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010182128753513098, 0.0010182128753513098, 0.0010182128753513098, 0.0010182128753513098, 0.0010182128753513098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010182128753513098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57508039
Iteration 2/25 | Loss: 0.00127297
Iteration 3/25 | Loss: 0.00127295
Iteration 4/25 | Loss: 0.00127295
Iteration 5/25 | Loss: 0.00127295
Iteration 6/25 | Loss: 0.00127295
Iteration 7/25 | Loss: 0.00127295
Iteration 8/25 | Loss: 0.00127295
Iteration 9/25 | Loss: 0.00127295
Iteration 10/25 | Loss: 0.00127295
Iteration 11/25 | Loss: 0.00127295
Iteration 12/25 | Loss: 0.00127295
Iteration 13/25 | Loss: 0.00127295
Iteration 14/25 | Loss: 0.00127295
Iteration 15/25 | Loss: 0.00127295
Iteration 16/25 | Loss: 0.00127295
Iteration 17/25 | Loss: 0.00127295
Iteration 18/25 | Loss: 0.00127295
Iteration 19/25 | Loss: 0.00127295
Iteration 20/25 | Loss: 0.00127295
Iteration 21/25 | Loss: 0.00127295
Iteration 22/25 | Loss: 0.00127295
Iteration 23/25 | Loss: 0.00127295
Iteration 24/25 | Loss: 0.00127295
Iteration 25/25 | Loss: 0.00127295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127295
Iteration 2/1000 | Loss: 0.00003626
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002085
Iteration 5/1000 | Loss: 0.00001967
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001710
Iteration 10/1000 | Loss: 0.00001683
Iteration 11/1000 | Loss: 0.00001659
Iteration 12/1000 | Loss: 0.00001642
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001619
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001617
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00001614
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001611
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001608
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001603
Iteration 46/1000 | Loss: 0.00001603
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001600
Iteration 52/1000 | Loss: 0.00001600
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001599
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001597
Iteration 58/1000 | Loss: 0.00001597
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001596
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001596
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001595
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001594
Iteration 70/1000 | Loss: 0.00001594
Iteration 71/1000 | Loss: 0.00001594
Iteration 72/1000 | Loss: 0.00001594
Iteration 73/1000 | Loss: 0.00001594
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001594
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001593
Iteration 84/1000 | Loss: 0.00001593
Iteration 85/1000 | Loss: 0.00001593
Iteration 86/1000 | Loss: 0.00001593
Iteration 87/1000 | Loss: 0.00001593
Iteration 88/1000 | Loss: 0.00001593
Iteration 89/1000 | Loss: 0.00001593
Iteration 90/1000 | Loss: 0.00001593
Iteration 91/1000 | Loss: 0.00001593
Iteration 92/1000 | Loss: 0.00001593
Iteration 93/1000 | Loss: 0.00001593
Iteration 94/1000 | Loss: 0.00001593
Iteration 95/1000 | Loss: 0.00001593
Iteration 96/1000 | Loss: 0.00001593
Iteration 97/1000 | Loss: 0.00001593
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.5930905647110194e-05, 1.5930905647110194e-05, 1.5930905647110194e-05, 1.5930905647110194e-05, 1.5930905647110194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5930905647110194e-05

Optimization complete. Final v2v error: 3.3512728214263916 mm

Highest mean error: 3.6397593021392822 mm for frame 86

Lowest mean error: 2.945110321044922 mm for frame 134

Saving results

Total time: 33.96247577667236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079688
Iteration 2/25 | Loss: 0.01079688
Iteration 3/25 | Loss: 0.01079688
Iteration 4/25 | Loss: 0.00553901
Iteration 5/25 | Loss: 0.00387492
Iteration 6/25 | Loss: 0.00273787
Iteration 7/25 | Loss: 0.00240494
Iteration 8/25 | Loss: 0.00228364
Iteration 9/25 | Loss: 0.00223875
Iteration 10/25 | Loss: 0.00213670
Iteration 11/25 | Loss: 0.00198399
Iteration 12/25 | Loss: 0.00206681
Iteration 13/25 | Loss: 0.00174930
Iteration 14/25 | Loss: 0.00166974
Iteration 15/25 | Loss: 0.00159801
Iteration 16/25 | Loss: 0.00160259
Iteration 17/25 | Loss: 0.00154990
Iteration 18/25 | Loss: 0.00152329
Iteration 19/25 | Loss: 0.00147700
Iteration 20/25 | Loss: 0.00146000
Iteration 21/25 | Loss: 0.00145263
Iteration 22/25 | Loss: 0.00144802
Iteration 23/25 | Loss: 0.00144482
Iteration 24/25 | Loss: 0.00144564
Iteration 25/25 | Loss: 0.00144146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61953199
Iteration 2/25 | Loss: 0.00382036
Iteration 3/25 | Loss: 0.00313929
Iteration 4/25 | Loss: 0.00313929
Iteration 5/25 | Loss: 0.00313929
Iteration 6/25 | Loss: 0.00313929
Iteration 7/25 | Loss: 0.00313929
Iteration 8/25 | Loss: 0.00313929
Iteration 9/25 | Loss: 0.00313929
Iteration 10/25 | Loss: 0.00313928
Iteration 11/25 | Loss: 0.00313928
Iteration 12/25 | Loss: 0.00313928
Iteration 13/25 | Loss: 0.00313928
Iteration 14/25 | Loss: 0.00313928
Iteration 15/25 | Loss: 0.00313928
Iteration 16/25 | Loss: 0.00313928
Iteration 17/25 | Loss: 0.00313928
Iteration 18/25 | Loss: 0.00313928
Iteration 19/25 | Loss: 0.00313928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0031392837408930063, 0.0031392837408930063, 0.0031392837408930063, 0.0031392837408930063, 0.0031392837408930063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031392837408930063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313928
Iteration 2/1000 | Loss: 0.00262293
Iteration 3/1000 | Loss: 0.00066051
Iteration 4/1000 | Loss: 0.00057154
Iteration 5/1000 | Loss: 0.00078444
Iteration 6/1000 | Loss: 0.00048319
Iteration 7/1000 | Loss: 0.00056813
Iteration 8/1000 | Loss: 0.00035579
Iteration 9/1000 | Loss: 0.00038176
Iteration 10/1000 | Loss: 0.00032611
Iteration 11/1000 | Loss: 0.00024615
Iteration 12/1000 | Loss: 0.00030729
Iteration 13/1000 | Loss: 0.00037371
Iteration 14/1000 | Loss: 0.00029003
Iteration 15/1000 | Loss: 0.00024339
Iteration 16/1000 | Loss: 0.00022262
Iteration 17/1000 | Loss: 0.00022116
Iteration 18/1000 | Loss: 0.00017817
Iteration 19/1000 | Loss: 0.00019624
Iteration 20/1000 | Loss: 0.00017360
Iteration 21/1000 | Loss: 0.00020124
Iteration 22/1000 | Loss: 0.00018098
Iteration 23/1000 | Loss: 0.00021705
Iteration 24/1000 | Loss: 0.00019085
Iteration 25/1000 | Loss: 0.00021180
Iteration 26/1000 | Loss: 0.00027176
Iteration 27/1000 | Loss: 0.00025977
Iteration 28/1000 | Loss: 0.00018178
Iteration 29/1000 | Loss: 0.00009702
Iteration 30/1000 | Loss: 0.00011561
Iteration 31/1000 | Loss: 0.00008812
Iteration 32/1000 | Loss: 0.00011012
Iteration 33/1000 | Loss: 0.00014018
Iteration 34/1000 | Loss: 0.00010836
Iteration 35/1000 | Loss: 0.00015299
Iteration 36/1000 | Loss: 0.00014096
Iteration 37/1000 | Loss: 0.00013320
Iteration 38/1000 | Loss: 0.00015572
Iteration 39/1000 | Loss: 0.00011306
Iteration 40/1000 | Loss: 0.00015409
Iteration 41/1000 | Loss: 0.00012169
Iteration 42/1000 | Loss: 0.00008218
Iteration 43/1000 | Loss: 0.00008800
Iteration 44/1000 | Loss: 0.00033330
Iteration 45/1000 | Loss: 0.00089036
Iteration 46/1000 | Loss: 0.00017036
Iteration 47/1000 | Loss: 0.00008947
Iteration 48/1000 | Loss: 0.00007131
Iteration 49/1000 | Loss: 0.00006206
Iteration 50/1000 | Loss: 0.00015482
Iteration 51/1000 | Loss: 0.00005947
Iteration 52/1000 | Loss: 0.00005499
Iteration 53/1000 | Loss: 0.00005146
Iteration 54/1000 | Loss: 0.00004888
Iteration 55/1000 | Loss: 0.00004645
Iteration 56/1000 | Loss: 0.00004464
Iteration 57/1000 | Loss: 0.00023771
Iteration 58/1000 | Loss: 0.00016353
Iteration 59/1000 | Loss: 0.00005261
Iteration 60/1000 | Loss: 0.00004547
Iteration 61/1000 | Loss: 0.00015115
Iteration 62/1000 | Loss: 0.00004486
Iteration 63/1000 | Loss: 0.00004131
Iteration 64/1000 | Loss: 0.00003853
Iteration 65/1000 | Loss: 0.00014921
Iteration 66/1000 | Loss: 0.00004216
Iteration 67/1000 | Loss: 0.00003869
Iteration 68/1000 | Loss: 0.00003637
Iteration 69/1000 | Loss: 0.00003463
Iteration 70/1000 | Loss: 0.00003363
Iteration 71/1000 | Loss: 0.00014658
Iteration 72/1000 | Loss: 0.00003907
Iteration 73/1000 | Loss: 0.00003527
Iteration 74/1000 | Loss: 0.00003386
Iteration 75/1000 | Loss: 0.00003202
Iteration 76/1000 | Loss: 0.00014585
Iteration 77/1000 | Loss: 0.00004021
Iteration 78/1000 | Loss: 0.00003399
Iteration 79/1000 | Loss: 0.00003244
Iteration 80/1000 | Loss: 0.00003071
Iteration 81/1000 | Loss: 0.00003014
Iteration 82/1000 | Loss: 0.00002966
Iteration 83/1000 | Loss: 0.00002941
Iteration 84/1000 | Loss: 0.00002923
Iteration 85/1000 | Loss: 0.00002916
Iteration 86/1000 | Loss: 0.00002916
Iteration 87/1000 | Loss: 0.00002915
Iteration 88/1000 | Loss: 0.00002915
Iteration 89/1000 | Loss: 0.00002915
Iteration 90/1000 | Loss: 0.00002915
Iteration 91/1000 | Loss: 0.00002915
Iteration 92/1000 | Loss: 0.00002915
Iteration 93/1000 | Loss: 0.00002915
Iteration 94/1000 | Loss: 0.00002914
Iteration 95/1000 | Loss: 0.00002914
Iteration 96/1000 | Loss: 0.00002914
Iteration 97/1000 | Loss: 0.00002914
Iteration 98/1000 | Loss: 0.00002906
Iteration 99/1000 | Loss: 0.00002906
Iteration 100/1000 | Loss: 0.00002906
Iteration 101/1000 | Loss: 0.00002902
Iteration 102/1000 | Loss: 0.00002902
Iteration 103/1000 | Loss: 0.00002902
Iteration 104/1000 | Loss: 0.00002901
Iteration 105/1000 | Loss: 0.00002901
Iteration 106/1000 | Loss: 0.00002901
Iteration 107/1000 | Loss: 0.00002901
Iteration 108/1000 | Loss: 0.00002901
Iteration 109/1000 | Loss: 0.00002901
Iteration 110/1000 | Loss: 0.00002900
Iteration 111/1000 | Loss: 0.00002900
Iteration 112/1000 | Loss: 0.00002894
Iteration 113/1000 | Loss: 0.00002894
Iteration 114/1000 | Loss: 0.00002893
Iteration 115/1000 | Loss: 0.00002893
Iteration 116/1000 | Loss: 0.00002893
Iteration 117/1000 | Loss: 0.00002891
Iteration 118/1000 | Loss: 0.00002891
Iteration 119/1000 | Loss: 0.00002890
Iteration 120/1000 | Loss: 0.00002890
Iteration 121/1000 | Loss: 0.00002890
Iteration 122/1000 | Loss: 0.00002890
Iteration 123/1000 | Loss: 0.00002889
Iteration 124/1000 | Loss: 0.00002888
Iteration 125/1000 | Loss: 0.00002888
Iteration 126/1000 | Loss: 0.00002888
Iteration 127/1000 | Loss: 0.00002888
Iteration 128/1000 | Loss: 0.00002888
Iteration 129/1000 | Loss: 0.00002888
Iteration 130/1000 | Loss: 0.00002888
Iteration 131/1000 | Loss: 0.00002888
Iteration 132/1000 | Loss: 0.00002888
Iteration 133/1000 | Loss: 0.00002888
Iteration 134/1000 | Loss: 0.00002888
Iteration 135/1000 | Loss: 0.00002888
Iteration 136/1000 | Loss: 0.00002888
Iteration 137/1000 | Loss: 0.00002888
Iteration 138/1000 | Loss: 0.00002888
Iteration 139/1000 | Loss: 0.00002888
Iteration 140/1000 | Loss: 0.00002887
Iteration 141/1000 | Loss: 0.00002887
Iteration 142/1000 | Loss: 0.00002887
Iteration 143/1000 | Loss: 0.00002887
Iteration 144/1000 | Loss: 0.00002887
Iteration 145/1000 | Loss: 0.00002887
Iteration 146/1000 | Loss: 0.00002887
Iteration 147/1000 | Loss: 0.00002887
Iteration 148/1000 | Loss: 0.00002887
Iteration 149/1000 | Loss: 0.00002887
Iteration 150/1000 | Loss: 0.00002886
Iteration 151/1000 | Loss: 0.00002886
Iteration 152/1000 | Loss: 0.00002886
Iteration 153/1000 | Loss: 0.00002886
Iteration 154/1000 | Loss: 0.00002886
Iteration 155/1000 | Loss: 0.00002886
Iteration 156/1000 | Loss: 0.00002885
Iteration 157/1000 | Loss: 0.00002885
Iteration 158/1000 | Loss: 0.00002885
Iteration 159/1000 | Loss: 0.00002885
Iteration 160/1000 | Loss: 0.00002885
Iteration 161/1000 | Loss: 0.00002885
Iteration 162/1000 | Loss: 0.00002885
Iteration 163/1000 | Loss: 0.00002885
Iteration 164/1000 | Loss: 0.00002884
Iteration 165/1000 | Loss: 0.00002884
Iteration 166/1000 | Loss: 0.00002884
Iteration 167/1000 | Loss: 0.00002884
Iteration 168/1000 | Loss: 0.00002884
Iteration 169/1000 | Loss: 0.00002884
Iteration 170/1000 | Loss: 0.00002884
Iteration 171/1000 | Loss: 0.00002884
Iteration 172/1000 | Loss: 0.00002884
Iteration 173/1000 | Loss: 0.00002884
Iteration 174/1000 | Loss: 0.00002884
Iteration 175/1000 | Loss: 0.00002884
Iteration 176/1000 | Loss: 0.00002884
Iteration 177/1000 | Loss: 0.00002884
Iteration 178/1000 | Loss: 0.00002884
Iteration 179/1000 | Loss: 0.00002884
Iteration 180/1000 | Loss: 0.00002884
Iteration 181/1000 | Loss: 0.00002883
Iteration 182/1000 | Loss: 0.00002883
Iteration 183/1000 | Loss: 0.00002883
Iteration 184/1000 | Loss: 0.00002883
Iteration 185/1000 | Loss: 0.00002883
Iteration 186/1000 | Loss: 0.00002883
Iteration 187/1000 | Loss: 0.00002883
Iteration 188/1000 | Loss: 0.00002883
Iteration 189/1000 | Loss: 0.00002883
Iteration 190/1000 | Loss: 0.00002883
Iteration 191/1000 | Loss: 0.00002883
Iteration 192/1000 | Loss: 0.00002883
Iteration 193/1000 | Loss: 0.00002883
Iteration 194/1000 | Loss: 0.00002883
Iteration 195/1000 | Loss: 0.00002883
Iteration 196/1000 | Loss: 0.00002882
Iteration 197/1000 | Loss: 0.00002882
Iteration 198/1000 | Loss: 0.00002882
Iteration 199/1000 | Loss: 0.00002882
Iteration 200/1000 | Loss: 0.00002882
Iteration 201/1000 | Loss: 0.00002882
Iteration 202/1000 | Loss: 0.00002882
Iteration 203/1000 | Loss: 0.00002882
Iteration 204/1000 | Loss: 0.00002882
Iteration 205/1000 | Loss: 0.00002882
Iteration 206/1000 | Loss: 0.00002882
Iteration 207/1000 | Loss: 0.00002882
Iteration 208/1000 | Loss: 0.00002882
Iteration 209/1000 | Loss: 0.00002882
Iteration 210/1000 | Loss: 0.00002882
Iteration 211/1000 | Loss: 0.00002882
Iteration 212/1000 | Loss: 0.00002882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.8820271836593747e-05, 2.8820271836593747e-05, 2.8820271836593747e-05, 2.8820271836593747e-05, 2.8820271836593747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8820271836593747e-05

Optimization complete. Final v2v error: 3.9466614723205566 mm

Highest mean error: 10.912250518798828 mm for frame 153

Lowest mean error: 3.5080490112304688 mm for frame 32

Saving results

Total time: 193.65393924713135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421690
Iteration 2/25 | Loss: 0.00114247
Iteration 3/25 | Loss: 0.00100291
Iteration 4/25 | Loss: 0.00098449
Iteration 5/25 | Loss: 0.00097965
Iteration 6/25 | Loss: 0.00097803
Iteration 7/25 | Loss: 0.00097762
Iteration 8/25 | Loss: 0.00097762
Iteration 9/25 | Loss: 0.00097762
Iteration 10/25 | Loss: 0.00097762
Iteration 11/25 | Loss: 0.00097762
Iteration 12/25 | Loss: 0.00097762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009776223450899124, 0.0009776223450899124, 0.0009776223450899124, 0.0009776223450899124, 0.0009776223450899124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009776223450899124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42174399
Iteration 2/25 | Loss: 0.00118603
Iteration 3/25 | Loss: 0.00118603
Iteration 4/25 | Loss: 0.00118603
Iteration 5/25 | Loss: 0.00118602
Iteration 6/25 | Loss: 0.00118602
Iteration 7/25 | Loss: 0.00118602
Iteration 8/25 | Loss: 0.00118602
Iteration 9/25 | Loss: 0.00118602
Iteration 10/25 | Loss: 0.00118602
Iteration 11/25 | Loss: 0.00118602
Iteration 12/25 | Loss: 0.00118602
Iteration 13/25 | Loss: 0.00118602
Iteration 14/25 | Loss: 0.00118602
Iteration 15/25 | Loss: 0.00118602
Iteration 16/25 | Loss: 0.00118602
Iteration 17/25 | Loss: 0.00118602
Iteration 18/25 | Loss: 0.00118602
Iteration 19/25 | Loss: 0.00118602
Iteration 20/25 | Loss: 0.00118602
Iteration 21/25 | Loss: 0.00118602
Iteration 22/25 | Loss: 0.00118602
Iteration 23/25 | Loss: 0.00118602
Iteration 24/25 | Loss: 0.00118602
Iteration 25/25 | Loss: 0.00118602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011860221857205033, 0.0011860221857205033, 0.0011860221857205033, 0.0011860221857205033, 0.0011860221857205033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011860221857205033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118602
Iteration 2/1000 | Loss: 0.00003355
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001477
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001403
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001318
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001309
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001309
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001308
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001307
Iteration 134/1000 | Loss: 0.00001307
Iteration 135/1000 | Loss: 0.00001307
Iteration 136/1000 | Loss: 0.00001307
Iteration 137/1000 | Loss: 0.00001307
Iteration 138/1000 | Loss: 0.00001307
Iteration 139/1000 | Loss: 0.00001307
Iteration 140/1000 | Loss: 0.00001307
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001306
Iteration 145/1000 | Loss: 0.00001306
Iteration 146/1000 | Loss: 0.00001306
Iteration 147/1000 | Loss: 0.00001306
Iteration 148/1000 | Loss: 0.00001306
Iteration 149/1000 | Loss: 0.00001306
Iteration 150/1000 | Loss: 0.00001306
Iteration 151/1000 | Loss: 0.00001306
Iteration 152/1000 | Loss: 0.00001306
Iteration 153/1000 | Loss: 0.00001306
Iteration 154/1000 | Loss: 0.00001306
Iteration 155/1000 | Loss: 0.00001306
Iteration 156/1000 | Loss: 0.00001306
Iteration 157/1000 | Loss: 0.00001306
Iteration 158/1000 | Loss: 0.00001306
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001305
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Iteration 170/1000 | Loss: 0.00001305
Iteration 171/1000 | Loss: 0.00001305
Iteration 172/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.304844772676006e-05, 1.304844772676006e-05, 1.304844772676006e-05, 1.304844772676006e-05, 1.304844772676006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.304844772676006e-05

Optimization complete. Final v2v error: 3.083390712738037 mm

Highest mean error: 3.9143640995025635 mm for frame 48

Lowest mean error: 2.6437008380889893 mm for frame 5

Saving results

Total time: 39.06633019447327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032232
Iteration 2/25 | Loss: 0.00203898
Iteration 3/25 | Loss: 0.00138749
Iteration 4/25 | Loss: 0.00120017
Iteration 5/25 | Loss: 0.00120163
Iteration 6/25 | Loss: 0.00118418
Iteration 7/25 | Loss: 0.00110491
Iteration 8/25 | Loss: 0.00106726
Iteration 9/25 | Loss: 0.00103396
Iteration 10/25 | Loss: 0.00102920
Iteration 11/25 | Loss: 0.00101897
Iteration 12/25 | Loss: 0.00099818
Iteration 13/25 | Loss: 0.00098559
Iteration 14/25 | Loss: 0.00098719
Iteration 15/25 | Loss: 0.00098447
Iteration 16/25 | Loss: 0.00098323
Iteration 17/25 | Loss: 0.00098028
Iteration 18/25 | Loss: 0.00097557
Iteration 19/25 | Loss: 0.00097388
Iteration 20/25 | Loss: 0.00097522
Iteration 21/25 | Loss: 0.00097762
Iteration 22/25 | Loss: 0.00097604
Iteration 23/25 | Loss: 0.00097065
Iteration 24/25 | Loss: 0.00096745
Iteration 25/25 | Loss: 0.00096612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25736821
Iteration 2/25 | Loss: 0.00130326
Iteration 3/25 | Loss: 0.00125727
Iteration 4/25 | Loss: 0.00125727
Iteration 5/25 | Loss: 0.00125726
Iteration 6/25 | Loss: 0.00125726
Iteration 7/25 | Loss: 0.00125726
Iteration 8/25 | Loss: 0.00125726
Iteration 9/25 | Loss: 0.00125726
Iteration 10/25 | Loss: 0.00125726
Iteration 11/25 | Loss: 0.00125726
Iteration 12/25 | Loss: 0.00125726
Iteration 13/25 | Loss: 0.00125726
Iteration 14/25 | Loss: 0.00125726
Iteration 15/25 | Loss: 0.00125726
Iteration 16/25 | Loss: 0.00125726
Iteration 17/25 | Loss: 0.00125726
Iteration 18/25 | Loss: 0.00125726
Iteration 19/25 | Loss: 0.00125726
Iteration 20/25 | Loss: 0.00125726
Iteration 21/25 | Loss: 0.00125726
Iteration 22/25 | Loss: 0.00125726
Iteration 23/25 | Loss: 0.00125726
Iteration 24/25 | Loss: 0.00125726
Iteration 25/25 | Loss: 0.00125726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125726
Iteration 2/1000 | Loss: 0.00012023
Iteration 3/1000 | Loss: 0.00003722
Iteration 4/1000 | Loss: 0.00002964
Iteration 5/1000 | Loss: 0.00004277
Iteration 6/1000 | Loss: 0.00003457
Iteration 7/1000 | Loss: 0.00029883
Iteration 8/1000 | Loss: 0.00003268
Iteration 9/1000 | Loss: 0.00002483
Iteration 10/1000 | Loss: 0.00002340
Iteration 11/1000 | Loss: 0.00002272
Iteration 12/1000 | Loss: 0.00002218
Iteration 13/1000 | Loss: 0.00002179
Iteration 14/1000 | Loss: 0.00028689
Iteration 15/1000 | Loss: 0.00029817
Iteration 16/1000 | Loss: 0.00014216
Iteration 17/1000 | Loss: 0.00002564
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00003602
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001679
Iteration 24/1000 | Loss: 0.00004843
Iteration 25/1000 | Loss: 0.00001633
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001560
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001557
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001549
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001546
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001541
Iteration 72/1000 | Loss: 0.00001541
Iteration 73/1000 | Loss: 0.00001541
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001540
Iteration 85/1000 | Loss: 0.00001540
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001539
Iteration 91/1000 | Loss: 0.00001538
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001537
Iteration 96/1000 | Loss: 0.00001537
Iteration 97/1000 | Loss: 0.00001537
Iteration 98/1000 | Loss: 0.00001537
Iteration 99/1000 | Loss: 0.00001537
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001536
Iteration 104/1000 | Loss: 0.00001536
Iteration 105/1000 | Loss: 0.00001536
Iteration 106/1000 | Loss: 0.00001536
Iteration 107/1000 | Loss: 0.00001535
Iteration 108/1000 | Loss: 0.00001535
Iteration 109/1000 | Loss: 0.00001535
Iteration 110/1000 | Loss: 0.00001535
Iteration 111/1000 | Loss: 0.00001535
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001535
Iteration 117/1000 | Loss: 0.00001535
Iteration 118/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.535443152533844e-05, 1.535443152533844e-05, 1.535443152533844e-05, 1.535443152533844e-05, 1.535443152533844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.535443152533844e-05

Optimization complete. Final v2v error: 3.22894287109375 mm

Highest mean error: 9.029193878173828 mm for frame 122

Lowest mean error: 2.706291675567627 mm for frame 5

Saving results

Total time: 94.17017841339111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374636
Iteration 2/25 | Loss: 0.00111197
Iteration 3/25 | Loss: 0.00098594
Iteration 4/25 | Loss: 0.00096821
Iteration 5/25 | Loss: 0.00096222
Iteration 6/25 | Loss: 0.00096061
Iteration 7/25 | Loss: 0.00096061
Iteration 8/25 | Loss: 0.00096061
Iteration 9/25 | Loss: 0.00096061
Iteration 10/25 | Loss: 0.00096061
Iteration 11/25 | Loss: 0.00096061
Iteration 12/25 | Loss: 0.00096061
Iteration 13/25 | Loss: 0.00096061
Iteration 14/25 | Loss: 0.00096061
Iteration 15/25 | Loss: 0.00096061
Iteration 16/25 | Loss: 0.00096061
Iteration 17/25 | Loss: 0.00096061
Iteration 18/25 | Loss: 0.00096061
Iteration 19/25 | Loss: 0.00096061
Iteration 20/25 | Loss: 0.00096061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009606138919480145, 0.0009606138919480145, 0.0009606138919480145, 0.0009606138919480145, 0.0009606138919480145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009606138919480145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.64461160
Iteration 2/25 | Loss: 0.00179272
Iteration 3/25 | Loss: 0.00179267
Iteration 4/25 | Loss: 0.00179266
Iteration 5/25 | Loss: 0.00179266
Iteration 6/25 | Loss: 0.00179266
Iteration 7/25 | Loss: 0.00179266
Iteration 8/25 | Loss: 0.00179266
Iteration 9/25 | Loss: 0.00179266
Iteration 10/25 | Loss: 0.00179266
Iteration 11/25 | Loss: 0.00179266
Iteration 12/25 | Loss: 0.00179266
Iteration 13/25 | Loss: 0.00179266
Iteration 14/25 | Loss: 0.00179266
Iteration 15/25 | Loss: 0.00179266
Iteration 16/25 | Loss: 0.00179266
Iteration 17/25 | Loss: 0.00179266
Iteration 18/25 | Loss: 0.00179266
Iteration 19/25 | Loss: 0.00179266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001792662194930017, 0.001792662194930017, 0.001792662194930017, 0.001792662194930017, 0.001792662194930017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001792662194930017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179266
Iteration 2/1000 | Loss: 0.00005281
Iteration 3/1000 | Loss: 0.00003268
Iteration 4/1000 | Loss: 0.00001762
Iteration 5/1000 | Loss: 0.00001549
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001241
Iteration 10/1000 | Loss: 0.00001210
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001169
Iteration 21/1000 | Loss: 0.00001169
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001159
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001139
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001138
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001120
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001118
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001114
Iteration 71/1000 | Loss: 0.00001114
Iteration 72/1000 | Loss: 0.00001114
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001112
Iteration 85/1000 | Loss: 0.00001112
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001110
Iteration 95/1000 | Loss: 0.00001110
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001108
Iteration 117/1000 | Loss: 0.00001108
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001106
Iteration 140/1000 | Loss: 0.00001106
Iteration 141/1000 | Loss: 0.00001106
Iteration 142/1000 | Loss: 0.00001106
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001105
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001104
Iteration 155/1000 | Loss: 0.00001104
Iteration 156/1000 | Loss: 0.00001104
Iteration 157/1000 | Loss: 0.00001104
Iteration 158/1000 | Loss: 0.00001104
Iteration 159/1000 | Loss: 0.00001103
Iteration 160/1000 | Loss: 0.00001103
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001103
Iteration 163/1000 | Loss: 0.00001103
Iteration 164/1000 | Loss: 0.00001103
Iteration 165/1000 | Loss: 0.00001103
Iteration 166/1000 | Loss: 0.00001103
Iteration 167/1000 | Loss: 0.00001103
Iteration 168/1000 | Loss: 0.00001103
Iteration 169/1000 | Loss: 0.00001103
Iteration 170/1000 | Loss: 0.00001103
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001102
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001102
Iteration 179/1000 | Loss: 0.00001102
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001102
Iteration 188/1000 | Loss: 0.00001102
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001101
Iteration 200/1000 | Loss: 0.00001101
Iteration 201/1000 | Loss: 0.00001101
Iteration 202/1000 | Loss: 0.00001101
Iteration 203/1000 | Loss: 0.00001101
Iteration 204/1000 | Loss: 0.00001101
Iteration 205/1000 | Loss: 0.00001101
Iteration 206/1000 | Loss: 0.00001101
Iteration 207/1000 | Loss: 0.00001101
Iteration 208/1000 | Loss: 0.00001101
Iteration 209/1000 | Loss: 0.00001101
Iteration 210/1000 | Loss: 0.00001100
Iteration 211/1000 | Loss: 0.00001100
Iteration 212/1000 | Loss: 0.00001100
Iteration 213/1000 | Loss: 0.00001100
Iteration 214/1000 | Loss: 0.00001100
Iteration 215/1000 | Loss: 0.00001100
Iteration 216/1000 | Loss: 0.00001100
Iteration 217/1000 | Loss: 0.00001100
Iteration 218/1000 | Loss: 0.00001100
Iteration 219/1000 | Loss: 0.00001100
Iteration 220/1000 | Loss: 0.00001100
Iteration 221/1000 | Loss: 0.00001100
Iteration 222/1000 | Loss: 0.00001100
Iteration 223/1000 | Loss: 0.00001100
Iteration 224/1000 | Loss: 0.00001100
Iteration 225/1000 | Loss: 0.00001100
Iteration 226/1000 | Loss: 0.00001100
Iteration 227/1000 | Loss: 0.00001100
Iteration 228/1000 | Loss: 0.00001100
Iteration 229/1000 | Loss: 0.00001100
Iteration 230/1000 | Loss: 0.00001100
Iteration 231/1000 | Loss: 0.00001100
Iteration 232/1000 | Loss: 0.00001100
Iteration 233/1000 | Loss: 0.00001100
Iteration 234/1000 | Loss: 0.00001100
Iteration 235/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.0996204764524009e-05, 1.0996204764524009e-05, 1.0996204764524009e-05, 1.0996204764524009e-05, 1.0996204764524009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0996204764524009e-05

Optimization complete. Final v2v error: 2.8619344234466553 mm

Highest mean error: 3.4661402702331543 mm for frame 10

Lowest mean error: 2.3036985397338867 mm for frame 23

Saving results

Total time: 42.539756298065186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00704841
Iteration 2/25 | Loss: 0.00144010
Iteration 3/25 | Loss: 0.00106688
Iteration 4/25 | Loss: 0.00101154
Iteration 5/25 | Loss: 0.00099396
Iteration 6/25 | Loss: 0.00098898
Iteration 7/25 | Loss: 0.00098740
Iteration 8/25 | Loss: 0.00098676
Iteration 9/25 | Loss: 0.00098636
Iteration 10/25 | Loss: 0.00098585
Iteration 11/25 | Loss: 0.00098459
Iteration 12/25 | Loss: 0.00098385
Iteration 13/25 | Loss: 0.00098339
Iteration 14/25 | Loss: 0.00098316
Iteration 15/25 | Loss: 0.00098303
Iteration 16/25 | Loss: 0.00098291
Iteration 17/25 | Loss: 0.00098283
Iteration 18/25 | Loss: 0.00098283
Iteration 19/25 | Loss: 0.00098282
Iteration 20/25 | Loss: 0.00098281
Iteration 21/25 | Loss: 0.00098281
Iteration 22/25 | Loss: 0.00098281
Iteration 23/25 | Loss: 0.00098281
Iteration 24/25 | Loss: 0.00098281
Iteration 25/25 | Loss: 0.00098281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41123676
Iteration 2/25 | Loss: 0.00138827
Iteration 3/25 | Loss: 0.00138825
Iteration 4/25 | Loss: 0.00138825
Iteration 5/25 | Loss: 0.00138825
Iteration 6/25 | Loss: 0.00138825
Iteration 7/25 | Loss: 0.00138825
Iteration 8/25 | Loss: 0.00138825
Iteration 9/25 | Loss: 0.00138825
Iteration 10/25 | Loss: 0.00138825
Iteration 11/25 | Loss: 0.00138825
Iteration 12/25 | Loss: 0.00138825
Iteration 13/25 | Loss: 0.00138825
Iteration 14/25 | Loss: 0.00138825
Iteration 15/25 | Loss: 0.00138825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013882509665563703, 0.0013882509665563703, 0.0013882509665563703, 0.0013882509665563703, 0.0013882509665563703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013882509665563703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138825
Iteration 2/1000 | Loss: 0.00009903
Iteration 3/1000 | Loss: 0.00006990
Iteration 4/1000 | Loss: 0.00012716
Iteration 5/1000 | Loss: 0.00005114
Iteration 6/1000 | Loss: 0.00005760
Iteration 7/1000 | Loss: 0.00010678
Iteration 8/1000 | Loss: 0.00002500
Iteration 9/1000 | Loss: 0.00002246
Iteration 10/1000 | Loss: 0.00002130
Iteration 11/1000 | Loss: 0.00002072
Iteration 12/1000 | Loss: 0.00008590
Iteration 13/1000 | Loss: 0.00010192
Iteration 14/1000 | Loss: 0.00001954
Iteration 15/1000 | Loss: 0.00005920
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001678
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00003638
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001538
Iteration 22/1000 | Loss: 0.00001530
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001514
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001509
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001506
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001498
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001497
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001494
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001492
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001491
Iteration 66/1000 | Loss: 0.00001491
Iteration 67/1000 | Loss: 0.00001491
Iteration 68/1000 | Loss: 0.00001491
Iteration 69/1000 | Loss: 0.00001491
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001490
Iteration 72/1000 | Loss: 0.00001490
Iteration 73/1000 | Loss: 0.00001490
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001489
Iteration 78/1000 | Loss: 0.00001489
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00007978
Iteration 114/1000 | Loss: 0.00001634
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001484
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001481
Iteration 119/1000 | Loss: 0.00001479
Iteration 120/1000 | Loss: 0.00001479
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001477
Iteration 127/1000 | Loss: 0.00001477
Iteration 128/1000 | Loss: 0.00001477
Iteration 129/1000 | Loss: 0.00001477
Iteration 130/1000 | Loss: 0.00001477
Iteration 131/1000 | Loss: 0.00001477
Iteration 132/1000 | Loss: 0.00001476
Iteration 133/1000 | Loss: 0.00001476
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00001475
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001475
Iteration 139/1000 | Loss: 0.00001475
Iteration 140/1000 | Loss: 0.00001475
Iteration 141/1000 | Loss: 0.00001475
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001474
Iteration 148/1000 | Loss: 0.00001474
Iteration 149/1000 | Loss: 0.00001474
Iteration 150/1000 | Loss: 0.00001474
Iteration 151/1000 | Loss: 0.00001474
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001473
Iteration 158/1000 | Loss: 0.00001473
Iteration 159/1000 | Loss: 0.00001473
Iteration 160/1000 | Loss: 0.00001473
Iteration 161/1000 | Loss: 0.00001473
Iteration 162/1000 | Loss: 0.00001473
Iteration 163/1000 | Loss: 0.00001473
Iteration 164/1000 | Loss: 0.00001473
Iteration 165/1000 | Loss: 0.00001473
Iteration 166/1000 | Loss: 0.00001473
Iteration 167/1000 | Loss: 0.00001473
Iteration 168/1000 | Loss: 0.00001473
Iteration 169/1000 | Loss: 0.00001473
Iteration 170/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.4726860172231682e-05, 1.4726860172231682e-05, 1.4726860172231682e-05, 1.4726860172231682e-05, 1.4726860172231682e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4726860172231682e-05

Optimization complete. Final v2v error: 3.1470789909362793 mm

Highest mean error: 6.989537239074707 mm for frame 88

Lowest mean error: 2.2408506870269775 mm for frame 11

Saving results

Total time: 87.10127401351929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_26_us_0655/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_26_us_0655/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544209
Iteration 2/25 | Loss: 0.00099143
Iteration 3/25 | Loss: 0.00093347
Iteration 4/25 | Loss: 0.00092617
Iteration 5/25 | Loss: 0.00092376
Iteration 6/25 | Loss: 0.00092342
Iteration 7/25 | Loss: 0.00092342
Iteration 8/25 | Loss: 0.00092342
Iteration 9/25 | Loss: 0.00092342
Iteration 10/25 | Loss: 0.00092342
Iteration 11/25 | Loss: 0.00092342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009234200115315616, 0.0009234200115315616, 0.0009234200115315616, 0.0009234200115315616, 0.0009234200115315616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009234200115315616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.11110401
Iteration 2/25 | Loss: 0.00128271
Iteration 3/25 | Loss: 0.00128271
Iteration 4/25 | Loss: 0.00128271
Iteration 5/25 | Loss: 0.00128271
Iteration 6/25 | Loss: 0.00128271
Iteration 7/25 | Loss: 0.00128271
Iteration 8/25 | Loss: 0.00128271
Iteration 9/25 | Loss: 0.00128271
Iteration 10/25 | Loss: 0.00128271
Iteration 11/25 | Loss: 0.00128271
Iteration 12/25 | Loss: 0.00128271
Iteration 13/25 | Loss: 0.00128271
Iteration 14/25 | Loss: 0.00128271
Iteration 15/25 | Loss: 0.00128271
Iteration 16/25 | Loss: 0.00128271
Iteration 17/25 | Loss: 0.00128271
Iteration 18/25 | Loss: 0.00128271
Iteration 19/25 | Loss: 0.00128271
Iteration 20/25 | Loss: 0.00128271
Iteration 21/25 | Loss: 0.00128271
Iteration 22/25 | Loss: 0.00128271
Iteration 23/25 | Loss: 0.00128271
Iteration 24/25 | Loss: 0.00128271
Iteration 25/25 | Loss: 0.00128271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128271
Iteration 2/1000 | Loss: 0.00002988
Iteration 3/1000 | Loss: 0.00001928
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001161
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001135
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001125
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001113
Iteration 24/1000 | Loss: 0.00001113
Iteration 25/1000 | Loss: 0.00001112
Iteration 26/1000 | Loss: 0.00001112
Iteration 27/1000 | Loss: 0.00001111
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001101
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001101
Iteration 33/1000 | Loss: 0.00001101
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001101
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001100
Iteration 43/1000 | Loss: 0.00001099
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001099
Iteration 47/1000 | Loss: 0.00001098
Iteration 48/1000 | Loss: 0.00001097
Iteration 49/1000 | Loss: 0.00001097
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001097
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001095
Iteration 59/1000 | Loss: 0.00001095
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001092
Iteration 82/1000 | Loss: 0.00001092
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001088
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001087
Iteration 106/1000 | Loss: 0.00001087
Iteration 107/1000 | Loss: 0.00001087
Iteration 108/1000 | Loss: 0.00001087
Iteration 109/1000 | Loss: 0.00001087
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.0865799595194403e-05, 1.0865799595194403e-05, 1.0865799595194403e-05, 1.0865799595194403e-05, 1.0865799595194403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0865799595194403e-05

Optimization complete. Final v2v error: 2.8747804164886475 mm

Highest mean error: 3.2081332206726074 mm for frame 86

Lowest mean error: 2.4812090396881104 mm for frame 20

Saving results

Total time: 37.65722894668579
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397731
Iteration 2/25 | Loss: 0.00098045
Iteration 3/25 | Loss: 0.00074409
Iteration 4/25 | Loss: 0.00071533
Iteration 5/25 | Loss: 0.00070799
Iteration 6/25 | Loss: 0.00070586
Iteration 7/25 | Loss: 0.00070538
Iteration 8/25 | Loss: 0.00070538
Iteration 9/25 | Loss: 0.00070538
Iteration 10/25 | Loss: 0.00070538
Iteration 11/25 | Loss: 0.00070538
Iteration 12/25 | Loss: 0.00070538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007053764420561492, 0.0007053764420561492, 0.0007053764420561492, 0.0007053764420561492, 0.0007053764420561492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007053764420561492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47123969
Iteration 2/25 | Loss: 0.00029010
Iteration 3/25 | Loss: 0.00029009
Iteration 4/25 | Loss: 0.00029009
Iteration 5/25 | Loss: 0.00029009
Iteration 6/25 | Loss: 0.00029009
Iteration 7/25 | Loss: 0.00029009
Iteration 8/25 | Loss: 0.00029009
Iteration 9/25 | Loss: 0.00029009
Iteration 10/25 | Loss: 0.00029009
Iteration 11/25 | Loss: 0.00029009
Iteration 12/25 | Loss: 0.00029009
Iteration 13/25 | Loss: 0.00029009
Iteration 14/25 | Loss: 0.00029009
Iteration 15/25 | Loss: 0.00029009
Iteration 16/25 | Loss: 0.00029009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002900892577599734, 0.0002900892577599734, 0.0002900892577599734, 0.0002900892577599734, 0.0002900892577599734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002900892577599734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029009
Iteration 2/1000 | Loss: 0.00002183
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001194
Iteration 9/1000 | Loss: 0.00001184
Iteration 10/1000 | Loss: 0.00001183
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001177
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001157
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001153
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001149
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001145
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.1451835234765895e-05, 1.1451835234765895e-05, 1.1451835234765895e-05, 1.1451835234765895e-05, 1.1451835234765895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1451835234765895e-05

Optimization complete. Final v2v error: 2.9185330867767334 mm

Highest mean error: 3.0046896934509277 mm for frame 85

Lowest mean error: 2.843432664871216 mm for frame 3

Saving results

Total time: 27.630205154418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816084
Iteration 2/25 | Loss: 0.00088691
Iteration 3/25 | Loss: 0.00074042
Iteration 4/25 | Loss: 0.00071648
Iteration 5/25 | Loss: 0.00071128
Iteration 6/25 | Loss: 0.00070998
Iteration 7/25 | Loss: 0.00070983
Iteration 8/25 | Loss: 0.00070983
Iteration 9/25 | Loss: 0.00070983
Iteration 10/25 | Loss: 0.00070983
Iteration 11/25 | Loss: 0.00070983
Iteration 12/25 | Loss: 0.00070983
Iteration 13/25 | Loss: 0.00070983
Iteration 14/25 | Loss: 0.00070983
Iteration 15/25 | Loss: 0.00070983
Iteration 16/25 | Loss: 0.00070983
Iteration 17/25 | Loss: 0.00070983
Iteration 18/25 | Loss: 0.00070983
Iteration 19/25 | Loss: 0.00070983
Iteration 20/25 | Loss: 0.00070983
Iteration 21/25 | Loss: 0.00070983
Iteration 22/25 | Loss: 0.00070983
Iteration 23/25 | Loss: 0.00070983
Iteration 24/25 | Loss: 0.00070983
Iteration 25/25 | Loss: 0.00070983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47736943
Iteration 2/25 | Loss: 0.00028999
Iteration 3/25 | Loss: 0.00028999
Iteration 4/25 | Loss: 0.00028998
Iteration 5/25 | Loss: 0.00028998
Iteration 6/25 | Loss: 0.00028998
Iteration 7/25 | Loss: 0.00028998
Iteration 8/25 | Loss: 0.00028998
Iteration 9/25 | Loss: 0.00028998
Iteration 10/25 | Loss: 0.00028998
Iteration 11/25 | Loss: 0.00028998
Iteration 12/25 | Loss: 0.00028998
Iteration 13/25 | Loss: 0.00028998
Iteration 14/25 | Loss: 0.00028998
Iteration 15/25 | Loss: 0.00028998
Iteration 16/25 | Loss: 0.00028998
Iteration 17/25 | Loss: 0.00028998
Iteration 18/25 | Loss: 0.00028998
Iteration 19/25 | Loss: 0.00028998
Iteration 20/25 | Loss: 0.00028998
Iteration 21/25 | Loss: 0.00028998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002899829705711454, 0.0002899829705711454, 0.0002899829705711454, 0.0002899829705711454, 0.0002899829705711454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002899829705711454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028998
Iteration 2/1000 | Loss: 0.00002069
Iteration 3/1000 | Loss: 0.00001318
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001162
Iteration 6/1000 | Loss: 0.00001113
Iteration 7/1000 | Loss: 0.00001087
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001078
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001076
Iteration 13/1000 | Loss: 0.00001074
Iteration 14/1000 | Loss: 0.00001073
Iteration 15/1000 | Loss: 0.00001072
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001065
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001063
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001060
Iteration 28/1000 | Loss: 0.00001059
Iteration 29/1000 | Loss: 0.00001059
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001055
Iteration 32/1000 | Loss: 0.00001054
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001047
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001040
Iteration 39/1000 | Loss: 0.00001039
Iteration 40/1000 | Loss: 0.00001039
Iteration 41/1000 | Loss: 0.00001038
Iteration 42/1000 | Loss: 0.00001038
Iteration 43/1000 | Loss: 0.00001038
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001036
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001036
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001035
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001033
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001029
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001029
Iteration 61/1000 | Loss: 0.00001029
Iteration 62/1000 | Loss: 0.00001028
Iteration 63/1000 | Loss: 0.00001028
Iteration 64/1000 | Loss: 0.00001028
Iteration 65/1000 | Loss: 0.00001028
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001028
Iteration 68/1000 | Loss: 0.00001028
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001026
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001026
Iteration 76/1000 | Loss: 0.00001026
Iteration 77/1000 | Loss: 0.00001026
Iteration 78/1000 | Loss: 0.00001026
Iteration 79/1000 | Loss: 0.00001025
Iteration 80/1000 | Loss: 0.00001025
Iteration 81/1000 | Loss: 0.00001025
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001025
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001024
Iteration 87/1000 | Loss: 0.00001024
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001023
Iteration 90/1000 | Loss: 0.00001023
Iteration 91/1000 | Loss: 0.00001023
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001023
Iteration 96/1000 | Loss: 0.00001022
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001022
Iteration 99/1000 | Loss: 0.00001022
Iteration 100/1000 | Loss: 0.00001022
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001022
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001020
Iteration 111/1000 | Loss: 0.00001020
Iteration 112/1000 | Loss: 0.00001020
Iteration 113/1000 | Loss: 0.00001020
Iteration 114/1000 | Loss: 0.00001020
Iteration 115/1000 | Loss: 0.00001020
Iteration 116/1000 | Loss: 0.00001020
Iteration 117/1000 | Loss: 0.00001019
Iteration 118/1000 | Loss: 0.00001019
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001019
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001018
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001018
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001016
Iteration 134/1000 | Loss: 0.00001016
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001015
Iteration 138/1000 | Loss: 0.00001015
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001014
Iteration 142/1000 | Loss: 0.00001014
Iteration 143/1000 | Loss: 0.00001014
Iteration 144/1000 | Loss: 0.00001014
Iteration 145/1000 | Loss: 0.00001013
Iteration 146/1000 | Loss: 0.00001013
Iteration 147/1000 | Loss: 0.00001013
Iteration 148/1000 | Loss: 0.00001013
Iteration 149/1000 | Loss: 0.00001012
Iteration 150/1000 | Loss: 0.00001012
Iteration 151/1000 | Loss: 0.00001012
Iteration 152/1000 | Loss: 0.00001012
Iteration 153/1000 | Loss: 0.00001012
Iteration 154/1000 | Loss: 0.00001011
Iteration 155/1000 | Loss: 0.00001011
Iteration 156/1000 | Loss: 0.00001011
Iteration 157/1000 | Loss: 0.00001011
Iteration 158/1000 | Loss: 0.00001011
Iteration 159/1000 | Loss: 0.00001011
Iteration 160/1000 | Loss: 0.00001011
Iteration 161/1000 | Loss: 0.00001011
Iteration 162/1000 | Loss: 0.00001011
Iteration 163/1000 | Loss: 0.00001011
Iteration 164/1000 | Loss: 0.00001011
Iteration 165/1000 | Loss: 0.00001011
Iteration 166/1000 | Loss: 0.00001010
Iteration 167/1000 | Loss: 0.00001010
Iteration 168/1000 | Loss: 0.00001010
Iteration 169/1000 | Loss: 0.00001010
Iteration 170/1000 | Loss: 0.00001010
Iteration 171/1000 | Loss: 0.00001010
Iteration 172/1000 | Loss: 0.00001010
Iteration 173/1000 | Loss: 0.00001010
Iteration 174/1000 | Loss: 0.00001010
Iteration 175/1000 | Loss: 0.00001010
Iteration 176/1000 | Loss: 0.00001010
Iteration 177/1000 | Loss: 0.00001009
Iteration 178/1000 | Loss: 0.00001009
Iteration 179/1000 | Loss: 0.00001009
Iteration 180/1000 | Loss: 0.00001009
Iteration 181/1000 | Loss: 0.00001009
Iteration 182/1000 | Loss: 0.00001009
Iteration 183/1000 | Loss: 0.00001009
Iteration 184/1000 | Loss: 0.00001009
Iteration 185/1000 | Loss: 0.00001009
Iteration 186/1000 | Loss: 0.00001009
Iteration 187/1000 | Loss: 0.00001009
Iteration 188/1000 | Loss: 0.00001009
Iteration 189/1000 | Loss: 0.00001009
Iteration 190/1000 | Loss: 0.00001009
Iteration 191/1000 | Loss: 0.00001009
Iteration 192/1000 | Loss: 0.00001009
Iteration 193/1000 | Loss: 0.00001009
Iteration 194/1000 | Loss: 0.00001009
Iteration 195/1000 | Loss: 0.00001009
Iteration 196/1000 | Loss: 0.00001009
Iteration 197/1000 | Loss: 0.00001009
Iteration 198/1000 | Loss: 0.00001009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.0091675903822761e-05, 1.0091675903822761e-05, 1.0091675903822761e-05, 1.0091675903822761e-05, 1.0091675903822761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0091675903822761e-05

Optimization complete. Final v2v error: 2.6835548877716064 mm

Highest mean error: 2.837423801422119 mm for frame 84

Lowest mean error: 2.545520067214966 mm for frame 145

Saving results

Total time: 36.29160928726196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838490
Iteration 2/25 | Loss: 0.00122769
Iteration 3/25 | Loss: 0.00085227
Iteration 4/25 | Loss: 0.00081887
Iteration 5/25 | Loss: 0.00081064
Iteration 6/25 | Loss: 0.00080740
Iteration 7/25 | Loss: 0.00081456
Iteration 8/25 | Loss: 0.00081322
Iteration 9/25 | Loss: 0.00081004
Iteration 10/25 | Loss: 0.00080414
Iteration 11/25 | Loss: 0.00080072
Iteration 12/25 | Loss: 0.00080711
Iteration 13/25 | Loss: 0.00079932
Iteration 14/25 | Loss: 0.00079679
Iteration 15/25 | Loss: 0.00079647
Iteration 16/25 | Loss: 0.00079634
Iteration 17/25 | Loss: 0.00079633
Iteration 18/25 | Loss: 0.00079633
Iteration 19/25 | Loss: 0.00079633
Iteration 20/25 | Loss: 0.00079633
Iteration 21/25 | Loss: 0.00079632
Iteration 22/25 | Loss: 0.00079632
Iteration 23/25 | Loss: 0.00079632
Iteration 24/25 | Loss: 0.00079632
Iteration 25/25 | Loss: 0.00079632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.43339348
Iteration 2/25 | Loss: 0.00038419
Iteration 3/25 | Loss: 0.00038415
Iteration 4/25 | Loss: 0.00038415
Iteration 5/25 | Loss: 0.00038415
Iteration 6/25 | Loss: 0.00038415
Iteration 7/25 | Loss: 0.00038415
Iteration 8/25 | Loss: 0.00038415
Iteration 9/25 | Loss: 0.00038415
Iteration 10/25 | Loss: 0.00038415
Iteration 11/25 | Loss: 0.00038415
Iteration 12/25 | Loss: 0.00038415
Iteration 13/25 | Loss: 0.00038415
Iteration 14/25 | Loss: 0.00038415
Iteration 15/25 | Loss: 0.00038415
Iteration 16/25 | Loss: 0.00038415
Iteration 17/25 | Loss: 0.00038415
Iteration 18/25 | Loss: 0.00038415
Iteration 19/25 | Loss: 0.00038415
Iteration 20/25 | Loss: 0.00038415
Iteration 21/25 | Loss: 0.00038415
Iteration 22/25 | Loss: 0.00038415
Iteration 23/25 | Loss: 0.00038415
Iteration 24/25 | Loss: 0.00038415
Iteration 25/25 | Loss: 0.00038415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038415
Iteration 2/1000 | Loss: 0.00003184
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001861
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00001787
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001781
Iteration 19/1000 | Loss: 0.00001777
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001776
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00001775
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001774
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001771
Iteration 32/1000 | Loss: 0.00001771
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001765
Iteration 46/1000 | Loss: 0.00001765
Iteration 47/1000 | Loss: 0.00001765
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001764
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001757
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001756
Iteration 78/1000 | Loss: 0.00001756
Iteration 79/1000 | Loss: 0.00001756
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001755
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00001755
Iteration 86/1000 | Loss: 0.00001754
Iteration 87/1000 | Loss: 0.00001754
Iteration 88/1000 | Loss: 0.00001754
Iteration 89/1000 | Loss: 0.00001754
Iteration 90/1000 | Loss: 0.00001754
Iteration 91/1000 | Loss: 0.00001754
Iteration 92/1000 | Loss: 0.00001754
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001753
Iteration 95/1000 | Loss: 0.00001753
Iteration 96/1000 | Loss: 0.00001753
Iteration 97/1000 | Loss: 0.00001753
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001753
Iteration 100/1000 | Loss: 0.00001753
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001752
Iteration 105/1000 | Loss: 0.00001752
Iteration 106/1000 | Loss: 0.00001752
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001751
Iteration 110/1000 | Loss: 0.00001751
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001750
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001748
Iteration 128/1000 | Loss: 0.00001748
Iteration 129/1000 | Loss: 0.00001748
Iteration 130/1000 | Loss: 0.00001748
Iteration 131/1000 | Loss: 0.00001748
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001747
Iteration 138/1000 | Loss: 0.00001747
Iteration 139/1000 | Loss: 0.00001747
Iteration 140/1000 | Loss: 0.00001747
Iteration 141/1000 | Loss: 0.00001747
Iteration 142/1000 | Loss: 0.00001747
Iteration 143/1000 | Loss: 0.00001747
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00001747
Iteration 146/1000 | Loss: 0.00001747
Iteration 147/1000 | Loss: 0.00001747
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00001746
Iteration 152/1000 | Loss: 0.00001746
Iteration 153/1000 | Loss: 0.00001746
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001745
Iteration 159/1000 | Loss: 0.00001745
Iteration 160/1000 | Loss: 0.00001745
Iteration 161/1000 | Loss: 0.00001745
Iteration 162/1000 | Loss: 0.00001745
Iteration 163/1000 | Loss: 0.00001745
Iteration 164/1000 | Loss: 0.00001745
Iteration 165/1000 | Loss: 0.00001744
Iteration 166/1000 | Loss: 0.00001744
Iteration 167/1000 | Loss: 0.00001744
Iteration 168/1000 | Loss: 0.00001744
Iteration 169/1000 | Loss: 0.00001744
Iteration 170/1000 | Loss: 0.00001744
Iteration 171/1000 | Loss: 0.00001744
Iteration 172/1000 | Loss: 0.00001744
Iteration 173/1000 | Loss: 0.00001744
Iteration 174/1000 | Loss: 0.00001744
Iteration 175/1000 | Loss: 0.00001744
Iteration 176/1000 | Loss: 0.00001744
Iteration 177/1000 | Loss: 0.00001743
Iteration 178/1000 | Loss: 0.00001743
Iteration 179/1000 | Loss: 0.00001743
Iteration 180/1000 | Loss: 0.00001743
Iteration 181/1000 | Loss: 0.00001743
Iteration 182/1000 | Loss: 0.00001743
Iteration 183/1000 | Loss: 0.00001743
Iteration 184/1000 | Loss: 0.00001743
Iteration 185/1000 | Loss: 0.00001743
Iteration 186/1000 | Loss: 0.00001743
Iteration 187/1000 | Loss: 0.00001743
Iteration 188/1000 | Loss: 0.00001743
Iteration 189/1000 | Loss: 0.00001742
Iteration 190/1000 | Loss: 0.00001742
Iteration 191/1000 | Loss: 0.00001742
Iteration 192/1000 | Loss: 0.00001742
Iteration 193/1000 | Loss: 0.00001742
Iteration 194/1000 | Loss: 0.00001742
Iteration 195/1000 | Loss: 0.00001741
Iteration 196/1000 | Loss: 0.00001741
Iteration 197/1000 | Loss: 0.00001741
Iteration 198/1000 | Loss: 0.00001741
Iteration 199/1000 | Loss: 0.00001741
Iteration 200/1000 | Loss: 0.00001741
Iteration 201/1000 | Loss: 0.00001741
Iteration 202/1000 | Loss: 0.00001741
Iteration 203/1000 | Loss: 0.00001741
Iteration 204/1000 | Loss: 0.00001741
Iteration 205/1000 | Loss: 0.00001741
Iteration 206/1000 | Loss: 0.00001741
Iteration 207/1000 | Loss: 0.00001741
Iteration 208/1000 | Loss: 0.00001740
Iteration 209/1000 | Loss: 0.00001740
Iteration 210/1000 | Loss: 0.00001740
Iteration 211/1000 | Loss: 0.00001740
Iteration 212/1000 | Loss: 0.00001740
Iteration 213/1000 | Loss: 0.00001740
Iteration 214/1000 | Loss: 0.00001740
Iteration 215/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.740453444654122e-05, 1.740453444654122e-05, 1.740453444654122e-05, 1.740453444654122e-05, 1.740453444654122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.740453444654122e-05

Optimization complete. Final v2v error: 3.462245464324951 mm

Highest mean error: 4.118717193603516 mm for frame 96

Lowest mean error: 2.9394102096557617 mm for frame 48

Saving results

Total time: 55.062528133392334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00285139
Iteration 2/25 | Loss: 0.00098264
Iteration 3/25 | Loss: 0.00080471
Iteration 4/25 | Loss: 0.00076468
Iteration 5/25 | Loss: 0.00075593
Iteration 6/25 | Loss: 0.00075532
Iteration 7/25 | Loss: 0.00075532
Iteration 8/25 | Loss: 0.00075532
Iteration 9/25 | Loss: 0.00075532
Iteration 10/25 | Loss: 0.00075532
Iteration 11/25 | Loss: 0.00075532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000755317450966686, 0.000755317450966686, 0.000755317450966686, 0.000755317450966686, 0.000755317450966686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000755317450966686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52287173
Iteration 2/25 | Loss: 0.00036109
Iteration 3/25 | Loss: 0.00036109
Iteration 4/25 | Loss: 0.00036109
Iteration 5/25 | Loss: 0.00036109
Iteration 6/25 | Loss: 0.00036109
Iteration 7/25 | Loss: 0.00036108
Iteration 8/25 | Loss: 0.00036108
Iteration 9/25 | Loss: 0.00036108
Iteration 10/25 | Loss: 0.00036108
Iteration 11/25 | Loss: 0.00036108
Iteration 12/25 | Loss: 0.00036108
Iteration 13/25 | Loss: 0.00036108
Iteration 14/25 | Loss: 0.00036108
Iteration 15/25 | Loss: 0.00036108
Iteration 16/25 | Loss: 0.00036108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00036108429776504636, 0.00036108429776504636, 0.00036108429776504636, 0.00036108429776504636, 0.00036108429776504636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036108429776504636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036108
Iteration 2/1000 | Loss: 0.00004007
Iteration 3/1000 | Loss: 0.00002752
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002216
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002072
Iteration 8/1000 | Loss: 0.00002029
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001943
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001932
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001927
Iteration 18/1000 | Loss: 0.00001925
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001916
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001916
Iteration 23/1000 | Loss: 0.00001915
Iteration 24/1000 | Loss: 0.00001915
Iteration 25/1000 | Loss: 0.00001915
Iteration 26/1000 | Loss: 0.00001914
Iteration 27/1000 | Loss: 0.00001914
Iteration 28/1000 | Loss: 0.00001914
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001912
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001910
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001907
Iteration 39/1000 | Loss: 0.00001907
Iteration 40/1000 | Loss: 0.00001907
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001904
Iteration 45/1000 | Loss: 0.00001904
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001903
Iteration 51/1000 | Loss: 0.00001903
Iteration 52/1000 | Loss: 0.00001903
Iteration 53/1000 | Loss: 0.00001903
Iteration 54/1000 | Loss: 0.00001903
Iteration 55/1000 | Loss: 0.00001903
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001901
Iteration 67/1000 | Loss: 0.00001901
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001898
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001898
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001897
Iteration 85/1000 | Loss: 0.00001897
Iteration 86/1000 | Loss: 0.00001897
Iteration 87/1000 | Loss: 0.00001897
Iteration 88/1000 | Loss: 0.00001897
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001897
Iteration 93/1000 | Loss: 0.00001897
Iteration 94/1000 | Loss: 0.00001897
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001896
Iteration 98/1000 | Loss: 0.00001896
Iteration 99/1000 | Loss: 0.00001896
Iteration 100/1000 | Loss: 0.00001896
Iteration 101/1000 | Loss: 0.00001896
Iteration 102/1000 | Loss: 0.00001896
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001895
Iteration 108/1000 | Loss: 0.00001895
Iteration 109/1000 | Loss: 0.00001895
Iteration 110/1000 | Loss: 0.00001895
Iteration 111/1000 | Loss: 0.00001895
Iteration 112/1000 | Loss: 0.00001895
Iteration 113/1000 | Loss: 0.00001895
Iteration 114/1000 | Loss: 0.00001895
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001895
Iteration 117/1000 | Loss: 0.00001895
Iteration 118/1000 | Loss: 0.00001895
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001894
Iteration 125/1000 | Loss: 0.00001894
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001894
Iteration 128/1000 | Loss: 0.00001894
Iteration 129/1000 | Loss: 0.00001894
Iteration 130/1000 | Loss: 0.00001894
Iteration 131/1000 | Loss: 0.00001894
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001893
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001893
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001892
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.892361615318805e-05, 1.892361615318805e-05, 1.892361615318805e-05, 1.892361615318805e-05, 1.892361615318805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.892361615318805e-05

Optimization complete. Final v2v error: 3.5875606536865234 mm

Highest mean error: 3.905736207962036 mm for frame 208

Lowest mean error: 3.2732670307159424 mm for frame 7

Saving results

Total time: 41.55741596221924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01204372
Iteration 2/25 | Loss: 0.00500574
Iteration 3/25 | Loss: 0.00326243
Iteration 4/25 | Loss: 0.00277072
Iteration 5/25 | Loss: 0.00239126
Iteration 6/25 | Loss: 0.00204018
Iteration 7/25 | Loss: 0.00185975
Iteration 8/25 | Loss: 0.00182428
Iteration 9/25 | Loss: 0.00179452
Iteration 10/25 | Loss: 0.00175537
Iteration 11/25 | Loss: 0.00176074
Iteration 12/25 | Loss: 0.00176149
Iteration 13/25 | Loss: 0.00173399
Iteration 14/25 | Loss: 0.00172964
Iteration 15/25 | Loss: 0.00171866
Iteration 16/25 | Loss: 0.00173015
Iteration 17/25 | Loss: 0.00170847
Iteration 18/25 | Loss: 0.00170859
Iteration 19/25 | Loss: 0.00170097
Iteration 20/25 | Loss: 0.00169963
Iteration 21/25 | Loss: 0.00169856
Iteration 22/25 | Loss: 0.00169641
Iteration 23/25 | Loss: 0.00169501
Iteration 24/25 | Loss: 0.00169446
Iteration 25/25 | Loss: 0.00169389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52591282
Iteration 2/25 | Loss: 0.04997488
Iteration 3/25 | Loss: 0.00656918
Iteration 4/25 | Loss: 0.00656907
Iteration 5/25 | Loss: 0.00656907
Iteration 6/25 | Loss: 0.00656906
Iteration 7/25 | Loss: 0.00656906
Iteration 8/25 | Loss: 0.00656906
Iteration 9/25 | Loss: 0.00656906
Iteration 10/25 | Loss: 0.00656906
Iteration 11/25 | Loss: 0.00656906
Iteration 12/25 | Loss: 0.00656906
Iteration 13/25 | Loss: 0.00656906
Iteration 14/25 | Loss: 0.00656906
Iteration 15/25 | Loss: 0.00656906
Iteration 16/25 | Loss: 0.00656906
Iteration 17/25 | Loss: 0.00656906
Iteration 18/25 | Loss: 0.00656906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.006569061893969774, 0.006569061893969774, 0.006569061893969774, 0.006569061893969774, 0.006569061893969774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006569061893969774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00656906
Iteration 2/1000 | Loss: 0.01329692
Iteration 3/1000 | Loss: 0.00749508
Iteration 4/1000 | Loss: 0.00086619
Iteration 5/1000 | Loss: 0.00355659
Iteration 6/1000 | Loss: 0.00176218
Iteration 7/1000 | Loss: 0.00047738
Iteration 8/1000 | Loss: 0.00039806
Iteration 9/1000 | Loss: 0.00079334
Iteration 10/1000 | Loss: 0.00756342
Iteration 11/1000 | Loss: 0.01483704
Iteration 12/1000 | Loss: 0.00781598
Iteration 13/1000 | Loss: 0.01849979
Iteration 14/1000 | Loss: 0.00500358
Iteration 15/1000 | Loss: 0.00157419
Iteration 16/1000 | Loss: 0.00103876
Iteration 17/1000 | Loss: 0.00280244
Iteration 18/1000 | Loss: 0.00238519
Iteration 19/1000 | Loss: 0.00454477
Iteration 20/1000 | Loss: 0.00244478
Iteration 21/1000 | Loss: 0.00118891
Iteration 22/1000 | Loss: 0.00068903
Iteration 23/1000 | Loss: 0.00017609
Iteration 24/1000 | Loss: 0.00050332
Iteration 25/1000 | Loss: 0.00027521
Iteration 26/1000 | Loss: 0.00088312
Iteration 27/1000 | Loss: 0.00043733
Iteration 28/1000 | Loss: 0.00038307
Iteration 29/1000 | Loss: 0.00035735
Iteration 30/1000 | Loss: 0.00031476
Iteration 31/1000 | Loss: 0.00026066
Iteration 32/1000 | Loss: 0.00022243
Iteration 33/1000 | Loss: 0.00024387
Iteration 34/1000 | Loss: 0.00009120
Iteration 35/1000 | Loss: 0.00010523
Iteration 36/1000 | Loss: 0.00008958
Iteration 37/1000 | Loss: 0.00020362
Iteration 38/1000 | Loss: 0.00010481
Iteration 39/1000 | Loss: 0.00017566
Iteration 40/1000 | Loss: 0.00015011
Iteration 41/1000 | Loss: 0.00008346
Iteration 42/1000 | Loss: 0.00014288
Iteration 43/1000 | Loss: 0.00006501
Iteration 44/1000 | Loss: 0.00010717
Iteration 45/1000 | Loss: 0.00011701
Iteration 46/1000 | Loss: 0.00009685
Iteration 47/1000 | Loss: 0.00017471
Iteration 48/1000 | Loss: 0.00012033
Iteration 49/1000 | Loss: 0.00015793
Iteration 50/1000 | Loss: 0.00022473
Iteration 51/1000 | Loss: 0.00021365
Iteration 52/1000 | Loss: 0.00019613
Iteration 53/1000 | Loss: 0.00020738
Iteration 54/1000 | Loss: 0.00011644
Iteration 55/1000 | Loss: 0.00009472
Iteration 56/1000 | Loss: 0.00010325
Iteration 57/1000 | Loss: 0.00013305
Iteration 58/1000 | Loss: 0.00008302
Iteration 59/1000 | Loss: 0.00012862
Iteration 60/1000 | Loss: 0.00012405
Iteration 61/1000 | Loss: 0.00007935
Iteration 62/1000 | Loss: 0.00015550
Iteration 63/1000 | Loss: 0.00016468
Iteration 64/1000 | Loss: 0.00019007
Iteration 65/1000 | Loss: 0.00015512
Iteration 66/1000 | Loss: 0.00016938
Iteration 67/1000 | Loss: 0.00027599
Iteration 68/1000 | Loss: 0.00020416
Iteration 69/1000 | Loss: 0.00023011
Iteration 70/1000 | Loss: 0.00022198
Iteration 71/1000 | Loss: 0.00052134
Iteration 72/1000 | Loss: 0.00024761
Iteration 73/1000 | Loss: 0.00029926
Iteration 74/1000 | Loss: 0.00008332
Iteration 75/1000 | Loss: 0.00006260
Iteration 76/1000 | Loss: 0.00005983
Iteration 77/1000 | Loss: 0.00005787
Iteration 78/1000 | Loss: 0.00005670
Iteration 79/1000 | Loss: 0.00005564
Iteration 80/1000 | Loss: 0.00005472
Iteration 81/1000 | Loss: 0.00082457
Iteration 82/1000 | Loss: 0.00039640
Iteration 83/1000 | Loss: 0.00079374
Iteration 84/1000 | Loss: 0.00007863
Iteration 85/1000 | Loss: 0.00006268
Iteration 86/1000 | Loss: 0.00005627
Iteration 87/1000 | Loss: 0.00005356
Iteration 88/1000 | Loss: 0.00005165
Iteration 89/1000 | Loss: 0.00005073
Iteration 90/1000 | Loss: 0.00004997
Iteration 91/1000 | Loss: 0.00004932
Iteration 92/1000 | Loss: 0.00004900
Iteration 93/1000 | Loss: 0.00004877
Iteration 94/1000 | Loss: 0.00004864
Iteration 95/1000 | Loss: 0.00004860
Iteration 96/1000 | Loss: 0.00004859
Iteration 97/1000 | Loss: 0.00004859
Iteration 98/1000 | Loss: 0.00004858
Iteration 99/1000 | Loss: 0.00004858
Iteration 100/1000 | Loss: 0.00004848
Iteration 101/1000 | Loss: 0.00004838
Iteration 102/1000 | Loss: 0.00004832
Iteration 103/1000 | Loss: 0.00004826
Iteration 104/1000 | Loss: 0.00004826
Iteration 105/1000 | Loss: 0.00004826
Iteration 106/1000 | Loss: 0.00004826
Iteration 107/1000 | Loss: 0.00004824
Iteration 108/1000 | Loss: 0.00004822
Iteration 109/1000 | Loss: 0.00004822
Iteration 110/1000 | Loss: 0.00004821
Iteration 111/1000 | Loss: 0.00004821
Iteration 112/1000 | Loss: 0.00004820
Iteration 113/1000 | Loss: 0.00004820
Iteration 114/1000 | Loss: 0.00004820
Iteration 115/1000 | Loss: 0.00004820
Iteration 116/1000 | Loss: 0.00004820
Iteration 117/1000 | Loss: 0.00004820
Iteration 118/1000 | Loss: 0.00004820
Iteration 119/1000 | Loss: 0.00004818
Iteration 120/1000 | Loss: 0.00004818
Iteration 121/1000 | Loss: 0.00004817
Iteration 122/1000 | Loss: 0.00004817
Iteration 123/1000 | Loss: 0.00004817
Iteration 124/1000 | Loss: 0.00004817
Iteration 125/1000 | Loss: 0.00004817
Iteration 126/1000 | Loss: 0.00004817
Iteration 127/1000 | Loss: 0.00004816
Iteration 128/1000 | Loss: 0.00004816
Iteration 129/1000 | Loss: 0.00004816
Iteration 130/1000 | Loss: 0.00004816
Iteration 131/1000 | Loss: 0.00004816
Iteration 132/1000 | Loss: 0.00004816
Iteration 133/1000 | Loss: 0.00004816
Iteration 134/1000 | Loss: 0.00004815
Iteration 135/1000 | Loss: 0.00004815
Iteration 136/1000 | Loss: 0.00004815
Iteration 137/1000 | Loss: 0.00004815
Iteration 138/1000 | Loss: 0.00004815
Iteration 139/1000 | Loss: 0.00004815
Iteration 140/1000 | Loss: 0.00004815
Iteration 141/1000 | Loss: 0.00004814
Iteration 142/1000 | Loss: 0.00004814
Iteration 143/1000 | Loss: 0.00004814
Iteration 144/1000 | Loss: 0.00004814
Iteration 145/1000 | Loss: 0.00004814
Iteration 146/1000 | Loss: 0.00004813
Iteration 147/1000 | Loss: 0.00004813
Iteration 148/1000 | Loss: 0.00004813
Iteration 149/1000 | Loss: 0.00004813
Iteration 150/1000 | Loss: 0.00004812
Iteration 151/1000 | Loss: 0.00004812
Iteration 152/1000 | Loss: 0.00004812
Iteration 153/1000 | Loss: 0.00004811
Iteration 154/1000 | Loss: 0.00004811
Iteration 155/1000 | Loss: 0.00004811
Iteration 156/1000 | Loss: 0.00004810
Iteration 157/1000 | Loss: 0.00004810
Iteration 158/1000 | Loss: 0.00004810
Iteration 159/1000 | Loss: 0.00004810
Iteration 160/1000 | Loss: 0.00004809
Iteration 161/1000 | Loss: 0.00004809
Iteration 162/1000 | Loss: 0.00004808
Iteration 163/1000 | Loss: 0.00004808
Iteration 164/1000 | Loss: 0.00004808
Iteration 165/1000 | Loss: 0.00004807
Iteration 166/1000 | Loss: 0.00004807
Iteration 167/1000 | Loss: 0.00004807
Iteration 168/1000 | Loss: 0.00004807
Iteration 169/1000 | Loss: 0.00004807
Iteration 170/1000 | Loss: 0.00004807
Iteration 171/1000 | Loss: 0.00004806
Iteration 172/1000 | Loss: 0.00004806
Iteration 173/1000 | Loss: 0.00004806
Iteration 174/1000 | Loss: 0.00004806
Iteration 175/1000 | Loss: 0.00004806
Iteration 176/1000 | Loss: 0.00004806
Iteration 177/1000 | Loss: 0.00004805
Iteration 178/1000 | Loss: 0.00004805
Iteration 179/1000 | Loss: 0.00004805
Iteration 180/1000 | Loss: 0.00004805
Iteration 181/1000 | Loss: 0.00004805
Iteration 182/1000 | Loss: 0.00004805
Iteration 183/1000 | Loss: 0.00004805
Iteration 184/1000 | Loss: 0.00004805
Iteration 185/1000 | Loss: 0.00004805
Iteration 186/1000 | Loss: 0.00004804
Iteration 187/1000 | Loss: 0.00004804
Iteration 188/1000 | Loss: 0.00004804
Iteration 189/1000 | Loss: 0.00004804
Iteration 190/1000 | Loss: 0.00004803
Iteration 191/1000 | Loss: 0.00004803
Iteration 192/1000 | Loss: 0.00004803
Iteration 193/1000 | Loss: 0.00004803
Iteration 194/1000 | Loss: 0.00004803
Iteration 195/1000 | Loss: 0.00004803
Iteration 196/1000 | Loss: 0.00004803
Iteration 197/1000 | Loss: 0.00004803
Iteration 198/1000 | Loss: 0.00004803
Iteration 199/1000 | Loss: 0.00004803
Iteration 200/1000 | Loss: 0.00004803
Iteration 201/1000 | Loss: 0.00004803
Iteration 202/1000 | Loss: 0.00004803
Iteration 203/1000 | Loss: 0.00004803
Iteration 204/1000 | Loss: 0.00004803
Iteration 205/1000 | Loss: 0.00004803
Iteration 206/1000 | Loss: 0.00004803
Iteration 207/1000 | Loss: 0.00004803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [4.802917828783393e-05, 4.802917828783393e-05, 4.802917828783393e-05, 4.802917828783393e-05, 4.802917828783393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.802917828783393e-05

Optimization complete. Final v2v error: 5.494716644287109 mm

Highest mean error: 6.330485820770264 mm for frame 135

Lowest mean error: 3.9546396732330322 mm for frame 6

Saving results

Total time: 214.03220438957214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004706
Iteration 2/25 | Loss: 0.00145341
Iteration 3/25 | Loss: 0.00094885
Iteration 4/25 | Loss: 0.00085788
Iteration 5/25 | Loss: 0.00082747
Iteration 6/25 | Loss: 0.00082277
Iteration 7/25 | Loss: 0.00082237
Iteration 8/25 | Loss: 0.00082234
Iteration 9/25 | Loss: 0.00082232
Iteration 10/25 | Loss: 0.00082232
Iteration 11/25 | Loss: 0.00082232
Iteration 12/25 | Loss: 0.00082232
Iteration 13/25 | Loss: 0.00082232
Iteration 14/25 | Loss: 0.00082232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008223241893574595, 0.0008223241893574595, 0.0008223241893574595, 0.0008223241893574595, 0.0008223241893574595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008223241893574595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09142160
Iteration 2/25 | Loss: 0.00035163
Iteration 3/25 | Loss: 0.00035163
Iteration 4/25 | Loss: 0.00035163
Iteration 5/25 | Loss: 0.00035163
Iteration 6/25 | Loss: 0.00035163
Iteration 7/25 | Loss: 0.00035163
Iteration 8/25 | Loss: 0.00035163
Iteration 9/25 | Loss: 0.00035163
Iteration 10/25 | Loss: 0.00035163
Iteration 11/25 | Loss: 0.00035163
Iteration 12/25 | Loss: 0.00035163
Iteration 13/25 | Loss: 0.00035163
Iteration 14/25 | Loss: 0.00035163
Iteration 15/25 | Loss: 0.00035163
Iteration 16/25 | Loss: 0.00035163
Iteration 17/25 | Loss: 0.00035163
Iteration 18/25 | Loss: 0.00035163
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003516290453262627, 0.0003516290453262627, 0.0003516290453262627, 0.0003516290453262627, 0.0003516290453262627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003516290453262627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035163
Iteration 2/1000 | Loss: 0.00002906
Iteration 3/1000 | Loss: 0.00002268
Iteration 4/1000 | Loss: 0.00002133
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001929
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001853
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001814
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001800
Iteration 13/1000 | Loss: 0.00001800
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001792
Iteration 16/1000 | Loss: 0.00001789
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001789
Iteration 19/1000 | Loss: 0.00001789
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001783
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001783
Iteration 29/1000 | Loss: 0.00001783
Iteration 30/1000 | Loss: 0.00001782
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001782
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001782
Iteration 37/1000 | Loss: 0.00001782
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001782
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001781
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001777
Iteration 86/1000 | Loss: 0.00001777
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00001777
Iteration 89/1000 | Loss: 0.00001777
Iteration 90/1000 | Loss: 0.00001777
Iteration 91/1000 | Loss: 0.00001777
Iteration 92/1000 | Loss: 0.00001777
Iteration 93/1000 | Loss: 0.00001777
Iteration 94/1000 | Loss: 0.00001777
Iteration 95/1000 | Loss: 0.00001777
Iteration 96/1000 | Loss: 0.00001777
Iteration 97/1000 | Loss: 0.00001777
Iteration 98/1000 | Loss: 0.00001777
Iteration 99/1000 | Loss: 0.00001777
Iteration 100/1000 | Loss: 0.00001777
Iteration 101/1000 | Loss: 0.00001777
Iteration 102/1000 | Loss: 0.00001777
Iteration 103/1000 | Loss: 0.00001777
Iteration 104/1000 | Loss: 0.00001777
Iteration 105/1000 | Loss: 0.00001777
Iteration 106/1000 | Loss: 0.00001777
Iteration 107/1000 | Loss: 0.00001777
Iteration 108/1000 | Loss: 0.00001777
Iteration 109/1000 | Loss: 0.00001777
Iteration 110/1000 | Loss: 0.00001777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.7771468264982104e-05, 1.7771468264982104e-05, 1.7771468264982104e-05, 1.7771468264982104e-05, 1.7771468264982104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7771468264982104e-05

Optimization complete. Final v2v error: 3.558723211288452 mm

Highest mean error: 3.839776039123535 mm for frame 118

Lowest mean error: 3.410140037536621 mm for frame 87

Saving results

Total time: 31.826722145080566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00697171
Iteration 2/25 | Loss: 0.00152507
Iteration 3/25 | Loss: 0.00103133
Iteration 4/25 | Loss: 0.00099768
Iteration 5/25 | Loss: 0.00098846
Iteration 6/25 | Loss: 0.00098577
Iteration 7/25 | Loss: 0.00098469
Iteration 8/25 | Loss: 0.00098451
Iteration 9/25 | Loss: 0.00098451
Iteration 10/25 | Loss: 0.00098451
Iteration 11/25 | Loss: 0.00098451
Iteration 12/25 | Loss: 0.00098451
Iteration 13/25 | Loss: 0.00098451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009845084277912974, 0.0009845084277912974, 0.0009845084277912974, 0.0009845084277912974, 0.0009845084277912974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009845084277912974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61585259
Iteration 2/25 | Loss: 0.00029265
Iteration 3/25 | Loss: 0.00029265
Iteration 4/25 | Loss: 0.00029265
Iteration 5/25 | Loss: 0.00029265
Iteration 6/25 | Loss: 0.00029265
Iteration 7/25 | Loss: 0.00029265
Iteration 8/25 | Loss: 0.00029265
Iteration 9/25 | Loss: 0.00029265
Iteration 10/25 | Loss: 0.00029265
Iteration 11/25 | Loss: 0.00029265
Iteration 12/25 | Loss: 0.00029265
Iteration 13/25 | Loss: 0.00029265
Iteration 14/25 | Loss: 0.00029265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0002926460874732584, 0.0002926460874732584, 0.0002926460874732584, 0.0002926460874732584, 0.0002926460874732584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002926460874732584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029265
Iteration 2/1000 | Loss: 0.00004647
Iteration 3/1000 | Loss: 0.00003610
Iteration 4/1000 | Loss: 0.00003243
Iteration 5/1000 | Loss: 0.00003150
Iteration 6/1000 | Loss: 0.00003090
Iteration 7/1000 | Loss: 0.00003017
Iteration 8/1000 | Loss: 0.00002952
Iteration 9/1000 | Loss: 0.00002909
Iteration 10/1000 | Loss: 0.00002876
Iteration 11/1000 | Loss: 0.00002842
Iteration 12/1000 | Loss: 0.00002818
Iteration 13/1000 | Loss: 0.00002806
Iteration 14/1000 | Loss: 0.00002795
Iteration 15/1000 | Loss: 0.00002784
Iteration 16/1000 | Loss: 0.00002783
Iteration 17/1000 | Loss: 0.00002780
Iteration 18/1000 | Loss: 0.00002779
Iteration 19/1000 | Loss: 0.00002777
Iteration 20/1000 | Loss: 0.00002776
Iteration 21/1000 | Loss: 0.00002776
Iteration 22/1000 | Loss: 0.00002775
Iteration 23/1000 | Loss: 0.00002762
Iteration 24/1000 | Loss: 0.00002756
Iteration 25/1000 | Loss: 0.00002756
Iteration 26/1000 | Loss: 0.00002754
Iteration 27/1000 | Loss: 0.00002753
Iteration 28/1000 | Loss: 0.00002752
Iteration 29/1000 | Loss: 0.00002751
Iteration 30/1000 | Loss: 0.00002748
Iteration 31/1000 | Loss: 0.00002746
Iteration 32/1000 | Loss: 0.00002746
Iteration 33/1000 | Loss: 0.00002746
Iteration 34/1000 | Loss: 0.00002745
Iteration 35/1000 | Loss: 0.00002745
Iteration 36/1000 | Loss: 0.00002745
Iteration 37/1000 | Loss: 0.00002744
Iteration 38/1000 | Loss: 0.00002741
Iteration 39/1000 | Loss: 0.00002741
Iteration 40/1000 | Loss: 0.00002741
Iteration 41/1000 | Loss: 0.00002740
Iteration 42/1000 | Loss: 0.00002740
Iteration 43/1000 | Loss: 0.00002740
Iteration 44/1000 | Loss: 0.00002740
Iteration 45/1000 | Loss: 0.00002740
Iteration 46/1000 | Loss: 0.00002740
Iteration 47/1000 | Loss: 0.00002740
Iteration 48/1000 | Loss: 0.00002740
Iteration 49/1000 | Loss: 0.00002740
Iteration 50/1000 | Loss: 0.00002740
Iteration 51/1000 | Loss: 0.00002740
Iteration 52/1000 | Loss: 0.00002739
Iteration 53/1000 | Loss: 0.00002739
Iteration 54/1000 | Loss: 0.00002739
Iteration 55/1000 | Loss: 0.00002739
Iteration 56/1000 | Loss: 0.00002739
Iteration 57/1000 | Loss: 0.00002739
Iteration 58/1000 | Loss: 0.00002739
Iteration 59/1000 | Loss: 0.00002739
Iteration 60/1000 | Loss: 0.00002738
Iteration 61/1000 | Loss: 0.00002738
Iteration 62/1000 | Loss: 0.00002738
Iteration 63/1000 | Loss: 0.00002738
Iteration 64/1000 | Loss: 0.00002737
Iteration 65/1000 | Loss: 0.00002737
Iteration 66/1000 | Loss: 0.00002736
Iteration 67/1000 | Loss: 0.00002736
Iteration 68/1000 | Loss: 0.00002736
Iteration 69/1000 | Loss: 0.00002736
Iteration 70/1000 | Loss: 0.00002735
Iteration 71/1000 | Loss: 0.00002733
Iteration 72/1000 | Loss: 0.00002732
Iteration 73/1000 | Loss: 0.00002732
Iteration 74/1000 | Loss: 0.00002731
Iteration 75/1000 | Loss: 0.00002731
Iteration 76/1000 | Loss: 0.00002729
Iteration 77/1000 | Loss: 0.00002729
Iteration 78/1000 | Loss: 0.00002729
Iteration 79/1000 | Loss: 0.00002728
Iteration 80/1000 | Loss: 0.00002728
Iteration 81/1000 | Loss: 0.00002728
Iteration 82/1000 | Loss: 0.00002728
Iteration 83/1000 | Loss: 0.00002728
Iteration 84/1000 | Loss: 0.00002728
Iteration 85/1000 | Loss: 0.00002728
Iteration 86/1000 | Loss: 0.00002728
Iteration 87/1000 | Loss: 0.00002728
Iteration 88/1000 | Loss: 0.00002728
Iteration 89/1000 | Loss: 0.00002728
Iteration 90/1000 | Loss: 0.00002728
Iteration 91/1000 | Loss: 0.00002727
Iteration 92/1000 | Loss: 0.00002727
Iteration 93/1000 | Loss: 0.00002727
Iteration 94/1000 | Loss: 0.00002727
Iteration 95/1000 | Loss: 0.00002727
Iteration 96/1000 | Loss: 0.00002724
Iteration 97/1000 | Loss: 0.00002724
Iteration 98/1000 | Loss: 0.00002724
Iteration 99/1000 | Loss: 0.00002723
Iteration 100/1000 | Loss: 0.00002723
Iteration 101/1000 | Loss: 0.00002723
Iteration 102/1000 | Loss: 0.00002723
Iteration 103/1000 | Loss: 0.00002723
Iteration 104/1000 | Loss: 0.00002723
Iteration 105/1000 | Loss: 0.00002723
Iteration 106/1000 | Loss: 0.00002723
Iteration 107/1000 | Loss: 0.00002723
Iteration 108/1000 | Loss: 0.00002722
Iteration 109/1000 | Loss: 0.00002722
Iteration 110/1000 | Loss: 0.00002722
Iteration 111/1000 | Loss: 0.00002722
Iteration 112/1000 | Loss: 0.00002722
Iteration 113/1000 | Loss: 0.00002722
Iteration 114/1000 | Loss: 0.00002722
Iteration 115/1000 | Loss: 0.00002722
Iteration 116/1000 | Loss: 0.00002722
Iteration 117/1000 | Loss: 0.00002721
Iteration 118/1000 | Loss: 0.00002721
Iteration 119/1000 | Loss: 0.00002721
Iteration 120/1000 | Loss: 0.00002721
Iteration 121/1000 | Loss: 0.00002721
Iteration 122/1000 | Loss: 0.00002721
Iteration 123/1000 | Loss: 0.00002721
Iteration 124/1000 | Loss: 0.00002721
Iteration 125/1000 | Loss: 0.00002720
Iteration 126/1000 | Loss: 0.00002720
Iteration 127/1000 | Loss: 0.00002720
Iteration 128/1000 | Loss: 0.00002720
Iteration 129/1000 | Loss: 0.00002720
Iteration 130/1000 | Loss: 0.00002720
Iteration 131/1000 | Loss: 0.00002719
Iteration 132/1000 | Loss: 0.00002719
Iteration 133/1000 | Loss: 0.00002717
Iteration 134/1000 | Loss: 0.00002717
Iteration 135/1000 | Loss: 0.00002717
Iteration 136/1000 | Loss: 0.00002717
Iteration 137/1000 | Loss: 0.00002717
Iteration 138/1000 | Loss: 0.00002717
Iteration 139/1000 | Loss: 0.00002716
Iteration 140/1000 | Loss: 0.00002716
Iteration 141/1000 | Loss: 0.00002716
Iteration 142/1000 | Loss: 0.00002716
Iteration 143/1000 | Loss: 0.00002716
Iteration 144/1000 | Loss: 0.00002716
Iteration 145/1000 | Loss: 0.00002716
Iteration 146/1000 | Loss: 0.00002716
Iteration 147/1000 | Loss: 0.00002715
Iteration 148/1000 | Loss: 0.00002715
Iteration 149/1000 | Loss: 0.00002715
Iteration 150/1000 | Loss: 0.00002715
Iteration 151/1000 | Loss: 0.00002715
Iteration 152/1000 | Loss: 0.00002715
Iteration 153/1000 | Loss: 0.00002715
Iteration 154/1000 | Loss: 0.00002715
Iteration 155/1000 | Loss: 0.00002715
Iteration 156/1000 | Loss: 0.00002714
Iteration 157/1000 | Loss: 0.00002714
Iteration 158/1000 | Loss: 0.00002714
Iteration 159/1000 | Loss: 0.00002714
Iteration 160/1000 | Loss: 0.00002714
Iteration 161/1000 | Loss: 0.00002714
Iteration 162/1000 | Loss: 0.00002714
Iteration 163/1000 | Loss: 0.00002714
Iteration 164/1000 | Loss: 0.00002714
Iteration 165/1000 | Loss: 0.00002714
Iteration 166/1000 | Loss: 0.00002714
Iteration 167/1000 | Loss: 0.00002714
Iteration 168/1000 | Loss: 0.00002714
Iteration 169/1000 | Loss: 0.00002714
Iteration 170/1000 | Loss: 0.00002714
Iteration 171/1000 | Loss: 0.00002714
Iteration 172/1000 | Loss: 0.00002714
Iteration 173/1000 | Loss: 0.00002714
Iteration 174/1000 | Loss: 0.00002714
Iteration 175/1000 | Loss: 0.00002714
Iteration 176/1000 | Loss: 0.00002714
Iteration 177/1000 | Loss: 0.00002714
Iteration 178/1000 | Loss: 0.00002714
Iteration 179/1000 | Loss: 0.00002714
Iteration 180/1000 | Loss: 0.00002714
Iteration 181/1000 | Loss: 0.00002714
Iteration 182/1000 | Loss: 0.00002714
Iteration 183/1000 | Loss: 0.00002714
Iteration 184/1000 | Loss: 0.00002714
Iteration 185/1000 | Loss: 0.00002714
Iteration 186/1000 | Loss: 0.00002714
Iteration 187/1000 | Loss: 0.00002714
Iteration 188/1000 | Loss: 0.00002714
Iteration 189/1000 | Loss: 0.00002714
Iteration 190/1000 | Loss: 0.00002714
Iteration 191/1000 | Loss: 0.00002714
Iteration 192/1000 | Loss: 0.00002714
Iteration 193/1000 | Loss: 0.00002714
Iteration 194/1000 | Loss: 0.00002714
Iteration 195/1000 | Loss: 0.00002714
Iteration 196/1000 | Loss: 0.00002714
Iteration 197/1000 | Loss: 0.00002714
Iteration 198/1000 | Loss: 0.00002714
Iteration 199/1000 | Loss: 0.00002714
Iteration 200/1000 | Loss: 0.00002714
Iteration 201/1000 | Loss: 0.00002714
Iteration 202/1000 | Loss: 0.00002714
Iteration 203/1000 | Loss: 0.00002714
Iteration 204/1000 | Loss: 0.00002714
Iteration 205/1000 | Loss: 0.00002714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.71353310381528e-05, 2.71353310381528e-05, 2.71353310381528e-05, 2.71353310381528e-05, 2.71353310381528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.71353310381528e-05

Optimization complete. Final v2v error: 4.180703163146973 mm

Highest mean error: 4.618777275085449 mm for frame 92

Lowest mean error: 3.75433349609375 mm for frame 27

Saving results

Total time: 45.8065824508667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091001
Iteration 2/25 | Loss: 0.01091001
Iteration 3/25 | Loss: 0.01091000
Iteration 4/25 | Loss: 0.01091000
Iteration 5/25 | Loss: 0.01091000
Iteration 6/25 | Loss: 0.01091000
Iteration 7/25 | Loss: 0.01091000
Iteration 8/25 | Loss: 0.01091000
Iteration 9/25 | Loss: 0.01091000
Iteration 10/25 | Loss: 0.01091000
Iteration 11/25 | Loss: 0.01091000
Iteration 12/25 | Loss: 0.01091000
Iteration 13/25 | Loss: 0.01091000
Iteration 14/25 | Loss: 0.01091000
Iteration 15/25 | Loss: 0.01090999
Iteration 16/25 | Loss: 0.01090999
Iteration 17/25 | Loss: 0.01090999
Iteration 18/25 | Loss: 0.01090999
Iteration 19/25 | Loss: 0.01090999
Iteration 20/25 | Loss: 0.01090999
Iteration 21/25 | Loss: 0.01090999
Iteration 22/25 | Loss: 0.01090999
Iteration 23/25 | Loss: 0.01090999
Iteration 24/25 | Loss: 0.01090999
Iteration 25/25 | Loss: 0.01090998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77891874
Iteration 2/25 | Loss: 0.07658809
Iteration 3/25 | Loss: 0.07657161
Iteration 4/25 | Loss: 0.07657158
Iteration 5/25 | Loss: 0.07657157
Iteration 6/25 | Loss: 0.07657157
Iteration 7/25 | Loss: 0.07657156
Iteration 8/25 | Loss: 0.07657156
Iteration 9/25 | Loss: 0.07657155
Iteration 10/25 | Loss: 0.07657155
Iteration 11/25 | Loss: 0.07657155
Iteration 12/25 | Loss: 0.07657155
Iteration 13/25 | Loss: 0.07657155
Iteration 14/25 | Loss: 0.07657155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.07657154649496078, 0.07657154649496078, 0.07657154649496078, 0.07657154649496078, 0.07657154649496078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07657154649496078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07657155
Iteration 2/1000 | Loss: 0.00051309
Iteration 3/1000 | Loss: 0.00012159
Iteration 4/1000 | Loss: 0.00005266
Iteration 5/1000 | Loss: 0.00003274
Iteration 6/1000 | Loss: 0.00002912
Iteration 7/1000 | Loss: 0.00002569
Iteration 8/1000 | Loss: 0.00002363
Iteration 9/1000 | Loss: 0.00002170
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001846
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001655
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001398
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001295
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001172
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001152
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001147
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001141
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001133
Iteration 123/1000 | Loss: 0.00001133
Iteration 124/1000 | Loss: 0.00001133
Iteration 125/1000 | Loss: 0.00001133
Iteration 126/1000 | Loss: 0.00001133
Iteration 127/1000 | Loss: 0.00001133
Iteration 128/1000 | Loss: 0.00001133
Iteration 129/1000 | Loss: 0.00001133
Iteration 130/1000 | Loss: 0.00001133
Iteration 131/1000 | Loss: 0.00001133
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001133
Iteration 134/1000 | Loss: 0.00001133
Iteration 135/1000 | Loss: 0.00001133
Iteration 136/1000 | Loss: 0.00001133
Iteration 137/1000 | Loss: 0.00001133
Iteration 138/1000 | Loss: 0.00001133
Iteration 139/1000 | Loss: 0.00001133
Iteration 140/1000 | Loss: 0.00001133
Iteration 141/1000 | Loss: 0.00001133
Iteration 142/1000 | Loss: 0.00001133
Iteration 143/1000 | Loss: 0.00001133
Iteration 144/1000 | Loss: 0.00001133
Iteration 145/1000 | Loss: 0.00001133
Iteration 146/1000 | Loss: 0.00001133
Iteration 147/1000 | Loss: 0.00001133
Iteration 148/1000 | Loss: 0.00001133
Iteration 149/1000 | Loss: 0.00001133
Iteration 150/1000 | Loss: 0.00001133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.1327650099701714e-05, 1.1327650099701714e-05, 1.1327650099701714e-05, 1.1327650099701714e-05, 1.1327650099701714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1327650099701714e-05

Optimization complete. Final v2v error: 2.8834822177886963 mm

Highest mean error: 3.0615127086639404 mm for frame 1

Lowest mean error: 2.7149879932403564 mm for frame 185

Saving results

Total time: 47.35927605628967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067506
Iteration 2/25 | Loss: 0.00287460
Iteration 3/25 | Loss: 0.00159346
Iteration 4/25 | Loss: 0.00149629
Iteration 5/25 | Loss: 0.00122588
Iteration 6/25 | Loss: 0.00106600
Iteration 7/25 | Loss: 0.00110677
Iteration 8/25 | Loss: 0.00119218
Iteration 9/25 | Loss: 0.00105262
Iteration 10/25 | Loss: 0.00097884
Iteration 11/25 | Loss: 0.00093374
Iteration 12/25 | Loss: 0.00089373
Iteration 13/25 | Loss: 0.00087652
Iteration 14/25 | Loss: 0.00086914
Iteration 15/25 | Loss: 0.00084939
Iteration 16/25 | Loss: 0.00086908
Iteration 17/25 | Loss: 0.00084851
Iteration 18/25 | Loss: 0.00083974
Iteration 19/25 | Loss: 0.00083235
Iteration 20/25 | Loss: 0.00083067
Iteration 21/25 | Loss: 0.00083802
Iteration 22/25 | Loss: 0.00082615
Iteration 23/25 | Loss: 0.00082359
Iteration 24/25 | Loss: 0.00082807
Iteration 25/25 | Loss: 0.00083052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55797458
Iteration 2/25 | Loss: 0.00141467
Iteration 3/25 | Loss: 0.00097751
Iteration 4/25 | Loss: 0.00097751
Iteration 5/25 | Loss: 0.00097751
Iteration 6/25 | Loss: 0.00097751
Iteration 7/25 | Loss: 0.00097751
Iteration 8/25 | Loss: 0.00097751
Iteration 9/25 | Loss: 0.00097751
Iteration 10/25 | Loss: 0.00097751
Iteration 11/25 | Loss: 0.00097751
Iteration 12/25 | Loss: 0.00097751
Iteration 13/25 | Loss: 0.00097751
Iteration 14/25 | Loss: 0.00097751
Iteration 15/25 | Loss: 0.00097751
Iteration 16/25 | Loss: 0.00097751
Iteration 17/25 | Loss: 0.00097751
Iteration 18/25 | Loss: 0.00097751
Iteration 19/25 | Loss: 0.00097751
Iteration 20/25 | Loss: 0.00097751
Iteration 21/25 | Loss: 0.00097751
Iteration 22/25 | Loss: 0.00097751
Iteration 23/25 | Loss: 0.00097751
Iteration 24/25 | Loss: 0.00097751
Iteration 25/25 | Loss: 0.00097751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097751
Iteration 2/1000 | Loss: 0.00129059
Iteration 3/1000 | Loss: 0.00059296
Iteration 4/1000 | Loss: 0.00084468
Iteration 5/1000 | Loss: 0.00057187
Iteration 6/1000 | Loss: 0.00056626
Iteration 7/1000 | Loss: 0.00077340
Iteration 8/1000 | Loss: 0.00064935
Iteration 9/1000 | Loss: 0.00042604
Iteration 10/1000 | Loss: 0.00058088
Iteration 11/1000 | Loss: 0.00039624
Iteration 12/1000 | Loss: 0.00032769
Iteration 13/1000 | Loss: 0.00056389
Iteration 14/1000 | Loss: 0.00042595
Iteration 15/1000 | Loss: 0.00032562
Iteration 16/1000 | Loss: 0.00069955
Iteration 17/1000 | Loss: 0.00100026
Iteration 18/1000 | Loss: 0.00060316
Iteration 19/1000 | Loss: 0.00083009
Iteration 20/1000 | Loss: 0.00050726
Iteration 21/1000 | Loss: 0.00036287
Iteration 22/1000 | Loss: 0.00065450
Iteration 23/1000 | Loss: 0.00060839
Iteration 24/1000 | Loss: 0.00057266
Iteration 25/1000 | Loss: 0.00170814
Iteration 26/1000 | Loss: 0.00140394
Iteration 27/1000 | Loss: 0.00062061
Iteration 28/1000 | Loss: 0.00094729
Iteration 29/1000 | Loss: 0.00086557
Iteration 30/1000 | Loss: 0.00042015
Iteration 31/1000 | Loss: 0.00078771
Iteration 32/1000 | Loss: 0.00204589
Iteration 33/1000 | Loss: 0.00104401
Iteration 34/1000 | Loss: 0.00052224
Iteration 35/1000 | Loss: 0.00062262
Iteration 36/1000 | Loss: 0.00038344
Iteration 37/1000 | Loss: 0.00100738
Iteration 38/1000 | Loss: 0.00069480
Iteration 39/1000 | Loss: 0.00058923
Iteration 40/1000 | Loss: 0.00074476
Iteration 41/1000 | Loss: 0.00030609
Iteration 42/1000 | Loss: 0.00063546
Iteration 43/1000 | Loss: 0.00040314
Iteration 44/1000 | Loss: 0.00046694
Iteration 45/1000 | Loss: 0.00069853
Iteration 46/1000 | Loss: 0.00024929
Iteration 47/1000 | Loss: 0.00047469
Iteration 48/1000 | Loss: 0.00062168
Iteration 49/1000 | Loss: 0.00070230
Iteration 50/1000 | Loss: 0.00127633
Iteration 51/1000 | Loss: 0.00045498
Iteration 52/1000 | Loss: 0.00051099
Iteration 53/1000 | Loss: 0.00032372
Iteration 54/1000 | Loss: 0.00024374
Iteration 55/1000 | Loss: 0.00022484
Iteration 56/1000 | Loss: 0.00012820
Iteration 57/1000 | Loss: 0.00022854
Iteration 58/1000 | Loss: 0.00036568
Iteration 59/1000 | Loss: 0.00034226
Iteration 60/1000 | Loss: 0.00103724
Iteration 61/1000 | Loss: 0.00068620
Iteration 62/1000 | Loss: 0.00043930
Iteration 63/1000 | Loss: 0.00040155
Iteration 64/1000 | Loss: 0.00038252
Iteration 65/1000 | Loss: 0.00045671
Iteration 66/1000 | Loss: 0.00035373
Iteration 67/1000 | Loss: 0.00023080
Iteration 68/1000 | Loss: 0.00104154
Iteration 69/1000 | Loss: 0.00109228
Iteration 70/1000 | Loss: 0.00101603
Iteration 71/1000 | Loss: 0.00049069
Iteration 72/1000 | Loss: 0.00071498
Iteration 73/1000 | Loss: 0.00061646
Iteration 74/1000 | Loss: 0.00035618
Iteration 75/1000 | Loss: 0.00024076
Iteration 76/1000 | Loss: 0.00023453
Iteration 77/1000 | Loss: 0.00036699
Iteration 78/1000 | Loss: 0.00037801
Iteration 79/1000 | Loss: 0.00030071
Iteration 80/1000 | Loss: 0.00037929
Iteration 81/1000 | Loss: 0.00036148
Iteration 82/1000 | Loss: 0.00055658
Iteration 83/1000 | Loss: 0.00044892
Iteration 84/1000 | Loss: 0.00046105
Iteration 85/1000 | Loss: 0.00062779
Iteration 86/1000 | Loss: 0.00042531
Iteration 87/1000 | Loss: 0.00040222
Iteration 88/1000 | Loss: 0.00021349
Iteration 89/1000 | Loss: 0.00039660
Iteration 90/1000 | Loss: 0.00046004
Iteration 91/1000 | Loss: 0.00005260
Iteration 92/1000 | Loss: 0.00004366
Iteration 93/1000 | Loss: 0.00033350
Iteration 94/1000 | Loss: 0.00029085
Iteration 95/1000 | Loss: 0.00005851
Iteration 96/1000 | Loss: 0.00019097
Iteration 97/1000 | Loss: 0.00018145
Iteration 98/1000 | Loss: 0.00008206
Iteration 99/1000 | Loss: 0.00003315
Iteration 100/1000 | Loss: 0.00044504
Iteration 101/1000 | Loss: 0.00019235
Iteration 102/1000 | Loss: 0.00020469
Iteration 103/1000 | Loss: 0.00037298
Iteration 104/1000 | Loss: 0.00026628
Iteration 105/1000 | Loss: 0.00002911
Iteration 106/1000 | Loss: 0.00004414
Iteration 107/1000 | Loss: 0.00055652
Iteration 108/1000 | Loss: 0.00009186
Iteration 109/1000 | Loss: 0.00074219
Iteration 110/1000 | Loss: 0.00040571
Iteration 111/1000 | Loss: 0.00021384
Iteration 112/1000 | Loss: 0.00003961
Iteration 113/1000 | Loss: 0.00023833
Iteration 114/1000 | Loss: 0.00002735
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00008476
Iteration 117/1000 | Loss: 0.00002913
Iteration 118/1000 | Loss: 0.00001901
Iteration 119/1000 | Loss: 0.00001771
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001609
Iteration 122/1000 | Loss: 0.00001569
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001531
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001483
Iteration 130/1000 | Loss: 0.00001482
Iteration 131/1000 | Loss: 0.00001481
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001473
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001469
Iteration 140/1000 | Loss: 0.00001469
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Iteration 146/1000 | Loss: 0.00001468
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001467
Iteration 154/1000 | Loss: 0.00001467
Iteration 155/1000 | Loss: 0.00001467
Iteration 156/1000 | Loss: 0.00001466
Iteration 157/1000 | Loss: 0.00001466
Iteration 158/1000 | Loss: 0.00001466
Iteration 159/1000 | Loss: 0.00001465
Iteration 160/1000 | Loss: 0.00001465
Iteration 161/1000 | Loss: 0.00001465
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001462
Iteration 167/1000 | Loss: 0.00001462
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001461
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001460
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001460
Iteration 175/1000 | Loss: 0.00001458
Iteration 176/1000 | Loss: 0.00001458
Iteration 177/1000 | Loss: 0.00001458
Iteration 178/1000 | Loss: 0.00001458
Iteration 179/1000 | Loss: 0.00001458
Iteration 180/1000 | Loss: 0.00001458
Iteration 181/1000 | Loss: 0.00001457
Iteration 182/1000 | Loss: 0.00001457
Iteration 183/1000 | Loss: 0.00001457
Iteration 184/1000 | Loss: 0.00001457
Iteration 185/1000 | Loss: 0.00001457
Iteration 186/1000 | Loss: 0.00001457
Iteration 187/1000 | Loss: 0.00001457
Iteration 188/1000 | Loss: 0.00001457
Iteration 189/1000 | Loss: 0.00001457
Iteration 190/1000 | Loss: 0.00001457
Iteration 191/1000 | Loss: 0.00001457
Iteration 192/1000 | Loss: 0.00001457
Iteration 193/1000 | Loss: 0.00001457
Iteration 194/1000 | Loss: 0.00001456
Iteration 195/1000 | Loss: 0.00001456
Iteration 196/1000 | Loss: 0.00001456
Iteration 197/1000 | Loss: 0.00001456
Iteration 198/1000 | Loss: 0.00001456
Iteration 199/1000 | Loss: 0.00001456
Iteration 200/1000 | Loss: 0.00001456
Iteration 201/1000 | Loss: 0.00001456
Iteration 202/1000 | Loss: 0.00001456
Iteration 203/1000 | Loss: 0.00001456
Iteration 204/1000 | Loss: 0.00001456
Iteration 205/1000 | Loss: 0.00001456
Iteration 206/1000 | Loss: 0.00001456
Iteration 207/1000 | Loss: 0.00001456
Iteration 208/1000 | Loss: 0.00001456
Iteration 209/1000 | Loss: 0.00001456
Iteration 210/1000 | Loss: 0.00001455
Iteration 211/1000 | Loss: 0.00001455
Iteration 212/1000 | Loss: 0.00001455
Iteration 213/1000 | Loss: 0.00001455
Iteration 214/1000 | Loss: 0.00001455
Iteration 215/1000 | Loss: 0.00001455
Iteration 216/1000 | Loss: 0.00001455
Iteration 217/1000 | Loss: 0.00001455
Iteration 218/1000 | Loss: 0.00001455
Iteration 219/1000 | Loss: 0.00001455
Iteration 220/1000 | Loss: 0.00001455
Iteration 221/1000 | Loss: 0.00001455
Iteration 222/1000 | Loss: 0.00001455
Iteration 223/1000 | Loss: 0.00001455
Iteration 224/1000 | Loss: 0.00001454
Iteration 225/1000 | Loss: 0.00001454
Iteration 226/1000 | Loss: 0.00001454
Iteration 227/1000 | Loss: 0.00001454
Iteration 228/1000 | Loss: 0.00001454
Iteration 229/1000 | Loss: 0.00001454
Iteration 230/1000 | Loss: 0.00001454
Iteration 231/1000 | Loss: 0.00001454
Iteration 232/1000 | Loss: 0.00001454
Iteration 233/1000 | Loss: 0.00001454
Iteration 234/1000 | Loss: 0.00001454
Iteration 235/1000 | Loss: 0.00001454
Iteration 236/1000 | Loss: 0.00001454
Iteration 237/1000 | Loss: 0.00001454
Iteration 238/1000 | Loss: 0.00001454
Iteration 239/1000 | Loss: 0.00001453
Iteration 240/1000 | Loss: 0.00001453
Iteration 241/1000 | Loss: 0.00001453
Iteration 242/1000 | Loss: 0.00001453
Iteration 243/1000 | Loss: 0.00001453
Iteration 244/1000 | Loss: 0.00001453
Iteration 245/1000 | Loss: 0.00001453
Iteration 246/1000 | Loss: 0.00001453
Iteration 247/1000 | Loss: 0.00001453
Iteration 248/1000 | Loss: 0.00001453
Iteration 249/1000 | Loss: 0.00001453
Iteration 250/1000 | Loss: 0.00001453
Iteration 251/1000 | Loss: 0.00001453
Iteration 252/1000 | Loss: 0.00001453
Iteration 253/1000 | Loss: 0.00001453
Iteration 254/1000 | Loss: 0.00001453
Iteration 255/1000 | Loss: 0.00001453
Iteration 256/1000 | Loss: 0.00001453
Iteration 257/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.4533536159433424e-05, 1.4533536159433424e-05, 1.4533536159433424e-05, 1.4533536159433424e-05, 1.4533536159433424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4533536159433424e-05

Optimization complete. Final v2v error: 3.208395004272461 mm

Highest mean error: 4.09199857711792 mm for frame 57

Lowest mean error: 2.911909580230713 mm for frame 12

Saving results

Total time: 225.7184510231018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749798
Iteration 2/25 | Loss: 0.00093787
Iteration 3/25 | Loss: 0.00078688
Iteration 4/25 | Loss: 0.00075122
Iteration 5/25 | Loss: 0.00074570
Iteration 6/25 | Loss: 0.00074495
Iteration 7/25 | Loss: 0.00074495
Iteration 8/25 | Loss: 0.00074495
Iteration 9/25 | Loss: 0.00074495
Iteration 10/25 | Loss: 0.00074495
Iteration 11/25 | Loss: 0.00074495
Iteration 12/25 | Loss: 0.00074495
Iteration 13/25 | Loss: 0.00074495
Iteration 14/25 | Loss: 0.00074495
Iteration 15/25 | Loss: 0.00074495
Iteration 16/25 | Loss: 0.00074495
Iteration 17/25 | Loss: 0.00074495
Iteration 18/25 | Loss: 0.00074495
Iteration 19/25 | Loss: 0.00074495
Iteration 20/25 | Loss: 0.00074495
Iteration 21/25 | Loss: 0.00074495
Iteration 22/25 | Loss: 0.00074495
Iteration 23/25 | Loss: 0.00074495
Iteration 24/25 | Loss: 0.00074495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007449485128745437, 0.0007449485128745437, 0.0007449485128745437, 0.0007449485128745437, 0.0007449485128745437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007449485128745437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48680246
Iteration 2/25 | Loss: 0.00022420
Iteration 3/25 | Loss: 0.00022413
Iteration 4/25 | Loss: 0.00022413
Iteration 5/25 | Loss: 0.00022413
Iteration 6/25 | Loss: 0.00022413
Iteration 7/25 | Loss: 0.00022413
Iteration 8/25 | Loss: 0.00022413
Iteration 9/25 | Loss: 0.00022413
Iteration 10/25 | Loss: 0.00022413
Iteration 11/25 | Loss: 0.00022413
Iteration 12/25 | Loss: 0.00022413
Iteration 13/25 | Loss: 0.00022413
Iteration 14/25 | Loss: 0.00022413
Iteration 15/25 | Loss: 0.00022413
Iteration 16/25 | Loss: 0.00022413
Iteration 17/25 | Loss: 0.00022413
Iteration 18/25 | Loss: 0.00022413
Iteration 19/25 | Loss: 0.00022413
Iteration 20/25 | Loss: 0.00022413
Iteration 21/25 | Loss: 0.00022413
Iteration 22/25 | Loss: 0.00022413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00022413002443499863, 0.00022413002443499863, 0.00022413002443499863, 0.00022413002443499863, 0.00022413002443499863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022413002443499863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022413
Iteration 2/1000 | Loss: 0.00002548
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001506
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001350
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001333
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001324
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001323
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001305
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001296
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001295
Iteration 65/1000 | Loss: 0.00001295
Iteration 66/1000 | Loss: 0.00001295
Iteration 67/1000 | Loss: 0.00001295
Iteration 68/1000 | Loss: 0.00001295
Iteration 69/1000 | Loss: 0.00001294
Iteration 70/1000 | Loss: 0.00001294
Iteration 71/1000 | Loss: 0.00001294
Iteration 72/1000 | Loss: 0.00001294
Iteration 73/1000 | Loss: 0.00001294
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001293
Iteration 77/1000 | Loss: 0.00001293
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001292
Iteration 80/1000 | Loss: 0.00001292
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001292
Iteration 83/1000 | Loss: 0.00001292
Iteration 84/1000 | Loss: 0.00001292
Iteration 85/1000 | Loss: 0.00001292
Iteration 86/1000 | Loss: 0.00001292
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001292
Iteration 96/1000 | Loss: 0.00001292
Iteration 97/1000 | Loss: 0.00001292
Iteration 98/1000 | Loss: 0.00001292
Iteration 99/1000 | Loss: 0.00001292
Iteration 100/1000 | Loss: 0.00001292
Iteration 101/1000 | Loss: 0.00001292
Iteration 102/1000 | Loss: 0.00001292
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001292
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001292
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.2915042134409305e-05, 1.2915042134409305e-05, 1.2915042134409305e-05, 1.2915042134409305e-05, 1.2915042134409305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2915042134409305e-05

Optimization complete. Final v2v error: 3.09089732170105 mm

Highest mean error: 3.3251895904541016 mm for frame 72

Lowest mean error: 2.879399299621582 mm for frame 155

Saving results

Total time: 38.516639709472656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987761
Iteration 2/25 | Loss: 0.00537149
Iteration 3/25 | Loss: 0.00363545
Iteration 4/25 | Loss: 0.00286197
Iteration 5/25 | Loss: 0.00234005
Iteration 6/25 | Loss: 0.00203888
Iteration 7/25 | Loss: 0.00192933
Iteration 8/25 | Loss: 0.00180292
Iteration 9/25 | Loss: 0.00176523
Iteration 10/25 | Loss: 0.00174069
Iteration 11/25 | Loss: 0.00176489
Iteration 12/25 | Loss: 0.00162648
Iteration 13/25 | Loss: 0.00151975
Iteration 14/25 | Loss: 0.00146597
Iteration 15/25 | Loss: 0.00146087
Iteration 16/25 | Loss: 0.00145440
Iteration 17/25 | Loss: 0.00145369
Iteration 18/25 | Loss: 0.00144312
Iteration 19/25 | Loss: 0.00142750
Iteration 20/25 | Loss: 0.00141326
Iteration 21/25 | Loss: 0.00141348
Iteration 22/25 | Loss: 0.00140940
Iteration 23/25 | Loss: 0.00140067
Iteration 24/25 | Loss: 0.00139796
Iteration 25/25 | Loss: 0.00139728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46390212
Iteration 2/25 | Loss: 0.00683425
Iteration 3/25 | Loss: 0.00668910
Iteration 4/25 | Loss: 0.00668910
Iteration 5/25 | Loss: 0.00668909
Iteration 6/25 | Loss: 0.00668909
Iteration 7/25 | Loss: 0.00668909
Iteration 8/25 | Loss: 0.00668909
Iteration 9/25 | Loss: 0.00668909
Iteration 10/25 | Loss: 0.00668909
Iteration 11/25 | Loss: 0.00668909
Iteration 12/25 | Loss: 0.00668909
Iteration 13/25 | Loss: 0.00668909
Iteration 14/25 | Loss: 0.00668909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.006689092610031366, 0.006689092610031366, 0.006689092610031366, 0.006689092610031366, 0.006689092610031366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006689092610031366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00668909
Iteration 2/1000 | Loss: 0.00214116
Iteration 3/1000 | Loss: 0.00115593
Iteration 4/1000 | Loss: 0.00136342
Iteration 5/1000 | Loss: 0.00077288
Iteration 6/1000 | Loss: 0.00050583
Iteration 7/1000 | Loss: 0.00039706
Iteration 8/1000 | Loss: 0.00041273
Iteration 9/1000 | Loss: 0.00029138
Iteration 10/1000 | Loss: 0.00122537
Iteration 11/1000 | Loss: 0.00081616
Iteration 12/1000 | Loss: 0.00078465
Iteration 13/1000 | Loss: 0.00040602
Iteration 14/1000 | Loss: 0.00058470
Iteration 15/1000 | Loss: 0.00077088
Iteration 16/1000 | Loss: 0.00024659
Iteration 17/1000 | Loss: 0.00092064
Iteration 18/1000 | Loss: 0.00843362
Iteration 19/1000 | Loss: 0.00784389
Iteration 20/1000 | Loss: 0.00134749
Iteration 21/1000 | Loss: 0.00065561
Iteration 22/1000 | Loss: 0.00025357
Iteration 23/1000 | Loss: 0.00030556
Iteration 24/1000 | Loss: 0.00016784
Iteration 25/1000 | Loss: 0.00012793
Iteration 26/1000 | Loss: 0.00009419
Iteration 27/1000 | Loss: 0.00008154
Iteration 28/1000 | Loss: 0.00007515
Iteration 29/1000 | Loss: 0.00017901
Iteration 30/1000 | Loss: 0.00007789
Iteration 31/1000 | Loss: 0.00006478
Iteration 32/1000 | Loss: 0.00006223
Iteration 33/1000 | Loss: 0.00006022
Iteration 34/1000 | Loss: 0.00005875
Iteration 35/1000 | Loss: 0.00005782
Iteration 36/1000 | Loss: 0.00005701
Iteration 37/1000 | Loss: 0.00005692
Iteration 38/1000 | Loss: 0.00005603
Iteration 39/1000 | Loss: 0.00005555
Iteration 40/1000 | Loss: 0.00005525
Iteration 41/1000 | Loss: 0.00005503
Iteration 42/1000 | Loss: 0.00005497
Iteration 43/1000 | Loss: 0.00005474
Iteration 44/1000 | Loss: 0.00005465
Iteration 45/1000 | Loss: 0.00005463
Iteration 46/1000 | Loss: 0.00005463
Iteration 47/1000 | Loss: 0.00005450
Iteration 48/1000 | Loss: 0.00005445
Iteration 49/1000 | Loss: 0.00005442
Iteration 50/1000 | Loss: 0.00005442
Iteration 51/1000 | Loss: 0.00005442
Iteration 52/1000 | Loss: 0.00005442
Iteration 53/1000 | Loss: 0.00005440
Iteration 54/1000 | Loss: 0.00005433
Iteration 55/1000 | Loss: 0.00005418
Iteration 56/1000 | Loss: 0.00005411
Iteration 57/1000 | Loss: 0.00072542
Iteration 58/1000 | Loss: 0.00022286
Iteration 59/1000 | Loss: 0.00005462
Iteration 60/1000 | Loss: 0.00005400
Iteration 61/1000 | Loss: 0.00072177
Iteration 62/1000 | Loss: 0.00156389
Iteration 63/1000 | Loss: 0.00069056
Iteration 64/1000 | Loss: 0.00044531
Iteration 65/1000 | Loss: 0.00030832
Iteration 66/1000 | Loss: 0.00006267
Iteration 67/1000 | Loss: 0.00005769
Iteration 68/1000 | Loss: 0.00005585
Iteration 69/1000 | Loss: 0.00005512
Iteration 70/1000 | Loss: 0.00005448
Iteration 71/1000 | Loss: 0.00005404
Iteration 72/1000 | Loss: 0.00005344
Iteration 73/1000 | Loss: 0.00005316
Iteration 74/1000 | Loss: 0.00005288
Iteration 75/1000 | Loss: 0.00005284
Iteration 76/1000 | Loss: 0.00005279
Iteration 77/1000 | Loss: 0.00026177
Iteration 78/1000 | Loss: 0.00180872
Iteration 79/1000 | Loss: 0.00057727
Iteration 80/1000 | Loss: 0.00024949
Iteration 81/1000 | Loss: 0.00018499
Iteration 82/1000 | Loss: 0.00006531
Iteration 83/1000 | Loss: 0.00006030
Iteration 84/1000 | Loss: 0.00019851
Iteration 85/1000 | Loss: 0.00005964
Iteration 86/1000 | Loss: 0.00016998
Iteration 87/1000 | Loss: 0.00043998
Iteration 88/1000 | Loss: 0.00048508
Iteration 89/1000 | Loss: 0.00018053
Iteration 90/1000 | Loss: 0.00068970
Iteration 91/1000 | Loss: 0.00067429
Iteration 92/1000 | Loss: 0.00018615
Iteration 93/1000 | Loss: 0.00015890
Iteration 94/1000 | Loss: 0.00054151
Iteration 95/1000 | Loss: 0.00008076
Iteration 96/1000 | Loss: 0.00049050
Iteration 97/1000 | Loss: 0.00006180
Iteration 98/1000 | Loss: 0.00005886
Iteration 99/1000 | Loss: 0.00005674
Iteration 100/1000 | Loss: 0.00028577
Iteration 101/1000 | Loss: 0.00026070
Iteration 102/1000 | Loss: 0.00043337
Iteration 103/1000 | Loss: 0.00160900
Iteration 104/1000 | Loss: 0.00057850
Iteration 105/1000 | Loss: 0.00139759
Iteration 106/1000 | Loss: 0.00033215
Iteration 107/1000 | Loss: 0.00085480
Iteration 108/1000 | Loss: 0.00025003
Iteration 109/1000 | Loss: 0.00086756
Iteration 110/1000 | Loss: 0.00046039
Iteration 111/1000 | Loss: 0.00071710
Iteration 112/1000 | Loss: 0.00041069
Iteration 113/1000 | Loss: 0.00012457
Iteration 114/1000 | Loss: 0.00027679
Iteration 115/1000 | Loss: 0.00013746
Iteration 116/1000 | Loss: 0.00012388
Iteration 117/1000 | Loss: 0.00035994
Iteration 118/1000 | Loss: 0.00034830
Iteration 119/1000 | Loss: 0.00025015
Iteration 120/1000 | Loss: 0.00023688
Iteration 121/1000 | Loss: 0.00032479
Iteration 122/1000 | Loss: 0.00079209
Iteration 123/1000 | Loss: 0.00032017
Iteration 124/1000 | Loss: 0.00076760
Iteration 125/1000 | Loss: 0.00016795
Iteration 126/1000 | Loss: 0.00063868
Iteration 127/1000 | Loss: 0.00024847
Iteration 128/1000 | Loss: 0.00016129
Iteration 129/1000 | Loss: 0.00005928
Iteration 130/1000 | Loss: 0.00005503
Iteration 131/1000 | Loss: 0.00066157
Iteration 132/1000 | Loss: 0.00043889
Iteration 133/1000 | Loss: 0.00008370
Iteration 134/1000 | Loss: 0.00011138
Iteration 135/1000 | Loss: 0.00005211
Iteration 136/1000 | Loss: 0.00005063
Iteration 137/1000 | Loss: 0.00029865
Iteration 138/1000 | Loss: 0.00071083
Iteration 139/1000 | Loss: 0.00018881
Iteration 140/1000 | Loss: 0.00005559
Iteration 141/1000 | Loss: 0.00024512
Iteration 142/1000 | Loss: 0.00021651
Iteration 143/1000 | Loss: 0.00024449
Iteration 144/1000 | Loss: 0.00015328
Iteration 145/1000 | Loss: 0.00005469
Iteration 146/1000 | Loss: 0.00052237
Iteration 147/1000 | Loss: 0.00026422
Iteration 148/1000 | Loss: 0.00062106
Iteration 149/1000 | Loss: 0.00007888
Iteration 150/1000 | Loss: 0.00005191
Iteration 151/1000 | Loss: 0.00005136
Iteration 152/1000 | Loss: 0.00004903
Iteration 153/1000 | Loss: 0.00004666
Iteration 154/1000 | Loss: 0.00004525
Iteration 155/1000 | Loss: 0.00004483
Iteration 156/1000 | Loss: 0.00004446
Iteration 157/1000 | Loss: 0.00004782
Iteration 158/1000 | Loss: 0.00004413
Iteration 159/1000 | Loss: 0.00004381
Iteration 160/1000 | Loss: 0.00004358
Iteration 161/1000 | Loss: 0.00137908
Iteration 162/1000 | Loss: 0.00044777
Iteration 163/1000 | Loss: 0.00043100
Iteration 164/1000 | Loss: 0.00005158
Iteration 165/1000 | Loss: 0.00011803
Iteration 166/1000 | Loss: 0.00006776
Iteration 167/1000 | Loss: 0.00005831
Iteration 168/1000 | Loss: 0.00005254
Iteration 169/1000 | Loss: 0.00004421
Iteration 170/1000 | Loss: 0.00004395
Iteration 171/1000 | Loss: 0.00004387
Iteration 172/1000 | Loss: 0.00004380
Iteration 173/1000 | Loss: 0.00004364
Iteration 174/1000 | Loss: 0.00004349
Iteration 175/1000 | Loss: 0.00020227
Iteration 176/1000 | Loss: 0.00073871
Iteration 177/1000 | Loss: 0.00026722
Iteration 178/1000 | Loss: 0.00005094
Iteration 179/1000 | Loss: 0.00017136
Iteration 180/1000 | Loss: 0.00005584
Iteration 181/1000 | Loss: 0.00004891
Iteration 182/1000 | Loss: 0.00004455
Iteration 183/1000 | Loss: 0.00004358
Iteration 184/1000 | Loss: 0.00004305
Iteration 185/1000 | Loss: 0.00093372
Iteration 186/1000 | Loss: 0.00120027
Iteration 187/1000 | Loss: 0.00008544
Iteration 188/1000 | Loss: 0.00004745
Iteration 189/1000 | Loss: 0.00036760
Iteration 190/1000 | Loss: 0.00031677
Iteration 191/1000 | Loss: 0.00036554
Iteration 192/1000 | Loss: 0.00020322
Iteration 193/1000 | Loss: 0.00035814
Iteration 194/1000 | Loss: 0.00142882
Iteration 195/1000 | Loss: 0.00136524
Iteration 196/1000 | Loss: 0.00034009
Iteration 197/1000 | Loss: 0.00007777
Iteration 198/1000 | Loss: 0.00006679
Iteration 199/1000 | Loss: 0.00006187
Iteration 200/1000 | Loss: 0.00016208
Iteration 201/1000 | Loss: 0.00019152
Iteration 202/1000 | Loss: 0.00007875
Iteration 203/1000 | Loss: 0.00008358
Iteration 204/1000 | Loss: 0.00004690
Iteration 205/1000 | Loss: 0.00004407
Iteration 206/1000 | Loss: 0.00003932
Iteration 207/1000 | Loss: 0.00016683
Iteration 208/1000 | Loss: 0.00004488
Iteration 209/1000 | Loss: 0.00005880
Iteration 210/1000 | Loss: 0.00046350
Iteration 211/1000 | Loss: 0.00011537
Iteration 212/1000 | Loss: 0.00004273
Iteration 213/1000 | Loss: 0.00004077
Iteration 214/1000 | Loss: 0.00005658
Iteration 215/1000 | Loss: 0.00007879
Iteration 216/1000 | Loss: 0.00021901
Iteration 217/1000 | Loss: 0.00004628
Iteration 218/1000 | Loss: 0.00006600
Iteration 219/1000 | Loss: 0.00006227
Iteration 220/1000 | Loss: 0.00005433
Iteration 221/1000 | Loss: 0.00020535
Iteration 222/1000 | Loss: 0.00014435
Iteration 223/1000 | Loss: 0.00015515
Iteration 224/1000 | Loss: 0.00006443
Iteration 225/1000 | Loss: 0.00004544
Iteration 226/1000 | Loss: 0.00003529
Iteration 227/1000 | Loss: 0.00004476
Iteration 228/1000 | Loss: 0.00006319
Iteration 229/1000 | Loss: 0.00005944
Iteration 230/1000 | Loss: 0.00006303
Iteration 231/1000 | Loss: 0.00005582
Iteration 232/1000 | Loss: 0.00006230
Iteration 233/1000 | Loss: 0.00005607
Iteration 234/1000 | Loss: 0.00005376
Iteration 235/1000 | Loss: 0.00005126
Iteration 236/1000 | Loss: 0.00005328
Iteration 237/1000 | Loss: 0.00004707
Iteration 238/1000 | Loss: 0.00005798
Iteration 239/1000 | Loss: 0.00004869
Iteration 240/1000 | Loss: 0.00006130
Iteration 241/1000 | Loss: 0.00004856
Iteration 242/1000 | Loss: 0.00003860
Iteration 243/1000 | Loss: 0.00003501
Iteration 244/1000 | Loss: 0.00003384
Iteration 245/1000 | Loss: 0.00003342
Iteration 246/1000 | Loss: 0.00062787
Iteration 247/1000 | Loss: 0.00005188
Iteration 248/1000 | Loss: 0.00003897
Iteration 249/1000 | Loss: 0.00003665
Iteration 250/1000 | Loss: 0.00003531
Iteration 251/1000 | Loss: 0.00003472
Iteration 252/1000 | Loss: 0.00024016
Iteration 253/1000 | Loss: 0.00020009
Iteration 254/1000 | Loss: 0.00014388
Iteration 255/1000 | Loss: 0.00024197
Iteration 256/1000 | Loss: 0.00026812
Iteration 257/1000 | Loss: 0.00028948
Iteration 258/1000 | Loss: 0.00037331
Iteration 259/1000 | Loss: 0.00027243
Iteration 260/1000 | Loss: 0.00003906
Iteration 261/1000 | Loss: 0.00033965
Iteration 262/1000 | Loss: 0.00022135
Iteration 263/1000 | Loss: 0.00023550
Iteration 264/1000 | Loss: 0.00015731
Iteration 265/1000 | Loss: 0.00014841
Iteration 266/1000 | Loss: 0.00003928
Iteration 267/1000 | Loss: 0.00003652
Iteration 268/1000 | Loss: 0.00003391
Iteration 269/1000 | Loss: 0.00003184
Iteration 270/1000 | Loss: 0.00003083
Iteration 271/1000 | Loss: 0.00003041
Iteration 272/1000 | Loss: 0.00003016
Iteration 273/1000 | Loss: 0.00002997
Iteration 274/1000 | Loss: 0.00002995
Iteration 275/1000 | Loss: 0.00002994
Iteration 276/1000 | Loss: 0.00002976
Iteration 277/1000 | Loss: 0.00002968
Iteration 278/1000 | Loss: 0.00002964
Iteration 279/1000 | Loss: 0.00002964
Iteration 280/1000 | Loss: 0.00002962
Iteration 281/1000 | Loss: 0.00002962
Iteration 282/1000 | Loss: 0.00002962
Iteration 283/1000 | Loss: 0.00002961
Iteration 284/1000 | Loss: 0.00002961
Iteration 285/1000 | Loss: 0.00002960
Iteration 286/1000 | Loss: 0.00002960
Iteration 287/1000 | Loss: 0.00002960
Iteration 288/1000 | Loss: 0.00002960
Iteration 289/1000 | Loss: 0.00002960
Iteration 290/1000 | Loss: 0.00002960
Iteration 291/1000 | Loss: 0.00002960
Iteration 292/1000 | Loss: 0.00002960
Iteration 293/1000 | Loss: 0.00002960
Iteration 294/1000 | Loss: 0.00002959
Iteration 295/1000 | Loss: 0.00002959
Iteration 296/1000 | Loss: 0.00002959
Iteration 297/1000 | Loss: 0.00002959
Iteration 298/1000 | Loss: 0.00002959
Iteration 299/1000 | Loss: 0.00002958
Iteration 300/1000 | Loss: 0.00002958
Iteration 301/1000 | Loss: 0.00002958
Iteration 302/1000 | Loss: 0.00002958
Iteration 303/1000 | Loss: 0.00002957
Iteration 304/1000 | Loss: 0.00002957
Iteration 305/1000 | Loss: 0.00002957
Iteration 306/1000 | Loss: 0.00002957
Iteration 307/1000 | Loss: 0.00002957
Iteration 308/1000 | Loss: 0.00002956
Iteration 309/1000 | Loss: 0.00002956
Iteration 310/1000 | Loss: 0.00002956
Iteration 311/1000 | Loss: 0.00002955
Iteration 312/1000 | Loss: 0.00002955
Iteration 313/1000 | Loss: 0.00002954
Iteration 314/1000 | Loss: 0.00002954
Iteration 315/1000 | Loss: 0.00002954
Iteration 316/1000 | Loss: 0.00002954
Iteration 317/1000 | Loss: 0.00002954
Iteration 318/1000 | Loss: 0.00002954
Iteration 319/1000 | Loss: 0.00002954
Iteration 320/1000 | Loss: 0.00002954
Iteration 321/1000 | Loss: 0.00002954
Iteration 322/1000 | Loss: 0.00002954
Iteration 323/1000 | Loss: 0.00002953
Iteration 324/1000 | Loss: 0.00002953
Iteration 325/1000 | Loss: 0.00002953
Iteration 326/1000 | Loss: 0.00002953
Iteration 327/1000 | Loss: 0.00002953
Iteration 328/1000 | Loss: 0.00002953
Iteration 329/1000 | Loss: 0.00002953
Iteration 330/1000 | Loss: 0.00002952
Iteration 331/1000 | Loss: 0.00002952
Iteration 332/1000 | Loss: 0.00002952
Iteration 333/1000 | Loss: 0.00002952
Iteration 334/1000 | Loss: 0.00002952
Iteration 335/1000 | Loss: 0.00002952
Iteration 336/1000 | Loss: 0.00002952
Iteration 337/1000 | Loss: 0.00002952
Iteration 338/1000 | Loss: 0.00002952
Iteration 339/1000 | Loss: 0.00002952
Iteration 340/1000 | Loss: 0.00002952
Iteration 341/1000 | Loss: 0.00002951
Iteration 342/1000 | Loss: 0.00002951
Iteration 343/1000 | Loss: 0.00002951
Iteration 344/1000 | Loss: 0.00002951
Iteration 345/1000 | Loss: 0.00002951
Iteration 346/1000 | Loss: 0.00002951
Iteration 347/1000 | Loss: 0.00002950
Iteration 348/1000 | Loss: 0.00002950
Iteration 349/1000 | Loss: 0.00002950
Iteration 350/1000 | Loss: 0.00002950
Iteration 351/1000 | Loss: 0.00002950
Iteration 352/1000 | Loss: 0.00002950
Iteration 353/1000 | Loss: 0.00002950
Iteration 354/1000 | Loss: 0.00002949
Iteration 355/1000 | Loss: 0.00002949
Iteration 356/1000 | Loss: 0.00002949
Iteration 357/1000 | Loss: 0.00002949
Iteration 358/1000 | Loss: 0.00002949
Iteration 359/1000 | Loss: 0.00002949
Iteration 360/1000 | Loss: 0.00002949
Iteration 361/1000 | Loss: 0.00002949
Iteration 362/1000 | Loss: 0.00002949
Iteration 363/1000 | Loss: 0.00002949
Iteration 364/1000 | Loss: 0.00002949
Iteration 365/1000 | Loss: 0.00002949
Iteration 366/1000 | Loss: 0.00002949
Iteration 367/1000 | Loss: 0.00002949
Iteration 368/1000 | Loss: 0.00002948
Iteration 369/1000 | Loss: 0.00002948
Iteration 370/1000 | Loss: 0.00002948
Iteration 371/1000 | Loss: 0.00002948
Iteration 372/1000 | Loss: 0.00002948
Iteration 373/1000 | Loss: 0.00002948
Iteration 374/1000 | Loss: 0.00002948
Iteration 375/1000 | Loss: 0.00002948
Iteration 376/1000 | Loss: 0.00002948
Iteration 377/1000 | Loss: 0.00002948
Iteration 378/1000 | Loss: 0.00002948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 378. Stopping optimization.
Last 5 losses: [2.9483073376468383e-05, 2.9483073376468383e-05, 2.9483073376468383e-05, 2.9483073376468383e-05, 2.9483073376468383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9483073376468383e-05

Optimization complete. Final v2v error: 3.90640926361084 mm

Highest mean error: 11.396401405334473 mm for frame 226

Lowest mean error: 3.5388081073760986 mm for frame 235

Saving results

Total time: 472.36304092407227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501622
Iteration 2/25 | Loss: 0.00121376
Iteration 3/25 | Loss: 0.00084155
Iteration 4/25 | Loss: 0.00079371
Iteration 5/25 | Loss: 0.00078365
Iteration 6/25 | Loss: 0.00078205
Iteration 7/25 | Loss: 0.00078205
Iteration 8/25 | Loss: 0.00078171
Iteration 9/25 | Loss: 0.00078171
Iteration 10/25 | Loss: 0.00078171
Iteration 11/25 | Loss: 0.00078171
Iteration 12/25 | Loss: 0.00078171
Iteration 13/25 | Loss: 0.00078171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007817068253643811, 0.0007817068253643811, 0.0007817068253643811, 0.0007817068253643811, 0.0007817068253643811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007817068253643811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66537923
Iteration 2/25 | Loss: 0.00028497
Iteration 3/25 | Loss: 0.00028496
Iteration 4/25 | Loss: 0.00028496
Iteration 5/25 | Loss: 0.00028496
Iteration 6/25 | Loss: 0.00028496
Iteration 7/25 | Loss: 0.00028496
Iteration 8/25 | Loss: 0.00028496
Iteration 9/25 | Loss: 0.00028496
Iteration 10/25 | Loss: 0.00028496
Iteration 11/25 | Loss: 0.00028496
Iteration 12/25 | Loss: 0.00028496
Iteration 13/25 | Loss: 0.00028496
Iteration 14/25 | Loss: 0.00028496
Iteration 15/25 | Loss: 0.00028496
Iteration 16/25 | Loss: 0.00028496
Iteration 17/25 | Loss: 0.00028496
Iteration 18/25 | Loss: 0.00028496
Iteration 19/25 | Loss: 0.00028496
Iteration 20/25 | Loss: 0.00028496
Iteration 21/25 | Loss: 0.00028496
Iteration 22/25 | Loss: 0.00028496
Iteration 23/25 | Loss: 0.00028496
Iteration 24/25 | Loss: 0.00028496
Iteration 25/25 | Loss: 0.00028496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028496
Iteration 2/1000 | Loss: 0.00003700
Iteration 3/1000 | Loss: 0.00002514
Iteration 4/1000 | Loss: 0.00002317
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002124
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001924
Iteration 13/1000 | Loss: 0.00001912
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001870
Iteration 20/1000 | Loss: 0.00001870
Iteration 21/1000 | Loss: 0.00001869
Iteration 22/1000 | Loss: 0.00001868
Iteration 23/1000 | Loss: 0.00001868
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001862
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001859
Iteration 38/1000 | Loss: 0.00001857
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001854
Iteration 43/1000 | Loss: 0.00001853
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001850
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001847
Iteration 59/1000 | Loss: 0.00001847
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001846
Iteration 62/1000 | Loss: 0.00001846
Iteration 63/1000 | Loss: 0.00001846
Iteration 64/1000 | Loss: 0.00001846
Iteration 65/1000 | Loss: 0.00001846
Iteration 66/1000 | Loss: 0.00001846
Iteration 67/1000 | Loss: 0.00001846
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001845
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001843
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001834
Iteration 113/1000 | Loss: 0.00001834
Iteration 114/1000 | Loss: 0.00001834
Iteration 115/1000 | Loss: 0.00001834
Iteration 116/1000 | Loss: 0.00001833
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001833
Iteration 119/1000 | Loss: 0.00001833
Iteration 120/1000 | Loss: 0.00001833
Iteration 121/1000 | Loss: 0.00001832
Iteration 122/1000 | Loss: 0.00001832
Iteration 123/1000 | Loss: 0.00001832
Iteration 124/1000 | Loss: 0.00001831
Iteration 125/1000 | Loss: 0.00001831
Iteration 126/1000 | Loss: 0.00001831
Iteration 127/1000 | Loss: 0.00001830
Iteration 128/1000 | Loss: 0.00001830
Iteration 129/1000 | Loss: 0.00001829
Iteration 130/1000 | Loss: 0.00001829
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001828
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001827
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001826
Iteration 138/1000 | Loss: 0.00001826
Iteration 139/1000 | Loss: 0.00001826
Iteration 140/1000 | Loss: 0.00001825
Iteration 141/1000 | Loss: 0.00001825
Iteration 142/1000 | Loss: 0.00001825
Iteration 143/1000 | Loss: 0.00001825
Iteration 144/1000 | Loss: 0.00001825
Iteration 145/1000 | Loss: 0.00001825
Iteration 146/1000 | Loss: 0.00001825
Iteration 147/1000 | Loss: 0.00001825
Iteration 148/1000 | Loss: 0.00001825
Iteration 149/1000 | Loss: 0.00001824
Iteration 150/1000 | Loss: 0.00001824
Iteration 151/1000 | Loss: 0.00001824
Iteration 152/1000 | Loss: 0.00001824
Iteration 153/1000 | Loss: 0.00001824
Iteration 154/1000 | Loss: 0.00001824
Iteration 155/1000 | Loss: 0.00001824
Iteration 156/1000 | Loss: 0.00001824
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001823
Iteration 160/1000 | Loss: 0.00001823
Iteration 161/1000 | Loss: 0.00001823
Iteration 162/1000 | Loss: 0.00001823
Iteration 163/1000 | Loss: 0.00001823
Iteration 164/1000 | Loss: 0.00001823
Iteration 165/1000 | Loss: 0.00001823
Iteration 166/1000 | Loss: 0.00001823
Iteration 167/1000 | Loss: 0.00001822
Iteration 168/1000 | Loss: 0.00001822
Iteration 169/1000 | Loss: 0.00001822
Iteration 170/1000 | Loss: 0.00001822
Iteration 171/1000 | Loss: 0.00001822
Iteration 172/1000 | Loss: 0.00001822
Iteration 173/1000 | Loss: 0.00001822
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Iteration 176/1000 | Loss: 0.00001821
Iteration 177/1000 | Loss: 0.00001821
Iteration 178/1000 | Loss: 0.00001821
Iteration 179/1000 | Loss: 0.00001821
Iteration 180/1000 | Loss: 0.00001821
Iteration 181/1000 | Loss: 0.00001821
Iteration 182/1000 | Loss: 0.00001821
Iteration 183/1000 | Loss: 0.00001821
Iteration 184/1000 | Loss: 0.00001820
Iteration 185/1000 | Loss: 0.00001820
Iteration 186/1000 | Loss: 0.00001820
Iteration 187/1000 | Loss: 0.00001820
Iteration 188/1000 | Loss: 0.00001820
Iteration 189/1000 | Loss: 0.00001819
Iteration 190/1000 | Loss: 0.00001819
Iteration 191/1000 | Loss: 0.00001819
Iteration 192/1000 | Loss: 0.00001819
Iteration 193/1000 | Loss: 0.00001819
Iteration 194/1000 | Loss: 0.00001819
Iteration 195/1000 | Loss: 0.00001819
Iteration 196/1000 | Loss: 0.00001818
Iteration 197/1000 | Loss: 0.00001818
Iteration 198/1000 | Loss: 0.00001818
Iteration 199/1000 | Loss: 0.00001818
Iteration 200/1000 | Loss: 0.00001818
Iteration 201/1000 | Loss: 0.00001818
Iteration 202/1000 | Loss: 0.00001818
Iteration 203/1000 | Loss: 0.00001818
Iteration 204/1000 | Loss: 0.00001818
Iteration 205/1000 | Loss: 0.00001818
Iteration 206/1000 | Loss: 0.00001818
Iteration 207/1000 | Loss: 0.00001817
Iteration 208/1000 | Loss: 0.00001817
Iteration 209/1000 | Loss: 0.00001817
Iteration 210/1000 | Loss: 0.00001817
Iteration 211/1000 | Loss: 0.00001817
Iteration 212/1000 | Loss: 0.00001817
Iteration 213/1000 | Loss: 0.00001817
Iteration 214/1000 | Loss: 0.00001817
Iteration 215/1000 | Loss: 0.00001817
Iteration 216/1000 | Loss: 0.00001817
Iteration 217/1000 | Loss: 0.00001817
Iteration 218/1000 | Loss: 0.00001817
Iteration 219/1000 | Loss: 0.00001817
Iteration 220/1000 | Loss: 0.00001817
Iteration 221/1000 | Loss: 0.00001817
Iteration 222/1000 | Loss: 0.00001817
Iteration 223/1000 | Loss: 0.00001817
Iteration 224/1000 | Loss: 0.00001817
Iteration 225/1000 | Loss: 0.00001817
Iteration 226/1000 | Loss: 0.00001817
Iteration 227/1000 | Loss: 0.00001817
Iteration 228/1000 | Loss: 0.00001817
Iteration 229/1000 | Loss: 0.00001817
Iteration 230/1000 | Loss: 0.00001817
Iteration 231/1000 | Loss: 0.00001817
Iteration 232/1000 | Loss: 0.00001817
Iteration 233/1000 | Loss: 0.00001817
Iteration 234/1000 | Loss: 0.00001817
Iteration 235/1000 | Loss: 0.00001817
Iteration 236/1000 | Loss: 0.00001817
Iteration 237/1000 | Loss: 0.00001817
Iteration 238/1000 | Loss: 0.00001817
Iteration 239/1000 | Loss: 0.00001817
Iteration 240/1000 | Loss: 0.00001817
Iteration 241/1000 | Loss: 0.00001817
Iteration 242/1000 | Loss: 0.00001817
Iteration 243/1000 | Loss: 0.00001817
Iteration 244/1000 | Loss: 0.00001817
Iteration 245/1000 | Loss: 0.00001817
Iteration 246/1000 | Loss: 0.00001817
Iteration 247/1000 | Loss: 0.00001817
Iteration 248/1000 | Loss: 0.00001817
Iteration 249/1000 | Loss: 0.00001817
Iteration 250/1000 | Loss: 0.00001817
Iteration 251/1000 | Loss: 0.00001817
Iteration 252/1000 | Loss: 0.00001817
Iteration 253/1000 | Loss: 0.00001817
Iteration 254/1000 | Loss: 0.00001817
Iteration 255/1000 | Loss: 0.00001817
Iteration 256/1000 | Loss: 0.00001817
Iteration 257/1000 | Loss: 0.00001817
Iteration 258/1000 | Loss: 0.00001817
Iteration 259/1000 | Loss: 0.00001817
Iteration 260/1000 | Loss: 0.00001817
Iteration 261/1000 | Loss: 0.00001817
Iteration 262/1000 | Loss: 0.00001817
Iteration 263/1000 | Loss: 0.00001817
Iteration 264/1000 | Loss: 0.00001817
Iteration 265/1000 | Loss: 0.00001817
Iteration 266/1000 | Loss: 0.00001817
Iteration 267/1000 | Loss: 0.00001817
Iteration 268/1000 | Loss: 0.00001817
Iteration 269/1000 | Loss: 0.00001817
Iteration 270/1000 | Loss: 0.00001817
Iteration 271/1000 | Loss: 0.00001817
Iteration 272/1000 | Loss: 0.00001817
Iteration 273/1000 | Loss: 0.00001817
Iteration 274/1000 | Loss: 0.00001817
Iteration 275/1000 | Loss: 0.00001817
Iteration 276/1000 | Loss: 0.00001817
Iteration 277/1000 | Loss: 0.00001817
Iteration 278/1000 | Loss: 0.00001817
Iteration 279/1000 | Loss: 0.00001817
Iteration 280/1000 | Loss: 0.00001817
Iteration 281/1000 | Loss: 0.00001817
Iteration 282/1000 | Loss: 0.00001817
Iteration 283/1000 | Loss: 0.00001817
Iteration 284/1000 | Loss: 0.00001817
Iteration 285/1000 | Loss: 0.00001817
Iteration 286/1000 | Loss: 0.00001817
Iteration 287/1000 | Loss: 0.00001817
Iteration 288/1000 | Loss: 0.00001817
Iteration 289/1000 | Loss: 0.00001817
Iteration 290/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.8171933334087953e-05, 1.8171933334087953e-05, 1.8171933334087953e-05, 1.8171933334087953e-05, 1.8171933334087953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8171933334087953e-05

Optimization complete. Final v2v error: 3.6044702529907227 mm

Highest mean error: 3.994424343109131 mm for frame 237

Lowest mean error: 3.261871099472046 mm for frame 1

Saving results

Total time: 56.53098154067993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984626
Iteration 2/25 | Loss: 0.00346053
Iteration 3/25 | Loss: 0.00288109
Iteration 4/25 | Loss: 0.00213127
Iteration 5/25 | Loss: 0.00160772
Iteration 6/25 | Loss: 0.00135014
Iteration 7/25 | Loss: 0.00139845
Iteration 8/25 | Loss: 0.00157546
Iteration 9/25 | Loss: 0.00172947
Iteration 10/25 | Loss: 0.00175902
Iteration 11/25 | Loss: 0.00178077
Iteration 12/25 | Loss: 0.00169624
Iteration 13/25 | Loss: 0.00167432
Iteration 14/25 | Loss: 0.00168450
Iteration 15/25 | Loss: 0.00162952
Iteration 16/25 | Loss: 0.00149267
Iteration 17/25 | Loss: 0.00146625
Iteration 18/25 | Loss: 0.00137731
Iteration 19/25 | Loss: 0.00145115
Iteration 20/25 | Loss: 0.00140711
Iteration 21/25 | Loss: 0.00129201
Iteration 22/25 | Loss: 0.00151413
Iteration 23/25 | Loss: 0.00151994
Iteration 24/25 | Loss: 0.00139843
Iteration 25/25 | Loss: 0.00134239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43661761
Iteration 2/25 | Loss: 0.01182342
Iteration 3/25 | Loss: 0.01118430
Iteration 4/25 | Loss: 0.01118429
Iteration 5/25 | Loss: 0.01118429
Iteration 6/25 | Loss: 0.01118429
Iteration 7/25 | Loss: 0.01118429
Iteration 8/25 | Loss: 0.01118429
Iteration 9/25 | Loss: 0.01118429
Iteration 10/25 | Loss: 0.01118429
Iteration 11/25 | Loss: 0.01118429
Iteration 12/25 | Loss: 0.01118429
Iteration 13/25 | Loss: 0.01118429
Iteration 14/25 | Loss: 0.01118429
Iteration 15/25 | Loss: 0.01118429
Iteration 16/25 | Loss: 0.01118429
Iteration 17/25 | Loss: 0.01118429
Iteration 18/25 | Loss: 0.01118429
Iteration 19/25 | Loss: 0.01118429
Iteration 20/25 | Loss: 0.01118429
Iteration 21/25 | Loss: 0.01118429
Iteration 22/25 | Loss: 0.01118429
Iteration 23/25 | Loss: 0.01118429
Iteration 24/25 | Loss: 0.01118429
Iteration 25/25 | Loss: 0.01118429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01118429
Iteration 2/1000 | Loss: 0.00856835
Iteration 3/1000 | Loss: 0.00326138
Iteration 4/1000 | Loss: 0.00186727
Iteration 5/1000 | Loss: 0.00216428
Iteration 6/1000 | Loss: 0.00168580
Iteration 7/1000 | Loss: 0.00190321
Iteration 8/1000 | Loss: 0.00121172
Iteration 9/1000 | Loss: 0.00094921
Iteration 10/1000 | Loss: 0.00123935
Iteration 11/1000 | Loss: 0.00140172
Iteration 12/1000 | Loss: 0.00113309
Iteration 13/1000 | Loss: 0.00112524
Iteration 14/1000 | Loss: 0.00181475
Iteration 15/1000 | Loss: 0.00141037
Iteration 16/1000 | Loss: 0.00135513
Iteration 17/1000 | Loss: 0.00227142
Iteration 18/1000 | Loss: 0.00109007
Iteration 19/1000 | Loss: 0.00147306
Iteration 20/1000 | Loss: 0.00247449
Iteration 21/1000 | Loss: 0.00133825
Iteration 22/1000 | Loss: 0.00067857
Iteration 23/1000 | Loss: 0.00173017
Iteration 24/1000 | Loss: 0.00135049
Iteration 25/1000 | Loss: 0.00556077
Iteration 26/1000 | Loss: 0.00365878
Iteration 27/1000 | Loss: 0.00341160
Iteration 28/1000 | Loss: 0.00312047
Iteration 29/1000 | Loss: 0.00252722
Iteration 30/1000 | Loss: 0.00187075
Iteration 31/1000 | Loss: 0.00241507
Iteration 32/1000 | Loss: 0.00486391
Iteration 33/1000 | Loss: 0.00535376
Iteration 34/1000 | Loss: 0.00338504
Iteration 35/1000 | Loss: 0.00331234
Iteration 36/1000 | Loss: 0.00442476
Iteration 37/1000 | Loss: 0.00316945
Iteration 38/1000 | Loss: 0.00277766
Iteration 39/1000 | Loss: 0.00333537
Iteration 40/1000 | Loss: 0.00326550
Iteration 41/1000 | Loss: 0.00296387
Iteration 42/1000 | Loss: 0.00184595
Iteration 43/1000 | Loss: 0.00267259
Iteration 44/1000 | Loss: 0.00120161
Iteration 45/1000 | Loss: 0.00204152
Iteration 46/1000 | Loss: 0.00234273
Iteration 47/1000 | Loss: 0.00118143
Iteration 48/1000 | Loss: 0.00202730
Iteration 49/1000 | Loss: 0.00102536
Iteration 50/1000 | Loss: 0.00144889
Iteration 51/1000 | Loss: 0.00278110
Iteration 52/1000 | Loss: 0.00170740
Iteration 53/1000 | Loss: 0.00086423
Iteration 54/1000 | Loss: 0.00051300
Iteration 55/1000 | Loss: 0.00059100
Iteration 56/1000 | Loss: 0.00058926
Iteration 57/1000 | Loss: 0.00033183
Iteration 58/1000 | Loss: 0.00068525
Iteration 59/1000 | Loss: 0.00133531
Iteration 60/1000 | Loss: 0.00106315
Iteration 61/1000 | Loss: 0.00099061
Iteration 62/1000 | Loss: 0.00078697
Iteration 63/1000 | Loss: 0.00071836
Iteration 64/1000 | Loss: 0.00123254
Iteration 65/1000 | Loss: 0.00065766
Iteration 66/1000 | Loss: 0.00069673
Iteration 67/1000 | Loss: 0.00106973
Iteration 68/1000 | Loss: 0.00089174
Iteration 69/1000 | Loss: 0.00031040
Iteration 70/1000 | Loss: 0.00070136
Iteration 71/1000 | Loss: 0.00128030
Iteration 72/1000 | Loss: 0.00155831
Iteration 73/1000 | Loss: 0.00126923
Iteration 74/1000 | Loss: 0.00117426
Iteration 75/1000 | Loss: 0.00124496
Iteration 76/1000 | Loss: 0.00150819
Iteration 77/1000 | Loss: 0.00163876
Iteration 78/1000 | Loss: 0.00069350
Iteration 79/1000 | Loss: 0.00049416
Iteration 80/1000 | Loss: 0.00049560
Iteration 81/1000 | Loss: 0.00070588
Iteration 82/1000 | Loss: 0.00041468
Iteration 83/1000 | Loss: 0.00020529
Iteration 84/1000 | Loss: 0.00039336
Iteration 85/1000 | Loss: 0.00080196
Iteration 86/1000 | Loss: 0.00127339
Iteration 87/1000 | Loss: 0.00065455
Iteration 88/1000 | Loss: 0.00042299
Iteration 89/1000 | Loss: 0.00054130
Iteration 90/1000 | Loss: 0.00036950
Iteration 91/1000 | Loss: 0.00015044
Iteration 92/1000 | Loss: 0.00048772
Iteration 93/1000 | Loss: 0.00103203
Iteration 94/1000 | Loss: 0.00040264
Iteration 95/1000 | Loss: 0.00045582
Iteration 96/1000 | Loss: 0.00014812
Iteration 97/1000 | Loss: 0.00044678
Iteration 98/1000 | Loss: 0.00044745
Iteration 99/1000 | Loss: 0.00057744
Iteration 100/1000 | Loss: 0.00053744
Iteration 101/1000 | Loss: 0.00052117
Iteration 102/1000 | Loss: 0.00019647
Iteration 103/1000 | Loss: 0.00054147
Iteration 104/1000 | Loss: 0.00137436
Iteration 105/1000 | Loss: 0.00126547
Iteration 106/1000 | Loss: 0.00066711
Iteration 107/1000 | Loss: 0.00055043
Iteration 108/1000 | Loss: 0.00087124
Iteration 109/1000 | Loss: 0.00073383
Iteration 110/1000 | Loss: 0.00128123
Iteration 111/1000 | Loss: 0.00057112
Iteration 112/1000 | Loss: 0.00044244
Iteration 113/1000 | Loss: 0.00058040
Iteration 114/1000 | Loss: 0.00070391
Iteration 115/1000 | Loss: 0.00037037
Iteration 116/1000 | Loss: 0.00040236
Iteration 117/1000 | Loss: 0.00041161
Iteration 118/1000 | Loss: 0.00038102
Iteration 119/1000 | Loss: 0.00015022
Iteration 120/1000 | Loss: 0.00011537
Iteration 121/1000 | Loss: 0.00013265
Iteration 122/1000 | Loss: 0.00061080
Iteration 123/1000 | Loss: 0.00163020
Iteration 124/1000 | Loss: 0.00097287
Iteration 125/1000 | Loss: 0.00117923
Iteration 126/1000 | Loss: 0.00013600
Iteration 127/1000 | Loss: 0.00045483
Iteration 128/1000 | Loss: 0.00012742
Iteration 129/1000 | Loss: 0.00012689
Iteration 130/1000 | Loss: 0.00030690
Iteration 131/1000 | Loss: 0.00012400
Iteration 132/1000 | Loss: 0.00009439
Iteration 133/1000 | Loss: 0.00009677
Iteration 134/1000 | Loss: 0.00043390
Iteration 135/1000 | Loss: 0.00065330
Iteration 136/1000 | Loss: 0.00133313
Iteration 137/1000 | Loss: 0.00013896
Iteration 138/1000 | Loss: 0.00010344
Iteration 139/1000 | Loss: 0.00079873
Iteration 140/1000 | Loss: 0.00007604
Iteration 141/1000 | Loss: 0.00007922
Iteration 142/1000 | Loss: 0.00025757
Iteration 143/1000 | Loss: 0.00051225
Iteration 144/1000 | Loss: 0.00019711
Iteration 145/1000 | Loss: 0.00048895
Iteration 146/1000 | Loss: 0.00092567
Iteration 147/1000 | Loss: 0.00084238
Iteration 148/1000 | Loss: 0.00043606
Iteration 149/1000 | Loss: 0.00072807
Iteration 150/1000 | Loss: 0.00020199
Iteration 151/1000 | Loss: 0.00038451
Iteration 152/1000 | Loss: 0.00011294
Iteration 153/1000 | Loss: 0.00066572
Iteration 154/1000 | Loss: 0.00065969
Iteration 155/1000 | Loss: 0.00016668
Iteration 156/1000 | Loss: 0.00012564
Iteration 157/1000 | Loss: 0.00008210
Iteration 158/1000 | Loss: 0.00066423
Iteration 159/1000 | Loss: 0.00011416
Iteration 160/1000 | Loss: 0.00088905
Iteration 161/1000 | Loss: 0.00118356
Iteration 162/1000 | Loss: 0.00035484
Iteration 163/1000 | Loss: 0.00038548
Iteration 164/1000 | Loss: 0.00030922
Iteration 165/1000 | Loss: 0.00028643
Iteration 166/1000 | Loss: 0.00064358
Iteration 167/1000 | Loss: 0.00033133
Iteration 168/1000 | Loss: 0.00064971
Iteration 169/1000 | Loss: 0.00028016
Iteration 170/1000 | Loss: 0.00032167
Iteration 171/1000 | Loss: 0.00039461
Iteration 172/1000 | Loss: 0.00009953
Iteration 173/1000 | Loss: 0.00011209
Iteration 174/1000 | Loss: 0.00061136
Iteration 175/1000 | Loss: 0.00045233
Iteration 176/1000 | Loss: 0.00008615
Iteration 177/1000 | Loss: 0.00009050
Iteration 178/1000 | Loss: 0.00007700
Iteration 179/1000 | Loss: 0.00030340
Iteration 180/1000 | Loss: 0.00036336
Iteration 181/1000 | Loss: 0.00047964
Iteration 182/1000 | Loss: 0.00010234
Iteration 183/1000 | Loss: 0.00008958
Iteration 184/1000 | Loss: 0.00007187
Iteration 185/1000 | Loss: 0.00032917
Iteration 186/1000 | Loss: 0.00008503
Iteration 187/1000 | Loss: 0.00032510
Iteration 188/1000 | Loss: 0.00007646
Iteration 189/1000 | Loss: 0.00006568
Iteration 190/1000 | Loss: 0.00046776
Iteration 191/1000 | Loss: 0.00018033
Iteration 192/1000 | Loss: 0.00026709
Iteration 193/1000 | Loss: 0.00082807
Iteration 194/1000 | Loss: 0.00007843
Iteration 195/1000 | Loss: 0.00005932
Iteration 196/1000 | Loss: 0.00005610
Iteration 197/1000 | Loss: 0.00031480
Iteration 198/1000 | Loss: 0.00031473
Iteration 199/1000 | Loss: 0.00062094
Iteration 200/1000 | Loss: 0.00014464
Iteration 201/1000 | Loss: 0.00025442
Iteration 202/1000 | Loss: 0.00013237
Iteration 203/1000 | Loss: 0.00011450
Iteration 204/1000 | Loss: 0.00006523
Iteration 205/1000 | Loss: 0.00062582
Iteration 206/1000 | Loss: 0.00011307
Iteration 207/1000 | Loss: 0.00017930
Iteration 208/1000 | Loss: 0.00017501
Iteration 209/1000 | Loss: 0.00006570
Iteration 210/1000 | Loss: 0.00009327
Iteration 211/1000 | Loss: 0.00008185
Iteration 212/1000 | Loss: 0.00008740
Iteration 213/1000 | Loss: 0.00034364
Iteration 214/1000 | Loss: 0.00063006
Iteration 215/1000 | Loss: 0.00040270
Iteration 216/1000 | Loss: 0.00047777
Iteration 217/1000 | Loss: 0.00147450
Iteration 218/1000 | Loss: 0.00158651
Iteration 219/1000 | Loss: 0.00014972
Iteration 220/1000 | Loss: 0.00120458
Iteration 221/1000 | Loss: 0.00052760
Iteration 222/1000 | Loss: 0.00042912
Iteration 223/1000 | Loss: 0.00035964
Iteration 224/1000 | Loss: 0.00059404
Iteration 225/1000 | Loss: 0.00020626
Iteration 226/1000 | Loss: 0.00080699
Iteration 227/1000 | Loss: 0.00011688
Iteration 228/1000 | Loss: 0.00006667
Iteration 229/1000 | Loss: 0.00007800
Iteration 230/1000 | Loss: 0.00006205
Iteration 231/1000 | Loss: 0.00070564
Iteration 232/1000 | Loss: 0.00086498
Iteration 233/1000 | Loss: 0.00064860
Iteration 234/1000 | Loss: 0.00021162
Iteration 235/1000 | Loss: 0.00008627
Iteration 236/1000 | Loss: 0.00007502
Iteration 237/1000 | Loss: 0.00035454
Iteration 238/1000 | Loss: 0.00062595
Iteration 239/1000 | Loss: 0.00091022
Iteration 240/1000 | Loss: 0.00025144
Iteration 241/1000 | Loss: 0.00091683
Iteration 242/1000 | Loss: 0.00014560
Iteration 243/1000 | Loss: 0.00007990
Iteration 244/1000 | Loss: 0.00060742
Iteration 245/1000 | Loss: 0.00109817
Iteration 246/1000 | Loss: 0.00016926
Iteration 247/1000 | Loss: 0.00011597
Iteration 248/1000 | Loss: 0.00010084
Iteration 249/1000 | Loss: 0.00064279
Iteration 250/1000 | Loss: 0.00036510
Iteration 251/1000 | Loss: 0.00056882
Iteration 252/1000 | Loss: 0.00044353
Iteration 253/1000 | Loss: 0.00048478
Iteration 254/1000 | Loss: 0.00071401
Iteration 255/1000 | Loss: 0.00034381
Iteration 256/1000 | Loss: 0.00008141
Iteration 257/1000 | Loss: 0.00062495
Iteration 258/1000 | Loss: 0.00037342
Iteration 259/1000 | Loss: 0.00042438
Iteration 260/1000 | Loss: 0.00071848
Iteration 261/1000 | Loss: 0.00063923
Iteration 262/1000 | Loss: 0.00011932
Iteration 263/1000 | Loss: 0.00010522
Iteration 264/1000 | Loss: 0.00038474
Iteration 265/1000 | Loss: 0.00069395
Iteration 266/1000 | Loss: 0.00028174
Iteration 267/1000 | Loss: 0.00114360
Iteration 268/1000 | Loss: 0.00086849
Iteration 269/1000 | Loss: 0.00066046
Iteration 270/1000 | Loss: 0.00028756
Iteration 271/1000 | Loss: 0.00008394
Iteration 272/1000 | Loss: 0.00066824
Iteration 273/1000 | Loss: 0.00033961
Iteration 274/1000 | Loss: 0.00029918
Iteration 275/1000 | Loss: 0.00007028
Iteration 276/1000 | Loss: 0.00006425
Iteration 277/1000 | Loss: 0.00034229
Iteration 278/1000 | Loss: 0.00008354
Iteration 279/1000 | Loss: 0.00008258
Iteration 280/1000 | Loss: 0.00005927
Iteration 281/1000 | Loss: 0.00006424
Iteration 282/1000 | Loss: 0.00007549
Iteration 283/1000 | Loss: 0.00005627
Iteration 284/1000 | Loss: 0.00005852
Iteration 285/1000 | Loss: 0.00010106
Iteration 286/1000 | Loss: 0.00042571
Iteration 287/1000 | Loss: 0.00152400
Iteration 288/1000 | Loss: 0.00094946
Iteration 289/1000 | Loss: 0.00025979
Iteration 290/1000 | Loss: 0.00009077
Iteration 291/1000 | Loss: 0.00043121
Iteration 292/1000 | Loss: 0.00006716
Iteration 293/1000 | Loss: 0.00006774
Iteration 294/1000 | Loss: 0.00006563
Iteration 295/1000 | Loss: 0.00007234
Iteration 296/1000 | Loss: 0.00056883
Iteration 297/1000 | Loss: 0.00025839
Iteration 298/1000 | Loss: 0.00024291
Iteration 299/1000 | Loss: 0.00023556
Iteration 300/1000 | Loss: 0.00005546
Iteration 301/1000 | Loss: 0.00006195
Iteration 302/1000 | Loss: 0.00006662
Iteration 303/1000 | Loss: 0.00006450
Iteration 304/1000 | Loss: 0.00006268
Iteration 305/1000 | Loss: 0.00005108
Iteration 306/1000 | Loss: 0.00006423
Iteration 307/1000 | Loss: 0.00005600
Iteration 308/1000 | Loss: 0.00006035
Iteration 309/1000 | Loss: 0.00006912
Iteration 310/1000 | Loss: 0.00005666
Iteration 311/1000 | Loss: 0.00033143
Iteration 312/1000 | Loss: 0.00007268
Iteration 313/1000 | Loss: 0.00034343
Iteration 314/1000 | Loss: 0.00031652
Iteration 315/1000 | Loss: 0.00006567
Iteration 316/1000 | Loss: 0.00062896
Iteration 317/1000 | Loss: 0.00036397
Iteration 318/1000 | Loss: 0.00023864
Iteration 319/1000 | Loss: 0.00006069
Iteration 320/1000 | Loss: 0.00119907
Iteration 321/1000 | Loss: 0.00009509
Iteration 322/1000 | Loss: 0.00006410
Iteration 323/1000 | Loss: 0.00032441
Iteration 324/1000 | Loss: 0.00006815
Iteration 325/1000 | Loss: 0.00034096
Iteration 326/1000 | Loss: 0.00007215
Iteration 327/1000 | Loss: 0.00033573
Iteration 328/1000 | Loss: 0.00006801
Iteration 329/1000 | Loss: 0.00035246
Iteration 330/1000 | Loss: 0.00007502
Iteration 331/1000 | Loss: 0.00006499
Iteration 332/1000 | Loss: 0.00009222
Iteration 333/1000 | Loss: 0.00034425
Iteration 334/1000 | Loss: 0.00009906
Iteration 335/1000 | Loss: 0.00006230
Iteration 336/1000 | Loss: 0.00036893
Iteration 337/1000 | Loss: 0.00006514
Iteration 338/1000 | Loss: 0.00005950
Iteration 339/1000 | Loss: 0.00032736
Iteration 340/1000 | Loss: 0.00006028
Iteration 341/1000 | Loss: 0.00032010
Iteration 342/1000 | Loss: 0.00062893
Iteration 343/1000 | Loss: 0.00009976
Iteration 344/1000 | Loss: 0.00008832
Iteration 345/1000 | Loss: 0.00007433
Iteration 346/1000 | Loss: 0.00006792
Iteration 347/1000 | Loss: 0.00005481
Iteration 348/1000 | Loss: 0.00005163
Iteration 349/1000 | Loss: 0.00033412
Iteration 350/1000 | Loss: 0.00005892
Iteration 351/1000 | Loss: 0.00004963
Iteration 352/1000 | Loss: 0.00004712
Iteration 353/1000 | Loss: 0.00034253
Iteration 354/1000 | Loss: 0.00073669
Iteration 355/1000 | Loss: 0.00009021
Iteration 356/1000 | Loss: 0.00035371
Iteration 357/1000 | Loss: 0.00028487
Iteration 358/1000 | Loss: 0.00004413
Iteration 359/1000 | Loss: 0.00031764
Iteration 360/1000 | Loss: 0.00333579
Iteration 361/1000 | Loss: 0.00207401
Iteration 362/1000 | Loss: 0.00172175
Iteration 363/1000 | Loss: 0.00168854
Iteration 364/1000 | Loss: 0.00175974
Iteration 365/1000 | Loss: 0.00210768
Iteration 366/1000 | Loss: 0.00107981
Iteration 367/1000 | Loss: 0.00062973
Iteration 368/1000 | Loss: 0.00221569
Iteration 369/1000 | Loss: 0.00151049
Iteration 370/1000 | Loss: 0.00092154
Iteration 371/1000 | Loss: 0.00065135
Iteration 372/1000 | Loss: 0.00039325
Iteration 373/1000 | Loss: 0.00048410
Iteration 374/1000 | Loss: 0.00060471
Iteration 375/1000 | Loss: 0.00063487
Iteration 376/1000 | Loss: 0.00056588
Iteration 377/1000 | Loss: 0.00041389
Iteration 378/1000 | Loss: 0.00039635
Iteration 379/1000 | Loss: 0.00007305
Iteration 380/1000 | Loss: 0.00051524
Iteration 381/1000 | Loss: 0.00008401
Iteration 382/1000 | Loss: 0.00006999
Iteration 383/1000 | Loss: 0.00006310
Iteration 384/1000 | Loss: 0.00028534
Iteration 385/1000 | Loss: 0.00070919
Iteration 386/1000 | Loss: 0.00024834
Iteration 387/1000 | Loss: 0.00030779
Iteration 388/1000 | Loss: 0.00057539
Iteration 389/1000 | Loss: 0.00020327
Iteration 390/1000 | Loss: 0.00005975
Iteration 391/1000 | Loss: 0.00005334
Iteration 392/1000 | Loss: 0.00006719
Iteration 393/1000 | Loss: 0.00005499
Iteration 394/1000 | Loss: 0.00004941
Iteration 395/1000 | Loss: 0.00004522
Iteration 396/1000 | Loss: 0.00005549
Iteration 397/1000 | Loss: 0.00004317
Iteration 398/1000 | Loss: 0.00004032
Iteration 399/1000 | Loss: 0.00003886
Iteration 400/1000 | Loss: 0.00003807
Iteration 401/1000 | Loss: 0.00003720
Iteration 402/1000 | Loss: 0.00003644
Iteration 403/1000 | Loss: 0.00057466
Iteration 404/1000 | Loss: 0.00285367
Iteration 405/1000 | Loss: 0.00203506
Iteration 406/1000 | Loss: 0.00127011
Iteration 407/1000 | Loss: 0.00005446
Iteration 408/1000 | Loss: 0.00004392
Iteration 409/1000 | Loss: 0.00004094
Iteration 410/1000 | Loss: 0.00003937
Iteration 411/1000 | Loss: 0.00003758
Iteration 412/1000 | Loss: 0.00032364
Iteration 413/1000 | Loss: 0.00227998
Iteration 414/1000 | Loss: 0.00225266
Iteration 415/1000 | Loss: 0.00082786
Iteration 416/1000 | Loss: 0.00136402
Iteration 417/1000 | Loss: 0.00104189
Iteration 418/1000 | Loss: 0.00044259
Iteration 419/1000 | Loss: 0.00145531
Iteration 420/1000 | Loss: 0.00085573
Iteration 421/1000 | Loss: 0.00063028
Iteration 422/1000 | Loss: 0.00078016
Iteration 423/1000 | Loss: 0.00034832
Iteration 424/1000 | Loss: 0.00012162
Iteration 425/1000 | Loss: 0.00006600
Iteration 426/1000 | Loss: 0.00004934
Iteration 427/1000 | Loss: 0.00004554
Iteration 428/1000 | Loss: 0.00004361
Iteration 429/1000 | Loss: 0.00032816
Iteration 430/1000 | Loss: 0.00024777
Iteration 431/1000 | Loss: 0.00006294
Iteration 432/1000 | Loss: 0.00015989
Iteration 433/1000 | Loss: 0.00005770
Iteration 434/1000 | Loss: 0.00039987
Iteration 435/1000 | Loss: 0.00129132
Iteration 436/1000 | Loss: 0.00042793
Iteration 437/1000 | Loss: 0.00040370
Iteration 438/1000 | Loss: 0.00017997
Iteration 439/1000 | Loss: 0.00007259
Iteration 440/1000 | Loss: 0.00005091
Iteration 441/1000 | Loss: 0.00004769
Iteration 442/1000 | Loss: 0.00091828
Iteration 443/1000 | Loss: 0.00010872
Iteration 444/1000 | Loss: 0.00017147
Iteration 445/1000 | Loss: 0.00006111
Iteration 446/1000 | Loss: 0.00032100
Iteration 447/1000 | Loss: 0.00269376
Iteration 448/1000 | Loss: 0.00185591
Iteration 449/1000 | Loss: 0.00080157
Iteration 450/1000 | Loss: 0.00020063
Iteration 451/1000 | Loss: 0.00004818
Iteration 452/1000 | Loss: 0.00004549
Iteration 453/1000 | Loss: 0.00004334
Iteration 454/1000 | Loss: 0.00004173
Iteration 455/1000 | Loss: 0.00004073
Iteration 456/1000 | Loss: 0.00030638
Iteration 457/1000 | Loss: 0.00032019
Iteration 458/1000 | Loss: 0.00005252
Iteration 459/1000 | Loss: 0.00004478
Iteration 460/1000 | Loss: 0.00031339
Iteration 461/1000 | Loss: 0.00059173
Iteration 462/1000 | Loss: 0.00100796
Iteration 463/1000 | Loss: 0.00013573
Iteration 464/1000 | Loss: 0.00005962
Iteration 465/1000 | Loss: 0.00005280
Iteration 466/1000 | Loss: 0.00005029
Iteration 467/1000 | Loss: 0.00031363
Iteration 468/1000 | Loss: 0.00063133
Iteration 469/1000 | Loss: 0.00008103
Iteration 470/1000 | Loss: 0.00005835
Iteration 471/1000 | Loss: 0.00005157
Iteration 472/1000 | Loss: 0.00004863
Iteration 473/1000 | Loss: 0.00036573
Iteration 474/1000 | Loss: 0.00006973
Iteration 475/1000 | Loss: 0.00005226
Iteration 476/1000 | Loss: 0.00004737
Iteration 477/1000 | Loss: 0.00063679
Iteration 478/1000 | Loss: 0.00224801
Iteration 479/1000 | Loss: 0.00022706
Iteration 480/1000 | Loss: 0.00009320
Iteration 481/1000 | Loss: 0.00012035
Iteration 482/1000 | Loss: 0.00008633
Iteration 483/1000 | Loss: 0.00007871
Iteration 484/1000 | Loss: 0.00006140
Iteration 485/1000 | Loss: 0.00005528
Iteration 486/1000 | Loss: 0.00005213
Iteration 487/1000 | Loss: 0.00004805
Iteration 488/1000 | Loss: 0.00004423
Iteration 489/1000 | Loss: 0.00004182
Iteration 490/1000 | Loss: 0.00003957
Iteration 491/1000 | Loss: 0.00003804
Iteration 492/1000 | Loss: 0.00030232
Iteration 493/1000 | Loss: 0.00170475
Iteration 494/1000 | Loss: 0.00080212
Iteration 495/1000 | Loss: 0.00043232
Iteration 496/1000 | Loss: 0.00004512
Iteration 497/1000 | Loss: 0.00003926
Iteration 498/1000 | Loss: 0.00003709
Iteration 499/1000 | Loss: 0.00118518
Iteration 500/1000 | Loss: 0.00063795
Iteration 501/1000 | Loss: 0.00013143
Iteration 502/1000 | Loss: 0.00003645
Iteration 503/1000 | Loss: 0.00030193
Iteration 504/1000 | Loss: 0.00015348
Iteration 505/1000 | Loss: 0.00033168
Iteration 506/1000 | Loss: 0.00018083
Iteration 507/1000 | Loss: 0.00017435
Iteration 508/1000 | Loss: 0.00062660
Iteration 509/1000 | Loss: 0.00042028
Iteration 510/1000 | Loss: 0.00030481
Iteration 511/1000 | Loss: 0.00022353
Iteration 512/1000 | Loss: 0.00003657
Iteration 513/1000 | Loss: 0.00003491
Iteration 514/1000 | Loss: 0.00089022
Iteration 515/1000 | Loss: 0.00040742
Iteration 516/1000 | Loss: 0.00083915
Iteration 517/1000 | Loss: 0.00309834
Iteration 518/1000 | Loss: 0.00171439
Iteration 519/1000 | Loss: 0.00142675
Iteration 520/1000 | Loss: 0.00096835
Iteration 521/1000 | Loss: 0.00086988
Iteration 522/1000 | Loss: 0.00253856
Iteration 523/1000 | Loss: 0.00137504
Iteration 524/1000 | Loss: 0.00138557
Iteration 525/1000 | Loss: 0.00257205
Iteration 526/1000 | Loss: 0.00090169
Iteration 527/1000 | Loss: 0.00055103
Iteration 528/1000 | Loss: 0.00059755
Iteration 529/1000 | Loss: 0.00087307
Iteration 530/1000 | Loss: 0.00053082
Iteration 531/1000 | Loss: 0.00023202
Iteration 532/1000 | Loss: 0.00019499
Iteration 533/1000 | Loss: 0.00008930
Iteration 534/1000 | Loss: 0.00006472
Iteration 535/1000 | Loss: 0.00009568
Iteration 536/1000 | Loss: 0.00005832
Iteration 537/1000 | Loss: 0.00005421
Iteration 538/1000 | Loss: 0.00030706
Iteration 539/1000 | Loss: 0.00009307
Iteration 540/1000 | Loss: 0.00006555
Iteration 541/1000 | Loss: 0.00005293
Iteration 542/1000 | Loss: 0.00004880
Iteration 543/1000 | Loss: 0.00004727
Iteration 544/1000 | Loss: 0.00074374
Iteration 545/1000 | Loss: 0.00039812
Iteration 546/1000 | Loss: 0.00034708
Iteration 547/1000 | Loss: 0.00032799
Iteration 548/1000 | Loss: 0.00023353
Iteration 549/1000 | Loss: 0.00007605
Iteration 550/1000 | Loss: 0.00005509
Iteration 551/1000 | Loss: 0.00004607
Iteration 552/1000 | Loss: 0.00027979
Iteration 553/1000 | Loss: 0.00004503
Iteration 554/1000 | Loss: 0.00012294
Iteration 555/1000 | Loss: 0.00004156
Iteration 556/1000 | Loss: 0.00005145
Iteration 557/1000 | Loss: 0.00004476
Iteration 558/1000 | Loss: 0.00004251
Iteration 559/1000 | Loss: 0.00031491
Iteration 560/1000 | Loss: 0.00170036
Iteration 561/1000 | Loss: 0.00085747
Iteration 562/1000 | Loss: 0.00005591
Iteration 563/1000 | Loss: 0.00004691
Iteration 564/1000 | Loss: 0.00005105
Iteration 565/1000 | Loss: 0.00060078
Iteration 566/1000 | Loss: 0.00115018
Iteration 567/1000 | Loss: 0.00069367
Iteration 568/1000 | Loss: 0.00015914
Iteration 569/1000 | Loss: 0.00026493
Iteration 570/1000 | Loss: 0.00005280
Iteration 571/1000 | Loss: 0.00004737
Iteration 572/1000 | Loss: 0.00004145
Iteration 573/1000 | Loss: 0.00003819
Iteration 574/1000 | Loss: 0.00003678
Iteration 575/1000 | Loss: 0.00113663
Iteration 576/1000 | Loss: 0.00010065
Iteration 577/1000 | Loss: 0.00005212
Iteration 578/1000 | Loss: 0.00061684
Iteration 579/1000 | Loss: 0.00028819
Iteration 580/1000 | Loss: 0.00045903
Iteration 581/1000 | Loss: 0.00096164
Iteration 582/1000 | Loss: 0.00029894
Iteration 583/1000 | Loss: 0.00021274
Iteration 584/1000 | Loss: 0.00004646
Iteration 585/1000 | Loss: 0.00004374
Iteration 586/1000 | Loss: 0.00004222
Iteration 587/1000 | Loss: 0.00033097
Iteration 588/1000 | Loss: 0.00064440
Iteration 589/1000 | Loss: 0.00045051
Iteration 590/1000 | Loss: 0.00027058
Iteration 591/1000 | Loss: 0.00011017
Iteration 592/1000 | Loss: 0.00037202
Iteration 593/1000 | Loss: 0.00005803
Iteration 594/1000 | Loss: 0.00004911
Iteration 595/1000 | Loss: 0.00058829
Iteration 596/1000 | Loss: 0.00096519
Iteration 597/1000 | Loss: 0.00014847
Iteration 598/1000 | Loss: 0.00029288
Iteration 599/1000 | Loss: 0.00037422
Iteration 600/1000 | Loss: 0.00007980
Iteration 601/1000 | Loss: 0.00006624
Iteration 602/1000 | Loss: 0.00007188
Iteration 603/1000 | Loss: 0.00004449
Iteration 604/1000 | Loss: 0.00004122
Iteration 605/1000 | Loss: 0.00003919
Iteration 606/1000 | Loss: 0.00003692
Iteration 607/1000 | Loss: 0.00003495
Iteration 608/1000 | Loss: 0.00003369
Iteration 609/1000 | Loss: 0.00054334
Iteration 610/1000 | Loss: 0.00249285
Iteration 611/1000 | Loss: 0.00214490
Iteration 612/1000 | Loss: 0.00070521
Iteration 613/1000 | Loss: 0.00005914
Iteration 614/1000 | Loss: 0.00028575
Iteration 615/1000 | Loss: 0.00024639
Iteration 616/1000 | Loss: 0.00005156
Iteration 617/1000 | Loss: 0.00004259
Iteration 618/1000 | Loss: 0.00003766
Iteration 619/1000 | Loss: 0.00003502
Iteration 620/1000 | Loss: 0.00003369
Iteration 621/1000 | Loss: 0.00003243
Iteration 622/1000 | Loss: 0.00058618
Iteration 623/1000 | Loss: 0.00171505
Iteration 624/1000 | Loss: 0.00080082
Iteration 625/1000 | Loss: 0.00005437
Iteration 626/1000 | Loss: 0.00004084
Iteration 627/1000 | Loss: 0.00003670
Iteration 628/1000 | Loss: 0.00003491
Iteration 629/1000 | Loss: 0.00003315
Iteration 630/1000 | Loss: 0.00032853
Iteration 631/1000 | Loss: 0.00004068
Iteration 632/1000 | Loss: 0.00003571
Iteration 633/1000 | Loss: 0.00003376
Iteration 634/1000 | Loss: 0.00003314
Iteration 635/1000 | Loss: 0.00058043
Iteration 636/1000 | Loss: 0.00029804
Iteration 637/1000 | Loss: 0.00010682
Iteration 638/1000 | Loss: 0.00031025
Iteration 639/1000 | Loss: 0.00024512
Iteration 640/1000 | Loss: 0.00023838
Iteration 641/1000 | Loss: 0.00005273
Iteration 642/1000 | Loss: 0.00004558
Iteration 643/1000 | Loss: 0.00004087
Iteration 644/1000 | Loss: 0.00003828
Iteration 645/1000 | Loss: 0.00030433
Iteration 646/1000 | Loss: 0.00059629
Iteration 647/1000 | Loss: 0.00041619
Iteration 648/1000 | Loss: 0.00004127
Iteration 649/1000 | Loss: 0.00032391
Iteration 650/1000 | Loss: 0.00004268
Iteration 651/1000 | Loss: 0.00003758
Iteration 652/1000 | Loss: 0.00003602
Iteration 653/1000 | Loss: 0.00033871
Iteration 654/1000 | Loss: 0.00038091
Iteration 655/1000 | Loss: 0.00074360
Iteration 656/1000 | Loss: 0.00027179
Iteration 657/1000 | Loss: 0.00026726
Iteration 658/1000 | Loss: 0.00030354
Iteration 659/1000 | Loss: 0.00036042
Iteration 660/1000 | Loss: 0.00035423
Iteration 661/1000 | Loss: 0.00032086
Iteration 662/1000 | Loss: 0.00027526
Iteration 663/1000 | Loss: 0.00004676
Iteration 664/1000 | Loss: 0.00046303
Iteration 665/1000 | Loss: 0.00029900
Iteration 666/1000 | Loss: 0.00020863
Iteration 667/1000 | Loss: 0.00020935
Iteration 668/1000 | Loss: 0.00003954
Iteration 669/1000 | Loss: 0.00003531
Iteration 670/1000 | Loss: 0.00018905
Iteration 671/1000 | Loss: 0.00003276
Iteration 672/1000 | Loss: 0.00003147
Iteration 673/1000 | Loss: 0.00003078
Iteration 674/1000 | Loss: 0.00003027
Iteration 675/1000 | Loss: 0.00002967
Iteration 676/1000 | Loss: 0.00058850
Iteration 677/1000 | Loss: 0.00197215
Iteration 678/1000 | Loss: 0.00102304
Iteration 679/1000 | Loss: 0.00037075
Iteration 680/1000 | Loss: 0.00049865
Iteration 681/1000 | Loss: 0.00012546
Iteration 682/1000 | Loss: 0.00196511
Iteration 683/1000 | Loss: 0.00157199
Iteration 684/1000 | Loss: 0.00022334
Iteration 685/1000 | Loss: 0.00003903
Iteration 686/1000 | Loss: 0.00003496
Iteration 687/1000 | Loss: 0.00003247
Iteration 688/1000 | Loss: 0.00003125
Iteration 689/1000 | Loss: 0.00003038
Iteration 690/1000 | Loss: 0.00002960
Iteration 691/1000 | Loss: 0.00002906
Iteration 692/1000 | Loss: 0.00002870
Iteration 693/1000 | Loss: 0.00002868
Iteration 694/1000 | Loss: 0.00031212
Iteration 695/1000 | Loss: 0.00166477
Iteration 696/1000 | Loss: 0.00105404
Iteration 697/1000 | Loss: 0.00003696
Iteration 698/1000 | Loss: 0.00003234
Iteration 699/1000 | Loss: 0.00003088
Iteration 700/1000 | Loss: 0.00003010
Iteration 701/1000 | Loss: 0.00002952
Iteration 702/1000 | Loss: 0.00002920
Iteration 703/1000 | Loss: 0.00002873
Iteration 704/1000 | Loss: 0.00194115
Iteration 705/1000 | Loss: 0.00034263
Iteration 706/1000 | Loss: 0.00007552
Iteration 707/1000 | Loss: 0.00061895
Iteration 708/1000 | Loss: 0.00037931
Iteration 709/1000 | Loss: 0.00011902
Iteration 710/1000 | Loss: 0.00004752
Iteration 711/1000 | Loss: 0.00004354
Iteration 712/1000 | Loss: 0.00142468
Iteration 713/1000 | Loss: 0.00250417
Iteration 714/1000 | Loss: 0.00158673
Iteration 715/1000 | Loss: 0.00055322
Iteration 716/1000 | Loss: 0.00012002
Iteration 717/1000 | Loss: 0.00019939
Iteration 718/1000 | Loss: 0.00018809
Iteration 719/1000 | Loss: 0.00015614
Iteration 720/1000 | Loss: 0.00006405
Iteration 721/1000 | Loss: 0.00005430
Iteration 722/1000 | Loss: 0.00033744
Iteration 723/1000 | Loss: 0.00012365
Iteration 724/1000 | Loss: 0.00004109
Iteration 725/1000 | Loss: 0.00003804
Iteration 726/1000 | Loss: 0.00003644
Iteration 727/1000 | Loss: 0.00003414
Iteration 728/1000 | Loss: 0.00003275
Iteration 729/1000 | Loss: 0.00003174
Iteration 730/1000 | Loss: 0.00003098
Iteration 731/1000 | Loss: 0.00003032
Iteration 732/1000 | Loss: 0.00002951
Iteration 733/1000 | Loss: 0.00002857
Iteration 734/1000 | Loss: 0.00002769
Iteration 735/1000 | Loss: 0.00002720
Iteration 736/1000 | Loss: 0.00002671
Iteration 737/1000 | Loss: 0.00002637
Iteration 738/1000 | Loss: 0.00002607
Iteration 739/1000 | Loss: 0.00002580
Iteration 740/1000 | Loss: 0.00030508
Iteration 741/1000 | Loss: 0.00264876
Iteration 742/1000 | Loss: 0.00066706
Iteration 743/1000 | Loss: 0.00174900
Iteration 744/1000 | Loss: 0.00050984
Iteration 745/1000 | Loss: 0.00020963
Iteration 746/1000 | Loss: 0.00006386
Iteration 747/1000 | Loss: 0.00004987
Iteration 748/1000 | Loss: 0.00004254
Iteration 749/1000 | Loss: 0.00025552
Iteration 750/1000 | Loss: 0.00026415
Iteration 751/1000 | Loss: 0.00015597
Iteration 752/1000 | Loss: 0.00003963
Iteration 753/1000 | Loss: 0.00003650
Iteration 754/1000 | Loss: 0.00003431
Iteration 755/1000 | Loss: 0.00027670
Iteration 756/1000 | Loss: 0.00016444
Iteration 757/1000 | Loss: 0.00003413
Iteration 758/1000 | Loss: 0.00003092
Iteration 759/1000 | Loss: 0.00028454
Iteration 760/1000 | Loss: 0.00029394
Iteration 761/1000 | Loss: 0.00020489
Iteration 762/1000 | Loss: 0.00027104
Iteration 763/1000 | Loss: 0.00017385
Iteration 764/1000 | Loss: 0.00028202
Iteration 765/1000 | Loss: 0.00011397
Iteration 766/1000 | Loss: 0.00020644
Iteration 767/1000 | Loss: 0.00003897
Iteration 768/1000 | Loss: 0.00003371
Iteration 769/1000 | Loss: 0.00027450
Iteration 770/1000 | Loss: 0.00004093
Iteration 771/1000 | Loss: 0.00003604
Iteration 772/1000 | Loss: 0.00003228
Iteration 773/1000 | Loss: 0.00003020
Iteration 774/1000 | Loss: 0.00002858
Iteration 775/1000 | Loss: 0.00007043
Iteration 776/1000 | Loss: 0.00003495
Iteration 777/1000 | Loss: 0.00002923
Iteration 778/1000 | Loss: 0.00002725
Iteration 779/1000 | Loss: 0.00002668
Iteration 780/1000 | Loss: 0.00002619
Iteration 781/1000 | Loss: 0.00002554
Iteration 782/1000 | Loss: 0.00002503
Iteration 783/1000 | Loss: 0.00002476
Iteration 784/1000 | Loss: 0.00002462
Iteration 785/1000 | Loss: 0.00002460
Iteration 786/1000 | Loss: 0.00002454
Iteration 787/1000 | Loss: 0.00002453
Iteration 788/1000 | Loss: 0.00002451
Iteration 789/1000 | Loss: 0.00002448
Iteration 790/1000 | Loss: 0.00002446
Iteration 791/1000 | Loss: 0.00002446
Iteration 792/1000 | Loss: 0.00002445
Iteration 793/1000 | Loss: 0.00002445
Iteration 794/1000 | Loss: 0.00002444
Iteration 795/1000 | Loss: 0.00002443
Iteration 796/1000 | Loss: 0.00002443
Iteration 797/1000 | Loss: 0.00002442
Iteration 798/1000 | Loss: 0.00002434
Iteration 799/1000 | Loss: 0.00002433
Iteration 800/1000 | Loss: 0.00002432
Iteration 801/1000 | Loss: 0.00002432
Iteration 802/1000 | Loss: 0.00002432
Iteration 803/1000 | Loss: 0.00002432
Iteration 804/1000 | Loss: 0.00002432
Iteration 805/1000 | Loss: 0.00002432
Iteration 806/1000 | Loss: 0.00002432
Iteration 807/1000 | Loss: 0.00002432
Iteration 808/1000 | Loss: 0.00002428
Iteration 809/1000 | Loss: 0.00002427
Iteration 810/1000 | Loss: 0.00002423
Iteration 811/1000 | Loss: 0.00002423
Iteration 812/1000 | Loss: 0.00002423
Iteration 813/1000 | Loss: 0.00002423
Iteration 814/1000 | Loss: 0.00002423
Iteration 815/1000 | Loss: 0.00002423
Iteration 816/1000 | Loss: 0.00002423
Iteration 817/1000 | Loss: 0.00002423
Iteration 818/1000 | Loss: 0.00002422
Iteration 819/1000 | Loss: 0.00002421
Iteration 820/1000 | Loss: 0.00002420
Iteration 821/1000 | Loss: 0.00002420
Iteration 822/1000 | Loss: 0.00002420
Iteration 823/1000 | Loss: 0.00002419
Iteration 824/1000 | Loss: 0.00002419
Iteration 825/1000 | Loss: 0.00002419
Iteration 826/1000 | Loss: 0.00002419
Iteration 827/1000 | Loss: 0.00002419
Iteration 828/1000 | Loss: 0.00002419
Iteration 829/1000 | Loss: 0.00002419
Iteration 830/1000 | Loss: 0.00002419
Iteration 831/1000 | Loss: 0.00002419
Iteration 832/1000 | Loss: 0.00002419
Iteration 833/1000 | Loss: 0.00002419
Iteration 834/1000 | Loss: 0.00002419
Iteration 835/1000 | Loss: 0.00002418
Iteration 836/1000 | Loss: 0.00002418
Iteration 837/1000 | Loss: 0.00002418
Iteration 838/1000 | Loss: 0.00002418
Iteration 839/1000 | Loss: 0.00002418
Iteration 840/1000 | Loss: 0.00002418
Iteration 841/1000 | Loss: 0.00002418
Iteration 842/1000 | Loss: 0.00002417
Iteration 843/1000 | Loss: 0.00002417
Iteration 844/1000 | Loss: 0.00002417
Iteration 845/1000 | Loss: 0.00002417
Iteration 846/1000 | Loss: 0.00002417
Iteration 847/1000 | Loss: 0.00002417
Iteration 848/1000 | Loss: 0.00002416
Iteration 849/1000 | Loss: 0.00002416
Iteration 850/1000 | Loss: 0.00002416
Iteration 851/1000 | Loss: 0.00002416
Iteration 852/1000 | Loss: 0.00002415
Iteration 853/1000 | Loss: 0.00002415
Iteration 854/1000 | Loss: 0.00002415
Iteration 855/1000 | Loss: 0.00002415
Iteration 856/1000 | Loss: 0.00002415
Iteration 857/1000 | Loss: 0.00002415
Iteration 858/1000 | Loss: 0.00002414
Iteration 859/1000 | Loss: 0.00002414
Iteration 860/1000 | Loss: 0.00002414
Iteration 861/1000 | Loss: 0.00002413
Iteration 862/1000 | Loss: 0.00002413
Iteration 863/1000 | Loss: 0.00002413
Iteration 864/1000 | Loss: 0.00002413
Iteration 865/1000 | Loss: 0.00002413
Iteration 866/1000 | Loss: 0.00002413
Iteration 867/1000 | Loss: 0.00002413
Iteration 868/1000 | Loss: 0.00002413
Iteration 869/1000 | Loss: 0.00002413
Iteration 870/1000 | Loss: 0.00002413
Iteration 871/1000 | Loss: 0.00002413
Iteration 872/1000 | Loss: 0.00002413
Iteration 873/1000 | Loss: 0.00002413
Iteration 874/1000 | Loss: 0.00002412
Iteration 875/1000 | Loss: 0.00002412
Iteration 876/1000 | Loss: 0.00002412
Iteration 877/1000 | Loss: 0.00002412
Iteration 878/1000 | Loss: 0.00002411
Iteration 879/1000 | Loss: 0.00002411
Iteration 880/1000 | Loss: 0.00002411
Iteration 881/1000 | Loss: 0.00002411
Iteration 882/1000 | Loss: 0.00002411
Iteration 883/1000 | Loss: 0.00002411
Iteration 884/1000 | Loss: 0.00002411
Iteration 885/1000 | Loss: 0.00002411
Iteration 886/1000 | Loss: 0.00002411
Iteration 887/1000 | Loss: 0.00002411
Iteration 888/1000 | Loss: 0.00002411
Iteration 889/1000 | Loss: 0.00002411
Iteration 890/1000 | Loss: 0.00002411
Iteration 891/1000 | Loss: 0.00002411
Iteration 892/1000 | Loss: 0.00002411
Iteration 893/1000 | Loss: 0.00002411
Iteration 894/1000 | Loss: 0.00002411
Iteration 895/1000 | Loss: 0.00002411
Iteration 896/1000 | Loss: 0.00002411
Iteration 897/1000 | Loss: 0.00002411
Iteration 898/1000 | Loss: 0.00002411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 898. Stopping optimization.
Last 5 losses: [2.4109394871629775e-05, 2.4109394871629775e-05, 2.4109394871629775e-05, 2.4109394871629775e-05, 2.4109394871629775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4109394871629775e-05

Optimization complete. Final v2v error: 3.8901917934417725 mm

Highest mean error: 5.835537433624268 mm for frame 67

Lowest mean error: 3.7025723457336426 mm for frame 117

Saving results

Total time: 1121.868924856186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067014
Iteration 2/25 | Loss: 0.00436045
Iteration 3/25 | Loss: 0.00276371
Iteration 4/25 | Loss: 0.00228807
Iteration 5/25 | Loss: 0.00206558
Iteration 6/25 | Loss: 0.00181253
Iteration 7/25 | Loss: 0.00175350
Iteration 8/25 | Loss: 0.00173685
Iteration 9/25 | Loss: 0.00160119
Iteration 10/25 | Loss: 0.00144663
Iteration 11/25 | Loss: 0.00141106
Iteration 12/25 | Loss: 0.00139027
Iteration 13/25 | Loss: 0.00143197
Iteration 14/25 | Loss: 0.00139594
Iteration 15/25 | Loss: 0.00132452
Iteration 16/25 | Loss: 0.00130211
Iteration 17/25 | Loss: 0.00128512
Iteration 18/25 | Loss: 0.00127696
Iteration 19/25 | Loss: 0.00126990
Iteration 20/25 | Loss: 0.00128129
Iteration 21/25 | Loss: 0.00128440
Iteration 22/25 | Loss: 0.00127346
Iteration 23/25 | Loss: 0.00123867
Iteration 24/25 | Loss: 0.00123016
Iteration 25/25 | Loss: 0.00122545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48390591
Iteration 2/25 | Loss: 0.00379221
Iteration 3/25 | Loss: 0.00254147
Iteration 4/25 | Loss: 0.00254140
Iteration 5/25 | Loss: 0.00254139
Iteration 6/25 | Loss: 0.00254139
Iteration 7/25 | Loss: 0.00254139
Iteration 8/25 | Loss: 0.00254139
Iteration 9/25 | Loss: 0.00254139
Iteration 10/25 | Loss: 0.00254139
Iteration 11/25 | Loss: 0.00254139
Iteration 12/25 | Loss: 0.00254139
Iteration 13/25 | Loss: 0.00254139
Iteration 14/25 | Loss: 0.00254139
Iteration 15/25 | Loss: 0.00254139
Iteration 16/25 | Loss: 0.00254139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025413930416107178, 0.0025413930416107178, 0.0025413930416107178, 0.0025413930416107178, 0.0025413930416107178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025413930416107178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254139
Iteration 2/1000 | Loss: 0.00224185
Iteration 3/1000 | Loss: 0.00074097
Iteration 4/1000 | Loss: 0.00045861
Iteration 5/1000 | Loss: 0.00062118
Iteration 6/1000 | Loss: 0.00026768
Iteration 7/1000 | Loss: 0.00096445
Iteration 8/1000 | Loss: 0.00033650
Iteration 9/1000 | Loss: 0.00233017
Iteration 10/1000 | Loss: 0.00091744
Iteration 11/1000 | Loss: 0.00070946
Iteration 12/1000 | Loss: 0.00035728
Iteration 13/1000 | Loss: 0.00047925
Iteration 14/1000 | Loss: 0.00051593
Iteration 15/1000 | Loss: 0.00096588
Iteration 16/1000 | Loss: 0.00108738
Iteration 17/1000 | Loss: 0.00152336
Iteration 18/1000 | Loss: 0.00073680
Iteration 19/1000 | Loss: 0.00034793
Iteration 20/1000 | Loss: 0.00023947
Iteration 21/1000 | Loss: 0.00057604
Iteration 22/1000 | Loss: 0.00019087
Iteration 23/1000 | Loss: 0.00052291
Iteration 24/1000 | Loss: 0.00092589
Iteration 25/1000 | Loss: 0.00058394
Iteration 26/1000 | Loss: 0.00058291
Iteration 27/1000 | Loss: 0.00083227
Iteration 28/1000 | Loss: 0.00042943
Iteration 29/1000 | Loss: 0.00018312
Iteration 30/1000 | Loss: 0.00024209
Iteration 31/1000 | Loss: 0.00016361
Iteration 32/1000 | Loss: 0.00015753
Iteration 33/1000 | Loss: 0.00061838
Iteration 34/1000 | Loss: 0.00022828
Iteration 35/1000 | Loss: 0.00018860
Iteration 36/1000 | Loss: 0.00060155
Iteration 37/1000 | Loss: 0.00412468
Iteration 38/1000 | Loss: 0.00151606
Iteration 39/1000 | Loss: 0.00269792
Iteration 40/1000 | Loss: 0.00145272
Iteration 41/1000 | Loss: 0.00069824
Iteration 42/1000 | Loss: 0.00328588
Iteration 43/1000 | Loss: 0.00100993
Iteration 44/1000 | Loss: 0.00188906
Iteration 45/1000 | Loss: 0.00019095
Iteration 46/1000 | Loss: 0.00050054
Iteration 47/1000 | Loss: 0.00010110
Iteration 48/1000 | Loss: 0.00010214
Iteration 49/1000 | Loss: 0.00006056
Iteration 50/1000 | Loss: 0.00015571
Iteration 51/1000 | Loss: 0.00005167
Iteration 52/1000 | Loss: 0.00035063
Iteration 53/1000 | Loss: 0.00004545
Iteration 54/1000 | Loss: 0.00004199
Iteration 55/1000 | Loss: 0.00003932
Iteration 56/1000 | Loss: 0.00008917
Iteration 57/1000 | Loss: 0.00003570
Iteration 58/1000 | Loss: 0.00003439
Iteration 59/1000 | Loss: 0.00034295
Iteration 60/1000 | Loss: 0.00011312
Iteration 61/1000 | Loss: 0.00003273
Iteration 62/1000 | Loss: 0.00003207
Iteration 63/1000 | Loss: 0.00003158
Iteration 64/1000 | Loss: 0.00003114
Iteration 65/1000 | Loss: 0.00003092
Iteration 66/1000 | Loss: 0.00003076
Iteration 67/1000 | Loss: 0.00003076
Iteration 68/1000 | Loss: 0.00003075
Iteration 69/1000 | Loss: 0.00003069
Iteration 70/1000 | Loss: 0.00003069
Iteration 71/1000 | Loss: 0.00003065
Iteration 72/1000 | Loss: 0.00003056
Iteration 73/1000 | Loss: 0.00003056
Iteration 74/1000 | Loss: 0.00003055
Iteration 75/1000 | Loss: 0.00003055
Iteration 76/1000 | Loss: 0.00003052
Iteration 77/1000 | Loss: 0.00003049
Iteration 78/1000 | Loss: 0.00003047
Iteration 79/1000 | Loss: 0.00003044
Iteration 80/1000 | Loss: 0.00003044
Iteration 81/1000 | Loss: 0.00003044
Iteration 82/1000 | Loss: 0.00003043
Iteration 83/1000 | Loss: 0.00003043
Iteration 84/1000 | Loss: 0.00003043
Iteration 85/1000 | Loss: 0.00003043
Iteration 86/1000 | Loss: 0.00003043
Iteration 87/1000 | Loss: 0.00003043
Iteration 88/1000 | Loss: 0.00003043
Iteration 89/1000 | Loss: 0.00003043
Iteration 90/1000 | Loss: 0.00003043
Iteration 91/1000 | Loss: 0.00003043
Iteration 92/1000 | Loss: 0.00003043
Iteration 93/1000 | Loss: 0.00003043
Iteration 94/1000 | Loss: 0.00003043
Iteration 95/1000 | Loss: 0.00003043
Iteration 96/1000 | Loss: 0.00003043
Iteration 97/1000 | Loss: 0.00003042
Iteration 98/1000 | Loss: 0.00003042
Iteration 99/1000 | Loss: 0.00003041
Iteration 100/1000 | Loss: 0.00003041
Iteration 101/1000 | Loss: 0.00003041
Iteration 102/1000 | Loss: 0.00003041
Iteration 103/1000 | Loss: 0.00003041
Iteration 104/1000 | Loss: 0.00003041
Iteration 105/1000 | Loss: 0.00003041
Iteration 106/1000 | Loss: 0.00003041
Iteration 107/1000 | Loss: 0.00003041
Iteration 108/1000 | Loss: 0.00003041
Iteration 109/1000 | Loss: 0.00003040
Iteration 110/1000 | Loss: 0.00003040
Iteration 111/1000 | Loss: 0.00003038
Iteration 112/1000 | Loss: 0.00003037
Iteration 113/1000 | Loss: 0.00003037
Iteration 114/1000 | Loss: 0.00003037
Iteration 115/1000 | Loss: 0.00003037
Iteration 116/1000 | Loss: 0.00003037
Iteration 117/1000 | Loss: 0.00003037
Iteration 118/1000 | Loss: 0.00003037
Iteration 119/1000 | Loss: 0.00003037
Iteration 120/1000 | Loss: 0.00003037
Iteration 121/1000 | Loss: 0.00003037
Iteration 122/1000 | Loss: 0.00003036
Iteration 123/1000 | Loss: 0.00003036
Iteration 124/1000 | Loss: 0.00003036
Iteration 125/1000 | Loss: 0.00003036
Iteration 126/1000 | Loss: 0.00003036
Iteration 127/1000 | Loss: 0.00003036
Iteration 128/1000 | Loss: 0.00003036
Iteration 129/1000 | Loss: 0.00003035
Iteration 130/1000 | Loss: 0.00003034
Iteration 131/1000 | Loss: 0.00003033
Iteration 132/1000 | Loss: 0.00003033
Iteration 133/1000 | Loss: 0.00003032
Iteration 134/1000 | Loss: 0.00003031
Iteration 135/1000 | Loss: 0.00003030
Iteration 136/1000 | Loss: 0.00003030
Iteration 137/1000 | Loss: 0.00003030
Iteration 138/1000 | Loss: 0.00003030
Iteration 139/1000 | Loss: 0.00003030
Iteration 140/1000 | Loss: 0.00003029
Iteration 141/1000 | Loss: 0.00003029
Iteration 142/1000 | Loss: 0.00003029
Iteration 143/1000 | Loss: 0.00003029
Iteration 144/1000 | Loss: 0.00003029
Iteration 145/1000 | Loss: 0.00003028
Iteration 146/1000 | Loss: 0.00003027
Iteration 147/1000 | Loss: 0.00003027
Iteration 148/1000 | Loss: 0.00003027
Iteration 149/1000 | Loss: 0.00003026
Iteration 150/1000 | Loss: 0.00003026
Iteration 151/1000 | Loss: 0.00003026
Iteration 152/1000 | Loss: 0.00003026
Iteration 153/1000 | Loss: 0.00003026
Iteration 154/1000 | Loss: 0.00003026
Iteration 155/1000 | Loss: 0.00003026
Iteration 156/1000 | Loss: 0.00003025
Iteration 157/1000 | Loss: 0.00003025
Iteration 158/1000 | Loss: 0.00003025
Iteration 159/1000 | Loss: 0.00003025
Iteration 160/1000 | Loss: 0.00003025
Iteration 161/1000 | Loss: 0.00003025
Iteration 162/1000 | Loss: 0.00003025
Iteration 163/1000 | Loss: 0.00003024
Iteration 164/1000 | Loss: 0.00003024
Iteration 165/1000 | Loss: 0.00003024
Iteration 166/1000 | Loss: 0.00003024
Iteration 167/1000 | Loss: 0.00003024
Iteration 168/1000 | Loss: 0.00003024
Iteration 169/1000 | Loss: 0.00003024
Iteration 170/1000 | Loss: 0.00003024
Iteration 171/1000 | Loss: 0.00003023
Iteration 172/1000 | Loss: 0.00003023
Iteration 173/1000 | Loss: 0.00003023
Iteration 174/1000 | Loss: 0.00003023
Iteration 175/1000 | Loss: 0.00003023
Iteration 176/1000 | Loss: 0.00003023
Iteration 177/1000 | Loss: 0.00003023
Iteration 178/1000 | Loss: 0.00003023
Iteration 179/1000 | Loss: 0.00003023
Iteration 180/1000 | Loss: 0.00003023
Iteration 181/1000 | Loss: 0.00003023
Iteration 182/1000 | Loss: 0.00003022
Iteration 183/1000 | Loss: 0.00003022
Iteration 184/1000 | Loss: 0.00003022
Iteration 185/1000 | Loss: 0.00003022
Iteration 186/1000 | Loss: 0.00003022
Iteration 187/1000 | Loss: 0.00003022
Iteration 188/1000 | Loss: 0.00003022
Iteration 189/1000 | Loss: 0.00003022
Iteration 190/1000 | Loss: 0.00003022
Iteration 191/1000 | Loss: 0.00003022
Iteration 192/1000 | Loss: 0.00003022
Iteration 193/1000 | Loss: 0.00003022
Iteration 194/1000 | Loss: 0.00003022
Iteration 195/1000 | Loss: 0.00003021
Iteration 196/1000 | Loss: 0.00003021
Iteration 197/1000 | Loss: 0.00003021
Iteration 198/1000 | Loss: 0.00003021
Iteration 199/1000 | Loss: 0.00003021
Iteration 200/1000 | Loss: 0.00003021
Iteration 201/1000 | Loss: 0.00003021
Iteration 202/1000 | Loss: 0.00003020
Iteration 203/1000 | Loss: 0.00003020
Iteration 204/1000 | Loss: 0.00003020
Iteration 205/1000 | Loss: 0.00003020
Iteration 206/1000 | Loss: 0.00003020
Iteration 207/1000 | Loss: 0.00003020
Iteration 208/1000 | Loss: 0.00003020
Iteration 209/1000 | Loss: 0.00003020
Iteration 210/1000 | Loss: 0.00003020
Iteration 211/1000 | Loss: 0.00003020
Iteration 212/1000 | Loss: 0.00003020
Iteration 213/1000 | Loss: 0.00003020
Iteration 214/1000 | Loss: 0.00003020
Iteration 215/1000 | Loss: 0.00003020
Iteration 216/1000 | Loss: 0.00003020
Iteration 217/1000 | Loss: 0.00003020
Iteration 218/1000 | Loss: 0.00003020
Iteration 219/1000 | Loss: 0.00003019
Iteration 220/1000 | Loss: 0.00003019
Iteration 221/1000 | Loss: 0.00003019
Iteration 222/1000 | Loss: 0.00003019
Iteration 223/1000 | Loss: 0.00003019
Iteration 224/1000 | Loss: 0.00003019
Iteration 225/1000 | Loss: 0.00003019
Iteration 226/1000 | Loss: 0.00003019
Iteration 227/1000 | Loss: 0.00003019
Iteration 228/1000 | Loss: 0.00003019
Iteration 229/1000 | Loss: 0.00003019
Iteration 230/1000 | Loss: 0.00003019
Iteration 231/1000 | Loss: 0.00003019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [3.0191447876859456e-05, 3.0191447876859456e-05, 3.0191447876859456e-05, 3.0191447876859456e-05, 3.0191447876859456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0191447876859456e-05

Optimization complete. Final v2v error: 3.9747650623321533 mm

Highest mean error: 5.456643581390381 mm for frame 63

Lowest mean error: 3.2917709350585938 mm for frame 88

Saving results

Total time: 147.56306147575378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861324
Iteration 2/25 | Loss: 0.00097135
Iteration 3/25 | Loss: 0.00081121
Iteration 4/25 | Loss: 0.00078737
Iteration 5/25 | Loss: 0.00078279
Iteration 6/25 | Loss: 0.00078173
Iteration 7/25 | Loss: 0.00078155
Iteration 8/25 | Loss: 0.00078155
Iteration 9/25 | Loss: 0.00078155
Iteration 10/25 | Loss: 0.00078155
Iteration 11/25 | Loss: 0.00078155
Iteration 12/25 | Loss: 0.00078155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000781548151280731, 0.000781548151280731, 0.000781548151280731, 0.000781548151280731, 0.000781548151280731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000781548151280731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46138513
Iteration 2/25 | Loss: 0.00037687
Iteration 3/25 | Loss: 0.00037687
Iteration 4/25 | Loss: 0.00037687
Iteration 5/25 | Loss: 0.00037687
Iteration 6/25 | Loss: 0.00037687
Iteration 7/25 | Loss: 0.00037687
Iteration 8/25 | Loss: 0.00037687
Iteration 9/25 | Loss: 0.00037687
Iteration 10/25 | Loss: 0.00037687
Iteration 11/25 | Loss: 0.00037687
Iteration 12/25 | Loss: 0.00037687
Iteration 13/25 | Loss: 0.00037687
Iteration 14/25 | Loss: 0.00037687
Iteration 15/25 | Loss: 0.00037687
Iteration 16/25 | Loss: 0.00037687
Iteration 17/25 | Loss: 0.00037687
Iteration 18/25 | Loss: 0.00037687
Iteration 19/25 | Loss: 0.00037687
Iteration 20/25 | Loss: 0.00037687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003768702154047787, 0.0003768702154047787, 0.0003768702154047787, 0.0003768702154047787, 0.0003768702154047787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003768702154047787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037687
Iteration 2/1000 | Loss: 0.00002869
Iteration 3/1000 | Loss: 0.00002113
Iteration 4/1000 | Loss: 0.00001991
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001787
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001749
Iteration 14/1000 | Loss: 0.00001749
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001739
Iteration 24/1000 | Loss: 0.00001739
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001735
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001732
Iteration 46/1000 | Loss: 0.00001732
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001731
Iteration 51/1000 | Loss: 0.00001731
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001730
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001727
Iteration 71/1000 | Loss: 0.00001727
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001727
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001727
Iteration 79/1000 | Loss: 0.00001727
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001725
Iteration 88/1000 | Loss: 0.00001725
Iteration 89/1000 | Loss: 0.00001725
Iteration 90/1000 | Loss: 0.00001725
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001724
Iteration 94/1000 | Loss: 0.00001724
Iteration 95/1000 | Loss: 0.00001724
Iteration 96/1000 | Loss: 0.00001724
Iteration 97/1000 | Loss: 0.00001724
Iteration 98/1000 | Loss: 0.00001724
Iteration 99/1000 | Loss: 0.00001724
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001723
Iteration 102/1000 | Loss: 0.00001723
Iteration 103/1000 | Loss: 0.00001723
Iteration 104/1000 | Loss: 0.00001723
Iteration 105/1000 | Loss: 0.00001723
Iteration 106/1000 | Loss: 0.00001723
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001722
Iteration 109/1000 | Loss: 0.00001722
Iteration 110/1000 | Loss: 0.00001722
Iteration 111/1000 | Loss: 0.00001722
Iteration 112/1000 | Loss: 0.00001722
Iteration 113/1000 | Loss: 0.00001722
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001721
Iteration 119/1000 | Loss: 0.00001721
Iteration 120/1000 | Loss: 0.00001721
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001720
Iteration 146/1000 | Loss: 0.00001720
Iteration 147/1000 | Loss: 0.00001720
Iteration 148/1000 | Loss: 0.00001720
Iteration 149/1000 | Loss: 0.00001720
Iteration 150/1000 | Loss: 0.00001720
Iteration 151/1000 | Loss: 0.00001720
Iteration 152/1000 | Loss: 0.00001720
Iteration 153/1000 | Loss: 0.00001720
Iteration 154/1000 | Loss: 0.00001720
Iteration 155/1000 | Loss: 0.00001720
Iteration 156/1000 | Loss: 0.00001720
Iteration 157/1000 | Loss: 0.00001720
Iteration 158/1000 | Loss: 0.00001720
Iteration 159/1000 | Loss: 0.00001720
Iteration 160/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.719545434752945e-05, 1.719545434752945e-05, 1.719545434752945e-05, 1.719545434752945e-05, 1.719545434752945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.719545434752945e-05

Optimization complete. Final v2v error: 3.5178494453430176 mm

Highest mean error: 4.020026206970215 mm for frame 158

Lowest mean error: 3.124234437942505 mm for frame 20

Saving results

Total time: 33.387288093566895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479772
Iteration 2/25 | Loss: 0.00110300
Iteration 3/25 | Loss: 0.00082430
Iteration 4/25 | Loss: 0.00077478
Iteration 5/25 | Loss: 0.00076528
Iteration 6/25 | Loss: 0.00076315
Iteration 7/25 | Loss: 0.00076257
Iteration 8/25 | Loss: 0.00076257
Iteration 9/25 | Loss: 0.00076257
Iteration 10/25 | Loss: 0.00076257
Iteration 11/25 | Loss: 0.00076257
Iteration 12/25 | Loss: 0.00076257
Iteration 13/25 | Loss: 0.00076257
Iteration 14/25 | Loss: 0.00076257
Iteration 15/25 | Loss: 0.00076257
Iteration 16/25 | Loss: 0.00076257
Iteration 17/25 | Loss: 0.00076257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007625680300407112, 0.0007625680300407112, 0.0007625680300407112, 0.0007625680300407112, 0.0007625680300407112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007625680300407112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06069756
Iteration 2/25 | Loss: 0.00031605
Iteration 3/25 | Loss: 0.00031604
Iteration 4/25 | Loss: 0.00031604
Iteration 5/25 | Loss: 0.00031604
Iteration 6/25 | Loss: 0.00031604
Iteration 7/25 | Loss: 0.00031604
Iteration 8/25 | Loss: 0.00031604
Iteration 9/25 | Loss: 0.00031604
Iteration 10/25 | Loss: 0.00031604
Iteration 11/25 | Loss: 0.00031604
Iteration 12/25 | Loss: 0.00031604
Iteration 13/25 | Loss: 0.00031604
Iteration 14/25 | Loss: 0.00031604
Iteration 15/25 | Loss: 0.00031604
Iteration 16/25 | Loss: 0.00031604
Iteration 17/25 | Loss: 0.00031604
Iteration 18/25 | Loss: 0.00031604
Iteration 19/25 | Loss: 0.00031604
Iteration 20/25 | Loss: 0.00031604
Iteration 21/25 | Loss: 0.00031604
Iteration 22/25 | Loss: 0.00031604
Iteration 23/25 | Loss: 0.00031604
Iteration 24/25 | Loss: 0.00031604
Iteration 25/25 | Loss: 0.00031604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00031604067771695554, 0.00031604067771695554, 0.00031604067771695554, 0.00031604067771695554, 0.00031604067771695554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031604067771695554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031604
Iteration 2/1000 | Loss: 0.00003400
Iteration 3/1000 | Loss: 0.00002351
Iteration 4/1000 | Loss: 0.00002098
Iteration 5/1000 | Loss: 0.00001997
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00001789
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001715
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001712
Iteration 28/1000 | Loss: 0.00001712
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001711
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001709
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001696
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001695
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001694
Iteration 84/1000 | Loss: 0.00001694
Iteration 85/1000 | Loss: 0.00001694
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001688
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6860885807545856e-05, 1.6860885807545856e-05, 1.6860885807545856e-05, 1.6860885807545856e-05, 1.6860885807545856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6860885807545856e-05

Optimization complete. Final v2v error: 3.4390578269958496 mm

Highest mean error: 4.504849910736084 mm for frame 22

Lowest mean error: 2.879159688949585 mm for frame 68

Saving results

Total time: 42.6340651512146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811973
Iteration 2/25 | Loss: 0.00105923
Iteration 3/25 | Loss: 0.00084406
Iteration 4/25 | Loss: 0.00081006
Iteration 5/25 | Loss: 0.00080158
Iteration 6/25 | Loss: 0.00079971
Iteration 7/25 | Loss: 0.00079952
Iteration 8/25 | Loss: 0.00079952
Iteration 9/25 | Loss: 0.00079952
Iteration 10/25 | Loss: 0.00079952
Iteration 11/25 | Loss: 0.00079952
Iteration 12/25 | Loss: 0.00079952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007995202904567122, 0.0007995202904567122, 0.0007995202904567122, 0.0007995202904567122, 0.0007995202904567122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007995202904567122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48493099
Iteration 2/25 | Loss: 0.00033629
Iteration 3/25 | Loss: 0.00033629
Iteration 4/25 | Loss: 0.00033629
Iteration 5/25 | Loss: 0.00033629
Iteration 6/25 | Loss: 0.00033629
Iteration 7/25 | Loss: 0.00033629
Iteration 8/25 | Loss: 0.00033629
Iteration 9/25 | Loss: 0.00033628
Iteration 10/25 | Loss: 0.00033628
Iteration 11/25 | Loss: 0.00033628
Iteration 12/25 | Loss: 0.00033628
Iteration 13/25 | Loss: 0.00033628
Iteration 14/25 | Loss: 0.00033628
Iteration 15/25 | Loss: 0.00033628
Iteration 16/25 | Loss: 0.00033628
Iteration 17/25 | Loss: 0.00033628
Iteration 18/25 | Loss: 0.00033628
Iteration 19/25 | Loss: 0.00033628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003362848365213722, 0.0003362848365213722, 0.0003362848365213722, 0.0003362848365213722, 0.0003362848365213722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003362848365213722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033628
Iteration 2/1000 | Loss: 0.00003751
Iteration 3/1000 | Loss: 0.00002530
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001880
Iteration 13/1000 | Loss: 0.00001878
Iteration 14/1000 | Loss: 0.00001870
Iteration 15/1000 | Loss: 0.00001869
Iteration 16/1000 | Loss: 0.00001868
Iteration 17/1000 | Loss: 0.00001867
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001863
Iteration 22/1000 | Loss: 0.00001863
Iteration 23/1000 | Loss: 0.00001862
Iteration 24/1000 | Loss: 0.00001862
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001860
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00001859
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001859
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00001856
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001856
Iteration 42/1000 | Loss: 0.00001856
Iteration 43/1000 | Loss: 0.00001856
Iteration 44/1000 | Loss: 0.00001855
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001854
Iteration 48/1000 | Loss: 0.00001854
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001853
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001853
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001851
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001850
Iteration 78/1000 | Loss: 0.00001849
Iteration 79/1000 | Loss: 0.00001849
Iteration 80/1000 | Loss: 0.00001849
Iteration 81/1000 | Loss: 0.00001849
Iteration 82/1000 | Loss: 0.00001849
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001848
Iteration 89/1000 | Loss: 0.00001848
Iteration 90/1000 | Loss: 0.00001848
Iteration 91/1000 | Loss: 0.00001848
Iteration 92/1000 | Loss: 0.00001848
Iteration 93/1000 | Loss: 0.00001848
Iteration 94/1000 | Loss: 0.00001848
Iteration 95/1000 | Loss: 0.00001848
Iteration 96/1000 | Loss: 0.00001848
Iteration 97/1000 | Loss: 0.00001848
Iteration 98/1000 | Loss: 0.00001848
Iteration 99/1000 | Loss: 0.00001848
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001846
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001844
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001843
Iteration 123/1000 | Loss: 0.00001843
Iteration 124/1000 | Loss: 0.00001843
Iteration 125/1000 | Loss: 0.00001843
Iteration 126/1000 | Loss: 0.00001843
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.8425755115458742e-05, 1.8425755115458742e-05, 1.8425755115458742e-05, 1.8425755115458742e-05, 1.8425755115458742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8425755115458742e-05

Optimization complete. Final v2v error: 3.6185710430145264 mm

Highest mean error: 4.020458221435547 mm for frame 79

Lowest mean error: 3.282775640487671 mm for frame 53

Saving results

Total time: 34.65787053108215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837877
Iteration 2/25 | Loss: 0.00092800
Iteration 3/25 | Loss: 0.00074467
Iteration 4/25 | Loss: 0.00072330
Iteration 5/25 | Loss: 0.00071723
Iteration 6/25 | Loss: 0.00071484
Iteration 7/25 | Loss: 0.00071422
Iteration 8/25 | Loss: 0.00071422
Iteration 9/25 | Loss: 0.00071422
Iteration 10/25 | Loss: 0.00071422
Iteration 11/25 | Loss: 0.00071422
Iteration 12/25 | Loss: 0.00071422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007142230751924217, 0.0007142230751924217, 0.0007142230751924217, 0.0007142230751924217, 0.0007142230751924217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007142230751924217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47101700
Iteration 2/25 | Loss: 0.00028896
Iteration 3/25 | Loss: 0.00028895
Iteration 4/25 | Loss: 0.00028895
Iteration 5/25 | Loss: 0.00028895
Iteration 6/25 | Loss: 0.00028895
Iteration 7/25 | Loss: 0.00028895
Iteration 8/25 | Loss: 0.00028895
Iteration 9/25 | Loss: 0.00028895
Iteration 10/25 | Loss: 0.00028895
Iteration 11/25 | Loss: 0.00028895
Iteration 12/25 | Loss: 0.00028895
Iteration 13/25 | Loss: 0.00028895
Iteration 14/25 | Loss: 0.00028895
Iteration 15/25 | Loss: 0.00028895
Iteration 16/25 | Loss: 0.00028895
Iteration 17/25 | Loss: 0.00028895
Iteration 18/25 | Loss: 0.00028895
Iteration 19/25 | Loss: 0.00028895
Iteration 20/25 | Loss: 0.00028895
Iteration 21/25 | Loss: 0.00028895
Iteration 22/25 | Loss: 0.00028895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002889478637371212, 0.0002889478637371212, 0.0002889478637371212, 0.0002889478637371212, 0.0002889478637371212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002889478637371212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028895
Iteration 2/1000 | Loss: 0.00002186
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00001202
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001118
Iteration 15/1000 | Loss: 0.00001117
Iteration 16/1000 | Loss: 0.00001117
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001114
Iteration 20/1000 | Loss: 0.00001114
Iteration 21/1000 | Loss: 0.00001113
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001112
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001111
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001108
Iteration 30/1000 | Loss: 0.00001108
Iteration 31/1000 | Loss: 0.00001108
Iteration 32/1000 | Loss: 0.00001107
Iteration 33/1000 | Loss: 0.00001107
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001104
Iteration 37/1000 | Loss: 0.00001103
Iteration 38/1000 | Loss: 0.00001103
Iteration 39/1000 | Loss: 0.00001102
Iteration 40/1000 | Loss: 0.00001102
Iteration 41/1000 | Loss: 0.00001101
Iteration 42/1000 | Loss: 0.00001101
Iteration 43/1000 | Loss: 0.00001101
Iteration 44/1000 | Loss: 0.00001100
Iteration 45/1000 | Loss: 0.00001100
Iteration 46/1000 | Loss: 0.00001100
Iteration 47/1000 | Loss: 0.00001100
Iteration 48/1000 | Loss: 0.00001099
Iteration 49/1000 | Loss: 0.00001099
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001097
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001094
Iteration 59/1000 | Loss: 0.00001094
Iteration 60/1000 | Loss: 0.00001094
Iteration 61/1000 | Loss: 0.00001094
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001093
Iteration 67/1000 | Loss: 0.00001093
Iteration 68/1000 | Loss: 0.00001093
Iteration 69/1000 | Loss: 0.00001093
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001091
Iteration 86/1000 | Loss: 0.00001091
Iteration 87/1000 | Loss: 0.00001091
Iteration 88/1000 | Loss: 0.00001090
Iteration 89/1000 | Loss: 0.00001090
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001090
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001089
Iteration 108/1000 | Loss: 0.00001089
Iteration 109/1000 | Loss: 0.00001089
Iteration 110/1000 | Loss: 0.00001089
Iteration 111/1000 | Loss: 0.00001089
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001088
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001087
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0869789548451081e-05, 1.0869789548451081e-05, 1.0869789548451081e-05, 1.0869789548451081e-05, 1.0869789548451081e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0869789548451081e-05

Optimization complete. Final v2v error: 2.747377872467041 mm

Highest mean error: 3.6109979152679443 mm for frame 59

Lowest mean error: 2.412203073501587 mm for frame 132

Saving results

Total time: 34.16729283332825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034161
Iteration 2/25 | Loss: 0.00218805
Iteration 3/25 | Loss: 0.00145230
Iteration 4/25 | Loss: 0.00117619
Iteration 5/25 | Loss: 0.00116479
Iteration 6/25 | Loss: 0.00116785
Iteration 7/25 | Loss: 0.00106324
Iteration 8/25 | Loss: 0.00102424
Iteration 9/25 | Loss: 0.00098511
Iteration 10/25 | Loss: 0.00095245
Iteration 11/25 | Loss: 0.00096086
Iteration 12/25 | Loss: 0.00093947
Iteration 13/25 | Loss: 0.00092036
Iteration 14/25 | Loss: 0.00091652
Iteration 15/25 | Loss: 0.00089307
Iteration 16/25 | Loss: 0.00086929
Iteration 17/25 | Loss: 0.00086039
Iteration 18/25 | Loss: 0.00088544
Iteration 19/25 | Loss: 0.00091116
Iteration 20/25 | Loss: 0.00086779
Iteration 21/25 | Loss: 0.00082489
Iteration 22/25 | Loss: 0.00081373
Iteration 23/25 | Loss: 0.00079796
Iteration 24/25 | Loss: 0.00079948
Iteration 25/25 | Loss: 0.00078783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50553322
Iteration 2/25 | Loss: 0.00063649
Iteration 3/25 | Loss: 0.00063649
Iteration 4/25 | Loss: 0.00063649
Iteration 5/25 | Loss: 0.00063649
Iteration 6/25 | Loss: 0.00063649
Iteration 7/25 | Loss: 0.00063649
Iteration 8/25 | Loss: 0.00062601
Iteration 9/25 | Loss: 0.00062601
Iteration 10/25 | Loss: 0.00062601
Iteration 11/25 | Loss: 0.00062601
Iteration 12/25 | Loss: 0.00062601
Iteration 13/25 | Loss: 0.00062601
Iteration 14/25 | Loss: 0.00062601
Iteration 15/25 | Loss: 0.00062601
Iteration 16/25 | Loss: 0.00062601
Iteration 17/25 | Loss: 0.00062601
Iteration 18/25 | Loss: 0.00062601
Iteration 19/25 | Loss: 0.00062601
Iteration 20/25 | Loss: 0.00062601
Iteration 21/25 | Loss: 0.00062601
Iteration 22/25 | Loss: 0.00062601
Iteration 23/25 | Loss: 0.00062601
Iteration 24/25 | Loss: 0.00062601
Iteration 25/25 | Loss: 0.00062601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062601
Iteration 2/1000 | Loss: 0.00040025
Iteration 3/1000 | Loss: 0.00050511
Iteration 4/1000 | Loss: 0.00064140
Iteration 5/1000 | Loss: 0.00088862
Iteration 6/1000 | Loss: 0.00060885
Iteration 7/1000 | Loss: 0.00075162
Iteration 8/1000 | Loss: 0.00065567
Iteration 9/1000 | Loss: 0.00071193
Iteration 10/1000 | Loss: 0.00019574
Iteration 11/1000 | Loss: 0.00029996
Iteration 12/1000 | Loss: 0.00008859
Iteration 13/1000 | Loss: 0.00023908
Iteration 14/1000 | Loss: 0.00033280
Iteration 15/1000 | Loss: 0.00028679
Iteration 16/1000 | Loss: 0.00039379
Iteration 17/1000 | Loss: 0.00040155
Iteration 18/1000 | Loss: 0.00007688
Iteration 19/1000 | Loss: 0.00039109
Iteration 20/1000 | Loss: 0.00064635
Iteration 21/1000 | Loss: 0.00046530
Iteration 22/1000 | Loss: 0.00039745
Iteration 23/1000 | Loss: 0.00029692
Iteration 24/1000 | Loss: 0.00045067
Iteration 25/1000 | Loss: 0.00025982
Iteration 26/1000 | Loss: 0.00023697
Iteration 27/1000 | Loss: 0.00078334
Iteration 28/1000 | Loss: 0.00015073
Iteration 29/1000 | Loss: 0.00037830
Iteration 30/1000 | Loss: 0.00029072
Iteration 31/1000 | Loss: 0.00028788
Iteration 32/1000 | Loss: 0.00035757
Iteration 33/1000 | Loss: 0.00032888
Iteration 34/1000 | Loss: 0.00077527
Iteration 35/1000 | Loss: 0.00037966
Iteration 36/1000 | Loss: 0.00029101
Iteration 37/1000 | Loss: 0.00049219
Iteration 38/1000 | Loss: 0.00034355
Iteration 39/1000 | Loss: 0.00104393
Iteration 40/1000 | Loss: 0.00043179
Iteration 41/1000 | Loss: 0.00048590
Iteration 42/1000 | Loss: 0.00061899
Iteration 43/1000 | Loss: 0.00034538
Iteration 44/1000 | Loss: 0.00042933
Iteration 45/1000 | Loss: 0.00035305
Iteration 46/1000 | Loss: 0.00055262
Iteration 47/1000 | Loss: 0.00054482
Iteration 48/1000 | Loss: 0.00006621
Iteration 49/1000 | Loss: 0.00002837
Iteration 50/1000 | Loss: 0.00029704
Iteration 51/1000 | Loss: 0.00003884
Iteration 52/1000 | Loss: 0.00003118
Iteration 53/1000 | Loss: 0.00002739
Iteration 54/1000 | Loss: 0.00002573
Iteration 55/1000 | Loss: 0.00002434
Iteration 56/1000 | Loss: 0.00023291
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002050
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001646
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001594
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001584
Iteration 70/1000 | Loss: 0.00001584
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001581
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001579
Iteration 79/1000 | Loss: 0.00001576
Iteration 80/1000 | Loss: 0.00001576
Iteration 81/1000 | Loss: 0.00001576
Iteration 82/1000 | Loss: 0.00001576
Iteration 83/1000 | Loss: 0.00001576
Iteration 84/1000 | Loss: 0.00001576
Iteration 85/1000 | Loss: 0.00001575
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001574
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001573
Iteration 90/1000 | Loss: 0.00001573
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001567
Iteration 107/1000 | Loss: 0.00001567
Iteration 108/1000 | Loss: 0.00001567
Iteration 109/1000 | Loss: 0.00001567
Iteration 110/1000 | Loss: 0.00001566
Iteration 111/1000 | Loss: 0.00001566
Iteration 112/1000 | Loss: 0.00001566
Iteration 113/1000 | Loss: 0.00001565
Iteration 114/1000 | Loss: 0.00001565
Iteration 115/1000 | Loss: 0.00001565
Iteration 116/1000 | Loss: 0.00001564
Iteration 117/1000 | Loss: 0.00001564
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001562
Iteration 122/1000 | Loss: 0.00001562
Iteration 123/1000 | Loss: 0.00001562
Iteration 124/1000 | Loss: 0.00001562
Iteration 125/1000 | Loss: 0.00001561
Iteration 126/1000 | Loss: 0.00001561
Iteration 127/1000 | Loss: 0.00001561
Iteration 128/1000 | Loss: 0.00001561
Iteration 129/1000 | Loss: 0.00001560
Iteration 130/1000 | Loss: 0.00001560
Iteration 131/1000 | Loss: 0.00001560
Iteration 132/1000 | Loss: 0.00001559
Iteration 133/1000 | Loss: 0.00001559
Iteration 134/1000 | Loss: 0.00001559
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001556
Iteration 140/1000 | Loss: 0.00001556
Iteration 141/1000 | Loss: 0.00001556
Iteration 142/1000 | Loss: 0.00001556
Iteration 143/1000 | Loss: 0.00001555
Iteration 144/1000 | Loss: 0.00001555
Iteration 145/1000 | Loss: 0.00001555
Iteration 146/1000 | Loss: 0.00001555
Iteration 147/1000 | Loss: 0.00001555
Iteration 148/1000 | Loss: 0.00001554
Iteration 149/1000 | Loss: 0.00001554
Iteration 150/1000 | Loss: 0.00001554
Iteration 151/1000 | Loss: 0.00001553
Iteration 152/1000 | Loss: 0.00001553
Iteration 153/1000 | Loss: 0.00001553
Iteration 154/1000 | Loss: 0.00001552
Iteration 155/1000 | Loss: 0.00001552
Iteration 156/1000 | Loss: 0.00001551
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001545
Iteration 160/1000 | Loss: 0.00001545
Iteration 161/1000 | Loss: 0.00001545
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001543
Iteration 168/1000 | Loss: 0.00001542
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001538
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001536
Iteration 175/1000 | Loss: 0.00001535
Iteration 176/1000 | Loss: 0.00001535
Iteration 177/1000 | Loss: 0.00001534
Iteration 178/1000 | Loss: 0.00001534
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Iteration 181/1000 | Loss: 0.00001532
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001531
Iteration 185/1000 | Loss: 0.00001531
Iteration 186/1000 | Loss: 0.00001531
Iteration 187/1000 | Loss: 0.00001531
Iteration 188/1000 | Loss: 0.00001530
Iteration 189/1000 | Loss: 0.00001530
Iteration 190/1000 | Loss: 0.00001530
Iteration 191/1000 | Loss: 0.00001528
Iteration 192/1000 | Loss: 0.00001525
Iteration 193/1000 | Loss: 0.00001525
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001524
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00001523
Iteration 198/1000 | Loss: 0.00001523
Iteration 199/1000 | Loss: 0.00001523
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001521
Iteration 204/1000 | Loss: 0.00001520
Iteration 205/1000 | Loss: 0.00001516
Iteration 206/1000 | Loss: 0.00001515
Iteration 207/1000 | Loss: 0.00001515
Iteration 208/1000 | Loss: 0.00001515
Iteration 209/1000 | Loss: 0.00001515
Iteration 210/1000 | Loss: 0.00001514
Iteration 211/1000 | Loss: 0.00001514
Iteration 212/1000 | Loss: 0.00001514
Iteration 213/1000 | Loss: 0.00001514
Iteration 214/1000 | Loss: 0.00001514
Iteration 215/1000 | Loss: 0.00001514
Iteration 216/1000 | Loss: 0.00001513
Iteration 217/1000 | Loss: 0.00001513
Iteration 218/1000 | Loss: 0.00001513
Iteration 219/1000 | Loss: 0.00001513
Iteration 220/1000 | Loss: 0.00001513
Iteration 221/1000 | Loss: 0.00001512
Iteration 222/1000 | Loss: 0.00001512
Iteration 223/1000 | Loss: 0.00001512
Iteration 224/1000 | Loss: 0.00001512
Iteration 225/1000 | Loss: 0.00001512
Iteration 226/1000 | Loss: 0.00001511
Iteration 227/1000 | Loss: 0.00001511
Iteration 228/1000 | Loss: 0.00001511
Iteration 229/1000 | Loss: 0.00001510
Iteration 230/1000 | Loss: 0.00001510
Iteration 231/1000 | Loss: 0.00001510
Iteration 232/1000 | Loss: 0.00001510
Iteration 233/1000 | Loss: 0.00001509
Iteration 234/1000 | Loss: 0.00001509
Iteration 235/1000 | Loss: 0.00001509
Iteration 236/1000 | Loss: 0.00001509
Iteration 237/1000 | Loss: 0.00001509
Iteration 238/1000 | Loss: 0.00001509
Iteration 239/1000 | Loss: 0.00001508
Iteration 240/1000 | Loss: 0.00001508
Iteration 241/1000 | Loss: 0.00001508
Iteration 242/1000 | Loss: 0.00001508
Iteration 243/1000 | Loss: 0.00001508
Iteration 244/1000 | Loss: 0.00001508
Iteration 245/1000 | Loss: 0.00001508
Iteration 246/1000 | Loss: 0.00001507
Iteration 247/1000 | Loss: 0.00001507
Iteration 248/1000 | Loss: 0.00001507
Iteration 249/1000 | Loss: 0.00001507
Iteration 250/1000 | Loss: 0.00001506
Iteration 251/1000 | Loss: 0.00001506
Iteration 252/1000 | Loss: 0.00001506
Iteration 253/1000 | Loss: 0.00001506
Iteration 254/1000 | Loss: 0.00001506
Iteration 255/1000 | Loss: 0.00001506
Iteration 256/1000 | Loss: 0.00001506
Iteration 257/1000 | Loss: 0.00001506
Iteration 258/1000 | Loss: 0.00001506
Iteration 259/1000 | Loss: 0.00001506
Iteration 260/1000 | Loss: 0.00001506
Iteration 261/1000 | Loss: 0.00001506
Iteration 262/1000 | Loss: 0.00001506
Iteration 263/1000 | Loss: 0.00001505
Iteration 264/1000 | Loss: 0.00001505
Iteration 265/1000 | Loss: 0.00001505
Iteration 266/1000 | Loss: 0.00001505
Iteration 267/1000 | Loss: 0.00001505
Iteration 268/1000 | Loss: 0.00001504
Iteration 269/1000 | Loss: 0.00001504
Iteration 270/1000 | Loss: 0.00001504
Iteration 271/1000 | Loss: 0.00001504
Iteration 272/1000 | Loss: 0.00001504
Iteration 273/1000 | Loss: 0.00001504
Iteration 274/1000 | Loss: 0.00001504
Iteration 275/1000 | Loss: 0.00001504
Iteration 276/1000 | Loss: 0.00001503
Iteration 277/1000 | Loss: 0.00001503
Iteration 278/1000 | Loss: 0.00001503
Iteration 279/1000 | Loss: 0.00001503
Iteration 280/1000 | Loss: 0.00001503
Iteration 281/1000 | Loss: 0.00001503
Iteration 282/1000 | Loss: 0.00001502
Iteration 283/1000 | Loss: 0.00001502
Iteration 284/1000 | Loss: 0.00001502
Iteration 285/1000 | Loss: 0.00001502
Iteration 286/1000 | Loss: 0.00001501
Iteration 287/1000 | Loss: 0.00001501
Iteration 288/1000 | Loss: 0.00001501
Iteration 289/1000 | Loss: 0.00001501
Iteration 290/1000 | Loss: 0.00001501
Iteration 291/1000 | Loss: 0.00001501
Iteration 292/1000 | Loss: 0.00001501
Iteration 293/1000 | Loss: 0.00001500
Iteration 294/1000 | Loss: 0.00001500
Iteration 295/1000 | Loss: 0.00001500
Iteration 296/1000 | Loss: 0.00001500
Iteration 297/1000 | Loss: 0.00001500
Iteration 298/1000 | Loss: 0.00001499
Iteration 299/1000 | Loss: 0.00001499
Iteration 300/1000 | Loss: 0.00001499
Iteration 301/1000 | Loss: 0.00001499
Iteration 302/1000 | Loss: 0.00001498
Iteration 303/1000 | Loss: 0.00001498
Iteration 304/1000 | Loss: 0.00001497
Iteration 305/1000 | Loss: 0.00001497
Iteration 306/1000 | Loss: 0.00001497
Iteration 307/1000 | Loss: 0.00001497
Iteration 308/1000 | Loss: 0.00001497
Iteration 309/1000 | Loss: 0.00001497
Iteration 310/1000 | Loss: 0.00001497
Iteration 311/1000 | Loss: 0.00001496
Iteration 312/1000 | Loss: 0.00001496
Iteration 313/1000 | Loss: 0.00001496
Iteration 314/1000 | Loss: 0.00001496
Iteration 315/1000 | Loss: 0.00001496
Iteration 316/1000 | Loss: 0.00001496
Iteration 317/1000 | Loss: 0.00001496
Iteration 318/1000 | Loss: 0.00001496
Iteration 319/1000 | Loss: 0.00001495
Iteration 320/1000 | Loss: 0.00001495
Iteration 321/1000 | Loss: 0.00001495
Iteration 322/1000 | Loss: 0.00001495
Iteration 323/1000 | Loss: 0.00001495
Iteration 324/1000 | Loss: 0.00001494
Iteration 325/1000 | Loss: 0.00001494
Iteration 326/1000 | Loss: 0.00001494
Iteration 327/1000 | Loss: 0.00001494
Iteration 328/1000 | Loss: 0.00001493
Iteration 329/1000 | Loss: 0.00001493
Iteration 330/1000 | Loss: 0.00001493
Iteration 331/1000 | Loss: 0.00001493
Iteration 332/1000 | Loss: 0.00001493
Iteration 333/1000 | Loss: 0.00001493
Iteration 334/1000 | Loss: 0.00001493
Iteration 335/1000 | Loss: 0.00001493
Iteration 336/1000 | Loss: 0.00001493
Iteration 337/1000 | Loss: 0.00001493
Iteration 338/1000 | Loss: 0.00001493
Iteration 339/1000 | Loss: 0.00001493
Iteration 340/1000 | Loss: 0.00001493
Iteration 341/1000 | Loss: 0.00001492
Iteration 342/1000 | Loss: 0.00001492
Iteration 343/1000 | Loss: 0.00001492
Iteration 344/1000 | Loss: 0.00001492
Iteration 345/1000 | Loss: 0.00001492
Iteration 346/1000 | Loss: 0.00001492
Iteration 347/1000 | Loss: 0.00001492
Iteration 348/1000 | Loss: 0.00001492
Iteration 349/1000 | Loss: 0.00001492
Iteration 350/1000 | Loss: 0.00001492
Iteration 351/1000 | Loss: 0.00001492
Iteration 352/1000 | Loss: 0.00001491
Iteration 353/1000 | Loss: 0.00001491
Iteration 354/1000 | Loss: 0.00001491
Iteration 355/1000 | Loss: 0.00001491
Iteration 356/1000 | Loss: 0.00001491
Iteration 357/1000 | Loss: 0.00001491
Iteration 358/1000 | Loss: 0.00001491
Iteration 359/1000 | Loss: 0.00001491
Iteration 360/1000 | Loss: 0.00001491
Iteration 361/1000 | Loss: 0.00001490
Iteration 362/1000 | Loss: 0.00001490
Iteration 363/1000 | Loss: 0.00001490
Iteration 364/1000 | Loss: 0.00001490
Iteration 365/1000 | Loss: 0.00001490
Iteration 366/1000 | Loss: 0.00001490
Iteration 367/1000 | Loss: 0.00001490
Iteration 368/1000 | Loss: 0.00001490
Iteration 369/1000 | Loss: 0.00001490
Iteration 370/1000 | Loss: 0.00001490
Iteration 371/1000 | Loss: 0.00001490
Iteration 372/1000 | Loss: 0.00001490
Iteration 373/1000 | Loss: 0.00001490
Iteration 374/1000 | Loss: 0.00001490
Iteration 375/1000 | Loss: 0.00001490
Iteration 376/1000 | Loss: 0.00001490
Iteration 377/1000 | Loss: 0.00001490
Iteration 378/1000 | Loss: 0.00001490
Iteration 379/1000 | Loss: 0.00001490
Iteration 380/1000 | Loss: 0.00001490
Iteration 381/1000 | Loss: 0.00001490
Iteration 382/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 382. Stopping optimization.
Last 5 losses: [1.4900279893481638e-05, 1.4900279893481638e-05, 1.4900279893481638e-05, 1.4900279893481638e-05, 1.4900279893481638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4900279893481638e-05

Optimization complete. Final v2v error: 3.1759092807769775 mm

Highest mean error: 6.244151592254639 mm for frame 52

Lowest mean error: 2.6929163932800293 mm for frame 109

Saving results

Total time: 160.2020788192749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769770
Iteration 2/25 | Loss: 0.00100952
Iteration 3/25 | Loss: 0.00081245
Iteration 4/25 | Loss: 0.00076958
Iteration 5/25 | Loss: 0.00076286
Iteration 6/25 | Loss: 0.00076201
Iteration 7/25 | Loss: 0.00076201
Iteration 8/25 | Loss: 0.00076201
Iteration 9/25 | Loss: 0.00076201
Iteration 10/25 | Loss: 0.00076201
Iteration 11/25 | Loss: 0.00076201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007620149990543723, 0.0007620149990543723, 0.0007620149990543723, 0.0007620149990543723, 0.0007620149990543723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007620149990543723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41961145
Iteration 2/25 | Loss: 0.00029268
Iteration 3/25 | Loss: 0.00029265
Iteration 4/25 | Loss: 0.00029265
Iteration 5/25 | Loss: 0.00029265
Iteration 6/25 | Loss: 0.00029265
Iteration 7/25 | Loss: 0.00029265
Iteration 8/25 | Loss: 0.00029265
Iteration 9/25 | Loss: 0.00029265
Iteration 10/25 | Loss: 0.00029265
Iteration 11/25 | Loss: 0.00029265
Iteration 12/25 | Loss: 0.00029265
Iteration 13/25 | Loss: 0.00029265
Iteration 14/25 | Loss: 0.00029265
Iteration 15/25 | Loss: 0.00029265
Iteration 16/25 | Loss: 0.00029265
Iteration 17/25 | Loss: 0.00029265
Iteration 18/25 | Loss: 0.00029265
Iteration 19/25 | Loss: 0.00029265
Iteration 20/25 | Loss: 0.00029265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00029264637851156294, 0.00029264637851156294, 0.00029264637851156294, 0.00029264637851156294, 0.00029264637851156294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029264637851156294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029265
Iteration 2/1000 | Loss: 0.00003269
Iteration 3/1000 | Loss: 0.00002500
Iteration 4/1000 | Loss: 0.00002122
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001900
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001826
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001816
Iteration 11/1000 | Loss: 0.00001815
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001797
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001784
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001773
Iteration 23/1000 | Loss: 0.00001772
Iteration 24/1000 | Loss: 0.00001771
Iteration 25/1000 | Loss: 0.00001771
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001770
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001770
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00001770
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001769
Iteration 42/1000 | Loss: 0.00001769
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001768
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001758
Iteration 49/1000 | Loss: 0.00001758
Iteration 50/1000 | Loss: 0.00001757
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001756
Iteration 54/1000 | Loss: 0.00001754
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001752
Iteration 69/1000 | Loss: 0.00001752
Iteration 70/1000 | Loss: 0.00001752
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001748
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001747
Iteration 78/1000 | Loss: 0.00001746
Iteration 79/1000 | Loss: 0.00001746
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001745
Iteration 84/1000 | Loss: 0.00001745
Iteration 85/1000 | Loss: 0.00001745
Iteration 86/1000 | Loss: 0.00001745
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001745
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001738
Iteration 113/1000 | Loss: 0.00001738
Iteration 114/1000 | Loss: 0.00001738
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001737
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001736
Iteration 139/1000 | Loss: 0.00001736
Iteration 140/1000 | Loss: 0.00001736
Iteration 141/1000 | Loss: 0.00001736
Iteration 142/1000 | Loss: 0.00001736
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001735
Iteration 148/1000 | Loss: 0.00001735
Iteration 149/1000 | Loss: 0.00001735
Iteration 150/1000 | Loss: 0.00001735
Iteration 151/1000 | Loss: 0.00001735
Iteration 152/1000 | Loss: 0.00001735
Iteration 153/1000 | Loss: 0.00001735
Iteration 154/1000 | Loss: 0.00001735
Iteration 155/1000 | Loss: 0.00001735
Iteration 156/1000 | Loss: 0.00001735
Iteration 157/1000 | Loss: 0.00001735
Iteration 158/1000 | Loss: 0.00001735
Iteration 159/1000 | Loss: 0.00001735
Iteration 160/1000 | Loss: 0.00001735
Iteration 161/1000 | Loss: 0.00001734
Iteration 162/1000 | Loss: 0.00001734
Iteration 163/1000 | Loss: 0.00001734
Iteration 164/1000 | Loss: 0.00001734
Iteration 165/1000 | Loss: 0.00001734
Iteration 166/1000 | Loss: 0.00001734
Iteration 167/1000 | Loss: 0.00001734
Iteration 168/1000 | Loss: 0.00001734
Iteration 169/1000 | Loss: 0.00001734
Iteration 170/1000 | Loss: 0.00001734
Iteration 171/1000 | Loss: 0.00001734
Iteration 172/1000 | Loss: 0.00001734
Iteration 173/1000 | Loss: 0.00001734
Iteration 174/1000 | Loss: 0.00001734
Iteration 175/1000 | Loss: 0.00001734
Iteration 176/1000 | Loss: 0.00001734
Iteration 177/1000 | Loss: 0.00001734
Iteration 178/1000 | Loss: 0.00001734
Iteration 179/1000 | Loss: 0.00001734
Iteration 180/1000 | Loss: 0.00001734
Iteration 181/1000 | Loss: 0.00001734
Iteration 182/1000 | Loss: 0.00001734
Iteration 183/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.7337835743092e-05, 1.7337835743092e-05, 1.7337835743092e-05, 1.7337835743092e-05, 1.7337835743092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7337835743092e-05

Optimization complete. Final v2v error: 3.5608978271484375 mm

Highest mean error: 4.014845371246338 mm for frame 4

Lowest mean error: 3.2902467250823975 mm for frame 239

Saving results

Total time: 42.711806774139404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508893
Iteration 2/25 | Loss: 0.00098504
Iteration 3/25 | Loss: 0.00077246
Iteration 4/25 | Loss: 0.00075750
Iteration 5/25 | Loss: 0.00075702
Iteration 6/25 | Loss: 0.00076402
Iteration 7/25 | Loss: 0.00072724
Iteration 8/25 | Loss: 0.00074494
Iteration 9/25 | Loss: 0.00072697
Iteration 10/25 | Loss: 0.00072662
Iteration 11/25 | Loss: 0.00072660
Iteration 12/25 | Loss: 0.00072660
Iteration 13/25 | Loss: 0.00072660
Iteration 14/25 | Loss: 0.00072660
Iteration 15/25 | Loss: 0.00072660
Iteration 16/25 | Loss: 0.00072660
Iteration 17/25 | Loss: 0.00072660
Iteration 18/25 | Loss: 0.00072660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007265950553119183, 0.0007265950553119183, 0.0007265950553119183, 0.0007265950553119183, 0.0007265950553119183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007265950553119183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.92066813
Iteration 2/25 | Loss: 0.00035747
Iteration 3/25 | Loss: 0.00033793
Iteration 4/25 | Loss: 0.00033793
Iteration 5/25 | Loss: 0.00033793
Iteration 6/25 | Loss: 0.00033793
Iteration 7/25 | Loss: 0.00033793
Iteration 8/25 | Loss: 0.00033793
Iteration 9/25 | Loss: 0.00033793
Iteration 10/25 | Loss: 0.00033793
Iteration 11/25 | Loss: 0.00033793
Iteration 12/25 | Loss: 0.00033793
Iteration 13/25 | Loss: 0.00033793
Iteration 14/25 | Loss: 0.00033793
Iteration 15/25 | Loss: 0.00033793
Iteration 16/25 | Loss: 0.00033793
Iteration 17/25 | Loss: 0.00033793
Iteration 18/25 | Loss: 0.00033793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003379250119905919, 0.0003379250119905919, 0.0003379250119905919, 0.0003379250119905919, 0.0003379250119905919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003379250119905919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033793
Iteration 2/1000 | Loss: 0.00007369
Iteration 3/1000 | Loss: 0.00009331
Iteration 4/1000 | Loss: 0.00002008
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00015404
Iteration 7/1000 | Loss: 0.00001696
Iteration 8/1000 | Loss: 0.00002370
Iteration 9/1000 | Loss: 0.00001587
Iteration 10/1000 | Loss: 0.00001580
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001520
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001499
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00003960
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00004755
Iteration 27/1000 | Loss: 0.00016003
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001454
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001454
Iteration 36/1000 | Loss: 0.00001454
Iteration 37/1000 | Loss: 0.00001454
Iteration 38/1000 | Loss: 0.00001454
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00004549
Iteration 52/1000 | Loss: 0.00001456
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001435
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001434
Iteration 92/1000 | Loss: 0.00001434
Iteration 93/1000 | Loss: 0.00001434
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001430
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001429
Iteration 117/1000 | Loss: 0.00001429
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001429
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001428
Iteration 134/1000 | Loss: 0.00001428
Iteration 135/1000 | Loss: 0.00001428
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001427
Iteration 140/1000 | Loss: 0.00001427
Iteration 141/1000 | Loss: 0.00001427
Iteration 142/1000 | Loss: 0.00001427
Iteration 143/1000 | Loss: 0.00001427
Iteration 144/1000 | Loss: 0.00001427
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00001426
Iteration 147/1000 | Loss: 0.00001426
Iteration 148/1000 | Loss: 0.00001426
Iteration 149/1000 | Loss: 0.00001426
Iteration 150/1000 | Loss: 0.00001426
Iteration 151/1000 | Loss: 0.00001426
Iteration 152/1000 | Loss: 0.00001426
Iteration 153/1000 | Loss: 0.00001426
Iteration 154/1000 | Loss: 0.00001426
Iteration 155/1000 | Loss: 0.00001425
Iteration 156/1000 | Loss: 0.00001425
Iteration 157/1000 | Loss: 0.00001425
Iteration 158/1000 | Loss: 0.00001425
Iteration 159/1000 | Loss: 0.00001425
Iteration 160/1000 | Loss: 0.00001425
Iteration 161/1000 | Loss: 0.00001425
Iteration 162/1000 | Loss: 0.00001425
Iteration 163/1000 | Loss: 0.00001425
Iteration 164/1000 | Loss: 0.00001425
Iteration 165/1000 | Loss: 0.00001425
Iteration 166/1000 | Loss: 0.00001424
Iteration 167/1000 | Loss: 0.00001424
Iteration 168/1000 | Loss: 0.00001424
Iteration 169/1000 | Loss: 0.00001424
Iteration 170/1000 | Loss: 0.00001424
Iteration 171/1000 | Loss: 0.00001424
Iteration 172/1000 | Loss: 0.00001424
Iteration 173/1000 | Loss: 0.00001424
Iteration 174/1000 | Loss: 0.00001424
Iteration 175/1000 | Loss: 0.00001423
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001423
Iteration 184/1000 | Loss: 0.00001422
Iteration 185/1000 | Loss: 0.00001422
Iteration 186/1000 | Loss: 0.00001422
Iteration 187/1000 | Loss: 0.00001422
Iteration 188/1000 | Loss: 0.00001422
Iteration 189/1000 | Loss: 0.00001422
Iteration 190/1000 | Loss: 0.00001422
Iteration 191/1000 | Loss: 0.00001422
Iteration 192/1000 | Loss: 0.00001421
Iteration 193/1000 | Loss: 0.00001421
Iteration 194/1000 | Loss: 0.00001421
Iteration 195/1000 | Loss: 0.00001421
Iteration 196/1000 | Loss: 0.00001421
Iteration 197/1000 | Loss: 0.00001421
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001421
Iteration 207/1000 | Loss: 0.00001421
Iteration 208/1000 | Loss: 0.00001421
Iteration 209/1000 | Loss: 0.00001421
Iteration 210/1000 | Loss: 0.00001421
Iteration 211/1000 | Loss: 0.00001421
Iteration 212/1000 | Loss: 0.00001421
Iteration 213/1000 | Loss: 0.00001421
Iteration 214/1000 | Loss: 0.00001421
Iteration 215/1000 | Loss: 0.00001421
Iteration 216/1000 | Loss: 0.00001421
Iteration 217/1000 | Loss: 0.00001421
Iteration 218/1000 | Loss: 0.00001421
Iteration 219/1000 | Loss: 0.00001421
Iteration 220/1000 | Loss: 0.00001421
Iteration 221/1000 | Loss: 0.00001421
Iteration 222/1000 | Loss: 0.00001421
Iteration 223/1000 | Loss: 0.00001421
Iteration 224/1000 | Loss: 0.00001421
Iteration 225/1000 | Loss: 0.00001421
Iteration 226/1000 | Loss: 0.00001421
Iteration 227/1000 | Loss: 0.00001421
Iteration 228/1000 | Loss: 0.00001421
Iteration 229/1000 | Loss: 0.00001421
Iteration 230/1000 | Loss: 0.00001421
Iteration 231/1000 | Loss: 0.00001421
Iteration 232/1000 | Loss: 0.00001421
Iteration 233/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.42100316224969e-05, 1.42100316224969e-05, 1.42100316224969e-05, 1.42100316224969e-05, 1.42100316224969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.42100316224969e-05

Optimization complete. Final v2v error: 3.1793065071105957 mm

Highest mean error: 3.837595224380493 mm for frame 199

Lowest mean error: 2.7842938899993896 mm for frame 10

Saving results

Total time: 66.50911688804626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846570
Iteration 2/25 | Loss: 0.00101921
Iteration 3/25 | Loss: 0.00082821
Iteration 4/25 | Loss: 0.00077800
Iteration 5/25 | Loss: 0.00076399
Iteration 6/25 | Loss: 0.00077244
Iteration 7/25 | Loss: 0.00078688
Iteration 8/25 | Loss: 0.00078855
Iteration 9/25 | Loss: 0.00076979
Iteration 10/25 | Loss: 0.00076836
Iteration 11/25 | Loss: 0.00076015
Iteration 12/25 | Loss: 0.00075722
Iteration 13/25 | Loss: 0.00074656
Iteration 14/25 | Loss: 0.00074327
Iteration 15/25 | Loss: 0.00074168
Iteration 16/25 | Loss: 0.00074127
Iteration 17/25 | Loss: 0.00074446
Iteration 18/25 | Loss: 0.00074400
Iteration 19/25 | Loss: 0.00074366
Iteration 20/25 | Loss: 0.00074332
Iteration 21/25 | Loss: 0.00074391
Iteration 22/25 | Loss: 0.00074255
Iteration 23/25 | Loss: 0.00074082
Iteration 24/25 | Loss: 0.00074194
Iteration 25/25 | Loss: 0.00073941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48649156
Iteration 2/25 | Loss: 0.00030174
Iteration 3/25 | Loss: 0.00030174
Iteration 4/25 | Loss: 0.00030174
Iteration 5/25 | Loss: 0.00030174
Iteration 6/25 | Loss: 0.00030174
Iteration 7/25 | Loss: 0.00030174
Iteration 8/25 | Loss: 0.00030174
Iteration 9/25 | Loss: 0.00030174
Iteration 10/25 | Loss: 0.00030174
Iteration 11/25 | Loss: 0.00030174
Iteration 12/25 | Loss: 0.00030174
Iteration 13/25 | Loss: 0.00030174
Iteration 14/25 | Loss: 0.00030174
Iteration 15/25 | Loss: 0.00030174
Iteration 16/25 | Loss: 0.00030174
Iteration 17/25 | Loss: 0.00030174
Iteration 18/25 | Loss: 0.00030174
Iteration 19/25 | Loss: 0.00030174
Iteration 20/25 | Loss: 0.00030174
Iteration 21/25 | Loss: 0.00030174
Iteration 22/25 | Loss: 0.00030174
Iteration 23/25 | Loss: 0.00030174
Iteration 24/25 | Loss: 0.00030174
Iteration 25/25 | Loss: 0.00030174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030174
Iteration 2/1000 | Loss: 0.00003463
Iteration 3/1000 | Loss: 0.00002628
Iteration 4/1000 | Loss: 0.00002357
Iteration 5/1000 | Loss: 0.00002244
Iteration 6/1000 | Loss: 0.00002127
Iteration 7/1000 | Loss: 0.00002043
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001898
Iteration 11/1000 | Loss: 0.00001877
Iteration 12/1000 | Loss: 0.00001872
Iteration 13/1000 | Loss: 0.00001855
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00001843
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001836
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001831
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001822
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001816
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001809
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001805
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001803
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001799
Iteration 49/1000 | Loss: 0.00001799
Iteration 50/1000 | Loss: 0.00001798
Iteration 51/1000 | Loss: 0.00001798
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001796
Iteration 56/1000 | Loss: 0.00001796
Iteration 57/1000 | Loss: 0.00001796
Iteration 58/1000 | Loss: 0.00001795
Iteration 59/1000 | Loss: 0.00001795
Iteration 60/1000 | Loss: 0.00001795
Iteration 61/1000 | Loss: 0.00001795
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001794
Iteration 64/1000 | Loss: 0.00001794
Iteration 65/1000 | Loss: 0.00001794
Iteration 66/1000 | Loss: 0.00001794
Iteration 67/1000 | Loss: 0.00001794
Iteration 68/1000 | Loss: 0.00001794
Iteration 69/1000 | Loss: 0.00001793
Iteration 70/1000 | Loss: 0.00001793
Iteration 71/1000 | Loss: 0.00001793
Iteration 72/1000 | Loss: 0.00001793
Iteration 73/1000 | Loss: 0.00001793
Iteration 74/1000 | Loss: 0.00001793
Iteration 75/1000 | Loss: 0.00001793
Iteration 76/1000 | Loss: 0.00001792
Iteration 77/1000 | Loss: 0.00001792
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001792
Iteration 80/1000 | Loss: 0.00001791
Iteration 81/1000 | Loss: 0.00001791
Iteration 82/1000 | Loss: 0.00001791
Iteration 83/1000 | Loss: 0.00001790
Iteration 84/1000 | Loss: 0.00001790
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001790
Iteration 87/1000 | Loss: 0.00001789
Iteration 88/1000 | Loss: 0.00001789
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001788
Iteration 94/1000 | Loss: 0.00001788
Iteration 95/1000 | Loss: 0.00001788
Iteration 96/1000 | Loss: 0.00001788
Iteration 97/1000 | Loss: 0.00001788
Iteration 98/1000 | Loss: 0.00001787
Iteration 99/1000 | Loss: 0.00001787
Iteration 100/1000 | Loss: 0.00001787
Iteration 101/1000 | Loss: 0.00001787
Iteration 102/1000 | Loss: 0.00001787
Iteration 103/1000 | Loss: 0.00001787
Iteration 104/1000 | Loss: 0.00001786
Iteration 105/1000 | Loss: 0.00001786
Iteration 106/1000 | Loss: 0.00001786
Iteration 107/1000 | Loss: 0.00001786
Iteration 108/1000 | Loss: 0.00001786
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001785
Iteration 111/1000 | Loss: 0.00001785
Iteration 112/1000 | Loss: 0.00001785
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001785
Iteration 115/1000 | Loss: 0.00001785
Iteration 116/1000 | Loss: 0.00001784
Iteration 117/1000 | Loss: 0.00001784
Iteration 118/1000 | Loss: 0.00001784
Iteration 119/1000 | Loss: 0.00001784
Iteration 120/1000 | Loss: 0.00001784
Iteration 121/1000 | Loss: 0.00001784
Iteration 122/1000 | Loss: 0.00001784
Iteration 123/1000 | Loss: 0.00001784
Iteration 124/1000 | Loss: 0.00001784
Iteration 125/1000 | Loss: 0.00001784
Iteration 126/1000 | Loss: 0.00001784
Iteration 127/1000 | Loss: 0.00001784
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001784
Iteration 130/1000 | Loss: 0.00001783
Iteration 131/1000 | Loss: 0.00001783
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001783
Iteration 137/1000 | Loss: 0.00001783
Iteration 138/1000 | Loss: 0.00001783
Iteration 139/1000 | Loss: 0.00001783
Iteration 140/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.782773142622318e-05, 1.782773142622318e-05, 1.782773142622318e-05, 1.782773142622318e-05, 1.782773142622318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.782773142622318e-05

Optimization complete. Final v2v error: 3.459185838699341 mm

Highest mean error: 5.557404518127441 mm for frame 65

Lowest mean error: 2.8058481216430664 mm for frame 205

Saving results

Total time: 86.16736578941345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831844
Iteration 2/25 | Loss: 0.00159661
Iteration 3/25 | Loss: 0.00100206
Iteration 4/25 | Loss: 0.00092814
Iteration 5/25 | Loss: 0.00091796
Iteration 6/25 | Loss: 0.00091547
Iteration 7/25 | Loss: 0.00091532
Iteration 8/25 | Loss: 0.00091532
Iteration 9/25 | Loss: 0.00091532
Iteration 10/25 | Loss: 0.00091532
Iteration 11/25 | Loss: 0.00091532
Iteration 12/25 | Loss: 0.00091532
Iteration 13/25 | Loss: 0.00091532
Iteration 14/25 | Loss: 0.00091532
Iteration 15/25 | Loss: 0.00091532
Iteration 16/25 | Loss: 0.00091532
Iteration 17/25 | Loss: 0.00091532
Iteration 18/25 | Loss: 0.00091532
Iteration 19/25 | Loss: 0.00091532
Iteration 20/25 | Loss: 0.00091532
Iteration 21/25 | Loss: 0.00091532
Iteration 22/25 | Loss: 0.00091532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009153187274932861, 0.0009153187274932861, 0.0009153187274932861, 0.0009153187274932861, 0.0009153187274932861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009153187274932861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.44015315
Iteration 2/25 | Loss: 0.00051028
Iteration 3/25 | Loss: 0.00051025
Iteration 4/25 | Loss: 0.00051025
Iteration 5/25 | Loss: 0.00051025
Iteration 6/25 | Loss: 0.00051025
Iteration 7/25 | Loss: 0.00051025
Iteration 8/25 | Loss: 0.00051025
Iteration 9/25 | Loss: 0.00051025
Iteration 10/25 | Loss: 0.00051025
Iteration 11/25 | Loss: 0.00051025
Iteration 12/25 | Loss: 0.00051025
Iteration 13/25 | Loss: 0.00051025
Iteration 14/25 | Loss: 0.00051025
Iteration 15/25 | Loss: 0.00051025
Iteration 16/25 | Loss: 0.00051025
Iteration 17/25 | Loss: 0.00051025
Iteration 18/25 | Loss: 0.00051025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005102487048134208, 0.0005102487048134208, 0.0005102487048134208, 0.0005102487048134208, 0.0005102487048134208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005102487048134208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051025
Iteration 2/1000 | Loss: 0.00006980
Iteration 3/1000 | Loss: 0.00004741
Iteration 4/1000 | Loss: 0.00004116
Iteration 5/1000 | Loss: 0.00003886
Iteration 6/1000 | Loss: 0.00003682
Iteration 7/1000 | Loss: 0.00003581
Iteration 8/1000 | Loss: 0.00003474
Iteration 9/1000 | Loss: 0.00003404
Iteration 10/1000 | Loss: 0.00003342
Iteration 11/1000 | Loss: 0.00003304
Iteration 12/1000 | Loss: 0.00003270
Iteration 13/1000 | Loss: 0.00003239
Iteration 14/1000 | Loss: 0.00003214
Iteration 15/1000 | Loss: 0.00003198
Iteration 16/1000 | Loss: 0.00003191
Iteration 17/1000 | Loss: 0.00003176
Iteration 18/1000 | Loss: 0.00003156
Iteration 19/1000 | Loss: 0.00003138
Iteration 20/1000 | Loss: 0.00003134
Iteration 21/1000 | Loss: 0.00003133
Iteration 22/1000 | Loss: 0.00003132
Iteration 23/1000 | Loss: 0.00003127
Iteration 24/1000 | Loss: 0.00003123
Iteration 25/1000 | Loss: 0.00003123
Iteration 26/1000 | Loss: 0.00003114
Iteration 27/1000 | Loss: 0.00003109
Iteration 28/1000 | Loss: 0.00003106
Iteration 29/1000 | Loss: 0.00003106
Iteration 30/1000 | Loss: 0.00003106
Iteration 31/1000 | Loss: 0.00003105
Iteration 32/1000 | Loss: 0.00003105
Iteration 33/1000 | Loss: 0.00003103
Iteration 34/1000 | Loss: 0.00003100
Iteration 35/1000 | Loss: 0.00003100
Iteration 36/1000 | Loss: 0.00003098
Iteration 37/1000 | Loss: 0.00003097
Iteration 38/1000 | Loss: 0.00003097
Iteration 39/1000 | Loss: 0.00003096
Iteration 40/1000 | Loss: 0.00003096
Iteration 41/1000 | Loss: 0.00003087
Iteration 42/1000 | Loss: 0.00003086
Iteration 43/1000 | Loss: 0.00003086
Iteration 44/1000 | Loss: 0.00003084
Iteration 45/1000 | Loss: 0.00003084
Iteration 46/1000 | Loss: 0.00003083
Iteration 47/1000 | Loss: 0.00003083
Iteration 48/1000 | Loss: 0.00003083
Iteration 49/1000 | Loss: 0.00003083
Iteration 50/1000 | Loss: 0.00003083
Iteration 51/1000 | Loss: 0.00003083
Iteration 52/1000 | Loss: 0.00003082
Iteration 53/1000 | Loss: 0.00003082
Iteration 54/1000 | Loss: 0.00003082
Iteration 55/1000 | Loss: 0.00003082
Iteration 56/1000 | Loss: 0.00003081
Iteration 57/1000 | Loss: 0.00003081
Iteration 58/1000 | Loss: 0.00003081
Iteration 59/1000 | Loss: 0.00003081
Iteration 60/1000 | Loss: 0.00003081
Iteration 61/1000 | Loss: 0.00003080
Iteration 62/1000 | Loss: 0.00003080
Iteration 63/1000 | Loss: 0.00003080
Iteration 64/1000 | Loss: 0.00003080
Iteration 65/1000 | Loss: 0.00003080
Iteration 66/1000 | Loss: 0.00003080
Iteration 67/1000 | Loss: 0.00003080
Iteration 68/1000 | Loss: 0.00003080
Iteration 69/1000 | Loss: 0.00003080
Iteration 70/1000 | Loss: 0.00003080
Iteration 71/1000 | Loss: 0.00003080
Iteration 72/1000 | Loss: 0.00003080
Iteration 73/1000 | Loss: 0.00003080
Iteration 74/1000 | Loss: 0.00003079
Iteration 75/1000 | Loss: 0.00003079
Iteration 76/1000 | Loss: 0.00003079
Iteration 77/1000 | Loss: 0.00003079
Iteration 78/1000 | Loss: 0.00003079
Iteration 79/1000 | Loss: 0.00003079
Iteration 80/1000 | Loss: 0.00003079
Iteration 81/1000 | Loss: 0.00003079
Iteration 82/1000 | Loss: 0.00003079
Iteration 83/1000 | Loss: 0.00003079
Iteration 84/1000 | Loss: 0.00003078
Iteration 85/1000 | Loss: 0.00003078
Iteration 86/1000 | Loss: 0.00003077
Iteration 87/1000 | Loss: 0.00003077
Iteration 88/1000 | Loss: 0.00003077
Iteration 89/1000 | Loss: 0.00003077
Iteration 90/1000 | Loss: 0.00003077
Iteration 91/1000 | Loss: 0.00003077
Iteration 92/1000 | Loss: 0.00003077
Iteration 93/1000 | Loss: 0.00003077
Iteration 94/1000 | Loss: 0.00003077
Iteration 95/1000 | Loss: 0.00003077
Iteration 96/1000 | Loss: 0.00003077
Iteration 97/1000 | Loss: 0.00003077
Iteration 98/1000 | Loss: 0.00003077
Iteration 99/1000 | Loss: 0.00003076
Iteration 100/1000 | Loss: 0.00003076
Iteration 101/1000 | Loss: 0.00003076
Iteration 102/1000 | Loss: 0.00003076
Iteration 103/1000 | Loss: 0.00003076
Iteration 104/1000 | Loss: 0.00003075
Iteration 105/1000 | Loss: 0.00003075
Iteration 106/1000 | Loss: 0.00003075
Iteration 107/1000 | Loss: 0.00003075
Iteration 108/1000 | Loss: 0.00003074
Iteration 109/1000 | Loss: 0.00003074
Iteration 110/1000 | Loss: 0.00003074
Iteration 111/1000 | Loss: 0.00003074
Iteration 112/1000 | Loss: 0.00003074
Iteration 113/1000 | Loss: 0.00003073
Iteration 114/1000 | Loss: 0.00003073
Iteration 115/1000 | Loss: 0.00003073
Iteration 116/1000 | Loss: 0.00003072
Iteration 117/1000 | Loss: 0.00003072
Iteration 118/1000 | Loss: 0.00003072
Iteration 119/1000 | Loss: 0.00003072
Iteration 120/1000 | Loss: 0.00003071
Iteration 121/1000 | Loss: 0.00003071
Iteration 122/1000 | Loss: 0.00003071
Iteration 123/1000 | Loss: 0.00003071
Iteration 124/1000 | Loss: 0.00003070
Iteration 125/1000 | Loss: 0.00003070
Iteration 126/1000 | Loss: 0.00003070
Iteration 127/1000 | Loss: 0.00003070
Iteration 128/1000 | Loss: 0.00003070
Iteration 129/1000 | Loss: 0.00003069
Iteration 130/1000 | Loss: 0.00003069
Iteration 131/1000 | Loss: 0.00003069
Iteration 132/1000 | Loss: 0.00003069
Iteration 133/1000 | Loss: 0.00003069
Iteration 134/1000 | Loss: 0.00003069
Iteration 135/1000 | Loss: 0.00003068
Iteration 136/1000 | Loss: 0.00003068
Iteration 137/1000 | Loss: 0.00003068
Iteration 138/1000 | Loss: 0.00003068
Iteration 139/1000 | Loss: 0.00003068
Iteration 140/1000 | Loss: 0.00003068
Iteration 141/1000 | Loss: 0.00003068
Iteration 142/1000 | Loss: 0.00003068
Iteration 143/1000 | Loss: 0.00003068
Iteration 144/1000 | Loss: 0.00003068
Iteration 145/1000 | Loss: 0.00003068
Iteration 146/1000 | Loss: 0.00003068
Iteration 147/1000 | Loss: 0.00003068
Iteration 148/1000 | Loss: 0.00003068
Iteration 149/1000 | Loss: 0.00003068
Iteration 150/1000 | Loss: 0.00003067
Iteration 151/1000 | Loss: 0.00003067
Iteration 152/1000 | Loss: 0.00003067
Iteration 153/1000 | Loss: 0.00003067
Iteration 154/1000 | Loss: 0.00003067
Iteration 155/1000 | Loss: 0.00003067
Iteration 156/1000 | Loss: 0.00003067
Iteration 157/1000 | Loss: 0.00003067
Iteration 158/1000 | Loss: 0.00003067
Iteration 159/1000 | Loss: 0.00003067
Iteration 160/1000 | Loss: 0.00003067
Iteration 161/1000 | Loss: 0.00003067
Iteration 162/1000 | Loss: 0.00003067
Iteration 163/1000 | Loss: 0.00003067
Iteration 164/1000 | Loss: 0.00003066
Iteration 165/1000 | Loss: 0.00003066
Iteration 166/1000 | Loss: 0.00003066
Iteration 167/1000 | Loss: 0.00003066
Iteration 168/1000 | Loss: 0.00003066
Iteration 169/1000 | Loss: 0.00003066
Iteration 170/1000 | Loss: 0.00003066
Iteration 171/1000 | Loss: 0.00003066
Iteration 172/1000 | Loss: 0.00003066
Iteration 173/1000 | Loss: 0.00003066
Iteration 174/1000 | Loss: 0.00003066
Iteration 175/1000 | Loss: 0.00003065
Iteration 176/1000 | Loss: 0.00003065
Iteration 177/1000 | Loss: 0.00003065
Iteration 178/1000 | Loss: 0.00003065
Iteration 179/1000 | Loss: 0.00003065
Iteration 180/1000 | Loss: 0.00003065
Iteration 181/1000 | Loss: 0.00003065
Iteration 182/1000 | Loss: 0.00003065
Iteration 183/1000 | Loss: 0.00003065
Iteration 184/1000 | Loss: 0.00003065
Iteration 185/1000 | Loss: 0.00003065
Iteration 186/1000 | Loss: 0.00003064
Iteration 187/1000 | Loss: 0.00003064
Iteration 188/1000 | Loss: 0.00003064
Iteration 189/1000 | Loss: 0.00003064
Iteration 190/1000 | Loss: 0.00003064
Iteration 191/1000 | Loss: 0.00003064
Iteration 192/1000 | Loss: 0.00003064
Iteration 193/1000 | Loss: 0.00003063
Iteration 194/1000 | Loss: 0.00003063
Iteration 195/1000 | Loss: 0.00003063
Iteration 196/1000 | Loss: 0.00003063
Iteration 197/1000 | Loss: 0.00003063
Iteration 198/1000 | Loss: 0.00003063
Iteration 199/1000 | Loss: 0.00003063
Iteration 200/1000 | Loss: 0.00003063
Iteration 201/1000 | Loss: 0.00003063
Iteration 202/1000 | Loss: 0.00003063
Iteration 203/1000 | Loss: 0.00003063
Iteration 204/1000 | Loss: 0.00003063
Iteration 205/1000 | Loss: 0.00003062
Iteration 206/1000 | Loss: 0.00003062
Iteration 207/1000 | Loss: 0.00003062
Iteration 208/1000 | Loss: 0.00003062
Iteration 209/1000 | Loss: 0.00003062
Iteration 210/1000 | Loss: 0.00003062
Iteration 211/1000 | Loss: 0.00003062
Iteration 212/1000 | Loss: 0.00003062
Iteration 213/1000 | Loss: 0.00003062
Iteration 214/1000 | Loss: 0.00003062
Iteration 215/1000 | Loss: 0.00003062
Iteration 216/1000 | Loss: 0.00003062
Iteration 217/1000 | Loss: 0.00003062
Iteration 218/1000 | Loss: 0.00003062
Iteration 219/1000 | Loss: 0.00003062
Iteration 220/1000 | Loss: 0.00003062
Iteration 221/1000 | Loss: 0.00003062
Iteration 222/1000 | Loss: 0.00003062
Iteration 223/1000 | Loss: 0.00003062
Iteration 224/1000 | Loss: 0.00003062
Iteration 225/1000 | Loss: 0.00003062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [3.061951792915352e-05, 3.061951792915352e-05, 3.061951792915352e-05, 3.061951792915352e-05, 3.061951792915352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.061951792915352e-05

Optimization complete. Final v2v error: 4.365131855010986 mm

Highest mean error: 5.946305274963379 mm for frame 58

Lowest mean error: 3.121833086013794 mm for frame 101

Saving results

Total time: 56.80542206764221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843928
Iteration 2/25 | Loss: 0.00088715
Iteration 3/25 | Loss: 0.00072894
Iteration 4/25 | Loss: 0.00070767
Iteration 5/25 | Loss: 0.00070230
Iteration 6/25 | Loss: 0.00070026
Iteration 7/25 | Loss: 0.00069992
Iteration 8/25 | Loss: 0.00069992
Iteration 9/25 | Loss: 0.00069992
Iteration 10/25 | Loss: 0.00069992
Iteration 11/25 | Loss: 0.00069992
Iteration 12/25 | Loss: 0.00069992
Iteration 13/25 | Loss: 0.00069992
Iteration 14/25 | Loss: 0.00069992
Iteration 15/25 | Loss: 0.00069992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006999212200753391, 0.0006999212200753391, 0.0006999212200753391, 0.0006999212200753391, 0.0006999212200753391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006999212200753391

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47637546
Iteration 2/25 | Loss: 0.00030129
Iteration 3/25 | Loss: 0.00030127
Iteration 4/25 | Loss: 0.00030127
Iteration 5/25 | Loss: 0.00030127
Iteration 6/25 | Loss: 0.00030127
Iteration 7/25 | Loss: 0.00030127
Iteration 8/25 | Loss: 0.00030127
Iteration 9/25 | Loss: 0.00030127
Iteration 10/25 | Loss: 0.00030127
Iteration 11/25 | Loss: 0.00030127
Iteration 12/25 | Loss: 0.00030127
Iteration 13/25 | Loss: 0.00030127
Iteration 14/25 | Loss: 0.00030127
Iteration 15/25 | Loss: 0.00030127
Iteration 16/25 | Loss: 0.00030127
Iteration 17/25 | Loss: 0.00030127
Iteration 18/25 | Loss: 0.00030127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00030126547790132463, 0.00030126547790132463, 0.00030126547790132463, 0.00030126547790132463, 0.00030126547790132463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030126547790132463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030127
Iteration 2/1000 | Loss: 0.00001808
Iteration 3/1000 | Loss: 0.00001446
Iteration 4/1000 | Loss: 0.00001314
Iteration 5/1000 | Loss: 0.00001204
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001113
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001079
Iteration 13/1000 | Loss: 0.00001075
Iteration 14/1000 | Loss: 0.00001073
Iteration 15/1000 | Loss: 0.00001072
Iteration 16/1000 | Loss: 0.00001072
Iteration 17/1000 | Loss: 0.00001071
Iteration 18/1000 | Loss: 0.00001066
Iteration 19/1000 | Loss: 0.00001058
Iteration 20/1000 | Loss: 0.00001056
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001043
Iteration 35/1000 | Loss: 0.00001043
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001040
Iteration 50/1000 | Loss: 0.00001040
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001039
Iteration 58/1000 | Loss: 0.00001039
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001037
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001034
Iteration 96/1000 | Loss: 0.00001034
Iteration 97/1000 | Loss: 0.00001034
Iteration 98/1000 | Loss: 0.00001034
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001034
Iteration 102/1000 | Loss: 0.00001034
Iteration 103/1000 | Loss: 0.00001034
Iteration 104/1000 | Loss: 0.00001034
Iteration 105/1000 | Loss: 0.00001034
Iteration 106/1000 | Loss: 0.00001034
Iteration 107/1000 | Loss: 0.00001034
Iteration 108/1000 | Loss: 0.00001034
Iteration 109/1000 | Loss: 0.00001033
Iteration 110/1000 | Loss: 0.00001033
Iteration 111/1000 | Loss: 0.00001033
Iteration 112/1000 | Loss: 0.00001033
Iteration 113/1000 | Loss: 0.00001033
Iteration 114/1000 | Loss: 0.00001033
Iteration 115/1000 | Loss: 0.00001033
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001033
Iteration 122/1000 | Loss: 0.00001033
Iteration 123/1000 | Loss: 0.00001033
Iteration 124/1000 | Loss: 0.00001033
Iteration 125/1000 | Loss: 0.00001033
Iteration 126/1000 | Loss: 0.00001033
Iteration 127/1000 | Loss: 0.00001033
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001032
Iteration 131/1000 | Loss: 0.00001032
Iteration 132/1000 | Loss: 0.00001032
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001032
Iteration 137/1000 | Loss: 0.00001032
Iteration 138/1000 | Loss: 0.00001032
Iteration 139/1000 | Loss: 0.00001032
Iteration 140/1000 | Loss: 0.00001032
Iteration 141/1000 | Loss: 0.00001032
Iteration 142/1000 | Loss: 0.00001032
Iteration 143/1000 | Loss: 0.00001032
Iteration 144/1000 | Loss: 0.00001032
Iteration 145/1000 | Loss: 0.00001032
Iteration 146/1000 | Loss: 0.00001032
Iteration 147/1000 | Loss: 0.00001032
Iteration 148/1000 | Loss: 0.00001032
Iteration 149/1000 | Loss: 0.00001032
Iteration 150/1000 | Loss: 0.00001032
Iteration 151/1000 | Loss: 0.00001032
Iteration 152/1000 | Loss: 0.00001032
Iteration 153/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.0316861335013527e-05, 1.0316861335013527e-05, 1.0316861335013527e-05, 1.0316861335013527e-05, 1.0316861335013527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0316861335013527e-05

Optimization complete. Final v2v error: 2.7360496520996094 mm

Highest mean error: 2.887887954711914 mm for frame 12

Lowest mean error: 2.592771053314209 mm for frame 124

Saving results

Total time: 32.7426335811615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457506
Iteration 2/25 | Loss: 0.00114861
Iteration 3/25 | Loss: 0.00082825
Iteration 4/25 | Loss: 0.00078122
Iteration 5/25 | Loss: 0.00077625
Iteration 6/25 | Loss: 0.00077486
Iteration 7/25 | Loss: 0.00077483
Iteration 8/25 | Loss: 0.00077483
Iteration 9/25 | Loss: 0.00077483
Iteration 10/25 | Loss: 0.00077483
Iteration 11/25 | Loss: 0.00077483
Iteration 12/25 | Loss: 0.00077483
Iteration 13/25 | Loss: 0.00077483
Iteration 14/25 | Loss: 0.00077483
Iteration 15/25 | Loss: 0.00077483
Iteration 16/25 | Loss: 0.00077483
Iteration 17/25 | Loss: 0.00077483
Iteration 18/25 | Loss: 0.00077483
Iteration 19/25 | Loss: 0.00077483
Iteration 20/25 | Loss: 0.00077483
Iteration 21/25 | Loss: 0.00077483
Iteration 22/25 | Loss: 0.00077483
Iteration 23/25 | Loss: 0.00077483
Iteration 24/25 | Loss: 0.00077483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007748277275823057, 0.0007748277275823057, 0.0007748277275823057, 0.0007748277275823057, 0.0007748277275823057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007748277275823057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46175206
Iteration 2/25 | Loss: 0.00032422
Iteration 3/25 | Loss: 0.00032422
Iteration 4/25 | Loss: 0.00032422
Iteration 5/25 | Loss: 0.00032422
Iteration 6/25 | Loss: 0.00032422
Iteration 7/25 | Loss: 0.00032422
Iteration 8/25 | Loss: 0.00032422
Iteration 9/25 | Loss: 0.00032422
Iteration 10/25 | Loss: 0.00032422
Iteration 11/25 | Loss: 0.00032422
Iteration 12/25 | Loss: 0.00032422
Iteration 13/25 | Loss: 0.00032422
Iteration 14/25 | Loss: 0.00032422
Iteration 15/25 | Loss: 0.00032422
Iteration 16/25 | Loss: 0.00032422
Iteration 17/25 | Loss: 0.00032422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00032421559444628656, 0.00032421559444628656, 0.00032421559444628656, 0.00032421559444628656, 0.00032421559444628656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032421559444628656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032422
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00002524
Iteration 4/1000 | Loss: 0.00002297
Iteration 5/1000 | Loss: 0.00002149
Iteration 6/1000 | Loss: 0.00002051
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001921
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00001884
Iteration 12/1000 | Loss: 0.00001866
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001860
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001859
Iteration 17/1000 | Loss: 0.00001856
Iteration 18/1000 | Loss: 0.00001856
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001850
Iteration 28/1000 | Loss: 0.00001850
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001850
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001849
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001848
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001848
Iteration 47/1000 | Loss: 0.00001848
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001847
Iteration 51/1000 | Loss: 0.00001847
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001846
Iteration 62/1000 | Loss: 0.00001846
Iteration 63/1000 | Loss: 0.00001846
Iteration 64/1000 | Loss: 0.00001846
Iteration 65/1000 | Loss: 0.00001845
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001845
Iteration 68/1000 | Loss: 0.00001845
Iteration 69/1000 | Loss: 0.00001845
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001844
Iteration 73/1000 | Loss: 0.00001844
Iteration 74/1000 | Loss: 0.00001844
Iteration 75/1000 | Loss: 0.00001844
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001844
Iteration 83/1000 | Loss: 0.00001844
Iteration 84/1000 | Loss: 0.00001844
Iteration 85/1000 | Loss: 0.00001844
Iteration 86/1000 | Loss: 0.00001844
Iteration 87/1000 | Loss: 0.00001844
Iteration 88/1000 | Loss: 0.00001844
Iteration 89/1000 | Loss: 0.00001844
Iteration 90/1000 | Loss: 0.00001844
Iteration 91/1000 | Loss: 0.00001844
Iteration 92/1000 | Loss: 0.00001844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.844025609898381e-05, 1.844025609898381e-05, 1.844025609898381e-05, 1.844025609898381e-05, 1.844025609898381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.844025609898381e-05

Optimization complete. Final v2v error: 3.54170298576355 mm

Highest mean error: 3.680800199508667 mm for frame 87

Lowest mean error: 3.4323344230651855 mm for frame 16

Saving results

Total time: 30.625866174697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739397
Iteration 2/25 | Loss: 0.00185448
Iteration 3/25 | Loss: 0.00112873
Iteration 4/25 | Loss: 0.00099535
Iteration 5/25 | Loss: 0.00093954
Iteration 6/25 | Loss: 0.00092396
Iteration 7/25 | Loss: 0.00091908
Iteration 8/25 | Loss: 0.00092872
Iteration 9/25 | Loss: 0.00092009
Iteration 10/25 | Loss: 0.00091669
Iteration 11/25 | Loss: 0.00094848
Iteration 12/25 | Loss: 0.00093943
Iteration 13/25 | Loss: 0.00094434
Iteration 14/25 | Loss: 0.00090633
Iteration 15/25 | Loss: 0.00090568
Iteration 16/25 | Loss: 0.00090329
Iteration 17/25 | Loss: 0.00090357
Iteration 18/25 | Loss: 0.00090363
Iteration 19/25 | Loss: 0.00090192
Iteration 20/25 | Loss: 0.00090544
Iteration 21/25 | Loss: 0.00090135
Iteration 22/25 | Loss: 0.00089639
Iteration 23/25 | Loss: 0.00089456
Iteration 24/25 | Loss: 0.00089340
Iteration 25/25 | Loss: 0.00089304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.21467018
Iteration 2/25 | Loss: 0.00120979
Iteration 3/25 | Loss: 0.00120826
Iteration 4/25 | Loss: 0.00120826
Iteration 5/25 | Loss: 0.00120826
Iteration 6/25 | Loss: 0.00120826
Iteration 7/25 | Loss: 0.00120826
Iteration 8/25 | Loss: 0.00120826
Iteration 9/25 | Loss: 0.00120826
Iteration 10/25 | Loss: 0.00120826
Iteration 11/25 | Loss: 0.00120826
Iteration 12/25 | Loss: 0.00120826
Iteration 13/25 | Loss: 0.00120826
Iteration 14/25 | Loss: 0.00120826
Iteration 15/25 | Loss: 0.00120826
Iteration 16/25 | Loss: 0.00120826
Iteration 17/25 | Loss: 0.00120826
Iteration 18/25 | Loss: 0.00120826
Iteration 19/25 | Loss: 0.00120826
Iteration 20/25 | Loss: 0.00120826
Iteration 21/25 | Loss: 0.00120826
Iteration 22/25 | Loss: 0.00120826
Iteration 23/25 | Loss: 0.00120826
Iteration 24/25 | Loss: 0.00120826
Iteration 25/25 | Loss: 0.00120826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120826
Iteration 2/1000 | Loss: 0.00024753
Iteration 3/1000 | Loss: 0.00013564
Iteration 4/1000 | Loss: 0.00010748
Iteration 5/1000 | Loss: 0.00010871
Iteration 6/1000 | Loss: 0.00011644
Iteration 7/1000 | Loss: 0.00008310
Iteration 8/1000 | Loss: 0.00038979
Iteration 9/1000 | Loss: 0.00021415
Iteration 10/1000 | Loss: 0.00017752
Iteration 11/1000 | Loss: 0.00008439
Iteration 12/1000 | Loss: 0.00007768
Iteration 13/1000 | Loss: 0.00020215
Iteration 14/1000 | Loss: 0.00085685
Iteration 15/1000 | Loss: 0.00065417
Iteration 16/1000 | Loss: 0.00023831
Iteration 17/1000 | Loss: 0.00014526
Iteration 18/1000 | Loss: 0.00008188
Iteration 19/1000 | Loss: 0.00007483
Iteration 20/1000 | Loss: 0.00038451
Iteration 21/1000 | Loss: 0.00044644
Iteration 22/1000 | Loss: 0.00034369
Iteration 23/1000 | Loss: 0.00007657
Iteration 24/1000 | Loss: 0.00036975
Iteration 25/1000 | Loss: 0.00008615
Iteration 26/1000 | Loss: 0.00007177
Iteration 27/1000 | Loss: 0.00032158
Iteration 28/1000 | Loss: 0.00093397
Iteration 29/1000 | Loss: 0.00030550
Iteration 30/1000 | Loss: 0.00006944
Iteration 31/1000 | Loss: 0.00102041
Iteration 32/1000 | Loss: 0.00031585
Iteration 33/1000 | Loss: 0.00015204
Iteration 34/1000 | Loss: 0.00006585
Iteration 35/1000 | Loss: 0.00169952
Iteration 36/1000 | Loss: 0.00069960
Iteration 37/1000 | Loss: 0.00007896
Iteration 38/1000 | Loss: 0.00006477
Iteration 39/1000 | Loss: 0.00050655
Iteration 40/1000 | Loss: 0.00308907
Iteration 41/1000 | Loss: 0.00114379
Iteration 42/1000 | Loss: 0.00007620
Iteration 43/1000 | Loss: 0.00006420
Iteration 44/1000 | Loss: 0.00025669
Iteration 45/1000 | Loss: 0.00054347
Iteration 46/1000 | Loss: 0.00046146
Iteration 47/1000 | Loss: 0.00262054
Iteration 48/1000 | Loss: 0.00064790
Iteration 49/1000 | Loss: 0.00042706
Iteration 50/1000 | Loss: 0.00033421
Iteration 51/1000 | Loss: 0.00041319
Iteration 52/1000 | Loss: 0.00011725
Iteration 53/1000 | Loss: 0.00017784
Iteration 54/1000 | Loss: 0.00048100
Iteration 55/1000 | Loss: 0.00038016
Iteration 56/1000 | Loss: 0.00029916
Iteration 57/1000 | Loss: 0.00036976
Iteration 58/1000 | Loss: 0.00035320
Iteration 59/1000 | Loss: 0.00030386
Iteration 60/1000 | Loss: 0.00209540
Iteration 61/1000 | Loss: 0.00034311
Iteration 62/1000 | Loss: 0.00007091
Iteration 63/1000 | Loss: 0.00010799
Iteration 64/1000 | Loss: 0.00009768
Iteration 65/1000 | Loss: 0.00011127
Iteration 66/1000 | Loss: 0.00009113
Iteration 67/1000 | Loss: 0.00011792
Iteration 68/1000 | Loss: 0.00009347
Iteration 69/1000 | Loss: 0.00010037
Iteration 70/1000 | Loss: 0.00006377
Iteration 71/1000 | Loss: 0.00128524
Iteration 72/1000 | Loss: 0.00172672
Iteration 73/1000 | Loss: 0.00027880
Iteration 74/1000 | Loss: 0.00013655
Iteration 75/1000 | Loss: 0.00009496
Iteration 76/1000 | Loss: 0.00161874
Iteration 77/1000 | Loss: 0.00111900
Iteration 78/1000 | Loss: 0.00134988
Iteration 79/1000 | Loss: 0.00087282
Iteration 80/1000 | Loss: 0.00027963
Iteration 81/1000 | Loss: 0.00041875
Iteration 82/1000 | Loss: 0.00101268
Iteration 83/1000 | Loss: 0.00164421
Iteration 84/1000 | Loss: 0.00110622
Iteration 85/1000 | Loss: 0.00091565
Iteration 86/1000 | Loss: 0.00104309
Iteration 87/1000 | Loss: 0.00159402
Iteration 88/1000 | Loss: 0.00027512
Iteration 89/1000 | Loss: 0.00116222
Iteration 90/1000 | Loss: 0.00089596
Iteration 91/1000 | Loss: 0.00154805
Iteration 92/1000 | Loss: 0.00096112
Iteration 93/1000 | Loss: 0.00033925
Iteration 94/1000 | Loss: 0.00056763
Iteration 95/1000 | Loss: 0.00038757
Iteration 96/1000 | Loss: 0.00051922
Iteration 97/1000 | Loss: 0.00044614
Iteration 98/1000 | Loss: 0.00006356
Iteration 99/1000 | Loss: 0.00031338
Iteration 100/1000 | Loss: 0.00096735
Iteration 101/1000 | Loss: 0.00062158
Iteration 102/1000 | Loss: 0.00017733
Iteration 103/1000 | Loss: 0.00030992
Iteration 104/1000 | Loss: 0.00007294
Iteration 105/1000 | Loss: 0.00006467
Iteration 106/1000 | Loss: 0.00006148
Iteration 107/1000 | Loss: 0.00033383
Iteration 108/1000 | Loss: 0.00023563
Iteration 109/1000 | Loss: 0.00081317
Iteration 110/1000 | Loss: 0.00109140
Iteration 111/1000 | Loss: 0.00087058
Iteration 112/1000 | Loss: 0.00148488
Iteration 113/1000 | Loss: 0.00117649
Iteration 114/1000 | Loss: 0.00137316
Iteration 115/1000 | Loss: 0.00079154
Iteration 116/1000 | Loss: 0.00039190
Iteration 117/1000 | Loss: 0.00036422
Iteration 118/1000 | Loss: 0.00085675
Iteration 119/1000 | Loss: 0.00071210
Iteration 120/1000 | Loss: 0.00082228
Iteration 121/1000 | Loss: 0.00090051
Iteration 122/1000 | Loss: 0.00081600
Iteration 123/1000 | Loss: 0.00075933
Iteration 124/1000 | Loss: 0.00095228
Iteration 125/1000 | Loss: 0.00037972
Iteration 126/1000 | Loss: 0.00059678
Iteration 127/1000 | Loss: 0.00106787
Iteration 128/1000 | Loss: 0.00108343
Iteration 129/1000 | Loss: 0.00100168
Iteration 130/1000 | Loss: 0.00069995
Iteration 131/1000 | Loss: 0.00025730
Iteration 132/1000 | Loss: 0.00007591
Iteration 133/1000 | Loss: 0.00019657
Iteration 134/1000 | Loss: 0.00101105
Iteration 135/1000 | Loss: 0.00080209
Iteration 136/1000 | Loss: 0.00053221
Iteration 137/1000 | Loss: 0.00121535
Iteration 138/1000 | Loss: 0.00105316
Iteration 139/1000 | Loss: 0.00136251
Iteration 140/1000 | Loss: 0.00081170
Iteration 141/1000 | Loss: 0.00056201
Iteration 142/1000 | Loss: 0.00123249
Iteration 143/1000 | Loss: 0.00030831
Iteration 144/1000 | Loss: 0.00030839
Iteration 145/1000 | Loss: 0.00029783
Iteration 146/1000 | Loss: 0.00044988
Iteration 147/1000 | Loss: 0.00038448
Iteration 148/1000 | Loss: 0.00033218
Iteration 149/1000 | Loss: 0.00016570
Iteration 150/1000 | Loss: 0.00075124
Iteration 151/1000 | Loss: 0.00054537
Iteration 152/1000 | Loss: 0.00042304
Iteration 153/1000 | Loss: 0.00071080
Iteration 154/1000 | Loss: 0.00032670
Iteration 155/1000 | Loss: 0.00025342
Iteration 156/1000 | Loss: 0.00066527
Iteration 157/1000 | Loss: 0.00034568
Iteration 158/1000 | Loss: 0.00053934
Iteration 159/1000 | Loss: 0.00012466
Iteration 160/1000 | Loss: 0.00050724
Iteration 161/1000 | Loss: 0.00032114
Iteration 162/1000 | Loss: 0.00058122
Iteration 163/1000 | Loss: 0.00035668
Iteration 164/1000 | Loss: 0.00006450
Iteration 165/1000 | Loss: 0.00107424
Iteration 166/1000 | Loss: 0.00025146
Iteration 167/1000 | Loss: 0.00007163
Iteration 168/1000 | Loss: 0.00049264
Iteration 169/1000 | Loss: 0.00056124
Iteration 170/1000 | Loss: 0.00083096
Iteration 171/1000 | Loss: 0.00052464
Iteration 172/1000 | Loss: 0.00035580
Iteration 173/1000 | Loss: 0.00047037
Iteration 174/1000 | Loss: 0.00061459
Iteration 175/1000 | Loss: 0.00011686
Iteration 176/1000 | Loss: 0.00059295
Iteration 177/1000 | Loss: 0.00025251
Iteration 178/1000 | Loss: 0.00077360
Iteration 179/1000 | Loss: 0.00069077
Iteration 180/1000 | Loss: 0.00054992
Iteration 181/1000 | Loss: 0.00065002
Iteration 182/1000 | Loss: 0.00049623
Iteration 183/1000 | Loss: 0.00028001
Iteration 184/1000 | Loss: 0.00052778
Iteration 185/1000 | Loss: 0.00061476
Iteration 186/1000 | Loss: 0.00088158
Iteration 187/1000 | Loss: 0.00109108
Iteration 188/1000 | Loss: 0.00038015
Iteration 189/1000 | Loss: 0.00053114
Iteration 190/1000 | Loss: 0.00006542
Iteration 191/1000 | Loss: 0.00005776
Iteration 192/1000 | Loss: 0.00033840
Iteration 193/1000 | Loss: 0.00115416
Iteration 194/1000 | Loss: 0.00092136
Iteration 195/1000 | Loss: 0.00007742
Iteration 196/1000 | Loss: 0.00060615
Iteration 197/1000 | Loss: 0.00005557
Iteration 198/1000 | Loss: 0.00005179
Iteration 199/1000 | Loss: 0.00005007
Iteration 200/1000 | Loss: 0.00017841
Iteration 201/1000 | Loss: 0.00084930
Iteration 202/1000 | Loss: 0.00007179
Iteration 203/1000 | Loss: 0.00005291
Iteration 204/1000 | Loss: 0.00004932
Iteration 205/1000 | Loss: 0.00004817
Iteration 206/1000 | Loss: 0.00006064
Iteration 207/1000 | Loss: 0.00004744
Iteration 208/1000 | Loss: 0.00023610
Iteration 209/1000 | Loss: 0.00011486
Iteration 210/1000 | Loss: 0.00238350
Iteration 211/1000 | Loss: 0.00066851
Iteration 212/1000 | Loss: 0.00033816
Iteration 213/1000 | Loss: 0.00026844
Iteration 214/1000 | Loss: 0.00049389
Iteration 215/1000 | Loss: 0.00030005
Iteration 216/1000 | Loss: 0.00053765
Iteration 217/1000 | Loss: 0.00029661
Iteration 218/1000 | Loss: 0.00042918
Iteration 219/1000 | Loss: 0.00017085
Iteration 220/1000 | Loss: 0.00029926
Iteration 221/1000 | Loss: 0.00026036
Iteration 222/1000 | Loss: 0.00014923
Iteration 223/1000 | Loss: 0.00007297
Iteration 224/1000 | Loss: 0.00011669
Iteration 225/1000 | Loss: 0.00005764
Iteration 226/1000 | Loss: 0.00027494
Iteration 227/1000 | Loss: 0.00027712
Iteration 228/1000 | Loss: 0.00051481
Iteration 229/1000 | Loss: 0.00006502
Iteration 230/1000 | Loss: 0.00043031
Iteration 231/1000 | Loss: 0.00007155
Iteration 232/1000 | Loss: 0.00048382
Iteration 233/1000 | Loss: 0.00050228
Iteration 234/1000 | Loss: 0.00073599
Iteration 235/1000 | Loss: 0.00023474
Iteration 236/1000 | Loss: 0.00004688
Iteration 237/1000 | Loss: 0.00028366
Iteration 238/1000 | Loss: 0.00021709
Iteration 239/1000 | Loss: 0.00006609
Iteration 240/1000 | Loss: 0.00005154
Iteration 241/1000 | Loss: 0.00004828
Iteration 242/1000 | Loss: 0.00004700
Iteration 243/1000 | Loss: 0.00004626
Iteration 244/1000 | Loss: 0.00004539
Iteration 245/1000 | Loss: 0.00004426
Iteration 246/1000 | Loss: 0.00004325
Iteration 247/1000 | Loss: 0.00004243
Iteration 248/1000 | Loss: 0.00004179
Iteration 249/1000 | Loss: 0.00004133
Iteration 250/1000 | Loss: 0.00004088
Iteration 251/1000 | Loss: 0.00004051
Iteration 252/1000 | Loss: 0.00004025
Iteration 253/1000 | Loss: 0.00004013
Iteration 254/1000 | Loss: 0.00004005
Iteration 255/1000 | Loss: 0.00004001
Iteration 256/1000 | Loss: 0.00003997
Iteration 257/1000 | Loss: 0.00003996
Iteration 258/1000 | Loss: 0.00003990
Iteration 259/1000 | Loss: 0.00003987
Iteration 260/1000 | Loss: 0.00003984
Iteration 261/1000 | Loss: 0.00003981
Iteration 262/1000 | Loss: 0.00003981
Iteration 263/1000 | Loss: 0.00003981
Iteration 264/1000 | Loss: 0.00003980
Iteration 265/1000 | Loss: 0.00003980
Iteration 266/1000 | Loss: 0.00003980
Iteration 267/1000 | Loss: 0.00003979
Iteration 268/1000 | Loss: 0.00003979
Iteration 269/1000 | Loss: 0.00003979
Iteration 270/1000 | Loss: 0.00003978
Iteration 271/1000 | Loss: 0.00003977
Iteration 272/1000 | Loss: 0.00003969
Iteration 273/1000 | Loss: 0.00003969
Iteration 274/1000 | Loss: 0.00003968
Iteration 275/1000 | Loss: 0.00003968
Iteration 276/1000 | Loss: 0.00003968
Iteration 277/1000 | Loss: 0.00003967
Iteration 278/1000 | Loss: 0.00008572
Iteration 279/1000 | Loss: 0.00008572
Iteration 280/1000 | Loss: 0.00005832
Iteration 281/1000 | Loss: 0.00008464
Iteration 282/1000 | Loss: 0.00005039
Iteration 283/1000 | Loss: 0.00004507
Iteration 284/1000 | Loss: 0.00004367
Iteration 285/1000 | Loss: 0.00004195
Iteration 286/1000 | Loss: 0.00004105
Iteration 287/1000 | Loss: 0.00004050
Iteration 288/1000 | Loss: 0.00004002
Iteration 289/1000 | Loss: 0.00003956
Iteration 290/1000 | Loss: 0.00003931
Iteration 291/1000 | Loss: 0.00003924
Iteration 292/1000 | Loss: 0.00003923
Iteration 293/1000 | Loss: 0.00003920
Iteration 294/1000 | Loss: 0.00003917
Iteration 295/1000 | Loss: 0.00003917
Iteration 296/1000 | Loss: 0.00003916
Iteration 297/1000 | Loss: 0.00003916
Iteration 298/1000 | Loss: 0.00003916
Iteration 299/1000 | Loss: 0.00003915
Iteration 300/1000 | Loss: 0.00003915
Iteration 301/1000 | Loss: 0.00003915
Iteration 302/1000 | Loss: 0.00003914
Iteration 303/1000 | Loss: 0.00003914
Iteration 304/1000 | Loss: 0.00003914
Iteration 305/1000 | Loss: 0.00003913
Iteration 306/1000 | Loss: 0.00003913
Iteration 307/1000 | Loss: 0.00003913
Iteration 308/1000 | Loss: 0.00003913
Iteration 309/1000 | Loss: 0.00003913
Iteration 310/1000 | Loss: 0.00003912
Iteration 311/1000 | Loss: 0.00003912
Iteration 312/1000 | Loss: 0.00003912
Iteration 313/1000 | Loss: 0.00003911
Iteration 314/1000 | Loss: 0.00003911
Iteration 315/1000 | Loss: 0.00003911
Iteration 316/1000 | Loss: 0.00003910
Iteration 317/1000 | Loss: 0.00003910
Iteration 318/1000 | Loss: 0.00003910
Iteration 319/1000 | Loss: 0.00003910
Iteration 320/1000 | Loss: 0.00003910
Iteration 321/1000 | Loss: 0.00003910
Iteration 322/1000 | Loss: 0.00003910
Iteration 323/1000 | Loss: 0.00003909
Iteration 324/1000 | Loss: 0.00003909
Iteration 325/1000 | Loss: 0.00003909
Iteration 326/1000 | Loss: 0.00003909
Iteration 327/1000 | Loss: 0.00003909
Iteration 328/1000 | Loss: 0.00003909
Iteration 329/1000 | Loss: 0.00003909
Iteration 330/1000 | Loss: 0.00003909
Iteration 331/1000 | Loss: 0.00003909
Iteration 332/1000 | Loss: 0.00003909
Iteration 333/1000 | Loss: 0.00003909
Iteration 334/1000 | Loss: 0.00003909
Iteration 335/1000 | Loss: 0.00003909
Iteration 336/1000 | Loss: 0.00003909
Iteration 337/1000 | Loss: 0.00003909
Iteration 338/1000 | Loss: 0.00003909
Iteration 339/1000 | Loss: 0.00003908
Iteration 340/1000 | Loss: 0.00003908
Iteration 341/1000 | Loss: 0.00003908
Iteration 342/1000 | Loss: 0.00003908
Iteration 343/1000 | Loss: 0.00003908
Iteration 344/1000 | Loss: 0.00003908
Iteration 345/1000 | Loss: 0.00003908
Iteration 346/1000 | Loss: 0.00003908
Iteration 347/1000 | Loss: 0.00003908
Iteration 348/1000 | Loss: 0.00003908
Iteration 349/1000 | Loss: 0.00003907
Iteration 350/1000 | Loss: 0.00003907
Iteration 351/1000 | Loss: 0.00003907
Iteration 352/1000 | Loss: 0.00003907
Iteration 353/1000 | Loss: 0.00003907
Iteration 354/1000 | Loss: 0.00003907
Iteration 355/1000 | Loss: 0.00003907
Iteration 356/1000 | Loss: 0.00003907
Iteration 357/1000 | Loss: 0.00003907
Iteration 358/1000 | Loss: 0.00003907
Iteration 359/1000 | Loss: 0.00003907
Iteration 360/1000 | Loss: 0.00003907
Iteration 361/1000 | Loss: 0.00003907
Iteration 362/1000 | Loss: 0.00003907
Iteration 363/1000 | Loss: 0.00003907
Iteration 364/1000 | Loss: 0.00003907
Iteration 365/1000 | Loss: 0.00003907
Iteration 366/1000 | Loss: 0.00003907
Iteration 367/1000 | Loss: 0.00003907
Iteration 368/1000 | Loss: 0.00003907
Iteration 369/1000 | Loss: 0.00003907
Iteration 370/1000 | Loss: 0.00003907
Iteration 371/1000 | Loss: 0.00003907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 371. Stopping optimization.
Last 5 losses: [3.906842175638303e-05, 3.906842175638303e-05, 3.906842175638303e-05, 3.906842175638303e-05, 3.906842175638303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.906842175638303e-05

Optimization complete. Final v2v error: 4.527783393859863 mm

Highest mean error: 12.602643013000488 mm for frame 125

Lowest mean error: 3.2817208766937256 mm for frame 14

Saving results

Total time: 425.3221287727356
