Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=186, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10416-10471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999508
Iteration 2/25 | Loss: 0.00247799
Iteration 3/25 | Loss: 0.00198743
Iteration 4/25 | Loss: 0.00201024
Iteration 5/25 | Loss: 0.00197333
Iteration 6/25 | Loss: 0.00190031
Iteration 7/25 | Loss: 0.00176426
Iteration 8/25 | Loss: 0.00172355
Iteration 9/25 | Loss: 0.00166358
Iteration 10/25 | Loss: 0.00167867
Iteration 11/25 | Loss: 0.00162415
Iteration 12/25 | Loss: 0.00156816
Iteration 13/25 | Loss: 0.00161688
Iteration 14/25 | Loss: 0.00158746
Iteration 15/25 | Loss: 0.00154067
Iteration 16/25 | Loss: 0.00152558
Iteration 17/25 | Loss: 0.00151454
Iteration 18/25 | Loss: 0.00150593
Iteration 19/25 | Loss: 0.00150630
Iteration 20/25 | Loss: 0.00150592
Iteration 21/25 | Loss: 0.00150612
Iteration 22/25 | Loss: 0.00151334
Iteration 23/25 | Loss: 0.00150953
Iteration 24/25 | Loss: 0.00150630
Iteration 25/25 | Loss: 0.00150523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33360875
Iteration 2/25 | Loss: 0.00279111
Iteration 3/25 | Loss: 0.00279111
Iteration 4/25 | Loss: 0.00279111
Iteration 5/25 | Loss: 0.00279111
Iteration 6/25 | Loss: 0.00279111
Iteration 7/25 | Loss: 0.00279111
Iteration 8/25 | Loss: 0.00279111
Iteration 9/25 | Loss: 0.00279111
Iteration 10/25 | Loss: 0.00279111
Iteration 11/25 | Loss: 0.00279111
Iteration 12/25 | Loss: 0.00279111
Iteration 13/25 | Loss: 0.00279111
Iteration 14/25 | Loss: 0.00279111
Iteration 15/25 | Loss: 0.00279111
Iteration 16/25 | Loss: 0.00279111
Iteration 17/25 | Loss: 0.00279111
Iteration 18/25 | Loss: 0.00279111
Iteration 19/25 | Loss: 0.00279111
Iteration 20/25 | Loss: 0.00279111
Iteration 21/25 | Loss: 0.00279111
Iteration 22/25 | Loss: 0.00279111
Iteration 23/25 | Loss: 0.00279111
Iteration 24/25 | Loss: 0.00279111
Iteration 25/25 | Loss: 0.00279111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279111
Iteration 2/1000 | Loss: 0.00006773
Iteration 3/1000 | Loss: 0.00024720
Iteration 4/1000 | Loss: 0.00006262
Iteration 5/1000 | Loss: 0.00018266
Iteration 6/1000 | Loss: 0.00006328
Iteration 7/1000 | Loss: 0.00004889
Iteration 8/1000 | Loss: 0.00004102
Iteration 9/1000 | Loss: 0.00027402
Iteration 10/1000 | Loss: 0.00021774
Iteration 11/1000 | Loss: 0.00028949
Iteration 12/1000 | Loss: 0.00016355
Iteration 13/1000 | Loss: 0.00021540
Iteration 14/1000 | Loss: 0.00020845
Iteration 15/1000 | Loss: 0.00008032
Iteration 16/1000 | Loss: 0.00052646
Iteration 17/1000 | Loss: 0.00011664
Iteration 18/1000 | Loss: 0.00019822
Iteration 19/1000 | Loss: 0.00027253
Iteration 20/1000 | Loss: 0.00061469
Iteration 21/1000 | Loss: 0.00037156
Iteration 22/1000 | Loss: 0.00081131
Iteration 23/1000 | Loss: 0.00036493
Iteration 24/1000 | Loss: 0.00075419
Iteration 25/1000 | Loss: 0.00038874
Iteration 26/1000 | Loss: 0.00034749
Iteration 27/1000 | Loss: 0.00006174
Iteration 28/1000 | Loss: 0.00004244
Iteration 29/1000 | Loss: 0.00003890
Iteration 30/1000 | Loss: 0.00003073
Iteration 31/1000 | Loss: 0.00003963
Iteration 32/1000 | Loss: 0.00002794
Iteration 33/1000 | Loss: 0.00002566
Iteration 34/1000 | Loss: 0.00003179
Iteration 35/1000 | Loss: 0.00002404
Iteration 36/1000 | Loss: 0.00003171
Iteration 37/1000 | Loss: 0.00002419
Iteration 38/1000 | Loss: 0.00002312
Iteration 39/1000 | Loss: 0.00057390
Iteration 40/1000 | Loss: 0.00048213
Iteration 41/1000 | Loss: 0.00007112
Iteration 42/1000 | Loss: 0.00003498
Iteration 43/1000 | Loss: 0.00003249
Iteration 44/1000 | Loss: 0.00002292
Iteration 45/1000 | Loss: 0.00022119
Iteration 46/1000 | Loss: 0.00017123
Iteration 47/1000 | Loss: 0.00016432
Iteration 48/1000 | Loss: 0.00065916
Iteration 49/1000 | Loss: 0.00052851
Iteration 50/1000 | Loss: 0.00046904
Iteration 51/1000 | Loss: 0.00003732
Iteration 52/1000 | Loss: 0.00002804
Iteration 53/1000 | Loss: 0.00002363
Iteration 54/1000 | Loss: 0.00002420
Iteration 55/1000 | Loss: 0.00007783
Iteration 56/1000 | Loss: 0.00002963
Iteration 57/1000 | Loss: 0.00002275
Iteration 58/1000 | Loss: 0.00001882
Iteration 59/1000 | Loss: 0.00003216
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00001850
Iteration 63/1000 | Loss: 0.00001796
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00002015
Iteration 66/1000 | Loss: 0.00001767
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001708
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001708
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001707
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001706
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001703
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001698
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00002193
Iteration 113/1000 | Loss: 0.00002193
Iteration 114/1000 | Loss: 0.00018575
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001714
Iteration 117/1000 | Loss: 0.00001692
Iteration 118/1000 | Loss: 0.00001692
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001691
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001703
Iteration 127/1000 | Loss: 0.00001691
Iteration 128/1000 | Loss: 0.00001691
Iteration 129/1000 | Loss: 0.00001691
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001690
Iteration 132/1000 | Loss: 0.00001690
Iteration 133/1000 | Loss: 0.00001690
Iteration 134/1000 | Loss: 0.00001690
Iteration 135/1000 | Loss: 0.00001690
Iteration 136/1000 | Loss: 0.00001690
Iteration 137/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.69021532201441e-05, 1.69021532201441e-05, 1.69021532201441e-05, 1.69021532201441e-05, 1.69021532201441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.69021532201441e-05

Optimization complete. Final v2v error: 3.522061824798584 mm

Highest mean error: 4.947162628173828 mm for frame 67

Lowest mean error: 3.089820146560669 mm for frame 106

Saving results

Total time: 152.72768783569336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411866
Iteration 2/25 | Loss: 0.00150303
Iteration 3/25 | Loss: 0.00144931
Iteration 4/25 | Loss: 0.00144143
Iteration 5/25 | Loss: 0.00143869
Iteration 6/25 | Loss: 0.00143862
Iteration 7/25 | Loss: 0.00143862
Iteration 8/25 | Loss: 0.00143862
Iteration 9/25 | Loss: 0.00143862
Iteration 10/25 | Loss: 0.00143862
Iteration 11/25 | Loss: 0.00143862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014386194525286555, 0.0014386194525286555, 0.0014386194525286555, 0.0014386194525286555, 0.0014386194525286555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014386194525286555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75951028
Iteration 2/25 | Loss: 0.00228786
Iteration 3/25 | Loss: 0.00228785
Iteration 4/25 | Loss: 0.00228785
Iteration 5/25 | Loss: 0.00228785
Iteration 6/25 | Loss: 0.00228784
Iteration 7/25 | Loss: 0.00228784
Iteration 8/25 | Loss: 0.00228784
Iteration 9/25 | Loss: 0.00228784
Iteration 10/25 | Loss: 0.00228784
Iteration 11/25 | Loss: 0.00228784
Iteration 12/25 | Loss: 0.00228784
Iteration 13/25 | Loss: 0.00228784
Iteration 14/25 | Loss: 0.00228784
Iteration 15/25 | Loss: 0.00228784
Iteration 16/25 | Loss: 0.00228784
Iteration 17/25 | Loss: 0.00228784
Iteration 18/25 | Loss: 0.00228784
Iteration 19/25 | Loss: 0.00228784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0022878421004861593, 0.0022878421004861593, 0.0022878421004861593, 0.0022878421004861593, 0.0022878421004861593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022878421004861593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228784
Iteration 2/1000 | Loss: 0.00002467
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001549
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001451
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001338
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001316
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00001290
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001280
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001275
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001266
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001260
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001250
Iteration 93/1000 | Loss: 0.00001250
Iteration 94/1000 | Loss: 0.00001250
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001249
Iteration 97/1000 | Loss: 0.00001249
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001245
Iteration 106/1000 | Loss: 0.00001245
Iteration 107/1000 | Loss: 0.00001245
Iteration 108/1000 | Loss: 0.00001244
Iteration 109/1000 | Loss: 0.00001244
Iteration 110/1000 | Loss: 0.00001244
Iteration 111/1000 | Loss: 0.00001243
Iteration 112/1000 | Loss: 0.00001243
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001242
Iteration 115/1000 | Loss: 0.00001242
Iteration 116/1000 | Loss: 0.00001242
Iteration 117/1000 | Loss: 0.00001242
Iteration 118/1000 | Loss: 0.00001242
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001240
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001239
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Iteration 148/1000 | Loss: 0.00001239
Iteration 149/1000 | Loss: 0.00001239
Iteration 150/1000 | Loss: 0.00001239
Iteration 151/1000 | Loss: 0.00001239
Iteration 152/1000 | Loss: 0.00001239
Iteration 153/1000 | Loss: 0.00001239
Iteration 154/1000 | Loss: 0.00001239
Iteration 155/1000 | Loss: 0.00001239
Iteration 156/1000 | Loss: 0.00001239
Iteration 157/1000 | Loss: 0.00001239
Iteration 158/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.2390242773108184e-05, 1.2390242773108184e-05, 1.2390242773108184e-05, 1.2390242773108184e-05, 1.2390242773108184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2390242773108184e-05

Optimization complete. Final v2v error: 3.071251153945923 mm

Highest mean error: 3.2709367275238037 mm for frame 120

Lowest mean error: 2.992530345916748 mm for frame 2

Saving results

Total time: 42.18556499481201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813707
Iteration 2/25 | Loss: 0.00156393
Iteration 3/25 | Loss: 0.00148307
Iteration 4/25 | Loss: 0.00147209
Iteration 5/25 | Loss: 0.00146856
Iteration 6/25 | Loss: 0.00146829
Iteration 7/25 | Loss: 0.00146829
Iteration 8/25 | Loss: 0.00146829
Iteration 9/25 | Loss: 0.00146829
Iteration 10/25 | Loss: 0.00146829
Iteration 11/25 | Loss: 0.00146829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001468291855417192, 0.001468291855417192, 0.001468291855417192, 0.001468291855417192, 0.001468291855417192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001468291855417192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92557156
Iteration 2/25 | Loss: 0.00248742
Iteration 3/25 | Loss: 0.00248742
Iteration 4/25 | Loss: 0.00248742
Iteration 5/25 | Loss: 0.00248742
Iteration 6/25 | Loss: 0.00248742
Iteration 7/25 | Loss: 0.00248742
Iteration 8/25 | Loss: 0.00248742
Iteration 9/25 | Loss: 0.00248742
Iteration 10/25 | Loss: 0.00248742
Iteration 11/25 | Loss: 0.00248742
Iteration 12/25 | Loss: 0.00248742
Iteration 13/25 | Loss: 0.00248742
Iteration 14/25 | Loss: 0.00248742
Iteration 15/25 | Loss: 0.00248742
Iteration 16/25 | Loss: 0.00248742
Iteration 17/25 | Loss: 0.00248742
Iteration 18/25 | Loss: 0.00248742
Iteration 19/25 | Loss: 0.00248742
Iteration 20/25 | Loss: 0.00248742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002487416611984372, 0.002487416611984372, 0.002487416611984372, 0.002487416611984372, 0.002487416611984372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002487416611984372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248742
Iteration 2/1000 | Loss: 0.00002818
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001844
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001516
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001430
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001342
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001329
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001328
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001318
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001317
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001306
Iteration 50/1000 | Loss: 0.00001306
Iteration 51/1000 | Loss: 0.00001305
Iteration 52/1000 | Loss: 0.00001305
Iteration 53/1000 | Loss: 0.00001305
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001304
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001304
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001303
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001299
Iteration 79/1000 | Loss: 0.00001299
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001298
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001293
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001293
Iteration 120/1000 | Loss: 0.00001293
Iteration 121/1000 | Loss: 0.00001293
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001292
Iteration 124/1000 | Loss: 0.00001292
Iteration 125/1000 | Loss: 0.00001292
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.2913219507026952e-05, 1.2913219507026952e-05, 1.2913219507026952e-05, 1.2913219507026952e-05, 1.2913219507026952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2913219507026952e-05

Optimization complete. Final v2v error: 3.076724052429199 mm

Highest mean error: 3.7723615169525146 mm for frame 91

Lowest mean error: 2.8531510829925537 mm for frame 147

Saving results

Total time: 42.065786361694336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511415
Iteration 2/25 | Loss: 0.00169555
Iteration 3/25 | Loss: 0.00154124
Iteration 4/25 | Loss: 0.00152954
Iteration 5/25 | Loss: 0.00152628
Iteration 6/25 | Loss: 0.00152532
Iteration 7/25 | Loss: 0.00152532
Iteration 8/25 | Loss: 0.00152532
Iteration 9/25 | Loss: 0.00152532
Iteration 10/25 | Loss: 0.00152532
Iteration 11/25 | Loss: 0.00152532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015253208111971617, 0.0015253208111971617, 0.0015253208111971617, 0.0015253208111971617, 0.0015253208111971617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015253208111971617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10906005
Iteration 2/25 | Loss: 0.00213715
Iteration 3/25 | Loss: 0.00213712
Iteration 4/25 | Loss: 0.00213712
Iteration 5/25 | Loss: 0.00213712
Iteration 6/25 | Loss: 0.00213712
Iteration 7/25 | Loss: 0.00213712
Iteration 8/25 | Loss: 0.00213712
Iteration 9/25 | Loss: 0.00213712
Iteration 10/25 | Loss: 0.00213712
Iteration 11/25 | Loss: 0.00213712
Iteration 12/25 | Loss: 0.00213712
Iteration 13/25 | Loss: 0.00213711
Iteration 14/25 | Loss: 0.00213711
Iteration 15/25 | Loss: 0.00213711
Iteration 16/25 | Loss: 0.00213711
Iteration 17/25 | Loss: 0.00213711
Iteration 18/25 | Loss: 0.00213711
Iteration 19/25 | Loss: 0.00213711
Iteration 20/25 | Loss: 0.00213711
Iteration 21/25 | Loss: 0.00213711
Iteration 22/25 | Loss: 0.00213711
Iteration 23/25 | Loss: 0.00213711
Iteration 24/25 | Loss: 0.00213711
Iteration 25/25 | Loss: 0.00213711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213711
Iteration 2/1000 | Loss: 0.00006026
Iteration 3/1000 | Loss: 0.00003819
Iteration 4/1000 | Loss: 0.00003084
Iteration 5/1000 | Loss: 0.00002741
Iteration 6/1000 | Loss: 0.00002585
Iteration 7/1000 | Loss: 0.00002491
Iteration 8/1000 | Loss: 0.00002419
Iteration 9/1000 | Loss: 0.00002363
Iteration 10/1000 | Loss: 0.00002323
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002233
Iteration 13/1000 | Loss: 0.00002199
Iteration 14/1000 | Loss: 0.00002168
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002116
Iteration 17/1000 | Loss: 0.00002094
Iteration 18/1000 | Loss: 0.00002077
Iteration 19/1000 | Loss: 0.00002069
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002062
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002053
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002051
Iteration 29/1000 | Loss: 0.00002051
Iteration 30/1000 | Loss: 0.00002051
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00002051
Iteration 33/1000 | Loss: 0.00002051
Iteration 34/1000 | Loss: 0.00002049
Iteration 35/1000 | Loss: 0.00002047
Iteration 36/1000 | Loss: 0.00002046
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002044
Iteration 41/1000 | Loss: 0.00002044
Iteration 42/1000 | Loss: 0.00002044
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002044
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002041
Iteration 50/1000 | Loss: 0.00002041
Iteration 51/1000 | Loss: 0.00002041
Iteration 52/1000 | Loss: 0.00002040
Iteration 53/1000 | Loss: 0.00002040
Iteration 54/1000 | Loss: 0.00002040
Iteration 55/1000 | Loss: 0.00002039
Iteration 56/1000 | Loss: 0.00002039
Iteration 57/1000 | Loss: 0.00002038
Iteration 58/1000 | Loss: 0.00002038
Iteration 59/1000 | Loss: 0.00002038
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002037
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002034
Iteration 71/1000 | Loss: 0.00002033
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002032
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002030
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002030
Iteration 82/1000 | Loss: 0.00002030
Iteration 83/1000 | Loss: 0.00002029
Iteration 84/1000 | Loss: 0.00002029
Iteration 85/1000 | Loss: 0.00002029
Iteration 86/1000 | Loss: 0.00002029
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00002029
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002029
Iteration 102/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.028561539191287e-05, 2.028561539191287e-05, 2.028561539191287e-05, 2.028561539191287e-05, 2.028561539191287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.028561539191287e-05

Optimization complete. Final v2v error: 3.659821033477783 mm

Highest mean error: 5.581076622009277 mm for frame 81

Lowest mean error: 2.955479145050049 mm for frame 131

Saving results

Total time: 44.48327612876892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676134
Iteration 2/25 | Loss: 0.00191408
Iteration 3/25 | Loss: 0.00169196
Iteration 4/25 | Loss: 0.00163401
Iteration 5/25 | Loss: 0.00159005
Iteration 6/25 | Loss: 0.00159687
Iteration 7/25 | Loss: 0.00160340
Iteration 8/25 | Loss: 0.00158985
Iteration 9/25 | Loss: 0.00158536
Iteration 10/25 | Loss: 0.00158290
Iteration 11/25 | Loss: 0.00158127
Iteration 12/25 | Loss: 0.00158707
Iteration 13/25 | Loss: 0.00158283
Iteration 14/25 | Loss: 0.00157642
Iteration 15/25 | Loss: 0.00157404
Iteration 16/25 | Loss: 0.00157341
Iteration 17/25 | Loss: 0.00157335
Iteration 18/25 | Loss: 0.00157335
Iteration 19/25 | Loss: 0.00157335
Iteration 20/25 | Loss: 0.00157335
Iteration 21/25 | Loss: 0.00157334
Iteration 22/25 | Loss: 0.00157334
Iteration 23/25 | Loss: 0.00157334
Iteration 24/25 | Loss: 0.00157334
Iteration 25/25 | Loss: 0.00157334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14026523
Iteration 2/25 | Loss: 0.00251674
Iteration 3/25 | Loss: 0.00232544
Iteration 4/25 | Loss: 0.00232544
Iteration 5/25 | Loss: 0.00232544
Iteration 6/25 | Loss: 0.00232544
Iteration 7/25 | Loss: 0.00232544
Iteration 8/25 | Loss: 0.00232544
Iteration 9/25 | Loss: 0.00232544
Iteration 10/25 | Loss: 0.00232544
Iteration 11/25 | Loss: 0.00232544
Iteration 12/25 | Loss: 0.00232544
Iteration 13/25 | Loss: 0.00232544
Iteration 14/25 | Loss: 0.00232544
Iteration 15/25 | Loss: 0.00232544
Iteration 16/25 | Loss: 0.00232544
Iteration 17/25 | Loss: 0.00232544
Iteration 18/25 | Loss: 0.00232544
Iteration 19/25 | Loss: 0.00232544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023254393599927425, 0.0023254393599927425, 0.0023254393599927425, 0.0023254393599927425, 0.0023254393599927425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023254393599927425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232544
Iteration 2/1000 | Loss: 0.00016530
Iteration 3/1000 | Loss: 0.00003340
Iteration 4/1000 | Loss: 0.00008289
Iteration 5/1000 | Loss: 0.00005448
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002344
Iteration 8/1000 | Loss: 0.00002243
Iteration 9/1000 | Loss: 0.00002187
Iteration 10/1000 | Loss: 0.00002116
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00020261
Iteration 13/1000 | Loss: 0.00015720
Iteration 14/1000 | Loss: 0.00002034
Iteration 15/1000 | Loss: 0.00001995
Iteration 16/1000 | Loss: 0.00022362
Iteration 17/1000 | Loss: 0.00013217
Iteration 18/1000 | Loss: 0.00002252
Iteration 19/1000 | Loss: 0.00021963
Iteration 20/1000 | Loss: 0.00014602
Iteration 21/1000 | Loss: 0.00041398
Iteration 22/1000 | Loss: 0.00010088
Iteration 23/1000 | Loss: 0.00010043
Iteration 24/1000 | Loss: 0.00009356
Iteration 25/1000 | Loss: 0.00002399
Iteration 26/1000 | Loss: 0.00002228
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00001979
Iteration 30/1000 | Loss: 0.00001942
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001834
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001833
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001832
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001826
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001821
Iteration 62/1000 | Loss: 0.00001821
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001820
Iteration 65/1000 | Loss: 0.00001820
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001819
Iteration 68/1000 | Loss: 0.00001819
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001816
Iteration 80/1000 | Loss: 0.00001816
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001816
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001815
Iteration 91/1000 | Loss: 0.00001815
Iteration 92/1000 | Loss: 0.00001815
Iteration 93/1000 | Loss: 0.00001815
Iteration 94/1000 | Loss: 0.00001815
Iteration 95/1000 | Loss: 0.00001814
Iteration 96/1000 | Loss: 0.00001814
Iteration 97/1000 | Loss: 0.00001814
Iteration 98/1000 | Loss: 0.00001814
Iteration 99/1000 | Loss: 0.00001814
Iteration 100/1000 | Loss: 0.00001814
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Iteration 109/1000 | Loss: 0.00001813
Iteration 110/1000 | Loss: 0.00001812
Iteration 111/1000 | Loss: 0.00001812
Iteration 112/1000 | Loss: 0.00001812
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001811
Iteration 116/1000 | Loss: 0.00001811
Iteration 117/1000 | Loss: 0.00001811
Iteration 118/1000 | Loss: 0.00001810
Iteration 119/1000 | Loss: 0.00001810
Iteration 120/1000 | Loss: 0.00001810
Iteration 121/1000 | Loss: 0.00001809
Iteration 122/1000 | Loss: 0.00001809
Iteration 123/1000 | Loss: 0.00001809
Iteration 124/1000 | Loss: 0.00001809
Iteration 125/1000 | Loss: 0.00001808
Iteration 126/1000 | Loss: 0.00001808
Iteration 127/1000 | Loss: 0.00001808
Iteration 128/1000 | Loss: 0.00001808
Iteration 129/1000 | Loss: 0.00001808
Iteration 130/1000 | Loss: 0.00001808
Iteration 131/1000 | Loss: 0.00001808
Iteration 132/1000 | Loss: 0.00001808
Iteration 133/1000 | Loss: 0.00001808
Iteration 134/1000 | Loss: 0.00001808
Iteration 135/1000 | Loss: 0.00001808
Iteration 136/1000 | Loss: 0.00001807
Iteration 137/1000 | Loss: 0.00001807
Iteration 138/1000 | Loss: 0.00001807
Iteration 139/1000 | Loss: 0.00001807
Iteration 140/1000 | Loss: 0.00001807
Iteration 141/1000 | Loss: 0.00001807
Iteration 142/1000 | Loss: 0.00001807
Iteration 143/1000 | Loss: 0.00001807
Iteration 144/1000 | Loss: 0.00001807
Iteration 145/1000 | Loss: 0.00001807
Iteration 146/1000 | Loss: 0.00001807
Iteration 147/1000 | Loss: 0.00001807
Iteration 148/1000 | Loss: 0.00001807
Iteration 149/1000 | Loss: 0.00001807
Iteration 150/1000 | Loss: 0.00001807
Iteration 151/1000 | Loss: 0.00001807
Iteration 152/1000 | Loss: 0.00001807
Iteration 153/1000 | Loss: 0.00001807
Iteration 154/1000 | Loss: 0.00001807
Iteration 155/1000 | Loss: 0.00001807
Iteration 156/1000 | Loss: 0.00001807
Iteration 157/1000 | Loss: 0.00001807
Iteration 158/1000 | Loss: 0.00001807
Iteration 159/1000 | Loss: 0.00001807
Iteration 160/1000 | Loss: 0.00001807
Iteration 161/1000 | Loss: 0.00001807
Iteration 162/1000 | Loss: 0.00001807
Iteration 163/1000 | Loss: 0.00001807
Iteration 164/1000 | Loss: 0.00001807
Iteration 165/1000 | Loss: 0.00001807
Iteration 166/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.807375520002097e-05, 1.807375520002097e-05, 1.807375520002097e-05, 1.807375520002097e-05, 1.807375520002097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.807375520002097e-05

Optimization complete. Final v2v error: 3.573918581008911 mm

Highest mean error: 4.441928863525391 mm for frame 17

Lowest mean error: 3.181544303894043 mm for frame 133

Saving results

Total time: 88.04180264472961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955878
Iteration 2/25 | Loss: 0.00260247
Iteration 3/25 | Loss: 0.00240282
Iteration 4/25 | Loss: 0.00219745
Iteration 5/25 | Loss: 0.00191464
Iteration 6/25 | Loss: 0.00207115
Iteration 7/25 | Loss: 0.00217454
Iteration 8/25 | Loss: 0.00188827
Iteration 9/25 | Loss: 0.00172939
Iteration 10/25 | Loss: 0.00161066
Iteration 11/25 | Loss: 0.00160441
Iteration 12/25 | Loss: 0.00159200
Iteration 13/25 | Loss: 0.00158386
Iteration 14/25 | Loss: 0.00158357
Iteration 15/25 | Loss: 0.00158357
Iteration 16/25 | Loss: 0.00158214
Iteration 17/25 | Loss: 0.00158333
Iteration 18/25 | Loss: 0.00158492
Iteration 19/25 | Loss: 0.00158329
Iteration 20/25 | Loss: 0.00158262
Iteration 21/25 | Loss: 0.00158329
Iteration 22/25 | Loss: 0.00158400
Iteration 23/25 | Loss: 0.00158197
Iteration 24/25 | Loss: 0.00158457
Iteration 25/25 | Loss: 0.00158395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22196889
Iteration 2/25 | Loss: 0.00227775
Iteration 3/25 | Loss: 0.00227775
Iteration 4/25 | Loss: 0.00227775
Iteration 5/25 | Loss: 0.00227775
Iteration 6/25 | Loss: 0.00227775
Iteration 7/25 | Loss: 0.00227775
Iteration 8/25 | Loss: 0.00227775
Iteration 9/25 | Loss: 0.00227775
Iteration 10/25 | Loss: 0.00227775
Iteration 11/25 | Loss: 0.00227775
Iteration 12/25 | Loss: 0.00227775
Iteration 13/25 | Loss: 0.00227775
Iteration 14/25 | Loss: 0.00227775
Iteration 15/25 | Loss: 0.00227775
Iteration 16/25 | Loss: 0.00227775
Iteration 17/25 | Loss: 0.00227775
Iteration 18/25 | Loss: 0.00227775
Iteration 19/25 | Loss: 0.00227775
Iteration 20/25 | Loss: 0.00227775
Iteration 21/25 | Loss: 0.00227775
Iteration 22/25 | Loss: 0.00227775
Iteration 23/25 | Loss: 0.00227775
Iteration 24/25 | Loss: 0.00227775
Iteration 25/25 | Loss: 0.00227775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227775
Iteration 2/1000 | Loss: 0.00008877
Iteration 3/1000 | Loss: 0.00006723
Iteration 4/1000 | Loss: 0.00005891
Iteration 5/1000 | Loss: 0.00003837
Iteration 6/1000 | Loss: 0.00005674
Iteration 7/1000 | Loss: 0.00008444
Iteration 8/1000 | Loss: 0.00003782
Iteration 9/1000 | Loss: 0.00003330
Iteration 10/1000 | Loss: 0.00005119
Iteration 11/1000 | Loss: 0.00004803
Iteration 12/1000 | Loss: 0.00003200
Iteration 13/1000 | Loss: 0.00005489
Iteration 14/1000 | Loss: 0.00005529
Iteration 15/1000 | Loss: 0.00005330
Iteration 16/1000 | Loss: 0.00004797
Iteration 17/1000 | Loss: 0.00005096
Iteration 18/1000 | Loss: 0.00004987
Iteration 19/1000 | Loss: 0.00004966
Iteration 20/1000 | Loss: 0.00004956
Iteration 21/1000 | Loss: 0.00004021
Iteration 22/1000 | Loss: 0.00004080
Iteration 23/1000 | Loss: 0.00004726
Iteration 24/1000 | Loss: 0.00003685
Iteration 25/1000 | Loss: 0.00003258
Iteration 26/1000 | Loss: 0.00004371
Iteration 27/1000 | Loss: 0.00004898
Iteration 28/1000 | Loss: 0.00006248
Iteration 29/1000 | Loss: 0.00003482
Iteration 30/1000 | Loss: 0.00003030
Iteration 31/1000 | Loss: 0.00002943
Iteration 32/1000 | Loss: 0.00002864
Iteration 33/1000 | Loss: 0.00002821
Iteration 34/1000 | Loss: 0.00002786
Iteration 35/1000 | Loss: 0.00002755
Iteration 36/1000 | Loss: 0.00002727
Iteration 37/1000 | Loss: 0.00002702
Iteration 38/1000 | Loss: 0.00002678
Iteration 39/1000 | Loss: 0.00002671
Iteration 40/1000 | Loss: 0.00002671
Iteration 41/1000 | Loss: 0.00002669
Iteration 42/1000 | Loss: 0.00002669
Iteration 43/1000 | Loss: 0.00002668
Iteration 44/1000 | Loss: 0.00002664
Iteration 45/1000 | Loss: 0.00002662
Iteration 46/1000 | Loss: 0.00002662
Iteration 47/1000 | Loss: 0.00002662
Iteration 48/1000 | Loss: 0.00002660
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002658
Iteration 51/1000 | Loss: 0.00002657
Iteration 52/1000 | Loss: 0.00002657
Iteration 53/1000 | Loss: 0.00002657
Iteration 54/1000 | Loss: 0.00002657
Iteration 55/1000 | Loss: 0.00002657
Iteration 56/1000 | Loss: 0.00002657
Iteration 57/1000 | Loss: 0.00002657
Iteration 58/1000 | Loss: 0.00002657
Iteration 59/1000 | Loss: 0.00002657
Iteration 60/1000 | Loss: 0.00002657
Iteration 61/1000 | Loss: 0.00002657
Iteration 62/1000 | Loss: 0.00002657
Iteration 63/1000 | Loss: 0.00002656
Iteration 64/1000 | Loss: 0.00002655
Iteration 65/1000 | Loss: 0.00002654
Iteration 66/1000 | Loss: 0.00002654
Iteration 67/1000 | Loss: 0.00002654
Iteration 68/1000 | Loss: 0.00002654
Iteration 69/1000 | Loss: 0.00002654
Iteration 70/1000 | Loss: 0.00002654
Iteration 71/1000 | Loss: 0.00002654
Iteration 72/1000 | Loss: 0.00002654
Iteration 73/1000 | Loss: 0.00002653
Iteration 74/1000 | Loss: 0.00002653
Iteration 75/1000 | Loss: 0.00002653
Iteration 76/1000 | Loss: 0.00002653
Iteration 77/1000 | Loss: 0.00002653
Iteration 78/1000 | Loss: 0.00002653
Iteration 79/1000 | Loss: 0.00002653
Iteration 80/1000 | Loss: 0.00002653
Iteration 81/1000 | Loss: 0.00002653
Iteration 82/1000 | Loss: 0.00002653
Iteration 83/1000 | Loss: 0.00002653
Iteration 84/1000 | Loss: 0.00002653
Iteration 85/1000 | Loss: 0.00002653
Iteration 86/1000 | Loss: 0.00002653
Iteration 87/1000 | Loss: 0.00002653
Iteration 88/1000 | Loss: 0.00002653
Iteration 89/1000 | Loss: 0.00002653
Iteration 90/1000 | Loss: 0.00002653
Iteration 91/1000 | Loss: 0.00002653
Iteration 92/1000 | Loss: 0.00002653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.6531894036452286e-05, 2.6531894036452286e-05, 2.6531894036452286e-05, 2.6531894036452286e-05, 2.6531894036452286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6531894036452286e-05

Optimization complete. Final v2v error: 4.4539475440979 mm

Highest mean error: 5.1157402992248535 mm for frame 46

Lowest mean error: 4.113373279571533 mm for frame 0

Saving results

Total time: 98.79178237915039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002382
Iteration 2/25 | Loss: 0.00208437
Iteration 3/25 | Loss: 0.00173235
Iteration 4/25 | Loss: 0.00172103
Iteration 5/25 | Loss: 0.00171812
Iteration 6/25 | Loss: 0.00171777
Iteration 7/25 | Loss: 0.00171777
Iteration 8/25 | Loss: 0.00171777
Iteration 9/25 | Loss: 0.00171777
Iteration 10/25 | Loss: 0.00171777
Iteration 11/25 | Loss: 0.00171777
Iteration 12/25 | Loss: 0.00171777
Iteration 13/25 | Loss: 0.00171777
Iteration 14/25 | Loss: 0.00171777
Iteration 15/25 | Loss: 0.00171777
Iteration 16/25 | Loss: 0.00171777
Iteration 17/25 | Loss: 0.00171777
Iteration 18/25 | Loss: 0.00171777
Iteration 19/25 | Loss: 0.00171777
Iteration 20/25 | Loss: 0.00171777
Iteration 21/25 | Loss: 0.00171777
Iteration 22/25 | Loss: 0.00171777
Iteration 23/25 | Loss: 0.00171777
Iteration 24/25 | Loss: 0.00171777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017177686095237732, 0.0017177686095237732, 0.0017177686095237732, 0.0017177686095237732, 0.0017177686095237732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017177686095237732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63259602
Iteration 2/25 | Loss: 0.00184002
Iteration 3/25 | Loss: 0.00184002
Iteration 4/25 | Loss: 0.00184002
Iteration 5/25 | Loss: 0.00184002
Iteration 6/25 | Loss: 0.00184002
Iteration 7/25 | Loss: 0.00184002
Iteration 8/25 | Loss: 0.00184002
Iteration 9/25 | Loss: 0.00184002
Iteration 10/25 | Loss: 0.00184002
Iteration 11/25 | Loss: 0.00184002
Iteration 12/25 | Loss: 0.00184002
Iteration 13/25 | Loss: 0.00184002
Iteration 14/25 | Loss: 0.00184002
Iteration 15/25 | Loss: 0.00184002
Iteration 16/25 | Loss: 0.00184002
Iteration 17/25 | Loss: 0.00184002
Iteration 18/25 | Loss: 0.00184002
Iteration 19/25 | Loss: 0.00184002
Iteration 20/25 | Loss: 0.00184002
Iteration 21/25 | Loss: 0.00184002
Iteration 22/25 | Loss: 0.00184002
Iteration 23/25 | Loss: 0.00184002
Iteration 24/25 | Loss: 0.00184002
Iteration 25/25 | Loss: 0.00184002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184002
Iteration 2/1000 | Loss: 0.00012675
Iteration 3/1000 | Loss: 0.00006915
Iteration 4/1000 | Loss: 0.00005574
Iteration 5/1000 | Loss: 0.00005215
Iteration 6/1000 | Loss: 0.00005035
Iteration 7/1000 | Loss: 0.00004921
Iteration 8/1000 | Loss: 0.00004865
Iteration 9/1000 | Loss: 0.00004808
Iteration 10/1000 | Loss: 0.00004740
Iteration 11/1000 | Loss: 0.00004700
Iteration 12/1000 | Loss: 0.00004666
Iteration 13/1000 | Loss: 0.00004633
Iteration 14/1000 | Loss: 0.00004606
Iteration 15/1000 | Loss: 0.00004588
Iteration 16/1000 | Loss: 0.00004567
Iteration 17/1000 | Loss: 0.00004547
Iteration 18/1000 | Loss: 0.00004538
Iteration 19/1000 | Loss: 0.00004532
Iteration 20/1000 | Loss: 0.00004526
Iteration 21/1000 | Loss: 0.00004517
Iteration 22/1000 | Loss: 0.00004509
Iteration 23/1000 | Loss: 0.00004503
Iteration 24/1000 | Loss: 0.00004503
Iteration 25/1000 | Loss: 0.00004499
Iteration 26/1000 | Loss: 0.00004499
Iteration 27/1000 | Loss: 0.00004499
Iteration 28/1000 | Loss: 0.00004499
Iteration 29/1000 | Loss: 0.00004499
Iteration 30/1000 | Loss: 0.00004499
Iteration 31/1000 | Loss: 0.00004499
Iteration 32/1000 | Loss: 0.00004498
Iteration 33/1000 | Loss: 0.00004498
Iteration 34/1000 | Loss: 0.00004498
Iteration 35/1000 | Loss: 0.00004498
Iteration 36/1000 | Loss: 0.00004497
Iteration 37/1000 | Loss: 0.00004497
Iteration 38/1000 | Loss: 0.00004497
Iteration 39/1000 | Loss: 0.00004497
Iteration 40/1000 | Loss: 0.00004497
Iteration 41/1000 | Loss: 0.00004496
Iteration 42/1000 | Loss: 0.00004496
Iteration 43/1000 | Loss: 0.00004496
Iteration 44/1000 | Loss: 0.00004495
Iteration 45/1000 | Loss: 0.00004495
Iteration 46/1000 | Loss: 0.00004495
Iteration 47/1000 | Loss: 0.00004494
Iteration 48/1000 | Loss: 0.00004494
Iteration 49/1000 | Loss: 0.00004493
Iteration 50/1000 | Loss: 0.00004492
Iteration 51/1000 | Loss: 0.00004491
Iteration 52/1000 | Loss: 0.00004491
Iteration 53/1000 | Loss: 0.00004491
Iteration 54/1000 | Loss: 0.00004491
Iteration 55/1000 | Loss: 0.00004491
Iteration 56/1000 | Loss: 0.00004491
Iteration 57/1000 | Loss: 0.00004491
Iteration 58/1000 | Loss: 0.00004491
Iteration 59/1000 | Loss: 0.00004491
Iteration 60/1000 | Loss: 0.00004490
Iteration 61/1000 | Loss: 0.00004490
Iteration 62/1000 | Loss: 0.00004490
Iteration 63/1000 | Loss: 0.00004490
Iteration 64/1000 | Loss: 0.00004489
Iteration 65/1000 | Loss: 0.00004489
Iteration 66/1000 | Loss: 0.00004489
Iteration 67/1000 | Loss: 0.00004489
Iteration 68/1000 | Loss: 0.00004489
Iteration 69/1000 | Loss: 0.00004489
Iteration 70/1000 | Loss: 0.00004489
Iteration 71/1000 | Loss: 0.00004488
Iteration 72/1000 | Loss: 0.00004488
Iteration 73/1000 | Loss: 0.00004488
Iteration 74/1000 | Loss: 0.00004488
Iteration 75/1000 | Loss: 0.00004487
Iteration 76/1000 | Loss: 0.00004487
Iteration 77/1000 | Loss: 0.00004487
Iteration 78/1000 | Loss: 0.00004486
Iteration 79/1000 | Loss: 0.00004486
Iteration 80/1000 | Loss: 0.00004486
Iteration 81/1000 | Loss: 0.00004486
Iteration 82/1000 | Loss: 0.00004486
Iteration 83/1000 | Loss: 0.00004486
Iteration 84/1000 | Loss: 0.00004486
Iteration 85/1000 | Loss: 0.00004486
Iteration 86/1000 | Loss: 0.00004486
Iteration 87/1000 | Loss: 0.00004485
Iteration 88/1000 | Loss: 0.00004485
Iteration 89/1000 | Loss: 0.00004485
Iteration 90/1000 | Loss: 0.00004485
Iteration 91/1000 | Loss: 0.00004485
Iteration 92/1000 | Loss: 0.00004484
Iteration 93/1000 | Loss: 0.00004484
Iteration 94/1000 | Loss: 0.00004484
Iteration 95/1000 | Loss: 0.00004483
Iteration 96/1000 | Loss: 0.00004483
Iteration 97/1000 | Loss: 0.00004483
Iteration 98/1000 | Loss: 0.00004483
Iteration 99/1000 | Loss: 0.00004482
Iteration 100/1000 | Loss: 0.00004482
Iteration 101/1000 | Loss: 0.00004482
Iteration 102/1000 | Loss: 0.00004482
Iteration 103/1000 | Loss: 0.00004482
Iteration 104/1000 | Loss: 0.00004482
Iteration 105/1000 | Loss: 0.00004482
Iteration 106/1000 | Loss: 0.00004481
Iteration 107/1000 | Loss: 0.00004481
Iteration 108/1000 | Loss: 0.00004481
Iteration 109/1000 | Loss: 0.00004481
Iteration 110/1000 | Loss: 0.00004481
Iteration 111/1000 | Loss: 0.00004481
Iteration 112/1000 | Loss: 0.00004480
Iteration 113/1000 | Loss: 0.00004480
Iteration 114/1000 | Loss: 0.00004480
Iteration 115/1000 | Loss: 0.00004480
Iteration 116/1000 | Loss: 0.00004480
Iteration 117/1000 | Loss: 0.00004480
Iteration 118/1000 | Loss: 0.00004480
Iteration 119/1000 | Loss: 0.00004480
Iteration 120/1000 | Loss: 0.00004480
Iteration 121/1000 | Loss: 0.00004480
Iteration 122/1000 | Loss: 0.00004480
Iteration 123/1000 | Loss: 0.00004480
Iteration 124/1000 | Loss: 0.00004479
Iteration 125/1000 | Loss: 0.00004479
Iteration 126/1000 | Loss: 0.00004479
Iteration 127/1000 | Loss: 0.00004479
Iteration 128/1000 | Loss: 0.00004479
Iteration 129/1000 | Loss: 0.00004479
Iteration 130/1000 | Loss: 0.00004479
Iteration 131/1000 | Loss: 0.00004479
Iteration 132/1000 | Loss: 0.00004479
Iteration 133/1000 | Loss: 0.00004479
Iteration 134/1000 | Loss: 0.00004479
Iteration 135/1000 | Loss: 0.00004479
Iteration 136/1000 | Loss: 0.00004478
Iteration 137/1000 | Loss: 0.00004478
Iteration 138/1000 | Loss: 0.00004478
Iteration 139/1000 | Loss: 0.00004478
Iteration 140/1000 | Loss: 0.00004478
Iteration 141/1000 | Loss: 0.00004478
Iteration 142/1000 | Loss: 0.00004478
Iteration 143/1000 | Loss: 0.00004478
Iteration 144/1000 | Loss: 0.00004477
Iteration 145/1000 | Loss: 0.00004477
Iteration 146/1000 | Loss: 0.00004477
Iteration 147/1000 | Loss: 0.00004477
Iteration 148/1000 | Loss: 0.00004477
Iteration 149/1000 | Loss: 0.00004477
Iteration 150/1000 | Loss: 0.00004477
Iteration 151/1000 | Loss: 0.00004476
Iteration 152/1000 | Loss: 0.00004476
Iteration 153/1000 | Loss: 0.00004476
Iteration 154/1000 | Loss: 0.00004476
Iteration 155/1000 | Loss: 0.00004476
Iteration 156/1000 | Loss: 0.00004476
Iteration 157/1000 | Loss: 0.00004476
Iteration 158/1000 | Loss: 0.00004476
Iteration 159/1000 | Loss: 0.00004476
Iteration 160/1000 | Loss: 0.00004476
Iteration 161/1000 | Loss: 0.00004475
Iteration 162/1000 | Loss: 0.00004475
Iteration 163/1000 | Loss: 0.00004475
Iteration 164/1000 | Loss: 0.00004475
Iteration 165/1000 | Loss: 0.00004475
Iteration 166/1000 | Loss: 0.00004475
Iteration 167/1000 | Loss: 0.00004475
Iteration 168/1000 | Loss: 0.00004474
Iteration 169/1000 | Loss: 0.00004474
Iteration 170/1000 | Loss: 0.00004474
Iteration 171/1000 | Loss: 0.00004474
Iteration 172/1000 | Loss: 0.00004474
Iteration 173/1000 | Loss: 0.00004474
Iteration 174/1000 | Loss: 0.00004474
Iteration 175/1000 | Loss: 0.00004474
Iteration 176/1000 | Loss: 0.00004473
Iteration 177/1000 | Loss: 0.00004473
Iteration 178/1000 | Loss: 0.00004473
Iteration 179/1000 | Loss: 0.00004473
Iteration 180/1000 | Loss: 0.00004473
Iteration 181/1000 | Loss: 0.00004473
Iteration 182/1000 | Loss: 0.00004473
Iteration 183/1000 | Loss: 0.00004473
Iteration 184/1000 | Loss: 0.00004473
Iteration 185/1000 | Loss: 0.00004472
Iteration 186/1000 | Loss: 0.00004472
Iteration 187/1000 | Loss: 0.00004472
Iteration 188/1000 | Loss: 0.00004472
Iteration 189/1000 | Loss: 0.00004472
Iteration 190/1000 | Loss: 0.00004472
Iteration 191/1000 | Loss: 0.00004472
Iteration 192/1000 | Loss: 0.00004472
Iteration 193/1000 | Loss: 0.00004472
Iteration 194/1000 | Loss: 0.00004472
Iteration 195/1000 | Loss: 0.00004472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [4.4723426981363446e-05, 4.4723426981363446e-05, 4.4723426981363446e-05, 4.4723426981363446e-05, 4.4723426981363446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4723426981363446e-05

Optimization complete. Final v2v error: 5.08816385269165 mm

Highest mean error: 5.655425071716309 mm for frame 64

Lowest mean error: 3.9313971996307373 mm for frame 19

Saving results

Total time: 47.57282614707947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408640
Iteration 2/25 | Loss: 0.00153511
Iteration 3/25 | Loss: 0.00146233
Iteration 4/25 | Loss: 0.00145473
Iteration 5/25 | Loss: 0.00145286
Iteration 6/25 | Loss: 0.00145280
Iteration 7/25 | Loss: 0.00145280
Iteration 8/25 | Loss: 0.00145280
Iteration 9/25 | Loss: 0.00145280
Iteration 10/25 | Loss: 0.00145280
Iteration 11/25 | Loss: 0.00145280
Iteration 12/25 | Loss: 0.00145280
Iteration 13/25 | Loss: 0.00145280
Iteration 14/25 | Loss: 0.00145280
Iteration 15/25 | Loss: 0.00145280
Iteration 16/25 | Loss: 0.00145280
Iteration 17/25 | Loss: 0.00145280
Iteration 18/25 | Loss: 0.00145280
Iteration 19/25 | Loss: 0.00145280
Iteration 20/25 | Loss: 0.00145280
Iteration 21/25 | Loss: 0.00145280
Iteration 22/25 | Loss: 0.00145280
Iteration 23/25 | Loss: 0.00145280
Iteration 24/25 | Loss: 0.00145280
Iteration 25/25 | Loss: 0.00145280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24205089
Iteration 2/25 | Loss: 0.00244835
Iteration 3/25 | Loss: 0.00244835
Iteration 4/25 | Loss: 0.00244835
Iteration 5/25 | Loss: 0.00244835
Iteration 6/25 | Loss: 0.00244835
Iteration 7/25 | Loss: 0.00244835
Iteration 8/25 | Loss: 0.00244835
Iteration 9/25 | Loss: 0.00244835
Iteration 10/25 | Loss: 0.00244835
Iteration 11/25 | Loss: 0.00244835
Iteration 12/25 | Loss: 0.00244835
Iteration 13/25 | Loss: 0.00244835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00244834553450346, 0.00244834553450346, 0.00244834553450346, 0.00244834553450346, 0.00244834553450346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00244834553450346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244835
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00001774
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001221
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001200
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001195
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001176
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001170
Iteration 30/1000 | Loss: 0.00001169
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001169
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00001167
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001166
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001165
Iteration 42/1000 | Loss: 0.00001165
Iteration 43/1000 | Loss: 0.00001165
Iteration 44/1000 | Loss: 0.00001164
Iteration 45/1000 | Loss: 0.00001164
Iteration 46/1000 | Loss: 0.00001163
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001150
Iteration 55/1000 | Loss: 0.00001149
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001147
Iteration 59/1000 | Loss: 0.00001147
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001143
Iteration 68/1000 | Loss: 0.00001143
Iteration 69/1000 | Loss: 0.00001142
Iteration 70/1000 | Loss: 0.00001142
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001141
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001138
Iteration 78/1000 | Loss: 0.00001138
Iteration 79/1000 | Loss: 0.00001138
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001137
Iteration 85/1000 | Loss: 0.00001137
Iteration 86/1000 | Loss: 0.00001137
Iteration 87/1000 | Loss: 0.00001137
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001134
Iteration 111/1000 | Loss: 0.00001134
Iteration 112/1000 | Loss: 0.00001134
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001127
Iteration 138/1000 | Loss: 0.00001127
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001125
Iteration 142/1000 | Loss: 0.00001125
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001124
Iteration 145/1000 | Loss: 0.00001122
Iteration 146/1000 | Loss: 0.00001121
Iteration 147/1000 | Loss: 0.00001121
Iteration 148/1000 | Loss: 0.00001121
Iteration 149/1000 | Loss: 0.00001120
Iteration 150/1000 | Loss: 0.00001120
Iteration 151/1000 | Loss: 0.00001120
Iteration 152/1000 | Loss: 0.00001120
Iteration 153/1000 | Loss: 0.00001120
Iteration 154/1000 | Loss: 0.00001119
Iteration 155/1000 | Loss: 0.00001119
Iteration 156/1000 | Loss: 0.00001118
Iteration 157/1000 | Loss: 0.00001118
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001117
Iteration 160/1000 | Loss: 0.00001117
Iteration 161/1000 | Loss: 0.00001116
Iteration 162/1000 | Loss: 0.00001116
Iteration 163/1000 | Loss: 0.00001116
Iteration 164/1000 | Loss: 0.00001116
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001115
Iteration 172/1000 | Loss: 0.00001115
Iteration 173/1000 | Loss: 0.00001115
Iteration 174/1000 | Loss: 0.00001115
Iteration 175/1000 | Loss: 0.00001115
Iteration 176/1000 | Loss: 0.00001115
Iteration 177/1000 | Loss: 0.00001115
Iteration 178/1000 | Loss: 0.00001114
Iteration 179/1000 | Loss: 0.00001114
Iteration 180/1000 | Loss: 0.00001114
Iteration 181/1000 | Loss: 0.00001113
Iteration 182/1000 | Loss: 0.00001113
Iteration 183/1000 | Loss: 0.00001113
Iteration 184/1000 | Loss: 0.00001113
Iteration 185/1000 | Loss: 0.00001113
Iteration 186/1000 | Loss: 0.00001112
Iteration 187/1000 | Loss: 0.00001112
Iteration 188/1000 | Loss: 0.00001112
Iteration 189/1000 | Loss: 0.00001112
Iteration 190/1000 | Loss: 0.00001112
Iteration 191/1000 | Loss: 0.00001112
Iteration 192/1000 | Loss: 0.00001112
Iteration 193/1000 | Loss: 0.00001112
Iteration 194/1000 | Loss: 0.00001112
Iteration 195/1000 | Loss: 0.00001111
Iteration 196/1000 | Loss: 0.00001111
Iteration 197/1000 | Loss: 0.00001111
Iteration 198/1000 | Loss: 0.00001111
Iteration 199/1000 | Loss: 0.00001111
Iteration 200/1000 | Loss: 0.00001111
Iteration 201/1000 | Loss: 0.00001111
Iteration 202/1000 | Loss: 0.00001111
Iteration 203/1000 | Loss: 0.00001111
Iteration 204/1000 | Loss: 0.00001111
Iteration 205/1000 | Loss: 0.00001111
Iteration 206/1000 | Loss: 0.00001111
Iteration 207/1000 | Loss: 0.00001111
Iteration 208/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.111216170102125e-05, 1.111216170102125e-05, 1.111216170102125e-05, 1.111216170102125e-05, 1.111216170102125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.111216170102125e-05

Optimization complete. Final v2v error: 2.8931500911712646 mm

Highest mean error: 2.94382905960083 mm for frame 79

Lowest mean error: 2.8548696041107178 mm for frame 21

Saving results

Total time: 43.15298414230347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390413
Iteration 2/25 | Loss: 0.00150914
Iteration 3/25 | Loss: 0.00145441
Iteration 4/25 | Loss: 0.00144959
Iteration 5/25 | Loss: 0.00144867
Iteration 6/25 | Loss: 0.00144867
Iteration 7/25 | Loss: 0.00144867
Iteration 8/25 | Loss: 0.00144867
Iteration 9/25 | Loss: 0.00144867
Iteration 10/25 | Loss: 0.00144867
Iteration 11/25 | Loss: 0.00144867
Iteration 12/25 | Loss: 0.00144867
Iteration 13/25 | Loss: 0.00144867
Iteration 14/25 | Loss: 0.00144867
Iteration 15/25 | Loss: 0.00144867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014486743602901697, 0.0014486743602901697, 0.0014486743602901697, 0.0014486743602901697, 0.0014486743602901697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014486743602901697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24136519
Iteration 2/25 | Loss: 0.00263911
Iteration 3/25 | Loss: 0.00263910
Iteration 4/25 | Loss: 0.00263910
Iteration 5/25 | Loss: 0.00263910
Iteration 6/25 | Loss: 0.00263910
Iteration 7/25 | Loss: 0.00263910
Iteration 8/25 | Loss: 0.00263910
Iteration 9/25 | Loss: 0.00263910
Iteration 10/25 | Loss: 0.00263910
Iteration 11/25 | Loss: 0.00263910
Iteration 12/25 | Loss: 0.00263910
Iteration 13/25 | Loss: 0.00263910
Iteration 14/25 | Loss: 0.00263910
Iteration 15/25 | Loss: 0.00263910
Iteration 16/25 | Loss: 0.00263910
Iteration 17/25 | Loss: 0.00263910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002639099955558777, 0.002639099955558777, 0.002639099955558777, 0.002639099955558777, 0.002639099955558777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002639099955558777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263910
Iteration 2/1000 | Loss: 0.00002938
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001667
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001430
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001189
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001162
Iteration 20/1000 | Loss: 0.00001162
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001144
Iteration 29/1000 | Loss: 0.00001143
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001108
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001104
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001096
Iteration 64/1000 | Loss: 0.00001096
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001096
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001092
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001089
Iteration 91/1000 | Loss: 0.00001089
Iteration 92/1000 | Loss: 0.00001088
Iteration 93/1000 | Loss: 0.00001088
Iteration 94/1000 | Loss: 0.00001088
Iteration 95/1000 | Loss: 0.00001088
Iteration 96/1000 | Loss: 0.00001088
Iteration 97/1000 | Loss: 0.00001088
Iteration 98/1000 | Loss: 0.00001087
Iteration 99/1000 | Loss: 0.00001087
Iteration 100/1000 | Loss: 0.00001087
Iteration 101/1000 | Loss: 0.00001087
Iteration 102/1000 | Loss: 0.00001087
Iteration 103/1000 | Loss: 0.00001087
Iteration 104/1000 | Loss: 0.00001086
Iteration 105/1000 | Loss: 0.00001086
Iteration 106/1000 | Loss: 0.00001086
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001085
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001084
Iteration 115/1000 | Loss: 0.00001084
Iteration 116/1000 | Loss: 0.00001084
Iteration 117/1000 | Loss: 0.00001083
Iteration 118/1000 | Loss: 0.00001083
Iteration 119/1000 | Loss: 0.00001083
Iteration 120/1000 | Loss: 0.00001082
Iteration 121/1000 | Loss: 0.00001082
Iteration 122/1000 | Loss: 0.00001082
Iteration 123/1000 | Loss: 0.00001082
Iteration 124/1000 | Loss: 0.00001081
Iteration 125/1000 | Loss: 0.00001081
Iteration 126/1000 | Loss: 0.00001081
Iteration 127/1000 | Loss: 0.00001080
Iteration 128/1000 | Loss: 0.00001079
Iteration 129/1000 | Loss: 0.00001079
Iteration 130/1000 | Loss: 0.00001079
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001077
Iteration 134/1000 | Loss: 0.00001077
Iteration 135/1000 | Loss: 0.00001076
Iteration 136/1000 | Loss: 0.00001076
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001073
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001072
Iteration 150/1000 | Loss: 0.00001072
Iteration 151/1000 | Loss: 0.00001072
Iteration 152/1000 | Loss: 0.00001071
Iteration 153/1000 | Loss: 0.00001071
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001071
Iteration 156/1000 | Loss: 0.00001071
Iteration 157/1000 | Loss: 0.00001070
Iteration 158/1000 | Loss: 0.00001070
Iteration 159/1000 | Loss: 0.00001070
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001068
Iteration 164/1000 | Loss: 0.00001068
Iteration 165/1000 | Loss: 0.00001068
Iteration 166/1000 | Loss: 0.00001068
Iteration 167/1000 | Loss: 0.00001067
Iteration 168/1000 | Loss: 0.00001067
Iteration 169/1000 | Loss: 0.00001067
Iteration 170/1000 | Loss: 0.00001067
Iteration 171/1000 | Loss: 0.00001066
Iteration 172/1000 | Loss: 0.00001066
Iteration 173/1000 | Loss: 0.00001065
Iteration 174/1000 | Loss: 0.00001065
Iteration 175/1000 | Loss: 0.00001065
Iteration 176/1000 | Loss: 0.00001065
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001063
Iteration 181/1000 | Loss: 0.00001063
Iteration 182/1000 | Loss: 0.00001063
Iteration 183/1000 | Loss: 0.00001063
Iteration 184/1000 | Loss: 0.00001062
Iteration 185/1000 | Loss: 0.00001062
Iteration 186/1000 | Loss: 0.00001062
Iteration 187/1000 | Loss: 0.00001062
Iteration 188/1000 | Loss: 0.00001062
Iteration 189/1000 | Loss: 0.00001062
Iteration 190/1000 | Loss: 0.00001062
Iteration 191/1000 | Loss: 0.00001062
Iteration 192/1000 | Loss: 0.00001061
Iteration 193/1000 | Loss: 0.00001061
Iteration 194/1000 | Loss: 0.00001061
Iteration 195/1000 | Loss: 0.00001061
Iteration 196/1000 | Loss: 0.00001061
Iteration 197/1000 | Loss: 0.00001061
Iteration 198/1000 | Loss: 0.00001060
Iteration 199/1000 | Loss: 0.00001060
Iteration 200/1000 | Loss: 0.00001060
Iteration 201/1000 | Loss: 0.00001060
Iteration 202/1000 | Loss: 0.00001060
Iteration 203/1000 | Loss: 0.00001060
Iteration 204/1000 | Loss: 0.00001059
Iteration 205/1000 | Loss: 0.00001059
Iteration 206/1000 | Loss: 0.00001059
Iteration 207/1000 | Loss: 0.00001059
Iteration 208/1000 | Loss: 0.00001059
Iteration 209/1000 | Loss: 0.00001058
Iteration 210/1000 | Loss: 0.00001058
Iteration 211/1000 | Loss: 0.00001058
Iteration 212/1000 | Loss: 0.00001058
Iteration 213/1000 | Loss: 0.00001058
Iteration 214/1000 | Loss: 0.00001058
Iteration 215/1000 | Loss: 0.00001058
Iteration 216/1000 | Loss: 0.00001057
Iteration 217/1000 | Loss: 0.00001057
Iteration 218/1000 | Loss: 0.00001057
Iteration 219/1000 | Loss: 0.00001057
Iteration 220/1000 | Loss: 0.00001057
Iteration 221/1000 | Loss: 0.00001057
Iteration 222/1000 | Loss: 0.00001056
Iteration 223/1000 | Loss: 0.00001056
Iteration 224/1000 | Loss: 0.00001056
Iteration 225/1000 | Loss: 0.00001056
Iteration 226/1000 | Loss: 0.00001055
Iteration 227/1000 | Loss: 0.00001055
Iteration 228/1000 | Loss: 0.00001055
Iteration 229/1000 | Loss: 0.00001055
Iteration 230/1000 | Loss: 0.00001055
Iteration 231/1000 | Loss: 0.00001055
Iteration 232/1000 | Loss: 0.00001054
Iteration 233/1000 | Loss: 0.00001054
Iteration 234/1000 | Loss: 0.00001054
Iteration 235/1000 | Loss: 0.00001054
Iteration 236/1000 | Loss: 0.00001054
Iteration 237/1000 | Loss: 0.00001054
Iteration 238/1000 | Loss: 0.00001054
Iteration 239/1000 | Loss: 0.00001054
Iteration 240/1000 | Loss: 0.00001054
Iteration 241/1000 | Loss: 0.00001054
Iteration 242/1000 | Loss: 0.00001054
Iteration 243/1000 | Loss: 0.00001054
Iteration 244/1000 | Loss: 0.00001054
Iteration 245/1000 | Loss: 0.00001054
Iteration 246/1000 | Loss: 0.00001054
Iteration 247/1000 | Loss: 0.00001054
Iteration 248/1000 | Loss: 0.00001053
Iteration 249/1000 | Loss: 0.00001053
Iteration 250/1000 | Loss: 0.00001053
Iteration 251/1000 | Loss: 0.00001053
Iteration 252/1000 | Loss: 0.00001053
Iteration 253/1000 | Loss: 0.00001053
Iteration 254/1000 | Loss: 0.00001053
Iteration 255/1000 | Loss: 0.00001053
Iteration 256/1000 | Loss: 0.00001053
Iteration 257/1000 | Loss: 0.00001053
Iteration 258/1000 | Loss: 0.00001052
Iteration 259/1000 | Loss: 0.00001052
Iteration 260/1000 | Loss: 0.00001052
Iteration 261/1000 | Loss: 0.00001052
Iteration 262/1000 | Loss: 0.00001052
Iteration 263/1000 | Loss: 0.00001052
Iteration 264/1000 | Loss: 0.00001052
Iteration 265/1000 | Loss: 0.00001052
Iteration 266/1000 | Loss: 0.00001052
Iteration 267/1000 | Loss: 0.00001052
Iteration 268/1000 | Loss: 0.00001052
Iteration 269/1000 | Loss: 0.00001052
Iteration 270/1000 | Loss: 0.00001052
Iteration 271/1000 | Loss: 0.00001052
Iteration 272/1000 | Loss: 0.00001052
Iteration 273/1000 | Loss: 0.00001052
Iteration 274/1000 | Loss: 0.00001052
Iteration 275/1000 | Loss: 0.00001052
Iteration 276/1000 | Loss: 0.00001052
Iteration 277/1000 | Loss: 0.00001051
Iteration 278/1000 | Loss: 0.00001051
Iteration 279/1000 | Loss: 0.00001051
Iteration 280/1000 | Loss: 0.00001051
Iteration 281/1000 | Loss: 0.00001051
Iteration 282/1000 | Loss: 0.00001051
Iteration 283/1000 | Loss: 0.00001050
Iteration 284/1000 | Loss: 0.00001050
Iteration 285/1000 | Loss: 0.00001049
Iteration 286/1000 | Loss: 0.00001049
Iteration 287/1000 | Loss: 0.00001049
Iteration 288/1000 | Loss: 0.00001049
Iteration 289/1000 | Loss: 0.00001049
Iteration 290/1000 | Loss: 0.00001048
Iteration 291/1000 | Loss: 0.00001048
Iteration 292/1000 | Loss: 0.00001048
Iteration 293/1000 | Loss: 0.00001048
Iteration 294/1000 | Loss: 0.00001048
Iteration 295/1000 | Loss: 0.00001048
Iteration 296/1000 | Loss: 0.00001048
Iteration 297/1000 | Loss: 0.00001048
Iteration 298/1000 | Loss: 0.00001047
Iteration 299/1000 | Loss: 0.00001047
Iteration 300/1000 | Loss: 0.00001047
Iteration 301/1000 | Loss: 0.00001047
Iteration 302/1000 | Loss: 0.00001046
Iteration 303/1000 | Loss: 0.00001046
Iteration 304/1000 | Loss: 0.00001046
Iteration 305/1000 | Loss: 0.00001046
Iteration 306/1000 | Loss: 0.00001046
Iteration 307/1000 | Loss: 0.00001046
Iteration 308/1000 | Loss: 0.00001046
Iteration 309/1000 | Loss: 0.00001046
Iteration 310/1000 | Loss: 0.00001046
Iteration 311/1000 | Loss: 0.00001046
Iteration 312/1000 | Loss: 0.00001045
Iteration 313/1000 | Loss: 0.00001045
Iteration 314/1000 | Loss: 0.00001045
Iteration 315/1000 | Loss: 0.00001045
Iteration 316/1000 | Loss: 0.00001045
Iteration 317/1000 | Loss: 0.00001045
Iteration 318/1000 | Loss: 0.00001045
Iteration 319/1000 | Loss: 0.00001045
Iteration 320/1000 | Loss: 0.00001045
Iteration 321/1000 | Loss: 0.00001045
Iteration 322/1000 | Loss: 0.00001045
Iteration 323/1000 | Loss: 0.00001045
Iteration 324/1000 | Loss: 0.00001045
Iteration 325/1000 | Loss: 0.00001045
Iteration 326/1000 | Loss: 0.00001045
Iteration 327/1000 | Loss: 0.00001045
Iteration 328/1000 | Loss: 0.00001045
Iteration 329/1000 | Loss: 0.00001045
Iteration 330/1000 | Loss: 0.00001045
Iteration 331/1000 | Loss: 0.00001045
Iteration 332/1000 | Loss: 0.00001044
Iteration 333/1000 | Loss: 0.00001044
Iteration 334/1000 | Loss: 0.00001044
Iteration 335/1000 | Loss: 0.00001044
Iteration 336/1000 | Loss: 0.00001044
Iteration 337/1000 | Loss: 0.00001044
Iteration 338/1000 | Loss: 0.00001044
Iteration 339/1000 | Loss: 0.00001044
Iteration 340/1000 | Loss: 0.00001044
Iteration 341/1000 | Loss: 0.00001044
Iteration 342/1000 | Loss: 0.00001044
Iteration 343/1000 | Loss: 0.00001044
Iteration 344/1000 | Loss: 0.00001044
Iteration 345/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [1.044168857333716e-05, 1.044168857333716e-05, 1.044168857333716e-05, 1.044168857333716e-05, 1.044168857333716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.044168857333716e-05

Optimization complete. Final v2v error: 2.7960317134857178 mm

Highest mean error: 2.9102163314819336 mm for frame 77

Lowest mean error: 2.7423365116119385 mm for frame 145

Saving results

Total time: 53.7197802066803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478462
Iteration 2/25 | Loss: 0.00158551
Iteration 3/25 | Loss: 0.00151024
Iteration 4/25 | Loss: 0.00150482
Iteration 5/25 | Loss: 0.00150333
Iteration 6/25 | Loss: 0.00150333
Iteration 7/25 | Loss: 0.00150333
Iteration 8/25 | Loss: 0.00150333
Iteration 9/25 | Loss: 0.00150333
Iteration 10/25 | Loss: 0.00150333
Iteration 11/25 | Loss: 0.00150333
Iteration 12/25 | Loss: 0.00150333
Iteration 13/25 | Loss: 0.00150333
Iteration 14/25 | Loss: 0.00150333
Iteration 15/25 | Loss: 0.00150333
Iteration 16/25 | Loss: 0.00150333
Iteration 17/25 | Loss: 0.00150333
Iteration 18/25 | Loss: 0.00150333
Iteration 19/25 | Loss: 0.00150333
Iteration 20/25 | Loss: 0.00150333
Iteration 21/25 | Loss: 0.00150333
Iteration 22/25 | Loss: 0.00150333
Iteration 23/25 | Loss: 0.00150333
Iteration 24/25 | Loss: 0.00150333
Iteration 25/25 | Loss: 0.00150333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25528002
Iteration 2/25 | Loss: 0.00255138
Iteration 3/25 | Loss: 0.00255138
Iteration 4/25 | Loss: 0.00255137
Iteration 5/25 | Loss: 0.00255137
Iteration 6/25 | Loss: 0.00255137
Iteration 7/25 | Loss: 0.00255137
Iteration 8/25 | Loss: 0.00255137
Iteration 9/25 | Loss: 0.00255137
Iteration 10/25 | Loss: 0.00255137
Iteration 11/25 | Loss: 0.00255137
Iteration 12/25 | Loss: 0.00255137
Iteration 13/25 | Loss: 0.00255137
Iteration 14/25 | Loss: 0.00255137
Iteration 15/25 | Loss: 0.00255137
Iteration 16/25 | Loss: 0.00255137
Iteration 17/25 | Loss: 0.00255137
Iteration 18/25 | Loss: 0.00255137
Iteration 19/25 | Loss: 0.00255137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0025513709988445044, 0.0025513709988445044, 0.0025513709988445044, 0.0025513709988445044, 0.0025513709988445044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025513709988445044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255137
Iteration 2/1000 | Loss: 0.00003482
Iteration 3/1000 | Loss: 0.00002678
Iteration 4/1000 | Loss: 0.00002495
Iteration 5/1000 | Loss: 0.00002372
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002212
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002114
Iteration 10/1000 | Loss: 0.00002062
Iteration 11/1000 | Loss: 0.00002034
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001963
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001938
Iteration 18/1000 | Loss: 0.00001928
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001925
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00001922
Iteration 23/1000 | Loss: 0.00001920
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001907
Iteration 28/1000 | Loss: 0.00001900
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001897
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001896
Iteration 33/1000 | Loss: 0.00001895
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001895
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001892
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001892
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001890
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001890
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001890
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001890
Iteration 76/1000 | Loss: 0.00001890
Iteration 77/1000 | Loss: 0.00001890
Iteration 78/1000 | Loss: 0.00001890
Iteration 79/1000 | Loss: 0.00001890
Iteration 80/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.889528903120663e-05, 1.889528903120663e-05, 1.889528903120663e-05, 1.889528903120663e-05, 1.889528903120663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.889528903120663e-05

Optimization complete. Final v2v error: 3.720670461654663 mm

Highest mean error: 4.496176242828369 mm for frame 58

Lowest mean error: 3.1999599933624268 mm for frame 209

Saving results

Total time: 42.70232272148132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584821
Iteration 2/25 | Loss: 0.00151873
Iteration 3/25 | Loss: 0.00145966
Iteration 4/25 | Loss: 0.00145298
Iteration 5/25 | Loss: 0.00145075
Iteration 6/25 | Loss: 0.00145075
Iteration 7/25 | Loss: 0.00145075
Iteration 8/25 | Loss: 0.00145075
Iteration 9/25 | Loss: 0.00145075
Iteration 10/25 | Loss: 0.00145075
Iteration 11/25 | Loss: 0.00145075
Iteration 12/25 | Loss: 0.00145075
Iteration 13/25 | Loss: 0.00145075
Iteration 14/25 | Loss: 0.00145075
Iteration 15/25 | Loss: 0.00145075
Iteration 16/25 | Loss: 0.00145075
Iteration 17/25 | Loss: 0.00145075
Iteration 18/25 | Loss: 0.00145075
Iteration 19/25 | Loss: 0.00145075
Iteration 20/25 | Loss: 0.00145075
Iteration 21/25 | Loss: 0.00145075
Iteration 22/25 | Loss: 0.00145075
Iteration 23/25 | Loss: 0.00145075
Iteration 24/25 | Loss: 0.00145075
Iteration 25/25 | Loss: 0.00145075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59952450
Iteration 2/25 | Loss: 0.00238590
Iteration 3/25 | Loss: 0.00238590
Iteration 4/25 | Loss: 0.00238590
Iteration 5/25 | Loss: 0.00238590
Iteration 6/25 | Loss: 0.00238590
Iteration 7/25 | Loss: 0.00238590
Iteration 8/25 | Loss: 0.00238590
Iteration 9/25 | Loss: 0.00238590
Iteration 10/25 | Loss: 0.00238590
Iteration 11/25 | Loss: 0.00238590
Iteration 12/25 | Loss: 0.00238590
Iteration 13/25 | Loss: 0.00238590
Iteration 14/25 | Loss: 0.00238590
Iteration 15/25 | Loss: 0.00238590
Iteration 16/25 | Loss: 0.00238590
Iteration 17/25 | Loss: 0.00238590
Iteration 18/25 | Loss: 0.00238590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023858980275690556, 0.0023858980275690556, 0.0023858980275690556, 0.0023858980275690556, 0.0023858980275690556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023858980275690556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238590
Iteration 2/1000 | Loss: 0.00002545
Iteration 3/1000 | Loss: 0.00001963
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001602
Iteration 7/1000 | Loss: 0.00001550
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001468
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001402
Iteration 14/1000 | Loss: 0.00001400
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001364
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001334
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001326
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001323
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001315
Iteration 71/1000 | Loss: 0.00001315
Iteration 72/1000 | Loss: 0.00001315
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001315
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001309
Iteration 91/1000 | Loss: 0.00001309
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001308
Iteration 99/1000 | Loss: 0.00001308
Iteration 100/1000 | Loss: 0.00001308
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001307
Iteration 103/1000 | Loss: 0.00001307
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001306
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001306
Iteration 111/1000 | Loss: 0.00001306
Iteration 112/1000 | Loss: 0.00001306
Iteration 113/1000 | Loss: 0.00001306
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001303
Iteration 137/1000 | Loss: 0.00001302
Iteration 138/1000 | Loss: 0.00001302
Iteration 139/1000 | Loss: 0.00001302
Iteration 140/1000 | Loss: 0.00001302
Iteration 141/1000 | Loss: 0.00001302
Iteration 142/1000 | Loss: 0.00001302
Iteration 143/1000 | Loss: 0.00001302
Iteration 144/1000 | Loss: 0.00001302
Iteration 145/1000 | Loss: 0.00001302
Iteration 146/1000 | Loss: 0.00001302
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001301
Iteration 151/1000 | Loss: 0.00001301
Iteration 152/1000 | Loss: 0.00001301
Iteration 153/1000 | Loss: 0.00001301
Iteration 154/1000 | Loss: 0.00001301
Iteration 155/1000 | Loss: 0.00001301
Iteration 156/1000 | Loss: 0.00001301
Iteration 157/1000 | Loss: 0.00001301
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Iteration 162/1000 | Loss: 0.00001300
Iteration 163/1000 | Loss: 0.00001300
Iteration 164/1000 | Loss: 0.00001300
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001299
Iteration 168/1000 | Loss: 0.00001299
Iteration 169/1000 | Loss: 0.00001299
Iteration 170/1000 | Loss: 0.00001299
Iteration 171/1000 | Loss: 0.00001299
Iteration 172/1000 | Loss: 0.00001299
Iteration 173/1000 | Loss: 0.00001299
Iteration 174/1000 | Loss: 0.00001299
Iteration 175/1000 | Loss: 0.00001299
Iteration 176/1000 | Loss: 0.00001299
Iteration 177/1000 | Loss: 0.00001299
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001299
Iteration 180/1000 | Loss: 0.00001299
Iteration 181/1000 | Loss: 0.00001299
Iteration 182/1000 | Loss: 0.00001299
Iteration 183/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.2986238289158791e-05, 1.2986238289158791e-05, 1.2986238289158791e-05, 1.2986238289158791e-05, 1.2986238289158791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2986238289158791e-05

Optimization complete. Final v2v error: 3.122478485107422 mm

Highest mean error: 3.564291000366211 mm for frame 117

Lowest mean error: 2.904367685317993 mm for frame 166

Saving results

Total time: 42.81205749511719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989792
Iteration 2/25 | Loss: 0.00294321
Iteration 3/25 | Loss: 0.00241404
Iteration 4/25 | Loss: 0.00207169
Iteration 5/25 | Loss: 0.00270453
Iteration 6/25 | Loss: 0.00261433
Iteration 7/25 | Loss: 0.00247133
Iteration 8/25 | Loss: 0.00199710
Iteration 9/25 | Loss: 0.00182732
Iteration 10/25 | Loss: 0.00179559
Iteration 11/25 | Loss: 0.00178725
Iteration 12/25 | Loss: 0.00172823
Iteration 13/25 | Loss: 0.00170179
Iteration 14/25 | Loss: 0.00169003
Iteration 15/25 | Loss: 0.00167650
Iteration 16/25 | Loss: 0.00167405
Iteration 17/25 | Loss: 0.00166694
Iteration 18/25 | Loss: 0.00166919
Iteration 19/25 | Loss: 0.00165714
Iteration 20/25 | Loss: 0.00166127
Iteration 21/25 | Loss: 0.00165306
Iteration 22/25 | Loss: 0.00165417
Iteration 23/25 | Loss: 0.00166001
Iteration 24/25 | Loss: 0.00165166
Iteration 25/25 | Loss: 0.00165253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13564968
Iteration 2/25 | Loss: 0.00232736
Iteration 3/25 | Loss: 0.00227655
Iteration 4/25 | Loss: 0.00227655
Iteration 5/25 | Loss: 0.00227655
Iteration 6/25 | Loss: 0.00227655
Iteration 7/25 | Loss: 0.00227655
Iteration 8/25 | Loss: 0.00227655
Iteration 9/25 | Loss: 0.00227655
Iteration 10/25 | Loss: 0.00227655
Iteration 11/25 | Loss: 0.00227655
Iteration 12/25 | Loss: 0.00227655
Iteration 13/25 | Loss: 0.00227655
Iteration 14/25 | Loss: 0.00227655
Iteration 15/25 | Loss: 0.00227655
Iteration 16/25 | Loss: 0.00227655
Iteration 17/25 | Loss: 0.00227655
Iteration 18/25 | Loss: 0.00227655
Iteration 19/25 | Loss: 0.00227655
Iteration 20/25 | Loss: 0.00227655
Iteration 21/25 | Loss: 0.00227655
Iteration 22/25 | Loss: 0.00227655
Iteration 23/25 | Loss: 0.00227655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002276550279930234, 0.002276550279930234, 0.002276550279930234, 0.002276550279930234, 0.002276550279930234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002276550279930234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227655
Iteration 2/1000 | Loss: 0.00069714
Iteration 3/1000 | Loss: 0.00050554
Iteration 4/1000 | Loss: 0.00061250
Iteration 5/1000 | Loss: 0.00054232
Iteration 6/1000 | Loss: 0.00099441
Iteration 7/1000 | Loss: 0.00086135
Iteration 8/1000 | Loss: 0.00022846
Iteration 9/1000 | Loss: 0.00035033
Iteration 10/1000 | Loss: 0.00030682
Iteration 11/1000 | Loss: 0.00040002
Iteration 12/1000 | Loss: 0.00028453
Iteration 13/1000 | Loss: 0.00023453
Iteration 14/1000 | Loss: 0.00040038
Iteration 15/1000 | Loss: 0.00032976
Iteration 16/1000 | Loss: 0.00063880
Iteration 17/1000 | Loss: 0.00055928
Iteration 18/1000 | Loss: 0.00025382
Iteration 19/1000 | Loss: 0.00031135
Iteration 20/1000 | Loss: 0.00028300
Iteration 21/1000 | Loss: 0.00038141
Iteration 22/1000 | Loss: 0.00073927
Iteration 23/1000 | Loss: 0.00042796
Iteration 24/1000 | Loss: 0.00041012
Iteration 25/1000 | Loss: 0.00034389
Iteration 26/1000 | Loss: 0.00039301
Iteration 27/1000 | Loss: 0.00030485
Iteration 28/1000 | Loss: 0.00030762
Iteration 29/1000 | Loss: 0.00054984
Iteration 30/1000 | Loss: 0.00043856
Iteration 31/1000 | Loss: 0.00103915
Iteration 32/1000 | Loss: 0.00041947
Iteration 33/1000 | Loss: 0.00029663
Iteration 34/1000 | Loss: 0.00145829
Iteration 35/1000 | Loss: 0.00011422
Iteration 36/1000 | Loss: 0.00016373
Iteration 37/1000 | Loss: 0.00009407
Iteration 38/1000 | Loss: 0.00009923
Iteration 39/1000 | Loss: 0.00012453
Iteration 40/1000 | Loss: 0.00011071
Iteration 41/1000 | Loss: 0.00010575
Iteration 42/1000 | Loss: 0.00007139
Iteration 43/1000 | Loss: 0.00008110
Iteration 44/1000 | Loss: 0.00013255
Iteration 45/1000 | Loss: 0.00009524
Iteration 46/1000 | Loss: 0.00009633
Iteration 47/1000 | Loss: 0.00009619
Iteration 48/1000 | Loss: 0.00009676
Iteration 49/1000 | Loss: 0.00009528
Iteration 50/1000 | Loss: 0.00009008
Iteration 51/1000 | Loss: 0.00012300
Iteration 52/1000 | Loss: 0.00010021
Iteration 53/1000 | Loss: 0.00010642
Iteration 54/1000 | Loss: 0.00012190
Iteration 55/1000 | Loss: 0.00010906
Iteration 56/1000 | Loss: 0.00010095
Iteration 57/1000 | Loss: 0.00009362
Iteration 58/1000 | Loss: 0.00009766
Iteration 59/1000 | Loss: 0.00040874
Iteration 60/1000 | Loss: 0.00037672
Iteration 61/1000 | Loss: 0.00030050
Iteration 62/1000 | Loss: 0.00023672
Iteration 63/1000 | Loss: 0.00024014
Iteration 64/1000 | Loss: 0.00022809
Iteration 65/1000 | Loss: 0.00019242
Iteration 66/1000 | Loss: 0.00025167
Iteration 67/1000 | Loss: 0.00015667
Iteration 68/1000 | Loss: 0.00012872
Iteration 69/1000 | Loss: 0.00022650
Iteration 70/1000 | Loss: 0.00012021
Iteration 71/1000 | Loss: 0.00009314
Iteration 72/1000 | Loss: 0.00016676
Iteration 73/1000 | Loss: 0.00008989
Iteration 74/1000 | Loss: 0.00010627
Iteration 75/1000 | Loss: 0.00008933
Iteration 76/1000 | Loss: 0.00009296
Iteration 77/1000 | Loss: 0.00016121
Iteration 78/1000 | Loss: 0.00030831
Iteration 79/1000 | Loss: 0.00024353
Iteration 80/1000 | Loss: 0.00004075
Iteration 81/1000 | Loss: 0.00015574
Iteration 82/1000 | Loss: 0.00003255
Iteration 83/1000 | Loss: 0.00008462
Iteration 84/1000 | Loss: 0.00015811
Iteration 85/1000 | Loss: 0.00004678
Iteration 86/1000 | Loss: 0.00002827
Iteration 87/1000 | Loss: 0.00002687
Iteration 88/1000 | Loss: 0.00002597
Iteration 89/1000 | Loss: 0.00002522
Iteration 90/1000 | Loss: 0.00002467
Iteration 91/1000 | Loss: 0.00002414
Iteration 92/1000 | Loss: 0.00007114
Iteration 93/1000 | Loss: 0.00002335
Iteration 94/1000 | Loss: 0.00007609
Iteration 95/1000 | Loss: 0.00002289
Iteration 96/1000 | Loss: 0.00002246
Iteration 97/1000 | Loss: 0.00002242
Iteration 98/1000 | Loss: 0.00002225
Iteration 99/1000 | Loss: 0.00004511
Iteration 100/1000 | Loss: 0.00008784
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002197
Iteration 103/1000 | Loss: 0.00004172
Iteration 104/1000 | Loss: 0.00005279
Iteration 105/1000 | Loss: 0.00003766
Iteration 106/1000 | Loss: 0.00005366
Iteration 107/1000 | Loss: 0.00007780
Iteration 108/1000 | Loss: 0.00002930
Iteration 109/1000 | Loss: 0.00002190
Iteration 110/1000 | Loss: 0.00002189
Iteration 111/1000 | Loss: 0.00002189
Iteration 112/1000 | Loss: 0.00002188
Iteration 113/1000 | Loss: 0.00002188
Iteration 114/1000 | Loss: 0.00002188
Iteration 115/1000 | Loss: 0.00002188
Iteration 116/1000 | Loss: 0.00002187
Iteration 117/1000 | Loss: 0.00002187
Iteration 118/1000 | Loss: 0.00002187
Iteration 119/1000 | Loss: 0.00002187
Iteration 120/1000 | Loss: 0.00002187
Iteration 121/1000 | Loss: 0.00002187
Iteration 122/1000 | Loss: 0.00002187
Iteration 123/1000 | Loss: 0.00002187
Iteration 124/1000 | Loss: 0.00002187
Iteration 125/1000 | Loss: 0.00002186
Iteration 126/1000 | Loss: 0.00002186
Iteration 127/1000 | Loss: 0.00002186
Iteration 128/1000 | Loss: 0.00002186
Iteration 129/1000 | Loss: 0.00002186
Iteration 130/1000 | Loss: 0.00002186
Iteration 131/1000 | Loss: 0.00002186
Iteration 132/1000 | Loss: 0.00002186
Iteration 133/1000 | Loss: 0.00002186
Iteration 134/1000 | Loss: 0.00002185
Iteration 135/1000 | Loss: 0.00002185
Iteration 136/1000 | Loss: 0.00002185
Iteration 137/1000 | Loss: 0.00002185
Iteration 138/1000 | Loss: 0.00002185
Iteration 139/1000 | Loss: 0.00002185
Iteration 140/1000 | Loss: 0.00002185
Iteration 141/1000 | Loss: 0.00002185
Iteration 142/1000 | Loss: 0.00002185
Iteration 143/1000 | Loss: 0.00002185
Iteration 144/1000 | Loss: 0.00002185
Iteration 145/1000 | Loss: 0.00002185
Iteration 146/1000 | Loss: 0.00002185
Iteration 147/1000 | Loss: 0.00002185
Iteration 148/1000 | Loss: 0.00002185
Iteration 149/1000 | Loss: 0.00002184
Iteration 150/1000 | Loss: 0.00002184
Iteration 151/1000 | Loss: 0.00002184
Iteration 152/1000 | Loss: 0.00002183
Iteration 153/1000 | Loss: 0.00002183
Iteration 154/1000 | Loss: 0.00002183
Iteration 155/1000 | Loss: 0.00002183
Iteration 156/1000 | Loss: 0.00002183
Iteration 157/1000 | Loss: 0.00002183
Iteration 158/1000 | Loss: 0.00002182
Iteration 159/1000 | Loss: 0.00002182
Iteration 160/1000 | Loss: 0.00002181
Iteration 161/1000 | Loss: 0.00002181
Iteration 162/1000 | Loss: 0.00002181
Iteration 163/1000 | Loss: 0.00002181
Iteration 164/1000 | Loss: 0.00002181
Iteration 165/1000 | Loss: 0.00002180
Iteration 166/1000 | Loss: 0.00002180
Iteration 167/1000 | Loss: 0.00002180
Iteration 168/1000 | Loss: 0.00002180
Iteration 169/1000 | Loss: 0.00002180
Iteration 170/1000 | Loss: 0.00002180
Iteration 171/1000 | Loss: 0.00002180
Iteration 172/1000 | Loss: 0.00002180
Iteration 173/1000 | Loss: 0.00002180
Iteration 174/1000 | Loss: 0.00002180
Iteration 175/1000 | Loss: 0.00002180
Iteration 176/1000 | Loss: 0.00002180
Iteration 177/1000 | Loss: 0.00002180
Iteration 178/1000 | Loss: 0.00002180
Iteration 179/1000 | Loss: 0.00002180
Iteration 180/1000 | Loss: 0.00002180
Iteration 181/1000 | Loss: 0.00002180
Iteration 182/1000 | Loss: 0.00002180
Iteration 183/1000 | Loss: 0.00002180
Iteration 184/1000 | Loss: 0.00002180
Iteration 185/1000 | Loss: 0.00002180
Iteration 186/1000 | Loss: 0.00002180
Iteration 187/1000 | Loss: 0.00002180
Iteration 188/1000 | Loss: 0.00002180
Iteration 189/1000 | Loss: 0.00002180
Iteration 190/1000 | Loss: 0.00002180
Iteration 191/1000 | Loss: 0.00002180
Iteration 192/1000 | Loss: 0.00002180
Iteration 193/1000 | Loss: 0.00002180
Iteration 194/1000 | Loss: 0.00002180
Iteration 195/1000 | Loss: 0.00002180
Iteration 196/1000 | Loss: 0.00002180
Iteration 197/1000 | Loss: 0.00002180
Iteration 198/1000 | Loss: 0.00002180
Iteration 199/1000 | Loss: 0.00002180
Iteration 200/1000 | Loss: 0.00002180
Iteration 201/1000 | Loss: 0.00002180
Iteration 202/1000 | Loss: 0.00002180
Iteration 203/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.1795620341436006e-05, 2.1795620341436006e-05, 2.1795620341436006e-05, 2.1795620341436006e-05, 2.1795620341436006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1795620341436006e-05

Optimization complete. Final v2v error: 3.9201467037200928 mm

Highest mean error: 5.8361124992370605 mm for frame 113

Lowest mean error: 3.7639596462249756 mm for frame 0

Saving results

Total time: 194.07457280158997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790918
Iteration 2/25 | Loss: 0.00177805
Iteration 3/25 | Loss: 0.00153814
Iteration 4/25 | Loss: 0.00151617
Iteration 5/25 | Loss: 0.00151287
Iteration 6/25 | Loss: 0.00151253
Iteration 7/25 | Loss: 0.00151253
Iteration 8/25 | Loss: 0.00151253
Iteration 9/25 | Loss: 0.00151253
Iteration 10/25 | Loss: 0.00151253
Iteration 11/25 | Loss: 0.00151253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015125276986509562, 0.0015125276986509562, 0.0015125276986509562, 0.0015125276986509562, 0.0015125276986509562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015125276986509562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16564357
Iteration 2/25 | Loss: 0.00205249
Iteration 3/25 | Loss: 0.00205248
Iteration 4/25 | Loss: 0.00205248
Iteration 5/25 | Loss: 0.00205248
Iteration 6/25 | Loss: 0.00205248
Iteration 7/25 | Loss: 0.00205248
Iteration 8/25 | Loss: 0.00205248
Iteration 9/25 | Loss: 0.00205248
Iteration 10/25 | Loss: 0.00205248
Iteration 11/25 | Loss: 0.00205248
Iteration 12/25 | Loss: 0.00205248
Iteration 13/25 | Loss: 0.00205248
Iteration 14/25 | Loss: 0.00205248
Iteration 15/25 | Loss: 0.00205248
Iteration 16/25 | Loss: 0.00205248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002052475931122899, 0.002052475931122899, 0.002052475931122899, 0.002052475931122899, 0.002052475931122899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002052475931122899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205248
Iteration 2/1000 | Loss: 0.00003981
Iteration 3/1000 | Loss: 0.00002683
Iteration 4/1000 | Loss: 0.00002409
Iteration 5/1000 | Loss: 0.00002233
Iteration 6/1000 | Loss: 0.00002092
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001950
Iteration 9/1000 | Loss: 0.00001893
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001829
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001795
Iteration 14/1000 | Loss: 0.00001795
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001757
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001739
Iteration 20/1000 | Loss: 0.00001738
Iteration 21/1000 | Loss: 0.00001737
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001714
Iteration 28/1000 | Loss: 0.00001707
Iteration 29/1000 | Loss: 0.00001705
Iteration 30/1000 | Loss: 0.00001704
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001699
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001694
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001683
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001682
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001681
Iteration 94/1000 | Loss: 0.00001681
Iteration 95/1000 | Loss: 0.00001681
Iteration 96/1000 | Loss: 0.00001681
Iteration 97/1000 | Loss: 0.00001681
Iteration 98/1000 | Loss: 0.00001681
Iteration 99/1000 | Loss: 0.00001681
Iteration 100/1000 | Loss: 0.00001681
Iteration 101/1000 | Loss: 0.00001681
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001680
Iteration 104/1000 | Loss: 0.00001680
Iteration 105/1000 | Loss: 0.00001680
Iteration 106/1000 | Loss: 0.00001680
Iteration 107/1000 | Loss: 0.00001680
Iteration 108/1000 | Loss: 0.00001680
Iteration 109/1000 | Loss: 0.00001680
Iteration 110/1000 | Loss: 0.00001680
Iteration 111/1000 | Loss: 0.00001680
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001680
Iteration 121/1000 | Loss: 0.00001680
Iteration 122/1000 | Loss: 0.00001680
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001680
Iteration 125/1000 | Loss: 0.00001680
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.680298191786278e-05, 1.680298191786278e-05, 1.680298191786278e-05, 1.680298191786278e-05, 1.680298191786278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680298191786278e-05

Optimization complete. Final v2v error: 3.50840425491333 mm

Highest mean error: 4.237543106079102 mm for frame 89

Lowest mean error: 3.0718255043029785 mm for frame 52

Saving results

Total time: 46.59439182281494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002579
Iteration 2/25 | Loss: 0.00223673
Iteration 3/25 | Loss: 0.00178970
Iteration 4/25 | Loss: 0.00186345
Iteration 5/25 | Loss: 0.00171569
Iteration 6/25 | Loss: 0.00172397
Iteration 7/25 | Loss: 0.00170624
Iteration 8/25 | Loss: 0.00163156
Iteration 9/25 | Loss: 0.00158480
Iteration 10/25 | Loss: 0.00157414
Iteration 11/25 | Loss: 0.00155516
Iteration 12/25 | Loss: 0.00154467
Iteration 13/25 | Loss: 0.00154006
Iteration 14/25 | Loss: 0.00153972
Iteration 15/25 | Loss: 0.00154254
Iteration 16/25 | Loss: 0.00153382
Iteration 17/25 | Loss: 0.00155579
Iteration 18/25 | Loss: 0.00152428
Iteration 19/25 | Loss: 0.00151741
Iteration 20/25 | Loss: 0.00151571
Iteration 21/25 | Loss: 0.00151523
Iteration 22/25 | Loss: 0.00151511
Iteration 23/25 | Loss: 0.00151503
Iteration 24/25 | Loss: 0.00151503
Iteration 25/25 | Loss: 0.00151503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52960396
Iteration 2/25 | Loss: 0.00269565
Iteration 3/25 | Loss: 0.00269565
Iteration 4/25 | Loss: 0.00269565
Iteration 5/25 | Loss: 0.00269565
Iteration 6/25 | Loss: 0.00269565
Iteration 7/25 | Loss: 0.00269565
Iteration 8/25 | Loss: 0.00269565
Iteration 9/25 | Loss: 0.00269565
Iteration 10/25 | Loss: 0.00269565
Iteration 11/25 | Loss: 0.00269565
Iteration 12/25 | Loss: 0.00269565
Iteration 13/25 | Loss: 0.00269565
Iteration 14/25 | Loss: 0.00269565
Iteration 15/25 | Loss: 0.00269565
Iteration 16/25 | Loss: 0.00269565
Iteration 17/25 | Loss: 0.00269565
Iteration 18/25 | Loss: 0.00269565
Iteration 19/25 | Loss: 0.00269565
Iteration 20/25 | Loss: 0.00269565
Iteration 21/25 | Loss: 0.00269565
Iteration 22/25 | Loss: 0.00269565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002695647068321705, 0.002695647068321705, 0.002695647068321705, 0.002695647068321705, 0.002695647068321705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002695647068321705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269565
Iteration 2/1000 | Loss: 0.00008539
Iteration 3/1000 | Loss: 0.00021693
Iteration 4/1000 | Loss: 0.00005766
Iteration 5/1000 | Loss: 0.00018764
Iteration 6/1000 | Loss: 0.00020013
Iteration 7/1000 | Loss: 0.00013429
Iteration 8/1000 | Loss: 0.00004255
Iteration 9/1000 | Loss: 0.00035076
Iteration 10/1000 | Loss: 0.00005632
Iteration 11/1000 | Loss: 0.00004574
Iteration 12/1000 | Loss: 0.00004138
Iteration 13/1000 | Loss: 0.00023547
Iteration 14/1000 | Loss: 0.00020876
Iteration 15/1000 | Loss: 0.00023411
Iteration 16/1000 | Loss: 0.00004387
Iteration 17/1000 | Loss: 0.00017666
Iteration 18/1000 | Loss: 0.00009776
Iteration 19/1000 | Loss: 0.00014024
Iteration 20/1000 | Loss: 0.00003586
Iteration 21/1000 | Loss: 0.00003401
Iteration 22/1000 | Loss: 0.00003283
Iteration 23/1000 | Loss: 0.00003194
Iteration 24/1000 | Loss: 0.00018704
Iteration 25/1000 | Loss: 0.00019924
Iteration 26/1000 | Loss: 0.00003224
Iteration 27/1000 | Loss: 0.00016941
Iteration 28/1000 | Loss: 0.00018426
Iteration 29/1000 | Loss: 0.00026173
Iteration 30/1000 | Loss: 0.00068849
Iteration 31/1000 | Loss: 0.00027609
Iteration 32/1000 | Loss: 0.00005645
Iteration 33/1000 | Loss: 0.00003684
Iteration 34/1000 | Loss: 0.00003274
Iteration 35/1000 | Loss: 0.00003149
Iteration 36/1000 | Loss: 0.00003024
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002941
Iteration 39/1000 | Loss: 0.00002903
Iteration 40/1000 | Loss: 0.00002871
Iteration 41/1000 | Loss: 0.00002844
Iteration 42/1000 | Loss: 0.00002827
Iteration 43/1000 | Loss: 0.00002808
Iteration 44/1000 | Loss: 0.00002799
Iteration 45/1000 | Loss: 0.00002797
Iteration 46/1000 | Loss: 0.00002797
Iteration 47/1000 | Loss: 0.00002796
Iteration 48/1000 | Loss: 0.00002793
Iteration 49/1000 | Loss: 0.00002788
Iteration 50/1000 | Loss: 0.00002788
Iteration 51/1000 | Loss: 0.00022596
Iteration 52/1000 | Loss: 0.00035503
Iteration 53/1000 | Loss: 0.00024037
Iteration 54/1000 | Loss: 0.00002636
Iteration 55/1000 | Loss: 0.00002277
Iteration 56/1000 | Loss: 0.00002049
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001651
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001614
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001585
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001561
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001536
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001533
Iteration 82/1000 | Loss: 0.00001532
Iteration 83/1000 | Loss: 0.00001530
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001525
Iteration 91/1000 | Loss: 0.00001525
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001522
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001518
Iteration 124/1000 | Loss: 0.00001518
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001515
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001515
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Iteration 157/1000 | Loss: 0.00001514
Iteration 158/1000 | Loss: 0.00001514
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001513
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001513
Iteration 164/1000 | Loss: 0.00001513
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001513
Iteration 168/1000 | Loss: 0.00001513
Iteration 169/1000 | Loss: 0.00001513
Iteration 170/1000 | Loss: 0.00001513
Iteration 171/1000 | Loss: 0.00001513
Iteration 172/1000 | Loss: 0.00001512
Iteration 173/1000 | Loss: 0.00001512
Iteration 174/1000 | Loss: 0.00001512
Iteration 175/1000 | Loss: 0.00001512
Iteration 176/1000 | Loss: 0.00001512
Iteration 177/1000 | Loss: 0.00001512
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001512
Iteration 180/1000 | Loss: 0.00001512
Iteration 181/1000 | Loss: 0.00001512
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001512
Iteration 184/1000 | Loss: 0.00001511
Iteration 185/1000 | Loss: 0.00001511
Iteration 186/1000 | Loss: 0.00001511
Iteration 187/1000 | Loss: 0.00001511
Iteration 188/1000 | Loss: 0.00001511
Iteration 189/1000 | Loss: 0.00001511
Iteration 190/1000 | Loss: 0.00001511
Iteration 191/1000 | Loss: 0.00001511
Iteration 192/1000 | Loss: 0.00001510
Iteration 193/1000 | Loss: 0.00001510
Iteration 194/1000 | Loss: 0.00001510
Iteration 195/1000 | Loss: 0.00001510
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001510
Iteration 201/1000 | Loss: 0.00001510
Iteration 202/1000 | Loss: 0.00001510
Iteration 203/1000 | Loss: 0.00001510
Iteration 204/1000 | Loss: 0.00001510
Iteration 205/1000 | Loss: 0.00001510
Iteration 206/1000 | Loss: 0.00001510
Iteration 207/1000 | Loss: 0.00001510
Iteration 208/1000 | Loss: 0.00001510
Iteration 209/1000 | Loss: 0.00001510
Iteration 210/1000 | Loss: 0.00001510
Iteration 211/1000 | Loss: 0.00001509
Iteration 212/1000 | Loss: 0.00001509
Iteration 213/1000 | Loss: 0.00001509
Iteration 214/1000 | Loss: 0.00001509
Iteration 215/1000 | Loss: 0.00001509
Iteration 216/1000 | Loss: 0.00001509
Iteration 217/1000 | Loss: 0.00001509
Iteration 218/1000 | Loss: 0.00001509
Iteration 219/1000 | Loss: 0.00001509
Iteration 220/1000 | Loss: 0.00001509
Iteration 221/1000 | Loss: 0.00001509
Iteration 222/1000 | Loss: 0.00001509
Iteration 223/1000 | Loss: 0.00001509
Iteration 224/1000 | Loss: 0.00001509
Iteration 225/1000 | Loss: 0.00001509
Iteration 226/1000 | Loss: 0.00001509
Iteration 227/1000 | Loss: 0.00001509
Iteration 228/1000 | Loss: 0.00001509
Iteration 229/1000 | Loss: 0.00001509
Iteration 230/1000 | Loss: 0.00001509
Iteration 231/1000 | Loss: 0.00001509
Iteration 232/1000 | Loss: 0.00001509
Iteration 233/1000 | Loss: 0.00001509
Iteration 234/1000 | Loss: 0.00001509
Iteration 235/1000 | Loss: 0.00001509
Iteration 236/1000 | Loss: 0.00001509
Iteration 237/1000 | Loss: 0.00001509
Iteration 238/1000 | Loss: 0.00001509
Iteration 239/1000 | Loss: 0.00001509
Iteration 240/1000 | Loss: 0.00001509
Iteration 241/1000 | Loss: 0.00001509
Iteration 242/1000 | Loss: 0.00001509
Iteration 243/1000 | Loss: 0.00001509
Iteration 244/1000 | Loss: 0.00001509
Iteration 245/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.5085518498381134e-05, 1.5085518498381134e-05, 1.5085518498381134e-05, 1.5085518498381134e-05, 1.5085518498381134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5085518498381134e-05

Optimization complete. Final v2v error: 3.3310179710388184 mm

Highest mean error: 5.38293981552124 mm for frame 94

Lowest mean error: 3.0026540756225586 mm for frame 59

Saving results

Total time: 135.83549809455872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568089
Iteration 2/25 | Loss: 0.00167689
Iteration 3/25 | Loss: 0.00155220
Iteration 4/25 | Loss: 0.00149039
Iteration 5/25 | Loss: 0.00148250
Iteration 6/25 | Loss: 0.00147619
Iteration 7/25 | Loss: 0.00147518
Iteration 8/25 | Loss: 0.00147819
Iteration 9/25 | Loss: 0.00147468
Iteration 10/25 | Loss: 0.00147747
Iteration 11/25 | Loss: 0.00147406
Iteration 12/25 | Loss: 0.00147245
Iteration 13/25 | Loss: 0.00147492
Iteration 14/25 | Loss: 0.00147270
Iteration 15/25 | Loss: 0.00147197
Iteration 16/25 | Loss: 0.00147192
Iteration 17/25 | Loss: 0.00147190
Iteration 18/25 | Loss: 0.00147190
Iteration 19/25 | Loss: 0.00147190
Iteration 20/25 | Loss: 0.00147190
Iteration 21/25 | Loss: 0.00147190
Iteration 22/25 | Loss: 0.00147190
Iteration 23/25 | Loss: 0.00147190
Iteration 24/25 | Loss: 0.00147190
Iteration 25/25 | Loss: 0.00147190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.04370117
Iteration 2/25 | Loss: 0.00235728
Iteration 3/25 | Loss: 0.00232766
Iteration 4/25 | Loss: 0.00232766
Iteration 5/25 | Loss: 0.00232766
Iteration 6/25 | Loss: 0.00232766
Iteration 7/25 | Loss: 0.00232766
Iteration 8/25 | Loss: 0.00232766
Iteration 9/25 | Loss: 0.00232766
Iteration 10/25 | Loss: 0.00232766
Iteration 11/25 | Loss: 0.00232766
Iteration 12/25 | Loss: 0.00232766
Iteration 13/25 | Loss: 0.00232766
Iteration 14/25 | Loss: 0.00232766
Iteration 15/25 | Loss: 0.00232766
Iteration 16/25 | Loss: 0.00232766
Iteration 17/25 | Loss: 0.00232766
Iteration 18/25 | Loss: 0.00232766
Iteration 19/25 | Loss: 0.00232766
Iteration 20/25 | Loss: 0.00232766
Iteration 21/25 | Loss: 0.00232766
Iteration 22/25 | Loss: 0.00232766
Iteration 23/25 | Loss: 0.00232766
Iteration 24/25 | Loss: 0.00232766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002327658934518695, 0.002327658934518695, 0.002327658934518695, 0.002327658934518695, 0.002327658934518695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002327658934518695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232766
Iteration 2/1000 | Loss: 0.00005567
Iteration 3/1000 | Loss: 0.00004173
Iteration 4/1000 | Loss: 0.00003178
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00004382
Iteration 8/1000 | Loss: 0.00002792
Iteration 9/1000 | Loss: 0.00001784
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00007313
Iteration 13/1000 | Loss: 0.00001557
Iteration 14/1000 | Loss: 0.00001530
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00004284
Iteration 20/1000 | Loss: 0.00001484
Iteration 21/1000 | Loss: 0.00001482
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00002072
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001455
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001454
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00001447
Iteration 40/1000 | Loss: 0.00001446
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001435
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001432
Iteration 68/1000 | Loss: 0.00001432
Iteration 69/1000 | Loss: 0.00001432
Iteration 70/1000 | Loss: 0.00001431
Iteration 71/1000 | Loss: 0.00001431
Iteration 72/1000 | Loss: 0.00001431
Iteration 73/1000 | Loss: 0.00001431
Iteration 74/1000 | Loss: 0.00001431
Iteration 75/1000 | Loss: 0.00001431
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001428
Iteration 90/1000 | Loss: 0.00001428
Iteration 91/1000 | Loss: 0.00001428
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001423
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4229584849090315e-05, 1.4229584849090315e-05, 1.4229584849090315e-05, 1.4229584849090315e-05, 1.4229584849090315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4229584849090315e-05

Optimization complete. Final v2v error: 3.254611015319824 mm

Highest mean error: 3.8573033809661865 mm for frame 170

Lowest mean error: 2.990429639816284 mm for frame 187

Saving results

Total time: 70.04391837120056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901606
Iteration 2/25 | Loss: 0.00196543
Iteration 3/25 | Loss: 0.00171253
Iteration 4/25 | Loss: 0.00168353
Iteration 5/25 | Loss: 0.00167861
Iteration 6/25 | Loss: 0.00167831
Iteration 7/25 | Loss: 0.00167831
Iteration 8/25 | Loss: 0.00167831
Iteration 9/25 | Loss: 0.00167831
Iteration 10/25 | Loss: 0.00167831
Iteration 11/25 | Loss: 0.00167831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001678312779404223, 0.001678312779404223, 0.001678312779404223, 0.001678312779404223, 0.001678312779404223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001678312779404223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67789412
Iteration 2/25 | Loss: 0.00140135
Iteration 3/25 | Loss: 0.00140135
Iteration 4/25 | Loss: 0.00140135
Iteration 5/25 | Loss: 0.00140135
Iteration 6/25 | Loss: 0.00140135
Iteration 7/25 | Loss: 0.00140134
Iteration 8/25 | Loss: 0.00140134
Iteration 9/25 | Loss: 0.00140134
Iteration 10/25 | Loss: 0.00140134
Iteration 11/25 | Loss: 0.00140134
Iteration 12/25 | Loss: 0.00140134
Iteration 13/25 | Loss: 0.00140134
Iteration 14/25 | Loss: 0.00140134
Iteration 15/25 | Loss: 0.00140134
Iteration 16/25 | Loss: 0.00140134
Iteration 17/25 | Loss: 0.00140134
Iteration 18/25 | Loss: 0.00140134
Iteration 19/25 | Loss: 0.00140134
Iteration 20/25 | Loss: 0.00140134
Iteration 21/25 | Loss: 0.00140134
Iteration 22/25 | Loss: 0.00140134
Iteration 23/25 | Loss: 0.00140134
Iteration 24/25 | Loss: 0.00140134
Iteration 25/25 | Loss: 0.00140134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140134
Iteration 2/1000 | Loss: 0.00006649
Iteration 3/1000 | Loss: 0.00004496
Iteration 4/1000 | Loss: 0.00003955
Iteration 5/1000 | Loss: 0.00003820
Iteration 6/1000 | Loss: 0.00003711
Iteration 7/1000 | Loss: 0.00003636
Iteration 8/1000 | Loss: 0.00003582
Iteration 9/1000 | Loss: 0.00003552
Iteration 10/1000 | Loss: 0.00003527
Iteration 11/1000 | Loss: 0.00003504
Iteration 12/1000 | Loss: 0.00003484
Iteration 13/1000 | Loss: 0.00003462
Iteration 14/1000 | Loss: 0.00003444
Iteration 15/1000 | Loss: 0.00003430
Iteration 16/1000 | Loss: 0.00003429
Iteration 17/1000 | Loss: 0.00003420
Iteration 18/1000 | Loss: 0.00003416
Iteration 19/1000 | Loss: 0.00003416
Iteration 20/1000 | Loss: 0.00003416
Iteration 21/1000 | Loss: 0.00003414
Iteration 22/1000 | Loss: 0.00003414
Iteration 23/1000 | Loss: 0.00003414
Iteration 24/1000 | Loss: 0.00003408
Iteration 25/1000 | Loss: 0.00003402
Iteration 26/1000 | Loss: 0.00003399
Iteration 27/1000 | Loss: 0.00003399
Iteration 28/1000 | Loss: 0.00003399
Iteration 29/1000 | Loss: 0.00003399
Iteration 30/1000 | Loss: 0.00003394
Iteration 31/1000 | Loss: 0.00003394
Iteration 32/1000 | Loss: 0.00003393
Iteration 33/1000 | Loss: 0.00003392
Iteration 34/1000 | Loss: 0.00003391
Iteration 35/1000 | Loss: 0.00003391
Iteration 36/1000 | Loss: 0.00003391
Iteration 37/1000 | Loss: 0.00003390
Iteration 38/1000 | Loss: 0.00003390
Iteration 39/1000 | Loss: 0.00003390
Iteration 40/1000 | Loss: 0.00003390
Iteration 41/1000 | Loss: 0.00003390
Iteration 42/1000 | Loss: 0.00003390
Iteration 43/1000 | Loss: 0.00003389
Iteration 44/1000 | Loss: 0.00003389
Iteration 45/1000 | Loss: 0.00003389
Iteration 46/1000 | Loss: 0.00003389
Iteration 47/1000 | Loss: 0.00003388
Iteration 48/1000 | Loss: 0.00003388
Iteration 49/1000 | Loss: 0.00003388
Iteration 50/1000 | Loss: 0.00003388
Iteration 51/1000 | Loss: 0.00003388
Iteration 52/1000 | Loss: 0.00003388
Iteration 53/1000 | Loss: 0.00003387
Iteration 54/1000 | Loss: 0.00003387
Iteration 55/1000 | Loss: 0.00003387
Iteration 56/1000 | Loss: 0.00003387
Iteration 57/1000 | Loss: 0.00003387
Iteration 58/1000 | Loss: 0.00003387
Iteration 59/1000 | Loss: 0.00003386
Iteration 60/1000 | Loss: 0.00003386
Iteration 61/1000 | Loss: 0.00003386
Iteration 62/1000 | Loss: 0.00003386
Iteration 63/1000 | Loss: 0.00003386
Iteration 64/1000 | Loss: 0.00003386
Iteration 65/1000 | Loss: 0.00003386
Iteration 66/1000 | Loss: 0.00003386
Iteration 67/1000 | Loss: 0.00003385
Iteration 68/1000 | Loss: 0.00003385
Iteration 69/1000 | Loss: 0.00003385
Iteration 70/1000 | Loss: 0.00003385
Iteration 71/1000 | Loss: 0.00003385
Iteration 72/1000 | Loss: 0.00003384
Iteration 73/1000 | Loss: 0.00003384
Iteration 74/1000 | Loss: 0.00003384
Iteration 75/1000 | Loss: 0.00003384
Iteration 76/1000 | Loss: 0.00003384
Iteration 77/1000 | Loss: 0.00003384
Iteration 78/1000 | Loss: 0.00003384
Iteration 79/1000 | Loss: 0.00003384
Iteration 80/1000 | Loss: 0.00003384
Iteration 81/1000 | Loss: 0.00003384
Iteration 82/1000 | Loss: 0.00003384
Iteration 83/1000 | Loss: 0.00003384
Iteration 84/1000 | Loss: 0.00003384
Iteration 85/1000 | Loss: 0.00003384
Iteration 86/1000 | Loss: 0.00003383
Iteration 87/1000 | Loss: 0.00003383
Iteration 88/1000 | Loss: 0.00003383
Iteration 89/1000 | Loss: 0.00003383
Iteration 90/1000 | Loss: 0.00003383
Iteration 91/1000 | Loss: 0.00003383
Iteration 92/1000 | Loss: 0.00003382
Iteration 93/1000 | Loss: 0.00003382
Iteration 94/1000 | Loss: 0.00003382
Iteration 95/1000 | Loss: 0.00003382
Iteration 96/1000 | Loss: 0.00003381
Iteration 97/1000 | Loss: 0.00003381
Iteration 98/1000 | Loss: 0.00003381
Iteration 99/1000 | Loss: 0.00003381
Iteration 100/1000 | Loss: 0.00003381
Iteration 101/1000 | Loss: 0.00003381
Iteration 102/1000 | Loss: 0.00003381
Iteration 103/1000 | Loss: 0.00003381
Iteration 104/1000 | Loss: 0.00003380
Iteration 105/1000 | Loss: 0.00003380
Iteration 106/1000 | Loss: 0.00003379
Iteration 107/1000 | Loss: 0.00003379
Iteration 108/1000 | Loss: 0.00003379
Iteration 109/1000 | Loss: 0.00003379
Iteration 110/1000 | Loss: 0.00003379
Iteration 111/1000 | Loss: 0.00003379
Iteration 112/1000 | Loss: 0.00003379
Iteration 113/1000 | Loss: 0.00003379
Iteration 114/1000 | Loss: 0.00003379
Iteration 115/1000 | Loss: 0.00003378
Iteration 116/1000 | Loss: 0.00003378
Iteration 117/1000 | Loss: 0.00003378
Iteration 118/1000 | Loss: 0.00003378
Iteration 119/1000 | Loss: 0.00003378
Iteration 120/1000 | Loss: 0.00003378
Iteration 121/1000 | Loss: 0.00003378
Iteration 122/1000 | Loss: 0.00003378
Iteration 123/1000 | Loss: 0.00003378
Iteration 124/1000 | Loss: 0.00003378
Iteration 125/1000 | Loss: 0.00003378
Iteration 126/1000 | Loss: 0.00003378
Iteration 127/1000 | Loss: 0.00003378
Iteration 128/1000 | Loss: 0.00003378
Iteration 129/1000 | Loss: 0.00003378
Iteration 130/1000 | Loss: 0.00003378
Iteration 131/1000 | Loss: 0.00003378
Iteration 132/1000 | Loss: 0.00003378
Iteration 133/1000 | Loss: 0.00003378
Iteration 134/1000 | Loss: 0.00003378
Iteration 135/1000 | Loss: 0.00003378
Iteration 136/1000 | Loss: 0.00003378
Iteration 137/1000 | Loss: 0.00003378
Iteration 138/1000 | Loss: 0.00003378
Iteration 139/1000 | Loss: 0.00003378
Iteration 140/1000 | Loss: 0.00003378
Iteration 141/1000 | Loss: 0.00003378
Iteration 142/1000 | Loss: 0.00003378
Iteration 143/1000 | Loss: 0.00003378
Iteration 144/1000 | Loss: 0.00003378
Iteration 145/1000 | Loss: 0.00003378
Iteration 146/1000 | Loss: 0.00003378
Iteration 147/1000 | Loss: 0.00003378
Iteration 148/1000 | Loss: 0.00003378
Iteration 149/1000 | Loss: 0.00003378
Iteration 150/1000 | Loss: 0.00003378
Iteration 151/1000 | Loss: 0.00003378
Iteration 152/1000 | Loss: 0.00003378
Iteration 153/1000 | Loss: 0.00003378
Iteration 154/1000 | Loss: 0.00003378
Iteration 155/1000 | Loss: 0.00003378
Iteration 156/1000 | Loss: 0.00003378
Iteration 157/1000 | Loss: 0.00003378
Iteration 158/1000 | Loss: 0.00003378
Iteration 159/1000 | Loss: 0.00003378
Iteration 160/1000 | Loss: 0.00003378
Iteration 161/1000 | Loss: 0.00003378
Iteration 162/1000 | Loss: 0.00003378
Iteration 163/1000 | Loss: 0.00003378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.377567554707639e-05, 3.377567554707639e-05, 3.377567554707639e-05, 3.377567554707639e-05, 3.377567554707639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.377567554707639e-05

Optimization complete. Final v2v error: 4.873579502105713 mm

Highest mean error: 5.0842790603637695 mm for frame 36

Lowest mean error: 4.525028705596924 mm for frame 0

Saving results

Total time: 41.55291438102722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00306440
Iteration 2/25 | Loss: 0.00170581
Iteration 3/25 | Loss: 0.00152585
Iteration 4/25 | Loss: 0.00149096
Iteration 5/25 | Loss: 0.00148112
Iteration 6/25 | Loss: 0.00147726
Iteration 7/25 | Loss: 0.00147591
Iteration 8/25 | Loss: 0.00147517
Iteration 9/25 | Loss: 0.00147476
Iteration 10/25 | Loss: 0.00147461
Iteration 11/25 | Loss: 0.00147450
Iteration 12/25 | Loss: 0.00147430
Iteration 13/25 | Loss: 0.00147664
Iteration 14/25 | Loss: 0.00147759
Iteration 15/25 | Loss: 0.00147707
Iteration 16/25 | Loss: 0.00147541
Iteration 17/25 | Loss: 0.00147608
Iteration 18/25 | Loss: 0.00147483
Iteration 19/25 | Loss: 0.00147309
Iteration 20/25 | Loss: 0.00147251
Iteration 21/25 | Loss: 0.00147005
Iteration 22/25 | Loss: 0.00146919
Iteration 23/25 | Loss: 0.00146726
Iteration 24/25 | Loss: 0.00146673
Iteration 25/25 | Loss: 0.00146847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21198177
Iteration 2/25 | Loss: 0.00352601
Iteration 3/25 | Loss: 0.00352601
Iteration 4/25 | Loss: 0.00352601
Iteration 5/25 | Loss: 0.00352601
Iteration 6/25 | Loss: 0.00352601
Iteration 7/25 | Loss: 0.00352601
Iteration 8/25 | Loss: 0.00352601
Iteration 9/25 | Loss: 0.00352601
Iteration 10/25 | Loss: 0.00352601
Iteration 11/25 | Loss: 0.00352601
Iteration 12/25 | Loss: 0.00352601
Iteration 13/25 | Loss: 0.00352601
Iteration 14/25 | Loss: 0.00352601
Iteration 15/25 | Loss: 0.00352601
Iteration 16/25 | Loss: 0.00352601
Iteration 17/25 | Loss: 0.00352601
Iteration 18/25 | Loss: 0.00352601
Iteration 19/25 | Loss: 0.00352601
Iteration 20/25 | Loss: 0.00352601
Iteration 21/25 | Loss: 0.00352601
Iteration 22/25 | Loss: 0.00352601
Iteration 23/25 | Loss: 0.00352601
Iteration 24/25 | Loss: 0.00352601
Iteration 25/25 | Loss: 0.00352601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00352601
Iteration 2/1000 | Loss: 0.00011698
Iteration 3/1000 | Loss: 0.00009143
Iteration 4/1000 | Loss: 0.00005668
Iteration 5/1000 | Loss: 0.00005429
Iteration 6/1000 | Loss: 0.00003234
Iteration 7/1000 | Loss: 0.00003269
Iteration 8/1000 | Loss: 0.00004189
Iteration 9/1000 | Loss: 0.00004652
Iteration 10/1000 | Loss: 0.00004316
Iteration 11/1000 | Loss: 0.00004552
Iteration 12/1000 | Loss: 0.00003672
Iteration 13/1000 | Loss: 0.00003820
Iteration 14/1000 | Loss: 0.00003500
Iteration 15/1000 | Loss: 0.00003414
Iteration 16/1000 | Loss: 0.00003674
Iteration 17/1000 | Loss: 0.00005184
Iteration 18/1000 | Loss: 0.00002745
Iteration 19/1000 | Loss: 0.00002826
Iteration 20/1000 | Loss: 0.00002963
Iteration 21/1000 | Loss: 0.00003871
Iteration 22/1000 | Loss: 0.00004281
Iteration 23/1000 | Loss: 0.00005553
Iteration 24/1000 | Loss: 0.00004381
Iteration 25/1000 | Loss: 0.00004546
Iteration 26/1000 | Loss: 0.00004485
Iteration 27/1000 | Loss: 0.00005151
Iteration 28/1000 | Loss: 0.00004534
Iteration 29/1000 | Loss: 0.00004562
Iteration 30/1000 | Loss: 0.00002869
Iteration 31/1000 | Loss: 0.00002985
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002681
Iteration 34/1000 | Loss: 0.00002241
Iteration 35/1000 | Loss: 0.00002341
Iteration 36/1000 | Loss: 0.00002690
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002905
Iteration 39/1000 | Loss: 0.00002743
Iteration 40/1000 | Loss: 0.00002657
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002793
Iteration 43/1000 | Loss: 0.00002506
Iteration 44/1000 | Loss: 0.00003707
Iteration 45/1000 | Loss: 0.00002188
Iteration 46/1000 | Loss: 0.00002937
Iteration 47/1000 | Loss: 0.00003307
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001641
Iteration 52/1000 | Loss: 0.00001600
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001586
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001544
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001536
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001528
Iteration 78/1000 | Loss: 0.00001528
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001515
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001514
Iteration 122/1000 | Loss: 0.00001514
Iteration 123/1000 | Loss: 0.00001514
Iteration 124/1000 | Loss: 0.00001514
Iteration 125/1000 | Loss: 0.00001514
Iteration 126/1000 | Loss: 0.00001514
Iteration 127/1000 | Loss: 0.00001514
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001514
Iteration 131/1000 | Loss: 0.00001514
Iteration 132/1000 | Loss: 0.00001514
Iteration 133/1000 | Loss: 0.00001514
Iteration 134/1000 | Loss: 0.00001514
Iteration 135/1000 | Loss: 0.00001512
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001511
Iteration 145/1000 | Loss: 0.00001510
Iteration 146/1000 | Loss: 0.00001510
Iteration 147/1000 | Loss: 0.00001510
Iteration 148/1000 | Loss: 0.00001510
Iteration 149/1000 | Loss: 0.00001510
Iteration 150/1000 | Loss: 0.00001510
Iteration 151/1000 | Loss: 0.00001510
Iteration 152/1000 | Loss: 0.00001510
Iteration 153/1000 | Loss: 0.00001509
Iteration 154/1000 | Loss: 0.00001509
Iteration 155/1000 | Loss: 0.00001506
Iteration 156/1000 | Loss: 0.00001506
Iteration 157/1000 | Loss: 0.00001505
Iteration 158/1000 | Loss: 0.00001505
Iteration 159/1000 | Loss: 0.00001504
Iteration 160/1000 | Loss: 0.00001503
Iteration 161/1000 | Loss: 0.00001502
Iteration 162/1000 | Loss: 0.00001502
Iteration 163/1000 | Loss: 0.00001502
Iteration 164/1000 | Loss: 0.00001502
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001500
Iteration 170/1000 | Loss: 0.00001500
Iteration 171/1000 | Loss: 0.00001500
Iteration 172/1000 | Loss: 0.00001500
Iteration 173/1000 | Loss: 0.00001500
Iteration 174/1000 | Loss: 0.00001500
Iteration 175/1000 | Loss: 0.00001500
Iteration 176/1000 | Loss: 0.00001500
Iteration 177/1000 | Loss: 0.00001499
Iteration 178/1000 | Loss: 0.00001499
Iteration 179/1000 | Loss: 0.00001499
Iteration 180/1000 | Loss: 0.00001499
Iteration 181/1000 | Loss: 0.00001499
Iteration 182/1000 | Loss: 0.00001499
Iteration 183/1000 | Loss: 0.00001499
Iteration 184/1000 | Loss: 0.00001499
Iteration 185/1000 | Loss: 0.00001499
Iteration 186/1000 | Loss: 0.00001499
Iteration 187/1000 | Loss: 0.00001499
Iteration 188/1000 | Loss: 0.00001499
Iteration 189/1000 | Loss: 0.00001499
Iteration 190/1000 | Loss: 0.00001499
Iteration 191/1000 | Loss: 0.00001499
Iteration 192/1000 | Loss: 0.00001499
Iteration 193/1000 | Loss: 0.00001499
Iteration 194/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.498735036875587e-05, 1.498735036875587e-05, 1.498735036875587e-05, 1.498735036875587e-05, 1.498735036875587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.498735036875587e-05

Optimization complete. Final v2v error: 3.3539228439331055 mm

Highest mean error: 4.204986095428467 mm for frame 48

Lowest mean error: 3.0873446464538574 mm for frame 120

Saving results

Total time: 138.77099657058716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813804
Iteration 2/25 | Loss: 0.00180345
Iteration 3/25 | Loss: 0.00157522
Iteration 4/25 | Loss: 0.00152921
Iteration 5/25 | Loss: 0.00151849
Iteration 6/25 | Loss: 0.00151646
Iteration 7/25 | Loss: 0.00151628
Iteration 8/25 | Loss: 0.00151430
Iteration 9/25 | Loss: 0.00151613
Iteration 10/25 | Loss: 0.00151124
Iteration 11/25 | Loss: 0.00150991
Iteration 12/25 | Loss: 0.00150961
Iteration 13/25 | Loss: 0.00151020
Iteration 14/25 | Loss: 0.00150901
Iteration 15/25 | Loss: 0.00150859
Iteration 16/25 | Loss: 0.00150848
Iteration 17/25 | Loss: 0.00150846
Iteration 18/25 | Loss: 0.00150845
Iteration 19/25 | Loss: 0.00150844
Iteration 20/25 | Loss: 0.00150843
Iteration 21/25 | Loss: 0.00150843
Iteration 22/25 | Loss: 0.00150843
Iteration 23/25 | Loss: 0.00150843
Iteration 24/25 | Loss: 0.00150843
Iteration 25/25 | Loss: 0.00150843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.73061728
Iteration 2/25 | Loss: 0.00228535
Iteration 3/25 | Loss: 0.00228535
Iteration 4/25 | Loss: 0.00228535
Iteration 5/25 | Loss: 0.00228535
Iteration 6/25 | Loss: 0.00228535
Iteration 7/25 | Loss: 0.00228535
Iteration 8/25 | Loss: 0.00228535
Iteration 9/25 | Loss: 0.00228534
Iteration 10/25 | Loss: 0.00228534
Iteration 11/25 | Loss: 0.00228534
Iteration 12/25 | Loss: 0.00228534
Iteration 13/25 | Loss: 0.00228534
Iteration 14/25 | Loss: 0.00228534
Iteration 15/25 | Loss: 0.00228534
Iteration 16/25 | Loss: 0.00228534
Iteration 17/25 | Loss: 0.00228534
Iteration 18/25 | Loss: 0.00228534
Iteration 19/25 | Loss: 0.00228534
Iteration 20/25 | Loss: 0.00228534
Iteration 21/25 | Loss: 0.00228534
Iteration 22/25 | Loss: 0.00228534
Iteration 23/25 | Loss: 0.00228534
Iteration 24/25 | Loss: 0.00228534
Iteration 25/25 | Loss: 0.00228534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228534
Iteration 2/1000 | Loss: 0.00002728
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002132
Iteration 5/1000 | Loss: 0.00007939
Iteration 6/1000 | Loss: 0.00001995
Iteration 7/1000 | Loss: 0.00005028
Iteration 8/1000 | Loss: 0.00001913
Iteration 9/1000 | Loss: 0.00008082
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001784
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001705
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00007656
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001688
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001688
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001687
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001687
Iteration 30/1000 | Loss: 0.00001686
Iteration 31/1000 | Loss: 0.00001685
Iteration 32/1000 | Loss: 0.00005101
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001681
Iteration 40/1000 | Loss: 0.00001681
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001680
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00005310
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001851
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001673
Iteration 60/1000 | Loss: 0.00001671
Iteration 61/1000 | Loss: 0.00001670
Iteration 62/1000 | Loss: 0.00001670
Iteration 63/1000 | Loss: 0.00004563
Iteration 64/1000 | Loss: 0.00001918
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001673
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001672
Iteration 93/1000 | Loss: 0.00001672
Iteration 94/1000 | Loss: 0.00001672
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001672
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001671
Iteration 104/1000 | Loss: 0.00001671
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001670
Iteration 107/1000 | Loss: 0.00001670
Iteration 108/1000 | Loss: 0.00001670
Iteration 109/1000 | Loss: 0.00001670
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001669
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001668
Iteration 130/1000 | Loss: 0.00001668
Iteration 131/1000 | Loss: 0.00001668
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001667
Iteration 135/1000 | Loss: 0.00001667
Iteration 136/1000 | Loss: 0.00001667
Iteration 137/1000 | Loss: 0.00001667
Iteration 138/1000 | Loss: 0.00001666
Iteration 139/1000 | Loss: 0.00001666
Iteration 140/1000 | Loss: 0.00001666
Iteration 141/1000 | Loss: 0.00001666
Iteration 142/1000 | Loss: 0.00001666
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001665
Iteration 149/1000 | Loss: 0.00001665
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001664
Iteration 152/1000 | Loss: 0.00001664
Iteration 153/1000 | Loss: 0.00004795
Iteration 154/1000 | Loss: 0.00001689
Iteration 155/1000 | Loss: 0.00002280
Iteration 156/1000 | Loss: 0.00001827
Iteration 157/1000 | Loss: 0.00002175
Iteration 158/1000 | Loss: 0.00002463
Iteration 159/1000 | Loss: 0.00001668
Iteration 160/1000 | Loss: 0.00001667
Iteration 161/1000 | Loss: 0.00001667
Iteration 162/1000 | Loss: 0.00001667
Iteration 163/1000 | Loss: 0.00001667
Iteration 164/1000 | Loss: 0.00001667
Iteration 165/1000 | Loss: 0.00001667
Iteration 166/1000 | Loss: 0.00001667
Iteration 167/1000 | Loss: 0.00001667
Iteration 168/1000 | Loss: 0.00001667
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001666
Iteration 171/1000 | Loss: 0.00001666
Iteration 172/1000 | Loss: 0.00001666
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Iteration 175/1000 | Loss: 0.00001666
Iteration 176/1000 | Loss: 0.00001666
Iteration 177/1000 | Loss: 0.00001666
Iteration 178/1000 | Loss: 0.00001666
Iteration 179/1000 | Loss: 0.00001666
Iteration 180/1000 | Loss: 0.00001666
Iteration 181/1000 | Loss: 0.00001666
Iteration 182/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.666045318415854e-05, 1.666045318415854e-05, 1.666045318415854e-05, 1.666045318415854e-05, 1.666045318415854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.666045318415854e-05

Optimization complete. Final v2v error: 3.496833562850952 mm

Highest mean error: 4.520843029022217 mm for frame 55

Lowest mean error: 3.144566297531128 mm for frame 238

Saving results

Total time: 95.66156697273254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046672
Iteration 2/25 | Loss: 0.00297203
Iteration 3/25 | Loss: 0.00249835
Iteration 4/25 | Loss: 0.00172227
Iteration 5/25 | Loss: 0.00166518
Iteration 6/25 | Loss: 0.00165077
Iteration 7/25 | Loss: 0.00163959
Iteration 8/25 | Loss: 0.00163200
Iteration 9/25 | Loss: 0.00164898
Iteration 10/25 | Loss: 0.00163959
Iteration 11/25 | Loss: 0.00160590
Iteration 12/25 | Loss: 0.00160318
Iteration 13/25 | Loss: 0.00160164
Iteration 14/25 | Loss: 0.00160041
Iteration 15/25 | Loss: 0.00159934
Iteration 16/25 | Loss: 0.00159896
Iteration 17/25 | Loss: 0.00159887
Iteration 18/25 | Loss: 0.00159884
Iteration 19/25 | Loss: 0.00159883
Iteration 20/25 | Loss: 0.00159883
Iteration 21/25 | Loss: 0.00159883
Iteration 22/25 | Loss: 0.00159883
Iteration 23/25 | Loss: 0.00159882
Iteration 24/25 | Loss: 0.00159882
Iteration 25/25 | Loss: 0.00159882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90853190
Iteration 2/25 | Loss: 0.00249556
Iteration 3/25 | Loss: 0.00249556
Iteration 4/25 | Loss: 0.00249556
Iteration 5/25 | Loss: 0.00249556
Iteration 6/25 | Loss: 0.00249556
Iteration 7/25 | Loss: 0.00249556
Iteration 8/25 | Loss: 0.00249556
Iteration 9/25 | Loss: 0.00249556
Iteration 10/25 | Loss: 0.00249556
Iteration 11/25 | Loss: 0.00249556
Iteration 12/25 | Loss: 0.00249556
Iteration 13/25 | Loss: 0.00249556
Iteration 14/25 | Loss: 0.00249556
Iteration 15/25 | Loss: 0.00249556
Iteration 16/25 | Loss: 0.00249556
Iteration 17/25 | Loss: 0.00249556
Iteration 18/25 | Loss: 0.00249556
Iteration 19/25 | Loss: 0.00249556
Iteration 20/25 | Loss: 0.00249556
Iteration 21/25 | Loss: 0.00249556
Iteration 22/25 | Loss: 0.00249556
Iteration 23/25 | Loss: 0.00249556
Iteration 24/25 | Loss: 0.00249556
Iteration 25/25 | Loss: 0.00249556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00249556
Iteration 2/1000 | Loss: 0.00005736
Iteration 3/1000 | Loss: 0.00003985
Iteration 4/1000 | Loss: 0.00003347
Iteration 5/1000 | Loss: 0.00003091
Iteration 6/1000 | Loss: 0.00002981
Iteration 7/1000 | Loss: 0.00002879
Iteration 8/1000 | Loss: 0.00002818
Iteration 9/1000 | Loss: 0.00002760
Iteration 10/1000 | Loss: 0.00002730
Iteration 11/1000 | Loss: 0.00002691
Iteration 12/1000 | Loss: 0.00002667
Iteration 13/1000 | Loss: 0.00002641
Iteration 14/1000 | Loss: 0.00002622
Iteration 15/1000 | Loss: 0.00002602
Iteration 16/1000 | Loss: 0.00002589
Iteration 17/1000 | Loss: 0.00002586
Iteration 18/1000 | Loss: 0.00002567
Iteration 19/1000 | Loss: 0.00002566
Iteration 20/1000 | Loss: 0.00002558
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00002554
Iteration 23/1000 | Loss: 0.00002553
Iteration 24/1000 | Loss: 0.00002551
Iteration 25/1000 | Loss: 0.00002551
Iteration 26/1000 | Loss: 0.00002549
Iteration 27/1000 | Loss: 0.00002548
Iteration 28/1000 | Loss: 0.00002548
Iteration 29/1000 | Loss: 0.00002547
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002544
Iteration 32/1000 | Loss: 0.00002541
Iteration 33/1000 | Loss: 0.00002540
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00002539
Iteration 36/1000 | Loss: 0.00002539
Iteration 37/1000 | Loss: 0.00002538
Iteration 38/1000 | Loss: 0.00002535
Iteration 39/1000 | Loss: 0.00002531
Iteration 40/1000 | Loss: 0.00002529
Iteration 41/1000 | Loss: 0.00002528
Iteration 42/1000 | Loss: 0.00002527
Iteration 43/1000 | Loss: 0.00002527
Iteration 44/1000 | Loss: 0.00002527
Iteration 45/1000 | Loss: 0.00002527
Iteration 46/1000 | Loss: 0.00002527
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002526
Iteration 49/1000 | Loss: 0.00002526
Iteration 50/1000 | Loss: 0.00002526
Iteration 51/1000 | Loss: 0.00002525
Iteration 52/1000 | Loss: 0.00002525
Iteration 53/1000 | Loss: 0.00002525
Iteration 54/1000 | Loss: 0.00002525
Iteration 55/1000 | Loss: 0.00002525
Iteration 56/1000 | Loss: 0.00002524
Iteration 57/1000 | Loss: 0.00002524
Iteration 58/1000 | Loss: 0.00002524
Iteration 59/1000 | Loss: 0.00002524
Iteration 60/1000 | Loss: 0.00002523
Iteration 61/1000 | Loss: 0.00002523
Iteration 62/1000 | Loss: 0.00002523
Iteration 63/1000 | Loss: 0.00002523
Iteration 64/1000 | Loss: 0.00002523
Iteration 65/1000 | Loss: 0.00002523
Iteration 66/1000 | Loss: 0.00002523
Iteration 67/1000 | Loss: 0.00002522
Iteration 68/1000 | Loss: 0.00002522
Iteration 69/1000 | Loss: 0.00002522
Iteration 70/1000 | Loss: 0.00002522
Iteration 71/1000 | Loss: 0.00002521
Iteration 72/1000 | Loss: 0.00002521
Iteration 73/1000 | Loss: 0.00002521
Iteration 74/1000 | Loss: 0.00002521
Iteration 75/1000 | Loss: 0.00002520
Iteration 76/1000 | Loss: 0.00002520
Iteration 77/1000 | Loss: 0.00002520
Iteration 78/1000 | Loss: 0.00002520
Iteration 79/1000 | Loss: 0.00002520
Iteration 80/1000 | Loss: 0.00002520
Iteration 81/1000 | Loss: 0.00002520
Iteration 82/1000 | Loss: 0.00002520
Iteration 83/1000 | Loss: 0.00002519
Iteration 84/1000 | Loss: 0.00002519
Iteration 85/1000 | Loss: 0.00002519
Iteration 86/1000 | Loss: 0.00002519
Iteration 87/1000 | Loss: 0.00002519
Iteration 88/1000 | Loss: 0.00002519
Iteration 89/1000 | Loss: 0.00002518
Iteration 90/1000 | Loss: 0.00002518
Iteration 91/1000 | Loss: 0.00002518
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002518
Iteration 95/1000 | Loss: 0.00002517
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002517
Iteration 98/1000 | Loss: 0.00002517
Iteration 99/1000 | Loss: 0.00002517
Iteration 100/1000 | Loss: 0.00002517
Iteration 101/1000 | Loss: 0.00002517
Iteration 102/1000 | Loss: 0.00002517
Iteration 103/1000 | Loss: 0.00002517
Iteration 104/1000 | Loss: 0.00002517
Iteration 105/1000 | Loss: 0.00002516
Iteration 106/1000 | Loss: 0.00002516
Iteration 107/1000 | Loss: 0.00002516
Iteration 108/1000 | Loss: 0.00002516
Iteration 109/1000 | Loss: 0.00002516
Iteration 110/1000 | Loss: 0.00002516
Iteration 111/1000 | Loss: 0.00002516
Iteration 112/1000 | Loss: 0.00002516
Iteration 113/1000 | Loss: 0.00002516
Iteration 114/1000 | Loss: 0.00002516
Iteration 115/1000 | Loss: 0.00002515
Iteration 116/1000 | Loss: 0.00002515
Iteration 117/1000 | Loss: 0.00002515
Iteration 118/1000 | Loss: 0.00002515
Iteration 119/1000 | Loss: 0.00002515
Iteration 120/1000 | Loss: 0.00002515
Iteration 121/1000 | Loss: 0.00002515
Iteration 122/1000 | Loss: 0.00002515
Iteration 123/1000 | Loss: 0.00002515
Iteration 124/1000 | Loss: 0.00002515
Iteration 125/1000 | Loss: 0.00002515
Iteration 126/1000 | Loss: 0.00002515
Iteration 127/1000 | Loss: 0.00002515
Iteration 128/1000 | Loss: 0.00002514
Iteration 129/1000 | Loss: 0.00002514
Iteration 130/1000 | Loss: 0.00002514
Iteration 131/1000 | Loss: 0.00002514
Iteration 132/1000 | Loss: 0.00002514
Iteration 133/1000 | Loss: 0.00002514
Iteration 134/1000 | Loss: 0.00002514
Iteration 135/1000 | Loss: 0.00002514
Iteration 136/1000 | Loss: 0.00002514
Iteration 137/1000 | Loss: 0.00002514
Iteration 138/1000 | Loss: 0.00002514
Iteration 139/1000 | Loss: 0.00002514
Iteration 140/1000 | Loss: 0.00002514
Iteration 141/1000 | Loss: 0.00002514
Iteration 142/1000 | Loss: 0.00002514
Iteration 143/1000 | Loss: 0.00002514
Iteration 144/1000 | Loss: 0.00002514
Iteration 145/1000 | Loss: 0.00002514
Iteration 146/1000 | Loss: 0.00002514
Iteration 147/1000 | Loss: 0.00002514
Iteration 148/1000 | Loss: 0.00002514
Iteration 149/1000 | Loss: 0.00002514
Iteration 150/1000 | Loss: 0.00002514
Iteration 151/1000 | Loss: 0.00002514
Iteration 152/1000 | Loss: 0.00002514
Iteration 153/1000 | Loss: 0.00002514
Iteration 154/1000 | Loss: 0.00002514
Iteration 155/1000 | Loss: 0.00002514
Iteration 156/1000 | Loss: 0.00002514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.513937943149358e-05, 2.513937943149358e-05, 2.513937943149358e-05, 2.513937943149358e-05, 2.513937943149358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.513937943149358e-05

Optimization complete. Final v2v error: 4.183683395385742 mm

Highest mean error: 4.84466552734375 mm for frame 120

Lowest mean error: 3.502549171447754 mm for frame 27

Saving results

Total time: 65.7111029624939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622951
Iteration 2/25 | Loss: 0.00153990
Iteration 3/25 | Loss: 0.00147427
Iteration 4/25 | Loss: 0.00146566
Iteration 5/25 | Loss: 0.00146257
Iteration 6/25 | Loss: 0.00146181
Iteration 7/25 | Loss: 0.00146181
Iteration 8/25 | Loss: 0.00146181
Iteration 9/25 | Loss: 0.00146181
Iteration 10/25 | Loss: 0.00146181
Iteration 11/25 | Loss: 0.00146181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014618135755881667, 0.0014618135755881667, 0.0014618135755881667, 0.0014618135755881667, 0.0014618135755881667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014618135755881667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27285779
Iteration 2/25 | Loss: 0.00232189
Iteration 3/25 | Loss: 0.00232188
Iteration 4/25 | Loss: 0.00232188
Iteration 5/25 | Loss: 0.00232188
Iteration 6/25 | Loss: 0.00232188
Iteration 7/25 | Loss: 0.00232188
Iteration 8/25 | Loss: 0.00232188
Iteration 9/25 | Loss: 0.00232188
Iteration 10/25 | Loss: 0.00232188
Iteration 11/25 | Loss: 0.00232188
Iteration 12/25 | Loss: 0.00232188
Iteration 13/25 | Loss: 0.00232188
Iteration 14/25 | Loss: 0.00232188
Iteration 15/25 | Loss: 0.00232188
Iteration 16/25 | Loss: 0.00232188
Iteration 17/25 | Loss: 0.00232188
Iteration 18/25 | Loss: 0.00232188
Iteration 19/25 | Loss: 0.00232188
Iteration 20/25 | Loss: 0.00232188
Iteration 21/25 | Loss: 0.00232188
Iteration 22/25 | Loss: 0.00232188
Iteration 23/25 | Loss: 0.00232188
Iteration 24/25 | Loss: 0.00232188
Iteration 25/25 | Loss: 0.00232188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232188
Iteration 2/1000 | Loss: 0.00002467
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001205
Iteration 15/1000 | Loss: 0.00001202
Iteration 16/1000 | Loss: 0.00001196
Iteration 17/1000 | Loss: 0.00001192
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001180
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001179
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001176
Iteration 29/1000 | Loss: 0.00001176
Iteration 30/1000 | Loss: 0.00001176
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001175
Iteration 34/1000 | Loss: 0.00001174
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001167
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001165
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001161
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001158
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001150
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001148
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001147
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001143
Iteration 147/1000 | Loss: 0.00001143
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001142
Iteration 151/1000 | Loss: 0.00001142
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001140
Iteration 156/1000 | Loss: 0.00001140
Iteration 157/1000 | Loss: 0.00001140
Iteration 158/1000 | Loss: 0.00001140
Iteration 159/1000 | Loss: 0.00001140
Iteration 160/1000 | Loss: 0.00001140
Iteration 161/1000 | Loss: 0.00001140
Iteration 162/1000 | Loss: 0.00001140
Iteration 163/1000 | Loss: 0.00001140
Iteration 164/1000 | Loss: 0.00001140
Iteration 165/1000 | Loss: 0.00001140
Iteration 166/1000 | Loss: 0.00001140
Iteration 167/1000 | Loss: 0.00001140
Iteration 168/1000 | Loss: 0.00001140
Iteration 169/1000 | Loss: 0.00001140
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001140
Iteration 173/1000 | Loss: 0.00001140
Iteration 174/1000 | Loss: 0.00001140
Iteration 175/1000 | Loss: 0.00001140
Iteration 176/1000 | Loss: 0.00001140
Iteration 177/1000 | Loss: 0.00001140
Iteration 178/1000 | Loss: 0.00001140
Iteration 179/1000 | Loss: 0.00001140
Iteration 180/1000 | Loss: 0.00001140
Iteration 181/1000 | Loss: 0.00001140
Iteration 182/1000 | Loss: 0.00001140
Iteration 183/1000 | Loss: 0.00001140
Iteration 184/1000 | Loss: 0.00001140
Iteration 185/1000 | Loss: 0.00001140
Iteration 186/1000 | Loss: 0.00001140
Iteration 187/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.1398864444345236e-05, 1.1398864444345236e-05, 1.1398864444345236e-05, 1.1398864444345236e-05, 1.1398864444345236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1398864444345236e-05

Optimization complete. Final v2v error: 2.9347078800201416 mm

Highest mean error: 3.595510721206665 mm for frame 80

Lowest mean error: 2.763270854949951 mm for frame 105

Saving results

Total time: 39.18094778060913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898499
Iteration 2/25 | Loss: 0.00168111
Iteration 3/25 | Loss: 0.00159419
Iteration 4/25 | Loss: 0.00158463
Iteration 5/25 | Loss: 0.00158171
Iteration 6/25 | Loss: 0.00158171
Iteration 7/25 | Loss: 0.00158171
Iteration 8/25 | Loss: 0.00158171
Iteration 9/25 | Loss: 0.00158171
Iteration 10/25 | Loss: 0.00158171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015817080857232213, 0.0015817080857232213, 0.0015817080857232213, 0.0015817080857232213, 0.0015817080857232213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015817080857232213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90347707
Iteration 2/25 | Loss: 0.00243134
Iteration 3/25 | Loss: 0.00243130
Iteration 4/25 | Loss: 0.00243130
Iteration 5/25 | Loss: 0.00243130
Iteration 6/25 | Loss: 0.00243130
Iteration 7/25 | Loss: 0.00243130
Iteration 8/25 | Loss: 0.00243130
Iteration 9/25 | Loss: 0.00243130
Iteration 10/25 | Loss: 0.00243130
Iteration 11/25 | Loss: 0.00243130
Iteration 12/25 | Loss: 0.00243130
Iteration 13/25 | Loss: 0.00243130
Iteration 14/25 | Loss: 0.00243130
Iteration 15/25 | Loss: 0.00243130
Iteration 16/25 | Loss: 0.00243130
Iteration 17/25 | Loss: 0.00243130
Iteration 18/25 | Loss: 0.00243130
Iteration 19/25 | Loss: 0.00243130
Iteration 20/25 | Loss: 0.00243130
Iteration 21/25 | Loss: 0.00243130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002431298140436411, 0.002431298140436411, 0.002431298140436411, 0.002431298140436411, 0.002431298140436411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002431298140436411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243130
Iteration 2/1000 | Loss: 0.00005803
Iteration 3/1000 | Loss: 0.00003322
Iteration 4/1000 | Loss: 0.00002936
Iteration 5/1000 | Loss: 0.00002746
Iteration 6/1000 | Loss: 0.00002622
Iteration 7/1000 | Loss: 0.00002559
Iteration 8/1000 | Loss: 0.00002508
Iteration 9/1000 | Loss: 0.00002468
Iteration 10/1000 | Loss: 0.00002419
Iteration 11/1000 | Loss: 0.00002387
Iteration 12/1000 | Loss: 0.00002361
Iteration 13/1000 | Loss: 0.00002341
Iteration 14/1000 | Loss: 0.00002321
Iteration 15/1000 | Loss: 0.00002305
Iteration 16/1000 | Loss: 0.00002305
Iteration 17/1000 | Loss: 0.00002295
Iteration 18/1000 | Loss: 0.00002282
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002267
Iteration 22/1000 | Loss: 0.00002265
Iteration 23/1000 | Loss: 0.00002265
Iteration 24/1000 | Loss: 0.00002258
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002251
Iteration 27/1000 | Loss: 0.00002250
Iteration 28/1000 | Loss: 0.00002248
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002248
Iteration 31/1000 | Loss: 0.00002248
Iteration 32/1000 | Loss: 0.00002247
Iteration 33/1000 | Loss: 0.00002246
Iteration 34/1000 | Loss: 0.00002246
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00002246
Iteration 37/1000 | Loss: 0.00002246
Iteration 38/1000 | Loss: 0.00002246
Iteration 39/1000 | Loss: 0.00002246
Iteration 40/1000 | Loss: 0.00002246
Iteration 41/1000 | Loss: 0.00002246
Iteration 42/1000 | Loss: 0.00002246
Iteration 43/1000 | Loss: 0.00002245
Iteration 44/1000 | Loss: 0.00002245
Iteration 45/1000 | Loss: 0.00002244
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002243
Iteration 50/1000 | Loss: 0.00002242
Iteration 51/1000 | Loss: 0.00002242
Iteration 52/1000 | Loss: 0.00002242
Iteration 53/1000 | Loss: 0.00002241
Iteration 54/1000 | Loss: 0.00002240
Iteration 55/1000 | Loss: 0.00002240
Iteration 56/1000 | Loss: 0.00002240
Iteration 57/1000 | Loss: 0.00002240
Iteration 58/1000 | Loss: 0.00002240
Iteration 59/1000 | Loss: 0.00002240
Iteration 60/1000 | Loss: 0.00002240
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002237
Iteration 70/1000 | Loss: 0.00002237
Iteration 71/1000 | Loss: 0.00002237
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002235
Iteration 82/1000 | Loss: 0.00002235
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00002234
Iteration 85/1000 | Loss: 0.00002234
Iteration 86/1000 | Loss: 0.00002234
Iteration 87/1000 | Loss: 0.00002234
Iteration 88/1000 | Loss: 0.00002234
Iteration 89/1000 | Loss: 0.00002234
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002233
Iteration 102/1000 | Loss: 0.00002233
Iteration 103/1000 | Loss: 0.00002233
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002232
Iteration 109/1000 | Loss: 0.00002232
Iteration 110/1000 | Loss: 0.00002232
Iteration 111/1000 | Loss: 0.00002232
Iteration 112/1000 | Loss: 0.00002232
Iteration 113/1000 | Loss: 0.00002232
Iteration 114/1000 | Loss: 0.00002232
Iteration 115/1000 | Loss: 0.00002232
Iteration 116/1000 | Loss: 0.00002232
Iteration 117/1000 | Loss: 0.00002231
Iteration 118/1000 | Loss: 0.00002231
Iteration 119/1000 | Loss: 0.00002231
Iteration 120/1000 | Loss: 0.00002231
Iteration 121/1000 | Loss: 0.00002231
Iteration 122/1000 | Loss: 0.00002231
Iteration 123/1000 | Loss: 0.00002231
Iteration 124/1000 | Loss: 0.00002231
Iteration 125/1000 | Loss: 0.00002231
Iteration 126/1000 | Loss: 0.00002230
Iteration 127/1000 | Loss: 0.00002230
Iteration 128/1000 | Loss: 0.00002230
Iteration 129/1000 | Loss: 0.00002230
Iteration 130/1000 | Loss: 0.00002230
Iteration 131/1000 | Loss: 0.00002230
Iteration 132/1000 | Loss: 0.00002230
Iteration 133/1000 | Loss: 0.00002229
Iteration 134/1000 | Loss: 0.00002229
Iteration 135/1000 | Loss: 0.00002229
Iteration 136/1000 | Loss: 0.00002229
Iteration 137/1000 | Loss: 0.00002229
Iteration 138/1000 | Loss: 0.00002228
Iteration 139/1000 | Loss: 0.00002228
Iteration 140/1000 | Loss: 0.00002228
Iteration 141/1000 | Loss: 0.00002228
Iteration 142/1000 | Loss: 0.00002228
Iteration 143/1000 | Loss: 0.00002228
Iteration 144/1000 | Loss: 0.00002228
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002228
Iteration 148/1000 | Loss: 0.00002228
Iteration 149/1000 | Loss: 0.00002227
Iteration 150/1000 | Loss: 0.00002227
Iteration 151/1000 | Loss: 0.00002227
Iteration 152/1000 | Loss: 0.00002227
Iteration 153/1000 | Loss: 0.00002227
Iteration 154/1000 | Loss: 0.00002227
Iteration 155/1000 | Loss: 0.00002227
Iteration 156/1000 | Loss: 0.00002227
Iteration 157/1000 | Loss: 0.00002227
Iteration 158/1000 | Loss: 0.00002226
Iteration 159/1000 | Loss: 0.00002226
Iteration 160/1000 | Loss: 0.00002226
Iteration 161/1000 | Loss: 0.00002226
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Iteration 165/1000 | Loss: 0.00002225
Iteration 166/1000 | Loss: 0.00002225
Iteration 167/1000 | Loss: 0.00002225
Iteration 168/1000 | Loss: 0.00002225
Iteration 169/1000 | Loss: 0.00002225
Iteration 170/1000 | Loss: 0.00002225
Iteration 171/1000 | Loss: 0.00002224
Iteration 172/1000 | Loss: 0.00002224
Iteration 173/1000 | Loss: 0.00002224
Iteration 174/1000 | Loss: 0.00002224
Iteration 175/1000 | Loss: 0.00002224
Iteration 176/1000 | Loss: 0.00002224
Iteration 177/1000 | Loss: 0.00002224
Iteration 178/1000 | Loss: 0.00002223
Iteration 179/1000 | Loss: 0.00002223
Iteration 180/1000 | Loss: 0.00002223
Iteration 181/1000 | Loss: 0.00002223
Iteration 182/1000 | Loss: 0.00002223
Iteration 183/1000 | Loss: 0.00002223
Iteration 184/1000 | Loss: 0.00002223
Iteration 185/1000 | Loss: 0.00002223
Iteration 186/1000 | Loss: 0.00002223
Iteration 187/1000 | Loss: 0.00002223
Iteration 188/1000 | Loss: 0.00002223
Iteration 189/1000 | Loss: 0.00002223
Iteration 190/1000 | Loss: 0.00002223
Iteration 191/1000 | Loss: 0.00002223
Iteration 192/1000 | Loss: 0.00002223
Iteration 193/1000 | Loss: 0.00002223
Iteration 194/1000 | Loss: 0.00002222
Iteration 195/1000 | Loss: 0.00002222
Iteration 196/1000 | Loss: 0.00002222
Iteration 197/1000 | Loss: 0.00002222
Iteration 198/1000 | Loss: 0.00002222
Iteration 199/1000 | Loss: 0.00002222
Iteration 200/1000 | Loss: 0.00002222
Iteration 201/1000 | Loss: 0.00002221
Iteration 202/1000 | Loss: 0.00002221
Iteration 203/1000 | Loss: 0.00002221
Iteration 204/1000 | Loss: 0.00002221
Iteration 205/1000 | Loss: 0.00002221
Iteration 206/1000 | Loss: 0.00002221
Iteration 207/1000 | Loss: 0.00002221
Iteration 208/1000 | Loss: 0.00002221
Iteration 209/1000 | Loss: 0.00002220
Iteration 210/1000 | Loss: 0.00002220
Iteration 211/1000 | Loss: 0.00002220
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002220
Iteration 214/1000 | Loss: 0.00002219
Iteration 215/1000 | Loss: 0.00002219
Iteration 216/1000 | Loss: 0.00002219
Iteration 217/1000 | Loss: 0.00002219
Iteration 218/1000 | Loss: 0.00002219
Iteration 219/1000 | Loss: 0.00002219
Iteration 220/1000 | Loss: 0.00002219
Iteration 221/1000 | Loss: 0.00002219
Iteration 222/1000 | Loss: 0.00002219
Iteration 223/1000 | Loss: 0.00002219
Iteration 224/1000 | Loss: 0.00002218
Iteration 225/1000 | Loss: 0.00002218
Iteration 226/1000 | Loss: 0.00002218
Iteration 227/1000 | Loss: 0.00002218
Iteration 228/1000 | Loss: 0.00002218
Iteration 229/1000 | Loss: 0.00002218
Iteration 230/1000 | Loss: 0.00002218
Iteration 231/1000 | Loss: 0.00002218
Iteration 232/1000 | Loss: 0.00002218
Iteration 233/1000 | Loss: 0.00002218
Iteration 234/1000 | Loss: 0.00002218
Iteration 235/1000 | Loss: 0.00002218
Iteration 236/1000 | Loss: 0.00002218
Iteration 237/1000 | Loss: 0.00002218
Iteration 238/1000 | Loss: 0.00002218
Iteration 239/1000 | Loss: 0.00002218
Iteration 240/1000 | Loss: 0.00002218
Iteration 241/1000 | Loss: 0.00002218
Iteration 242/1000 | Loss: 0.00002218
Iteration 243/1000 | Loss: 0.00002218
Iteration 244/1000 | Loss: 0.00002218
Iteration 245/1000 | Loss: 0.00002218
Iteration 246/1000 | Loss: 0.00002218
Iteration 247/1000 | Loss: 0.00002218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.2181835447554477e-05, 2.2181835447554477e-05, 2.2181835447554477e-05, 2.2181835447554477e-05, 2.2181835447554477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2181835447554477e-05

Optimization complete. Final v2v error: 3.943361282348633 mm

Highest mean error: 4.615478515625 mm for frame 81

Lowest mean error: 3.4542176723480225 mm for frame 0

Saving results

Total time: 49.93991231918335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801640
Iteration 2/25 | Loss: 0.00170372
Iteration 3/25 | Loss: 0.00151823
Iteration 4/25 | Loss: 0.00150432
Iteration 5/25 | Loss: 0.00150432
Iteration 6/25 | Loss: 0.00150432
Iteration 7/25 | Loss: 0.00150432
Iteration 8/25 | Loss: 0.00150432
Iteration 9/25 | Loss: 0.00150432
Iteration 10/25 | Loss: 0.00150432
Iteration 11/25 | Loss: 0.00150432
Iteration 12/25 | Loss: 0.00150432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015043189050629735, 0.0015043189050629735, 0.0015043189050629735, 0.0015043189050629735, 0.0015043189050629735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015043189050629735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88418186
Iteration 2/25 | Loss: 0.00144973
Iteration 3/25 | Loss: 0.00144973
Iteration 4/25 | Loss: 0.00144973
Iteration 5/25 | Loss: 0.00144973
Iteration 6/25 | Loss: 0.00144973
Iteration 7/25 | Loss: 0.00144972
Iteration 8/25 | Loss: 0.00144972
Iteration 9/25 | Loss: 0.00144972
Iteration 10/25 | Loss: 0.00144972
Iteration 11/25 | Loss: 0.00144972
Iteration 12/25 | Loss: 0.00144972
Iteration 13/25 | Loss: 0.00144972
Iteration 14/25 | Loss: 0.00144972
Iteration 15/25 | Loss: 0.00144972
Iteration 16/25 | Loss: 0.00144972
Iteration 17/25 | Loss: 0.00144972
Iteration 18/25 | Loss: 0.00144972
Iteration 19/25 | Loss: 0.00144972
Iteration 20/25 | Loss: 0.00144972
Iteration 21/25 | Loss: 0.00144972
Iteration 22/25 | Loss: 0.00144972
Iteration 23/25 | Loss: 0.00144972
Iteration 24/25 | Loss: 0.00144972
Iteration 25/25 | Loss: 0.00144972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144972
Iteration 2/1000 | Loss: 0.00003527
Iteration 3/1000 | Loss: 0.00002899
Iteration 4/1000 | Loss: 0.00002688
Iteration 5/1000 | Loss: 0.00002535
Iteration 6/1000 | Loss: 0.00002453
Iteration 7/1000 | Loss: 0.00002406
Iteration 8/1000 | Loss: 0.00002334
Iteration 9/1000 | Loss: 0.00002300
Iteration 10/1000 | Loss: 0.00002271
Iteration 11/1000 | Loss: 0.00002244
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002192
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002167
Iteration 27/1000 | Loss: 0.00002167
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002167
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00002167
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002167
Iteration 34/1000 | Loss: 0.00002167
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002150
Iteration 37/1000 | Loss: 0.00002147
Iteration 38/1000 | Loss: 0.00002147
Iteration 39/1000 | Loss: 0.00002146
Iteration 40/1000 | Loss: 0.00002146
Iteration 41/1000 | Loss: 0.00002145
Iteration 42/1000 | Loss: 0.00002144
Iteration 43/1000 | Loss: 0.00002142
Iteration 44/1000 | Loss: 0.00002141
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002137
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002136
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002136
Iteration 53/1000 | Loss: 0.00002135
Iteration 54/1000 | Loss: 0.00002135
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002135
Iteration 57/1000 | Loss: 0.00002135
Iteration 58/1000 | Loss: 0.00002135
Iteration 59/1000 | Loss: 0.00002135
Iteration 60/1000 | Loss: 0.00002135
Iteration 61/1000 | Loss: 0.00002135
Iteration 62/1000 | Loss: 0.00002135
Iteration 63/1000 | Loss: 0.00002135
Iteration 64/1000 | Loss: 0.00002135
Iteration 65/1000 | Loss: 0.00002135
Iteration 66/1000 | Loss: 0.00002135
Iteration 67/1000 | Loss: 0.00002135
Iteration 68/1000 | Loss: 0.00002135
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002135
Iteration 73/1000 | Loss: 0.00002135
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002135
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002135
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002135
Iteration 82/1000 | Loss: 0.00002135
Iteration 83/1000 | Loss: 0.00002135
Iteration 84/1000 | Loss: 0.00002135
Iteration 85/1000 | Loss: 0.00002135
Iteration 86/1000 | Loss: 0.00002135
Iteration 87/1000 | Loss: 0.00002135
Iteration 88/1000 | Loss: 0.00002135
Iteration 89/1000 | Loss: 0.00002135
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00002135
Iteration 94/1000 | Loss: 0.00002135
Iteration 95/1000 | Loss: 0.00002135
Iteration 96/1000 | Loss: 0.00002135
Iteration 97/1000 | Loss: 0.00002135
Iteration 98/1000 | Loss: 0.00002135
Iteration 99/1000 | Loss: 0.00002135
Iteration 100/1000 | Loss: 0.00002135
Iteration 101/1000 | Loss: 0.00002135
Iteration 102/1000 | Loss: 0.00002135
Iteration 103/1000 | Loss: 0.00002135
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002135
Iteration 109/1000 | Loss: 0.00002135
Iteration 110/1000 | Loss: 0.00002135
Iteration 111/1000 | Loss: 0.00002135
Iteration 112/1000 | Loss: 0.00002135
Iteration 113/1000 | Loss: 0.00002135
Iteration 114/1000 | Loss: 0.00002135
Iteration 115/1000 | Loss: 0.00002135
Iteration 116/1000 | Loss: 0.00002135
Iteration 117/1000 | Loss: 0.00002135
Iteration 118/1000 | Loss: 0.00002135
Iteration 119/1000 | Loss: 0.00002135
Iteration 120/1000 | Loss: 0.00002135
Iteration 121/1000 | Loss: 0.00002135
Iteration 122/1000 | Loss: 0.00002135
Iteration 123/1000 | Loss: 0.00002135
Iteration 124/1000 | Loss: 0.00002135
Iteration 125/1000 | Loss: 0.00002135
Iteration 126/1000 | Loss: 0.00002135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.1345922505133785e-05, 2.1345922505133785e-05, 2.1345922505133785e-05, 2.1345922505133785e-05, 2.1345922505133785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1345922505133785e-05

Optimization complete. Final v2v error: 3.9159533977508545 mm

Highest mean error: 3.99916934967041 mm for frame 101

Lowest mean error: 3.8062374591827393 mm for frame 239

Saving results

Total time: 39.668179988861084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972493
Iteration 2/25 | Loss: 0.00271455
Iteration 3/25 | Loss: 0.00229158
Iteration 4/25 | Loss: 0.00222193
Iteration 5/25 | Loss: 0.00201986
Iteration 6/25 | Loss: 0.00173830
Iteration 7/25 | Loss: 0.00162923
Iteration 8/25 | Loss: 0.00160757
Iteration 9/25 | Loss: 0.00160183
Iteration 10/25 | Loss: 0.00160257
Iteration 11/25 | Loss: 0.00159766
Iteration 12/25 | Loss: 0.00159731
Iteration 13/25 | Loss: 0.00159719
Iteration 14/25 | Loss: 0.00159711
Iteration 15/25 | Loss: 0.00159710
Iteration 16/25 | Loss: 0.00159708
Iteration 17/25 | Loss: 0.00159708
Iteration 18/25 | Loss: 0.00159708
Iteration 19/25 | Loss: 0.00159707
Iteration 20/25 | Loss: 0.00159707
Iteration 21/25 | Loss: 0.00159707
Iteration 22/25 | Loss: 0.00159707
Iteration 23/25 | Loss: 0.00159707
Iteration 24/25 | Loss: 0.00159707
Iteration 25/25 | Loss: 0.00159707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23286831
Iteration 2/25 | Loss: 0.00227414
Iteration 3/25 | Loss: 0.00227414
Iteration 4/25 | Loss: 0.00227414
Iteration 5/25 | Loss: 0.00227414
Iteration 6/25 | Loss: 0.00227414
Iteration 7/25 | Loss: 0.00227414
Iteration 8/25 | Loss: 0.00227414
Iteration 9/25 | Loss: 0.00227414
Iteration 10/25 | Loss: 0.00227414
Iteration 11/25 | Loss: 0.00227414
Iteration 12/25 | Loss: 0.00227414
Iteration 13/25 | Loss: 0.00227414
Iteration 14/25 | Loss: 0.00227414
Iteration 15/25 | Loss: 0.00227414
Iteration 16/25 | Loss: 0.00227414
Iteration 17/25 | Loss: 0.00227414
Iteration 18/25 | Loss: 0.00227414
Iteration 19/25 | Loss: 0.00227414
Iteration 20/25 | Loss: 0.00227414
Iteration 21/25 | Loss: 0.00227414
Iteration 22/25 | Loss: 0.00227414
Iteration 23/25 | Loss: 0.00227414
Iteration 24/25 | Loss: 0.00227414
Iteration 25/25 | Loss: 0.00227414
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0022741369903087616, 0.0022741369903087616, 0.0022741369903087616, 0.0022741369903087616, 0.0022741369903087616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022741369903087616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227414
Iteration 2/1000 | Loss: 0.00007974
Iteration 3/1000 | Loss: 0.00005862
Iteration 4/1000 | Loss: 0.00005340
Iteration 5/1000 | Loss: 0.00005077
Iteration 6/1000 | Loss: 0.00004836
Iteration 7/1000 | Loss: 0.00004697
Iteration 8/1000 | Loss: 0.00004556
Iteration 9/1000 | Loss: 0.00004475
Iteration 10/1000 | Loss: 0.00004384
Iteration 11/1000 | Loss: 0.00004331
Iteration 12/1000 | Loss: 0.00004265
Iteration 13/1000 | Loss: 0.00103035
Iteration 14/1000 | Loss: 0.00024992
Iteration 15/1000 | Loss: 0.00005328
Iteration 16/1000 | Loss: 0.00004473
Iteration 17/1000 | Loss: 0.00003818
Iteration 18/1000 | Loss: 0.00003340
Iteration 19/1000 | Loss: 0.00002944
Iteration 20/1000 | Loss: 0.00002761
Iteration 21/1000 | Loss: 0.00002649
Iteration 22/1000 | Loss: 0.00002594
Iteration 23/1000 | Loss: 0.00002539
Iteration 24/1000 | Loss: 0.00002487
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002436
Iteration 27/1000 | Loss: 0.00002421
Iteration 28/1000 | Loss: 0.00002418
Iteration 29/1000 | Loss: 0.00002417
Iteration 30/1000 | Loss: 0.00002416
Iteration 31/1000 | Loss: 0.00002415
Iteration 32/1000 | Loss: 0.00002415
Iteration 33/1000 | Loss: 0.00002412
Iteration 34/1000 | Loss: 0.00002406
Iteration 35/1000 | Loss: 0.00002398
Iteration 36/1000 | Loss: 0.00002396
Iteration 37/1000 | Loss: 0.00002396
Iteration 38/1000 | Loss: 0.00002396
Iteration 39/1000 | Loss: 0.00002396
Iteration 40/1000 | Loss: 0.00002396
Iteration 41/1000 | Loss: 0.00002396
Iteration 42/1000 | Loss: 0.00002396
Iteration 43/1000 | Loss: 0.00002396
Iteration 44/1000 | Loss: 0.00002396
Iteration 45/1000 | Loss: 0.00002396
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002396
Iteration 49/1000 | Loss: 0.00002396
Iteration 50/1000 | Loss: 0.00002396
Iteration 51/1000 | Loss: 0.00002396
Iteration 52/1000 | Loss: 0.00002396
Iteration 53/1000 | Loss: 0.00002396
Iteration 54/1000 | Loss: 0.00002396
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002396
Iteration 57/1000 | Loss: 0.00002396
Iteration 58/1000 | Loss: 0.00002396
Iteration 59/1000 | Loss: 0.00002396
Iteration 60/1000 | Loss: 0.00002396
Iteration 61/1000 | Loss: 0.00002396
Iteration 62/1000 | Loss: 0.00002396
Iteration 63/1000 | Loss: 0.00002396
Iteration 64/1000 | Loss: 0.00002396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [2.39576656895224e-05, 2.39576656895224e-05, 2.39576656895224e-05, 2.39576656895224e-05, 2.39576656895224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.39576656895224e-05

Optimization complete. Final v2v error: 4.204255104064941 mm

Highest mean error: 4.449435710906982 mm for frame 148

Lowest mean error: 3.73085355758667 mm for frame 5

Saving results

Total time: 65.72641921043396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769230
Iteration 2/25 | Loss: 0.00192323
Iteration 3/25 | Loss: 0.00163116
Iteration 4/25 | Loss: 0.00156652
Iteration 5/25 | Loss: 0.00157169
Iteration 6/25 | Loss: 0.00155800
Iteration 7/25 | Loss: 0.00151756
Iteration 8/25 | Loss: 0.00151288
Iteration 9/25 | Loss: 0.00148682
Iteration 10/25 | Loss: 0.00147542
Iteration 11/25 | Loss: 0.00146644
Iteration 12/25 | Loss: 0.00146292
Iteration 13/25 | Loss: 0.00146228
Iteration 14/25 | Loss: 0.00146213
Iteration 15/25 | Loss: 0.00146212
Iteration 16/25 | Loss: 0.00146212
Iteration 17/25 | Loss: 0.00146211
Iteration 18/25 | Loss: 0.00146211
Iteration 19/25 | Loss: 0.00146211
Iteration 20/25 | Loss: 0.00146211
Iteration 21/25 | Loss: 0.00146211
Iteration 22/25 | Loss: 0.00146211
Iteration 23/25 | Loss: 0.00146211
Iteration 24/25 | Loss: 0.00146211
Iteration 25/25 | Loss: 0.00146211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90296817
Iteration 2/25 | Loss: 0.00234345
Iteration 3/25 | Loss: 0.00234345
Iteration 4/25 | Loss: 0.00234345
Iteration 5/25 | Loss: 0.00234345
Iteration 6/25 | Loss: 0.00234345
Iteration 7/25 | Loss: 0.00234345
Iteration 8/25 | Loss: 0.00234345
Iteration 9/25 | Loss: 0.00234345
Iteration 10/25 | Loss: 0.00234345
Iteration 11/25 | Loss: 0.00234345
Iteration 12/25 | Loss: 0.00234345
Iteration 13/25 | Loss: 0.00234345
Iteration 14/25 | Loss: 0.00234345
Iteration 15/25 | Loss: 0.00234345
Iteration 16/25 | Loss: 0.00234345
Iteration 17/25 | Loss: 0.00234345
Iteration 18/25 | Loss: 0.00234345
Iteration 19/25 | Loss: 0.00234345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00234344694763422, 0.00234344694763422, 0.00234344694763422, 0.00234344694763422, 0.00234344694763422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00234344694763422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234345
Iteration 2/1000 | Loss: 0.00002795
Iteration 3/1000 | Loss: 0.00013529
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00003714
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00004531
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001629
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001576
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001542
Iteration 15/1000 | Loss: 0.00001540
Iteration 16/1000 | Loss: 0.00001533
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001520
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001511
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001490
Iteration 35/1000 | Loss: 0.00001490
Iteration 36/1000 | Loss: 0.00001490
Iteration 37/1000 | Loss: 0.00001486
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001482
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00003186
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001478
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001471
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001471
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001470
Iteration 87/1000 | Loss: 0.00001470
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001468
Iteration 91/1000 | Loss: 0.00001468
Iteration 92/1000 | Loss: 0.00001468
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.464230354031315e-05, 1.464230354031315e-05, 1.464230354031315e-05, 1.464230354031315e-05, 1.464230354031315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.464230354031315e-05

Optimization complete. Final v2v error: 3.2970025539398193 mm

Highest mean error: 3.6259377002716064 mm for frame 77

Lowest mean error: 3.1430516242980957 mm for frame 163

Saving results

Total time: 57.38374423980713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021108
Iteration 2/25 | Loss: 0.01021108
Iteration 3/25 | Loss: 0.01021107
Iteration 4/25 | Loss: 0.01021107
Iteration 5/25 | Loss: 0.01021107
Iteration 6/25 | Loss: 0.01021107
Iteration 7/25 | Loss: 0.01021106
Iteration 8/25 | Loss: 0.01021106
Iteration 9/25 | Loss: 0.01021106
Iteration 10/25 | Loss: 0.01021106
Iteration 11/25 | Loss: 0.01021106
Iteration 12/25 | Loss: 0.01021106
Iteration 13/25 | Loss: 0.01021105
Iteration 14/25 | Loss: 0.01021105
Iteration 15/25 | Loss: 0.01021105
Iteration 16/25 | Loss: 0.01021105
Iteration 17/25 | Loss: 0.01021104
Iteration 18/25 | Loss: 0.01021104
Iteration 19/25 | Loss: 0.01021104
Iteration 20/25 | Loss: 0.01021104
Iteration 21/25 | Loss: 0.01021104
Iteration 22/25 | Loss: 0.01021104
Iteration 23/25 | Loss: 0.01021104
Iteration 24/25 | Loss: 0.01021103
Iteration 25/25 | Loss: 0.01021103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49428999
Iteration 2/25 | Loss: 0.09035049
Iteration 3/25 | Loss: 0.09010534
Iteration 4/25 | Loss: 0.08985965
Iteration 5/25 | Loss: 0.08973375
Iteration 6/25 | Loss: 0.08973373
Iteration 7/25 | Loss: 0.08973373
Iteration 8/25 | Loss: 0.08973373
Iteration 9/25 | Loss: 0.08973373
Iteration 10/25 | Loss: 0.08973373
Iteration 11/25 | Loss: 0.08973373
Iteration 12/25 | Loss: 0.08973373
Iteration 13/25 | Loss: 0.08973373
Iteration 14/25 | Loss: 0.08973373
Iteration 15/25 | Loss: 0.08973373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08973372727632523, 0.08973372727632523, 0.08973372727632523, 0.08973372727632523, 0.08973372727632523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08973372727632523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08973373
Iteration 2/1000 | Loss: 0.00156810
Iteration 3/1000 | Loss: 0.00025060
Iteration 4/1000 | Loss: 0.00008784
Iteration 5/1000 | Loss: 0.00028156
Iteration 6/1000 | Loss: 0.00008762
Iteration 7/1000 | Loss: 0.00011578
Iteration 8/1000 | Loss: 0.00027452
Iteration 9/1000 | Loss: 0.00003849
Iteration 10/1000 | Loss: 0.00009016
Iteration 11/1000 | Loss: 0.00008356
Iteration 12/1000 | Loss: 0.00008514
Iteration 13/1000 | Loss: 0.00014308
Iteration 14/1000 | Loss: 0.00006090
Iteration 15/1000 | Loss: 0.00002346
Iteration 16/1000 | Loss: 0.00008201
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00006292
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00015259
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00004120
Iteration 23/1000 | Loss: 0.00004678
Iteration 24/1000 | Loss: 0.00041903
Iteration 25/1000 | Loss: 0.00006274
Iteration 26/1000 | Loss: 0.00013759
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00003133
Iteration 29/1000 | Loss: 0.00003501
Iteration 30/1000 | Loss: 0.00004400
Iteration 31/1000 | Loss: 0.00033047
Iteration 32/1000 | Loss: 0.00009946
Iteration 33/1000 | Loss: 0.00001602
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00002934
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00004335
Iteration 38/1000 | Loss: 0.00013292
Iteration 39/1000 | Loss: 0.00011375
Iteration 40/1000 | Loss: 0.00002120
Iteration 41/1000 | Loss: 0.00002699
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00002761
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00015548
Iteration 47/1000 | Loss: 0.00070820
Iteration 48/1000 | Loss: 0.00006341
Iteration 49/1000 | Loss: 0.00003846
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00004124
Iteration 63/1000 | Loss: 0.00021286
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00002268
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00002060
Iteration 68/1000 | Loss: 0.00002765
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001319
Iteration 71/1000 | Loss: 0.00002862
Iteration 72/1000 | Loss: 0.00002525
Iteration 73/1000 | Loss: 0.00001201
Iteration 74/1000 | Loss: 0.00002966
Iteration 75/1000 | Loss: 0.00002966
Iteration 76/1000 | Loss: 0.00012162
Iteration 77/1000 | Loss: 0.00007199
Iteration 78/1000 | Loss: 0.00013244
Iteration 79/1000 | Loss: 0.00005439
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00007127
Iteration 82/1000 | Loss: 0.00001935
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001122
Iteration 101/1000 | Loss: 0.00001122
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001122
Iteration 106/1000 | Loss: 0.00001122
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001565
Iteration 110/1000 | Loss: 0.00004144
Iteration 111/1000 | Loss: 0.00001250
Iteration 112/1000 | Loss: 0.00005322
Iteration 113/1000 | Loss: 0.00004614
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001175
Iteration 125/1000 | Loss: 0.00001114
Iteration 126/1000 | Loss: 0.00001114
Iteration 127/1000 | Loss: 0.00001114
Iteration 128/1000 | Loss: 0.00001114
Iteration 129/1000 | Loss: 0.00001114
Iteration 130/1000 | Loss: 0.00001114
Iteration 131/1000 | Loss: 0.00001114
Iteration 132/1000 | Loss: 0.00001114
Iteration 133/1000 | Loss: 0.00001114
Iteration 134/1000 | Loss: 0.00001114
Iteration 135/1000 | Loss: 0.00001114
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001114
Iteration 138/1000 | Loss: 0.00001114
Iteration 139/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1140845344925765e-05, 1.1140845344925765e-05, 1.1140845344925765e-05, 1.1140845344925765e-05, 1.1140845344925765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1140845344925765e-05

Optimization complete. Final v2v error: 2.9255189895629883 mm

Highest mean error: 3.135395050048828 mm for frame 108

Lowest mean error: 2.7705676555633545 mm for frame 47

Saving results

Total time: 119.90980577468872
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895397
Iteration 2/25 | Loss: 0.00171826
Iteration 3/25 | Loss: 0.00169553
Iteration 4/25 | Loss: 0.00152599
Iteration 5/25 | Loss: 0.00151828
Iteration 6/25 | Loss: 0.00151600
Iteration 7/25 | Loss: 0.00152187
Iteration 8/25 | Loss: 0.00151978
Iteration 9/25 | Loss: 0.00151718
Iteration 10/25 | Loss: 0.00151558
Iteration 11/25 | Loss: 0.00151393
Iteration 12/25 | Loss: 0.00151216
Iteration 13/25 | Loss: 0.00151208
Iteration 14/25 | Loss: 0.00151207
Iteration 15/25 | Loss: 0.00151207
Iteration 16/25 | Loss: 0.00151207
Iteration 17/25 | Loss: 0.00151207
Iteration 18/25 | Loss: 0.00151207
Iteration 19/25 | Loss: 0.00151207
Iteration 20/25 | Loss: 0.00151207
Iteration 21/25 | Loss: 0.00151207
Iteration 22/25 | Loss: 0.00151207
Iteration 23/25 | Loss: 0.00151207
Iteration 24/25 | Loss: 0.00151207
Iteration 25/25 | Loss: 0.00151207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37318063
Iteration 2/25 | Loss: 0.00242966
Iteration 3/25 | Loss: 0.00242963
Iteration 4/25 | Loss: 0.00242963
Iteration 5/25 | Loss: 0.00242963
Iteration 6/25 | Loss: 0.00242963
Iteration 7/25 | Loss: 0.00242963
Iteration 8/25 | Loss: 0.00242963
Iteration 9/25 | Loss: 0.00242963
Iteration 10/25 | Loss: 0.00242963
Iteration 11/25 | Loss: 0.00242963
Iteration 12/25 | Loss: 0.00242963
Iteration 13/25 | Loss: 0.00242963
Iteration 14/25 | Loss: 0.00242963
Iteration 15/25 | Loss: 0.00242963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0024296301417052746, 0.0024296301417052746, 0.0024296301417052746, 0.0024296301417052746, 0.0024296301417052746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024296301417052746

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242963
Iteration 2/1000 | Loss: 0.00004152
Iteration 3/1000 | Loss: 0.00007675
Iteration 4/1000 | Loss: 0.00013293
Iteration 5/1000 | Loss: 0.00002302
Iteration 6/1000 | Loss: 0.00002196
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00002026
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001950
Iteration 11/1000 | Loss: 0.00001914
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001855
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001781
Iteration 19/1000 | Loss: 0.00001780
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001778
Iteration 24/1000 | Loss: 0.00001777
Iteration 25/1000 | Loss: 0.00001777
Iteration 26/1000 | Loss: 0.00001777
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001775
Iteration 29/1000 | Loss: 0.00001769
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001762
Iteration 43/1000 | Loss: 0.00001762
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001760
Iteration 51/1000 | Loss: 0.00001760
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001749
Iteration 90/1000 | Loss: 0.00001749
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001747
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001746
Iteration 116/1000 | Loss: 0.00001746
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001745
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001743
Iteration 134/1000 | Loss: 0.00001743
Iteration 135/1000 | Loss: 0.00001743
Iteration 136/1000 | Loss: 0.00001743
Iteration 137/1000 | Loss: 0.00001743
Iteration 138/1000 | Loss: 0.00001743
Iteration 139/1000 | Loss: 0.00001743
Iteration 140/1000 | Loss: 0.00001743
Iteration 141/1000 | Loss: 0.00001742
Iteration 142/1000 | Loss: 0.00001742
Iteration 143/1000 | Loss: 0.00001742
Iteration 144/1000 | Loss: 0.00001742
Iteration 145/1000 | Loss: 0.00001742
Iteration 146/1000 | Loss: 0.00001742
Iteration 147/1000 | Loss: 0.00001742
Iteration 148/1000 | Loss: 0.00001742
Iteration 149/1000 | Loss: 0.00001742
Iteration 150/1000 | Loss: 0.00001742
Iteration 151/1000 | Loss: 0.00001742
Iteration 152/1000 | Loss: 0.00001742
Iteration 153/1000 | Loss: 0.00001741
Iteration 154/1000 | Loss: 0.00001741
Iteration 155/1000 | Loss: 0.00001741
Iteration 156/1000 | Loss: 0.00001741
Iteration 157/1000 | Loss: 0.00001741
Iteration 158/1000 | Loss: 0.00001741
Iteration 159/1000 | Loss: 0.00001741
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001741
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001740
Iteration 166/1000 | Loss: 0.00001740
Iteration 167/1000 | Loss: 0.00001740
Iteration 168/1000 | Loss: 0.00001740
Iteration 169/1000 | Loss: 0.00001740
Iteration 170/1000 | Loss: 0.00001740
Iteration 171/1000 | Loss: 0.00001740
Iteration 172/1000 | Loss: 0.00001740
Iteration 173/1000 | Loss: 0.00001740
Iteration 174/1000 | Loss: 0.00001740
Iteration 175/1000 | Loss: 0.00001740
Iteration 176/1000 | Loss: 0.00001740
Iteration 177/1000 | Loss: 0.00001740
Iteration 178/1000 | Loss: 0.00001740
Iteration 179/1000 | Loss: 0.00001739
Iteration 180/1000 | Loss: 0.00001739
Iteration 181/1000 | Loss: 0.00001739
Iteration 182/1000 | Loss: 0.00001739
Iteration 183/1000 | Loss: 0.00001739
Iteration 184/1000 | Loss: 0.00001739
Iteration 185/1000 | Loss: 0.00001738
Iteration 186/1000 | Loss: 0.00001738
Iteration 187/1000 | Loss: 0.00001738
Iteration 188/1000 | Loss: 0.00001738
Iteration 189/1000 | Loss: 0.00001738
Iteration 190/1000 | Loss: 0.00001738
Iteration 191/1000 | Loss: 0.00001738
Iteration 192/1000 | Loss: 0.00001738
Iteration 193/1000 | Loss: 0.00001738
Iteration 194/1000 | Loss: 0.00001738
Iteration 195/1000 | Loss: 0.00001738
Iteration 196/1000 | Loss: 0.00001738
Iteration 197/1000 | Loss: 0.00001738
Iteration 198/1000 | Loss: 0.00001738
Iteration 199/1000 | Loss: 0.00001738
Iteration 200/1000 | Loss: 0.00001738
Iteration 201/1000 | Loss: 0.00001738
Iteration 202/1000 | Loss: 0.00001738
Iteration 203/1000 | Loss: 0.00001738
Iteration 204/1000 | Loss: 0.00001738
Iteration 205/1000 | Loss: 0.00001738
Iteration 206/1000 | Loss: 0.00001738
Iteration 207/1000 | Loss: 0.00001738
Iteration 208/1000 | Loss: 0.00001738
Iteration 209/1000 | Loss: 0.00001738
Iteration 210/1000 | Loss: 0.00001738
Iteration 211/1000 | Loss: 0.00001738
Iteration 212/1000 | Loss: 0.00001738
Iteration 213/1000 | Loss: 0.00001738
Iteration 214/1000 | Loss: 0.00001738
Iteration 215/1000 | Loss: 0.00001738
Iteration 216/1000 | Loss: 0.00001738
Iteration 217/1000 | Loss: 0.00001738
Iteration 218/1000 | Loss: 0.00001738
Iteration 219/1000 | Loss: 0.00001738
Iteration 220/1000 | Loss: 0.00001738
Iteration 221/1000 | Loss: 0.00001738
Iteration 222/1000 | Loss: 0.00001738
Iteration 223/1000 | Loss: 0.00001738
Iteration 224/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.7379406926920637e-05, 1.7379406926920637e-05, 1.7379406926920637e-05, 1.7379406926920637e-05, 1.7379406926920637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7379406926920637e-05

Optimization complete. Final v2v error: 3.5734872817993164 mm

Highest mean error: 4.245316028594971 mm for frame 39

Lowest mean error: 3.1549744606018066 mm for frame 121

Saving results

Total time: 60.3386754989624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507789
Iteration 2/25 | Loss: 0.00163743
Iteration 3/25 | Loss: 0.00154805
Iteration 4/25 | Loss: 0.00153725
Iteration 5/25 | Loss: 0.00153466
Iteration 6/25 | Loss: 0.00153405
Iteration 7/25 | Loss: 0.00153405
Iteration 8/25 | Loss: 0.00153405
Iteration 9/25 | Loss: 0.00153405
Iteration 10/25 | Loss: 0.00153405
Iteration 11/25 | Loss: 0.00153405
Iteration 12/25 | Loss: 0.00153405
Iteration 13/25 | Loss: 0.00153405
Iteration 14/25 | Loss: 0.00153405
Iteration 15/25 | Loss: 0.00153405
Iteration 16/25 | Loss: 0.00153405
Iteration 17/25 | Loss: 0.00153405
Iteration 18/25 | Loss: 0.00153405
Iteration 19/25 | Loss: 0.00153405
Iteration 20/25 | Loss: 0.00153405
Iteration 21/25 | Loss: 0.00153405
Iteration 22/25 | Loss: 0.00153405
Iteration 23/25 | Loss: 0.00153405
Iteration 24/25 | Loss: 0.00153405
Iteration 25/25 | Loss: 0.00153405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31350553
Iteration 2/25 | Loss: 0.00221022
Iteration 3/25 | Loss: 0.00221019
Iteration 4/25 | Loss: 0.00221019
Iteration 5/25 | Loss: 0.00221019
Iteration 6/25 | Loss: 0.00221018
Iteration 7/25 | Loss: 0.00221018
Iteration 8/25 | Loss: 0.00221018
Iteration 9/25 | Loss: 0.00221018
Iteration 10/25 | Loss: 0.00221018
Iteration 11/25 | Loss: 0.00221018
Iteration 12/25 | Loss: 0.00221018
Iteration 13/25 | Loss: 0.00221018
Iteration 14/25 | Loss: 0.00221018
Iteration 15/25 | Loss: 0.00221018
Iteration 16/25 | Loss: 0.00221018
Iteration 17/25 | Loss: 0.00221018
Iteration 18/25 | Loss: 0.00221018
Iteration 19/25 | Loss: 0.00221018
Iteration 20/25 | Loss: 0.00221018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022101837676018476, 0.0022101837676018476, 0.0022101837676018476, 0.0022101837676018476, 0.0022101837676018476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022101837676018476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221018
Iteration 2/1000 | Loss: 0.00005641
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00002437
Iteration 6/1000 | Loss: 0.00002327
Iteration 7/1000 | Loss: 0.00002256
Iteration 8/1000 | Loss: 0.00002192
Iteration 9/1000 | Loss: 0.00002124
Iteration 10/1000 | Loss: 0.00002084
Iteration 11/1000 | Loss: 0.00002057
Iteration 12/1000 | Loss: 0.00002030
Iteration 13/1000 | Loss: 0.00002004
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001971
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001954
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001935
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001931
Iteration 32/1000 | Loss: 0.00001930
Iteration 33/1000 | Loss: 0.00001929
Iteration 34/1000 | Loss: 0.00001928
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001927
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001924
Iteration 41/1000 | Loss: 0.00001924
Iteration 42/1000 | Loss: 0.00001924
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001920
Iteration 54/1000 | Loss: 0.00001919
Iteration 55/1000 | Loss: 0.00001919
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001918
Iteration 60/1000 | Loss: 0.00001918
Iteration 61/1000 | Loss: 0.00001918
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001917
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001916
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001913
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001911
Iteration 81/1000 | Loss: 0.00001911
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001911
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001911
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001910
Iteration 89/1000 | Loss: 0.00001910
Iteration 90/1000 | Loss: 0.00001909
Iteration 91/1000 | Loss: 0.00001909
Iteration 92/1000 | Loss: 0.00001909
Iteration 93/1000 | Loss: 0.00001909
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001907
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001907
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001907
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001906
Iteration 106/1000 | Loss: 0.00001906
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001904
Iteration 111/1000 | Loss: 0.00001904
Iteration 112/1000 | Loss: 0.00001904
Iteration 113/1000 | Loss: 0.00001904
Iteration 114/1000 | Loss: 0.00001904
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001903
Iteration 118/1000 | Loss: 0.00001903
Iteration 119/1000 | Loss: 0.00001903
Iteration 120/1000 | Loss: 0.00001903
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001902
Iteration 128/1000 | Loss: 0.00001902
Iteration 129/1000 | Loss: 0.00001902
Iteration 130/1000 | Loss: 0.00001902
Iteration 131/1000 | Loss: 0.00001902
Iteration 132/1000 | Loss: 0.00001902
Iteration 133/1000 | Loss: 0.00001901
Iteration 134/1000 | Loss: 0.00001900
Iteration 135/1000 | Loss: 0.00001900
Iteration 136/1000 | Loss: 0.00001900
Iteration 137/1000 | Loss: 0.00001900
Iteration 138/1000 | Loss: 0.00001900
Iteration 139/1000 | Loss: 0.00001900
Iteration 140/1000 | Loss: 0.00001899
Iteration 141/1000 | Loss: 0.00001899
Iteration 142/1000 | Loss: 0.00001899
Iteration 143/1000 | Loss: 0.00001899
Iteration 144/1000 | Loss: 0.00001899
Iteration 145/1000 | Loss: 0.00001899
Iteration 146/1000 | Loss: 0.00001899
Iteration 147/1000 | Loss: 0.00001898
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001898
Iteration 150/1000 | Loss: 0.00001898
Iteration 151/1000 | Loss: 0.00001897
Iteration 152/1000 | Loss: 0.00001897
Iteration 153/1000 | Loss: 0.00001897
Iteration 154/1000 | Loss: 0.00001897
Iteration 155/1000 | Loss: 0.00001897
Iteration 156/1000 | Loss: 0.00001897
Iteration 157/1000 | Loss: 0.00001896
Iteration 158/1000 | Loss: 0.00001896
Iteration 159/1000 | Loss: 0.00001896
Iteration 160/1000 | Loss: 0.00001896
Iteration 161/1000 | Loss: 0.00001896
Iteration 162/1000 | Loss: 0.00001896
Iteration 163/1000 | Loss: 0.00001896
Iteration 164/1000 | Loss: 0.00001896
Iteration 165/1000 | Loss: 0.00001896
Iteration 166/1000 | Loss: 0.00001896
Iteration 167/1000 | Loss: 0.00001895
Iteration 168/1000 | Loss: 0.00001895
Iteration 169/1000 | Loss: 0.00001895
Iteration 170/1000 | Loss: 0.00001895
Iteration 171/1000 | Loss: 0.00001895
Iteration 172/1000 | Loss: 0.00001895
Iteration 173/1000 | Loss: 0.00001895
Iteration 174/1000 | Loss: 0.00001894
Iteration 175/1000 | Loss: 0.00001894
Iteration 176/1000 | Loss: 0.00001894
Iteration 177/1000 | Loss: 0.00001894
Iteration 178/1000 | Loss: 0.00001894
Iteration 179/1000 | Loss: 0.00001893
Iteration 180/1000 | Loss: 0.00001893
Iteration 181/1000 | Loss: 0.00001893
Iteration 182/1000 | Loss: 0.00001893
Iteration 183/1000 | Loss: 0.00001893
Iteration 184/1000 | Loss: 0.00001893
Iteration 185/1000 | Loss: 0.00001893
Iteration 186/1000 | Loss: 0.00001893
Iteration 187/1000 | Loss: 0.00001893
Iteration 188/1000 | Loss: 0.00001893
Iteration 189/1000 | Loss: 0.00001892
Iteration 190/1000 | Loss: 0.00001892
Iteration 191/1000 | Loss: 0.00001892
Iteration 192/1000 | Loss: 0.00001892
Iteration 193/1000 | Loss: 0.00001891
Iteration 194/1000 | Loss: 0.00001891
Iteration 195/1000 | Loss: 0.00001891
Iteration 196/1000 | Loss: 0.00001891
Iteration 197/1000 | Loss: 0.00001891
Iteration 198/1000 | Loss: 0.00001891
Iteration 199/1000 | Loss: 0.00001891
Iteration 200/1000 | Loss: 0.00001891
Iteration 201/1000 | Loss: 0.00001891
Iteration 202/1000 | Loss: 0.00001891
Iteration 203/1000 | Loss: 0.00001891
Iteration 204/1000 | Loss: 0.00001890
Iteration 205/1000 | Loss: 0.00001890
Iteration 206/1000 | Loss: 0.00001890
Iteration 207/1000 | Loss: 0.00001890
Iteration 208/1000 | Loss: 0.00001890
Iteration 209/1000 | Loss: 0.00001890
Iteration 210/1000 | Loss: 0.00001890
Iteration 211/1000 | Loss: 0.00001890
Iteration 212/1000 | Loss: 0.00001890
Iteration 213/1000 | Loss: 0.00001890
Iteration 214/1000 | Loss: 0.00001890
Iteration 215/1000 | Loss: 0.00001890
Iteration 216/1000 | Loss: 0.00001890
Iteration 217/1000 | Loss: 0.00001890
Iteration 218/1000 | Loss: 0.00001890
Iteration 219/1000 | Loss: 0.00001890
Iteration 220/1000 | Loss: 0.00001890
Iteration 221/1000 | Loss: 0.00001890
Iteration 222/1000 | Loss: 0.00001890
Iteration 223/1000 | Loss: 0.00001889
Iteration 224/1000 | Loss: 0.00001889
Iteration 225/1000 | Loss: 0.00001889
Iteration 226/1000 | Loss: 0.00001889
Iteration 227/1000 | Loss: 0.00001889
Iteration 228/1000 | Loss: 0.00001888
Iteration 229/1000 | Loss: 0.00001888
Iteration 230/1000 | Loss: 0.00001888
Iteration 231/1000 | Loss: 0.00001888
Iteration 232/1000 | Loss: 0.00001888
Iteration 233/1000 | Loss: 0.00001888
Iteration 234/1000 | Loss: 0.00001888
Iteration 235/1000 | Loss: 0.00001888
Iteration 236/1000 | Loss: 0.00001888
Iteration 237/1000 | Loss: 0.00001888
Iteration 238/1000 | Loss: 0.00001887
Iteration 239/1000 | Loss: 0.00001887
Iteration 240/1000 | Loss: 0.00001887
Iteration 241/1000 | Loss: 0.00001887
Iteration 242/1000 | Loss: 0.00001887
Iteration 243/1000 | Loss: 0.00001887
Iteration 244/1000 | Loss: 0.00001887
Iteration 245/1000 | Loss: 0.00001887
Iteration 246/1000 | Loss: 0.00001887
Iteration 247/1000 | Loss: 0.00001887
Iteration 248/1000 | Loss: 0.00001887
Iteration 249/1000 | Loss: 0.00001887
Iteration 250/1000 | Loss: 0.00001887
Iteration 251/1000 | Loss: 0.00001887
Iteration 252/1000 | Loss: 0.00001887
Iteration 253/1000 | Loss: 0.00001887
Iteration 254/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.8870734493248165e-05, 1.8870734493248165e-05, 1.8870734493248165e-05, 1.8870734493248165e-05, 1.8870734493248165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8870734493248165e-05

Optimization complete. Final v2v error: 3.502598762512207 mm

Highest mean error: 5.453202247619629 mm for frame 60

Lowest mean error: 2.8837268352508545 mm for frame 87

Saving results

Total time: 48.83765530586243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859976
Iteration 2/25 | Loss: 0.00159396
Iteration 3/25 | Loss: 0.00150972
Iteration 4/25 | Loss: 0.00149892
Iteration 5/25 | Loss: 0.00149664
Iteration 6/25 | Loss: 0.00149604
Iteration 7/25 | Loss: 0.00149604
Iteration 8/25 | Loss: 0.00149604
Iteration 9/25 | Loss: 0.00149604
Iteration 10/25 | Loss: 0.00149604
Iteration 11/25 | Loss: 0.00149604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001496042124927044, 0.001496042124927044, 0.001496042124927044, 0.001496042124927044, 0.001496042124927044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001496042124927044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26567078
Iteration 2/25 | Loss: 0.00211736
Iteration 3/25 | Loss: 0.00211736
Iteration 4/25 | Loss: 0.00211736
Iteration 5/25 | Loss: 0.00211736
Iteration 6/25 | Loss: 0.00211736
Iteration 7/25 | Loss: 0.00211736
Iteration 8/25 | Loss: 0.00211736
Iteration 9/25 | Loss: 0.00211736
Iteration 10/25 | Loss: 0.00211736
Iteration 11/25 | Loss: 0.00211736
Iteration 12/25 | Loss: 0.00211736
Iteration 13/25 | Loss: 0.00211736
Iteration 14/25 | Loss: 0.00211736
Iteration 15/25 | Loss: 0.00211735
Iteration 16/25 | Loss: 0.00211735
Iteration 17/25 | Loss: 0.00211735
Iteration 18/25 | Loss: 0.00211735
Iteration 19/25 | Loss: 0.00211735
Iteration 20/25 | Loss: 0.00211735
Iteration 21/25 | Loss: 0.00211735
Iteration 22/25 | Loss: 0.00211735
Iteration 23/25 | Loss: 0.00211735
Iteration 24/25 | Loss: 0.00211735
Iteration 25/25 | Loss: 0.00211735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211735
Iteration 2/1000 | Loss: 0.00003508
Iteration 3/1000 | Loss: 0.00002632
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002277
Iteration 6/1000 | Loss: 0.00002198
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002079
Iteration 9/1000 | Loss: 0.00002049
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001970
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001922
Iteration 17/1000 | Loss: 0.00001921
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001918
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001917
Iteration 24/1000 | Loss: 0.00001916
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001909
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001908
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001888
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001887
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001887
Iteration 88/1000 | Loss: 0.00001887
Iteration 89/1000 | Loss: 0.00001887
Iteration 90/1000 | Loss: 0.00001887
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00001887
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001887
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00001887
Iteration 101/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.8865926904254593e-05, 1.8865926904254593e-05, 1.8865926904254593e-05, 1.8865926904254593e-05, 1.8865926904254593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8865926904254593e-05

Optimization complete. Final v2v error: 3.771246910095215 mm

Highest mean error: 4.0726799964904785 mm for frame 6

Lowest mean error: 3.4719150066375732 mm for frame 33

Saving results

Total time: 35.70557379722595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861444
Iteration 2/25 | Loss: 0.00165789
Iteration 3/25 | Loss: 0.00154676
Iteration 4/25 | Loss: 0.00153177
Iteration 5/25 | Loss: 0.00152877
Iteration 6/25 | Loss: 0.00152228
Iteration 7/25 | Loss: 0.00151392
Iteration 8/25 | Loss: 0.00151327
Iteration 9/25 | Loss: 0.00151300
Iteration 10/25 | Loss: 0.00151295
Iteration 11/25 | Loss: 0.00151295
Iteration 12/25 | Loss: 0.00151295
Iteration 13/25 | Loss: 0.00151294
Iteration 14/25 | Loss: 0.00151294
Iteration 15/25 | Loss: 0.00151294
Iteration 16/25 | Loss: 0.00151294
Iteration 17/25 | Loss: 0.00151294
Iteration 18/25 | Loss: 0.00151294
Iteration 19/25 | Loss: 0.00151294
Iteration 20/25 | Loss: 0.00151294
Iteration 21/25 | Loss: 0.00151294
Iteration 22/25 | Loss: 0.00151294
Iteration 23/25 | Loss: 0.00151294
Iteration 24/25 | Loss: 0.00151294
Iteration 25/25 | Loss: 0.00151294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20494723
Iteration 2/25 | Loss: 0.00248890
Iteration 3/25 | Loss: 0.00248889
Iteration 4/25 | Loss: 0.00248889
Iteration 5/25 | Loss: 0.00248889
Iteration 6/25 | Loss: 0.00248889
Iteration 7/25 | Loss: 0.00248889
Iteration 8/25 | Loss: 0.00248889
Iteration 9/25 | Loss: 0.00248889
Iteration 10/25 | Loss: 0.00248889
Iteration 11/25 | Loss: 0.00248889
Iteration 12/25 | Loss: 0.00248889
Iteration 13/25 | Loss: 0.00248889
Iteration 14/25 | Loss: 0.00248889
Iteration 15/25 | Loss: 0.00248889
Iteration 16/25 | Loss: 0.00248889
Iteration 17/25 | Loss: 0.00248889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00248888717032969, 0.00248888717032969, 0.00248888717032969, 0.00248888717032969, 0.00248888717032969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00248888717032969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248889
Iteration 2/1000 | Loss: 0.00014298
Iteration 3/1000 | Loss: 0.00003411
Iteration 4/1000 | Loss: 0.00018675
Iteration 5/1000 | Loss: 0.00002397
Iteration 6/1000 | Loss: 0.00002215
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00019260
Iteration 12/1000 | Loss: 0.00007224
Iteration 13/1000 | Loss: 0.00030212
Iteration 14/1000 | Loss: 0.00001967
Iteration 15/1000 | Loss: 0.00006755
Iteration 16/1000 | Loss: 0.00009003
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001758
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001737
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00008914
Iteration 38/1000 | Loss: 0.00008913
Iteration 39/1000 | Loss: 0.00013463
Iteration 40/1000 | Loss: 0.00012730
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00009684
Iteration 54/1000 | Loss: 0.00003557
Iteration 55/1000 | Loss: 0.00001698
Iteration 56/1000 | Loss: 0.00001693
Iteration 57/1000 | Loss: 0.00001693
Iteration 58/1000 | Loss: 0.00001693
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00017366
Iteration 63/1000 | Loss: 0.00004323
Iteration 64/1000 | Loss: 0.00002804
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001683
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001681
Iteration 69/1000 | Loss: 0.00001681
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001679
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001678
Iteration 76/1000 | Loss: 0.00001678
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001677
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001677
Iteration 84/1000 | Loss: 0.00001677
Iteration 85/1000 | Loss: 0.00001677
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001675
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001673
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001667
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001666
Iteration 104/1000 | Loss: 0.00001666
Iteration 105/1000 | Loss: 0.00001666
Iteration 106/1000 | Loss: 0.00001665
Iteration 107/1000 | Loss: 0.00001665
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001664
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001664
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001662
Iteration 126/1000 | Loss: 0.00001662
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001661
Iteration 130/1000 | Loss: 0.00001661
Iteration 131/1000 | Loss: 0.00001661
Iteration 132/1000 | Loss: 0.00001661
Iteration 133/1000 | Loss: 0.00001661
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001659
Iteration 142/1000 | Loss: 0.00001658
Iteration 143/1000 | Loss: 0.00001658
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001655
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001655
Iteration 151/1000 | Loss: 0.00016031
Iteration 152/1000 | Loss: 0.00007015
Iteration 153/1000 | Loss: 0.00001796
Iteration 154/1000 | Loss: 0.00006648
Iteration 155/1000 | Loss: 0.00001675
Iteration 156/1000 | Loss: 0.00001650
Iteration 157/1000 | Loss: 0.00001650
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001648
Iteration 160/1000 | Loss: 0.00001647
Iteration 161/1000 | Loss: 0.00001647
Iteration 162/1000 | Loss: 0.00001646
Iteration 163/1000 | Loss: 0.00001646
Iteration 164/1000 | Loss: 0.00001646
Iteration 165/1000 | Loss: 0.00001646
Iteration 166/1000 | Loss: 0.00001646
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001646
Iteration 169/1000 | Loss: 0.00001646
Iteration 170/1000 | Loss: 0.00001645
Iteration 171/1000 | Loss: 0.00001645
Iteration 172/1000 | Loss: 0.00001645
Iteration 173/1000 | Loss: 0.00001645
Iteration 174/1000 | Loss: 0.00001645
Iteration 175/1000 | Loss: 0.00001645
Iteration 176/1000 | Loss: 0.00001645
Iteration 177/1000 | Loss: 0.00001645
Iteration 178/1000 | Loss: 0.00001645
Iteration 179/1000 | Loss: 0.00001645
Iteration 180/1000 | Loss: 0.00001645
Iteration 181/1000 | Loss: 0.00001644
Iteration 182/1000 | Loss: 0.00001644
Iteration 183/1000 | Loss: 0.00001643
Iteration 184/1000 | Loss: 0.00009055
Iteration 185/1000 | Loss: 0.00001757
Iteration 186/1000 | Loss: 0.00003741
Iteration 187/1000 | Loss: 0.00001658
Iteration 188/1000 | Loss: 0.00001646
Iteration 189/1000 | Loss: 0.00001646
Iteration 190/1000 | Loss: 0.00001646
Iteration 191/1000 | Loss: 0.00001645
Iteration 192/1000 | Loss: 0.00001644
Iteration 193/1000 | Loss: 0.00001644
Iteration 194/1000 | Loss: 0.00001644
Iteration 195/1000 | Loss: 0.00001644
Iteration 196/1000 | Loss: 0.00001644
Iteration 197/1000 | Loss: 0.00001643
Iteration 198/1000 | Loss: 0.00001643
Iteration 199/1000 | Loss: 0.00001643
Iteration 200/1000 | Loss: 0.00001642
Iteration 201/1000 | Loss: 0.00001642
Iteration 202/1000 | Loss: 0.00001642
Iteration 203/1000 | Loss: 0.00001642
Iteration 204/1000 | Loss: 0.00001642
Iteration 205/1000 | Loss: 0.00001642
Iteration 206/1000 | Loss: 0.00001641
Iteration 207/1000 | Loss: 0.00001641
Iteration 208/1000 | Loss: 0.00001641
Iteration 209/1000 | Loss: 0.00001641
Iteration 210/1000 | Loss: 0.00001641
Iteration 211/1000 | Loss: 0.00001641
Iteration 212/1000 | Loss: 0.00001641
Iteration 213/1000 | Loss: 0.00001641
Iteration 214/1000 | Loss: 0.00001641
Iteration 215/1000 | Loss: 0.00001641
Iteration 216/1000 | Loss: 0.00001641
Iteration 217/1000 | Loss: 0.00001641
Iteration 218/1000 | Loss: 0.00001641
Iteration 219/1000 | Loss: 0.00001640
Iteration 220/1000 | Loss: 0.00001640
Iteration 221/1000 | Loss: 0.00001640
Iteration 222/1000 | Loss: 0.00001640
Iteration 223/1000 | Loss: 0.00001640
Iteration 224/1000 | Loss: 0.00001640
Iteration 225/1000 | Loss: 0.00001640
Iteration 226/1000 | Loss: 0.00001640
Iteration 227/1000 | Loss: 0.00001640
Iteration 228/1000 | Loss: 0.00001640
Iteration 229/1000 | Loss: 0.00001640
Iteration 230/1000 | Loss: 0.00001640
Iteration 231/1000 | Loss: 0.00001640
Iteration 232/1000 | Loss: 0.00001640
Iteration 233/1000 | Loss: 0.00001640
Iteration 234/1000 | Loss: 0.00001640
Iteration 235/1000 | Loss: 0.00001640
Iteration 236/1000 | Loss: 0.00001640
Iteration 237/1000 | Loss: 0.00001640
Iteration 238/1000 | Loss: 0.00001640
Iteration 239/1000 | Loss: 0.00001640
Iteration 240/1000 | Loss: 0.00001640
Iteration 241/1000 | Loss: 0.00001640
Iteration 242/1000 | Loss: 0.00001640
Iteration 243/1000 | Loss: 0.00001640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.640433765714988e-05, 1.640433765714988e-05, 1.640433765714988e-05, 1.640433765714988e-05, 1.640433765714988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.640433765714988e-05

Optimization complete. Final v2v error: 3.433166265487671 mm

Highest mean error: 4.2079949378967285 mm for frame 183

Lowest mean error: 3.004741668701172 mm for frame 204

Saving results

Total time: 102.83067631721497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477078
Iteration 2/25 | Loss: 0.00155639
Iteration 3/25 | Loss: 0.00149774
Iteration 4/25 | Loss: 0.00148835
Iteration 5/25 | Loss: 0.00148496
Iteration 6/25 | Loss: 0.00148452
Iteration 7/25 | Loss: 0.00148452
Iteration 8/25 | Loss: 0.00148452
Iteration 9/25 | Loss: 0.00148452
Iteration 10/25 | Loss: 0.00148452
Iteration 11/25 | Loss: 0.00148452
Iteration 12/25 | Loss: 0.00148452
Iteration 13/25 | Loss: 0.00148452
Iteration 14/25 | Loss: 0.00148452
Iteration 15/25 | Loss: 0.00148452
Iteration 16/25 | Loss: 0.00148452
Iteration 17/25 | Loss: 0.00148452
Iteration 18/25 | Loss: 0.00148452
Iteration 19/25 | Loss: 0.00148452
Iteration 20/25 | Loss: 0.00148452
Iteration 21/25 | Loss: 0.00148452
Iteration 22/25 | Loss: 0.00148452
Iteration 23/25 | Loss: 0.00148452
Iteration 24/25 | Loss: 0.00148452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014845214318484068, 0.0014845214318484068, 0.0014845214318484068, 0.0014845214318484068, 0.0014845214318484068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014845214318484068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39475417
Iteration 2/25 | Loss: 0.00256661
Iteration 3/25 | Loss: 0.00256661
Iteration 4/25 | Loss: 0.00256661
Iteration 5/25 | Loss: 0.00256661
Iteration 6/25 | Loss: 0.00256661
Iteration 7/25 | Loss: 0.00256661
Iteration 8/25 | Loss: 0.00256661
Iteration 9/25 | Loss: 0.00256661
Iteration 10/25 | Loss: 0.00256661
Iteration 11/25 | Loss: 0.00256661
Iteration 12/25 | Loss: 0.00256661
Iteration 13/25 | Loss: 0.00256661
Iteration 14/25 | Loss: 0.00256661
Iteration 15/25 | Loss: 0.00256661
Iteration 16/25 | Loss: 0.00256661
Iteration 17/25 | Loss: 0.00256661
Iteration 18/25 | Loss: 0.00256661
Iteration 19/25 | Loss: 0.00256661
Iteration 20/25 | Loss: 0.00256661
Iteration 21/25 | Loss: 0.00256661
Iteration 22/25 | Loss: 0.00256661
Iteration 23/25 | Loss: 0.00256661
Iteration 24/25 | Loss: 0.00256661
Iteration 25/25 | Loss: 0.00256661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256661
Iteration 2/1000 | Loss: 0.00003556
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001852
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001548
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001470
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001459
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001458
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001436
Iteration 81/1000 | Loss: 0.00001436
Iteration 82/1000 | Loss: 0.00001435
Iteration 83/1000 | Loss: 0.00001435
Iteration 84/1000 | Loss: 0.00001435
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001434
Iteration 88/1000 | Loss: 0.00001434
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Iteration 100/1000 | Loss: 0.00001431
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001430
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001430
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001429
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001427
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001427
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001425
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001425
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001420
Iteration 161/1000 | Loss: 0.00001420
Iteration 162/1000 | Loss: 0.00001420
Iteration 163/1000 | Loss: 0.00001420
Iteration 164/1000 | Loss: 0.00001420
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001418
Iteration 182/1000 | Loss: 0.00001417
Iteration 183/1000 | Loss: 0.00001417
Iteration 184/1000 | Loss: 0.00001417
Iteration 185/1000 | Loss: 0.00001417
Iteration 186/1000 | Loss: 0.00001417
Iteration 187/1000 | Loss: 0.00001417
Iteration 188/1000 | Loss: 0.00001417
Iteration 189/1000 | Loss: 0.00001417
Iteration 190/1000 | Loss: 0.00001417
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001416
Iteration 193/1000 | Loss: 0.00001416
Iteration 194/1000 | Loss: 0.00001416
Iteration 195/1000 | Loss: 0.00001416
Iteration 196/1000 | Loss: 0.00001416
Iteration 197/1000 | Loss: 0.00001416
Iteration 198/1000 | Loss: 0.00001416
Iteration 199/1000 | Loss: 0.00001416
Iteration 200/1000 | Loss: 0.00001416
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Iteration 204/1000 | Loss: 0.00001415
Iteration 205/1000 | Loss: 0.00001415
Iteration 206/1000 | Loss: 0.00001415
Iteration 207/1000 | Loss: 0.00001415
Iteration 208/1000 | Loss: 0.00001415
Iteration 209/1000 | Loss: 0.00001415
Iteration 210/1000 | Loss: 0.00001415
Iteration 211/1000 | Loss: 0.00001414
Iteration 212/1000 | Loss: 0.00001414
Iteration 213/1000 | Loss: 0.00001414
Iteration 214/1000 | Loss: 0.00001414
Iteration 215/1000 | Loss: 0.00001414
Iteration 216/1000 | Loss: 0.00001414
Iteration 217/1000 | Loss: 0.00001414
Iteration 218/1000 | Loss: 0.00001414
Iteration 219/1000 | Loss: 0.00001414
Iteration 220/1000 | Loss: 0.00001414
Iteration 221/1000 | Loss: 0.00001414
Iteration 222/1000 | Loss: 0.00001414
Iteration 223/1000 | Loss: 0.00001414
Iteration 224/1000 | Loss: 0.00001414
Iteration 225/1000 | Loss: 0.00001414
Iteration 226/1000 | Loss: 0.00001414
Iteration 227/1000 | Loss: 0.00001413
Iteration 228/1000 | Loss: 0.00001413
Iteration 229/1000 | Loss: 0.00001413
Iteration 230/1000 | Loss: 0.00001413
Iteration 231/1000 | Loss: 0.00001413
Iteration 232/1000 | Loss: 0.00001413
Iteration 233/1000 | Loss: 0.00001413
Iteration 234/1000 | Loss: 0.00001413
Iteration 235/1000 | Loss: 0.00001413
Iteration 236/1000 | Loss: 0.00001413
Iteration 237/1000 | Loss: 0.00001413
Iteration 238/1000 | Loss: 0.00001413
Iteration 239/1000 | Loss: 0.00001412
Iteration 240/1000 | Loss: 0.00001412
Iteration 241/1000 | Loss: 0.00001412
Iteration 242/1000 | Loss: 0.00001412
Iteration 243/1000 | Loss: 0.00001412
Iteration 244/1000 | Loss: 0.00001412
Iteration 245/1000 | Loss: 0.00001412
Iteration 246/1000 | Loss: 0.00001412
Iteration 247/1000 | Loss: 0.00001412
Iteration 248/1000 | Loss: 0.00001411
Iteration 249/1000 | Loss: 0.00001411
Iteration 250/1000 | Loss: 0.00001411
Iteration 251/1000 | Loss: 0.00001411
Iteration 252/1000 | Loss: 0.00001411
Iteration 253/1000 | Loss: 0.00001411
Iteration 254/1000 | Loss: 0.00001411
Iteration 255/1000 | Loss: 0.00001411
Iteration 256/1000 | Loss: 0.00001411
Iteration 257/1000 | Loss: 0.00001411
Iteration 258/1000 | Loss: 0.00001411
Iteration 259/1000 | Loss: 0.00001411
Iteration 260/1000 | Loss: 0.00001411
Iteration 261/1000 | Loss: 0.00001411
Iteration 262/1000 | Loss: 0.00001410
Iteration 263/1000 | Loss: 0.00001410
Iteration 264/1000 | Loss: 0.00001410
Iteration 265/1000 | Loss: 0.00001410
Iteration 266/1000 | Loss: 0.00001410
Iteration 267/1000 | Loss: 0.00001410
Iteration 268/1000 | Loss: 0.00001410
Iteration 269/1000 | Loss: 0.00001410
Iteration 270/1000 | Loss: 0.00001410
Iteration 271/1000 | Loss: 0.00001410
Iteration 272/1000 | Loss: 0.00001410
Iteration 273/1000 | Loss: 0.00001410
Iteration 274/1000 | Loss: 0.00001410
Iteration 275/1000 | Loss: 0.00001410
Iteration 276/1000 | Loss: 0.00001410
Iteration 277/1000 | Loss: 0.00001410
Iteration 278/1000 | Loss: 0.00001410
Iteration 279/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.4099852705840021e-05, 1.4099852705840021e-05, 1.4099852705840021e-05, 1.4099852705840021e-05, 1.4099852705840021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4099852705840021e-05

Optimization complete. Final v2v error: 3.1876347064971924 mm

Highest mean error: 3.755251884460449 mm for frame 54

Lowest mean error: 2.952249050140381 mm for frame 97

Saving results

Total time: 49.304518938064575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945978
Iteration 2/25 | Loss: 0.00227744
Iteration 3/25 | Loss: 0.00198280
Iteration 4/25 | Loss: 0.00195442
Iteration 5/25 | Loss: 0.00195287
Iteration 6/25 | Loss: 0.00193069
Iteration 7/25 | Loss: 0.00188760
Iteration 8/25 | Loss: 0.00185989
Iteration 9/25 | Loss: 0.00183207
Iteration 10/25 | Loss: 0.00181842
Iteration 11/25 | Loss: 0.00180834
Iteration 12/25 | Loss: 0.00180428
Iteration 13/25 | Loss: 0.00180369
Iteration 14/25 | Loss: 0.00180319
Iteration 15/25 | Loss: 0.00180309
Iteration 16/25 | Loss: 0.00179998
Iteration 17/25 | Loss: 0.00179999
Iteration 18/25 | Loss: 0.00179912
Iteration 19/25 | Loss: 0.00179862
Iteration 20/25 | Loss: 0.00179534
Iteration 21/25 | Loss: 0.00179484
Iteration 22/25 | Loss: 0.00179462
Iteration 23/25 | Loss: 0.00179465
Iteration 24/25 | Loss: 0.00179051
Iteration 25/25 | Loss: 0.00178791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.16762382
Iteration 2/25 | Loss: 0.00386178
Iteration 3/25 | Loss: 0.00386178
Iteration 4/25 | Loss: 0.00386178
Iteration 5/25 | Loss: 0.00386178
Iteration 6/25 | Loss: 0.00386178
Iteration 7/25 | Loss: 0.00386178
Iteration 8/25 | Loss: 0.00386178
Iteration 9/25 | Loss: 0.00386178
Iteration 10/25 | Loss: 0.00386178
Iteration 11/25 | Loss: 0.00386178
Iteration 12/25 | Loss: 0.00386178
Iteration 13/25 | Loss: 0.00386178
Iteration 14/25 | Loss: 0.00386178
Iteration 15/25 | Loss: 0.00386178
Iteration 16/25 | Loss: 0.00386178
Iteration 17/25 | Loss: 0.00386178
Iteration 18/25 | Loss: 0.00386178
Iteration 19/25 | Loss: 0.00386178
Iteration 20/25 | Loss: 0.00386178
Iteration 21/25 | Loss: 0.00386178
Iteration 22/25 | Loss: 0.00386178
Iteration 23/25 | Loss: 0.00386178
Iteration 24/25 | Loss: 0.00386178
Iteration 25/25 | Loss: 0.00386178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00386178
Iteration 2/1000 | Loss: 0.00027209
Iteration 3/1000 | Loss: 0.00039305
Iteration 4/1000 | Loss: 0.00014874
Iteration 5/1000 | Loss: 0.00081336
Iteration 6/1000 | Loss: 0.00043649
Iteration 7/1000 | Loss: 0.00021586
Iteration 8/1000 | Loss: 0.00011261
Iteration 9/1000 | Loss: 0.00039383
Iteration 10/1000 | Loss: 0.00013759
Iteration 11/1000 | Loss: 0.00022499
Iteration 12/1000 | Loss: 0.00053112
Iteration 13/1000 | Loss: 0.00013086
Iteration 14/1000 | Loss: 0.00044629
Iteration 15/1000 | Loss: 0.00011215
Iteration 16/1000 | Loss: 0.00008489
Iteration 17/1000 | Loss: 0.00015340
Iteration 18/1000 | Loss: 0.00008025
Iteration 19/1000 | Loss: 0.00007520
Iteration 20/1000 | Loss: 0.00018335
Iteration 21/1000 | Loss: 0.00007101
Iteration 22/1000 | Loss: 0.00006792
Iteration 23/1000 | Loss: 0.00006471
Iteration 24/1000 | Loss: 0.00006302
Iteration 25/1000 | Loss: 0.00006155
Iteration 26/1000 | Loss: 0.00006008
Iteration 27/1000 | Loss: 0.00005907
Iteration 28/1000 | Loss: 0.00005838
Iteration 29/1000 | Loss: 0.00005768
Iteration 30/1000 | Loss: 0.00005725
Iteration 31/1000 | Loss: 0.00015558
Iteration 32/1000 | Loss: 0.00060651
Iteration 33/1000 | Loss: 0.00006198
Iteration 34/1000 | Loss: 0.00005667
Iteration 35/1000 | Loss: 0.00005417
Iteration 36/1000 | Loss: 0.00005284
Iteration 37/1000 | Loss: 0.00005155
Iteration 38/1000 | Loss: 0.00005056
Iteration 39/1000 | Loss: 0.00005010
Iteration 40/1000 | Loss: 0.00004968
Iteration 41/1000 | Loss: 0.00004934
Iteration 42/1000 | Loss: 0.00004908
Iteration 43/1000 | Loss: 0.00004875
Iteration 44/1000 | Loss: 0.00004853
Iteration 45/1000 | Loss: 0.00016488
Iteration 46/1000 | Loss: 0.00005034
Iteration 47/1000 | Loss: 0.00052770
Iteration 48/1000 | Loss: 0.00004896
Iteration 49/1000 | Loss: 0.00004634
Iteration 50/1000 | Loss: 0.00004502
Iteration 51/1000 | Loss: 0.00004419
Iteration 52/1000 | Loss: 0.00004327
Iteration 53/1000 | Loss: 0.00004276
Iteration 54/1000 | Loss: 0.00004248
Iteration 55/1000 | Loss: 0.00004234
Iteration 56/1000 | Loss: 0.00004217
Iteration 57/1000 | Loss: 0.00004204
Iteration 58/1000 | Loss: 0.00004193
Iteration 59/1000 | Loss: 0.00004191
Iteration 60/1000 | Loss: 0.00004191
Iteration 61/1000 | Loss: 0.00004191
Iteration 62/1000 | Loss: 0.00004190
Iteration 63/1000 | Loss: 0.00004189
Iteration 64/1000 | Loss: 0.00004189
Iteration 65/1000 | Loss: 0.00004189
Iteration 66/1000 | Loss: 0.00004186
Iteration 67/1000 | Loss: 0.00004186
Iteration 68/1000 | Loss: 0.00004186
Iteration 69/1000 | Loss: 0.00004186
Iteration 70/1000 | Loss: 0.00004186
Iteration 71/1000 | Loss: 0.00004185
Iteration 72/1000 | Loss: 0.00004185
Iteration 73/1000 | Loss: 0.00004185
Iteration 74/1000 | Loss: 0.00004185
Iteration 75/1000 | Loss: 0.00004185
Iteration 76/1000 | Loss: 0.00004184
Iteration 77/1000 | Loss: 0.00004184
Iteration 78/1000 | Loss: 0.00004183
Iteration 79/1000 | Loss: 0.00004183
Iteration 80/1000 | Loss: 0.00004183
Iteration 81/1000 | Loss: 0.00004183
Iteration 82/1000 | Loss: 0.00004182
Iteration 83/1000 | Loss: 0.00004182
Iteration 84/1000 | Loss: 0.00004182
Iteration 85/1000 | Loss: 0.00004182
Iteration 86/1000 | Loss: 0.00004181
Iteration 87/1000 | Loss: 0.00004181
Iteration 88/1000 | Loss: 0.00004181
Iteration 89/1000 | Loss: 0.00004181
Iteration 90/1000 | Loss: 0.00004181
Iteration 91/1000 | Loss: 0.00004180
Iteration 92/1000 | Loss: 0.00004180
Iteration 93/1000 | Loss: 0.00004180
Iteration 94/1000 | Loss: 0.00004179
Iteration 95/1000 | Loss: 0.00004179
Iteration 96/1000 | Loss: 0.00004178
Iteration 97/1000 | Loss: 0.00004178
Iteration 98/1000 | Loss: 0.00004178
Iteration 99/1000 | Loss: 0.00004178
Iteration 100/1000 | Loss: 0.00004177
Iteration 101/1000 | Loss: 0.00004177
Iteration 102/1000 | Loss: 0.00004177
Iteration 103/1000 | Loss: 0.00004177
Iteration 104/1000 | Loss: 0.00004177
Iteration 105/1000 | Loss: 0.00004177
Iteration 106/1000 | Loss: 0.00004177
Iteration 107/1000 | Loss: 0.00004177
Iteration 108/1000 | Loss: 0.00004177
Iteration 109/1000 | Loss: 0.00004176
Iteration 110/1000 | Loss: 0.00004176
Iteration 111/1000 | Loss: 0.00004176
Iteration 112/1000 | Loss: 0.00004176
Iteration 113/1000 | Loss: 0.00004176
Iteration 114/1000 | Loss: 0.00004176
Iteration 115/1000 | Loss: 0.00004176
Iteration 116/1000 | Loss: 0.00004176
Iteration 117/1000 | Loss: 0.00004176
Iteration 118/1000 | Loss: 0.00004176
Iteration 119/1000 | Loss: 0.00004176
Iteration 120/1000 | Loss: 0.00004176
Iteration 121/1000 | Loss: 0.00004176
Iteration 122/1000 | Loss: 0.00004176
Iteration 123/1000 | Loss: 0.00004176
Iteration 124/1000 | Loss: 0.00004175
Iteration 125/1000 | Loss: 0.00004175
Iteration 126/1000 | Loss: 0.00004175
Iteration 127/1000 | Loss: 0.00004175
Iteration 128/1000 | Loss: 0.00004175
Iteration 129/1000 | Loss: 0.00004175
Iteration 130/1000 | Loss: 0.00004175
Iteration 131/1000 | Loss: 0.00004175
Iteration 132/1000 | Loss: 0.00004175
Iteration 133/1000 | Loss: 0.00004175
Iteration 134/1000 | Loss: 0.00004175
Iteration 135/1000 | Loss: 0.00004174
Iteration 136/1000 | Loss: 0.00004174
Iteration 137/1000 | Loss: 0.00004174
Iteration 138/1000 | Loss: 0.00004174
Iteration 139/1000 | Loss: 0.00004174
Iteration 140/1000 | Loss: 0.00004173
Iteration 141/1000 | Loss: 0.00004173
Iteration 142/1000 | Loss: 0.00004173
Iteration 143/1000 | Loss: 0.00004173
Iteration 144/1000 | Loss: 0.00004173
Iteration 145/1000 | Loss: 0.00004172
Iteration 146/1000 | Loss: 0.00004172
Iteration 147/1000 | Loss: 0.00004172
Iteration 148/1000 | Loss: 0.00004172
Iteration 149/1000 | Loss: 0.00004172
Iteration 150/1000 | Loss: 0.00004171
Iteration 151/1000 | Loss: 0.00004171
Iteration 152/1000 | Loss: 0.00004171
Iteration 153/1000 | Loss: 0.00004171
Iteration 154/1000 | Loss: 0.00004171
Iteration 155/1000 | Loss: 0.00004171
Iteration 156/1000 | Loss: 0.00004171
Iteration 157/1000 | Loss: 0.00004171
Iteration 158/1000 | Loss: 0.00004171
Iteration 159/1000 | Loss: 0.00004171
Iteration 160/1000 | Loss: 0.00004171
Iteration 161/1000 | Loss: 0.00004171
Iteration 162/1000 | Loss: 0.00004171
Iteration 163/1000 | Loss: 0.00004171
Iteration 164/1000 | Loss: 0.00004171
Iteration 165/1000 | Loss: 0.00004170
Iteration 166/1000 | Loss: 0.00004170
Iteration 167/1000 | Loss: 0.00004170
Iteration 168/1000 | Loss: 0.00004170
Iteration 169/1000 | Loss: 0.00004170
Iteration 170/1000 | Loss: 0.00004170
Iteration 171/1000 | Loss: 0.00004170
Iteration 172/1000 | Loss: 0.00004170
Iteration 173/1000 | Loss: 0.00004170
Iteration 174/1000 | Loss: 0.00004170
Iteration 175/1000 | Loss: 0.00004170
Iteration 176/1000 | Loss: 0.00004170
Iteration 177/1000 | Loss: 0.00004170
Iteration 178/1000 | Loss: 0.00004170
Iteration 179/1000 | Loss: 0.00004170
Iteration 180/1000 | Loss: 0.00004170
Iteration 181/1000 | Loss: 0.00004170
Iteration 182/1000 | Loss: 0.00004169
Iteration 183/1000 | Loss: 0.00004169
Iteration 184/1000 | Loss: 0.00004169
Iteration 185/1000 | Loss: 0.00004169
Iteration 186/1000 | Loss: 0.00004169
Iteration 187/1000 | Loss: 0.00004169
Iteration 188/1000 | Loss: 0.00004169
Iteration 189/1000 | Loss: 0.00004169
Iteration 190/1000 | Loss: 0.00004169
Iteration 191/1000 | Loss: 0.00004169
Iteration 192/1000 | Loss: 0.00004169
Iteration 193/1000 | Loss: 0.00004169
Iteration 194/1000 | Loss: 0.00004169
Iteration 195/1000 | Loss: 0.00004169
Iteration 196/1000 | Loss: 0.00004169
Iteration 197/1000 | Loss: 0.00004169
Iteration 198/1000 | Loss: 0.00004169
Iteration 199/1000 | Loss: 0.00004168
Iteration 200/1000 | Loss: 0.00004168
Iteration 201/1000 | Loss: 0.00004168
Iteration 202/1000 | Loss: 0.00004168
Iteration 203/1000 | Loss: 0.00004168
Iteration 204/1000 | Loss: 0.00004168
Iteration 205/1000 | Loss: 0.00004168
Iteration 206/1000 | Loss: 0.00004168
Iteration 207/1000 | Loss: 0.00004168
Iteration 208/1000 | Loss: 0.00004168
Iteration 209/1000 | Loss: 0.00004168
Iteration 210/1000 | Loss: 0.00004168
Iteration 211/1000 | Loss: 0.00004168
Iteration 212/1000 | Loss: 0.00004168
Iteration 213/1000 | Loss: 0.00004168
Iteration 214/1000 | Loss: 0.00004168
Iteration 215/1000 | Loss: 0.00004168
Iteration 216/1000 | Loss: 0.00004168
Iteration 217/1000 | Loss: 0.00004168
Iteration 218/1000 | Loss: 0.00004168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [4.16806542489212e-05, 4.16806542489212e-05, 4.16806542489212e-05, 4.16806542489212e-05, 4.16806542489212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.16806542489212e-05

Optimization complete. Final v2v error: 5.017540454864502 mm

Highest mean error: 12.051526069641113 mm for frame 21

Lowest mean error: 4.16037130355835 mm for frame 84

Saving results

Total time: 132.80921125411987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429327
Iteration 2/25 | Loss: 0.00149661
Iteration 3/25 | Loss: 0.00145117
Iteration 4/25 | Loss: 0.00144541
Iteration 5/25 | Loss: 0.00144356
Iteration 6/25 | Loss: 0.00144345
Iteration 7/25 | Loss: 0.00144345
Iteration 8/25 | Loss: 0.00144345
Iteration 9/25 | Loss: 0.00144345
Iteration 10/25 | Loss: 0.00144345
Iteration 11/25 | Loss: 0.00144345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014434525510296226, 0.0014434525510296226, 0.0014434525510296226, 0.0014434525510296226, 0.0014434525510296226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014434525510296226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91809249
Iteration 2/25 | Loss: 0.00243308
Iteration 3/25 | Loss: 0.00243308
Iteration 4/25 | Loss: 0.00243308
Iteration 5/25 | Loss: 0.00243308
Iteration 6/25 | Loss: 0.00243308
Iteration 7/25 | Loss: 0.00243308
Iteration 8/25 | Loss: 0.00243308
Iteration 9/25 | Loss: 0.00243308
Iteration 10/25 | Loss: 0.00243308
Iteration 11/25 | Loss: 0.00243307
Iteration 12/25 | Loss: 0.00243307
Iteration 13/25 | Loss: 0.00243308
Iteration 14/25 | Loss: 0.00243307
Iteration 15/25 | Loss: 0.00243307
Iteration 16/25 | Loss: 0.00243307
Iteration 17/25 | Loss: 0.00243307
Iteration 18/25 | Loss: 0.00243308
Iteration 19/25 | Loss: 0.00243307
Iteration 20/25 | Loss: 0.00243307
Iteration 21/25 | Loss: 0.00243307
Iteration 22/25 | Loss: 0.00243307
Iteration 23/25 | Loss: 0.00243307
Iteration 24/25 | Loss: 0.00243307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0024330748710781336, 0.0024330748710781336, 0.0024330748710781336, 0.0024330748710781336, 0.0024330748710781336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024330748710781336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243307
Iteration 2/1000 | Loss: 0.00002692
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00001564
Iteration 7/1000 | Loss: 0.00001522
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001432
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001364
Iteration 15/1000 | Loss: 0.00001357
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001347
Iteration 19/1000 | Loss: 0.00001330
Iteration 20/1000 | Loss: 0.00001319
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001281
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001280
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001278
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001274
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001267
Iteration 86/1000 | Loss: 0.00001267
Iteration 87/1000 | Loss: 0.00001267
Iteration 88/1000 | Loss: 0.00001267
Iteration 89/1000 | Loss: 0.00001266
Iteration 90/1000 | Loss: 0.00001266
Iteration 91/1000 | Loss: 0.00001266
Iteration 92/1000 | Loss: 0.00001266
Iteration 93/1000 | Loss: 0.00001266
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001265
Iteration 96/1000 | Loss: 0.00001265
Iteration 97/1000 | Loss: 0.00001265
Iteration 98/1000 | Loss: 0.00001265
Iteration 99/1000 | Loss: 0.00001265
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001262
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001261
Iteration 114/1000 | Loss: 0.00001261
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001260
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00001260
Iteration 121/1000 | Loss: 0.00001260
Iteration 122/1000 | Loss: 0.00001260
Iteration 123/1000 | Loss: 0.00001260
Iteration 124/1000 | Loss: 0.00001260
Iteration 125/1000 | Loss: 0.00001260
Iteration 126/1000 | Loss: 0.00001259
Iteration 127/1000 | Loss: 0.00001259
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001259
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001258
Iteration 133/1000 | Loss: 0.00001258
Iteration 134/1000 | Loss: 0.00001258
Iteration 135/1000 | Loss: 0.00001257
Iteration 136/1000 | Loss: 0.00001257
Iteration 137/1000 | Loss: 0.00001257
Iteration 138/1000 | Loss: 0.00001257
Iteration 139/1000 | Loss: 0.00001257
Iteration 140/1000 | Loss: 0.00001257
Iteration 141/1000 | Loss: 0.00001257
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.256874020327814e-05, 1.256874020327814e-05, 1.256874020327814e-05, 1.256874020327814e-05, 1.256874020327814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.256874020327814e-05

Optimization complete. Final v2v error: 3.1055078506469727 mm

Highest mean error: 3.28889799118042 mm for frame 106

Lowest mean error: 2.975456476211548 mm for frame 177

Saving results

Total time: 43.27997159957886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757728
Iteration 2/25 | Loss: 0.00190156
Iteration 3/25 | Loss: 0.00166122
Iteration 4/25 | Loss: 0.00163647
Iteration 5/25 | Loss: 0.00160867
Iteration 6/25 | Loss: 0.00161480
Iteration 7/25 | Loss: 0.00163007
Iteration 8/25 | Loss: 0.00161447
Iteration 9/25 | Loss: 0.00160214
Iteration 10/25 | Loss: 0.00160086
Iteration 11/25 | Loss: 0.00160554
Iteration 12/25 | Loss: 0.00159401
Iteration 13/25 | Loss: 0.00158394
Iteration 14/25 | Loss: 0.00157871
Iteration 15/25 | Loss: 0.00157700
Iteration 16/25 | Loss: 0.00157841
Iteration 17/25 | Loss: 0.00158062
Iteration 18/25 | Loss: 0.00157434
Iteration 19/25 | Loss: 0.00157379
Iteration 20/25 | Loss: 0.00157096
Iteration 21/25 | Loss: 0.00157175
Iteration 22/25 | Loss: 0.00157186
Iteration 23/25 | Loss: 0.00157653
Iteration 24/25 | Loss: 0.00157476
Iteration 25/25 | Loss: 0.00156928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22156417
Iteration 2/25 | Loss: 0.00229027
Iteration 3/25 | Loss: 0.00229026
Iteration 4/25 | Loss: 0.00229026
Iteration 5/25 | Loss: 0.00229026
Iteration 6/25 | Loss: 0.00229026
Iteration 7/25 | Loss: 0.00229026
Iteration 8/25 | Loss: 0.00229026
Iteration 9/25 | Loss: 0.00229026
Iteration 10/25 | Loss: 0.00229026
Iteration 11/25 | Loss: 0.00229026
Iteration 12/25 | Loss: 0.00229026
Iteration 13/25 | Loss: 0.00229026
Iteration 14/25 | Loss: 0.00229026
Iteration 15/25 | Loss: 0.00229026
Iteration 16/25 | Loss: 0.00229026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022902577184140682, 0.0022902577184140682, 0.0022902577184140682, 0.0022902577184140682, 0.0022902577184140682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022902577184140682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229026
Iteration 2/1000 | Loss: 0.00005600
Iteration 3/1000 | Loss: 0.00004273
Iteration 4/1000 | Loss: 0.00004887
Iteration 5/1000 | Loss: 0.00004389
Iteration 6/1000 | Loss: 0.00003974
Iteration 7/1000 | Loss: 0.00003276
Iteration 8/1000 | Loss: 0.00003834
Iteration 9/1000 | Loss: 0.00020907
Iteration 10/1000 | Loss: 0.00015502
Iteration 11/1000 | Loss: 0.00017021
Iteration 12/1000 | Loss: 0.00017223
Iteration 13/1000 | Loss: 0.00019321
Iteration 14/1000 | Loss: 0.00018068
Iteration 15/1000 | Loss: 0.00005865
Iteration 16/1000 | Loss: 0.00003736
Iteration 17/1000 | Loss: 0.00004192
Iteration 18/1000 | Loss: 0.00003911
Iteration 19/1000 | Loss: 0.00003716
Iteration 20/1000 | Loss: 0.00004686
Iteration 21/1000 | Loss: 0.00004470
Iteration 22/1000 | Loss: 0.00004986
Iteration 23/1000 | Loss: 0.00021406
Iteration 24/1000 | Loss: 0.00019740
Iteration 25/1000 | Loss: 0.00023880
Iteration 26/1000 | Loss: 0.00020384
Iteration 27/1000 | Loss: 0.00021353
Iteration 28/1000 | Loss: 0.00025684
Iteration 29/1000 | Loss: 0.00010598
Iteration 30/1000 | Loss: 0.00004756
Iteration 31/1000 | Loss: 0.00003690
Iteration 32/1000 | Loss: 0.00003372
Iteration 33/1000 | Loss: 0.00004335
Iteration 34/1000 | Loss: 0.00003534
Iteration 35/1000 | Loss: 0.00003227
Iteration 36/1000 | Loss: 0.00004043
Iteration 37/1000 | Loss: 0.00003378
Iteration 38/1000 | Loss: 0.00004196
Iteration 39/1000 | Loss: 0.00004438
Iteration 40/1000 | Loss: 0.00003836
Iteration 41/1000 | Loss: 0.00004156
Iteration 42/1000 | Loss: 0.00004214
Iteration 43/1000 | Loss: 0.00004472
Iteration 44/1000 | Loss: 0.00003838
Iteration 45/1000 | Loss: 0.00004043
Iteration 46/1000 | Loss: 0.00004219
Iteration 47/1000 | Loss: 0.00004177
Iteration 48/1000 | Loss: 0.00004177
Iteration 49/1000 | Loss: 0.00004192
Iteration 50/1000 | Loss: 0.00003818
Iteration 51/1000 | Loss: 0.00004269
Iteration 52/1000 | Loss: 0.00003255
Iteration 53/1000 | Loss: 0.00003644
Iteration 54/1000 | Loss: 0.00003854
Iteration 55/1000 | Loss: 0.00004145
Iteration 56/1000 | Loss: 0.00004403
Iteration 57/1000 | Loss: 0.00004128
Iteration 58/1000 | Loss: 0.00004346
Iteration 59/1000 | Loss: 0.00004110
Iteration 60/1000 | Loss: 0.00004114
Iteration 61/1000 | Loss: 0.00004163
Iteration 62/1000 | Loss: 0.00004426
Iteration 63/1000 | Loss: 0.00004072
Iteration 64/1000 | Loss: 0.00004084
Iteration 65/1000 | Loss: 0.00003614
Iteration 66/1000 | Loss: 0.00003765
Iteration 67/1000 | Loss: 0.00003572
Iteration 68/1000 | Loss: 0.00003688
Iteration 69/1000 | Loss: 0.00003789
Iteration 70/1000 | Loss: 0.00003930
Iteration 71/1000 | Loss: 0.00004080
Iteration 72/1000 | Loss: 0.00003145
Iteration 73/1000 | Loss: 0.00003033
Iteration 74/1000 | Loss: 0.00002861
Iteration 75/1000 | Loss: 0.00002771
Iteration 76/1000 | Loss: 0.00002745
Iteration 77/1000 | Loss: 0.00002730
Iteration 78/1000 | Loss: 0.00002723
Iteration 79/1000 | Loss: 0.00002723
Iteration 80/1000 | Loss: 0.00002722
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00002720
Iteration 83/1000 | Loss: 0.00002716
Iteration 84/1000 | Loss: 0.00002715
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002711
Iteration 87/1000 | Loss: 0.00002710
Iteration 88/1000 | Loss: 0.00002706
Iteration 89/1000 | Loss: 0.00002706
Iteration 90/1000 | Loss: 0.00002705
Iteration 91/1000 | Loss: 0.00002698
Iteration 92/1000 | Loss: 0.00002696
Iteration 93/1000 | Loss: 0.00002694
Iteration 94/1000 | Loss: 0.00002691
Iteration 95/1000 | Loss: 0.00013070
Iteration 96/1000 | Loss: 0.00004339
Iteration 97/1000 | Loss: 0.00003005
Iteration 98/1000 | Loss: 0.00002809
Iteration 99/1000 | Loss: 0.00002744
Iteration 100/1000 | Loss: 0.00002685
Iteration 101/1000 | Loss: 0.00002629
Iteration 102/1000 | Loss: 0.00002586
Iteration 103/1000 | Loss: 0.00002558
Iteration 104/1000 | Loss: 0.00002544
Iteration 105/1000 | Loss: 0.00002541
Iteration 106/1000 | Loss: 0.00002540
Iteration 107/1000 | Loss: 0.00002538
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002536
Iteration 110/1000 | Loss: 0.00002535
Iteration 111/1000 | Loss: 0.00002535
Iteration 112/1000 | Loss: 0.00002534
Iteration 113/1000 | Loss: 0.00002533
Iteration 114/1000 | Loss: 0.00002533
Iteration 115/1000 | Loss: 0.00002532
Iteration 116/1000 | Loss: 0.00002532
Iteration 117/1000 | Loss: 0.00002531
Iteration 118/1000 | Loss: 0.00002531
Iteration 119/1000 | Loss: 0.00002530
Iteration 120/1000 | Loss: 0.00002530
Iteration 121/1000 | Loss: 0.00002527
Iteration 122/1000 | Loss: 0.00002525
Iteration 123/1000 | Loss: 0.00002524
Iteration 124/1000 | Loss: 0.00002524
Iteration 125/1000 | Loss: 0.00002523
Iteration 126/1000 | Loss: 0.00002523
Iteration 127/1000 | Loss: 0.00002523
Iteration 128/1000 | Loss: 0.00002523
Iteration 129/1000 | Loss: 0.00002523
Iteration 130/1000 | Loss: 0.00002523
Iteration 131/1000 | Loss: 0.00002523
Iteration 132/1000 | Loss: 0.00002523
Iteration 133/1000 | Loss: 0.00002523
Iteration 134/1000 | Loss: 0.00002523
Iteration 135/1000 | Loss: 0.00002523
Iteration 136/1000 | Loss: 0.00002523
Iteration 137/1000 | Loss: 0.00002522
Iteration 138/1000 | Loss: 0.00002522
Iteration 139/1000 | Loss: 0.00002522
Iteration 140/1000 | Loss: 0.00002522
Iteration 141/1000 | Loss: 0.00002522
Iteration 142/1000 | Loss: 0.00002522
Iteration 143/1000 | Loss: 0.00002522
Iteration 144/1000 | Loss: 0.00002522
Iteration 145/1000 | Loss: 0.00002522
Iteration 146/1000 | Loss: 0.00002522
Iteration 147/1000 | Loss: 0.00002521
Iteration 148/1000 | Loss: 0.00002521
Iteration 149/1000 | Loss: 0.00002521
Iteration 150/1000 | Loss: 0.00002521
Iteration 151/1000 | Loss: 0.00002520
Iteration 152/1000 | Loss: 0.00002520
Iteration 153/1000 | Loss: 0.00002520
Iteration 154/1000 | Loss: 0.00002520
Iteration 155/1000 | Loss: 0.00002520
Iteration 156/1000 | Loss: 0.00002520
Iteration 157/1000 | Loss: 0.00002519
Iteration 158/1000 | Loss: 0.00002519
Iteration 159/1000 | Loss: 0.00002519
Iteration 160/1000 | Loss: 0.00002519
Iteration 161/1000 | Loss: 0.00002519
Iteration 162/1000 | Loss: 0.00002519
Iteration 163/1000 | Loss: 0.00002519
Iteration 164/1000 | Loss: 0.00002519
Iteration 165/1000 | Loss: 0.00002519
Iteration 166/1000 | Loss: 0.00002519
Iteration 167/1000 | Loss: 0.00002519
Iteration 168/1000 | Loss: 0.00002519
Iteration 169/1000 | Loss: 0.00002519
Iteration 170/1000 | Loss: 0.00002519
Iteration 171/1000 | Loss: 0.00002518
Iteration 172/1000 | Loss: 0.00002518
Iteration 173/1000 | Loss: 0.00002518
Iteration 174/1000 | Loss: 0.00002518
Iteration 175/1000 | Loss: 0.00002518
Iteration 176/1000 | Loss: 0.00002518
Iteration 177/1000 | Loss: 0.00002517
Iteration 178/1000 | Loss: 0.00002517
Iteration 179/1000 | Loss: 0.00002517
Iteration 180/1000 | Loss: 0.00002517
Iteration 181/1000 | Loss: 0.00002516
Iteration 182/1000 | Loss: 0.00002516
Iteration 183/1000 | Loss: 0.00002516
Iteration 184/1000 | Loss: 0.00002516
Iteration 185/1000 | Loss: 0.00002516
Iteration 186/1000 | Loss: 0.00002516
Iteration 187/1000 | Loss: 0.00002516
Iteration 188/1000 | Loss: 0.00002516
Iteration 189/1000 | Loss: 0.00002516
Iteration 190/1000 | Loss: 0.00002515
Iteration 191/1000 | Loss: 0.00002515
Iteration 192/1000 | Loss: 0.00002515
Iteration 193/1000 | Loss: 0.00002515
Iteration 194/1000 | Loss: 0.00002515
Iteration 195/1000 | Loss: 0.00002515
Iteration 196/1000 | Loss: 0.00002515
Iteration 197/1000 | Loss: 0.00002514
Iteration 198/1000 | Loss: 0.00002514
Iteration 199/1000 | Loss: 0.00002514
Iteration 200/1000 | Loss: 0.00002514
Iteration 201/1000 | Loss: 0.00002514
Iteration 202/1000 | Loss: 0.00002514
Iteration 203/1000 | Loss: 0.00002514
Iteration 204/1000 | Loss: 0.00002514
Iteration 205/1000 | Loss: 0.00002514
Iteration 206/1000 | Loss: 0.00002513
Iteration 207/1000 | Loss: 0.00002513
Iteration 208/1000 | Loss: 0.00002513
Iteration 209/1000 | Loss: 0.00002512
Iteration 210/1000 | Loss: 0.00002512
Iteration 211/1000 | Loss: 0.00002512
Iteration 212/1000 | Loss: 0.00002512
Iteration 213/1000 | Loss: 0.00002512
Iteration 214/1000 | Loss: 0.00002512
Iteration 215/1000 | Loss: 0.00002512
Iteration 216/1000 | Loss: 0.00002512
Iteration 217/1000 | Loss: 0.00002512
Iteration 218/1000 | Loss: 0.00002511
Iteration 219/1000 | Loss: 0.00002511
Iteration 220/1000 | Loss: 0.00002511
Iteration 221/1000 | Loss: 0.00002511
Iteration 222/1000 | Loss: 0.00002510
Iteration 223/1000 | Loss: 0.00002510
Iteration 224/1000 | Loss: 0.00002509
Iteration 225/1000 | Loss: 0.00002509
Iteration 226/1000 | Loss: 0.00002509
Iteration 227/1000 | Loss: 0.00002509
Iteration 228/1000 | Loss: 0.00002509
Iteration 229/1000 | Loss: 0.00002509
Iteration 230/1000 | Loss: 0.00002509
Iteration 231/1000 | Loss: 0.00002509
Iteration 232/1000 | Loss: 0.00002509
Iteration 233/1000 | Loss: 0.00002509
Iteration 234/1000 | Loss: 0.00002508
Iteration 235/1000 | Loss: 0.00002508
Iteration 236/1000 | Loss: 0.00002508
Iteration 237/1000 | Loss: 0.00002508
Iteration 238/1000 | Loss: 0.00002507
Iteration 239/1000 | Loss: 0.00002507
Iteration 240/1000 | Loss: 0.00002507
Iteration 241/1000 | Loss: 0.00002507
Iteration 242/1000 | Loss: 0.00002507
Iteration 243/1000 | Loss: 0.00002507
Iteration 244/1000 | Loss: 0.00002507
Iteration 245/1000 | Loss: 0.00002507
Iteration 246/1000 | Loss: 0.00002507
Iteration 247/1000 | Loss: 0.00002507
Iteration 248/1000 | Loss: 0.00002507
Iteration 249/1000 | Loss: 0.00002507
Iteration 250/1000 | Loss: 0.00002506
Iteration 251/1000 | Loss: 0.00002505
Iteration 252/1000 | Loss: 0.00002505
Iteration 253/1000 | Loss: 0.00002505
Iteration 254/1000 | Loss: 0.00002505
Iteration 255/1000 | Loss: 0.00002504
Iteration 256/1000 | Loss: 0.00002504
Iteration 257/1000 | Loss: 0.00002504
Iteration 258/1000 | Loss: 0.00002504
Iteration 259/1000 | Loss: 0.00002504
Iteration 260/1000 | Loss: 0.00002503
Iteration 261/1000 | Loss: 0.00002503
Iteration 262/1000 | Loss: 0.00002503
Iteration 263/1000 | Loss: 0.00002503
Iteration 264/1000 | Loss: 0.00002503
Iteration 265/1000 | Loss: 0.00002502
Iteration 266/1000 | Loss: 0.00002502
Iteration 267/1000 | Loss: 0.00002502
Iteration 268/1000 | Loss: 0.00002502
Iteration 269/1000 | Loss: 0.00002502
Iteration 270/1000 | Loss: 0.00002502
Iteration 271/1000 | Loss: 0.00002502
Iteration 272/1000 | Loss: 0.00002502
Iteration 273/1000 | Loss: 0.00002502
Iteration 274/1000 | Loss: 0.00002502
Iteration 275/1000 | Loss: 0.00002502
Iteration 276/1000 | Loss: 0.00002502
Iteration 277/1000 | Loss: 0.00002502
Iteration 278/1000 | Loss: 0.00002501
Iteration 279/1000 | Loss: 0.00002501
Iteration 280/1000 | Loss: 0.00002501
Iteration 281/1000 | Loss: 0.00002501
Iteration 282/1000 | Loss: 0.00002501
Iteration 283/1000 | Loss: 0.00002501
Iteration 284/1000 | Loss: 0.00002501
Iteration 285/1000 | Loss: 0.00002501
Iteration 286/1000 | Loss: 0.00002501
Iteration 287/1000 | Loss: 0.00002501
Iteration 288/1000 | Loss: 0.00002501
Iteration 289/1000 | Loss: 0.00002501
Iteration 290/1000 | Loss: 0.00002501
Iteration 291/1000 | Loss: 0.00002501
Iteration 292/1000 | Loss: 0.00002501
Iteration 293/1000 | Loss: 0.00002501
Iteration 294/1000 | Loss: 0.00002501
Iteration 295/1000 | Loss: 0.00002501
Iteration 296/1000 | Loss: 0.00002501
Iteration 297/1000 | Loss: 0.00002501
Iteration 298/1000 | Loss: 0.00002501
Iteration 299/1000 | Loss: 0.00002501
Iteration 300/1000 | Loss: 0.00002501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [2.5011446268763393e-05, 2.5011446268763393e-05, 2.5011446268763393e-05, 2.5011446268763393e-05, 2.5011446268763393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5011446268763393e-05

Optimization complete. Final v2v error: 4.151233673095703 mm

Highest mean error: 5.137453556060791 mm for frame 104

Lowest mean error: 3.4572744369506836 mm for frame 189

Saving results

Total time: 207.770920753479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757854
Iteration 2/25 | Loss: 0.00163799
Iteration 3/25 | Loss: 0.00149364
Iteration 4/25 | Loss: 0.00147692
Iteration 5/25 | Loss: 0.00147264
Iteration 6/25 | Loss: 0.00147186
Iteration 7/25 | Loss: 0.00147186
Iteration 8/25 | Loss: 0.00147186
Iteration 9/25 | Loss: 0.00147186
Iteration 10/25 | Loss: 0.00147186
Iteration 11/25 | Loss: 0.00147186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001471858937293291, 0.001471858937293291, 0.001471858937293291, 0.001471858937293291, 0.001471858937293291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001471858937293291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14927459
Iteration 2/25 | Loss: 0.00217342
Iteration 3/25 | Loss: 0.00217340
Iteration 4/25 | Loss: 0.00217340
Iteration 5/25 | Loss: 0.00217340
Iteration 6/25 | Loss: 0.00217340
Iteration 7/25 | Loss: 0.00217340
Iteration 8/25 | Loss: 0.00217340
Iteration 9/25 | Loss: 0.00217340
Iteration 10/25 | Loss: 0.00217340
Iteration 11/25 | Loss: 0.00217340
Iteration 12/25 | Loss: 0.00217340
Iteration 13/25 | Loss: 0.00217340
Iteration 14/25 | Loss: 0.00217340
Iteration 15/25 | Loss: 0.00217340
Iteration 16/25 | Loss: 0.00217340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021734004840254784, 0.0021734004840254784, 0.0021734004840254784, 0.0021734004840254784, 0.0021734004840254784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021734004840254784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217340
Iteration 2/1000 | Loss: 0.00004904
Iteration 3/1000 | Loss: 0.00003533
Iteration 4/1000 | Loss: 0.00002768
Iteration 5/1000 | Loss: 0.00002514
Iteration 6/1000 | Loss: 0.00002346
Iteration 7/1000 | Loss: 0.00002254
Iteration 8/1000 | Loss: 0.00002188
Iteration 9/1000 | Loss: 0.00002118
Iteration 10/1000 | Loss: 0.00002077
Iteration 11/1000 | Loss: 0.00002044
Iteration 12/1000 | Loss: 0.00002005
Iteration 13/1000 | Loss: 0.00001977
Iteration 14/1000 | Loss: 0.00001953
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001941
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001917
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001875
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001866
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001863
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001862
Iteration 59/1000 | Loss: 0.00001862
Iteration 60/1000 | Loss: 0.00001862
Iteration 61/1000 | Loss: 0.00001862
Iteration 62/1000 | Loss: 0.00001862
Iteration 63/1000 | Loss: 0.00001862
Iteration 64/1000 | Loss: 0.00001861
Iteration 65/1000 | Loss: 0.00001861
Iteration 66/1000 | Loss: 0.00001861
Iteration 67/1000 | Loss: 0.00001861
Iteration 68/1000 | Loss: 0.00001861
Iteration 69/1000 | Loss: 0.00001861
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001859
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001845
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001844
Iteration 128/1000 | Loss: 0.00001844
Iteration 129/1000 | Loss: 0.00001844
Iteration 130/1000 | Loss: 0.00001844
Iteration 131/1000 | Loss: 0.00001844
Iteration 132/1000 | Loss: 0.00001844
Iteration 133/1000 | Loss: 0.00001844
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001843
Iteration 139/1000 | Loss: 0.00001843
Iteration 140/1000 | Loss: 0.00001843
Iteration 141/1000 | Loss: 0.00001843
Iteration 142/1000 | Loss: 0.00001843
Iteration 143/1000 | Loss: 0.00001843
Iteration 144/1000 | Loss: 0.00001843
Iteration 145/1000 | Loss: 0.00001843
Iteration 146/1000 | Loss: 0.00001843
Iteration 147/1000 | Loss: 0.00001843
Iteration 148/1000 | Loss: 0.00001843
Iteration 149/1000 | Loss: 0.00001843
Iteration 150/1000 | Loss: 0.00001843
Iteration 151/1000 | Loss: 0.00001843
Iteration 152/1000 | Loss: 0.00001843
Iteration 153/1000 | Loss: 0.00001843
Iteration 154/1000 | Loss: 0.00001843
Iteration 155/1000 | Loss: 0.00001842
Iteration 156/1000 | Loss: 0.00001842
Iteration 157/1000 | Loss: 0.00001842
Iteration 158/1000 | Loss: 0.00001842
Iteration 159/1000 | Loss: 0.00001842
Iteration 160/1000 | Loss: 0.00001842
Iteration 161/1000 | Loss: 0.00001842
Iteration 162/1000 | Loss: 0.00001842
Iteration 163/1000 | Loss: 0.00001842
Iteration 164/1000 | Loss: 0.00001842
Iteration 165/1000 | Loss: 0.00001842
Iteration 166/1000 | Loss: 0.00001842
Iteration 167/1000 | Loss: 0.00001842
Iteration 168/1000 | Loss: 0.00001842
Iteration 169/1000 | Loss: 0.00001842
Iteration 170/1000 | Loss: 0.00001842
Iteration 171/1000 | Loss: 0.00001842
Iteration 172/1000 | Loss: 0.00001842
Iteration 173/1000 | Loss: 0.00001842
Iteration 174/1000 | Loss: 0.00001842
Iteration 175/1000 | Loss: 0.00001841
Iteration 176/1000 | Loss: 0.00001841
Iteration 177/1000 | Loss: 0.00001841
Iteration 178/1000 | Loss: 0.00001841
Iteration 179/1000 | Loss: 0.00001841
Iteration 180/1000 | Loss: 0.00001841
Iteration 181/1000 | Loss: 0.00001841
Iteration 182/1000 | Loss: 0.00001841
Iteration 183/1000 | Loss: 0.00001841
Iteration 184/1000 | Loss: 0.00001841
Iteration 185/1000 | Loss: 0.00001841
Iteration 186/1000 | Loss: 0.00001841
Iteration 187/1000 | Loss: 0.00001841
Iteration 188/1000 | Loss: 0.00001841
Iteration 189/1000 | Loss: 0.00001841
Iteration 190/1000 | Loss: 0.00001841
Iteration 191/1000 | Loss: 0.00001840
Iteration 192/1000 | Loss: 0.00001840
Iteration 193/1000 | Loss: 0.00001840
Iteration 194/1000 | Loss: 0.00001840
Iteration 195/1000 | Loss: 0.00001840
Iteration 196/1000 | Loss: 0.00001840
Iteration 197/1000 | Loss: 0.00001840
Iteration 198/1000 | Loss: 0.00001840
Iteration 199/1000 | Loss: 0.00001840
Iteration 200/1000 | Loss: 0.00001840
Iteration 201/1000 | Loss: 0.00001840
Iteration 202/1000 | Loss: 0.00001840
Iteration 203/1000 | Loss: 0.00001840
Iteration 204/1000 | Loss: 0.00001840
Iteration 205/1000 | Loss: 0.00001840
Iteration 206/1000 | Loss: 0.00001840
Iteration 207/1000 | Loss: 0.00001840
Iteration 208/1000 | Loss: 0.00001840
Iteration 209/1000 | Loss: 0.00001840
Iteration 210/1000 | Loss: 0.00001840
Iteration 211/1000 | Loss: 0.00001840
Iteration 212/1000 | Loss: 0.00001840
Iteration 213/1000 | Loss: 0.00001840
Iteration 214/1000 | Loss: 0.00001840
Iteration 215/1000 | Loss: 0.00001839
Iteration 216/1000 | Loss: 0.00001839
Iteration 217/1000 | Loss: 0.00001839
Iteration 218/1000 | Loss: 0.00001839
Iteration 219/1000 | Loss: 0.00001839
Iteration 220/1000 | Loss: 0.00001839
Iteration 221/1000 | Loss: 0.00001839
Iteration 222/1000 | Loss: 0.00001839
Iteration 223/1000 | Loss: 0.00001839
Iteration 224/1000 | Loss: 0.00001839
Iteration 225/1000 | Loss: 0.00001839
Iteration 226/1000 | Loss: 0.00001839
Iteration 227/1000 | Loss: 0.00001839
Iteration 228/1000 | Loss: 0.00001839
Iteration 229/1000 | Loss: 0.00001839
Iteration 230/1000 | Loss: 0.00001839
Iteration 231/1000 | Loss: 0.00001839
Iteration 232/1000 | Loss: 0.00001839
Iteration 233/1000 | Loss: 0.00001839
Iteration 234/1000 | Loss: 0.00001839
Iteration 235/1000 | Loss: 0.00001839
Iteration 236/1000 | Loss: 0.00001839
Iteration 237/1000 | Loss: 0.00001839
Iteration 238/1000 | Loss: 0.00001839
Iteration 239/1000 | Loss: 0.00001839
Iteration 240/1000 | Loss: 0.00001839
Iteration 241/1000 | Loss: 0.00001839
Iteration 242/1000 | Loss: 0.00001839
Iteration 243/1000 | Loss: 0.00001839
Iteration 244/1000 | Loss: 0.00001839
Iteration 245/1000 | Loss: 0.00001839
Iteration 246/1000 | Loss: 0.00001839
Iteration 247/1000 | Loss: 0.00001839
Iteration 248/1000 | Loss: 0.00001839
Iteration 249/1000 | Loss: 0.00001839
Iteration 250/1000 | Loss: 0.00001839
Iteration 251/1000 | Loss: 0.00001839
Iteration 252/1000 | Loss: 0.00001839
Iteration 253/1000 | Loss: 0.00001839
Iteration 254/1000 | Loss: 0.00001839
Iteration 255/1000 | Loss: 0.00001839
Iteration 256/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.838764910644386e-05, 1.838764910644386e-05, 1.838764910644386e-05, 1.838764910644386e-05, 1.838764910644386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.838764910644386e-05

Optimization complete. Final v2v error: 3.6927051544189453 mm

Highest mean error: 4.116461753845215 mm for frame 105

Lowest mean error: 3.082871675491333 mm for frame 0

Saving results

Total time: 47.49145841598511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432761
Iteration 2/25 | Loss: 0.00155275
Iteration 3/25 | Loss: 0.00148946
Iteration 4/25 | Loss: 0.00147518
Iteration 5/25 | Loss: 0.00147015
Iteration 6/25 | Loss: 0.00146925
Iteration 7/25 | Loss: 0.00146925
Iteration 8/25 | Loss: 0.00146925
Iteration 9/25 | Loss: 0.00146925
Iteration 10/25 | Loss: 0.00146925
Iteration 11/25 | Loss: 0.00146925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014692542608827353, 0.0014692542608827353, 0.0014692542608827353, 0.0014692542608827353, 0.0014692542608827353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014692542608827353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25575900
Iteration 2/25 | Loss: 0.00235346
Iteration 3/25 | Loss: 0.00235346
Iteration 4/25 | Loss: 0.00235346
Iteration 5/25 | Loss: 0.00235346
Iteration 6/25 | Loss: 0.00235346
Iteration 7/25 | Loss: 0.00235346
Iteration 8/25 | Loss: 0.00235346
Iteration 9/25 | Loss: 0.00235346
Iteration 10/25 | Loss: 0.00235346
Iteration 11/25 | Loss: 0.00235346
Iteration 12/25 | Loss: 0.00235346
Iteration 13/25 | Loss: 0.00235346
Iteration 14/25 | Loss: 0.00235346
Iteration 15/25 | Loss: 0.00235346
Iteration 16/25 | Loss: 0.00235346
Iteration 17/25 | Loss: 0.00235346
Iteration 18/25 | Loss: 0.00235346
Iteration 19/25 | Loss: 0.00235346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023534575011581182, 0.0023534575011581182, 0.0023534575011581182, 0.0023534575011581182, 0.0023534575011581182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023534575011581182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235346
Iteration 2/1000 | Loss: 0.00002582
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001837
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001550
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001518
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001506
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001491
Iteration 25/1000 | Loss: 0.00001491
Iteration 26/1000 | Loss: 0.00001490
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001475
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001473
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001473
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001465
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001464
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001459
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001456
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001454
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001454
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001452
Iteration 92/1000 | Loss: 0.00001452
Iteration 93/1000 | Loss: 0.00001452
Iteration 94/1000 | Loss: 0.00001452
Iteration 95/1000 | Loss: 0.00001452
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001451
Iteration 99/1000 | Loss: 0.00001451
Iteration 100/1000 | Loss: 0.00001451
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001451
Iteration 103/1000 | Loss: 0.00001451
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001451
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001451
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001450
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00001449
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001447
Iteration 125/1000 | Loss: 0.00001447
Iteration 126/1000 | Loss: 0.00001447
Iteration 127/1000 | Loss: 0.00001446
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001446
Iteration 130/1000 | Loss: 0.00001446
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001446
Iteration 138/1000 | Loss: 0.00001446
Iteration 139/1000 | Loss: 0.00001446
Iteration 140/1000 | Loss: 0.00001446
Iteration 141/1000 | Loss: 0.00001446
Iteration 142/1000 | Loss: 0.00001446
Iteration 143/1000 | Loss: 0.00001446
Iteration 144/1000 | Loss: 0.00001446
Iteration 145/1000 | Loss: 0.00001446
Iteration 146/1000 | Loss: 0.00001446
Iteration 147/1000 | Loss: 0.00001446
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.445570796931861e-05, 1.445570796931861e-05, 1.445570796931861e-05, 1.445570796931861e-05, 1.445570796931861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.445570796931861e-05

Optimization complete. Final v2v error: 3.3217906951904297 mm

Highest mean error: 3.5183868408203125 mm for frame 112

Lowest mean error: 3.163811445236206 mm for frame 157

Saving results

Total time: 38.765671253204346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416850
Iteration 2/25 | Loss: 0.00160876
Iteration 3/25 | Loss: 0.00150350
Iteration 4/25 | Loss: 0.00148546
Iteration 5/25 | Loss: 0.00148024
Iteration 6/25 | Loss: 0.00147863
Iteration 7/25 | Loss: 0.00147837
Iteration 8/25 | Loss: 0.00147837
Iteration 9/25 | Loss: 0.00147837
Iteration 10/25 | Loss: 0.00147837
Iteration 11/25 | Loss: 0.00147837
Iteration 12/25 | Loss: 0.00147837
Iteration 13/25 | Loss: 0.00147837
Iteration 14/25 | Loss: 0.00147837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014783665537834167, 0.0014783665537834167, 0.0014783665537834167, 0.0014783665537834167, 0.0014783665537834167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014783665537834167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23846197
Iteration 2/25 | Loss: 0.00275814
Iteration 3/25 | Loss: 0.00275812
Iteration 4/25 | Loss: 0.00275812
Iteration 5/25 | Loss: 0.00275812
Iteration 6/25 | Loss: 0.00275812
Iteration 7/25 | Loss: 0.00275812
Iteration 8/25 | Loss: 0.00275812
Iteration 9/25 | Loss: 0.00275811
Iteration 10/25 | Loss: 0.00275811
Iteration 11/25 | Loss: 0.00275811
Iteration 12/25 | Loss: 0.00275811
Iteration 13/25 | Loss: 0.00275811
Iteration 14/25 | Loss: 0.00275811
Iteration 15/25 | Loss: 0.00275811
Iteration 16/25 | Loss: 0.00275811
Iteration 17/25 | Loss: 0.00275811
Iteration 18/25 | Loss: 0.00275811
Iteration 19/25 | Loss: 0.00275811
Iteration 20/25 | Loss: 0.00275811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00275811436586082, 0.00275811436586082, 0.00275811436586082, 0.00275811436586082, 0.00275811436586082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00275811436586082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275811
Iteration 2/1000 | Loss: 0.00006149
Iteration 3/1000 | Loss: 0.00004215
Iteration 4/1000 | Loss: 0.00003055
Iteration 5/1000 | Loss: 0.00002708
Iteration 6/1000 | Loss: 0.00002476
Iteration 7/1000 | Loss: 0.00002294
Iteration 8/1000 | Loss: 0.00002208
Iteration 9/1000 | Loss: 0.00002132
Iteration 10/1000 | Loss: 0.00002073
Iteration 11/1000 | Loss: 0.00002036
Iteration 12/1000 | Loss: 0.00002002
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001948
Iteration 16/1000 | Loss: 0.00001946
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001943
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001933
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001884
Iteration 35/1000 | Loss: 0.00001883
Iteration 36/1000 | Loss: 0.00001882
Iteration 37/1000 | Loss: 0.00001881
Iteration 38/1000 | Loss: 0.00001881
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001878
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001874
Iteration 47/1000 | Loss: 0.00001874
Iteration 48/1000 | Loss: 0.00001870
Iteration 49/1000 | Loss: 0.00001870
Iteration 50/1000 | Loss: 0.00001869
Iteration 51/1000 | Loss: 0.00001868
Iteration 52/1000 | Loss: 0.00001868
Iteration 53/1000 | Loss: 0.00001868
Iteration 54/1000 | Loss: 0.00001868
Iteration 55/1000 | Loss: 0.00001867
Iteration 56/1000 | Loss: 0.00001867
Iteration 57/1000 | Loss: 0.00001867
Iteration 58/1000 | Loss: 0.00001867
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001866
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001862
Iteration 72/1000 | Loss: 0.00001862
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001859
Iteration 77/1000 | Loss: 0.00001859
Iteration 78/1000 | Loss: 0.00001859
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001849
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001846
Iteration 104/1000 | Loss: 0.00001846
Iteration 105/1000 | Loss: 0.00001845
Iteration 106/1000 | Loss: 0.00001845
Iteration 107/1000 | Loss: 0.00001845
Iteration 108/1000 | Loss: 0.00001845
Iteration 109/1000 | Loss: 0.00001845
Iteration 110/1000 | Loss: 0.00001845
Iteration 111/1000 | Loss: 0.00001845
Iteration 112/1000 | Loss: 0.00001845
Iteration 113/1000 | Loss: 0.00001845
Iteration 114/1000 | Loss: 0.00001845
Iteration 115/1000 | Loss: 0.00001845
Iteration 116/1000 | Loss: 0.00001845
Iteration 117/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.84497930604266e-05, 1.84497930604266e-05, 1.84497930604266e-05, 1.84497930604266e-05, 1.84497930604266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.84497930604266e-05

Optimization complete. Final v2v error: 3.5885605812072754 mm

Highest mean error: 5.54044771194458 mm for frame 87

Lowest mean error: 2.959181308746338 mm for frame 15

Saving results

Total time: 44.188395977020264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770740
Iteration 2/25 | Loss: 0.00166158
Iteration 3/25 | Loss: 0.00157965
Iteration 4/25 | Loss: 0.00157320
Iteration 5/25 | Loss: 0.00157320
Iteration 6/25 | Loss: 0.00157320
Iteration 7/25 | Loss: 0.00157320
Iteration 8/25 | Loss: 0.00157320
Iteration 9/25 | Loss: 0.00157320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0015731977764517069, 0.0015731977764517069, 0.0015731977764517069, 0.0015731977764517069, 0.0015731977764517069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015731977764517069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84271824
Iteration 2/25 | Loss: 0.00201285
Iteration 3/25 | Loss: 0.00201283
Iteration 4/25 | Loss: 0.00201283
Iteration 5/25 | Loss: 0.00201283
Iteration 6/25 | Loss: 0.00201283
Iteration 7/25 | Loss: 0.00201283
Iteration 8/25 | Loss: 0.00201283
Iteration 9/25 | Loss: 0.00201283
Iteration 10/25 | Loss: 0.00201283
Iteration 11/25 | Loss: 0.00201283
Iteration 12/25 | Loss: 0.00201283
Iteration 13/25 | Loss: 0.00201283
Iteration 14/25 | Loss: 0.00201283
Iteration 15/25 | Loss: 0.00201283
Iteration 16/25 | Loss: 0.00201283
Iteration 17/25 | Loss: 0.00201283
Iteration 18/25 | Loss: 0.00201283
Iteration 19/25 | Loss: 0.00201283
Iteration 20/25 | Loss: 0.00201283
Iteration 21/25 | Loss: 0.00201283
Iteration 22/25 | Loss: 0.00201283
Iteration 23/25 | Loss: 0.00201283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020128260366618633, 0.0020128260366618633, 0.0020128260366618633, 0.0020128260366618633, 0.0020128260366618633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020128260366618633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201283
Iteration 2/1000 | Loss: 0.00003446
Iteration 3/1000 | Loss: 0.00002604
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002307
Iteration 6/1000 | Loss: 0.00002260
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002189
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002087
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002034
Iteration 15/1000 | Loss: 0.00002016
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001983
Iteration 21/1000 | Loss: 0.00001982
Iteration 22/1000 | Loss: 0.00001980
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001977
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001962
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001954
Iteration 31/1000 | Loss: 0.00001954
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001951
Iteration 40/1000 | Loss: 0.00001951
Iteration 41/1000 | Loss: 0.00001951
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001949
Iteration 45/1000 | Loss: 0.00001949
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001948
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001948
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001948
Iteration 61/1000 | Loss: 0.00001948
Iteration 62/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.9480634364299476e-05, 1.9480634364299476e-05, 1.9480634364299476e-05, 1.9480634364299476e-05, 1.9480634364299476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9480634364299476e-05

Optimization complete. Final v2v error: 3.6957740783691406 mm

Highest mean error: 4.054043292999268 mm for frame 173

Lowest mean error: 3.450507879257202 mm for frame 53

Saving results

Total time: 37.96594166755676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446626
Iteration 2/25 | Loss: 0.00154745
Iteration 3/25 | Loss: 0.00150137
Iteration 4/25 | Loss: 0.00149353
Iteration 5/25 | Loss: 0.00149345
Iteration 6/25 | Loss: 0.00149345
Iteration 7/25 | Loss: 0.00149345
Iteration 8/25 | Loss: 0.00149345
Iteration 9/25 | Loss: 0.00149345
Iteration 10/25 | Loss: 0.00149345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014934486243873835, 0.0014934486243873835, 0.0014934486243873835, 0.0014934486243873835, 0.0014934486243873835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014934486243873835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26123643
Iteration 2/25 | Loss: 0.00224622
Iteration 3/25 | Loss: 0.00224621
Iteration 4/25 | Loss: 0.00224621
Iteration 5/25 | Loss: 0.00224621
Iteration 6/25 | Loss: 0.00224621
Iteration 7/25 | Loss: 0.00224621
Iteration 8/25 | Loss: 0.00224621
Iteration 9/25 | Loss: 0.00224621
Iteration 10/25 | Loss: 0.00224621
Iteration 11/25 | Loss: 0.00224621
Iteration 12/25 | Loss: 0.00224621
Iteration 13/25 | Loss: 0.00224621
Iteration 14/25 | Loss: 0.00224621
Iteration 15/25 | Loss: 0.00224621
Iteration 16/25 | Loss: 0.00224621
Iteration 17/25 | Loss: 0.00224621
Iteration 18/25 | Loss: 0.00224621
Iteration 19/25 | Loss: 0.00224621
Iteration 20/25 | Loss: 0.00224621
Iteration 21/25 | Loss: 0.00224621
Iteration 22/25 | Loss: 0.00224621
Iteration 23/25 | Loss: 0.00224621
Iteration 24/25 | Loss: 0.00224621
Iteration 25/25 | Loss: 0.00224621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224621
Iteration 2/1000 | Loss: 0.00002718
Iteration 3/1000 | Loss: 0.00002102
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001823
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001698
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001611
Iteration 10/1000 | Loss: 0.00001580
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001517
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00001453
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001443
Iteration 39/1000 | Loss: 0.00001442
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001422
Iteration 90/1000 | Loss: 0.00001422
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001421
Iteration 96/1000 | Loss: 0.00001421
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001420
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001419
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001419
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001418
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001417
Iteration 123/1000 | Loss: 0.00001417
Iteration 124/1000 | Loss: 0.00001417
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001415
Iteration 128/1000 | Loss: 0.00001415
Iteration 129/1000 | Loss: 0.00001415
Iteration 130/1000 | Loss: 0.00001415
Iteration 131/1000 | Loss: 0.00001415
Iteration 132/1000 | Loss: 0.00001415
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00001414
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001413
Iteration 139/1000 | Loss: 0.00001413
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001413
Iteration 142/1000 | Loss: 0.00001413
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001413
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001412
Iteration 150/1000 | Loss: 0.00001412
Iteration 151/1000 | Loss: 0.00001412
Iteration 152/1000 | Loss: 0.00001412
Iteration 153/1000 | Loss: 0.00001411
Iteration 154/1000 | Loss: 0.00001411
Iteration 155/1000 | Loss: 0.00001411
Iteration 156/1000 | Loss: 0.00001411
Iteration 157/1000 | Loss: 0.00001411
Iteration 158/1000 | Loss: 0.00001411
Iteration 159/1000 | Loss: 0.00001411
Iteration 160/1000 | Loss: 0.00001411
Iteration 161/1000 | Loss: 0.00001410
Iteration 162/1000 | Loss: 0.00001410
Iteration 163/1000 | Loss: 0.00001410
Iteration 164/1000 | Loss: 0.00001410
Iteration 165/1000 | Loss: 0.00001410
Iteration 166/1000 | Loss: 0.00001410
Iteration 167/1000 | Loss: 0.00001409
Iteration 168/1000 | Loss: 0.00001409
Iteration 169/1000 | Loss: 0.00001409
Iteration 170/1000 | Loss: 0.00001408
Iteration 171/1000 | Loss: 0.00001408
Iteration 172/1000 | Loss: 0.00001408
Iteration 173/1000 | Loss: 0.00001408
Iteration 174/1000 | Loss: 0.00001408
Iteration 175/1000 | Loss: 0.00001408
Iteration 176/1000 | Loss: 0.00001408
Iteration 177/1000 | Loss: 0.00001407
Iteration 178/1000 | Loss: 0.00001407
Iteration 179/1000 | Loss: 0.00001407
Iteration 180/1000 | Loss: 0.00001407
Iteration 181/1000 | Loss: 0.00001406
Iteration 182/1000 | Loss: 0.00001406
Iteration 183/1000 | Loss: 0.00001406
Iteration 184/1000 | Loss: 0.00001406
Iteration 185/1000 | Loss: 0.00001406
Iteration 186/1000 | Loss: 0.00001405
Iteration 187/1000 | Loss: 0.00001404
Iteration 188/1000 | Loss: 0.00001404
Iteration 189/1000 | Loss: 0.00001403
Iteration 190/1000 | Loss: 0.00001403
Iteration 191/1000 | Loss: 0.00001403
Iteration 192/1000 | Loss: 0.00001403
Iteration 193/1000 | Loss: 0.00001403
Iteration 194/1000 | Loss: 0.00001403
Iteration 195/1000 | Loss: 0.00001403
Iteration 196/1000 | Loss: 0.00001403
Iteration 197/1000 | Loss: 0.00001403
Iteration 198/1000 | Loss: 0.00001402
Iteration 199/1000 | Loss: 0.00001402
Iteration 200/1000 | Loss: 0.00001402
Iteration 201/1000 | Loss: 0.00001402
Iteration 202/1000 | Loss: 0.00001402
Iteration 203/1000 | Loss: 0.00001401
Iteration 204/1000 | Loss: 0.00001401
Iteration 205/1000 | Loss: 0.00001401
Iteration 206/1000 | Loss: 0.00001401
Iteration 207/1000 | Loss: 0.00001401
Iteration 208/1000 | Loss: 0.00001401
Iteration 209/1000 | Loss: 0.00001401
Iteration 210/1000 | Loss: 0.00001401
Iteration 211/1000 | Loss: 0.00001400
Iteration 212/1000 | Loss: 0.00001400
Iteration 213/1000 | Loss: 0.00001400
Iteration 214/1000 | Loss: 0.00001400
Iteration 215/1000 | Loss: 0.00001400
Iteration 216/1000 | Loss: 0.00001400
Iteration 217/1000 | Loss: 0.00001400
Iteration 218/1000 | Loss: 0.00001400
Iteration 219/1000 | Loss: 0.00001400
Iteration 220/1000 | Loss: 0.00001400
Iteration 221/1000 | Loss: 0.00001400
Iteration 222/1000 | Loss: 0.00001400
Iteration 223/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.3997648238728289e-05, 1.3997648238728289e-05, 1.3997648238728289e-05, 1.3997648238728289e-05, 1.3997648238728289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3997648238728289e-05

Optimization complete. Final v2v error: 3.223309278488159 mm

Highest mean error: 3.3854846954345703 mm for frame 76

Lowest mean error: 3.0816636085510254 mm for frame 2

Saving results

Total time: 53.5182363986969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986128
Iteration 2/25 | Loss: 0.00329969
Iteration 3/25 | Loss: 0.00259168
Iteration 4/25 | Loss: 0.00215877
Iteration 5/25 | Loss: 0.00213364
Iteration 6/25 | Loss: 0.00189058
Iteration 7/25 | Loss: 0.00186807
Iteration 8/25 | Loss: 0.00178127
Iteration 9/25 | Loss: 0.00183417
Iteration 10/25 | Loss: 0.00172277
Iteration 11/25 | Loss: 0.00166880
Iteration 12/25 | Loss: 0.00164710
Iteration 13/25 | Loss: 0.00165704
Iteration 14/25 | Loss: 0.00160313
Iteration 15/25 | Loss: 0.00159245
Iteration 16/25 | Loss: 0.00159914
Iteration 17/25 | Loss: 0.00160577
Iteration 18/25 | Loss: 0.00158141
Iteration 19/25 | Loss: 0.00158210
Iteration 20/25 | Loss: 0.00158025
Iteration 21/25 | Loss: 0.00157332
Iteration 22/25 | Loss: 0.00157792
Iteration 23/25 | Loss: 0.00157744
Iteration 24/25 | Loss: 0.00157052
Iteration 25/25 | Loss: 0.00157545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31152904
Iteration 2/25 | Loss: 0.00302819
Iteration 3/25 | Loss: 0.00250396
Iteration 4/25 | Loss: 0.00250396
Iteration 5/25 | Loss: 0.00250396
Iteration 6/25 | Loss: 0.00250396
Iteration 7/25 | Loss: 0.00250396
Iteration 8/25 | Loss: 0.00250395
Iteration 9/25 | Loss: 0.00250395
Iteration 10/25 | Loss: 0.00250395
Iteration 11/25 | Loss: 0.00250395
Iteration 12/25 | Loss: 0.00250395
Iteration 13/25 | Loss: 0.00250395
Iteration 14/25 | Loss: 0.00250395
Iteration 15/25 | Loss: 0.00250395
Iteration 16/25 | Loss: 0.00250395
Iteration 17/25 | Loss: 0.00250395
Iteration 18/25 | Loss: 0.00250395
Iteration 19/25 | Loss: 0.00250395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002503952942788601, 0.002503952942788601, 0.002503952942788601, 0.002503952942788601, 0.002503952942788601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002503952942788601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250395
Iteration 2/1000 | Loss: 0.00023969
Iteration 3/1000 | Loss: 0.00084246
Iteration 4/1000 | Loss: 0.00013326
Iteration 5/1000 | Loss: 0.00060073
Iteration 6/1000 | Loss: 0.00006300
Iteration 7/1000 | Loss: 0.00005823
Iteration 8/1000 | Loss: 0.00042210
Iteration 9/1000 | Loss: 0.00128314
Iteration 10/1000 | Loss: 0.00017568
Iteration 11/1000 | Loss: 0.00006877
Iteration 12/1000 | Loss: 0.00005062
Iteration 13/1000 | Loss: 0.00016789
Iteration 14/1000 | Loss: 0.00026712
Iteration 15/1000 | Loss: 0.00014515
Iteration 16/1000 | Loss: 0.00073787
Iteration 17/1000 | Loss: 0.00008971
Iteration 18/1000 | Loss: 0.00014132
Iteration 19/1000 | Loss: 0.00004683
Iteration 20/1000 | Loss: 0.00008575
Iteration 21/1000 | Loss: 0.00004570
Iteration 22/1000 | Loss: 0.00030836
Iteration 23/1000 | Loss: 0.00085606
Iteration 24/1000 | Loss: 0.00101494
Iteration 25/1000 | Loss: 0.00110182
Iteration 26/1000 | Loss: 0.00046974
Iteration 27/1000 | Loss: 0.00063175
Iteration 28/1000 | Loss: 0.00038803
Iteration 29/1000 | Loss: 0.00014420
Iteration 30/1000 | Loss: 0.00010069
Iteration 31/1000 | Loss: 0.00004394
Iteration 32/1000 | Loss: 0.00017461
Iteration 33/1000 | Loss: 0.00003769
Iteration 34/1000 | Loss: 0.00014543
Iteration 35/1000 | Loss: 0.00026905
Iteration 36/1000 | Loss: 0.00045898
Iteration 37/1000 | Loss: 0.00076755
Iteration 38/1000 | Loss: 0.00026825
Iteration 39/1000 | Loss: 0.00011919
Iteration 40/1000 | Loss: 0.00003267
Iteration 41/1000 | Loss: 0.00011835
Iteration 42/1000 | Loss: 0.00005400
Iteration 43/1000 | Loss: 0.00006915
Iteration 44/1000 | Loss: 0.00008903
Iteration 45/1000 | Loss: 0.00060032
Iteration 46/1000 | Loss: 0.00024893
Iteration 47/1000 | Loss: 0.00016646
Iteration 48/1000 | Loss: 0.00011457
Iteration 49/1000 | Loss: 0.00005618
Iteration 50/1000 | Loss: 0.00002998
Iteration 51/1000 | Loss: 0.00004142
Iteration 52/1000 | Loss: 0.00002753
Iteration 53/1000 | Loss: 0.00003717
Iteration 54/1000 | Loss: 0.00006277
Iteration 55/1000 | Loss: 0.00002658
Iteration 56/1000 | Loss: 0.00014042
Iteration 57/1000 | Loss: 0.00002620
Iteration 58/1000 | Loss: 0.00002569
Iteration 59/1000 | Loss: 0.00002556
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002524
Iteration 62/1000 | Loss: 0.00002522
Iteration 63/1000 | Loss: 0.00006163
Iteration 64/1000 | Loss: 0.00002527
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002509
Iteration 67/1000 | Loss: 0.00002508
Iteration 68/1000 | Loss: 0.00002507
Iteration 69/1000 | Loss: 0.00002504
Iteration 70/1000 | Loss: 0.00002503
Iteration 71/1000 | Loss: 0.00002503
Iteration 72/1000 | Loss: 0.00002502
Iteration 73/1000 | Loss: 0.00002502
Iteration 74/1000 | Loss: 0.00002502
Iteration 75/1000 | Loss: 0.00002502
Iteration 76/1000 | Loss: 0.00002502
Iteration 77/1000 | Loss: 0.00002501
Iteration 78/1000 | Loss: 0.00002501
Iteration 79/1000 | Loss: 0.00002501
Iteration 80/1000 | Loss: 0.00002501
Iteration 81/1000 | Loss: 0.00002500
Iteration 82/1000 | Loss: 0.00002500
Iteration 83/1000 | Loss: 0.00002499
Iteration 84/1000 | Loss: 0.00002499
Iteration 85/1000 | Loss: 0.00002497
Iteration 86/1000 | Loss: 0.00002497
Iteration 87/1000 | Loss: 0.00006107
Iteration 88/1000 | Loss: 0.00008994
Iteration 89/1000 | Loss: 0.00002942
Iteration 90/1000 | Loss: 0.00002888
Iteration 91/1000 | Loss: 0.00002496
Iteration 92/1000 | Loss: 0.00002492
Iteration 93/1000 | Loss: 0.00002492
Iteration 94/1000 | Loss: 0.00002490
Iteration 95/1000 | Loss: 0.00002490
Iteration 96/1000 | Loss: 0.00002490
Iteration 97/1000 | Loss: 0.00002490
Iteration 98/1000 | Loss: 0.00002490
Iteration 99/1000 | Loss: 0.00002490
Iteration 100/1000 | Loss: 0.00002490
Iteration 101/1000 | Loss: 0.00002490
Iteration 102/1000 | Loss: 0.00002490
Iteration 103/1000 | Loss: 0.00002490
Iteration 104/1000 | Loss: 0.00002490
Iteration 105/1000 | Loss: 0.00002490
Iteration 106/1000 | Loss: 0.00002490
Iteration 107/1000 | Loss: 0.00002490
Iteration 108/1000 | Loss: 0.00002490
Iteration 109/1000 | Loss: 0.00002490
Iteration 110/1000 | Loss: 0.00002490
Iteration 111/1000 | Loss: 0.00002490
Iteration 112/1000 | Loss: 0.00002490
Iteration 113/1000 | Loss: 0.00002490
Iteration 114/1000 | Loss: 0.00002490
Iteration 115/1000 | Loss: 0.00002490
Iteration 116/1000 | Loss: 0.00002490
Iteration 117/1000 | Loss: 0.00002490
Iteration 118/1000 | Loss: 0.00002490
Iteration 119/1000 | Loss: 0.00002490
Iteration 120/1000 | Loss: 0.00002490
Iteration 121/1000 | Loss: 0.00002490
Iteration 122/1000 | Loss: 0.00002490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.4898596166167408e-05, 2.4898596166167408e-05, 2.4898596166167408e-05, 2.4898596166167408e-05, 2.4898596166167408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4898596166167408e-05

Optimization complete. Final v2v error: 4.054377555847168 mm

Highest mean error: 11.344461441040039 mm for frame 199

Lowest mean error: 3.576720952987671 mm for frame 137

Saving results

Total time: 161.65887832641602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812615
Iteration 2/25 | Loss: 0.00165968
Iteration 3/25 | Loss: 0.00152060
Iteration 4/25 | Loss: 0.00149937
Iteration 5/25 | Loss: 0.00150050
Iteration 6/25 | Loss: 0.00149896
Iteration 7/25 | Loss: 0.00149652
Iteration 8/25 | Loss: 0.00149753
Iteration 9/25 | Loss: 0.00149296
Iteration 10/25 | Loss: 0.00149454
Iteration 11/25 | Loss: 0.00149477
Iteration 12/25 | Loss: 0.00149130
Iteration 13/25 | Loss: 0.00148918
Iteration 14/25 | Loss: 0.00148938
Iteration 15/25 | Loss: 0.00148954
Iteration 16/25 | Loss: 0.00148942
Iteration 17/25 | Loss: 0.00148968
Iteration 18/25 | Loss: 0.00148971
Iteration 19/25 | Loss: 0.00148980
Iteration 20/25 | Loss: 0.00149018
Iteration 21/25 | Loss: 0.00149019
Iteration 22/25 | Loss: 0.00148879
Iteration 23/25 | Loss: 0.00148957
Iteration 24/25 | Loss: 0.00148975
Iteration 25/25 | Loss: 0.00148975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.54632473
Iteration 2/25 | Loss: 0.00226511
Iteration 3/25 | Loss: 0.00226506
Iteration 4/25 | Loss: 0.00226506
Iteration 5/25 | Loss: 0.00226506
Iteration 6/25 | Loss: 0.00226506
Iteration 7/25 | Loss: 0.00226506
Iteration 8/25 | Loss: 0.00226506
Iteration 9/25 | Loss: 0.00226506
Iteration 10/25 | Loss: 0.00226506
Iteration 11/25 | Loss: 0.00226506
Iteration 12/25 | Loss: 0.00226506
Iteration 13/25 | Loss: 0.00226506
Iteration 14/25 | Loss: 0.00226506
Iteration 15/25 | Loss: 0.00226506
Iteration 16/25 | Loss: 0.00226506
Iteration 17/25 | Loss: 0.00226506
Iteration 18/25 | Loss: 0.00226506
Iteration 19/25 | Loss: 0.00226506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0022650565952062607, 0.0022650565952062607, 0.0022650565952062607, 0.0022650565952062607, 0.0022650565952062607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022650565952062607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226506
Iteration 2/1000 | Loss: 0.00003841
Iteration 3/1000 | Loss: 0.00003013
Iteration 4/1000 | Loss: 0.00002618
Iteration 5/1000 | Loss: 0.00003773
Iteration 6/1000 | Loss: 0.00003087
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00002985
Iteration 9/1000 | Loss: 0.00003469
Iteration 10/1000 | Loss: 0.00002669
Iteration 11/1000 | Loss: 0.00003437
Iteration 12/1000 | Loss: 0.00003557
Iteration 13/1000 | Loss: 0.00002591
Iteration 14/1000 | Loss: 0.00002856
Iteration 15/1000 | Loss: 0.00002618
Iteration 16/1000 | Loss: 0.00002859
Iteration 17/1000 | Loss: 0.00004938
Iteration 18/1000 | Loss: 0.00002957
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002220
Iteration 21/1000 | Loss: 0.00004123
Iteration 22/1000 | Loss: 0.00002396
Iteration 23/1000 | Loss: 0.00002070
Iteration 24/1000 | Loss: 0.00028498
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001758
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001753
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001744
Iteration 39/1000 | Loss: 0.00001739
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001732
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001696
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001688
Iteration 64/1000 | Loss: 0.00001688
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001682
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001680
Iteration 88/1000 | Loss: 0.00001680
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001679
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001679
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001679
Iteration 99/1000 | Loss: 0.00001679
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001678
Iteration 107/1000 | Loss: 0.00001678
Iteration 108/1000 | Loss: 0.00001678
Iteration 109/1000 | Loss: 0.00001678
Iteration 110/1000 | Loss: 0.00001678
Iteration 111/1000 | Loss: 0.00001678
Iteration 112/1000 | Loss: 0.00001678
Iteration 113/1000 | Loss: 0.00001678
Iteration 114/1000 | Loss: 0.00001678
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001677
Iteration 117/1000 | Loss: 0.00001677
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001677
Iteration 124/1000 | Loss: 0.00001676
Iteration 125/1000 | Loss: 0.00001676
Iteration 126/1000 | Loss: 0.00001676
Iteration 127/1000 | Loss: 0.00001676
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001675
Iteration 131/1000 | Loss: 0.00001675
Iteration 132/1000 | Loss: 0.00001675
Iteration 133/1000 | Loss: 0.00001675
Iteration 134/1000 | Loss: 0.00001675
Iteration 135/1000 | Loss: 0.00001675
Iteration 136/1000 | Loss: 0.00001675
Iteration 137/1000 | Loss: 0.00001675
Iteration 138/1000 | Loss: 0.00001675
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001675
Iteration 141/1000 | Loss: 0.00001675
Iteration 142/1000 | Loss: 0.00001675
Iteration 143/1000 | Loss: 0.00001675
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001675
Iteration 154/1000 | Loss: 0.00001675
Iteration 155/1000 | Loss: 0.00001675
Iteration 156/1000 | Loss: 0.00001675
Iteration 157/1000 | Loss: 0.00001675
Iteration 158/1000 | Loss: 0.00001675
Iteration 159/1000 | Loss: 0.00001675
Iteration 160/1000 | Loss: 0.00001675
Iteration 161/1000 | Loss: 0.00001675
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001675
Iteration 164/1000 | Loss: 0.00001675
Iteration 165/1000 | Loss: 0.00001675
Iteration 166/1000 | Loss: 0.00001675
Iteration 167/1000 | Loss: 0.00001675
Iteration 168/1000 | Loss: 0.00001675
Iteration 169/1000 | Loss: 0.00001675
Iteration 170/1000 | Loss: 0.00001675
Iteration 171/1000 | Loss: 0.00001675
Iteration 172/1000 | Loss: 0.00001675
Iteration 173/1000 | Loss: 0.00001675
Iteration 174/1000 | Loss: 0.00001675
Iteration 175/1000 | Loss: 0.00001675
Iteration 176/1000 | Loss: 0.00001675
Iteration 177/1000 | Loss: 0.00001675
Iteration 178/1000 | Loss: 0.00001675
Iteration 179/1000 | Loss: 0.00001675
Iteration 180/1000 | Loss: 0.00001675
Iteration 181/1000 | Loss: 0.00001675
Iteration 182/1000 | Loss: 0.00001675
Iteration 183/1000 | Loss: 0.00001675
Iteration 184/1000 | Loss: 0.00001675
Iteration 185/1000 | Loss: 0.00001675
Iteration 186/1000 | Loss: 0.00001675
Iteration 187/1000 | Loss: 0.00001675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.675404200796038e-05, 1.675404200796038e-05, 1.675404200796038e-05, 1.675404200796038e-05, 1.675404200796038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.675404200796038e-05

Optimization complete. Final v2v error: 3.5045642852783203 mm

Highest mean error: 4.682218074798584 mm for frame 182

Lowest mean error: 3.100656270980835 mm for frame 233

Saving results

Total time: 113.05662274360657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462215
Iteration 2/25 | Loss: 0.00168421
Iteration 3/25 | Loss: 0.00154834
Iteration 4/25 | Loss: 0.00154244
Iteration 5/25 | Loss: 0.00154244
Iteration 6/25 | Loss: 0.00154244
Iteration 7/25 | Loss: 0.00154244
Iteration 8/25 | Loss: 0.00154244
Iteration 9/25 | Loss: 0.00154244
Iteration 10/25 | Loss: 0.00154244
Iteration 11/25 | Loss: 0.00154244
Iteration 12/25 | Loss: 0.00154244
Iteration 13/25 | Loss: 0.00154244
Iteration 14/25 | Loss: 0.00154244
Iteration 15/25 | Loss: 0.00154244
Iteration 16/25 | Loss: 0.00154244
Iteration 17/25 | Loss: 0.00154244
Iteration 18/25 | Loss: 0.00154244
Iteration 19/25 | Loss: 0.00154244
Iteration 20/25 | Loss: 0.00154244
Iteration 21/25 | Loss: 0.00154244
Iteration 22/25 | Loss: 0.00154244
Iteration 23/25 | Loss: 0.00154244
Iteration 24/25 | Loss: 0.00154244
Iteration 25/25 | Loss: 0.00154244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20509183
Iteration 2/25 | Loss: 0.00199351
Iteration 3/25 | Loss: 0.00199351
Iteration 4/25 | Loss: 0.00199351
Iteration 5/25 | Loss: 0.00199351
Iteration 6/25 | Loss: 0.00199351
Iteration 7/25 | Loss: 0.00199351
Iteration 8/25 | Loss: 0.00199350
Iteration 9/25 | Loss: 0.00199350
Iteration 10/25 | Loss: 0.00199350
Iteration 11/25 | Loss: 0.00199350
Iteration 12/25 | Loss: 0.00199350
Iteration 13/25 | Loss: 0.00199350
Iteration 14/25 | Loss: 0.00199350
Iteration 15/25 | Loss: 0.00199350
Iteration 16/25 | Loss: 0.00199350
Iteration 17/25 | Loss: 0.00199350
Iteration 18/25 | Loss: 0.00199350
Iteration 19/25 | Loss: 0.00199350
Iteration 20/25 | Loss: 0.00199350
Iteration 21/25 | Loss: 0.00199350
Iteration 22/25 | Loss: 0.00199350
Iteration 23/25 | Loss: 0.00199350
Iteration 24/25 | Loss: 0.00199350
Iteration 25/25 | Loss: 0.00199350

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199350
Iteration 2/1000 | Loss: 0.00003385
Iteration 3/1000 | Loss: 0.00002118
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001746
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001590
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001529
Iteration 12/1000 | Loss: 0.00001503
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001465
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001424
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001394
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001392
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001375
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001371
Iteration 46/1000 | Loss: 0.00001371
Iteration 47/1000 | Loss: 0.00001371
Iteration 48/1000 | Loss: 0.00001370
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001366
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001363
Iteration 76/1000 | Loss: 0.00001363
Iteration 77/1000 | Loss: 0.00001363
Iteration 78/1000 | Loss: 0.00001363
Iteration 79/1000 | Loss: 0.00001363
Iteration 80/1000 | Loss: 0.00001363
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001362
Iteration 84/1000 | Loss: 0.00001362
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001360
Iteration 103/1000 | Loss: 0.00001360
Iteration 104/1000 | Loss: 0.00001360
Iteration 105/1000 | Loss: 0.00001360
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001360
Iteration 113/1000 | Loss: 0.00001360
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001360
Iteration 116/1000 | Loss: 0.00001360
Iteration 117/1000 | Loss: 0.00001360
Iteration 118/1000 | Loss: 0.00001360
Iteration 119/1000 | Loss: 0.00001360
Iteration 120/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3595563359558582e-05, 1.3595563359558582e-05, 1.3595563359558582e-05, 1.3595563359558582e-05, 1.3595563359558582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3595563359558582e-05

Optimization complete. Final v2v error: 3.2239973545074463 mm

Highest mean error: 3.4477591514587402 mm for frame 94

Lowest mean error: 3.042978286743164 mm for frame 198

Saving results

Total time: 40.34989809989929
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977484
Iteration 2/25 | Loss: 0.00330066
Iteration 3/25 | Loss: 0.00227105
Iteration 4/25 | Loss: 0.00207221
Iteration 5/25 | Loss: 0.00199203
Iteration 6/25 | Loss: 0.00196367
Iteration 7/25 | Loss: 0.00192690
Iteration 8/25 | Loss: 0.00185380
Iteration 9/25 | Loss: 0.00179927
Iteration 10/25 | Loss: 0.00180700
Iteration 11/25 | Loss: 0.00178709
Iteration 12/25 | Loss: 0.00176672
Iteration 13/25 | Loss: 0.00174523
Iteration 14/25 | Loss: 0.00172902
Iteration 15/25 | Loss: 0.00172651
Iteration 16/25 | Loss: 0.00172571
Iteration 17/25 | Loss: 0.00172172
Iteration 18/25 | Loss: 0.00171221
Iteration 19/25 | Loss: 0.00171650
Iteration 20/25 | Loss: 0.00170487
Iteration 21/25 | Loss: 0.00170693
Iteration 22/25 | Loss: 0.00170509
Iteration 23/25 | Loss: 0.00170020
Iteration 24/25 | Loss: 0.00169387
Iteration 25/25 | Loss: 0.00169586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22501421
Iteration 2/25 | Loss: 0.00539210
Iteration 3/25 | Loss: 0.00464130
Iteration 4/25 | Loss: 0.00444731
Iteration 5/25 | Loss: 0.00444715
Iteration 6/25 | Loss: 0.00444715
Iteration 7/25 | Loss: 0.00444715
Iteration 8/25 | Loss: 0.00444715
Iteration 9/25 | Loss: 0.00444714
Iteration 10/25 | Loss: 0.00444714
Iteration 11/25 | Loss: 0.00444714
Iteration 12/25 | Loss: 0.00444714
Iteration 13/25 | Loss: 0.00444714
Iteration 14/25 | Loss: 0.00444714
Iteration 15/25 | Loss: 0.00444714
Iteration 16/25 | Loss: 0.00444714
Iteration 17/25 | Loss: 0.00444714
Iteration 18/25 | Loss: 0.00444714
Iteration 19/25 | Loss: 0.00444714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004447143524885178, 0.004447143524885178, 0.004447143524885178, 0.004447143524885178, 0.004447143524885178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004447143524885178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00444714
Iteration 2/1000 | Loss: 0.00047967
Iteration 3/1000 | Loss: 0.00128548
Iteration 4/1000 | Loss: 0.00062796
Iteration 5/1000 | Loss: 0.00296025
Iteration 6/1000 | Loss: 0.00243099
Iteration 7/1000 | Loss: 0.00329091
Iteration 8/1000 | Loss: 0.00038947
Iteration 9/1000 | Loss: 0.00025216
Iteration 10/1000 | Loss: 0.00035489
Iteration 11/1000 | Loss: 0.00020609
Iteration 12/1000 | Loss: 0.00069687
Iteration 13/1000 | Loss: 0.00022457
Iteration 14/1000 | Loss: 0.00052458
Iteration 15/1000 | Loss: 0.00107378
Iteration 16/1000 | Loss: 0.00016284
Iteration 17/1000 | Loss: 0.00017538
Iteration 18/1000 | Loss: 0.00014515
Iteration 19/1000 | Loss: 0.00018052
Iteration 20/1000 | Loss: 0.00015780
Iteration 21/1000 | Loss: 0.00014286
Iteration 22/1000 | Loss: 0.00067402
Iteration 23/1000 | Loss: 0.00013901
Iteration 24/1000 | Loss: 0.00029395
Iteration 25/1000 | Loss: 0.00044605
Iteration 26/1000 | Loss: 0.00014507
Iteration 27/1000 | Loss: 0.00035124
Iteration 28/1000 | Loss: 0.00012665
Iteration 29/1000 | Loss: 0.00011847
Iteration 30/1000 | Loss: 0.00012132
Iteration 31/1000 | Loss: 0.00012809
Iteration 32/1000 | Loss: 0.00012912
Iteration 33/1000 | Loss: 0.00014428
Iteration 34/1000 | Loss: 0.00014834
Iteration 35/1000 | Loss: 0.00012579
Iteration 36/1000 | Loss: 0.00011418
Iteration 37/1000 | Loss: 0.00039138
Iteration 38/1000 | Loss: 0.00012202
Iteration 39/1000 | Loss: 0.00032272
Iteration 40/1000 | Loss: 0.00026441
Iteration 41/1000 | Loss: 0.00011265
Iteration 42/1000 | Loss: 0.00025695
Iteration 43/1000 | Loss: 0.00025937
Iteration 44/1000 | Loss: 0.00011079
Iteration 45/1000 | Loss: 0.00026728
Iteration 46/1000 | Loss: 0.00012069
Iteration 47/1000 | Loss: 0.00017640
Iteration 48/1000 | Loss: 0.00011396
Iteration 49/1000 | Loss: 0.00011317
Iteration 50/1000 | Loss: 0.00012247
Iteration 51/1000 | Loss: 0.00011937
Iteration 52/1000 | Loss: 0.00010848
Iteration 53/1000 | Loss: 0.00012241
Iteration 54/1000 | Loss: 0.00012152
Iteration 55/1000 | Loss: 0.00012482
Iteration 56/1000 | Loss: 0.00061160
Iteration 57/1000 | Loss: 0.00011864
Iteration 58/1000 | Loss: 0.00011952
Iteration 59/1000 | Loss: 0.00018997
Iteration 60/1000 | Loss: 0.00011237
Iteration 61/1000 | Loss: 0.00011199
Iteration 62/1000 | Loss: 0.00011798
Iteration 63/1000 | Loss: 0.00011272
Iteration 64/1000 | Loss: 0.00012837
Iteration 65/1000 | Loss: 0.00011942
Iteration 66/1000 | Loss: 0.00011021
Iteration 67/1000 | Loss: 0.00021882
Iteration 68/1000 | Loss: 0.00011500
Iteration 69/1000 | Loss: 0.00011212
Iteration 70/1000 | Loss: 0.00011897
Iteration 71/1000 | Loss: 0.00019509
Iteration 72/1000 | Loss: 0.00017281
Iteration 73/1000 | Loss: 0.00012364
Iteration 74/1000 | Loss: 0.00011065
Iteration 75/1000 | Loss: 0.00023779
Iteration 76/1000 | Loss: 0.00014469
Iteration 77/1000 | Loss: 0.00011986
Iteration 78/1000 | Loss: 0.00011282
Iteration 79/1000 | Loss: 0.00022871
Iteration 80/1000 | Loss: 0.00012480
Iteration 81/1000 | Loss: 0.00014684
Iteration 82/1000 | Loss: 0.00012410
Iteration 83/1000 | Loss: 0.00012242
Iteration 84/1000 | Loss: 0.00014002
Iteration 85/1000 | Loss: 0.00011188
Iteration 86/1000 | Loss: 0.00010711
Iteration 87/1000 | Loss: 0.00011463
Iteration 88/1000 | Loss: 0.00012004
Iteration 89/1000 | Loss: 0.00011345
Iteration 90/1000 | Loss: 0.00020131
Iteration 91/1000 | Loss: 0.00014086
Iteration 92/1000 | Loss: 0.00018253
Iteration 93/1000 | Loss: 0.00012770
Iteration 94/1000 | Loss: 0.00012671
Iteration 95/1000 | Loss: 0.00010250
Iteration 96/1000 | Loss: 0.00010213
Iteration 97/1000 | Loss: 0.00011306
Iteration 98/1000 | Loss: 0.00010848
Iteration 99/1000 | Loss: 0.00012578
Iteration 100/1000 | Loss: 0.00010101
Iteration 101/1000 | Loss: 0.00010647
Iteration 102/1000 | Loss: 0.00010740
Iteration 103/1000 | Loss: 0.00011097
Iteration 104/1000 | Loss: 0.00009815
Iteration 105/1000 | Loss: 0.00009675
Iteration 106/1000 | Loss: 0.00010691
Iteration 107/1000 | Loss: 0.00010745
Iteration 108/1000 | Loss: 0.00017442
Iteration 109/1000 | Loss: 0.00011111
Iteration 110/1000 | Loss: 0.00011029
Iteration 111/1000 | Loss: 0.00010807
Iteration 112/1000 | Loss: 0.00013715
Iteration 113/1000 | Loss: 0.00014591
Iteration 114/1000 | Loss: 0.00010471
Iteration 115/1000 | Loss: 0.00010899
Iteration 116/1000 | Loss: 0.00013836
Iteration 117/1000 | Loss: 0.00012741
Iteration 118/1000 | Loss: 0.00012290
Iteration 119/1000 | Loss: 0.00010775
Iteration 120/1000 | Loss: 0.00011549
Iteration 121/1000 | Loss: 0.00020135
Iteration 122/1000 | Loss: 0.00011427
Iteration 123/1000 | Loss: 0.00011328
Iteration 124/1000 | Loss: 0.00011722
Iteration 125/1000 | Loss: 0.00011212
Iteration 126/1000 | Loss: 0.00011016
Iteration 127/1000 | Loss: 0.00010803
Iteration 128/1000 | Loss: 0.00010895
Iteration 129/1000 | Loss: 0.00010561
Iteration 130/1000 | Loss: 0.00010850
Iteration 131/1000 | Loss: 0.00010413
Iteration 132/1000 | Loss: 0.00010794
Iteration 133/1000 | Loss: 0.00018022
Iteration 134/1000 | Loss: 0.00018672
Iteration 135/1000 | Loss: 0.00011839
Iteration 136/1000 | Loss: 0.00010855
Iteration 137/1000 | Loss: 0.00011039
Iteration 138/1000 | Loss: 0.00010799
Iteration 139/1000 | Loss: 0.00010769
Iteration 140/1000 | Loss: 0.00010835
Iteration 141/1000 | Loss: 0.00010874
Iteration 142/1000 | Loss: 0.00010893
Iteration 143/1000 | Loss: 0.00019800
Iteration 144/1000 | Loss: 0.00012061
Iteration 145/1000 | Loss: 0.00014797
Iteration 146/1000 | Loss: 0.00010763
Iteration 147/1000 | Loss: 0.00010871
Iteration 148/1000 | Loss: 0.00010500
Iteration 149/1000 | Loss: 0.00014114
Iteration 150/1000 | Loss: 0.00010622
Iteration 151/1000 | Loss: 0.00018981
Iteration 152/1000 | Loss: 0.00011203
Iteration 153/1000 | Loss: 0.00019878
Iteration 154/1000 | Loss: 0.00011151
Iteration 155/1000 | Loss: 0.00013115
Iteration 156/1000 | Loss: 0.00010816
Iteration 157/1000 | Loss: 0.00010769
Iteration 158/1000 | Loss: 0.00010390
Iteration 159/1000 | Loss: 0.00012478
Iteration 160/1000 | Loss: 0.00010672
Iteration 161/1000 | Loss: 0.00012153
Iteration 162/1000 | Loss: 0.00011050
Iteration 163/1000 | Loss: 0.00011644
Iteration 164/1000 | Loss: 0.00011534
Iteration 165/1000 | Loss: 0.00011029
Iteration 166/1000 | Loss: 0.00010267
Iteration 167/1000 | Loss: 0.00012269
Iteration 168/1000 | Loss: 0.00010942
Iteration 169/1000 | Loss: 0.00011180
Iteration 170/1000 | Loss: 0.00010965
Iteration 171/1000 | Loss: 0.00011423
Iteration 172/1000 | Loss: 0.00009717
Iteration 173/1000 | Loss: 0.00017889
Iteration 174/1000 | Loss: 0.00011137
Iteration 175/1000 | Loss: 0.00025406
Iteration 176/1000 | Loss: 0.00014900
Iteration 177/1000 | Loss: 0.00016599
Iteration 178/1000 | Loss: 0.00012014
Iteration 179/1000 | Loss: 0.00009756
Iteration 180/1000 | Loss: 0.00009263
Iteration 181/1000 | Loss: 0.00010213
Iteration 182/1000 | Loss: 0.00012302
Iteration 183/1000 | Loss: 0.00011129
Iteration 184/1000 | Loss: 0.00009398
Iteration 185/1000 | Loss: 0.00010445
Iteration 186/1000 | Loss: 0.00010500
Iteration 187/1000 | Loss: 0.00010150
Iteration 188/1000 | Loss: 0.00010110
Iteration 189/1000 | Loss: 0.00010083
Iteration 190/1000 | Loss: 0.00011511
Iteration 191/1000 | Loss: 0.00011921
Iteration 192/1000 | Loss: 0.00011323
Iteration 193/1000 | Loss: 0.00011112
Iteration 194/1000 | Loss: 0.00011310
Iteration 195/1000 | Loss: 0.00010812
Iteration 196/1000 | Loss: 0.00012318
Iteration 197/1000 | Loss: 0.00010835
Iteration 198/1000 | Loss: 0.00011069
Iteration 199/1000 | Loss: 0.00011933
Iteration 200/1000 | Loss: 0.00041992
Iteration 201/1000 | Loss: 0.00021009
Iteration 202/1000 | Loss: 0.00018420
Iteration 203/1000 | Loss: 0.00017257
Iteration 204/1000 | Loss: 0.00012343
Iteration 205/1000 | Loss: 0.00018813
Iteration 206/1000 | Loss: 0.00009991
Iteration 207/1000 | Loss: 0.00009311
Iteration 208/1000 | Loss: 0.00009606
Iteration 209/1000 | Loss: 0.00009393
Iteration 210/1000 | Loss: 0.00015489
Iteration 211/1000 | Loss: 0.00011572
Iteration 212/1000 | Loss: 0.00009170
Iteration 213/1000 | Loss: 0.00009512
Iteration 214/1000 | Loss: 0.00009330
Iteration 215/1000 | Loss: 0.00009088
Iteration 216/1000 | Loss: 0.00009391
Iteration 217/1000 | Loss: 0.00009051
Iteration 218/1000 | Loss: 0.00009048
Iteration 219/1000 | Loss: 0.00010639
Iteration 220/1000 | Loss: 0.00009696
Iteration 221/1000 | Loss: 0.00009132
Iteration 222/1000 | Loss: 0.00010613
Iteration 223/1000 | Loss: 0.00009113
Iteration 224/1000 | Loss: 0.00022796
Iteration 225/1000 | Loss: 0.00098667
Iteration 226/1000 | Loss: 0.00036563
Iteration 227/1000 | Loss: 0.00032283
Iteration 228/1000 | Loss: 0.00031846
Iteration 229/1000 | Loss: 0.00034744
Iteration 230/1000 | Loss: 0.00068521
Iteration 231/1000 | Loss: 0.00016525
Iteration 232/1000 | Loss: 0.00010200
Iteration 233/1000 | Loss: 0.00009384
Iteration 234/1000 | Loss: 0.00029291
Iteration 235/1000 | Loss: 0.00055839
Iteration 236/1000 | Loss: 0.00014675
Iteration 237/1000 | Loss: 0.00011012
Iteration 238/1000 | Loss: 0.00009355
Iteration 239/1000 | Loss: 0.00008948
Iteration 240/1000 | Loss: 0.00009739
Iteration 241/1000 | Loss: 0.00028710
Iteration 242/1000 | Loss: 0.00016785
Iteration 243/1000 | Loss: 0.00008938
Iteration 244/1000 | Loss: 0.00009274
Iteration 245/1000 | Loss: 0.00008703
Iteration 246/1000 | Loss: 0.00008642
Iteration 247/1000 | Loss: 0.00008598
Iteration 248/1000 | Loss: 0.00008569
Iteration 249/1000 | Loss: 0.00008558
Iteration 250/1000 | Loss: 0.00008553
Iteration 251/1000 | Loss: 0.00008535
Iteration 252/1000 | Loss: 0.00008518
Iteration 253/1000 | Loss: 0.00008518
Iteration 254/1000 | Loss: 0.00008502
Iteration 255/1000 | Loss: 0.00008501
Iteration 256/1000 | Loss: 0.00008501
Iteration 257/1000 | Loss: 0.00033230
Iteration 258/1000 | Loss: 0.00019541
Iteration 259/1000 | Loss: 0.00016553
Iteration 260/1000 | Loss: 0.00008659
Iteration 261/1000 | Loss: 0.00008515
Iteration 262/1000 | Loss: 0.00018684
Iteration 263/1000 | Loss: 0.00009088
Iteration 264/1000 | Loss: 0.00008679
Iteration 265/1000 | Loss: 0.00008560
Iteration 266/1000 | Loss: 0.00008487
Iteration 267/1000 | Loss: 0.00021324
Iteration 268/1000 | Loss: 0.00009499
Iteration 269/1000 | Loss: 0.00008933
Iteration 270/1000 | Loss: 0.00008777
Iteration 271/1000 | Loss: 0.00008704
Iteration 272/1000 | Loss: 0.00008649
Iteration 273/1000 | Loss: 0.00008603
Iteration 274/1000 | Loss: 0.00008577
Iteration 275/1000 | Loss: 0.00008574
Iteration 276/1000 | Loss: 0.00008571
Iteration 277/1000 | Loss: 0.00008571
Iteration 278/1000 | Loss: 0.00033376
Iteration 279/1000 | Loss: 0.00014768
Iteration 280/1000 | Loss: 0.00028797
Iteration 281/1000 | Loss: 0.00017341
Iteration 282/1000 | Loss: 0.00016199
Iteration 283/1000 | Loss: 0.00026061
Iteration 284/1000 | Loss: 0.00025654
Iteration 285/1000 | Loss: 0.00011106
Iteration 286/1000 | Loss: 0.00009134
Iteration 287/1000 | Loss: 0.00008808
Iteration 288/1000 | Loss: 0.00011522
Iteration 289/1000 | Loss: 0.00009541
Iteration 290/1000 | Loss: 0.00023950
Iteration 291/1000 | Loss: 0.00013185
Iteration 292/1000 | Loss: 0.00025746
Iteration 293/1000 | Loss: 0.00031944
Iteration 294/1000 | Loss: 0.00021972
Iteration 295/1000 | Loss: 0.00018068
Iteration 296/1000 | Loss: 0.00017987
Iteration 297/1000 | Loss: 0.00008705
Iteration 298/1000 | Loss: 0.00019404
Iteration 299/1000 | Loss: 0.00009054
Iteration 300/1000 | Loss: 0.00008540
Iteration 301/1000 | Loss: 0.00008488
Iteration 302/1000 | Loss: 0.00009678
Iteration 303/1000 | Loss: 0.00009343
Iteration 304/1000 | Loss: 0.00008441
Iteration 305/1000 | Loss: 0.00008351
Iteration 306/1000 | Loss: 0.00008337
Iteration 307/1000 | Loss: 0.00008333
Iteration 308/1000 | Loss: 0.00008332
Iteration 309/1000 | Loss: 0.00008330
Iteration 310/1000 | Loss: 0.00008330
Iteration 311/1000 | Loss: 0.00008330
Iteration 312/1000 | Loss: 0.00008330
Iteration 313/1000 | Loss: 0.00008330
Iteration 314/1000 | Loss: 0.00008329
Iteration 315/1000 | Loss: 0.00008329
Iteration 316/1000 | Loss: 0.00008329
Iteration 317/1000 | Loss: 0.00008329
Iteration 318/1000 | Loss: 0.00008329
Iteration 319/1000 | Loss: 0.00008329
Iteration 320/1000 | Loss: 0.00008329
Iteration 321/1000 | Loss: 0.00008329
Iteration 322/1000 | Loss: 0.00008329
Iteration 323/1000 | Loss: 0.00008385
Iteration 324/1000 | Loss: 0.00008337
Iteration 325/1000 | Loss: 0.00008324
Iteration 326/1000 | Loss: 0.00008323
Iteration 327/1000 | Loss: 0.00008323
Iteration 328/1000 | Loss: 0.00008323
Iteration 329/1000 | Loss: 0.00008323
Iteration 330/1000 | Loss: 0.00008323
Iteration 331/1000 | Loss: 0.00008324
Iteration 332/1000 | Loss: 0.00008324
Iteration 333/1000 | Loss: 0.00008324
Iteration 334/1000 | Loss: 0.00008324
Iteration 335/1000 | Loss: 0.00008324
Iteration 336/1000 | Loss: 0.00008324
Iteration 337/1000 | Loss: 0.00008324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [8.324068039655685e-05, 8.324068039655685e-05, 8.324068039655685e-05, 8.324068039655685e-05, 8.324068039655685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.324068039655685e-05

Optimization complete. Final v2v error: 4.854516506195068 mm

Highest mean error: 11.849883079528809 mm for frame 150

Lowest mean error: 2.8668372631073 mm for frame 126

Saving results

Total time: 532.5915114879608
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978718
Iteration 2/25 | Loss: 0.00219248
Iteration 3/25 | Loss: 0.00185109
Iteration 4/25 | Loss: 0.00185422
Iteration 5/25 | Loss: 0.00177385
Iteration 6/25 | Loss: 0.00177827
Iteration 7/25 | Loss: 0.00167118
Iteration 8/25 | Loss: 0.00163417
Iteration 9/25 | Loss: 0.00159733
Iteration 10/25 | Loss: 0.00158889
Iteration 11/25 | Loss: 0.00156760
Iteration 12/25 | Loss: 0.00155274
Iteration 13/25 | Loss: 0.00155235
Iteration 14/25 | Loss: 0.00155678
Iteration 15/25 | Loss: 0.00155601
Iteration 16/25 | Loss: 0.00154791
Iteration 17/25 | Loss: 0.00154833
Iteration 18/25 | Loss: 0.00153859
Iteration 19/25 | Loss: 0.00154410
Iteration 20/25 | Loss: 0.00154135
Iteration 21/25 | Loss: 0.00153191
Iteration 22/25 | Loss: 0.00153351
Iteration 23/25 | Loss: 0.00152716
Iteration 24/25 | Loss: 0.00152194
Iteration 25/25 | Loss: 0.00152237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36563921
Iteration 2/25 | Loss: 0.00272040
Iteration 3/25 | Loss: 0.00271819
Iteration 4/25 | Loss: 0.00271819
Iteration 5/25 | Loss: 0.00271819
Iteration 6/25 | Loss: 0.00271819
Iteration 7/25 | Loss: 0.00271819
Iteration 8/25 | Loss: 0.00271819
Iteration 9/25 | Loss: 0.00271818
Iteration 10/25 | Loss: 0.00271818
Iteration 11/25 | Loss: 0.00271818
Iteration 12/25 | Loss: 0.00271818
Iteration 13/25 | Loss: 0.00271818
Iteration 14/25 | Loss: 0.00271818
Iteration 15/25 | Loss: 0.00271818
Iteration 16/25 | Loss: 0.00271818
Iteration 17/25 | Loss: 0.00271818
Iteration 18/25 | Loss: 0.00271818
Iteration 19/25 | Loss: 0.00271818
Iteration 20/25 | Loss: 0.00271818
Iteration 21/25 | Loss: 0.00271818
Iteration 22/25 | Loss: 0.00271818
Iteration 23/25 | Loss: 0.00271818
Iteration 24/25 | Loss: 0.00271818
Iteration 25/25 | Loss: 0.00271818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271818
Iteration 2/1000 | Loss: 0.00046226
Iteration 3/1000 | Loss: 0.00015128
Iteration 4/1000 | Loss: 0.00005373
Iteration 5/1000 | Loss: 0.00006892
Iteration 6/1000 | Loss: 0.00005179
Iteration 7/1000 | Loss: 0.00004555
Iteration 8/1000 | Loss: 0.00005111
Iteration 9/1000 | Loss: 0.00004398
Iteration 10/1000 | Loss: 0.00003133
Iteration 11/1000 | Loss: 0.00027028
Iteration 12/1000 | Loss: 0.00023592
Iteration 13/1000 | Loss: 0.00024653
Iteration 14/1000 | Loss: 0.00026997
Iteration 15/1000 | Loss: 0.00005850
Iteration 16/1000 | Loss: 0.00029479
Iteration 17/1000 | Loss: 0.00006594
Iteration 18/1000 | Loss: 0.00004688
Iteration 19/1000 | Loss: 0.00004978
Iteration 20/1000 | Loss: 0.00027559
Iteration 21/1000 | Loss: 0.00051868
Iteration 22/1000 | Loss: 0.00044817
Iteration 23/1000 | Loss: 0.00045678
Iteration 24/1000 | Loss: 0.00064332
Iteration 25/1000 | Loss: 0.00056830
Iteration 26/1000 | Loss: 0.00042610
Iteration 27/1000 | Loss: 0.00030665
Iteration 28/1000 | Loss: 0.00033115
Iteration 29/1000 | Loss: 0.00024809
Iteration 30/1000 | Loss: 0.00005883
Iteration 31/1000 | Loss: 0.00004548
Iteration 32/1000 | Loss: 0.00031981
Iteration 33/1000 | Loss: 0.00027994
Iteration 34/1000 | Loss: 0.00028225
Iteration 35/1000 | Loss: 0.00020304
Iteration 36/1000 | Loss: 0.00025915
Iteration 37/1000 | Loss: 0.00015041
Iteration 38/1000 | Loss: 0.00005891
Iteration 39/1000 | Loss: 0.00007833
Iteration 40/1000 | Loss: 0.00005863
Iteration 41/1000 | Loss: 0.00005226
Iteration 42/1000 | Loss: 0.00005960
Iteration 43/1000 | Loss: 0.00003390
Iteration 44/1000 | Loss: 0.00003770
Iteration 45/1000 | Loss: 0.00005433
Iteration 46/1000 | Loss: 0.00004878
Iteration 47/1000 | Loss: 0.00004352
Iteration 48/1000 | Loss: 0.00004705
Iteration 49/1000 | Loss: 0.00004443
Iteration 50/1000 | Loss: 0.00003556
Iteration 51/1000 | Loss: 0.00002432
Iteration 52/1000 | Loss: 0.00005397
Iteration 53/1000 | Loss: 0.00006155
Iteration 54/1000 | Loss: 0.00005756
Iteration 55/1000 | Loss: 0.00006403
Iteration 56/1000 | Loss: 0.00005387
Iteration 57/1000 | Loss: 0.00006112
Iteration 58/1000 | Loss: 0.00005671
Iteration 59/1000 | Loss: 0.00006393
Iteration 60/1000 | Loss: 0.00005604
Iteration 61/1000 | Loss: 0.00005444
Iteration 62/1000 | Loss: 0.00004671
Iteration 63/1000 | Loss: 0.00006140
Iteration 64/1000 | Loss: 0.00004675
Iteration 65/1000 | Loss: 0.00004085
Iteration 66/1000 | Loss: 0.00004090
Iteration 67/1000 | Loss: 0.00003716
Iteration 68/1000 | Loss: 0.00002486
Iteration 69/1000 | Loss: 0.00002468
Iteration 70/1000 | Loss: 0.00004265
Iteration 71/1000 | Loss: 0.00003924
Iteration 72/1000 | Loss: 0.00003086
Iteration 73/1000 | Loss: 0.00020281
Iteration 74/1000 | Loss: 0.00013590
Iteration 75/1000 | Loss: 0.00021932
Iteration 76/1000 | Loss: 0.00025948
Iteration 77/1000 | Loss: 0.00018230
Iteration 78/1000 | Loss: 0.00022394
Iteration 79/1000 | Loss: 0.00017458
Iteration 80/1000 | Loss: 0.00019982
Iteration 81/1000 | Loss: 0.00016499
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002451
Iteration 84/1000 | Loss: 0.00002210
Iteration 85/1000 | Loss: 0.00026631
Iteration 86/1000 | Loss: 0.00002831
Iteration 87/1000 | Loss: 0.00002202
Iteration 88/1000 | Loss: 0.00002077
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001723
Iteration 93/1000 | Loss: 0.00001700
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001686
Iteration 98/1000 | Loss: 0.00001684
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001649
Iteration 104/1000 | Loss: 0.00001648
Iteration 105/1000 | Loss: 0.00001648
Iteration 106/1000 | Loss: 0.00001647
Iteration 107/1000 | Loss: 0.00001647
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001644
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001641
Iteration 118/1000 | Loss: 0.00001641
Iteration 119/1000 | Loss: 0.00001641
Iteration 120/1000 | Loss: 0.00001640
Iteration 121/1000 | Loss: 0.00001640
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001638
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001636
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001634
Iteration 134/1000 | Loss: 0.00001634
Iteration 135/1000 | Loss: 0.00001634
Iteration 136/1000 | Loss: 0.00001633
Iteration 137/1000 | Loss: 0.00001633
Iteration 138/1000 | Loss: 0.00001633
Iteration 139/1000 | Loss: 0.00001632
Iteration 140/1000 | Loss: 0.00001632
Iteration 141/1000 | Loss: 0.00001631
Iteration 142/1000 | Loss: 0.00001628
Iteration 143/1000 | Loss: 0.00001627
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001626
Iteration 146/1000 | Loss: 0.00001626
Iteration 147/1000 | Loss: 0.00001626
Iteration 148/1000 | Loss: 0.00001626
Iteration 149/1000 | Loss: 0.00001626
Iteration 150/1000 | Loss: 0.00001626
Iteration 151/1000 | Loss: 0.00001626
Iteration 152/1000 | Loss: 0.00001626
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001624
Iteration 165/1000 | Loss: 0.00001624
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001623
Iteration 168/1000 | Loss: 0.00001622
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00001620
Iteration 172/1000 | Loss: 0.00001620
Iteration 173/1000 | Loss: 0.00001620
Iteration 174/1000 | Loss: 0.00001619
Iteration 175/1000 | Loss: 0.00001619
Iteration 176/1000 | Loss: 0.00001619
Iteration 177/1000 | Loss: 0.00001619
Iteration 178/1000 | Loss: 0.00001618
Iteration 179/1000 | Loss: 0.00001618
Iteration 180/1000 | Loss: 0.00001618
Iteration 181/1000 | Loss: 0.00001618
Iteration 182/1000 | Loss: 0.00001618
Iteration 183/1000 | Loss: 0.00001618
Iteration 184/1000 | Loss: 0.00001618
Iteration 185/1000 | Loss: 0.00001618
Iteration 186/1000 | Loss: 0.00001617
Iteration 187/1000 | Loss: 0.00001617
Iteration 188/1000 | Loss: 0.00001617
Iteration 189/1000 | Loss: 0.00001617
Iteration 190/1000 | Loss: 0.00001616
Iteration 191/1000 | Loss: 0.00001616
Iteration 192/1000 | Loss: 0.00001615
Iteration 193/1000 | Loss: 0.00001615
Iteration 194/1000 | Loss: 0.00001614
Iteration 195/1000 | Loss: 0.00001614
Iteration 196/1000 | Loss: 0.00001614
Iteration 197/1000 | Loss: 0.00001614
Iteration 198/1000 | Loss: 0.00001614
Iteration 199/1000 | Loss: 0.00001613
Iteration 200/1000 | Loss: 0.00001613
Iteration 201/1000 | Loss: 0.00001613
Iteration 202/1000 | Loss: 0.00001613
Iteration 203/1000 | Loss: 0.00001613
Iteration 204/1000 | Loss: 0.00001613
Iteration 205/1000 | Loss: 0.00001613
Iteration 206/1000 | Loss: 0.00001613
Iteration 207/1000 | Loss: 0.00001613
Iteration 208/1000 | Loss: 0.00001612
Iteration 209/1000 | Loss: 0.00001612
Iteration 210/1000 | Loss: 0.00001612
Iteration 211/1000 | Loss: 0.00001612
Iteration 212/1000 | Loss: 0.00001612
Iteration 213/1000 | Loss: 0.00001612
Iteration 214/1000 | Loss: 0.00001612
Iteration 215/1000 | Loss: 0.00001612
Iteration 216/1000 | Loss: 0.00001612
Iteration 217/1000 | Loss: 0.00001611
Iteration 218/1000 | Loss: 0.00001611
Iteration 219/1000 | Loss: 0.00001611
Iteration 220/1000 | Loss: 0.00001611
Iteration 221/1000 | Loss: 0.00001611
Iteration 222/1000 | Loss: 0.00001611
Iteration 223/1000 | Loss: 0.00001611
Iteration 224/1000 | Loss: 0.00001611
Iteration 225/1000 | Loss: 0.00001611
Iteration 226/1000 | Loss: 0.00001611
Iteration 227/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.6112617231556214e-05, 1.6112617231556214e-05, 1.6112617231556214e-05, 1.6112617231556214e-05, 1.6112617231556214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6112617231556214e-05

Optimization complete. Final v2v error: 3.4443588256835938 mm

Highest mean error: 4.575112819671631 mm for frame 68

Lowest mean error: 3.070026159286499 mm for frame 22

Saving results

Total time: 187.04186058044434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760118
Iteration 2/25 | Loss: 0.00180115
Iteration 3/25 | Loss: 0.00162679
Iteration 4/25 | Loss: 0.00161573
Iteration 5/25 | Loss: 0.00161385
Iteration 6/25 | Loss: 0.00161385
Iteration 7/25 | Loss: 0.00161385
Iteration 8/25 | Loss: 0.00161385
Iteration 9/25 | Loss: 0.00161385
Iteration 10/25 | Loss: 0.00161385
Iteration 11/25 | Loss: 0.00161385
Iteration 12/25 | Loss: 0.00161385
Iteration 13/25 | Loss: 0.00161385
Iteration 14/25 | Loss: 0.00161385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001613850356079638, 0.001613850356079638, 0.001613850356079638, 0.001613850356079638, 0.001613850356079638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001613850356079638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21539938
Iteration 2/25 | Loss: 0.00226948
Iteration 3/25 | Loss: 0.00226945
Iteration 4/25 | Loss: 0.00226945
Iteration 5/25 | Loss: 0.00226945
Iteration 6/25 | Loss: 0.00226945
Iteration 7/25 | Loss: 0.00226945
Iteration 8/25 | Loss: 0.00226945
Iteration 9/25 | Loss: 0.00226945
Iteration 10/25 | Loss: 0.00226945
Iteration 11/25 | Loss: 0.00226945
Iteration 12/25 | Loss: 0.00226945
Iteration 13/25 | Loss: 0.00226945
Iteration 14/25 | Loss: 0.00226945
Iteration 15/25 | Loss: 0.00226945
Iteration 16/25 | Loss: 0.00226945
Iteration 17/25 | Loss: 0.00226945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022694466169923544, 0.0022694466169923544, 0.0022694466169923544, 0.0022694466169923544, 0.0022694466169923544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022694466169923544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226945
Iteration 2/1000 | Loss: 0.00004704
Iteration 3/1000 | Loss: 0.00003533
Iteration 4/1000 | Loss: 0.00003189
Iteration 5/1000 | Loss: 0.00003043
Iteration 6/1000 | Loss: 0.00002953
Iteration 7/1000 | Loss: 0.00002891
Iteration 8/1000 | Loss: 0.00002824
Iteration 9/1000 | Loss: 0.00002779
Iteration 10/1000 | Loss: 0.00002738
Iteration 11/1000 | Loss: 0.00002695
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002598
Iteration 15/1000 | Loss: 0.00002571
Iteration 16/1000 | Loss: 0.00002555
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002548
Iteration 19/1000 | Loss: 0.00002547
Iteration 20/1000 | Loss: 0.00002547
Iteration 21/1000 | Loss: 0.00002547
Iteration 22/1000 | Loss: 0.00002546
Iteration 23/1000 | Loss: 0.00002546
Iteration 24/1000 | Loss: 0.00002544
Iteration 25/1000 | Loss: 0.00002542
Iteration 26/1000 | Loss: 0.00002542
Iteration 27/1000 | Loss: 0.00002542
Iteration 28/1000 | Loss: 0.00002542
Iteration 29/1000 | Loss: 0.00002542
Iteration 30/1000 | Loss: 0.00002542
Iteration 31/1000 | Loss: 0.00002542
Iteration 32/1000 | Loss: 0.00002541
Iteration 33/1000 | Loss: 0.00002541
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00002538
Iteration 36/1000 | Loss: 0.00002527
Iteration 37/1000 | Loss: 0.00002527
Iteration 38/1000 | Loss: 0.00002527
Iteration 39/1000 | Loss: 0.00002527
Iteration 40/1000 | Loss: 0.00002526
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002525
Iteration 43/1000 | Loss: 0.00002525
Iteration 44/1000 | Loss: 0.00002525
Iteration 45/1000 | Loss: 0.00002525
Iteration 46/1000 | Loss: 0.00002524
Iteration 47/1000 | Loss: 0.00002524
Iteration 48/1000 | Loss: 0.00002523
Iteration 49/1000 | Loss: 0.00002523
Iteration 50/1000 | Loss: 0.00002522
Iteration 51/1000 | Loss: 0.00002521
Iteration 52/1000 | Loss: 0.00002521
Iteration 53/1000 | Loss: 0.00002520
Iteration 54/1000 | Loss: 0.00002520
Iteration 55/1000 | Loss: 0.00002520
Iteration 56/1000 | Loss: 0.00002520
Iteration 57/1000 | Loss: 0.00002520
Iteration 58/1000 | Loss: 0.00002520
Iteration 59/1000 | Loss: 0.00002519
Iteration 60/1000 | Loss: 0.00002519
Iteration 61/1000 | Loss: 0.00002519
Iteration 62/1000 | Loss: 0.00002518
Iteration 63/1000 | Loss: 0.00002518
Iteration 64/1000 | Loss: 0.00002518
Iteration 65/1000 | Loss: 0.00002518
Iteration 66/1000 | Loss: 0.00002518
Iteration 67/1000 | Loss: 0.00002517
Iteration 68/1000 | Loss: 0.00002517
Iteration 69/1000 | Loss: 0.00002517
Iteration 70/1000 | Loss: 0.00002517
Iteration 71/1000 | Loss: 0.00002516
Iteration 72/1000 | Loss: 0.00002516
Iteration 73/1000 | Loss: 0.00002516
Iteration 74/1000 | Loss: 0.00002516
Iteration 75/1000 | Loss: 0.00002515
Iteration 76/1000 | Loss: 0.00002515
Iteration 77/1000 | Loss: 0.00002515
Iteration 78/1000 | Loss: 0.00002515
Iteration 79/1000 | Loss: 0.00002515
Iteration 80/1000 | Loss: 0.00002515
Iteration 81/1000 | Loss: 0.00002515
Iteration 82/1000 | Loss: 0.00002515
Iteration 83/1000 | Loss: 0.00002515
Iteration 84/1000 | Loss: 0.00002515
Iteration 85/1000 | Loss: 0.00002515
Iteration 86/1000 | Loss: 0.00002515
Iteration 87/1000 | Loss: 0.00002515
Iteration 88/1000 | Loss: 0.00002515
Iteration 89/1000 | Loss: 0.00002515
Iteration 90/1000 | Loss: 0.00002515
Iteration 91/1000 | Loss: 0.00002515
Iteration 92/1000 | Loss: 0.00002515
Iteration 93/1000 | Loss: 0.00002515
Iteration 94/1000 | Loss: 0.00002515
Iteration 95/1000 | Loss: 0.00002515
Iteration 96/1000 | Loss: 0.00002515
Iteration 97/1000 | Loss: 0.00002515
Iteration 98/1000 | Loss: 0.00002515
Iteration 99/1000 | Loss: 0.00002515
Iteration 100/1000 | Loss: 0.00002515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.514707375667058e-05, 2.514707375667058e-05, 2.514707375667058e-05, 2.514707375667058e-05, 2.514707375667058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.514707375667058e-05

Optimization complete. Final v2v error: 4.206379413604736 mm

Highest mean error: 4.414871692657471 mm for frame 75

Lowest mean error: 4.0520100593566895 mm for frame 6

Saving results

Total time: 36.68231463432312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485637
Iteration 2/25 | Loss: 0.00160433
Iteration 3/25 | Loss: 0.00153734
Iteration 4/25 | Loss: 0.00153187
Iteration 5/25 | Loss: 0.00153107
Iteration 6/25 | Loss: 0.00153107
Iteration 7/25 | Loss: 0.00153107
Iteration 8/25 | Loss: 0.00153107
Iteration 9/25 | Loss: 0.00153107
Iteration 10/25 | Loss: 0.00153107
Iteration 11/25 | Loss: 0.00153107
Iteration 12/25 | Loss: 0.00153107
Iteration 13/25 | Loss: 0.00153107
Iteration 14/25 | Loss: 0.00153107
Iteration 15/25 | Loss: 0.00153107
Iteration 16/25 | Loss: 0.00153107
Iteration 17/25 | Loss: 0.00153107
Iteration 18/25 | Loss: 0.00153107
Iteration 19/25 | Loss: 0.00153107
Iteration 20/25 | Loss: 0.00153107
Iteration 21/25 | Loss: 0.00153107
Iteration 22/25 | Loss: 0.00153107
Iteration 23/25 | Loss: 0.00153107
Iteration 24/25 | Loss: 0.00153107
Iteration 25/25 | Loss: 0.00153107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24572265
Iteration 2/25 | Loss: 0.00255486
Iteration 3/25 | Loss: 0.00255486
Iteration 4/25 | Loss: 0.00255486
Iteration 5/25 | Loss: 0.00255486
Iteration 6/25 | Loss: 0.00255486
Iteration 7/25 | Loss: 0.00255486
Iteration 8/25 | Loss: 0.00255486
Iteration 9/25 | Loss: 0.00255486
Iteration 10/25 | Loss: 0.00255486
Iteration 11/25 | Loss: 0.00255486
Iteration 12/25 | Loss: 0.00255486
Iteration 13/25 | Loss: 0.00255486
Iteration 14/25 | Loss: 0.00255486
Iteration 15/25 | Loss: 0.00255486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002554855076596141, 0.002554855076596141, 0.002554855076596141, 0.002554855076596141, 0.002554855076596141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002554855076596141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255486
Iteration 2/1000 | Loss: 0.00003595
Iteration 3/1000 | Loss: 0.00002606
Iteration 4/1000 | Loss: 0.00002356
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002133
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00002015
Iteration 9/1000 | Loss: 0.00001983
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001921
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001898
Iteration 15/1000 | Loss: 0.00001878
Iteration 16/1000 | Loss: 0.00001866
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001847
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001816
Iteration 23/1000 | Loss: 0.00001814
Iteration 24/1000 | Loss: 0.00001805
Iteration 25/1000 | Loss: 0.00001804
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001785
Iteration 32/1000 | Loss: 0.00001784
Iteration 33/1000 | Loss: 0.00001783
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001782
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001781
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001768
Iteration 41/1000 | Loss: 0.00001768
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001756
Iteration 59/1000 | Loss: 0.00001756
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001753
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001741
Iteration 75/1000 | Loss: 0.00001741
Iteration 76/1000 | Loss: 0.00001740
Iteration 77/1000 | Loss: 0.00001740
Iteration 78/1000 | Loss: 0.00001739
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001737
Iteration 83/1000 | Loss: 0.00001737
Iteration 84/1000 | Loss: 0.00001737
Iteration 85/1000 | Loss: 0.00001737
Iteration 86/1000 | Loss: 0.00001737
Iteration 87/1000 | Loss: 0.00001737
Iteration 88/1000 | Loss: 0.00001737
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001735
Iteration 92/1000 | Loss: 0.00001735
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001730
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001729
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001727
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001727
Iteration 126/1000 | Loss: 0.00001727
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001726
Iteration 130/1000 | Loss: 0.00001725
Iteration 131/1000 | Loss: 0.00001725
Iteration 132/1000 | Loss: 0.00001725
Iteration 133/1000 | Loss: 0.00001725
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001725
Iteration 136/1000 | Loss: 0.00001725
Iteration 137/1000 | Loss: 0.00001725
Iteration 138/1000 | Loss: 0.00001724
Iteration 139/1000 | Loss: 0.00001724
Iteration 140/1000 | Loss: 0.00001724
Iteration 141/1000 | Loss: 0.00001724
Iteration 142/1000 | Loss: 0.00001724
Iteration 143/1000 | Loss: 0.00001724
Iteration 144/1000 | Loss: 0.00001724
Iteration 145/1000 | Loss: 0.00001724
Iteration 146/1000 | Loss: 0.00001724
Iteration 147/1000 | Loss: 0.00001723
Iteration 148/1000 | Loss: 0.00001723
Iteration 149/1000 | Loss: 0.00001723
Iteration 150/1000 | Loss: 0.00001723
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001722
Iteration 156/1000 | Loss: 0.00001722
Iteration 157/1000 | Loss: 0.00001722
Iteration 158/1000 | Loss: 0.00001722
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001721
Iteration 164/1000 | Loss: 0.00001721
Iteration 165/1000 | Loss: 0.00001721
Iteration 166/1000 | Loss: 0.00001721
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001721
Iteration 173/1000 | Loss: 0.00001721
Iteration 174/1000 | Loss: 0.00001721
Iteration 175/1000 | Loss: 0.00001721
Iteration 176/1000 | Loss: 0.00001721
Iteration 177/1000 | Loss: 0.00001721
Iteration 178/1000 | Loss: 0.00001721
Iteration 179/1000 | Loss: 0.00001721
Iteration 180/1000 | Loss: 0.00001721
Iteration 181/1000 | Loss: 0.00001721
Iteration 182/1000 | Loss: 0.00001721
Iteration 183/1000 | Loss: 0.00001721
Iteration 184/1000 | Loss: 0.00001721
Iteration 185/1000 | Loss: 0.00001721
Iteration 186/1000 | Loss: 0.00001721
Iteration 187/1000 | Loss: 0.00001721
Iteration 188/1000 | Loss: 0.00001721
Iteration 189/1000 | Loss: 0.00001721
Iteration 190/1000 | Loss: 0.00001721
Iteration 191/1000 | Loss: 0.00001721
Iteration 192/1000 | Loss: 0.00001721
Iteration 193/1000 | Loss: 0.00001721
Iteration 194/1000 | Loss: 0.00001721
Iteration 195/1000 | Loss: 0.00001721
Iteration 196/1000 | Loss: 0.00001721
Iteration 197/1000 | Loss: 0.00001721
Iteration 198/1000 | Loss: 0.00001721
Iteration 199/1000 | Loss: 0.00001721
Iteration 200/1000 | Loss: 0.00001721
Iteration 201/1000 | Loss: 0.00001721
Iteration 202/1000 | Loss: 0.00001721
Iteration 203/1000 | Loss: 0.00001721
Iteration 204/1000 | Loss: 0.00001721
Iteration 205/1000 | Loss: 0.00001721
Iteration 206/1000 | Loss: 0.00001721
Iteration 207/1000 | Loss: 0.00001721
Iteration 208/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.7207676137331873e-05, 1.7207676137331873e-05, 1.7207676137331873e-05, 1.7207676137331873e-05, 1.7207676137331873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7207676137331873e-05

Optimization complete. Final v2v error: 3.515709161758423 mm

Highest mean error: 3.9127399921417236 mm for frame 61

Lowest mean error: 3.3566155433654785 mm for frame 12

Saving results

Total time: 49.52194809913635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904476
Iteration 2/25 | Loss: 0.00219416
Iteration 3/25 | Loss: 0.00179653
Iteration 4/25 | Loss: 0.00173075
Iteration 5/25 | Loss: 0.00173845
Iteration 6/25 | Loss: 0.00171321
Iteration 7/25 | Loss: 0.00170368
Iteration 8/25 | Loss: 0.00168580
Iteration 9/25 | Loss: 0.00168444
Iteration 10/25 | Loss: 0.00167993
Iteration 11/25 | Loss: 0.00167531
Iteration 12/25 | Loss: 0.00167318
Iteration 13/25 | Loss: 0.00166860
Iteration 14/25 | Loss: 0.00167151
Iteration 15/25 | Loss: 0.00166820
Iteration 16/25 | Loss: 0.00166654
Iteration 17/25 | Loss: 0.00166599
Iteration 18/25 | Loss: 0.00166738
Iteration 19/25 | Loss: 0.00166534
Iteration 20/25 | Loss: 0.00166595
Iteration 21/25 | Loss: 0.00166334
Iteration 22/25 | Loss: 0.00165969
Iteration 23/25 | Loss: 0.00165930
Iteration 24/25 | Loss: 0.00165918
Iteration 25/25 | Loss: 0.00165918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.23866510
Iteration 2/25 | Loss: 0.00396925
Iteration 3/25 | Loss: 0.00396925
Iteration 4/25 | Loss: 0.00396925
Iteration 5/25 | Loss: 0.00396925
Iteration 6/25 | Loss: 0.00396925
Iteration 7/25 | Loss: 0.00396924
Iteration 8/25 | Loss: 0.00396924
Iteration 9/25 | Loss: 0.00396924
Iteration 10/25 | Loss: 0.00396924
Iteration 11/25 | Loss: 0.00396924
Iteration 12/25 | Loss: 0.00396924
Iteration 13/25 | Loss: 0.00396924
Iteration 14/25 | Loss: 0.00396924
Iteration 15/25 | Loss: 0.00396924
Iteration 16/25 | Loss: 0.00396924
Iteration 17/25 | Loss: 0.00396924
Iteration 18/25 | Loss: 0.00396924
Iteration 19/25 | Loss: 0.00396924
Iteration 20/25 | Loss: 0.00396924
Iteration 21/25 | Loss: 0.00396924
Iteration 22/25 | Loss: 0.00396924
Iteration 23/25 | Loss: 0.00396924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00396924139931798, 0.00396924139931798, 0.00396924139931798, 0.00396924139931798, 0.00396924139931798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00396924139931798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00396924
Iteration 2/1000 | Loss: 0.00032044
Iteration 3/1000 | Loss: 0.00098519
Iteration 4/1000 | Loss: 0.00258564
Iteration 5/1000 | Loss: 0.00247097
Iteration 6/1000 | Loss: 0.00126864
Iteration 7/1000 | Loss: 0.00126023
Iteration 8/1000 | Loss: 0.00051632
Iteration 9/1000 | Loss: 0.00105513
Iteration 10/1000 | Loss: 0.00091540
Iteration 11/1000 | Loss: 0.00051814
Iteration 12/1000 | Loss: 0.00213602
Iteration 13/1000 | Loss: 0.00009309
Iteration 14/1000 | Loss: 0.00097225
Iteration 15/1000 | Loss: 0.00054550
Iteration 16/1000 | Loss: 0.00006655
Iteration 17/1000 | Loss: 0.00052677
Iteration 18/1000 | Loss: 0.00005448
Iteration 19/1000 | Loss: 0.00004513
Iteration 20/1000 | Loss: 0.00004101
Iteration 21/1000 | Loss: 0.00003843
Iteration 22/1000 | Loss: 0.00003650
Iteration 23/1000 | Loss: 0.00003546
Iteration 24/1000 | Loss: 0.00003454
Iteration 25/1000 | Loss: 0.00053644
Iteration 26/1000 | Loss: 0.00003814
Iteration 27/1000 | Loss: 0.00003225
Iteration 28/1000 | Loss: 0.00003071
Iteration 29/1000 | Loss: 0.00002982
Iteration 30/1000 | Loss: 0.00056625
Iteration 31/1000 | Loss: 0.00003034
Iteration 32/1000 | Loss: 0.00002779
Iteration 33/1000 | Loss: 0.00002660
Iteration 34/1000 | Loss: 0.00002607
Iteration 35/1000 | Loss: 0.00002569
Iteration 36/1000 | Loss: 0.00002533
Iteration 37/1000 | Loss: 0.00002517
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002497
Iteration 41/1000 | Loss: 0.00002479
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00002441
Iteration 45/1000 | Loss: 0.00002434
Iteration 46/1000 | Loss: 0.00002431
Iteration 47/1000 | Loss: 0.00002430
Iteration 48/1000 | Loss: 0.00002429
Iteration 49/1000 | Loss: 0.00002428
Iteration 50/1000 | Loss: 0.00002427
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002420
Iteration 54/1000 | Loss: 0.00002419
Iteration 55/1000 | Loss: 0.00002419
Iteration 56/1000 | Loss: 0.00002418
Iteration 57/1000 | Loss: 0.00002415
Iteration 58/1000 | Loss: 0.00002415
Iteration 59/1000 | Loss: 0.00002415
Iteration 60/1000 | Loss: 0.00002415
Iteration 61/1000 | Loss: 0.00002414
Iteration 62/1000 | Loss: 0.00002414
Iteration 63/1000 | Loss: 0.00002413
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002412
Iteration 66/1000 | Loss: 0.00002412
Iteration 67/1000 | Loss: 0.00002411
Iteration 68/1000 | Loss: 0.00002411
Iteration 69/1000 | Loss: 0.00002411
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002411
Iteration 72/1000 | Loss: 0.00002411
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002409
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002408
Iteration 80/1000 | Loss: 0.00002408
Iteration 81/1000 | Loss: 0.00002408
Iteration 82/1000 | Loss: 0.00002408
Iteration 83/1000 | Loss: 0.00002407
Iteration 84/1000 | Loss: 0.00002407
Iteration 85/1000 | Loss: 0.00002406
Iteration 86/1000 | Loss: 0.00002406
Iteration 87/1000 | Loss: 0.00002406
Iteration 88/1000 | Loss: 0.00002405
Iteration 89/1000 | Loss: 0.00002405
Iteration 90/1000 | Loss: 0.00002405
Iteration 91/1000 | Loss: 0.00002405
Iteration 92/1000 | Loss: 0.00002404
Iteration 93/1000 | Loss: 0.00002404
Iteration 94/1000 | Loss: 0.00002404
Iteration 95/1000 | Loss: 0.00002404
Iteration 96/1000 | Loss: 0.00002403
Iteration 97/1000 | Loss: 0.00002403
Iteration 98/1000 | Loss: 0.00002403
Iteration 99/1000 | Loss: 0.00002402
Iteration 100/1000 | Loss: 0.00002402
Iteration 101/1000 | Loss: 0.00002402
Iteration 102/1000 | Loss: 0.00002399
Iteration 103/1000 | Loss: 0.00002399
Iteration 104/1000 | Loss: 0.00002398
Iteration 105/1000 | Loss: 0.00002397
Iteration 106/1000 | Loss: 0.00002397
Iteration 107/1000 | Loss: 0.00002396
Iteration 108/1000 | Loss: 0.00002395
Iteration 109/1000 | Loss: 0.00002395
Iteration 110/1000 | Loss: 0.00002394
Iteration 111/1000 | Loss: 0.00002394
Iteration 112/1000 | Loss: 0.00002393
Iteration 113/1000 | Loss: 0.00002392
Iteration 114/1000 | Loss: 0.00002392
Iteration 115/1000 | Loss: 0.00002390
Iteration 116/1000 | Loss: 0.00002390
Iteration 117/1000 | Loss: 0.00002389
Iteration 118/1000 | Loss: 0.00002389
Iteration 119/1000 | Loss: 0.00002389
Iteration 120/1000 | Loss: 0.00002388
Iteration 121/1000 | Loss: 0.00002388
Iteration 122/1000 | Loss: 0.00002387
Iteration 123/1000 | Loss: 0.00002387
Iteration 124/1000 | Loss: 0.00002386
Iteration 125/1000 | Loss: 0.00002386
Iteration 126/1000 | Loss: 0.00002386
Iteration 127/1000 | Loss: 0.00002386
Iteration 128/1000 | Loss: 0.00002386
Iteration 129/1000 | Loss: 0.00002385
Iteration 130/1000 | Loss: 0.00002385
Iteration 131/1000 | Loss: 0.00002385
Iteration 132/1000 | Loss: 0.00002384
Iteration 133/1000 | Loss: 0.00002384
Iteration 134/1000 | Loss: 0.00002384
Iteration 135/1000 | Loss: 0.00002384
Iteration 136/1000 | Loss: 0.00002383
Iteration 137/1000 | Loss: 0.00002383
Iteration 138/1000 | Loss: 0.00002383
Iteration 139/1000 | Loss: 0.00002383
Iteration 140/1000 | Loss: 0.00002382
Iteration 141/1000 | Loss: 0.00002382
Iteration 142/1000 | Loss: 0.00002382
Iteration 143/1000 | Loss: 0.00002382
Iteration 144/1000 | Loss: 0.00002382
Iteration 145/1000 | Loss: 0.00002381
Iteration 146/1000 | Loss: 0.00002381
Iteration 147/1000 | Loss: 0.00002381
Iteration 148/1000 | Loss: 0.00002381
Iteration 149/1000 | Loss: 0.00002381
Iteration 150/1000 | Loss: 0.00002381
Iteration 151/1000 | Loss: 0.00002381
Iteration 152/1000 | Loss: 0.00002381
Iteration 153/1000 | Loss: 0.00002380
Iteration 154/1000 | Loss: 0.00002380
Iteration 155/1000 | Loss: 0.00002380
Iteration 156/1000 | Loss: 0.00002380
Iteration 157/1000 | Loss: 0.00002380
Iteration 158/1000 | Loss: 0.00002380
Iteration 159/1000 | Loss: 0.00002380
Iteration 160/1000 | Loss: 0.00002380
Iteration 161/1000 | Loss: 0.00002380
Iteration 162/1000 | Loss: 0.00002380
Iteration 163/1000 | Loss: 0.00002379
Iteration 164/1000 | Loss: 0.00002379
Iteration 165/1000 | Loss: 0.00002379
Iteration 166/1000 | Loss: 0.00002379
Iteration 167/1000 | Loss: 0.00002379
Iteration 168/1000 | Loss: 0.00002379
Iteration 169/1000 | Loss: 0.00002379
Iteration 170/1000 | Loss: 0.00002379
Iteration 171/1000 | Loss: 0.00002379
Iteration 172/1000 | Loss: 0.00002379
Iteration 173/1000 | Loss: 0.00002379
Iteration 174/1000 | Loss: 0.00002379
Iteration 175/1000 | Loss: 0.00002379
Iteration 176/1000 | Loss: 0.00002379
Iteration 177/1000 | Loss: 0.00002379
Iteration 178/1000 | Loss: 0.00002379
Iteration 179/1000 | Loss: 0.00002379
Iteration 180/1000 | Loss: 0.00002379
Iteration 181/1000 | Loss: 0.00002379
Iteration 182/1000 | Loss: 0.00002378
Iteration 183/1000 | Loss: 0.00002378
Iteration 184/1000 | Loss: 0.00002378
Iteration 185/1000 | Loss: 0.00002378
Iteration 186/1000 | Loss: 0.00002378
Iteration 187/1000 | Loss: 0.00002378
Iteration 188/1000 | Loss: 0.00002378
Iteration 189/1000 | Loss: 0.00002378
Iteration 190/1000 | Loss: 0.00002378
Iteration 191/1000 | Loss: 0.00002378
Iteration 192/1000 | Loss: 0.00002378
Iteration 193/1000 | Loss: 0.00002377
Iteration 194/1000 | Loss: 0.00002377
Iteration 195/1000 | Loss: 0.00002377
Iteration 196/1000 | Loss: 0.00002377
Iteration 197/1000 | Loss: 0.00002377
Iteration 198/1000 | Loss: 0.00002377
Iteration 199/1000 | Loss: 0.00002377
Iteration 200/1000 | Loss: 0.00002377
Iteration 201/1000 | Loss: 0.00002376
Iteration 202/1000 | Loss: 0.00002376
Iteration 203/1000 | Loss: 0.00002376
Iteration 204/1000 | Loss: 0.00002376
Iteration 205/1000 | Loss: 0.00002376
Iteration 206/1000 | Loss: 0.00002376
Iteration 207/1000 | Loss: 0.00002376
Iteration 208/1000 | Loss: 0.00002376
Iteration 209/1000 | Loss: 0.00002376
Iteration 210/1000 | Loss: 0.00002375
Iteration 211/1000 | Loss: 0.00002375
Iteration 212/1000 | Loss: 0.00002375
Iteration 213/1000 | Loss: 0.00002375
Iteration 214/1000 | Loss: 0.00002375
Iteration 215/1000 | Loss: 0.00002375
Iteration 216/1000 | Loss: 0.00002375
Iteration 217/1000 | Loss: 0.00002375
Iteration 218/1000 | Loss: 0.00002375
Iteration 219/1000 | Loss: 0.00002375
Iteration 220/1000 | Loss: 0.00002375
Iteration 221/1000 | Loss: 0.00002375
Iteration 222/1000 | Loss: 0.00002374
Iteration 223/1000 | Loss: 0.00002374
Iteration 224/1000 | Loss: 0.00002374
Iteration 225/1000 | Loss: 0.00002374
Iteration 226/1000 | Loss: 0.00002374
Iteration 227/1000 | Loss: 0.00002374
Iteration 228/1000 | Loss: 0.00002374
Iteration 229/1000 | Loss: 0.00002374
Iteration 230/1000 | Loss: 0.00002373
Iteration 231/1000 | Loss: 0.00002373
Iteration 232/1000 | Loss: 0.00002373
Iteration 233/1000 | Loss: 0.00002373
Iteration 234/1000 | Loss: 0.00002373
Iteration 235/1000 | Loss: 0.00002373
Iteration 236/1000 | Loss: 0.00002373
Iteration 237/1000 | Loss: 0.00002373
Iteration 238/1000 | Loss: 0.00002373
Iteration 239/1000 | Loss: 0.00002373
Iteration 240/1000 | Loss: 0.00002373
Iteration 241/1000 | Loss: 0.00002373
Iteration 242/1000 | Loss: 0.00002373
Iteration 243/1000 | Loss: 0.00002372
Iteration 244/1000 | Loss: 0.00002372
Iteration 245/1000 | Loss: 0.00002372
Iteration 246/1000 | Loss: 0.00002372
Iteration 247/1000 | Loss: 0.00002372
Iteration 248/1000 | Loss: 0.00002372
Iteration 249/1000 | Loss: 0.00002372
Iteration 250/1000 | Loss: 0.00002372
Iteration 251/1000 | Loss: 0.00002372
Iteration 252/1000 | Loss: 0.00002372
Iteration 253/1000 | Loss: 0.00002372
Iteration 254/1000 | Loss: 0.00002372
Iteration 255/1000 | Loss: 0.00002372
Iteration 256/1000 | Loss: 0.00002372
Iteration 257/1000 | Loss: 0.00002372
Iteration 258/1000 | Loss: 0.00002371
Iteration 259/1000 | Loss: 0.00002371
Iteration 260/1000 | Loss: 0.00002371
Iteration 261/1000 | Loss: 0.00002371
Iteration 262/1000 | Loss: 0.00002371
Iteration 263/1000 | Loss: 0.00002371
Iteration 264/1000 | Loss: 0.00002371
Iteration 265/1000 | Loss: 0.00002371
Iteration 266/1000 | Loss: 0.00002371
Iteration 267/1000 | Loss: 0.00002371
Iteration 268/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [2.3711354515398853e-05, 2.3711354515398853e-05, 2.3711354515398853e-05, 2.3711354515398853e-05, 2.3711354515398853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3711354515398853e-05

Optimization complete. Final v2v error: 3.610947370529175 mm

Highest mean error: 12.71141529083252 mm for frame 54

Lowest mean error: 2.9277143478393555 mm for frame 105

Saving results

Total time: 117.22133493423462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776835
Iteration 2/25 | Loss: 0.00282219
Iteration 3/25 | Loss: 0.00206640
Iteration 4/25 | Loss: 0.00213138
Iteration 5/25 | Loss: 0.00209780
Iteration 6/25 | Loss: 0.00208070
Iteration 7/25 | Loss: 0.00187533
Iteration 8/25 | Loss: 0.00180150
Iteration 9/25 | Loss: 0.00184112
Iteration 10/25 | Loss: 0.00183404
Iteration 11/25 | Loss: 0.00173265
Iteration 12/25 | Loss: 0.00168673
Iteration 13/25 | Loss: 0.00167648
Iteration 14/25 | Loss: 0.00164915
Iteration 15/25 | Loss: 0.00165290
Iteration 16/25 | Loss: 0.00165638
Iteration 17/25 | Loss: 0.00168784
Iteration 18/25 | Loss: 0.00162290
Iteration 19/25 | Loss: 0.00160924
Iteration 20/25 | Loss: 0.00158935
Iteration 21/25 | Loss: 0.00159350
Iteration 22/25 | Loss: 0.00159463
Iteration 23/25 | Loss: 0.00156209
Iteration 24/25 | Loss: 0.00156284
Iteration 25/25 | Loss: 0.00155061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67938602
Iteration 2/25 | Loss: 0.00218602
Iteration 3/25 | Loss: 0.00188962
Iteration 4/25 | Loss: 0.00185159
Iteration 5/25 | Loss: 0.00185159
Iteration 6/25 | Loss: 0.00185159
Iteration 7/25 | Loss: 0.00185159
Iteration 8/25 | Loss: 0.00185159
Iteration 9/25 | Loss: 0.00185159
Iteration 10/25 | Loss: 0.00185159
Iteration 11/25 | Loss: 0.00185159
Iteration 12/25 | Loss: 0.00185159
Iteration 13/25 | Loss: 0.00185159
Iteration 14/25 | Loss: 0.00185159
Iteration 15/25 | Loss: 0.00185159
Iteration 16/25 | Loss: 0.00185159
Iteration 17/25 | Loss: 0.00185159
Iteration 18/25 | Loss: 0.00185159
Iteration 19/25 | Loss: 0.00185159
Iteration 20/25 | Loss: 0.00185159
Iteration 21/25 | Loss: 0.00185159
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018515896517783403, 0.0018515896517783403, 0.0018515896517783403, 0.0018515896517783403, 0.0018515896517783403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018515896517783403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185159
Iteration 2/1000 | Loss: 0.00014255
Iteration 3/1000 | Loss: 0.00045934
Iteration 4/1000 | Loss: 0.00035908
Iteration 5/1000 | Loss: 0.00017723
Iteration 6/1000 | Loss: 0.00096700
Iteration 7/1000 | Loss: 0.00013184
Iteration 8/1000 | Loss: 0.00003023
Iteration 9/1000 | Loss: 0.00006612
Iteration 10/1000 | Loss: 0.00002893
Iteration 11/1000 | Loss: 0.00002822
Iteration 12/1000 | Loss: 0.00019876
Iteration 13/1000 | Loss: 0.00002713
Iteration 14/1000 | Loss: 0.00012786
Iteration 15/1000 | Loss: 0.00002650
Iteration 16/1000 | Loss: 0.00002620
Iteration 17/1000 | Loss: 0.00005278
Iteration 18/1000 | Loss: 0.00005665
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00002542
Iteration 21/1000 | Loss: 0.00005748
Iteration 22/1000 | Loss: 0.00008285
Iteration 23/1000 | Loss: 0.00002596
Iteration 24/1000 | Loss: 0.00049833
Iteration 25/1000 | Loss: 0.00089208
Iteration 26/1000 | Loss: 0.00003796
Iteration 27/1000 | Loss: 0.00004495
Iteration 28/1000 | Loss: 0.00020380
Iteration 29/1000 | Loss: 0.00451006
Iteration 30/1000 | Loss: 0.00017351
Iteration 31/1000 | Loss: 0.00006172
Iteration 32/1000 | Loss: 0.00008342
Iteration 33/1000 | Loss: 0.00002542
Iteration 34/1000 | Loss: 0.00004887
Iteration 35/1000 | Loss: 0.00009981
Iteration 36/1000 | Loss: 0.00002195
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002982
Iteration 39/1000 | Loss: 0.00004195
Iteration 40/1000 | Loss: 0.00008047
Iteration 41/1000 | Loss: 0.00002071
Iteration 42/1000 | Loss: 0.00002764
Iteration 43/1000 | Loss: 0.00002041
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002027
Iteration 46/1000 | Loss: 0.00002007
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001992
Iteration 50/1000 | Loss: 0.00001985
Iteration 51/1000 | Loss: 0.00001981
Iteration 52/1000 | Loss: 0.00001981
Iteration 53/1000 | Loss: 0.00001981
Iteration 54/1000 | Loss: 0.00005765
Iteration 55/1000 | Loss: 0.00002496
Iteration 56/1000 | Loss: 0.00003232
Iteration 57/1000 | Loss: 0.00001977
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001975
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001973
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001969
Iteration 102/1000 | Loss: 0.00001969
Iteration 103/1000 | Loss: 0.00001968
Iteration 104/1000 | Loss: 0.00001968
Iteration 105/1000 | Loss: 0.00001968
Iteration 106/1000 | Loss: 0.00001968
Iteration 107/1000 | Loss: 0.00001967
Iteration 108/1000 | Loss: 0.00001967
Iteration 109/1000 | Loss: 0.00001967
Iteration 110/1000 | Loss: 0.00001967
Iteration 111/1000 | Loss: 0.00001967
Iteration 112/1000 | Loss: 0.00001967
Iteration 113/1000 | Loss: 0.00001966
Iteration 114/1000 | Loss: 0.00001966
Iteration 115/1000 | Loss: 0.00001966
Iteration 116/1000 | Loss: 0.00001966
Iteration 117/1000 | Loss: 0.00001966
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001965
Iteration 120/1000 | Loss: 0.00001965
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00005070
Iteration 123/1000 | Loss: 0.00005560
Iteration 124/1000 | Loss: 0.00002040
Iteration 125/1000 | Loss: 0.00002842
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001963
Iteration 129/1000 | Loss: 0.00001963
Iteration 130/1000 | Loss: 0.00001963
Iteration 131/1000 | Loss: 0.00001963
Iteration 132/1000 | Loss: 0.00001963
Iteration 133/1000 | Loss: 0.00001963
Iteration 134/1000 | Loss: 0.00001963
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001962
Iteration 138/1000 | Loss: 0.00001961
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001961
Iteration 141/1000 | Loss: 0.00001961
Iteration 142/1000 | Loss: 0.00001961
Iteration 143/1000 | Loss: 0.00001961
Iteration 144/1000 | Loss: 0.00001961
Iteration 145/1000 | Loss: 0.00001961
Iteration 146/1000 | Loss: 0.00001961
Iteration 147/1000 | Loss: 0.00001961
Iteration 148/1000 | Loss: 0.00001961
Iteration 149/1000 | Loss: 0.00001961
Iteration 150/1000 | Loss: 0.00001960
Iteration 151/1000 | Loss: 0.00001960
Iteration 152/1000 | Loss: 0.00001960
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001958
Iteration 155/1000 | Loss: 0.00001958
Iteration 156/1000 | Loss: 0.00001958
Iteration 157/1000 | Loss: 0.00001958
Iteration 158/1000 | Loss: 0.00001958
Iteration 159/1000 | Loss: 0.00001958
Iteration 160/1000 | Loss: 0.00001958
Iteration 161/1000 | Loss: 0.00001958
Iteration 162/1000 | Loss: 0.00002365
Iteration 163/1000 | Loss: 0.00001998
Iteration 164/1000 | Loss: 0.00001960
Iteration 165/1000 | Loss: 0.00001959
Iteration 166/1000 | Loss: 0.00001959
Iteration 167/1000 | Loss: 0.00001959
Iteration 168/1000 | Loss: 0.00001959
Iteration 169/1000 | Loss: 0.00001959
Iteration 170/1000 | Loss: 0.00001959
Iteration 171/1000 | Loss: 0.00001959
Iteration 172/1000 | Loss: 0.00001959
Iteration 173/1000 | Loss: 0.00001959
Iteration 174/1000 | Loss: 0.00001959
Iteration 175/1000 | Loss: 0.00001959
Iteration 176/1000 | Loss: 0.00001997
Iteration 177/1000 | Loss: 0.00001962
Iteration 178/1000 | Loss: 0.00001961
Iteration 179/1000 | Loss: 0.00001960
Iteration 180/1000 | Loss: 0.00001960
Iteration 181/1000 | Loss: 0.00001960
Iteration 182/1000 | Loss: 0.00001960
Iteration 183/1000 | Loss: 0.00001960
Iteration 184/1000 | Loss: 0.00001960
Iteration 185/1000 | Loss: 0.00001960
Iteration 186/1000 | Loss: 0.00001960
Iteration 187/1000 | Loss: 0.00001960
Iteration 188/1000 | Loss: 0.00001960
Iteration 189/1000 | Loss: 0.00001960
Iteration 190/1000 | Loss: 0.00001960
Iteration 191/1000 | Loss: 0.00001960
Iteration 192/1000 | Loss: 0.00001960
Iteration 193/1000 | Loss: 0.00001960
Iteration 194/1000 | Loss: 0.00001960
Iteration 195/1000 | Loss: 0.00001959
Iteration 196/1000 | Loss: 0.00001959
Iteration 197/1000 | Loss: 0.00001960
Iteration 198/1000 | Loss: 0.00001960
Iteration 199/1000 | Loss: 0.00001959
Iteration 200/1000 | Loss: 0.00001959
Iteration 201/1000 | Loss: 0.00001959
Iteration 202/1000 | Loss: 0.00001959
Iteration 203/1000 | Loss: 0.00001959
Iteration 204/1000 | Loss: 0.00001959
Iteration 205/1000 | Loss: 0.00001959
Iteration 206/1000 | Loss: 0.00001959
Iteration 207/1000 | Loss: 0.00001959
Iteration 208/1000 | Loss: 0.00001959
Iteration 209/1000 | Loss: 0.00001959
Iteration 210/1000 | Loss: 0.00001959
Iteration 211/1000 | Loss: 0.00001959
Iteration 212/1000 | Loss: 0.00001959
Iteration 213/1000 | Loss: 0.00001959
Iteration 214/1000 | Loss: 0.00001959
Iteration 215/1000 | Loss: 0.00001959
Iteration 216/1000 | Loss: 0.00001959
Iteration 217/1000 | Loss: 0.00001959
Iteration 218/1000 | Loss: 0.00001959
Iteration 219/1000 | Loss: 0.00001959
Iteration 220/1000 | Loss: 0.00001959
Iteration 221/1000 | Loss: 0.00001959
Iteration 222/1000 | Loss: 0.00001959
Iteration 223/1000 | Loss: 0.00001959
Iteration 224/1000 | Loss: 0.00001959
Iteration 225/1000 | Loss: 0.00001959
Iteration 226/1000 | Loss: 0.00001959
Iteration 227/1000 | Loss: 0.00001959
Iteration 228/1000 | Loss: 0.00001959
Iteration 229/1000 | Loss: 0.00001959
Iteration 230/1000 | Loss: 0.00001959
Iteration 231/1000 | Loss: 0.00001959
Iteration 232/1000 | Loss: 0.00001959
Iteration 233/1000 | Loss: 0.00001959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.9589064322644845e-05, 1.9589064322644845e-05, 1.9589064322644845e-05, 1.9589064322644845e-05, 1.9589064322644845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9589064322644845e-05

Optimization complete. Final v2v error: 3.768810510635376 mm

Highest mean error: 4.225475788116455 mm for frame 18

Lowest mean error: 3.4197709560394287 mm for frame 123

Saving results

Total time: 128.15501236915588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998193
Iteration 2/25 | Loss: 0.00998193
Iteration 3/25 | Loss: 0.00998193
Iteration 4/25 | Loss: 0.00998193
Iteration 5/25 | Loss: 0.00998193
Iteration 6/25 | Loss: 0.00998193
Iteration 7/25 | Loss: 0.00998193
Iteration 8/25 | Loss: 0.00998193
Iteration 9/25 | Loss: 0.00998192
Iteration 10/25 | Loss: 0.00998192
Iteration 11/25 | Loss: 0.00998192
Iteration 12/25 | Loss: 0.00998192
Iteration 13/25 | Loss: 0.00998192
Iteration 14/25 | Loss: 0.00998192
Iteration 15/25 | Loss: 0.00998192
Iteration 16/25 | Loss: 0.00998192
Iteration 17/25 | Loss: 0.00998191
Iteration 18/25 | Loss: 0.00998191
Iteration 19/25 | Loss: 0.00998191
Iteration 20/25 | Loss: 0.00998191
Iteration 21/25 | Loss: 0.00998191
Iteration 22/25 | Loss: 0.00998191
Iteration 23/25 | Loss: 0.00998190
Iteration 24/25 | Loss: 0.00998190
Iteration 25/25 | Loss: 0.00998190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39268208
Iteration 2/25 | Loss: 0.10067160
Iteration 3/25 | Loss: 0.09944492
Iteration 4/25 | Loss: 0.09906323
Iteration 5/25 | Loss: 0.09912171
Iteration 6/25 | Loss: 0.09912167
Iteration 7/25 | Loss: 0.09902845
Iteration 8/25 | Loss: 0.09898685
Iteration 9/25 | Loss: 0.09898683
Iteration 10/25 | Loss: 0.09898683
Iteration 11/25 | Loss: 0.09898681
Iteration 12/25 | Loss: 0.09898681
Iteration 13/25 | Loss: 0.09898681
Iteration 14/25 | Loss: 0.09898681
Iteration 15/25 | Loss: 0.09898681
Iteration 16/25 | Loss: 0.09898681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.09898681193590164, 0.09898681193590164, 0.09898681193590164, 0.09898681193590164, 0.09898681193590164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09898681193590164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09898681
Iteration 2/1000 | Loss: 0.00686566
Iteration 3/1000 | Loss: 0.00997696
Iteration 4/1000 | Loss: 0.00093992
Iteration 5/1000 | Loss: 0.00417984
Iteration 6/1000 | Loss: 0.00101817
Iteration 7/1000 | Loss: 0.00060637
Iteration 8/1000 | Loss: 0.00023640
Iteration 9/1000 | Loss: 0.00040896
Iteration 10/1000 | Loss: 0.00185461
Iteration 11/1000 | Loss: 0.00015104
Iteration 12/1000 | Loss: 0.00070540
Iteration 13/1000 | Loss: 0.00010898
Iteration 14/1000 | Loss: 0.00121785
Iteration 15/1000 | Loss: 0.00014822
Iteration 16/1000 | Loss: 0.00037422
Iteration 17/1000 | Loss: 0.00009617
Iteration 18/1000 | Loss: 0.00042149
Iteration 19/1000 | Loss: 0.00054256
Iteration 20/1000 | Loss: 0.00008404
Iteration 21/1000 | Loss: 0.00021740
Iteration 22/1000 | Loss: 0.00010490
Iteration 23/1000 | Loss: 0.00020542
Iteration 24/1000 | Loss: 0.00030382
Iteration 25/1000 | Loss: 0.00005349
Iteration 26/1000 | Loss: 0.00014617
Iteration 27/1000 | Loss: 0.00044456
Iteration 28/1000 | Loss: 0.00008438
Iteration 29/1000 | Loss: 0.00013816
Iteration 30/1000 | Loss: 0.00012489
Iteration 31/1000 | Loss: 0.00003885
Iteration 32/1000 | Loss: 0.00016671
Iteration 33/1000 | Loss: 0.00047839
Iteration 34/1000 | Loss: 0.00005102
Iteration 35/1000 | Loss: 0.00006031
Iteration 36/1000 | Loss: 0.00017741
Iteration 37/1000 | Loss: 0.00031629
Iteration 38/1000 | Loss: 0.00004893
Iteration 39/1000 | Loss: 0.00004729
Iteration 40/1000 | Loss: 0.00002866
Iteration 41/1000 | Loss: 0.00028486
Iteration 42/1000 | Loss: 0.00006352
Iteration 43/1000 | Loss: 0.00010988
Iteration 44/1000 | Loss: 0.00020181
Iteration 45/1000 | Loss: 0.00002720
Iteration 46/1000 | Loss: 0.00005446
Iteration 47/1000 | Loss: 0.00002680
Iteration 48/1000 | Loss: 0.00002684
Iteration 49/1000 | Loss: 0.00023063
Iteration 50/1000 | Loss: 0.00004050
Iteration 51/1000 | Loss: 0.00002583
Iteration 52/1000 | Loss: 0.00010495
Iteration 53/1000 | Loss: 0.00004024
Iteration 54/1000 | Loss: 0.00006803
Iteration 55/1000 | Loss: 0.00003407
Iteration 56/1000 | Loss: 0.00002658
Iteration 57/1000 | Loss: 0.00007501
Iteration 58/1000 | Loss: 0.00003564
Iteration 59/1000 | Loss: 0.00002515
Iteration 60/1000 | Loss: 0.00003504
Iteration 61/1000 | Loss: 0.00002531
Iteration 62/1000 | Loss: 0.00002504
Iteration 63/1000 | Loss: 0.00002503
Iteration 64/1000 | Loss: 0.00002503
Iteration 65/1000 | Loss: 0.00002503
Iteration 66/1000 | Loss: 0.00002503
Iteration 67/1000 | Loss: 0.00002503
Iteration 68/1000 | Loss: 0.00002503
Iteration 69/1000 | Loss: 0.00002503
Iteration 70/1000 | Loss: 0.00002503
Iteration 71/1000 | Loss: 0.00002503
Iteration 72/1000 | Loss: 0.00002503
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00007684
Iteration 75/1000 | Loss: 0.00011876
Iteration 76/1000 | Loss: 0.00015084
Iteration 77/1000 | Loss: 0.00002508
Iteration 78/1000 | Loss: 0.00002895
Iteration 79/1000 | Loss: 0.00002532
Iteration 80/1000 | Loss: 0.00002462
Iteration 81/1000 | Loss: 0.00002462
Iteration 82/1000 | Loss: 0.00002516
Iteration 83/1000 | Loss: 0.00002468
Iteration 84/1000 | Loss: 0.00002457
Iteration 85/1000 | Loss: 0.00002457
Iteration 86/1000 | Loss: 0.00002457
Iteration 87/1000 | Loss: 0.00002457
Iteration 88/1000 | Loss: 0.00002457
Iteration 89/1000 | Loss: 0.00002457
Iteration 90/1000 | Loss: 0.00002457
Iteration 91/1000 | Loss: 0.00002457
Iteration 92/1000 | Loss: 0.00002457
Iteration 93/1000 | Loss: 0.00002457
Iteration 94/1000 | Loss: 0.00002457
Iteration 95/1000 | Loss: 0.00002457
Iteration 96/1000 | Loss: 0.00002457
Iteration 97/1000 | Loss: 0.00002456
Iteration 98/1000 | Loss: 0.00002456
Iteration 99/1000 | Loss: 0.00002456
Iteration 100/1000 | Loss: 0.00002455
Iteration 101/1000 | Loss: 0.00002455
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002454
Iteration 105/1000 | Loss: 0.00002453
Iteration 106/1000 | Loss: 0.00002453
Iteration 107/1000 | Loss: 0.00002453
Iteration 108/1000 | Loss: 0.00002452
Iteration 109/1000 | Loss: 0.00002452
Iteration 110/1000 | Loss: 0.00002448
Iteration 111/1000 | Loss: 0.00007501
Iteration 112/1000 | Loss: 0.00010357
Iteration 113/1000 | Loss: 0.00002467
Iteration 114/1000 | Loss: 0.00002470
Iteration 115/1000 | Loss: 0.00002438
Iteration 116/1000 | Loss: 0.00002438
Iteration 117/1000 | Loss: 0.00002438
Iteration 118/1000 | Loss: 0.00002438
Iteration 119/1000 | Loss: 0.00002438
Iteration 120/1000 | Loss: 0.00002437
Iteration 121/1000 | Loss: 0.00002437
Iteration 122/1000 | Loss: 0.00002437
Iteration 123/1000 | Loss: 0.00002437
Iteration 124/1000 | Loss: 0.00002437
Iteration 125/1000 | Loss: 0.00002437
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002437
Iteration 128/1000 | Loss: 0.00002437
Iteration 129/1000 | Loss: 0.00002437
Iteration 130/1000 | Loss: 0.00002437
Iteration 131/1000 | Loss: 0.00002437
Iteration 132/1000 | Loss: 0.00002437
Iteration 133/1000 | Loss: 0.00002437
Iteration 134/1000 | Loss: 0.00002437
Iteration 135/1000 | Loss: 0.00002437
Iteration 136/1000 | Loss: 0.00002436
Iteration 137/1000 | Loss: 0.00002436
Iteration 138/1000 | Loss: 0.00002436
Iteration 139/1000 | Loss: 0.00002436
Iteration 140/1000 | Loss: 0.00002435
Iteration 141/1000 | Loss: 0.00002434
Iteration 142/1000 | Loss: 0.00002433
Iteration 143/1000 | Loss: 0.00002433
Iteration 144/1000 | Loss: 0.00002433
Iteration 145/1000 | Loss: 0.00002433
Iteration 146/1000 | Loss: 0.00002433
Iteration 147/1000 | Loss: 0.00002432
Iteration 148/1000 | Loss: 0.00002432
Iteration 149/1000 | Loss: 0.00002432
Iteration 150/1000 | Loss: 0.00002432
Iteration 151/1000 | Loss: 0.00002431
Iteration 152/1000 | Loss: 0.00002431
Iteration 153/1000 | Loss: 0.00002431
Iteration 154/1000 | Loss: 0.00002431
Iteration 155/1000 | Loss: 0.00002430
Iteration 156/1000 | Loss: 0.00002430
Iteration 157/1000 | Loss: 0.00002430
Iteration 158/1000 | Loss: 0.00002429
Iteration 159/1000 | Loss: 0.00002429
Iteration 160/1000 | Loss: 0.00002429
Iteration 161/1000 | Loss: 0.00002429
Iteration 162/1000 | Loss: 0.00002429
Iteration 163/1000 | Loss: 0.00002429
Iteration 164/1000 | Loss: 0.00002429
Iteration 165/1000 | Loss: 0.00002428
Iteration 166/1000 | Loss: 0.00002428
Iteration 167/1000 | Loss: 0.00002428
Iteration 168/1000 | Loss: 0.00002428
Iteration 169/1000 | Loss: 0.00002428
Iteration 170/1000 | Loss: 0.00002428
Iteration 171/1000 | Loss: 0.00002428
Iteration 172/1000 | Loss: 0.00002428
Iteration 173/1000 | Loss: 0.00002427
Iteration 174/1000 | Loss: 0.00002427
Iteration 175/1000 | Loss: 0.00002427
Iteration 176/1000 | Loss: 0.00002427
Iteration 177/1000 | Loss: 0.00002426
Iteration 178/1000 | Loss: 0.00002426
Iteration 179/1000 | Loss: 0.00002426
Iteration 180/1000 | Loss: 0.00002425
Iteration 181/1000 | Loss: 0.00002425
Iteration 182/1000 | Loss: 0.00002425
Iteration 183/1000 | Loss: 0.00002424
Iteration 184/1000 | Loss: 0.00002424
Iteration 185/1000 | Loss: 0.00002424
Iteration 186/1000 | Loss: 0.00002424
Iteration 187/1000 | Loss: 0.00002424
Iteration 188/1000 | Loss: 0.00002423
Iteration 189/1000 | Loss: 0.00002423
Iteration 190/1000 | Loss: 0.00002423
Iteration 191/1000 | Loss: 0.00002423
Iteration 192/1000 | Loss: 0.00002423
Iteration 193/1000 | Loss: 0.00002423
Iteration 194/1000 | Loss: 0.00002423
Iteration 195/1000 | Loss: 0.00002422
Iteration 196/1000 | Loss: 0.00002422
Iteration 197/1000 | Loss: 0.00002422
Iteration 198/1000 | Loss: 0.00002422
Iteration 199/1000 | Loss: 0.00002422
Iteration 200/1000 | Loss: 0.00002422
Iteration 201/1000 | Loss: 0.00002422
Iteration 202/1000 | Loss: 0.00002422
Iteration 203/1000 | Loss: 0.00002422
Iteration 204/1000 | Loss: 0.00002422
Iteration 205/1000 | Loss: 0.00002421
Iteration 206/1000 | Loss: 0.00002421
Iteration 207/1000 | Loss: 0.00002421
Iteration 208/1000 | Loss: 0.00002421
Iteration 209/1000 | Loss: 0.00002421
Iteration 210/1000 | Loss: 0.00002421
Iteration 211/1000 | Loss: 0.00002421
Iteration 212/1000 | Loss: 0.00002421
Iteration 213/1000 | Loss: 0.00002421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.4214126824517734e-05, 2.4214126824517734e-05, 2.4214126824517734e-05, 2.4214126824517734e-05, 2.4214126824517734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4214126824517734e-05

Optimization complete. Final v2v error: 4.2896270751953125 mm

Highest mean error: 5.251123905181885 mm for frame 99

Lowest mean error: 3.798722505569458 mm for frame 118

Saving results

Total time: 135.38540959358215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883950
Iteration 2/25 | Loss: 0.00165469
Iteration 3/25 | Loss: 0.00154661
Iteration 4/25 | Loss: 0.00152945
Iteration 5/25 | Loss: 0.00152393
Iteration 6/25 | Loss: 0.00152267
Iteration 7/25 | Loss: 0.00152267
Iteration 8/25 | Loss: 0.00152267
Iteration 9/25 | Loss: 0.00152267
Iteration 10/25 | Loss: 0.00152267
Iteration 11/25 | Loss: 0.00152267
Iteration 12/25 | Loss: 0.00152267
Iteration 13/25 | Loss: 0.00152267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015226711984723806, 0.0015226711984723806, 0.0015226711984723806, 0.0015226711984723806, 0.0015226711984723806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015226711984723806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25923097
Iteration 2/25 | Loss: 0.00224780
Iteration 3/25 | Loss: 0.00224778
Iteration 4/25 | Loss: 0.00224778
Iteration 5/25 | Loss: 0.00224778
Iteration 6/25 | Loss: 0.00224778
Iteration 7/25 | Loss: 0.00224778
Iteration 8/25 | Loss: 0.00224778
Iteration 9/25 | Loss: 0.00224778
Iteration 10/25 | Loss: 0.00224778
Iteration 11/25 | Loss: 0.00224778
Iteration 12/25 | Loss: 0.00224778
Iteration 13/25 | Loss: 0.00224778
Iteration 14/25 | Loss: 0.00224778
Iteration 15/25 | Loss: 0.00224778
Iteration 16/25 | Loss: 0.00224778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002247776370495558, 0.002247776370495558, 0.002247776370495558, 0.002247776370495558, 0.002247776370495558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002247776370495558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224778
Iteration 2/1000 | Loss: 0.00004838
Iteration 3/1000 | Loss: 0.00003711
Iteration 4/1000 | Loss: 0.00003089
Iteration 5/1000 | Loss: 0.00002891
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00002640
Iteration 8/1000 | Loss: 0.00002553
Iteration 9/1000 | Loss: 0.00002480
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00002409
Iteration 12/1000 | Loss: 0.00002388
Iteration 13/1000 | Loss: 0.00002362
Iteration 14/1000 | Loss: 0.00002352
Iteration 15/1000 | Loss: 0.00002345
Iteration 16/1000 | Loss: 0.00002328
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002320
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002317
Iteration 21/1000 | Loss: 0.00002315
Iteration 22/1000 | Loss: 0.00002314
Iteration 23/1000 | Loss: 0.00002313
Iteration 24/1000 | Loss: 0.00002312
Iteration 25/1000 | Loss: 0.00002312
Iteration 26/1000 | Loss: 0.00002311
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002306
Iteration 29/1000 | Loss: 0.00002301
Iteration 30/1000 | Loss: 0.00002298
Iteration 31/1000 | Loss: 0.00002292
Iteration 32/1000 | Loss: 0.00002291
Iteration 33/1000 | Loss: 0.00002290
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002280
Iteration 39/1000 | Loss: 0.00002280
Iteration 40/1000 | Loss: 0.00002279
Iteration 41/1000 | Loss: 0.00002276
Iteration 42/1000 | Loss: 0.00002272
Iteration 43/1000 | Loss: 0.00002272
Iteration 44/1000 | Loss: 0.00002272
Iteration 45/1000 | Loss: 0.00002271
Iteration 46/1000 | Loss: 0.00002271
Iteration 47/1000 | Loss: 0.00002271
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00002271
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002270
Iteration 53/1000 | Loss: 0.00002270
Iteration 54/1000 | Loss: 0.00002270
Iteration 55/1000 | Loss: 0.00002269
Iteration 56/1000 | Loss: 0.00002268
Iteration 57/1000 | Loss: 0.00002268
Iteration 58/1000 | Loss: 0.00002267
Iteration 59/1000 | Loss: 0.00002265
Iteration 60/1000 | Loss: 0.00002264
Iteration 61/1000 | Loss: 0.00002264
Iteration 62/1000 | Loss: 0.00002263
Iteration 63/1000 | Loss: 0.00002263
Iteration 64/1000 | Loss: 0.00002262
Iteration 65/1000 | Loss: 0.00002261
Iteration 66/1000 | Loss: 0.00002261
Iteration 67/1000 | Loss: 0.00002261
Iteration 68/1000 | Loss: 0.00002261
Iteration 69/1000 | Loss: 0.00002261
Iteration 70/1000 | Loss: 0.00002260
Iteration 71/1000 | Loss: 0.00002260
Iteration 72/1000 | Loss: 0.00002260
Iteration 73/1000 | Loss: 0.00002260
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002260
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002260
Iteration 78/1000 | Loss: 0.00002260
Iteration 79/1000 | Loss: 0.00002259
Iteration 80/1000 | Loss: 0.00002259
Iteration 81/1000 | Loss: 0.00002259
Iteration 82/1000 | Loss: 0.00002259
Iteration 83/1000 | Loss: 0.00002259
Iteration 84/1000 | Loss: 0.00002259
Iteration 85/1000 | Loss: 0.00002259
Iteration 86/1000 | Loss: 0.00002259
Iteration 87/1000 | Loss: 0.00002259
Iteration 88/1000 | Loss: 0.00002259
Iteration 89/1000 | Loss: 0.00002259
Iteration 90/1000 | Loss: 0.00002258
Iteration 91/1000 | Loss: 0.00002258
Iteration 92/1000 | Loss: 0.00002258
Iteration 93/1000 | Loss: 0.00002256
Iteration 94/1000 | Loss: 0.00002256
Iteration 95/1000 | Loss: 0.00002256
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002256
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002256
Iteration 103/1000 | Loss: 0.00002256
Iteration 104/1000 | Loss: 0.00002256
Iteration 105/1000 | Loss: 0.00002256
Iteration 106/1000 | Loss: 0.00002255
Iteration 107/1000 | Loss: 0.00002255
Iteration 108/1000 | Loss: 0.00002255
Iteration 109/1000 | Loss: 0.00002254
Iteration 110/1000 | Loss: 0.00002254
Iteration 111/1000 | Loss: 0.00002254
Iteration 112/1000 | Loss: 0.00002254
Iteration 113/1000 | Loss: 0.00002254
Iteration 114/1000 | Loss: 0.00002254
Iteration 115/1000 | Loss: 0.00002253
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Iteration 118/1000 | Loss: 0.00002253
Iteration 119/1000 | Loss: 0.00002253
Iteration 120/1000 | Loss: 0.00002253
Iteration 121/1000 | Loss: 0.00002253
Iteration 122/1000 | Loss: 0.00002253
Iteration 123/1000 | Loss: 0.00002253
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002251
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002250
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002249
Iteration 144/1000 | Loss: 0.00002249
Iteration 145/1000 | Loss: 0.00002249
Iteration 146/1000 | Loss: 0.00002249
Iteration 147/1000 | Loss: 0.00002249
Iteration 148/1000 | Loss: 0.00002249
Iteration 149/1000 | Loss: 0.00002249
Iteration 150/1000 | Loss: 0.00002249
Iteration 151/1000 | Loss: 0.00002249
Iteration 152/1000 | Loss: 0.00002248
Iteration 153/1000 | Loss: 0.00002248
Iteration 154/1000 | Loss: 0.00002248
Iteration 155/1000 | Loss: 0.00002248
Iteration 156/1000 | Loss: 0.00002248
Iteration 157/1000 | Loss: 0.00002248
Iteration 158/1000 | Loss: 0.00002248
Iteration 159/1000 | Loss: 0.00002248
Iteration 160/1000 | Loss: 0.00002248
Iteration 161/1000 | Loss: 0.00002248
Iteration 162/1000 | Loss: 0.00002248
Iteration 163/1000 | Loss: 0.00002248
Iteration 164/1000 | Loss: 0.00002248
Iteration 165/1000 | Loss: 0.00002248
Iteration 166/1000 | Loss: 0.00002248
Iteration 167/1000 | Loss: 0.00002248
Iteration 168/1000 | Loss: 0.00002248
Iteration 169/1000 | Loss: 0.00002248
Iteration 170/1000 | Loss: 0.00002248
Iteration 171/1000 | Loss: 0.00002248
Iteration 172/1000 | Loss: 0.00002248
Iteration 173/1000 | Loss: 0.00002248
Iteration 174/1000 | Loss: 0.00002248
Iteration 175/1000 | Loss: 0.00002248
Iteration 176/1000 | Loss: 0.00002248
Iteration 177/1000 | Loss: 0.00002248
Iteration 178/1000 | Loss: 0.00002248
Iteration 179/1000 | Loss: 0.00002248
Iteration 180/1000 | Loss: 0.00002248
Iteration 181/1000 | Loss: 0.00002248
Iteration 182/1000 | Loss: 0.00002248
Iteration 183/1000 | Loss: 0.00002248
Iteration 184/1000 | Loss: 0.00002248
Iteration 185/1000 | Loss: 0.00002248
Iteration 186/1000 | Loss: 0.00002248
Iteration 187/1000 | Loss: 0.00002248
Iteration 188/1000 | Loss: 0.00002248
Iteration 189/1000 | Loss: 0.00002248
Iteration 190/1000 | Loss: 0.00002248
Iteration 191/1000 | Loss: 0.00002248
Iteration 192/1000 | Loss: 0.00002248
Iteration 193/1000 | Loss: 0.00002248
Iteration 194/1000 | Loss: 0.00002248
Iteration 195/1000 | Loss: 0.00002248
Iteration 196/1000 | Loss: 0.00002248
Iteration 197/1000 | Loss: 0.00002248
Iteration 198/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.247640077257529e-05, 2.247640077257529e-05, 2.247640077257529e-05, 2.247640077257529e-05, 2.247640077257529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.247640077257529e-05

Optimization complete. Final v2v error: 4.026833534240723 mm

Highest mean error: 5.553643703460693 mm for frame 67

Lowest mean error: 3.4498801231384277 mm for frame 44

Saving results

Total time: 45.92812252044678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828971
Iteration 2/25 | Loss: 0.00168916
Iteration 3/25 | Loss: 0.00152075
Iteration 4/25 | Loss: 0.00150111
Iteration 5/25 | Loss: 0.00149795
Iteration 6/25 | Loss: 0.00149750
Iteration 7/25 | Loss: 0.00149750
Iteration 8/25 | Loss: 0.00149750
Iteration 9/25 | Loss: 0.00149750
Iteration 10/25 | Loss: 0.00149750
Iteration 11/25 | Loss: 0.00149750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001497502438724041, 0.001497502438724041, 0.001497502438724041, 0.001497502438724041, 0.001497502438724041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001497502438724041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83532721
Iteration 2/25 | Loss: 0.00159848
Iteration 3/25 | Loss: 0.00159848
Iteration 4/25 | Loss: 0.00159848
Iteration 5/25 | Loss: 0.00159848
Iteration 6/25 | Loss: 0.00159848
Iteration 7/25 | Loss: 0.00159847
Iteration 8/25 | Loss: 0.00159847
Iteration 9/25 | Loss: 0.00159847
Iteration 10/25 | Loss: 0.00159847
Iteration 11/25 | Loss: 0.00159847
Iteration 12/25 | Loss: 0.00159847
Iteration 13/25 | Loss: 0.00159847
Iteration 14/25 | Loss: 0.00159847
Iteration 15/25 | Loss: 0.00159847
Iteration 16/25 | Loss: 0.00159847
Iteration 17/25 | Loss: 0.00159847
Iteration 18/25 | Loss: 0.00159847
Iteration 19/25 | Loss: 0.00159847
Iteration 20/25 | Loss: 0.00159847
Iteration 21/25 | Loss: 0.00159847
Iteration 22/25 | Loss: 0.00159847
Iteration 23/25 | Loss: 0.00159847
Iteration 24/25 | Loss: 0.00159847
Iteration 25/25 | Loss: 0.00159847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159847
Iteration 2/1000 | Loss: 0.00004203
Iteration 3/1000 | Loss: 0.00003157
Iteration 4/1000 | Loss: 0.00002846
Iteration 5/1000 | Loss: 0.00002722
Iteration 6/1000 | Loss: 0.00002603
Iteration 7/1000 | Loss: 0.00002507
Iteration 8/1000 | Loss: 0.00002446
Iteration 9/1000 | Loss: 0.00002378
Iteration 10/1000 | Loss: 0.00002319
Iteration 11/1000 | Loss: 0.00002284
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002241
Iteration 14/1000 | Loss: 0.00002231
Iteration 15/1000 | Loss: 0.00002219
Iteration 16/1000 | Loss: 0.00002218
Iteration 17/1000 | Loss: 0.00002216
Iteration 18/1000 | Loss: 0.00002213
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002207
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002197
Iteration 24/1000 | Loss: 0.00002197
Iteration 25/1000 | Loss: 0.00002197
Iteration 26/1000 | Loss: 0.00002196
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002195
Iteration 29/1000 | Loss: 0.00002194
Iteration 30/1000 | Loss: 0.00002193
Iteration 31/1000 | Loss: 0.00002193
Iteration 32/1000 | Loss: 0.00002192
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00002186
Iteration 37/1000 | Loss: 0.00002185
Iteration 38/1000 | Loss: 0.00002185
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002185
Iteration 41/1000 | Loss: 0.00002185
Iteration 42/1000 | Loss: 0.00002184
Iteration 43/1000 | Loss: 0.00002184
Iteration 44/1000 | Loss: 0.00002184
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002184
Iteration 48/1000 | Loss: 0.00002184
Iteration 49/1000 | Loss: 0.00002184
Iteration 50/1000 | Loss: 0.00002183
Iteration 51/1000 | Loss: 0.00002183
Iteration 52/1000 | Loss: 0.00002183
Iteration 53/1000 | Loss: 0.00002183
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002182
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002181
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002177
Iteration 74/1000 | Loss: 0.00002177
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002175
Iteration 85/1000 | Loss: 0.00002175
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002175
Iteration 89/1000 | Loss: 0.00002175
Iteration 90/1000 | Loss: 0.00002175
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002174
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002174
Iteration 95/1000 | Loss: 0.00002174
Iteration 96/1000 | Loss: 0.00002174
Iteration 97/1000 | Loss: 0.00002174
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002173
Iteration 100/1000 | Loss: 0.00002173
Iteration 101/1000 | Loss: 0.00002173
Iteration 102/1000 | Loss: 0.00002173
Iteration 103/1000 | Loss: 0.00002173
Iteration 104/1000 | Loss: 0.00002173
Iteration 105/1000 | Loss: 0.00002173
Iteration 106/1000 | Loss: 0.00002173
Iteration 107/1000 | Loss: 0.00002173
Iteration 108/1000 | Loss: 0.00002173
Iteration 109/1000 | Loss: 0.00002173
Iteration 110/1000 | Loss: 0.00002173
Iteration 111/1000 | Loss: 0.00002173
Iteration 112/1000 | Loss: 0.00002173
Iteration 113/1000 | Loss: 0.00002173
Iteration 114/1000 | Loss: 0.00002173
Iteration 115/1000 | Loss: 0.00002173
Iteration 116/1000 | Loss: 0.00002173
Iteration 117/1000 | Loss: 0.00002173
Iteration 118/1000 | Loss: 0.00002173
Iteration 119/1000 | Loss: 0.00002173
Iteration 120/1000 | Loss: 0.00002173
Iteration 121/1000 | Loss: 0.00002173
Iteration 122/1000 | Loss: 0.00002173
Iteration 123/1000 | Loss: 0.00002173
Iteration 124/1000 | Loss: 0.00002173
Iteration 125/1000 | Loss: 0.00002173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.1727788407588378e-05, 2.1727788407588378e-05, 2.1727788407588378e-05, 2.1727788407588378e-05, 2.1727788407588378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1727788407588378e-05

Optimization complete. Final v2v error: 4.007153034210205 mm

Highest mean error: 4.150679111480713 mm for frame 41

Lowest mean error: 3.9019932746887207 mm for frame 66

Saving results

Total time: 38.57761073112488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846105
Iteration 2/25 | Loss: 0.00181004
Iteration 3/25 | Loss: 0.00158777
Iteration 4/25 | Loss: 0.00156433
Iteration 5/25 | Loss: 0.00155850
Iteration 6/25 | Loss: 0.00155850
Iteration 7/25 | Loss: 0.00155850
Iteration 8/25 | Loss: 0.00155850
Iteration 9/25 | Loss: 0.00155850
Iteration 10/25 | Loss: 0.00155850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015584987122565508, 0.0015584987122565508, 0.0015584987122565508, 0.0015584987122565508, 0.0015584987122565508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015584987122565508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.25068760
Iteration 2/25 | Loss: 0.00246920
Iteration 3/25 | Loss: 0.00246904
Iteration 4/25 | Loss: 0.00246904
Iteration 5/25 | Loss: 0.00246904
Iteration 6/25 | Loss: 0.00246904
Iteration 7/25 | Loss: 0.00246904
Iteration 8/25 | Loss: 0.00246904
Iteration 9/25 | Loss: 0.00246904
Iteration 10/25 | Loss: 0.00246904
Iteration 11/25 | Loss: 0.00246904
Iteration 12/25 | Loss: 0.00246904
Iteration 13/25 | Loss: 0.00246904
Iteration 14/25 | Loss: 0.00246904
Iteration 15/25 | Loss: 0.00246904
Iteration 16/25 | Loss: 0.00246904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024690397549420595, 0.0024690397549420595, 0.0024690397549420595, 0.0024690397549420595, 0.0024690397549420595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024690397549420595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246904
Iteration 2/1000 | Loss: 0.00004177
Iteration 3/1000 | Loss: 0.00002446
Iteration 4/1000 | Loss: 0.00002176
Iteration 5/1000 | Loss: 0.00002071
Iteration 6/1000 | Loss: 0.00001987
Iteration 7/1000 | Loss: 0.00001948
Iteration 8/1000 | Loss: 0.00001909
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001758
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001751
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001750
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001735
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00001726
Iteration 31/1000 | Loss: 0.00001726
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001725
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001707
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001693
Iteration 92/1000 | Loss: 0.00001693
Iteration 93/1000 | Loss: 0.00001693
Iteration 94/1000 | Loss: 0.00001693
Iteration 95/1000 | Loss: 0.00001693
Iteration 96/1000 | Loss: 0.00001692
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001692
Iteration 99/1000 | Loss: 0.00001692
Iteration 100/1000 | Loss: 0.00001692
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001691
Iteration 112/1000 | Loss: 0.00001691
Iteration 113/1000 | Loss: 0.00001691
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001689
Iteration 133/1000 | Loss: 0.00001689
Iteration 134/1000 | Loss: 0.00001689
Iteration 135/1000 | Loss: 0.00001689
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001688
Iteration 138/1000 | Loss: 0.00001688
Iteration 139/1000 | Loss: 0.00001688
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001688
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.6880885596037842e-05, 1.6880885596037842e-05, 1.6880885596037842e-05, 1.6880885596037842e-05, 1.6880885596037842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6880885596037842e-05

Optimization complete. Final v2v error: 3.5234901905059814 mm

Highest mean error: 3.6779513359069824 mm for frame 20

Lowest mean error: 3.3665926456451416 mm for frame 207

Saving results

Total time: 44.129343032836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796928
Iteration 2/25 | Loss: 0.00177545
Iteration 3/25 | Loss: 0.00156811
Iteration 4/25 | Loss: 0.00153774
Iteration 5/25 | Loss: 0.00153035
Iteration 6/25 | Loss: 0.00152977
Iteration 7/25 | Loss: 0.00152649
Iteration 8/25 | Loss: 0.00152272
Iteration 9/25 | Loss: 0.00152481
Iteration 10/25 | Loss: 0.00152014
Iteration 11/25 | Loss: 0.00151934
Iteration 12/25 | Loss: 0.00152116
Iteration 13/25 | Loss: 0.00151960
Iteration 14/25 | Loss: 0.00151927
Iteration 15/25 | Loss: 0.00151831
Iteration 16/25 | Loss: 0.00152243
Iteration 17/25 | Loss: 0.00152187
Iteration 18/25 | Loss: 0.00152079
Iteration 19/25 | Loss: 0.00151986
Iteration 20/25 | Loss: 0.00152246
Iteration 21/25 | Loss: 0.00151966
Iteration 22/25 | Loss: 0.00152249
Iteration 23/25 | Loss: 0.00151980
Iteration 24/25 | Loss: 0.00151952
Iteration 25/25 | Loss: 0.00152017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23579538
Iteration 2/25 | Loss: 0.00260296
Iteration 3/25 | Loss: 0.00260296
Iteration 4/25 | Loss: 0.00260296
Iteration 5/25 | Loss: 0.00260296
Iteration 6/25 | Loss: 0.00260296
Iteration 7/25 | Loss: 0.00260296
Iteration 8/25 | Loss: 0.00260296
Iteration 9/25 | Loss: 0.00260296
Iteration 10/25 | Loss: 0.00260296
Iteration 11/25 | Loss: 0.00260296
Iteration 12/25 | Loss: 0.00260296
Iteration 13/25 | Loss: 0.00260296
Iteration 14/25 | Loss: 0.00260296
Iteration 15/25 | Loss: 0.00260296
Iteration 16/25 | Loss: 0.00260296
Iteration 17/25 | Loss: 0.00260296
Iteration 18/25 | Loss: 0.00260296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0026029557920992374, 0.0026029557920992374, 0.0026029557920992374, 0.0026029557920992374, 0.0026029557920992374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026029557920992374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260296
Iteration 2/1000 | Loss: 0.00010771
Iteration 3/1000 | Loss: 0.00013454
Iteration 4/1000 | Loss: 0.00009408
Iteration 5/1000 | Loss: 0.00008959
Iteration 6/1000 | Loss: 0.00019365
Iteration 7/1000 | Loss: 0.00016109
Iteration 8/1000 | Loss: 0.00012822
Iteration 9/1000 | Loss: 0.00015875
Iteration 10/1000 | Loss: 0.00015291
Iteration 11/1000 | Loss: 0.00013228
Iteration 12/1000 | Loss: 0.00015196
Iteration 13/1000 | Loss: 0.00014787
Iteration 14/1000 | Loss: 0.00014373
Iteration 15/1000 | Loss: 0.00015155
Iteration 16/1000 | Loss: 0.00020307
Iteration 17/1000 | Loss: 0.00029860
Iteration 18/1000 | Loss: 0.00022529
Iteration 19/1000 | Loss: 0.00017312
Iteration 20/1000 | Loss: 0.00016212
Iteration 21/1000 | Loss: 0.00017882
Iteration 22/1000 | Loss: 0.00016708
Iteration 23/1000 | Loss: 0.00016411
Iteration 24/1000 | Loss: 0.00017082
Iteration 25/1000 | Loss: 0.00015252
Iteration 26/1000 | Loss: 0.00010936
Iteration 27/1000 | Loss: 0.00012292
Iteration 28/1000 | Loss: 0.00016527
Iteration 29/1000 | Loss: 0.00016745
Iteration 30/1000 | Loss: 0.00017546
Iteration 31/1000 | Loss: 0.00020086
Iteration 32/1000 | Loss: 0.00018815
Iteration 33/1000 | Loss: 0.00018987
Iteration 34/1000 | Loss: 0.00019099
Iteration 35/1000 | Loss: 0.00020550
Iteration 36/1000 | Loss: 0.00019789
Iteration 37/1000 | Loss: 0.00019235
Iteration 38/1000 | Loss: 0.00019160
Iteration 39/1000 | Loss: 0.00009385
Iteration 40/1000 | Loss: 0.00011473
Iteration 41/1000 | Loss: 0.00004968
Iteration 42/1000 | Loss: 0.00005011
Iteration 43/1000 | Loss: 0.00006225
Iteration 44/1000 | Loss: 0.00005774
Iteration 45/1000 | Loss: 0.00006234
Iteration 46/1000 | Loss: 0.00007975
Iteration 47/1000 | Loss: 0.00008051
Iteration 48/1000 | Loss: 0.00008884
Iteration 49/1000 | Loss: 0.00008125
Iteration 50/1000 | Loss: 0.00010875
Iteration 51/1000 | Loss: 0.00010977
Iteration 52/1000 | Loss: 0.00008777
Iteration 53/1000 | Loss: 0.00010149
Iteration 54/1000 | Loss: 0.00010308
Iteration 55/1000 | Loss: 0.00011760
Iteration 56/1000 | Loss: 0.00010572
Iteration 57/1000 | Loss: 0.00008538
Iteration 58/1000 | Loss: 0.00005752
Iteration 59/1000 | Loss: 0.00004255
Iteration 60/1000 | Loss: 0.00007814
Iteration 61/1000 | Loss: 0.00007303
Iteration 62/1000 | Loss: 0.00007105
Iteration 63/1000 | Loss: 0.00007342
Iteration 64/1000 | Loss: 0.00007776
Iteration 65/1000 | Loss: 0.00007070
Iteration 66/1000 | Loss: 0.00007014
Iteration 67/1000 | Loss: 0.00011162
Iteration 68/1000 | Loss: 0.00004962
Iteration 69/1000 | Loss: 0.00007451
Iteration 70/1000 | Loss: 0.00007704
Iteration 71/1000 | Loss: 0.00011383
Iteration 72/1000 | Loss: 0.00008765
Iteration 73/1000 | Loss: 0.00012966
Iteration 74/1000 | Loss: 0.00010289
Iteration 75/1000 | Loss: 0.00008912
Iteration 76/1000 | Loss: 0.00008542
Iteration 77/1000 | Loss: 0.00008644
Iteration 78/1000 | Loss: 0.00007696
Iteration 79/1000 | Loss: 0.00005826
Iteration 80/1000 | Loss: 0.00007036
Iteration 81/1000 | Loss: 0.00010923
Iteration 82/1000 | Loss: 0.00010766
Iteration 83/1000 | Loss: 0.00006548
Iteration 84/1000 | Loss: 0.00005365
Iteration 85/1000 | Loss: 0.00007041
Iteration 86/1000 | Loss: 0.00004760
Iteration 87/1000 | Loss: 0.00005167
Iteration 88/1000 | Loss: 0.00007848
Iteration 89/1000 | Loss: 0.00009521
Iteration 90/1000 | Loss: 0.00006129
Iteration 91/1000 | Loss: 0.00005520
Iteration 92/1000 | Loss: 0.00006252
Iteration 93/1000 | Loss: 0.00009054
Iteration 94/1000 | Loss: 0.00004877
Iteration 95/1000 | Loss: 0.00005678
Iteration 96/1000 | Loss: 0.00006824
Iteration 97/1000 | Loss: 0.00006943
Iteration 98/1000 | Loss: 0.00008648
Iteration 99/1000 | Loss: 0.00005317
Iteration 100/1000 | Loss: 0.00008338
Iteration 101/1000 | Loss: 0.00007198
Iteration 102/1000 | Loss: 0.00007183
Iteration 103/1000 | Loss: 0.00008022
Iteration 104/1000 | Loss: 0.00008627
Iteration 105/1000 | Loss: 0.00009038
Iteration 106/1000 | Loss: 0.00008239
Iteration 107/1000 | Loss: 0.00007944
Iteration 108/1000 | Loss: 0.00008060
Iteration 109/1000 | Loss: 0.00007945
Iteration 110/1000 | Loss: 0.00010022
Iteration 111/1000 | Loss: 0.00009740
Iteration 112/1000 | Loss: 0.00008545
Iteration 113/1000 | Loss: 0.00006588
Iteration 114/1000 | Loss: 0.00007189
Iteration 115/1000 | Loss: 0.00007978
Iteration 116/1000 | Loss: 0.00007790
Iteration 117/1000 | Loss: 0.00008766
Iteration 118/1000 | Loss: 0.00008012
Iteration 119/1000 | Loss: 0.00008991
Iteration 120/1000 | Loss: 0.00008040
Iteration 121/1000 | Loss: 0.00008187
Iteration 122/1000 | Loss: 0.00008448
Iteration 123/1000 | Loss: 0.00004933
Iteration 124/1000 | Loss: 0.00006920
Iteration 125/1000 | Loss: 0.00005693
Iteration 126/1000 | Loss: 0.00006877
Iteration 127/1000 | Loss: 0.00006082
Iteration 128/1000 | Loss: 0.00007535
Iteration 129/1000 | Loss: 0.00004269
Iteration 130/1000 | Loss: 0.00004671
Iteration 131/1000 | Loss: 0.00003910
Iteration 132/1000 | Loss: 0.00002177
Iteration 133/1000 | Loss: 0.00002052
Iteration 134/1000 | Loss: 0.00002888
Iteration 135/1000 | Loss: 0.00001942
Iteration 136/1000 | Loss: 0.00001894
Iteration 137/1000 | Loss: 0.00002850
Iteration 138/1000 | Loss: 0.00002301
Iteration 139/1000 | Loss: 0.00001870
Iteration 140/1000 | Loss: 0.00003718
Iteration 141/1000 | Loss: 0.00003008
Iteration 142/1000 | Loss: 0.00002756
Iteration 143/1000 | Loss: 0.00002483
Iteration 144/1000 | Loss: 0.00003003
Iteration 145/1000 | Loss: 0.00002466
Iteration 146/1000 | Loss: 0.00004035
Iteration 147/1000 | Loss: 0.00004095
Iteration 148/1000 | Loss: 0.00004357
Iteration 149/1000 | Loss: 0.00005007
Iteration 150/1000 | Loss: 0.00003762
Iteration 151/1000 | Loss: 0.00004414
Iteration 152/1000 | Loss: 0.00002812
Iteration 153/1000 | Loss: 0.00001640
Iteration 154/1000 | Loss: 0.00003691
Iteration 155/1000 | Loss: 0.00003393
Iteration 156/1000 | Loss: 0.00002523
Iteration 157/1000 | Loss: 0.00003117
Iteration 158/1000 | Loss: 0.00002956
Iteration 159/1000 | Loss: 0.00003957
Iteration 160/1000 | Loss: 0.00004427
Iteration 161/1000 | Loss: 0.00002214
Iteration 162/1000 | Loss: 0.00002212
Iteration 163/1000 | Loss: 0.00003541
Iteration 164/1000 | Loss: 0.00004518
Iteration 165/1000 | Loss: 0.00004860
Iteration 166/1000 | Loss: 0.00004618
Iteration 167/1000 | Loss: 0.00004856
Iteration 168/1000 | Loss: 0.00004885
Iteration 169/1000 | Loss: 0.00004150
Iteration 170/1000 | Loss: 0.00002251
Iteration 171/1000 | Loss: 0.00001847
Iteration 172/1000 | Loss: 0.00001709
Iteration 173/1000 | Loss: 0.00001648
Iteration 174/1000 | Loss: 0.00001610
Iteration 175/1000 | Loss: 0.00002791
Iteration 176/1000 | Loss: 0.00004052
Iteration 177/1000 | Loss: 0.00002321
Iteration 178/1000 | Loss: 0.00001995
Iteration 179/1000 | Loss: 0.00004598
Iteration 180/1000 | Loss: 0.00004186
Iteration 181/1000 | Loss: 0.00004435
Iteration 182/1000 | Loss: 0.00004106
Iteration 183/1000 | Loss: 0.00001852
Iteration 184/1000 | Loss: 0.00002548
Iteration 185/1000 | Loss: 0.00001748
Iteration 186/1000 | Loss: 0.00001628
Iteration 187/1000 | Loss: 0.00001592
Iteration 188/1000 | Loss: 0.00001576
Iteration 189/1000 | Loss: 0.00001573
Iteration 190/1000 | Loss: 0.00001558
Iteration 191/1000 | Loss: 0.00001553
Iteration 192/1000 | Loss: 0.00001535
Iteration 193/1000 | Loss: 0.00002101
Iteration 194/1000 | Loss: 0.00001676
Iteration 195/1000 | Loss: 0.00001601
Iteration 196/1000 | Loss: 0.00001528
Iteration 197/1000 | Loss: 0.00001478
Iteration 198/1000 | Loss: 0.00001442
Iteration 199/1000 | Loss: 0.00001404
Iteration 200/1000 | Loss: 0.00001379
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001359
Iteration 203/1000 | Loss: 0.00001356
Iteration 204/1000 | Loss: 0.00001355
Iteration 205/1000 | Loss: 0.00001352
Iteration 206/1000 | Loss: 0.00001350
Iteration 207/1000 | Loss: 0.00001346
Iteration 208/1000 | Loss: 0.00001344
Iteration 209/1000 | Loss: 0.00001343
Iteration 210/1000 | Loss: 0.00001342
Iteration 211/1000 | Loss: 0.00001341
Iteration 212/1000 | Loss: 0.00001341
Iteration 213/1000 | Loss: 0.00001340
Iteration 214/1000 | Loss: 0.00001339
Iteration 215/1000 | Loss: 0.00001339
Iteration 216/1000 | Loss: 0.00001339
Iteration 217/1000 | Loss: 0.00001338
Iteration 218/1000 | Loss: 0.00001337
Iteration 219/1000 | Loss: 0.00001337
Iteration 220/1000 | Loss: 0.00001337
Iteration 221/1000 | Loss: 0.00001337
Iteration 222/1000 | Loss: 0.00001337
Iteration 223/1000 | Loss: 0.00001336
Iteration 224/1000 | Loss: 0.00001336
Iteration 225/1000 | Loss: 0.00001336
Iteration 226/1000 | Loss: 0.00001336
Iteration 227/1000 | Loss: 0.00001336
Iteration 228/1000 | Loss: 0.00001336
Iteration 229/1000 | Loss: 0.00001336
Iteration 230/1000 | Loss: 0.00001336
Iteration 231/1000 | Loss: 0.00001335
Iteration 232/1000 | Loss: 0.00001335
Iteration 233/1000 | Loss: 0.00001333
Iteration 234/1000 | Loss: 0.00001332
Iteration 235/1000 | Loss: 0.00001332
Iteration 236/1000 | Loss: 0.00001331
Iteration 237/1000 | Loss: 0.00001331
Iteration 238/1000 | Loss: 0.00001331
Iteration 239/1000 | Loss: 0.00001331
Iteration 240/1000 | Loss: 0.00001331
Iteration 241/1000 | Loss: 0.00001331
Iteration 242/1000 | Loss: 0.00001331
Iteration 243/1000 | Loss: 0.00001330
Iteration 244/1000 | Loss: 0.00001330
Iteration 245/1000 | Loss: 0.00001330
Iteration 246/1000 | Loss: 0.00001330
Iteration 247/1000 | Loss: 0.00001330
Iteration 248/1000 | Loss: 0.00001330
Iteration 249/1000 | Loss: 0.00001330
Iteration 250/1000 | Loss: 0.00001330
Iteration 251/1000 | Loss: 0.00001330
Iteration 252/1000 | Loss: 0.00001330
Iteration 253/1000 | Loss: 0.00001330
Iteration 254/1000 | Loss: 0.00001330
Iteration 255/1000 | Loss: 0.00001330
Iteration 256/1000 | Loss: 0.00001330
Iteration 257/1000 | Loss: 0.00001330
Iteration 258/1000 | Loss: 0.00001330
Iteration 259/1000 | Loss: 0.00001330
Iteration 260/1000 | Loss: 0.00001329
Iteration 261/1000 | Loss: 0.00001329
Iteration 262/1000 | Loss: 0.00001329
Iteration 263/1000 | Loss: 0.00001329
Iteration 264/1000 | Loss: 0.00001329
Iteration 265/1000 | Loss: 0.00001329
Iteration 266/1000 | Loss: 0.00001329
Iteration 267/1000 | Loss: 0.00001329
Iteration 268/1000 | Loss: 0.00001329
Iteration 269/1000 | Loss: 0.00001329
Iteration 270/1000 | Loss: 0.00001328
Iteration 271/1000 | Loss: 0.00001328
Iteration 272/1000 | Loss: 0.00001328
Iteration 273/1000 | Loss: 0.00001328
Iteration 274/1000 | Loss: 0.00001328
Iteration 275/1000 | Loss: 0.00001328
Iteration 276/1000 | Loss: 0.00001328
Iteration 277/1000 | Loss: 0.00001328
Iteration 278/1000 | Loss: 0.00001328
Iteration 279/1000 | Loss: 0.00001328
Iteration 280/1000 | Loss: 0.00001328
Iteration 281/1000 | Loss: 0.00001328
Iteration 282/1000 | Loss: 0.00001328
Iteration 283/1000 | Loss: 0.00001328
Iteration 284/1000 | Loss: 0.00001328
Iteration 285/1000 | Loss: 0.00001327
Iteration 286/1000 | Loss: 0.00001327
Iteration 287/1000 | Loss: 0.00001327
Iteration 288/1000 | Loss: 0.00001327
Iteration 289/1000 | Loss: 0.00001327
Iteration 290/1000 | Loss: 0.00001327
Iteration 291/1000 | Loss: 0.00001327
Iteration 292/1000 | Loss: 0.00001327
Iteration 293/1000 | Loss: 0.00001327
Iteration 294/1000 | Loss: 0.00001326
Iteration 295/1000 | Loss: 0.00001326
Iteration 296/1000 | Loss: 0.00001326
Iteration 297/1000 | Loss: 0.00001326
Iteration 298/1000 | Loss: 0.00001326
Iteration 299/1000 | Loss: 0.00001326
Iteration 300/1000 | Loss: 0.00001326
Iteration 301/1000 | Loss: 0.00001326
Iteration 302/1000 | Loss: 0.00001326
Iteration 303/1000 | Loss: 0.00001326
Iteration 304/1000 | Loss: 0.00001326
Iteration 305/1000 | Loss: 0.00001326
Iteration 306/1000 | Loss: 0.00001326
Iteration 307/1000 | Loss: 0.00001326
Iteration 308/1000 | Loss: 0.00001326
Iteration 309/1000 | Loss: 0.00001326
Iteration 310/1000 | Loss: 0.00001325
Iteration 311/1000 | Loss: 0.00001325
Iteration 312/1000 | Loss: 0.00001325
Iteration 313/1000 | Loss: 0.00001325
Iteration 314/1000 | Loss: 0.00001325
Iteration 315/1000 | Loss: 0.00001325
Iteration 316/1000 | Loss: 0.00001325
Iteration 317/1000 | Loss: 0.00001325
Iteration 318/1000 | Loss: 0.00001325
Iteration 319/1000 | Loss: 0.00001325
Iteration 320/1000 | Loss: 0.00001325
Iteration 321/1000 | Loss: 0.00001325
Iteration 322/1000 | Loss: 0.00001325
Iteration 323/1000 | Loss: 0.00001325
Iteration 324/1000 | Loss: 0.00001325
Iteration 325/1000 | Loss: 0.00001325
Iteration 326/1000 | Loss: 0.00001325
Iteration 327/1000 | Loss: 0.00001325
Iteration 328/1000 | Loss: 0.00001325
Iteration 329/1000 | Loss: 0.00001325
Iteration 330/1000 | Loss: 0.00001325
Iteration 331/1000 | Loss: 0.00001325
Iteration 332/1000 | Loss: 0.00001324
Iteration 333/1000 | Loss: 0.00001324
Iteration 334/1000 | Loss: 0.00001324
Iteration 335/1000 | Loss: 0.00001324
Iteration 336/1000 | Loss: 0.00001324
Iteration 337/1000 | Loss: 0.00001324
Iteration 338/1000 | Loss: 0.00001324
Iteration 339/1000 | Loss: 0.00001324
Iteration 340/1000 | Loss: 0.00001324
Iteration 341/1000 | Loss: 0.00001324
Iteration 342/1000 | Loss: 0.00001324
Iteration 343/1000 | Loss: 0.00001324
Iteration 344/1000 | Loss: 0.00001324
Iteration 345/1000 | Loss: 0.00001324
Iteration 346/1000 | Loss: 0.00001324
Iteration 347/1000 | Loss: 0.00001324
Iteration 348/1000 | Loss: 0.00001324
Iteration 349/1000 | Loss: 0.00001324
Iteration 350/1000 | Loss: 0.00001323
Iteration 351/1000 | Loss: 0.00001323
Iteration 352/1000 | Loss: 0.00001323
Iteration 353/1000 | Loss: 0.00001323
Iteration 354/1000 | Loss: 0.00001323
Iteration 355/1000 | Loss: 0.00001323
Iteration 356/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 356. Stopping optimization.
Last 5 losses: [1.3234763173386455e-05, 1.3234763173386455e-05, 1.3234763173386455e-05, 1.3234763173386455e-05, 1.3234763173386455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3234763173386455e-05

Optimization complete. Final v2v error: 3.1548802852630615 mm

Highest mean error: 4.3588104248046875 mm for frame 61

Lowest mean error: 2.974348783493042 mm for frame 41

Saving results

Total time: 378.1713984012604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452648
Iteration 2/25 | Loss: 0.00159154
Iteration 3/25 | Loss: 0.00151051
Iteration 4/25 | Loss: 0.00149766
Iteration 5/25 | Loss: 0.00149365
Iteration 6/25 | Loss: 0.00149306
Iteration 7/25 | Loss: 0.00149306
Iteration 8/25 | Loss: 0.00149306
Iteration 9/25 | Loss: 0.00149306
Iteration 10/25 | Loss: 0.00149306
Iteration 11/25 | Loss: 0.00149306
Iteration 12/25 | Loss: 0.00149306
Iteration 13/25 | Loss: 0.00149306
Iteration 14/25 | Loss: 0.00149306
Iteration 15/25 | Loss: 0.00149306
Iteration 16/25 | Loss: 0.00149306
Iteration 17/25 | Loss: 0.00149306
Iteration 18/25 | Loss: 0.00149306
Iteration 19/25 | Loss: 0.00149306
Iteration 20/25 | Loss: 0.00149306
Iteration 21/25 | Loss: 0.00149306
Iteration 22/25 | Loss: 0.00149306
Iteration 23/25 | Loss: 0.00149306
Iteration 24/25 | Loss: 0.00149306
Iteration 25/25 | Loss: 0.00149306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23784912
Iteration 2/25 | Loss: 0.00243170
Iteration 3/25 | Loss: 0.00243170
Iteration 4/25 | Loss: 0.00243170
Iteration 5/25 | Loss: 0.00243170
Iteration 6/25 | Loss: 0.00243169
Iteration 7/25 | Loss: 0.00243169
Iteration 8/25 | Loss: 0.00243169
Iteration 9/25 | Loss: 0.00243169
Iteration 10/25 | Loss: 0.00243169
Iteration 11/25 | Loss: 0.00243169
Iteration 12/25 | Loss: 0.00243169
Iteration 13/25 | Loss: 0.00243169
Iteration 14/25 | Loss: 0.00243169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0024316939525306225, 0.0024316939525306225, 0.0024316939525306225, 0.0024316939525306225, 0.0024316939525306225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024316939525306225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243169
Iteration 2/1000 | Loss: 0.00003717
Iteration 3/1000 | Loss: 0.00002936
Iteration 4/1000 | Loss: 0.00002575
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002358
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002224
Iteration 9/1000 | Loss: 0.00002177
Iteration 10/1000 | Loss: 0.00002138
Iteration 11/1000 | Loss: 0.00002094
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002040
Iteration 14/1000 | Loss: 0.00002024
Iteration 15/1000 | Loss: 0.00002022
Iteration 16/1000 | Loss: 0.00002008
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001975
Iteration 20/1000 | Loss: 0.00001974
Iteration 21/1000 | Loss: 0.00001970
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001969
Iteration 24/1000 | Loss: 0.00001968
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001965
Iteration 28/1000 | Loss: 0.00001963
Iteration 29/1000 | Loss: 0.00001961
Iteration 30/1000 | Loss: 0.00001960
Iteration 31/1000 | Loss: 0.00001959
Iteration 32/1000 | Loss: 0.00001958
Iteration 33/1000 | Loss: 0.00001958
Iteration 34/1000 | Loss: 0.00001957
Iteration 35/1000 | Loss: 0.00001957
Iteration 36/1000 | Loss: 0.00001957
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001956
Iteration 39/1000 | Loss: 0.00001955
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001949
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001948
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001945
Iteration 68/1000 | Loss: 0.00001945
Iteration 69/1000 | Loss: 0.00001945
Iteration 70/1000 | Loss: 0.00001945
Iteration 71/1000 | Loss: 0.00001944
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001944
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00001943
Iteration 76/1000 | Loss: 0.00001943
Iteration 77/1000 | Loss: 0.00001942
Iteration 78/1000 | Loss: 0.00001942
Iteration 79/1000 | Loss: 0.00001942
Iteration 80/1000 | Loss: 0.00001942
Iteration 81/1000 | Loss: 0.00001942
Iteration 82/1000 | Loss: 0.00001942
Iteration 83/1000 | Loss: 0.00001942
Iteration 84/1000 | Loss: 0.00001941
Iteration 85/1000 | Loss: 0.00001941
Iteration 86/1000 | Loss: 0.00001941
Iteration 87/1000 | Loss: 0.00001941
Iteration 88/1000 | Loss: 0.00001941
Iteration 89/1000 | Loss: 0.00001941
Iteration 90/1000 | Loss: 0.00001941
Iteration 91/1000 | Loss: 0.00001941
Iteration 92/1000 | Loss: 0.00001940
Iteration 93/1000 | Loss: 0.00001940
Iteration 94/1000 | Loss: 0.00001940
Iteration 95/1000 | Loss: 0.00001940
Iteration 96/1000 | Loss: 0.00001940
Iteration 97/1000 | Loss: 0.00001940
Iteration 98/1000 | Loss: 0.00001940
Iteration 99/1000 | Loss: 0.00001940
Iteration 100/1000 | Loss: 0.00001940
Iteration 101/1000 | Loss: 0.00001940
Iteration 102/1000 | Loss: 0.00001940
Iteration 103/1000 | Loss: 0.00001940
Iteration 104/1000 | Loss: 0.00001939
Iteration 105/1000 | Loss: 0.00001939
Iteration 106/1000 | Loss: 0.00001939
Iteration 107/1000 | Loss: 0.00001939
Iteration 108/1000 | Loss: 0.00001939
Iteration 109/1000 | Loss: 0.00001938
Iteration 110/1000 | Loss: 0.00001938
Iteration 111/1000 | Loss: 0.00001938
Iteration 112/1000 | Loss: 0.00001938
Iteration 113/1000 | Loss: 0.00001938
Iteration 114/1000 | Loss: 0.00001938
Iteration 115/1000 | Loss: 0.00001938
Iteration 116/1000 | Loss: 0.00001938
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001937
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001937
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001936
Iteration 131/1000 | Loss: 0.00001936
Iteration 132/1000 | Loss: 0.00001936
Iteration 133/1000 | Loss: 0.00001936
Iteration 134/1000 | Loss: 0.00001936
Iteration 135/1000 | Loss: 0.00001936
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001936
Iteration 138/1000 | Loss: 0.00001936
Iteration 139/1000 | Loss: 0.00001936
Iteration 140/1000 | Loss: 0.00001936
Iteration 141/1000 | Loss: 0.00001936
Iteration 142/1000 | Loss: 0.00001936
Iteration 143/1000 | Loss: 0.00001936
Iteration 144/1000 | Loss: 0.00001936
Iteration 145/1000 | Loss: 0.00001936
Iteration 146/1000 | Loss: 0.00001936
Iteration 147/1000 | Loss: 0.00001936
Iteration 148/1000 | Loss: 0.00001936
Iteration 149/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.936201260832604e-05, 1.936201260832604e-05, 1.936201260832604e-05, 1.936201260832604e-05, 1.936201260832604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.936201260832604e-05

Optimization complete. Final v2v error: 3.8050389289855957 mm

Highest mean error: 4.375587463378906 mm for frame 29

Lowest mean error: 3.6646745204925537 mm for frame 3

Saving results

Total time: 40.921499252319336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400380
Iteration 2/25 | Loss: 0.00155952
Iteration 3/25 | Loss: 0.00148261
Iteration 4/25 | Loss: 0.00146723
Iteration 5/25 | Loss: 0.00146162
Iteration 6/25 | Loss: 0.00146080
Iteration 7/25 | Loss: 0.00146080
Iteration 8/25 | Loss: 0.00146080
Iteration 9/25 | Loss: 0.00146080
Iteration 10/25 | Loss: 0.00146080
Iteration 11/25 | Loss: 0.00146080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014608008787035942, 0.0014608008787035942, 0.0014608008787035942, 0.0014608008787035942, 0.0014608008787035942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014608008787035942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32997084
Iteration 2/25 | Loss: 0.00312644
Iteration 3/25 | Loss: 0.00312644
Iteration 4/25 | Loss: 0.00312644
Iteration 5/25 | Loss: 0.00312644
Iteration 6/25 | Loss: 0.00312644
Iteration 7/25 | Loss: 0.00312644
Iteration 8/25 | Loss: 0.00312644
Iteration 9/25 | Loss: 0.00312644
Iteration 10/25 | Loss: 0.00312644
Iteration 11/25 | Loss: 0.00312644
Iteration 12/25 | Loss: 0.00312644
Iteration 13/25 | Loss: 0.00312644
Iteration 14/25 | Loss: 0.00312644
Iteration 15/25 | Loss: 0.00312644
Iteration 16/25 | Loss: 0.00312644
Iteration 17/25 | Loss: 0.00312644
Iteration 18/25 | Loss: 0.00312644
Iteration 19/25 | Loss: 0.00312644
Iteration 20/25 | Loss: 0.00312644
Iteration 21/25 | Loss: 0.00312644
Iteration 22/25 | Loss: 0.00312643
Iteration 23/25 | Loss: 0.00312643
Iteration 24/25 | Loss: 0.00312643
Iteration 25/25 | Loss: 0.00312644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312643
Iteration 2/1000 | Loss: 0.00003367
Iteration 3/1000 | Loss: 0.00002548
Iteration 4/1000 | Loss: 0.00002267
Iteration 5/1000 | Loss: 0.00002146
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00001996
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001896
Iteration 13/1000 | Loss: 0.00001891
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001874
Iteration 17/1000 | Loss: 0.00001866
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001864
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001864
Iteration 22/1000 | Loss: 0.00001859
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001855
Iteration 26/1000 | Loss: 0.00001854
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001839
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001833
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001826
Iteration 56/1000 | Loss: 0.00001825
Iteration 57/1000 | Loss: 0.00001825
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001824
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001816
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001815
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001813
Iteration 82/1000 | Loss: 0.00001813
Iteration 83/1000 | Loss: 0.00001812
Iteration 84/1000 | Loss: 0.00001812
Iteration 85/1000 | Loss: 0.00001812
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00001811
Iteration 88/1000 | Loss: 0.00001811
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001811
Iteration 91/1000 | Loss: 0.00001811
Iteration 92/1000 | Loss: 0.00001811
Iteration 93/1000 | Loss: 0.00001810
Iteration 94/1000 | Loss: 0.00001810
Iteration 95/1000 | Loss: 0.00001810
Iteration 96/1000 | Loss: 0.00001810
Iteration 97/1000 | Loss: 0.00001810
Iteration 98/1000 | Loss: 0.00001810
Iteration 99/1000 | Loss: 0.00001810
Iteration 100/1000 | Loss: 0.00001809
Iteration 101/1000 | Loss: 0.00001809
Iteration 102/1000 | Loss: 0.00001809
Iteration 103/1000 | Loss: 0.00001809
Iteration 104/1000 | Loss: 0.00001808
Iteration 105/1000 | Loss: 0.00001808
Iteration 106/1000 | Loss: 0.00001808
Iteration 107/1000 | Loss: 0.00001808
Iteration 108/1000 | Loss: 0.00001807
Iteration 109/1000 | Loss: 0.00001807
Iteration 110/1000 | Loss: 0.00001807
Iteration 111/1000 | Loss: 0.00001807
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001807
Iteration 115/1000 | Loss: 0.00001806
Iteration 116/1000 | Loss: 0.00001806
Iteration 117/1000 | Loss: 0.00001806
Iteration 118/1000 | Loss: 0.00001806
Iteration 119/1000 | Loss: 0.00001806
Iteration 120/1000 | Loss: 0.00001806
Iteration 121/1000 | Loss: 0.00001806
Iteration 122/1000 | Loss: 0.00001806
Iteration 123/1000 | Loss: 0.00001806
Iteration 124/1000 | Loss: 0.00001806
Iteration 125/1000 | Loss: 0.00001806
Iteration 126/1000 | Loss: 0.00001806
Iteration 127/1000 | Loss: 0.00001806
Iteration 128/1000 | Loss: 0.00001806
Iteration 129/1000 | Loss: 0.00001805
Iteration 130/1000 | Loss: 0.00001805
Iteration 131/1000 | Loss: 0.00001805
Iteration 132/1000 | Loss: 0.00001805
Iteration 133/1000 | Loss: 0.00001805
Iteration 134/1000 | Loss: 0.00001805
Iteration 135/1000 | Loss: 0.00001805
Iteration 136/1000 | Loss: 0.00001805
Iteration 137/1000 | Loss: 0.00001805
Iteration 138/1000 | Loss: 0.00001805
Iteration 139/1000 | Loss: 0.00001805
Iteration 140/1000 | Loss: 0.00001805
Iteration 141/1000 | Loss: 0.00001805
Iteration 142/1000 | Loss: 0.00001805
Iteration 143/1000 | Loss: 0.00001805
Iteration 144/1000 | Loss: 0.00001805
Iteration 145/1000 | Loss: 0.00001804
Iteration 146/1000 | Loss: 0.00001804
Iteration 147/1000 | Loss: 0.00001804
Iteration 148/1000 | Loss: 0.00001804
Iteration 149/1000 | Loss: 0.00001804
Iteration 150/1000 | Loss: 0.00001804
Iteration 151/1000 | Loss: 0.00001804
Iteration 152/1000 | Loss: 0.00001804
Iteration 153/1000 | Loss: 0.00001804
Iteration 154/1000 | Loss: 0.00001804
Iteration 155/1000 | Loss: 0.00001804
Iteration 156/1000 | Loss: 0.00001804
Iteration 157/1000 | Loss: 0.00001804
Iteration 158/1000 | Loss: 0.00001804
Iteration 159/1000 | Loss: 0.00001804
Iteration 160/1000 | Loss: 0.00001804
Iteration 161/1000 | Loss: 0.00001804
Iteration 162/1000 | Loss: 0.00001804
Iteration 163/1000 | Loss: 0.00001804
Iteration 164/1000 | Loss: 0.00001804
Iteration 165/1000 | Loss: 0.00001804
Iteration 166/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.8041320799966343e-05, 1.8041320799966343e-05, 1.8041320799966343e-05, 1.8041320799966343e-05, 1.8041320799966343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8041320799966343e-05

Optimization complete. Final v2v error: 3.6505255699157715 mm

Highest mean error: 4.039280891418457 mm for frame 110

Lowest mean error: 3.0299766063690186 mm for frame 5

Saving results

Total time: 39.71585774421692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622314
Iteration 2/25 | Loss: 0.00170917
Iteration 3/25 | Loss: 0.00157345
Iteration 4/25 | Loss: 0.00154883
Iteration 5/25 | Loss: 0.00153421
Iteration 6/25 | Loss: 0.00153838
Iteration 7/25 | Loss: 0.00153430
Iteration 8/25 | Loss: 0.00152934
Iteration 9/25 | Loss: 0.00153608
Iteration 10/25 | Loss: 0.00153467
Iteration 11/25 | Loss: 0.00153404
Iteration 12/25 | Loss: 0.00153020
Iteration 13/25 | Loss: 0.00152991
Iteration 14/25 | Loss: 0.00152167
Iteration 15/25 | Loss: 0.00152086
Iteration 16/25 | Loss: 0.00152071
Iteration 17/25 | Loss: 0.00152071
Iteration 18/25 | Loss: 0.00152070
Iteration 19/25 | Loss: 0.00152070
Iteration 20/25 | Loss: 0.00152070
Iteration 21/25 | Loss: 0.00152070
Iteration 22/25 | Loss: 0.00152070
Iteration 23/25 | Loss: 0.00152070
Iteration 24/25 | Loss: 0.00152070
Iteration 25/25 | Loss: 0.00152070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53033531
Iteration 2/25 | Loss: 0.00383894
Iteration 3/25 | Loss: 0.00383893
Iteration 4/25 | Loss: 0.00383893
Iteration 5/25 | Loss: 0.00383893
Iteration 6/25 | Loss: 0.00383893
Iteration 7/25 | Loss: 0.00383893
Iteration 8/25 | Loss: 0.00383893
Iteration 9/25 | Loss: 0.00383893
Iteration 10/25 | Loss: 0.00383893
Iteration 11/25 | Loss: 0.00383893
Iteration 12/25 | Loss: 0.00383893
Iteration 13/25 | Loss: 0.00383893
Iteration 14/25 | Loss: 0.00383893
Iteration 15/25 | Loss: 0.00383893
Iteration 16/25 | Loss: 0.00383893
Iteration 17/25 | Loss: 0.00383893
Iteration 18/25 | Loss: 0.00383893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0038389270193874836, 0.0038389270193874836, 0.0038389270193874836, 0.0038389270193874836, 0.0038389270193874836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038389270193874836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00383893
Iteration 2/1000 | Loss: 0.00006781
Iteration 3/1000 | Loss: 0.00004455
Iteration 4/1000 | Loss: 0.00003681
Iteration 5/1000 | Loss: 0.00003399
Iteration 6/1000 | Loss: 0.00003205
Iteration 7/1000 | Loss: 0.00003070
Iteration 8/1000 | Loss: 0.00002987
Iteration 9/1000 | Loss: 0.00002902
Iteration 10/1000 | Loss: 0.00002851
Iteration 11/1000 | Loss: 0.00002809
Iteration 12/1000 | Loss: 0.00002772
Iteration 13/1000 | Loss: 0.00002740
Iteration 14/1000 | Loss: 0.00002720
Iteration 15/1000 | Loss: 0.00002697
Iteration 16/1000 | Loss: 0.00002669
Iteration 17/1000 | Loss: 0.00002649
Iteration 18/1000 | Loss: 0.00002631
Iteration 19/1000 | Loss: 0.00002624
Iteration 20/1000 | Loss: 0.00002611
Iteration 21/1000 | Loss: 0.00002605
Iteration 22/1000 | Loss: 0.00002601
Iteration 23/1000 | Loss: 0.00002601
Iteration 24/1000 | Loss: 0.00002600
Iteration 25/1000 | Loss: 0.00002599
Iteration 26/1000 | Loss: 0.00002590
Iteration 27/1000 | Loss: 0.00002588
Iteration 28/1000 | Loss: 0.00002587
Iteration 29/1000 | Loss: 0.00002587
Iteration 30/1000 | Loss: 0.00002586
Iteration 31/1000 | Loss: 0.00002585
Iteration 32/1000 | Loss: 0.00002585
Iteration 33/1000 | Loss: 0.00002583
Iteration 34/1000 | Loss: 0.00002581
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002577
Iteration 37/1000 | Loss: 0.00002576
Iteration 38/1000 | Loss: 0.00002571
Iteration 39/1000 | Loss: 0.00002571
Iteration 40/1000 | Loss: 0.00002568
Iteration 41/1000 | Loss: 0.00002568
Iteration 42/1000 | Loss: 0.00002568
Iteration 43/1000 | Loss: 0.00002567
Iteration 44/1000 | Loss: 0.00002567
Iteration 45/1000 | Loss: 0.00002567
Iteration 46/1000 | Loss: 0.00002567
Iteration 47/1000 | Loss: 0.00002567
Iteration 48/1000 | Loss: 0.00002566
Iteration 49/1000 | Loss: 0.00002566
Iteration 50/1000 | Loss: 0.00002566
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002564
Iteration 53/1000 | Loss: 0.00002564
Iteration 54/1000 | Loss: 0.00002563
Iteration 55/1000 | Loss: 0.00002563
Iteration 56/1000 | Loss: 0.00002563
Iteration 57/1000 | Loss: 0.00002562
Iteration 58/1000 | Loss: 0.00002562
Iteration 59/1000 | Loss: 0.00002561
Iteration 60/1000 | Loss: 0.00002560
Iteration 61/1000 | Loss: 0.00002560
Iteration 62/1000 | Loss: 0.00002560
Iteration 63/1000 | Loss: 0.00002559
Iteration 64/1000 | Loss: 0.00002559
Iteration 65/1000 | Loss: 0.00002559
Iteration 66/1000 | Loss: 0.00002559
Iteration 67/1000 | Loss: 0.00002559
Iteration 68/1000 | Loss: 0.00002559
Iteration 69/1000 | Loss: 0.00002559
Iteration 70/1000 | Loss: 0.00002558
Iteration 71/1000 | Loss: 0.00002557
Iteration 72/1000 | Loss: 0.00002557
Iteration 73/1000 | Loss: 0.00002557
Iteration 74/1000 | Loss: 0.00002557
Iteration 75/1000 | Loss: 0.00002557
Iteration 76/1000 | Loss: 0.00002557
Iteration 77/1000 | Loss: 0.00002556
Iteration 78/1000 | Loss: 0.00002556
Iteration 79/1000 | Loss: 0.00002556
Iteration 80/1000 | Loss: 0.00002556
Iteration 81/1000 | Loss: 0.00002555
Iteration 82/1000 | Loss: 0.00002555
Iteration 83/1000 | Loss: 0.00002554
Iteration 84/1000 | Loss: 0.00002554
Iteration 85/1000 | Loss: 0.00002554
Iteration 86/1000 | Loss: 0.00002554
Iteration 87/1000 | Loss: 0.00002554
Iteration 88/1000 | Loss: 0.00002554
Iteration 89/1000 | Loss: 0.00002554
Iteration 90/1000 | Loss: 0.00002554
Iteration 91/1000 | Loss: 0.00002553
Iteration 92/1000 | Loss: 0.00002553
Iteration 93/1000 | Loss: 0.00002553
Iteration 94/1000 | Loss: 0.00002553
Iteration 95/1000 | Loss: 0.00002553
Iteration 96/1000 | Loss: 0.00002553
Iteration 97/1000 | Loss: 0.00002553
Iteration 98/1000 | Loss: 0.00002553
Iteration 99/1000 | Loss: 0.00002553
Iteration 100/1000 | Loss: 0.00002553
Iteration 101/1000 | Loss: 0.00002552
Iteration 102/1000 | Loss: 0.00002552
Iteration 103/1000 | Loss: 0.00002552
Iteration 104/1000 | Loss: 0.00002552
Iteration 105/1000 | Loss: 0.00002552
Iteration 106/1000 | Loss: 0.00002552
Iteration 107/1000 | Loss: 0.00002552
Iteration 108/1000 | Loss: 0.00002552
Iteration 109/1000 | Loss: 0.00002552
Iteration 110/1000 | Loss: 0.00002552
Iteration 111/1000 | Loss: 0.00002552
Iteration 112/1000 | Loss: 0.00002551
Iteration 113/1000 | Loss: 0.00002551
Iteration 114/1000 | Loss: 0.00002551
Iteration 115/1000 | Loss: 0.00002551
Iteration 116/1000 | Loss: 0.00002551
Iteration 117/1000 | Loss: 0.00002551
Iteration 118/1000 | Loss: 0.00002551
Iteration 119/1000 | Loss: 0.00002551
Iteration 120/1000 | Loss: 0.00002551
Iteration 121/1000 | Loss: 0.00002551
Iteration 122/1000 | Loss: 0.00002551
Iteration 123/1000 | Loss: 0.00002551
Iteration 124/1000 | Loss: 0.00002551
Iteration 125/1000 | Loss: 0.00002551
Iteration 126/1000 | Loss: 0.00002551
Iteration 127/1000 | Loss: 0.00002551
Iteration 128/1000 | Loss: 0.00002551
Iteration 129/1000 | Loss: 0.00002551
Iteration 130/1000 | Loss: 0.00002551
Iteration 131/1000 | Loss: 0.00002551
Iteration 132/1000 | Loss: 0.00002551
Iteration 133/1000 | Loss: 0.00002551
Iteration 134/1000 | Loss: 0.00002551
Iteration 135/1000 | Loss: 0.00002551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.5514607841614634e-05, 2.5514607841614634e-05, 2.5514607841614634e-05, 2.5514607841614634e-05, 2.5514607841614634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5514607841614634e-05

Optimization complete. Final v2v error: 4.284823894500732 mm

Highest mean error: 5.126661777496338 mm for frame 59

Lowest mean error: 3.6098439693450928 mm for frame 81

Saving results

Total time: 66.31186246871948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eve_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eve_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742601
Iteration 2/25 | Loss: 0.00162953
Iteration 3/25 | Loss: 0.00154828
Iteration 4/25 | Loss: 0.00153322
Iteration 5/25 | Loss: 0.00152812
Iteration 6/25 | Loss: 0.00152738
Iteration 7/25 | Loss: 0.00152738
Iteration 8/25 | Loss: 0.00152738
Iteration 9/25 | Loss: 0.00152738
Iteration 10/25 | Loss: 0.00152738
Iteration 11/25 | Loss: 0.00152738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015273750759661198, 0.0015273750759661198, 0.0015273750759661198, 0.0015273750759661198, 0.0015273750759661198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015273750759661198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22330713
Iteration 2/25 | Loss: 0.00296785
Iteration 3/25 | Loss: 0.00296784
Iteration 4/25 | Loss: 0.00296784
Iteration 5/25 | Loss: 0.00296784
Iteration 6/25 | Loss: 0.00296784
Iteration 7/25 | Loss: 0.00296784
Iteration 8/25 | Loss: 0.00296784
Iteration 9/25 | Loss: 0.00296784
Iteration 10/25 | Loss: 0.00296784
Iteration 11/25 | Loss: 0.00296784
Iteration 12/25 | Loss: 0.00296784
Iteration 13/25 | Loss: 0.00296784
Iteration 14/25 | Loss: 0.00296784
Iteration 15/25 | Loss: 0.00296784
Iteration 16/25 | Loss: 0.00296784
Iteration 17/25 | Loss: 0.00296784
Iteration 18/25 | Loss: 0.00296784
Iteration 19/25 | Loss: 0.00296784
Iteration 20/25 | Loss: 0.00296784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0029678393620997667, 0.0029678393620997667, 0.0029678393620997667, 0.0029678393620997667, 0.0029678393620997667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029678393620997667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296784
Iteration 2/1000 | Loss: 0.00006070
Iteration 3/1000 | Loss: 0.00004490
Iteration 4/1000 | Loss: 0.00003778
Iteration 5/1000 | Loss: 0.00003542
Iteration 6/1000 | Loss: 0.00003376
Iteration 7/1000 | Loss: 0.00003269
Iteration 8/1000 | Loss: 0.00003192
Iteration 9/1000 | Loss: 0.00003124
Iteration 10/1000 | Loss: 0.00003085
Iteration 11/1000 | Loss: 0.00003042
Iteration 12/1000 | Loss: 0.00003010
Iteration 13/1000 | Loss: 0.00002989
Iteration 14/1000 | Loss: 0.00002966
Iteration 15/1000 | Loss: 0.00002955
Iteration 16/1000 | Loss: 0.00002935
Iteration 17/1000 | Loss: 0.00002929
Iteration 18/1000 | Loss: 0.00002921
Iteration 19/1000 | Loss: 0.00002916
Iteration 20/1000 | Loss: 0.00002914
Iteration 21/1000 | Loss: 0.00002911
Iteration 22/1000 | Loss: 0.00002904
Iteration 23/1000 | Loss: 0.00002904
Iteration 24/1000 | Loss: 0.00002903
Iteration 25/1000 | Loss: 0.00002902
Iteration 26/1000 | Loss: 0.00002901
Iteration 27/1000 | Loss: 0.00002901
Iteration 28/1000 | Loss: 0.00002901
Iteration 29/1000 | Loss: 0.00002900
Iteration 30/1000 | Loss: 0.00002898
Iteration 31/1000 | Loss: 0.00002898
Iteration 32/1000 | Loss: 0.00002897
Iteration 33/1000 | Loss: 0.00002896
Iteration 34/1000 | Loss: 0.00002896
Iteration 35/1000 | Loss: 0.00002896
Iteration 36/1000 | Loss: 0.00002895
Iteration 37/1000 | Loss: 0.00002894
Iteration 38/1000 | Loss: 0.00002894
Iteration 39/1000 | Loss: 0.00002893
Iteration 40/1000 | Loss: 0.00002893
Iteration 41/1000 | Loss: 0.00002892
Iteration 42/1000 | Loss: 0.00002891
Iteration 43/1000 | Loss: 0.00002889
Iteration 44/1000 | Loss: 0.00002889
Iteration 45/1000 | Loss: 0.00002888
Iteration 46/1000 | Loss: 0.00002888
Iteration 47/1000 | Loss: 0.00002885
Iteration 48/1000 | Loss: 0.00002884
Iteration 49/1000 | Loss: 0.00002884
Iteration 50/1000 | Loss: 0.00002884
Iteration 51/1000 | Loss: 0.00002883
Iteration 52/1000 | Loss: 0.00002883
Iteration 53/1000 | Loss: 0.00002882
Iteration 54/1000 | Loss: 0.00002880
Iteration 55/1000 | Loss: 0.00002879
Iteration 56/1000 | Loss: 0.00002879
Iteration 57/1000 | Loss: 0.00002879
Iteration 58/1000 | Loss: 0.00002878
Iteration 59/1000 | Loss: 0.00002878
Iteration 60/1000 | Loss: 0.00002878
Iteration 61/1000 | Loss: 0.00002878
Iteration 62/1000 | Loss: 0.00002877
Iteration 63/1000 | Loss: 0.00002877
Iteration 64/1000 | Loss: 0.00002877
Iteration 65/1000 | Loss: 0.00002876
Iteration 66/1000 | Loss: 0.00002876
Iteration 67/1000 | Loss: 0.00002876
Iteration 68/1000 | Loss: 0.00002875
Iteration 69/1000 | Loss: 0.00002875
Iteration 70/1000 | Loss: 0.00002874
Iteration 71/1000 | Loss: 0.00002874
Iteration 72/1000 | Loss: 0.00002873
Iteration 73/1000 | Loss: 0.00002873
Iteration 74/1000 | Loss: 0.00002873
Iteration 75/1000 | Loss: 0.00002872
Iteration 76/1000 | Loss: 0.00002872
Iteration 77/1000 | Loss: 0.00002872
Iteration 78/1000 | Loss: 0.00002871
Iteration 79/1000 | Loss: 0.00002871
Iteration 80/1000 | Loss: 0.00002870
Iteration 81/1000 | Loss: 0.00002869
Iteration 82/1000 | Loss: 0.00002869
Iteration 83/1000 | Loss: 0.00002869
Iteration 84/1000 | Loss: 0.00002868
Iteration 85/1000 | Loss: 0.00002868
Iteration 86/1000 | Loss: 0.00002867
Iteration 87/1000 | Loss: 0.00002867
Iteration 88/1000 | Loss: 0.00002867
Iteration 89/1000 | Loss: 0.00002867
Iteration 90/1000 | Loss: 0.00002867
Iteration 91/1000 | Loss: 0.00002866
Iteration 92/1000 | Loss: 0.00002866
Iteration 93/1000 | Loss: 0.00002866
Iteration 94/1000 | Loss: 0.00002865
Iteration 95/1000 | Loss: 0.00002865
Iteration 96/1000 | Loss: 0.00002864
Iteration 97/1000 | Loss: 0.00002864
Iteration 98/1000 | Loss: 0.00002864
Iteration 99/1000 | Loss: 0.00002863
Iteration 100/1000 | Loss: 0.00002863
Iteration 101/1000 | Loss: 0.00002863
Iteration 102/1000 | Loss: 0.00002863
Iteration 103/1000 | Loss: 0.00002863
Iteration 104/1000 | Loss: 0.00002862
Iteration 105/1000 | Loss: 0.00002862
Iteration 106/1000 | Loss: 0.00002862
Iteration 107/1000 | Loss: 0.00002862
Iteration 108/1000 | Loss: 0.00002861
Iteration 109/1000 | Loss: 0.00002861
Iteration 110/1000 | Loss: 0.00002860
Iteration 111/1000 | Loss: 0.00002860
Iteration 112/1000 | Loss: 0.00002860
Iteration 113/1000 | Loss: 0.00002860
Iteration 114/1000 | Loss: 0.00002859
Iteration 115/1000 | Loss: 0.00002859
Iteration 116/1000 | Loss: 0.00002859
Iteration 117/1000 | Loss: 0.00002859
Iteration 118/1000 | Loss: 0.00002858
Iteration 119/1000 | Loss: 0.00002858
Iteration 120/1000 | Loss: 0.00002858
Iteration 121/1000 | Loss: 0.00002858
Iteration 122/1000 | Loss: 0.00002858
Iteration 123/1000 | Loss: 0.00002858
Iteration 124/1000 | Loss: 0.00002858
Iteration 125/1000 | Loss: 0.00002857
Iteration 126/1000 | Loss: 0.00002857
Iteration 127/1000 | Loss: 0.00002857
Iteration 128/1000 | Loss: 0.00002857
Iteration 129/1000 | Loss: 0.00002857
Iteration 130/1000 | Loss: 0.00002857
Iteration 131/1000 | Loss: 0.00002857
Iteration 132/1000 | Loss: 0.00002856
Iteration 133/1000 | Loss: 0.00002856
Iteration 134/1000 | Loss: 0.00002856
Iteration 135/1000 | Loss: 0.00002856
Iteration 136/1000 | Loss: 0.00002856
Iteration 137/1000 | Loss: 0.00002856
Iteration 138/1000 | Loss: 0.00002856
Iteration 139/1000 | Loss: 0.00002856
Iteration 140/1000 | Loss: 0.00002855
Iteration 141/1000 | Loss: 0.00002855
Iteration 142/1000 | Loss: 0.00002855
Iteration 143/1000 | Loss: 0.00002855
Iteration 144/1000 | Loss: 0.00002855
Iteration 145/1000 | Loss: 0.00002855
Iteration 146/1000 | Loss: 0.00002854
Iteration 147/1000 | Loss: 0.00002854
Iteration 148/1000 | Loss: 0.00002854
Iteration 149/1000 | Loss: 0.00002854
Iteration 150/1000 | Loss: 0.00002854
Iteration 151/1000 | Loss: 0.00002854
Iteration 152/1000 | Loss: 0.00002853
Iteration 153/1000 | Loss: 0.00002853
Iteration 154/1000 | Loss: 0.00002853
Iteration 155/1000 | Loss: 0.00002853
Iteration 156/1000 | Loss: 0.00002853
Iteration 157/1000 | Loss: 0.00002853
Iteration 158/1000 | Loss: 0.00002853
Iteration 159/1000 | Loss: 0.00002853
Iteration 160/1000 | Loss: 0.00002853
Iteration 161/1000 | Loss: 0.00002853
Iteration 162/1000 | Loss: 0.00002853
Iteration 163/1000 | Loss: 0.00002853
Iteration 164/1000 | Loss: 0.00002853
Iteration 165/1000 | Loss: 0.00002853
Iteration 166/1000 | Loss: 0.00002852
Iteration 167/1000 | Loss: 0.00002852
Iteration 168/1000 | Loss: 0.00002852
Iteration 169/1000 | Loss: 0.00002852
Iteration 170/1000 | Loss: 0.00002852
Iteration 171/1000 | Loss: 0.00002852
Iteration 172/1000 | Loss: 0.00002852
Iteration 173/1000 | Loss: 0.00002852
Iteration 174/1000 | Loss: 0.00002852
Iteration 175/1000 | Loss: 0.00002852
Iteration 176/1000 | Loss: 0.00002851
Iteration 177/1000 | Loss: 0.00002851
Iteration 178/1000 | Loss: 0.00002851
Iteration 179/1000 | Loss: 0.00002851
Iteration 180/1000 | Loss: 0.00002851
Iteration 181/1000 | Loss: 0.00002851
Iteration 182/1000 | Loss: 0.00002851
Iteration 183/1000 | Loss: 0.00002851
Iteration 184/1000 | Loss: 0.00002851
Iteration 185/1000 | Loss: 0.00002851
Iteration 186/1000 | Loss: 0.00002851
Iteration 187/1000 | Loss: 0.00002851
Iteration 188/1000 | Loss: 0.00002851
Iteration 189/1000 | Loss: 0.00002851
Iteration 190/1000 | Loss: 0.00002851
Iteration 191/1000 | Loss: 0.00002851
Iteration 192/1000 | Loss: 0.00002850
Iteration 193/1000 | Loss: 0.00002850
Iteration 194/1000 | Loss: 0.00002850
Iteration 195/1000 | Loss: 0.00002850
Iteration 196/1000 | Loss: 0.00002850
Iteration 197/1000 | Loss: 0.00002850
Iteration 198/1000 | Loss: 0.00002850
Iteration 199/1000 | Loss: 0.00002850
Iteration 200/1000 | Loss: 0.00002850
Iteration 201/1000 | Loss: 0.00002850
Iteration 202/1000 | Loss: 0.00002850
Iteration 203/1000 | Loss: 0.00002850
Iteration 204/1000 | Loss: 0.00002850
Iteration 205/1000 | Loss: 0.00002850
Iteration 206/1000 | Loss: 0.00002850
Iteration 207/1000 | Loss: 0.00002849
Iteration 208/1000 | Loss: 0.00002849
Iteration 209/1000 | Loss: 0.00002849
Iteration 210/1000 | Loss: 0.00002849
Iteration 211/1000 | Loss: 0.00002849
Iteration 212/1000 | Loss: 0.00002849
Iteration 213/1000 | Loss: 0.00002849
Iteration 214/1000 | Loss: 0.00002849
Iteration 215/1000 | Loss: 0.00002849
Iteration 216/1000 | Loss: 0.00002849
Iteration 217/1000 | Loss: 0.00002849
Iteration 218/1000 | Loss: 0.00002849
Iteration 219/1000 | Loss: 0.00002849
Iteration 220/1000 | Loss: 0.00002849
Iteration 221/1000 | Loss: 0.00002849
Iteration 222/1000 | Loss: 0.00002848
Iteration 223/1000 | Loss: 0.00002848
Iteration 224/1000 | Loss: 0.00002848
Iteration 225/1000 | Loss: 0.00002848
Iteration 226/1000 | Loss: 0.00002848
Iteration 227/1000 | Loss: 0.00002848
Iteration 228/1000 | Loss: 0.00002848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.848366420948878e-05, 2.848366420948878e-05, 2.848366420948878e-05, 2.848366420948878e-05, 2.848366420948878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.848366420948878e-05

Optimization complete. Final v2v error: 4.50417947769165 mm

Highest mean error: 5.5606279373168945 mm for frame 20

Lowest mean error: 3.530906915664673 mm for frame 130

Saving results

Total time: 49.84258008003235
