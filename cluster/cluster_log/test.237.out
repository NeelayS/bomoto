Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=237, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13272-13327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1070
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382360
Iteration 2/25 | Loss: 0.00128158
Iteration 3/25 | Loss: 0.00119980
Iteration 4/25 | Loss: 0.00119185
Iteration 5/25 | Loss: 0.00118899
Iteration 6/25 | Loss: 0.00118899
Iteration 7/25 | Loss: 0.00118899
Iteration 8/25 | Loss: 0.00118899
Iteration 9/25 | Loss: 0.00118899
Iteration 10/25 | Loss: 0.00118899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011889919405803084, 0.0011889919405803084, 0.0011889919405803084, 0.0011889919405803084, 0.0011889919405803084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011889919405803084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50729978
Iteration 2/25 | Loss: 0.00093745
Iteration 3/25 | Loss: 0.00093745
Iteration 4/25 | Loss: 0.00093745
Iteration 5/25 | Loss: 0.00093745
Iteration 6/25 | Loss: 0.00093745
Iteration 7/25 | Loss: 0.00093745
Iteration 8/25 | Loss: 0.00093745
Iteration 9/25 | Loss: 0.00093745
Iteration 10/25 | Loss: 0.00093744
Iteration 11/25 | Loss: 0.00093744
Iteration 12/25 | Loss: 0.00093744
Iteration 13/25 | Loss: 0.00093744
Iteration 14/25 | Loss: 0.00093744
Iteration 15/25 | Loss: 0.00093744
Iteration 16/25 | Loss: 0.00093744
Iteration 17/25 | Loss: 0.00093744
Iteration 18/25 | Loss: 0.00093744
Iteration 19/25 | Loss: 0.00093744
Iteration 20/25 | Loss: 0.00093744
Iteration 21/25 | Loss: 0.00093744
Iteration 22/25 | Loss: 0.00093744
Iteration 23/25 | Loss: 0.00093744
Iteration 24/25 | Loss: 0.00093744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009374447399750352, 0.0009374447399750352, 0.0009374447399750352, 0.0009374447399750352, 0.0009374447399750352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009374447399750352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093744
Iteration 2/1000 | Loss: 0.00002877
Iteration 3/1000 | Loss: 0.00001686
Iteration 4/1000 | Loss: 0.00001463
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001246
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001091
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001068
Iteration 14/1000 | Loss: 0.00001067
Iteration 15/1000 | Loss: 0.00001063
Iteration 16/1000 | Loss: 0.00001063
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001062
Iteration 19/1000 | Loss: 0.00001061
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001060
Iteration 22/1000 | Loss: 0.00001058
Iteration 23/1000 | Loss: 0.00001058
Iteration 24/1000 | Loss: 0.00001058
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001058
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001058
Iteration 29/1000 | Loss: 0.00001058
Iteration 30/1000 | Loss: 0.00001057
Iteration 31/1000 | Loss: 0.00001057
Iteration 32/1000 | Loss: 0.00001057
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001055
Iteration 35/1000 | Loss: 0.00001054
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001053
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001049
Iteration 48/1000 | Loss: 0.00001049
Iteration 49/1000 | Loss: 0.00001049
Iteration 50/1000 | Loss: 0.00001049
Iteration 51/1000 | Loss: 0.00001049
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001049
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001048
Iteration 59/1000 | Loss: 0.00001048
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001045
Iteration 63/1000 | Loss: 0.00001045
Iteration 64/1000 | Loss: 0.00001045
Iteration 65/1000 | Loss: 0.00001045
Iteration 66/1000 | Loss: 0.00001045
Iteration 67/1000 | Loss: 0.00001044
Iteration 68/1000 | Loss: 0.00001044
Iteration 69/1000 | Loss: 0.00001043
Iteration 70/1000 | Loss: 0.00001043
Iteration 71/1000 | Loss: 0.00001043
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001043
Iteration 74/1000 | Loss: 0.00001042
Iteration 75/1000 | Loss: 0.00001042
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001041
Iteration 78/1000 | Loss: 0.00001041
Iteration 79/1000 | Loss: 0.00001041
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001040
Iteration 86/1000 | Loss: 0.00001040
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001038
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001037
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001037
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Iteration 103/1000 | Loss: 0.00001037
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001035
Iteration 109/1000 | Loss: 0.00001035
Iteration 110/1000 | Loss: 0.00001035
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001034
Iteration 113/1000 | Loss: 0.00001034
Iteration 114/1000 | Loss: 0.00001033
Iteration 115/1000 | Loss: 0.00001033
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001033
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001032
Iteration 124/1000 | Loss: 0.00001032
Iteration 125/1000 | Loss: 0.00001032
Iteration 126/1000 | Loss: 0.00001031
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001031
Iteration 129/1000 | Loss: 0.00001031
Iteration 130/1000 | Loss: 0.00001031
Iteration 131/1000 | Loss: 0.00001031
Iteration 132/1000 | Loss: 0.00001031
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001029
Iteration 141/1000 | Loss: 0.00001029
Iteration 142/1000 | Loss: 0.00001029
Iteration 143/1000 | Loss: 0.00001029
Iteration 144/1000 | Loss: 0.00001029
Iteration 145/1000 | Loss: 0.00001029
Iteration 146/1000 | Loss: 0.00001028
Iteration 147/1000 | Loss: 0.00001028
Iteration 148/1000 | Loss: 0.00001028
Iteration 149/1000 | Loss: 0.00001028
Iteration 150/1000 | Loss: 0.00001028
Iteration 151/1000 | Loss: 0.00001028
Iteration 152/1000 | Loss: 0.00001027
Iteration 153/1000 | Loss: 0.00001027
Iteration 154/1000 | Loss: 0.00001027
Iteration 155/1000 | Loss: 0.00001027
Iteration 156/1000 | Loss: 0.00001026
Iteration 157/1000 | Loss: 0.00001026
Iteration 158/1000 | Loss: 0.00001026
Iteration 159/1000 | Loss: 0.00001026
Iteration 160/1000 | Loss: 0.00001026
Iteration 161/1000 | Loss: 0.00001026
Iteration 162/1000 | Loss: 0.00001026
Iteration 163/1000 | Loss: 0.00001025
Iteration 164/1000 | Loss: 0.00001025
Iteration 165/1000 | Loss: 0.00001025
Iteration 166/1000 | Loss: 0.00001025
Iteration 167/1000 | Loss: 0.00001025
Iteration 168/1000 | Loss: 0.00001025
Iteration 169/1000 | Loss: 0.00001025
Iteration 170/1000 | Loss: 0.00001025
Iteration 171/1000 | Loss: 0.00001025
Iteration 172/1000 | Loss: 0.00001025
Iteration 173/1000 | Loss: 0.00001025
Iteration 174/1000 | Loss: 0.00001025
Iteration 175/1000 | Loss: 0.00001024
Iteration 176/1000 | Loss: 0.00001024
Iteration 177/1000 | Loss: 0.00001024
Iteration 178/1000 | Loss: 0.00001024
Iteration 179/1000 | Loss: 0.00001024
Iteration 180/1000 | Loss: 0.00001023
Iteration 181/1000 | Loss: 0.00001023
Iteration 182/1000 | Loss: 0.00001023
Iteration 183/1000 | Loss: 0.00001023
Iteration 184/1000 | Loss: 0.00001022
Iteration 185/1000 | Loss: 0.00001022
Iteration 186/1000 | Loss: 0.00001022
Iteration 187/1000 | Loss: 0.00001022
Iteration 188/1000 | Loss: 0.00001022
Iteration 189/1000 | Loss: 0.00001022
Iteration 190/1000 | Loss: 0.00001022
Iteration 191/1000 | Loss: 0.00001021
Iteration 192/1000 | Loss: 0.00001021
Iteration 193/1000 | Loss: 0.00001021
Iteration 194/1000 | Loss: 0.00001021
Iteration 195/1000 | Loss: 0.00001020
Iteration 196/1000 | Loss: 0.00001020
Iteration 197/1000 | Loss: 0.00001020
Iteration 198/1000 | Loss: 0.00001019
Iteration 199/1000 | Loss: 0.00001019
Iteration 200/1000 | Loss: 0.00001019
Iteration 201/1000 | Loss: 0.00001019
Iteration 202/1000 | Loss: 0.00001018
Iteration 203/1000 | Loss: 0.00001018
Iteration 204/1000 | Loss: 0.00001018
Iteration 205/1000 | Loss: 0.00001017
Iteration 206/1000 | Loss: 0.00001017
Iteration 207/1000 | Loss: 0.00001017
Iteration 208/1000 | Loss: 0.00001017
Iteration 209/1000 | Loss: 0.00001017
Iteration 210/1000 | Loss: 0.00001017
Iteration 211/1000 | Loss: 0.00001017
Iteration 212/1000 | Loss: 0.00001017
Iteration 213/1000 | Loss: 0.00001016
Iteration 214/1000 | Loss: 0.00001016
Iteration 215/1000 | Loss: 0.00001016
Iteration 216/1000 | Loss: 0.00001016
Iteration 217/1000 | Loss: 0.00001015
Iteration 218/1000 | Loss: 0.00001015
Iteration 219/1000 | Loss: 0.00001015
Iteration 220/1000 | Loss: 0.00001014
Iteration 221/1000 | Loss: 0.00001014
Iteration 222/1000 | Loss: 0.00001014
Iteration 223/1000 | Loss: 0.00001014
Iteration 224/1000 | Loss: 0.00001013
Iteration 225/1000 | Loss: 0.00001013
Iteration 226/1000 | Loss: 0.00001013
Iteration 227/1000 | Loss: 0.00001013
Iteration 228/1000 | Loss: 0.00001013
Iteration 229/1000 | Loss: 0.00001012
Iteration 230/1000 | Loss: 0.00001012
Iteration 231/1000 | Loss: 0.00001012
Iteration 232/1000 | Loss: 0.00001012
Iteration 233/1000 | Loss: 0.00001012
Iteration 234/1000 | Loss: 0.00001012
Iteration 235/1000 | Loss: 0.00001012
Iteration 236/1000 | Loss: 0.00001012
Iteration 237/1000 | Loss: 0.00001012
Iteration 238/1000 | Loss: 0.00001012
Iteration 239/1000 | Loss: 0.00001012
Iteration 240/1000 | Loss: 0.00001012
Iteration 241/1000 | Loss: 0.00001012
Iteration 242/1000 | Loss: 0.00001012
Iteration 243/1000 | Loss: 0.00001011
Iteration 244/1000 | Loss: 0.00001011
Iteration 245/1000 | Loss: 0.00001011
Iteration 246/1000 | Loss: 0.00001011
Iteration 247/1000 | Loss: 0.00001011
Iteration 248/1000 | Loss: 0.00001011
Iteration 249/1000 | Loss: 0.00001011
Iteration 250/1000 | Loss: 0.00001011
Iteration 251/1000 | Loss: 0.00001011
Iteration 252/1000 | Loss: 0.00001011
Iteration 253/1000 | Loss: 0.00001011
Iteration 254/1000 | Loss: 0.00001011
Iteration 255/1000 | Loss: 0.00001011
Iteration 256/1000 | Loss: 0.00001011
Iteration 257/1000 | Loss: 0.00001011
Iteration 258/1000 | Loss: 0.00001011
Iteration 259/1000 | Loss: 0.00001010
Iteration 260/1000 | Loss: 0.00001010
Iteration 261/1000 | Loss: 0.00001010
Iteration 262/1000 | Loss: 0.00001010
Iteration 263/1000 | Loss: 0.00001010
Iteration 264/1000 | Loss: 0.00001010
Iteration 265/1000 | Loss: 0.00001010
Iteration 266/1000 | Loss: 0.00001010
Iteration 267/1000 | Loss: 0.00001010
Iteration 268/1000 | Loss: 0.00001010
Iteration 269/1000 | Loss: 0.00001010
Iteration 270/1000 | Loss: 0.00001010
Iteration 271/1000 | Loss: 0.00001010
Iteration 272/1000 | Loss: 0.00001010
Iteration 273/1000 | Loss: 0.00001010
Iteration 274/1000 | Loss: 0.00001010
Iteration 275/1000 | Loss: 0.00001010
Iteration 276/1000 | Loss: 0.00001010
Iteration 277/1000 | Loss: 0.00001010
Iteration 278/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.0100237886945251e-05, 1.0100237886945251e-05, 1.0100237886945251e-05, 1.0100237886945251e-05, 1.0100237886945251e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0100237886945251e-05

Optimization complete. Final v2v error: 2.721719980239868 mm

Highest mean error: 3.1072471141815186 mm for frame 130

Lowest mean error: 2.519728183746338 mm for frame 184

Saving results

Total time: 1646.472469329834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1045
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351001
Iteration 2/25 | Loss: 0.00153416
Iteration 3/25 | Loss: 0.00125866
Iteration 4/25 | Loss: 0.00121844
Iteration 5/25 | Loss: 0.00121150
Iteration 6/25 | Loss: 0.00120923
Iteration 7/25 | Loss: 0.00120847
Iteration 8/25 | Loss: 0.00120847
Iteration 9/25 | Loss: 0.00120847
Iteration 10/25 | Loss: 0.00120847
Iteration 11/25 | Loss: 0.00120847
Iteration 12/25 | Loss: 0.00120847
Iteration 13/25 | Loss: 0.00120847
Iteration 14/25 | Loss: 0.00120847
Iteration 15/25 | Loss: 0.00120847
Iteration 16/25 | Loss: 0.00120847
Iteration 17/25 | Loss: 0.00120847
Iteration 18/25 | Loss: 0.00120847
Iteration 19/25 | Loss: 0.00120847
Iteration 20/25 | Loss: 0.00120847
Iteration 21/25 | Loss: 0.00120847
Iteration 22/25 | Loss: 0.00120847
Iteration 23/25 | Loss: 0.00120847
Iteration 24/25 | Loss: 0.00120847
Iteration 25/25 | Loss: 0.00120847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38203967
Iteration 2/25 | Loss: 0.00105572
Iteration 3/25 | Loss: 0.00105572
Iteration 4/25 | Loss: 0.00105572
Iteration 5/25 | Loss: 0.00105572
Iteration 6/25 | Loss: 0.00105572
Iteration 7/25 | Loss: 0.00105572
Iteration 8/25 | Loss: 0.00105572
Iteration 9/25 | Loss: 0.00105572
Iteration 10/25 | Loss: 0.00105572
Iteration 11/25 | Loss: 0.00105572
Iteration 12/25 | Loss: 0.00105572
Iteration 13/25 | Loss: 0.00105572
Iteration 14/25 | Loss: 0.00105572
Iteration 15/25 | Loss: 0.00105572
Iteration 16/25 | Loss: 0.00105572
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010557195637375116, 0.0010557195637375116, 0.0010557195637375116, 0.0010557195637375116, 0.0010557195637375116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010557195637375116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105572
Iteration 2/1000 | Loss: 0.00004903
Iteration 3/1000 | Loss: 0.00002672
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001568
Iteration 8/1000 | Loss: 0.00001518
Iteration 9/1000 | Loss: 0.00001473
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001393
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001384
Iteration 21/1000 | Loss: 0.00001384
Iteration 22/1000 | Loss: 0.00001384
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001383
Iteration 25/1000 | Loss: 0.00001383
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001382
Iteration 29/1000 | Loss: 0.00001382
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001381
Iteration 33/1000 | Loss: 0.00001380
Iteration 34/1000 | Loss: 0.00001380
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001380
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001379
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001377
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001368
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.3663317076861858e-05, 1.3663317076861858e-05, 1.3663317076861858e-05, 1.3663317076861858e-05, 1.3663317076861858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3663317076861858e-05

Optimization complete. Final v2v error: 3.169409990310669 mm

Highest mean error: 3.5646166801452637 mm for frame 28

Lowest mean error: 2.7198808193206787 mm for frame 13

Saving results

Total time: 602.6599729061127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1048
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047980
Iteration 2/25 | Loss: 0.01047980
Iteration 3/25 | Loss: 0.01047979
Iteration 4/25 | Loss: 0.01047979
Iteration 5/25 | Loss: 0.01047979
Iteration 6/25 | Loss: 0.01047979
Iteration 7/25 | Loss: 0.01047979
Iteration 8/25 | Loss: 0.01047979
Iteration 9/25 | Loss: 0.01047978
Iteration 10/25 | Loss: 0.01047978
Iteration 11/25 | Loss: 0.01047978
Iteration 12/25 | Loss: 0.01047978
Iteration 13/25 | Loss: 0.01047978
Iteration 14/25 | Loss: 0.01047978
Iteration 15/25 | Loss: 0.01047977
Iteration 16/25 | Loss: 0.01047977
Iteration 17/25 | Loss: 0.01047977
Iteration 18/25 | Loss: 0.01047977
Iteration 19/25 | Loss: 0.01047977
Iteration 20/25 | Loss: 0.01047977
Iteration 21/25 | Loss: 0.01047976
Iteration 22/25 | Loss: 0.01047976
Iteration 23/25 | Loss: 0.01047976
Iteration 24/25 | Loss: 0.01047976
Iteration 25/25 | Loss: 0.01047975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95335567
Iteration 2/25 | Loss: 0.09394018
Iteration 3/25 | Loss: 0.09247988
Iteration 4/25 | Loss: 0.09206662
Iteration 5/25 | Loss: 0.09206662
Iteration 6/25 | Loss: 0.09206147
Iteration 7/25 | Loss: 0.09206147
Iteration 8/25 | Loss: 0.09206147
Iteration 9/25 | Loss: 0.09206147
Iteration 10/25 | Loss: 0.09206147
Iteration 11/25 | Loss: 0.09206147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.09206146746873856, 0.09206146746873856, 0.09206146746873856, 0.09206146746873856, 0.09206146746873856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09206146746873856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09206147
Iteration 2/1000 | Loss: 0.00148826
Iteration 3/1000 | Loss: 0.00283012
Iteration 4/1000 | Loss: 0.00229903
Iteration 5/1000 | Loss: 0.00355408
Iteration 6/1000 | Loss: 0.00186757
Iteration 7/1000 | Loss: 0.00062912
Iteration 8/1000 | Loss: 0.00215757
Iteration 9/1000 | Loss: 0.00130324
Iteration 10/1000 | Loss: 0.00008373
Iteration 11/1000 | Loss: 0.00005732
Iteration 12/1000 | Loss: 0.00006967
Iteration 13/1000 | Loss: 0.00021635
Iteration 14/1000 | Loss: 0.00014248
Iteration 15/1000 | Loss: 0.00008746
Iteration 16/1000 | Loss: 0.00008254
Iteration 17/1000 | Loss: 0.00030316
Iteration 18/1000 | Loss: 0.00067578
Iteration 19/1000 | Loss: 0.00003252
Iteration 20/1000 | Loss: 0.00016815
Iteration 21/1000 | Loss: 0.00004068
Iteration 22/1000 | Loss: 0.00014392
Iteration 23/1000 | Loss: 0.00004918
Iteration 24/1000 | Loss: 0.00008260
Iteration 25/1000 | Loss: 0.00003089
Iteration 26/1000 | Loss: 0.00003283
Iteration 27/1000 | Loss: 0.00007153
Iteration 28/1000 | Loss: 0.00015631
Iteration 29/1000 | Loss: 0.00003225
Iteration 30/1000 | Loss: 0.00002759
Iteration 31/1000 | Loss: 0.00009646
Iteration 32/1000 | Loss: 0.00007482
Iteration 33/1000 | Loss: 0.00008211
Iteration 34/1000 | Loss: 0.00003799
Iteration 35/1000 | Loss: 0.00037470
Iteration 36/1000 | Loss: 0.00005772
Iteration 37/1000 | Loss: 0.00002330
Iteration 38/1000 | Loss: 0.00003127
Iteration 39/1000 | Loss: 0.00002377
Iteration 40/1000 | Loss: 0.00002150
Iteration 41/1000 | Loss: 0.00005396
Iteration 42/1000 | Loss: 0.00007886
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002537
Iteration 45/1000 | Loss: 0.00002263
Iteration 46/1000 | Loss: 0.00008318
Iteration 47/1000 | Loss: 0.00007872
Iteration 48/1000 | Loss: 0.00002308
Iteration 49/1000 | Loss: 0.00002402
Iteration 50/1000 | Loss: 0.00009789
Iteration 51/1000 | Loss: 0.00004382
Iteration 52/1000 | Loss: 0.00004757
Iteration 53/1000 | Loss: 0.00003688
Iteration 54/1000 | Loss: 0.00003362
Iteration 55/1000 | Loss: 0.00003019
Iteration 56/1000 | Loss: 0.00026044
Iteration 57/1000 | Loss: 0.00006055
Iteration 58/1000 | Loss: 0.00003981
Iteration 59/1000 | Loss: 0.00001994
Iteration 60/1000 | Loss: 0.00002947
Iteration 61/1000 | Loss: 0.00003177
Iteration 62/1000 | Loss: 0.00002390
Iteration 63/1000 | Loss: 0.00002414
Iteration 64/1000 | Loss: 0.00001958
Iteration 65/1000 | Loss: 0.00001944
Iteration 66/1000 | Loss: 0.00001944
Iteration 67/1000 | Loss: 0.00001944
Iteration 68/1000 | Loss: 0.00001944
Iteration 69/1000 | Loss: 0.00001944
Iteration 70/1000 | Loss: 0.00001944
Iteration 71/1000 | Loss: 0.00001944
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001943
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002929
Iteration 76/1000 | Loss: 0.00003240
Iteration 77/1000 | Loss: 0.00005961
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00001950
Iteration 80/1000 | Loss: 0.00001950
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00001896
Iteration 83/1000 | Loss: 0.00001888
Iteration 84/1000 | Loss: 0.00001881
Iteration 85/1000 | Loss: 0.00001881
Iteration 86/1000 | Loss: 0.00008054
Iteration 87/1000 | Loss: 0.00008054
Iteration 88/1000 | Loss: 0.00016644
Iteration 89/1000 | Loss: 0.00003011
Iteration 90/1000 | Loss: 0.00004145
Iteration 91/1000 | Loss: 0.00024626
Iteration 92/1000 | Loss: 0.00004789
Iteration 93/1000 | Loss: 0.00011832
Iteration 94/1000 | Loss: 0.00003245
Iteration 95/1000 | Loss: 0.00001949
Iteration 96/1000 | Loss: 0.00002720
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00005802
Iteration 100/1000 | Loss: 0.00006775
Iteration 101/1000 | Loss: 0.00005627
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002813
Iteration 104/1000 | Loss: 0.00002813
Iteration 105/1000 | Loss: 0.00001923
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00002528
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001825
Iteration 110/1000 | Loss: 0.00001825
Iteration 111/1000 | Loss: 0.00001824
Iteration 112/1000 | Loss: 0.00001824
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001824
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00001824
Iteration 118/1000 | Loss: 0.00001823
Iteration 119/1000 | Loss: 0.00001823
Iteration 120/1000 | Loss: 0.00001823
Iteration 121/1000 | Loss: 0.00001823
Iteration 122/1000 | Loss: 0.00001823
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001823
Iteration 125/1000 | Loss: 0.00001823
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001823
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001823
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001823
Iteration 138/1000 | Loss: 0.00001823
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001823
Iteration 143/1000 | Loss: 0.00001823
Iteration 144/1000 | Loss: 0.00001823
Iteration 145/1000 | Loss: 0.00001823
Iteration 146/1000 | Loss: 0.00001823
Iteration 147/1000 | Loss: 0.00001823
Iteration 148/1000 | Loss: 0.00001823
Iteration 149/1000 | Loss: 0.00001823
Iteration 150/1000 | Loss: 0.00001823
Iteration 151/1000 | Loss: 0.00001823
Iteration 152/1000 | Loss: 0.00001823
Iteration 153/1000 | Loss: 0.00001823
Iteration 154/1000 | Loss: 0.00001823
Iteration 155/1000 | Loss: 0.00001823
Iteration 156/1000 | Loss: 0.00001823
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001823
Iteration 160/1000 | Loss: 0.00001823
Iteration 161/1000 | Loss: 0.00001823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.8225393432658166e-05, 1.8225393432658166e-05, 1.8225393432658166e-05, 1.8225393432658166e-05, 1.8225393432658166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8225393432658166e-05

Optimization complete. Final v2v error: 3.6238646507263184 mm

Highest mean error: 5.1610565185546875 mm for frame 27

Lowest mean error: 2.849336862564087 mm for frame 237

Saving results

Total time: 3979.081385612488
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0008
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043847
Iteration 2/25 | Loss: 0.00179598
Iteration 3/25 | Loss: 0.00120945
Iteration 4/25 | Loss: 0.00116749
Iteration 5/25 | Loss: 0.00115386
Iteration 6/25 | Loss: 0.00114967
Iteration 7/25 | Loss: 0.00114946
Iteration 8/25 | Loss: 0.00114946
Iteration 9/25 | Loss: 0.00114946
Iteration 10/25 | Loss: 0.00114946
Iteration 11/25 | Loss: 0.00114946
Iteration 12/25 | Loss: 0.00114946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011494645150378346, 0.0011494645150378346, 0.0011494645150378346, 0.0011494645150378346, 0.0011494645150378346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011494645150378346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61717099
Iteration 2/25 | Loss: 0.00038768
Iteration 3/25 | Loss: 0.00038768
Iteration 4/25 | Loss: 0.00038768
Iteration 5/25 | Loss: 0.00038767
Iteration 6/25 | Loss: 0.00038767
Iteration 7/25 | Loss: 0.00038767
Iteration 8/25 | Loss: 0.00038767
Iteration 9/25 | Loss: 0.00038767
Iteration 10/25 | Loss: 0.00038767
Iteration 11/25 | Loss: 0.00038767
Iteration 12/25 | Loss: 0.00038767
Iteration 13/25 | Loss: 0.00038767
Iteration 14/25 | Loss: 0.00038767
Iteration 15/25 | Loss: 0.00038767
Iteration 16/25 | Loss: 0.00038767
Iteration 17/25 | Loss: 0.00038767
Iteration 18/25 | Loss: 0.00038767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00038767262594774365, 0.00038767262594774365, 0.00038767262594774365, 0.00038767262594774365, 0.00038767262594774365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038767262594774365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038767
Iteration 2/1000 | Loss: 0.00008476
Iteration 3/1000 | Loss: 0.00006584
Iteration 4/1000 | Loss: 0.00006145
Iteration 5/1000 | Loss: 0.00005795
Iteration 6/1000 | Loss: 0.00005637
Iteration 7/1000 | Loss: 0.00005511
Iteration 8/1000 | Loss: 0.00005428
Iteration 9/1000 | Loss: 0.00005374
Iteration 10/1000 | Loss: 0.00005327
Iteration 11/1000 | Loss: 0.00005302
Iteration 12/1000 | Loss: 0.00005285
Iteration 13/1000 | Loss: 0.00005278
Iteration 14/1000 | Loss: 0.00005278
Iteration 15/1000 | Loss: 0.00005278
Iteration 16/1000 | Loss: 0.00005278
Iteration 17/1000 | Loss: 0.00005277
Iteration 18/1000 | Loss: 0.00005277
Iteration 19/1000 | Loss: 0.00005273
Iteration 20/1000 | Loss: 0.00005273
Iteration 21/1000 | Loss: 0.00005270
Iteration 22/1000 | Loss: 0.00005270
Iteration 23/1000 | Loss: 0.00005266
Iteration 24/1000 | Loss: 0.00005266
Iteration 25/1000 | Loss: 0.00005266
Iteration 26/1000 | Loss: 0.00005266
Iteration 27/1000 | Loss: 0.00005265
Iteration 28/1000 | Loss: 0.00005265
Iteration 29/1000 | Loss: 0.00005265
Iteration 30/1000 | Loss: 0.00005265
Iteration 31/1000 | Loss: 0.00005265
Iteration 32/1000 | Loss: 0.00005265
Iteration 33/1000 | Loss: 0.00005264
Iteration 34/1000 | Loss: 0.00005264
Iteration 35/1000 | Loss: 0.00005262
Iteration 36/1000 | Loss: 0.00005262
Iteration 37/1000 | Loss: 0.00005262
Iteration 38/1000 | Loss: 0.00005262
Iteration 39/1000 | Loss: 0.00005262
Iteration 40/1000 | Loss: 0.00005262
Iteration 41/1000 | Loss: 0.00005261
Iteration 42/1000 | Loss: 0.00005261
Iteration 43/1000 | Loss: 0.00005261
Iteration 44/1000 | Loss: 0.00005261
Iteration 45/1000 | Loss: 0.00005261
Iteration 46/1000 | Loss: 0.00005261
Iteration 47/1000 | Loss: 0.00005260
Iteration 48/1000 | Loss: 0.00005257
Iteration 49/1000 | Loss: 0.00005257
Iteration 50/1000 | Loss: 0.00005257
Iteration 51/1000 | Loss: 0.00005257
Iteration 52/1000 | Loss: 0.00005256
Iteration 53/1000 | Loss: 0.00005256
Iteration 54/1000 | Loss: 0.00005255
Iteration 55/1000 | Loss: 0.00005254
Iteration 56/1000 | Loss: 0.00005254
Iteration 57/1000 | Loss: 0.00005254
Iteration 58/1000 | Loss: 0.00005253
Iteration 59/1000 | Loss: 0.00005253
Iteration 60/1000 | Loss: 0.00005252
Iteration 61/1000 | Loss: 0.00005252
Iteration 62/1000 | Loss: 0.00005252
Iteration 63/1000 | Loss: 0.00005251
Iteration 64/1000 | Loss: 0.00005251
Iteration 65/1000 | Loss: 0.00005251
Iteration 66/1000 | Loss: 0.00005251
Iteration 67/1000 | Loss: 0.00005251
Iteration 68/1000 | Loss: 0.00005250
Iteration 69/1000 | Loss: 0.00005250
Iteration 70/1000 | Loss: 0.00005250
Iteration 71/1000 | Loss: 0.00005250
Iteration 72/1000 | Loss: 0.00005250
Iteration 73/1000 | Loss: 0.00005250
Iteration 74/1000 | Loss: 0.00005249
Iteration 75/1000 | Loss: 0.00005249
Iteration 76/1000 | Loss: 0.00005249
Iteration 77/1000 | Loss: 0.00005249
Iteration 78/1000 | Loss: 0.00005249
Iteration 79/1000 | Loss: 0.00005249
Iteration 80/1000 | Loss: 0.00005249
Iteration 81/1000 | Loss: 0.00005249
Iteration 82/1000 | Loss: 0.00005249
Iteration 83/1000 | Loss: 0.00005248
Iteration 84/1000 | Loss: 0.00005248
Iteration 85/1000 | Loss: 0.00005248
Iteration 86/1000 | Loss: 0.00005248
Iteration 87/1000 | Loss: 0.00005248
Iteration 88/1000 | Loss: 0.00005248
Iteration 89/1000 | Loss: 0.00005248
Iteration 90/1000 | Loss: 0.00005248
Iteration 91/1000 | Loss: 0.00005247
Iteration 92/1000 | Loss: 0.00005247
Iteration 93/1000 | Loss: 0.00005247
Iteration 94/1000 | Loss: 0.00005247
Iteration 95/1000 | Loss: 0.00005247
Iteration 96/1000 | Loss: 0.00005247
Iteration 97/1000 | Loss: 0.00005247
Iteration 98/1000 | Loss: 0.00005247
Iteration 99/1000 | Loss: 0.00005246
Iteration 100/1000 | Loss: 0.00005246
Iteration 101/1000 | Loss: 0.00005246
Iteration 102/1000 | Loss: 0.00005246
Iteration 103/1000 | Loss: 0.00005246
Iteration 104/1000 | Loss: 0.00005246
Iteration 105/1000 | Loss: 0.00005246
Iteration 106/1000 | Loss: 0.00005246
Iteration 107/1000 | Loss: 0.00005246
Iteration 108/1000 | Loss: 0.00005246
Iteration 109/1000 | Loss: 0.00005246
Iteration 110/1000 | Loss: 0.00005246
Iteration 111/1000 | Loss: 0.00005246
Iteration 112/1000 | Loss: 0.00005246
Iteration 113/1000 | Loss: 0.00005246
Iteration 114/1000 | Loss: 0.00005246
Iteration 115/1000 | Loss: 0.00005245
Iteration 116/1000 | Loss: 0.00005245
Iteration 117/1000 | Loss: 0.00005245
Iteration 118/1000 | Loss: 0.00005245
Iteration 119/1000 | Loss: 0.00005245
Iteration 120/1000 | Loss: 0.00005245
Iteration 121/1000 | Loss: 0.00005245
Iteration 122/1000 | Loss: 0.00005245
Iteration 123/1000 | Loss: 0.00005245
Iteration 124/1000 | Loss: 0.00005245
Iteration 125/1000 | Loss: 0.00005245
Iteration 126/1000 | Loss: 0.00005245
Iteration 127/1000 | Loss: 0.00005245
Iteration 128/1000 | Loss: 0.00005245
Iteration 129/1000 | Loss: 0.00005244
Iteration 130/1000 | Loss: 0.00005244
Iteration 131/1000 | Loss: 0.00005244
Iteration 132/1000 | Loss: 0.00005244
Iteration 133/1000 | Loss: 0.00005244
Iteration 134/1000 | Loss: 0.00005244
Iteration 135/1000 | Loss: 0.00005244
Iteration 136/1000 | Loss: 0.00005244
Iteration 137/1000 | Loss: 0.00005244
Iteration 138/1000 | Loss: 0.00005244
Iteration 139/1000 | Loss: 0.00005244
Iteration 140/1000 | Loss: 0.00005244
Iteration 141/1000 | Loss: 0.00005244
Iteration 142/1000 | Loss: 0.00005244
Iteration 143/1000 | Loss: 0.00005244
Iteration 144/1000 | Loss: 0.00005244
Iteration 145/1000 | Loss: 0.00005244
Iteration 146/1000 | Loss: 0.00005244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [5.244112981017679e-05, 5.244112981017679e-05, 5.244112981017679e-05, 5.244112981017679e-05, 5.244112981017679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.244112981017679e-05

Optimization complete. Final v2v error: 5.939476490020752 mm

Highest mean error: 6.5977044105529785 mm for frame 7

Lowest mean error: 5.656145095825195 mm for frame 62

Saving results

Total time: 900.586110830307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0014
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549589
Iteration 2/25 | Loss: 0.00150667
Iteration 3/25 | Loss: 0.00118343
Iteration 4/25 | Loss: 0.00113220
Iteration 5/25 | Loss: 0.00116823
Iteration 6/25 | Loss: 0.00116815
Iteration 7/25 | Loss: 0.00111778
Iteration 8/25 | Loss: 0.00108788
Iteration 9/25 | Loss: 0.00107854
Iteration 10/25 | Loss: 0.00107571
Iteration 11/25 | Loss: 0.00107807
Iteration 12/25 | Loss: 0.00107528
Iteration 13/25 | Loss: 0.00107389
Iteration 14/25 | Loss: 0.00107337
Iteration 15/25 | Loss: 0.00107327
Iteration 16/25 | Loss: 0.00107327
Iteration 17/25 | Loss: 0.00107327
Iteration 18/25 | Loss: 0.00107327
Iteration 19/25 | Loss: 0.00107327
Iteration 20/25 | Loss: 0.00107327
Iteration 21/25 | Loss: 0.00107327
Iteration 22/25 | Loss: 0.00107327
Iteration 23/25 | Loss: 0.00107327
Iteration 24/25 | Loss: 0.00107327
Iteration 25/25 | Loss: 0.00107327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38322687
Iteration 2/25 | Loss: 0.00060307
Iteration 3/25 | Loss: 0.00060306
Iteration 4/25 | Loss: 0.00060306
Iteration 5/25 | Loss: 0.00060306
Iteration 6/25 | Loss: 0.00060306
Iteration 7/25 | Loss: 0.00060306
Iteration 8/25 | Loss: 0.00060306
Iteration 9/25 | Loss: 0.00060306
Iteration 10/25 | Loss: 0.00060306
Iteration 11/25 | Loss: 0.00060306
Iteration 12/25 | Loss: 0.00060306
Iteration 13/25 | Loss: 0.00060306
Iteration 14/25 | Loss: 0.00060306
Iteration 15/25 | Loss: 0.00060306
Iteration 16/25 | Loss: 0.00060306
Iteration 17/25 | Loss: 0.00060306
Iteration 18/25 | Loss: 0.00060306
Iteration 19/25 | Loss: 0.00060306
Iteration 20/25 | Loss: 0.00060306
Iteration 21/25 | Loss: 0.00060306
Iteration 22/25 | Loss: 0.00060306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006030603544786572, 0.0006030603544786572, 0.0006030603544786572, 0.0006030603544786572, 0.0006030603544786572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006030603544786572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060306
Iteration 2/1000 | Loss: 0.00077522
Iteration 3/1000 | Loss: 0.00027837
Iteration 4/1000 | Loss: 0.00071105
Iteration 5/1000 | Loss: 0.00064690
Iteration 6/1000 | Loss: 0.00026915
Iteration 7/1000 | Loss: 0.00006381
Iteration 8/1000 | Loss: 0.00004876
Iteration 9/1000 | Loss: 0.00004218
Iteration 10/1000 | Loss: 0.00003834
Iteration 11/1000 | Loss: 0.00003505
Iteration 12/1000 | Loss: 0.00003349
Iteration 13/1000 | Loss: 0.00003250
Iteration 14/1000 | Loss: 0.00003190
Iteration 15/1000 | Loss: 0.00003156
Iteration 16/1000 | Loss: 0.00003124
Iteration 17/1000 | Loss: 0.00003092
Iteration 18/1000 | Loss: 0.00003063
Iteration 19/1000 | Loss: 0.00003044
Iteration 20/1000 | Loss: 0.00003024
Iteration 21/1000 | Loss: 0.00003007
Iteration 22/1000 | Loss: 0.00003005
Iteration 23/1000 | Loss: 0.00002999
Iteration 24/1000 | Loss: 0.00002996
Iteration 25/1000 | Loss: 0.00002996
Iteration 26/1000 | Loss: 0.00002995
Iteration 27/1000 | Loss: 0.00002995
Iteration 28/1000 | Loss: 0.00002995
Iteration 29/1000 | Loss: 0.00002995
Iteration 30/1000 | Loss: 0.00002994
Iteration 31/1000 | Loss: 0.00002994
Iteration 32/1000 | Loss: 0.00002994
Iteration 33/1000 | Loss: 0.00002994
Iteration 34/1000 | Loss: 0.00002994
Iteration 35/1000 | Loss: 0.00002994
Iteration 36/1000 | Loss: 0.00002994
Iteration 37/1000 | Loss: 0.00002993
Iteration 38/1000 | Loss: 0.00002993
Iteration 39/1000 | Loss: 0.00002993
Iteration 40/1000 | Loss: 0.00002993
Iteration 41/1000 | Loss: 0.00002992
Iteration 42/1000 | Loss: 0.00002992
Iteration 43/1000 | Loss: 0.00002991
Iteration 44/1000 | Loss: 0.00002991
Iteration 45/1000 | Loss: 0.00002991
Iteration 46/1000 | Loss: 0.00002991
Iteration 47/1000 | Loss: 0.00002990
Iteration 48/1000 | Loss: 0.00002990
Iteration 49/1000 | Loss: 0.00002990
Iteration 50/1000 | Loss: 0.00002990
Iteration 51/1000 | Loss: 0.00002990
Iteration 52/1000 | Loss: 0.00002989
Iteration 53/1000 | Loss: 0.00002989
Iteration 54/1000 | Loss: 0.00002989
Iteration 55/1000 | Loss: 0.00002988
Iteration 56/1000 | Loss: 0.00002987
Iteration 57/1000 | Loss: 0.00002987
Iteration 58/1000 | Loss: 0.00002986
Iteration 59/1000 | Loss: 0.00002985
Iteration 60/1000 | Loss: 0.00002985
Iteration 61/1000 | Loss: 0.00002985
Iteration 62/1000 | Loss: 0.00002984
Iteration 63/1000 | Loss: 0.00002984
Iteration 64/1000 | Loss: 0.00002984
Iteration 65/1000 | Loss: 0.00002984
Iteration 66/1000 | Loss: 0.00002984
Iteration 67/1000 | Loss: 0.00002984
Iteration 68/1000 | Loss: 0.00002984
Iteration 69/1000 | Loss: 0.00002984
Iteration 70/1000 | Loss: 0.00002984
Iteration 71/1000 | Loss: 0.00002984
Iteration 72/1000 | Loss: 0.00002984
Iteration 73/1000 | Loss: 0.00002984
Iteration 74/1000 | Loss: 0.00002984
Iteration 75/1000 | Loss: 0.00002983
Iteration 76/1000 | Loss: 0.00002983
Iteration 77/1000 | Loss: 0.00002983
Iteration 78/1000 | Loss: 0.00002983
Iteration 79/1000 | Loss: 0.00002982
Iteration 80/1000 | Loss: 0.00002982
Iteration 81/1000 | Loss: 0.00002982
Iteration 82/1000 | Loss: 0.00002982
Iteration 83/1000 | Loss: 0.00002982
Iteration 84/1000 | Loss: 0.00002981
Iteration 85/1000 | Loss: 0.00002981
Iteration 86/1000 | Loss: 0.00002981
Iteration 87/1000 | Loss: 0.00002981
Iteration 88/1000 | Loss: 0.00002981
Iteration 89/1000 | Loss: 0.00002981
Iteration 90/1000 | Loss: 0.00002981
Iteration 91/1000 | Loss: 0.00002980
Iteration 92/1000 | Loss: 0.00002980
Iteration 93/1000 | Loss: 0.00002980
Iteration 94/1000 | Loss: 0.00002980
Iteration 95/1000 | Loss: 0.00002980
Iteration 96/1000 | Loss: 0.00002980
Iteration 97/1000 | Loss: 0.00002980
Iteration 98/1000 | Loss: 0.00002979
Iteration 99/1000 | Loss: 0.00002979
Iteration 100/1000 | Loss: 0.00002979
Iteration 101/1000 | Loss: 0.00002979
Iteration 102/1000 | Loss: 0.00002979
Iteration 103/1000 | Loss: 0.00002979
Iteration 104/1000 | Loss: 0.00002979
Iteration 105/1000 | Loss: 0.00002978
Iteration 106/1000 | Loss: 0.00002978
Iteration 107/1000 | Loss: 0.00002978
Iteration 108/1000 | Loss: 0.00002978
Iteration 109/1000 | Loss: 0.00002977
Iteration 110/1000 | Loss: 0.00002977
Iteration 111/1000 | Loss: 0.00002977
Iteration 112/1000 | Loss: 0.00002977
Iteration 113/1000 | Loss: 0.00002977
Iteration 114/1000 | Loss: 0.00002976
Iteration 115/1000 | Loss: 0.00002976
Iteration 116/1000 | Loss: 0.00002976
Iteration 117/1000 | Loss: 0.00002975
Iteration 118/1000 | Loss: 0.00002975
Iteration 119/1000 | Loss: 0.00002975
Iteration 120/1000 | Loss: 0.00002975
Iteration 121/1000 | Loss: 0.00002975
Iteration 122/1000 | Loss: 0.00002975
Iteration 123/1000 | Loss: 0.00002975
Iteration 124/1000 | Loss: 0.00002975
Iteration 125/1000 | Loss: 0.00002975
Iteration 126/1000 | Loss: 0.00002975
Iteration 127/1000 | Loss: 0.00002975
Iteration 128/1000 | Loss: 0.00002975
Iteration 129/1000 | Loss: 0.00002975
Iteration 130/1000 | Loss: 0.00002975
Iteration 131/1000 | Loss: 0.00002975
Iteration 132/1000 | Loss: 0.00002974
Iteration 133/1000 | Loss: 0.00002974
Iteration 134/1000 | Loss: 0.00002974
Iteration 135/1000 | Loss: 0.00002974
Iteration 136/1000 | Loss: 0.00002974
Iteration 137/1000 | Loss: 0.00002974
Iteration 138/1000 | Loss: 0.00002974
Iteration 139/1000 | Loss: 0.00002974
Iteration 140/1000 | Loss: 0.00002974
Iteration 141/1000 | Loss: 0.00002974
Iteration 142/1000 | Loss: 0.00002974
Iteration 143/1000 | Loss: 0.00002974
Iteration 144/1000 | Loss: 0.00002974
Iteration 145/1000 | Loss: 0.00011159
Iteration 146/1000 | Loss: 0.00041042
Iteration 147/1000 | Loss: 0.00016258
Iteration 148/1000 | Loss: 0.00003438
Iteration 149/1000 | Loss: 0.00003077
Iteration 150/1000 | Loss: 0.00003007
Iteration 151/1000 | Loss: 0.00002985
Iteration 152/1000 | Loss: 0.00002966
Iteration 153/1000 | Loss: 0.00002954
Iteration 154/1000 | Loss: 0.00002954
Iteration 155/1000 | Loss: 0.00002953
Iteration 156/1000 | Loss: 0.00002953
Iteration 157/1000 | Loss: 0.00002953
Iteration 158/1000 | Loss: 0.00002952
Iteration 159/1000 | Loss: 0.00002952
Iteration 160/1000 | Loss: 0.00002952
Iteration 161/1000 | Loss: 0.00002952
Iteration 162/1000 | Loss: 0.00002951
Iteration 163/1000 | Loss: 0.00002951
Iteration 164/1000 | Loss: 0.00002951
Iteration 165/1000 | Loss: 0.00002951
Iteration 166/1000 | Loss: 0.00002951
Iteration 167/1000 | Loss: 0.00002951
Iteration 168/1000 | Loss: 0.00002951
Iteration 169/1000 | Loss: 0.00002950
Iteration 170/1000 | Loss: 0.00002950
Iteration 171/1000 | Loss: 0.00002950
Iteration 172/1000 | Loss: 0.00002950
Iteration 173/1000 | Loss: 0.00002950
Iteration 174/1000 | Loss: 0.00002950
Iteration 175/1000 | Loss: 0.00002950
Iteration 176/1000 | Loss: 0.00002950
Iteration 177/1000 | Loss: 0.00002950
Iteration 178/1000 | Loss: 0.00002950
Iteration 179/1000 | Loss: 0.00002950
Iteration 180/1000 | Loss: 0.00002950
Iteration 181/1000 | Loss: 0.00002950
Iteration 182/1000 | Loss: 0.00002950
Iteration 183/1000 | Loss: 0.00002950
Iteration 184/1000 | Loss: 0.00002949
Iteration 185/1000 | Loss: 0.00002949
Iteration 186/1000 | Loss: 0.00002949
Iteration 187/1000 | Loss: 0.00002949
Iteration 188/1000 | Loss: 0.00002949
Iteration 189/1000 | Loss: 0.00002949
Iteration 190/1000 | Loss: 0.00002949
Iteration 191/1000 | Loss: 0.00002949
Iteration 192/1000 | Loss: 0.00002949
Iteration 193/1000 | Loss: 0.00002949
Iteration 194/1000 | Loss: 0.00002949
Iteration 195/1000 | Loss: 0.00002949
Iteration 196/1000 | Loss: 0.00002949
Iteration 197/1000 | Loss: 0.00002949
Iteration 198/1000 | Loss: 0.00002949
Iteration 199/1000 | Loss: 0.00002949
Iteration 200/1000 | Loss: 0.00002949
Iteration 201/1000 | Loss: 0.00002949
Iteration 202/1000 | Loss: 0.00002949
Iteration 203/1000 | Loss: 0.00002949
Iteration 204/1000 | Loss: 0.00002949
Iteration 205/1000 | Loss: 0.00002948
Iteration 206/1000 | Loss: 0.00002948
Iteration 207/1000 | Loss: 0.00002948
Iteration 208/1000 | Loss: 0.00002948
Iteration 209/1000 | Loss: 0.00002948
Iteration 210/1000 | Loss: 0.00002948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.9484695915016346e-05, 2.9484695915016346e-05, 2.9484695915016346e-05, 2.9484695915016346e-05, 2.9484695915016346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9484695915016346e-05

Optimization complete. Final v2v error: 4.4317240715026855 mm

Highest mean error: 6.122005462646484 mm for frame 11

Lowest mean error: 4.130961894989014 mm for frame 91

Saving results

Total time: 2720.564641714096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0001
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821874
Iteration 2/25 | Loss: 0.00188892
Iteration 3/25 | Loss: 0.00113110
Iteration 4/25 | Loss: 0.00103344
Iteration 5/25 | Loss: 0.00099467
Iteration 6/25 | Loss: 0.00103570
Iteration 7/25 | Loss: 0.00098624
Iteration 8/25 | Loss: 0.00094234
Iteration 9/25 | Loss: 0.00093031
Iteration 10/25 | Loss: 0.00092802
Iteration 11/25 | Loss: 0.00093390
Iteration 12/25 | Loss: 0.00093091
Iteration 13/25 | Loss: 0.00092512
Iteration 14/25 | Loss: 0.00092182
Iteration 15/25 | Loss: 0.00092083
Iteration 16/25 | Loss: 0.00092032
Iteration 17/25 | Loss: 0.00091999
Iteration 18/25 | Loss: 0.00092033
Iteration 19/25 | Loss: 0.00092001
Iteration 20/25 | Loss: 0.00091964
Iteration 21/25 | Loss: 0.00091954
Iteration 22/25 | Loss: 0.00091954
Iteration 23/25 | Loss: 0.00091954
Iteration 24/25 | Loss: 0.00091953
Iteration 25/25 | Loss: 0.00091953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54378414
Iteration 2/25 | Loss: 0.00073908
Iteration 3/25 | Loss: 0.00073903
Iteration 4/25 | Loss: 0.00073903
Iteration 5/25 | Loss: 0.00073903
Iteration 6/25 | Loss: 0.00073903
Iteration 7/25 | Loss: 0.00073903
Iteration 8/25 | Loss: 0.00073903
Iteration 9/25 | Loss: 0.00073903
Iteration 10/25 | Loss: 0.00073903
Iteration 11/25 | Loss: 0.00073903
Iteration 12/25 | Loss: 0.00073903
Iteration 13/25 | Loss: 0.00073903
Iteration 14/25 | Loss: 0.00073903
Iteration 15/25 | Loss: 0.00073903
Iteration 16/25 | Loss: 0.00073903
Iteration 17/25 | Loss: 0.00073903
Iteration 18/25 | Loss: 0.00073903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007390258833765984, 0.0007390258833765984, 0.0007390258833765984, 0.0007390258833765984, 0.0007390258833765984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007390258833765984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073903
Iteration 2/1000 | Loss: 0.00010727
Iteration 3/1000 | Loss: 0.00006329
Iteration 4/1000 | Loss: 0.00003977
Iteration 5/1000 | Loss: 0.00003270
Iteration 6/1000 | Loss: 0.00006797
Iteration 7/1000 | Loss: 0.00003405
Iteration 8/1000 | Loss: 0.00002805
Iteration 9/1000 | Loss: 0.00003245
Iteration 10/1000 | Loss: 0.00002557
Iteration 11/1000 | Loss: 0.00002653
Iteration 12/1000 | Loss: 0.00002548
Iteration 13/1000 | Loss: 0.00002456
Iteration 14/1000 | Loss: 0.00002429
Iteration 15/1000 | Loss: 0.00003008
Iteration 16/1000 | Loss: 0.00002804
Iteration 17/1000 | Loss: 0.00002896
Iteration 18/1000 | Loss: 0.00002505
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002334
Iteration 21/1000 | Loss: 0.00002309
Iteration 22/1000 | Loss: 0.00002305
Iteration 23/1000 | Loss: 0.00002300
Iteration 24/1000 | Loss: 0.00002298
Iteration 25/1000 | Loss: 0.00002298
Iteration 26/1000 | Loss: 0.00002297
Iteration 27/1000 | Loss: 0.00002297
Iteration 28/1000 | Loss: 0.00002297
Iteration 29/1000 | Loss: 0.00002296
Iteration 30/1000 | Loss: 0.00002285
Iteration 31/1000 | Loss: 0.00002278
Iteration 32/1000 | Loss: 0.00002276
Iteration 33/1000 | Loss: 0.00002276
Iteration 34/1000 | Loss: 0.00002271
Iteration 35/1000 | Loss: 0.00002271
Iteration 36/1000 | Loss: 0.00002271
Iteration 37/1000 | Loss: 0.00002270
Iteration 38/1000 | Loss: 0.00002270
Iteration 39/1000 | Loss: 0.00002270
Iteration 40/1000 | Loss: 0.00002270
Iteration 41/1000 | Loss: 0.00002270
Iteration 42/1000 | Loss: 0.00002270
Iteration 43/1000 | Loss: 0.00002270
Iteration 44/1000 | Loss: 0.00002270
Iteration 45/1000 | Loss: 0.00002269
Iteration 46/1000 | Loss: 0.00002269
Iteration 47/1000 | Loss: 0.00002268
Iteration 48/1000 | Loss: 0.00002266
Iteration 49/1000 | Loss: 0.00002266
Iteration 50/1000 | Loss: 0.00002266
Iteration 51/1000 | Loss: 0.00002266
Iteration 52/1000 | Loss: 0.00002266
Iteration 53/1000 | Loss: 0.00002266
Iteration 54/1000 | Loss: 0.00002266
Iteration 55/1000 | Loss: 0.00002266
Iteration 56/1000 | Loss: 0.00002266
Iteration 57/1000 | Loss: 0.00002265
Iteration 58/1000 | Loss: 0.00002265
Iteration 59/1000 | Loss: 0.00002265
Iteration 60/1000 | Loss: 0.00002265
Iteration 61/1000 | Loss: 0.00002265
Iteration 62/1000 | Loss: 0.00002265
Iteration 63/1000 | Loss: 0.00002265
Iteration 64/1000 | Loss: 0.00002265
Iteration 65/1000 | Loss: 0.00002265
Iteration 66/1000 | Loss: 0.00002264
Iteration 67/1000 | Loss: 0.00002264
Iteration 68/1000 | Loss: 0.00002263
Iteration 69/1000 | Loss: 0.00002263
Iteration 70/1000 | Loss: 0.00002263
Iteration 71/1000 | Loss: 0.00002263
Iteration 72/1000 | Loss: 0.00002263
Iteration 73/1000 | Loss: 0.00002263
Iteration 74/1000 | Loss: 0.00002263
Iteration 75/1000 | Loss: 0.00002262
Iteration 76/1000 | Loss: 0.00002262
Iteration 77/1000 | Loss: 0.00002262
Iteration 78/1000 | Loss: 0.00002262
Iteration 79/1000 | Loss: 0.00002262
Iteration 80/1000 | Loss: 0.00002262
Iteration 81/1000 | Loss: 0.00002262
Iteration 82/1000 | Loss: 0.00002262
Iteration 83/1000 | Loss: 0.00002262
Iteration 84/1000 | Loss: 0.00002262
Iteration 85/1000 | Loss: 0.00002262
Iteration 86/1000 | Loss: 0.00002262
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002262
Iteration 93/1000 | Loss: 0.00002262
Iteration 94/1000 | Loss: 0.00002262
Iteration 95/1000 | Loss: 0.00002262
Iteration 96/1000 | Loss: 0.00002262
Iteration 97/1000 | Loss: 0.00002262
Iteration 98/1000 | Loss: 0.00002262
Iteration 99/1000 | Loss: 0.00002262
Iteration 100/1000 | Loss: 0.00002262
Iteration 101/1000 | Loss: 0.00002262
Iteration 102/1000 | Loss: 0.00002262
Iteration 103/1000 | Loss: 0.00002262
Iteration 104/1000 | Loss: 0.00002262
Iteration 105/1000 | Loss: 0.00002262
Iteration 106/1000 | Loss: 0.00002262
Iteration 107/1000 | Loss: 0.00002262
Iteration 108/1000 | Loss: 0.00002262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.2620650270255283e-05, 2.2620650270255283e-05, 2.2620650270255283e-05, 2.2620650270255283e-05, 2.2620650270255283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2620650270255283e-05

Optimization complete. Final v2v error: 4.007538795471191 mm

Highest mean error: 10.723790168762207 mm for frame 12

Lowest mean error: 3.6158816814422607 mm for frame 177

Saving results

Total time: 2869.9635519981384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0011
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01123214
Iteration 2/25 | Loss: 0.00191908
Iteration 3/25 | Loss: 0.00139758
Iteration 4/25 | Loss: 0.00121142
Iteration 5/25 | Loss: 0.00117742
Iteration 6/25 | Loss: 0.00111016
Iteration 7/25 | Loss: 0.00106278
Iteration 8/25 | Loss: 0.00105914
Iteration 9/25 | Loss: 0.00105664
Iteration 10/25 | Loss: 0.00101170
Iteration 11/25 | Loss: 0.00099472
Iteration 12/25 | Loss: 0.00098885
Iteration 13/25 | Loss: 0.00098142
Iteration 14/25 | Loss: 0.00098867
Iteration 15/25 | Loss: 0.00097502
Iteration 16/25 | Loss: 0.00097742
Iteration 17/25 | Loss: 0.00097077
Iteration 18/25 | Loss: 0.00096878
Iteration 19/25 | Loss: 0.00096823
Iteration 20/25 | Loss: 0.00097040
Iteration 21/25 | Loss: 0.00096502
Iteration 22/25 | Loss: 0.00096316
Iteration 23/25 | Loss: 0.00096241
Iteration 24/25 | Loss: 0.00096169
Iteration 25/25 | Loss: 0.00096190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38064766
Iteration 2/25 | Loss: 0.00151130
Iteration 3/25 | Loss: 0.00133566
Iteration 4/25 | Loss: 0.00133566
Iteration 5/25 | Loss: 0.00133565
Iteration 6/25 | Loss: 0.00133565
Iteration 7/25 | Loss: 0.00133565
Iteration 8/25 | Loss: 0.00133565
Iteration 9/25 | Loss: 0.00133565
Iteration 10/25 | Loss: 0.00133565
Iteration 11/25 | Loss: 0.00133565
Iteration 12/25 | Loss: 0.00133565
Iteration 13/25 | Loss: 0.00133565
Iteration 14/25 | Loss: 0.00133565
Iteration 15/25 | Loss: 0.00133565
Iteration 16/25 | Loss: 0.00133565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013356533600017428, 0.0013356533600017428, 0.0013356533600017428, 0.0013356533600017428, 0.0013356533600017428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013356533600017428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133565
Iteration 2/1000 | Loss: 0.00053295
Iteration 3/1000 | Loss: 0.00068053
Iteration 4/1000 | Loss: 0.00129042
Iteration 5/1000 | Loss: 0.00064890
Iteration 6/1000 | Loss: 0.00105551
Iteration 7/1000 | Loss: 0.00120896
Iteration 8/1000 | Loss: 0.00071180
Iteration 9/1000 | Loss: 0.00069617
Iteration 10/1000 | Loss: 0.00040784
Iteration 11/1000 | Loss: 0.00072517
Iteration 12/1000 | Loss: 0.00073383
Iteration 13/1000 | Loss: 0.00101200
Iteration 14/1000 | Loss: 0.00112091
Iteration 15/1000 | Loss: 0.00032042
Iteration 16/1000 | Loss: 0.00066258
Iteration 17/1000 | Loss: 0.00047959
Iteration 18/1000 | Loss: 0.00063908
Iteration 19/1000 | Loss: 0.00090580
Iteration 20/1000 | Loss: 0.00108379
Iteration 21/1000 | Loss: 0.00084934
Iteration 22/1000 | Loss: 0.00051576
Iteration 23/1000 | Loss: 0.00093218
Iteration 24/1000 | Loss: 0.00018602
Iteration 25/1000 | Loss: 0.00026338
Iteration 26/1000 | Loss: 0.00022433
Iteration 27/1000 | Loss: 0.00033985
Iteration 28/1000 | Loss: 0.00049168
Iteration 29/1000 | Loss: 0.00034602
Iteration 30/1000 | Loss: 0.00035224
Iteration 31/1000 | Loss: 0.00086461
Iteration 32/1000 | Loss: 0.00036541
Iteration 33/1000 | Loss: 0.00032474
Iteration 34/1000 | Loss: 0.00031634
Iteration 35/1000 | Loss: 0.00027376
Iteration 36/1000 | Loss: 0.00031193
Iteration 37/1000 | Loss: 0.00048490
Iteration 38/1000 | Loss: 0.00012776
Iteration 39/1000 | Loss: 0.00008430
Iteration 40/1000 | Loss: 0.00006646
Iteration 41/1000 | Loss: 0.00008983
Iteration 42/1000 | Loss: 0.00043304
Iteration 43/1000 | Loss: 0.00032906
Iteration 44/1000 | Loss: 0.00023063
Iteration 45/1000 | Loss: 0.00056226
Iteration 46/1000 | Loss: 0.00070233
Iteration 47/1000 | Loss: 0.00040195
Iteration 48/1000 | Loss: 0.00061620
Iteration 49/1000 | Loss: 0.00029085
Iteration 50/1000 | Loss: 0.00073987
Iteration 51/1000 | Loss: 0.00049886
Iteration 52/1000 | Loss: 0.00008410
Iteration 53/1000 | Loss: 0.00007474
Iteration 54/1000 | Loss: 0.00036668
Iteration 55/1000 | Loss: 0.00063250
Iteration 56/1000 | Loss: 0.00061274
Iteration 57/1000 | Loss: 0.00032831
Iteration 58/1000 | Loss: 0.00006649
Iteration 59/1000 | Loss: 0.00013809
Iteration 60/1000 | Loss: 0.00007010
Iteration 61/1000 | Loss: 0.00021448
Iteration 62/1000 | Loss: 0.00010129
Iteration 63/1000 | Loss: 0.00008171
Iteration 64/1000 | Loss: 0.00007048
Iteration 65/1000 | Loss: 0.00005071
Iteration 66/1000 | Loss: 0.00024447
Iteration 67/1000 | Loss: 0.00020386
Iteration 68/1000 | Loss: 0.00006764
Iteration 69/1000 | Loss: 0.00023114
Iteration 70/1000 | Loss: 0.00027158
Iteration 71/1000 | Loss: 0.00005596
Iteration 72/1000 | Loss: 0.00013247
Iteration 73/1000 | Loss: 0.00016498
Iteration 74/1000 | Loss: 0.00018786
Iteration 75/1000 | Loss: 0.00012128
Iteration 76/1000 | Loss: 0.00008260
Iteration 77/1000 | Loss: 0.00037761
Iteration 78/1000 | Loss: 0.00075579
Iteration 79/1000 | Loss: 0.00033952
Iteration 80/1000 | Loss: 0.00031467
Iteration 81/1000 | Loss: 0.00030248
Iteration 82/1000 | Loss: 0.00099939
Iteration 83/1000 | Loss: 0.00012774
Iteration 84/1000 | Loss: 0.00059495
Iteration 85/1000 | Loss: 0.00007541
Iteration 86/1000 | Loss: 0.00069779
Iteration 87/1000 | Loss: 0.00074844
Iteration 88/1000 | Loss: 0.00029673
Iteration 89/1000 | Loss: 0.00022413
Iteration 90/1000 | Loss: 0.00013410
Iteration 91/1000 | Loss: 0.00012172
Iteration 92/1000 | Loss: 0.00003865
Iteration 93/1000 | Loss: 0.00007452
Iteration 94/1000 | Loss: 0.00004271
Iteration 95/1000 | Loss: 0.00003372
Iteration 96/1000 | Loss: 0.00004414
Iteration 97/1000 | Loss: 0.00002697
Iteration 98/1000 | Loss: 0.00003370
Iteration 99/1000 | Loss: 0.00003195
Iteration 100/1000 | Loss: 0.00003385
Iteration 101/1000 | Loss: 0.00003637
Iteration 102/1000 | Loss: 0.00023715
Iteration 103/1000 | Loss: 0.00007541
Iteration 104/1000 | Loss: 0.00030192
Iteration 105/1000 | Loss: 0.00004789
Iteration 106/1000 | Loss: 0.00009805
Iteration 107/1000 | Loss: 0.00003677
Iteration 108/1000 | Loss: 0.00004405
Iteration 109/1000 | Loss: 0.00003259
Iteration 110/1000 | Loss: 0.00007402
Iteration 111/1000 | Loss: 0.00009172
Iteration 112/1000 | Loss: 0.00003146
Iteration 113/1000 | Loss: 0.00004905
Iteration 114/1000 | Loss: 0.00003564
Iteration 115/1000 | Loss: 0.00003116
Iteration 116/1000 | Loss: 0.00003304
Iteration 117/1000 | Loss: 0.00002808
Iteration 118/1000 | Loss: 0.00003278
Iteration 119/1000 | Loss: 0.00002996
Iteration 120/1000 | Loss: 0.00003108
Iteration 121/1000 | Loss: 0.00002952
Iteration 122/1000 | Loss: 0.00003016
Iteration 123/1000 | Loss: 0.00003602
Iteration 124/1000 | Loss: 0.00003483
Iteration 125/1000 | Loss: 0.00004160
Iteration 126/1000 | Loss: 0.00003263
Iteration 127/1000 | Loss: 0.00005845
Iteration 128/1000 | Loss: 0.00006770
Iteration 129/1000 | Loss: 0.00003283
Iteration 130/1000 | Loss: 0.00002808
Iteration 131/1000 | Loss: 0.00003154
Iteration 132/1000 | Loss: 0.00003270
Iteration 133/1000 | Loss: 0.00003959
Iteration 134/1000 | Loss: 0.00003944
Iteration 135/1000 | Loss: 0.00003207
Iteration 136/1000 | Loss: 0.00004339
Iteration 137/1000 | Loss: 0.00003296
Iteration 138/1000 | Loss: 0.00003505
Iteration 139/1000 | Loss: 0.00003852
Iteration 140/1000 | Loss: 0.00003143
Iteration 141/1000 | Loss: 0.00003410
Iteration 142/1000 | Loss: 0.00002768
Iteration 143/1000 | Loss: 0.00002988
Iteration 144/1000 | Loss: 0.00002980
Iteration 145/1000 | Loss: 0.00002834
Iteration 146/1000 | Loss: 0.00003901
Iteration 147/1000 | Loss: 0.00006673
Iteration 148/1000 | Loss: 0.00010413
Iteration 149/1000 | Loss: 0.00002745
Iteration 150/1000 | Loss: 0.00010111
Iteration 151/1000 | Loss: 0.00004558
Iteration 152/1000 | Loss: 0.00002145
Iteration 153/1000 | Loss: 0.00001986
Iteration 154/1000 | Loss: 0.00005730
Iteration 155/1000 | Loss: 0.00001896
Iteration 156/1000 | Loss: 0.00002285
Iteration 157/1000 | Loss: 0.00001894
Iteration 158/1000 | Loss: 0.00003806
Iteration 159/1000 | Loss: 0.00005403
Iteration 160/1000 | Loss: 0.00001825
Iteration 161/1000 | Loss: 0.00001818
Iteration 162/1000 | Loss: 0.00002785
Iteration 163/1000 | Loss: 0.00001812
Iteration 164/1000 | Loss: 0.00001812
Iteration 165/1000 | Loss: 0.00001812
Iteration 166/1000 | Loss: 0.00001812
Iteration 167/1000 | Loss: 0.00001811
Iteration 168/1000 | Loss: 0.00001811
Iteration 169/1000 | Loss: 0.00001811
Iteration 170/1000 | Loss: 0.00001811
Iteration 171/1000 | Loss: 0.00001811
Iteration 172/1000 | Loss: 0.00001811
Iteration 173/1000 | Loss: 0.00001810
Iteration 174/1000 | Loss: 0.00001810
Iteration 175/1000 | Loss: 0.00001810
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001809
Iteration 179/1000 | Loss: 0.00001809
Iteration 180/1000 | Loss: 0.00001809
Iteration 181/1000 | Loss: 0.00001809
Iteration 182/1000 | Loss: 0.00001809
Iteration 183/1000 | Loss: 0.00001809
Iteration 184/1000 | Loss: 0.00001809
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001809
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00001809
Iteration 189/1000 | Loss: 0.00001809
Iteration 190/1000 | Loss: 0.00001809
Iteration 191/1000 | Loss: 0.00001809
Iteration 192/1000 | Loss: 0.00001809
Iteration 193/1000 | Loss: 0.00001809
Iteration 194/1000 | Loss: 0.00001809
Iteration 195/1000 | Loss: 0.00001809
Iteration 196/1000 | Loss: 0.00001809
Iteration 197/1000 | Loss: 0.00001809
Iteration 198/1000 | Loss: 0.00001809
Iteration 199/1000 | Loss: 0.00001809
Iteration 200/1000 | Loss: 0.00001809
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Iteration 203/1000 | Loss: 0.00001809
Iteration 204/1000 | Loss: 0.00001809
Iteration 205/1000 | Loss: 0.00001809
Iteration 206/1000 | Loss: 0.00001809
Iteration 207/1000 | Loss: 0.00001809
Iteration 208/1000 | Loss: 0.00001809
Iteration 209/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.8091692254529335e-05, 1.8091692254529335e-05, 1.8091692254529335e-05, 1.8091692254529335e-05, 1.8091692254529335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8091692254529335e-05

Optimization complete. Final v2v error: 3.604567289352417 mm

Highest mean error: 8.5789794921875 mm for frame 210

Lowest mean error: 3.119386672973633 mm for frame 0

Saving results

Total time: 8381.084646701813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875223
Iteration 2/25 | Loss: 0.00110681
Iteration 3/25 | Loss: 0.00099096
Iteration 4/25 | Loss: 0.00096029
Iteration 5/25 | Loss: 0.00094762
Iteration 6/25 | Loss: 0.00094506
Iteration 7/25 | Loss: 0.00094462
Iteration 8/25 | Loss: 0.00094462
Iteration 9/25 | Loss: 0.00094462
Iteration 10/25 | Loss: 0.00094462
Iteration 11/25 | Loss: 0.00094462
Iteration 12/25 | Loss: 0.00094462
Iteration 13/25 | Loss: 0.00094462
Iteration 14/25 | Loss: 0.00094462
Iteration 15/25 | Loss: 0.00094462
Iteration 16/25 | Loss: 0.00094462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009446156909689307, 0.0009446156909689307, 0.0009446156909689307, 0.0009446156909689307, 0.0009446156909689307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009446156909689307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41748381
Iteration 2/25 | Loss: 0.00043132
Iteration 3/25 | Loss: 0.00043132
Iteration 4/25 | Loss: 0.00043131
Iteration 5/25 | Loss: 0.00043131
Iteration 6/25 | Loss: 0.00043131
Iteration 7/25 | Loss: 0.00043131
Iteration 8/25 | Loss: 0.00043131
Iteration 9/25 | Loss: 0.00043131
Iteration 10/25 | Loss: 0.00043131
Iteration 11/25 | Loss: 0.00043131
Iteration 12/25 | Loss: 0.00043131
Iteration 13/25 | Loss: 0.00043131
Iteration 14/25 | Loss: 0.00043131
Iteration 15/25 | Loss: 0.00043131
Iteration 16/25 | Loss: 0.00043131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004313132376410067, 0.0004313132376410067, 0.0004313132376410067, 0.0004313132376410067, 0.0004313132376410067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004313132376410067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043131
Iteration 2/1000 | Loss: 0.00004362
Iteration 3/1000 | Loss: 0.00003332
Iteration 4/1000 | Loss: 0.00002956
Iteration 5/1000 | Loss: 0.00002828
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002660
Iteration 8/1000 | Loss: 0.00002617
Iteration 9/1000 | Loss: 0.00002575
Iteration 10/1000 | Loss: 0.00002549
Iteration 11/1000 | Loss: 0.00002534
Iteration 12/1000 | Loss: 0.00002533
Iteration 13/1000 | Loss: 0.00002533
Iteration 14/1000 | Loss: 0.00002532
Iteration 15/1000 | Loss: 0.00002532
Iteration 16/1000 | Loss: 0.00002531
Iteration 17/1000 | Loss: 0.00002529
Iteration 18/1000 | Loss: 0.00002529
Iteration 19/1000 | Loss: 0.00002529
Iteration 20/1000 | Loss: 0.00002529
Iteration 21/1000 | Loss: 0.00002529
Iteration 22/1000 | Loss: 0.00002529
Iteration 23/1000 | Loss: 0.00002529
Iteration 24/1000 | Loss: 0.00002529
Iteration 25/1000 | Loss: 0.00002529
Iteration 26/1000 | Loss: 0.00002528
Iteration 27/1000 | Loss: 0.00002528
Iteration 28/1000 | Loss: 0.00002528
Iteration 29/1000 | Loss: 0.00002527
Iteration 30/1000 | Loss: 0.00002527
Iteration 31/1000 | Loss: 0.00002527
Iteration 32/1000 | Loss: 0.00002527
Iteration 33/1000 | Loss: 0.00002527
Iteration 34/1000 | Loss: 0.00002527
Iteration 35/1000 | Loss: 0.00002526
Iteration 36/1000 | Loss: 0.00002526
Iteration 37/1000 | Loss: 0.00002526
Iteration 38/1000 | Loss: 0.00002526
Iteration 39/1000 | Loss: 0.00002526
Iteration 40/1000 | Loss: 0.00002526
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002526
Iteration 43/1000 | Loss: 0.00002526
Iteration 44/1000 | Loss: 0.00002526
Iteration 45/1000 | Loss: 0.00002526
Iteration 46/1000 | Loss: 0.00002526
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002526
Iteration 49/1000 | Loss: 0.00002526
Iteration 50/1000 | Loss: 0.00002526
Iteration 51/1000 | Loss: 0.00002525
Iteration 52/1000 | Loss: 0.00002525
Iteration 53/1000 | Loss: 0.00002525
Iteration 54/1000 | Loss: 0.00002525
Iteration 55/1000 | Loss: 0.00002525
Iteration 56/1000 | Loss: 0.00002525
Iteration 57/1000 | Loss: 0.00002525
Iteration 58/1000 | Loss: 0.00002525
Iteration 59/1000 | Loss: 0.00002525
Iteration 60/1000 | Loss: 0.00002525
Iteration 61/1000 | Loss: 0.00002525
Iteration 62/1000 | Loss: 0.00002525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [2.5253819330828264e-05, 2.5253819330828264e-05, 2.5253819330828264e-05, 2.5253819330828264e-05, 2.5253819330828264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5253819330828264e-05

Optimization complete. Final v2v error: 4.2584228515625 mm

Highest mean error: 4.666848182678223 mm for frame 80

Lowest mean error: 3.858280897140503 mm for frame 0

Saving results

Total time: 1131.0282382965088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0002
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434010
Iteration 2/25 | Loss: 0.00111401
Iteration 3/25 | Loss: 0.00098865
Iteration 4/25 | Loss: 0.00095703
Iteration 5/25 | Loss: 0.00094916
Iteration 6/25 | Loss: 0.00094772
Iteration 7/25 | Loss: 0.00094734
Iteration 8/25 | Loss: 0.00094734
Iteration 9/25 | Loss: 0.00094734
Iteration 10/25 | Loss: 0.00094734
Iteration 11/25 | Loss: 0.00094734
Iteration 12/25 | Loss: 0.00094734
Iteration 13/25 | Loss: 0.00094734
Iteration 14/25 | Loss: 0.00094734
Iteration 15/25 | Loss: 0.00094734
Iteration 16/25 | Loss: 0.00094734
Iteration 17/25 | Loss: 0.00094734
Iteration 18/25 | Loss: 0.00094734
Iteration 19/25 | Loss: 0.00094734
Iteration 20/25 | Loss: 0.00094734
Iteration 21/25 | Loss: 0.00094734
Iteration 22/25 | Loss: 0.00094734
Iteration 23/25 | Loss: 0.00094734
Iteration 24/25 | Loss: 0.00094734
Iteration 25/25 | Loss: 0.00094734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37477922
Iteration 2/25 | Loss: 0.00045015
Iteration 3/25 | Loss: 0.00045015
Iteration 4/25 | Loss: 0.00045015
Iteration 5/25 | Loss: 0.00045015
Iteration 6/25 | Loss: 0.00045015
Iteration 7/25 | Loss: 0.00045015
Iteration 8/25 | Loss: 0.00045015
Iteration 9/25 | Loss: 0.00045015
Iteration 10/25 | Loss: 0.00045015
Iteration 11/25 | Loss: 0.00045015
Iteration 12/25 | Loss: 0.00045015
Iteration 13/25 | Loss: 0.00045015
Iteration 14/25 | Loss: 0.00045015
Iteration 15/25 | Loss: 0.00045015
Iteration 16/25 | Loss: 0.00045015
Iteration 17/25 | Loss: 0.00045015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004501471994444728, 0.0004501471994444728, 0.0004501471994444728, 0.0004501471994444728, 0.0004501471994444728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004501471994444728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045015
Iteration 2/1000 | Loss: 0.00005537
Iteration 3/1000 | Loss: 0.00004117
Iteration 4/1000 | Loss: 0.00003778
Iteration 5/1000 | Loss: 0.00003579
Iteration 6/1000 | Loss: 0.00003455
Iteration 7/1000 | Loss: 0.00003371
Iteration 8/1000 | Loss: 0.00003329
Iteration 9/1000 | Loss: 0.00003294
Iteration 10/1000 | Loss: 0.00003265
Iteration 11/1000 | Loss: 0.00003252
Iteration 12/1000 | Loss: 0.00003251
Iteration 13/1000 | Loss: 0.00003248
Iteration 14/1000 | Loss: 0.00003247
Iteration 15/1000 | Loss: 0.00003247
Iteration 16/1000 | Loss: 0.00003247
Iteration 17/1000 | Loss: 0.00003246
Iteration 18/1000 | Loss: 0.00003246
Iteration 19/1000 | Loss: 0.00003246
Iteration 20/1000 | Loss: 0.00003245
Iteration 21/1000 | Loss: 0.00003244
Iteration 22/1000 | Loss: 0.00003244
Iteration 23/1000 | Loss: 0.00003244
Iteration 24/1000 | Loss: 0.00003244
Iteration 25/1000 | Loss: 0.00003244
Iteration 26/1000 | Loss: 0.00003244
Iteration 27/1000 | Loss: 0.00003243
Iteration 28/1000 | Loss: 0.00003243
Iteration 29/1000 | Loss: 0.00003243
Iteration 30/1000 | Loss: 0.00003243
Iteration 31/1000 | Loss: 0.00003243
Iteration 32/1000 | Loss: 0.00003243
Iteration 33/1000 | Loss: 0.00003243
Iteration 34/1000 | Loss: 0.00003243
Iteration 35/1000 | Loss: 0.00003243
Iteration 36/1000 | Loss: 0.00003243
Iteration 37/1000 | Loss: 0.00003243
Iteration 38/1000 | Loss: 0.00003243
Iteration 39/1000 | Loss: 0.00003243
Iteration 40/1000 | Loss: 0.00003242
Iteration 41/1000 | Loss: 0.00003242
Iteration 42/1000 | Loss: 0.00003242
Iteration 43/1000 | Loss: 0.00003242
Iteration 44/1000 | Loss: 0.00003242
Iteration 45/1000 | Loss: 0.00003242
Iteration 46/1000 | Loss: 0.00003242
Iteration 47/1000 | Loss: 0.00003242
Iteration 48/1000 | Loss: 0.00003242
Iteration 49/1000 | Loss: 0.00003242
Iteration 50/1000 | Loss: 0.00003242
Iteration 51/1000 | Loss: 0.00003242
Iteration 52/1000 | Loss: 0.00003242
Iteration 53/1000 | Loss: 0.00003242
Iteration 54/1000 | Loss: 0.00003242
Iteration 55/1000 | Loss: 0.00003242
Iteration 56/1000 | Loss: 0.00003242
Iteration 57/1000 | Loss: 0.00003242
Iteration 58/1000 | Loss: 0.00003242
Iteration 59/1000 | Loss: 0.00003242
Iteration 60/1000 | Loss: 0.00003242
Iteration 61/1000 | Loss: 0.00003242
Iteration 62/1000 | Loss: 0.00003242
Iteration 63/1000 | Loss: 0.00003242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [3.241609374526888e-05, 3.241609374526888e-05, 3.241609374526888e-05, 3.241609374526888e-05, 3.241609374526888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.241609374526888e-05

Optimization complete. Final v2v error: 4.7161641120910645 mm

Highest mean error: 5.058480739593506 mm for frame 105

Lowest mean error: 4.196674823760986 mm for frame 46

Saving results

Total time: 692.3356068134308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0022
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125920
Iteration 2/25 | Loss: 0.00355600
Iteration 3/25 | Loss: 0.00189630
Iteration 4/25 | Loss: 0.00155598
Iteration 5/25 | Loss: 0.00143592
Iteration 6/25 | Loss: 0.00145329
Iteration 7/25 | Loss: 0.00144505
Iteration 8/25 | Loss: 0.00129369
Iteration 9/25 | Loss: 0.00120542
Iteration 10/25 | Loss: 0.00108025
Iteration 11/25 | Loss: 0.00103129
Iteration 12/25 | Loss: 0.00101888
Iteration 13/25 | Loss: 0.00099666
Iteration 14/25 | Loss: 0.00099564
Iteration 15/25 | Loss: 0.00098056
Iteration 16/25 | Loss: 0.00096848
Iteration 17/25 | Loss: 0.00095985
Iteration 18/25 | Loss: 0.00095218
Iteration 19/25 | Loss: 0.00094588
Iteration 20/25 | Loss: 0.00094937
Iteration 21/25 | Loss: 0.00094767
Iteration 22/25 | Loss: 0.00094662
Iteration 23/25 | Loss: 0.00093669
Iteration 24/25 | Loss: 0.00093695
Iteration 25/25 | Loss: 0.00093181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97255147
Iteration 2/25 | Loss: 0.00054527
Iteration 3/25 | Loss: 0.00050807
Iteration 4/25 | Loss: 0.00050807
Iteration 5/25 | Loss: 0.00050807
Iteration 6/25 | Loss: 0.00050807
Iteration 7/25 | Loss: 0.00050807
Iteration 8/25 | Loss: 0.00050806
Iteration 9/25 | Loss: 0.00050806
Iteration 10/25 | Loss: 0.00050806
Iteration 11/25 | Loss: 0.00050806
Iteration 12/25 | Loss: 0.00050806
Iteration 13/25 | Loss: 0.00050806
Iteration 14/25 | Loss: 0.00050806
Iteration 15/25 | Loss: 0.00050806
Iteration 16/25 | Loss: 0.00050806
Iteration 17/25 | Loss: 0.00050806
Iteration 18/25 | Loss: 0.00050806
Iteration 19/25 | Loss: 0.00050806
Iteration 20/25 | Loss: 0.00050806
Iteration 21/25 | Loss: 0.00050806
Iteration 22/25 | Loss: 0.00050806
Iteration 23/25 | Loss: 0.00050806
Iteration 24/25 | Loss: 0.00050806
Iteration 25/25 | Loss: 0.00050806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050806
Iteration 2/1000 | Loss: 0.00037642
Iteration 3/1000 | Loss: 0.00075172
Iteration 4/1000 | Loss: 0.00030909
Iteration 5/1000 | Loss: 0.00024182
Iteration 6/1000 | Loss: 0.00022469
Iteration 7/1000 | Loss: 0.00016693
Iteration 8/1000 | Loss: 0.00005974
Iteration 9/1000 | Loss: 0.00015013
Iteration 10/1000 | Loss: 0.00005064
Iteration 11/1000 | Loss: 0.00006816
Iteration 12/1000 | Loss: 0.00030602
Iteration 13/1000 | Loss: 0.00019884
Iteration 14/1000 | Loss: 0.00004368
Iteration 15/1000 | Loss: 0.00004095
Iteration 16/1000 | Loss: 0.00005226
Iteration 17/1000 | Loss: 0.00014487
Iteration 18/1000 | Loss: 0.00006341
Iteration 19/1000 | Loss: 0.00004398
Iteration 20/1000 | Loss: 0.00007843
Iteration 21/1000 | Loss: 0.00003494
Iteration 22/1000 | Loss: 0.00048641
Iteration 23/1000 | Loss: 0.00050616
Iteration 24/1000 | Loss: 0.00004732
Iteration 25/1000 | Loss: 0.00048067
Iteration 26/1000 | Loss: 0.00052281
Iteration 27/1000 | Loss: 0.00002859
Iteration 28/1000 | Loss: 0.00006966
Iteration 29/1000 | Loss: 0.00044703
Iteration 30/1000 | Loss: 0.00038437
Iteration 31/1000 | Loss: 0.00002736
Iteration 32/1000 | Loss: 0.00002966
Iteration 33/1000 | Loss: 0.00031564
Iteration 34/1000 | Loss: 0.00014335
Iteration 35/1000 | Loss: 0.00002682
Iteration 36/1000 | Loss: 0.00014316
Iteration 37/1000 | Loss: 0.00008130
Iteration 38/1000 | Loss: 0.00030667
Iteration 39/1000 | Loss: 0.00015243
Iteration 40/1000 | Loss: 0.00003223
Iteration 41/1000 | Loss: 0.00023575
Iteration 42/1000 | Loss: 0.00003548
Iteration 43/1000 | Loss: 0.00003551
Iteration 44/1000 | Loss: 0.00003160
Iteration 45/1000 | Loss: 0.00002640
Iteration 46/1000 | Loss: 0.00005080
Iteration 47/1000 | Loss: 0.00002520
Iteration 48/1000 | Loss: 0.00004579
Iteration 49/1000 | Loss: 0.00002225
Iteration 50/1000 | Loss: 0.00002199
Iteration 51/1000 | Loss: 0.00034074
Iteration 52/1000 | Loss: 0.00002292
Iteration 53/1000 | Loss: 0.00002511
Iteration 54/1000 | Loss: 0.00005613
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00005370
Iteration 57/1000 | Loss: 0.00001931
Iteration 58/1000 | Loss: 0.00003228
Iteration 59/1000 | Loss: 0.00001918
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001911
Iteration 63/1000 | Loss: 0.00001911
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001910
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001907
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001901
Iteration 77/1000 | Loss: 0.00001901
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001897
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001896
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00002639
Iteration 104/1000 | Loss: 0.00001895
Iteration 105/1000 | Loss: 0.00001895
Iteration 106/1000 | Loss: 0.00001894
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001893
Iteration 109/1000 | Loss: 0.00001892
Iteration 110/1000 | Loss: 0.00001892
Iteration 111/1000 | Loss: 0.00001892
Iteration 112/1000 | Loss: 0.00001891
Iteration 113/1000 | Loss: 0.00001891
Iteration 114/1000 | Loss: 0.00001891
Iteration 115/1000 | Loss: 0.00001891
Iteration 116/1000 | Loss: 0.00001891
Iteration 117/1000 | Loss: 0.00001891
Iteration 118/1000 | Loss: 0.00001891
Iteration 119/1000 | Loss: 0.00001891
Iteration 120/1000 | Loss: 0.00001891
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001890
Iteration 123/1000 | Loss: 0.00001890
Iteration 124/1000 | Loss: 0.00001890
Iteration 125/1000 | Loss: 0.00001890
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001890
Iteration 129/1000 | Loss: 0.00001890
Iteration 130/1000 | Loss: 0.00001890
Iteration 131/1000 | Loss: 0.00001890
Iteration 132/1000 | Loss: 0.00001890
Iteration 133/1000 | Loss: 0.00001890
Iteration 134/1000 | Loss: 0.00001890
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001888
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001888
Iteration 153/1000 | Loss: 0.00001888
Iteration 154/1000 | Loss: 0.00001888
Iteration 155/1000 | Loss: 0.00001888
Iteration 156/1000 | Loss: 0.00001888
Iteration 157/1000 | Loss: 0.00001888
Iteration 158/1000 | Loss: 0.00001888
Iteration 159/1000 | Loss: 0.00001888
Iteration 160/1000 | Loss: 0.00001888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.887898179120384e-05, 1.887898179120384e-05, 1.887898179120384e-05, 1.887898179120384e-05, 1.887898179120384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.887898179120384e-05

Optimization complete. Final v2v error: 3.621640682220459 mm

Highest mean error: 4.565028667449951 mm for frame 134

Lowest mean error: 3.2290332317352295 mm for frame 68

Saving results

Total time: 3742.245436668396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0021
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128523
Iteration 2/25 | Loss: 0.00235196
Iteration 3/25 | Loss: 0.00116920
Iteration 4/25 | Loss: 0.00118749
Iteration 5/25 | Loss: 0.00126940
Iteration 6/25 | Loss: 0.00105204
Iteration 7/25 | Loss: 0.00116288
Iteration 8/25 | Loss: 0.00100806
Iteration 9/25 | Loss: 0.00102165
Iteration 10/25 | Loss: 0.00096125
Iteration 11/25 | Loss: 0.00097665
Iteration 12/25 | Loss: 0.00095790
Iteration 13/25 | Loss: 0.00094163
Iteration 14/25 | Loss: 0.00089794
Iteration 15/25 | Loss: 0.00089843
Iteration 16/25 | Loss: 0.00089545
Iteration 17/25 | Loss: 0.00088879
Iteration 18/25 | Loss: 0.00089086
Iteration 19/25 | Loss: 0.00088686
Iteration 20/25 | Loss: 0.00089050
Iteration 21/25 | Loss: 0.00089002
Iteration 22/25 | Loss: 0.00088904
Iteration 23/25 | Loss: 0.00088459
Iteration 24/25 | Loss: 0.00088807
Iteration 25/25 | Loss: 0.00088709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52388763
Iteration 2/25 | Loss: 0.00191763
Iteration 3/25 | Loss: 0.00191762
Iteration 4/25 | Loss: 0.00190079
Iteration 5/25 | Loss: 0.00190078
Iteration 6/25 | Loss: 0.00190078
Iteration 7/25 | Loss: 0.00190078
Iteration 8/25 | Loss: 0.00190078
Iteration 9/25 | Loss: 0.00190078
Iteration 10/25 | Loss: 0.00190078
Iteration 11/25 | Loss: 0.00190078
Iteration 12/25 | Loss: 0.00190078
Iteration 13/25 | Loss: 0.00190078
Iteration 14/25 | Loss: 0.00190078
Iteration 15/25 | Loss: 0.00190078
Iteration 16/25 | Loss: 0.00190078
Iteration 17/25 | Loss: 0.00190078
Iteration 18/25 | Loss: 0.00190078
Iteration 19/25 | Loss: 0.00190078
Iteration 20/25 | Loss: 0.00190078
Iteration 21/25 | Loss: 0.00190078
Iteration 22/25 | Loss: 0.00190078
Iteration 23/25 | Loss: 0.00190078
Iteration 24/25 | Loss: 0.00190078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019007772207260132, 0.0019007772207260132, 0.0019007772207260132, 0.0019007772207260132, 0.0019007772207260132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019007772207260132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190078
Iteration 2/1000 | Loss: 0.00018954
Iteration 3/1000 | Loss: 0.00006440
Iteration 4/1000 | Loss: 0.00143778
Iteration 5/1000 | Loss: 0.00067070
Iteration 6/1000 | Loss: 0.00148380
Iteration 7/1000 | Loss: 0.00141088
Iteration 8/1000 | Loss: 0.00033526
Iteration 9/1000 | Loss: 0.00254845
Iteration 10/1000 | Loss: 0.00020590
Iteration 11/1000 | Loss: 0.00033117
Iteration 12/1000 | Loss: 0.00033584
Iteration 13/1000 | Loss: 0.00088113
Iteration 14/1000 | Loss: 0.00073347
Iteration 15/1000 | Loss: 0.00003437
Iteration 16/1000 | Loss: 0.00006231
Iteration 17/1000 | Loss: 0.00009827
Iteration 18/1000 | Loss: 0.00015968
Iteration 19/1000 | Loss: 0.00016433
Iteration 20/1000 | Loss: 0.00024386
Iteration 21/1000 | Loss: 0.00005418
Iteration 22/1000 | Loss: 0.00005008
Iteration 23/1000 | Loss: 0.00020197
Iteration 24/1000 | Loss: 0.00010435
Iteration 25/1000 | Loss: 0.00021483
Iteration 26/1000 | Loss: 0.00011797
Iteration 27/1000 | Loss: 0.00019893
Iteration 28/1000 | Loss: 0.00017774
Iteration 29/1000 | Loss: 0.00016780
Iteration 30/1000 | Loss: 0.00023217
Iteration 31/1000 | Loss: 0.00014964
Iteration 32/1000 | Loss: 0.00017797
Iteration 33/1000 | Loss: 0.00015609
Iteration 34/1000 | Loss: 0.00003530
Iteration 35/1000 | Loss: 0.00026699
Iteration 36/1000 | Loss: 0.00002570
Iteration 37/1000 | Loss: 0.00002336
Iteration 38/1000 | Loss: 0.00002132
Iteration 39/1000 | Loss: 0.00002074
Iteration 40/1000 | Loss: 0.00002031
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001951
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001934
Iteration 45/1000 | Loss: 0.00001933
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001914
Iteration 50/1000 | Loss: 0.00001910
Iteration 51/1000 | Loss: 0.00001910
Iteration 52/1000 | Loss: 0.00001909
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001909
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001908
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001907
Iteration 66/1000 | Loss: 0.00001907
Iteration 67/1000 | Loss: 0.00001907
Iteration 68/1000 | Loss: 0.00001907
Iteration 69/1000 | Loss: 0.00001907
Iteration 70/1000 | Loss: 0.00001907
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001907
Iteration 73/1000 | Loss: 0.00001907
Iteration 74/1000 | Loss: 0.00001907
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001906
Iteration 78/1000 | Loss: 0.00001906
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001906
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001906
Iteration 86/1000 | Loss: 0.00001906
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001905
Iteration 96/1000 | Loss: 0.00001905
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001905
Iteration 111/1000 | Loss: 0.00001904
Iteration 112/1000 | Loss: 0.00001904
Iteration 113/1000 | Loss: 0.00001904
Iteration 114/1000 | Loss: 0.00001904
Iteration 115/1000 | Loss: 0.00001904
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001904
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.9027693269890733e-05, 1.9027693269890733e-05, 1.9027693269890733e-05, 1.9027693269890733e-05, 1.9027693269890733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9027693269890733e-05

Optimization complete. Final v2v error: 3.729732036590576 mm

Highest mean error: 4.5368194580078125 mm for frame 73

Lowest mean error: 3.3912439346313477 mm for frame 0

Saving results

Total time: 2658.2331445217133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0013
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00185349
Iteration 2/25 | Loss: 0.00101358
Iteration 3/25 | Loss: 0.00095250
Iteration 4/25 | Loss: 0.00093187
Iteration 5/25 | Loss: 0.00092301
Iteration 6/25 | Loss: 0.00092019
Iteration 7/25 | Loss: 0.00091854
Iteration 8/25 | Loss: 0.00091832
Iteration 9/25 | Loss: 0.00091832
Iteration 10/25 | Loss: 0.00091832
Iteration 11/25 | Loss: 0.00091832
Iteration 12/25 | Loss: 0.00091832
Iteration 13/25 | Loss: 0.00091832
Iteration 14/25 | Loss: 0.00091832
Iteration 15/25 | Loss: 0.00091832
Iteration 16/25 | Loss: 0.00091832
Iteration 17/25 | Loss: 0.00091832
Iteration 18/25 | Loss: 0.00091832
Iteration 19/25 | Loss: 0.00091832
Iteration 20/25 | Loss: 0.00091832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000918322941288352, 0.000918322941288352, 0.000918322941288352, 0.000918322941288352, 0.000918322941288352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000918322941288352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40347004
Iteration 2/25 | Loss: 0.00038160
Iteration 3/25 | Loss: 0.00038160
Iteration 4/25 | Loss: 0.00038160
Iteration 5/25 | Loss: 0.00038160
Iteration 6/25 | Loss: 0.00038160
Iteration 7/25 | Loss: 0.00038160
Iteration 8/25 | Loss: 0.00038160
Iteration 9/25 | Loss: 0.00038160
Iteration 10/25 | Loss: 0.00038160
Iteration 11/25 | Loss: 0.00038159
Iteration 12/25 | Loss: 0.00038159
Iteration 13/25 | Loss: 0.00038159
Iteration 14/25 | Loss: 0.00038159
Iteration 15/25 | Loss: 0.00038159
Iteration 16/25 | Loss: 0.00038159
Iteration 17/25 | Loss: 0.00038159
Iteration 18/25 | Loss: 0.00038159
Iteration 19/25 | Loss: 0.00038159
Iteration 20/25 | Loss: 0.00038159
Iteration 21/25 | Loss: 0.00038159
Iteration 22/25 | Loss: 0.00038159
Iteration 23/25 | Loss: 0.00038159
Iteration 24/25 | Loss: 0.00038159
Iteration 25/25 | Loss: 0.00038159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038159
Iteration 2/1000 | Loss: 0.00003591
Iteration 3/1000 | Loss: 0.00002477
Iteration 4/1000 | Loss: 0.00002225
Iteration 5/1000 | Loss: 0.00002109
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001844
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001773
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001745
Iteration 20/1000 | Loss: 0.00001743
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001742
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001742
Iteration 28/1000 | Loss: 0.00001742
Iteration 29/1000 | Loss: 0.00001742
Iteration 30/1000 | Loss: 0.00001741
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001738
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001737
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.730416261125356e-05, 1.730416261125356e-05, 1.730416261125356e-05, 1.730416261125356e-05, 1.730416261125356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.730416261125356e-05

Optimization complete. Final v2v error: 3.511306047439575 mm

Highest mean error: 3.7391505241394043 mm for frame 56

Lowest mean error: 3.194232940673828 mm for frame 1

Saving results

Total time: 1242.3837625980377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0005
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911726
Iteration 2/25 | Loss: 0.00156482
Iteration 3/25 | Loss: 0.00122058
Iteration 4/25 | Loss: 0.00115237
Iteration 5/25 | Loss: 0.00112363
Iteration 6/25 | Loss: 0.00110740
Iteration 7/25 | Loss: 0.00108620
Iteration 8/25 | Loss: 0.00102918
Iteration 9/25 | Loss: 0.00102253
Iteration 10/25 | Loss: 0.00102200
Iteration 11/25 | Loss: 0.00102185
Iteration 12/25 | Loss: 0.00102181
Iteration 13/25 | Loss: 0.00102181
Iteration 14/25 | Loss: 0.00102181
Iteration 15/25 | Loss: 0.00102181
Iteration 16/25 | Loss: 0.00102181
Iteration 17/25 | Loss: 0.00102180
Iteration 18/25 | Loss: 0.00102180
Iteration 19/25 | Loss: 0.00102180
Iteration 20/25 | Loss: 0.00102180
Iteration 21/25 | Loss: 0.00102180
Iteration 22/25 | Loss: 0.00102180
Iteration 23/25 | Loss: 0.00102180
Iteration 24/25 | Loss: 0.00102180
Iteration 25/25 | Loss: 0.00102180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94879091
Iteration 2/25 | Loss: 0.00044234
Iteration 3/25 | Loss: 0.00044233
Iteration 4/25 | Loss: 0.00044233
Iteration 5/25 | Loss: 0.00044233
Iteration 6/25 | Loss: 0.00044233
Iteration 7/25 | Loss: 0.00044233
Iteration 8/25 | Loss: 0.00044233
Iteration 9/25 | Loss: 0.00044233
Iteration 10/25 | Loss: 0.00044233
Iteration 11/25 | Loss: 0.00044233
Iteration 12/25 | Loss: 0.00044233
Iteration 13/25 | Loss: 0.00044233
Iteration 14/25 | Loss: 0.00044233
Iteration 15/25 | Loss: 0.00044233
Iteration 16/25 | Loss: 0.00044233
Iteration 17/25 | Loss: 0.00044233
Iteration 18/25 | Loss: 0.00044233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00044232927029952407, 0.00044232927029952407, 0.00044232927029952407, 0.00044232927029952407, 0.00044232927029952407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044232927029952407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044233
Iteration 2/1000 | Loss: 0.00004438
Iteration 3/1000 | Loss: 0.00003470
Iteration 4/1000 | Loss: 0.00003212
Iteration 5/1000 | Loss: 0.00003012
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00002840
Iteration 8/1000 | Loss: 0.00002794
Iteration 9/1000 | Loss: 0.00002755
Iteration 10/1000 | Loss: 0.00002736
Iteration 11/1000 | Loss: 0.00002730
Iteration 12/1000 | Loss: 0.00002727
Iteration 13/1000 | Loss: 0.00002726
Iteration 14/1000 | Loss: 0.00002726
Iteration 15/1000 | Loss: 0.00002725
Iteration 16/1000 | Loss: 0.00002725
Iteration 17/1000 | Loss: 0.00002724
Iteration 18/1000 | Loss: 0.00002723
Iteration 19/1000 | Loss: 0.00002722
Iteration 20/1000 | Loss: 0.00002722
Iteration 21/1000 | Loss: 0.00002722
Iteration 22/1000 | Loss: 0.00002721
Iteration 23/1000 | Loss: 0.00002721
Iteration 24/1000 | Loss: 0.00002721
Iteration 25/1000 | Loss: 0.00002721
Iteration 26/1000 | Loss: 0.00002721
Iteration 27/1000 | Loss: 0.00002721
Iteration 28/1000 | Loss: 0.00002720
Iteration 29/1000 | Loss: 0.00002720
Iteration 30/1000 | Loss: 0.00002720
Iteration 31/1000 | Loss: 0.00002720
Iteration 32/1000 | Loss: 0.00002720
Iteration 33/1000 | Loss: 0.00002720
Iteration 34/1000 | Loss: 0.00002720
Iteration 35/1000 | Loss: 0.00002719
Iteration 36/1000 | Loss: 0.00002719
Iteration 37/1000 | Loss: 0.00002719
Iteration 38/1000 | Loss: 0.00002719
Iteration 39/1000 | Loss: 0.00002718
Iteration 40/1000 | Loss: 0.00002718
Iteration 41/1000 | Loss: 0.00002718
Iteration 42/1000 | Loss: 0.00002718
Iteration 43/1000 | Loss: 0.00002717
Iteration 44/1000 | Loss: 0.00002717
Iteration 45/1000 | Loss: 0.00002717
Iteration 46/1000 | Loss: 0.00002717
Iteration 47/1000 | Loss: 0.00002717
Iteration 48/1000 | Loss: 0.00002717
Iteration 49/1000 | Loss: 0.00002717
Iteration 50/1000 | Loss: 0.00002717
Iteration 51/1000 | Loss: 0.00002717
Iteration 52/1000 | Loss: 0.00002717
Iteration 53/1000 | Loss: 0.00002717
Iteration 54/1000 | Loss: 0.00002716
Iteration 55/1000 | Loss: 0.00002716
Iteration 56/1000 | Loss: 0.00002716
Iteration 57/1000 | Loss: 0.00002716
Iteration 58/1000 | Loss: 0.00002716
Iteration 59/1000 | Loss: 0.00002716
Iteration 60/1000 | Loss: 0.00002716
Iteration 61/1000 | Loss: 0.00002716
Iteration 62/1000 | Loss: 0.00002716
Iteration 63/1000 | Loss: 0.00002716
Iteration 64/1000 | Loss: 0.00002715
Iteration 65/1000 | Loss: 0.00002715
Iteration 66/1000 | Loss: 0.00002715
Iteration 67/1000 | Loss: 0.00002715
Iteration 68/1000 | Loss: 0.00002715
Iteration 69/1000 | Loss: 0.00002715
Iteration 70/1000 | Loss: 0.00002715
Iteration 71/1000 | Loss: 0.00002715
Iteration 72/1000 | Loss: 0.00002714
Iteration 73/1000 | Loss: 0.00002714
Iteration 74/1000 | Loss: 0.00002714
Iteration 75/1000 | Loss: 0.00002714
Iteration 76/1000 | Loss: 0.00002714
Iteration 77/1000 | Loss: 0.00002714
Iteration 78/1000 | Loss: 0.00002714
Iteration 79/1000 | Loss: 0.00002714
Iteration 80/1000 | Loss: 0.00002714
Iteration 81/1000 | Loss: 0.00002714
Iteration 82/1000 | Loss: 0.00002714
Iteration 83/1000 | Loss: 0.00002713
Iteration 84/1000 | Loss: 0.00002713
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002713
Iteration 87/1000 | Loss: 0.00002713
Iteration 88/1000 | Loss: 0.00002713
Iteration 89/1000 | Loss: 0.00002713
Iteration 90/1000 | Loss: 0.00002713
Iteration 91/1000 | Loss: 0.00002713
Iteration 92/1000 | Loss: 0.00002713
Iteration 93/1000 | Loss: 0.00002713
Iteration 94/1000 | Loss: 0.00002713
Iteration 95/1000 | Loss: 0.00002713
Iteration 96/1000 | Loss: 0.00002713
Iteration 97/1000 | Loss: 0.00002713
Iteration 98/1000 | Loss: 0.00002713
Iteration 99/1000 | Loss: 0.00002713
Iteration 100/1000 | Loss: 0.00002713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.712670357141178e-05, 2.712670357141178e-05, 2.712670357141178e-05, 2.712670357141178e-05, 2.712670357141178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.712670357141178e-05

Optimization complete. Final v2v error: 4.377542018890381 mm

Highest mean error: 4.593289375305176 mm for frame 124

Lowest mean error: 4.059164047241211 mm for frame 24

Saving results

Total time: 1013.8854730129242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0010
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892543
Iteration 2/25 | Loss: 0.00127575
Iteration 3/25 | Loss: 0.00101967
Iteration 4/25 | Loss: 0.00097664
Iteration 5/25 | Loss: 0.00094961
Iteration 6/25 | Loss: 0.00094757
Iteration 7/25 | Loss: 0.00093984
Iteration 8/25 | Loss: 0.00093972
Iteration 9/25 | Loss: 0.00093652
Iteration 10/25 | Loss: 0.00092810
Iteration 11/25 | Loss: 0.00092651
Iteration 12/25 | Loss: 0.00092573
Iteration 13/25 | Loss: 0.00092556
Iteration 14/25 | Loss: 0.00092552
Iteration 15/25 | Loss: 0.00092552
Iteration 16/25 | Loss: 0.00092552
Iteration 17/25 | Loss: 0.00092551
Iteration 18/25 | Loss: 0.00092551
Iteration 19/25 | Loss: 0.00092551
Iteration 20/25 | Loss: 0.00092551
Iteration 21/25 | Loss: 0.00092551
Iteration 22/25 | Loss: 0.00092551
Iteration 23/25 | Loss: 0.00092551
Iteration 24/25 | Loss: 0.00092551
Iteration 25/25 | Loss: 0.00092551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95315242
Iteration 2/25 | Loss: 0.00045271
Iteration 3/25 | Loss: 0.00045270
Iteration 4/25 | Loss: 0.00045270
Iteration 5/25 | Loss: 0.00045270
Iteration 6/25 | Loss: 0.00045270
Iteration 7/25 | Loss: 0.00045270
Iteration 8/25 | Loss: 0.00045270
Iteration 9/25 | Loss: 0.00045270
Iteration 10/25 | Loss: 0.00045270
Iteration 11/25 | Loss: 0.00045270
Iteration 12/25 | Loss: 0.00045270
Iteration 13/25 | Loss: 0.00045270
Iteration 14/25 | Loss: 0.00045270
Iteration 15/25 | Loss: 0.00045270
Iteration 16/25 | Loss: 0.00045270
Iteration 17/25 | Loss: 0.00045270
Iteration 18/25 | Loss: 0.00045270
Iteration 19/25 | Loss: 0.00045270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000452699838206172, 0.000452699838206172, 0.000452699838206172, 0.000452699838206172, 0.000452699838206172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000452699838206172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045270
Iteration 2/1000 | Loss: 0.00004299
Iteration 3/1000 | Loss: 0.00002569
Iteration 4/1000 | Loss: 0.00002223
Iteration 5/1000 | Loss: 0.00002070
Iteration 6/1000 | Loss: 0.00001954
Iteration 7/1000 | Loss: 0.00001888
Iteration 8/1000 | Loss: 0.00001850
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001823
Iteration 11/1000 | Loss: 0.00001817
Iteration 12/1000 | Loss: 0.00001812
Iteration 13/1000 | Loss: 0.00001805
Iteration 14/1000 | Loss: 0.00001804
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001790
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001786
Iteration 33/1000 | Loss: 0.00001786
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001784
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001783
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001778
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001775
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001774
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001773
Iteration 65/1000 | Loss: 0.00001773
Iteration 66/1000 | Loss: 0.00001773
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001772
Iteration 69/1000 | Loss: 0.00001772
Iteration 70/1000 | Loss: 0.00001772
Iteration 71/1000 | Loss: 0.00001772
Iteration 72/1000 | Loss: 0.00001772
Iteration 73/1000 | Loss: 0.00001771
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001770
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001769
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001768
Iteration 86/1000 | Loss: 0.00001768
Iteration 87/1000 | Loss: 0.00001768
Iteration 88/1000 | Loss: 0.00001768
Iteration 89/1000 | Loss: 0.00001768
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001766
Iteration 103/1000 | Loss: 0.00001766
Iteration 104/1000 | Loss: 0.00001766
Iteration 105/1000 | Loss: 0.00001766
Iteration 106/1000 | Loss: 0.00001766
Iteration 107/1000 | Loss: 0.00001766
Iteration 108/1000 | Loss: 0.00001766
Iteration 109/1000 | Loss: 0.00001766
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001766
Iteration 115/1000 | Loss: 0.00001766
Iteration 116/1000 | Loss: 0.00001766
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001766
Iteration 122/1000 | Loss: 0.00001766
Iteration 123/1000 | Loss: 0.00001766
Iteration 124/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.7662910977378488e-05, 1.7662910977378488e-05, 1.7662910977378488e-05, 1.7662910977378488e-05, 1.7662910977378488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7662910977378488e-05

Optimization complete. Final v2v error: 3.467254638671875 mm

Highest mean error: 4.047412395477295 mm for frame 82

Lowest mean error: 3.0812671184539795 mm for frame 127

Saving results

Total time: 904.11141705513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0004
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441651
Iteration 2/25 | Loss: 0.00111377
Iteration 3/25 | Loss: 0.00094842
Iteration 4/25 | Loss: 0.00092465
Iteration 5/25 | Loss: 0.00091560
Iteration 6/25 | Loss: 0.00091379
Iteration 7/25 | Loss: 0.00091372
Iteration 8/25 | Loss: 0.00091372
Iteration 9/25 | Loss: 0.00091372
Iteration 10/25 | Loss: 0.00091372
Iteration 11/25 | Loss: 0.00091372
Iteration 12/25 | Loss: 0.00091372
Iteration 13/25 | Loss: 0.00091372
Iteration 14/25 | Loss: 0.00091372
Iteration 15/25 | Loss: 0.00091372
Iteration 16/25 | Loss: 0.00091372
Iteration 17/25 | Loss: 0.00091372
Iteration 18/25 | Loss: 0.00091372
Iteration 19/25 | Loss: 0.00091372
Iteration 20/25 | Loss: 0.00091372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009137204033322632, 0.0009137204033322632, 0.0009137204033322632, 0.0009137204033322632, 0.0009137204033322632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009137204033322632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06441355
Iteration 2/25 | Loss: 0.00040463
Iteration 3/25 | Loss: 0.00040460
Iteration 4/25 | Loss: 0.00040460
Iteration 5/25 | Loss: 0.00040460
Iteration 6/25 | Loss: 0.00040460
Iteration 7/25 | Loss: 0.00040460
Iteration 8/25 | Loss: 0.00040460
Iteration 9/25 | Loss: 0.00040459
Iteration 10/25 | Loss: 0.00040459
Iteration 11/25 | Loss: 0.00040459
Iteration 12/25 | Loss: 0.00040459
Iteration 13/25 | Loss: 0.00040459
Iteration 14/25 | Loss: 0.00040459
Iteration 15/25 | Loss: 0.00040459
Iteration 16/25 | Loss: 0.00040459
Iteration 17/25 | Loss: 0.00040459
Iteration 18/25 | Loss: 0.00040459
Iteration 19/25 | Loss: 0.00040459
Iteration 20/25 | Loss: 0.00040459
Iteration 21/25 | Loss: 0.00040459
Iteration 22/25 | Loss: 0.00040459
Iteration 23/25 | Loss: 0.00040459
Iteration 24/25 | Loss: 0.00040459
Iteration 25/25 | Loss: 0.00040459

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040459
Iteration 2/1000 | Loss: 0.00003777
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002562
Iteration 5/1000 | Loss: 0.00002428
Iteration 6/1000 | Loss: 0.00002353
Iteration 7/1000 | Loss: 0.00002319
Iteration 8/1000 | Loss: 0.00002300
Iteration 9/1000 | Loss: 0.00002281
Iteration 10/1000 | Loss: 0.00002277
Iteration 11/1000 | Loss: 0.00002272
Iteration 12/1000 | Loss: 0.00002271
Iteration 13/1000 | Loss: 0.00002267
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002267
Iteration 16/1000 | Loss: 0.00002266
Iteration 17/1000 | Loss: 0.00002265
Iteration 18/1000 | Loss: 0.00002258
Iteration 19/1000 | Loss: 0.00002258
Iteration 20/1000 | Loss: 0.00002255
Iteration 21/1000 | Loss: 0.00002255
Iteration 22/1000 | Loss: 0.00002255
Iteration 23/1000 | Loss: 0.00002255
Iteration 24/1000 | Loss: 0.00002254
Iteration 25/1000 | Loss: 0.00002254
Iteration 26/1000 | Loss: 0.00002254
Iteration 27/1000 | Loss: 0.00002254
Iteration 28/1000 | Loss: 0.00002254
Iteration 29/1000 | Loss: 0.00002253
Iteration 30/1000 | Loss: 0.00002253
Iteration 31/1000 | Loss: 0.00002253
Iteration 32/1000 | Loss: 0.00002253
Iteration 33/1000 | Loss: 0.00002253
Iteration 34/1000 | Loss: 0.00002253
Iteration 35/1000 | Loss: 0.00002253
Iteration 36/1000 | Loss: 0.00002253
Iteration 37/1000 | Loss: 0.00002253
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002252
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002252
Iteration 44/1000 | Loss: 0.00002252
Iteration 45/1000 | Loss: 0.00002252
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002251
Iteration 49/1000 | Loss: 0.00002251
Iteration 50/1000 | Loss: 0.00002251
Iteration 51/1000 | Loss: 0.00002251
Iteration 52/1000 | Loss: 0.00002250
Iteration 53/1000 | Loss: 0.00002250
Iteration 54/1000 | Loss: 0.00002250
Iteration 55/1000 | Loss: 0.00002250
Iteration 56/1000 | Loss: 0.00002250
Iteration 57/1000 | Loss: 0.00002250
Iteration 58/1000 | Loss: 0.00002250
Iteration 59/1000 | Loss: 0.00002250
Iteration 60/1000 | Loss: 0.00002250
Iteration 61/1000 | Loss: 0.00002250
Iteration 62/1000 | Loss: 0.00002250
Iteration 63/1000 | Loss: 0.00002250
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002250
Iteration 66/1000 | Loss: 0.00002250
Iteration 67/1000 | Loss: 0.00002250
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002249
Iteration 70/1000 | Loss: 0.00002249
Iteration 71/1000 | Loss: 0.00002249
Iteration 72/1000 | Loss: 0.00002249
Iteration 73/1000 | Loss: 0.00002249
Iteration 74/1000 | Loss: 0.00002249
Iteration 75/1000 | Loss: 0.00002249
Iteration 76/1000 | Loss: 0.00002249
Iteration 77/1000 | Loss: 0.00002249
Iteration 78/1000 | Loss: 0.00002249
Iteration 79/1000 | Loss: 0.00002249
Iteration 80/1000 | Loss: 0.00002249
Iteration 81/1000 | Loss: 0.00002249
Iteration 82/1000 | Loss: 0.00002248
Iteration 83/1000 | Loss: 0.00002248
Iteration 84/1000 | Loss: 0.00002248
Iteration 85/1000 | Loss: 0.00002248
Iteration 86/1000 | Loss: 0.00002247
Iteration 87/1000 | Loss: 0.00002247
Iteration 88/1000 | Loss: 0.00002247
Iteration 89/1000 | Loss: 0.00002247
Iteration 90/1000 | Loss: 0.00002247
Iteration 91/1000 | Loss: 0.00002247
Iteration 92/1000 | Loss: 0.00002247
Iteration 93/1000 | Loss: 0.00002247
Iteration 94/1000 | Loss: 0.00002247
Iteration 95/1000 | Loss: 0.00002246
Iteration 96/1000 | Loss: 0.00002246
Iteration 97/1000 | Loss: 0.00002246
Iteration 98/1000 | Loss: 0.00002246
Iteration 99/1000 | Loss: 0.00002246
Iteration 100/1000 | Loss: 0.00002246
Iteration 101/1000 | Loss: 0.00002246
Iteration 102/1000 | Loss: 0.00002246
Iteration 103/1000 | Loss: 0.00002246
Iteration 104/1000 | Loss: 0.00002246
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002246
Iteration 111/1000 | Loss: 0.00002246
Iteration 112/1000 | Loss: 0.00002246
Iteration 113/1000 | Loss: 0.00002246
Iteration 114/1000 | Loss: 0.00002246
Iteration 115/1000 | Loss: 0.00002246
Iteration 116/1000 | Loss: 0.00002246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.245911309728399e-05, 2.245911309728399e-05, 2.245911309728399e-05, 2.245911309728399e-05, 2.245911309728399e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.245911309728399e-05

Optimization complete. Final v2v error: 3.989642858505249 mm

Highest mean error: 4.281060695648193 mm for frame 65

Lowest mean error: 3.7714483737945557 mm for frame 2

Saving results

Total time: 710.2695484161377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0017
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417814
Iteration 2/25 | Loss: 0.00103754
Iteration 3/25 | Loss: 0.00094088
Iteration 4/25 | Loss: 0.00091878
Iteration 5/25 | Loss: 0.00090898
Iteration 6/25 | Loss: 0.00090680
Iteration 7/25 | Loss: 0.00090626
Iteration 8/25 | Loss: 0.00090626
Iteration 9/25 | Loss: 0.00090626
Iteration 10/25 | Loss: 0.00090626
Iteration 11/25 | Loss: 0.00090626
Iteration 12/25 | Loss: 0.00090626
Iteration 13/25 | Loss: 0.00090626
Iteration 14/25 | Loss: 0.00090626
Iteration 15/25 | Loss: 0.00090626
Iteration 16/25 | Loss: 0.00090626
Iteration 17/25 | Loss: 0.00090626
Iteration 18/25 | Loss: 0.00090626
Iteration 19/25 | Loss: 0.00090626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000906259985640645, 0.000906259985640645, 0.000906259985640645, 0.000906259985640645, 0.000906259985640645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000906259985640645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80623960
Iteration 2/25 | Loss: 0.00041892
Iteration 3/25 | Loss: 0.00041892
Iteration 4/25 | Loss: 0.00041892
Iteration 5/25 | Loss: 0.00041892
Iteration 6/25 | Loss: 0.00041892
Iteration 7/25 | Loss: 0.00041892
Iteration 8/25 | Loss: 0.00041892
Iteration 9/25 | Loss: 0.00041892
Iteration 10/25 | Loss: 0.00041892
Iteration 11/25 | Loss: 0.00041892
Iteration 12/25 | Loss: 0.00041892
Iteration 13/25 | Loss: 0.00041892
Iteration 14/25 | Loss: 0.00041892
Iteration 15/25 | Loss: 0.00041892
Iteration 16/25 | Loss: 0.00041892
Iteration 17/25 | Loss: 0.00041892
Iteration 18/25 | Loss: 0.00041892
Iteration 19/25 | Loss: 0.00041892
Iteration 20/25 | Loss: 0.00041892
Iteration 21/25 | Loss: 0.00041892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004189170431345701, 0.0004189170431345701, 0.0004189170431345701, 0.0004189170431345701, 0.0004189170431345701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004189170431345701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041892
Iteration 2/1000 | Loss: 0.00004618
Iteration 3/1000 | Loss: 0.00003050
Iteration 4/1000 | Loss: 0.00002785
Iteration 5/1000 | Loss: 0.00002662
Iteration 6/1000 | Loss: 0.00002557
Iteration 7/1000 | Loss: 0.00002492
Iteration 8/1000 | Loss: 0.00002460
Iteration 9/1000 | Loss: 0.00002451
Iteration 10/1000 | Loss: 0.00002437
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002418
Iteration 13/1000 | Loss: 0.00002412
Iteration 14/1000 | Loss: 0.00002409
Iteration 15/1000 | Loss: 0.00002408
Iteration 16/1000 | Loss: 0.00002406
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002405
Iteration 19/1000 | Loss: 0.00002405
Iteration 20/1000 | Loss: 0.00002405
Iteration 21/1000 | Loss: 0.00002405
Iteration 22/1000 | Loss: 0.00002405
Iteration 23/1000 | Loss: 0.00002405
Iteration 24/1000 | Loss: 0.00002405
Iteration 25/1000 | Loss: 0.00002405
Iteration 26/1000 | Loss: 0.00002405
Iteration 27/1000 | Loss: 0.00002405
Iteration 28/1000 | Loss: 0.00002405
Iteration 29/1000 | Loss: 0.00002405
Iteration 30/1000 | Loss: 0.00002404
Iteration 31/1000 | Loss: 0.00002404
Iteration 32/1000 | Loss: 0.00002404
Iteration 33/1000 | Loss: 0.00002403
Iteration 34/1000 | Loss: 0.00002403
Iteration 35/1000 | Loss: 0.00002403
Iteration 36/1000 | Loss: 0.00002403
Iteration 37/1000 | Loss: 0.00002403
Iteration 38/1000 | Loss: 0.00002402
Iteration 39/1000 | Loss: 0.00002402
Iteration 40/1000 | Loss: 0.00002402
Iteration 41/1000 | Loss: 0.00002402
Iteration 42/1000 | Loss: 0.00002402
Iteration 43/1000 | Loss: 0.00002402
Iteration 44/1000 | Loss: 0.00002402
Iteration 45/1000 | Loss: 0.00002402
Iteration 46/1000 | Loss: 0.00002402
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002402
Iteration 49/1000 | Loss: 0.00002402
Iteration 50/1000 | Loss: 0.00002402
Iteration 51/1000 | Loss: 0.00002402
Iteration 52/1000 | Loss: 0.00002402
Iteration 53/1000 | Loss: 0.00002402
Iteration 54/1000 | Loss: 0.00002402
Iteration 55/1000 | Loss: 0.00002402
Iteration 56/1000 | Loss: 0.00002402
Iteration 57/1000 | Loss: 0.00002402
Iteration 58/1000 | Loss: 0.00002402
Iteration 59/1000 | Loss: 0.00002401
Iteration 60/1000 | Loss: 0.00002401
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002401
Iteration 64/1000 | Loss: 0.00002400
Iteration 65/1000 | Loss: 0.00002400
Iteration 66/1000 | Loss: 0.00002400
Iteration 67/1000 | Loss: 0.00002400
Iteration 68/1000 | Loss: 0.00002399
Iteration 69/1000 | Loss: 0.00002399
Iteration 70/1000 | Loss: 0.00002399
Iteration 71/1000 | Loss: 0.00002399
Iteration 72/1000 | Loss: 0.00002399
Iteration 73/1000 | Loss: 0.00002399
Iteration 74/1000 | Loss: 0.00002399
Iteration 75/1000 | Loss: 0.00002399
Iteration 76/1000 | Loss: 0.00002399
Iteration 77/1000 | Loss: 0.00002399
Iteration 78/1000 | Loss: 0.00002399
Iteration 79/1000 | Loss: 0.00002399
Iteration 80/1000 | Loss: 0.00002399
Iteration 81/1000 | Loss: 0.00002399
Iteration 82/1000 | Loss: 0.00002399
Iteration 83/1000 | Loss: 0.00002398
Iteration 84/1000 | Loss: 0.00002398
Iteration 85/1000 | Loss: 0.00002398
Iteration 86/1000 | Loss: 0.00002398
Iteration 87/1000 | Loss: 0.00002398
Iteration 88/1000 | Loss: 0.00002398
Iteration 89/1000 | Loss: 0.00002398
Iteration 90/1000 | Loss: 0.00002398
Iteration 91/1000 | Loss: 0.00002398
Iteration 92/1000 | Loss: 0.00002398
Iteration 93/1000 | Loss: 0.00002398
Iteration 94/1000 | Loss: 0.00002398
Iteration 95/1000 | Loss: 0.00002398
Iteration 96/1000 | Loss: 0.00002398
Iteration 97/1000 | Loss: 0.00002398
Iteration 98/1000 | Loss: 0.00002398
Iteration 99/1000 | Loss: 0.00002397
Iteration 100/1000 | Loss: 0.00002397
Iteration 101/1000 | Loss: 0.00002397
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002397
Iteration 104/1000 | Loss: 0.00002397
Iteration 105/1000 | Loss: 0.00002397
Iteration 106/1000 | Loss: 0.00002397
Iteration 107/1000 | Loss: 0.00002397
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002397
Iteration 112/1000 | Loss: 0.00002397
Iteration 113/1000 | Loss: 0.00002397
Iteration 114/1000 | Loss: 0.00002397
Iteration 115/1000 | Loss: 0.00002397
Iteration 116/1000 | Loss: 0.00002397
Iteration 117/1000 | Loss: 0.00002396
Iteration 118/1000 | Loss: 0.00002396
Iteration 119/1000 | Loss: 0.00002396
Iteration 120/1000 | Loss: 0.00002396
Iteration 121/1000 | Loss: 0.00002396
Iteration 122/1000 | Loss: 0.00002396
Iteration 123/1000 | Loss: 0.00002396
Iteration 124/1000 | Loss: 0.00002396
Iteration 125/1000 | Loss: 0.00002396
Iteration 126/1000 | Loss: 0.00002396
Iteration 127/1000 | Loss: 0.00002396
Iteration 128/1000 | Loss: 0.00002396
Iteration 129/1000 | Loss: 0.00002396
Iteration 130/1000 | Loss: 0.00002396
Iteration 131/1000 | Loss: 0.00002396
Iteration 132/1000 | Loss: 0.00002396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.3958145902724937e-05, 2.3958145902724937e-05, 2.3958145902724937e-05, 2.3958145902724937e-05, 2.3958145902724937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3958145902724937e-05

Optimization complete. Final v2v error: 4.122542381286621 mm

Highest mean error: 4.6008782386779785 mm for frame 63

Lowest mean error: 3.5558159351348877 mm for frame 72

Saving results

Total time: 627.3820235729218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0020
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072603
Iteration 2/25 | Loss: 0.00327878
Iteration 3/25 | Loss: 0.00224174
Iteration 4/25 | Loss: 0.00183254
Iteration 5/25 | Loss: 0.00195469
Iteration 6/25 | Loss: 0.00168448
Iteration 7/25 | Loss: 0.00149191
Iteration 8/25 | Loss: 0.00145095
Iteration 9/25 | Loss: 0.00144647
Iteration 10/25 | Loss: 0.00145975
Iteration 11/25 | Loss: 0.00141757
Iteration 12/25 | Loss: 0.00142274
Iteration 13/25 | Loss: 0.00139306
Iteration 14/25 | Loss: 0.00139051
Iteration 15/25 | Loss: 0.00138815
Iteration 16/25 | Loss: 0.00138479
Iteration 17/25 | Loss: 0.00140324
Iteration 18/25 | Loss: 0.00138830
Iteration 19/25 | Loss: 0.00138844
Iteration 20/25 | Loss: 0.00138220
Iteration 21/25 | Loss: 0.00137921
Iteration 22/25 | Loss: 0.00138193
Iteration 23/25 | Loss: 0.00138284
Iteration 24/25 | Loss: 0.00137837
Iteration 25/25 | Loss: 0.00137799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39571142
Iteration 2/25 | Loss: 0.00427941
Iteration 3/25 | Loss: 0.00427936
Iteration 4/25 | Loss: 0.00427936
Iteration 5/25 | Loss: 0.00427936
Iteration 6/25 | Loss: 0.00427936
Iteration 7/25 | Loss: 0.00427936
Iteration 8/25 | Loss: 0.00427936
Iteration 9/25 | Loss: 0.00427936
Iteration 10/25 | Loss: 0.00427936
Iteration 11/25 | Loss: 0.00427936
Iteration 12/25 | Loss: 0.00427936
Iteration 13/25 | Loss: 0.00427936
Iteration 14/25 | Loss: 0.00427936
Iteration 15/25 | Loss: 0.00427936
Iteration 16/25 | Loss: 0.00427936
Iteration 17/25 | Loss: 0.00427936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004279355052858591, 0.004279355052858591, 0.004279355052858591, 0.004279355052858591, 0.004279355052858591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004279355052858591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00427936
Iteration 2/1000 | Loss: 0.00086506
Iteration 3/1000 | Loss: 0.00675479
Iteration 4/1000 | Loss: 0.00912449
Iteration 5/1000 | Loss: 0.01041720
Iteration 6/1000 | Loss: 0.00452034
Iteration 7/1000 | Loss: 0.00104856
Iteration 8/1000 | Loss: 0.00130533
Iteration 9/1000 | Loss: 0.00040351
Iteration 10/1000 | Loss: 0.00016269
Iteration 11/1000 | Loss: 0.00104900
Iteration 12/1000 | Loss: 0.00011382
Iteration 13/1000 | Loss: 0.00027733
Iteration 14/1000 | Loss: 0.00008588
Iteration 15/1000 | Loss: 0.00138450
Iteration 16/1000 | Loss: 0.00041711
Iteration 17/1000 | Loss: 0.00013455
Iteration 18/1000 | Loss: 0.00007414
Iteration 19/1000 | Loss: 0.00043387
Iteration 20/1000 | Loss: 0.00040179
Iteration 21/1000 | Loss: 0.00042288
Iteration 22/1000 | Loss: 0.00038787
Iteration 23/1000 | Loss: 0.00040284
Iteration 24/1000 | Loss: 0.00030038
Iteration 25/1000 | Loss: 0.00006043
Iteration 26/1000 | Loss: 0.00017611
Iteration 27/1000 | Loss: 0.00011225
Iteration 28/1000 | Loss: 0.00005279
Iteration 29/1000 | Loss: 0.00048332
Iteration 30/1000 | Loss: 0.00034359
Iteration 31/1000 | Loss: 0.00006481
Iteration 32/1000 | Loss: 0.00015905
Iteration 33/1000 | Loss: 0.00029895
Iteration 34/1000 | Loss: 0.00039551
Iteration 35/1000 | Loss: 0.00019988
Iteration 36/1000 | Loss: 0.00005642
Iteration 37/1000 | Loss: 0.00004966
Iteration 38/1000 | Loss: 0.00004603
Iteration 39/1000 | Loss: 0.00004322
Iteration 40/1000 | Loss: 0.00004166
Iteration 41/1000 | Loss: 0.00004136
Iteration 42/1000 | Loss: 0.00003927
Iteration 43/1000 | Loss: 0.00003860
Iteration 44/1000 | Loss: 0.00003800
Iteration 45/1000 | Loss: 0.00003734
Iteration 46/1000 | Loss: 0.00003690
Iteration 47/1000 | Loss: 0.00003644
Iteration 48/1000 | Loss: 0.00003608
Iteration 49/1000 | Loss: 0.00003584
Iteration 50/1000 | Loss: 0.00099668
Iteration 51/1000 | Loss: 0.00040438
Iteration 52/1000 | Loss: 0.00004741
Iteration 53/1000 | Loss: 0.00003791
Iteration 54/1000 | Loss: 0.00003650
Iteration 55/1000 | Loss: 0.00003570
Iteration 56/1000 | Loss: 0.00003544
Iteration 57/1000 | Loss: 0.00003524
Iteration 58/1000 | Loss: 0.00003517
Iteration 59/1000 | Loss: 0.00003517
Iteration 60/1000 | Loss: 0.00003514
Iteration 61/1000 | Loss: 0.00003514
Iteration 62/1000 | Loss: 0.00003514
Iteration 63/1000 | Loss: 0.00003514
Iteration 64/1000 | Loss: 0.00003513
Iteration 65/1000 | Loss: 0.00003513
Iteration 66/1000 | Loss: 0.00003513
Iteration 67/1000 | Loss: 0.00003513
Iteration 68/1000 | Loss: 0.00003513
Iteration 69/1000 | Loss: 0.00003513
Iteration 70/1000 | Loss: 0.00003513
Iteration 71/1000 | Loss: 0.00003513
Iteration 72/1000 | Loss: 0.00003513
Iteration 73/1000 | Loss: 0.00003513
Iteration 74/1000 | Loss: 0.00003512
Iteration 75/1000 | Loss: 0.00003511
Iteration 76/1000 | Loss: 0.00003509
Iteration 77/1000 | Loss: 0.00003507
Iteration 78/1000 | Loss: 0.00003506
Iteration 79/1000 | Loss: 0.00003505
Iteration 80/1000 | Loss: 0.00003505
Iteration 81/1000 | Loss: 0.00003504
Iteration 82/1000 | Loss: 0.00003504
Iteration 83/1000 | Loss: 0.00003503
Iteration 84/1000 | Loss: 0.00003502
Iteration 85/1000 | Loss: 0.00003502
Iteration 86/1000 | Loss: 0.00003502
Iteration 87/1000 | Loss: 0.00003502
Iteration 88/1000 | Loss: 0.00003501
Iteration 89/1000 | Loss: 0.00003501
Iteration 90/1000 | Loss: 0.00003501
Iteration 91/1000 | Loss: 0.00003501
Iteration 92/1000 | Loss: 0.00003501
Iteration 93/1000 | Loss: 0.00003501
Iteration 94/1000 | Loss: 0.00003501
Iteration 95/1000 | Loss: 0.00003501
Iteration 96/1000 | Loss: 0.00003501
Iteration 97/1000 | Loss: 0.00003501
Iteration 98/1000 | Loss: 0.00003501
Iteration 99/1000 | Loss: 0.00003501
Iteration 100/1000 | Loss: 0.00003500
Iteration 101/1000 | Loss: 0.00003500
Iteration 102/1000 | Loss: 0.00003500
Iteration 103/1000 | Loss: 0.00003500
Iteration 104/1000 | Loss: 0.00003500
Iteration 105/1000 | Loss: 0.00003500
Iteration 106/1000 | Loss: 0.00003499
Iteration 107/1000 | Loss: 0.00003499
Iteration 108/1000 | Loss: 0.00003498
Iteration 109/1000 | Loss: 0.00003497
Iteration 110/1000 | Loss: 0.00003497
Iteration 111/1000 | Loss: 0.00003497
Iteration 112/1000 | Loss: 0.00003493
Iteration 113/1000 | Loss: 0.00003493
Iteration 114/1000 | Loss: 0.00003492
Iteration 115/1000 | Loss: 0.00003492
Iteration 116/1000 | Loss: 0.00003492
Iteration 117/1000 | Loss: 0.00003491
Iteration 118/1000 | Loss: 0.00003491
Iteration 119/1000 | Loss: 0.00003491
Iteration 120/1000 | Loss: 0.00003490
Iteration 121/1000 | Loss: 0.00003490
Iteration 122/1000 | Loss: 0.00003490
Iteration 123/1000 | Loss: 0.00003490
Iteration 124/1000 | Loss: 0.00003490
Iteration 125/1000 | Loss: 0.00003490
Iteration 126/1000 | Loss: 0.00003489
Iteration 127/1000 | Loss: 0.00003489
Iteration 128/1000 | Loss: 0.00003489
Iteration 129/1000 | Loss: 0.00003489
Iteration 130/1000 | Loss: 0.00003489
Iteration 131/1000 | Loss: 0.00003488
Iteration 132/1000 | Loss: 0.00003488
Iteration 133/1000 | Loss: 0.00003488
Iteration 134/1000 | Loss: 0.00003487
Iteration 135/1000 | Loss: 0.00003487
Iteration 136/1000 | Loss: 0.00003487
Iteration 137/1000 | Loss: 0.00003487
Iteration 138/1000 | Loss: 0.00003486
Iteration 139/1000 | Loss: 0.00003486
Iteration 140/1000 | Loss: 0.00003486
Iteration 141/1000 | Loss: 0.00003486
Iteration 142/1000 | Loss: 0.00003486
Iteration 143/1000 | Loss: 0.00003486
Iteration 144/1000 | Loss: 0.00003486
Iteration 145/1000 | Loss: 0.00003486
Iteration 146/1000 | Loss: 0.00003486
Iteration 147/1000 | Loss: 0.00003486
Iteration 148/1000 | Loss: 0.00003485
Iteration 149/1000 | Loss: 0.00003485
Iteration 150/1000 | Loss: 0.00003485
Iteration 151/1000 | Loss: 0.00003485
Iteration 152/1000 | Loss: 0.00003485
Iteration 153/1000 | Loss: 0.00003485
Iteration 154/1000 | Loss: 0.00003484
Iteration 155/1000 | Loss: 0.00003484
Iteration 156/1000 | Loss: 0.00003484
Iteration 157/1000 | Loss: 0.00003484
Iteration 158/1000 | Loss: 0.00003483
Iteration 159/1000 | Loss: 0.00003483
Iteration 160/1000 | Loss: 0.00003483
Iteration 161/1000 | Loss: 0.00003483
Iteration 162/1000 | Loss: 0.00003483
Iteration 163/1000 | Loss: 0.00003483
Iteration 164/1000 | Loss: 0.00003483
Iteration 165/1000 | Loss: 0.00003483
Iteration 166/1000 | Loss: 0.00003483
Iteration 167/1000 | Loss: 0.00003483
Iteration 168/1000 | Loss: 0.00003483
Iteration 169/1000 | Loss: 0.00003483
Iteration 170/1000 | Loss: 0.00003483
Iteration 171/1000 | Loss: 0.00003483
Iteration 172/1000 | Loss: 0.00003483
Iteration 173/1000 | Loss: 0.00003483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.482764077489264e-05, 3.482764077489264e-05, 3.482764077489264e-05, 3.482764077489264e-05, 3.482764077489264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.482764077489264e-05

Optimization complete. Final v2v error: 4.673498630523682 mm

Highest mean error: 14.51834487915039 mm for frame 45

Lowest mean error: 3.8234050273895264 mm for frame 138

Saving results

Total time: 4201.629198074341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0015
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839567
Iteration 2/25 | Loss: 0.00136828
Iteration 3/25 | Loss: 0.00102221
Iteration 4/25 | Loss: 0.00095949
Iteration 5/25 | Loss: 0.00094913
Iteration 6/25 | Loss: 0.00094706
Iteration 7/25 | Loss: 0.00094706
Iteration 8/25 | Loss: 0.00094706
Iteration 9/25 | Loss: 0.00094706
Iteration 10/25 | Loss: 0.00094706
Iteration 11/25 | Loss: 0.00094706
Iteration 12/25 | Loss: 0.00094706
Iteration 13/25 | Loss: 0.00094706
Iteration 14/25 | Loss: 0.00094706
Iteration 15/25 | Loss: 0.00094706
Iteration 16/25 | Loss: 0.00094706
Iteration 17/25 | Loss: 0.00094706
Iteration 18/25 | Loss: 0.00094706
Iteration 19/25 | Loss: 0.00094706
Iteration 20/25 | Loss: 0.00094706
Iteration 21/25 | Loss: 0.00094706
Iteration 22/25 | Loss: 0.00094706
Iteration 23/25 | Loss: 0.00094706
Iteration 24/25 | Loss: 0.00094706
Iteration 25/25 | Loss: 0.00094706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36869812
Iteration 2/25 | Loss: 0.00045729
Iteration 3/25 | Loss: 0.00045726
Iteration 4/25 | Loss: 0.00045726
Iteration 5/25 | Loss: 0.00045725
Iteration 6/25 | Loss: 0.00045725
Iteration 7/25 | Loss: 0.00045725
Iteration 8/25 | Loss: 0.00045725
Iteration 9/25 | Loss: 0.00045725
Iteration 10/25 | Loss: 0.00045725
Iteration 11/25 | Loss: 0.00045725
Iteration 12/25 | Loss: 0.00045725
Iteration 13/25 | Loss: 0.00045725
Iteration 14/25 | Loss: 0.00045725
Iteration 15/25 | Loss: 0.00045725
Iteration 16/25 | Loss: 0.00045725
Iteration 17/25 | Loss: 0.00045725
Iteration 18/25 | Loss: 0.00045725
Iteration 19/25 | Loss: 0.00045725
Iteration 20/25 | Loss: 0.00045725
Iteration 21/25 | Loss: 0.00045725
Iteration 22/25 | Loss: 0.00045725
Iteration 23/25 | Loss: 0.00045725
Iteration 24/25 | Loss: 0.00045725
Iteration 25/25 | Loss: 0.00045725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045725
Iteration 2/1000 | Loss: 0.00003448
Iteration 3/1000 | Loss: 0.00002808
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002321
Iteration 7/1000 | Loss: 0.00002246
Iteration 8/1000 | Loss: 0.00002189
Iteration 9/1000 | Loss: 0.00002152
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002120
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002093
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00002087
Iteration 16/1000 | Loss: 0.00002083
Iteration 17/1000 | Loss: 0.00002082
Iteration 18/1000 | Loss: 0.00002082
Iteration 19/1000 | Loss: 0.00002081
Iteration 20/1000 | Loss: 0.00002080
Iteration 21/1000 | Loss: 0.00002080
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002079
Iteration 24/1000 | Loss: 0.00002079
Iteration 25/1000 | Loss: 0.00002079
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002078
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002077
Iteration 31/1000 | Loss: 0.00002077
Iteration 32/1000 | Loss: 0.00002076
Iteration 33/1000 | Loss: 0.00002076
Iteration 34/1000 | Loss: 0.00002075
Iteration 35/1000 | Loss: 0.00002075
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002075
Iteration 38/1000 | Loss: 0.00002074
Iteration 39/1000 | Loss: 0.00002074
Iteration 40/1000 | Loss: 0.00002074
Iteration 41/1000 | Loss: 0.00002074
Iteration 42/1000 | Loss: 0.00002074
Iteration 43/1000 | Loss: 0.00002072
Iteration 44/1000 | Loss: 0.00002072
Iteration 45/1000 | Loss: 0.00002072
Iteration 46/1000 | Loss: 0.00002071
Iteration 47/1000 | Loss: 0.00002071
Iteration 48/1000 | Loss: 0.00002071
Iteration 49/1000 | Loss: 0.00002070
Iteration 50/1000 | Loss: 0.00002070
Iteration 51/1000 | Loss: 0.00002070
Iteration 52/1000 | Loss: 0.00002070
Iteration 53/1000 | Loss: 0.00002070
Iteration 54/1000 | Loss: 0.00002070
Iteration 55/1000 | Loss: 0.00002070
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002069
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002068
Iteration 65/1000 | Loss: 0.00002068
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002066
Iteration 75/1000 | Loss: 0.00002066
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002065
Iteration 79/1000 | Loss: 0.00002065
Iteration 80/1000 | Loss: 0.00002065
Iteration 81/1000 | Loss: 0.00002065
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.064664295176044e-05, 2.064664295176044e-05, 2.064664295176044e-05, 2.064664295176044e-05, 2.064664295176044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.064664295176044e-05

Optimization complete. Final v2v error: 3.940201759338379 mm

Highest mean error: 4.555981636047363 mm for frame 21

Lowest mean error: 3.5906667709350586 mm for frame 239

Saving results

Total time: 1155.0948839187622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0016
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460963
Iteration 2/25 | Loss: 0.00117740
Iteration 3/25 | Loss: 0.00094836
Iteration 4/25 | Loss: 0.00090838
Iteration 5/25 | Loss: 0.00089690
Iteration 6/25 | Loss: 0.00089474
Iteration 7/25 | Loss: 0.00089448
Iteration 8/25 | Loss: 0.00089448
Iteration 9/25 | Loss: 0.00089448
Iteration 10/25 | Loss: 0.00089448
Iteration 11/25 | Loss: 0.00089448
Iteration 12/25 | Loss: 0.00089448
Iteration 13/25 | Loss: 0.00089448
Iteration 14/25 | Loss: 0.00089448
Iteration 15/25 | Loss: 0.00089448
Iteration 16/25 | Loss: 0.00089448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008944845758378506, 0.0008944845758378506, 0.0008944845758378506, 0.0008944845758378506, 0.0008944845758378506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008944845758378506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37539697
Iteration 2/25 | Loss: 0.00037166
Iteration 3/25 | Loss: 0.00037166
Iteration 4/25 | Loss: 0.00037166
Iteration 5/25 | Loss: 0.00037166
Iteration 6/25 | Loss: 0.00037166
Iteration 7/25 | Loss: 0.00037166
Iteration 8/25 | Loss: 0.00037166
Iteration 9/25 | Loss: 0.00037166
Iteration 10/25 | Loss: 0.00037166
Iteration 11/25 | Loss: 0.00037166
Iteration 12/25 | Loss: 0.00037166
Iteration 13/25 | Loss: 0.00037166
Iteration 14/25 | Loss: 0.00037166
Iteration 15/25 | Loss: 0.00037166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0003716609498951584, 0.0003716609498951584, 0.0003716609498951584, 0.0003716609498951584, 0.0003716609498951584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003716609498951584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037166
Iteration 2/1000 | Loss: 0.00003471
Iteration 3/1000 | Loss: 0.00002577
Iteration 4/1000 | Loss: 0.00002291
Iteration 5/1000 | Loss: 0.00002109
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001913
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001839
Iteration 11/1000 | Loss: 0.00001815
Iteration 12/1000 | Loss: 0.00001794
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001745
Iteration 31/1000 | Loss: 0.00001745
Iteration 32/1000 | Loss: 0.00001744
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001734
Iteration 76/1000 | Loss: 0.00001734
Iteration 77/1000 | Loss: 0.00001734
Iteration 78/1000 | Loss: 0.00001734
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001731
Iteration 93/1000 | Loss: 0.00001731
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001730
Iteration 97/1000 | Loss: 0.00001730
Iteration 98/1000 | Loss: 0.00001730
Iteration 99/1000 | Loss: 0.00001730
Iteration 100/1000 | Loss: 0.00001730
Iteration 101/1000 | Loss: 0.00001730
Iteration 102/1000 | Loss: 0.00001730
Iteration 103/1000 | Loss: 0.00001729
Iteration 104/1000 | Loss: 0.00001729
Iteration 105/1000 | Loss: 0.00001729
Iteration 106/1000 | Loss: 0.00001729
Iteration 107/1000 | Loss: 0.00001729
Iteration 108/1000 | Loss: 0.00001729
Iteration 109/1000 | Loss: 0.00001729
Iteration 110/1000 | Loss: 0.00001729
Iteration 111/1000 | Loss: 0.00001729
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001727
Iteration 126/1000 | Loss: 0.00001727
Iteration 127/1000 | Loss: 0.00001727
Iteration 128/1000 | Loss: 0.00001727
Iteration 129/1000 | Loss: 0.00001727
Iteration 130/1000 | Loss: 0.00001727
Iteration 131/1000 | Loss: 0.00001727
Iteration 132/1000 | Loss: 0.00001726
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00001726
Iteration 135/1000 | Loss: 0.00001726
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001726
Iteration 140/1000 | Loss: 0.00001726
Iteration 141/1000 | Loss: 0.00001725
Iteration 142/1000 | Loss: 0.00001725
Iteration 143/1000 | Loss: 0.00001725
Iteration 144/1000 | Loss: 0.00001725
Iteration 145/1000 | Loss: 0.00001725
Iteration 146/1000 | Loss: 0.00001725
Iteration 147/1000 | Loss: 0.00001725
Iteration 148/1000 | Loss: 0.00001725
Iteration 149/1000 | Loss: 0.00001725
Iteration 150/1000 | Loss: 0.00001725
Iteration 151/1000 | Loss: 0.00001725
Iteration 152/1000 | Loss: 0.00001725
Iteration 153/1000 | Loss: 0.00001725
Iteration 154/1000 | Loss: 0.00001725
Iteration 155/1000 | Loss: 0.00001725
Iteration 156/1000 | Loss: 0.00001725
Iteration 157/1000 | Loss: 0.00001725
Iteration 158/1000 | Loss: 0.00001724
Iteration 159/1000 | Loss: 0.00001724
Iteration 160/1000 | Loss: 0.00001724
Iteration 161/1000 | Loss: 0.00001724
Iteration 162/1000 | Loss: 0.00001724
Iteration 163/1000 | Loss: 0.00001724
Iteration 164/1000 | Loss: 0.00001724
Iteration 165/1000 | Loss: 0.00001724
Iteration 166/1000 | Loss: 0.00001724
Iteration 167/1000 | Loss: 0.00001724
Iteration 168/1000 | Loss: 0.00001724
Iteration 169/1000 | Loss: 0.00001724
Iteration 170/1000 | Loss: 0.00001724
Iteration 171/1000 | Loss: 0.00001724
Iteration 172/1000 | Loss: 0.00001724
Iteration 173/1000 | Loss: 0.00001724
Iteration 174/1000 | Loss: 0.00001724
Iteration 175/1000 | Loss: 0.00001724
Iteration 176/1000 | Loss: 0.00001724
Iteration 177/1000 | Loss: 0.00001724
Iteration 178/1000 | Loss: 0.00001724
Iteration 179/1000 | Loss: 0.00001724
Iteration 180/1000 | Loss: 0.00001724
Iteration 181/1000 | Loss: 0.00001724
Iteration 182/1000 | Loss: 0.00001724
Iteration 183/1000 | Loss: 0.00001724
Iteration 184/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.7237760403077118e-05, 1.7237760403077118e-05, 1.7237760403077118e-05, 1.7237760403077118e-05, 1.7237760403077118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7237760403077118e-05

Optimization complete. Final v2v error: 3.3751566410064697 mm

Highest mean error: 4.85921049118042 mm for frame 239

Lowest mean error: 3.0297160148620605 mm for frame 203

Saving results

Total time: 1360.8075625896454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0006
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796050
Iteration 2/25 | Loss: 0.00138551
Iteration 3/25 | Loss: 0.00109979
Iteration 4/25 | Loss: 0.00106059
Iteration 5/25 | Loss: 0.00104834
Iteration 6/25 | Loss: 0.00104564
Iteration 7/25 | Loss: 0.00104542
Iteration 8/25 | Loss: 0.00104542
Iteration 9/25 | Loss: 0.00104542
Iteration 10/25 | Loss: 0.00104542
Iteration 11/25 | Loss: 0.00104542
Iteration 12/25 | Loss: 0.00104542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010454208822920918, 0.0010454208822920918, 0.0010454208822920918, 0.0010454208822920918, 0.0010454208822920918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010454208822920918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.33617496
Iteration 2/25 | Loss: 0.00043584
Iteration 3/25 | Loss: 0.00043584
Iteration 4/25 | Loss: 0.00043584
Iteration 5/25 | Loss: 0.00043584
Iteration 6/25 | Loss: 0.00043584
Iteration 7/25 | Loss: 0.00043584
Iteration 8/25 | Loss: 0.00043584
Iteration 9/25 | Loss: 0.00043584
Iteration 10/25 | Loss: 0.00043584
Iteration 11/25 | Loss: 0.00043584
Iteration 12/25 | Loss: 0.00043584
Iteration 13/25 | Loss: 0.00043584
Iteration 14/25 | Loss: 0.00043584
Iteration 15/25 | Loss: 0.00043584
Iteration 16/25 | Loss: 0.00043584
Iteration 17/25 | Loss: 0.00043584
Iteration 18/25 | Loss: 0.00043584
Iteration 19/25 | Loss: 0.00043584
Iteration 20/25 | Loss: 0.00043584
Iteration 21/25 | Loss: 0.00043584
Iteration 22/25 | Loss: 0.00043584
Iteration 23/25 | Loss: 0.00043584
Iteration 24/25 | Loss: 0.00043584
Iteration 25/25 | Loss: 0.00043584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043584
Iteration 2/1000 | Loss: 0.00006190
Iteration 3/1000 | Loss: 0.00004310
Iteration 4/1000 | Loss: 0.00003960
Iteration 5/1000 | Loss: 0.00003752
Iteration 6/1000 | Loss: 0.00003648
Iteration 7/1000 | Loss: 0.00003543
Iteration 8/1000 | Loss: 0.00003473
Iteration 9/1000 | Loss: 0.00003433
Iteration 10/1000 | Loss: 0.00003400
Iteration 11/1000 | Loss: 0.00003374
Iteration 12/1000 | Loss: 0.00003356
Iteration 13/1000 | Loss: 0.00003342
Iteration 14/1000 | Loss: 0.00003339
Iteration 15/1000 | Loss: 0.00003335
Iteration 16/1000 | Loss: 0.00003332
Iteration 17/1000 | Loss: 0.00003330
Iteration 18/1000 | Loss: 0.00003328
Iteration 19/1000 | Loss: 0.00003328
Iteration 20/1000 | Loss: 0.00003328
Iteration 21/1000 | Loss: 0.00003327
Iteration 22/1000 | Loss: 0.00003327
Iteration 23/1000 | Loss: 0.00003327
Iteration 24/1000 | Loss: 0.00003327
Iteration 25/1000 | Loss: 0.00003327
Iteration 26/1000 | Loss: 0.00003326
Iteration 27/1000 | Loss: 0.00003325
Iteration 28/1000 | Loss: 0.00003325
Iteration 29/1000 | Loss: 0.00003324
Iteration 30/1000 | Loss: 0.00003323
Iteration 31/1000 | Loss: 0.00003323
Iteration 32/1000 | Loss: 0.00003322
Iteration 33/1000 | Loss: 0.00003322
Iteration 34/1000 | Loss: 0.00003322
Iteration 35/1000 | Loss: 0.00003322
Iteration 36/1000 | Loss: 0.00003322
Iteration 37/1000 | Loss: 0.00003321
Iteration 38/1000 | Loss: 0.00003321
Iteration 39/1000 | Loss: 0.00003321
Iteration 40/1000 | Loss: 0.00003320
Iteration 41/1000 | Loss: 0.00003320
Iteration 42/1000 | Loss: 0.00003319
Iteration 43/1000 | Loss: 0.00003319
Iteration 44/1000 | Loss: 0.00003319
Iteration 45/1000 | Loss: 0.00003318
Iteration 46/1000 | Loss: 0.00003318
Iteration 47/1000 | Loss: 0.00003317
Iteration 48/1000 | Loss: 0.00003317
Iteration 49/1000 | Loss: 0.00003317
Iteration 50/1000 | Loss: 0.00003316
Iteration 51/1000 | Loss: 0.00003316
Iteration 52/1000 | Loss: 0.00003316
Iteration 53/1000 | Loss: 0.00003315
Iteration 54/1000 | Loss: 0.00003315
Iteration 55/1000 | Loss: 0.00003315
Iteration 56/1000 | Loss: 0.00003315
Iteration 57/1000 | Loss: 0.00003315
Iteration 58/1000 | Loss: 0.00003315
Iteration 59/1000 | Loss: 0.00003314
Iteration 60/1000 | Loss: 0.00003314
Iteration 61/1000 | Loss: 0.00003314
Iteration 62/1000 | Loss: 0.00003314
Iteration 63/1000 | Loss: 0.00003313
Iteration 64/1000 | Loss: 0.00003313
Iteration 65/1000 | Loss: 0.00003313
Iteration 66/1000 | Loss: 0.00003313
Iteration 67/1000 | Loss: 0.00003312
Iteration 68/1000 | Loss: 0.00003312
Iteration 69/1000 | Loss: 0.00003312
Iteration 70/1000 | Loss: 0.00003311
Iteration 71/1000 | Loss: 0.00003311
Iteration 72/1000 | Loss: 0.00003311
Iteration 73/1000 | Loss: 0.00003310
Iteration 74/1000 | Loss: 0.00003310
Iteration 75/1000 | Loss: 0.00003309
Iteration 76/1000 | Loss: 0.00003309
Iteration 77/1000 | Loss: 0.00003309
Iteration 78/1000 | Loss: 0.00003309
Iteration 79/1000 | Loss: 0.00003309
Iteration 80/1000 | Loss: 0.00003309
Iteration 81/1000 | Loss: 0.00003309
Iteration 82/1000 | Loss: 0.00003309
Iteration 83/1000 | Loss: 0.00003309
Iteration 84/1000 | Loss: 0.00003308
Iteration 85/1000 | Loss: 0.00003308
Iteration 86/1000 | Loss: 0.00003308
Iteration 87/1000 | Loss: 0.00003308
Iteration 88/1000 | Loss: 0.00003308
Iteration 89/1000 | Loss: 0.00003308
Iteration 90/1000 | Loss: 0.00003307
Iteration 91/1000 | Loss: 0.00003307
Iteration 92/1000 | Loss: 0.00003307
Iteration 93/1000 | Loss: 0.00003307
Iteration 94/1000 | Loss: 0.00003307
Iteration 95/1000 | Loss: 0.00003307
Iteration 96/1000 | Loss: 0.00003307
Iteration 97/1000 | Loss: 0.00003307
Iteration 98/1000 | Loss: 0.00003307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [3.307260703877546e-05, 3.307260703877546e-05, 3.307260703877546e-05, 3.307260703877546e-05, 3.307260703877546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.307260703877546e-05

Optimization complete. Final v2v error: 4.8000264167785645 mm

Highest mean error: 5.391922473907471 mm for frame 199

Lowest mean error: 4.244208335876465 mm for frame 83

Saving results

Total time: 986.1803929805756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0023
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01175499
Iteration 2/25 | Loss: 0.00208515
Iteration 3/25 | Loss: 0.00159496
Iteration 4/25 | Loss: 0.00167859
Iteration 5/25 | Loss: 0.00175813
Iteration 6/25 | Loss: 0.00163003
Iteration 7/25 | Loss: 0.00133813
Iteration 8/25 | Loss: 0.00134056
Iteration 9/25 | Loss: 0.00130581
Iteration 10/25 | Loss: 0.00130139
Iteration 11/25 | Loss: 0.00123527
Iteration 12/25 | Loss: 0.00122562
Iteration 13/25 | Loss: 0.00119416
Iteration 14/25 | Loss: 0.00122073
Iteration 15/25 | Loss: 0.00121095
Iteration 16/25 | Loss: 0.00120099
Iteration 17/25 | Loss: 0.00121319
Iteration 18/25 | Loss: 0.00120198
Iteration 19/25 | Loss: 0.00120288
Iteration 20/25 | Loss: 0.00117856
Iteration 21/25 | Loss: 0.00120569
Iteration 22/25 | Loss: 0.00120257
Iteration 23/25 | Loss: 0.00119790
Iteration 24/25 | Loss: 0.00119229
Iteration 25/25 | Loss: 0.00115054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12715816
Iteration 2/25 | Loss: 0.00245288
Iteration 3/25 | Loss: 0.00245288
Iteration 4/25 | Loss: 0.00245288
Iteration 5/25 | Loss: 0.00245288
Iteration 6/25 | Loss: 0.00245288
Iteration 7/25 | Loss: 0.00245288
Iteration 8/25 | Loss: 0.00245288
Iteration 9/25 | Loss: 0.00245288
Iteration 10/25 | Loss: 0.00245288
Iteration 11/25 | Loss: 0.00245288
Iteration 12/25 | Loss: 0.00245288
Iteration 13/25 | Loss: 0.00245288
Iteration 14/25 | Loss: 0.00245288
Iteration 15/25 | Loss: 0.00245288
Iteration 16/25 | Loss: 0.00245288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024528803769499063, 0.0024528803769499063, 0.0024528803769499063, 0.0024528803769499063, 0.0024528803769499063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024528803769499063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245288
Iteration 2/1000 | Loss: 0.00108824
Iteration 3/1000 | Loss: 0.00242580
Iteration 4/1000 | Loss: 0.00286575
Iteration 5/1000 | Loss: 0.00201269
Iteration 6/1000 | Loss: 0.00154844
Iteration 7/1000 | Loss: 0.00143927
Iteration 8/1000 | Loss: 0.00123534
Iteration 9/1000 | Loss: 0.00112497
Iteration 10/1000 | Loss: 0.00107102
Iteration 11/1000 | Loss: 0.00102158
Iteration 12/1000 | Loss: 0.00116765
Iteration 13/1000 | Loss: 0.00126794
Iteration 14/1000 | Loss: 0.00131691
Iteration 15/1000 | Loss: 0.00150973
Iteration 16/1000 | Loss: 0.00149521
Iteration 17/1000 | Loss: 0.00340567
Iteration 18/1000 | Loss: 0.00193654
Iteration 19/1000 | Loss: 0.00401707
Iteration 20/1000 | Loss: 0.00324904
Iteration 21/1000 | Loss: 0.00452055
Iteration 22/1000 | Loss: 0.00372400
Iteration 23/1000 | Loss: 0.00143062
Iteration 24/1000 | Loss: 0.00145582
Iteration 25/1000 | Loss: 0.00492832
Iteration 26/1000 | Loss: 0.00287266
Iteration 27/1000 | Loss: 0.00238438
Iteration 28/1000 | Loss: 0.00366927
Iteration 29/1000 | Loss: 0.00329663
Iteration 30/1000 | Loss: 0.00360156
Iteration 31/1000 | Loss: 0.00302625
Iteration 32/1000 | Loss: 0.00182079
Iteration 33/1000 | Loss: 0.00181934
Iteration 34/1000 | Loss: 0.00244667
Iteration 35/1000 | Loss: 0.00224030
Iteration 36/1000 | Loss: 0.00158313
Iteration 37/1000 | Loss: 0.00302713
Iteration 38/1000 | Loss: 0.00077559
Iteration 39/1000 | Loss: 0.00101113
Iteration 40/1000 | Loss: 0.00096474
Iteration 41/1000 | Loss: 0.00054632
Iteration 42/1000 | Loss: 0.00068715
Iteration 43/1000 | Loss: 0.00068082
Iteration 44/1000 | Loss: 0.00060388
Iteration 45/1000 | Loss: 0.00036929
Iteration 46/1000 | Loss: 0.00073936
Iteration 47/1000 | Loss: 0.00090117
Iteration 48/1000 | Loss: 0.00295854
Iteration 49/1000 | Loss: 0.00192809
Iteration 50/1000 | Loss: 0.00059811
Iteration 51/1000 | Loss: 0.00116533
Iteration 52/1000 | Loss: 0.00222879
Iteration 53/1000 | Loss: 0.00260411
Iteration 54/1000 | Loss: 0.00191573
Iteration 55/1000 | Loss: 0.00384169
Iteration 56/1000 | Loss: 0.00231921
Iteration 57/1000 | Loss: 0.00148666
Iteration 58/1000 | Loss: 0.00114296
Iteration 59/1000 | Loss: 0.00256061
Iteration 60/1000 | Loss: 0.00175966
Iteration 61/1000 | Loss: 0.00222922
Iteration 62/1000 | Loss: 0.00187417
Iteration 63/1000 | Loss: 0.00181050
Iteration 64/1000 | Loss: 0.00144996
Iteration 65/1000 | Loss: 0.00324949
Iteration 66/1000 | Loss: 0.00063072
Iteration 67/1000 | Loss: 0.00297468
Iteration 68/1000 | Loss: 0.00439908
Iteration 69/1000 | Loss: 0.00391834
Iteration 70/1000 | Loss: 0.00066916
Iteration 71/1000 | Loss: 0.00060156
Iteration 72/1000 | Loss: 0.00038276
Iteration 73/1000 | Loss: 0.00039318
Iteration 74/1000 | Loss: 0.00253552
Iteration 75/1000 | Loss: 0.00037305
Iteration 76/1000 | Loss: 0.00031681
Iteration 77/1000 | Loss: 0.00017086
Iteration 78/1000 | Loss: 0.00055164
Iteration 79/1000 | Loss: 0.00071835
Iteration 80/1000 | Loss: 0.00056365
Iteration 81/1000 | Loss: 0.00055858
Iteration 82/1000 | Loss: 0.00090697
Iteration 83/1000 | Loss: 0.00044798
Iteration 84/1000 | Loss: 0.00053163
Iteration 85/1000 | Loss: 0.00055727
Iteration 86/1000 | Loss: 0.00058869
Iteration 87/1000 | Loss: 0.00061860
Iteration 88/1000 | Loss: 0.00062538
Iteration 89/1000 | Loss: 0.00039470
Iteration 90/1000 | Loss: 0.00043244
Iteration 91/1000 | Loss: 0.00045640
Iteration 92/1000 | Loss: 0.00026146
Iteration 93/1000 | Loss: 0.00022669
Iteration 94/1000 | Loss: 0.00023962
Iteration 95/1000 | Loss: 0.00050409
Iteration 96/1000 | Loss: 0.00021523
Iteration 97/1000 | Loss: 0.00024454
Iteration 98/1000 | Loss: 0.00040990
Iteration 99/1000 | Loss: 0.00039219
Iteration 100/1000 | Loss: 0.00040958
Iteration 101/1000 | Loss: 0.00037673
Iteration 102/1000 | Loss: 0.00040459
Iteration 103/1000 | Loss: 0.00040957
Iteration 104/1000 | Loss: 0.00064594
Iteration 105/1000 | Loss: 0.00048982
Iteration 106/1000 | Loss: 0.00045726
Iteration 107/1000 | Loss: 0.00046140
Iteration 108/1000 | Loss: 0.00051820
Iteration 109/1000 | Loss: 0.00062163
Iteration 110/1000 | Loss: 0.00039439
Iteration 111/1000 | Loss: 0.00042614
Iteration 112/1000 | Loss: 0.00051783
Iteration 113/1000 | Loss: 0.00027763
Iteration 114/1000 | Loss: 0.00031013
Iteration 115/1000 | Loss: 0.00014432
Iteration 116/1000 | Loss: 0.00012250
Iteration 117/1000 | Loss: 0.00020357
Iteration 118/1000 | Loss: 0.00018722
Iteration 119/1000 | Loss: 0.00025603
Iteration 120/1000 | Loss: 0.00024621
Iteration 121/1000 | Loss: 0.00007886
Iteration 122/1000 | Loss: 0.00007006
Iteration 123/1000 | Loss: 0.00007275
Iteration 124/1000 | Loss: 0.00015184
Iteration 125/1000 | Loss: 0.00011726
Iteration 126/1000 | Loss: 0.00016137
Iteration 127/1000 | Loss: 0.00013398
Iteration 128/1000 | Loss: 0.00016536
Iteration 129/1000 | Loss: 0.00012637
Iteration 130/1000 | Loss: 0.00027715
Iteration 131/1000 | Loss: 0.00012854
Iteration 132/1000 | Loss: 0.00015978
Iteration 133/1000 | Loss: 0.00014984
Iteration 134/1000 | Loss: 0.00021402
Iteration 135/1000 | Loss: 0.00014613
Iteration 136/1000 | Loss: 0.00023201
Iteration 137/1000 | Loss: 0.00014319
Iteration 138/1000 | Loss: 0.00023148
Iteration 139/1000 | Loss: 0.00016473
Iteration 140/1000 | Loss: 0.00020205
Iteration 141/1000 | Loss: 0.00006558
Iteration 142/1000 | Loss: 0.00007072
Iteration 143/1000 | Loss: 0.00005806
Iteration 144/1000 | Loss: 0.00006114
Iteration 145/1000 | Loss: 0.00031077
Iteration 146/1000 | Loss: 0.00007482
Iteration 147/1000 | Loss: 0.00011177
Iteration 148/1000 | Loss: 0.00008569
Iteration 149/1000 | Loss: 0.00015018
Iteration 150/1000 | Loss: 0.00021627
Iteration 151/1000 | Loss: 0.00021457
Iteration 152/1000 | Loss: 0.00020832
Iteration 153/1000 | Loss: 0.00010810
Iteration 154/1000 | Loss: 0.00014373
Iteration 155/1000 | Loss: 0.00023683
Iteration 156/1000 | Loss: 0.00015126
Iteration 157/1000 | Loss: 0.00009880
Iteration 158/1000 | Loss: 0.00007543
Iteration 159/1000 | Loss: 0.00014452
Iteration 160/1000 | Loss: 0.00008635
Iteration 161/1000 | Loss: 0.00014814
Iteration 162/1000 | Loss: 0.00019839
Iteration 163/1000 | Loss: 0.00073722
Iteration 164/1000 | Loss: 0.00057993
Iteration 165/1000 | Loss: 0.00034477
Iteration 166/1000 | Loss: 0.00052394
Iteration 167/1000 | Loss: 0.00030886
Iteration 168/1000 | Loss: 0.00051188
Iteration 169/1000 | Loss: 0.00026473
Iteration 170/1000 | Loss: 0.00015655
Iteration 171/1000 | Loss: 0.00004782
Iteration 172/1000 | Loss: 0.00004044
Iteration 173/1000 | Loss: 0.00003769
Iteration 174/1000 | Loss: 0.00003605
Iteration 175/1000 | Loss: 0.00003531
Iteration 176/1000 | Loss: 0.00003488
Iteration 177/1000 | Loss: 0.00004487
Iteration 178/1000 | Loss: 0.00007040
Iteration 179/1000 | Loss: 0.00005321
Iteration 180/1000 | Loss: 0.00006119
Iteration 181/1000 | Loss: 0.00005635
Iteration 182/1000 | Loss: 0.00005486
Iteration 183/1000 | Loss: 0.00004094
Iteration 184/1000 | Loss: 0.00003946
Iteration 185/1000 | Loss: 0.00003750
Iteration 186/1000 | Loss: 0.00003574
Iteration 187/1000 | Loss: 0.00003418
Iteration 188/1000 | Loss: 0.00003327
Iteration 189/1000 | Loss: 0.00003281
Iteration 190/1000 | Loss: 0.00003250
Iteration 191/1000 | Loss: 0.00003227
Iteration 192/1000 | Loss: 0.00003218
Iteration 193/1000 | Loss: 0.00003214
Iteration 194/1000 | Loss: 0.00003204
Iteration 195/1000 | Loss: 0.00003194
Iteration 196/1000 | Loss: 0.00003188
Iteration 197/1000 | Loss: 0.00003188
Iteration 198/1000 | Loss: 0.00003188
Iteration 199/1000 | Loss: 0.00003187
Iteration 200/1000 | Loss: 0.00003187
Iteration 201/1000 | Loss: 0.00003187
Iteration 202/1000 | Loss: 0.00003187
Iteration 203/1000 | Loss: 0.00003187
Iteration 204/1000 | Loss: 0.00003187
Iteration 205/1000 | Loss: 0.00003186
Iteration 206/1000 | Loss: 0.00003186
Iteration 207/1000 | Loss: 0.00003186
Iteration 208/1000 | Loss: 0.00003186
Iteration 209/1000 | Loss: 0.00003186
Iteration 210/1000 | Loss: 0.00003185
Iteration 211/1000 | Loss: 0.00003185
Iteration 212/1000 | Loss: 0.00003185
Iteration 213/1000 | Loss: 0.00003185
Iteration 214/1000 | Loss: 0.00003185
Iteration 215/1000 | Loss: 0.00003185
Iteration 216/1000 | Loss: 0.00003184
Iteration 217/1000 | Loss: 0.00003184
Iteration 218/1000 | Loss: 0.00003184
Iteration 219/1000 | Loss: 0.00003184
Iteration 220/1000 | Loss: 0.00003184
Iteration 221/1000 | Loss: 0.00003184
Iteration 222/1000 | Loss: 0.00003183
Iteration 223/1000 | Loss: 0.00003183
Iteration 224/1000 | Loss: 0.00003183
Iteration 225/1000 | Loss: 0.00003183
Iteration 226/1000 | Loss: 0.00003183
Iteration 227/1000 | Loss: 0.00003183
Iteration 228/1000 | Loss: 0.00003183
Iteration 229/1000 | Loss: 0.00003183
Iteration 230/1000 | Loss: 0.00003183
Iteration 231/1000 | Loss: 0.00003183
Iteration 232/1000 | Loss: 0.00003182
Iteration 233/1000 | Loss: 0.00003182
Iteration 234/1000 | Loss: 0.00003182
Iteration 235/1000 | Loss: 0.00003182
Iteration 236/1000 | Loss: 0.00003182
Iteration 237/1000 | Loss: 0.00003182
Iteration 238/1000 | Loss: 0.00003182
Iteration 239/1000 | Loss: 0.00003182
Iteration 240/1000 | Loss: 0.00003182
Iteration 241/1000 | Loss: 0.00003182
Iteration 242/1000 | Loss: 0.00003182
Iteration 243/1000 | Loss: 0.00003181
Iteration 244/1000 | Loss: 0.00003181
Iteration 245/1000 | Loss: 0.00003181
Iteration 246/1000 | Loss: 0.00003181
Iteration 247/1000 | Loss: 0.00003181
Iteration 248/1000 | Loss: 0.00003181
Iteration 249/1000 | Loss: 0.00003181
Iteration 250/1000 | Loss: 0.00003181
Iteration 251/1000 | Loss: 0.00003181
Iteration 252/1000 | Loss: 0.00003181
Iteration 253/1000 | Loss: 0.00003181
Iteration 254/1000 | Loss: 0.00003181
Iteration 255/1000 | Loss: 0.00003181
Iteration 256/1000 | Loss: 0.00003181
Iteration 257/1000 | Loss: 0.00003181
Iteration 258/1000 | Loss: 0.00003180
Iteration 259/1000 | Loss: 0.00003180
Iteration 260/1000 | Loss: 0.00003180
Iteration 261/1000 | Loss: 0.00003180
Iteration 262/1000 | Loss: 0.00003180
Iteration 263/1000 | Loss: 0.00003180
Iteration 264/1000 | Loss: 0.00003180
Iteration 265/1000 | Loss: 0.00003180
Iteration 266/1000 | Loss: 0.00003180
Iteration 267/1000 | Loss: 0.00003180
Iteration 268/1000 | Loss: 0.00003180
Iteration 269/1000 | Loss: 0.00003180
Iteration 270/1000 | Loss: 0.00003180
Iteration 271/1000 | Loss: 0.00003180
Iteration 272/1000 | Loss: 0.00003180
Iteration 273/1000 | Loss: 0.00003180
Iteration 274/1000 | Loss: 0.00003180
Iteration 275/1000 | Loss: 0.00003180
Iteration 276/1000 | Loss: 0.00003180
Iteration 277/1000 | Loss: 0.00003179
Iteration 278/1000 | Loss: 0.00003179
Iteration 279/1000 | Loss: 0.00003179
Iteration 280/1000 | Loss: 0.00003179
Iteration 281/1000 | Loss: 0.00003179
Iteration 282/1000 | Loss: 0.00003179
Iteration 283/1000 | Loss: 0.00003179
Iteration 284/1000 | Loss: 0.00003179
Iteration 285/1000 | Loss: 0.00003179
Iteration 286/1000 | Loss: 0.00003179
Iteration 287/1000 | Loss: 0.00003179
Iteration 288/1000 | Loss: 0.00003179
Iteration 289/1000 | Loss: 0.00003179
Iteration 290/1000 | Loss: 0.00003179
Iteration 291/1000 | Loss: 0.00003179
Iteration 292/1000 | Loss: 0.00003179
Iteration 293/1000 | Loss: 0.00003179
Iteration 294/1000 | Loss: 0.00003179
Iteration 295/1000 | Loss: 0.00003178
Iteration 296/1000 | Loss: 0.00003178
Iteration 297/1000 | Loss: 0.00003178
Iteration 298/1000 | Loss: 0.00003178
Iteration 299/1000 | Loss: 0.00003178
Iteration 300/1000 | Loss: 0.00003178
Iteration 301/1000 | Loss: 0.00003178
Iteration 302/1000 | Loss: 0.00003178
Iteration 303/1000 | Loss: 0.00003178
Iteration 304/1000 | Loss: 0.00003178
Iteration 305/1000 | Loss: 0.00003178
Iteration 306/1000 | Loss: 0.00003178
Iteration 307/1000 | Loss: 0.00003178
Iteration 308/1000 | Loss: 0.00003178
Iteration 309/1000 | Loss: 0.00003178
Iteration 310/1000 | Loss: 0.00003178
Iteration 311/1000 | Loss: 0.00003178
Iteration 312/1000 | Loss: 0.00003178
Iteration 313/1000 | Loss: 0.00003178
Iteration 314/1000 | Loss: 0.00003178
Iteration 315/1000 | Loss: 0.00003178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [3.1784435122972354e-05, 3.1784435122972354e-05, 3.1784435122972354e-05, 3.1784435122972354e-05, 3.1784435122972354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1784435122972354e-05

Optimization complete. Final v2v error: 4.507541179656982 mm

Highest mean error: 6.041983604431152 mm for frame 53

Lowest mean error: 4.028993606567383 mm for frame 100

Saving results

Total time: 7254.289274215698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0024
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540977
Iteration 2/25 | Loss: 0.00110318
Iteration 3/25 | Loss: 0.00096314
Iteration 4/25 | Loss: 0.00093823
Iteration 5/25 | Loss: 0.00093358
Iteration 6/25 | Loss: 0.00093301
Iteration 7/25 | Loss: 0.00093301
Iteration 8/25 | Loss: 0.00093301
Iteration 9/25 | Loss: 0.00093301
Iteration 10/25 | Loss: 0.00093301
Iteration 11/25 | Loss: 0.00093301
Iteration 12/25 | Loss: 0.00093301
Iteration 13/25 | Loss: 0.00093301
Iteration 14/25 | Loss: 0.00093301
Iteration 15/25 | Loss: 0.00093301
Iteration 16/25 | Loss: 0.00093301
Iteration 17/25 | Loss: 0.00093301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009330105385743082, 0.0009330105385743082, 0.0009330105385743082, 0.0009330105385743082, 0.0009330105385743082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009330105385743082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37415993
Iteration 2/25 | Loss: 0.00038112
Iteration 3/25 | Loss: 0.00038108
Iteration 4/25 | Loss: 0.00038108
Iteration 5/25 | Loss: 0.00038108
Iteration 6/25 | Loss: 0.00038108
Iteration 7/25 | Loss: 0.00038108
Iteration 8/25 | Loss: 0.00038108
Iteration 9/25 | Loss: 0.00038108
Iteration 10/25 | Loss: 0.00038108
Iteration 11/25 | Loss: 0.00038108
Iteration 12/25 | Loss: 0.00038108
Iteration 13/25 | Loss: 0.00038108
Iteration 14/25 | Loss: 0.00038108
Iteration 15/25 | Loss: 0.00038108
Iteration 16/25 | Loss: 0.00038108
Iteration 17/25 | Loss: 0.00038108
Iteration 18/25 | Loss: 0.00038108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003810778434854001, 0.0003810778434854001, 0.0003810778434854001, 0.0003810778434854001, 0.0003810778434854001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003810778434854001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038108
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00002478
Iteration 4/1000 | Loss: 0.00002194
Iteration 5/1000 | Loss: 0.00002071
Iteration 6/1000 | Loss: 0.00001963
Iteration 7/1000 | Loss: 0.00001888
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001765
Iteration 12/1000 | Loss: 0.00001760
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001754
Iteration 17/1000 | Loss: 0.00001750
Iteration 18/1000 | Loss: 0.00001750
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001742
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001736
Iteration 38/1000 | Loss: 0.00001736
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001735
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001731
Iteration 54/1000 | Loss: 0.00001731
Iteration 55/1000 | Loss: 0.00001731
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001729
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001728
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001728
Iteration 73/1000 | Loss: 0.00001728
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001727
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001725
Iteration 81/1000 | Loss: 0.00001725
Iteration 82/1000 | Loss: 0.00001725
Iteration 83/1000 | Loss: 0.00001725
Iteration 84/1000 | Loss: 0.00001725
Iteration 85/1000 | Loss: 0.00001724
Iteration 86/1000 | Loss: 0.00001724
Iteration 87/1000 | Loss: 0.00001724
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001723
Iteration 90/1000 | Loss: 0.00001723
Iteration 91/1000 | Loss: 0.00001723
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001721
Iteration 100/1000 | Loss: 0.00001720
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001719
Iteration 107/1000 | Loss: 0.00001719
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001717
Iteration 118/1000 | Loss: 0.00001717
Iteration 119/1000 | Loss: 0.00001717
Iteration 120/1000 | Loss: 0.00001717
Iteration 121/1000 | Loss: 0.00001717
Iteration 122/1000 | Loss: 0.00001717
Iteration 123/1000 | Loss: 0.00001717
Iteration 124/1000 | Loss: 0.00001717
Iteration 125/1000 | Loss: 0.00001717
Iteration 126/1000 | Loss: 0.00001717
Iteration 127/1000 | Loss: 0.00001717
Iteration 128/1000 | Loss: 0.00001716
Iteration 129/1000 | Loss: 0.00001716
Iteration 130/1000 | Loss: 0.00001716
Iteration 131/1000 | Loss: 0.00001716
Iteration 132/1000 | Loss: 0.00001716
Iteration 133/1000 | Loss: 0.00001716
Iteration 134/1000 | Loss: 0.00001716
Iteration 135/1000 | Loss: 0.00001716
Iteration 136/1000 | Loss: 0.00001716
Iteration 137/1000 | Loss: 0.00001716
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001715
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001715
Iteration 157/1000 | Loss: 0.00001715
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001714
Iteration 162/1000 | Loss: 0.00001714
Iteration 163/1000 | Loss: 0.00001714
Iteration 164/1000 | Loss: 0.00001714
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00001714
Iteration 167/1000 | Loss: 0.00001714
Iteration 168/1000 | Loss: 0.00001714
Iteration 169/1000 | Loss: 0.00001714
Iteration 170/1000 | Loss: 0.00001714
Iteration 171/1000 | Loss: 0.00001714
Iteration 172/1000 | Loss: 0.00001714
Iteration 173/1000 | Loss: 0.00001714
Iteration 174/1000 | Loss: 0.00001714
Iteration 175/1000 | Loss: 0.00001714
Iteration 176/1000 | Loss: 0.00001714
Iteration 177/1000 | Loss: 0.00001714
Iteration 178/1000 | Loss: 0.00001714
Iteration 179/1000 | Loss: 0.00001714
Iteration 180/1000 | Loss: 0.00001714
Iteration 181/1000 | Loss: 0.00001714
Iteration 182/1000 | Loss: 0.00001714
Iteration 183/1000 | Loss: 0.00001714
Iteration 184/1000 | Loss: 0.00001713
Iteration 185/1000 | Loss: 0.00001713
Iteration 186/1000 | Loss: 0.00001713
Iteration 187/1000 | Loss: 0.00001713
Iteration 188/1000 | Loss: 0.00001713
Iteration 189/1000 | Loss: 0.00001713
Iteration 190/1000 | Loss: 0.00001713
Iteration 191/1000 | Loss: 0.00001713
Iteration 192/1000 | Loss: 0.00001713
Iteration 193/1000 | Loss: 0.00001713
Iteration 194/1000 | Loss: 0.00001713
Iteration 195/1000 | Loss: 0.00001713
Iteration 196/1000 | Loss: 0.00001713
Iteration 197/1000 | Loss: 0.00001713
Iteration 198/1000 | Loss: 0.00001713
Iteration 199/1000 | Loss: 0.00001713
Iteration 200/1000 | Loss: 0.00001713
Iteration 201/1000 | Loss: 0.00001713
Iteration 202/1000 | Loss: 0.00001713
Iteration 203/1000 | Loss: 0.00001713
Iteration 204/1000 | Loss: 0.00001713
Iteration 205/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.7128888430306688e-05, 1.7128888430306688e-05, 1.7128888430306688e-05, 1.7128888430306688e-05, 1.7128888430306688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7128888430306688e-05

Optimization complete. Final v2v error: 3.5776336193084717 mm

Highest mean error: 3.8423116207122803 mm for frame 248

Lowest mean error: 3.354951858520508 mm for frame 96

Saving results

Total time: 1440.055146932602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0012
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035598
Iteration 2/25 | Loss: 0.00227277
Iteration 3/25 | Loss: 0.00171996
Iteration 4/25 | Loss: 0.00123896
Iteration 5/25 | Loss: 0.00120385
Iteration 6/25 | Loss: 0.00117460
Iteration 7/25 | Loss: 0.00116102
Iteration 8/25 | Loss: 0.00113160
Iteration 9/25 | Loss: 0.00115084
Iteration 10/25 | Loss: 0.00114043
Iteration 11/25 | Loss: 0.00111820
Iteration 12/25 | Loss: 0.00111321
Iteration 13/25 | Loss: 0.00110745
Iteration 14/25 | Loss: 0.00110603
Iteration 15/25 | Loss: 0.00110550
Iteration 16/25 | Loss: 0.00110226
Iteration 17/25 | Loss: 0.00110088
Iteration 18/25 | Loss: 0.00110349
Iteration 19/25 | Loss: 0.00110086
Iteration 20/25 | Loss: 0.00110112
Iteration 21/25 | Loss: 0.00110249
Iteration 22/25 | Loss: 0.00110116
Iteration 23/25 | Loss: 0.00109940
Iteration 24/25 | Loss: 0.00109852
Iteration 25/25 | Loss: 0.00109821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40732241
Iteration 2/25 | Loss: 0.00249539
Iteration 3/25 | Loss: 0.00249539
Iteration 4/25 | Loss: 0.00219012
Iteration 5/25 | Loss: 0.00218981
Iteration 6/25 | Loss: 0.00218981
Iteration 7/25 | Loss: 0.00218980
Iteration 8/25 | Loss: 0.00218980
Iteration 9/25 | Loss: 0.00218980
Iteration 10/25 | Loss: 0.00218980
Iteration 11/25 | Loss: 0.00218980
Iteration 12/25 | Loss: 0.00218980
Iteration 13/25 | Loss: 0.00218980
Iteration 14/25 | Loss: 0.00218980
Iteration 15/25 | Loss: 0.00218980
Iteration 16/25 | Loss: 0.00218980
Iteration 17/25 | Loss: 0.00218980
Iteration 18/25 | Loss: 0.00218980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021898034028708935, 0.0021898034028708935, 0.0021898034028708935, 0.0021898034028708935, 0.0021898034028708935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021898034028708935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218980
Iteration 2/1000 | Loss: 0.00052930
Iteration 3/1000 | Loss: 0.00078040
Iteration 4/1000 | Loss: 0.00176580
Iteration 5/1000 | Loss: 0.00160538
Iteration 6/1000 | Loss: 0.00097800
Iteration 7/1000 | Loss: 0.00154080
Iteration 8/1000 | Loss: 0.00141438
Iteration 9/1000 | Loss: 0.00053193
Iteration 10/1000 | Loss: 0.00036777
Iteration 11/1000 | Loss: 0.00123679
Iteration 12/1000 | Loss: 0.00119850
Iteration 13/1000 | Loss: 0.00102988
Iteration 14/1000 | Loss: 0.00047950
Iteration 15/1000 | Loss: 0.00063582
Iteration 16/1000 | Loss: 0.00068399
Iteration 17/1000 | Loss: 0.00071530
Iteration 18/1000 | Loss: 0.00134121
Iteration 19/1000 | Loss: 0.00065291
Iteration 20/1000 | Loss: 0.00018503
Iteration 21/1000 | Loss: 0.00036839
Iteration 22/1000 | Loss: 0.00036828
Iteration 23/1000 | Loss: 0.00145665
Iteration 24/1000 | Loss: 0.00041520
Iteration 25/1000 | Loss: 0.00031301
Iteration 26/1000 | Loss: 0.00086963
Iteration 27/1000 | Loss: 0.00051053
Iteration 28/1000 | Loss: 0.00048406
Iteration 29/1000 | Loss: 0.00042967
Iteration 30/1000 | Loss: 0.00012701
Iteration 31/1000 | Loss: 0.00022393
Iteration 32/1000 | Loss: 0.00018930
Iteration 33/1000 | Loss: 0.00010300
Iteration 34/1000 | Loss: 0.00014950
Iteration 35/1000 | Loss: 0.00015125
Iteration 36/1000 | Loss: 0.00016564
Iteration 37/1000 | Loss: 0.00015667
Iteration 38/1000 | Loss: 0.00011038
Iteration 39/1000 | Loss: 0.00057115
Iteration 40/1000 | Loss: 0.00117120
Iteration 41/1000 | Loss: 0.00058622
Iteration 42/1000 | Loss: 0.00052117
Iteration 43/1000 | Loss: 0.00052927
Iteration 44/1000 | Loss: 0.00072498
Iteration 45/1000 | Loss: 0.00131714
Iteration 46/1000 | Loss: 0.00031287
Iteration 47/1000 | Loss: 0.00042385
Iteration 48/1000 | Loss: 0.00007289
Iteration 49/1000 | Loss: 0.00006128
Iteration 50/1000 | Loss: 0.00082671
Iteration 51/1000 | Loss: 0.00074838
Iteration 52/1000 | Loss: 0.00056800
Iteration 53/1000 | Loss: 0.00051626
Iteration 54/1000 | Loss: 0.00039319
Iteration 55/1000 | Loss: 0.00045941
Iteration 56/1000 | Loss: 0.00009265
Iteration 57/1000 | Loss: 0.00140069
Iteration 58/1000 | Loss: 0.00081771
Iteration 59/1000 | Loss: 0.00072364
Iteration 60/1000 | Loss: 0.00007424
Iteration 61/1000 | Loss: 0.00005662
Iteration 62/1000 | Loss: 0.00004830
Iteration 63/1000 | Loss: 0.00004434
Iteration 64/1000 | Loss: 0.00004117
Iteration 65/1000 | Loss: 0.00044758
Iteration 66/1000 | Loss: 0.00019863
Iteration 67/1000 | Loss: 0.00003730
Iteration 68/1000 | Loss: 0.00003564
Iteration 69/1000 | Loss: 0.00003474
Iteration 70/1000 | Loss: 0.00039858
Iteration 71/1000 | Loss: 0.00004920
Iteration 72/1000 | Loss: 0.00003698
Iteration 73/1000 | Loss: 0.00003516
Iteration 74/1000 | Loss: 0.00003463
Iteration 75/1000 | Loss: 0.00003427
Iteration 76/1000 | Loss: 0.00003388
Iteration 77/1000 | Loss: 0.00003364
Iteration 78/1000 | Loss: 0.00052597
Iteration 79/1000 | Loss: 0.00127635
Iteration 80/1000 | Loss: 0.00018568
Iteration 81/1000 | Loss: 0.00005305
Iteration 82/1000 | Loss: 0.00004279
Iteration 83/1000 | Loss: 0.00031748
Iteration 84/1000 | Loss: 0.00003920
Iteration 85/1000 | Loss: 0.00024362
Iteration 86/1000 | Loss: 0.00040013
Iteration 87/1000 | Loss: 0.00034497
Iteration 88/1000 | Loss: 0.00048328
Iteration 89/1000 | Loss: 0.00044420
Iteration 90/1000 | Loss: 0.00047282
Iteration 91/1000 | Loss: 0.00045136
Iteration 92/1000 | Loss: 0.00040234
Iteration 93/1000 | Loss: 0.00060863
Iteration 94/1000 | Loss: 0.00030283
Iteration 95/1000 | Loss: 0.00004765
Iteration 96/1000 | Loss: 0.00005470
Iteration 97/1000 | Loss: 0.00041769
Iteration 98/1000 | Loss: 0.00029215
Iteration 99/1000 | Loss: 0.00044948
Iteration 100/1000 | Loss: 0.00048967
Iteration 101/1000 | Loss: 0.00005190
Iteration 102/1000 | Loss: 0.00036469
Iteration 103/1000 | Loss: 0.00004546
Iteration 104/1000 | Loss: 0.00038001
Iteration 105/1000 | Loss: 0.00048589
Iteration 106/1000 | Loss: 0.00028823
Iteration 107/1000 | Loss: 0.00030613
Iteration 108/1000 | Loss: 0.00060754
Iteration 109/1000 | Loss: 0.00035823
Iteration 110/1000 | Loss: 0.00025986
Iteration 111/1000 | Loss: 0.00062093
Iteration 112/1000 | Loss: 0.00034902
Iteration 113/1000 | Loss: 0.00026529
Iteration 114/1000 | Loss: 0.00019522
Iteration 115/1000 | Loss: 0.00004066
Iteration 116/1000 | Loss: 0.00003488
Iteration 117/1000 | Loss: 0.00003324
Iteration 118/1000 | Loss: 0.00003290
Iteration 119/1000 | Loss: 0.00003286
Iteration 120/1000 | Loss: 0.00038691
Iteration 121/1000 | Loss: 0.00004347
Iteration 122/1000 | Loss: 0.00005015
Iteration 123/1000 | Loss: 0.00021047
Iteration 124/1000 | Loss: 0.00027420
Iteration 125/1000 | Loss: 0.00050379
Iteration 126/1000 | Loss: 0.00040812
Iteration 127/1000 | Loss: 0.00046590
Iteration 128/1000 | Loss: 0.00044606
Iteration 129/1000 | Loss: 0.00051436
Iteration 130/1000 | Loss: 0.00019936
Iteration 131/1000 | Loss: 0.00005281
Iteration 132/1000 | Loss: 0.00004498
Iteration 133/1000 | Loss: 0.00003949
Iteration 134/1000 | Loss: 0.00022850
Iteration 135/1000 | Loss: 0.00009192
Iteration 136/1000 | Loss: 0.00010233
Iteration 137/1000 | Loss: 0.00009240
Iteration 138/1000 | Loss: 0.00037579
Iteration 139/1000 | Loss: 0.00037990
Iteration 140/1000 | Loss: 0.00029203
Iteration 141/1000 | Loss: 0.00048021
Iteration 142/1000 | Loss: 0.00028730
Iteration 143/1000 | Loss: 0.00008991
Iteration 144/1000 | Loss: 0.00022162
Iteration 145/1000 | Loss: 0.00026237
Iteration 146/1000 | Loss: 0.00043087
Iteration 147/1000 | Loss: 0.00047967
Iteration 148/1000 | Loss: 0.00054451
Iteration 149/1000 | Loss: 0.00019367
Iteration 150/1000 | Loss: 0.00028113
Iteration 151/1000 | Loss: 0.00020025
Iteration 152/1000 | Loss: 0.00041590
Iteration 153/1000 | Loss: 0.00012465
Iteration 154/1000 | Loss: 0.00003584
Iteration 155/1000 | Loss: 0.00003173
Iteration 156/1000 | Loss: 0.00003084
Iteration 157/1000 | Loss: 0.00039591
Iteration 158/1000 | Loss: 0.00008155
Iteration 159/1000 | Loss: 0.00003308
Iteration 160/1000 | Loss: 0.00003114
Iteration 161/1000 | Loss: 0.00003035
Iteration 162/1000 | Loss: 0.00003007
Iteration 163/1000 | Loss: 0.00003006
Iteration 164/1000 | Loss: 0.00003006
Iteration 165/1000 | Loss: 0.00003004
Iteration 166/1000 | Loss: 0.00003004
Iteration 167/1000 | Loss: 0.00003003
Iteration 168/1000 | Loss: 0.00003003
Iteration 169/1000 | Loss: 0.00003003
Iteration 170/1000 | Loss: 0.00003002
Iteration 171/1000 | Loss: 0.00002999
Iteration 172/1000 | Loss: 0.00002996
Iteration 173/1000 | Loss: 0.00002995
Iteration 174/1000 | Loss: 0.00002995
Iteration 175/1000 | Loss: 0.00002994
Iteration 176/1000 | Loss: 0.00002994
Iteration 177/1000 | Loss: 0.00002993
Iteration 178/1000 | Loss: 0.00002993
Iteration 179/1000 | Loss: 0.00002993
Iteration 180/1000 | Loss: 0.00002993
Iteration 181/1000 | Loss: 0.00002993
Iteration 182/1000 | Loss: 0.00002992
Iteration 183/1000 | Loss: 0.00002992
Iteration 184/1000 | Loss: 0.00002992
Iteration 185/1000 | Loss: 0.00002992
Iteration 186/1000 | Loss: 0.00002992
Iteration 187/1000 | Loss: 0.00002991
Iteration 188/1000 | Loss: 0.00002991
Iteration 189/1000 | Loss: 0.00002991
Iteration 190/1000 | Loss: 0.00002991
Iteration 191/1000 | Loss: 0.00002990
Iteration 192/1000 | Loss: 0.00002990
Iteration 193/1000 | Loss: 0.00002990
Iteration 194/1000 | Loss: 0.00002989
Iteration 195/1000 | Loss: 0.00002989
Iteration 196/1000 | Loss: 0.00002989
Iteration 197/1000 | Loss: 0.00002989
Iteration 198/1000 | Loss: 0.00002989
Iteration 199/1000 | Loss: 0.00002988
Iteration 200/1000 | Loss: 0.00002988
Iteration 201/1000 | Loss: 0.00002987
Iteration 202/1000 | Loss: 0.00002987
Iteration 203/1000 | Loss: 0.00002987
Iteration 204/1000 | Loss: 0.00002987
Iteration 205/1000 | Loss: 0.00002987
Iteration 206/1000 | Loss: 0.00002987
Iteration 207/1000 | Loss: 0.00002987
Iteration 208/1000 | Loss: 0.00002987
Iteration 209/1000 | Loss: 0.00002987
Iteration 210/1000 | Loss: 0.00002987
Iteration 211/1000 | Loss: 0.00002984
Iteration 212/1000 | Loss: 0.00002984
Iteration 213/1000 | Loss: 0.00002984
Iteration 214/1000 | Loss: 0.00002984
Iteration 215/1000 | Loss: 0.00002984
Iteration 216/1000 | Loss: 0.00002984
Iteration 217/1000 | Loss: 0.00002984
Iteration 218/1000 | Loss: 0.00002983
Iteration 219/1000 | Loss: 0.00002983
Iteration 220/1000 | Loss: 0.00002983
Iteration 221/1000 | Loss: 0.00002983
Iteration 222/1000 | Loss: 0.00002982
Iteration 223/1000 | Loss: 0.00002982
Iteration 224/1000 | Loss: 0.00002982
Iteration 225/1000 | Loss: 0.00002982
Iteration 226/1000 | Loss: 0.00002981
Iteration 227/1000 | Loss: 0.00002981
Iteration 228/1000 | Loss: 0.00002981
Iteration 229/1000 | Loss: 0.00002981
Iteration 230/1000 | Loss: 0.00002981
Iteration 231/1000 | Loss: 0.00002981
Iteration 232/1000 | Loss: 0.00002980
Iteration 233/1000 | Loss: 0.00002980
Iteration 234/1000 | Loss: 0.00002980
Iteration 235/1000 | Loss: 0.00002980
Iteration 236/1000 | Loss: 0.00002980
Iteration 237/1000 | Loss: 0.00002979
Iteration 238/1000 | Loss: 0.00002979
Iteration 239/1000 | Loss: 0.00002979
Iteration 240/1000 | Loss: 0.00002979
Iteration 241/1000 | Loss: 0.00002979
Iteration 242/1000 | Loss: 0.00002979
Iteration 243/1000 | Loss: 0.00002979
Iteration 244/1000 | Loss: 0.00002979
Iteration 245/1000 | Loss: 0.00002978
Iteration 246/1000 | Loss: 0.00002978
Iteration 247/1000 | Loss: 0.00002978
Iteration 248/1000 | Loss: 0.00002978
Iteration 249/1000 | Loss: 0.00002978
Iteration 250/1000 | Loss: 0.00002978
Iteration 251/1000 | Loss: 0.00002978
Iteration 252/1000 | Loss: 0.00002978
Iteration 253/1000 | Loss: 0.00002978
Iteration 254/1000 | Loss: 0.00002978
Iteration 255/1000 | Loss: 0.00002978
Iteration 256/1000 | Loss: 0.00002978
Iteration 257/1000 | Loss: 0.00002978
Iteration 258/1000 | Loss: 0.00002978
Iteration 259/1000 | Loss: 0.00002978
Iteration 260/1000 | Loss: 0.00002978
Iteration 261/1000 | Loss: 0.00002978
Iteration 262/1000 | Loss: 0.00002978
Iteration 263/1000 | Loss: 0.00002978
Iteration 264/1000 | Loss: 0.00002978
Iteration 265/1000 | Loss: 0.00002978
Iteration 266/1000 | Loss: 0.00002978
Iteration 267/1000 | Loss: 0.00002978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.978241718665231e-05, 2.978241718665231e-05, 2.978241718665231e-05, 2.978241718665231e-05, 2.978241718665231e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.978241718665231e-05

Optimization complete. Final v2v error: 4.331122398376465 mm

Highest mean error: 12.201593399047852 mm for frame 122

Lowest mean error: 3.3822317123413086 mm for frame 126

Saving results

Total time: 6893.63109087944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0007
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937750
Iteration 2/25 | Loss: 0.00161026
Iteration 3/25 | Loss: 0.00112834
Iteration 4/25 | Loss: 0.00110966
Iteration 5/25 | Loss: 0.00112137
Iteration 6/25 | Loss: 0.00103203
Iteration 7/25 | Loss: 0.00097687
Iteration 8/25 | Loss: 0.00094601
Iteration 9/25 | Loss: 0.00093739
Iteration 10/25 | Loss: 0.00092668
Iteration 11/25 | Loss: 0.00092435
Iteration 12/25 | Loss: 0.00092386
Iteration 13/25 | Loss: 0.00092376
Iteration 14/25 | Loss: 0.00092376
Iteration 15/25 | Loss: 0.00092376
Iteration 16/25 | Loss: 0.00092375
Iteration 17/25 | Loss: 0.00092375
Iteration 18/25 | Loss: 0.00092375
Iteration 19/25 | Loss: 0.00092375
Iteration 20/25 | Loss: 0.00092375
Iteration 21/25 | Loss: 0.00092375
Iteration 22/25 | Loss: 0.00092375
Iteration 23/25 | Loss: 0.00092375
Iteration 24/25 | Loss: 0.00092375
Iteration 25/25 | Loss: 0.00092375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42268038
Iteration 2/25 | Loss: 0.00042597
Iteration 3/25 | Loss: 0.00042597
Iteration 4/25 | Loss: 0.00042597
Iteration 5/25 | Loss: 0.00042597
Iteration 6/25 | Loss: 0.00042597
Iteration 7/25 | Loss: 0.00042597
Iteration 8/25 | Loss: 0.00042596
Iteration 9/25 | Loss: 0.00042596
Iteration 10/25 | Loss: 0.00042596
Iteration 11/25 | Loss: 0.00042596
Iteration 12/25 | Loss: 0.00042596
Iteration 13/25 | Loss: 0.00042596
Iteration 14/25 | Loss: 0.00042596
Iteration 15/25 | Loss: 0.00042596
Iteration 16/25 | Loss: 0.00042596
Iteration 17/25 | Loss: 0.00042596
Iteration 18/25 | Loss: 0.00042596
Iteration 19/25 | Loss: 0.00042596
Iteration 20/25 | Loss: 0.00042596
Iteration 21/25 | Loss: 0.00042596
Iteration 22/25 | Loss: 0.00042596
Iteration 23/25 | Loss: 0.00042596
Iteration 24/25 | Loss: 0.00042596
Iteration 25/25 | Loss: 0.00042596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042596
Iteration 2/1000 | Loss: 0.00005345
Iteration 3/1000 | Loss: 0.00003057
Iteration 4/1000 | Loss: 0.00002423
Iteration 5/1000 | Loss: 0.00002303
Iteration 6/1000 | Loss: 0.00002208
Iteration 7/1000 | Loss: 0.00002142
Iteration 8/1000 | Loss: 0.00002094
Iteration 9/1000 | Loss: 0.00002066
Iteration 10/1000 | Loss: 0.00006940
Iteration 11/1000 | Loss: 0.00022998
Iteration 12/1000 | Loss: 0.00002421
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002035
Iteration 15/1000 | Loss: 0.00002020
Iteration 16/1000 | Loss: 0.00002020
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002019
Iteration 19/1000 | Loss: 0.00002019
Iteration 20/1000 | Loss: 0.00002019
Iteration 21/1000 | Loss: 0.00002019
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002018
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00002017
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002016
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002015
Iteration 32/1000 | Loss: 0.00002015
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002008
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002005
Iteration 43/1000 | Loss: 0.00002005
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002003
Iteration 52/1000 | Loss: 0.00002003
Iteration 53/1000 | Loss: 0.00002003
Iteration 54/1000 | Loss: 0.00002003
Iteration 55/1000 | Loss: 0.00002003
Iteration 56/1000 | Loss: 0.00002003
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002002
Iteration 59/1000 | Loss: 0.00002002
Iteration 60/1000 | Loss: 0.00002002
Iteration 61/1000 | Loss: 0.00002002
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002000
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001999
Iteration 73/1000 | Loss: 0.00001999
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001998
Iteration 82/1000 | Loss: 0.00001998
Iteration 83/1000 | Loss: 0.00001998
Iteration 84/1000 | Loss: 0.00001998
Iteration 85/1000 | Loss: 0.00001998
Iteration 86/1000 | Loss: 0.00001998
Iteration 87/1000 | Loss: 0.00001998
Iteration 88/1000 | Loss: 0.00001998
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.9984676328022033e-05, 1.9984676328022033e-05, 1.9984676328022033e-05, 1.9984676328022033e-05, 1.9984676328022033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9984676328022033e-05

Optimization complete. Final v2v error: 3.7788422107696533 mm

Highest mean error: 4.048943519592285 mm for frame 121

Lowest mean error: 3.3454031944274902 mm for frame 83

Saving results

Total time: 844.8624494075775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0003
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450071
Iteration 2/25 | Loss: 0.00111116
Iteration 3/25 | Loss: 0.00098138
Iteration 4/25 | Loss: 0.00096711
Iteration 5/25 | Loss: 0.00096424
Iteration 6/25 | Loss: 0.00096356
Iteration 7/25 | Loss: 0.00096356
Iteration 8/25 | Loss: 0.00096356
Iteration 9/25 | Loss: 0.00096356
Iteration 10/25 | Loss: 0.00096356
Iteration 11/25 | Loss: 0.00096356
Iteration 12/25 | Loss: 0.00096356
Iteration 13/25 | Loss: 0.00096356
Iteration 14/25 | Loss: 0.00096356
Iteration 15/25 | Loss: 0.00096356
Iteration 16/25 | Loss: 0.00096356
Iteration 17/25 | Loss: 0.00096356
Iteration 18/25 | Loss: 0.00096356
Iteration 19/25 | Loss: 0.00096356
Iteration 20/25 | Loss: 0.00096356
Iteration 21/25 | Loss: 0.00096356
Iteration 22/25 | Loss: 0.00096356
Iteration 23/25 | Loss: 0.00096356
Iteration 24/25 | Loss: 0.00096356
Iteration 25/25 | Loss: 0.00096356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009635636815801263, 0.0009635636815801263, 0.0009635636815801263, 0.0009635636815801263, 0.0009635636815801263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009635636815801263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15637577
Iteration 2/25 | Loss: 0.00044138
Iteration 3/25 | Loss: 0.00044136
Iteration 4/25 | Loss: 0.00044136
Iteration 5/25 | Loss: 0.00044136
Iteration 6/25 | Loss: 0.00044136
Iteration 7/25 | Loss: 0.00044136
Iteration 8/25 | Loss: 0.00044136
Iteration 9/25 | Loss: 0.00044136
Iteration 10/25 | Loss: 0.00044136
Iteration 11/25 | Loss: 0.00044136
Iteration 12/25 | Loss: 0.00044136
Iteration 13/25 | Loss: 0.00044136
Iteration 14/25 | Loss: 0.00044136
Iteration 15/25 | Loss: 0.00044136
Iteration 16/25 | Loss: 0.00044136
Iteration 17/25 | Loss: 0.00044136
Iteration 18/25 | Loss: 0.00044136
Iteration 19/25 | Loss: 0.00044136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000441359996329993, 0.000441359996329993, 0.000441359996329993, 0.000441359996329993, 0.000441359996329993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000441359996329993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044136
Iteration 2/1000 | Loss: 0.00002613
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001862
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001706
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001625
Iteration 12/1000 | Loss: 0.00001620
Iteration 13/1000 | Loss: 0.00001611
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001609
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001607
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001606
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00001606
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001606
Iteration 28/1000 | Loss: 0.00001606
Iteration 29/1000 | Loss: 0.00001606
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001606
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001605
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001604
Iteration 40/1000 | Loss: 0.00001604
Iteration 41/1000 | Loss: 0.00001604
Iteration 42/1000 | Loss: 0.00001604
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001603
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001602
Iteration 59/1000 | Loss: 0.00001602
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001600
Iteration 70/1000 | Loss: 0.00001600
Iteration 71/1000 | Loss: 0.00001600
Iteration 72/1000 | Loss: 0.00001600
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001599
Iteration 76/1000 | Loss: 0.00001599
Iteration 77/1000 | Loss: 0.00001599
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001599
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001599
Iteration 87/1000 | Loss: 0.00001599
Iteration 88/1000 | Loss: 0.00001599
Iteration 89/1000 | Loss: 0.00001599
Iteration 90/1000 | Loss: 0.00001599
Iteration 91/1000 | Loss: 0.00001599
Iteration 92/1000 | Loss: 0.00001599
Iteration 93/1000 | Loss: 0.00001599
Iteration 94/1000 | Loss: 0.00001599
Iteration 95/1000 | Loss: 0.00001599
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001599
Iteration 98/1000 | Loss: 0.00001599
Iteration 99/1000 | Loss: 0.00001599
Iteration 100/1000 | Loss: 0.00001599
Iteration 101/1000 | Loss: 0.00001599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.5992971384548582e-05, 1.5992971384548582e-05, 1.5992971384548582e-05, 1.5992971384548582e-05, 1.5992971384548582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5992971384548582e-05

Optimization complete. Final v2v error: 3.40555739402771 mm

Highest mean error: 3.5554022789001465 mm for frame 100

Lowest mean error: 3.2620363235473633 mm for frame 119

Saving results

Total time: 500.2098913192749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0018
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821092
Iteration 2/25 | Loss: 0.00125430
Iteration 3/25 | Loss: 0.00105895
Iteration 4/25 | Loss: 0.00102205
Iteration 5/25 | Loss: 0.00101050
Iteration 6/25 | Loss: 0.00100819
Iteration 7/25 | Loss: 0.00100813
Iteration 8/25 | Loss: 0.00100813
Iteration 9/25 | Loss: 0.00100813
Iteration 10/25 | Loss: 0.00100813
Iteration 11/25 | Loss: 0.00100813
Iteration 12/25 | Loss: 0.00100813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001008132821880281, 0.001008132821880281, 0.001008132821880281, 0.001008132821880281, 0.001008132821880281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001008132821880281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38389564
Iteration 2/25 | Loss: 0.00049935
Iteration 3/25 | Loss: 0.00049932
Iteration 4/25 | Loss: 0.00049932
Iteration 5/25 | Loss: 0.00049932
Iteration 6/25 | Loss: 0.00049932
Iteration 7/25 | Loss: 0.00049932
Iteration 8/25 | Loss: 0.00049932
Iteration 9/25 | Loss: 0.00049932
Iteration 10/25 | Loss: 0.00049932
Iteration 11/25 | Loss: 0.00049932
Iteration 12/25 | Loss: 0.00049932
Iteration 13/25 | Loss: 0.00049932
Iteration 14/25 | Loss: 0.00049932
Iteration 15/25 | Loss: 0.00049932
Iteration 16/25 | Loss: 0.00049932
Iteration 17/25 | Loss: 0.00049932
Iteration 18/25 | Loss: 0.00049932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004993194597773254, 0.0004993194597773254, 0.0004993194597773254, 0.0004993194597773254, 0.0004993194597773254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004993194597773254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049932
Iteration 2/1000 | Loss: 0.00004569
Iteration 3/1000 | Loss: 0.00003455
Iteration 4/1000 | Loss: 0.00003223
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00003002
Iteration 7/1000 | Loss: 0.00002918
Iteration 8/1000 | Loss: 0.00002842
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002787
Iteration 11/1000 | Loss: 0.00002775
Iteration 12/1000 | Loss: 0.00002759
Iteration 13/1000 | Loss: 0.00002755
Iteration 14/1000 | Loss: 0.00002752
Iteration 15/1000 | Loss: 0.00002752
Iteration 16/1000 | Loss: 0.00002752
Iteration 17/1000 | Loss: 0.00002752
Iteration 18/1000 | Loss: 0.00002752
Iteration 19/1000 | Loss: 0.00002752
Iteration 20/1000 | Loss: 0.00002752
Iteration 21/1000 | Loss: 0.00002752
Iteration 22/1000 | Loss: 0.00002752
Iteration 23/1000 | Loss: 0.00002752
Iteration 24/1000 | Loss: 0.00002751
Iteration 25/1000 | Loss: 0.00002751
Iteration 26/1000 | Loss: 0.00002751
Iteration 27/1000 | Loss: 0.00002751
Iteration 28/1000 | Loss: 0.00002751
Iteration 29/1000 | Loss: 0.00002751
Iteration 30/1000 | Loss: 0.00002751
Iteration 31/1000 | Loss: 0.00002750
Iteration 32/1000 | Loss: 0.00002750
Iteration 33/1000 | Loss: 0.00002750
Iteration 34/1000 | Loss: 0.00002750
Iteration 35/1000 | Loss: 0.00002750
Iteration 36/1000 | Loss: 0.00002750
Iteration 37/1000 | Loss: 0.00002750
Iteration 38/1000 | Loss: 0.00002750
Iteration 39/1000 | Loss: 0.00002750
Iteration 40/1000 | Loss: 0.00002749
Iteration 41/1000 | Loss: 0.00002749
Iteration 42/1000 | Loss: 0.00002749
Iteration 43/1000 | Loss: 0.00002749
Iteration 44/1000 | Loss: 0.00002749
Iteration 45/1000 | Loss: 0.00002749
Iteration 46/1000 | Loss: 0.00002749
Iteration 47/1000 | Loss: 0.00002749
Iteration 48/1000 | Loss: 0.00002749
Iteration 49/1000 | Loss: 0.00002749
Iteration 50/1000 | Loss: 0.00002749
Iteration 51/1000 | Loss: 0.00002748
Iteration 52/1000 | Loss: 0.00002748
Iteration 53/1000 | Loss: 0.00002748
Iteration 54/1000 | Loss: 0.00002748
Iteration 55/1000 | Loss: 0.00002748
Iteration 56/1000 | Loss: 0.00002748
Iteration 57/1000 | Loss: 0.00002747
Iteration 58/1000 | Loss: 0.00002747
Iteration 59/1000 | Loss: 0.00002747
Iteration 60/1000 | Loss: 0.00002747
Iteration 61/1000 | Loss: 0.00002747
Iteration 62/1000 | Loss: 0.00002747
Iteration 63/1000 | Loss: 0.00002747
Iteration 64/1000 | Loss: 0.00002747
Iteration 65/1000 | Loss: 0.00002747
Iteration 66/1000 | Loss: 0.00002746
Iteration 67/1000 | Loss: 0.00002746
Iteration 68/1000 | Loss: 0.00002746
Iteration 69/1000 | Loss: 0.00002746
Iteration 70/1000 | Loss: 0.00002746
Iteration 71/1000 | Loss: 0.00002746
Iteration 72/1000 | Loss: 0.00002746
Iteration 73/1000 | Loss: 0.00002746
Iteration 74/1000 | Loss: 0.00002745
Iteration 75/1000 | Loss: 0.00002745
Iteration 76/1000 | Loss: 0.00002745
Iteration 77/1000 | Loss: 0.00002745
Iteration 78/1000 | Loss: 0.00002745
Iteration 79/1000 | Loss: 0.00002745
Iteration 80/1000 | Loss: 0.00002745
Iteration 81/1000 | Loss: 0.00002745
Iteration 82/1000 | Loss: 0.00002745
Iteration 83/1000 | Loss: 0.00002745
Iteration 84/1000 | Loss: 0.00002745
Iteration 85/1000 | Loss: 0.00002745
Iteration 86/1000 | Loss: 0.00002745
Iteration 87/1000 | Loss: 0.00002745
Iteration 88/1000 | Loss: 0.00002745
Iteration 89/1000 | Loss: 0.00002745
Iteration 90/1000 | Loss: 0.00002745
Iteration 91/1000 | Loss: 0.00002745
Iteration 92/1000 | Loss: 0.00002745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.745219535427168e-05, 2.745219535427168e-05, 2.745219535427168e-05, 2.745219535427168e-05, 2.745219535427168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.745219535427168e-05

Optimization complete. Final v2v error: 4.278580665588379 mm

Highest mean error: 4.594559192657471 mm for frame 239

Lowest mean error: 3.9189252853393555 mm for frame 214

Saving results

Total time: 1164.4076480865479
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0019
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416383
Iteration 2/25 | Loss: 0.00111996
Iteration 3/25 | Loss: 0.00094837
Iteration 4/25 | Loss: 0.00091971
Iteration 5/25 | Loss: 0.00090980
Iteration 6/25 | Loss: 0.00090646
Iteration 7/25 | Loss: 0.00090575
Iteration 8/25 | Loss: 0.00090575
Iteration 9/25 | Loss: 0.00090575
Iteration 10/25 | Loss: 0.00090575
Iteration 11/25 | Loss: 0.00090575
Iteration 12/25 | Loss: 0.00090575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009057495626620948, 0.0009057495626620948, 0.0009057495626620948, 0.0009057495626620948, 0.0009057495626620948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009057495626620948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32144713
Iteration 2/25 | Loss: 0.00039582
Iteration 3/25 | Loss: 0.00039580
Iteration 4/25 | Loss: 0.00039580
Iteration 5/25 | Loss: 0.00039580
Iteration 6/25 | Loss: 0.00039580
Iteration 7/25 | Loss: 0.00039580
Iteration 8/25 | Loss: 0.00039580
Iteration 9/25 | Loss: 0.00039580
Iteration 10/25 | Loss: 0.00039580
Iteration 11/25 | Loss: 0.00039580
Iteration 12/25 | Loss: 0.00039580
Iteration 13/25 | Loss: 0.00039580
Iteration 14/25 | Loss: 0.00039580
Iteration 15/25 | Loss: 0.00039580
Iteration 16/25 | Loss: 0.00039580
Iteration 17/25 | Loss: 0.00039580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00039579623262397945, 0.00039579623262397945, 0.00039579623262397945, 0.00039579623262397945, 0.00039579623262397945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039579623262397945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039580
Iteration 2/1000 | Loss: 0.00003728
Iteration 3/1000 | Loss: 0.00002589
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001770
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001672
Iteration 11/1000 | Loss: 0.00001667
Iteration 12/1000 | Loss: 0.00001665
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001660
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001625
Iteration 18/1000 | Loss: 0.00001622
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001609
Iteration 22/1000 | Loss: 0.00001608
Iteration 23/1000 | Loss: 0.00001608
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001602
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001602
Iteration 32/1000 | Loss: 0.00001602
Iteration 33/1000 | Loss: 0.00001602
Iteration 34/1000 | Loss: 0.00001601
Iteration 35/1000 | Loss: 0.00001601
Iteration 36/1000 | Loss: 0.00001601
Iteration 37/1000 | Loss: 0.00001600
Iteration 38/1000 | Loss: 0.00001599
Iteration 39/1000 | Loss: 0.00001598
Iteration 40/1000 | Loss: 0.00001598
Iteration 41/1000 | Loss: 0.00001597
Iteration 42/1000 | Loss: 0.00001597
Iteration 43/1000 | Loss: 0.00001596
Iteration 44/1000 | Loss: 0.00001596
Iteration 45/1000 | Loss: 0.00001596
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001594
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.589646126376465e-05, 1.589646126376465e-05, 1.589646126376465e-05, 1.589646126376465e-05, 1.589646126376465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.589646126376465e-05

Optimization complete. Final v2v error: 3.2607877254486084 mm

Highest mean error: 4.74573278427124 mm for frame 85

Lowest mean error: 2.7893099784851074 mm for frame 125

Saving results

Total time: 999.1537597179413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_nl_1040/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_nl_1040/0009
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055228
Iteration 2/25 | Loss: 0.00295159
Iteration 3/25 | Loss: 0.00238549
Iteration 4/25 | Loss: 0.00226007
Iteration 5/25 | Loss: 0.00217794
Iteration 6/25 | Loss: 0.00216804
Iteration 7/25 | Loss: 0.00207186
Iteration 8/25 | Loss: 0.00173351
Iteration 9/25 | Loss: 0.00163660
Iteration 10/25 | Loss: 0.00156720
Iteration 11/25 | Loss: 0.00156384
Iteration 12/25 | Loss: 0.00153462
Iteration 13/25 | Loss: 0.00149045
Iteration 14/25 | Loss: 0.00148316
Iteration 15/25 | Loss: 0.00147699
Iteration 16/25 | Loss: 0.00147293
Iteration 17/25 | Loss: 0.00147145
Iteration 18/25 | Loss: 0.00147089
Iteration 19/25 | Loss: 0.00147056
Iteration 20/25 | Loss: 0.00147041
Iteration 21/25 | Loss: 0.00147030
Iteration 22/25 | Loss: 0.00147011
Iteration 23/25 | Loss: 0.00147000
Iteration 24/25 | Loss: 0.00146999
Iteration 25/25 | Loss: 0.00146999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34829557
Iteration 2/25 | Loss: 0.00450039
Iteration 3/25 | Loss: 0.00450038
Iteration 4/25 | Loss: 0.00450038
Iteration 5/25 | Loss: 0.00450038
Iteration 6/25 | Loss: 0.00450038
Iteration 7/25 | Loss: 0.00450038
Iteration 8/25 | Loss: 0.00450038
Iteration 9/25 | Loss: 0.00450038
Iteration 10/25 | Loss: 0.00450038
Iteration 11/25 | Loss: 0.00450038
Iteration 12/25 | Loss: 0.00450038
Iteration 13/25 | Loss: 0.00450038
Iteration 14/25 | Loss: 0.00450038
Iteration 15/25 | Loss: 0.00450038
Iteration 16/25 | Loss: 0.00450038
Iteration 17/25 | Loss: 0.00450038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00450037932023406, 0.00450037932023406, 0.00450037932023406, 0.00450037932023406, 0.00450037932023406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00450037932023406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00450038
Iteration 2/1000 | Loss: 0.00087715
Iteration 3/1000 | Loss: 0.00067392
Iteration 4/1000 | Loss: 0.00052487
Iteration 5/1000 | Loss: 0.00044170
Iteration 6/1000 | Loss: 0.00040224
Iteration 7/1000 | Loss: 0.00037256
Iteration 8/1000 | Loss: 0.00034304
Iteration 9/1000 | Loss: 0.00032322
Iteration 10/1000 | Loss: 0.00075724
Iteration 11/1000 | Loss: 0.00106550
Iteration 12/1000 | Loss: 0.00904954
Iteration 13/1000 | Loss: 0.00791434
Iteration 14/1000 | Loss: 0.00117402
Iteration 15/1000 | Loss: 0.00047114
Iteration 16/1000 | Loss: 0.00030517
Iteration 17/1000 | Loss: 0.00017664
Iteration 18/1000 | Loss: 0.00010918
Iteration 19/1000 | Loss: 0.00007787
Iteration 20/1000 | Loss: 0.00005908
Iteration 21/1000 | Loss: 0.00004492
Iteration 22/1000 | Loss: 0.00003659
Iteration 23/1000 | Loss: 0.00003253
Iteration 24/1000 | Loss: 0.00002929
Iteration 25/1000 | Loss: 0.00002675
Iteration 26/1000 | Loss: 0.00002502
Iteration 27/1000 | Loss: 0.00002372
Iteration 28/1000 | Loss: 0.00002286
Iteration 29/1000 | Loss: 0.00002215
Iteration 30/1000 | Loss: 0.00002174
Iteration 31/1000 | Loss: 0.00002155
Iteration 32/1000 | Loss: 0.00002152
Iteration 33/1000 | Loss: 0.00002151
Iteration 34/1000 | Loss: 0.00002135
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002127
Iteration 37/1000 | Loss: 0.00002127
Iteration 38/1000 | Loss: 0.00002126
Iteration 39/1000 | Loss: 0.00002126
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002123
Iteration 42/1000 | Loss: 0.00002122
Iteration 43/1000 | Loss: 0.00002122
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002121
Iteration 47/1000 | Loss: 0.00002121
Iteration 48/1000 | Loss: 0.00002121
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00002118
Iteration 52/1000 | Loss: 0.00002117
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002115
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.1149211534066126e-05, 2.1149211534066126e-05, 2.1149211534066126e-05, 2.1149211534066126e-05, 2.1149211534066126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1149211534066126e-05

Optimization complete. Final v2v error: 3.930821657180786 mm

Highest mean error: 4.358110427856445 mm for frame 73

Lowest mean error: 3.513737678527832 mm for frame 3

Saving results

Total time: 1942.2647433280945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0008
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017709
Iteration 2/25 | Loss: 0.00163682
Iteration 3/25 | Loss: 0.00097452
Iteration 4/25 | Loss: 0.00091853
Iteration 5/25 | Loss: 0.00089991
Iteration 6/25 | Loss: 0.00089462
Iteration 7/25 | Loss: 0.00089352
Iteration 8/25 | Loss: 0.00089339
Iteration 9/25 | Loss: 0.00089339
Iteration 10/25 | Loss: 0.00089339
Iteration 11/25 | Loss: 0.00089339
Iteration 12/25 | Loss: 0.00089339
Iteration 13/25 | Loss: 0.00089339
Iteration 14/25 | Loss: 0.00089339
Iteration 15/25 | Loss: 0.00089339
Iteration 16/25 | Loss: 0.00089339
Iteration 17/25 | Loss: 0.00089339
Iteration 18/25 | Loss: 0.00089339
Iteration 19/25 | Loss: 0.00089339
Iteration 20/25 | Loss: 0.00089339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008933910867199302, 0.0008933910867199302, 0.0008933910867199302, 0.0008933910867199302, 0.0008933910867199302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008933910867199302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82643509
Iteration 2/25 | Loss: 0.00034886
Iteration 3/25 | Loss: 0.00034886
Iteration 4/25 | Loss: 0.00034886
Iteration 5/25 | Loss: 0.00034886
Iteration 6/25 | Loss: 0.00034886
Iteration 7/25 | Loss: 0.00034886
Iteration 8/25 | Loss: 0.00034886
Iteration 9/25 | Loss: 0.00034886
Iteration 10/25 | Loss: 0.00034886
Iteration 11/25 | Loss: 0.00034886
Iteration 12/25 | Loss: 0.00034886
Iteration 13/25 | Loss: 0.00034886
Iteration 14/25 | Loss: 0.00034886
Iteration 15/25 | Loss: 0.00034886
Iteration 16/25 | Loss: 0.00034886
Iteration 17/25 | Loss: 0.00034886
Iteration 18/25 | Loss: 0.00034886
Iteration 19/25 | Loss: 0.00034886
Iteration 20/25 | Loss: 0.00034886
Iteration 21/25 | Loss: 0.00034886
Iteration 22/25 | Loss: 0.00034886
Iteration 23/25 | Loss: 0.00034886
Iteration 24/25 | Loss: 0.00034886
Iteration 25/25 | Loss: 0.00034886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034886
Iteration 2/1000 | Loss: 0.00007322
Iteration 3/1000 | Loss: 0.00005433
Iteration 4/1000 | Loss: 0.00004973
Iteration 5/1000 | Loss: 0.00004820
Iteration 6/1000 | Loss: 0.00004642
Iteration 7/1000 | Loss: 0.00004527
Iteration 8/1000 | Loss: 0.00004441
Iteration 9/1000 | Loss: 0.00004391
Iteration 10/1000 | Loss: 0.00004350
Iteration 11/1000 | Loss: 0.00004310
Iteration 12/1000 | Loss: 0.00004283
Iteration 13/1000 | Loss: 0.00004267
Iteration 14/1000 | Loss: 0.00004250
Iteration 15/1000 | Loss: 0.00004246
Iteration 16/1000 | Loss: 0.00004240
Iteration 17/1000 | Loss: 0.00004224
Iteration 18/1000 | Loss: 0.00004209
Iteration 19/1000 | Loss: 0.00004200
Iteration 20/1000 | Loss: 0.00004195
Iteration 21/1000 | Loss: 0.00004188
Iteration 22/1000 | Loss: 0.00004182
Iteration 23/1000 | Loss: 0.00004177
Iteration 24/1000 | Loss: 0.00004171
Iteration 25/1000 | Loss: 0.00004166
Iteration 26/1000 | Loss: 0.00004166
Iteration 27/1000 | Loss: 0.00004165
Iteration 28/1000 | Loss: 0.00004164
Iteration 29/1000 | Loss: 0.00004163
Iteration 30/1000 | Loss: 0.00004163
Iteration 31/1000 | Loss: 0.00004163
Iteration 32/1000 | Loss: 0.00004162
Iteration 33/1000 | Loss: 0.00004162
Iteration 34/1000 | Loss: 0.00004162
Iteration 35/1000 | Loss: 0.00004162
Iteration 36/1000 | Loss: 0.00004162
Iteration 37/1000 | Loss: 0.00004162
Iteration 38/1000 | Loss: 0.00004162
Iteration 39/1000 | Loss: 0.00004162
Iteration 40/1000 | Loss: 0.00004162
Iteration 41/1000 | Loss: 0.00004162
Iteration 42/1000 | Loss: 0.00004162
Iteration 43/1000 | Loss: 0.00004162
Iteration 44/1000 | Loss: 0.00004161
Iteration 45/1000 | Loss: 0.00004161
Iteration 46/1000 | Loss: 0.00004159
Iteration 47/1000 | Loss: 0.00004159
Iteration 48/1000 | Loss: 0.00004158
Iteration 49/1000 | Loss: 0.00004157
Iteration 50/1000 | Loss: 0.00004153
Iteration 51/1000 | Loss: 0.00004153
Iteration 52/1000 | Loss: 0.00004152
Iteration 53/1000 | Loss: 0.00004151
Iteration 54/1000 | Loss: 0.00004151
Iteration 55/1000 | Loss: 0.00004151
Iteration 56/1000 | Loss: 0.00004151
Iteration 57/1000 | Loss: 0.00004151
Iteration 58/1000 | Loss: 0.00004151
Iteration 59/1000 | Loss: 0.00004151
Iteration 60/1000 | Loss: 0.00004151
Iteration 61/1000 | Loss: 0.00004151
Iteration 62/1000 | Loss: 0.00004151
Iteration 63/1000 | Loss: 0.00004150
Iteration 64/1000 | Loss: 0.00004150
Iteration 65/1000 | Loss: 0.00004150
Iteration 66/1000 | Loss: 0.00004150
Iteration 67/1000 | Loss: 0.00004150
Iteration 68/1000 | Loss: 0.00004150
Iteration 69/1000 | Loss: 0.00004150
Iteration 70/1000 | Loss: 0.00004150
Iteration 71/1000 | Loss: 0.00004150
Iteration 72/1000 | Loss: 0.00004150
Iteration 73/1000 | Loss: 0.00004149
Iteration 74/1000 | Loss: 0.00004148
Iteration 75/1000 | Loss: 0.00004148
Iteration 76/1000 | Loss: 0.00004148
Iteration 77/1000 | Loss: 0.00004148
Iteration 78/1000 | Loss: 0.00004148
Iteration 79/1000 | Loss: 0.00004147
Iteration 80/1000 | Loss: 0.00004147
Iteration 81/1000 | Loss: 0.00004147
Iteration 82/1000 | Loss: 0.00004147
Iteration 83/1000 | Loss: 0.00004147
Iteration 84/1000 | Loss: 0.00004147
Iteration 85/1000 | Loss: 0.00004147
Iteration 86/1000 | Loss: 0.00004147
Iteration 87/1000 | Loss: 0.00004147
Iteration 88/1000 | Loss: 0.00004147
Iteration 89/1000 | Loss: 0.00004146
Iteration 90/1000 | Loss: 0.00004146
Iteration 91/1000 | Loss: 0.00004146
Iteration 92/1000 | Loss: 0.00004146
Iteration 93/1000 | Loss: 0.00004146
Iteration 94/1000 | Loss: 0.00004146
Iteration 95/1000 | Loss: 0.00004146
Iteration 96/1000 | Loss: 0.00004145
Iteration 97/1000 | Loss: 0.00004145
Iteration 98/1000 | Loss: 0.00004145
Iteration 99/1000 | Loss: 0.00004145
Iteration 100/1000 | Loss: 0.00004145
Iteration 101/1000 | Loss: 0.00004145
Iteration 102/1000 | Loss: 0.00004145
Iteration 103/1000 | Loss: 0.00004145
Iteration 104/1000 | Loss: 0.00004145
Iteration 105/1000 | Loss: 0.00004145
Iteration 106/1000 | Loss: 0.00004144
Iteration 107/1000 | Loss: 0.00004144
Iteration 108/1000 | Loss: 0.00004144
Iteration 109/1000 | Loss: 0.00004144
Iteration 110/1000 | Loss: 0.00004144
Iteration 111/1000 | Loss: 0.00004144
Iteration 112/1000 | Loss: 0.00004144
Iteration 113/1000 | Loss: 0.00004144
Iteration 114/1000 | Loss: 0.00004144
Iteration 115/1000 | Loss: 0.00004144
Iteration 116/1000 | Loss: 0.00004144
Iteration 117/1000 | Loss: 0.00004143
Iteration 118/1000 | Loss: 0.00004143
Iteration 119/1000 | Loss: 0.00004143
Iteration 120/1000 | Loss: 0.00004143
Iteration 121/1000 | Loss: 0.00004143
Iteration 122/1000 | Loss: 0.00004142
Iteration 123/1000 | Loss: 0.00004142
Iteration 124/1000 | Loss: 0.00004142
Iteration 125/1000 | Loss: 0.00004142
Iteration 126/1000 | Loss: 0.00004142
Iteration 127/1000 | Loss: 0.00004142
Iteration 128/1000 | Loss: 0.00004142
Iteration 129/1000 | Loss: 0.00004142
Iteration 130/1000 | Loss: 0.00004141
Iteration 131/1000 | Loss: 0.00004141
Iteration 132/1000 | Loss: 0.00004141
Iteration 133/1000 | Loss: 0.00004141
Iteration 134/1000 | Loss: 0.00004141
Iteration 135/1000 | Loss: 0.00004141
Iteration 136/1000 | Loss: 0.00004141
Iteration 137/1000 | Loss: 0.00004141
Iteration 138/1000 | Loss: 0.00004141
Iteration 139/1000 | Loss: 0.00004141
Iteration 140/1000 | Loss: 0.00004140
Iteration 141/1000 | Loss: 0.00004140
Iteration 142/1000 | Loss: 0.00004140
Iteration 143/1000 | Loss: 0.00004140
Iteration 144/1000 | Loss: 0.00004140
Iteration 145/1000 | Loss: 0.00004140
Iteration 146/1000 | Loss: 0.00004140
Iteration 147/1000 | Loss: 0.00004140
Iteration 148/1000 | Loss: 0.00004140
Iteration 149/1000 | Loss: 0.00004140
Iteration 150/1000 | Loss: 0.00004140
Iteration 151/1000 | Loss: 0.00004140
Iteration 152/1000 | Loss: 0.00004140
Iteration 153/1000 | Loss: 0.00004140
Iteration 154/1000 | Loss: 0.00004140
Iteration 155/1000 | Loss: 0.00004140
Iteration 156/1000 | Loss: 0.00004140
Iteration 157/1000 | Loss: 0.00004140
Iteration 158/1000 | Loss: 0.00004140
Iteration 159/1000 | Loss: 0.00004140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [4.139922384638339e-05, 4.139922384638339e-05, 4.139922384638339e-05, 4.139922384638339e-05, 4.139922384638339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.139922384638339e-05

Optimization complete. Final v2v error: 5.117796421051025 mm

Highest mean error: 6.0542521476745605 mm for frame 105

Lowest mean error: 4.244076728820801 mm for frame 34

Saving results

Total time: 1304.855515241623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0014
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404551
Iteration 2/25 | Loss: 0.00080232
Iteration 3/25 | Loss: 0.00067448
Iteration 4/25 | Loss: 0.00064760
Iteration 5/25 | Loss: 0.00063714
Iteration 6/25 | Loss: 0.00063510
Iteration 7/25 | Loss: 0.00063472
Iteration 8/25 | Loss: 0.00063472
Iteration 9/25 | Loss: 0.00063472
Iteration 10/25 | Loss: 0.00063472
Iteration 11/25 | Loss: 0.00063472
Iteration 12/25 | Loss: 0.00063472
Iteration 13/25 | Loss: 0.00063472
Iteration 14/25 | Loss: 0.00063472
Iteration 15/25 | Loss: 0.00063472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006347169983200729, 0.0006347169983200729, 0.0006347169983200729, 0.0006347169983200729, 0.0006347169983200729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006347169983200729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32997966
Iteration 2/25 | Loss: 0.00011156
Iteration 3/25 | Loss: 0.00011155
Iteration 4/25 | Loss: 0.00011155
Iteration 5/25 | Loss: 0.00011155
Iteration 6/25 | Loss: 0.00011155
Iteration 7/25 | Loss: 0.00011155
Iteration 8/25 | Loss: 0.00011155
Iteration 9/25 | Loss: 0.00011155
Iteration 10/25 | Loss: 0.00011155
Iteration 11/25 | Loss: 0.00011155
Iteration 12/25 | Loss: 0.00011155
Iteration 13/25 | Loss: 0.00011154
Iteration 14/25 | Loss: 0.00011154
Iteration 15/25 | Loss: 0.00011154
Iteration 16/25 | Loss: 0.00011154
Iteration 17/25 | Loss: 0.00011154
Iteration 18/25 | Loss: 0.00011154
Iteration 19/25 | Loss: 0.00011154
Iteration 20/25 | Loss: 0.00011154
Iteration 21/25 | Loss: 0.00011154
Iteration 22/25 | Loss: 0.00011154
Iteration 23/25 | Loss: 0.00011154
Iteration 24/25 | Loss: 0.00011154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00011154490493936464, 0.00011154490493936464, 0.00011154490493936464, 0.00011154490493936464, 0.00011154490493936464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011154490493936464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011154
Iteration 2/1000 | Loss: 0.00003759
Iteration 3/1000 | Loss: 0.00002999
Iteration 4/1000 | Loss: 0.00002804
Iteration 5/1000 | Loss: 0.00002649
Iteration 6/1000 | Loss: 0.00002549
Iteration 7/1000 | Loss: 0.00002471
Iteration 8/1000 | Loss: 0.00002431
Iteration 9/1000 | Loss: 0.00002406
Iteration 10/1000 | Loss: 0.00002396
Iteration 11/1000 | Loss: 0.00002395
Iteration 12/1000 | Loss: 0.00002394
Iteration 13/1000 | Loss: 0.00002387
Iteration 14/1000 | Loss: 0.00002387
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002386
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002371
Iteration 19/1000 | Loss: 0.00002370
Iteration 20/1000 | Loss: 0.00002370
Iteration 21/1000 | Loss: 0.00002369
Iteration 22/1000 | Loss: 0.00002365
Iteration 23/1000 | Loss: 0.00002356
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002355
Iteration 26/1000 | Loss: 0.00002354
Iteration 27/1000 | Loss: 0.00002354
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002353
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002351
Iteration 32/1000 | Loss: 0.00002348
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00002346
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00002345
Iteration 37/1000 | Loss: 0.00002345
Iteration 38/1000 | Loss: 0.00002345
Iteration 39/1000 | Loss: 0.00002344
Iteration 40/1000 | Loss: 0.00002344
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002343
Iteration 44/1000 | Loss: 0.00002343
Iteration 45/1000 | Loss: 0.00002342
Iteration 46/1000 | Loss: 0.00002342
Iteration 47/1000 | Loss: 0.00002342
Iteration 48/1000 | Loss: 0.00002342
Iteration 49/1000 | Loss: 0.00002342
Iteration 50/1000 | Loss: 0.00002341
Iteration 51/1000 | Loss: 0.00002341
Iteration 52/1000 | Loss: 0.00002341
Iteration 53/1000 | Loss: 0.00002341
Iteration 54/1000 | Loss: 0.00002341
Iteration 55/1000 | Loss: 0.00002340
Iteration 56/1000 | Loss: 0.00002340
Iteration 57/1000 | Loss: 0.00002340
Iteration 58/1000 | Loss: 0.00002339
Iteration 59/1000 | Loss: 0.00002339
Iteration 60/1000 | Loss: 0.00002339
Iteration 61/1000 | Loss: 0.00002338
Iteration 62/1000 | Loss: 0.00002338
Iteration 63/1000 | Loss: 0.00002338
Iteration 64/1000 | Loss: 0.00002337
Iteration 65/1000 | Loss: 0.00002337
Iteration 66/1000 | Loss: 0.00002337
Iteration 67/1000 | Loss: 0.00002336
Iteration 68/1000 | Loss: 0.00002336
Iteration 69/1000 | Loss: 0.00002336
Iteration 70/1000 | Loss: 0.00002335
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002335
Iteration 74/1000 | Loss: 0.00002335
Iteration 75/1000 | Loss: 0.00002335
Iteration 76/1000 | Loss: 0.00002334
Iteration 77/1000 | Loss: 0.00002334
Iteration 78/1000 | Loss: 0.00002334
Iteration 79/1000 | Loss: 0.00002334
Iteration 80/1000 | Loss: 0.00002333
Iteration 81/1000 | Loss: 0.00002333
Iteration 82/1000 | Loss: 0.00002333
Iteration 83/1000 | Loss: 0.00002333
Iteration 84/1000 | Loss: 0.00002333
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002332
Iteration 88/1000 | Loss: 0.00002332
Iteration 89/1000 | Loss: 0.00002332
Iteration 90/1000 | Loss: 0.00002332
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002332
Iteration 93/1000 | Loss: 0.00002332
Iteration 94/1000 | Loss: 0.00002332
Iteration 95/1000 | Loss: 0.00002332
Iteration 96/1000 | Loss: 0.00002332
Iteration 97/1000 | Loss: 0.00002332
Iteration 98/1000 | Loss: 0.00002332
Iteration 99/1000 | Loss: 0.00002332
Iteration 100/1000 | Loss: 0.00002332
Iteration 101/1000 | Loss: 0.00002332
Iteration 102/1000 | Loss: 0.00002332
Iteration 103/1000 | Loss: 0.00002332
Iteration 104/1000 | Loss: 0.00002332
Iteration 105/1000 | Loss: 0.00002332
Iteration 106/1000 | Loss: 0.00002332
Iteration 107/1000 | Loss: 0.00002332
Iteration 108/1000 | Loss: 0.00002332
Iteration 109/1000 | Loss: 0.00002332
Iteration 110/1000 | Loss: 0.00002332
Iteration 111/1000 | Loss: 0.00002332
Iteration 112/1000 | Loss: 0.00002332
Iteration 113/1000 | Loss: 0.00002332
Iteration 114/1000 | Loss: 0.00002332
Iteration 115/1000 | Loss: 0.00002332
Iteration 116/1000 | Loss: 0.00002332
Iteration 117/1000 | Loss: 0.00002332
Iteration 118/1000 | Loss: 0.00002332
Iteration 119/1000 | Loss: 0.00002332
Iteration 120/1000 | Loss: 0.00002332
Iteration 121/1000 | Loss: 0.00002332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.3315860744332895e-05, 2.3315860744332895e-05, 2.3315860744332895e-05, 2.3315860744332895e-05, 2.3315860744332895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3315860744332895e-05

Optimization complete. Final v2v error: 3.9861626625061035 mm

Highest mean error: 4.76744270324707 mm for frame 88

Lowest mean error: 3.2228763103485107 mm for frame 0

Saving results

Total time: 996.4481263160706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0001
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678288
Iteration 2/25 | Loss: 0.00126684
Iteration 3/25 | Loss: 0.00075064
Iteration 4/25 | Loss: 0.00067220
Iteration 5/25 | Loss: 0.00065465
Iteration 6/25 | Loss: 0.00065155
Iteration 7/25 | Loss: 0.00065090
Iteration 8/25 | Loss: 0.00065084
Iteration 9/25 | Loss: 0.00065084
Iteration 10/25 | Loss: 0.00065084
Iteration 11/25 | Loss: 0.00065084
Iteration 12/25 | Loss: 0.00065084
Iteration 13/25 | Loss: 0.00065084
Iteration 14/25 | Loss: 0.00065084
Iteration 15/25 | Loss: 0.00065084
Iteration 16/25 | Loss: 0.00065084
Iteration 17/25 | Loss: 0.00065084
Iteration 18/25 | Loss: 0.00065084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006508373189717531, 0.0006508373189717531, 0.0006508373189717531, 0.0006508373189717531, 0.0006508373189717531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006508373189717531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.78338432
Iteration 2/25 | Loss: 0.00019127
Iteration 3/25 | Loss: 0.00019120
Iteration 4/25 | Loss: 0.00019119
Iteration 5/25 | Loss: 0.00019119
Iteration 6/25 | Loss: 0.00019119
Iteration 7/25 | Loss: 0.00019119
Iteration 8/25 | Loss: 0.00019119
Iteration 9/25 | Loss: 0.00019119
Iteration 10/25 | Loss: 0.00019119
Iteration 11/25 | Loss: 0.00019119
Iteration 12/25 | Loss: 0.00019119
Iteration 13/25 | Loss: 0.00019119
Iteration 14/25 | Loss: 0.00019119
Iteration 15/25 | Loss: 0.00019119
Iteration 16/25 | Loss: 0.00019119
Iteration 17/25 | Loss: 0.00019119
Iteration 18/25 | Loss: 0.00019119
Iteration 19/25 | Loss: 0.00019119
Iteration 20/25 | Loss: 0.00019119
Iteration 21/25 | Loss: 0.00019119
Iteration 22/25 | Loss: 0.00019119
Iteration 23/25 | Loss: 0.00019119
Iteration 24/25 | Loss: 0.00019119
Iteration 25/25 | Loss: 0.00019119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019119
Iteration 2/1000 | Loss: 0.00003525
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00002045
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001819
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001792
Iteration 11/1000 | Loss: 0.00001791
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001783
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00001777
Iteration 17/1000 | Loss: 0.00001772
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001749
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001746
Iteration 27/1000 | Loss: 0.00001745
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001744
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001738
Iteration 37/1000 | Loss: 0.00001738
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001737
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001737
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001735
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001728
Iteration 95/1000 | Loss: 0.00001728
Iteration 96/1000 | Loss: 0.00001728
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001727
Iteration 106/1000 | Loss: 0.00001727
Iteration 107/1000 | Loss: 0.00001727
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001726
Iteration 114/1000 | Loss: 0.00001726
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001725
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001725
Iteration 123/1000 | Loss: 0.00001725
Iteration 124/1000 | Loss: 0.00001725
Iteration 125/1000 | Loss: 0.00001725
Iteration 126/1000 | Loss: 0.00001724
Iteration 127/1000 | Loss: 0.00001724
Iteration 128/1000 | Loss: 0.00001724
Iteration 129/1000 | Loss: 0.00001724
Iteration 130/1000 | Loss: 0.00001724
Iteration 131/1000 | Loss: 0.00001724
Iteration 132/1000 | Loss: 0.00001724
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001722
Iteration 140/1000 | Loss: 0.00001722
Iteration 141/1000 | Loss: 0.00001722
Iteration 142/1000 | Loss: 0.00001722
Iteration 143/1000 | Loss: 0.00001722
Iteration 144/1000 | Loss: 0.00001722
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001721
Iteration 147/1000 | Loss: 0.00001721
Iteration 148/1000 | Loss: 0.00001721
Iteration 149/1000 | Loss: 0.00001721
Iteration 150/1000 | Loss: 0.00001721
Iteration 151/1000 | Loss: 0.00001721
Iteration 152/1000 | Loss: 0.00001721
Iteration 153/1000 | Loss: 0.00001721
Iteration 154/1000 | Loss: 0.00001721
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001721
Iteration 160/1000 | Loss: 0.00001721
Iteration 161/1000 | Loss: 0.00001721
Iteration 162/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.7209640645887703e-05, 1.7209640645887703e-05, 1.7209640645887703e-05, 1.7209640645887703e-05, 1.7209640645887703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7209640645887703e-05

Optimization complete. Final v2v error: 3.487114191055298 mm

Highest mean error: 4.044916152954102 mm for frame 239

Lowest mean error: 3.072978973388672 mm for frame 13

Saving results

Total time: 1422.7505402565002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0011
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020771
Iteration 2/25 | Loss: 0.01020771
Iteration 3/25 | Loss: 0.01020771
Iteration 4/25 | Loss: 0.00316762
Iteration 5/25 | Loss: 0.00214533
Iteration 6/25 | Loss: 0.00166449
Iteration 7/25 | Loss: 0.00193383
Iteration 8/25 | Loss: 0.00157529
Iteration 9/25 | Loss: 0.00146384
Iteration 10/25 | Loss: 0.00144839
Iteration 11/25 | Loss: 0.00145986
Iteration 12/25 | Loss: 0.00145462
Iteration 13/25 | Loss: 0.00142445
Iteration 14/25 | Loss: 0.00142380
Iteration 15/25 | Loss: 0.00142353
Iteration 16/25 | Loss: 0.00142312
Iteration 17/25 | Loss: 0.00142282
Iteration 18/25 | Loss: 0.00142724
Iteration 19/25 | Loss: 0.00142140
Iteration 20/25 | Loss: 0.00142034
Iteration 21/25 | Loss: 0.00142273
Iteration 22/25 | Loss: 0.00142380
Iteration 23/25 | Loss: 0.00142614
Iteration 24/25 | Loss: 0.00141929
Iteration 25/25 | Loss: 0.00141676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32108974
Iteration 2/25 | Loss: 0.00763807
Iteration 3/25 | Loss: 0.00531619
Iteration 4/25 | Loss: 0.00531618
Iteration 5/25 | Loss: 0.00531618
Iteration 6/25 | Loss: 0.00531618
Iteration 7/25 | Loss: 0.00531618
Iteration 8/25 | Loss: 0.00531618
Iteration 9/25 | Loss: 0.00531618
Iteration 10/25 | Loss: 0.00531618
Iteration 11/25 | Loss: 0.00531618
Iteration 12/25 | Loss: 0.00531618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.005316180642694235, 0.005316180642694235, 0.005316180642694235, 0.005316180642694235, 0.005316180642694235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005316180642694235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00531618
Iteration 2/1000 | Loss: 0.00822433
Iteration 3/1000 | Loss: 0.00277448
Iteration 4/1000 | Loss: 0.00176873
Iteration 5/1000 | Loss: 0.00249971
Iteration 6/1000 | Loss: 0.00049956
Iteration 7/1000 | Loss: 0.00043731
Iteration 8/1000 | Loss: 0.00040258
Iteration 9/1000 | Loss: 0.00141451
Iteration 10/1000 | Loss: 0.02639594
Iteration 11/1000 | Loss: 0.00770502
Iteration 12/1000 | Loss: 0.00172777
Iteration 13/1000 | Loss: 0.00111262
Iteration 14/1000 | Loss: 0.00085598
Iteration 15/1000 | Loss: 0.00077655
Iteration 16/1000 | Loss: 0.00122596
Iteration 17/1000 | Loss: 0.00300899
Iteration 18/1000 | Loss: 0.00135708
Iteration 19/1000 | Loss: 0.00324880
Iteration 20/1000 | Loss: 0.00281000
Iteration 21/1000 | Loss: 0.00350450
Iteration 22/1000 | Loss: 0.00258826
Iteration 23/1000 | Loss: 0.00230518
Iteration 24/1000 | Loss: 0.00092709
Iteration 25/1000 | Loss: 0.00011727
Iteration 26/1000 | Loss: 0.00016847
Iteration 27/1000 | Loss: 0.00053452
Iteration 28/1000 | Loss: 0.00066672
Iteration 29/1000 | Loss: 0.00026679
Iteration 30/1000 | Loss: 0.00004631
Iteration 31/1000 | Loss: 0.00006705
Iteration 32/1000 | Loss: 0.00012557
Iteration 33/1000 | Loss: 0.00055684
Iteration 34/1000 | Loss: 0.00028131
Iteration 35/1000 | Loss: 0.00046646
Iteration 36/1000 | Loss: 0.00121689
Iteration 37/1000 | Loss: 0.00006969
Iteration 38/1000 | Loss: 0.00016050
Iteration 39/1000 | Loss: 0.00002735
Iteration 40/1000 | Loss: 0.00027719
Iteration 41/1000 | Loss: 0.00022566
Iteration 42/1000 | Loss: 0.00007927
Iteration 43/1000 | Loss: 0.00007698
Iteration 44/1000 | Loss: 0.00025487
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002249
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002173
Iteration 51/1000 | Loss: 0.00002161
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00017045
Iteration 54/1000 | Loss: 0.00073279
Iteration 55/1000 | Loss: 0.00002261
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002148
Iteration 64/1000 | Loss: 0.00002148
Iteration 65/1000 | Loss: 0.00002148
Iteration 66/1000 | Loss: 0.00002148
Iteration 67/1000 | Loss: 0.00002147
Iteration 68/1000 | Loss: 0.00002147
Iteration 69/1000 | Loss: 0.00002146
Iteration 70/1000 | Loss: 0.00002146
Iteration 71/1000 | Loss: 0.00002145
Iteration 72/1000 | Loss: 0.00002145
Iteration 73/1000 | Loss: 0.00008062
Iteration 74/1000 | Loss: 0.00008062
Iteration 75/1000 | Loss: 0.00008062
Iteration 76/1000 | Loss: 0.00008061
Iteration 77/1000 | Loss: 0.00233398
Iteration 78/1000 | Loss: 0.00021563
Iteration 79/1000 | Loss: 0.00015105
Iteration 80/1000 | Loss: 0.00006146
Iteration 81/1000 | Loss: 0.00007249
Iteration 82/1000 | Loss: 0.00008984
Iteration 83/1000 | Loss: 0.00051401
Iteration 84/1000 | Loss: 0.00011298
Iteration 85/1000 | Loss: 0.00003494
Iteration 86/1000 | Loss: 0.00002167
Iteration 87/1000 | Loss: 0.00003923
Iteration 88/1000 | Loss: 0.00024621
Iteration 89/1000 | Loss: 0.00008808
Iteration 90/1000 | Loss: 0.00006635
Iteration 91/1000 | Loss: 0.00002157
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002143
Iteration 94/1000 | Loss: 0.00002143
Iteration 95/1000 | Loss: 0.00002143
Iteration 96/1000 | Loss: 0.00002143
Iteration 97/1000 | Loss: 0.00004276
Iteration 98/1000 | Loss: 0.00002146
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002140
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002140
Iteration 103/1000 | Loss: 0.00002140
Iteration 104/1000 | Loss: 0.00002140
Iteration 105/1000 | Loss: 0.00002140
Iteration 106/1000 | Loss: 0.00002140
Iteration 107/1000 | Loss: 0.00002140
Iteration 108/1000 | Loss: 0.00002140
Iteration 109/1000 | Loss: 0.00002140
Iteration 110/1000 | Loss: 0.00002140
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002139
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002138
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002138
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002137
Iteration 124/1000 | Loss: 0.00002137
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002137
Iteration 128/1000 | Loss: 0.00002136
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002136
Iteration 131/1000 | Loss: 0.00002136
Iteration 132/1000 | Loss: 0.00002136
Iteration 133/1000 | Loss: 0.00002136
Iteration 134/1000 | Loss: 0.00002136
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002136
Iteration 138/1000 | Loss: 0.00002136
Iteration 139/1000 | Loss: 0.00002136
Iteration 140/1000 | Loss: 0.00002136
Iteration 141/1000 | Loss: 0.00002136
Iteration 142/1000 | Loss: 0.00002136
Iteration 143/1000 | Loss: 0.00002136
Iteration 144/1000 | Loss: 0.00002136
Iteration 145/1000 | Loss: 0.00002136
Iteration 146/1000 | Loss: 0.00002136
Iteration 147/1000 | Loss: 0.00002136
Iteration 148/1000 | Loss: 0.00002136
Iteration 149/1000 | Loss: 0.00002136
Iteration 150/1000 | Loss: 0.00002136
Iteration 151/1000 | Loss: 0.00002136
Iteration 152/1000 | Loss: 0.00002136
Iteration 153/1000 | Loss: 0.00002136
Iteration 154/1000 | Loss: 0.00002136
Iteration 155/1000 | Loss: 0.00002136
Iteration 156/1000 | Loss: 0.00002136
Iteration 157/1000 | Loss: 0.00002136
Iteration 158/1000 | Loss: 0.00002136
Iteration 159/1000 | Loss: 0.00002136
Iteration 160/1000 | Loss: 0.00002136
Iteration 161/1000 | Loss: 0.00002136
Iteration 162/1000 | Loss: 0.00002136
Iteration 163/1000 | Loss: 0.00002136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.135510658263229e-05, 2.135510658263229e-05, 2.135510658263229e-05, 2.135510658263229e-05, 2.135510658263229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.135510658263229e-05

Optimization complete. Final v2v error: 3.9293365478515625 mm

Highest mean error: 4.226535797119141 mm for frame 23

Lowest mean error: 3.5171396732330322 mm for frame 0

Saving results

Total time: 4973.192178487778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0000
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862477
Iteration 2/25 | Loss: 0.00103984
Iteration 3/25 | Loss: 0.00072912
Iteration 4/25 | Loss: 0.00069617
Iteration 5/25 | Loss: 0.00069080
Iteration 6/25 | Loss: 0.00068962
Iteration 7/25 | Loss: 0.00068950
Iteration 8/25 | Loss: 0.00068950
Iteration 9/25 | Loss: 0.00068950
Iteration 10/25 | Loss: 0.00068950
Iteration 11/25 | Loss: 0.00068950
Iteration 12/25 | Loss: 0.00068950
Iteration 13/25 | Loss: 0.00068950
Iteration 14/25 | Loss: 0.00068950
Iteration 15/25 | Loss: 0.00068950
Iteration 16/25 | Loss: 0.00068950
Iteration 17/25 | Loss: 0.00068950
Iteration 18/25 | Loss: 0.00068950
Iteration 19/25 | Loss: 0.00068950
Iteration 20/25 | Loss: 0.00068950
Iteration 21/25 | Loss: 0.00068950
Iteration 22/25 | Loss: 0.00068950
Iteration 23/25 | Loss: 0.00068950
Iteration 24/25 | Loss: 0.00068950
Iteration 25/25 | Loss: 0.00068950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000689504318870604, 0.000689504318870604, 0.000689504318870604, 0.000689504318870604, 0.000689504318870604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000689504318870604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90394199
Iteration 2/25 | Loss: 0.00024042
Iteration 3/25 | Loss: 0.00024042
Iteration 4/25 | Loss: 0.00024042
Iteration 5/25 | Loss: 0.00024042
Iteration 6/25 | Loss: 0.00024042
Iteration 7/25 | Loss: 0.00024042
Iteration 8/25 | Loss: 0.00024042
Iteration 9/25 | Loss: 0.00024042
Iteration 10/25 | Loss: 0.00024042
Iteration 11/25 | Loss: 0.00024042
Iteration 12/25 | Loss: 0.00024042
Iteration 13/25 | Loss: 0.00024042
Iteration 14/25 | Loss: 0.00024042
Iteration 15/25 | Loss: 0.00024042
Iteration 16/25 | Loss: 0.00024042
Iteration 17/25 | Loss: 0.00024042
Iteration 18/25 | Loss: 0.00024042
Iteration 19/25 | Loss: 0.00024042
Iteration 20/25 | Loss: 0.00024042
Iteration 21/25 | Loss: 0.00024042
Iteration 22/25 | Loss: 0.00024042
Iteration 23/25 | Loss: 0.00024042
Iteration 24/25 | Loss: 0.00024042
Iteration 25/25 | Loss: 0.00024042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024042
Iteration 2/1000 | Loss: 0.00005109
Iteration 3/1000 | Loss: 0.00003875
Iteration 4/1000 | Loss: 0.00003467
Iteration 5/1000 | Loss: 0.00003254
Iteration 6/1000 | Loss: 0.00003090
Iteration 7/1000 | Loss: 0.00002984
Iteration 8/1000 | Loss: 0.00002904
Iteration 9/1000 | Loss: 0.00002852
Iteration 10/1000 | Loss: 0.00002815
Iteration 11/1000 | Loss: 0.00002789
Iteration 12/1000 | Loss: 0.00002773
Iteration 13/1000 | Loss: 0.00002755
Iteration 14/1000 | Loss: 0.00002747
Iteration 15/1000 | Loss: 0.00002738
Iteration 16/1000 | Loss: 0.00002734
Iteration 17/1000 | Loss: 0.00002730
Iteration 18/1000 | Loss: 0.00002730
Iteration 19/1000 | Loss: 0.00002729
Iteration 20/1000 | Loss: 0.00002728
Iteration 21/1000 | Loss: 0.00002728
Iteration 22/1000 | Loss: 0.00002727
Iteration 23/1000 | Loss: 0.00002727
Iteration 24/1000 | Loss: 0.00002727
Iteration 25/1000 | Loss: 0.00002726
Iteration 26/1000 | Loss: 0.00002723
Iteration 27/1000 | Loss: 0.00002723
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00002723
Iteration 30/1000 | Loss: 0.00002722
Iteration 31/1000 | Loss: 0.00002721
Iteration 32/1000 | Loss: 0.00002720
Iteration 33/1000 | Loss: 0.00002720
Iteration 34/1000 | Loss: 0.00002719
Iteration 35/1000 | Loss: 0.00002718
Iteration 36/1000 | Loss: 0.00002718
Iteration 37/1000 | Loss: 0.00002718
Iteration 38/1000 | Loss: 0.00002716
Iteration 39/1000 | Loss: 0.00002716
Iteration 40/1000 | Loss: 0.00002715
Iteration 41/1000 | Loss: 0.00002715
Iteration 42/1000 | Loss: 0.00002715
Iteration 43/1000 | Loss: 0.00002715
Iteration 44/1000 | Loss: 0.00002714
Iteration 45/1000 | Loss: 0.00002714
Iteration 46/1000 | Loss: 0.00002714
Iteration 47/1000 | Loss: 0.00002714
Iteration 48/1000 | Loss: 0.00002714
Iteration 49/1000 | Loss: 0.00002714
Iteration 50/1000 | Loss: 0.00002714
Iteration 51/1000 | Loss: 0.00002714
Iteration 52/1000 | Loss: 0.00002714
Iteration 53/1000 | Loss: 0.00002713
Iteration 54/1000 | Loss: 0.00002713
Iteration 55/1000 | Loss: 0.00002713
Iteration 56/1000 | Loss: 0.00002713
Iteration 57/1000 | Loss: 0.00002713
Iteration 58/1000 | Loss: 0.00002713
Iteration 59/1000 | Loss: 0.00002713
Iteration 60/1000 | Loss: 0.00002713
Iteration 61/1000 | Loss: 0.00002713
Iteration 62/1000 | Loss: 0.00002713
Iteration 63/1000 | Loss: 0.00002712
Iteration 64/1000 | Loss: 0.00002712
Iteration 65/1000 | Loss: 0.00002712
Iteration 66/1000 | Loss: 0.00002712
Iteration 67/1000 | Loss: 0.00002712
Iteration 68/1000 | Loss: 0.00002712
Iteration 69/1000 | Loss: 0.00002711
Iteration 70/1000 | Loss: 0.00002711
Iteration 71/1000 | Loss: 0.00002711
Iteration 72/1000 | Loss: 0.00002711
Iteration 73/1000 | Loss: 0.00002711
Iteration 74/1000 | Loss: 0.00002711
Iteration 75/1000 | Loss: 0.00002711
Iteration 76/1000 | Loss: 0.00002711
Iteration 77/1000 | Loss: 0.00002711
Iteration 78/1000 | Loss: 0.00002711
Iteration 79/1000 | Loss: 0.00002711
Iteration 80/1000 | Loss: 0.00002711
Iteration 81/1000 | Loss: 0.00002711
Iteration 82/1000 | Loss: 0.00002711
Iteration 83/1000 | Loss: 0.00002711
Iteration 84/1000 | Loss: 0.00002711
Iteration 85/1000 | Loss: 0.00002711
Iteration 86/1000 | Loss: 0.00002711
Iteration 87/1000 | Loss: 0.00002711
Iteration 88/1000 | Loss: 0.00002711
Iteration 89/1000 | Loss: 0.00002711
Iteration 90/1000 | Loss: 0.00002711
Iteration 91/1000 | Loss: 0.00002711
Iteration 92/1000 | Loss: 0.00002711
Iteration 93/1000 | Loss: 0.00002711
Iteration 94/1000 | Loss: 0.00002711
Iteration 95/1000 | Loss: 0.00002711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.7110578230349347e-05, 2.7110578230349347e-05, 2.7110578230349347e-05, 2.7110578230349347e-05, 2.7110578230349347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7110578230349347e-05

Optimization complete. Final v2v error: 4.421073913574219 mm

Highest mean error: 4.642740249633789 mm for frame 20

Lowest mean error: 4.199060916900635 mm for frame 89

Saving results

Total time: 766.997641324997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0002
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602254
Iteration 2/25 | Loss: 0.00121847
Iteration 3/25 | Loss: 0.00072595
Iteration 4/25 | Loss: 0.00065192
Iteration 5/25 | Loss: 0.00063577
Iteration 6/25 | Loss: 0.00063240
Iteration 7/25 | Loss: 0.00063200
Iteration 8/25 | Loss: 0.00063200
Iteration 9/25 | Loss: 0.00063200
Iteration 10/25 | Loss: 0.00063200
Iteration 11/25 | Loss: 0.00063200
Iteration 12/25 | Loss: 0.00063200
Iteration 13/25 | Loss: 0.00063200
Iteration 14/25 | Loss: 0.00063200
Iteration 15/25 | Loss: 0.00063200
Iteration 16/25 | Loss: 0.00063200
Iteration 17/25 | Loss: 0.00063200
Iteration 18/25 | Loss: 0.00063200
Iteration 19/25 | Loss: 0.00063200
Iteration 20/25 | Loss: 0.00063200
Iteration 21/25 | Loss: 0.00063200
Iteration 22/25 | Loss: 0.00063200
Iteration 23/25 | Loss: 0.00063200
Iteration 24/25 | Loss: 0.00063200
Iteration 25/25 | Loss: 0.00063200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006319978856481612, 0.0006319978856481612, 0.0006319978856481612, 0.0006319978856481612, 0.0006319978856481612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006319978856481612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23093796
Iteration 2/25 | Loss: 0.00012261
Iteration 3/25 | Loss: 0.00012261
Iteration 4/25 | Loss: 0.00012261
Iteration 5/25 | Loss: 0.00012261
Iteration 6/25 | Loss: 0.00012261
Iteration 7/25 | Loss: 0.00012261
Iteration 8/25 | Loss: 0.00012261
Iteration 9/25 | Loss: 0.00012261
Iteration 10/25 | Loss: 0.00012261
Iteration 11/25 | Loss: 0.00012261
Iteration 12/25 | Loss: 0.00012261
Iteration 13/25 | Loss: 0.00012261
Iteration 14/25 | Loss: 0.00012261
Iteration 15/25 | Loss: 0.00012261
Iteration 16/25 | Loss: 0.00012261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00012260687071830034, 0.00012260687071830034, 0.00012260687071830034, 0.00012260687071830034, 0.00012260687071830034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012260687071830034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012261
Iteration 2/1000 | Loss: 0.00003656
Iteration 3/1000 | Loss: 0.00002319
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001837
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001725
Iteration 9/1000 | Loss: 0.00001688
Iteration 10/1000 | Loss: 0.00001666
Iteration 11/1000 | Loss: 0.00001653
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001628
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001619
Iteration 20/1000 | Loss: 0.00001619
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001616
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001611
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001610
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001610
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001608
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001608
Iteration 53/1000 | Loss: 0.00001608
Iteration 54/1000 | Loss: 0.00001607
Iteration 55/1000 | Loss: 0.00001607
Iteration 56/1000 | Loss: 0.00001607
Iteration 57/1000 | Loss: 0.00001607
Iteration 58/1000 | Loss: 0.00001607
Iteration 59/1000 | Loss: 0.00001606
Iteration 60/1000 | Loss: 0.00001606
Iteration 61/1000 | Loss: 0.00001606
Iteration 62/1000 | Loss: 0.00001606
Iteration 63/1000 | Loss: 0.00001606
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001605
Iteration 68/1000 | Loss: 0.00001605
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001605
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001602
Iteration 99/1000 | Loss: 0.00001602
Iteration 100/1000 | Loss: 0.00001602
Iteration 101/1000 | Loss: 0.00001602
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001601
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001601
Iteration 107/1000 | Loss: 0.00001601
Iteration 108/1000 | Loss: 0.00001601
Iteration 109/1000 | Loss: 0.00001601
Iteration 110/1000 | Loss: 0.00001601
Iteration 111/1000 | Loss: 0.00001601
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001601
Iteration 114/1000 | Loss: 0.00001600
Iteration 115/1000 | Loss: 0.00001600
Iteration 116/1000 | Loss: 0.00001600
Iteration 117/1000 | Loss: 0.00001600
Iteration 118/1000 | Loss: 0.00001600
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001600
Iteration 122/1000 | Loss: 0.00001600
Iteration 123/1000 | Loss: 0.00001600
Iteration 124/1000 | Loss: 0.00001600
Iteration 125/1000 | Loss: 0.00001600
Iteration 126/1000 | Loss: 0.00001600
Iteration 127/1000 | Loss: 0.00001600
Iteration 128/1000 | Loss: 0.00001600
Iteration 129/1000 | Loss: 0.00001600
Iteration 130/1000 | Loss: 0.00001600
Iteration 131/1000 | Loss: 0.00001600
Iteration 132/1000 | Loss: 0.00001600
Iteration 133/1000 | Loss: 0.00001600
Iteration 134/1000 | Loss: 0.00001600
Iteration 135/1000 | Loss: 0.00001600
Iteration 136/1000 | Loss: 0.00001600
Iteration 137/1000 | Loss: 0.00001600
Iteration 138/1000 | Loss: 0.00001600
Iteration 139/1000 | Loss: 0.00001600
Iteration 140/1000 | Loss: 0.00001600
Iteration 141/1000 | Loss: 0.00001600
Iteration 142/1000 | Loss: 0.00001600
Iteration 143/1000 | Loss: 0.00001600
Iteration 144/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5995174180716276e-05, 1.5995174180716276e-05, 1.5995174180716276e-05, 1.5995174180716276e-05, 1.5995174180716276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5995174180716276e-05

Optimization complete. Final v2v error: 3.4633712768554688 mm

Highest mean error: 4.239336967468262 mm for frame 84

Lowest mean error: 3.0063095092773438 mm for frame 189

Saving results

Total time: 999.8373382091522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0022
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074267
Iteration 2/25 | Loss: 0.01074267
Iteration 3/25 | Loss: 0.00180908
Iteration 4/25 | Loss: 0.00100315
Iteration 5/25 | Loss: 0.00112587
Iteration 6/25 | Loss: 0.00116165
Iteration 7/25 | Loss: 0.00085772
Iteration 8/25 | Loss: 0.00075875
Iteration 9/25 | Loss: 0.00074587
Iteration 10/25 | Loss: 0.00072465
Iteration 11/25 | Loss: 0.00072046
Iteration 12/25 | Loss: 0.00071801
Iteration 13/25 | Loss: 0.00071663
Iteration 14/25 | Loss: 0.00074512
Iteration 15/25 | Loss: 0.00071082
Iteration 16/25 | Loss: 0.00070769
Iteration 17/25 | Loss: 0.00070686
Iteration 18/25 | Loss: 0.00070576
Iteration 19/25 | Loss: 0.00070777
Iteration 20/25 | Loss: 0.00070453
Iteration 21/25 | Loss: 0.00070280
Iteration 22/25 | Loss: 0.00070203
Iteration 23/25 | Loss: 0.00070176
Iteration 24/25 | Loss: 0.00070173
Iteration 25/25 | Loss: 0.00070173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47930014
Iteration 2/25 | Loss: 0.00018305
Iteration 3/25 | Loss: 0.00018305
Iteration 4/25 | Loss: 0.00018305
Iteration 5/25 | Loss: 0.00018305
Iteration 6/25 | Loss: 0.00018305
Iteration 7/25 | Loss: 0.00018305
Iteration 8/25 | Loss: 0.00018305
Iteration 9/25 | Loss: 0.00018305
Iteration 10/25 | Loss: 0.00018305
Iteration 11/25 | Loss: 0.00018305
Iteration 12/25 | Loss: 0.00018305
Iteration 13/25 | Loss: 0.00018305
Iteration 14/25 | Loss: 0.00018305
Iteration 15/25 | Loss: 0.00018305
Iteration 16/25 | Loss: 0.00018305
Iteration 17/25 | Loss: 0.00018305
Iteration 18/25 | Loss: 0.00018305
Iteration 19/25 | Loss: 0.00018305
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00018305137928109616, 0.00018305137928109616, 0.00018305137928109616, 0.00018305137928109616, 0.00018305137928109616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018305137928109616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018305
Iteration 2/1000 | Loss: 0.00004795
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002780
Iteration 6/1000 | Loss: 0.00002681
Iteration 7/1000 | Loss: 0.00002598
Iteration 8/1000 | Loss: 0.00026025
Iteration 9/1000 | Loss: 0.00002698
Iteration 10/1000 | Loss: 0.00002467
Iteration 11/1000 | Loss: 0.00002391
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002302
Iteration 14/1000 | Loss: 0.00002270
Iteration 15/1000 | Loss: 0.00002264
Iteration 16/1000 | Loss: 0.00002259
Iteration 17/1000 | Loss: 0.00002258
Iteration 18/1000 | Loss: 0.00002252
Iteration 19/1000 | Loss: 0.00002244
Iteration 20/1000 | Loss: 0.00002243
Iteration 21/1000 | Loss: 0.00002239
Iteration 22/1000 | Loss: 0.00002234
Iteration 23/1000 | Loss: 0.00002233
Iteration 24/1000 | Loss: 0.00002232
Iteration 25/1000 | Loss: 0.00002231
Iteration 26/1000 | Loss: 0.00002231
Iteration 27/1000 | Loss: 0.00002228
Iteration 28/1000 | Loss: 0.00002227
Iteration 29/1000 | Loss: 0.00002227
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002218
Iteration 32/1000 | Loss: 0.00002216
Iteration 33/1000 | Loss: 0.00002215
Iteration 34/1000 | Loss: 0.00002215
Iteration 35/1000 | Loss: 0.00002215
Iteration 36/1000 | Loss: 0.00002214
Iteration 37/1000 | Loss: 0.00002213
Iteration 38/1000 | Loss: 0.00002213
Iteration 39/1000 | Loss: 0.00002213
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002213
Iteration 42/1000 | Loss: 0.00002213
Iteration 43/1000 | Loss: 0.00002212
Iteration 44/1000 | Loss: 0.00002212
Iteration 45/1000 | Loss: 0.00002212
Iteration 46/1000 | Loss: 0.00002212
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002211
Iteration 49/1000 | Loss: 0.00002210
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002209
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002208
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002208
Iteration 56/1000 | Loss: 0.00002208
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002207
Iteration 59/1000 | Loss: 0.00002207
Iteration 60/1000 | Loss: 0.00002206
Iteration 61/1000 | Loss: 0.00002205
Iteration 62/1000 | Loss: 0.00002205
Iteration 63/1000 | Loss: 0.00002205
Iteration 64/1000 | Loss: 0.00002205
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002204
Iteration 67/1000 | Loss: 0.00002204
Iteration 68/1000 | Loss: 0.00002204
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002203
Iteration 72/1000 | Loss: 0.00002203
Iteration 73/1000 | Loss: 0.00002203
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002202
Iteration 76/1000 | Loss: 0.00002202
Iteration 77/1000 | Loss: 0.00002202
Iteration 78/1000 | Loss: 0.00002202
Iteration 79/1000 | Loss: 0.00002202
Iteration 80/1000 | Loss: 0.00002202
Iteration 81/1000 | Loss: 0.00002202
Iteration 82/1000 | Loss: 0.00002202
Iteration 83/1000 | Loss: 0.00002202
Iteration 84/1000 | Loss: 0.00002202
Iteration 85/1000 | Loss: 0.00002202
Iteration 86/1000 | Loss: 0.00002202
Iteration 87/1000 | Loss: 0.00002202
Iteration 88/1000 | Loss: 0.00002202
Iteration 89/1000 | Loss: 0.00002201
Iteration 90/1000 | Loss: 0.00002201
Iteration 91/1000 | Loss: 0.00002201
Iteration 92/1000 | Loss: 0.00002201
Iteration 93/1000 | Loss: 0.00002201
Iteration 94/1000 | Loss: 0.00002201
Iteration 95/1000 | Loss: 0.00002201
Iteration 96/1000 | Loss: 0.00002201
Iteration 97/1000 | Loss: 0.00002201
Iteration 98/1000 | Loss: 0.00002201
Iteration 99/1000 | Loss: 0.00002201
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002199
Iteration 103/1000 | Loss: 0.00002199
Iteration 104/1000 | Loss: 0.00002199
Iteration 105/1000 | Loss: 0.00002199
Iteration 106/1000 | Loss: 0.00002199
Iteration 107/1000 | Loss: 0.00002198
Iteration 108/1000 | Loss: 0.00002198
Iteration 109/1000 | Loss: 0.00002198
Iteration 110/1000 | Loss: 0.00002197
Iteration 111/1000 | Loss: 0.00002197
Iteration 112/1000 | Loss: 0.00002197
Iteration 113/1000 | Loss: 0.00002197
Iteration 114/1000 | Loss: 0.00002196
Iteration 115/1000 | Loss: 0.00002196
Iteration 116/1000 | Loss: 0.00002196
Iteration 117/1000 | Loss: 0.00002195
Iteration 118/1000 | Loss: 0.00002195
Iteration 119/1000 | Loss: 0.00002195
Iteration 120/1000 | Loss: 0.00002194
Iteration 121/1000 | Loss: 0.00002194
Iteration 122/1000 | Loss: 0.00002194
Iteration 123/1000 | Loss: 0.00002194
Iteration 124/1000 | Loss: 0.00002194
Iteration 125/1000 | Loss: 0.00002194
Iteration 126/1000 | Loss: 0.00002194
Iteration 127/1000 | Loss: 0.00002194
Iteration 128/1000 | Loss: 0.00002194
Iteration 129/1000 | Loss: 0.00002194
Iteration 130/1000 | Loss: 0.00002193
Iteration 131/1000 | Loss: 0.00002193
Iteration 132/1000 | Loss: 0.00002193
Iteration 133/1000 | Loss: 0.00002193
Iteration 134/1000 | Loss: 0.00002193
Iteration 135/1000 | Loss: 0.00002193
Iteration 136/1000 | Loss: 0.00002193
Iteration 137/1000 | Loss: 0.00002193
Iteration 138/1000 | Loss: 0.00002193
Iteration 139/1000 | Loss: 0.00002193
Iteration 140/1000 | Loss: 0.00002193
Iteration 141/1000 | Loss: 0.00002193
Iteration 142/1000 | Loss: 0.00002193
Iteration 143/1000 | Loss: 0.00002193
Iteration 144/1000 | Loss: 0.00002193
Iteration 145/1000 | Loss: 0.00002193
Iteration 146/1000 | Loss: 0.00002193
Iteration 147/1000 | Loss: 0.00002193
Iteration 148/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.192758438468445e-05, 2.192758438468445e-05, 2.192758438468445e-05, 2.192758438468445e-05, 2.192758438468445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.192758438468445e-05

Optimization complete. Final v2v error: 3.995938777923584 mm

Highest mean error: 4.478179454803467 mm for frame 150

Lowest mean error: 3.4420461654663086 mm for frame 97

Saving results

Total time: 2722.522385120392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0021
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666770
Iteration 2/25 | Loss: 0.00079903
Iteration 3/25 | Loss: 0.00061927
Iteration 4/25 | Loss: 0.00059200
Iteration 5/25 | Loss: 0.00058538
Iteration 6/25 | Loss: 0.00058379
Iteration 7/25 | Loss: 0.00058364
Iteration 8/25 | Loss: 0.00058364
Iteration 9/25 | Loss: 0.00058364
Iteration 10/25 | Loss: 0.00058364
Iteration 11/25 | Loss: 0.00058364
Iteration 12/25 | Loss: 0.00058364
Iteration 13/25 | Loss: 0.00058364
Iteration 14/25 | Loss: 0.00058364
Iteration 15/25 | Loss: 0.00058364
Iteration 16/25 | Loss: 0.00058364
Iteration 17/25 | Loss: 0.00058364
Iteration 18/25 | Loss: 0.00058364
Iteration 19/25 | Loss: 0.00058364
Iteration 20/25 | Loss: 0.00058364
Iteration 21/25 | Loss: 0.00058364
Iteration 22/25 | Loss: 0.00058364
Iteration 23/25 | Loss: 0.00058364
Iteration 24/25 | Loss: 0.00058364
Iteration 25/25 | Loss: 0.00058364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09807444
Iteration 2/25 | Loss: 0.00011331
Iteration 3/25 | Loss: 0.00011322
Iteration 4/25 | Loss: 0.00011322
Iteration 5/25 | Loss: 0.00011322
Iteration 6/25 | Loss: 0.00011322
Iteration 7/25 | Loss: 0.00011322
Iteration 8/25 | Loss: 0.00011322
Iteration 9/25 | Loss: 0.00011322
Iteration 10/25 | Loss: 0.00011322
Iteration 11/25 | Loss: 0.00011322
Iteration 12/25 | Loss: 0.00011322
Iteration 13/25 | Loss: 0.00011322
Iteration 14/25 | Loss: 0.00011322
Iteration 15/25 | Loss: 0.00011322
Iteration 16/25 | Loss: 0.00011322
Iteration 17/25 | Loss: 0.00011322
Iteration 18/25 | Loss: 0.00011322
Iteration 19/25 | Loss: 0.00011322
Iteration 20/25 | Loss: 0.00011322
Iteration 21/25 | Loss: 0.00011322
Iteration 22/25 | Loss: 0.00011322
Iteration 23/25 | Loss: 0.00011322
Iteration 24/25 | Loss: 0.00011322
Iteration 25/25 | Loss: 0.00011322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011322
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00002660
Iteration 4/1000 | Loss: 0.00002462
Iteration 5/1000 | Loss: 0.00002255
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00002074
Iteration 8/1000 | Loss: 0.00002037
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001976
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001974
Iteration 17/1000 | Loss: 0.00001973
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00001970
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001969
Iteration 23/1000 | Loss: 0.00001969
Iteration 24/1000 | Loss: 0.00001969
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001966
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001966
Iteration 29/1000 | Loss: 0.00001966
Iteration 30/1000 | Loss: 0.00001965
Iteration 31/1000 | Loss: 0.00001965
Iteration 32/1000 | Loss: 0.00001964
Iteration 33/1000 | Loss: 0.00001962
Iteration 34/1000 | Loss: 0.00001962
Iteration 35/1000 | Loss: 0.00001962
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001961
Iteration 38/1000 | Loss: 0.00001961
Iteration 39/1000 | Loss: 0.00001960
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001959
Iteration 43/1000 | Loss: 0.00001958
Iteration 44/1000 | Loss: 0.00001958
Iteration 45/1000 | Loss: 0.00001958
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001956
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001955
Iteration 51/1000 | Loss: 0.00001955
Iteration 52/1000 | Loss: 0.00001955
Iteration 53/1000 | Loss: 0.00001954
Iteration 54/1000 | Loss: 0.00001954
Iteration 55/1000 | Loss: 0.00001954
Iteration 56/1000 | Loss: 0.00001953
Iteration 57/1000 | Loss: 0.00001953
Iteration 58/1000 | Loss: 0.00001953
Iteration 59/1000 | Loss: 0.00001953
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001952
Iteration 62/1000 | Loss: 0.00001952
Iteration 63/1000 | Loss: 0.00001951
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001950
Iteration 67/1000 | Loss: 0.00001950
Iteration 68/1000 | Loss: 0.00001950
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001947
Iteration 79/1000 | Loss: 0.00001947
Iteration 80/1000 | Loss: 0.00001947
Iteration 81/1000 | Loss: 0.00001947
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001946
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00001944
Iteration 96/1000 | Loss: 0.00001944
Iteration 97/1000 | Loss: 0.00001944
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001943
Iteration 100/1000 | Loss: 0.00001943
Iteration 101/1000 | Loss: 0.00001943
Iteration 102/1000 | Loss: 0.00001943
Iteration 103/1000 | Loss: 0.00001943
Iteration 104/1000 | Loss: 0.00001943
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001942
Iteration 109/1000 | Loss: 0.00001942
Iteration 110/1000 | Loss: 0.00001942
Iteration 111/1000 | Loss: 0.00001942
Iteration 112/1000 | Loss: 0.00001942
Iteration 113/1000 | Loss: 0.00001942
Iteration 114/1000 | Loss: 0.00001942
Iteration 115/1000 | Loss: 0.00001942
Iteration 116/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.9424836864345707e-05, 1.9424836864345707e-05, 1.9424836864345707e-05, 1.9424836864345707e-05, 1.9424836864345707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9424836864345707e-05

Optimization complete. Final v2v error: 3.717778444290161 mm

Highest mean error: 3.9588165283203125 mm for frame 40

Lowest mean error: 3.0562727451324463 mm for frame 0

Saving results

Total time: 872.6761045455933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0013
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563407
Iteration 2/25 | Loss: 0.00106868
Iteration 3/25 | Loss: 0.00078685
Iteration 4/25 | Loss: 0.00074702
Iteration 5/25 | Loss: 0.00074002
Iteration 6/25 | Loss: 0.00073909
Iteration 7/25 | Loss: 0.00073909
Iteration 8/25 | Loss: 0.00073909
Iteration 9/25 | Loss: 0.00073909
Iteration 10/25 | Loss: 0.00073909
Iteration 11/25 | Loss: 0.00073909
Iteration 12/25 | Loss: 0.00073909
Iteration 13/25 | Loss: 0.00073909
Iteration 14/25 | Loss: 0.00073909
Iteration 15/25 | Loss: 0.00073909
Iteration 16/25 | Loss: 0.00073909
Iteration 17/25 | Loss: 0.00073909
Iteration 18/25 | Loss: 0.00073909
Iteration 19/25 | Loss: 0.00073909
Iteration 20/25 | Loss: 0.00073909
Iteration 21/25 | Loss: 0.00073909
Iteration 22/25 | Loss: 0.00073909
Iteration 23/25 | Loss: 0.00073909
Iteration 24/25 | Loss: 0.00073909
Iteration 25/25 | Loss: 0.00073909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40576065
Iteration 2/25 | Loss: 0.00020787
Iteration 3/25 | Loss: 0.00020787
Iteration 4/25 | Loss: 0.00020787
Iteration 5/25 | Loss: 0.00020787
Iteration 6/25 | Loss: 0.00020787
Iteration 7/25 | Loss: 0.00020787
Iteration 8/25 | Loss: 0.00020787
Iteration 9/25 | Loss: 0.00020787
Iteration 10/25 | Loss: 0.00020787
Iteration 11/25 | Loss: 0.00020787
Iteration 12/25 | Loss: 0.00020787
Iteration 13/25 | Loss: 0.00020787
Iteration 14/25 | Loss: 0.00020787
Iteration 15/25 | Loss: 0.00020787
Iteration 16/25 | Loss: 0.00020787
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00020786623645108193, 0.00020786623645108193, 0.00020786623645108193, 0.00020786623645108193, 0.00020786623645108193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020786623645108193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020787
Iteration 2/1000 | Loss: 0.00004092
Iteration 3/1000 | Loss: 0.00003260
Iteration 4/1000 | Loss: 0.00003100
Iteration 5/1000 | Loss: 0.00002986
Iteration 6/1000 | Loss: 0.00002910
Iteration 7/1000 | Loss: 0.00002849
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002793
Iteration 10/1000 | Loss: 0.00002793
Iteration 11/1000 | Loss: 0.00002780
Iteration 12/1000 | Loss: 0.00002773
Iteration 13/1000 | Loss: 0.00002773
Iteration 14/1000 | Loss: 0.00002772
Iteration 15/1000 | Loss: 0.00002771
Iteration 16/1000 | Loss: 0.00002771
Iteration 17/1000 | Loss: 0.00002771
Iteration 18/1000 | Loss: 0.00002771
Iteration 19/1000 | Loss: 0.00002770
Iteration 20/1000 | Loss: 0.00002769
Iteration 21/1000 | Loss: 0.00002769
Iteration 22/1000 | Loss: 0.00002769
Iteration 23/1000 | Loss: 0.00002769
Iteration 24/1000 | Loss: 0.00002768
Iteration 25/1000 | Loss: 0.00002768
Iteration 26/1000 | Loss: 0.00002768
Iteration 27/1000 | Loss: 0.00002766
Iteration 28/1000 | Loss: 0.00002765
Iteration 29/1000 | Loss: 0.00002765
Iteration 30/1000 | Loss: 0.00002762
Iteration 31/1000 | Loss: 0.00002762
Iteration 32/1000 | Loss: 0.00002762
Iteration 33/1000 | Loss: 0.00002762
Iteration 34/1000 | Loss: 0.00002762
Iteration 35/1000 | Loss: 0.00002762
Iteration 36/1000 | Loss: 0.00002762
Iteration 37/1000 | Loss: 0.00002762
Iteration 38/1000 | Loss: 0.00002762
Iteration 39/1000 | Loss: 0.00002762
Iteration 40/1000 | Loss: 0.00002762
Iteration 41/1000 | Loss: 0.00002761
Iteration 42/1000 | Loss: 0.00002761
Iteration 43/1000 | Loss: 0.00002760
Iteration 44/1000 | Loss: 0.00002760
Iteration 45/1000 | Loss: 0.00002760
Iteration 46/1000 | Loss: 0.00002759
Iteration 47/1000 | Loss: 0.00002759
Iteration 48/1000 | Loss: 0.00002759
Iteration 49/1000 | Loss: 0.00002759
Iteration 50/1000 | Loss: 0.00002759
Iteration 51/1000 | Loss: 0.00002758
Iteration 52/1000 | Loss: 0.00002758
Iteration 53/1000 | Loss: 0.00002758
Iteration 54/1000 | Loss: 0.00002757
Iteration 55/1000 | Loss: 0.00002757
Iteration 56/1000 | Loss: 0.00002757
Iteration 57/1000 | Loss: 0.00002756
Iteration 58/1000 | Loss: 0.00002755
Iteration 59/1000 | Loss: 0.00002755
Iteration 60/1000 | Loss: 0.00002754
Iteration 61/1000 | Loss: 0.00002754
Iteration 62/1000 | Loss: 0.00002754
Iteration 63/1000 | Loss: 0.00002754
Iteration 64/1000 | Loss: 0.00002754
Iteration 65/1000 | Loss: 0.00002754
Iteration 66/1000 | Loss: 0.00002753
Iteration 67/1000 | Loss: 0.00002753
Iteration 68/1000 | Loss: 0.00002753
Iteration 69/1000 | Loss: 0.00002753
Iteration 70/1000 | Loss: 0.00002752
Iteration 71/1000 | Loss: 0.00002752
Iteration 72/1000 | Loss: 0.00002752
Iteration 73/1000 | Loss: 0.00002752
Iteration 74/1000 | Loss: 0.00002751
Iteration 75/1000 | Loss: 0.00002751
Iteration 76/1000 | Loss: 0.00002750
Iteration 77/1000 | Loss: 0.00002750
Iteration 78/1000 | Loss: 0.00002750
Iteration 79/1000 | Loss: 0.00002750
Iteration 80/1000 | Loss: 0.00002750
Iteration 81/1000 | Loss: 0.00002750
Iteration 82/1000 | Loss: 0.00002750
Iteration 83/1000 | Loss: 0.00002750
Iteration 84/1000 | Loss: 0.00002750
Iteration 85/1000 | Loss: 0.00002749
Iteration 86/1000 | Loss: 0.00002749
Iteration 87/1000 | Loss: 0.00002749
Iteration 88/1000 | Loss: 0.00002749
Iteration 89/1000 | Loss: 0.00002749
Iteration 90/1000 | Loss: 0.00002749
Iteration 91/1000 | Loss: 0.00002748
Iteration 92/1000 | Loss: 0.00002748
Iteration 93/1000 | Loss: 0.00002748
Iteration 94/1000 | Loss: 0.00002747
Iteration 95/1000 | Loss: 0.00002747
Iteration 96/1000 | Loss: 0.00002747
Iteration 97/1000 | Loss: 0.00002747
Iteration 98/1000 | Loss: 0.00002747
Iteration 99/1000 | Loss: 0.00002747
Iteration 100/1000 | Loss: 0.00002747
Iteration 101/1000 | Loss: 0.00002747
Iteration 102/1000 | Loss: 0.00002747
Iteration 103/1000 | Loss: 0.00002747
Iteration 104/1000 | Loss: 0.00002746
Iteration 105/1000 | Loss: 0.00002746
Iteration 106/1000 | Loss: 0.00002746
Iteration 107/1000 | Loss: 0.00002746
Iteration 108/1000 | Loss: 0.00002746
Iteration 109/1000 | Loss: 0.00002746
Iteration 110/1000 | Loss: 0.00002746
Iteration 111/1000 | Loss: 0.00002746
Iteration 112/1000 | Loss: 0.00002746
Iteration 113/1000 | Loss: 0.00002746
Iteration 114/1000 | Loss: 0.00002746
Iteration 115/1000 | Loss: 0.00002746
Iteration 116/1000 | Loss: 0.00002746
Iteration 117/1000 | Loss: 0.00002746
Iteration 118/1000 | Loss: 0.00002746
Iteration 119/1000 | Loss: 0.00002746
Iteration 120/1000 | Loss: 0.00002746
Iteration 121/1000 | Loss: 0.00002746
Iteration 122/1000 | Loss: 0.00002746
Iteration 123/1000 | Loss: 0.00002746
Iteration 124/1000 | Loss: 0.00002746
Iteration 125/1000 | Loss: 0.00002746
Iteration 126/1000 | Loss: 0.00002746
Iteration 127/1000 | Loss: 0.00002746
Iteration 128/1000 | Loss: 0.00002746
Iteration 129/1000 | Loss: 0.00002746
Iteration 130/1000 | Loss: 0.00002746
Iteration 131/1000 | Loss: 0.00002746
Iteration 132/1000 | Loss: 0.00002746
Iteration 133/1000 | Loss: 0.00002746
Iteration 134/1000 | Loss: 0.00002746
Iteration 135/1000 | Loss: 0.00002746
Iteration 136/1000 | Loss: 0.00002746
Iteration 137/1000 | Loss: 0.00002746
Iteration 138/1000 | Loss: 0.00002746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.7464477170724422e-05, 2.7464477170724422e-05, 2.7464477170724422e-05, 2.7464477170724422e-05, 2.7464477170724422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7464477170724422e-05

Optimization complete. Final v2v error: 4.324436664581299 mm

Highest mean error: 5.046886444091797 mm for frame 96

Lowest mean error: 3.82277774810791 mm for frame 144

Saving results

Total time: 1108.794946193695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0005
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812139
Iteration 2/25 | Loss: 0.00122788
Iteration 3/25 | Loss: 0.00080292
Iteration 4/25 | Loss: 0.00079087
Iteration 5/25 | Loss: 0.00078050
Iteration 6/25 | Loss: 0.00075380
Iteration 7/25 | Loss: 0.00069601
Iteration 8/25 | Loss: 0.00067970
Iteration 9/25 | Loss: 0.00065018
Iteration 10/25 | Loss: 0.00066403
Iteration 11/25 | Loss: 0.00064684
Iteration 12/25 | Loss: 0.00064462
Iteration 13/25 | Loss: 0.00063732
Iteration 14/25 | Loss: 0.00063699
Iteration 15/25 | Loss: 0.00063397
Iteration 16/25 | Loss: 0.00063294
Iteration 17/25 | Loss: 0.00063245
Iteration 18/25 | Loss: 0.00063054
Iteration 19/25 | Loss: 0.00062645
Iteration 20/25 | Loss: 0.00062515
Iteration 21/25 | Loss: 0.00062452
Iteration 22/25 | Loss: 0.00061918
Iteration 23/25 | Loss: 0.00063090
Iteration 24/25 | Loss: 0.00062997
Iteration 25/25 | Loss: 0.00063592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23454332
Iteration 2/25 | Loss: 0.00087631
Iteration 3/25 | Loss: 0.00087631
Iteration 4/25 | Loss: 0.00087631
Iteration 5/25 | Loss: 0.00087631
Iteration 6/25 | Loss: 0.00087631
Iteration 7/25 | Loss: 0.00087631
Iteration 8/25 | Loss: 0.00087631
Iteration 9/25 | Loss: 0.00087631
Iteration 10/25 | Loss: 0.00087631
Iteration 11/25 | Loss: 0.00087631
Iteration 12/25 | Loss: 0.00087631
Iteration 13/25 | Loss: 0.00087631
Iteration 14/25 | Loss: 0.00087631
Iteration 15/25 | Loss: 0.00087631
Iteration 16/25 | Loss: 0.00087631
Iteration 17/25 | Loss: 0.00087631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008763055666349828, 0.0008763055666349828, 0.0008763055666349828, 0.0008763055666349828, 0.0008763055666349828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008763055666349828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087631
Iteration 2/1000 | Loss: 0.00092678
Iteration 3/1000 | Loss: 0.00068942
Iteration 4/1000 | Loss: 0.00032089
Iteration 5/1000 | Loss: 0.00408467
Iteration 6/1000 | Loss: 0.00296029
Iteration 7/1000 | Loss: 0.00008025
Iteration 8/1000 | Loss: 0.00113147
Iteration 9/1000 | Loss: 0.00030633
Iteration 10/1000 | Loss: 0.00040624
Iteration 11/1000 | Loss: 0.00021972
Iteration 12/1000 | Loss: 0.00003732
Iteration 13/1000 | Loss: 0.00021361
Iteration 14/1000 | Loss: 0.00003043
Iteration 15/1000 | Loss: 0.00002526
Iteration 16/1000 | Loss: 0.00002390
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00046406
Iteration 19/1000 | Loss: 0.00284484
Iteration 20/1000 | Loss: 0.00176637
Iteration 21/1000 | Loss: 0.00228653
Iteration 22/1000 | Loss: 0.00105425
Iteration 23/1000 | Loss: 0.00040960
Iteration 24/1000 | Loss: 0.00175927
Iteration 25/1000 | Loss: 0.00060999
Iteration 26/1000 | Loss: 0.00026090
Iteration 27/1000 | Loss: 0.00042507
Iteration 28/1000 | Loss: 0.00040328
Iteration 29/1000 | Loss: 0.00002970
Iteration 30/1000 | Loss: 0.00002340
Iteration 31/1000 | Loss: 0.00002049
Iteration 32/1000 | Loss: 0.00125805
Iteration 33/1000 | Loss: 0.00003218
Iteration 34/1000 | Loss: 0.00018038
Iteration 35/1000 | Loss: 0.00001935
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001622
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001578
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001562
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001561
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001559
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001553
Iteration 115/1000 | Loss: 0.00001553
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001553
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001553
Iteration 122/1000 | Loss: 0.00001553
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Iteration 134/1000 | Loss: 0.00001552
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001552
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001552
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001552
Iteration 142/1000 | Loss: 0.00001552
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001552
Iteration 147/1000 | Loss: 0.00001552
Iteration 148/1000 | Loss: 0.00001552
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001551
Iteration 155/1000 | Loss: 0.00001551
Iteration 156/1000 | Loss: 0.00001551
Iteration 157/1000 | Loss: 0.00001551
Iteration 158/1000 | Loss: 0.00001551
Iteration 159/1000 | Loss: 0.00001551
Iteration 160/1000 | Loss: 0.00001551
Iteration 161/1000 | Loss: 0.00001551
Iteration 162/1000 | Loss: 0.00001551
Iteration 163/1000 | Loss: 0.00001551
Iteration 164/1000 | Loss: 0.00001551
Iteration 165/1000 | Loss: 0.00001551
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001551
Iteration 168/1000 | Loss: 0.00001550
Iteration 169/1000 | Loss: 0.00001550
Iteration 170/1000 | Loss: 0.00001550
Iteration 171/1000 | Loss: 0.00001550
Iteration 172/1000 | Loss: 0.00001550
Iteration 173/1000 | Loss: 0.00001550
Iteration 174/1000 | Loss: 0.00001550
Iteration 175/1000 | Loss: 0.00001550
Iteration 176/1000 | Loss: 0.00001550
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001550
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001550
Iteration 184/1000 | Loss: 0.00001550
Iteration 185/1000 | Loss: 0.00001549
Iteration 186/1000 | Loss: 0.00001549
Iteration 187/1000 | Loss: 0.00001549
Iteration 188/1000 | Loss: 0.00001549
Iteration 189/1000 | Loss: 0.00001549
Iteration 190/1000 | Loss: 0.00001549
Iteration 191/1000 | Loss: 0.00001549
Iteration 192/1000 | Loss: 0.00001549
Iteration 193/1000 | Loss: 0.00001549
Iteration 194/1000 | Loss: 0.00001549
Iteration 195/1000 | Loss: 0.00001548
Iteration 196/1000 | Loss: 0.00001548
Iteration 197/1000 | Loss: 0.00001548
Iteration 198/1000 | Loss: 0.00001548
Iteration 199/1000 | Loss: 0.00001548
Iteration 200/1000 | Loss: 0.00001548
Iteration 201/1000 | Loss: 0.00001548
Iteration 202/1000 | Loss: 0.00001548
Iteration 203/1000 | Loss: 0.00001548
Iteration 204/1000 | Loss: 0.00001548
Iteration 205/1000 | Loss: 0.00001548
Iteration 206/1000 | Loss: 0.00001548
Iteration 207/1000 | Loss: 0.00001548
Iteration 208/1000 | Loss: 0.00001548
Iteration 209/1000 | Loss: 0.00001548
Iteration 210/1000 | Loss: 0.00001548
Iteration 211/1000 | Loss: 0.00001547
Iteration 212/1000 | Loss: 0.00001547
Iteration 213/1000 | Loss: 0.00001547
Iteration 214/1000 | Loss: 0.00001547
Iteration 215/1000 | Loss: 0.00001547
Iteration 216/1000 | Loss: 0.00001547
Iteration 217/1000 | Loss: 0.00001547
Iteration 218/1000 | Loss: 0.00001547
Iteration 219/1000 | Loss: 0.00001546
Iteration 220/1000 | Loss: 0.00001546
Iteration 221/1000 | Loss: 0.00001546
Iteration 222/1000 | Loss: 0.00001546
Iteration 223/1000 | Loss: 0.00001546
Iteration 224/1000 | Loss: 0.00001546
Iteration 225/1000 | Loss: 0.00001546
Iteration 226/1000 | Loss: 0.00001545
Iteration 227/1000 | Loss: 0.00001545
Iteration 228/1000 | Loss: 0.00001545
Iteration 229/1000 | Loss: 0.00001545
Iteration 230/1000 | Loss: 0.00001545
Iteration 231/1000 | Loss: 0.00001545
Iteration 232/1000 | Loss: 0.00001545
Iteration 233/1000 | Loss: 0.00001545
Iteration 234/1000 | Loss: 0.00001545
Iteration 235/1000 | Loss: 0.00001545
Iteration 236/1000 | Loss: 0.00001545
Iteration 237/1000 | Loss: 0.00001545
Iteration 238/1000 | Loss: 0.00001545
Iteration 239/1000 | Loss: 0.00001545
Iteration 240/1000 | Loss: 0.00001545
Iteration 241/1000 | Loss: 0.00001545
Iteration 242/1000 | Loss: 0.00001545
Iteration 243/1000 | Loss: 0.00001545
Iteration 244/1000 | Loss: 0.00001545
Iteration 245/1000 | Loss: 0.00001545
Iteration 246/1000 | Loss: 0.00001545
Iteration 247/1000 | Loss: 0.00001545
Iteration 248/1000 | Loss: 0.00001545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.544912447570823e-05, 1.544912447570823e-05, 1.544912447570823e-05, 1.544912447570823e-05, 1.544912447570823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.544912447570823e-05

Optimization complete. Final v2v error: 3.264240026473999 mm

Highest mean error: 10.334606170654297 mm for frame 48

Lowest mean error: 2.645904064178467 mm for frame 217

Saving results

Total time: 4205.4851660728455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0010
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081418
Iteration 2/25 | Loss: 0.00428169
Iteration 3/25 | Loss: 0.00305901
Iteration 4/25 | Loss: 0.00205835
Iteration 5/25 | Loss: 0.00134135
Iteration 6/25 | Loss: 0.00117713
Iteration 7/25 | Loss: 0.00101795
Iteration 8/25 | Loss: 0.00091562
Iteration 9/25 | Loss: 0.00089787
Iteration 10/25 | Loss: 0.00080183
Iteration 11/25 | Loss: 0.00076383
Iteration 12/25 | Loss: 0.00071972
Iteration 13/25 | Loss: 0.00070112
Iteration 14/25 | Loss: 0.00068992
Iteration 15/25 | Loss: 0.00068678
Iteration 16/25 | Loss: 0.00068584
Iteration 17/25 | Loss: 0.00068548
Iteration 18/25 | Loss: 0.00068527
Iteration 19/25 | Loss: 0.00068528
Iteration 20/25 | Loss: 0.00068517
Iteration 21/25 | Loss: 0.00068517
Iteration 22/25 | Loss: 0.00068517
Iteration 23/25 | Loss: 0.00068517
Iteration 24/25 | Loss: 0.00068517
Iteration 25/25 | Loss: 0.00068517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36096036
Iteration 2/25 | Loss: 0.00021126
Iteration 3/25 | Loss: 0.00015519
Iteration 4/25 | Loss: 0.00015519
Iteration 5/25 | Loss: 0.00015519
Iteration 6/25 | Loss: 0.00015519
Iteration 7/25 | Loss: 0.00015519
Iteration 8/25 | Loss: 0.00015519
Iteration 9/25 | Loss: 0.00015519
Iteration 10/25 | Loss: 0.00015519
Iteration 11/25 | Loss: 0.00015519
Iteration 12/25 | Loss: 0.00015519
Iteration 13/25 | Loss: 0.00015519
Iteration 14/25 | Loss: 0.00015519
Iteration 15/25 | Loss: 0.00015519
Iteration 16/25 | Loss: 0.00015519
Iteration 17/25 | Loss: 0.00015519
Iteration 18/25 | Loss: 0.00015519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00015518769214395434, 0.00015518769214395434, 0.00015518769214395434, 0.00015518769214395434, 0.00015518769214395434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015518769214395434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015519
Iteration 2/1000 | Loss: 0.00010195
Iteration 3/1000 | Loss: 0.00008884
Iteration 4/1000 | Loss: 0.00016021
Iteration 5/1000 | Loss: 0.00002937
Iteration 6/1000 | Loss: 0.00017569
Iteration 7/1000 | Loss: 0.00002699
Iteration 8/1000 | Loss: 0.00002608
Iteration 9/1000 | Loss: 0.00002521
Iteration 10/1000 | Loss: 0.00002486
Iteration 11/1000 | Loss: 0.00002464
Iteration 12/1000 | Loss: 0.00002453
Iteration 13/1000 | Loss: 0.00002441
Iteration 14/1000 | Loss: 0.00002426
Iteration 15/1000 | Loss: 0.00002424
Iteration 16/1000 | Loss: 0.00002420
Iteration 17/1000 | Loss: 0.00002418
Iteration 18/1000 | Loss: 0.00002409
Iteration 19/1000 | Loss: 0.00002408
Iteration 20/1000 | Loss: 0.00002402
Iteration 21/1000 | Loss: 0.00002401
Iteration 22/1000 | Loss: 0.00002399
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002398
Iteration 25/1000 | Loss: 0.00002398
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00002396
Iteration 28/1000 | Loss: 0.00002394
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002393
Iteration 31/1000 | Loss: 0.00002393
Iteration 32/1000 | Loss: 0.00002392
Iteration 33/1000 | Loss: 0.00002391
Iteration 34/1000 | Loss: 0.00002390
Iteration 35/1000 | Loss: 0.00002390
Iteration 36/1000 | Loss: 0.00002389
Iteration 37/1000 | Loss: 0.00002389
Iteration 38/1000 | Loss: 0.00008532
Iteration 39/1000 | Loss: 0.00002393
Iteration 40/1000 | Loss: 0.00002390
Iteration 41/1000 | Loss: 0.00002382
Iteration 42/1000 | Loss: 0.00002381
Iteration 43/1000 | Loss: 0.00002380
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002380
Iteration 46/1000 | Loss: 0.00002380
Iteration 47/1000 | Loss: 0.00002380
Iteration 48/1000 | Loss: 0.00002380
Iteration 49/1000 | Loss: 0.00002380
Iteration 50/1000 | Loss: 0.00002379
Iteration 51/1000 | Loss: 0.00002379
Iteration 52/1000 | Loss: 0.00002379
Iteration 53/1000 | Loss: 0.00002379
Iteration 54/1000 | Loss: 0.00002379
Iteration 55/1000 | Loss: 0.00002378
Iteration 56/1000 | Loss: 0.00002378
Iteration 57/1000 | Loss: 0.00002378
Iteration 58/1000 | Loss: 0.00002378
Iteration 59/1000 | Loss: 0.00002378
Iteration 60/1000 | Loss: 0.00002378
Iteration 61/1000 | Loss: 0.00002378
Iteration 62/1000 | Loss: 0.00002378
Iteration 63/1000 | Loss: 0.00002378
Iteration 64/1000 | Loss: 0.00002377
Iteration 65/1000 | Loss: 0.00002377
Iteration 66/1000 | Loss: 0.00002376
Iteration 67/1000 | Loss: 0.00002376
Iteration 68/1000 | Loss: 0.00002376
Iteration 69/1000 | Loss: 0.00002376
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002376
Iteration 72/1000 | Loss: 0.00002376
Iteration 73/1000 | Loss: 0.00002375
Iteration 74/1000 | Loss: 0.00002375
Iteration 75/1000 | Loss: 0.00002375
Iteration 76/1000 | Loss: 0.00002375
Iteration 77/1000 | Loss: 0.00002375
Iteration 78/1000 | Loss: 0.00002375
Iteration 79/1000 | Loss: 0.00002375
Iteration 80/1000 | Loss: 0.00002375
Iteration 81/1000 | Loss: 0.00002374
Iteration 82/1000 | Loss: 0.00002374
Iteration 83/1000 | Loss: 0.00002374
Iteration 84/1000 | Loss: 0.00002374
Iteration 85/1000 | Loss: 0.00002374
Iteration 86/1000 | Loss: 0.00002374
Iteration 87/1000 | Loss: 0.00002373
Iteration 88/1000 | Loss: 0.00002373
Iteration 89/1000 | Loss: 0.00002373
Iteration 90/1000 | Loss: 0.00002373
Iteration 91/1000 | Loss: 0.00002373
Iteration 92/1000 | Loss: 0.00002373
Iteration 93/1000 | Loss: 0.00002373
Iteration 94/1000 | Loss: 0.00002373
Iteration 95/1000 | Loss: 0.00002373
Iteration 96/1000 | Loss: 0.00002373
Iteration 97/1000 | Loss: 0.00002373
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002372
Iteration 100/1000 | Loss: 0.00002372
Iteration 101/1000 | Loss: 0.00002372
Iteration 102/1000 | Loss: 0.00002372
Iteration 103/1000 | Loss: 0.00002372
Iteration 104/1000 | Loss: 0.00002372
Iteration 105/1000 | Loss: 0.00002372
Iteration 106/1000 | Loss: 0.00002372
Iteration 107/1000 | Loss: 0.00002372
Iteration 108/1000 | Loss: 0.00002372
Iteration 109/1000 | Loss: 0.00002372
Iteration 110/1000 | Loss: 0.00002372
Iteration 111/1000 | Loss: 0.00002372
Iteration 112/1000 | Loss: 0.00002372
Iteration 113/1000 | Loss: 0.00002372
Iteration 114/1000 | Loss: 0.00002372
Iteration 115/1000 | Loss: 0.00002372
Iteration 116/1000 | Loss: 0.00002372
Iteration 117/1000 | Loss: 0.00002372
Iteration 118/1000 | Loss: 0.00002372
Iteration 119/1000 | Loss: 0.00002372
Iteration 120/1000 | Loss: 0.00002372
Iteration 121/1000 | Loss: 0.00002372
Iteration 122/1000 | Loss: 0.00002372
Iteration 123/1000 | Loss: 0.00002372
Iteration 124/1000 | Loss: 0.00002372
Iteration 125/1000 | Loss: 0.00002372
Iteration 126/1000 | Loss: 0.00002372
Iteration 127/1000 | Loss: 0.00002372
Iteration 128/1000 | Loss: 0.00002372
Iteration 129/1000 | Loss: 0.00002372
Iteration 130/1000 | Loss: 0.00002372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.371528353251051e-05, 2.371528353251051e-05, 2.371528353251051e-05, 2.371528353251051e-05, 2.371528353251051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.371528353251051e-05

Optimization complete. Final v2v error: 4.123355388641357 mm

Highest mean error: 9.100852966308594 mm for frame 98

Lowest mean error: 3.747588634490967 mm for frame 143

Saving results

Total time: 2510.5363154411316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0004
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968559
Iteration 2/25 | Loss: 0.00117950
Iteration 3/25 | Loss: 0.00075304
Iteration 4/25 | Loss: 0.00071230
Iteration 5/25 | Loss: 0.00069321
Iteration 6/25 | Loss: 0.00069062
Iteration 7/25 | Loss: 0.00069006
Iteration 8/25 | Loss: 0.00069006
Iteration 9/25 | Loss: 0.00069006
Iteration 10/25 | Loss: 0.00069006
Iteration 11/25 | Loss: 0.00069006
Iteration 12/25 | Loss: 0.00069006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006900604930706322, 0.0006900604930706322, 0.0006900604930706322, 0.0006900604930706322, 0.0006900604930706322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006900604930706322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97370863
Iteration 2/25 | Loss: 0.00018695
Iteration 3/25 | Loss: 0.00018695
Iteration 4/25 | Loss: 0.00018695
Iteration 5/25 | Loss: 0.00018695
Iteration 6/25 | Loss: 0.00018695
Iteration 7/25 | Loss: 0.00018695
Iteration 8/25 | Loss: 0.00018695
Iteration 9/25 | Loss: 0.00018695
Iteration 10/25 | Loss: 0.00018695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0001869471016107127, 0.0001869471016107127, 0.0001869471016107127, 0.0001869471016107127, 0.0001869471016107127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001869471016107127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018695
Iteration 2/1000 | Loss: 0.00004115
Iteration 3/1000 | Loss: 0.00003097
Iteration 4/1000 | Loss: 0.00002889
Iteration 5/1000 | Loss: 0.00002785
Iteration 6/1000 | Loss: 0.00002723
Iteration 7/1000 | Loss: 0.00002685
Iteration 8/1000 | Loss: 0.00002644
Iteration 9/1000 | Loss: 0.00002625
Iteration 10/1000 | Loss: 0.00002607
Iteration 11/1000 | Loss: 0.00002591
Iteration 12/1000 | Loss: 0.00002589
Iteration 13/1000 | Loss: 0.00002588
Iteration 14/1000 | Loss: 0.00002587
Iteration 15/1000 | Loss: 0.00002583
Iteration 16/1000 | Loss: 0.00002579
Iteration 17/1000 | Loss: 0.00002579
Iteration 18/1000 | Loss: 0.00002578
Iteration 19/1000 | Loss: 0.00002577
Iteration 20/1000 | Loss: 0.00002574
Iteration 21/1000 | Loss: 0.00002570
Iteration 22/1000 | Loss: 0.00002569
Iteration 23/1000 | Loss: 0.00002568
Iteration 24/1000 | Loss: 0.00002567
Iteration 25/1000 | Loss: 0.00002567
Iteration 26/1000 | Loss: 0.00002567
Iteration 27/1000 | Loss: 0.00002567
Iteration 28/1000 | Loss: 0.00002567
Iteration 29/1000 | Loss: 0.00002567
Iteration 30/1000 | Loss: 0.00002567
Iteration 31/1000 | Loss: 0.00002567
Iteration 32/1000 | Loss: 0.00002567
Iteration 33/1000 | Loss: 0.00002567
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002566
Iteration 36/1000 | Loss: 0.00002566
Iteration 37/1000 | Loss: 0.00002566
Iteration 38/1000 | Loss: 0.00002566
Iteration 39/1000 | Loss: 0.00002566
Iteration 40/1000 | Loss: 0.00002566
Iteration 41/1000 | Loss: 0.00002565
Iteration 42/1000 | Loss: 0.00002565
Iteration 43/1000 | Loss: 0.00002564
Iteration 44/1000 | Loss: 0.00002564
Iteration 45/1000 | Loss: 0.00002564
Iteration 46/1000 | Loss: 0.00002564
Iteration 47/1000 | Loss: 0.00002564
Iteration 48/1000 | Loss: 0.00002563
Iteration 49/1000 | Loss: 0.00002563
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00002563
Iteration 52/1000 | Loss: 0.00002563
Iteration 53/1000 | Loss: 0.00002562
Iteration 54/1000 | Loss: 0.00002561
Iteration 55/1000 | Loss: 0.00002561
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002560
Iteration 58/1000 | Loss: 0.00002560
Iteration 59/1000 | Loss: 0.00002559
Iteration 60/1000 | Loss: 0.00002559
Iteration 61/1000 | Loss: 0.00002558
Iteration 62/1000 | Loss: 0.00002558
Iteration 63/1000 | Loss: 0.00002557
Iteration 64/1000 | Loss: 0.00002557
Iteration 65/1000 | Loss: 0.00002557
Iteration 66/1000 | Loss: 0.00002557
Iteration 67/1000 | Loss: 0.00002556
Iteration 68/1000 | Loss: 0.00002555
Iteration 69/1000 | Loss: 0.00002555
Iteration 70/1000 | Loss: 0.00002555
Iteration 71/1000 | Loss: 0.00002554
Iteration 72/1000 | Loss: 0.00002554
Iteration 73/1000 | Loss: 0.00002554
Iteration 74/1000 | Loss: 0.00002554
Iteration 75/1000 | Loss: 0.00002554
Iteration 76/1000 | Loss: 0.00002554
Iteration 77/1000 | Loss: 0.00002554
Iteration 78/1000 | Loss: 0.00002554
Iteration 79/1000 | Loss: 0.00002554
Iteration 80/1000 | Loss: 0.00002554
Iteration 81/1000 | Loss: 0.00002554
Iteration 82/1000 | Loss: 0.00002553
Iteration 83/1000 | Loss: 0.00002553
Iteration 84/1000 | Loss: 0.00002553
Iteration 85/1000 | Loss: 0.00002553
Iteration 86/1000 | Loss: 0.00002553
Iteration 87/1000 | Loss: 0.00002553
Iteration 88/1000 | Loss: 0.00002553
Iteration 89/1000 | Loss: 0.00002553
Iteration 90/1000 | Loss: 0.00002552
Iteration 91/1000 | Loss: 0.00002551
Iteration 92/1000 | Loss: 0.00002550
Iteration 93/1000 | Loss: 0.00002550
Iteration 94/1000 | Loss: 0.00002550
Iteration 95/1000 | Loss: 0.00002550
Iteration 96/1000 | Loss: 0.00002550
Iteration 97/1000 | Loss: 0.00002550
Iteration 98/1000 | Loss: 0.00002550
Iteration 99/1000 | Loss: 0.00002550
Iteration 100/1000 | Loss: 0.00002550
Iteration 101/1000 | Loss: 0.00002550
Iteration 102/1000 | Loss: 0.00002550
Iteration 103/1000 | Loss: 0.00002548
Iteration 104/1000 | Loss: 0.00002548
Iteration 105/1000 | Loss: 0.00002548
Iteration 106/1000 | Loss: 0.00002548
Iteration 107/1000 | Loss: 0.00002548
Iteration 108/1000 | Loss: 0.00002548
Iteration 109/1000 | Loss: 0.00002548
Iteration 110/1000 | Loss: 0.00002548
Iteration 111/1000 | Loss: 0.00002548
Iteration 112/1000 | Loss: 0.00002548
Iteration 113/1000 | Loss: 0.00002548
Iteration 114/1000 | Loss: 0.00002547
Iteration 115/1000 | Loss: 0.00002546
Iteration 116/1000 | Loss: 0.00002546
Iteration 117/1000 | Loss: 0.00002545
Iteration 118/1000 | Loss: 0.00002545
Iteration 119/1000 | Loss: 0.00002545
Iteration 120/1000 | Loss: 0.00002544
Iteration 121/1000 | Loss: 0.00002544
Iteration 122/1000 | Loss: 0.00002544
Iteration 123/1000 | Loss: 0.00002544
Iteration 124/1000 | Loss: 0.00002544
Iteration 125/1000 | Loss: 0.00002544
Iteration 126/1000 | Loss: 0.00002544
Iteration 127/1000 | Loss: 0.00002544
Iteration 128/1000 | Loss: 0.00002544
Iteration 129/1000 | Loss: 0.00002544
Iteration 130/1000 | Loss: 0.00002543
Iteration 131/1000 | Loss: 0.00002543
Iteration 132/1000 | Loss: 0.00002543
Iteration 133/1000 | Loss: 0.00002542
Iteration 134/1000 | Loss: 0.00002542
Iteration 135/1000 | Loss: 0.00002542
Iteration 136/1000 | Loss: 0.00002541
Iteration 137/1000 | Loss: 0.00002541
Iteration 138/1000 | Loss: 0.00002541
Iteration 139/1000 | Loss: 0.00002540
Iteration 140/1000 | Loss: 0.00002540
Iteration 141/1000 | Loss: 0.00002540
Iteration 142/1000 | Loss: 0.00002540
Iteration 143/1000 | Loss: 0.00002539
Iteration 144/1000 | Loss: 0.00002539
Iteration 145/1000 | Loss: 0.00002539
Iteration 146/1000 | Loss: 0.00002539
Iteration 147/1000 | Loss: 0.00002539
Iteration 148/1000 | Loss: 0.00002539
Iteration 149/1000 | Loss: 0.00002539
Iteration 150/1000 | Loss: 0.00002538
Iteration 151/1000 | Loss: 0.00002538
Iteration 152/1000 | Loss: 0.00002538
Iteration 153/1000 | Loss: 0.00002538
Iteration 154/1000 | Loss: 0.00002538
Iteration 155/1000 | Loss: 0.00002538
Iteration 156/1000 | Loss: 0.00002538
Iteration 157/1000 | Loss: 0.00002538
Iteration 158/1000 | Loss: 0.00002538
Iteration 159/1000 | Loss: 0.00002538
Iteration 160/1000 | Loss: 0.00002538
Iteration 161/1000 | Loss: 0.00002538
Iteration 162/1000 | Loss: 0.00002538
Iteration 163/1000 | Loss: 0.00002538
Iteration 164/1000 | Loss: 0.00002538
Iteration 165/1000 | Loss: 0.00002538
Iteration 166/1000 | Loss: 0.00002537
Iteration 167/1000 | Loss: 0.00002537
Iteration 168/1000 | Loss: 0.00002537
Iteration 169/1000 | Loss: 0.00002537
Iteration 170/1000 | Loss: 0.00002537
Iteration 171/1000 | Loss: 0.00002536
Iteration 172/1000 | Loss: 0.00002536
Iteration 173/1000 | Loss: 0.00002536
Iteration 174/1000 | Loss: 0.00002536
Iteration 175/1000 | Loss: 0.00002536
Iteration 176/1000 | Loss: 0.00002536
Iteration 177/1000 | Loss: 0.00002535
Iteration 178/1000 | Loss: 0.00002535
Iteration 179/1000 | Loss: 0.00002535
Iteration 180/1000 | Loss: 0.00002535
Iteration 181/1000 | Loss: 0.00002535
Iteration 182/1000 | Loss: 0.00002535
Iteration 183/1000 | Loss: 0.00002535
Iteration 184/1000 | Loss: 0.00002535
Iteration 185/1000 | Loss: 0.00002535
Iteration 186/1000 | Loss: 0.00002535
Iteration 187/1000 | Loss: 0.00002535
Iteration 188/1000 | Loss: 0.00002534
Iteration 189/1000 | Loss: 0.00002534
Iteration 190/1000 | Loss: 0.00002534
Iteration 191/1000 | Loss: 0.00002534
Iteration 192/1000 | Loss: 0.00002534
Iteration 193/1000 | Loss: 0.00002534
Iteration 194/1000 | Loss: 0.00002534
Iteration 195/1000 | Loss: 0.00002534
Iteration 196/1000 | Loss: 0.00002533
Iteration 197/1000 | Loss: 0.00002533
Iteration 198/1000 | Loss: 0.00002533
Iteration 199/1000 | Loss: 0.00002533
Iteration 200/1000 | Loss: 0.00002533
Iteration 201/1000 | Loss: 0.00002533
Iteration 202/1000 | Loss: 0.00002533
Iteration 203/1000 | Loss: 0.00002533
Iteration 204/1000 | Loss: 0.00002533
Iteration 205/1000 | Loss: 0.00002533
Iteration 206/1000 | Loss: 0.00002533
Iteration 207/1000 | Loss: 0.00002533
Iteration 208/1000 | Loss: 0.00002533
Iteration 209/1000 | Loss: 0.00002533
Iteration 210/1000 | Loss: 0.00002533
Iteration 211/1000 | Loss: 0.00002533
Iteration 212/1000 | Loss: 0.00002533
Iteration 213/1000 | Loss: 0.00002533
Iteration 214/1000 | Loss: 0.00002533
Iteration 215/1000 | Loss: 0.00002533
Iteration 216/1000 | Loss: 0.00002533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.5332541554234922e-05, 2.5332541554234922e-05, 2.5332541554234922e-05, 2.5332541554234922e-05, 2.5332541554234922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5332541554234922e-05

Optimization complete. Final v2v error: 4.16999626159668 mm

Highest mean error: 5.005717754364014 mm for frame 79

Lowest mean error: 3.5931034088134766 mm for frame 170

Saving results

Total time: 1049.0566420555115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_28_us_1430/0017/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/male_28_us_1430/0017. Skipping.
